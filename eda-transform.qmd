```{r echo = FALSE}
#| message: false
#| warning: false
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc, quantreg,
               multcomp, emmeans, ggpubr, multcompView, nlme, tinytable,
               see, patchwork, ggrepel, conflicted)
conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
```

```{r}
#| echo: false
#| message: false
#| warning: false
source("images/R/eda-transform.R")
```

# Transformieren von Daten {#sec-eda-transform}

*Letzte Änderung am `r format(fs::file_info("eda-transform.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"It's morphin' time! I never get tired of that! Now, go, go Power Rangers!" --- Power Rangers morphing phrase*

![](images/caution.png){fig-align="center" width="100%"}

::: {.callout-caution appearance="simple"}
## Stand des Kapitels: Baustelle (seit 06.2025)

Dieses Kapitel wird überarbeitet, da ich nochmal an den Kern der Transformation ran möchte. Daher kann es sein, dass es in den nächsten Wochen hier zu Problemen und nicht funktionierenden Code kommt. Das repariere ich dann aber meistens in den folgenden Tagen. Ziel ist es bis zum Start des WiSe 2025/26 das Kapitel neu aufgestellt zu haben.
:::

text

## Allgemeiner Hintergrund

Warum müssen wir Daten transformieren? Meistens hat dies drei Hauptgründe.

#### Das Modell {.unnumbered .unlisted}

1)  Wir wollen eine ANOVA oder eine Gaussian lineare Regression rechen und benötigen ein normalverteiltes Outcome $y$.
2)  Wir wollen einen Algorithmus zur Prädiktion (deu. *Vorhersage*) nutzen und haben sehr viele Einflussvariablen $x$ in sehr unterschiedlichen Einheiten.
3)  Wir wollen eine komplexere Analyse wie die Hauptkomponentenanalyse oder Clusteranalyse rechnen und brauchen Variablen, die alle eine ähnliche Spannweite haben.

![*foo.* Quelle: @yang2014duration](images/preface_transformation.jpeg){#fig-utest-intro fig-align="center" width="100%"}

Im ersten Fall wollen wir meist unsere Daten $log$-Transformieren um aus einem *nicht*-normalverteilten Outcome $y$ ein $log$-normalverteiltes $y$ zu erschaffen. Im zweiten Fall wollen wir unsere Daten Standardisieren oder Normalisieren. Wir brauchen normalisierte Daten später beim Klassifizieren im Rahmen von maschinellen Lernverfahren. Bitte beachte auch, dass die Transformationen hier eher für kleine Datensätze geeignet sind. Im [Kapitel zur Klassifikation](#sec-pre-processing) gehe ich nochmal auf die Automatisierung über mehrere Variablen ein. Wenn du also einen großen Datensatz hast, den du vielleicht oft bearbeiten musst, dann mag dir dort mehr geholfen sein.

|   $\boldsymbol{y}$    | $\boldsymbol{\lambda}$ |
|:---------------------:|:----------------------:|
|   $\cfrac{1}{x^2}$    |          $-2$          |
|    $\cfrac{1}{x}$     |          $-1$          |
| $\cfrac{1}{\sqrt{x}}$ |         $-1/2$         |
|       $\log{x}$       |          $0$           |
|      $\sqrt{x}$       |         $1/2$          |
|          $x$          |          $1$           |
|         $x^2$         |          $2$           |

: test {#tbl-tukey-ladder}

Ein Wort zum R Paket `{recipes}`

Wir wollen uns nun die Verfahren zur Transformation von Daten in den folgenden Abschnitten einmal näher anschauen.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-ggplot-utest-intro-sim-02
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "foo. **(A)** br. **(B)** fff. *[Zum Vergrößern anklicken]*"

p1_intro_pop + p2_intro_pop +
  plot_layout(ncol = 2) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

::: callout-tip
## Weitere Tutorien für das Transformieren von Daten.

Wir oben schon erwähnt, kann dieses Kapitel nicht die umfangreiche Literatur der Transformation von Daten abarbeiten. Daher präsentiere ich hier eine Liste von Literatur und Links, die mich für dieses Kapitel hier inspiriert haben. Nicht alles habe ich genutzt, aber vielleicht ist für dich was dabei.

-   Ich verweise hier auch nochmal auf das tolle Tutorium von Matus Seci auf dem [Coding Club - Transforming and scaling data: Understand the fundamental concepts of manipulating data distributions for modelling and visualization](https://ourcodingclub.github.io/tutorials/data-scaling/). Du findest auch dort mehr Informationen zu der Rücktransformation und der Anwendung von Transformationen auf einen komplexeren Datensatz.
-   [Transformations](https://onlinestatbook.com/2/transformations/contents.html)
-   [Data Transformation mit `dlookr`](https://choonghyunryu.github.io/dlookr/articles/transformation.html#standardization-and-resolving-skewness)
-   [Transforming Data](https://rcompanion.org/handbook/I_12.html)
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r}
#| message: false
#| warning: false
#| echo: true

pacman::p_load(tidyverse, magrittr, scales, see, MASS, bestNormalize,
               rcompanion, LambertW, trafo, conflicted)
conflicts_prefer(MASS::boxcox)
conflicts_prefer(dplyr::select)
conflicts_prefer(dplyr::filter)
conflicts_prefer(parameters::skewness)
conflicts_prefer(parameters::kurtosis)
conflicts_prefer(bestNormalize::boxcox)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", 
                "#009E73", "#F0E442", "#F5C710", 
                "#0072B2", "#D55E00", "#CC79A7")
```

Zwischenzeitlich musste ich das R Paket `{dlookr}` aus dem Code entfernt, da das Paket extra extern über GitHub installieren musstest. Das R Paket war zwischenzeitlich nicht mehr bei CRAN gelistet. Das führt bei mir zu Schluckauf im Code, so dass ich die Funktionen jetzt in einem Extraabschnitt bearbeite. Sollte das Paket sich mal nicht installieren lassen, dann gibt es hier noch einen alternativen Weg.

::: callout-caution
## Alternative Installation von `{dlookr}`

Ist das R Paket `{dlookr}` mal nicht per Standard aus RStudio zu installieren kannst du das R Paket auch über GitHub installieren. Dafür einmal das R Paket `{devtools}` installieren und dann folgenden Code ausführen.

```{r}
#| eval: false
devtools::install_github("choonghyunryu/dlookr")
```
:::

An der Seite des Kapitels findest du den Link *Quellcode anzeigen*, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.

## Daten

#### Schlupfzeiten von Flöhen {.unnumbered .unlisted}

Wir wollen uns in diesem Kapitel mit der normalverteilten Variable `jump_length` gemessen in \[cm\] und der nicht-normalverteilten Variable `hatch_time` gemessen in \[h\] aus dem Datensatz `flea_dog_cat_length_weight.csv"` beschäftigen. Wir wählen über die Funktion `select()` nur die beiden Spalten aus dem Datensatz, die wir benötigen.

```{r}
#| message: false

fac1_tbl <- read_excel("data/flea_dog_cat_length_weight.xlsx") |>
  select(animal, jump_length, weight, hatch_time) |> 
  filter(hatch_time <= 1500)
```

In der @tbl-trans-1 ist der Datensatz `fac1_tbl` nochmal dargestellt. Wir zeigen hier nur die ersten sieben zeilen des Datensatzes.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-trans-1
#| tbl-cap: "Selektierter Datensatz mit einer normalverteilten Variable `jump_length` und der nicht-normalverteilten Variable `hatch_time`. Wir betrachten die ersten sieben Zeilen des Datensatzes."

fac1_raw_tbl <- read_excel("data/flea_dog_cat_length_weight.xlsx") |>
  select(animal, jump_length, hatch_time) 

rbind(head(fac1_raw_tbl, n = 3),
      rep("...", times = ncol(fac1_raw_tbl)),
      tail(fac1_raw_tbl, n = 3)) |> 
  tt(width = 2/3, align = "c", theme = "striped")
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-data-intro
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "foo. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(fac1_tbl, aes(jump_length)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Sprungweiten in [cm]", y = "") +
  xlim(8, 33)

p2 <- ggplot(fac1_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in sqrt(h)", y = "") +
  xlim(-100, 1500)

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

Im Folgenden nutzen wir oft die Funktion `mutate()`. Schau dir im Zweifel nochmal im Kapitel zu Programmierung die Funktion `mutate()` an.

#### Das Alter von Tieren {.unnumbered .unlisted}

```{r}
#| message: false
#| warning: false
anage_tbl <- read_delim("data/anage_data.txt", delim = "\t") |> 
  select(name = "Common name", weight = "Adult weight (g)", 
         age = "Maximum longevity (yrs)") |> 
  mutate(weight = weight/1000) |> 
  na.omit() 
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-trans-weight-age
#| tbl-cap: "foo"

anage_raw_tbl <- read_delim("data/anage_data.txt", delim = "\t") |> 
  select("Common name", "Adult weight (g)", 
         "Maximum longevity (yrs)") |> 
  na.omit()

rbind(head(anage_raw_tbl, n = 3),
      rep("...", times = ncol(anage_raw_tbl)),
      tail(anage_raw_tbl, n = 3)) |> 
  tt(width = 2/3, align = "c", theme = "striped")
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-ggplot-weight-age
#| fig-align: center
#| fig-height: 5
#| fig-width: 8
#| fig-cap: "foo. **(A)** br. **(B)** fff. *[Zum Vergrößern anklicken]*"

anage_tbl |> 
  ggplot(aes(weight, age)) +
  theme_minimal() +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = 1e4, color = "gray") +
  geom_hline(yintercept = 150, color = "gray") +
  geom_text_repel(data = filter(anage_tbl, weight > 1e4),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  geom_text_repel(data = filter(anage_tbl, age > 150 & weight < 1e4),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  labs(x = "Gewicht in [kg]", y = "Alter in [yrs]") +
  scale_x_continuous(labels = scales::comma,
                     breaks = c(1e4, 5e4, 1e5)) +
  theme(axis.title.y = element_text(size = 16, face = 2),
        axis.text.y = element_text(size = 12),
        axis.title.x = element_text(size = 16, face = 2),
        axis.text.x = element_text(size = 12),  
        plot.title = element_text(size = 17),
        plot.subtitle = element_text(size = 12, face = "italic"),
        plot.caption = element_text(size = 12),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "top")
```

## Transformationen

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pretest-rank
#| fig-align: center
#| fig-height: 4
#| fig-width: 8
#| fig-cap: "Zusammenhang vom Mittelwert, Median und dem Modus zur Feststellung einer Normalverteilung. Der Modus ist hierbei der häufigste Wert. **(A)** Linksschiefe Verteilung. Der Modus ist größer als der Median ist größer als der Mittelwert. **(B)** Symmetrische Normalverteilung. Der Mittelwert und Median sowie Modus sind gleich. **(C)** Rechtsschiefe Verteilung der Mittelwert ist größer als der Median ist größer als der Modus. *[Zum Vergrößern anklicken]*"

p2kurt + p1kurt + p3kurt +
  plot_layout(ncol = 3) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

Quadratwurzel moderat schiefe Messwerte:

-   `sqrt(y)` für positiv, schiefe Messwerte
-   `sqrt(max(y+1) - y)` für negative, schiefe Messwerte

Logarithmus für starke schiefe Messwerte:

-   `log10(y)` für positiv, schiefe Messwerte
-   `log10(max(y+1) - y)` für negative, schiefe Messwerte

Die Inverse für extrem, schiefe Messwerte:

-   `1/y` für positiv, schiefe Messwerte
-   `1/(max(y+1) - y)` für negative, schiefe Messwerte

Die Transformation mit Rängen `rank()`

Schnelle Übersicht...

### Quadratwurzel

Die Quadratwurzel-Transformationen ist eine etwas seltenere Transformation. Meist wird die Quadratwurzel-Transformationen als die schwächere $log$-Transformation bezeichnet. Wir sehen in @fig-log-scale-2-2 den Grund dafür. Aber zuerst müssen wir aber über die Funktion `sqrt()` unsere Daten transformieren. Wir können auch die Funktion `transform()` aus dem R Paket `{dlookr}` verwenden und haben eine große Auswahl an möglichen Transformationen. Einfach mal die Hilfeseite von `transform()` aufrufen und nachschauen.

```{r}
sqrt_tbl <- fac1_tbl |> 
  mutate(sqrt_hatch_time = sqrt(hatch_time))
```

In der folgenden Abbildung sehen wir die nicht transformierte, rohe Daten sowie die transformierten Daten mit der Quadratwurzel. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam nach rechts aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Unser Ziel besser normalverteilte Daten vorliegen zu haben, haben wir aber mit der Quadratwurzel-Transformationen nicht erreicht. Die Daten sind immer noch rechtsschief.

```{r}
#| message: false
#| echo: false
#| label: fig-trans-sqrt
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $sqrt$-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(sqrt_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA)

p2 <- ggplot(sqrt_tbl, aes(sqrt_hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in sqrt(h)", y = "")

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

### Logarithmus

Wir nutzen die $log$-Transformation, wenn wir aus einem nicht-normalverteiltem Outcome $y$ ein approxomativ normalverteiltes Outcome $y$ machen wollen. Dabei ist wichtig, dass wir natürlich auch die Einheit mit $log$-transformieren.

Im Folgenden sehen wir die $log$-Transformation der Variable `hatch_time` mit der Funktion `log()`. Wir erschaffen eine neue Spalte im `tibble` damit wir die beiden Variable vor und nach der $log$-Transformation miteinander vergleichen können.

Das [R Paket `{dlookr}`](https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data/#standardization-and-resolving-skewness) hat eine große Auswahl an implementierten Funktionen für $y$-Transformationen.

```{r}

log_tbl <- fac1_tbl |> 
  mutate(log_hatch_time = log(hatch_time))

```

Wir können dann über ein Histogramm die beiden Verteilungen anschauen. In der folgenden Abbildung sehen wir die nicht transformierte, rohe Daten sowie die Log-transformierten Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Die Log-Transformation hat einigermaßen funktioniert. Wir sehen in diesem Fall approximativ normalverteilte Daten. Wir haben also ein $log$ normalverteiltes Outcome $y$ mit dem wir jetzt weiterechnen können.

```{r}
#| message: false
#| echo: false
#| label: fig-log
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $log$-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(log_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(log_tbl, aes(log_hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

In der folgenden Abbildung siehst du nochmal die Anwendung der $\log$-Skala in `{ggplot}` auf die Daten. Aber Achtung, die ursprünglichen Daten sind *nicht* transformiert. Du schaust dir hier nur an, wie die Daten transformiert aussehen würden. Es gibt auch hierzu ein kleines Tutorium zu [ggplot log scale transformation](https://www.datanovia.com/en/blog/ggplot-log-scale-transformation/). Da kannst du dann auch einmal nachschauen, wie die $\log_2$-Skala in `{ggplot}` funktioniert.

```{r}
#| message: false
#| echo: true
#| label: fig-log-scale-ggplot
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten Daten einmal auf der $log_{10}$-Skala in `ggplot` dargestellt. Achtung, hier ist nur die Darstellung transformiert. Die ursprünglichen Daten bleiben von der Transformation *in* `{ggplot}` unberührt."

ggplot(log_tbl, aes(hatch_time)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = expression("Zeit bis zum Schlüpfen in" ~ log[10](h)), y = "Anzahl") +
  ## here starts the log scale
  scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x), # <1>
                labels = trans_format("log10", math_format(10^.x))) + # <2>
  annotation_logticks(sides = "b")  # <3>

```

1.  Hier wird die $x$-Achse einmal auf die $\log$-Skala gestellt und die `breaks` entsprechend angepasst.
2.  Wir ergänzen noch die richtige Schreibweise.
3.  Die Ticks werden auch richtig gezeichnet. Du kannst mit `l` auch auf der $y$-Achse $\log$-Ticks anzeigen lassen.

### Inverse

```{r}
inverse_tbl <- fac1_tbl |> 
  mutate(inverse_hatch_time = 1/hatch_time)
```

```{r}
#| message: false
#| echo: false
#| label: fig-inverse
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $inverse$-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(inverse_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(inverse_tbl, aes(inverse_hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in (1/h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

### Inverse Normalverteilung

[Inverse normal transformation -- What is it and how to do it?](https://yingji15.github.io/2019-08-17-inverse-normal-transformation/)

```{r}
invers_norm <- function(x) qnorm((rank(x, na.last="keep") - 0.5)/sum(!is.na(x)))
```

[What is the inverse normal transformation (INT) and what are the reasons behind using it?](https://stats.stackexchange.com/questions/588992/what-is-the-inverse-normal-transformation-int-and-what-are-the-reasons-behind)

[Rank-Based Inverse Normal Transformations are Increasingly Used, But are They Merited?](https://pmc.ncbi.nlm.nih.gov/articles/PMC2921808/) von @beasley2009rank

```{r}
invers_norm_tbl <- fac1_tbl |> 
  mutate(inverse_hatch_time = invers_norm(hatch_time))
```

```{r}
#| message: false
#| echo: false
#| label: fig-inverse-norm
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $inverse$-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(invers_norm_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(invers_norm_tbl, aes(inverse_hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in (1/h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

### Rangtransformation

Der Ausweg schlechthin bis in die 90ziger Jahre war vermutlich die nichtparametrische Statistik, wenn es um nicht normalverteilte Messwerte ging. Es wird dann eben ein nichtparametrischer Test, wie der Wilcoxon oder eben Mann-Whitney gerechnet. Und hier kommt dann die Rangtransformations ins Spiel. *Eigentlich* ist die gesamte Nichtparametrik nur eine Rangtransformation auf der wir dann auch genauso gut dann einen t-Test oder eine ANOVA rechnen könnten. Dazu dann aber mehr in den jeweiligen Kapiteln zu den einzelnen nichtparametrischen Tests.

-   [Der Wilcoxon-Mann-Whitney-Test](#sec-utest) oder auch U-Test ist der t-Test auf den Rängen eines Messwertes. Wir vergleichen hier zwei Gruppen miteinander. Wenn wir mehr Gruppen haben, die wir vergleichen wollen, dann brauchen wir mehrere paarweise Wilcoxon Tests um die signifikanten Unterschiede zu bestimmen.
-   [Der Kruskal-Wallis-Test](#sec-kruskal) ist die einfaktorielle ANOVA auf den Rängen eines Messwertes. Wir vergleichen hier drei oder mehr Gruppen simultan miteinander. Wenn wir dann wissen wollen, welcher paarweise Vergleich signifikant ist, brauchen wir dann einen Posthoc-Test.

Was ist also die Rangtransformation? Wir geben einfach den sortierten Rang des Messwertes über alle Gruppen. Dann können wir auf dem rangierten Messwert weiterrechnen. In den folgenden Tabellen siehst du dann einmal die orginalen Sprungweiten gemessen in \[cm\] sowie deren rangierten Gegenstücke.

```{r}
signed_rank <- function(x) sign(x) * rank(abs(x))
```

`rank()`

## Skalierung

### Standardisierung

Die Standardisierung wird auch $z$-Transformation genannt. In dem Fall der Standardisierung schieben wir die Daten auf den Ursprung, in dem wir von jedem Datenpunkt $y_i$ den Mittelwert $\bar{y}$ abziehen. Dann setzen wir noch die Standardabweichung auf Eins in dem wir durch die Standardabweichung $y_s$ teilen. Unser standardisiertes $y$ ist nun standardnormalverteilt mit $\mathcal{N(0,1)}$. Wir nutzen für die Standardisierung folgende Formel.

$$
y_z = \cfrac{y_i - \bar{y}}{s_y} 
$$

In R können wir für die Standardisierung die Funktion `scale()` verwenden. Wir müssen auch nichts weiter in den Optionen von `scale()` angeben. Die Standardwerte der Funktion sind so eingestellt, dass eine Standardnormalverteilung berechnet wird. Wir können auch die Funktion `transform()` aus dem R Paket `{dlookr}` verwenden. Die Option wäre dann `zscore`.

```{r}
scale_tbl <- fac1_tbl |> 
  mutate(scale_jump_length = scale(jump_length))
```

In @fig-log-scale-3-1 sehen wir nochmal die nicht transformierten, rohen Daten. Wir haben in diesem Beispiel die normalvertielte Variable `jump_length` gewählt. Der Mittelwert von `jump_length` ist `r round(mean(fac1_tbl$jump_length), 2)` und die Standardabweichung ist `r round(sd(fac1_tbl$jump_length), 2)`. Ziehen wir nun von jedem Wert von `jump_length` den Mittelwert mit 19.3 ab, so haben wir einen neuen Schwerpunkt bei Null. Teilen wir dann jede Zahl durch 3.36 so haben wir eine reduzierte Spannweite der Verteilung. Es ergibt sich die @fig-log-scale-3-2 als Standardnormalverteilung. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die Sprungweite auf der $z$-Skala nicht mehr biologisch interpretieren.

```{r}
#| message: false
#| echo: false
#| label: fig-log-scale-3
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $inverse$-transformierte Daten. *[Zum Vergrößern anklicken]*"

p1 <- ggplot(scale_tbl, aes(jump_length)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = "Sprungweite in [cm]", y = "Anzahl")

p2 <- ggplot(scale_tbl, aes(scale_jump_length)) +
  geom_histogram(fill = cbbPalette[3], color = "black") +
  theme_minimal() +
  labs(x = "Sprungweite auf der z-Skala", y = "Anzahl")

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

### Normalisierung

Abschließend wollen wir uns nochmal die Normalisierung anschauen. In diesem Fall wollen wir die Daten so transformieren, dass die Daten nur noch in der Spannweite 0 bis 1 vorkommen. Egal wie die Einheiten vorher waren, alle Variablen haben jetzt nur noch eine Ausprägung von 0 bis 1. Das ist besonders wichtig wenn wir viele Variablen haben und anhand der Variablen eine Vorhersage machen wollen. Uns interessieren die Werte in den Variablen an sich nicht, sondern wir wollen ein Outcome vorhersagen. Wir brauchen die Normalisierung später für das maschinelle Lernen und die Klassifikation. Die Formel für die Normalisierung lautet wie folgt.

$$
y_n = \cfrac{y_i - \min(y)}{\max(y) - \min(y)} 
$$

In R gibt es die Normalisierungsfunktion nicht direkt. Wir könnten hier ein extra Paket laden, aber bei so einer simplen Formel können wir auch gleich die Berechnung in der Funktion `mutate()` machen. Wir müssen nur etwas mit den Klammern aufpassen. Wir können auch die Funktion `transform()` aus dem R Paket `{dlookr}` mit der Option `minmax` verwenden.

```{r}
norm_tbl <- fac1_tbl |> 
  mutate(norm_jump_length = (jump_length - min(jump_length))/(max(jump_length) - min(jump_length)))
```

In @fig-log-scale-4-1 sehen wir nochmal die nicht transformierten, rohen Daten. In @fig-log-scale-4-2 sehen wir die normalisierten Daten. Hier fällt dann auf, dass die normalisierten Sprungweiten nur noch Werte zwischen Null und Eins annehmen. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die normalisierten Sprungweiten nicht mehr biologisch interpretieren.

```{r}
#| message: false
#| echo: false
#| label: fig-log-scale-4
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $inverse$-transformierte Daten. *[Zum Vergrößern anklicken]*"

p1 <- ggplot(norm_tbl, aes(jump_length)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = "Sprungweite in [cm]", y = "Anzahl")

p2 <- ggplot(norm_tbl, aes(norm_jump_length)) +
  geom_histogram(fill = cbbPalette[3], color = "black") +
  theme_minimal() +
  labs(x = "Normalisierte Sprungweite", y = "Anzahl")

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

## Automatische Transformationen...

[Normalisierung vs. Standardisierung: Wie man den Unterschied erkennt](https://www.datacamp.com/de/tutorial/normalization-vs-standardization)

Die Standardisierung versucht auch, die Symmetrie mit dem Mittelwert 0 und der Standardabweichung 1 zu erzwingen. Bei der Normalisierung ist die Wahrscheinlichkeit einer Schiefe größer, da Ihre Skalierungsfaktoren die beiden extremsten Datenpunkte sind.

### ... mit `{bestNormalize}`

Dann sind wir bis hierher gekommen und fragen uns nun welche ist den jetzt die beste Transformation für unsere Daten um einigermaßen eine Normalverteilung hinzubekommen? Da hilft uns jetzt das [R Paket `{bestNormalize}`](https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html). Die [Kurzanleitung zum Paket](https://petersonr.github.io/bestNormalize/index.html) erlaubt es automatisiert auf einem Datenvektor die beste Transformation zu finden um die Daten in eine Normalverteilung zu verwandeln.

```{r}
#| message: false
#| echo: false
#| label: fig-trans-bestNorm-1
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm."
ggplot(fac1_tbl, aes(hatch_time)) +
  geom_histogram(fill = cbbPalette[4], color = "black") +
  theme_minimal() 
```

Jetzt können wir die Funktion `bestNormalize()` nutzen um uns die beste Transformationsmethode wiedergeben zu lassen. Das ist super praktisch in der Anwendung. Wir können uns theoretisch noch verschiedene Gütekriterien aussuchen aber für hier reicht die Standardimplementierung. Wichtig ist noch, dass wir hier eine Kreuzvalidierung durchführen, so dass die Ergebnisse und die Auswahl des Algorithmus robust sein sollte.

```{r}
bn_obj <- bestNormalize(fac1_tbl$hatch_time)
bn_obj
```

Laut dem Algorithmus sollen wir eine `Box Cox Transformation` durchführen. Okay, wie machen wir das jetzt? Wir hätten zwar die Parameter in der Ausgabe angegeben und die könnten wir dann auch in einer Veröffentlichung angeben, aber wenn es schneller gehen soll, dann können wir die Funktion `predict()` nutzen, die uns die transformierten `x` Werte wiedergibt.

```{r}
x_trans <- predict(bn_obj)
```

Dann nochmal schnell gucken, ob das auch mit der Rücktransformation klappen würde.

```{r}
x_trans_back <- predict(bn_obj, newdata = x_trans, inverse = TRUE)
```

Einmal dann der Vergleich ob alle `x_trans_back` Werte den ursprünglichen `x` Werten entsprechen. Ja, das sieht gut aus.

```{r}
all.equal(x_trans_back, fac1_tbl$hatch_time)
```

Und zum Abschluss nochmal eine Abbildung der transformierten `x` Werte.In der folgenden Abbildung siehst du dann einmal das entsprechende Histogramm. Das sieht doch sehr gut aus und wir mussten nicht zig verschiedene Algorithmen selber testen.

```{r}
#| message: false
#| echo: false
#| label: fig-trans-bestNorm-2
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der transformierten `x` Werte mit der Box-Cos Transformation aus dem R Paket `{bestNormalize}`."

p1 <- ggplot(log_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(as_tibble(x_trans), aes(x_trans)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

### ... mit `{rcompanion}`

`transformTukey()` aus dem R Paket `{rcompanion}`

```{r}
tt_obj <- transformTukey(fac1_tbl$hatch_time,
                         plotit = FALSE)
```

```{r}
#| message: false
#| echo: false
#| label: fig-trans-transformTukey
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm ."

p1 <- ggplot(log_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(as_tibble(tt_obj), aes(tt_obj)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

### ... mit `{LambertW}`

`Gaussianize()` aus dem [R Paket `{LambertW}`](https://cran.r-project.org/web/packages/LambertW/index.html)

-   type: what type of non-normality: symmetric heavy-tails "h" (default), skewed heavy-tails "hh", or just skewed "s".

```{r}
gaus_obj <- Gaussianize(fac1_tbl$hatch_time, type = "s")
```

```{r}
#| message: false
#| echo: false
#| label: fig-trans-Gaussianize
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm ."

p1 <- ggplot(log_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(as_tibble(gaus_obj), aes(gaus_obj)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

### ... mit `{MASS}`

Die Box-Cox-Transformation ist ein statistisches Verfahren zur Umwandlung von nicht normalverteilten Daten in eine Normalverteilung. Die Transformation ist nicht so einfach wie die logarithmische Transformation oder die Quadratwurzeltransformation und erfordert etwas mehr Erklärung. Beginnen wir zunächst die Gleichung zu verstehen, die die Transformation beschreibt. Die grundsätzliche Idee ist, dass wir unser $y$ als Outcome mit einem $\lambda$-Exponenten transformieren. Die Frage ist jetzt, welches $\lambda$ produziert die besten normalverteilten Daten? Wenn wir ein $\lambda$ von Null finden sollten, dann rechnen wir einfach eine $\log$-Transformation. Die Idee ist also recht simpel.

$$
y(\lambda)=\left\{\begin{matrix} \dfrac{y^{\lambda}-1} {\lambda} & \quad  \mathrm{f\ddot ur\;\;}\lambda \ne 0 \\[10pt] \log(y) &\quad  \mathrm{f\ddot ur\;\;}\lambda = 0\end{matrix}\right.
$$

Wir werden natürlich jetzt nicht händisch alle möglichen $\lambda$ durchprobieren bis wir das beste $\lambda$ gefunden haben. Dafür gibt es die Funktion `boxcox` aus dem R Paket `{MASS}`, die ein lineares Modell benötigt. Daher bauen wir usn erst unser lineares Modell und dann stecken wir das Modell in die Funktion `boxcox()`.

```{r}
jump_mod <- lm(jump_length ~ 1, data = fac1_tbl)
```

Jetzt einmal die Funktion `boxcox()` ausführen und danach das $\lambda$ extrahieren. Hier ist es etwas umständlicher, da das $\lambda$ in der Ausgabe der Funktion etwas vergraben ist. Dafür ist das R Paket `{MASS}` einfach nicht mehr das jüngste Paket und hat keine so guten Funktionen zum Erhalten von wichtigen Parametern. Ich möchte die Abbildung nicht haben, daher die Option `plotit = FALSE`.

```{r}
bc_obj <- MASS::boxcox(jump_mod, plotit = FALSE)
lambda <- bc_obj$x[which.max(bc_obj$y)]
lambda
```

Wir erhalten also ein $\lambda$ von `r lambda` wieder. Dieses $\lambda$ können wir dann nutzen um unsere Daten nach Box-Cox zu transformieren. Dafür übersetzen wir dann die obige Matheformel einmal in R Code.

```{r}
fac1_tbl <- fac1_tbl |>
  mutate(jump_boxcox = ((jump_length ^ lambda - 1)/lambda))
```

In der @fig-boxcox-ggplot sehen wir dann einmal das Ergebnis der Transformation. Sieht gar nicht mal so schlecht aus und noch besser als die reine $\log$-Transformation. Wie immer musst du aber auch hier rumprobieren, was dann am besten einer Normalverteilung folgt.

```{r}
#| message: false
#| echo: true
#| label: fig-boxcox-ggplot
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm der Box-Cox transfomierten Daten in `ggplot` dargestellt."
ggplot(data = fac1_tbl) +
  theme_minimal() +
  geom_histogram(aes(x = jump_boxcox), alpha = 0.9,
                 fill = cbbPalette[3], color = "black") +
  labs(x = 'Box-Cox transformierte Sprungweiten', y = 'Anzahl')

```

### ... mit `{trafo}`

[The R Package trafo for Transforming Linear Regression Models](https://cran.r-project.org/web/packages/trafo/vignettes/vignette_trafo.pdf)

```{r}
lm_fit <- lm(hatch_time ~ weight, data = fac1_tbl)
```

```{r}
trafo_lm(lm_fit)
```

### ... mit `{dlookr}`

::: callout-warning
## Achtung, bitte beachten!

Das R Paket `{dlookr}` lies sich eine Zeit nicht über CRAN einfach installieren.
:::

```{r}
#| message: false
#| warning: false
#| eval: false
pacman::p_load(dlookr)
```

::: {layout="[15,85]" layout-valign="top"}
![](images/personal_opinion.png){fig-align="center" width="100%"}

> *"foo." --- Jochen Kruppa-Scheetz, meiner bescheidener Meinung nach.*
:::

## Referenzen {.unnumbered}
