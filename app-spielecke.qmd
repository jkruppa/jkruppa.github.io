```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Spielecke {#sec-spielecke}

*Letzte Änderung am `r format(fs::file_info("app-spielecke.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"Denn, um es endlich auf einmal herauszusagen, der Mensch spielt nur, wo er in voller Bedeutung des Worts Mensch ist, und er ist nur da ganz Mensch, wo er spielt." --- Friedrich Schiller*

------------------------------------------------------------------------

![](images/caution.png){fig-align="center" width="50%"}

**Das ist hier *meine Spielecke*, wo ich Ideen und sonst so Zeug sammele, was mir über den Weg läuft und ich noch nicht so richtig weiter im Skript eingeordnet habe. Deshalb hat das hier auch keine Struktur, da mir die Gedanken eben auch noch wirr durch den Kopf geistern.**

------------------------------------------------------------------------

## Zitate {.unnumbered}

> *""Wer ein Warum hat, für das er lebt, kann fast jedes Wie ertragen." --- Friedrich Nietzsche*

> *"Leben heißt leiden, überleben heißt, im Leiden einen Sinn finden." --- Friedrich Nietzsche*

> *"Wachstum ist nicht alles, das ist wahr. Aber ohne Wachstum ist alles nichts." --- Angela Merkel*

> *"The world breaks every one and afterward many are strong at the broken places. But those that will not break it kills." --- Ernest Hemingway*

> *"Competition is for losers!" --- Peter Thiel*

> *"Das Pferd frisst keinen Gurkensalat" --- Philipp Reis erster 1981 telefonisch übertragende Satz*

> *"The days can be easy if the years are consistent. You can write a book or get in shape or code a piece of software in 30 minutes per day. But the key is you can't miss a bunch of days." --- James Clear*

> *"One glance at a book and you hear the voice of another person perhaps someone dead for thousands of years. Across the millennia the author is speaking clearly and silently inside your head, directly to YOU." --- Carl Sagan*

> *"Zu tun, worauf man Lust hat, und nicht, was man muss -- das ist eines der ältesten und wichtigsten Ziele der Menschheit. Wir arbeiten hart daran, das zu verdrängen." --- Die Not des Müßiggangs, Magazin Brand Eins*

> *"If you feel safe in the area that you're working in, you're not working in the right area. Always go a little further into the water than you feel you're capable of being in. Go a little bit out of your depth, and when you don't feel that your feet are quite touching the bottom, you're just about in the right place to do something exciting." -- David Bowie*

> *"(1) Alles was es schon gab, als Du geboren wurdest, ist normal und gewöhnlich. Diese Dinge werden als natürlich wahrgenommen und halten die Welt am Laufen. (2) Alles was zwischen Deinem 16ten und 36ten Lebensjahr erfunden wird ist neu, aufregend und revoltionär. Und vermutlich kannst Du in dem Bereich sogar Karriere machen. (3) Alles was nach dem 36ten Lebensjahr erfunden wird ist gegen die natürliche Ordnung der Dinge." --- Douglas Adams, Per Anhalter durch die Galaxis*

> *"Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it." --- Brian Kernighan, professor at Princeton University.*

> *"The three stages of career development are: 1. I want to be in the meeting; 2. I want to run the meeting; 3. I want to avoid meetings." --- Jay Ferro*

> *"Freude ist ein Akt des Trotzes. Mit Freude gewinnen wir, auch wenn wir verlieren. Gut gelebt zu haben ist alles was uns bleibt, denn sterben müssen wir alle." --- Jaghatai Khan, The Lost and the Damned*

> *"Freude ist ein Akt des Trotzes. Durch sie gewinnen wir, auch wenn wir verlieren. Denn sterben müssen wir alle und ein schönes Leben ist alles was uns bleibt." --- Jaghatai Khan, The Lost and the Damned*

> *"You cannot be a powerful and life-changing presence to some people without being a joke or an embarrassment to others. --- Mark Manson*

## Korrelation {.unnumbered}

[How does Polychoric Correlation Work? (aka Ordinal-to-Ordinal correlation)](https://www.r-bloggers.com/2021/02/how-does-polychoric-correlation-work-aka-ordinal-to-ordinal-correlation/)

[An Alternative to the Correlation Coefficient That Works For Numeric and Categorical Variables](https://rviews.rstudio.com/2021/04/15/an-alternative-to-the-correlation-coefficient-that-works-for-numeric-and-categorical-variables/)

## Pakete, die ich mal anschauen will... {.unnumbered}

[R Paket `{correlation}`](https://easystats.github.io/correlation/index.html)

[R Paket `{ggdist}`](https://mjskay.github.io/ggdist/index.html)

[R Paket `{modelbased}`](https://easystats.github.io/modelbased/)

Das R Paket `{visibly}` auf [An Introduction to Visibly](https://m-clark.github.io/visibly/articles/intro.html)

[R Paket `{gganimate}`](https://gganimate.com/)

[R Paket `{innsight}`](https://github.com/bips-hb/innsight)

```{r}
#| warning: false
#| eval: false


library(ggplot2)
library(gganimate)
library(gifski)

ggplot(mtcars, aes(factor(cyl), mpg)) + 
  geom_boxplot() + 
  # Here comes the gganimate code
  transition_states(
    gear,
    transition_length = 2,
    state_length = 1
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes('sine-in-out')
```

## Learning text {.unnumbered}

Wachsamkeit und Konzentration kann ein Mensch nur für 90 Minuten halten. Selbst dann ist Aufmerksamkeit ein Flackern von höherer und niedrigerer Intensität. Danach muss der Mensch 1-2 Stunden lang wirklich ruhen, bevor er wieder sehr hart arbeiten & lernen kann.

Folgende Dinge, die innerhalb von 4 Stunden nach diesen 90-minütigen Lerneinheiten durchgeführt werden, beschleunigen das Lernen.

-   Kurzes Nickerchen
-   Nichtschlafende tiefe Ruhe (NSDR)
-   Yoga Nidra
-   Formen der Meditation, die nicht viel fokussierte Konzentration erfordern,

Folgende Dinge helfen während der Lernphase das Lernen zu verstärken und zu festigen. Der Hippocampus wiederholt während dieser Zeit die Informationen mit 20-facher Geschwindigkeit und beschleunigt das Lernen und das Behalten der neu gelernten Informationen.

-   Mache ab und zu 10 Sekunden Pause vom Lernen, in denen du absolut nichts tust
-   Mache den Kopf frei (Lückeneffekt/Mikropausen),
-   Inkrementelles Lernen. Du kannst das Lernen in kleine, konzentrierte Einheiten aufteilen.
-   Stelle dir einen Timer für 3 Minuten ein, schalte das Telefon aus und verbringe die 3 Minuten damit, eine Sache intensiv zu lernen, auch wenn es sich anfühlt, als würde es aktuell nichts bringen.
-   Wenn du das wiederholt tust, können diese kleinen Schritte des Lernens zu einer übergroßen Menge des Lernens insgesamt führen.

[How to Learn Anything You Want \| Andrew Huberman](https://youtu.be/8oyA-ctqq3g)

## Zeitlicher Verlauf Ideen {.unnumbered}

```{r}
library(mgcv)

data_tbl <- tibble(x = 1:10,
                   y = c(2, 4, 5, 5.5, 6, 7, 6.5, 6.0, 5, 4))

data_tbl



fit_gam <- gam(y ~ s(x), data_tbl, family = gaussian)
fit_loess <- loess(y ~ x, data_tbl)

ggplot(data_tbl, aes(x, y)) +
  geom_point() +
  geom_line(aes(y = predict(fit_gam)), color = "red") +
  geom_line(aes(y = predict(fit_loess)), color = "blue", linetype = 2)

f_gam <- function(x) predict(fit_gam, tibble(x))
integrate(f_gam, 1, 10)  

f_loess <- function(x) predict(fit_loess, tibble(x))
integrate(f_loess, 1, 10) 

```

```{r}
#| warning: false
#| message: false


dd <- read.table(text="trt    rep    hour    mass
y      a      1       3
y      a      2       5
y      a      3       8
y      a      4       6
y      b      1       2
y      b      2       3
y      b      3       5
y      b      4       4
n      c      1       4
n      c      2       6
n      c      3       8
n      c      4       7
n      d      1       5
n      d      2       7
n      d      3       8
n      d      4       7 ", header=T) %>% 
  as_tibble()


fit_loess <- loess(mass ~ hour, data = dd)
f_loess <- function(x) predict(fit_loess, x)
integrate(f_loess, 1, 4)$value

dd %>% 
  ggplot(aes(hour, mass, color = rep)) +
  geom_point() +
  geom_line(aes(y = predict(fit_loess)), color = "blue", linetype = 2) +
  stat_smooth()
```

```{r}
#| warning: false
#| message: false
library(plyr)

dd %>% 
  split(.$rep) %>% 
  map(~loess(mass ~ hour, data = .x)) %>% 
  map(~function(x) predict(.x, x)) %>% 
  map(~integrate(.x, 1, 4)$value) %>%
  ldply(.id = "rep")
  
```

```{r}
#| eval: false
f_gam <- function(x) predict(fit_gam, tibble(x))

dd %>% 
  split(.$rep) %>% 
  map(~gam(mass ~ hour, data = .x, family = gaussian)) %>% 
  map(~function(x) predict(.x, tibble(x))) %>% 
  map(~integrate(.x, 1, 4))


get_area <- function(df) {
    fit <- loess(mass ~ hour, df)
    f <- function(x) predict(fit,newdata=x)
    integrate(f, 0, 60)$value  
}

dd %>% group_by(rep) %>% do(area=get_area(.))


```

## Weitere Datenquellen {.unnumbered}

[Food and agriculture data](https://www.fao.org/faostat/en/#home)

Mit dem [R Paket `{FAOSTAT}`](https://cran.r-hub.io/web/packages/FAOSTAT/) und der Vignette [FAOSTAT: Download Data from the FAOSTAT Database](https://cran.r-hub.io/web/packages/FAOSTAT/vignettes/FAOSTAT.pdf)

```{r}
#| eval: false


```

Mit dem [R Paket `{owidR}`](https://cran.r-project.org/web/packages/owidR/index.html) haben wir auch eine Möglichkeit direkt auf die Datenbank von [Our World in Data](https://ourworldindata.org/) zuzugreifen.

```{r}
#| eval: false

library(owidR)
foo <- owid_search("annual") 
owid("annual-co2-emissions-by-region")
owid(foo[3])
```

Eine wunderbare Sammlung von Datensätzen aus dem Bereich der Agarwissenschaften liefert das R Paket `{agridat}`. Über die Hilfeseite [agridat: Agricultural Datasets](https://cran.r-project.org/web/packages/agridat/index.html) findest du dann einmal einen gesamten Überblick und auch die Informationen über einige ausgewählte Datensätze aus Dutzenden von Datensätzen. Alle Datensätze der wichtigen Bücher zu dem experimentellen Designs sind dort eigentlich enthalten und einmal kuratiert.

Hier noch der Link zu [agridat - Datensätze mit Abbildungen in `{desplot}`](https://kwstat.github.io/agridat/reference/index.html). Du musst dann auf die jeweiligen Datensätze in der Liste klicken und dann kommst du zu dem Datensatz mit mehr Details sowie meistens auch einer Abbildung in `desplot`.

## Marginal effects {.unnumbered}

[Marginal Effects Zoo](https://marginaleffects.com)

[R Paket `{marginaleffects}`](https://marginaleffects.com)

[Marginal and conditional effects for GLMMs with `{marginaleffects}`](https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/)

[Marginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are](https://www.andrewheiss.com/blog/2022/05/20/marginalia/#what-about-marginal-things-in-statistics)

### Latent Class Analysis {.unnumbered}

Wie immer gibt es eine Reihe von Tutorien auf denen dieser Abschnitt aufbaut. Zum einen wirf einfach mal einen Blick in das Tutorium [Latent Class Analysis Using R](https://pop.princeton.edu/events/2020/latent-class-analysis-using-r). Eine leider etwas veraltete Übersicht über mögliche R Pakete liefert [Ways to do Latent Class Analysis in R](https://maksimrudnev.com/2016/12/28/latent-class-analysis-in-r/). Ich habe da immer mal quer geschaut und mich dann für die Pakete hier entschieden. Es gibt sicherlich noch andere Möglichkeiten eine *latent class analysis* zu rechnen.

Wenn du mehr über *latent class analysis* erfahren möchtest, dann kann ich dir nur das [LCA Frequently Asked Questions (FAQ)](https://www.john-uebersax.com/stat/faq.htm) empfehlen. Das FAQ ist sehr umfangreich und beschäftigt sich mit allen wichtigen Punkten. Wir wollen uns ja mit dem R Paket `poLCA` beschäftigen. Hier gibt es zwei Tutorien. Einmal gibt es das Tutorium [Example for a latent class analysis with the poLCA-package in R](https://statistics.ohlsen-web.de/latent-class-analysis-polca/) und das Tutroium [Latent Class Analysis](https://rpubs.com/eogawac/poLCA). Und natürlich die Litertur von @linzer2011polca mit der entsprechenden Veröffentlichung [poLCA: An R Package for Polytomous Variable Latent Class Analysis](https://www.sscnet.ucla.edu/polisci/faculty/lewis/pdf/poLCA-JSS-final.pdf)

Grundsätzlich basiert die *latent class analysis* nicht auf Distanzen sondern versucht über eine Modellierung der Klassenzugehörigkeitswahrscheinlichkeit getrennte Gruppen zu bilden. Wir wollen also $k$ Klassen haben und im idealen Fall können wir durch unsere Variablen in dem Datensatz jeweils mit einer 100% Wahrscheinlichkeit einer der drei Klassen zuordnen. Was dann diese $k$ Klassen aussagen, müssen wir dann selber anhand der zugewiesenen Variablen aus unseren Daten interpretieren.

```{r}
#| eval: false
pacman::p_load(tidyverse, magrittr, janitor, conflicted)

animals_tbl <- read_excel("data/cluster_animal.xlsx", sheet = 1) %>% 
  clean_names() 
```

```{r}
#| eval: false
pacman::p_load(poLCA)

poLCA(cbind(warm_blooded, fly, vertebrate, threatened, live_in_groups) ~ 1,
      nclass = 3,
      data = animals_tbl,
      nrep = 1,
      na.rm = FALSE,
      graphs = TRUE,
      maxiter = 100000
)

```

Hier hängen wir dann an der Interpretation. Da müssen wir nochmal tiefer schauen.

### Structural Equation Modeling {.unnumbered}

@van2023best [tidySEM](https://cjvanlissa.github.io/tidySEM/index.html)

[Structural Equation Modeling](https://bookdown.org/bean_jerry/using_r_for_social_work_research/structural-equation-modeling.html)

[Introduction to structural equation modeling (sem) in r with lavaan](https://stats.oarc.ucla.edu/r/seminars/rsem/)

[Intro to structural equation modeling](https://rpubs.com/Agrele/SEM)

Schöne Diagramme [Structural Equation Models](https://advstats.psychstat.org/book/sem/index.php)

### Autoregressive model {.unnumbered}

$$
AR = 
\begin{pmatrix}
 1 & \rho & \rho^2 & \rho^3 & \cdots & \rho^{n-1} \\
 \rho & 1 & \rho & \rho^2 & \cdots & \rho^{n-2} \\
 \rho & \rho^2 & 1 & \rho & \cdots & \rho^{n-3} \\
 \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
 \rho^{n-1} & \rho^{n-2} & \rho^{n-3} & \rho^{n-4} & \cdots & 1 \\
\end{pmatrix}
$$

```{r}
#| message: false
#| echo: false
#| eval: false
#| warning: false
#| label: fig-time-ar-1
#| fig-align: center
#| fig-height: 6
#| fig-width: 8
#| fig-cap: "AR(0); AR(1) mit AR-Parameter 0.3; AR(1) mit AR-Parameter 0.9; AR(2) mit AR-Parameter 0.3 und 0.3; und AR(2) mit AR-Parameter 0.9 und -0.8."

set.seed(202318080)

plot_tbl <- map(lst(0.01, 0.3, 0.9, c(0.3, 0.3), c(0.2, 0.7)), \(x) {
  arima.sim(model = list(ar = x), n = 100) %>% 
    as_tibble() 
}) %>% 
  bind_cols() %>% 
  set_names(c("AR(0) 0", "AR(1) 0.3", "AR(1) 0.9", "AR(2) 0.3; 0.3", "AR(2) 0.9; -0.8")) %>% 
  mutate(index = 1:100) %>% 
  pivot_longer(cols = matches("AR"),
               values_to = "value",
               names_to = "key") %>% 
  mutate(key = as_factor(key))

ggplot(plot_tbl, aes(index, value, fill = key)) +
  theme_bw() +
  facet_wrap(~ key, nrow = 5, strip.position="right") + 
  geom_line() +
  geom_hline(yintercept = 0) +
  geom_ribbon(aes(ymin = ifelse(value <= 0, value, 0), 
                  ymax = ifelse(value >= 0, value, 0)), 
              alpha=0.5) +
  ylim(-5, 5) +
  labs(x = "", y = "") +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        legend.position = "none", 
        strip.text = element_text(size = 10))

```

In der folgenden @fig-time-ar-1 siehst du nochmal verschiedene Beispiele für eine Autokorrelation bzw. einen autoregressiven Prozess (abk. *AR*). Der einfachste AR-Prozess ist ein Prozess der Ordnung 0 und daher schreiben wir dann auch AR(0). Bei einem AR(0) liegt keine Abhängigkeit zwischen den Zeitpunkten vor. Nur der Fehlerterm trägt zum Output des Prozesses bei, so dass AR(0) in der Abbildung weißem Rauschen entspricht.

Bei einem AR(1)-Prozess mit einem positiven $\varphi$ tragen nur der vorherige Term des Prozesses und der Rauschterm zum Output bei. Wenn $\varphi$ nahe bei 0 liegt, dann sieht der Prozess immer noch wie weißes Rauschen aus, aber wenn $\varphi$ sich 1 nähert, erhält die Ausgabe einen größeren Beitrag des vorhergehenden Terms im Verhältnis zum Rauschen. Dies führt zu einer "Glättung" oder Integration der Ausgabe, ähnlich wie bei einem Tiefpassfilter.

Bei einem AR(2)-Prozess tragen die beiden vorhergehenden Terme und der Rauschtext zur Ausgabe bei. Wenn beide $\varphi_1$ und $\varphi_2$ positiv sind, ähnelt das Ergebnis einem Tiefpassfilter, bei dem der Hochfrequenzanteil des Rauschens verringert wird. $\varphi_1$ positiv ist, während $\varphi_2$ negativ ist, dann begünstigt der Prozess Vorzeichenwechsel zwischen den Termen des Prozesses. Die Ausgabe oszilliert. Dies kann mit der Erkennung von Kanten oder Richtungsänderungen verglichen werden.

```{r}
#| message: false
#| eval: false
#| echo: false
#| warning: false
#| label: fig-time-ar-2
#| fig-align: center
#| fig-height: 6
#| fig-width: 8
#| fig-cap: "AR(0); AR(1) mit AR-Parameter 0.3; AR(1) mit AR-Parameter 0.9; AR(2) mit AR-Parameter 0.3 und 0.3; und AR(2) mit AR-Parameter 0.9 und -0.8."

set.seed(202318080)

plot_tbl <- map(lst(0.01, 0.3, 0.9, c(0.3, 0.3), c(0.2, 0.7)), \(x) {
  arima.sim(model = list(ar = x), n = 100) %>% 
    as_tibble() 
}) %>% 
  bind_cols() %>% 
  set_names(c("AR(0) 0", "AR(1) 0.3", "AR(1) 0.9", "AR(2) 0.3; 0.3", "AR(2) 0.9; -0.8")) %>% 
  mutate(index = 1:100) %>% 
  pivot_longer(cols = matches("AR"),
               values_to = "value",
               names_to = "key") %>% 
  mutate(key = as_factor(key))

ggplot(plot_tbl, aes(index, value, fill = key)) +
  theme_bw() +
  facet_wrap(~ key, nrow = 5, strip.position="right") + 
  geom_line() +
  geom_hline(yintercept = 0) +
  geom_ribbon(aes(ymin = ifelse(value <= 0, value, 0), 
                  ymax = ifelse(value >= 0, value, 0)), 
              alpha=0.5) +
  ylim(-5, 5) +
  labs(x = "", y = "") +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        legend.position = "none", 
        strip.text = element_text(size = 10))

```

## Links {.unnumbered}

[Analyzing reactor water level measurements in the Fukushima Daiichi 1 accident](https://www.sciencedirect.com/science/article/pii/S0149197023001427)

[sjPlot - Data Visualization for Statistics in Social Science](https://strengejacke.github.io/sjPlot/)

[The wages dataset](https://search.r-project.org/CRAN/refmans/QRegVCM/html/wages.html)

[Äther](https://de.wikipedia.org/wiki/%C3%84ther_(Physik))

## Steinbruch unsortierter Ideen {.unnumbered}

[Mathematical Tables Project](https://en.wikipedia.org/wiki/Mathematical_Tables_Project)

[10 Oldest Computers in The World](https://www.oldest.org/technology/computers/)

[The 2,500-Year-Old History of Adults Blaming the Younger Generation](https://historyhustle.com/2500-years-of-people-complaining-about-the-younger-generation/)

[Women And Children First: Technology And Moral Panic](https://www.wsj.com/articles/BL-TEB-2814)

[Biasarten](https://www.iqwig.de/sonstiges/glossar/biasarten.html)

https://stats.stackexchange.com/questions/555855/why-is-a-regression-coefficient-covariance-variance

https://denninginstitute.com/modules/dau/stat/regression/linregsn/nreg_6_frm.html#:\~:text=The%20Regression%20coefficient%20is%20defined,independent%20variable%2C%20x%20or%20y.

https://wiki.pathmind.com/eigenvector

https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues

Data Science

-   Real World Data @liu2022real
-   Warum **Data** Science @hariri2019uncertainty
-   Paradigmenwechsel?

Deutsches Sprichwort: Deutsch mit jemanden Reden; Verstehe nur Spanisch

Von den Daten zu Erkenntnis und zurück.

Wahrscheinlichkeitsbegriff sollte dann doch irgendwie bei Bayes mit rein...

> *"Gott würfelt nicht!" --- Albert Einstein*

-   [Frequentistischer Wahrscheinlichkeitsbegriff](https://de.wikipedia.org/wiki/Frequentistischer_Wahrscheinlichkeitsbegriff)
-   [Bayessche Wahrscheinlichkeitsbegriff](https://de.wikipedia.org/wiki/Bayesscher_Wahrscheinlichkeitsbegriff)

## Und heute? {.unnumbered}

Wie sieht es denn heute aus? Haben wir dort auch sehr viele Frauen, die sich in der Programmierung vortun? Leider nein. In der @fig-eda-preface-r-01 sehen wir den dramatischen Rückgang an Frauen in der Informatik und den Computerwissenschaften. Eigentlich bin ich ja hier schon zu spät dran mit meiner Lehre der Informatik und Programmierung, der größte Rückgang an Frauen wird schon in den früheren Jahren verursacht als ich junge Frauen unterrichte. Aber dennoch möchte ich hier natürlich dem Trend entgegenwirken. Nichtstun ist ja in so einem Fall auch keine wirkliche Option.

```{r}
#| warning: false
#| echo: false
#| message: false
#| label: fig-eda-preface-r-01
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "Im Jahr 1995 waren 37 % der Informatiker Frauen. Heute sind es nur noch 24 %. Der Prozentsatz wird weiter sinken, wenn wir nichts tun. Wir wissen, dass der größte Rückgang von Mädchen in der Informatik im Alter zwischen 13 und 17 Jahren stattfindet. Quelle: [Girls who code](https://girlswhocode.com/about-us)"

data_tbl <- tibble(year = c(1995, 2017, 2022),
                   percent = c(0.34, 0.24, 0.22))

ggplot(data_tbl, aes(year, percent, label = scales::percent(percent))) +
  theme_minimal() +
  geom_line() +
  geom_ribbon(aes(ymin = 0.1, ymax = percent), fill = "#F0E442", alpha=0.7, position = "identity") +
  geom_label() + 
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = c(1995, 2017, 2022)) +
  labs(y = "% of women in Computer Science", x = "")

```

Was sind den die Gründe für das [Gap in der Informatik und der Computerwissenschaften](https://en.wikipedia.org/wiki/Gender_disparity_in_computing)? Hier gibt es ntürlich nicht den einen Grund, aber ich fand einen der Gründe besonders spannend.

Auch unterrichte ich weniger Informatik als [Computational statistics](https://en.wikipedia.org/wiki/Computational_statistics) oder eben aktuell Data Science - die Wisenschaft der Daten. Eigentlich gibt es nur noch Computational statistics,

[Admiral Grace Murray Hopper: When Women Were Computers](https://www.nationalww2museum.org/war/articles/grace-hopper-woman-computer)

[Girls who code](https://girlswhocode.com/)

## Referenzen {.unnumbered}
