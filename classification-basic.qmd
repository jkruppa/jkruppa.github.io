```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Grundlagen der Klassifikation {#sec-class-basic}

*Letzte Änderung am `r format(fs::file_info("classification-basic.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

Dieses Kapitel dient als Einführung in die *Klassifikation* mit maschinellen Lernmethoden. Leider müssen wir wieder einiges an Worten lernen, damit wir überhaupt mit den Methoden anfangen können. Vieles dreht sich um die Aufbereitung der Daten, damit wir dann auch mit den Modellen anfangen können zu *arbeiten*. Ja ich meine wirklich Arbeiten, denn wir werden eher einen Prozess durchführen. Selten rechnet man einmal ein Modell und ist zufrieden. Meistens müssen wir noch die Modelle *tunen* um mehr aus den Modellen rauszuholen. Wir wollen bessere Vorhersagen mit einem kleineren Fehler erreichen. Das ganze können wir dann aber nicht in einem Schritt machen, sondern brauchen viele Schritte nacheinander. Damit müssen wir auch mir R umgehen können sonst ist der Prozess nicht mehr abzubilden.

[Mit *Tuning* bezeichnen wir den Prozess, ein Modell wiederholt zu verändern und dabei zu verbessern. Was wir verändern können, hängt vom gewählten Algorithums ab.]{.aside}

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

In dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt. Wir wählen auch nur ein kleines Subset aus den Daten aus, da wir in diesem Kapitel nur Funktion demonstrieren und nicht die Ergebnisse interpretieren.

```{r}
pig_tbl <- read_excel("data/infected_pigs.xlsx") %>% 
  mutate(pig_id = 1:n(),
         infected = as_factor(infected)) %>% 
  select(pig_id, infected, age:crp) %>% 
  select(pig_id, infected, everything())  
```

In @tbl-ml-basic-pig siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-ml-basic-pig
#| tbl-cap: Auszug aus dem Daten zu den kranken Ferkeln.

pig_raw_tbl <- pig_tbl %>% 
  mutate(infected = as.character(infected))

rbind(head(pig_raw_tbl),
      rep("...", times = ncol(pig_raw_tbl)),
      tail(pig_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

Gehen wir jetzt mal die Wörter und Begrifflichkeiten, die wir für das maschinelle Lernen später brauchen einmal durch.

## What he say's?

In diesem Teil des Skriptes werden wir wieder mit einer Menge neuer Begriffe konfrontiert. Deshalb steht hier auch eine Menge an neuen Worten drin. Leider ist es aber auch so, dass wir *bekanntes* neu bezeichnen. Wir tauchen jetzt ab in die Community der Klassifizierer und die haben dann eben die ein oder andere Sache neu benannt.

::: column-margin
Kurze Referenz zu [What he says?](https://youtu.be/RAidDYQLIK4)
:::

Die gute nachticht zuerst, wir haben ein relativ festes Vokabular. Das heißt, wir springen nicht so sehr zwischen den Begrifflichkeiten wie wir es in den anderen Teilen des Skriptes gemacht haben. Du kennst die Modellbezeichnungen wie folgt.

$$
y \sim x
$$

mit

-   $y$, als Outcome oder Endpunkt.
-   $x$, als Covariate oder Einflussvariable.

Das bauen wir jetzt um. Wir nennen in dem Bereich des maschinellen Lernen jetzt das $y$ und das $x$ wie folgt.

-   $y$ ist unser *label*, dafür gibt es kein deutsches Wort.
-   $x$ sind unsere *features* und mehrere Features bilden den *feature space*, dafür gibt es jeweils auch kein deutsches Wort.

[*Label* meint also das $y$ oder Outcome. *Feature* beschreibt das $x$ oder die Einflussvariablen.]{.aside}

Im folgenden Text werde ich also immer vom Label schreiben und dann damit das $y$ links von dem `~` in der Modellgleichung meinen. Wenn ich dann von den Features schreibe, meine ich alle $x$-Variablen rechts von dem `~` in der Modellgleichung. Ja, daran muss du dich dann gewöhnen. Es ist wieder ein anderer sprachlicher Akzent in einem anderen Gebiet der Statistik.

## Klassifikation vs. Regression

Wenn mich etwas aus der Bahn geworfen hat, dann waren es die Terme *classification* und *regression* im Kontext des maschinellen Lernens. Wenn ich von *classification* schreibe, dann wollen wir ein kategoriales Label vorhersagen. Das bedeutet wir haben ein $y$ vorliegen, was nur aus Klassen bzw. Kategorien besteht. Im Zweifel haben wir dann ein Label mit $0/1$ einträgen. Wenn mehr Klassen vorliegen, wird auch gerne von *multiclass* Klassifikation gesprochen.

Dazu steht im Kontrast der Term *regression*. In dem Kontext vom maschinellen Lernen meint *regression* die Vorhersage eines numerischen Labels. Das heißt, wir wollen die Körpergröße der Studierenden vorhersagen und nutzen dazu einen *regression* Klassifikator. Das ist am Anfang immer etwas verwirrend. Wir unterschieden hier nur die Typen der Label, sonst nichts. Wir fassen also wie folgt zusammen.

-   *classification*, wir haben ein Label bzw. $y$ mit Kategorien. Nehmen wir einmal unser Ferkelbeispiel. In unserer Spalte `infected` sind die Ferkel infiziert $(1)$ oder nicht-infiziert daher gesund $(0)$. Du wählst dann den Modus `set_mode("classification")`.
-   *regression*, wir haben ein Label bzw. $y$ mit kontinuierlichen Werten. Unsere Ferkel haben ein Gewicht in $kg$ und daher nehmen wir die Spalte `weight`. Du wählst dann den Modus `set_mode("regression")`.

Wir brauchen die Begriffe, da wir später in den Algorithmen spezifizieren müssen, welcher Typ die Klassifikation sein soll.

::: callout-note
## Wo ist die Regression?

Wir werden uns in diesen und den folgenden Kapiteln hauptsächlich mit der Klassifikation beschäftigen. Wenn du eine Regression rechnen willst, also ein kontinuierliches Label vorliegen hast, dann musst du bei dem Modellvergleich andere Maßzahlen nehmen und auch eine ROC Kurve passt dann nicht mehr. Du findest dann hier bei den [Metric types](https://yardstick.tidymodels.org/articles/metric-types.html) unter dem Abschnitt *numeric* Maßzahlen für die Güte der Regression in der Prädiktion.
:::

## Supervised vs. unsupervised

Der Unterschied zwischen einer *suprvised* Lernmethode oder Algorithmus ist, dass das Label bekannt ist. Das heißt, dass wir in unseren Daten eine $y$ Spalte haben an der wir unser Modell dann trainieren können. Das Modell weiß also an was es sich optimieren soll. In @tbl-class-supervised sehen wir einen kleinen Datensatz in einem *supervised* Setting. Wir haben ein $y$ in den Daten und können an diesem Label unser Modell optimieren. Oft sagen wir auch, dass wir gelabelte Daten vorliegen haben. Daher haben wir eine Spalte, die unser LAbel mit $0/1$ enthält.

[*Supervised* heißt, dass die Daten ein Label haben und damit eine $y$ Spalte haben. Wir sagen, dass die Daten gelabelt sind. *Unsupervised* heißt, dass wir ungelabelte Daten vorliegen haben. In dem Fall von *semi-supervised* Daten, haben wir Beobachtungen mit Label und ohne Label]{.aside}

| $y$ | $x_1$ | $x_2$ | $x_3$ |
|-----|-------|-------|-------|
| 1   | 0.2   | 1.3   | 1.2   |
| 0   | 0.1   | 0.8   | 0.6   |
| 1   | 0.3   | 2.3   | 0.9   |
| 1   | 0.2   | 9.1   | 1.1   |

: Beispieldatensatz für *supervised learning*. Unsere Daten haben eine Spalte $y$, die wir als Label in unserem Modell nutzen können. Wir haben gelabelte Daten vorliegen. {#tbl-class-supervised}

In der @tbl-class-unsupervised sehen wir als Beispiel einen Datensatz ohne eine Spalte, die wir als Label nutzen können. Nazürlich haben wir in echt dann keine freie Spalte. Ich habe das einmal so gebaut, damit du den Unterschied besser erkennen kannst. Beim *unsuoervised* Lernen muss der Algorithmus sich das Label selber bauen. Wir müssen meist vorgeben, wie viele Gruppen wir im Label erwarten würden. Dann können wir den Algorithmus starten.

|               | $x_1$ | $x_2$ | $x_3$ |
|---------------|-------|-------|-------|
| $\phantom{0}$ | 0.2   | 1.3   | 1.2   |
|               | 0.1   | 0.8   | 0.6   |
|               | 0.3   | 2.3   | 0.9   |
|               | 0.2   | 9.1   | 1.1   |

: Beispieldatensatz für *unsupervised learning*. Unsere Daten haben keine Spalte $y$, die wir als Label in unserem Modell nutzen können. Wir haben ungelabelte Daten vorliegen. {#tbl-class-unsupervised}

Dann gibt es natürlich auch den Fall, dass wir ein paar Beobachtungen mit einem Eintrag haben und wiederum andere Beobachtungen ohne eine Eintragung. Dann sprechen wir von einem *semi-supervised learning*. Im Prinzip ist es ein Mischmasch aus *supervised learning* und dem *unsupervised learning*. Es gibt hier aber keine genaue Grenze wie viele gelabelete Beobachtungen zu ungelabelten Beobachtungen da sein müssen.

Wir haben sehr oft eine *superised* Setting in unseren Daten vorliegen. Aber wie immer, du wirst vielleicht auch Cluster bilden wollen und dann ist das *unsupervised* Lernen eine Methode, die du gut nutzen kannst. Am Ende müssen jeder Beobachtung ein Label zugeordnet werden. Wer das dann macht, ist wiederum die Frage.

## Bias vs. Varianz

Im Bereich des maschinellen Lernens sprechen wir oft von einem Bias/Varianz Trade-off. Das heißt, wir haben zum einen eine Verzerrung (eng. *Bias*) in unserem Auswahlprozess des anzuwendenden Algorithmus. Zum anderen ist unser Algorithmus nur bedingt genau, das heißt wir haben auch eine Varianz die durch den Algorithmus hervorgerufen wird. Hierbei musst du dich etwas von dem Begriff *Varianz* im Sinne der deskriptiven Statistik lösen. Die Varianz beschreibt hier die Variabilität in der Vorhersage. Wir meinen hier schon eine Art Abweichung, aber das Wort Varianz mag hier etwas verwirrend sein. In @fig-class-bias-overview sehen wir nochmal den Zusammenhang zwischen dem Bias und der Varianz.

![Der Bias ist eine menschliche Komponente des Modells. Wir wählen das Modell aus und bringen damit eine *mögliche* Verzerrung in die Auswertung. Die Varianz wird vom Modell selber verursacht und beschreibt den Zusammenhang zwischen dem Traings- und Testdaten.](images/class-bias-overview.png){#fig-class-bias-overview fig-align="center" width="90%"}

Wir können daher wie folgt den Bias und die Varianz beschreiben. Wichtig ist hier nochmal, dass wir uns hier die Worte etwas anders benutzen, als wir es in der klassischen Statistik tun würden.

-   **Bias**: Der Bias (deu. *Verzerrung*) unseres Modells hat mit den Annahmen zu tun, die wir über die Daten machen. Und damit auch wie gut das Modell zu den Daten passt, auf denen das Modell trainiert wird. Ein Modell mit einem hohen Bias passt nicht gut zu den Trainingsdaten, hat eine begrenzte Flexibilität oder ist extrem einfach für die vorliegenden Daten. Wenn ein Modell zu simpel ist, führt es häufig zu einem hohen Trainingsfehler. Das Label der Traingsdaten wird daher nicht gut wiedergegeben.
-   **Varianz**: Die Varianz unseres Modells sagt aus, wie das Modell seine Vorhersageergebnisse in Abhängigkeit von den Traingsdaten variiert. Ein Modell mit hoher Varianz kann sich gut an die Trainingsdaten anpassen und hat daher Probleme bei der Verallgemeinerung auf die ungesehene Testdaten, was zu einem hohen Testfehler führt.

[Der Bias zeigt uns, wie gut unser Modell der Realität entspricht. Die Varianz sagt uns, wie gut unser Modell auf die Trainingsdaten abgestimmt ist.]{.aside}

In @fig-class-bias-var-overview shen wir den Zusammenhang zwischen Bias und Varianz an einer Dartscheibe dargestellt. Wenn wir eine hohe Varianz und einen hohen Bias haben, dann treffen wir großflächig daneben. Wenn sich der Bias verringert, dann treffen wir mit einer großen Streuung in die Mitte. Eine geringe Varianz und ein hoher Bias lässt uns präsize in daneben treffen. Erst mit einem niedrigen Bias und einer niedrigen Varianz treffen wir in die Mitte der Dartscheibe.

![Abstrakte Darstellung des Bias vs. Varianz Trade-off anhand einer Dartscheibe.](images/class-bias-variance-overview.png){#fig-class-bias-var-overview fig-align="center" width="90%"}

Der gesamte Fehler unseres Modells setzt sich dann wie folgt aus dem Bias und der Varianz zusammen. Wir können den Bias kontrollieren in dem wir verschiedene Algorithmen auf unsere Daten testen und überlegen, welcher Algorithmus am besten passt. Die Varianz können wir dadurch verringern, dass wir unsere Modelle tunen und daher mit verschiedenen Parametern laufen lassen. Am Ende haben wir aber immer einen Restfehler $\epsilon$, den wir nicht reduzieren können. Unser Modell wird niemals perfekt zu generalisieren sein. Wenn $\epsilon$ gegen Null laufen sollte, spricht es eher für ein Auswendiglernen des Modells als für eine gute Generalisierung.

$$
error = variance + bias + \epsilon
$$

In der @fig-class-trade-off sehen wir den Zusammenhang zwischen Bias und Varianz nochmal in einer Abbildung im Zusammenhang mit der Modellkomplxität gezeigt. Je größer die Modellkomplexität wird, desto geringer wird der Bias. Dafür wird das Modell aber überangepasst und die Varianz des Modells steigt. Daher gibt es ein Optimum des *total errors* bei dem der Bias und Varianz jeweils Minimal sind.

![Zusammenhang zwischen der Modellkomplexität, dem Bias und der Varainz. Es gibt ein Optimum des *total errors*.](images/class-trade-off.png){#fig-class-trade-off fig-align="center" width="80%"}

Jetzt wollen wir uns den Zusammenhang zwischen Bias und Varianz nochmal an der Bilderkennung veranschaulichen. Wir nutzen dafür die Bilderkennung um Meerschweinchen und Schafe auf Bildern zu erkennen. In der @fig-class-bias-03 sehen wir mich in einem Krokodilkostüm. Unser erstes Modell 1 klassifiziert mich als Meerschweinchen. Wir haben also ein sehr hohes Bias vorliegen. Ich bin kein Meerschweinchen.

![Unser erstes Modell hat ein hohes Bias. Daher klassifiziert mich das Modell 1 als ein Meerschweinchen, obwohl ich ein Krokodil bin.](images/class-bias-03.jpg){#fig-class-bias-03 fig-align="center" width="60%"}

In der @fig-class-bias-01 sehen wir ein durch Model 2 korrekt klassifiziertes Meerschweinchen. Nun hat dieses Modell 2 aber eine zu hohe Varianz. Die hohe Varianz in dem Modell 2 sehen wir in der @fig-class-bias-02. Das Meerschweinchen wird nicht als Meerschweinchen von Modell 2 erkannt, da es keine krausen Haare und eine andere Fellfarbe hat. Wir sind also auch mit diesem Modell 2 nicht zufrieden. Nur exakt die gleichen Meerschweinchen zu klassifizieren ist uns nicht genug.

::: {#fig-class-bias-0102 layout-ncol="2" layout-valign="center"}
![In unserem Trainingsdatensatz hat unser Modell 2 eine hohe Varianz. Das Modell 2 findet zwar das Meerschweinchen im Bild, aber hat Probleme auf dem folgenden Testdaten.](images/class-bias-01.png){#fig-class-bias-01 fig-align="center" width="80%"}

![In unseren Testdaten zu dem trainierten Modell 2 kann das Meerschweinchen im Bild nicht erkannt werden. Das Modell 2 hat eine zu hohe Varianz.](images/class-bias-02.jpg){#fig-class-bias-02 fig-align="center" width="100%"}

Unser zweites Modell hat eine hohe Varianz. Es erkennt zwar *perfekt* eine Meerschweinchenart, muss aber bei einer anderen Art passen.
:::

In der @fig-class-bias-02 sehen wir nun unser Modell 3 mit einem niedrigen Bias und einer niedrigen Varianz. Das Modell 3 kann Schafe in einer Herde als Schafe klassifizieren. Aber auch hier sehen wir gewisse Grenzen. Das Schaf welches den Kopf senkt, wird nicht von dem Modell 3 als ein Schaf erkannt. Das kann vorkommen, wenn in dem Traingsdaten so ein Schaf nicht als Bild vorlag. Häufig brauchen wir sehr viele gute Daten. Mit guten Daten, meine ich nicht immer die gleichen Beobachtungen oder Bilder sondern eine gute Bandbreite aller möglichen Gegebenheiten.

![Unser letztes Modell 3 hat eine niedrige Varianz und ist in der Lage die Schafe auch als Schafe zu entdecken. Ein Schaf senkt den Kopf und schon kann unser Modell 3 das Schaf nicht mehr finden.](images/class-bias-04.png){#fig-class-bias-04 fig-align="center" width="70%"}

Wir sehen also, das Thema Bias und Varianz beschäftigt uns bei der Auswahl des richtigen Modells und bei der Festlegung der Modellkomplexität. Du kannst dir merken, dass ein komplexeres Modell auf den Trainingsdaten meistens bessere Ergebnisse liefert und dann auf den Testdaten schlechtere. Ein komplexes Modell ist meist überangepasst (eng. *overfitted*).

## Problem der fehlenden Werte

::: callout-note
## Mehr zu fehlenden Werten

In dem @sec-pre-processing erfährst du, wie du mit den fehlenden Werten im maschinellen Lernen umgehst. Wir werden dort aber nicht alle Details wiederholen. In dem @sec-missing erfährst du dann mehr über die Hintergründe und die Verfahren zum Imputieren von fehlenden Werten.
:::

Ein wichtiger Punkt ist bei der Nutzung von maschinellen Lernen, dass wir *keine* fehlenden Beobachtungen in den Daten haben dürfen. Es darf kein einzelner Wert fehlen. Dann funktionieren die Algorithmen nicht und wir erhalten eine Fehlermeldung. Deshalb ist es die erste Statistikerpflicht darauf zu achten, dass wir nicht so viele fehlenden Werte in den Daten haben. Das ist natürlich nur begrenzt möglich. Wenn wir auf die Gummibärchendaten schauen, dann wurden die Daten ja von mir mit Erhoben. Dennoch haben wir viele fehlende Daten mit drin, da natürlich Studierende immer was eingetragen haben. Wenn du wissen willst, wie du mit fehlenden Werten umgehst, dann schaue einmal dazu das @sec-missing an. Wir gehen hier nicht nochmal auf alle Verfahren ein, werden aber die Verfahren zur Imputation von fehlenden Werten dann am Beispiel der Gummibärchendaten anwenden. Müssen wir ja auch, sonst könnten wir auch die Daten nicht für maschinelle Lernverfahren nutzen.

## Normalisierung

::: callout-note
## Mehr zur Normalisierung

In dem @sec-pre-processing erfährst du, wie du die Normalisierung von Daten im maschinellen Lernen anwendest. In dem @sec-eda-transform kannst du dann mehr über die Hintergründe und die Verfahren zur Normalisierung nachlesen. Wir wenden in hier nur die Verfahren an, gehen aber nicht auf die Details weiter ein.
:::

Unter Normalisierung der Daten fassen wir eigentlich ein *preprocessing* der Daten zusammen. Wir haben ja unsere Daten in einer ursprünglichen Form vorliegen. Häufig ist diese Form nicht geeignet um einen maschinellen Lernalgorithmus auf diese ursprüngliche Form der Daten anzuwenden. Deshalb müssen wir die Daten vorher einmal anpassen und in eine geleiche Form über alle Variablen bringen. Was meine ich so kryptisch damit? Schauen wir uns einmal in der @tbl-class-normal ein Beispiel für zu normalisierende Daten an.

| $y$ | $x_1$ | $x_2$ | $x_3$ |
|-----|-------|-------|-------|
| 1   | 0.2   | 1430  | 23.54 |
| 0   | 0.1   | 1096  | 18.78 |
| 1   | 0.4   | 2903  | 16.89 |
| 1   | 0.2   | 7861  | 12.98 |

: Beispieldatensatz für einen Datensatz der nomiert werden muss. Die einzelenen Spalten haben sehr unterschiedliche Wertebereiche eingetragen. {#tbl-class-normal}

Warum müssen diese Daten normalisiert werden? Wir haben mit $x_1$ eine Variable vorliegen, die im Iterval $[0;1]$ liegt. Die Variable $x_2$ liegt in einem zehntausendfach größeren Wertebereich. Die Werte der Variable $x_3$ ist auch im Vergleich immer noch hundertfach im Wertebereich unterschiedlich. Dieser großen Unterschiede im Wertebereich führen zu fehlern bei Modellieren. Wir können hierzu das @sec-eda-transform betrachten. Dort werden gängige Transformationen einmal erklärt. Wir gehen hier nicht nochmal auf alle Verfahren ein, sondern konzentrieren uns auf die häufigsten Anwendungen.

## Das Rezept mit `recipe()`

Wenn wir jetzt in den folgenden Kapiteln mit den maschinellen Lernverfahren arbeiten werden, nutzen wir das R Paket `recipes` um uns mit der Funktion `recipe()` ein Rezept der Klassifikation zu erstellen. Warum brauchen wir das? Wir werden sehen, dass wir auf verschiedene Datensätze immer wieder die gleichen Algorithmen anwenden. Auch wollen wir eine Reihe von Vorverarbeitungsschritten (eng. *preprocessing steps*) auf unsere Daten anwenden. Dann ist es einfacher, wenn wir alles an einem Ort abgelegt haben. Am Ende haben wir auch verschiedene Spalten in unseren Daten. Meistens eine Spalte mit dem Label und dann sehr viele Spalten für unsere Features oder Prediktoren. Vielleiht noch eine Spalte für die ID der Beobachtungen. Das macht alles sehr unübersichtlich. Deshalb nutzen wir `recipes` um mehr Ordnung in unsere Klassifikation zu bekommen.

::: column-margin
Du findest hier die [Introduction to recipes](https://recipes.tidymodels.org/) und dann eine Idee wie `recipes` funktionieren mit [Preprocess your data with recipes](https://www.tidymodels.org/start/recipes/).
:::

Wir gehen nun folgende vier Schritte für die Erstellung eines Modellfits mit dem R Paket `recipe` einmal durch. Am Ende haben wir dann unsere Klassifikation durchgeführt. Vorher haben wir aber unseren Algorithmus und damit unser Modell definiert und auch festgelegt, was in den Daten noch angepasst und transformiert werden soll. Alles zusammen bringen wir dann in ein `workflow` Objekt in dem alles, was wir mit den Daten machen wollen, gespeichert ist.

1)  Erstellen des Modells (`logreg_mod`),
2)  ein Vorverarbeitungsrezept (eng. *preprocessing*) für unseren Datensatz `pig_tbl` erstellen (`pig_rec`),
3)  das Modell und das Rezept in einem Wokflow bündeln (`pig_wflow`), und
4)  unseren Workflow mit einem einzigen Aufruf von `fit()` trainieren.

Es geht los in dem wir als erstes unser Modell definieren. Wir wollen hier aus einfachen Gründen eine logistische Regression rechnen. Dafür nutzen wir die Funktion `logistic_reg()` um eben eine logistische Regression zu rechnen. Es gibt aber eine große Anzahl an möglichen Implementierungen bzw. `engine` in R. Wir wählen hier die Implementierung des `glm` mit der Funktion `set_engine("glm")`. Faktisch haben wir hier also die Funktion `glm(..., family = binomial)` definiert. Nur ohne die Daten und die Formel.

::: column-margin
Du findest auf [Fitting and predicting with parsnip](https://parsnip.tidymodels.org/articles/Examples.html) eine große Auswahl an implementierten Algorithmen.
:::

```{r}
logreg_mod <- logistic_reg() %>% 
  set_engine("glm")
```

Nachdem wir den Algorithmus für unser Modell definiert haben, wollen wir natürlich noch festlegen, was jetzt gerechnet werden soll. Unser Modell definieren wir in der Funktion `recipe()`. Hier haben wir definiert, was in das Modell soll. Links steht das Outcome und rechts nur ein `.`. Damit haben wir alle anderen Spalten als Einflussvariablen ausgewählt. Das stimmt aber nur halb. Den in dem Rezept können wir auch Rollen für unsere Variablen definieren. Mit der Funktion `update_role()` definieren wir die Variable `pig_id` als `"ID"`. In der Klassifikation wird jetzt diese Variable nicht mehr berücksichtigt. Dann können wir noch Variablen transfomationen definieren. Wir wollen hier eine Dummykodierung für alle nominalen Prädiktoren, daher Faktoren, durchführen. Und wir wollen alle Variablen entfernen, in denen wir nur einen Eintrag haben oder eben eine Varianz von Null.

```{r}
pig_rec <- recipe(infected ~ ., data = pig_tbl) %>% 
  update_role(pig_id, new_role = "ID")  %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors())

pig_rec
```

Wir du siehst wird hier noch nichts gerechnet. Es gilt jetzt zu definieren was wir tun wollen. Damit wir das Rezept einfach immer wieder auf neue Daten anwenden können. Die Rollen der Variablen kannst du dir auch über die Funktion `summary()` wiedergeben lassen.

```{r}
summary(pig_rec)
```

Du siehst, dass die Variable `pig_id` eine ID ist und die Variable `infected` das Outcome darstellt. Der Rest sind die Prädiktoren mit ihren jeweiligen Typen. Wir können über die Hilfsfunktionen `all_predictor()` oder `all_nominal_predictor()` eben nur bestimmte Spalten für eine Transformation auswählen.

Im nächsten Schritt bringen wir das Modell `logreg_mod` und das Rezept `pig_rec` mit den Informationen über die Variablen und die notwendigen Transformationsschritte in einem `workflow()` zusammen. In diesem `workflow()` sind alle wichtigen Information drin und wir können den Workflow mit immer wieder neuen Subsets von unseren ursprünglichen Daten füttern.

```{r}
pig_wflow <- workflow() %>% 
  add_model(logreg_mod) %>% 
  add_recipe(pig_rec)

pig_wflow
```

Nun heißt es noch den Wirkflow mit echten Daten zu füttern. Wir rechnen also erst jetzt mit echten Daten. Vorher aber wir nur gesagt, was wir machen wollen. Erst die Funktion `fit()` rechnet das Modell auf den Daten mit den Regeln in dem Rezept. Wir nehmen hier wieder unsere ursprünglichen Daten, aber du könntest hier auch den Traingsdatensatz nehmen.

```{r}
pig_fit <- pig_wflow %>% 
  fit(data = pig_tbl)

pig_fit
```

Wir erhalten den klassischen Fit einer logististischen Regression wieder, wenn wir die Funktion `extract_fit_parsnip()` verwenden. Die Funktion gibt uns dann alle Informationen wieder. Dann können wir uns über die Funktion `tidy()` auch eine aufgeräumte Wiedergabe erstellen lassen.

```{r}
pig_fit %>% 
  extract_fit_parsnip() %>% 
  tidy() %>% 
  mutate(across(where(is.numeric), round, 2),
         p.value = pvalue(p.value))

```

Und was ist jetzt mit der Prädiktion? Dafür können wir entweder die Funktion `predict()` nutzen oder aber die Funktion `augment()`. Mir persönlich gefällt die Funktion `augment()` besser, da ich hier mehr Informationen zu den vorhergesagten Werten erhalte. Ich wähle mir dann die Spalte `infected` aus und alle Spalten, die ein `.pred` beinhalten. Dann runde ich noch auf die zweite Kommastelle.

```{r}
augment(pig_fit, new_data = pig_tbl) %>% 
  select(infected, matches(".pred")) %>% 
  mutate(across(where(is.numeric), round, 2))
```

Damit hätten wir einmal das Prinzip des Rezeptes für die Klassifikation in R durchgeführt. Dir wird das Rezept in den nächsten Kapiteln wieder über den Weg laufen. Für die Anwendung gibt es eigentlich keine schönere Art die Klassifikation sauber durchzuführen. Wir erhalten gute Ergebnisse und wissen auch was wir getan haben.
