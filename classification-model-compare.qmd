```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Vergleich von Algorithmen {#sec-class-model-compare}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

In diesem Kapitel wollen wir drei ausgewählte Algorithmen miteinander vergleichen. Ich habe hier den [$k$-NN Algorithmus](#sec-knn), den [Random Forest Algorithmus](#sec-class-rf) und den [xgboost Algorithmus](#sec-xgboost) ausgewählt. Das ist eigentlich eine relativ willkürliche Auswahl. Aber dann haben wir drei Modelle, die wir mit statistischen Maßzahlen vergleichen können. Ich rechne die Algorithmen hier relativ flott durch, wenn du mehr über die Algorithmen wissen willst, schau bitte dann in die entsprechenden Kapitel. Hier fallen also viele Dinge einfach so aus dem Himmel. Mir geht es aber auch am Ende darum, einmal die drei Algorithmen zu vergleichen.

Häufig stellt sich natürlich die Frage, welche der statistischen Maßzahlen soll ich denn nun nehmen? Wie immer ist die Antwort, kommt drauf an. Ich würde dir empfehlen, die ROC Kurve für die Klassifikation zu nehmen. Dann wird auch häufig die Accuarcy berichtet. Danach wird es dann schon schwammiger und es kommt dann auch darauf an an *wen* du berichtest. Schreibst du also deine Abschlussarbeit, dann musst du dich mit deinen Betreuern abstimmen. Bei einer wissenschaftlichen Veröffentlichung würde ich in den anderen Veröffentlichungen des Journals schauen, was dort im Rahmen des maschinellen Lernens für Gütekriterien veröffentlicht werden. In diesem Kapitel gehen wir jedenfalls eine Menge Maßzahlen für die Klassifikation einmal durch.

::: callout-caution
## Tuning und Resampling?

In diesem Kapitel werde ich auf das Tuning und das Resampling verzichten. Du findest aber in jedem Anwendungskapitel nochmal den R Code für das Tuning und das Resampling. Wenn du willst, kannst und solltest du auch die beiden Schritte noch in den Code mit dazwischen schalten. Auf der Webseite [Tidymodels - A predictive modeling case study](https://www.tidymodels.org/start/case-study/) findest du nochmal Hilfe dazu.
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, 
               janitor, xgboost, ranger, kknn,
               see, conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
##
set.seed(20234534)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

In diesem Kapitel wolle wir uns aber mal auf einen echten Datensatz anschauen und sehen wie sich drei Algorithmen auf diesem Daten so schlagen. Welcher Algorithmus ist am besten für die Klassifikation geeignet? Wir nutzen daher hier einmal als echten Datensatz den Gummibärchendatensatz. Als unser Label nehmen wir das Geschlecht `gender`. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir als Prädiktoren die Spalten `most_liked`, `age`, `semester`, und `height` mit in unsere Analysedaten.

```{r}
gummi_tbl <- read_excel("data/gummibears.xlsx") %>% 
  mutate(gender = as_factor(gender),
         most_liked = as_factor(most_liked)) %>% 
  select(gender, most_liked, age, semester, height) %>% 
  drop_na(gender)

```

Wir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein `NA` in der Variable `gender` haben. Wir haben dann insgesamt $n = `r nrow(gummi_tbl)`$ Beobachtungen vorliegen. In @tbl-gummi-model-compare sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-gummi-model-compare
#| tbl-cap: Auszug aus dem Daten zu den Gummibärchendaten.

gummi_raw_tbl <- gummi_tbl %>% 
  mutate(gender = as.character(gender),
         most_liked = as.character(most_liked))

rbind(head(gummi_raw_tbl),
      rep("...", times = ncol(gummi_raw_tbl)),
      tail(gummi_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

Unsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.

```{r}
gummi_data_split <- initial_split(gummi_tbl, prop = 3/4)
```

Wir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluß einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.

```{r}
gummi_train_data <- training(gummi_data_split)
gummi_test_data  <- testing(gummi_data_split)
```

Nachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht `gender` als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem `.` Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.

```{r}
gummi_rec <- recipe(gender ~ ., data = gummi_train_data) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_bag(all_nominal_predictors()) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors())

gummi_rec %>% summary()

```

Im Folgenden vergleichen wir einmal drei Algorithmen miteinander. Daher halten wir den Code für die Durchführung sehr kurz.

## $k$-NN Algorithm

::: callout-note
## Huch, der Code ist aber sehr kurz...

In diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den $k$-NN Algorithmus wissen willst, schaue bitte in @sec-knn.
:::

Für den $k$-NN Algorithmus nutzen wir $k=11$ Nachbarn. Mehr brauchen wir hier nicht angeben.

```{r}
knn_mod <- nearest_neighbor(neighbors = 11) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") 
```

Dann nehmen wir das Modell für den $k$-NN Algorithmus und verbinden das Modell mit dem Rezept für die Gummibärchendaten in einem Workflow.

```{r}
knn_wflow <- workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(gummi_rec)
```

Nun können wir auch schon den Fit des Modells rechnen und in einem Rutsch den Fit auch gleich auf die Testdaten anwenden.

```{r}
knn_aug <- knn_wflow %>% 
  parsnip::fit(gummi_train_data) %>% 
   augment(gummi_test_data)
```

Mehr wollen wir hier auch nicht. Wir brauchen nur die Prädiktion, da wir hier ja nur das Konzept der Modellvergleiche einmal durchgehen wollen.

## Random Forest

::: callout-note
## Huch, der Code ist aber sehr kurz...

In diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den Random Forest Algorithmus wissen willst, schaue bitte in @sec-rf.
:::

Für den Random Forest Algorithmus nutzen wir drei Variablen je Baum (`mtry = 3`), mindestens zehn Beobachtungen je Knoten (`min_n = 10`) sowei eintausend gewaschsene Bäume in unserem Wald (`trees = 1000`). Mehr brauchen wir hier nicht angeben.

```{r}
ranger_mod <- rand_forest(mtry = 3, min_n = 10, trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")
```

Dann nehmen wir das Modell für den Random Forest Algorithmus und verbinden das Modell mit dem Rezept für die Gummibärchendaten in einem Workflow.

```{r}
ranger_wflow <- workflow() %>% 
  add_model(ranger_mod) %>% 
  add_recipe(gummi_rec)
```

Nun können wir auch schon den Fit des Modells rechnen und in einem Rutsch den Fit auch gleich auf die Testdaten anwenden.

```{r}
ranger_aug <- ranger_wflow %>% 
  parsnip::fit(gummi_train_data) %>% 
  augment(gummi_test_data ) 
```

Mehr wollen wir hier auch nicht von dem Random Forest Algorithmus. Wir brauchen nur die Prädiktion, da wir hier ja nur das Konzept der Modellvergleiche einmal durchgehen wollen.

## xgboost

::: callout-note
## Huch, der Code ist aber sehr kurz...

In diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den xgboost Algorithmus wissen willst, schaue bitte in @sec-xgboost.
:::

Für den xgboost Algorithmus nutzen wir drei Variablen je Baum (`mtry = 3`), mindestens zehn Beobachtungen je Knoten (`min_n = 10`) sowei eintausend gewaschsene Bäume in unserem Wald (`trees = 1000`). Mehr brauchen wir hier nicht angeben.

```{r}
xgboost_mod <- boost_tree(mtry = 3, min_n = 10, trees = 1000) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```

Dann nehmen wir das Modell für den xgboost Algorithmus und verbinden das Modell mit dem Rezept für die Gummibärchendaten in einem Workflow.

```{r}
xgboost_wflow <- workflow() %>% 
  add_model(xgboost_mod) %>% 
  add_recipe(gummi_rec)
```

Nun können wir auch schon den Fit des Modells rechnen und in einem Rutsch den Fit auch gleich auf die Testdaten anwenden.

```{r}
xgboost_aug <- xgboost_wflow %>% 
  parsnip::fit(gummi_train_data) %>% 
  augment(gummi_test_data ) 
```

Das war jetzt der dritte und letzte Algorithmus. Wir brauchen auch hier nur die Prädiktion, da wir hier ja nur das Konzept der Modellvergleiche einmal durchgehen wollen.

## Vergleich der Modelle {#sec-class-model-compare}

```{r}
aug_lst <- lst(knn = knn_aug,
               rf = ranger_aug,
               xgboost = xgboost_aug)
```

```{r}
conf_mat_lst <- aug_lst %>% 
  map(~conf_mat(.x, gender, .pred_class))
```

```{r}

conf_mat_lst %>% 
  map(summary) %>% 
  map(~select(.x, .metric, .estimate)) %>% 
  reduce(left_join, by = ".metric") %>% 
  set_names(c("metric", "knn", "rf", "xboost")) %>% 
  mutate(across(where(is.numeric), round, 3))

```

Einmal die Konfusionsmatrix für den xgboost Algorithmus aus R.

```{r}
pluck(conf_mat_lst, "xgboost")
```

```{r}
xgboost_aug$.pred_class %>% tabyl 
```

```{r}
xgboost_aug$gender %>% tabyl 
```

Wir können auch *ganz viele* Beurteilungskriterien für die Klassifikation in einer [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) berechnen lassen.

::: {#tbl-confusion-matrix .column-page layout-ncol="2"}
|                |               |            |               |
|:--------------:|:-------------:|:----------:|:-------------:|
|                |               | **Truth**  |               |
|                |               | $Positiv$  | $Negativ$ (0) |
|                |               | $(PP)$ (1) |  $(PN)$ (0)   |
| **Prädiktion** | $Positiv$ (1) |    $TP$    |     $FP$      |
|                |   $(P)$ (1)   |            |               |
|                | $Negativ$ (0) |    $FN$    |     $TN$      |
|                |   $(N)$ (0)   |            |               |

: Die [Konfusionsmatrix](https://en.wikipedia.org/wiki/Confusion_matrix) als eine 2x2 Tabelle oder Vierfeldertafel {#tbl-confusion-matrix-theory}

|                |                |                 |                 |
|:--------------:|:--------------:|:---------------:|:---------------:|
|                |                |    **Truth**    |                 |
|                |                |  $Positiv$ (m)  |  $Negativ$ (w)  |
|                |                | $(PP = 63)$ (m) | $(PN = 56)$ (w) |
| **Prädiktion** | $Positiv$ (m)  |      $51$       |      $12$       |
|                | $(P = 58)$ (m) |                 |                 |
|                | $Negativ$ (w)  |       $7$       |      $49$       |
|                | $(N = 61)$ (w) |                 |                 |

: Die Konfusionsmatrix für den xgboost Algorithmus.

Main Caption
:::

##### Accuarcy {.unnumbered}

`accuracy` ist der Anteil der Label, die richtig vorhergesagt werden.

##### Kappa {.unnumbered}

`kap` beschreibt Kappa und damit ein ähnliches Maß wie die *accuracy*. Dabei wird aber Kappa durch die *accuarcy* normalisiert, die allein durch Zufall zu erwarten wäre. Damit ist Kappa sehr nützlich, wenn eine oder mehrere Klassen große Häufigkeitsverteilungen haben.

##### Sensitivität {.unnumbered}

`sens` beschreibt die Sensitivität oder die *true positive rate (TPR)*. Eine Methode die erkrankte Personen sehr zuverlässig als krank (1) erkennt hat eine hohe Sensitivität. Das heißt, sie übersieht kaum erkrankte (1) Personen.

$$
\mbox{Sensitivität} = \mbox{sens} = \cfrac{TP}{TP + FN} = \cfrac{51}{51 + 7} = 0.879
$$

##### Spezifität {.unnumbered}

`spec` beschreibt die Spezifität oder die *true negative rate (TNR)*. Eine Methode die gesunde Personen zuverlässig als gesund (0) einstuft, hat eine hohe Spezifität. Das heißt, die Methode liefert in der Regel nur bei Erkrankten ein positives Ergebnis.

$$
\mbox{Spezifität} = \mbox{spec} = \cfrac{TN}{TN + FP} = \cfrac{49}{49 + 12} = 0.803
$$

##### Positiver prädiktiver Wert {.unnumbered}

`ppv` beschreibt den positiven prädiktiven Wert (eng. *positive predictive value*).

$$
\mbox{Positiver prädiktiver Wert} = \mbox{ppv} = \cfrac{TP}{PP} = \cfrac{51}{63} = 0.81
$$

##### Negativer prädiktiver Wert {.unnumbered}

`npv` beschreibt den negativen prädiktiven Wert (eng. *negative predictive value*).

$$
\mbox{Negativer prädiktiver Wert} = \mbox{npv} = \cfrac{TN}{PN} = \cfrac{49}{56} = 0.875
$$

##### Matthews Korrelationskoeffizienten {.unnumbered}

`mcc` beschreibt den Matthews Korrelationskoeffizienten (eng. *Matthews correlation coefficient*). Der Matthews-Korrelationskoeffizient (MCC) ist ein zuverlässiger statistischer Wert, der nur dann einen hohen Wert hat, wenn die Vorhersage in allen vier Kategorien der Konfusionsmatrix (richtig positiv, falsch negativ, richtig negativ und falsch positiv) gute Ergebnisse erzielt. Wir berechnen den Wert hier jetzt nicht, da die Formel insgesamt acht zusammengesetzte Terme aus der Konfusionsmatrix beinhaltet. Für die Berechnung einmal beim [Matthews correlation coefficient](https://en.wikipedia.org/wiki/Phi_coefficient) nachlesen oder aber auch @chicco2020advantages berücksichtigen.

##### Youden-J-Statistik {.unnumbered}

`j_index` beschreibt die Youden-J-Statistik und ist definiert als $J = sens + spec - 1$. Wenn wir also eine hohe Sensitivität und eine hohe Spezifität haben dann nähert sich $J$ der Eins an.

$$
\mbox{Youden-J} = \mbox{j index} = sens + spec - 1 = 0.879 + 0.803 - 1  = 0.682
$$

##### Balancierte Accuarcy {.unnumbered}

`bal_accuracy` beschreibt die balancierte *accuarcy* und wird hier in der Funktion als der Durchschnitt von Sensitivität und Spezifität berechnet.

$$
\mbox{Balanced accuracy} = \cfrac{TPR + TNR}{2} = \cfrac{0.879 + 0.803}{2} = 0.841
$$

##### Entdeckungsprävalenz {.unnumbered}

`detection_prevalence` Die Entdeckungsprävalenz (eng. *detection prevalence*) ist definiert als die Anzahl der vorhergesagten positiven Ereignisse (sowohl richtig als auch falsch positiv) geteilt durch die Gesamtzahl der Vorhersagen.

$$
\mbox{Entdeckungsprävalenz} = \cfrac{TP + FP}{TP + FP + FN + TN} = \cfrac{51 + 12}{51 + 12 + 7 + 49} = 0.529
$$

##### Precision und Recall {.unnumbered}

![Darstellung .](images/class-pre-recall.png){#fig-pre-recall fig-align="center" width="100%"}

`precision` Bei der binären Klassifizierung ist die *precision* der positiv prädiktive Wert. Damit ist die *precision* die Anzahl der richtig positiven Ergebnisse geteilt durch die Anzahl aller positiven Ergebnisse, einschließlich derer, die nicht richtig erkannt wurden. Siehe auch [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).

$$
\mbox{Precision} = \mbox{Positiver prädiktiver Wert}  = \cfrac{TP}{PP} = \cfrac{51}{63} = 0.81
$$

`recall` Bei der binären Klassifizierung ist der *recall* die Sensitivität. Damit ist der *recall* die Anzahl der tatsächlich positiven Ergebnisse geteilt durch die Anzahl aller Ergebnisse, die als positiv hätten identifiziert werden müssen. Siehe auch [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).

$$
\mbox{Recall} = \mbox{Sensitivität} = \cfrac{TP}{TP + FN} = \cfrac{51}{51 + 7} = 0.879
$$

##### F$_1$ Score {.unnumbered}

`f_meas` beschreibt den F$_1$ Score und damit das harmonische Mittel aus Precision und Recall. Der höchstmögliche Wert eines F$_1$ Scores ist 1, was perfekte Präzision und Recall bedeutet, und der niedrigstmögliche Wert ist 0, wenn sowohl Präzision als auch Recall null sind. Siehe auch [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).

$$
F_1 = \cfrac{2 \cdot TP}{2 \cdot TP + FP + FN} = \cfrac{2 \cdot 51}{2 \cdot 51 + 12 + 7} = 0.843
$$

##### ROC & Precision recall Kurven {.unnumbered}

[In dem @sec-test-diag erfährst du mehr darüber was eine ROC Kurve ist und wie du die ROC Kurve interpretieren kannst.]{.aside}

```{r}
roc_tbl <- aug_lst %>% 
  map(~roc_curve(.x, gender, .pred_w, event_level = "second")) %>% 
  bind_rows(.id = "model")
```

```{r}
pr_tbl <- aug_lst %>% 
  map(~pr_curve(.x, gender, .pred_w, event_level = "second")) %>% 
  bind_rows(.id = "model")
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| label: fig-roc-pr-compare
#| fig-cap: "Darstellung der Vorhersagegüte der drei Modelle k-NN, ranger und xgboost."
#| fig-subcap: 
#|   - "Receiver operator curve."
#|   - "Precision recall curve."
#| layout-nrow: 1
#| column: page

roc_tbl %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  theme_minimal() +
  geom_path() +
  geom_abline(lty = 3) + 
  scale_color_okabeito()

pr_tbl %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  theme_minimal() +
  geom_path() +
  scale_color_okabeito()
```

## Referenzen {.unnumbered}
