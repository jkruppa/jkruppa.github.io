```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Statistisches Modellieren {#sec-stat-model-preface}

*Letzte Änderung am `r format(fs::file_info("stat-modeling-preface.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

::: callout-tip
## Modellierung in der Statistik

Du findest auf YouTube [Statistik und Data Science - Teil 15.0 - Modellierung in der Statistik](https://youtu.be/2fXExWzipDI) als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.
:::

In diesem und den nun folgenden Kapitel wollen wir uns mit den Grundlagen der *multiplen* linearen Regression beschäftigen. Deshalb hier nochmal die Wiederholung was eine [multiple lineare Regression](#sec-mult-reg-basic) ist. Wir erinnern uns, dass wir ein Outcome $y$, was einer Verteilung folgt, sowie mehrere Einflussvariable $x_1$ bis $x_p$ vorliegen haben. Wir wollen jetzt herausfinden, welchen Einfluss oder Effekt die $x_1, ..., x_p$ auf das $y$ hat. Sehr simple gesprochen legen wir eine Gerade durch eine *mehrdimensionale* Punktewolke.

Wir erinnern und nochmal, das ein *simples* lineares Modell nur ein $x_1$ hat:

$$
y \sim x_1
$$

Ein *multiples* lineares Modell hat von $x_1$ bis $x_p$ Einflussvariablen:

$$
y \sim x_1 + x_2 + ... + x_p
$$

Wir brauchen viele der Konzepte aus den vorherigen Kapiteln zur Modellgüte sowie Konfidenzintervallen. Vieles fällt hier zusammen. Schau dir ruhig nochmal die vorherigen Kapitel an, wenn dir etwas unklar ist. Häufig wollen wir auch etwas spezifischer sein. Wir schreiben daher einen Faktor mit einem $f$ wie folgt.

$$
y \sim f_1
$$

Wenn wir dann noch einen Block mit hinzunehmen, dann erweitern wir das Modell um einen Blockfaktor mit einem $b$ wie folgt.

$$
y \sim f_1 + b_1
$$

Es kann auch sein, dass wir noch einen zufälligen Effekt (eng. *random effect*) mit in das Modell nehmen wollen. Wir bezeichnen einen zufälligen Effekt mit einem $z$ wie folgt.

$$
y \sim f_1 + z_1
$$

## Genutzte R Pakete

Neben den R Paketen, die wir in den jeweiligen Kapiteln brauchen, kommen noch folgende R Pakete immer wieder dran. Deshalb sind die R Pakete hier schon mal mit den jeweiligen Internetseiten aufgeführt.

-   Das Buch [Tidy Modeling with R](https://www.tmwr.org/) gibt nochmal einen tieferen Einblick in das Modellieren in R. Wir immer, es ist ein Vorschlag aber kein Muss.
-   Das [R Paket `{parameters}`](https://easystats.github.io/parameters/index.html) nutzen wir um die Parameter eines Modells aus den Fits der Modelle zu extrahieren. Teilweise sind die Standardausgaben der Funktionen sehr unübersichtich. Hier hilft das R Paket.
-   Das [R Paket `{performance}`](https://easystats.github.io/performance/) hilft uns zu verstehen, ob die Modelle, die wir gefittet haben, auch funktioniert haben. In einen mathematischen Algorithmus können wir alles reinstecken, fast immer kommt eine Zahl wieder raus.
-   Das [R Paket `{tidymodels}`](https://tidymodels.tidymodels.org/) nutzen wir als *das* R Paket um mit Modellen umgehen zu können und eine *Vorhersage neuer Daten* zu berechnen. Das Paket `{tidymodels}` ist wie das Paket `{tidyverse}` eine Sammlung an anderen R Paketen, die wir brauchen werden.

## Generalisierung von `lm()` zu `glm()` und `[g]lmer()`

-   Die Funktion `lm()` nutzen wir, wenn das Outcome $y$ einer Normalverteilung folgt.
-   Die Funktion `glm()` nutzen wir, wenn das Outcome $y$ einer *andere* Verteilung folgt.
-   Die Funktion `lmer()` nutzen wir, wenn das Outcome $y$ einer Normalverteilung folgt *und* wir noch einen Block- oder Clusterfaktor vorliegen haben.
-   Die Funktion `glmer()` nutzen wir, wenn das Outcome $y$ einer *andere* Verteilung folgt *und* wir noch einen Block- oder Clusterfaktor vorliegen haben.

![Übersicht der Namen der Funktionen in R für das lm(), glm() und glmer().](images/glm-01.png){#fig-glm-01 fig-align="center" width="100%"}

In @fig-glm-02 sehen wir wie wir den Namen einer Regression bilden. Zuerst entscheiden wir, ob wir nur ein $x$ haben oder mehrere. Mit einem $x$ sprechen wir von einem simplen Modell, wenn wir mehrere $x$ haben wir ein multiples Modell. Im nächsten Schritt benennen wir die Verteilung für das Outcome $y$. Dann müssen wir noch entscheiden, ob wir ein gemischtes Modell vorliegen haben, dann schreiben wir das hin. Sonst lassen wir den Punkt leer. Anschließend kommt noch lineares Modell hinten ran.

![Wie bilde ich den Namen einer Regression? Erst beschreiben wir das $x$, dann das $y$. Am Ende müssen wir noch sagen, ob wir ein gemischtes Modell vorliegen haben oder nicht.](images/glm-02.png){#fig-glm-02 fig-align="center" width="100%"}

## Das Regressionskreuz

In diesem Kapitel wollen wir uns mit der Grundlage der multiplen linearen Regression beschäftigen. Das heist wir schauen uns Modelle mit mehreren $x$ an. Wir haben also nicht mehr nur eine Einflussvariable auf der rechten Seite der Gleichung sondern meist mehrere. Je nachdem wie diese $x$ beschffen sind, müssen wir die $x$ auch interpretieren. Wir unterscheiden in vier Arten von $x$.

1)  Das zu betrachtende $x$ ist eine Variable mit **kontinuierlichen Zahlen** (siehe @sec-interpret-x-cont)
2)  Das zu betrachtende $x$ ist eine Variablen mit **einem Faktor mit zwei Leveln** (siehe @sec-interpret-x-cat2).
3)  Das zu betrachtende $x$ ist eine Variablen mit **einem Faktor mit mehr als zwei Leveln** (siehe @sec-interpret-x-cat3).
4)  Das zu betrachtende $x$ ist eine Variable, die einen **Block oder Cluster** beschreibt (siehe @sec-mixed).

Wir finden die Fälle 1) bis 3) in der @fig-reg-cross in den Spalten wieder.

Neben dem $x$ müssen wir auch das $y$ in diesem Kapitel betrachten. Das $y$ kann aus verschiedenen Verteilungen kommen. Häufig nehmen wir an, dass das $y$ normalverteilt ist, das muss das $y$ aber nicht sein. Je nachdem wir das $y$ verteilt ist, rechen wir eine andere multiple Regression.

1)  Das zu betrachtende $y$ folgt einer **Normalverteilung** bzw. entstammt einer **Gaussian** Vertreilungsfamilie. Wir wollen dann eine multiple Gaussian Regression rechen.
2)  Das zu betrachtende $y$ folgt einer **Poissonverteilung** bzw. entstammt einer Poisson Vertreilungsfamilie. Wir wollen dann eine multiple Poisson Regression rechen.
3)  Das zu betrachtende $y$ folgt einer **Ordinalen- oder Multinominalenverteilung** bzw. entstammt einer Ordinalen- oder Multinominalen Vertreilungsfamilie. Wir wollen dann eine multiple ordinale oder multinominale Regression rechen.
4)  Das zu betrachtende $y$ folgt einer **Binomialverteilung** bzw. entstammt einer Binomialen Vertreilungsfamilie. Wir wollen dann eine multiple logistische Regression rechen.

Wir finden die Fälle 1) bis 4) in der @fig-reg-cross in den Zeilen wieder.

::: column-page
![Das Regressionskreuz als allgemeine Übersicht der Möglichkeiten einer multiplen linearen Regression.](images/Regressionskreut_advanced.png){#fig-reg-cross fig-align="center" width="100%"}
:::
