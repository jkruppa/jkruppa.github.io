```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra)
```

# Hauptkomponentenanalyse {#sec-pca-main}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

Die Hauptkomponentenanalyse (eng. *Principle Component Analysis*, abk. PCA) als Abschnitt im Ausreißerkapitel ist der Hauptkomponentenanalyse nicht würdig. Wir könnten hier ein eigenes Kapitel nur über die PCA, wie ich die Hauptkomponentenanalyse ab jetzt immer abkürzen werde, schreiben und das würde nicht reichen. Das hat vor allem damit zu tun, dass die PCA in den Sozialwissenschaften sehr weitreichend genutzt wird. Ebenso ist die Auswertung von Fragebögen allgemein ein Schwerpunkt der PCA. Wir nutzen die PCA hier jetzt um zu sehen, ob wir Ausreißer in unseren Beobachtungen haben. Das ist also eine sehr spezifische Anwendung. Vielleicht wird dieses Kapitel nochmal größer, aber jetzt bleiben wir einmal bei der Anwendung.

Das folgende Kaiptel basiert zum Teilen auf den [Articles - Principal Component Methods in R: Practical Guide](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/). Diese Sammlung an Tutorien geben einen wunderbaren Überblick über alle möglichen Methoden zu der Hauptkomponentenanalyse und deren verwandten Algorithmen. Wenn dich dazu mehr interessiert kann ich das Buch von @kassambara2017practical dir sehr ans herz legen. Dort findest du eine tolle Übersicht über die Hauptkomponentenanalyse in R.

::: column-margin
Neben viele anderen Tutorien gibt es hier noch ein weiteres zu den [PCA - Principal Component Analysis Essentials](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/).
:::

Was ist grob die Idee der PCA? Wir wollen unseren Daten, also die ganze Daten*matrix* einmal so transformieren, dass wir neue Komponenten aus den Daten extrahieren, die die Daten auf einer anderen Dimension beschreiben. Klingt etwas kryptisch, aber im Prinzip handelt es sich bei der PCA um eine Transformation der Daten. Wir nutzen dabei die Varianzstruktur und die Varianz/Covarianzmatrix. Im Prinzip also die Korrelation zwischen den einzelnen Variablen in dem Datensatz.

Neben der PCA existiert noch das *Multidimensional Scaling* (abk. MDS). Das MDS ist im Prinzip eine Spezialform der PCA. Im Unterschied zur PCA wird die MDS auf einer Distanzmatrix gerechnet. In einer MDS können wir nicht einfach so unsere Daten reinstecken sondern müssen zuerst die Daten in eine Distanzmatrix umrechnen. Dafür gibt es die Funktion `dist()` oder `as.dist()`, wenn wir schon Distanzen vorliegen haben. Daher ist die Anwendung einer MDS nicht besonders komplizierter.

Wenn wir eine PCA in R rechnen wollen, dann haben wir zuerst die Wahl zwischen den Funktionen `prcomp()` und `princomp()`. Laut der R-Hilfe hat die Funktion `prcomp()` eine etwas bessere numerische Genauigkeit. Daher ist die Funktion `prcomp()` gegenüber `princomp()` vorzuziehen. Es gibt aber noch eine *neuere* Implementierung der Funktionalität in dem R Paket `FactoMineR` und der Funktion `PCA()`. In diesem Kapitel nutzen wir also das R Paket `factoextra` um sich Faktoranalysen super anzuschauen und durchzuführen. Dur kannst mehr auf der Webseite [Factoextra R Package: Easy Multivariate Data Analyses and Elegant Visualization](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization) mehr über das Paket erfahren.

Es gibt eine *natürlich* große Anzahl an Quellen wie du in R eine PCA oder ein MDS durchführst. In der folgenden Box findest du eine Sammlung an Tutorien und R Code, der dir als Inspiration dienen mag. Ich werde teile von den Tutorien in der Folge verwenden, kann aber natürlich nichts alles nochmal machen.

::: {.callout-tip collapse="true"}
## Weitere Tutorien für die Principal Component Analysis

Wie immer gibt es eine Vielzahl an tollen Tutorien, die die PCA gut erklären. Ich habe hier einmal eine Auswahl zusammengestellt und du kannst dich da ja mal vertiefend mit beschäftigen, wenn du willst. Teile der Tutorien findest du vermutlich hier im Haupttext wieder.

-   [Principal Component Analysis (PCA) For Dummies](http://www.billconnelly.net/?p=697)
-   [Principal Component Analysis 4 Dummies: Eigenvectors, Eigenvalues and Dimension Reduction](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)
-   [The most gentle introduction to Principal Component Analysis](https://towardsdatascience.com/the-most-gentle-introduction-to-principal-component-analysis-9ffae371e93b)
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
set.seed(20230727)
pacman::p_load(tidyverse, magrittr, readxl,
               factoextra, FactoMineR, ggpubr,
               janitor, corrplot, conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflicts_prefer(magrittr::set_names)
conflicts_prefer(dlookr::transform)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Beginnen wir mit einem normierten Datensatz aus dem R Paket `cluster`. Der Datensatz `animals` wurde von mir noch mit ein paar Tieren ergänzt und schaut sich sechs Eigenschaften von 23 Tieren an. Wir wollen im Folgenden nun herausfinden, ob wir anhand der Eigenschaften in den Spalten die Tiere in den Zeilen in Gruppen einordnen können. Einige der Tiere sind ja näher miteinander verwandt als andere Tiere. Die ursprünglichen Daten liefen noch auf einem $1/2$-System, das ändern wir dann zu $0/1$ damit wir dann auch besser mit den Daten arbeiten können. Für die Algorithmen ist es egal, aber ich habe lieber $1$ gleich ja und $0$ gleich nein.

```{r}
animals_tbl <- read_excel("data/cluster_animal.xlsx", sheet = 1) %>% 
  clean_names() %>% 
  mutate(across(where(is.numeric), \(x) x - 1))
```

Schauen wir uns einmal den Datensatz in der @tbl-cluster-01 an. Wir sehen, dass wir noch einige fehlende Werte in den Daten vorliegen haben. Das ist manchmal ein Problem, deshalb werden wir im Laufe der Analyse die `NA` Werte mit `na.omit()` entfernen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-cluster-01
#| tbl-cap: "Übersicht über die 23 Tiere mit den sechs Eigenschaften in den Spalten. Eine 1 bedeutet, dass die Eigenschaft vorliegt; eine 0 das die Eigenschaft nicht vorliegt."

animals_tbl %>% 
  kable(align = "c", "pipe")
```

Der Tierdatensatz ist schön, da wir es hier nur mit 0/1 Werten zu tun haben. Wir werden später in dem preprocessing der Daten sehen, dass wir alle Spalten in der gleichen Spannweite der Werte wollen. Das klingt immer etwas kryptisch, aber der nächste Datensatz über verschiedene Kreaturen macht es deutlicher.

::: column-margin
Eine andere Art die Daten zu Gruppieren kannst du im Tutorium [Clustering Creatures](https://rpubs.com/askieswe/clustering) nochmal nachvollziehen.
:::

Im Folgenen einmal der Datensatz, den wir dann in der gleichen Exceldatei finden nur eben auf dem zweiten Tabellenblatt. Wir reinigen noch die Namen und setzen die `creature`-Spalte auf Klein geschrieben. Wie du siehst, haben wir dann nur 15 Kreaturen und drei Spalten mit dem Gewicht, der Herzrate und dem maximalen möglichen Alter.

```{r}
creature_tbl <- read_excel("data/cluster_animal.xlsx", sheet = 2) %>% 
  clean_names() %>% 
  mutate(creature = tolower(creature))

```

In der @tbl-cluster-02 sehen wir nochmal die Daten dargestellt und hier erkennst du auch gut, wo das Problem liegt. Die Masse der Tiere reicht von $6g$ beim Hamster bis $120000000g$ beim Wal. Diese Spannweiten in einer Spalte und zwischen den Spalten führt dann zu Problemen bei den Algorithmen. Deshalb müssen wir hier Daten nochmal normalisieren oder aber standardisieren. Je nachdem was da besser passt.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-cluster-02
#| tbl-cap: "Übersicht über die 15 Kreaturen mit den drei Eigenschaften in den Spalten. Wir haben hier sehr große Unterschiede in den Datenwerten. Daher müssen wir vor dem Clustern nochmal normalisieren."

creature_tbl %>% 
  kable(align = "c", "pipe")
```

## Daten standardisieren

Die Standardisierung zwingt Variablen in eine $\mathcal{N(0,1)}$ Standardnormalverteilung. Das heißt, wir transformieren alle Variablen auf einen Mittelwert von $0$ und einer Standardabweichung von $1$. Hier nochmal die Formel für die Standardisierung oder auch $z$-Transformation.

$$
z = \cfrac{x_i - \bar{x}}{s_x}
$$ Wie wir hier sehen ziehen wir von jeder $i$-ten Beobachtung den Mittlwert von allen Beobachtungen ab. Dann teilen wir noch durch die Standardabweichung alle Beobachtungen. Am Ende ist dann damit unser Mittelwert auf $0$ und unsere Standardabweichung auf $1$.

Die Standardisierung macht dann auch die Daten sehr schon gleichförmig. Hier nutzen wir auch die Funktion `transform()` aus dem R Paket `dlookr` mit der Option `zscore`. Damit wir auch auf jeden Fall sicher gehen, dass wir die richtige Funktion nutzen, schreiben wir `dlookr::transform()` und damit ist sichergestellt, dass wir auch die Funktion `transform()` aus dem R Paket `dlookr` nutzen.

```{r}
std_creature_tbl <- creature_tbl %>% 
  mutate(mass_grams = dlookr::transform(mass_grams, "zscore"),
         heart_rate_bpm = dlookr::transform(heart_rate_bpm, "zscore"),
         longevity_years = dlookr::transform(longevity_years, "zscore")) 
std_creature_tbl
```

Die Funktion `PCA()`, die wir im Folgenden verwenden wollen, wird zwar auch die Daten intern von sich aus standardisieren, wenn die nicht standardisiert wurden. Ich mag es aber nicht, wenn wichtige Schritte in Funktionen begraben werden, deshlab hier nochmal das Beispiel, wie man es macht.

## Das `data.frame()` Problem

Leider ist es so, dass fast alle Pakete im Kontext der Clusteranalyse mit den Zeilennamen bzw. `row.names()` eines `data.frame()` arbeiten. Das hat den Grund, dass wir gut das Label in den Zeilennamen parken können, ohne das uns eine Spalte in den Auswertungen stört. Meistens ist das Label ja ein `character` und soll gar nicht in den Clusteralgorithmus mit rein. Deshalb müssen wir hier einmal unsere `tibble()` in einen `data.frame()` umwandeln. Die `tibble()` haben aus gutem Grund keine Zeilennamen, die Zeilennamen sind ein Ärgernis und Quelle von Fehlern und aus gutem Grund nicht in einem `tibble()` drin. Hier brauchen wir die Zeilennamen aber.

Wir bauen uns also einmal einen `data.frame()` für unseren Tierdatensatz und setzen die Tiernamen als Zeilennamen bzw. `row.names()`. Wir entfernen dann auch noch schnell alle fehlenden Werte, denn wir wollen usn hier nicht noch mit der Imputation von fehlenden Werten beschäftigen.

```{r}
animals_df <- animals_tbl %>% 
  na.omit() %>% 
  as.data.frame() %>% 
  set_rownames(.$animal) %>% 
  select(-animal)
```

Das Ganze machen wir dann auch noch einmal für die normalisierten Kreaturendaten. Wir wollen dann ja nur auf den normalisierten Daten weitermachen.

```{r}
std_creature_df <- std_creature_tbl %>% 
  as.data.frame() %>% 
  set_rownames(.$creature) %>% 
  select(-creature)
```

## Hauptkomponentenanalyse

Es gibt viele Implementierungen der Hauptkomponentenanalyse in R. Wir nutzen die Funktion `PCA()` aus dem R Paket `FactoMineR`. Es kann nur so viele Hauptkomponenten (eng. *Principle Component Analysis*, abk. PCA) geben, wie wir auch Spalten in den Daten vorliegen haben.

-   `get_eigenvalue()`: Extrahiert die Eigenwerte/Varianzen der Hauptkomponenten
-   `fviz_eig()`: Visualisierung der Eigenwerte
-   `get_pca_ind()`, `get_pca_var(res.pca)`: Extrahiert die Ergebnisse für Individuen bzw. Variablen.
-   `fviz_pca_ind()`, `fviz_pca_var(res.pca)`: Visualisierung der Ergebnisse für Individuen bzw. Variablen.
-   `fviz_pca_biplot()`: Erstellt einen Biplot der Individuen und Variablen.

```{r}
pca_animals <- PCA(animals_df,  graph = FALSE)
```

```{r}
pca_creature <- PCA(std_creature_df,  graph = FALSE)
```

### Ebene der Eigenwerte

Die Eigenwerte messen die Menge der von jeder Hauptkomponente beibehaltenen Variation. Die Eigenwerte sind für die ersten Hauptkomponenten grundsätzlich groß und für die nachfolgenden Hauptkomponente immer kleiner. Das heißt, die ersten Hauptkomponente beschreiben Variablen mit der größten Variation im Datensatz. Wie schon gesagt, wir erschaffen neue Variablen aus den Daten, die wir Hauptkomponenten nennen. Jede Hauptkomponente hat einen Eigenwert, der beschreibt, wie viel Varianz die Hauptkomponente in den Daten erklären kann.

Mit Hilfe der Eigenwerte lässt sich die Anzahl der Hauptkomponenten bzw. Dimensionen bestimmen, die nach der PCA beibehalten werden sollen. Wir wollen selten alle Hauptkomponenten berücksichtigen. Es geht hier ja auch darum die Dimensionen der Daten zu reduzieren. Wenn wir alle Hauptkomponenten weiterverwenden würden, dann könnten wir auch den ursprünglichen Datensatz nutzen. Die Eigenwerte und der Anteil der Varianzen, die von den Hauptkomponenten beibehalten werden, können mit der Funktion `get_eigenvalue()` extrahiert werden.

Dabei bedeutet ein Eigenwert \> 1, dass die Hauptkomponenten mehr Varianz erklären als eine der ursprünglichen Variablen in den standardisierten Daten. Dies wird üblicherweise als Grenzwert für die Beibehaltung der Hauptkomponenten verwendet. Dies trifft nur zu, wenn die Daten standardisiert sind.

```{r}
eig_animals <- get_eigenvalue(pca_animals) %>% 
  as_tibble()
eig_animals 
```

Die Summe aller Eigenwerte für die Tiere ergibt eine Gesamtvarianz von `r sum(eig_animals$eigenvalue)`. Jetzt können wir ganz einfach den Anteil der erklärten Varianz von jedem Eigenwert berechnen. In der zweiten Spalte finden wir dann die Werte der Eigenwerte geteilt durch die Gesamtvarianz. Daher ist $42.35$ gleich $2.54$ geteilt durch $6$. Der kumulative Prozentsatz der erklärten Variation wird durch Addition der aufeinanderfolgenden Anteile der erklärten Variation ermittelt.

```{r}
eig_creature <- get_eigenvalue(pca_creature) %>% 
  as_tibble()
eig_creature
```

Die Summe aller Eigenwerte für die Kreaturen ergibt eine Gesamtvarianz von `r sum(eig_animals$eigenvalue)`.

Sie können die Anzahl der Komponenten auch auf die Anzahl beschränken, die einen bestimmten Anteil der Gesamtvarianz ausmacht. Wenn Sie beispielsweise mit 70 % der erklärten Gesamtvarianz zufrieden sind, verwenden Sie die Anzahl der Komponenten, um dies zu erreichen.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-cluster-pca-3
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix."
#| fig-subcap: 
#|   - "Richtung der Variablen (Spalten)"
#|   - "Verteilung der Beobachtungen (Zeilen)."
#| layout-nrow: 1
#| column: page

fviz_eig(pca_animals, addlabels = TRUE)
fviz_eig(pca_creature, addlabels = TRUE)
```

Leider gibt es keine allgemein anerkannte objektive Methode, um zu entscheiden, wie viele Hauptkomponenten ausreichend sind. Dies hängt von dem jeweiligen Anwendungsbereich und dem jeweiligen Datensatz ab. In der Praxis neigen wir dazu, die ersten paar Hauptkomponenten zu betrachten, um interessante Muster in den Daten zu finden.

In unserer Analyse erklären die ersten drei Hauptkomponenten 72 % der Variation. Dies ist ein akzeptabel hoher Prozentsatz.

Eine alternative Methode zur Bestimmung der Anzahl der Hauptkomponenten ist die Betrachtung eines Scree Plots, d. h. die Darstellung der Eigenwerte in der Reihenfolge vom größten zum kleinsten Wert. Die Anzahl der Komponenten wird an dem Punkt bestimmt, ab dem die verbleibenden Eigenwerte alle relativ klein und von vergleichbarer Größe sind

Übersetzt mit www.DeepL.com/Translator (kostenlose Version)

### Ebene der Variablen

```{r}
var_animals <- get_pca_var(pca_animals)
var_animals
```

Schauen wir uns einmal an was wir mit den Informationen über die Variablen durch die Funktion `get_pca_var()` machen können.

-   `coord`: Koordinaten der Variablen zur Erstellung eines Streudiagramms
-   `cos2`: stellt die Qualität der Darstellung der Variablen auf der Faktorkarte dar. Es wird berechnet als die quadrierten Koordinaten: `cos2 = coord * coord`
-   `contrib:` enthält die Beiträge (in Prozent) der Variablen zu den Hauptkomponenten. Der Beitrag einer Variablen zu einer bestimmten Hauptkomponente ist (in Prozent): (`cos2` \* 100) / (gesamter `cos2` der Komponente).

Beachte, dass es möglich ist, Variablen zu zeichnen und sie entweder nach i) ihrer Qualität auf der Faktorkarte `cos2` oder ii) ihren Beitragswerten zu den Hauptkomponenten `contrib` zu färben.

Die Qualität der Darstellung der Variablen auf der Faktorkarte wird als `cos2` (quadratischer Kosinus, quadratische Koordinaten) bezeichnet. Sie können auf den `cos2` wie folgt zugreifen:

```{r}
corrplot(var_animals$cos2, is.corr = FALSE)
```

```{r}
fviz_cos2(pca_animals, choice = "var", axes = 1:2)
```

Beachten Sie das,

-   Ein hoher cos2-Wert deutet auf eine gute Darstellung der Variablen auf der Hauptkomponente hin. In diesem Fall ist die Variable nahe am Umfang des Korrelationskreises positioniert.
-   Ein niedriger cos2-Wert zeigt an, dass die Variable nicht perfekt durch die PC's repräsentiert ist. In diesem Fall befindet sich die Variable in der Nähe der Kreismitte.
-   Für eine bestimmte Variable ist die Summe des cos2 aller Hauptkomponenten gleich eins.

Wenn eine Variable nur durch zwei Hauptkomponenten (Dim.1 & Dim.2) perfekt repräsentiert wird, ist die Summe des cos2 auf diesen beiden PC gleich eins. In diesem Fall werden die Variablen auf dem Korrelationskreis positioniert. In @fig-cluster-pca-2 wären die Namen direkt auf dem Korrelationskreis. Für einige der Variablen sind möglicherweise mehr als 2 Komponenten erforderlich, um die Daten perfekt zu repräsentieren. In diesem Fall werden die Variablen innerhalb des Korrelationskreises positioniert, was wir eher in der @fig-cluster-pca-2 sehen.

Die cos2-Werte werden verwendet, um die Qualität der Darstellung abzuschätzen Je näher eine Variable am Korrelationskreis liegt, desto besser ist ihre Darstellung auf der Faktorkarte (und desto wichtiger ist es, diese Komponenten zu interpretieren) Variablen, die sich in der Nähe der Mitte des Diagramms befinden, sind für die ersten Komponenten weniger wichtig.

Es ist möglich, Variablen nach ihren cos2-Werten zu färben, indem man das Argument col.var = "cos2" verwendet. Dies erzeugt einen Farbverlauf. In diesem Fall kann das Argument gradient.cols verwendet werden, um eine benutzerdefinierte Farbe anzugeben. Zum Beispiel bedeutet gradient.cols = c("weiß", "blau", "rot"), dass:

Variablen mit niedrigen cos2-Werten werden in "weiß" eingefärbt Variablen mit mittleren cos2-Werten werden in "blau" eingefärbt Variablen mit hohen cos2-Werten werden in "rot" eingefärbt

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-cluster-pca-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix."
#| fig-subcap: 
#|   - "Richtung der Variablen (Spalten)"
#|   - "Verteilung der Beobachtungen (Zeilen)."
#| layout-nrow: 1
#| column: page

fviz_pca_var(pca_animals, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )

fviz_pca_var(pca_animals, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )

```

```{r}
res.desc <- dimdesc(pca_animals, axes = c(1,2), proba = 0.05)
# Description of dimension 1
res.desc$Dim.1
```

### Ebene der Individuen

```{r}
ind_animals <- get_pca_ind(pca_animals)
ind_animals
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-cluster-pca-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix."
#| fig-subcap: 
#|   - "Verteilung der Beobachtungen (Zeilen)."
#|   - "Komibination Variablen und Individuen."
#| layout-nrow: 1
#| column: page

fviz_pca_ind(pca_animals,
             col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE) +
  scale_x_continuous(expand = expansion(add = c(0.5, 1))) +
  scale_y_continuous(expand = expansion(add = c(0.5, 0.5))) 

fviz_pca_biplot(pca_animals) +
  scale_x_continuous(expand = expansion(add = c(0.5, 1))) +
  scale_y_continuous(expand = expansion(add = c(0.5, 0.5))) 

```

Wir müssen uns jetzt leider etwas von dem `tibble` verabschieden. Für die PCA brauchen wir einen Datensatz, in dem nur Zahlen oder Faktoren stehen. Daher schieben wir die Namen der Beobachtungen oder die ID in die Zeilennamen. Eigentlich keine gute Idee für die Arbeit mit Daten, aber für die PCA passt es. Wir haben dann also den `data.frame()` als `longnose_pca_df` vorliegen. Mit diesem Datensatzobjekt können wir dann in die PCA starten.

```{r}
#| message: false
#| eval: false
#| echo: true
#| warning: false

longnose_pca_df <- longnose_tbl %>%
  select(-stream) %>% 
  as.data.frame() %>% 
  set_rownames(longnose_tbl$stream)
```

Wenn wir die Daten von jeglichen `character` Spalten gereinigt haben, dann können wir die Funktion `PCA()` nutzen. Wir wollen uns die Visualisierung gleich selber nochmal nachbauen, deshalb hier die Option mit `graph = FALSE`. Im folgenden schauen wir uns nur eine Auswahl an möglichen Abbildungen an. Davon natürlich die wichtigsten Abbildungen, aber das [Factoextra R Package: Easy Multivariate Data Analyses and Elegant Visualization](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization) kann natürlich noch viel mehr.

```{r}
#| message: false
#| eval: false
#| echo: true
#| warning: false

pca_res <- PCA(longnose_pca_df,  graph = FALSE)
```

Nachdem wir die PCA durchgeführt haben, schauen wir uns einmal an, ob es überhaupt irgendwas gebracht hat, dass wir die PCA durchgeführt haben. Wir haben ja unsere Daten transformiert und erhalten pro Variable eine neue Dimension wieder. In einem Scree Plot wie in @fig-pca-1 wird die erklärte Varianz pro Hauptkomponente gezeigt.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-pca-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Scree Plot der PCA mit der erklärten Varianz pro Hauptkomponente."
#| eval: false


fviz_screeplot(pca_res, addlabels = TRUE, ylim = c(0, 50))
```

Wir erkennen, dass unsere Hauptkomponenten teilweise viel Varianz erklären, aber wir keine Hauptkomponente gefunden haben, die sehr viel Varianz erklärt. Es gibt also in unseren erhobenen Daten keine Variable, die "alles" erklärt. Mit "alles" ist dann natürlich die Varianz gemeint. In @fig-pca-2-1 sehen wir das Diagramm der Variablen, also der Spalten in der Datenmatrix. Positiv korrelierte Variablen zeigen auf dieselbe Seite des Diagramms. Negativ korrelierte Variablen zeigen auf die gegenüberliegenden Seiten des Diagramms. In @fig-pca-2-2 ist das Diagramm der Beobachtungen, also den Zeilen in der Datenmatrix, dargestellt. Beobachtungen mit einem ähnlichen *Muster* über die Zeilen werden in Gruppen zusammengefasst. In der @fig-pca-2-3 sehen wir nochmal die beiden Abbilungen zusammen und übereinander.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-pca-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix."
#| fig-subcap: 
#|   - "Richtung der Variablen (Spalten)"
#|   - "Verteilung der Beobachtungen (Zeilen)."
#|   - "Komibination Variablen und Individuen."
#| layout-nrow: 1
#| column: page
#| eval: false

fviz_pca_var(pca_res, col.var = "black")

fviz_pca_ind(pca_res,
             col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

fviz_pca_biplot(pca_res)

```

Gut, und was sollte das ganze jetzt? Wenn wir uns die Abbildungen ansehen, dann erkennen wir zuerst, dass es zwar Variablen gibt, die sich sehr ähnlich sind. Die Variable `area` und `maxdepth` sind stark miteinander korreliert. Die Pfeile zeigen beide in die gleiche Richtung. Ebenso scheint es einen negativen Zusammenhang zwischen `so4` und `do2` zu geben. Die Richtung ist egal, wir können die Dimensionen nicht direkt interpretieren, aber die Zusammenhänge zwischen den Variablen. Abschließend sehen wir, dass wir eine zu erwartende Aufteilung der Variablen haben. Immerhin messen wir Werte eines Flusses, da sollten die Variablen was miteinander zu tun haben. Auch ist die Kausalität gegeben. Wir erwarten bei einer hohen Temperatur weniger Sauerstoff und umgekehrt.

Bei der Aufteilung der Beobachtungen sehen wir auch keine Auffälligkeiten. Wir sehen eine große Wolke mit keinen separaten Gruppen. Das heißt, obwohl wir weiter oben Ausreißer gefunden haben, würde ich hier alle Beoachtungen in den Daten lassen, wenn ich die PCA sehe. Es ist dann immer eine Abschätzung. Wir sehen aber keine sonderlich auffälligen Beobachtungen. Wir könnten noch überlegen, ob wir das Outcome `longnose` nicht doch lieber aus der PCA nehmen und festhalten, dass es im Outcome keine Ausreißer gibt. Kontroverse Entscheidung, dir wir uns überlegen müssten. Beobachtungen aus einem Datensatz zu entfernen, den man nicht selber erschaffen hat, ist immer eine sehr schwere Sache.

## Multi Dimensional Scaling (MDS)

Eine besondere Form der Hauptkomponentenanalyse ist das *Multidimensional Scaling* (abk. MDS). Im Prinzip sind die Mechanismen sehr ähnlich. Der Hauptunterschied ist aber, das wir für die MDS eine Distanzmatrix benötigen. Wir können dafür die Funktion `dist()` oder `as.dist()` nehmen, wenn wir schon Distanzen vorliegen haben. Nehmen wir als plakatives Beispiel einmal die Distanzen von europäischen Städten zueinander. Wir haben die Daten in der Exceldatei `distance.xlsx` vorliegen. Wir lesen die Daten einmal ein und schauen uns die ersten fünf Spalten und die ersten fünf Zeilen des Daten*satzes* einmal an.

```{r}
#| message: false
#| echo: true
#| warning: false

distance_tbl <- read_excel("data/distance.xlsx")

distance_tbl[1:5, 1:5]
```

Wenn wir jetzt auf diesem Datensatz jetzt ein MDS rechnen wollen, dann müssen wir zum einen alle Spalten mit einem `character` entfernen. Wir haben dann nur noch einen Datensatz bzw. Datenmatrix mit den Distanzen vorliegen. Dann kann wir das `tibble` in einen `dist`-Objekt mit der Funktion `as.dist()` umwandeln. Die eigentliche Berechnung für das *Multidimensional Scaling* findet in der Funktion `cmdscale()` statt. Mit der Option `k = 2` legen wir fest, dass wir nur zwei Hauptkomponenten bzw. Dimensionen bestimmen wollen. Wir machen also aus unserem 37x37 großen Datenmatrix durch *Multidimensional Scaling* eine Reduktion auf zwei Dimensionen bzw. Spalten.

```{r}
#| message: false
#| echo: true
#| warning: false

mds <- distance_tbl %>%
  select(-city) %>% 
  as.dist() %>%          
  cmdscale(k = 2) %>%
  as_tibble() %>% 
  mutate(V1 = -V1,
         V2 = -V2)
colnames(mds) <- c("Dim.1", "Dim.2")

```

In @fig-mds-1 sehen wir das Ergebnis der Dimensionsreduktion auf zwei Dimensionen. Wir erhalten die Zusammenhänge bzw. Distanzen aus der Datenmatrix in einem Scatterplot. Ein Scatterplot ist ja nichts anders als die Darstellung von zwei Dimensionen. Wie wir sehen können nimmt die Anordnung der Orte in etwa die Positionen von den Orten auf der Landkarte in Europa ein. Natürlich stimmen die Relationen nicht perfekt, aber das Abbild ist schon recht nahe dran. Wir können also auf diese Art und Weise auch Ausreißer bestimmen.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-mds-1
#| fig-align: center
#| fig-height: 6
#| fig-width: 7
#| fig-cap: "Scatterplot der zwei Dimensionen nach dem *Multidimensional Scaling* für den Abstand europäischer Städte."

ggscatter(mds, x = "Dim.1", y = "Dim.2", 
          label = distance_tbl$city,
          size = 1,
          repel = TRUE)
```

Wenn wir keine Distanzmatrix wie im obigen Beispiel zu den Entfernungen der europäischen Städte vorliegen haben, dann können wir uns die Distanzen auch mit der Funktion `dist()` berechnen lassen. Wir nutzen jetzt mal als Echtdaten die Daten der Gummibärchen. Mal sehen, ob wir hier irgendwelche Gruppen erkennen. Die Hilfeseite der Funktion `?dist` zeigt welche mathematischen Distanzmaße wir auf die Daten anwenden können. In unseren Fall berechnen wir die euklidische Distanz zwischen den Beobachtungen. Dann rufen wir über die Funkion `cmdsscale` das *Multidimensional Scaling* auf.

```{r}
#| message: false
#| echo: true
#| warning: false

mds <- animals_df %>%
  dist(method = "euclidean") %>%          
  cmdscale(k = 2) %>%
  as_tibble() %>% 
  set_names(c("Dim.1", "Dim.2"))
```

Das Ergebnis des *Multidimensional Scaling* hat keine Bedeutung für uns. Wir können die Zahlen nicht interpretieren. Was wir können ist das Ergebnis in einem Scatterplot wie in @fig-mds-2 zu visualisieren.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-mds-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Scatterplot der zwei Dimensionen nach dem *Multidimensional Scaling* für den Gummibärchendatensatz."

ggscatter(mds, x = "Dim.1", y = "Dim.2", 
          label = rownames(animals_df),
          size = 1,
          repel = TRUE)

```

Na endlich, wir sehen mal eine Gruppe von Beobachtungen oder Ausreißern, die nicht in der Wolke aller Beobachtungen liegen. Beobachtungen mit einem Wert kleiner $-20$ in der 2. Dimension könnten wir dann als Ausreißer entfernen. Der Rest bildet dann eine recht homogene Gruppe. Wir können uns aber auch das $k$-NN Verfahren aus dem @sec-knn nutzen um Cluster in den Daten zu finden. Das heißt wir nutzen das maschinelle Lernverfahren $k-NN$ um uns $k$ Cluster bestimmen zu lassen. Dafür nutzen wir die Funktion `kmeans()` und ziehen uns über die Funktion `pluck()` die Cluster raus. Daher erhalten wir einen Vektor mit Zahlen, die beschreiben in welchem Cluster die jeweilige $i$-te Beobachtung ist.

```{r}
# K-means clustering
clust <- kmeans(mds, centers = 5) %>%
  pluck("cluster") %>% 
  as.factor()
```

Wir wollen jetzt unser MDS Ergebnis von den Gummibärchen um eine Spalte für die Clusterergebnisse von $k$-NN ergänzen.

```{r}
mds <- mds %>%
  mutate(groups = clust)
```

Nun sehen in @fig-mds-3 die gleiche Abbildung wie oben nur ergänzt um die farbliche Hinterlegung der $k=5$ Clustern aus dem $k$-NN Algorithmus. Wir können jetzt die Ausreißer numerisch feststellen und dann aus den Daten entfernen, wenn wir dies wollen würden. Entweder machen wir das über die Clusterzuordnung vom gelben Cluster 2 über die Funktion `filter()` oder aber wir suchen uns die Beobachtungen und damit Zeilen raus, die wir nicht mehr in den Daten wollen. Die Nummern stehen ja dabei.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-mds-3
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Scatterplot der zwei Dimensionen nach dem *Multidimensional Scaling* für den Gummibärchendatensatz mit den $k=5$ Clustern aus dem $k$-NN Algorithmus."

ggscatter(mds, x = "Dim.1", y = "Dim.2", 
          label = rownames(animals_df),
          color = "groups",
          palette = "jco",
          size = 1, 
          ellipse = TRUE,
          ellipse.type = "convex",
          repel = TRUE)
```

## Referenzen {.unnumbered}
