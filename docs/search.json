[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bio Data Science",
    "section": "",
    "text": "Willkommen\nIn diesem Kochbuch findest du eine umfangreiche Sammlung an Rezepten aus den Bereichen Biostatistik, Biometrie, Statistik, Bio Data Science und R-Programmierung. Damit kannst du tiefgreifende Kenntnisse erwerben, ohne an meinen Kursen teilnehmen zu müssen. Du bist herzlich eingeladen, hier und dort dich einzulesen, um zu sehen, ob etwas für dich von Interesse ist und um dir dann dein eigenes Menü zusammenzustellen. Das Buch wird kontinuierlich von mir aktualisiert und eigentlich schreibe ich die ganze Zeit mal hier und dort weiter. Siehe dazu auch den Kasten zu den aktuellen Baustellen weiter unten. Zusätzlich zu den Texten stehen dir auch erläuternde YouTube-Videos zur Verfügung. Auf jeden Fall freut es mich, dass du daran interessiert bist, hier etwas Neues zu lernen, sei es aus eigenem Antrieb oder weil du dich auf eine anstehende Klausur vorbereiten möchtest. In jedem Fall empfehle ich dir, dich einfach um zuschauen. Lass dich nicht vom Umfang erschrecken, dass ist so passiert…\nDieses Buch soll wie immer sehr viele Ziele verfolgen. Damit wir nicht das Kind mit dem Bade ausschütten, habe ich im Folgenden nochmal aufgeschlüsselt, was oder wofür du eventuell lernen willst. Es macht nun mal einen nicht unerheblichen Unterschied, ob du die Klausur bestehen willst, eine Abschlussarbeit schreibst oder gar ein Projekt auswerten willst. Im Weiteren wirst du hier ab und zu das Bild eines niedergeschlagenen Engels der Statistik sehen.\nDeshalb ist alles manchmal nicht so mathematisch genau wie es sein könnte, aber es reicht um mit dem Material hier sinnvoll zu arbeiten. Wir wollen ja ins Tun kommen und nicht nur die Sachen hier theoretisch überdenken.",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "index.html#lernen-auf-youtube",
    "href": "index.html#lernen-auf-youtube",
    "title": "Bio Data Science",
    "section": "Lernen auf YouTube",
    "text": "Lernen auf YouTube\nDu liest gerade mein Buch für die Vorlesungen an der Hochschule Osnabrück in der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Um den Stoff, den ich vermitteln möchte, zu erlernen, stehen dir verschiedene Möglichkeiten offen. Neben meinen Vorlesungen kannst du auch auf YouTube lernen, indem du meine Lernvideos anschaust. In den Videos wiederhole ich Inhalte der Vorlesung sowie der R Programmierung. Der große Vorteil ist aber, dass du auf Pause drücken und dir Inhalte wiederholt anschauen kannst. Gerne kannst du dir einmal das Einführungsvideo anschauen.",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "index.html#lernen-für-die-klausur",
    "href": "index.html#lernen-für-die-klausur",
    "title": "Bio Data Science",
    "section": "Lernen für die Klausur",
    "text": "Lernen für die Klausur\nIm Weiteren findest du meine gesammelten Klausurfragen für alle Module auf GitHub unter folgendem Link: gesammelten Klausurfragen auf GitHub oder auf ILIAS im entsprechenden Modul. Die Klausurfragen zu den einzelnen Vorlesungen innerhalb eines Moduls werden in den entsprechenden Übungen behandelt. Zusätzlich gibt es ein Archiv, das alle bisherigen Klausuren über alle Studiengänge hinweg enthält. Dieses Archiv findest du hier: Archive aller bisherigen Klausuren. In der  Playlist der Fragen & Antworten findest du nochmal alle Antworten zu den Klausurfragen kurz besprochen.",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "index.html#lernen-für-ein-projekt",
    "href": "index.html#lernen-für-ein-projekt",
    "title": "Bio Data Science",
    "section": "Lernen für ein Projekt",
    "text": "Lernen für ein Projekt\nDir ist das alles irgendwie zu stückig und gekünstelt? Dann habe ich noch die Spielwiese in R für dich. Dort zeige ich an Beispielen wie die Statistik, das Programmieren in R und die Bio Data Science zusammenkommt. Da es hier dann doch recht eng wurde, habe ich den Code und die Daten für die Auswertungen auf GitHub dann einmal ausgelagert. Die passenden Videos findest du dann in der entsprechenden  Playlist Spielweise in R (Level 3). Vermutlich ist es besser sich erst die Videos anzuschauen und dann nochmal den entsprechenden Code auf GitHub. Gerne kannst du dir einmal das Einführungsvideo für die Playlist der Spielwiese anschauen.",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "index.html#kontakt",
    "href": "index.html#kontakt",
    "title": "Bio Data Science",
    "section": "Kontakt",
    "text": "Kontakt\nNoch Fragen? Wie erreichst du mich? Am einfachsten über die gute, alte E-Mail. Bitte beachte, dass gerade kurz vor den Prüfungen ich mehr E-Mails kriege. Leider kann es dann einen Tick dauern. Einfach an j.kruppa@hs-osnabrueck.de schreiben. Du findest hier auch eine kurze Formulierungshilfe. Bitte gib immer in deiner E-Mail dein Modul - was du belegst - mit an. Pro Semester unterrichte ich immer drei sehr ähnlich klingende Module. Daher schau nochmal hier in der Liste, wenn du unsicher bist.\n\n\n\n\n\n\nE-Mailvorlage mit beispielhafter Anrede\n\n\n\nHallo Herr Kruppa-Scheetz,\n… ich belege gerade Ihr Modul Modulname und hätte eine Bitte/Frage/Anregung…\n… ich benötige Hilfe bei der Planung/Auswertung meiner Bachelorarbeit…\nMit freundlichen Grüßen\nM. Muster",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "index.html#weiteres",
    "href": "index.html#weiteres",
    "title": "Bio Data Science",
    "section": "Weiteres",
    "text": "Weiteres\nHier finden sich wichtige Tools & Tipps für meinen organisatorischen Ablauf in meine Veranstaltungen. Also eigentlich nichts von allgemeinen Interesse.\n\nLink zu Tweedback\n\nLink zu Tweedback innerhalb einer Veranstaltung\n\nUhr für die Prüfung\n\nLink zur Uhr | PTB oder Link zur Uhr | Digital\n\nGummibärchen Short-URL\n\nshorturl.at/iouF4",
    "crumbs": [
      "Willkommen"
    ]
  },
  {
    "objectID": "organisation.html",
    "href": "organisation.html",
    "title": "1  Organisation",
    "section": "",
    "text": "1.1 Statistische Beratung\nNeben der klassischen Vorlesung biete ich auch Termine für die statistische Beratung von Abschlussarbeiten sowie Projekten an. Dieses Angebot gibt es für alle Mitglieder der Hochschule Osnabrück und teilweise für die Universität Osnabrück. Ich berate dabei primär für Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL), aber natürlich auch für alle anderen Fakultäten. Dafür musst du mir einfach eine E-Mail schreiben und dann schicke ich dir Terminvorschläge für ein Online-Meeting innerhalb der nächsten zwei Wochen.\nDie Beratung ist grundsätzlich anonym und vertraulich. Wenn du willst kannst du gerne noch dein:e Betreuer:in mitbringen. Das ist aber keine Voraussetzung oder Notwendigkeit. Meistens finden mehrere Besprechungen statt, wir versuchen aber natürlich zusammen zügig dein Problem zu lösen. Ziel ist der Beratung ist es dich in die Lage zu versetzen selbstständig deine Analyse zu rechnen.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#statistische-beratung",
    "href": "organisation.html#statistische-beratung",
    "title": "1  Organisation",
    "section": "",
    "text": "Abbildung 1.1— “Dad, is this completely safe?” “Research in 500 theme parks in 2010 showed that 1 in 10’000 of carriages were found to be malfunctioning, which can lead to 1 out 50’000 kids falling out… So your chances are okay, but there’s always a risk, honey…” Quelle: wumo.com",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#bachelorarbeit-in-der-bio-data-science",
    "href": "organisation.html#bachelorarbeit-in-der-bio-data-science",
    "title": "1  Organisation",
    "section": "1.2 Bachelorarbeit in der Bio Data Science?",
    "text": "1.2 Bachelorarbeit in der Bio Data Science?\nHier findest du keine Themen. Dafür musst du mich bitte ansprechen oder eine E-Mail schreiben. Die Themen finden sich dann etwa mit Kooperationspartern oder aber eher methodisch ohne echte Daten. Das müssen wir dann aber Absprechen. Bitte bachte dann dafür die Informationen in dem Kapitel zu der Abschlussarbeit.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#master-in-der-bio-data-science",
    "href": "organisation.html#master-in-der-bio-data-science",
    "title": "1  Organisation",
    "section": "1.3 Master in der Bio Data Science?",
    "text": "1.3 Master in der Bio Data Science?\nDisclaimer: Ich bin nicht in dem M.Sc. Studiengang “Umwelt- und Systemwissenschaften” an der Universität Osnabrück involviert.\nGibt es eigentlich einen Master in der Bio Data Science in Osnabrück? Nein. Aber es gibt die Möglichkeit einen ähnlichen Master mit Schwerpunkten in der Bio Data Science, zum Beispiel das Modellieren von Daten, zu studieren. Das wäre dann der Master Umwelt- und Systemwissenschaften der Uni Osnabrück. Bitte beachte in diesem Zusammenhang die aktuellen Informationen für Studierende der Systemwissenschaften. Der Master befasst sich mit der Modellierung von Ökosystemen/Ressourcen und nimmt auch nach jetzigem Stand (SoSe 2023) unkompliziert Studierende mit Abschluss aus dem Bachelorstudiengang Pflanzenbiologie mit der Vertiefung Pflanzentechnologie auf. Das ist eventuell jetzt deine Lösung für Osnabrück! Wenn du woanders hin willst, dann gibt es zig Studiengänge. Da musst du dann aber selber googlen was dich interessiert.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-vorlesungen-hs",
    "href": "organisation.html#sec-vorlesungen-hs",
    "title": "1  Organisation",
    "section": "1.4 Vorlesungen an der Hochschule Osnabrück",
    "text": "1.4 Vorlesungen an der Hochschule Osnabrück\nVon mir angebotene Vorlesungen werden an der Hochschule Osnabrück an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) in ILIAS verwaltet. Alle notwendigen Informationen und Materialien sind auf ILIAS unter https://lms.hs-osnabrueck.de/ zu finden. Wenn du in einem Modul nicht angemeldet bist, dann kontaktiere bitte das Studierendensekretariat. Auch die Kommunikation über Inhalte und Termine sowie Terminausfälle erfolgt von meiner Seite aus über ILIAS.\n\n\n\n\n\n\nAuf ILIAS findest du alle aktuellen Kursinformationen und erhälst auch die Mails, wenn Änderungen im Kursablauf stattfinden.\n\n\n\nWenn du nicht in der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) studierst oder aber in einem Studiengang, der meine Module nicht anbietet, steht es dir natürlich frei, sich in meine Vorlesungen zu setzten. Du findest im Folgenden eine Übersicht der angebotenen Module und auch die inhaltliche Ordnung nach Lernstufe. Bitte informiere dich in deinem Studierendensekretariat über die Modalitäten zur Prüfungsteilnahme.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-entwurf",
    "href": "organisation.html#sec-module-entwurf",
    "title": "1  Organisation",
    "section": "1.5 Modulbeschreibung meiner Vorlesungen",
    "text": "1.5 Modulbeschreibung meiner Vorlesungen\nIn der folgenden Tabelle findest du eine Übersicht der aktuell von mir angebotenen Module nach Semester sortiert. Bitte beachte, dass du nicht einfach ein Modul belegen kannst. Von meiner Seite ist es eher weniger das Problem, aber es muss ja auch in deinen Studiengang passen und anerkannt werden. Andersherum kannst du gerne auch meine Veranstaltungen aus anderen Studiengängen besuchen. Frag mich einfach, dann finden wir eine Lösung.\n\n\n\n\nTabelle 1.1— Angebotene Module der Bio Data Science an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Teilweise werden gleiche Module in verschiedenen Studiengängen unterrichtet. Die Semesterlage entspricht der Lage des Pflichtmoduls. Wahlpflichtmodule liegen in dem ersten möglichen Semester der Belegung. Abweichungen sind wie immer individuell möglich.\n\n\n\n\n\nSemester\nAngewandte Pflanzenbiologie – Gartenbau, Pflanzentechnologie\nLandwirtschaft\nWirtschafts- ingenieurwesen Agrar / Lebensmittel\nBioverfahrenstechnik in Agrar- und Lebensmittelwirtschaft\nAngewandte Nutztier- und Pflanzenwissenschaften\n\n\n\n\n1.\nMathematik und Statistik\nMathematik und Statistik\nStatistik\n\n\n\n\n2.\nAngewandte Statistik und Versuchswesen\n\n\nAngewandte Statistik für Bioverfahrenstechnik (läuft aus)\nBiostatistik\n\n\n3.\n\n\n\n\n\n\n\n4.\n\nModellierung landwirtschaftlicher Daten\n\nStatistische Bioinformatik\n\n\n\n5.\nSpezielle Statistik und Versuchswesen\n\n\n\n\n\n\n\n\n\n\n\nIm Folgenden finden sich hier die work in progress Modulbeschreibungen. Diese werden teilweise von den offiziellen Modulbeschreibungen abweichen. Es handelt sich hierbei um Entwürfe von Modulbeschreibungen deren langfristiges Ziel es ist in den offiziellen Modulbeschreibungen aufzugehen. Dabei werden sicherlich nicht alle Inhalte hier übernommen. Besonders die Prüfungsformen sind Änderungen unterworfen.\n\n\n\n\n\n\nModellierung landwirtschaftlicher Daten (frühestens SoSe 2026)\n\n\n\n\n\nModellierung landwirtschaftlicher Daten\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nDie Modellierung von komplexen Daten spielt eine zentrale Rolle in verschiedenen wissenschaftlichen Disziplinen der Agrarwissenschaften. Dabei ermöglicht die Modellierung nicht nur den Gewinn von neuen Erkenntnissen über biologische Prozesse, sondern auch die Vorhersage zukünftiger Ereignisse weit über die gängigen Gruppenvergleiche aus Feldexperimenten hinaus. Oftmals sind biologische Abhängigkeiten nicht linear, sondern folgen nicht-linearen Trends. Die Modellierung solcher nicht-linearen Abhängigkeiten erfordert spezielle Kompetenzen in der statistischen Analyse, insbesondere von Zeitreihen (eng. “time series”), räumlichen Daten (eng. “spatial data”) oder genetischen Daten und Sequenzen. In dem Modul “Modellierung landwirtschaftlicher Daten” werden die gängigen statistische Verfahren zur Auswertung dieser und weiterer Datenquellen vorgestellt und diskutiert. Im Weiteren spielt die Vorhersage von Ereignissen eine entscheidende Rolle in den Agrarwissenschaften, sei es für die Früherkennung von Risiken oder die Steuerung landwirtschaftlicher Prozesse. Im Modul werden die Grundlagen des maschinellen Lernens präsentiert und anhand von Fallbeispielen erläutert. Das Modul “Modellierung landwirtschaftlicher Daten” vermittelt den Studierenden fortgeschrittene Kenntnisse und Fähigkeiten im Bereich der Datenmodellierung und -analyse im Kontext der landwirtschaftlichen Anwendung. Der Fokus liegt in dem Modul auf der Darstellung, Verarbeitung und statistischer Modellierung komplexer landwirtschaftlicher Daten. Fallstudien aus verschiedenen Bereichen der Agrarwissenschaften werden verwendet, um die erworbenen theoretischen Kenntnisse in die Praxis umzusetzen. In der Anwendung wird R/Bioconductor für die Datenanalyse genutzt. Das Modul “Modellierung landwirtschaftlicher Daten” erweitert die bisherigen Kenntnisse der Studierenden in der Auswertung agrarwissenschaftlicher Daten und bereitet auf anspruchsvolle Aufgaben in diesem Bereich vor.\n\n\nLehr-Lerninhalte\n\nEinführung in die statistische Modellierung sowie deren Interpretation am Beispiel der multiplen linearen Regression.\nBesonderheiten der statistischen Modellierung von Zeitreihen und räumlichen Daten.\nDie explorative Datenanalyse und deren statistischen Maßzahlen sowie die Visualisierung von räumlichen und zeitlichen Daten.\nMultivariate statistische Analysen zur Erkennung von Gruppenzugehörigkeiten anhand von Clusteranalysen.\nEinführung in die klassischen experimentellen Designs in den Agrarwissenschaften.\nGrundlagen der Analyse von genetischen Daten anhand ausgewählter, beispielhafter Omics-Ebenen.\nGenetische Distanzen und polygenetische Bäume zur Darstellung evolutionärer Beziehungen.\nGrundlagen des maschinellen Lernens und der Klassifikation von Ereignissen sowie Maßzahlen der Bewertung eines maschinellen Lernalgorithmus.\nAnwendung der grundlegenden maschinellen Lernverfahren beispielhaft durch k-NN, Random Forest und Neuronale Netze.\nModellierungen an aktuellen Fallbeispielen aus der Anwendung.\nAutomatisierte Erstellung von Berichten in R Quatro.\nEinführung in die Erstellung von interaktiven R Shiny Apps.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\n\nDie Studierenden kennen die gängigen experimentellen Designs in den Agrarwissenschaften.\nDie Studierenden kennen die entsprechenden Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden kennen die gängigen Datenformate für räumliche und zeitliche Daten.\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen in der Genetik zu erkennen und zu benennen.\nDie Studierenden können die gängigen, vorgestellten statistischen Modellierungen benennen und unterscheiden.\nDie Studierenden kennen die gängigen Funktionen für die Modellierungen und Datenaufbereitung in R.\n\n\n\nWissensvertiefung\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können räumliche und zeitliche Daten visualisieren und interpretieren.\nDie Studierenden können das Ergebnis eines statistischen Modells im Kontext einer wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage anhand eines statistischen Modells eine Entscheidung zu treffen.\nDie Studierenden sind in der Lage Modellierungen mit den notwendigen Funktionen und Paketen in R durchzuführen.\nDie Studierenden können einen automatischen Bericht in R Quatro erstellen.\n\n\n\nWissensverständnis\n\nDie Studierenden können ein statistisches Modell mit einer explorativen Datenanalyse oder Visualisierung in einen Kontext bringen.\nDie Studierenden können verschiedene statistisches Modelle anhand verschiedener Maßzahlen miteinander vergleichen und eine informierte Modellauswahl treffen.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von technischen und biologischen Prozessen in der Landwirtschaft. Sie können Modelle für landwirtschaftliche Prozesse unter Verwendung von räumlichen Daten entwickeln und validieren. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage die Ergebnisse der Datenanalyse klar und verständlich zu kommunizieren, sowohl schriftlich als auch mündlich. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren. Die Studierenden sind in der Lage mit einer automatisierten Berichterstattung in R Quatro oder R Shiny eine Datenanalyse zu kommunizieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden können wissenschaftliche Publikationen und deren Modellierungen räumlicher und zeitlicher Fragestellungen in den Kontext des eigenen Berufsfeldes setzen. Unter der Hilfe der Modellierung sind die Studierenden in der Lage informierte Entscheidungen treffen. Die Studierende sind sich der inhärenten Unsicherheit statistischer Modellierungen bewusst und können die eigenen Forschungsergebnisse kritisch hinterfragen. Den Studierenden sind die algorithmischen Grenzen von Modellen bewusst.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nDas Skript zur Genetik und Bioinformatik unter https://jkruppa.github.io/bioinformatic/\n\n\n\nVoraussetzungen für die Teilnahme\nKeine.\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul sind Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik hilfreich, aber nicht notwendig, wie sie unter anderem in den Modulen “Mathematik und Statistik (44B0266)” oder “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen oder erweitern möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Modellierung landwirtschaftlicher Daten” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nSpezielle Statistik und Versuchswesen (44B0390)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nHausarbeit und Präsentation\n\n\n\n\n\n\n\n\n\n\n\nStatistische Bioinformatik (frühestens SoSe 2026)\n\n\n\n\n\nStatistische Bioinformatik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nDie statistische Bioinformatik spielt eine bedeutende Rolle in verschiedenen wissenschaftlichen Bereichen der Omics-Forschung. Sie wird nicht nur klassisch genutzt, um genetische Marker für Krankheiten oder genetische Veranlagungen in der Humangenetik zu identifizieren, sondern auch zur Entschlüsselung der genetischen Vererbung bei der Züchtung von Tieren und Pflanzen. Die algorithmische Auswertung von genetischen Daten ist sowohl biologisch umfassend als auch methodisch anspruchsvoll. Das Modul “Statistische Bioinformatik” folgt dem zentralen Dogma der Molekularbiologie, das besagt, dass Informationen von Genen zu Proteinen übertragen werden. Es beinhaltet die statistische Analyse von genetischen Markern auf DNA-Ebene sowie die Untersuchung der Genexpression über mRNA und die daraus resultierende Proteinexpression bis hin zur Phänomik. Auch regulatorische Elemente der DNA-Expression, wie die Methylierung, werden im Modul berücksichtigt. Das Modul “Statistische Bioinformatik” bietet zuerst eine Einführung in die klassische mendelsche Genetik sowie Grundlagen in der quantitativen Genetik. Den Großteil den Inhalts machen aktuelle algorithmische Verfahren zu Assoziationsstudien und Sequenzanalysen aus. Studierende erlernen somit die grundlegenden Fähigkeiten für zukünftige wissenschaftliche und angewandte Arbeiten im Bereich der Omics-Forschung. Der Schwerpunkt des Moduls liegt auf der Darstellung, Verarbeitung und statistischen Auswertung von hochdimensionalen genetischen Daten. Fallstudien aus den Bereichen der Agrarwissenschaften und Humanbiologie dienen als praktische Anwendungen. Gleichzeitig werden die erworbenen theoretischen Kenntnisse durch die Auswertung von Daten in R/Bioconductor in die Praxis umgesetzt. Das Modul “Statistische Bioinformatik” legt somit den Grundstein für die methodische Auswertung von Omics-Daten.\n\n\nLehr-Lerninhalte\n\nGrundlagen der klassischen, mendelschen Vererbung beinhaltend das Hardy-Weinberg-Gleichgewicht\nGrundlagen der Quantitativen Genetik mit Kopplungskarten und Linkage disequilibrium\nGenetische Distanz und polygenetische Bäume zur Darstellung evolutionärer Beziehungen\nMarkergestützte Selektion und indirekte Selektion\nStatistische Herausforderungen von hochdimensionalen Daten\nData Preprocessing und Qualitätskontrolle auf der Ebene der Marker und Individuen\nGängige Assoziationstests unter anderem Chi-Quadrat-Test und logistische Regression\nAuswertung von Genomeweite Assoziationsstudien, Microarraydaten, High throughput genotyping, Next generation sequencing, Whole genome sequencing und RNA-seq an Fallbeispielen\nVisualisierungen der Ergebnisse durch Manhattanplot, Vucanoplot und Regional Association Plot und weiteren.\nGrundlagen Multiomics und Pathway Analysen sowie deren Datenbanken\nGrundlegende Methoden zum Sequenzaligment\nAnwendung der Algorithmen an ausgewählten Fallbeispielen in R/Bioconductor\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu den jeweiligen Omics-Verfahren zu erkennen und zu benennen.\nDie Studierenden können das zentrale Dogma der Molekularbiologie erklären und visualisieren.\nDie Studierenden sind in der Lage den Unterschied zwischen der quantitativen und qualitativen Genetik zu erklären.\nDie Studierenden können Omics-Abbildungen erkennen und benennen.\nDie Studierenden können externe Programme aus R heraus starten und Ergebnisse einlesen.\n\n\n\nWissensvertiefung\n\nDie Studierenden können explorative Abbildungen in ausgewählten Omics-Analysen erstellen und interpretieren.\nDie Studierenden sind in der Lage anhand einer Omics-Analyse eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis einer Omics-Analyse im Kontext der wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage aus englischen Tutorien eine Lösung für ein Omics-Verfahren einzugrenzen.\nDie Studierenden können einfache Auswahl- und Filterregeln auf Datensätze in R anwenden.\nDie Studierenden können erste einfache R Code Blöcke miteinander in einen Kontext setzen.\n\n\n\nWissensverständnis\n\nDie Studierenden können verschiedene Ebenen der Omics-Analysen in einen wissenschaftlichen Kontext bringen und miteinander vergleichen.\nDie Studierenden sind in der Lage eine genetische Auswertung mit den notwendigen Funktionen und Paketen in R zu skizzieren und zu erklären.\nDie Studierenden können die einzelnen Schritte einer genetischen Analyse benennen und bei der Planung eines eigenen Experiments berücksichtigen. Sie identifizieren dabei die Probleme der jeweiligen biologischen Proben.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache Kosten- und Nutzenabschätzungen anhand von genetischen Analysen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in der Agrarwirtschaft und den Lebenswissenschaften. Sie können dabei die verschiedenen Ebenen der Omics-Foschung in den Kontext der Anwendung und der Phänomik setzen. Die Studierenden können dabei externe Literaturquellen und deren genetischen Analysen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen für die eigenen Analysen treffen.\n\n\nWissenschaftliche Innovation\nDie Studierende können genetische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen in der Genetik. Die Studierenden können genetische Visualisierungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage genetische Daten und Analysen mit anderen Forschenden zu teilen. Sie können die einzelnen Schritte der Analyse erklären und die Ergebnisse allgemein verständlich erklären. Ebenfalls sind die Studierenden in der Lage gängige genetische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden können wissenschaftliche Publikationen und deren genetischen Maßzahlen sowie Ergebnisse in den Kontext des eigenen Berufsfeldes setzen und somit informierte Entscheidungen treffen. Diese informierten Entscheiden betreffen sowohl die praktische Anwendung wie auch die Bewertung von Forschungsideen in der wissenschaftlichen Grundlagenforschung. Die Studierende sind sich der inhärenten Unsicherheit der wissenschaftlichen, genetischen Forschung bewusst und können die eigenen Forschungsergebnisse diesbezüglich kritisch hinterfragen.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nDas Skript zum Bioinformatikteil unter https://jkruppa.github.io/bioinformatic/\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. https://r4ds.had.co.nz\n\n\n\nVoraussetzungen für die Teilnahme\nKeine.\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul sind Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik hilfreich aber nicht notwendig, wie sie unter anderem in den Modulen “Mathematik und Statistik (44B0266)” oder “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen oder erweitern möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Statistische Bioinformatik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nEinführung in die Pflanzenzüchtung (44B0112)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nHausarbeit und Präsentation\n\n\n\n\n\n\n\n\n\n\n\nMathematik und Statistik\n\n\n\n\n\nMathematik und Statistik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIn den Pflanzenwissenschaften sowie in der Landwirtschaft werden vielen Prozesse und Phänomene durch mathematische und statistische Modelle beschrieben. In dem Modul “Mathematik und Statistik” lernen Studierende drei Schwerpunkte für das spätere wissenschaftliche und angewandte Arbeiten. Im ersten Teil des Moduls werden mathematische Grundkenntnisse wiederholt und im Verlauf des Moduls vertieft. Die mathematischen Formeln werden aus ihrer theoretischen, formalistischen Anwendung herausgelöst und in praktische Herausforderungen übertragen. Dabei werden Bereiche der Physik, Chemie sowie Biologie in den Kontext der Mathematik gesetzt. Im zweiten Teil des Moduls werden statistische Grundkenntnisse vermitteln. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Die für Landwirtschaft und Gartenbau relevanten mathematischen und statistischen Verfahren werden dargestellt und diskutiert. Im dritten Teil des Moduls werden die erworbenen theoretischen, mathematischen und statistischen Kenntnisse durch die Einführung in die Programmierung in R für die Studierenden umsetzbar und erfahrbar gemacht. In dem Modul “Mathematik und Statistik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben.\n\n\nLehr-Lerninhalte\nMathematischer Anteil\n\nMaßzahlen, Flächen und Volumen beinhaltend Berechnungen mit Maßeinheiten von sehr kleinen sowie sehr großen Zahlen. Berechnungen mit Flächen- sowie Volumenmaßen einschließlich Winkel- und Streckenbestimmung.\nBerechnungen mit Vektoren und Matrizen.\nMathematische Funktionen und Anwendung der Differential- und Integralrechnung einschließlich logarithmischer sowie exponentieller Funktionen. Lösung von quadratischer Gleichungen sowie Extremwertproblemen.\nWahrscheinlichkeiten mit Baumdiagramm und Pfadregeln sowie stochastische Prozesse. Wahrscheinlichkeitsverteilungen am Beispiel der Normalverteilung.\nLogische Operatoren sowie Mengenlehre.\n\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse mit Fokus auf dem Boxplot und dem Barplot und deren statistischen Maßzahlen.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test. Einführung in die Varianzanalyse.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nInformatorischer Anteil\n\nEinführung in die Programmierung in R anhand von Skalenarten sowie der Darstellung von Daten in R.\nKonzept von Objekten, Funktionen sowie Pipen und der Vorstellung des tidyverse in R.\nEinlesen von Daten und deren Bearbeitung sowie Visualisierung in R\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nMathematischer Anteil\n\nDie Studierenden sind in der Lage mathematische Formeln in der Literatur zu finden.\nDie Studierenden können ein Baumdiagramm für die Berechnung von Wahrscheinlichkeiten erstellen.\n\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen.\nDie Studierenden können einfache explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nInformatorischer Anteil\n\nDie Studierenden können die Anforderungen an einen Datensatz zur Verwendung in R benennen.\nDie Studierenden können in R Objekte, Funktionen und Zahlenvektoren unterscheiden und kennen die gängigen Operatoren in R.\nDie Studierenden können den Ablauf für die Erstellung einer explorativen Datenanalyse in R beschreiben.\n\n\n\nWissensvertiefung\nMathematischer Anteil\n\nDie Studierenden sind in der Lage mathematische Formeln in einem anwendungsorientierten Kontext anzuwenden.\nDie Studierenden können sinnvolle Abschätzungen von linearen und exponentiellen Wachstum vornehmen.\n\nStatistischer Anteil\n\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\n\n\n\nWissensverständnis\nMathematischer Anteil\n\nDie Studierenden können praktische Fragestellungen in einen formalisierten, mathematischen Kontext übersetzen.\nDie Studierenden sind in der Lage die Wahrscheinlichkeit für das Eintreten eines Ereignisses abzuschätzen.\n\nStatistischer Anteil\n\nDie Studierenden können einen einfachen statistischen Test mit einer explorativen Datenanalyse in einen Kontext setzen.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare und exponentielle Kosten- und Nutzenabschätzungen anhand von mathematischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von technischen und biologischen Prozesses in den Pflanzenwissenschaften sowie in der Landwirtschaft. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können eine Reihe von explorativen Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können einfachen R Code lesen und demonstrieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Mathematikteils des Moduls unter https://jkruppa.github.io/math/\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\n\n\n\nVoraussetzungen für die Teilnahme\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nEmpfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Mathematik und Statistik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nAngewandte Statistik und Versuchswesen (44B0400)\nChemie und Biochemie (44B0532)\nPhysikalische Grundlagen der Natur und Agrartechnik (44B0534)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur\n\n\n\n\n\n\n\n\n\n\n\nAngewandte Statistik und Versuchswesen\n\n\n\n\n\nAngewandte Statistik und Versuchswesen\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nDer wissenschaftliche Fortschritt in den Agrarwissenschaften ist wesentlich getragen durch eine intensive experimentelle Versuchstätigkeit. Um erfolgreich in diesem Bereich tätig zu sein sind neben statistischen Kenntnissen auch solche über die Techniken zur Versuchsdurchführung erforderlich. Für die Versuchsdurchführung müssen Messdaten und Beobachtungen aus Erhebungen sowie aus experimentellen Versuchen in einem Datensatz aufgearbeitet werden. In dem Modul “Angewandte Statistik und Versuchswesen” lernen Studierende die grundlegenden Algorithmen der Statistik für das spätere wissenschaftliche und angewandte Arbeiten kennen. Das Modul vermittelt die dafür notwendigen statistischen und algorithmischen praktischen Kenntnisse. Verschiedene statistische Verfahren zur Auswertung von experimentellen Daten werden vorgestellt und die statistischen Maßzahlen für das lineare Modellieren eingeübt. Einfache experimentelle Designs werden vorgestellt und Anwendungsmöglichkeiten diskutiert. Die vorhandenen Programmierkenntnisse in R werden weiter vertieft. Verschiedene einfache Fallbeispiele dienen als Einstieg für die Diskussion und der Reflexion der eigenen Versuchstätigkeit. Das Modul “Angewandte Statistik und Versuchswesen” schließt den Erwerb der Grundlagen in der Bio Data Science ab und ermöglicht den Studierenden somit einfache Experimente in den Agrarwissenschaften selbstständig zu planen und auszuwerten.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nDie explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in statistische Verteilungen anhand der Poisson- und Normalverteilung.\nDie Varianzanalyse beinhaltend die einfaktorielle sowie zweifaktorielle ANOVA.\nGrundlagen des nicht-parametrischen Tests beinhaltend Wilcoxon-Mann-Whitney-Test sowie Kruskal-Wallis-Test.\nGrundlagen der simplen linearen Regression und der multiplen linearen Regression sowie deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nDiagnostischen Testen und deren statistischen Maßzahlen.\nChi-Quadrat-Test für eine Vierfeldertafel.\nDas multiple Testen von mehreren Mittelwerten und deren Visualisierungen.\nEinführung in die klassischen experimentellen Designs in den Agrarwissenschaften sowie die einfache Versuchsplanung.\n\nInformatorischer Anteil\n\nDurchführung aller theoretisch erarbeiteten Inhalte in R.\nInterpretation und Bewertung von einfachen statistischen Modellierungen in R.\nEinfache Transformationen von Daten für die explorative Datenanalyse.\nDemonstration der automatisierten Erstellung von Berichten in Rmarkdown sowie in R Quatro.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden kennen einfache experimentelle Designs in den Agrarwissenschaften.\nDie Studierenden kennen einfache Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\nDie Studierenden sind in der Lage zwischen einen parametrischen und einem nicht-parametrischen Test zu unterscheiden.\n\nInformatorischer Anteil\n\nDie Studierenden kennen die gängigen Funktionen für die Datenaufbereitung in R.\nDie Studierenden können den Ablauf für die Erstellung einer einfachen Datenanalyse in R beschreiben.\nDie Studierenden sind in der Lage aus englischen Internetquellen eine Lösung für ein R Problem einzugrenzen.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können eine simple lineare Regression für eine Normalverteilung modellieren.\nDie Studierenden können eine Aussage über die Güte eines simplen linearen Modells abgeben.\nDie Studierenden können eine Korrelation berechnen und interpretieren.\nDie Studierenden können einen multiplen Gruppenvergleich für einen normalverteilten Endpunkt rechnen und die p-Werte entsprechend adjustieren.\nDie Studierenden sind in der Lage eine einfache explorative Datenanalyse mit einem multiplen Gruppenvergleich zu verbinden.\n\nInformatorischer Anteil\n\nDie Studierenden können Datensätze in R bearbeiten.\nDie Studierenden können einfache experimentelle Designs in R visualisieren.\nDie Studierenden können verschiedene Ausgaben von statistischen Tests in R visualisieren.\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden sind die der Lage eine wissenschaftliche Fragestellung mit einem einfachen experimentellen Design zu verbinden.\nDie Studierenden können einfache linearen Modellierungen bewerten und interpretieren.\n\nInformatorischer Anteil\n\nDie Studierenden können verschiedene statistische Tests und eine lineare Modellierung mit einer explorativen Datenanalye in einen Kontext setzen.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache Kosten- und Nutzenabschätzungen anhand von statistischen linearen Modellen durchzuführen. Diese Abschätzungen umfassen im Besonderen die Planung von einfachen experimentellen Designs in den Agrarwissenschaften. Die Studierenden können statistische Unterschiede aus multiplen Gruppenvergleichen berechnen und eine Risikoabschätzung treffen. Die Studierenden sind in der Lage selbständig einfache statistische Analysen auf Datensätzen in R durchzuführen. Die Studierenden können einfache experimentelle Designs für verschiedene Berufsfelder und Anwendungen abwägen und diskutieren.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden können selbständig eigene wissenschaftliche Fragestellungen mit Fallbeispielen abgleichen und entsprechend der eigenen Anforderungen modifizieren. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die wissenschaftliche Verwertbarkeit in R zu gewährleisten. Die Studierenden kennen die Möglichkeit über automatisierte Berichte die Reproduzierbarkeit der eigenen Forschungsergebnisse zu gewährleisten.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage die Daten der durchgeführten Experimente und entsprechende R Skripte der statistische Auswertungen mit anderen Forschenden zu teilen. Die Studierenden können die statistischen Analyseergebnisse vorstellen und Änderungswünsche entsprechend durchführen.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\nData Science for Agriculture in R unter https://schmidtpaul.github.io/DSFAIR/\n\n\n\nVoraussetzungen für die Teilnahme\nKeine.\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul werden Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie in dem Modul “Mathematik und Statistik (44B0266)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Angewandte Statistik und Versuchswesen” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nSteuerung der vegetativen Entwicklung krautiger Pflanzen (44B0608)\nProjektplanung und -management (44B0654)\nBerufspraktisches Projekt (BAP) (44B0595)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur\n\n\n\n\n\n\n\n\n\n\n\nSpezielle Statistik und Versuchswesen\n\n\n\n\n\nSpezielle Statistik und Versuchswesen\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nMit dem Fortschreiten der Digitalisierung können in den Pflanzenwissenschaften und der Landwirtschaft komplexere Experimente durchgeführt werden. Die Digitalisierung erlaubt die automatisierte Erfassung und Speicherung großer Datenmengen, die über entsprechende statistische Algorithmen aggregiert und ausgewertet werden müssen. Diese Daten können zur Steuerung der Produktion oder zur Erkennung von unerwünschten Ereignissen genutzt werden. Dadurch kann eine bessere Qualitätssicherung und Entwicklung gewährleistet werden. In dem Modul “Spezielle Statistik und Versuchswesen” lernen Studierende die fortgeschrittenen Algorithmen für das spätere wissenschaftliche und angewandte Arbeiten mit großen Datenmengen. Das Modul vermittelt die dafür notwendigen statistischen und algorithmischen praktischen Kenntnisse. Verschiedene statistische Verfahren werden vorgestellt und die statistischen Maßzahlen für die Modellselektion eingeübt. Im Weiteren werden maschinelle Lernverfahren präsentiert und auf Fallbeispiele angewendet. Der Fokus des Moduls liegt auf der praktischen Anwendung und Diskussion der Ergebnisse der statistischen Modellierungen. Die vorhandenen Programmierkenntnisse in R werden weiter vertieft und automatisierte Berichtserstellung mit Quarto und RMarkdown eingeübt. Das Arbeiten mit großen Datenmengen wird so für die Studierenden umsetzbar und erfahrbar gemacht. Das Modul “Spezielle Statistik und Versuchswesen” befähigt Studierende in dem Bereich der Bio Data Science in verschiedenen Anwendungsfeldern praktisch tätig zu sein.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die gängigen multiplen linearen Regressionen und deren Verteilungsfamilien beinhaltend die Gaussian, Poisson, Multinominal/Ordinal und Binomial.\nGrundlagen der statistischen Maßzahlen der Modellgüte einer multiplen linearen Regression sowie deren Effektschätzer.\nGrundlagen der Variablenselektion und Imputation von fehlenden Werten sowie Ausreißerdetektion.\nEinführung in die linearen gemischten Modelle und die Berücksichtigung von Messwiederholungen.\nEinführung in die nicht lineare Regression.\nVertiefte Auseinandersetzung mit multiplen Gruppenvergleichen und deren Möglichkeiten der Visualisierung von Gruppenunterschieden.\nEinführung in die Äquivalenz oder Nichtunterlegenheit in der praktischen Anwendung.\nEinführung in die klassischen experimentellen Designs in den Agrarwissenschaften.\nGrundlagen des maschinellen Lernens und der Klassifikation von Ereignissen sowie Maßzahlen der Bewertung eines maschinellen Lernalgorithmus.\nAnwendung der grundlegenden maschinellen Lernverfahren wie k-NN, Random Forest, Support Vector Machine und Neuronale Netze.\n\nInformatorischer Anteil\n\nDurchführung aller theoretisch erarbeiteten Inhalte in R.\nInterpretation und Bewertung von statistischen Modellierungen in R.\nFortgeschrittene Programmierung in R unter der Verwendung von regulären Ausdrücken.\nAutomatisierte Erstellung von Berichten in Rmarkdown sowie in R Quatro.\nEinführung in die Erstellung von interaktiven R Shiny Apps.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden kennen die gängigen experimentellen Designs in den Agrarwissenschaften.\nDie Studierenden kennen die entsprechenden Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden können die gängigen statistischen Modellierungen benennen und unterscheiden.\nDie Studierenden sind in der Lage zwischen einem kausalen und einem prädiktiven Modell zu unterscheiden.\n\nInformatorischer Anteil\n\nDie Studierenden kennen die gängigen Funktionen für die Datenaufbereitung in R.\nDie Studierenden sind in der Lage aus englischsprachigen Tutorien die statistische Analyseschritte für die eigenen Daten zu transferieren.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage anhand einer wissenschaftlichen Fragestellung eine statistische Auswertung zu gliedern und zu planen.\nDie Studierenden können wissenschaftliche Veröffentlichungen lesen und in den statistischen Kontext richtig einordnen.\nDie Studierenden können eine multiple lineare Regression oder einen maschinellen Lernalgorithmus entsprechend des Endpunktes modellieren und interpretieren.\nDie Studierenden können einen multiplen Gruppenvergleich für verschiedene Endpunkte rechnen und die p-Werte entsprechend adjustieren.\nDie Studierenden können verschiedene technische Messparameter miteinander vergleichen und eine Aussage über die Nichtunterlegenheit treffen.\n\nInformatorischer Anteil\n\nDie Studierenden können mit regulären Ausdrücken Datensätze bearbeiten.\nDie Studierenden sind in der Lage durch eine eine parallele Programmierung eine serielle Programmierungen zu optimieren.\nDie Studierenden sind in der Lage einen automatisierten Bericht in Rmarkdown oder R Quarto zu erstellen\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden sind die der Lage eine wissenschaftliche Fragestellung mit einem experimentellen Design und einer statistischen Modellierung zu verbinden.\nDie Studierenden können eine statistische Modellierung in einer Präsentation darstellen und vorstellen.\nDie Studierenden können eine wissenschaftliche Veröffentlichung anhand der verwendeten Statistik bewerten.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage in R eine statistische Modellierung zu planen und den entsprechenden R Code zu erstellen.\nDie Studierenden können R Code Chunks miteinander sinnvoll für die eigene Anwendung kombinieren und optimieren.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage Kosten- und Nutzenabschätzungen anhand von statistischen Modellen und deren Effektschätzern durchzuführen. Diese Abschätzungen umfassen im Besonderen die Planung von technischen und biologischen Prozesses in den Agrarwissenschaften. Die Studierenden können verschiedene technische Prozesse miteinander vergleichen und eine Aussage über die Nichtunterlegenheit oder den statistischen Unterschied treffen. Die beiden gegensätzlichen Konzepte von einem geplanten Experiment und einer technischen Nichtunterlegenheit können von den Studierenden unterschieden werden. Die Studierenden sind in der Lage selbständig Datenanalysen auf großen Datensätzen in R durchzuführen. Die Studierenden können die gängigen experimentellen Designs für verschiedene Berufsfelder und Anwendungen anpassen und durchführen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden sind in der Lage wissenschaftlich zu Arbeiten und eine praktische Fragestellung in einen wissenschaftlichen Erkenntnisprozess zu übersetzen. Die Studierenden können statistische Auswertungen aus wissenschaftlichen Publikationen verstehen und informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die wissenschaftliche Verwertbarkeit in R zu berücksichtigen. Die Studierenden können über die Erstellung von automatisierten Berichten die Reproduzierbarkeit der eigenen Forschungsergebnisse gewährleisten.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept der automatisierten Berichtserstattung durchgeführte Experimente und statistische Auswertungen mit anderen Forschenden zu teilen. Die Studierenden sind dadurch in der Lage in multidiziplinären, wissenschaftlichen Teams mitzuwirken. Die Studierenden können eine gemeinsam geplante Forschungsskizze in R umsetzen. Die Studierenden sind in der Lage die Ergebnisse einer statistischen Analyse auch Fachfremden zu erläutern.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden können wissenschaftliche Publikationen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeldes setzen und somit informierte Entscheidungen treffen. Die Studierende sind sich der inhärenten Unsicherheit der wissenschaftlichen Forschung bewusst und können die eigenen Forschungsergebnisse kritisch hinterfragen.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\nData Science for Agriculture in R unter https://schmidtpaul.github.io/DSFAIR/\nBruce, Peter, Andrew Bruce, und Peter Gedeck. 2020. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media.\n\n\n\nVoraussetzungen für die Teilnahme\nKeine. Bitte aber die empfohlenen Vorkenntnisse beachten.\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul werden vertiefte Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie in den Modulen “Mathematik und Statistik (44B0266)” und “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Spezielle Statistik und Versuchswesen” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nProjektauswertung und -vorstellung (44B0597)\nBerufspraktisches Projekt (BAP) (44B0595)\nBachelorarbeit (44B0365)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nHausarbeit\n\n\n\n\n\n\n\n\n\n\n\nStatistik\n\n\n\n\n\nStatistik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIn der Agrarwirtschaft, Lebensmittelwissenschaft und Gartenbau werden vielen Prozesse und Phänomene durch mathematische und statistische Modelle beschrieben. Entwicklung, Qualitätssicherung und Marketing sind wesentlich getragen durch eine statistische Analyse von Daten. In dem Modul “Statistik” lernen Studierende die Grundlagen für das spätere wissenschaftliche und angewandte Arbeiten. Das Modul vermittelt die dafür notwendigen statistischen Grundkenntnisse. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Die für die Agrarwirtschaft, Lebensmittelwissenschaft und Gartenbau relevanten statistischen Verfahren und Modellierungen werden dargestellt und diskutiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt. Parallel dazu werden in dem Modul die erworbenen theoretischen, statistischen Kenntnisse durch die Einführung in die Programmierung in R für die Studierenden umsetzbar und erfahrbar gemacht. In dem Modul “Statistik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in statistische Verteilungen anhand der Poisson- und Normalverteilung.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test.\nEinführung in die Varianzanalyse beinhaltend die einfaktorielle sowie zweifaktorielle ANOVA.\nGrundlagen des nicht-parametrischen Tests beinhaltend Wilcoxon-Mann-Whitney-Test sowie Kruskal-Wallis-Test.\nGrundlagen der simplen linearen Regression und der multiplen linearen Regression sowie deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nDiagnostischen Testen und deren statistischen Maßzahlen.\nChi-Quadrat-Test für eine Vierfeldertafel.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nInformatorischer Anteil\n\nEinführung in die Grundlagen der Programmierung in R anhand von Skalenarten.\nEinführung in die Darstellung von Daten in R und die Vorstellung des Konzepts der “tidy data”.\nKonzept von Objekten, Funktionen sowie Pipen und der Vorstellung des tidyverse in R.\nEinlesen von Daten und deren Bearbeitung sowie Visualisierung in R.\nDurchführung der gängigen statistischen Tests und die Interpretierung der jeweiligen R Ausgaben.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen und zu benennen.\nDie Studierenden können explorative Abbildungen erkennen und benennen.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nInformatorischer Anteil\n\nDie Studierenden können die Anforderungen an einen Datensatz zur Verwendung in R benennen.\nDie Studierenden können in R Objekte, Funktionen und Zahlenvektoren unterscheiden und kennen die gängigen Operatoren in R.\nDie Studierenden können den Ablauf für die Erstellung einer explorativen Datenanalyse in R beschreiben.\nDie Studierenden sind in der Lage aus englischen Internetquellen eine Lösung für ein R Problem einzugrenzen.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können aus explorative Abbildungen die entsprechende Datenstruktur zur Erstellung der Abbildungen wiedergeben.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage experimentelle Daten nach dem Konzept der der “tidy data” zu erheben.\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\nDie Studierenden können einfache Auswahl- und Filterregeln auf Datensätze in R anwenden.\nDie Studierenden können erste einfache R Code Blöcke miteinander in einen Kontext setzen.\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden können einen statistischen Test mit einer explorativen Datenanalyse in einen Kontext bringen.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage eine einfache statistische Auswertung mit den notwendigen Funktionen und Paketen in R zu skizzieren.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in den Agrarwirtschaften, Lebensmittelwissenschaften und Gartenbau. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen. Die Studierenden sind in der Lage die grundlegenden Konzepte der Programmierung in R in anderen Programmiersprachen zuerkennen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\n\n\n\nVoraussetzungen für die Teilnahme\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nEmpfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Statistik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nMarketing Praxis (44B0547)\nWeb Engineering (44B0585)\nWirtschaftsinformatik (44B0577)\nWissenschaftliches Arbeiten und Kommunikation (44B0573)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur\n\n\n\n\n\n\n\n\n\n\n\nAngewandte Statistik für Bioverfahrenstechnik\n\n\n\n\n\nAngewandte Statistik für Bioverfahrenstechnik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIn der Agrarwirtschaft und Ingenieurwissenschaften werden vielen Prozesse und Phänomene durch statistische Modelle beschrieben. Verfahrenstechnik, Qualitätssicherung und Marketing sind wesentlich getragen durch eine statistische Analyse von Daten. In dem Modul “Angewandte Statistik für Bioverfahrenstechnik” lernen Studierende die Grundlagen für das spätere wissenschaftliche und angewandte Arbeiten. Das Modul vermittelt die dafür notwendigen statistischen Grundkenntnisse. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Die für die Agrarwirtschaft und Ingenieurwissenschaften relevanten statistischen Verfahren und Modellierungen werden dargestellt und diskutiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt. Parallel dazu werden in dem Modul die erworbenen theoretischen, statistischen Kenntnisse durch die Einführung in die Programmierung in R für die Studierenden umsetzbar und erfahrbar gemacht. In dem Modul “Angewandte Statistik für Bioverfahrenstechnik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in statistische Verteilungen anhand der Poisson- und Normalverteilung.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test.\nEinführung in die Varianzanalyse beinhaltend die einfaktorielle sowie zweifaktorielle ANOVA.\nGrundlagen des nicht-parametrischen Tests beinhaltend Wilcoxon-Mann-Whitney-Test sowie Kruskal-Wallis-Test.\nGrundlagen der simplen linearen Regression und der multiplen linearen Regression sowie deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nDiagnostischen Testen und deren statistischen Maßzahlen.\nChi-Quadrat-Test für eine Vierfeldertafel.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nInformatorischer Anteil\n\nEinführung in die Grundlagen der Programmierung in R anhand von Skalenarten.\nEinführung in die Darstellung von Daten in R und die Vorstellung des Konzepts der “tidy data”.\nKonzept von Objekten, Funktionen sowie Pipen und der Vorstellung des tidyverse in R.\nEinlesen von Daten und deren Bearbeitung sowie Visualisierung in R.\nDurchführung der gängigen statistischen Tests und die Interpretierung der jeweiligen R Ausgaben.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen und zu benennen.\nDie Studierenden können explorative Abbildungen erkennen und benennen.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nInformatorischer Anteil\n\nDie Studierenden können die Anforderungen an einen Datensatz zur Verwendung in R benennen.\nDie Studierenden können in R Objekte, Funktionen und Zahlenvektoren unterscheiden und kennen die gängigen Operatoren in R.\nDie Studierenden können den Ablauf für die Erstellung einer explorativen Datenanalyse in R beschreiben.\nDie Studierenden sind in der Lage aus englischen Internetquellen eine Lösung für ein R Problem einzugrenzen.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können aus explorative Abbildungen die entsprechende Datenstruktur zur Erstellung der Abbildungen wiedergeben.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage experimentelle Daten nach dem Konzept der der “tidy data” zu erheben.\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\nDie Studierenden können einfache Auswahl- und Filterregeln auf Datensätze in R anwenden.\nDie Studierenden können erste einfache R Code Blöcke miteinander in einen Kontext setzen.\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden können einen statistischen Test mit einer explorativen Datenanalyse in einen Kontext bringen.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage eine einfache statistische Auswertung mit den notwendigen Funktionen und Paketen in R zu skizzieren.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in der Agrarwirtschaft und Ingenieurwissenschaften. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen. Die Studierenden sind in der Lage die grundlegenden Konzepte der Programmierung in R in anderen Programmiersprachen zuerkennen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. https://r4ds.had.co.nz\n\n\n\nVoraussetzungen für die Teilnahme\nInhalte des Moduls Mathematik für Bioverfahrenstechnik (44B0609)\n\n\nEmpfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Angewandte Statistik für Bioverfahrenstechnik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nProduktionssystem Pflanze (44B0261)\nEinführung in die Pflanzenzüchtung (44B0112)\nMessen, Regeln und Auswerten in der Biosystemtechnik (44B0549)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur\n\n\n\n\n\n\n\n\n\n\n\nAngewandte Mathematik und Statistik für Bioverfahrenstechnik (frühestens SoSe 2026)\n\n\n\n\n\nAngewandte Mathematik und Statistik für Bioverfahrenstechnik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIn der Agrarwirtschaft und Ingenieurwissenschaften werden vielen Prozesse und Phänomene durch statistische Modelle beschrieben. Verfahrenstechnik, Qualitätssicherung und Marketing sind wesentlich getragen durch eine statistische Analyse von Daten. In dem Teil des Moduls “Angewandte Statistik für Bioverfahrenstechnik” lernen Studierende die Grundlagen für das spätere wissenschaftliche und angewandte Arbeiten. Das Modul vermittelt die dafür notwendigen statistischen Grundkenntnisse. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt. In dem Modul “Angewandte Statistik für Bioverfahrenstechnik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben. Der Teil “Angewandte Mathematik für Bioverfahrenstechnik” des Moduls ist die Vertiefung und Erweiterung des Moduls “Mathematik für Bioverfahrenstechnik”. Schwerpunkt soll auf der Auswahl und Anwendung wichtiger mathematischer Werkzeuge liegen. Während im Modul “Mathematik für Bioverfahrenstechnik” Wert auf grundlegende Rechentechniken gelegt wird, ist in diesem Modul die Unterstützung durch die Software MATLAB zentrales Merkmal. Die grundlegende Funktionsweise der Software soll in diesem Rahmen vermittelt werden. Mittels der integrierten Funktionen der Software sollen die Studierenden lernen mathematische Berechnungen und numerische Analysen durchzuführen und zu visualisieren.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test.\nEinführung in die Varianzanalyse beinhaltend die einfaktorielle ANOVA.\nGrundlagen der simplen linearen Regression und deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nMathematischer Anteil\n\nVertiefung der Anwendung mathematischer Lösungsansätze und Methoden\nGrundlegende Einführung in die Software MATLAB\nAnwendung der Software MATLAB\nNäherungen und Näherungsmethoden\nSysteme von Differentialgleichungen\nVisualisierung von Daten\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen und zu benennen.\nDie Studierenden können explorative Abbildungen erkennen und benennen.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nMathematischer Anteil\n\nDie Studierenden sind in der Lage Lösungsverfahren auszuwählen, anzuwenden um Problemlösungen korrekt zu erarbeiten.\nDie Studierenden können Näherungsmethoden auswählen und anwenden.\nDie Studierenden sind in der Lage Daten zu visualisieren.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können aus explorative Abbildungen die entsprechende Datenstruktur zur Erstellung der Abbildungen wiedergeben.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\n\nMathematischer Anteil\n\nDie Studierenden können mathematische Problemstellungen unter Zuhilfenahme der Software MATLAB lösen.\nDie Studierenden sind in der Lage Näherungen mittels geeigneter Näherungsmethoden sinnvoll durchzuführen.\nDie Studierenden können Systeme von linearen Differentialgleichungen lösen.\nDie Studierenden können Ergebnisse zwei- und dreidimensional graphisch darstellen.\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden können einen statistischen Test mit einer explorativen Datenanalyse in einen Kontext bringen.\n\nMathematischer Anteil\n\nDie Studierenden können Aufgabenstellungen interpretieren, Lösungsverfahren zuordnen und unter Zuhilfenahme von Software korrekt anwenden.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in der Agrarwirtschaft und Ingenieurwissenschaften. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen. Die Studierenden sind in der Lage die grundlegenden Konzepte der Programmierung in R in anderen Programmiersprachen zuerkennen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. https://r4ds.had.co.nz\n\n\n\nVoraussetzungen für die Teilnahme\nInhalte des Moduls Mathematik für Bioverfahrenstechnik (44B0609)\n\n\nEmpfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Angewandte Statistik für Bioverfahrenstechnik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nProduktionssystem Pflanze (44B0261)\nEinführung in die Pflanzenzüchtung (44B0112)\nMessen, Regeln und Auswerten in der Biosystemtechnik (44B0549)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur\n\n\n\n\n\n\n\n\n\n\n\nBiostatistik\n\n\n\n\n\nBiostatistik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIm Masterstudiengang Nutztier- und Pflanzenwissenschaften wird angewandte Forschungs- und Entwicklungskompetenz in den Forschungsfeldern der Agrarwissenschaften vermittelt. Studierende lernen, wie sie aus Daten, die sich aus technischen Prozessen und wissenschaftlichen Experimenten ergeben, zuverlässige und objektive Entscheidungen treffen können. Dazu benötigen die Studierenden vertiefte und umfangreiche Kenntnisse über angewandte statistische Methoden. Das Modul “Biostatistik” vermittelt die notwendigen wissenschaftlichen und angewandten statistischen Modelle, um später wissenschaftlich und angewandt arbeiten zu können. Das wissenschaftliche Arbeiten, Strategien in der Forschung und ihre Beziehungen zu statistischen Methoden werden an Fallbeispielen eingeübt. Dabei werden verschiedene statistische Verfahren vorgestellt und die statistischen Maßzahlen für die Modellselektion diskutiert. Der Schwerpunkt des Moduls liegt auf dem forschenden Arbeiten und der wissenschaftlichen Diskussion der Ergebnisse der statistischen Modellierungen. Studierende werden außerdem dazu angeleitet, bereits vorhandene Programmierkenntnisse in R weiter zu vertiefen und somit das Arbeiten mit Daten umsetzbar und erfahrbar zu machen. Das Modul “Biostatistik” befähigt Studierende dazu, in verschiedenen Anwendungsfeldern der agrarwissenschaftlichen Forschung und Praxis forschend tätig zu sein und Daten auszuwerten.\n\n\nLehr-Lerninhalte\nAllgemeine Statistik\n\nVertiefung der gängigen multiplen linearen Regressionen und deren Verteilungsfamilien beinhaltend die Gaussian, Poisson, Multinominal/Ordinal und Binomial.\nVertiefung der statistischen Maßzahlen der Modellgüte einer multiplen linearen Regression sowie deren Effektschätzer.\nMethoden der Variablenselektion und Imputation von fehlenden Werten sowie Ausreißerdetektion.\nLineare gemischte Modelle und die Berücksichtigung von Messwiederholungen in der praktischen Anwendung.\nVertiefte Auseinandersetzung mit multiplen Gruppenvergleichen und deren Möglichkeiten der Visualisierung von Gruppenunterschieden.\nEinführung in die Äquivalenz oder Nichtunterlegenheit in der praktischen Anwendung.\nDie klassischen experimentellen Designs in den Agrarwissenschaften und deren Auswertung an Fallbeispielen.\nGrundlagen des maschinellen Lernens und der Klassifikation von Ereignissen sowie Maßzahlen der Bewertung eines maschinellen Lernalgorithmus.\nDurchführung aller theoretisch erarbeiteten Inhalte in R.\nInterpretation und Bewertung von statistischen Modellierungen in R.\n\nNutztierwissenschaften und Pflanzenwissenschaften\n\nSpezifische Strategien und statistische Methoden in der Forschung und ihre Beziehungen zu angewandten statistischen Methoden in der jeweiligen Fachrichtung.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\n\nDie Studierenden kennen die gängigen experimentellen Designs in den Agrarwissenschaften.\nDie Studierenden kennen die entsprechenden Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden können die gängigen statistischen Modellierungen benennen und unterscheiden.\nDie Studierenden können das Ergebnis eines statistischen Modells im Kontext der wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage zwischen einem kausalen und einem prädiktiven Modell zu unterscheiden.\n\n\n\nWissensvertiefung\n\nDie Studierenden sind in der Lage anhand einer wissenschaftlichen Fragestellung eine statistische Auswertung zu gliedern und zu planen.\nDie Studierenden können wissenschaftliche Veröffentlichungen lesen und in den statistischen Kontext richtig einordnen.\nDie Studierenden können eine multiple lineare Regression entsprechend des Endpunktes modellieren und interpretieren.\nDie Studierenden können einen multiplen Gruppenvergleich für verschiedene Endpunkte rechnen und die p-Werte entsprechend adjustieren.\nDie Studierenden können verschiedene technische Messparameter miteinander vergleichen und eine Aussage über die Nichtunterlegenheit treffen.\n\n\n\nWissensverständnis\n\nDie Studierenden sind die der Lage eine wissenschaftliche Fragestellung mit einem experimentellen Design und einer statistischen Modellierung zu verbinden.\nDie Studierenden können eine statistische Modellierung in einer Präsentation darstellen und vorstellen.\nDie Studierenden können eine wissenschaftliche Veröffentlichung anhand der verwendeten Statistik bewerten.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden können durch statistische Modelle Kosten- und Nutzenabschätzungen durchführen, um marktwirtschaftliche, technische und biologische Prozesse in den Forschungsfeldern der Agrarwissenschaften zu planen. Sie sind in der Lage, externe Literaturquellen zu nutzen und statistische Maßzahlen in den Kontext ihres Berufsfeldes zu setzen, um fundierte Entscheidungen zu treffen. Außerdem können die Studierenden grundlegende Konzepte der Programmierung in R erkennen und eine Datenauswertung durchführen.\n\n\nWissenschaftliche Innovation\nDie Studierenden verfügen über die Fähigkeit, statistische Maßzahlen aus wissenschaftlichen Publikationen in verschiedenen wissenschaftlichen Kontexten zu interpretieren und anzuwenden. Sie besitzen grundlegende Fähigkeiten des wissenschaftlichen Arbeitens, die es ihnen ermöglichen, praktische Fragestellungen in wissenschaftliche Erkenntnisprozesse zu übersetzen. Darüber hinaus können sie statistische Auswertungen aus wissenschaftlichen Publikationen verstehen und aufbauend darauf informierte Forschungsideen entwickeln.\n\n\n\nKommunikation und Kooperation\nDie oben genannten Fähigkeiten der Studierenden ermöglichen es ihnen, in multidisziplinären wissenschaftlichen Teams mitzuwirken. Sie können eine gemeinsam geplante Forschungsskizze von Drittmittelprojekten oder in Kooperation mit Wirtschaftspartnern und Forschungsanstalten umsetzen. Darüber hinaus sind die Studierenden in der Lage, die Ergebnisse einer statistischen Analyse auch Fachfremden verständlich zu erläutern.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden haben ein umfangreiches Verständnis für wissenschaftliche Publikationen und statistische Maßzahlen und können dieses Wissen auf ihr Berufsfeld anwenden. Dadurch sind sie in der Lage, komplexe naturwissenschaftliche und technische Fragestellungen aus den Forschungsfeldern der Agrarwissenschaften eigenständig und fundiert mit statistischen Methoden zu bearbeiten, was sie für anspruchsvolle Aufgaben in der agrarwissenschaftlichen Forschung und Praxis qualifiziert.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\nData Science for Agriculture in R unter https://schmidtpaul.github.io/DSFAIR/\nBruce, Peter, Andrew Bruce, und Peter Gedeck. 2020. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media.\n\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul werden vertiefte Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie in den Modulen “Mathematik und Statistik (44B0266)” und “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden. Zudem werden Grundregeln und -methoden des wissenschaftlichen Arbeitens aus dem Modul “Wissenschaftliches Arbeiten (44M0159)” als bekannt vorausgesetzt.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Biostatistik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nWissenschaftliche Publikation und Fachtagungen (44M0125)\nForschungs- und Entwicklungsprojekt (44M0043)\nMasterarbeit (44M0267)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur\n\n\n\n\n\n\n\n\nAbbildung 1.1— “Dad, is this completely safe?” “Research in 500 theme parks in 2010 showed that 1 in 10’000 of carriages were found to be malfunctioning, which can lead to 1 out 50’000 kids falling out… So your chances are okay, but there’s always a risk, honey…” Quelle: wumo.com",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-modellierung-landwirtschaft",
    "href": "organisation.html#sec-module-modellierung-landwirtschaft",
    "title": "1  Organisation",
    "section": "Modellierung landwirtschaftlicher Daten",
    "text": "Modellierung landwirtschaftlicher Daten\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nDie Modellierung von komplexen Daten spielt eine zentrale Rolle in verschiedenen wissenschaftlichen Disziplinen der Agrarwissenschaften. Dabei ermöglicht die Modellierung nicht nur den Gewinn von neuen Erkenntnissen über biologische Prozesse, sondern auch die Vorhersage zukünftiger Ereignisse weit über die gängigen Gruppenvergleiche aus Feldexperimenten hinaus. Oftmals sind biologische Abhängigkeiten nicht linear, sondern folgen nicht-linearen Trends. Die Modellierung solcher nicht-linearen Abhängigkeiten erfordert spezielle Kompetenzen in der statistischen Analyse, insbesondere von Zeitreihen (eng. “time series”), räumlichen Daten (eng. “spatial data”) oder genetischen Daten und Sequenzen. In dem Modul “Modellierung landwirtschaftlicher Daten” werden die gängigen statistische Verfahren zur Auswertung dieser und weiterer Datenquellen vorgestellt und diskutiert. Im Weiteren spielt die Vorhersage von Ereignissen eine entscheidende Rolle in den Agrarwissenschaften, sei es für die Früherkennung von Risiken oder die Steuerung landwirtschaftlicher Prozesse. Im Modul werden die Grundlagen des maschinellen Lernens präsentiert und anhand von Fallbeispielen erläutert. Das Modul “Modellierung landwirtschaftlicher Daten” vermittelt den Studierenden fortgeschrittene Kenntnisse und Fähigkeiten im Bereich der Datenmodellierung und -analyse im Kontext der landwirtschaftlichen Anwendung. Der Fokus liegt in dem Modul auf der Darstellung, Verarbeitung und statistischer Modellierung komplexer landwirtschaftlicher Daten. Fallstudien aus verschiedenen Bereichen der Agrarwissenschaften werden verwendet, um die erworbenen theoretischen Kenntnisse in die Praxis umzusetzen. In der Anwendung wird R/Bioconductor für die Datenanalyse genutzt. Das Modul “Modellierung landwirtschaftlicher Daten” erweitert die bisherigen Kenntnisse der Studierenden in der Auswertung agrarwissenschaftlicher Daten und bereitet auf anspruchsvolle Aufgaben in diesem Bereich vor.\n\n\nLehr-Lerninhalte\n\nEinführung in die statistische Modellierung sowie deren Interpretation am Beispiel der multiplen linearen Regression.\nBesonderheiten der statistischen Modellierung von Zeitreihen und räumlichen Daten.\nDie explorative Datenanalyse und deren statistischen Maßzahlen sowie die Visualisierung von räumlichen und zeitlichen Daten.\nMultivariate statistische Analysen zur Erkennung von Gruppenzugehörigkeiten anhand von Clusteranalysen.\nEinführung in die klassischen experimentellen Designs in den Agrarwissenschaften.\nGrundlagen der Analyse von genetischen Daten anhand ausgewählter, beispielhafter Omics-Ebenen.\nGenetische Distanzen und polygenetische Bäume zur Darstellung evolutionärer Beziehungen.\nGrundlagen des maschinellen Lernens und der Klassifikation von Ereignissen sowie Maßzahlen der Bewertung eines maschinellen Lernalgorithmus.\nAnwendung der grundlegenden maschinellen Lernverfahren beispielhaft durch k-NN, Random Forest und Neuronale Netze.\nModellierungen an aktuellen Fallbeispielen aus der Anwendung.\nAutomatisierte Erstellung von Berichten in R Quatro.\nEinführung in die Erstellung von interaktiven R Shiny Apps.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\n\nDie Studierenden kennen die gängigen experimentellen Designs in den Agrarwissenschaften.\nDie Studierenden kennen die entsprechenden Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden kennen die gängigen Datenformate für räumliche und zeitliche Daten.\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen in der Genetik zu erkennen und zu benennen.\nDie Studierenden können die gängigen, vorgestellten statistischen Modellierungen benennen und unterscheiden.\nDie Studierenden kennen die gängigen Funktionen für die Modellierungen und Datenaufbereitung in R.\n\n\n\nWissensvertiefung\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können räumliche und zeitliche Daten visualisieren und interpretieren.\nDie Studierenden können das Ergebnis eines statistischen Modells im Kontext einer wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage anhand eines statistischen Modells eine Entscheidung zu treffen.\nDie Studierenden sind in der Lage Modellierungen mit den notwendigen Funktionen und Paketen in R durchzuführen.\nDie Studierenden können einen automatischen Bericht in R Quatro erstellen.\n\n\n\nWissensverständnis\n\nDie Studierenden können ein statistisches Modell mit einer explorativen Datenanalyse oder Visualisierung in einen Kontext bringen.\nDie Studierenden können verschiedene statistisches Modelle anhand verschiedener Maßzahlen miteinander vergleichen und eine informierte Modellauswahl treffen.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von technischen und biologischen Prozessen in der Landwirtschaft. Sie können Modelle für landwirtschaftliche Prozesse unter Verwendung von räumlichen Daten entwickeln und validieren. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage die Ergebnisse der Datenanalyse klar und verständlich zu kommunizieren, sowohl schriftlich als auch mündlich. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren. Die Studierenden sind in der Lage mit einer automatisierten Berichterstattung in R Quatro oder R Shiny eine Datenanalyse zu kommunizieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden können wissenschaftliche Publikationen und deren Modellierungen räumlicher und zeitlicher Fragestellungen in den Kontext des eigenen Berufsfeldes setzen. Unter der Hilfe der Modellierung sind die Studierenden in der Lage informierte Entscheidungen treffen. Die Studierende sind sich der inhärenten Unsicherheit statistischer Modellierungen bewusst und können die eigenen Forschungsergebnisse kritisch hinterfragen. Den Studierenden sind die algorithmischen Grenzen von Modellen bewusst.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nDas Skript zur Genetik und Bioinformatik unter https://jkruppa.github.io/bioinformatic/\n\n\n\nVoraussetzungen für die Teilnahme\nKeine.\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul sind Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik hilfreich, aber nicht notwendig, wie sie unter anderem in den Modulen “Mathematik und Statistik (44B0266)” oder “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen oder erweitern möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Modellierung landwirtschaftlicher Daten” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nSpezielle Statistik und Versuchswesen (44B0390)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nHausarbeit und Präsentation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-bioinformatik",
    "href": "organisation.html#sec-module-bioinformatik",
    "title": "1  Organisation",
    "section": "Statistische Bioinformatik",
    "text": "Statistische Bioinformatik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nDie statistische Bioinformatik spielt eine bedeutende Rolle in verschiedenen wissenschaftlichen Bereichen der Omics-Forschung. Sie wird nicht nur klassisch genutzt, um genetische Marker für Krankheiten oder genetische Veranlagungen in der Humangenetik zu identifizieren, sondern auch zur Entschlüsselung der genetischen Vererbung bei der Züchtung von Tieren und Pflanzen. Die algorithmische Auswertung von genetischen Daten ist sowohl biologisch umfassend als auch methodisch anspruchsvoll. Das Modul “Statistische Bioinformatik” folgt dem zentralen Dogma der Molekularbiologie, das besagt, dass Informationen von Genen zu Proteinen übertragen werden. Es beinhaltet die statistische Analyse von genetischen Markern auf DNA-Ebene sowie die Untersuchung der Genexpression über mRNA und die daraus resultierende Proteinexpression bis hin zur Phänomik. Auch regulatorische Elemente der DNA-Expression, wie die Methylierung, werden im Modul berücksichtigt. Das Modul “Statistische Bioinformatik” bietet zuerst eine Einführung in die klassische mendelsche Genetik sowie Grundlagen in der quantitativen Genetik. Den Großteil den Inhalts machen aktuelle algorithmische Verfahren zu Assoziationsstudien und Sequenzanalysen aus. Studierende erlernen somit die grundlegenden Fähigkeiten für zukünftige wissenschaftliche und angewandte Arbeiten im Bereich der Omics-Forschung. Der Schwerpunkt des Moduls liegt auf der Darstellung, Verarbeitung und statistischen Auswertung von hochdimensionalen genetischen Daten. Fallstudien aus den Bereichen der Agrarwissenschaften und Humanbiologie dienen als praktische Anwendungen. Gleichzeitig werden die erworbenen theoretischen Kenntnisse durch die Auswertung von Daten in R/Bioconductor in die Praxis umgesetzt. Das Modul “Statistische Bioinformatik” legt somit den Grundstein für die methodische Auswertung von Omics-Daten.\n\n\nLehr-Lerninhalte\n\nGrundlagen der klassischen, mendelschen Vererbung beinhaltend das Hardy-Weinberg-Gleichgewicht\nGrundlagen der Quantitativen Genetik mit Kopplungskarten und Linkage disequilibrium\nGenetische Distanz und polygenetische Bäume zur Darstellung evolutionärer Beziehungen\nMarkergestützte Selektion und indirekte Selektion\nStatistische Herausforderungen von hochdimensionalen Daten\nData Preprocessing und Qualitätskontrolle auf der Ebene der Marker und Individuen\nGängige Assoziationstests unter anderem Chi-Quadrat-Test und logistische Regression\nAuswertung von Genomeweite Assoziationsstudien, Microarraydaten, High throughput genotyping, Next generation sequencing, Whole genome sequencing und RNA-seq an Fallbeispielen\nVisualisierungen der Ergebnisse durch Manhattanplot, Vucanoplot und Regional Association Plot und weiteren.\nGrundlagen Multiomics und Pathway Analysen sowie deren Datenbanken\nGrundlegende Methoden zum Sequenzaligment\nAnwendung der Algorithmen an ausgewählten Fallbeispielen in R/Bioconductor\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu den jeweiligen Omics-Verfahren zu erkennen und zu benennen.\nDie Studierenden können das zentrale Dogma der Molekularbiologie erklären und visualisieren.\nDie Studierenden sind in der Lage den Unterschied zwischen der quantitativen und qualitativen Genetik zu erklären.\nDie Studierenden können Omics-Abbildungen erkennen und benennen.\nDie Studierenden können externe Programme aus R heraus starten und Ergebnisse einlesen.\n\n\n\nWissensvertiefung\n\nDie Studierenden können explorative Abbildungen in ausgewählten Omics-Analysen erstellen und interpretieren.\nDie Studierenden sind in der Lage anhand einer Omics-Analyse eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis einer Omics-Analyse im Kontext der wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage aus englischen Tutorien eine Lösung für ein Omics-Verfahren einzugrenzen.\nDie Studierenden können einfache Auswahl- und Filterregeln auf Datensätze in R anwenden.\nDie Studierenden können erste einfache R Code Blöcke miteinander in einen Kontext setzen.\n\n\n\nWissensverständnis\n\nDie Studierenden können verschiedene Ebenen der Omics-Analysen in einen wissenschaftlichen Kontext bringen und miteinander vergleichen.\nDie Studierenden sind in der Lage eine genetische Auswertung mit den notwendigen Funktionen und Paketen in R zu skizzieren und zu erklären.\nDie Studierenden können die einzelnen Schritte einer genetischen Analyse benennen und bei der Planung eines eigenen Experiments berücksichtigen. Sie identifizieren dabei die Probleme der jeweiligen biologischen Proben.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache Kosten- und Nutzenabschätzungen anhand von genetischen Analysen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in der Agrarwirtschaft und den Lebenswissenschaften. Sie können dabei die verschiedenen Ebenen der Omics-Foschung in den Kontext der Anwendung und der Phänomik setzen. Die Studierenden können dabei externe Literaturquellen und deren genetischen Analysen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen für die eigenen Analysen treffen.\n\n\nWissenschaftliche Innovation\nDie Studierende können genetische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen in der Genetik. Die Studierenden können genetische Visualisierungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage genetische Daten und Analysen mit anderen Forschenden zu teilen. Sie können die einzelnen Schritte der Analyse erklären und die Ergebnisse allgemein verständlich erklären. Ebenfalls sind die Studierenden in der Lage gängige genetische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden können wissenschaftliche Publikationen und deren genetischen Maßzahlen sowie Ergebnisse in den Kontext des eigenen Berufsfeldes setzen und somit informierte Entscheidungen treffen. Diese informierten Entscheiden betreffen sowohl die praktische Anwendung wie auch die Bewertung von Forschungsideen in der wissenschaftlichen Grundlagenforschung. Die Studierende sind sich der inhärenten Unsicherheit der wissenschaftlichen, genetischen Forschung bewusst und können die eigenen Forschungsergebnisse diesbezüglich kritisch hinterfragen.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nDas Skript zum Bioinformatikteil unter https://jkruppa.github.io/bioinformatic/\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. https://r4ds.had.co.nz\n\n\n\nVoraussetzungen für die Teilnahme\nKeine.\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul sind Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik hilfreich aber nicht notwendig, wie sie unter anderem in den Modulen “Mathematik und Statistik (44B0266)” oder “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen oder erweitern möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Statistische Bioinformatik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nEinführung in die Pflanzenzüchtung (44B0112)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nHausarbeit und Präsentation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-mathematik",
    "href": "organisation.html#sec-module-mathematik",
    "title": "1  Organisation",
    "section": "Mathematik und Statistik",
    "text": "Mathematik und Statistik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIn den Pflanzenwissenschaften sowie in der Landwirtschaft werden vielen Prozesse und Phänomene durch mathematische und statistische Modelle beschrieben. In dem Modul “Mathematik und Statistik” lernen Studierende drei Schwerpunkte für das spätere wissenschaftliche und angewandte Arbeiten. Im ersten Teil des Moduls werden mathematische Grundkenntnisse wiederholt und im Verlauf des Moduls vertieft. Die mathematischen Formeln werden aus ihrer theoretischen, formalistischen Anwendung herausgelöst und in praktische Herausforderungen übertragen. Dabei werden Bereiche der Physik, Chemie sowie Biologie in den Kontext der Mathematik gesetzt. Im zweiten Teil des Moduls werden statistische Grundkenntnisse vermitteln. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Die für Landwirtschaft und Gartenbau relevanten mathematischen und statistischen Verfahren werden dargestellt und diskutiert. Im dritten Teil des Moduls werden die erworbenen theoretischen, mathematischen und statistischen Kenntnisse durch die Einführung in die Programmierung in R für die Studierenden umsetzbar und erfahrbar gemacht. In dem Modul “Mathematik und Statistik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben.\n\n\nLehr-Lerninhalte\nMathematischer Anteil\n\nMaßzahlen, Flächen und Volumen beinhaltend Berechnungen mit Maßeinheiten von sehr kleinen sowie sehr großen Zahlen. Berechnungen mit Flächen- sowie Volumenmaßen einschließlich Winkel- und Streckenbestimmung.\nBerechnungen mit Vektoren und Matrizen.\nMathematische Funktionen und Anwendung der Differential- und Integralrechnung einschließlich logarithmischer sowie exponentieller Funktionen. Lösung von quadratischer Gleichungen sowie Extremwertproblemen.\nWahrscheinlichkeiten mit Baumdiagramm und Pfadregeln sowie stochastische Prozesse. Wahrscheinlichkeitsverteilungen am Beispiel der Normalverteilung.\nLogische Operatoren sowie Mengenlehre.\n\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse mit Fokus auf dem Boxplot und dem Barplot und deren statistischen Maßzahlen.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test. Einführung in die Varianzanalyse.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nInformatorischer Anteil\n\nEinführung in die Programmierung in R anhand von Skalenarten sowie der Darstellung von Daten in R.\nKonzept von Objekten, Funktionen sowie Pipen und der Vorstellung des tidyverse in R.\nEinlesen von Daten und deren Bearbeitung sowie Visualisierung in R\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nMathematischer Anteil\n\nDie Studierenden sind in der Lage mathematische Formeln in der Literatur zu finden.\nDie Studierenden können ein Baumdiagramm für die Berechnung von Wahrscheinlichkeiten erstellen.\n\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen.\nDie Studierenden können einfache explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nInformatorischer Anteil\n\nDie Studierenden können die Anforderungen an einen Datensatz zur Verwendung in R benennen.\nDie Studierenden können in R Objekte, Funktionen und Zahlenvektoren unterscheiden und kennen die gängigen Operatoren in R.\nDie Studierenden können den Ablauf für die Erstellung einer explorativen Datenanalyse in R beschreiben.\n\n\n\nWissensvertiefung\nMathematischer Anteil\n\nDie Studierenden sind in der Lage mathematische Formeln in einem anwendungsorientierten Kontext anzuwenden.\nDie Studierenden können sinnvolle Abschätzungen von linearen und exponentiellen Wachstum vornehmen.\n\nStatistischer Anteil\n\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\n\n\n\nWissensverständnis\nMathematischer Anteil\n\nDie Studierenden können praktische Fragestellungen in einen formalisierten, mathematischen Kontext übersetzen.\nDie Studierenden sind in der Lage die Wahrscheinlichkeit für das Eintreten eines Ereignisses abzuschätzen.\n\nStatistischer Anteil\n\nDie Studierenden können einen einfachen statistischen Test mit einer explorativen Datenanalyse in einen Kontext setzen.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare und exponentielle Kosten- und Nutzenabschätzungen anhand von mathematischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von technischen und biologischen Prozesses in den Pflanzenwissenschaften sowie in der Landwirtschaft. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können eine Reihe von explorativen Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können einfachen R Code lesen und demonstrieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Mathematikteils des Moduls unter https://jkruppa.github.io/math/\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\n\n\n\nVoraussetzungen für die Teilnahme\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nEmpfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Mathematik und Statistik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nAngewandte Statistik und Versuchswesen (44B0400)\nChemie und Biochemie (44B0532)\nPhysikalische Grundlagen der Natur und Agrartechnik (44B0534)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-angewandte",
    "href": "organisation.html#sec-module-angewandte",
    "title": "1  Organisation",
    "section": "Angewandte Statistik und Versuchswesen",
    "text": "Angewandte Statistik und Versuchswesen\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nDer wissenschaftliche Fortschritt in den Agrarwissenschaften ist wesentlich getragen durch eine intensive experimentelle Versuchstätigkeit. Um erfolgreich in diesem Bereich tätig zu sein sind neben statistischen Kenntnissen auch solche über die Techniken zur Versuchsdurchführung erforderlich. Für die Versuchsdurchführung müssen Messdaten und Beobachtungen aus Erhebungen sowie aus experimentellen Versuchen in einem Datensatz aufgearbeitet werden. In dem Modul “Angewandte Statistik und Versuchswesen” lernen Studierende die grundlegenden Algorithmen der Statistik für das spätere wissenschaftliche und angewandte Arbeiten kennen. Das Modul vermittelt die dafür notwendigen statistischen und algorithmischen praktischen Kenntnisse. Verschiedene statistische Verfahren zur Auswertung von experimentellen Daten werden vorgestellt und die statistischen Maßzahlen für das lineare Modellieren eingeübt. Einfache experimentelle Designs werden vorgestellt und Anwendungsmöglichkeiten diskutiert. Die vorhandenen Programmierkenntnisse in R werden weiter vertieft. Verschiedene einfache Fallbeispiele dienen als Einstieg für die Diskussion und der Reflexion der eigenen Versuchstätigkeit. Das Modul “Angewandte Statistik und Versuchswesen” schließt den Erwerb der Grundlagen in der Bio Data Science ab und ermöglicht den Studierenden somit einfache Experimente in den Agrarwissenschaften selbstständig zu planen und auszuwerten.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nDie explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in statistische Verteilungen anhand der Poisson- und Normalverteilung.\nDie Varianzanalyse beinhaltend die einfaktorielle sowie zweifaktorielle ANOVA.\nGrundlagen des nicht-parametrischen Tests beinhaltend Wilcoxon-Mann-Whitney-Test sowie Kruskal-Wallis-Test.\nGrundlagen der simplen linearen Regression und der multiplen linearen Regression sowie deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nDiagnostischen Testen und deren statistischen Maßzahlen.\nChi-Quadrat-Test für eine Vierfeldertafel.\nDas multiple Testen von mehreren Mittelwerten und deren Visualisierungen.\nEinführung in die klassischen experimentellen Designs in den Agrarwissenschaften sowie die einfache Versuchsplanung.\n\nInformatorischer Anteil\n\nDurchführung aller theoretisch erarbeiteten Inhalte in R.\nInterpretation und Bewertung von einfachen statistischen Modellierungen in R.\nEinfache Transformationen von Daten für die explorative Datenanalyse.\nDemonstration der automatisierten Erstellung von Berichten in Rmarkdown sowie in R Quatro.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden kennen einfache experimentelle Designs in den Agrarwissenschaften.\nDie Studierenden kennen einfache Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\nDie Studierenden sind in der Lage zwischen einen parametrischen und einem nicht-parametrischen Test zu unterscheiden.\n\nInformatorischer Anteil\n\nDie Studierenden kennen die gängigen Funktionen für die Datenaufbereitung in R.\nDie Studierenden können den Ablauf für die Erstellung einer einfachen Datenanalyse in R beschreiben.\nDie Studierenden sind in der Lage aus englischen Internetquellen eine Lösung für ein R Problem einzugrenzen.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können eine simple lineare Regression für eine Normalverteilung modellieren.\nDie Studierenden können eine Aussage über die Güte eines simplen linearen Modells abgeben.\nDie Studierenden können eine Korrelation berechnen und interpretieren.\nDie Studierenden können einen multiplen Gruppenvergleich für einen normalverteilten Endpunkt rechnen und die p-Werte entsprechend adjustieren.\nDie Studierenden sind in der Lage eine einfache explorative Datenanalyse mit einem multiplen Gruppenvergleich zu verbinden.\n\nInformatorischer Anteil\n\nDie Studierenden können Datensätze in R bearbeiten.\nDie Studierenden können einfache experimentelle Designs in R visualisieren.\nDie Studierenden können verschiedene Ausgaben von statistischen Tests in R visualisieren.\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden sind die der Lage eine wissenschaftliche Fragestellung mit einem einfachen experimentellen Design zu verbinden.\nDie Studierenden können einfache linearen Modellierungen bewerten und interpretieren.\n\nInformatorischer Anteil\n\nDie Studierenden können verschiedene statistische Tests und eine lineare Modellierung mit einer explorativen Datenanalye in einen Kontext setzen.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache Kosten- und Nutzenabschätzungen anhand von statistischen linearen Modellen durchzuführen. Diese Abschätzungen umfassen im Besonderen die Planung von einfachen experimentellen Designs in den Agrarwissenschaften. Die Studierenden können statistische Unterschiede aus multiplen Gruppenvergleichen berechnen und eine Risikoabschätzung treffen. Die Studierenden sind in der Lage selbständig einfache statistische Analysen auf Datensätzen in R durchzuführen. Die Studierenden können einfache experimentelle Designs für verschiedene Berufsfelder und Anwendungen abwägen und diskutieren.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden können selbständig eigene wissenschaftliche Fragestellungen mit Fallbeispielen abgleichen und entsprechend der eigenen Anforderungen modifizieren. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die wissenschaftliche Verwertbarkeit in R zu gewährleisten. Die Studierenden kennen die Möglichkeit über automatisierte Berichte die Reproduzierbarkeit der eigenen Forschungsergebnisse zu gewährleisten.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage die Daten der durchgeführten Experimente und entsprechende R Skripte der statistische Auswertungen mit anderen Forschenden zu teilen. Die Studierenden können die statistischen Analyseergebnisse vorstellen und Änderungswünsche entsprechend durchführen.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\nData Science for Agriculture in R unter https://schmidtpaul.github.io/DSFAIR/\n\n\n\nVoraussetzungen für die Teilnahme\nKeine.\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul werden Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie in dem Modul “Mathematik und Statistik (44B0266)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Angewandte Statistik und Versuchswesen” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nSteuerung der vegetativen Entwicklung krautiger Pflanzen (44B0608)\nProjektplanung und -management (44B0654)\nBerufspraktisches Projekt (BAP) (44B0595)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-spezielle",
    "href": "organisation.html#sec-module-spezielle",
    "title": "1  Organisation",
    "section": "Spezielle Statistik und Versuchswesen",
    "text": "Spezielle Statistik und Versuchswesen\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nMit dem Fortschreiten der Digitalisierung können in den Pflanzenwissenschaften und der Landwirtschaft komplexere Experimente durchgeführt werden. Die Digitalisierung erlaubt die automatisierte Erfassung und Speicherung großer Datenmengen, die über entsprechende statistische Algorithmen aggregiert und ausgewertet werden müssen. Diese Daten können zur Steuerung der Produktion oder zur Erkennung von unerwünschten Ereignissen genutzt werden. Dadurch kann eine bessere Qualitätssicherung und Entwicklung gewährleistet werden. In dem Modul “Spezielle Statistik und Versuchswesen” lernen Studierende die fortgeschrittenen Algorithmen für das spätere wissenschaftliche und angewandte Arbeiten mit großen Datenmengen. Das Modul vermittelt die dafür notwendigen statistischen und algorithmischen praktischen Kenntnisse. Verschiedene statistische Verfahren werden vorgestellt und die statistischen Maßzahlen für die Modellselektion eingeübt. Im Weiteren werden maschinelle Lernverfahren präsentiert und auf Fallbeispiele angewendet. Der Fokus des Moduls liegt auf der praktischen Anwendung und Diskussion der Ergebnisse der statistischen Modellierungen. Die vorhandenen Programmierkenntnisse in R werden weiter vertieft und automatisierte Berichtserstellung mit Quarto und RMarkdown eingeübt. Das Arbeiten mit großen Datenmengen wird so für die Studierenden umsetzbar und erfahrbar gemacht. Das Modul “Spezielle Statistik und Versuchswesen” befähigt Studierende in dem Bereich der Bio Data Science in verschiedenen Anwendungsfeldern praktisch tätig zu sein.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die gängigen multiplen linearen Regressionen und deren Verteilungsfamilien beinhaltend die Gaussian, Poisson, Multinominal/Ordinal und Binomial.\nGrundlagen der statistischen Maßzahlen der Modellgüte einer multiplen linearen Regression sowie deren Effektschätzer.\nGrundlagen der Variablenselektion und Imputation von fehlenden Werten sowie Ausreißerdetektion.\nEinführung in die linearen gemischten Modelle und die Berücksichtigung von Messwiederholungen.\nEinführung in die nicht lineare Regression.\nVertiefte Auseinandersetzung mit multiplen Gruppenvergleichen und deren Möglichkeiten der Visualisierung von Gruppenunterschieden.\nEinführung in die Äquivalenz oder Nichtunterlegenheit in der praktischen Anwendung.\nEinführung in die klassischen experimentellen Designs in den Agrarwissenschaften.\nGrundlagen des maschinellen Lernens und der Klassifikation von Ereignissen sowie Maßzahlen der Bewertung eines maschinellen Lernalgorithmus.\nAnwendung der grundlegenden maschinellen Lernverfahren wie k-NN, Random Forest, Support Vector Machine und Neuronale Netze.\n\nInformatorischer Anteil\n\nDurchführung aller theoretisch erarbeiteten Inhalte in R.\nInterpretation und Bewertung von statistischen Modellierungen in R.\nFortgeschrittene Programmierung in R unter der Verwendung von regulären Ausdrücken.\nAutomatisierte Erstellung von Berichten in Rmarkdown sowie in R Quatro.\nEinführung in die Erstellung von interaktiven R Shiny Apps.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden kennen die gängigen experimentellen Designs in den Agrarwissenschaften.\nDie Studierenden kennen die entsprechenden Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden können die gängigen statistischen Modellierungen benennen und unterscheiden.\nDie Studierenden sind in der Lage zwischen einem kausalen und einem prädiktiven Modell zu unterscheiden.\n\nInformatorischer Anteil\n\nDie Studierenden kennen die gängigen Funktionen für die Datenaufbereitung in R.\nDie Studierenden sind in der Lage aus englischsprachigen Tutorien die statistische Analyseschritte für die eigenen Daten zu transferieren.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage anhand einer wissenschaftlichen Fragestellung eine statistische Auswertung zu gliedern und zu planen.\nDie Studierenden können wissenschaftliche Veröffentlichungen lesen und in den statistischen Kontext richtig einordnen.\nDie Studierenden können eine multiple lineare Regression oder einen maschinellen Lernalgorithmus entsprechend des Endpunktes modellieren und interpretieren.\nDie Studierenden können einen multiplen Gruppenvergleich für verschiedene Endpunkte rechnen und die p-Werte entsprechend adjustieren.\nDie Studierenden können verschiedene technische Messparameter miteinander vergleichen und eine Aussage über die Nichtunterlegenheit treffen.\n\nInformatorischer Anteil\n\nDie Studierenden können mit regulären Ausdrücken Datensätze bearbeiten.\nDie Studierenden sind in der Lage durch eine eine parallele Programmierung eine serielle Programmierungen zu optimieren.\nDie Studierenden sind in der Lage einen automatisierten Bericht in Rmarkdown oder R Quarto zu erstellen\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden sind die der Lage eine wissenschaftliche Fragestellung mit einem experimentellen Design und einer statistischen Modellierung zu verbinden.\nDie Studierenden können eine statistische Modellierung in einer Präsentation darstellen und vorstellen.\nDie Studierenden können eine wissenschaftliche Veröffentlichung anhand der verwendeten Statistik bewerten.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage in R eine statistische Modellierung zu planen und den entsprechenden R Code zu erstellen.\nDie Studierenden können R Code Chunks miteinander sinnvoll für die eigene Anwendung kombinieren und optimieren.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage Kosten- und Nutzenabschätzungen anhand von statistischen Modellen und deren Effektschätzern durchzuführen. Diese Abschätzungen umfassen im Besonderen die Planung von technischen und biologischen Prozesses in den Agrarwissenschaften. Die Studierenden können verschiedene technische Prozesse miteinander vergleichen und eine Aussage über die Nichtunterlegenheit oder den statistischen Unterschied treffen. Die beiden gegensätzlichen Konzepte von einem geplanten Experiment und einer technischen Nichtunterlegenheit können von den Studierenden unterschieden werden. Die Studierenden sind in der Lage selbständig Datenanalysen auf großen Datensätzen in R durchzuführen. Die Studierenden können die gängigen experimentellen Designs für verschiedene Berufsfelder und Anwendungen anpassen und durchführen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden sind in der Lage wissenschaftlich zu Arbeiten und eine praktische Fragestellung in einen wissenschaftlichen Erkenntnisprozess zu übersetzen. Die Studierenden können statistische Auswertungen aus wissenschaftlichen Publikationen verstehen und informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die wissenschaftliche Verwertbarkeit in R zu berücksichtigen. Die Studierenden können über die Erstellung von automatisierten Berichten die Reproduzierbarkeit der eigenen Forschungsergebnisse gewährleisten.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept der automatisierten Berichtserstattung durchgeführte Experimente und statistische Auswertungen mit anderen Forschenden zu teilen. Die Studierenden sind dadurch in der Lage in multidiziplinären, wissenschaftlichen Teams mitzuwirken. Die Studierenden können eine gemeinsam geplante Forschungsskizze in R umsetzen. Die Studierenden sind in der Lage die Ergebnisse einer statistischen Analyse auch Fachfremden zu erläutern.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden können wissenschaftliche Publikationen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeldes setzen und somit informierte Entscheidungen treffen. Die Studierende sind sich der inhärenten Unsicherheit der wissenschaftlichen Forschung bewusst und können die eigenen Forschungsergebnisse kritisch hinterfragen.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\nData Science for Agriculture in R unter https://schmidtpaul.github.io/DSFAIR/\nBruce, Peter, Andrew Bruce, und Peter Gedeck. 2020. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media.\n\n\n\nVoraussetzungen für die Teilnahme\nKeine. Bitte aber die empfohlenen Vorkenntnisse beachten.\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul werden vertiefte Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie in den Modulen “Mathematik und Statistik (44B0266)” und “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Spezielle Statistik und Versuchswesen” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nProjektauswertung und -vorstellung (44B0597)\nBerufspraktisches Projekt (BAP) (44B0595)\nBachelorarbeit (44B0365)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nHausarbeit",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-statistik",
    "href": "organisation.html#sec-module-statistik",
    "title": "1  Organisation",
    "section": "Statistik",
    "text": "Statistik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIn der Agrarwirtschaft, Lebensmittelwissenschaft und Gartenbau werden vielen Prozesse und Phänomene durch mathematische und statistische Modelle beschrieben. Entwicklung, Qualitätssicherung und Marketing sind wesentlich getragen durch eine statistische Analyse von Daten. In dem Modul “Statistik” lernen Studierende die Grundlagen für das spätere wissenschaftliche und angewandte Arbeiten. Das Modul vermittelt die dafür notwendigen statistischen Grundkenntnisse. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Die für die Agrarwirtschaft, Lebensmittelwissenschaft und Gartenbau relevanten statistischen Verfahren und Modellierungen werden dargestellt und diskutiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt. Parallel dazu werden in dem Modul die erworbenen theoretischen, statistischen Kenntnisse durch die Einführung in die Programmierung in R für die Studierenden umsetzbar und erfahrbar gemacht. In dem Modul “Statistik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in statistische Verteilungen anhand der Poisson- und Normalverteilung.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test.\nEinführung in die Varianzanalyse beinhaltend die einfaktorielle sowie zweifaktorielle ANOVA.\nGrundlagen des nicht-parametrischen Tests beinhaltend Wilcoxon-Mann-Whitney-Test sowie Kruskal-Wallis-Test.\nGrundlagen der simplen linearen Regression und der multiplen linearen Regression sowie deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nDiagnostischen Testen und deren statistischen Maßzahlen.\nChi-Quadrat-Test für eine Vierfeldertafel.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nInformatorischer Anteil\n\nEinführung in die Grundlagen der Programmierung in R anhand von Skalenarten.\nEinführung in die Darstellung von Daten in R und die Vorstellung des Konzepts der “tidy data”.\nKonzept von Objekten, Funktionen sowie Pipen und der Vorstellung des tidyverse in R.\nEinlesen von Daten und deren Bearbeitung sowie Visualisierung in R.\nDurchführung der gängigen statistischen Tests und die Interpretierung der jeweiligen R Ausgaben.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen und zu benennen.\nDie Studierenden können explorative Abbildungen erkennen und benennen.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nInformatorischer Anteil\n\nDie Studierenden können die Anforderungen an einen Datensatz zur Verwendung in R benennen.\nDie Studierenden können in R Objekte, Funktionen und Zahlenvektoren unterscheiden und kennen die gängigen Operatoren in R.\nDie Studierenden können den Ablauf für die Erstellung einer explorativen Datenanalyse in R beschreiben.\nDie Studierenden sind in der Lage aus englischen Internetquellen eine Lösung für ein R Problem einzugrenzen.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können aus explorative Abbildungen die entsprechende Datenstruktur zur Erstellung der Abbildungen wiedergeben.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage experimentelle Daten nach dem Konzept der der “tidy data” zu erheben.\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\nDie Studierenden können einfache Auswahl- und Filterregeln auf Datensätze in R anwenden.\nDie Studierenden können erste einfache R Code Blöcke miteinander in einen Kontext setzen.\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden können einen statistischen Test mit einer explorativen Datenanalyse in einen Kontext bringen.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage eine einfache statistische Auswertung mit den notwendigen Funktionen und Paketen in R zu skizzieren.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in den Agrarwirtschaften, Lebensmittelwissenschaften und Gartenbau. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen. Die Studierenden sind in der Lage die grundlegenden Konzepte der Programmierung in R in anderen Programmiersprachen zuerkennen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\n\n\n\nVoraussetzungen für die Teilnahme\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nEmpfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Statistik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nMarketing Praxis (44B0547)\nWeb Engineering (44B0585)\nWirtschaftsinformatik (44B0577)\nWissenschaftliches Arbeiten und Kommunikation (44B0573)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-bioverfahren",
    "href": "organisation.html#sec-module-bioverfahren",
    "title": "1  Organisation",
    "section": "Angewandte Statistik für Bioverfahrenstechnik",
    "text": "Angewandte Statistik für Bioverfahrenstechnik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIn der Agrarwirtschaft und Ingenieurwissenschaften werden vielen Prozesse und Phänomene durch statistische Modelle beschrieben. Verfahrenstechnik, Qualitätssicherung und Marketing sind wesentlich getragen durch eine statistische Analyse von Daten. In dem Modul “Angewandte Statistik für Bioverfahrenstechnik” lernen Studierende die Grundlagen für das spätere wissenschaftliche und angewandte Arbeiten. Das Modul vermittelt die dafür notwendigen statistischen Grundkenntnisse. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Die für die Agrarwirtschaft und Ingenieurwissenschaften relevanten statistischen Verfahren und Modellierungen werden dargestellt und diskutiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt. Parallel dazu werden in dem Modul die erworbenen theoretischen, statistischen Kenntnisse durch die Einführung in die Programmierung in R für die Studierenden umsetzbar und erfahrbar gemacht. In dem Modul “Angewandte Statistik für Bioverfahrenstechnik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in statistische Verteilungen anhand der Poisson- und Normalverteilung.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test.\nEinführung in die Varianzanalyse beinhaltend die einfaktorielle sowie zweifaktorielle ANOVA.\nGrundlagen des nicht-parametrischen Tests beinhaltend Wilcoxon-Mann-Whitney-Test sowie Kruskal-Wallis-Test.\nGrundlagen der simplen linearen Regression und der multiplen linearen Regression sowie deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nDiagnostischen Testen und deren statistischen Maßzahlen.\nChi-Quadrat-Test für eine Vierfeldertafel.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nInformatorischer Anteil\n\nEinführung in die Grundlagen der Programmierung in R anhand von Skalenarten.\nEinführung in die Darstellung von Daten in R und die Vorstellung des Konzepts der “tidy data”.\nKonzept von Objekten, Funktionen sowie Pipen und der Vorstellung des tidyverse in R.\nEinlesen von Daten und deren Bearbeitung sowie Visualisierung in R.\nDurchführung der gängigen statistischen Tests und die Interpretierung der jeweiligen R Ausgaben.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen und zu benennen.\nDie Studierenden können explorative Abbildungen erkennen und benennen.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nInformatorischer Anteil\n\nDie Studierenden können die Anforderungen an einen Datensatz zur Verwendung in R benennen.\nDie Studierenden können in R Objekte, Funktionen und Zahlenvektoren unterscheiden und kennen die gängigen Operatoren in R.\nDie Studierenden können den Ablauf für die Erstellung einer explorativen Datenanalyse in R beschreiben.\nDie Studierenden sind in der Lage aus englischen Internetquellen eine Lösung für ein R Problem einzugrenzen.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können aus explorative Abbildungen die entsprechende Datenstruktur zur Erstellung der Abbildungen wiedergeben.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage experimentelle Daten nach dem Konzept der der “tidy data” zu erheben.\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\nDie Studierenden können einfache Auswahl- und Filterregeln auf Datensätze in R anwenden.\nDie Studierenden können erste einfache R Code Blöcke miteinander in einen Kontext setzen.\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden können einen statistischen Test mit einer explorativen Datenanalyse in einen Kontext bringen.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage eine einfache statistische Auswertung mit den notwendigen Funktionen und Paketen in R zu skizzieren.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in der Agrarwirtschaft und Ingenieurwissenschaften. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen. Die Studierenden sind in der Lage die grundlegenden Konzepte der Programmierung in R in anderen Programmiersprachen zuerkennen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. https://r4ds.had.co.nz\n\n\n\nVoraussetzungen für die Teilnahme\nInhalte des Moduls Mathematik für Bioverfahrenstechnik (44B0609)\n\n\nEmpfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Angewandte Statistik für Bioverfahrenstechnik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nProduktionssystem Pflanze (44B0261)\nEinführung in die Pflanzenzüchtung (44B0112)\nMessen, Regeln und Auswerten in der Biosystemtechnik (44B0549)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-mathe-bioverfahren",
    "href": "organisation.html#sec-module-mathe-bioverfahren",
    "title": "1  Organisation",
    "section": "Angewandte Mathematik und Statistik für Bioverfahrenstechnik",
    "text": "Angewandte Mathematik und Statistik für Bioverfahrenstechnik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIn der Agrarwirtschaft und Ingenieurwissenschaften werden vielen Prozesse und Phänomene durch statistische Modelle beschrieben. Verfahrenstechnik, Qualitätssicherung und Marketing sind wesentlich getragen durch eine statistische Analyse von Daten. In dem Teil des Moduls “Angewandte Statistik für Bioverfahrenstechnik” lernen Studierende die Grundlagen für das spätere wissenschaftliche und angewandte Arbeiten. Das Modul vermittelt die dafür notwendigen statistischen Grundkenntnisse. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt. In dem Modul “Angewandte Statistik für Bioverfahrenstechnik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben. Der Teil “Angewandte Mathematik für Bioverfahrenstechnik” des Moduls ist die Vertiefung und Erweiterung des Moduls “Mathematik für Bioverfahrenstechnik”. Schwerpunkt soll auf der Auswahl und Anwendung wichtiger mathematischer Werkzeuge liegen. Während im Modul “Mathematik für Bioverfahrenstechnik” Wert auf grundlegende Rechentechniken gelegt wird, ist in diesem Modul die Unterstützung durch die Software MATLAB zentrales Merkmal. Die grundlegende Funktionsweise der Software soll in diesem Rahmen vermittelt werden. Mittels der integrierten Funktionen der Software sollen die Studierenden lernen mathematische Berechnungen und numerische Analysen durchzuführen und zu visualisieren.\n\n\nLehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test.\nEinführung in die Varianzanalyse beinhaltend die einfaktorielle ANOVA.\nGrundlagen der simplen linearen Regression und deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nMathematischer Anteil\n\nVertiefung der Anwendung mathematischer Lösungsansätze und Methoden\nGrundlegende Einführung in die Software MATLAB\nAnwendung der Software MATLAB\nNäherungen und Näherungsmethoden\nSysteme von Differentialgleichungen\nVisualisierung von Daten\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen und zu benennen.\nDie Studierenden können explorative Abbildungen erkennen und benennen.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nMathematischer Anteil\n\nDie Studierenden sind in der Lage Lösungsverfahren auszuwählen, anzuwenden um Problemlösungen korrekt zu erarbeiten.\nDie Studierenden können Näherungsmethoden auswählen und anwenden.\nDie Studierenden sind in der Lage Daten zu visualisieren.\n\n\n\nWissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können aus explorative Abbildungen die entsprechende Datenstruktur zur Erstellung der Abbildungen wiedergeben.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\n\nMathematischer Anteil\n\nDie Studierenden können mathematische Problemstellungen unter Zuhilfenahme der Software MATLAB lösen.\nDie Studierenden sind in der Lage Näherungen mittels geeigneter Näherungsmethoden sinnvoll durchzuführen.\nDie Studierenden können Systeme von linearen Differentialgleichungen lösen.\nDie Studierenden können Ergebnisse zwei- und dreidimensional graphisch darstellen.\n\n\n\nWissensverständnis\nStatistischer Anteil\n\nDie Studierenden können einen statistischen Test mit einer explorativen Datenanalyse in einen Kontext bringen.\n\nMathematischer Anteil\n\nDie Studierenden können Aufgabenstellungen interpretieren, Lösungsverfahren zuordnen und unter Zuhilfenahme von Software korrekt anwenden.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in der Agrarwirtschaft und Ingenieurwissenschaften. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen. Die Studierenden sind in der Lage die grundlegenden Konzepte der Programmierung in R in anderen Programmiersprachen zuerkennen.\n\n\nWissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\n\n\nKommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. https://r4ds.had.co.nz\n\n\n\nVoraussetzungen für die Teilnahme\nInhalte des Moduls Mathematik für Bioverfahrenstechnik (44B0609)\n\n\nEmpfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Angewandte Statistik für Bioverfahrenstechnik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nProduktionssystem Pflanze (44B0261)\nEinführung in die Pflanzenzüchtung (44B0112)\nMessen, Regeln und Auswerten in der Biosystemtechnik (44B0549)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "organisation.html#sec-module-biostatistik",
    "href": "organisation.html#sec-module-biostatistik",
    "title": "1  Organisation",
    "section": "Biostatistik",
    "text": "Biostatistik\n\nInhalte und Qualifikationsziele des Moduls\n\nKurzbeschreibung\nIm Masterstudiengang Nutztier- und Pflanzenwissenschaften wird angewandte Forschungs- und Entwicklungskompetenz in den Forschungsfeldern der Agrarwissenschaften vermittelt. Studierende lernen, wie sie aus Daten, die sich aus technischen Prozessen und wissenschaftlichen Experimenten ergeben, zuverlässige und objektive Entscheidungen treffen können. Dazu benötigen die Studierenden vertiefte und umfangreiche Kenntnisse über angewandte statistische Methoden. Das Modul “Biostatistik” vermittelt die notwendigen wissenschaftlichen und angewandten statistischen Modelle, um später wissenschaftlich und angewandt arbeiten zu können. Das wissenschaftliche Arbeiten, Strategien in der Forschung und ihre Beziehungen zu statistischen Methoden werden an Fallbeispielen eingeübt. Dabei werden verschiedene statistische Verfahren vorgestellt und die statistischen Maßzahlen für die Modellselektion diskutiert. Der Schwerpunkt des Moduls liegt auf dem forschenden Arbeiten und der wissenschaftlichen Diskussion der Ergebnisse der statistischen Modellierungen. Studierende werden außerdem dazu angeleitet, bereits vorhandene Programmierkenntnisse in R weiter zu vertiefen und somit das Arbeiten mit Daten umsetzbar und erfahrbar zu machen. Das Modul “Biostatistik” befähigt Studierende dazu, in verschiedenen Anwendungsfeldern der agrarwissenschaftlichen Forschung und Praxis forschend tätig zu sein und Daten auszuwerten.\n\n\nLehr-Lerninhalte\nAllgemeine Statistik\n\nVertiefung der gängigen multiplen linearen Regressionen und deren Verteilungsfamilien beinhaltend die Gaussian, Poisson, Multinominal/Ordinal und Binomial.\nVertiefung der statistischen Maßzahlen der Modellgüte einer multiplen linearen Regression sowie deren Effektschätzer.\nMethoden der Variablenselektion und Imputation von fehlenden Werten sowie Ausreißerdetektion.\nLineare gemischte Modelle und die Berücksichtigung von Messwiederholungen in der praktischen Anwendung.\nVertiefte Auseinandersetzung mit multiplen Gruppenvergleichen und deren Möglichkeiten der Visualisierung von Gruppenunterschieden.\nEinführung in die Äquivalenz oder Nichtunterlegenheit in der praktischen Anwendung.\nDie klassischen experimentellen Designs in den Agrarwissenschaften und deren Auswertung an Fallbeispielen.\nGrundlagen des maschinellen Lernens und der Klassifikation von Ereignissen sowie Maßzahlen der Bewertung eines maschinellen Lernalgorithmus.\nDurchführung aller theoretisch erarbeiteten Inhalte in R.\nInterpretation und Bewertung von statistischen Modellierungen in R.\n\nNutztierwissenschaften und Pflanzenwissenschaften\n\nSpezifische Strategien und statistische Methoden in der Forschung und ihre Beziehungen zu angewandten statistischen Methoden in der jeweiligen Fachrichtung.\n\n\n\n\nKompetenzorientierte Lernergebnisse\n\nWissen und Verstehen\n\nWissensverbreiterung\n\nDie Studierenden kennen die gängigen experimentellen Designs in den Agrarwissenschaften.\nDie Studierenden kennen die entsprechenden Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden können die gängigen statistischen Modellierungen benennen und unterscheiden.\nDie Studierenden können das Ergebnis eines statistischen Modells im Kontext der wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage zwischen einem kausalen und einem prädiktiven Modell zu unterscheiden.\n\n\n\nWissensvertiefung\n\nDie Studierenden sind in der Lage anhand einer wissenschaftlichen Fragestellung eine statistische Auswertung zu gliedern und zu planen.\nDie Studierenden können wissenschaftliche Veröffentlichungen lesen und in den statistischen Kontext richtig einordnen.\nDie Studierenden können eine multiple lineare Regression entsprechend des Endpunktes modellieren und interpretieren.\nDie Studierenden können einen multiplen Gruppenvergleich für verschiedene Endpunkte rechnen und die p-Werte entsprechend adjustieren.\nDie Studierenden können verschiedene technische Messparameter miteinander vergleichen und eine Aussage über die Nichtunterlegenheit treffen.\n\n\n\nWissensverständnis\n\nDie Studierenden sind die der Lage eine wissenschaftliche Fragestellung mit einem experimentellen Design und einer statistischen Modellierung zu verbinden.\nDie Studierenden können eine statistische Modellierung in einer Präsentation darstellen und vorstellen.\nDie Studierenden können eine wissenschaftliche Veröffentlichung anhand der verwendeten Statistik bewerten.\n\n\n\n\nEinsatz, Anwendung und Erzeugung von Wissen\n\nNutzung und Transfer\nDie Studierenden können durch statistische Modelle Kosten- und Nutzenabschätzungen durchführen, um marktwirtschaftliche, technische und biologische Prozesse in den Forschungsfeldern der Agrarwissenschaften zu planen. Sie sind in der Lage, externe Literaturquellen zu nutzen und statistische Maßzahlen in den Kontext ihres Berufsfeldes zu setzen, um fundierte Entscheidungen zu treffen. Außerdem können die Studierenden grundlegende Konzepte der Programmierung in R erkennen und eine Datenauswertung durchführen.\n\n\nWissenschaftliche Innovation\nDie Studierenden verfügen über die Fähigkeit, statistische Maßzahlen aus wissenschaftlichen Publikationen in verschiedenen wissenschaftlichen Kontexten zu interpretieren und anzuwenden. Sie besitzen grundlegende Fähigkeiten des wissenschaftlichen Arbeitens, die es ihnen ermöglichen, praktische Fragestellungen in wissenschaftliche Erkenntnisprozesse zu übersetzen. Darüber hinaus können sie statistische Auswertungen aus wissenschaftlichen Publikationen verstehen und aufbauend darauf informierte Forschungsideen entwickeln.\n\n\n\nKommunikation und Kooperation\nDie oben genannten Fähigkeiten der Studierenden ermöglichen es ihnen, in multidisziplinären wissenschaftlichen Teams mitzuwirken. Sie können eine gemeinsam geplante Forschungsskizze von Drittmittelprojekten oder in Kooperation mit Wirtschaftspartnern und Forschungsanstalten umsetzen. Darüber hinaus sind die Studierenden in der Lage, die Ergebnisse einer statistischen Analyse auch Fachfremden verständlich zu erläutern.\n\n\nWissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden haben ein umfangreiches Verständnis für wissenschaftliche Publikationen und statistische Maßzahlen und können dieses Wissen auf ihr Berufsfeld anwenden. Dadurch sind sie in der Lage, komplexe naturwissenschaftliche und technische Fragestellungen aus den Forschungsfeldern der Agrarwissenschaften eigenständig und fundiert mit statistischen Methoden zu bearbeiten, was sie für anspruchsvolle Aufgaben in der agrarwissenschaftlichen Forschung und Praxis qualifiziert.\n\n\n\nLiteratur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\nData Science for Agriculture in R unter https://schmidtpaul.github.io/DSFAIR/\nBruce, Peter, Andrew Bruce, und Peter Gedeck. 2020. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media.\n\n\n\nEmpfohlene Vorkenntnisse\nFür dieses Modul werden vertiefte Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie in den Modulen “Mathematik und Statistik (44B0266)” und “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden. Zudem werden Grundregeln und -methoden des wissenschaftlichen Arbeitens aus dem Modul “Wissenschaftliches Arbeiten (44M0159)” als bekannt vorausgesetzt.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\n\nZusammenhang mit anderen Modulen\nDas Modul “Biostatistik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nWissenschaftliche Publikation und Fachtagungen (44M0125)\nForschungs- und Entwicklungsprojekt (44M0043)\nMasterarbeit (44M0267)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\n\nVoraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nBenotete Prüfungsleistung\nKlausur",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Organisation</span>"
    ]
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "2  Bibliothek",
    "section": "",
    "text": "Bloggern, denen ich folge…\nUnd was lese ich so? Ich folge einigen Bloggern in der unteren Liste. Da schaue ich immer mal was die so machen, damit meine Fähigkeiten nicht einrosten. Schöne Sachen machen die dort! Auch lerne ich dort immer mal wieder neue R Tricks, die dann hier im Buch erscheinen.\nDas ist auch irgendwie so meine Linksammlung geworden. Im Weiteren dann vollständige Bücher, die du dir gerne anschauen kannst.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#bloggern-denen-ich-folge",
    "href": "literature.html#bloggern-denen-ich-folge",
    "title": "2  Bibliothek",
    "section": "",
    "text": "Econometrics and Free Software by Bruno Rodrigues\nThe 20% Statistician by Daniel Lakens\nBlog by Andrew Heiss\nData Visualization & Information Design by Cédric Scherer\nStatistics, Data, Science by Michael Clark\nChris Brown’s blog on ocean science, quantitative ecology and R programming\nThe broken bridge between biologists and statisticians\nCoding Club: a positive peer-learning community\nDave Tang’s Blog | Computational Biology and Genomics\nNicola Rennie’s Blog\nLayton R blog | Data Stories",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#parametrische-statistik",
    "href": "literature.html#parametrische-statistik",
    "title": "2  Bibliothek",
    "section": "Parametrische Statistik",
    "text": "Parametrische Statistik\nDormann (2013) liefert ein tolles deutsches Buch für die Vertiefung in die Statistik. Insbesondere wenn du wissenschaftlich Arbeiten willst weit über die Bachelorarbeit hinaus. Dormann baut in seinem Buch eine hervorragende Grundlage auf. Das Buch ist an der Hochschule Osnabrück kostenlos über den Link zu erhalten. Oder aber das Buch Angewandte Statistik für die biologischen Wissenschaften was ebenfalls von Dormann und Kühn (2009) geschrieben wurde. Beide Bücher überschneiden sich etwas, aber das ist eigentlich nicht so das Problem.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#experimental-methods-in-agriculture",
    "href": "literature.html#experimental-methods-in-agriculture",
    "title": "2  Bibliothek",
    "section": "Experimental methods in agriculture",
    "text": "Experimental methods in agriculture\nOnofri und Sacco (2021) haben das Buch Experimental methods in agriculture geschrieben. Wir werden auf dieses englische Buch ab und zu mal verweisen. Insbesondere der Einleitungstext zur Wissenschaft und dem Design von Experiementen ist immer wieder lesenswert. Spätere Teile des Buches sind etwas mathematischer und nicht für den Einstieg unbedingt geeignet. Aber schaue es dir selber an.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#r-for-data-science",
    "href": "literature.html#r-for-data-science",
    "title": "2  Bibliothek",
    "section": "R for Data Science",
    "text": "R for Data Science\nWickham (2016) ist die Grundlage für die R Programmierung. Das Material von Wickahm findet sich kostenlos online unter https://r4ds.hadley.nz/ und https://www.tidyverse.org/. Wir werden uns hauptsächlich mit R wie es Wickham lehrt beschäftigen. Somit ist Wickham unsere Grundlage für R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#moderne-datenanalyse-mit-r",
    "href": "literature.html#moderne-datenanalyse-mit-r",
    "title": "2  Bibliothek",
    "section": "Moderne Datenanalyse mit R",
    "text": "Moderne Datenanalyse mit R\nMit dem Buch Moderne Datenanalyse mit R – Daten einlesen, aufbereiten, visualisieren, modellieren und kommunizieren von Sauer (2019) steht noch eine freies, deutsches Buch zu Verfügung. Frei ist es dann über das Hochschulnetzwerk Osnabrück. Das Buch ist eine Alternative, wenn es eben dann noch Deutsch als Literatur sein soll.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#data-science-for-agriculture-in-r",
    "href": "literature.html#data-science-for-agriculture-in-r",
    "title": "2  Bibliothek",
    "section": "Data Science for Agriculture in R",
    "text": "Data Science for Agriculture in R\nSchmidt liefert auf der Webseite https://schmidtpaul.github.io/DSFAIR/index.html eine tolle Sammlung an experimentellen Designs bzw. Versuchsanlagen samt der Auswertung in R. Ohne Vorkenntnisse schwer zu verstehen. Sollte aber nach dem Besuch eines meiner Module dann möglich sein. Gerne hier auch mich fragen, dann können wir gemeinsam das passende Design raussuchen und besprechen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#big-book-of-r",
    "href": "literature.html#big-book-of-r",
    "title": "2  Bibliothek",
    "section": "Big Book of R",
    "text": "Big Book of R\nDas Big Book of R beinhaltet über 300 (!) frei verfügbare Bücher, die sich mit R und entsprechenden Themen beschäftigen. Wie der Autor oder Sammler des Buches so schön schreibt, du brauchst nur noch diesen Link, du findest hier alles.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#practical-statistics-for-data-scientists",
    "href": "literature.html#practical-statistics-for-data-scientists",
    "title": "2  Bibliothek",
    "section": "Practical Statistics for Data Scientists",
    "text": "Practical Statistics for Data Scientists\nBruce (2020) schreibt ein Buch für den Anwender. Ohne Vorkenntnisse ist das Buch vermutlich etwas schwer zu lesen. Dafür bietet das Buch aber nach einem Statistikkurs sehr gute Anknüpfungspunkte Richtung maschinelles Lernen und somit der Klassifikation. Das Buch ist auch hier in der englischen Version und hier in der deutschen Version zu erhalten. Beide Links benötigen den Zugang über die Hochschule Osnabrück.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#educational-papers-from-nature-biotechnology-and-plos-computational-biology",
    "href": "literature.html#educational-papers-from-nature-biotechnology-and-plos-computational-biology",
    "title": "2  Bibliothek",
    "section": "Educational papers from Nature Biotechnology and PLoS Computational Biology",
    "text": "Educational papers from Nature Biotechnology and PLoS Computational Biology\nDie Sammling Educational papers from Nature Biotechnology and PLoS Computational Biology liefert eine Übersicht an spannenden und gut zu lesenden wissenschaftlichen Veröffentlichungen, die komplexe Zusammenhänge einfach zu erklären versuchen. Auch kannst du hier schön Paper lesen üben. Schau einfach mal rein, ob da was passendes für dich dabei ist.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#ten-simple-rules",
    "href": "literature.html#ten-simple-rules",
    "title": "2  Bibliothek",
    "section": "Ten Simple Rules",
    "text": "Ten Simple Rules\nDie Sammlung von Ten Simple Rules bieten einen schnellen, konzentrierten Leitfaden für die Bewältigung einiger der beruflichen Herausforderungen, mit denen Forscher in ihrer Karriere konfrontiert sind. Ich fine die Sammlung fantastisch und finde immer wieder inspirierende Ideen. Nicht alles ist sinnvoll, aber vieles interessant und lohnt sich zum drüber nachdenken.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#a-biologists-guide-to-statistical-thinking-and-analysis",
    "href": "literature.html#a-biologists-guide-to-statistical-thinking-and-analysis",
    "title": "2  Bibliothek",
    "section": "A biologist’s guide to statistical thinking and analysis",
    "text": "A biologist’s guide to statistical thinking and analysis\nDas Buch A biologist’s guide to statistical thinking and analysis liefert eine wunderbare Übersicht von statistischen Ideen und Konzepten aus der Sicht von Biologen. Es dreht sich zar alles um Würmer, aber die Analysen und biologischen Schlussfolgerungen machen dennoch Sinn. Ich lese da immer mal wieder gerne quer. Besonders der Abschnitt A quick guide to interpreting different indicators of variation und Comparisons of more than two means zu multiplen Vergleichen fand ich sehr gut zu lesen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#advanced-data-analysis-from-an-elementary-point-of-view",
    "href": "literature.html#advanced-data-analysis-from-an-elementary-point-of-view",
    "title": "2  Bibliothek",
    "section": "Advanced Data Analysis from an Elementary Point of View",
    "text": "Advanced Data Analysis from an Elementary Point of View\nDas sehr lange Skript Advanced Data Analysis from an Elementary Point of View von Cosma Rohilla Shalizi gibt nochmal mehr Einblicke in die Welt der Statistik. Wie auch bei mir ist bei Cosma Shalizi alles immer größer geworden und mehr angewachsen. Wenn du mehr über Statistik lesen willst, ist es eine Goldgrube an Skripten und Ideen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#odds-ends",
    "href": "literature.html#odds-ends",
    "title": "2  Bibliothek",
    "section": "Odds & Ends",
    "text": "Odds & Ends\nAm Ende dann noch eine Mathebuch von Weisberg zu finden unter https://jonathanweisberg.org/vip/. Eigentlich eher ein Buch über Wahrscheinlichkeiten und wenn ein Buch am Ende stehen muss, dann ist es dieses Buch. Ich finde es sehr spannend zu lesen, aber das ist dann vermutlich special intrest.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "literature.html#referenzen",
    "href": "literature.html#referenzen",
    "title": "2  Bibliothek",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nBruce P, Bruce A, Gedeck P. 2020. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media.\n\n\nDormann CF. 2013. Parametrische Statistik. Springer.\n\n\nDormann CF, Kühn I. 2009. Angewandte Statistik für die biologischen Wissenschaften. Helmholtz Zentrum für Umweltforschung-UFZ 2.\n\n\nSauer S. 2019. Moderne Datenanalyse mit R: Daten einlesen, aufbereiten, visualisieren, modellieren und kommunizieren. Springer-Verlag.\n\n\nWickham H, Grolemund G. 2016. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bibliothek</span>"
    ]
  },
  {
    "objectID": "forschungsprozess.html",
    "href": "forschungsprozess.html",
    "title": "3  Forschungsprozess",
    "section": "",
    "text": "3.1 Historische Forschungsprozesse\nIn diesem Abschnitt wollen wir uns verschiedene Forschungsprozesse an verschiedenen historischen Beispielen anschauen. Spannend ist, dass einige dieser Forschungsprozesse nicht wirklich abgeschlossen sind. Selten ist der Anfang und das Ende der wissenschaftlichen Entwicklung wirklich präzise zu benennen. Das kann ja auch nicht sein, den Forschung ist ein fortschreitende Prozess, der kein definiertes Ende hat. Das Schwierigste am Forschen ist daher oft für sich das individuelle Ende zu benennen. Ein Ende zu finden und seine Ergebnisse zu publizieren und der Öffentlichkeit zu Verfügung zu stellen, obwohl man subjektiv noch gar nicht fertig ist. Zwar mögen Forschungszweige nicht mehr relevant sein und verschwinden, aber die ganz großen Fragen bleiben meist vollständig unbeantwortet. Vor allem die großen Fragen nach der Meachnik der Welt und dem Inneren des Menschen.\nIn Abbildung 3.1 sehen wir den Menschen der hinter das Himmelszelt schaut auf der Suche zu Verstehen wie die Himmelmechanik funktioniert. Dieses Bild von 1888 soll das mittelalterliche Weltbild symbolisieren. Der unbekannte Künstler zeigt in dem Holzstich die Suche des Menschen nach der Mechanik hinter der Welt. Die Welt folgt Regeln und diese Regeln sind zu entdecken. Damit sind diese Regeln auch determiniert oder man könnte auch sagen besiegelt. Nun wissen wir aber aus unserer heutigen Weltsicht, dass dieses Weltbild nicht stimmt. Die Welt in der wir leben ist nicht determiniert oder vorherbestimmt.\nIn der Tabelle 3.1 sind als Auswahl vier Kränkungen der Menschheit dargestellt. Es gibt je nach Aufzählung mal mehr oder weniger Kränkungen. Allen Kränkungen ist aber gemein, dass sie den Selbst- und Weltbezug der Menschen fundamental geändert haben. Jede dieser Kränkungen sorgte für einen Paradigmenwechsel in den folgenden Generationen. Nach diesen Entdeckungen war die Welt für die Menschen nicht mehr so, wie die Welt vor der Entdeckung war. In der Forschung werden diese Entdeckungen auch Kränkungen genannt, da sie den Menschen von dem Thron der Schöpfung stoßen und der Menschheit den Status der Ausgewähltheit nehmen. Am Ende sind wir dann doch nur felllose Trockennasenaffen, die auf einem x-beliebigen felsigen Planten im nirgendwo umhertreiben.\nIn den folgenden Abschnitten gehen wir auf die vier hier vorgestellten Kränkungen der Menschheit ein. Wir schauen uns an, was die Begleitumstände der Entdeckungen waren und wie sich die wissenschaftlichen Erkenntnisse einordnen lassen. Nach dem Motto ‘Was ist Wasser?’ schauen wir uns die verschiedenen Denkschulen und wissenschaftlichen Ideen einmal näher an. Was die endgültige Wahrheit ist, werden wir aber in der Folge nicht herausfinden können.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Forschungsprozess</span>"
    ]
  },
  {
    "objectID": "forschungsprozess.html#historische-forschungsprozesse",
    "href": "forschungsprozess.html#historische-forschungsprozesse",
    "title": "3  Forschungsprozess",
    "section": "",
    "text": "“Ein Schiff ist im Hafen sicher, doch dafür werden Schiffe nicht gebaut.” — John Augustus Shedd\n\n\n\n\n\n\n\n\n\nAbbildung 3.1— Der Mensch auf der Suche nach neuen Erkenntnissen durchstößt den Horizont und entdeckt die dahinterliegenden Gesetze.\n\n\n\n\n\n\n\nTabelle 3.1— Die vier Kränkungen der Menschheit. Je nach Quelle sind es auch drei oder eben mehr.\n\n\n\n\n\nArt der Kränkung\nQuelle\nBeschreibung\n\n\n\n\nkosmologisch\nKopernikus (1543)\nDie Erde und damit auch der Mensch ist nicht Mittelpunkt der Welt.\n\n\nbiologisch\nDarwin (1859)\nDie Menschheit ist in das Entwicklungssystem der Organismen eingegliedert.\n\n\npsychologisch\nFreud (1895)\nDer Mensch ist noch nicht Herr der eigenen Handlungen, sondern wird (vermutlich) vom Unbewussten gelenkt.\n\n\nethologisch\nHeinroth 1910\nNicht nur unser Körperbau, sondern auch unser Verhalten ist aus dem Tierreich hervorgegangen.\n\n\n\n\n\n\n\n\n3.1.1 Logik\n\n“Alles was ich kenne, ist Logik.” — Commander Spock\n\nWenn wir an Logik denken, dann kommt uns vermutlich als griechischer Vertreter Aristoteles (384–322 v. Chr.) in den Sinn. Wenn ich an Logik denke, dann habe ich Commander Spock (2230–2263 n. Chr.) und Commander Data (2338–2379 n. Chr.) vor den Augen, aber das ist ja popkulturell immer etwas unterschiedlich. Allgemein gesprochen, hat Logik nichts mit Gefühlen zu tun. Demnach wird logischen Argumenten auch die Emotion oder Empathie abgesprochen. Prinzipiell muss ein logischer Schluss nicht empathielos sein, da ein logischer Schluss aber ein allgemeingültige Aussage im Sinn hat, ist wenig Platz für das Individuum. Zwei zentrale Ideen der Logik sind dabei die Deduktion und Induktion. Beide Verfahren sind miteinander verwoben, was du nochmal in der Abbildung 3.2 nachvollziehen kannst.\n\n\n\n\n\n\nAbbildung 3.2— Eine vereinfachenden Übersicht über Deduktion und Induktion. In der Empirie werden Daten aus Einzelfällen erhoben. Aus diesen wird dann per Induktion eine allgemeine Theorie gewonnen. Aus der Theorie wiederum können per Deduktion Aussagen über Einzelfälle in der Empirie gewonnen werden.\n\n\n\nBetrachten wir nun als erstes die Deduktion. Die Deduktion ist ein logischer Denkprozess, bei dem spezifische Schlussfolgerungen aus allgemeinen Aussagen gezogen werden. In der Deduktion geht es darum, von bereits bekannten Informationen auf neue Erkenntnisse zu schließen. Dieser Prozess basiert auf der Anwendung von logischen Regeln. Ein klassisches Beispiel für die Deduktion ist das so genannte Syllogismus-Argument:\n\nAllgemeine Aussage: Alle Menschen sind sterblich.\nSpezifische Aussage: Sokrates ist ein Mensch.\nSchlussfolgerung: Daher ist Sokrates sterblich.\n\nIm Kontrast dazu steht die Induktion. Die Induktion ist ein logischer Denkprozess, bei dem allgemeine Aussagen aufgrund spezifischer Beobachtungen oder Erfahrungen abgeleitet werden. Induktives Denken basiert auf der Annahme, dass wiederholte Beobachtungen oder Erfahrungen auf allgemeine Muster oder Prinzipien hindeuten. Ein Beispiel für induktives Denken ist folgendes:\n\nBeobachtung: Jedes Mal, wenn ich einen Apfel fallen lasse, fällt er auf den Boden.\nBeobachtung: Jedes Mal, wenn meine Freunde einen Apfel fallen lassen, fällt er auf den Boden.\nBeobachtung: Jedes Mal, wenn Menschen auf der ganzen Welt einen Apfel fallen lassen, fällt er auf den Boden.\nSchlussfolgerung: Daher schließe ich, dass alle Objekte, die fallen gelassen werden, auf den Boden fallen.\n\nEs ist wichtig anzumerken, dass Induktion aufgrund der begrenzten Beobachtungen oder Erfahrungen immer zu gewissen Unsicherheiten führt. Eine Schlussfolgerung aufgrund induktiven Denkens kann immer widerlegt werden, wenn neue Beobachtungen oder Erfahrungen gemacht werden.\nZusammenfassend kann man sagen, dass Deduktion von Allgemeinem auf Spezifisches schließt, während Induktion von Spezifischem auf Allgemeines schließt. Deduktion bietet gewöhnlich eine hohe Gewissheit, während Induktion zu wahrscheinlichen Schlussfolgerungen führt, aber nicht notwendigerweise zu absoluten Wahrheiten. Die Deduktion und Induktion sind somit zwei grundlegende Methoden des logischen Denkens und des wissenschaftlichen Untersuchens. Sie werden in verschiedenen Bereichen der Philosophie, Mathematik, Logik und Wissenschaft angewendet.\n\n\n3.1.2 Determinismus\n\n“Alles ist bestimmt, der Anfang wie das Ende, durch Kräfte, über die wir keine Kontrolle haben. Es ist sowohl für das Insekt als auch für den Stern bestimmt. Ob Mensch, Gemüse oder kosmischer Staub, wir alle tanzen nach einer geheimnisvollen Melodie, die in der Ferne von einem unsichtbaren Pfeifer intoniert wird.” — Albert Einstein\n\nIst das Universum determiniert (lat. vorbestimmt) und lassen sich alle Ereignisse in der Vergangenheit und der Zukunft nach den Gesetzen der Physik vorhersagen? Wir haben die Logik der alten Griechen von unter anderem Aristoteles verstanden. Wir können über Deduktion und Induktion logische Schlüsse über die Welt um uns herum ziehen. Damit können wir durch das logische Verknüpfen von Aussagen zu allgemeinen Erkenntnissen gelangen. Nun fragen wir uns wie die Welt funktioniert.Damit sollte sich doch logisch die Realität und deren Gesetzte erklären lassen. Das hat sich auch Pierre-Simon Laplace (1749–1827) gedacht und kommt zu folgenden Schluss über die Welt.\n\n“Wir müssen also den gegenwärtigen Zustand des Universums als Folge eines früheren Zustandes ansehen und als Ursache des Zustandes, der danach kommt. Eine Intelligenz, die in einem gegebenen Augenblick alle Kräfte kennt, mit denen die Welt begabt ist, und die gegenwärtige Lage der Gebilde, die sie zusammensetzen, und die überdies umfassend genug wäre, diese Kenntnisse der Analyse zu unterwerfen, würde in der gleichen Formel die Bewegungen der größten Himmelskörper und die des leichtesten Atoms einbegreifen. Nichts wäre für sie ungewiss, Zukunft und Vergangenheit lägen klar vor ihren Augen.” — Pierre-Simon Laplace\n\nNach dieser Aussage wäre es theoretisch möglich, eine Weltformel aufzustellen. Wir nennen diese Veranschaulichung auch den Laplaceschen Dämon. Der Laplaceschen Dämon ist die Veranschaulichung der erkenntnis- und wissenschaftstheoretischen Auffassung, nach der es im Sinne der Vorstellung eines geschlossenen mathematischen Weltgleichungssystems möglich ist. Grundlage dieses Gedankens ist der Gesetzesdeterminismus. Das Universum gleiche einem logischen Uhrwerk; eine Intelligenz habe das Universum mit seinen Gesetzen so geschaffen, wie ein Uhrmacher die perfekte Uhr bauen würde.\nEin ein Beispiel für den Determinismus wollen wir uns einmal unser Sonnensystem anschauen. Der Aufbau unseres Sonnensystems war ja Jahrtausende unbekannt. Wir wollen die flache Erde gleich hinter uns lassen und betrachten den Globus Erde im Planetensystem. Die ersten Theorien setzten die Erde in den Mittelpunkt. In Abbildung 3.3 sehen wir einmal das sehr vereinfachte geozentrische Weltbild (altgriechisch erdzentriert), welches auf der Annahme basiert, dass die Erde und damit auch der Mensch im Universum eine zentrale Position einnehmen, so dass alle Himmelskörper, wie Mond, Sonne, die anderen Planeten und die Fixsterne, die Erde umkreisen.\n\n\n\n\n\n\nAbbildung 3.3— Geozentrische Darstellung des Sonnensystems mit der Erde im Zentrum und Planeten auf kreisförmigen Umlaufbahnen. Um 1540 entwarf Kopernikus die Idee des heliozentrische Weltbild mit der Sonne im Mittelpunkt. Erst um 1600 beschrieb Keppler die eliptischen Umlaufbahnen um die Sonne und ab 1821 wurde die Sonne nicht mehr als bewohnbaren Planet oder ‘terrestrische’ Sonne gesehen.\n\n\n\nDas geozentrische Weltbild entspricht dem unmittelbaren Augenschein und wurde schon im klassischen Altertum in Griechenland, insbesondere bei Aristoteles, detailliert ausgearbeitet. Genauso wie die Flache Erde dem Augenschein entspricht. Das geozentrische Weltbild ist somit auch wesentlich besser mit dem gesunden Menschenverstand übereinzustimmen als eine sich bewegende Erde. Bei der Bewegung der Erde müsste man doch einen Fahrtwind spüren und fallenden Gegenstände eine schräge Bahn besitzen.\nKoperinikus (1473–1543) und Kepler (1571–1630) haben die gängigen Ansichten über die Realität hinterfragt und versucht die Unstimmigkeiten in den Beobachtungen am Sternenhimmel durch ein besseres Modell der Wirklichkeit zu ersetzen. Dies gelang beiden auch bis zu einem Punkt. Zusammen mit der Gravitationstheorie von Newton konnte ein schlüssiges Modell des Sonnensystems erstellt werden. Es gab zwar noch ein paar Fragen, aber hier können wir einmal Philipp von Jolly (1809–1884) zitieren der dem jungen Max Planck (1858–1947) folgenden Rat mitgab.\n\n“Die Physik ist eine hochentwickelte, nahezu voll ausgereifte Wissenschaft, die nunmehr, nachdem ihr durch die Entdeckung der Energie gewissermaßen die Krone aufgesetzt wurde, wohl bald ihre endgültige stabile Form annehmen wird. Wohl gibt es vielleicht in einem oder dem anderen Winkel noch ein Stäubchen oder ein Bläschen zu prüfen und einzuordnen, aber das System als Ganzes steht ziemlich gesichert da, und die Theoretische Physik nähert sich merklich demjenigen Grade der Vollendung, wie ihn etwa die Geometrie schon seit Jahrhunderten besitzt.” – Philipp von Jolly zum jungen Max Planck 1874 (Ecker 2017)\n\nEine der Fragen, die Newotn nicht beantworten konnte, war die Frage nach der Übertragung der Information der Gravitation? Wenn die Gravitation eine Kraft ist, wie wird die Kraft übertragen. Wie weiß die Erde und der Mond, dass beide da sind? Woher weiß die Erde von der Graviation der Sonne? Wie wird die Kraft übertragen? Darüber hinaus stellte man noch eine andere zu beobachtende Anomalie fest. Diese Anomalie war nicht wegzudiskutieren, weil messbar. Im 19. Jahrhundert stellte man bei Bahnbeobachtungen des Merkur fest, dass seine tatsächliche Umlaufbahn von der Form einer Kepler-Ellipse abweicht. Der Fehler konnte mit verbesserter Teleskopen auf 43 Bogensekunden von 3600 Bogensekunden pro Jahrhundert reduziert werden. Somit ergab sich eine Abweichung von \\(43/3600 = 0.012 = 1.2\\%\\). Auch wenn dieser Betrag sehr klein ist, bleibt das Ergebnis unvereinbar mit der Himmelsmechanik nach Isaac Newton (1642–1726).\n\n\n\n\n\n\nAbbildung 3.4— Um die Abweichung der Umlaufbahn des Merkurs durch das Modell nach Newton zu erklären, wurde der Planet Vulkan als Hypothese in den wissenschaftlichen Diskurs gebracht. Quelle: Lith. of E. Jones & G.W. Newman - Library of Congress. https://www.loc.gov/resource/g3180.ct003790\n\n\n\nErst mit der allgemeinen Relativitätstheorie von Albert Einstein (1879–1955) war die Abweichung der Merkurlaufbahn hinreichend erklärbar. Massereiche Objekte krümmen die Raumzeit. Objekte laufen auf Bahnen entlang der gekrümmten Raumzeit und beeinflussen sich so gegenseitig. Es ist keine Kraft, die an den Objekten zieht sondern die Krümmung des Raums, welches Objekte beeinflusst. Somit war die theoretische Physik an einem Scheitelpunkt angekommen. Würde sich jetzt die Aussage von Philipp von Jolly erfüllen und alles wäre erklärt? Wie sich herausstellte gab es neben dem ganz Großen und deren Wechselwirkungen, den Planetenbewegungen und Objekten auf der Erde, noch die atomaren Wechselwirkungen. Die Effekte von Protonen auf Elektronen und das Verhalten von Atomkernen.\n\n\n\n\n\n\nZum Nachdenken I: Heisenberg und seine Unschärferelation\n\n\n\n\n\nIn der Abbildung 3.5 siehst du den Cartoon von Perscheid über den jungen Heisenberg. Kannst du erklären warum der Cartoon witzig ist, wenn du verstanden hast, was Heisenberg mit der Unschärferelation aussagen will?\n\n\n\n\n\n\nAbbildung 3.5— Warum ist der Cartoon witzig im Bezug auf Heisenberg und seine Unschärferelation? Was lernen wir aus dem Cartoon für die praktische Anwendung? Quelle: https://www.martin-perscheid.de/\n\n\n\n\n\n\nDann kam Werner Heisenberg (1901–1976) und formulierte 1927 die Heisenbergsche Unschärferelation, ein fundamentales Prinzip der Quantenmechanik. Die Heisenbergsche Unschärferelation besagt, dass zwei komplementäre Eigenschaften eines Teilchens nicht gleichzeitig beliebig genau bestimmbar sind. Die heisenbergsche Unschärferelation kann als Ausdruck des Wellencharakters der Materie betrachtet werden. Es herrscht ein Wellen-Teilchen-Dualismus vor. Wir können Elektronen oder Photonen als Welle oder aber als ein Teilchen beschrieben. Somit unterscheidet sich der konzeptionelle Aufbau der Quantenmechanik tiefgreifend von dem der klassischen Physik, so dass sich Einstein zu folgender Aussage hinreißen lies.\n\n“Gott würfelt nicht!” — Albert Einstein\n\nBis zu seinem Tod stand Einstein mit der Quantentheorie und der statistischen Physik von Heisenberg und Co. im Zwiespalt. Die Idee, dass es zwei Modelle geben könnte, eines für die makroskopische Welt und eines für die atomare Welt, blieb für Einstein unbefriedigend. Wenn das kleinste Atom den Regeln des Zufalls folgt, dann kann es kein Weltengesetz nach Laplace geben. Wie soll etwas im Großen determiniert sein, wenn es im Kleinsten seines Aufbaues dem Zufall folgt? Hier ist die physikalische Forschung seit über einem Jahrhundert auf der Suche nach der Weltenformel, die die Quantenmeachnik mit der Gravitation zusammenbringt. So enden wir diesen Abschnitt mit dem 1. clarkschen Gesetz.\n\n\nDie Unschärferelation gilt nicht nur in der Quantenwelt. Ort und Geschwindigkeit eines Elektrons lassen sich nicht gleichzeitig genau bestimmen. Doch auch in praktischen Bereichen wie Radarmessungen und Blitzern stößt man auf die Unschärferelation.\n\n“Wenn ein angesehener, aber älterer Wissenschaftler behauptet, dass etwas möglich ist, hat er mit an Sicherheit grenzender Wahrscheinlichkeit recht. Wenn er behauptet, dass etwas unmöglich ist, hat er höchstwahrscheinlich unrecht.” — Arthur C. Clarke, 1. Gesetz\n\n\n\n\n\n\n\nZum Nachdenken II: Der Dopplereffekt\n\n\n\n\n\nIn der Abbildung 3.6 siehst du den Cartoon von Perscheid über den Dopplereffekt. Kannst du erklären warum der Cartoon witzig ist, wenn du verstanden hast, was der Dopplereffekt bedeutet? Ich habe dir als Hilfe nochmal die Abbildung 3.7 dazugetan, die die Wellenlänge des Lichts beschreibt.\n\n\n\n\n\n\nAbbildung 3.6— Warum ist der Cartoon witzig im Bezug auf den Dopplereffekt und der konstanten Lichtgeschwindigkeit? Was lernen wir aus dem Cartoon für die praktische Anwendung? Quelle: https://www.martin-perscheid.de/\n\n\n\n\n\n\n\n\n\nAbbildung 3.7— Spektrum des sichtbaren Lichts un der Wellenlänge des Lichts.\n\n\n\nMehr Informationen zum Dopplereffekt und deren Anwendungen auf Wikipedia und ein toller Artikel, der nochmal anders ins Detail geht sowie auf Deutsch ist, ist die Seite zu Christian Doppler https://www.christian-doppler.net/\n\n\n\n\n\n\n\n\n\nZum Nachdenken III: Warum ist es nachts dunkel?\n\n\n\n\n\nEine spannende Frage. Weil die Sonne nicht scheint? Wenn du in jede Richtung in den Himmel schaust, müsstest du doch auf einen Stern schauen. Es gibt Trillionen an Sternen am Nachthimmel, so dass jeder beliebige Punkt mit einem Stern belegt sein muss. Und trotzdem ist der Nachthimmel nicht gleißend weiß. Mehr dazu auf der Welt der Physik - Warum ist es nachts dunkel? oder aber bei Spektrum - Warum ist es nachts dunkel?. Eine spannende Sache, wenn du die Erkenntnisse mit den Ergebnissen von Einstein abgleichst.\n\n\n\n\n\n3.1.3 Eine neue physikalische Theorie des Lebens\n\n“Die Wissenschaft fängt eigentlich erst da an interessant zu werden, wo sie aufhört.” — Justus von Liebig\n\nWarum vergeht eigentlich Zeit? Was ist eigentlich Zeit? Und warum haben wir eigentlich Uhren, die auf einem 12er Zählung bzw. auf einer 2-mal-12-Stunden-Zählung und dann auf einer 60ziger Zählung beruhen? Zuerst klären wir einmal diese komische Anonmalie. Wir haben ein 60ziger System, da die alten Griechen es von den Ägyptern und diese wiederum von den Babylonier übernommen haben. Die 12 ist eine sehr spannende Zahl und hat viele Teiler, so dass mathematisch interessierte Babylonier die 12 höher werteten als die 10. Niemand wusste ja, dass wir mal mit dem Unsinn programmieren würden wollen. Zwar gab es dann kurz den versuch die Zeit auf die Dezimalzeit umzustellen und es wurde im Rahmen der französichen Revolution auch ein neuer Französischer Revolutionskalender erstellt, aber die Sache hat sich dann nicht so durchgesetzt. Zeit selber spielt übrigens erst seit der Erfindung der Eisenbahn eine echte Rolle. Vorher war es recht egal, wie genau die Uhren in der einen Stadt im Bezug zur anderen Stadt liefen. Man war ja eine ganz schön lange Zeit unterwegs, so dass Abweichungen nun auch keine Rolle mehr spielten.\nAber warum schreitet die Zeit eigentlich voran? Also warum gibt es ein gestern, heute und morgen? Dabei sind doch alle Naturgesetze in der klassischen Physik in der Zeit symmetrisch. Die Zeit ist auch symmetrisch im Bereich der neueren Entwicklungen wie der Relativitätstheorie als auch der Quantenphysik. Wir können also die Zeit in die eine Richtung laufen lassen wie auch in die andere Richtung. Daher gibt es eigentlich keinen physikalischen Grund, warum die Zeit immer nur in eine Richtung fortschreiten sollte. Es könnte also auch die Zeit rückwärts laufen. Der Tee und die Milch könnte sich in der Tasse der Queen wieder entmischen. Das passiert aber nicht.\nEin einziges Naturgesetz ist daran schuld und zwar der Zweite Hauptsatz der Thermodynamik. Der Zweite Hauptsatz der Thermodynamik besagt, dass alle Vorgänge immer nur so ablaufen, dass dabei die “Unordnung” größer und niemals kleiner werden wird. Leider ist der Begriff “Unordnung” physikalisch wissenschaftlich schwer zu definieren. Unordnung ist ein sehr weiter Begriff und vieles kann Unordnung bedeuten. Hier kommt es dann auch stark auf den Kontext an. Deshalb haben Physiker dafür das Wort Entropie erschaffen. Daher ist es die stets wachsende Entropie, die der Zeit einen eindeutigen Richtungspfeil aufzwingt. So kommen wir dann zu diesem Zitat von Rudolf Clausius (1822–1888), dem Entdecker des zweiten Hauptsatzes der Thermodynamik, Schöpfer der Begriffe Entropie.\n\n“Bei jedem natürlichen Vorgang nimmt die Entropie zu.” — Rudolf Clausius\n\nDie Entropie steigt also immer weiter an. Damit stehen wir aber vor einer interessanten Frage, wenn natürliche Vorgänge stets einen Ausgang mit höherer Entropie als der Eingangszustand bevorzugen, warum gibt es dann komplexes Leben? Wieso haben sich Makromoleküle als Grundlage des Lebens in der Ursuppe der Urerde gebildet, wenn es doch der Zunahme der Entropie widerspricht. Hier kommt die Idee von Jeremy England (2013) ins Spiel. Folgendes Zitat statt aus seiner wissenschaftlichen Arbeit zur einer neuen physikalische Theorie des Lebens.\n\n“You start with a random clump of atoms, and if you shine light on it for long enough, it should not be so surprising that you get a plant.” — Jeremy England\n\n\n\nNoch ein ergänzender Artikel des The Quanta Magazine A New Physics Theory of Life und das PDF des Orginalartikels. Sowie der Vortrags von Jeremy England \\(\\;\\)No Turning Back: The Nonequilibrium Statistical Thermodynamics of becoming (and remaining) Life-Like\nDie Idee von Jeremy England ist, dass wir durch kurzfristige Erniedrigung der Entropie durch komplexe Strukturen wie sie das Leben hervorbringt, langfristig ein sehr viel schnellere Erhöhung der Entropie erreichen. Das heißt im Umkehrschluss, dass der zweiten Hauptsatzes der Thermodynamik Leben und komplexere Lebensformen begünstigt. Jeremy England gibt ein Beispiel mit energiegeladenen Photonen, die von der Sonne abgestrahlt werden. In der Abbildung 3.8 siehst du, wie die energiegeladenen Photonen als kurzwelliges blaues Licht einen belebten Planeten treffen. Durch das Leben wird die Energie der Photonen durch Photosynthese sowie andere biologische Prozesse absorbieren. Bei den biologischen Prozessen entstehen jedoch mehr langwellige Photonen als kurzwellige auf den Planeten treffen, so dass in Summe zwanzig energiearme Photonen pro energiereichen Proton als langwelliges rotes Licht die Erde verlassen.\n\n\n\n\n\n\nAbbildung 3.8— Ein energiereichen, kurzwelliges Photon trifft die Erde und durch Prozesse des Lebens werden zwanzig energiearme, langwellige Photonen abgestrahlt. Die Entropie erhöht sich stark.\n\n\n\nIn Abbildung 3.9 siehst du als Gegenbeispiel einen unbelebten Planeten, der von einem Stern angestrahlt wird. Zwar werden auch die hochenergetischen Photonen des Sterns von dem unbelebten Planeten absorbiert und wieder abgestrahlt, aber die Umwandlung ist weit unter dem Wert, die ein belebter Planet verursachen würde.\n\n\n\n\n\n\nAbbildung 3.9— Ein energiereichen, kurzwelliges Photon trifft einen unbelebten Planten und kurzwellige Photonen werden wiederum abgestrahlt. Die Entropie erhöht sich kaum.\n\n\n\nDa wir uns hier natürlich nicht in aller Tiefe mit der Idee beschäftigen können, gibt es noch ein tolles Video zu Vertiefung mit dem Titel \\(\\;\\)The Most Misunderstood Concept in Physics. Dort beschäftigt sich Veritasium mit der Frage, was erhalten wir eigentlich von der Sonne? Nein, Energie und Wärme ist nicht die richtige Antwort – mehr dazu dann im Video.\n\n\n3.1.4 Evolutionstheorie\n\n“It is not the strongest of the species that survives, nor the most intelligent that survives. It is the one that is most adaptable to change.” — Leon C. Megginson zugeschrieben Charles Darwin\n\nWürmer. Konkreter waren es das der Feld der Regenwürmer was Darwin am meisten beschäftige und Jahrzehnte seiner Forschung bestimmte. Anfang des 19. Jahrhunderts galten Regenwürmer als Schädlinge insbesondere außerhalb der Agrarwissenschaft. Darwins genaue Beobachtungen ihrer Lebensweise sowie seine Experimente über ihr Hörvermögen, ihre Lichtempfindlichkeit, ihr Kälte- und Wärmeempfinden und die Tätigkeit ihrer Reflexe führten dazu, dass sich das Wissen um die Nützlichkeit von Regenwürmern für den Ackerbau rasch verbreitete und auch außerhalb von Fachkreisen durchsetzte.\nDarwin entwickelte die Evolutionstheorie während seiner Fahrt mit dem Segelschiff Beagle, die von Ende Dezember 1831 bis Oktober 1836 dauerte und ihn zu Küstenregionen und Inseln der südlichen Erdhalbkugel führte. An Bord der Beagle war Darwin als unbezahlter Naturforscher tätig. Darwin präsentierte erstmals eine frühe Version seiner Evolutionstheorie im Jahr 1858. Das Buch, in dem er seine Theorie ausführlich darlegte, “On the Origin of Species by Means of Natural Selection”, wurde 1859 veröffentlicht und war am Tag seines Erscheinens ausverkauft. Darwin betonte, dass Selektion ein entscheidender Faktor bei der Evolution ist. Obwohl zu Darwins Zeiten nur begrenzte genetische Kenntnisse vorhanden waren, erkannte Darwin die Zusammenhänge, die er auf seinen Reisen beobachtete, richtig.\n\n\nDie Bone Wars war eine persönliche und wissenschaftliche Auseinandersetzung der beiden US-amerikanischen Paläontologen Othniel Charles Marsh und Edward Drinker Cope gegen Ende des 19. Jahrhunderts.\nIm Gegensatz zu den Theorien Darwins, stelle Lamarck eine andere Evolutionstheorie auf. Der nach ihm benannte Lamarckismus war eine der frühen Evolutionstheorien um 1800. Der Lamarckismus besagt dabei, dass jede Art eine eigne Urform besitzt und dass diese Urform den Drang hat, sich möglichst perfekt anzupassen. Jede Art hat also einen innewohnenden Hang zum Perfektionismus. Dabei beeinflussen sich die ändernden Umweltbedingungen und die wechselnden Bedürfnisse der Tiere gegenseitig. Dadurch führt der Gebrauch bestimmter Organe zu ihrer Ausprägung, während der Nichtgebrauch bestimmter Organe zu ihrer Rückbildung führt. Diese erworbenen Eigenschaften werden dann an die Nachkommen weitervererbt. Beim Lamarckismus ist die Entwicklung gerichtet, und Arten entwickeln sich von einfachen zu komplexen Organismen. Jede Art hat eine eigene lineare Evolutionslinie und keinen gemeinsamen Vorfahren. Ein bedeutendes Problem zur Zeit von Lamarck war die Frage nach dem möglichen Aussterben von Arten. Die Frage kam auf als immer mehr Funde von Fossilien auftauchten, die keinen heutigen Tieren zugeordnet werden konnten. Lamarck bestritt jedoch weitgehend, dass Arten aussterben können. Die Fossilien seien vielmehr Ausprägungen einer bekannten Tierart, die jetzt nicht mehr existieren, da die Ausprägung nicht mehr notwendig ist.\nDer Lamarckismus basiert auf einer willentlichen Ausübung der Evolution. Damit ist die Evolution von der Spezies gesteuert. Wir haben hier also wieder den Gedanken des Determinismus vorliegen. Das Schicksal einer Spezies ist in dem Sinn determiniert, dass es nur von einfach zu komplex geht und eine Spezies immer perfekter wird. Der Paradigmenwechsel von Darwin war die Idee, dass wir es mit Populationen von Arten zu tun haben und in diesen Population alle möglichen Eigenschaften schon in sich tragen. Die Populationen folgen einer Wahrscheinlichkeitsverteilung. Es gibt sehr viele Individuen mit den häufisgten Merkmalen aber auch ein paar wenige Individuen mit den seltenen Merkmalen. So kann es durch Naturereignisse, Zufälle oder anderer Ereignisse zu einer Selektion kommen. Durch die Selektion setzen sich vorher seltene Merkmale durch und die Merkmalsträger werden häufiger in der Population.\n\n\n\n\n\n\nZum Nachdenken IV: Die Weltenveränderer\n\n\n\n\n\nDie Große Sauerstoffkatastrophe (abk. GOE, eng. great oxygenation event) veränderte die Welt vor etwa 2,4 Milliarden Jahren grundlegend. Der Verursacher war eine Alge. Konkret die Blaualge. Somit spielt die Blaualge eine bedeutende Rolle in der Entwicklung des Lebens auf der Erde. Vor dem Auftreten der Blaualge war die Atmosphäre der Erde reduzierend, nach dem Auftreten der Blaualge oxidierend. Die Entwicklung der Erdatmosphäre machte einen erneuten Sprung. Große Mengen an Eisen oxidierten und sammelten sich in so genannten Bändererzen.\nIm Weiteren Verlauf der Evolution wurde die Blaualge in andere Organismen aufgenommen. Dieses Integrieren ist die Idee der Endosymbiontentheorie. Die Endosymbiontentheorie besagt, dass die Chloroplasten in pflanzlichen Lebewesen vermutlich durch die Aufnahme durch Phagozytose von Cyanobakterien in die Ur-Form der pflanzlichen Zelle entstanden sind. Dafür spricht unter anderem die eigene DNA der Chloroplasten. Das gleiche gilt auch für die Mitochondrien in eukaryotischen Zellen.\nSiehe dazu auch die Cyanobakterien oder Blaualgen: Bedeutung der Cyanobakterien für die Entwicklung des irdischen Lebens.\n\n\n\n\n\n3.1.5 Wiener Schule\n\n\n\n\n\n\nDisclaimer - Wichtig! Lesen!\n\n\n\nDer folgende Text ist ein Lehrtext für Studierende. Es handelt sich keinesfalls um eine textliche Beratung und gewiss nicht um eine medizinische Einordnung zur Tiefenpsychologie. Dieser Abschnitt beschäftigt sich einzig und allein mit dem philosophischen und geschichtlichen Hintergrund der Psychologie und deren Vertretern in dem Kontext ihrer Zeit am Anfang des 20. Jahrhunderts. Für Hilfe in akuten Krisen und Anlaufstellen informiert auch das Studentenwerk Osnabrück oder die entsprechenden Anlaufstellen des Kreises.\n\n\n\n“Wenn dich etwas Äußeres bedrückt, so liegt der Schmerz nicht an der Sache selbst, sondern an deiner Einschätzung derselben; und diese kannst du jederzeit widerrufen.” — Marcus Aurelius\n\nDer gute alte Determinismus mag zwar auf der Ebene des Universums und der physikalischen Naturgesetze nicht funktionieren, aber könnte er möglicherweise in der Psychologie eine Rolle spielen? Ist es denkbar, dass der Mensch von vorbestimmten psychologischen Abläufen gesteuert wird? Dies könnte als nächste Kränkung der Menschheit betrachtet werden. Sigmund Freud (1856–1939) behauptete, dass der Mensch noch nicht Herr seiner eigenen Handlungen ist, sondern vom Unbewussten gelenkt wird. Dies führt dazu, dass wir durch Freuds Perspektive wieder zum Determinismus zurückkehren.\nEs mag paradox erscheinen, aber es gibt etwas Befreiendes darin, anzunehmen, dass wir Opfer unseres eigenen Unbewussten sind. Wenn alle unsere negativen Eigenschaften und Gewohnheiten nicht direkt unter unserer Kontrolle stehen, fühlen wir uns vielleicht von der Verantwortung entlastet und können uns von Selbstkritik befreien. Es entsteht ein Gefühl, dass wir nicht persönlich für unser Verhalten verantwortlich gemacht werden können, da es durch unsichtbare Kräfte gelenkt wird.\nJedoch sollten wir bedenken, dass die Perspektive des Unbewussten als Determinismus auch bedeuten kann, dass wir uns unseren eigenen inneren Prozessen nicht bewusst sind und somit keine Möglichkeit haben, unser Verhalten bewusst zu lenken. Dies könnte uns daran hindern, uns selbst weiterzuentwickeln und Veränderungen in unserem Leben herbeizuführen. Fangen wir also an uns mal mit den verschiedenen Aspekten der Forschung in der Psychologie zu beschäftigen.\n\n\n\n\n\n\nWirksamkeit der Tiefenpsychologie\n\n\n\nVerschiedene Studien und Metaanalysen zeigen, dass die Psychoanalyse und andere Anwendungen und Verfahren der Tiefenpsychologie effektiv und wirksam in der Behandlung psychischer Störungen sind (De Maat u. a. 2009). Eine Diskussion über diese Ergebnisse findet hier nicht statt. Wir wollen uns mit dem Paradigmenwechsel und den Ideen hinten der Konzepten und Ideen beschäftigen. All dies ist nämlich ein Teil eines Forschungsprozess in dem Bereich der Tiefenpsychologie zu Beginn des 20. Jahrhundert.\n\n\nFreud ist aber nur ein Vertreter der Wiener Schule der Psychologie, die sich mit dem Individuum und seinen Seelenleben und Gefühlen beschäftigte. Zu Beginn des 20. Jahrhunderts entstanden aber parallel grob folgende drei Hauptströmungen der Psychologie. Jede dieser Richtungen hat andere philosophischen Annahmen an das Ich und dem Selbst. Daher schauen wir uns diese drei Richtungen mal etwas genauer an.\n\ndie Psychoanalyse nach Sigmund Freud (1856—1939),\ndie Existenzanalyse von Viktor Frankl (1905–1997)\ndie Individualpsychologie nach Alfred Adler (1870–1937)\n\nSigmund Freud erkannte die Existenz eines inneren unbewussten Bereichs im Menschen, der sein bewusstes Handeln maßgeblich lenkt. Er widmete sich der Erforschung dieses Aspekts durch die Anwendung der Psychoanalyse, bei der er spontane Äußerungen oder Träume seiner Patienten analysierte. Freud betrachtete diese Träume und Äußerungen als verschlüsselte Hinweise auf das Unbewusste. Diese Forschung führte zur Entstehung des Drei-Instanzen-Modells, bestehend aus dem “Ich”, dem “Es” und dem “Über-Ich”.\nIm “Es” lokalisierte Freud die primären Triebe, während das “Über-Ich” das Moralitätsprinzip und somit das Gewissen in sich trug. Das “Ich” nahm die Rolle eines Vermittlers zwischen diesen beiden Polen ein. Das “Ich” beinhaltet auch bewusste Überzeugungen und Auffassungen über die Umwelt und ist somit ein wesentlicher Bestandteil der Persönlichkeit.\nDie Psychoanalyse beruht auf der Annahme, dass ein Mensch nicht alle Faktoren kennt, die sein Leben beeinflussen. Laut Sigmund Freud werden einige, vielleicht sogar die meisten, Konflikte in unserem Leben ins Unterbewusstsein verdrängt. Diese unbewussten Konflikte können sich auf unser Verhalten und unsere Entscheidungen auswirken, ohne dass wir uns ihrer bewusst sind. Durch die psychoanalytische Arbeit können diese verdrängten Konflikte ans Licht gebracht und besser verstanden werden, was dem Individuum helfen kann, sich selbst und sein Verhalten besser zu verstehen und möglicherweise negative Muster zu durchbrechen.\n\nViktor Frankl , der etwas später als Freud lebte, entwickelte die Logotherapie oder Existenzanalyse. Der Begriff “Logotherapie” ist abgeleitet von logos, was Sinntherapie bedeutet. Somit ist die Existenzanalyse eine sinnzentrierte Psychotherapie mit dem Ziel, Lebensbejahung wiederzugewinnen und Lebensangst abzubauen. Die Logotherapie zielt darauf ab, Menschen dabei zu unterstützen, einen tieferen Sinn in ihrem Leben zu finden und dadurch seelische Gesundheit und Erfüllung zu erlangen. Im Zentrum steht die Idee, dass der Mensch in der Lage ist, auch unter schwierigsten Bedingungen Sinn und Bedeutung zu finden und dadurch seine innere Stärke zu entfalten. Er glaubte, dass der Mensch ein wesenbestimmtes, entscheidungs- und willensfreies Wesen ist.\n\n“Wer ein ‘Warum’ zum Leben hat, kann fast jedes ‘Wie’ ertragen.” — Viktor Frankl\n\nDie zentrale Triebkraft der Existenzanalyse liegt in der Definition des Lebenssinns. Der Mensch strebt danach, einen Sinn in seinem Leben zu finden und diesen zu verwirklichen. Die Existenzanalyse zielt darauf ab, den Menschen dabei zu unterstützen, ihren Lebenssinn zu entdecken und ihre Werte zu erkennen, um eine tiefere Erfüllung und Motivation im Leben zu erfahren.\n\n“Das Leben fragt und du musst antworten.” — Viktor Frankl\n\nEin bedeutendes Werk von Viktor Frankl ist sein Buch “…trotzdem Ja zum Leben sagen: Ein Psychologe erlebt das Konzentrationslager” (Frankl 2010), das im Jahr 1946 veröffentlicht wurde. In diesem Buch schildert Frankl seine Erlebnisse und Erfahrungen in vier verschiedenen Konzentrationslagern. Aus diesen Erfahrungen entwickelte er den Begriff der “Trotzmacht des Geistes”. Damit meinte er, dass die emotionale Ebene des Menschen, die aus dem Körperlichen und Seelischen besteht, sich mit der existenziellen Ebene auseinandersetzen und über sie erheben kann. Dies verdeutlicht die Freiheit des menschlichen Willens und die Fähigkeit, sich trotz widriger Umstände mit der eigenen Situation auseinanderzusetzen und daraus etwas zu machen. Viktor Frankl betonte, dass diese Fähigkeit unabhängig von den äußeren Umständen besteht, selbst wenn die allgemeine Lage äußerst schwierig ist.\n\n“Alles kann einem Menschen genommen werden, nur eines nicht: die letzte der menschlichen Freiheiten - die Wahl der eigenen Haltung in einer gegebenen Situation, die Wahl des eigenen Weges.” — Viktor Frankl\n\nDie existenzanalytische Psychotherapie hat das Ziel, dem Menschen dabei zu helfen, mit innerer Zustimmung zu seinem Handeln und Dasein zu leben. Dabei wird betont, dass jeder Mensch für sich selbst den Sinn des Lebens definiert. Dieser individuelle Sinn kann sich im Laufe des Lebens verändern, ähnlich wie das Leben selbst stetig im Wandel ist. In diesem Kontext ist auch das längere Zitat von Viktor Frankl zum Erfolg und Glück im Leben zu verstehen.\n\n“Zielen Sie nicht auf den Erfolg. Je mehr man ihn anvisiert und zu einem Ziel macht, desto mehr wird man ihn verfehlen. Denn der Erfolg kann, wie das Glück, nicht angestrebt werden; er muss sich einstellen, und zwar nur als unbeabsichtigte Nebenwirkung des persönlichen Einsatzes für eine Sache, die größer ist als man selbst, oder als Nebenprodukt der Hingabe an eine andere Person als die eigene. Das Glück muss sich einstellen, und dasselbe gilt für den Erfolg: Man muss ihn zulassen, indem man sich nicht darum kümmert. Ich möchte, dass du auf das hörst, was dein Gewissen dir befiehlt, und es nach bestem Wissen und Gewissen ausführst. Dann werden Sie erleben, dass Ihnen der Erfolg auf lange Sicht - ich sage auf lange Sicht - gerade deshalb folgen wird, weil Sie vergessen haben, daran zu denken.” — Viktor Frankl\n\nFrankl betont, dass was für eine Person als erfolgreich und glücklich empfunden wird, kann sich von dem unterscheiden, was für jemand anderen von Bedeutung ist. Jeder Mensch hat die Freiheit und Fähigkeit, seinen eigenen Sinn und seine eigenen Ziele zu entdecken und zu verfolgen.\n\nNeben Sigmund Freuds psychoanalytischer Theorie entwickelte Alfred Adler zur gleichen Zeit die Individualpsychologie. Adlers Schule betrachtet den Menschen als eine einzigartige und ganzheitliche Persönlichkeit, deren Handeln auf bestimmte Ziele ausgerichtet ist. Hierbei wird betont, dass der Mensch immer einen Zweck hat und somit zweckgetrieben ist. Adler betont, dass es keine individuelle Vergangenheit im Sinne von vorbestimmten Ereignissen gibt, die das Schicksal eines Menschen unveränderlich festlegen. Stattdessen glaubt Adler, dass der Mensch seinen eigenen “Zweck” konstruiert, also seine Ziele und Absichten selbst erschafft und verfolgt.\n\n\n\n\n\n\nAbbildung 3.10— Das Entscheidungsdreieck nach Adler: “Das arme Ich”, “Die bösen Anderen” und “Was sollte ich tun?”\n\n\n\nIm Gegensatz zum deterministischen Ansatz, der besagt, dass das Schicksal eines Menschen von vorherbestimmten Faktoren beeinflusst wird, betont die Individualpsychologie die aktive Rolle des Individuums bei der Gestaltung seines Lebens. Dieser Ansatz betont die Selbstbestimmung und die Verantwortung jedes Menschen für sein eigenes Handeln und seine Lebensgestaltung. Ein zentraler Fokus der Individualpsychologie liegt auf der Position des Einzelnen innerhalb seines sozialen Umfelds sowie auf den Mustern seiner Beziehungsgestaltung. Adler betonte die Bedeutung sozialer Interaktionen und Beziehungen für die individuelle Entwicklung und das Verständnis menschlichen Verhaltens. Dabei nimmt jeder Mensch die Verantwortung für seine Aufgaben in seinem Einflussbereich selbst in die Hand und lässt nicht zu, dass andere diese Aufgaben und Entscheidungen für ihn übernehmen.\n\n“Du lebst nicht um die Erwartungen anderer Menschen zu erfüllen” — Kishimi und Koga (2018)\n\nAdler glaubte, dass jeder Mensch bestrebt ist, sein Selbstwertgefühl zu stärken und sich in der Gesellschaft anzupassen. Hierzu entwickeln Individuen Kompensationsmechanismen, um wahrgenommene Minderwertigkeitsgefühle zu überwinden. Diese Mechanismen können sich auf unterschiedliche Weise manifestieren, wie beispielsweise durch Leistungsstreben, Machtstreben oder das Streben nach Anerkennung. Adler lehnte daher das Loben und das Tadeln als Werkzeug der Wertschätzung und Herabsetzung ab.\nDie beiden Bücher von Kishimi und Koga (2018) & (2020) stellen in Dialogform die Ideen Adlers vor. Interessanterweise haben die Ideen Adlers in Japan sehr viel Anklang gefunden. Hier ein paar zentrale Gedanken, die sich aus den Büchern ergeben.\n\nAlle Probleme sind zwischenmenschliche Beziehungsprobleme.\nAufgabentrennung, die Aufgaben anderer abweisen. Wer erhält am Ende das Resultat, das sich aus der Entscheidung ergibt?\nFreiheit bedeutet auch, von anderen nicht immer gemocht zu werden.\nGlück heißt, einen Beitrag leisten.\nLebe im Hier und Jetzt, ohne dich von der Vergangenheit oder einer unbestimmten Zukunft quälen zu lassen.\n\nDie Individualpsychologie von Alfred Adler hebt somit die Einzigartigkeit und Zielgerichtetheit jedes Individuums hervor und betont die Rolle sozialer Interaktionen und Beziehungen für die psychische Entwicklung. Sie unterstreicht auch die persönliche Verantwortung jedes Menschen für sein eigenes Handeln und die Gestaltung seines Lebens im Kontext seiner sozialen Umgebung.\n\nCarl Jung (1875–1961) brauchte uns ein anderes sehr populäres aber oft missverstandenes Konzept. Das Konzept der Introvertiertheit sowie der Extrovertiertheit wird dabei oft sehr grob und falsch zusammengefasst. Introvertierte Personen sind dabei schüchtern und in sich gekehrt. Extrovertierte Personen seien dagegen sehr sozial und offen. Das stimmt aber mit dem ursprünglichen Konzept von Jung nur zu sehr begrenzten Teilen überein. Das zentrale Konzept in Jungs Psychologie ist die Unterscheidung zwischen Introversion und Extraversion als grundlegende Persönlichkeitsmerkmale. Diese Begriffe wurden von Jung geprägt und haben einen wichtigen Einfluss auf das Verständnis der menschlichen Persönlichkeit.\nFür Jung repräsentiert dabei Introversion eine Ausrichtung auf die innere Welt des Individuums. Introvertierte Menschen neigen dazu, ihre Energie aus ihren eigenen Gedanken, Gefühlen und Empfindungen zu beziehen. Sie bevorzugen oft ruhige und abgeschiedene Umgebungen, um sich zu regenerieren und aufzuladen. Introvertierte sind in der Regel nach innen gerichtet und können tiefe Reflexionen und introspektive Fähigkeiten haben.\nIm Gegensatz dazu steht Extraversion für eine Ausrichtung auf die äußere Welt. Extrovertierte Menschen gewinnen Energie aus ihrer Interaktion mit der äußeren Umgebung, durch soziale Interaktionen und Aktivitäten. Sie sind oft kontaktfreudig, gesellig und suchen nach externen Reizen und Herausforderungen. Extravertierte neigen dazu, ihr Umfeld aktiv zu gestalten und zeigen oft eine offene und expressive Persönlichkeit.\nJung betonte, dass jeder Mensch eine individuelle Kombination von Introversion und Extraversion aufweist. Es geht nicht darum, eine dieser Ausrichtungen als besser oder schlechter zu bewerten, sondern vielmehr darum, die persönlichen Präferenzen und Stärken des Einzelnen zu erkennen und zu akzeptieren.\nSchaue dazu gerne noch mehr auf \\(\\;\\)Carl Jung’s Theory on Introverts, Extraverts, and Ambiverts.\n\nWie auch viele andere Wissenschaftler grenzte sich Erich Fromm (1900–1980) von Freud ab. Fromm lehnte das Freud’sche Konzept des Individuums als isoliertem, triebgesteuertem Wesen ab. Er betont dagegen die ursprüngliche Verbindung des Menschen zu seinen Mitmenschen. Fromm beruft sich nicht nur auf Klassiker der Philosophie, sondern auch auf den Zen-Buddhismus. Er kritisiert die westliche kapitalistische Gesellschaft, deren Prinzipien er für unvereinbar mit seinem Begriff von Liebe hält. Das kapitalistische Grundgefühl des Mangels als Triebfeder für den Konsum und das Wirtschaftssystem lasse sich nicht mit seinem Begriff der Liebe vereinbaren.\n\n“Liebe ist eine Entscheidung, ein Urteil, ein Versprechen. Wenn die Liebe nur ein Gefühl wäre, gäbe es keine Grundlage für das Versprechen, einander für immer zu lieben. Ein Gefühl kommt und kann gehen. Wie kann ich urteilen, dass es für immer bleibt, wenn meine Handlung kein Urteil und keine Entscheidung beinhaltet.” — Erich Fromm, Die Kunst des Liebens\n\nIn seinem Buch die Die Kunst des Liebes beschäftigt sich Fromm mit den Arten der Liebe und deren Herausforderungen. Zwar ist das Buch mittlerweile auch gesellschaftlich in die Jahre gekommen und einige Kapitel passen nicht mehr in den heutigen Zeitgeist, aber die Grundzüge und Prämissen sidn erstaunlich aktuell.\n\n“Liebe ist eine Aktivität und kein passiver Affekt. Sie ist etwas, das man in sich selbst entwickelt, nicht etwas, dem man verfällt.” — Erich Fromm, Die Kunst des Liebens\n\nLaut Fromm ist Liebe kein passiver Affekt, sondern eine Haltung, eine aktive Tätigkeit und Fähigkeit, die man genauso erlernen muss wie ein Handwerk. Dabei ist Lieben wichtiger als Geliebtwerden, Geben wichtiger als Empfangen. Voraussetzung für die reife und erfüllende Partnerliebe ist die Liebe zu allen Menschen und zu sich selbst.\n\n“Die infantile Liebe folgt dem Prinzip: ,Ich liebe, weil ich geliebt werde.’ Die reife Liebe folgt dem Prinzip: ,Ich werde geliebt, weil ich liebe.’ Die unreife Liebe sagt: ,Ich liebe dich, weil ich dich brauche.’ Die reife Liebe sagt: ,Ich brauche dich, weil ich dich liebe.” — Erich Fromm, Die Kunst des Liebens\n\nWeil es auch so ein wichtiges Thema ist, gibt es hierzu auch einige tolle Videos, die versuchen diesen Wirrniss aus Liebe und Pratnerschaftlichkeit einmal auseinanderzunehmen. Besuche doch einfach mal die \\(\\;\\)The School of Life - Learn, Heal, Grow\n\n“Intimacy is the capacity to be rather weird with someone - and finding that that’s ok with them.” — Alain de Botton\n\n\nAlles kalter Kaffee? Alles schon mal da gewesen? Mark Aurel (121–180) war von 161 bis 180 römischer Kaiser und als Philosoph der letzte bedeutende Vertreter der jüngeren stoischen Philosophie. Viele Ideen und Konzepte von Frankl, Adler und Fromm finden ihren Ursprung oder ihren Resonanzraum in den Ideen der Stoiker. Da es sich auch hier um eine ganze Schule handelt, kann ich nur kurz zusammenfassen, was die Kernideen der Stoiker sind. Dabei ist wichtig, das stoisch im allgmeinen Sprachgebrauch kalt oder emotionslos bedeutet, aber nicht weiter weg von der Idee der stoischen Philosophie sein kann. Wenn du dich mehr mit der stoischen Philosophie beschäftigen willst, dann gibt es einen tollen Kanal auf \\(\\;\\)Daily Stoic oder aber ein sehr schön animiertes Video mit dem Titel \\(\\;\\)Marcus Aurelius: The Man Who Solved the Universe. Schau da einfach mal rein.\nWas sind also die Kernaussagen der stoischen Philosophie? Einmal ein schneller und unvollständiger Überblick von mir.\n\nAmor fati (lat. Liebe zum Schicksal) — Es ist nicht dir passiert. Es ist für dich passiert. Das Schicksal hat es für dich ausgesucht. Akzeptiere es, nimm es an, ertrage es, mach etwas daraus.\nDie Frucht dieses Lebens ist ein guter Charakter und Taten für das Gemeinwohl.\nMemento morti. Du könntest jetzt aus dem Leben scheiden. Das Leben ist kurz.\nWir leiden mehr in der Phantasie als in der Wirklichkeit.\nWir alle lieben uns selbst mehr als andere Menschen, kümmern uns aber mehr um deren Meinung als um unsere eigene.\nDu hast immer das Recht, keine Meinung zu einem bestimmten Thema zu haben.\n\nWir sich schon aus den Kursen Leitsätzen ergibt, geht es in der stoischen Philosophie um die Kontrolle des Individuums über seine eigenen Emotionen. Es gibt viele Dinge außerhalb des eignen Wirkungsraum und nur ganz wenige Dinge im eigenen Wirkungsraum. Diese Dinge voneinander zu trennen und sich auf das eigene Tun zu konzentrieren ist ein wichtiger Punkt dabei. Dabei hat der Mensch jederzeit die Fähigkeit zu entscheiden, ob er sich mit einer Thematik beschäftigen möchte oder eben nicht. Der Mensch hat prinzipiell die Kontrolle über seine Emotionen, denn die Emotionen werden in jedem Menschen selbst erschaffen.\n\n“Ich werde dir ein Geheimnis verraten. Etwas, das sie dir in deinem Tempel nicht beibringen: die Götter sind neidisch auf uns. Sie beneiden uns, weil wir sterblich sind, weil jeder Augenblick unser letzter sein kann. Alles ist schöner, weil wir dem Tode geweiht sind. Du wirst nie schöner sein, als du jetzt bist. Wir werden nie wieder hier sein.” — Achilles, Troja\n\n\nJetzt haben wir uns verschiedene Strömungen der Wiener Schule angesehen und auch nochmal über den Rand geschaut. Dabei haben wir verschiedene Ideen über die Psyche des Menschen kennen gelernt. Je nachdem wie du jetzt selber gestrickt bist, mag dir das eine oder andere mehr zusagen. Vielleicht findest du die Ideen von Freud einleuchtend und Adlers Ideen eher schräg oder es ist genau andersherum. Wie es auch immer sei, steht natürlich jetzt die zentrale Frage im Raum: “Wer hat jetzt recht? Was ist denn nun wahr?” Hat Freud recht, dass wir alle vom ES bestimmt werden? Sind wir doch freier wie Frankl, Adler oder Fromm behaupten? Oder ist das alles schon durch seit der römischen Kaiser? Hier kommen wir dann zu Karl Popper (1902–1994). Popper begründete den kritischen Rationalismus und legte damit den Grundstein für das heutige wissenschaftliche Arbeiten.\n\n“Das Spiel der Wissenschaft ist im Prinzip ohne Ende. Wer eines Tages entscheidet, dass wissenschaftliche Aussagen keiner weiteren Prüfung bedürfen und als endgültig verifiziert gelten können, scheidet aus dem Spiel aus.” — Karl Popper\n\nDa wir niemals alles in der Wissenschaft untersuchen können, was möglich ist. Also hat Popper vorgeschlagen den Ansatz einmal zu drehen. Anstatt alle wissenschaftlichen Hypothesen zu belegen und dafür Jahrhunderte zu brauchen, stellen wir eine Theorie auf, die falsifizierbar ist. Wir sagen also, alle Schwäne sind weiß. Wir glauben an diese “Theorie” und halten an der Aussage fest, bis wir einen schwarzen Schwan finden. Wir können also mit der Theorie der weißen Schwäne weit erforschen ohne das wir jeden einzelnen Schwan anschauen müssen, bevor wir weitermachen können.\nLies gerne in dem Eassy Wie man Poppers philosophischen Knüppel in einen Blumenstrauss für die Psychoanalyse verwandelt weiter, wenn dich die Sachlage interessiert.\n\n\n3.1.6 Falsifikationsprinzip\n\n“Soweit eine wissenschaftliche Aussage über die Realität spricht, muss sie falsifizierbar sein; und soweit sie nicht falsifizierbar ist, spricht sie nicht über die Realität.” — Karl Popper\n\n\n\n\n\n\n\nGrundlagen der Wissenschaft und Falsifikationsprinzip\n\n\n\nDu findest auf YouTube Grundlagen der Wissenschaft und Falsifikationsprinzip als Video Reihe.\n\n\nIm vorherigen Abschnitt sind wir ja nochmal kurz auf Karl Popper eingegangen. Deshalb hier nochmal kurz in anderen Worten zusammengefasst, wenn du keine Lust hast nochmal woanders quer zu lesen. Karl Popper ging davon aus, dass die Annäherung an die Wahrheit für Menschen das höchste Ziel sei, jedoch eine endgültige Bestätigung oder Verifikation unmöglich sei, da nicht alle möglichen Fälle untersucht werden können. Wir können also nicht alle Experimente in endlicher Zeit durchführen bevor wir eine Theorie bewiesen und damit verifiziert haben.\nEinzig das Ergebnis einer Falsifikation könne als vorläufig gesichert gelten. Auf dieser Grundlage leitete Popper eine strenge, aber auch tolerante Vorgabe für neue und ungewöhnliche Theorien ab: Jede Idee könne als wissenschaftliche Theorie betrachtet werden – unter der Bedingung, dass sie durch Falsifikation überprüfbar ist! Dieser Ansatz sollte sicherstellen, dass man entweder mit widerlegten oder noch nicht ausreichend abgesicherten Theorien arbeitet. Da wissenschaftlicher Fortschritt oft auf bereits Vorhandenem aufbaut, erlaubte Popper jedoch vorübergehend, Theorien zu akzeptieren, die bereits mehreren Falsifikationsversuchen standgehalten hatten – natürlich nur so lange, bis sie schlussendlich widerlegt werden würden.\nNehmen wir einmal als Beispiel die Evolutionsthoerie nach Darwin. Wir können die Evolutionstheorie nicht verifizieren. Wir müssen für jedes Wesen auf dem Planeten nachweisen, dass es den Regeln und Gesetzen der Evolution nach Darwin unterliegt. Das ist purer Wahnsinn und nicht machbar. Besonders wenn wir noch vergangene und zukünftige Arten mit einschließen. Deshalb sehen wir die Evolutionstheorie als wahr an, bis wir Teile der Theorie falsifizieren können. Meistens können wir immer nur Teile einer Theorie falsifizieren und dann müssen wir schauen, ob der Rest noch standhält oder aber unsere ganze schöne Theorie zusammenbricht.\n\n“Die Phänomene der Natur, insbesondere diejenigen, die in den Bereich des Astronomen fallen, sind nicht nur mit der üblichen Aufmerksamkeit für die Tatsachen, wie sie auftreten, sondern mit dem Auge der Vernunft und der Erfahrung zu betrachten.” — William Herschel\n\nFangen wir einmal mit einem nicht so sehr bekannten Beispiel an. William Herschel (1738–1822) entdeckte den Planeten Uranus. Er war ein begnadeter Astronom der zu seiner Zeit eine Vielzahl an wichtigen Entdeckungen für den Bau immer besserer Teleskope erforscht hat. Dabei musste er einen Großteil einer Apparaturen selber bauen und zeigte große Experimentierfreudigkeit sowie Ingenieurskunst. Bei einer Sache, da verrannte sich Herschel aber zeitlebens und sein Zweig der Theorie über die Sonne sollte nicht bis heute überleben. Dabei war seine Idee den Griechen entlehnt. Planeten bedeute auf griechisch einfach nur Wanderer. Und sieben Objekte wanderten für die Griechen vor 2000 Jahren über das Himmelszelt: Mond, Sonne, Merkur, Venus, Saturn, Jupiter und Mars. Insgesamt sind es sieben “Planeten”, ein Planet für jeden Tag.\nSomit hinterfragte auch Herschel nicht, ob es eventuell einen Unterschied zwischen den “Planeten” geben könnte. Herschel glaubte an bewohnbare Planeten und Trabanten. Er beobachtete den Mond und erkannte in den Strukturen der Oberfläche Metropolen, ganze Städte und Dörfer. Straßen und Kanäle verbanden die Orte miteinander. Am Ende waren die Beschreibungen der bevölkerten Oberfläche des Mondes nicht von der englischen Landschaft (eng. english countryside) zu unterscheiden. Für Herschel war die englische Landschaft sein Modell was er in den Strukturen auf dem Mond wiederfand. Dieses Modell seiner Wirklichkeit wendete Herschel auch auf andere Trabanten und Planeten an und fand auch dort die bewohnten Oberflächen wieder.\nSo war es in seiner Logik auch vollkommen klar, dass es sich bei der Sonne um einen bewohnbaren Planeten handeln musste. Herschel stellten folgendes Gedankenmodell auf. Er betrachte die Erde vom Mond aus und schaute durch die Wolkenlücken auf die Erde herab. Das gleiche Modell konnte er nun auf die Erde und die Sonne anwenden. Die Sonne war damit ein mit Wolken umhüllter Planet. Die wenigen Wolkenlücken oder auch schwarzen Sonnenflecken erlaubten einen Blick auf die solide Oberfläche der Sonne. Daher war er Planet Sonne ein dunkler Planet mit einer leuchtenden Wolkenschicht. Die Abbildung 3.11 (a) zeigt hier nochmal den Zusammenhang. Damit die Bewohner der Sonne nicht von der leuchtenden Hülle gegrillt wurden, gab es eine schützende Wolkenschicht laut Herschel. Er entwickelt sein Sonnenmodell immer weiter wie in Abbildung 3.11 (b) zu sehen. Hier führte Herschel schon zwei Schutzschichten für die Bewohner der Sonne ein.\n\n\n\n\n\n\n\n\n\n\n\n(a) Quelle: Kawaler und Veverka (1981)\n\n\n\n\n\n\n\n\n\n\n\n(b) Quelle: Basalla (2006)\n\n\n\n\n\n\n\nAbbildung 3.11— Theorien von Herschel über den Aufbau der Sonne und deren möglichen Bewohner. Herschel prägte den Begriff der bewohnbaren Sonne (eng. habitable sun). Die Sonnenbewohner (eng. solar dwellers) sind dabei durch zwei Isolationsschichten von der leuchtenden Hülle (eng. luminous shell geschützt.)\n\n\n\nAm Ende waren es dann die Entwicklungen in der Spektrographie und andere Beobachtungen, die es Mitte des 19. Jahrhunderts nach Herschels Tod erlaubten die richtigen Schlüsse über die Sonne zu treffen. Die Sonne ist ein riesiger Ball aus heißem Gas. Spannenderweise sollte es noch bis in die 2000 Jahre dauern bis über Simulationsstudien über Computermodelle auf leistungsstarken Rechnern die Eigenschaften der Sonnenflecken durch die Umwälzungen im Inneren der Sonne und deren Magnetfeld erklärt werden konnte.\n\n“Schicken Sie mir einige von Ihren sogenannten Cholerabazillen, und ich will ihnen beweisen, wie harmlos sie sind!” — Max von Petterkofer in einem Brief an Robert Koch\n\nAls weiteres Beispiel nehmen wir den Begründer der Epidemiologie Max von Petterkofer (1818–1901). Max von Pettenkofer war ein bedeutender deutscher Hygieniker und erster deutscher Ordinarius für Hygiene. Er ist der Begründer der naturwissenschaftlich-experimentellen Hygiene und damit ein Wegbereiter für unsere sauberen Städte und Kanalisationen sowie Frischwasserversorgung. Seine ausgiebige statistische Erfassung und Auswertung des Seuchengeschehen ist noch heute in Grundzügen eine Säule der Epidemiologie.\n\n\n\n\n\n\nAbbildung 3.12— “What is this ‘perfume’ you are selling?” “Vinegar in a bottle. It’s the only thing around here that doesn’t completely smell like poo …” Quelle: wumo.com\n\n\n\nZur seiner Zeit wütete die Tuberkulose in der Welt. Um 1880 war in den Städten in der Altersgruppe der 15- bis 40-Jährigen jeder zweite Todesfall in Deutschland auf diese Krankheit zurückzuführen. Auch in ländlichen Gegenden stellte die Tuberkulose die häufigste Todesursache dar. Nun war es die Frage, was hilft gegen die Tuberkolose? Pettenkofer fand durch seine Experimente heraus, dass Zugang zu sauberen Wasser und strikte EInhaltung von Hygieneregeln die Ausbreitung der Tuberkulose stoppen konnte. Er war sorgte somit erst in München für ein weitreichende Verbesserung der hygienischen Alltagslebensbedindungen der Menschen und dann auch in anderen Teilen des damaligen deutschen Reiches. Nur eine Frage blieb dabei noch offen, was löste die Tuberkulose aus? Zwar konnte Pettenkofer alle Symptome der Tuberkulose systematisch abstellen, aber was ursächlich für den Ausbruch war, blieb ihm verborgen.\nPettenkofer war ein Positivist, das heißt, er erkannte ausschließlich tatsächliche, sinnlich wahrnehmbaren und überprüfbaren Befunden an. Der Positivismus hat in seiner philosophischen Form zur Erkenntnisgewinnung nicht das 20. Jahrhundert überlebt. Über den Auslöser der Tuberkulose begann ein jahrelanger wissenschaftlicher Streit mit Robert Koch (1843–1910) mit der Entdeckung des Mykobakterien als Auslöser der Tuberkulose im Jahre 1882. Pettenkofer vertrat die Ansicht, die Umweltbedingungen seien von erheblich größerer Bedeutung für die Entstehung einer Krankheit als die bloße Anwesenheit von Krankheitserregern. Er nannte den Auslöser “contagiöses Element Y” und weigerte sich an jedwede Bazillen zu glauben. Die Bedeutung der Umwelt und deren Auswirkungen auf Erkrankungen sollte dann in der aufkommenden Genetik eine bedeutende Frage der Wissenschaft werden.\n\n\n\n\n\n\nAbbildung 3.13— “This is a shit. Traditionally, something we all flock to. This conference asks the question: Why? Is it enviroment or genes? Or are we all just a bunch of idots?” Quelle: wumo.com\n\n\n\n\nIm Jahre 1456 … wurde ein Komet gesehen, der zwischen der Erde und der Sonne rückläufig war … Daher wage ich die Vorhersage, dass er im Jahr 1758 wiederkehren wird.” — Edmond Halley\n\nEdmond Halley (1656–1742) entdeckte mit dem Halleyscher Kometen den wohl populärsten Kometen der Neuzeit. Halley wurde wegen seiner Verdienste um die Bahnbestimmung auch von anderen Kometen 1720 königlicher Astronom und Leiter der Sternwarte in Greenwich ernannt. Obwohl das Erscheinen von Kometen bis zu seiner Zeit als unvorhersehbar angesehen wurde, gelang es Edmond Halley im Jahr 1705, eine bemerkenswerte Entdeckung zu machen. Er erkannte, dass der Komet, den der sächsische Bauer und Astronom Christoph Arnold im Jahr 1682 als Erster beobachtet hatte, tatsächlich mit früheren Sichtungen von Kometen in den Jahren 1531 und 1607 identisch sein musste. Basierend auf dieser Erkenntnis wagte Halley eine bemerkenswerte Vorhersage: Er sagte voraus, dass dieser Komet im Jahr 1758 wiederkehren würde. Nachdem andere Forscher seine Berechnungen überprüften, erhielt der Schweifstern den Namen „Halley”.\nDas Eintreffen der Vorhersage, dass ein Komet mit denselben Bahndaten wie die Kometen von 1531, 1607 und 1682 kommen würde, war 1758 ein großer, allen Menschen sichtbarer Erfolg der Newtonschen Gravitationstheorie. Es blieb aber bei diesem einmaligen Erfolg der Newtonschen Gravitationstheorie.\nDarüber hinaus war Halley ein begeisterter Förderer von Newton und half ihm auch seine Erkenntnisse zu publizieren. Neben der finanziellen Unterstützung half er auch Newton vermutlich mathematisch aus. Aus den Berechnungen Newtons zu den Wasserbewegungen der Meere in Abhängigkeit von Mondmasse und -position konnte eine Dichte des Mondes im Vergleich zu der Erde mit einem Verhältnis von 9:5 bestimmt werden. Leider war hierbei Newton ein Rechnenfehler unterlaufen, der aber erst nach Halley’s Tod geklärt werden wollte. Wenn der Mond und die Erde aus dem gleichen Gestein bestehen würde, dann müssten die Dichten gleich sein. Die Erde war aber viel leichter als der Mond und so schloss Halley, dass die Erde zum Teil hohl sein müsse. Mit dieser Erkenntnis modellierte er ein Bild vom Erdinneren aus vier ineinander steckenden, konzentrisch angeordneten Kugeln mit Hohlräumen dazwischen wie in Abbildung 3.14 (a) zu sehen. Mit seiner Theorie der ineinander beweglichen Kugeln mit eigenen Magnetpolen, konnte Halley auch mathematisch schlüssig die beobachteten Abweichungen in dem Magnetfeld der Erde erklären.\n\n\n\n\n\n\n\n\n\n\n\n(a) Hohlerde nach Halley. Quelle: wikepedia.org\n\n\n\n\n\n\n\n\n\n\n\n(b) Halley als Astronomer Royal. Quelle: wikepedia.org\n\n\n\n\n\n\n\nAbbildung 3.14— Darstellung des Aufbaus der Erde als ein System von Hohlkugeln nach Edmund Halley.\n\n\n\nBis hierhin wurde die Entwicklung seines Modells der konzentrischen Sphären innerhalb der hohlen Erde sowohl mathematisch als auch auf Grundlage von Naturgesetzen vorangetrieben. Alles basierte auf nachvollziehbaren Beobachtungen sowie den damals als plausibel erachteten Annahmen. In Abbildung 3.14 (b) ist zu sehen wie überzeugt Halley von seiner Hohlerdentheorie war. Er lies sich zu seiner Ernennung als Astronomer Royal mit der Hohlerde abbilden.\nNun tat Halley aber etwas, was sich noch als sehr nachteilig für die weitere Geschichte der Hohlerdentheorie herausstellen sollte. Er war ein Zeitgenosse von Herschel und zu der damaligen Zeit glaubte man an die Bewohnbarkeit von Planeten und allem anderen. So war es auch für Halley in sich schlüssig, dass die Hohlerde bevölkert war. Die Beleuchtung kam von den fluoreszierenden Unterseiten der jeweiligen Kugel. Als Beweis seiner Theorie sah er die 1716 in England und weiten Teilen Europas beobachteten sehr lichtstarke Polarlichter. Halley erklärte die Polarlichter damit, dass das Licht aus den Hohlräumen durchscheine, weil die Erdkruste in nördlichen Breiten dünner sei.\n\n“Konsequenz heißt, auch einen Holzweg zu Ende zu gehen!” — anonym\n\n\n\n3.1.7 Cargo Cult Science\n\n“The first principle is that you must not fool yourself and you are the easiest person to fool.” — Richard P. Feynman\n\nIn uns sind bestimmte Vorstellungen und Ideen integriert. Häufig hinterfragen wir diese Ideen und Konzepte auch nicht. Eins der stärksten Konzepte ist sicherlich die Kultur. Je nachdem aus welcher Kultur du stammst, hast du Vorstellungen “wie etwas zu laufen hat”. Du kannst dich sicher durch die Regeln eines Landes bewegen und merkst, dass du dich zu Hause fühlst, wenn deine Prinzipien und Ideen nicht “anecken”. Was sind aber diese Ideen und Prinzipien? Wir können vereinfacht von Memen sprechen. Meme sind wie Gene, nur das Meme für Gedanken und Konzepte stehen. Dabei ist sicherlich Kultur eine der komplexestens Ausprägungen hunderter von miteinander vernetzter Meme. Als Menschen lernen wir die Meme und die Meme werden uns über unsere Eltern und der Gesellschaft in der wir aufwachsen vererbt.\nDabei ist die Übertragung eines Mems durch Kommunikation nicht als Kopie oder Blaupause eines Gedankens von Gehirn zu Gehirn zu verstehen. Es wird der wesentliche Kern der Botschaft erfasst und weitergegeben wird. Ein Mem ist eher ein “Backrezept” zur Reproduktion desselben Gedankens. Und wie jedes gute Backrezept wird es von jedem Backenden etwas modifiziert. Wir handeln also nach inneren Prinzipien, die wir uns vermutlich nicht immer bewusst sind. Hat hier Freud dann doch recht? Kommen wir zum Kern des Abschnitts, wie betreiben wir eigentlich Forschung? Müssen wir uns da Gedanken drüber machen oder können wir einfach mal so machen? Hier wollen wir einmal auf die Gedanken von dem Physiker und Nobelpreisträger Richard Feynman (1918–1988) näher eingehen.\nFeynman (1998) sagt, dass der Erste den du selber reinlegst immer du selber bist. Du hast die Daten mit sehr viel Mühe erhoben und willst am Ende ja auch was rausbekommen. Aber so funktioniert das nicht. Wir müssen unserer eigener Advocatus Diaboli sein und uns immer kritisch hinterfragen. In seiner Rede am California Institute of Technology geht er auf verschiedene Strömungen in der Wissenschaft ein und beschreibt Wissenschaften, die keine Wissenschaften sind. Er nennt diese Pseudo-Wissenschaften Cargo Cult Science. Du betreibst Cargo Cult, wenn du eine Handlung formal korrekt durchführst, aber der Kontext keinen Sinn ergibt. Klingt jetzt erstmal kryptisch, aber viele Handlungen entspringen mehr einem Meme als einer rationalen Ursache. Schaue dir einmal Professor Zapinsky in der Abbildung 3.15 an. Professor Zapinsky führt seine Studie aus seiner Sicht absolut korrekt aus. Beide Behandlungen sind den gleichen Umweltbedingungen ausgesetzt, so dass er hier von einer Überlegenheit der Krake gegenüber der Katze ausgehen kann.\n\n\n\n\n\n\nAbbildung 3.15— “Professor Zapinsky proved thate the squid is more intelligent than the housecat when posed with puzzles under similar condistions.” Quelle: wumo.com\n\n\n\nSchauen wir uns jetzt einmal Cargo Cult selber an. Was war den der Cargo Cult? Der Cargo Cult entstand grob im Pazifik als die Amerikaner im zweiten Weltkrieg begannen die sorten Inseln für mit Gütern des täglichen Bedarfs (eng. cargo) auszustatten. Da die Amerikaner nicht wussten welchen Inselpfad sie nach Japan wählen würden, schmissen sie auch zufällig Güter auf Inseln ab. Der dortigen indigenen Bevölkerung kamen die Amerikaner wie Wesen aus einer anderen Welt vor. Oder um es in den Worten von Arthur Clarke mit folgendem Zitat zu beschrieben.\n\n“Jede hinreichend fortschrittliche Technologie ist von Magie nicht zu unterscheiden.” — Arthur C. Clarke, 3. Gesetz\n\nIn der Abbildung 3.16 (a) siehst du verschiedenen Cargo Cult Gegenstände der Bewohner Neuguineas, Neukaledoniens und der Salomonen. Alle Gegenstände sind perfekt nachgebildet. Die Handlungen sind alle formal korrekt ausgeführt, aber warum kein Flugzeug landet, bleibt den Bewohnern ein Rätsel. So ist es auch in anderen Bereichen der Forschung. Wir führen Dinge aus, verstehen die Dinge nicht und wundern uns das wir nichts finden. Auf der anderen Seite hinterfragen wir aber nicht die Resultate, wenn die Ergebnisse in den Bereich unserer Erwartungen fallen. Feyman nennt als Beispiel den berühmten Versuch von Millikan zur Ermittlung der Elementarladung des Elektrons. Millikan nahm einen falschen Parameter an und ermittelte so eine zu niedrige Ladung. Trotzdem dauerte es noch Jahrzehnte den richtigen Wert zu ermitteln, da man eher an sich zweifelte als an den Nobelpreisträger Millikan.\n\n\n\n\n\n\n\n\n\n\n\n(a) Bewohner Neuguineas, Neukaledoniens, der Salomonen. Quelle: tellmeboutblog.wordpress.com\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Bilder des Tages. Quelle: der-postillon.com\n\n\n\n\n\n\n\n\n\n\n\n(c) Birds Arent’t Real Kampagne. Quelle: nytimes.com\n\n\n\n\n\n\n\nAbbildung 3.16— Verschiedene Beispiele für Fake News im Spiegel des Cargo Cults. Durch moderne KI-gestützte Bildbearbeitung sind eigene Realitäten möglich, die es so nicht gibt.\n\n\n\nCargo Cult in der Wissenschaft ist ein Problem, was viele nicht bewusst vor Augen haben. Was wir jetzt aber immer mehr sehen, ist Fake News im Sinne der KI generierten Bilder. In Abbildung 3.16 (b) siehst du ein Beispiel für satirischen Auseinandersetzung mit der Reptilian conspiracy theory. Hier wird die Idee auf Korn genommen, dass die Menschheit von einer Gruppe Reptiloiden beherrscht wird. In Verschwörungstheorien wird jegliche Wissenschaftlichkeit über Bord geworfen und Ideen sowie Konzepte so zusammengerückt, dass es zu der Geschichte passt. Die Gruppe um Peter McIndoe in Abbildung 3.16 (c) versucht mit der Verschwörungstheorie Birds aren`t real einen Kontrast zu den anderen Verschwörungstheorien zu liefern und die Mechanismen satirisch zu hinterfragen. Offen muss bleiben, ob ihnen das so gelingt oder aber nur eine weitere Verschwörungstheorie erschafft.\nLeider bleiben wir auch in der Klimakrise nicht von Verschwörungstheorien verschont. Wenn dich mehr zum Thema “Verschwörungstheorien in der Klimakrise” interessiert, über das Warum und Wie, dann besuche doch Webseite der Bundeszentrale für politische Bildung und dort schaue in die Ausgabe Aufgeheizt - Verschwörungserzählungen rund um die Klimakrise. Du findest dort auch das kostenlose PDF zum Download.\n\n\n\n\n\n\nZum Nachdenken V: Hawking und die Aliens\n\n\n\n\n\nFolgend Zitatzusammenstellung stammt von Steven Hawking, der sich zu den Aussichten eines Kontaktes mit einer außerirdischen Zivilisation geäußert hat.\n\n“Wenn uns Außerirdische jemals besuchen, wird der Ausgang, so denke ich, genauso sein wie die Landung von Christopher Columbus in Amerika, was für die Eingeborenen nicht sehr gut ausging. […] Wir müssen nur auf uns selbst schauen, um zu sehen, wie sich aus intelligentem Leben etwas entwickelt, dem wir lieber nicht begegnen möchten. […]Fortschrittliche Aliens werden wohl ein nomadenhaftes Leben führen und versuchen, alle Planeten zu erobern und zu kolonisieren, die sie finden können.” — Stephen Hawking\n\nVerfällt hier Hawking des gleichen Trugschlüssen wie Herschel? Sind wir eventuell eingschüchtert wie beim Millikanexperiment? Ist der Gedankengang sauber oder gibt es vielleicht Risse in der Logik von Hawking?\n\n\n\n\n\n3.1.8 Anthropozän\nWir stehen an der Schwelle zum Anthropozän dem Zeitalter des Menschen. Jedenfalls behaupten dies einige Wissenschaftler. Die Menschheit ist zu einem geologischen Faktor geworden und somit endet das Holozän, die vor 11.700 Jahren begann. Jetzt gibt es in geologischen wissenschaftlichen Kreisen die Frage, ob nun das Holozän wirklich beendet ist und somit das Anthropozän formal in der Geologischen Zeittafel eingeführt werden kann.\nDie Blaualgen hat die Bändererze erschaffen und somit einen Marker in der Geologie hinterlassen. Jahrtausende später hat der Meteoriteneinschlag die K-P-Grenze erschaffen und den Beginn des Aussterben der Dinosaurier markiert. Nun ist die Frage, gibt es eine solche Grenze auch für die Menschheit im Gestein? Viele Jahre wurde diskutiert welche Grenze wir für die den Beginn des Anthropozän nehmen sollen. Es muss eine Grenze sein, die wir global in den Gesteinschichten finden.\nIm Crawfordsee in Ontario, Kanada, entstehen während warmer Sommermonate Kalzitminerale, die alljährlich herabrieseln und eine deutlich sichtbare weiße Schicht bilden – vergleichbar den Jahresringen eines Baumes. Doch eine bemerkenswerte Linie hebt sich von den anderen ab: Sie markiert die frühen 1950er-Jahre und wird durch das Isotop Plutonium-239 als nachweisbare globale Signatur von Kernwaffentests definiert. Hier stellt sich die Frage, ob der Crawfordsee möglicherweise als eine “Global Boundary Stratotype Section and Point” dienen könnte, um den Übergang in dieses neue geologische Zeitalter zu veranschaulichen.\nEin andrer potenzieller geologischer Marker könnte das im Jahr 2006 entdeckte Plastiglomerat sein. Plastiglomerat ist ein Gestein, das aus einer Mischung aus Sediment und anderen natürlichen Ablagerungen besteht, die durch Kunststoff zusammengehalten werden. Dabei kann es sehr hart und große Stücke bilden. Das Plastiglomerat wird ebenfalls als Marker für den Beginn der Industrialisierung vorgeschlagen. Plastiglomerate könnten so einen Markierungshorizont für die Verschmutzung durch den Menschen in den geologischen Aufzeichnungen bilden und als künftige Fossilien bestehen.\nHier siehst du einmal einen sehr aktuellen Forschungsprozess aus der Geologie, der gerade diskutiert wird. Eigentlich ist es ja für den Alltag egal, in welcher Zeit wir leben. Ob wir nun in dem Holozän oder dem Anthropozän leben ändert ja eigentlich nichts. Auf der anderen Seite mag es doch einen psychologischen Unterschied ausmachen, dass die Menschheit dem Planeten seinen Fußabdruck verewigt hat.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Forschungsprozess</span>"
    ]
  },
  {
    "objectID": "forschungsprozess.html#zurück-in-die-zukunft",
    "href": "forschungsprozess.html#zurück-in-die-zukunft",
    "title": "3  Forschungsprozess",
    "section": "3.2 Zurück in die Zukunft",
    "text": "3.2 Zurück in die Zukunft\n\n“So. Zeitleitung einschalten. Flux Kompensator - fluxuriert. Maschine läuft. Es kann los gehen.” — George McFly in Zurück in die Zukunft\n\nWo wollen wir jetzt eigentlich hin, nachdem wir uns so viele historische Forschungsprozesse angeschaut haben? Was ist denn jetzt der ganz große Bogen der alles überspannt? Die Lehre soll sich ja immer in die Zukunft (eng. future) richten und Wissen von einer Generation zu der nächsten Transferieren. Wir haben ja immer nur eine begrenzte Zeit zu Verfügung in der ich dir Fähigkeiten (eng. skills) vermitteln kann. Nun ist die Frage, was soll ich eigentlich unterrichten und was wird in der Zukunft den nun gebraucht? Beginnen wir erstmal mit der klassischen 7G-Lehre, die du bis jetzt immer erlebt hast.\n\n3.2.1 Klassischer 7G-Unterricht\n\n\n\n\n\n\nAbbildung 3.17— “After 40 years as a teacher, Gary McAdams finally manages to bore a student to death.” Quelle: wumo.com\n\n\n\nWas ist eigentliche die klassische Lehr- und Lernerfahrung aus der Schule? Oder andersherum, was ist Wasser im Bezug auf deine Schulerfahrung? Denn aus der Schule kommst du an die Hochschule und bringst für dich die Norm für deine Lehr- und Lernerfahrung mit. Deshalb hier einmal etwas mit Abstand auseinander genommen was klassischer Unterricht ist und warum dieser klassische Unterricht nach Helmke (2013) auch 7G-Unterricht genannt wird. Schule ist nicht so wie Schule ist, weil Schule so passiert, sondern weil ganz viele Akteure Schule so wollen.\n\n\n\n\n\n\n\ngleiches Alter\ngleicher Ort\ngleiche Zeit\ngleiche Art und Weise\ngleiches Lehrperson\ngleiche Inhalte\ngleiche Lernziele\n\n\n\nIm klassischen 7G-Unterricht bearbeiten gleichaltrige Schüler:innen im gleichen Raum zur gleichen Zeit auf die gleiche Art und Weise mit der gleichen Lehrperson die gleichen Inhalte – und orientieren sich dabei an den gleichen Zielen. Wir sehen also, es ist ein starker Fokus auf das Gleiche für jeden. Die individuellen Lernansprüche der Schüler:innen werden somit höchstens zufällig vom klassischen Unterricht erfüllt.\n\n\n\nDarüber hinaus haben wir in der Schule einen Fokus auf das Ergebnis und nicht den Prozess. Du bist darauf trainiert, bei einem ersten Versuch besser als der Durchschnitt zu sein und zu vergessen, was du getan hast, sobald du deine Klausur geschrieben hast. Es geht also primär um die Note am Ende und um möglichst effizient eien gute Note zu erreichen. César A. Hidalgo schrieb zum diesem antrainierten Verhalten in der Schule und dem Spannungsfeld der späteren Arbeit einen längeren Faden auf Twitter mit folgenden Eingangstweet.\n\nNachdem ich über ein Jahrzehnt lang Doktoranden und Masterstudierende beraten habe, stelle ich fest, dass die meisten Studierenden eine Sache erst einmal ablegen müssen: die Mentalität der Halbherzigkeit, die sie sich durch jahrelange Tests und Hausaufgaben angeeignet haben.\n\nWas ist also Wasser für dich in deiner Schulerfahrung? Hilft diese Erfahrung eigentlich zukünftige Herausforderungen zu meistern? Vor allem ändert sich jetzt vieles im Bezug auf die Hochschule. Am Anfang des Studiums ähnelt noch vieles der Schule oder dir kommt es eher noch so vor. Im Verlauf des Studiums erfährtst du dann eine immer stärker werdende Individualisierung. Du musst dich um viel mehr selber kümmern und auch auch die Lehr- und Lernformate ändern sich. Warum das so ist, wollen wir im nächsten Abschnitt einmal anschauen. Der Fokus geht nämlich weg vom 7G-Unterreicht und hin zu den Future Skills (eng. Zukunfstfähigkeiten).\n\n\n3.2.2 Future Skills\n\n“Prognosen sind schwierig, vor allem, wenn sie die Zukunft betreffen.” — Mark Twain\n\nNachdem wir uns etwas mit der Vergangenheit deiner Schulerfahrung beschäftigt haben wollen wir nun einen Blick in die Zukunft wagen. Wir fragen uns daher, welche Fähigkeiten brauchen wir um mit zukünftigen Herausforderungen an uns zu meistern? Was sind den nun die zukünftig notwendigen Fähigkeiten (eng. Future Skills)? Hierzu nutzen wir als die direkte Quelle den Stifterverband-Initiative “Future Skills”. Dort finden wir dann auch die Future Skills: Die Skills im Überblick. Grundsätzlich ist die Idee nicht so neu, auch schon das humboldtsche Bildungsideal spricht davon, soviel Welt wie möglich in sich aufzunehmen. Die Future Skills sind aber strukturierter und erlauben ein mehr gerichtetes Lehren und Lernen.\n\n\n\n\n\n\nAbbildung 3.18\n\n\n\nDie Future Skills teilen sich in vier Kompetenzen auf, die wiederum aus verschiedenen Inhalten bestehen. Sicherlich ist es nicht möglich alle Bereiche abzudecken oder ein tiefes Sachverständis zu erlangen. Dennoch sind das die Themenfelder, die zukünftig eine Rollen spielen werden. Und das Leben ist lang und lebenslanges Lernen bedeutet ja auch nichts anderes als immer wieder neue Fähigkeiten zu erlangen. Zum einen müssen wir zwischen Spezialisten für den Umgang mit transformativer Technologien und Schlüsselkompetenzen im Allgmeinen für alle Arbeitsbereiche. In der Abbildung 3.19 wird dieser Zusammenhang nochmal dargestellt. Auch wenn du kein Spezialist für transformative Technologien sein willst, so solltest du dich in den Fachbegriffen auskennen, da du sicherlich im Team mit diesen Spezialisten arbeiten wirst. Im Bereich der zukünftigen Schlüsselkompetenzen machen die klassischen Kompetenzen nur noch einen kleinen Teil aus.\n\n\n\n\n\n\nAbbildung 3.19— Darstellung der vier Kategorien der Future Skills und deren Verbindung untereinander sowie deren zukünfige Gewichtung\n\n\n\nBetrachten wir die einzelnen Kompetenzen und deren Inhalte in Tabelle 3.2 nochmal detaillierter. Dabei ist wichtig zu bachten, dass wir die vier Kompetenzfelder i) technologische Kompetenz, ii) digitale Schlüsselkompetenz, iii) klassische Kompetenz und iv) transformative Kompetenz vorliegen haben. In jeder dieser Kompetenz haben wir noch weitgefasste Inhaltsfelder. Besonders im Bereich der technologischen Kompetenzen lässt sich ein umfangreiches Wissen kaum erreichen. Hier sind wichtige zukünftige Inhaltsfelder benannt, die sich teilweise überschneiden. Die digitalen Schlüsselkompetenzen werden im Rahmen der Hochschullehre teilweise vermittelt. Die klassischen Kompetenzen sowie die die transformativen Kompetenzen gehören zum weiten Bereich der Persönlichkeitsentwicklung und sollen hier auch als Leitfaden und Orientierungshilfe dienen.\n\n\n\nTabelle 3.2— Die vier Kompetenzen der Future Skills zusammen mit den jeweiligen Inhalten.\n\n\n\n\n\n\n\n\n\nKompetenzen\nInhalte\n\n\n\n\nTechnologische Kompetenzen\nData Analytics & KI, Softwareentwicklung, Nutzerzentriertes Design, IT-Architektur, Hardware/Robotikentwicklung, Quantencomputing\n\n\nDigitale Schlüsselkompetenzen\nDigital Literacy, Digital Ethics, Digitale Kollaboration, Digital Learning, Agiles Arbeiten\n\n\nKlassische Kompetenzen\nLösungsfähigkeit, Kreativität, Unternehmerisches Handeln & Eigeninitiative, Interkulturelle Kommunikation, Resilienz\n\n\nTransformative Kompetenzen\nUrteilsfähigkeit, Innovationskompetenz, Missionsorientierung, Veränderungskompetenz, Dialog- und Konfliktfähigkeit\n\n\n\n\n\n\nWir immer im Leben gilt, dass niemand weiß, was in der Zukunft gebraucht wird und wirklich nützlich ist. Die Future Skills bieten aber einen guten Leitfaden zur Orientierung. Nicht alles muss man erfüllen um für die Zukunft gewappnet zu sein, wie wir dann noch später lernen werden. Die Idee zu definieren welche Fähigkeiten der Mensch für die Zukunft braucht ist nämlich auch schon sehr alt und wurde schon von vielen Philosophen durchdacht.\n\n\n\n\n\n\nZum Nachdenken VI: Anekdote zur Senkung der Arbeitsmoral\n\n\n\n\n\n\n“And you can get me work; But I can’t work for free; I’ve got a room downtown; With a bed and a big TV” — White Lies, Big TV\n\nHeinrich Böll schrieb zum Tage der Arbeit am 1. Mai 1963 die Anekdote zur Senkung der Arbeitsmoral. Vermutlich kennst du schon die Geschichte an deren Ende der Fischer nichts gewinnen würde, wenn er sehr viel Arbeit und sich nicht der Faulenzerei hingibt, denn er hätte dann nicht mehr als er jetzt sowieso schön hätte. Auch sehr spannend zu lesen ist das Buch von Price (2021) mit dem Titel Laziness does not exist. Ich fand einige der Gedanken sehr erhellend. Manachmal ist weniger dann auch mehr. Vor allem, wenn das mehr dann mehr von dem eigenen Leben beinhalten, was einen selber auch ausmacht.\nDie Parabel hat unterschiedliche Rezeptionen erfahren. So diskutiert die Karrierebibel in dem Artikel Warum die Parabel über den Fischer und Touristen gefährlich ist, ob die Parabel mehr nutzt als schadet. Oder ist es nicht eher so, dass das Verhalten des Touristen erst zur Tragödie des Dorsches in der Ostsee geführt hat? Wenn alle Fischer sich so Verhalten wie der Tourist vorschlägt, dann wird die Sache schon problematisch. Auch ist es ganz spannend mal zu hören, was so Tech-Milliardäre so denken. Wo sollen sie den die Bunker bauen? Höre dazu den Deutschlandfunk Im Angesicht der Katastrophe – Die Angst der Tech-Milliardäre vor ihrem Personal.\nAm Ende vielleicht hier noch die Antwort von Platon (428/427–348/347 v. Chr) auf die Frage, was Platon am Menschen im Allgemeinen am meisten überrascht.\n\n“Sie langweilen sich in der Kindheit und beeilen sich, erwachsen zu werden, aber dann vermissen sie ihre Kindheit. Sie verlieren ihre Gesundheit, um Geld zu verdienen, aber sie zahlen Geld, um ihre Gesundheit wiederzuerlangen. Sie sorgen sich um das Morgen und vergessen das Heute. Am Ende leben sie weder heute noch morgen. Sie leben, als ob sie nie sterben würden, aber sie sterben, als ob sie nie gelebt hätten.” — Platon\n\n\n\n\n\n\n3.2.3 Und jetzt? Was solltest du tun?\n\n“Wer etwas Großes will, der muß sich, wie Goethe sagt, zu beschränken wissen. Wer dagegen alles will, der will in der Tat nichts und bringt es zu nichts. Es gibt eine Menge interessante Dinge in der Welt; spanische Poesie, Chemie, Politik, Musik, d. ist alles sehr interessant, und man kann es keinem übel nehmen, der sich dafür interessiert; um aber als ein Individuum in einer bestimmten Lage etwas zustande zu bringen, muß man sich an etwas Bestimmtes halten und seine Kraft nicht nach vielen Seite hin zersplittern.” — Georg Hegel, deutscher Philosoph\n\nNun sind wir am Ende dieses sehr langen Kapitels angekommen. Ich habe hier lange dran geschrieben und du vermutlich auch etwas länger dran gelesen. Aber wie kommen wir jetzt zu einem Forschungsergebnis? Indem wir das Ergebnis vergessen und uns auf den Prozess konzentrieren. Dieses Kapitel diente dazu dir zu zeigen, dass es viele Ideen in der Wissenschaft gibt und das es eben ein anderer Denkprozess ist zu einem wissenschaftlichen Ergebnis zu kommen. Auf dem Weg zu einer neuen Theorie liegen viele Fallstricke und schnell hast du dich verrannt. Aber das ist kein Problem, denn verrannt haben sich schon viele großartige Wissenschaftler. Die Pflicht des Wissenschaftlers ist es seine Ergebnisse zu berichten, so dass andere deine Ergebnisse falsifizieren können.\n\n\nTeile dieses Kapitels basieren auch auf meiner lehrdiaktischen Forschung aus Kruppa u. a. (2021).\nGut, dass ist jetzt alles sehr groß und weiträumig. Wie fangen wir denn jetzt mal konkret an? Du hast jetzt ein Problem oder stehst vor der Herausforderung zu forschen. Also überhaupt den Prozess des Forschens zu beginnen. Hier möchte ich dir die Feynman Methode ans Herz legen, die dir erlaubt relativ effizient ein Themenfeld für dich abzuarbeiten. Der Fokus der Feynman Methode liegt dabei auf dem Verstehen und nicht auf dem Auswendig lernen.\nSchauen wir uns also mal die vier Schritte der Feynman Methode genauer an.\n\n\nBesuche gerne noch die Tutorien How to Use the Feynman Technique to Learn Faster und The Feynman Technique: Master the Art of Learning. Beide liefern nochmal sehr viel mehr Informationen, als ich es hier tun aknn.\n\nSchritt 1: Beginne damit, ein Blatt Papier vor dir zu nehmen und den Namen des Konzepts am oberen Rand zu notieren. Denke daran, dass du jedes erdenkliche Konzept oder jede Idee verwenden kannst. Die Anwendung der Feynman-Methode ist nicht ausschließlich auf die Naturwissenschaften beschränkt.\nSchritt 2: Jetzt geht es darum, das gewählte Konzept in eigenen Worten zu erklären, als würdest du es jemand anderem beibringen. Versetze dich in die Rolle eines Lehrers und verwende eine klare und einfache Sprache. Gehe über eine einfache Definition oder einen groben Überblick hinaus. Fordere dich selbst heraus, indem du ein oder zwei Beispiele durchdenkst, um sicherzustellen, dass du das Konzept wirklich verstanden hast.\nSchritt 3: Überprüfe deine Erklärung kritisch und identifiziere die Bereiche, in denen du unsicher bist oder deine Erklärung schwankt. Sobald du diese Stellen gefunden hast, gehe zurück zu den Quellen, Notizen oder Beispielen, um dein Verständnis zu vertiefen.\nSchritt 4: Solltest du in deiner Erklärung Fachbegriffe oder komplizierte Sprache verwenden, schreibe diese Passagen in einfacheren Worten neu. Stelle sicher, dass deine Erklärung von jemandem verstanden werden kann, der nicht über das gleiche Wissen verfügt wie du jetzt. Dies ist ein entscheidender Schritt, um sicherzustellen, dass du das Konzept wirklich durchdrungen hast.\n\nIm Folgenden siehst du in der Abbildung 3.20 einmal zwei Beispiele für meinen Umgang mit neuen Ideen und Lehrstoff. Ich zeichne mir da auch immer wild Sachen in mein Buch und versuche mich darüber zu strukturieren.\n\n\n\n\n\n\n\n\n\n\n\n(a) Beispiel für die Erstellung des Skripts Bioinformatik. Erste Ideen für die Zeichnung der Übersicht.\n\n\n\n\n\n\n\n\n\n\n\n(b) Beispiel für die Erstellung der experimentellen Designs Kapitels.\n\n\n\n\n\n\n\nAbbildung 3.20— Anwendung der Feynman-Methode in meinen eigenen Alltag der Lehre. Einmal der Versuch die Genetik für das zukünftige Skript Bioinformatik zu sortieren und einmal Ideen sowie Gedanken zum experimentellen Design Kapitel. Erkennt man im Endprodukt dann nicht wieder.\n\n\n\nDie Feynman Methode wird übrigens noch besser, wenn du dir eine Lerngruppe suchst und ihr euch untereinander verschiedene Konzepte erklärt. Dann sind auch die Rückfragen viel spannender und du erfährst, ob du wirklich was verstanden hast. Etwas einfach zu erklären ist nämlich will schwieriger als etwas verworren erzählen. Verworren kann jeder, das ist keine Kunst.\nBitte beachte auch das Blatt Papier in dem ersten Schritt. Ein Blatt Papier bleibt da, du siehst was du geschrieben hast und die Gedanken können sich dort besser ordnen. Dazu gibt es auch die Studie von Mueller und Oppenheimer (2014) mit dem Titel Why the pen its mightiger than the keyboard. Im Prinzip spricht auch nichts gegen ein Tablet, aber dann kannst du den Prozess nicht so schön nachvollziehen. Ich selber mag es nochmal im Prozess zurückzugehen und zu sehen was ich gelernt habe. Das geht für mich in einem Buch voll Blankopapier am einfachsten.\n\n\n\n\n\n\nZum Nachdenken VII: Das menschliche Gehirn?\n\n\n\n\n\nWas ist die Hauptaufgabe des menschlichen Gehirns? Das Vergessen, das menschliche Gehirn arbeitet jede Sekunde daran, zu vergessen und dich vor den herein brandenden Informationen und Sinneseindrücken zu schützen. Wenn du also mehr darüber erfahren willst, wie das menschliche Gehirn lernt, dann kann ich dir Zull (2006) als Einstieg empfehlen.\n\n\n\nWenn du in deiner wissenschaftlichen Karriere wissen willst, warum etwas in einem Forschungsfeld so gemacht wird wie es gemacht wird, dann musst du die Geschichte hinter der Methode kennen. Und die Geschichte hinter einer Entdeckung ist auch immer mit dem Leben eines Menschen mit all seinen Wirrungen eng verflochten. Meistens sind es jedoch mehrere verknüpfte Leben mit all ihren Konflikten und Kompromissen. Die historischen Beispiele in diesem Kapitel zeigen dir da einige dieser Lebenswege. Enden wir also hier mit einem wunderbaren Zitat zur Wissenschaft im Allgemeinen.\n\n“Um eine Wissenschaft zu verstehen, muss man ihre Geschichte kennen” — Auguste Comte",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Forschungsprozess</span>"
    ]
  },
  {
    "objectID": "forschungsprozess.html#referenzen",
    "href": "forschungsprozess.html#referenzen",
    "title": "3  Forschungsprozess",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 3.1— Der Mensch auf der Suche nach neuen Erkenntnissen durchstößt den Horizont und entdeckt die dahinterliegenden Gesetze.\nAbbildung 3.2— Eine vereinfachenden Übersicht über Deduktion und Induktion. In der Empirie werden Daten aus Einzelfällen erhoben. Aus diesen wird dann per Induktion eine allgemeine Theorie gewonnen. Aus der Theorie wiederum können per Deduktion Aussagen über Einzelfälle in der Empirie gewonnen werden.\nAbbildung 3.3— Geozentrische Darstellung des Sonnensystems mit der Erde im Zentrum und Planeten auf kreisförmigen Umlaufbahnen. Um 1540 entwarf Kopernikus die Idee des heliozentrische Weltbild mit der Sonne im Mittelpunkt. Erst um 1600 beschrieb Keppler die eliptischen Umlaufbahnen um die Sonne und ab 1821 wurde die Sonne nicht mehr als bewohnbaren Planet oder ‘terrestrische’ Sonne gesehen.\nAbbildung 3.4— Um die Abweichung der Umlaufbahn des Merkurs durch das Modell nach Newton zu erklären, wurde der Planet Vulkan als Hypothese in den wissenschaftlichen Diskurs gebracht. Quelle: Lith. of E. Jones & G.W. Newman - Library of Congress. https://www.loc.gov/resource/g3180.ct003790\nAbbildung 3.5— Warum ist der Cartoon witzig im Bezug auf Heisenberg und seine Unschärferelation? Was lernen wir aus dem Cartoon für die praktische Anwendung? Quelle: https://www.martin-perscheid.de/\nAbbildung 3.6— Warum ist der Cartoon witzig im Bezug auf den Dopplereffekt und der konstanten Lichtgeschwindigkeit? Was lernen wir aus dem Cartoon für die praktische Anwendung? Quelle: https://www.martin-perscheid.de/\nAbbildung 3.7— Spektrum des sichtbaren Lichts un der Wellenlänge des Lichts.\nAbbildung 3.8— Ein energiereichen, kurzwelliges Photon trifft die Erde und durch Prozesse des Lebens werden zwanzig energiearme, langwellige Photonen abgestrahlt. Die Entropie erhöht sich stark.\nAbbildung 3.9— Ein energiereichen, kurzwelliges Photon trifft einen unbelebten Planten und kurzwellige Photonen werden wiederum abgestrahlt. Die Entropie erhöht sich kaum.\nAbbildung 3.10— Das Entscheidungsdreieck nach Adler: “Das arme Ich”, “Die bösen Anderen” und “Was sollte ich tun?”\nAbbildung 3.11 (a)— Quelle: Kawaler und Veverka (1981)\nAbbildung 3.11 (b)— Quelle: Basalla (2006)\nAbbildung 3.12— “What is this ‘perfume’ you are selling?” “Vinegar in a bottle. It’s the only thing around here that doesn’t completely smell like poo …” Quelle: wumo.com\nAbbildung 3.13— “This is a shit. Traditionally, something we all flock to. This conference asks the question: Why? Is it enviroment or genes? Or are we all just a bunch of idots?” Quelle: wumo.com\nAbbildung 3.14 (a)— Hohlerde nach Halley. Quelle: wikepedia.org\nAbbildung 3.14 (b)— Halley als Astronomer Royal. Quelle: wikepedia.org\nAbbildung 3.15— “Professor Zapinsky proved thate the squid is more intelligent than the housecat when posed with puzzles under similar condistions.” Quelle: wumo.com\nAbbildung 3.16 (a)— Bewohner Neuguineas, Neukaledoniens, der Salomonen. Quelle: tellmeboutblog.wordpress.com\nAbbildung 3.16 (b)— Bilder des Tages. Quelle: der-postillon.com\nAbbildung 3.16 (c)— Birds Arent’t Real Kampagne. Quelle: nytimes.com\nAbbildung 3.17— “After 40 years as a teacher, Gary McAdams finally manages to bore a student to death.” Quelle: wumo.com\nAbbildung 3.18— \nAbbildung 3.19— Darstellung der vier Kategorien der Future Skills und deren Verbindung untereinander sowie deren zukünfige Gewichtung\nAbbildung 3.20 (a)— Beispiel für die Erstellung des Skripts Bioinformatik. Erste Ideen für die Zeichnung der Übersicht.\nAbbildung 3.20 (b)— Beispiel für die Erstellung der experimentellen Designs Kapitels.\n\n\n\nBasalla G. 2006. Civilized life in the universe: Scientists on intelligent extraterrestrials. Oxford University Press.\n\n\nDe Maat S, De Jonghe F, Schoevers R, Dekker J. 2009. The effectiveness of long-term psychoanalytic therapy: A systematic review of empirical studies. Harvard Review of Psychiatry 17: 1–23.\n\n\nEcker G. 2017. Teilchen, Felder, Quanten: Von der Quantenmechanik zum Standardmodell der Teilchenphysik. Springer-Verlag.\n\n\nEngland JL. 2013. Statistical physics of self-replication. The Journal of chemical physics 139.\n\n\nFeynman RP. 1998. Cargo cult science. Seiten 55–61 in. The art and science of analog circuit design. Elsevier.\n\n\nFrankl VE. 2010. trotzdem Ja zum Leben sagen: ein Psychologe erlebt das Konzentrationslager. Kösel-Verlag.\n\n\nHelmke A. 2013. Individualisierung: Hintergrund, Missverständnisse, Perspektiven. Pädagogik 35: 34–37.\n\n\nKawaler S, Veverka J. 1981. The Habitable Sun-One of Herschel, William’s Stranger Ideas. Journal of the Royal Astronomical Society of Canada 75: 46.\n\n\nKishimi I, Koga F. 2018. Du musst nicht von allen gemocht werden: vom Mut, sich nicht zu verbiegen. Rowohlt Verlag GmbH.\n\n\nKishimi I, Koga F. 2020. Du bist genug: vom Mut, glücklich zu sein. Rowohlt Taschenbuch Verlag.\n\n\nKruppa J, Rohmann J, Herrmann C, Sieg M, Rubarth K, Piper S. 2021. What statistics instructors need to know about concept acquisition to make statistics stick. Journal of University Teaching & Learning Practice 18: 02.\n\n\nMueller PA, Oppenheimer DM. 2014. The pen is mightier than the keyboard: Advantages of longhand over laptop note taking. Psychological science 25: 1159–1168.\n\n\nPrice D. 2021. Laziness does not exist. Simon; Schuster.\n\n\nSchimel J. 2012. Writing science: how to write papers that get cited and proposals that get funded. OUP USA.\n\n\nWallace DF. 2009. This is water: Some thoughts, delivered on a significant occasion, about living a compassionate life. Hachette UK.\n\n\nZull JE. 2006. Key aspects of how the brain learns. New directions for adult and continuing education 110: 3.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Forschungsprozess</span>"
    ]
  },
  {
    "objectID": "abschlussarbeit.html",
    "href": "abschlussarbeit.html",
    "title": "4  Abschlussarbeit",
    "section": "",
    "text": "Der individuelle Forschungsprozess\nWissenschaftliches Arbeiten ist nicht einfach. Aber es folgt einem Schema. Dabei ist das Forschen ein komplexer Arbeitsablauf, der aus verschiedenen miteinander verknüpften Aufgaben besteht. Teilweise müssen diese Aufgaben gefühlt parallel erfolgen, obwohl wir nur eine Aufgabe gleichzeitig bearbeiten können.\nWissenschaftliches Arbeiten ist damit auch ein kreativer und offener Prozess. Ein kreativer Prozess erschafft Neues. Damit erschafft dein wissenschaftliches Arbeiten von dir mit geprägtes Neues. Wir bauen hier keine Tisch und Stühle mit Anleitung, denn wir kennen den Tisch oder den Stuhl, den wir bauen wollen, nur als vage Idee. Im Wissenschaftlichen Arbeiten gibt es wenig klare Ziele und vorgegebenen Schritte. Wir tasten uns förmlich im Nebel nach vorne und wollen Neues entdecken. Im wahrsten Sinne ein Abenteuer, den der Ausgang steht noch nicht fest.\nKeiner kann dir sicher sagen, wohin deine Forschung führen wird. Auch ist der der genaue Weg von niemanden von Anfang an festgelegt. Zusätzlich dazu musst du den Anfang des Forschungsprozesses selbst finden, indem du ein Forschungsproblem identifizierst und definieren. In einer Bachelorarbeit mag dir mehr vorgegeben sein als in einer Promotion. Das kann dir Angst machen und das ist auch normal. Mir geht es auch immer so, dass ich am Anfang überwältigt von den Möglichkeiten des Forschens bin.\nWissenschaftliches Arbeiten kennt aber Regeln, die dir etwas Struktur bringen und dir dabei helfen dich zu fokusieren. Es ist nicht notwendig, dass du das Rad komplett neu erfindest, sei es in Bezug auf das Forschungsthema oder den Forschungsprozess. Stattdessen kannst du auf die Ergebnisse früherer Untersuchungen und die Erfahrungen anderer Forscher:innen zurückgreifen. Der Erfolg eines Forschungsprozesses hängt meistens von individuellen Faktoren und sachbezogenen methodischen Aspekten ab.\nBei dem wissenschaftliches Arbeiten muss der individuelle Forschungsprozess für dich und deine aktuellen Arbeitsprozesse berücksichtigt werden. Es ist leider nicht möglich, eine allgemeine Norm festzulegen, die einen gut funktionierenden und produktiven wissenschaftlichen Arbeitsprozess von einem schwierigen und bisweilen unproduktiven wissenschaftlichen Arbeitsprozess unterscheidet. Was für dich gut und richtig ist, kann für jemanden anders falsch sein. Wir können jedoch einige Faktoren benennen, die Einfluss auf die Produktivität des wissenschaftlichen Arbeitsprozesses haben könnten. Dazu werden wir gleich mal ein paar Fragen durchgehen, die dir helfen sollen dich zu reflektieren.\nDie Auseinandersetzung mit deinem eigenen forschenden und schreibenden Handeln ist ein wichtiger Bestandteil des wissenschaftlichen Arbeitens. Dein Ziel muss es sein, den eigenen Arbeitsprozess aktiv und gesund zu gestalten. Sich selbst bewusst zu machen, was du mit welchem Effekt tut kannst und wie du dich dabei fühlt, ist hierfür ebenso Voraussetzung wie ein Nachdenken darüber, was du brauchst, um produktiv arbeiten zu können. Was sind also wichtige Faktoren bei dem Erstellen deiner Abschlussarbeit?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Abschlussarbeit</span>"
    ]
  },
  {
    "objectID": "abschlussarbeit.html#der-individuelle-forschungsprozess",
    "href": "abschlussarbeit.html#der-individuelle-forschungsprozess",
    "title": "4  Abschlussarbeit",
    "section": "",
    "text": "“Ohne Kreativität gibt es keine Entwicklung.” — Commander Spock, Raumschiff Enterprise, Landru und die Ewigkeit\n\n\n\n\nDie Ideen zum Forschungsprozess entstammen auch der Seite der Universität Leipzig - Methodenportal und anderen tollen Seiten.\n\n\n\n\n\n\nEmotion\n\n“Wissenschaftliches Wissen, vor allem wenn es in Lehrbüchern steht, hat eine Aura der Objektivität - Es ist sicher, unbeeinflusst von dem, was wir vielleicht hoffen oder fürchten, und eine solide Behauptung dessen, was wahr ist. Zumindest sollten wir das glauben. Einmal gebildetes Wissen neigt dazu, sich von seinem menschlichen Ursprung zu entfremden. Das heißt, auch wenn das Wissen in Form von Büchern, Formeln, Beweisen, Theoremen und dergleichen bewahrt wird, dürfen wir nicht vergessen, dass vor all dem die Bildung von Wissen das Ergebnis menschlichen Denkens, menschlicher Anstrengung und menschlichen Verlangens war. Wissen ist ein Produkt menschlicher Hoffnungen und Ängste; unsere Emotionen sind entscheidend für seine Entwicklung, und seine Bedeutung kann nicht wirklich verstanden werden, wenn man es als ein blutleeres und emotionsloses Unternehmen betrachtet.” — Schimel (2012, aus dem Englischen)\n\nMit folgenden Fragen kannst du selber schauen, wie es dir mit deinen Emotionen bei der Erstellung der deiner Abschlussarbeit geht. Du kannst dir täglich oder wöchentlich aufschreiben, was die Abschlussarbeit mit dir emotional macht. Dann kannst du schneller reagieren und dir Hilfe holen. Auf und ab wird es auf jeden Fall gehen. Und dieses Auf und Ab ist auch vollkommen normal.\n\nWie geht es dir, wenn du an deiner Abschlussarbeit arbeitest?\nBei welchen Aufgaben fühlst du dich schlecht, unsicher oder überfordert?\nWelche Aufgaben erledigst du gerne und scheinbar wie von selbst?\nSiehst du einen Sinn in deiner Arbeit?\nDenkst du in Vor- oder Schaffensfreude an die nächsten Arbeitsschritte?\n\n\n\n\n\n\n\nAbbildung 4.3— Quelle: phdcomics.com\n\n\n\n\nMehr von dem was klappt, weniger von dem was nicht klappt!\n\nNotiere dir, was gut klappt und was eher nicht. Eine Abschlussarbeit besteht aus vielen Teilen und manchmal funktioniert eine Aufgabe nicht so richtig. Dann ist auch mal okay, gut genug. Also lass das Perfekte nicht der Feind des Guten sein. Konzentriere dich darauf erstmal alle Teile einer Abschlussarbeit zusammenzuhaben. Dann kannst du immer noch einzelne Abschnitte besser machen. Oder um es mal mit den Worten von Morgan (2006) zu sagen…\n\n\n\n“Perfectionism is the voice of the oppressor, the enemy of the people. It will keep you cramped and insane your whole life, and it is the main obstacle between you and a shitty first draft. I think perfectionism is based on the obsessive belief that if you run carefully enough, hitting each stepping-stone just right, you won’t have to die. The truth is that you will die anyway and that a lot of people who aren’t even looking at their feet are going to do a whole lot better than you, and have a lot more fun while they’re doing it.” — Anne Lamott, Bird by Bird”\n\n\n\nMotivation & Disziplin\n\n“Disziplin bedeutet: Dinge, die man hasst, so tun, als würde man sie lieben!” — Mike Tyson\n\nSo schlimm wie bei Mike Tyson ist es bei mir nicht, aber dennoch eine persönliche Notiz von mir zur Motivation. Ich schreibe dieses Skript weniger mit Motivation sondern mehr mit Disziplin. Es gibt Kapitel, die gefallen mir und es gibt Kapitel, die schreibe ich zäh wie Honig. Jeden Tag versuch ich ein paar Worte oder Gedanken hier zu verfassen. Mal sind es mehr, mal weniger, aber es geht immer weiter voran. Word by word, step by step….\nAber bei dir kann das anders sein, zwölf Wochen für eine Bachelorarbeit sind eine Zeit, da klappt es meistens noch mit der Motivation und etwas Disziplin. Also hier ein paar Fragen, die dich herausfordern über deine Motivation nachzudenken.\n\nFrage dich, worauf fußt deine Motivation für die bevorstehende Arbeit?\nIst die Arbeit für dich eher Pflicht oder Freude?\nInteressiert dich das Thema wirklich?\nWas motiviert oder demotiviert dich an deiner Arbeit insgesamt oder an einzelnen Aufgaben?\nWas bestärkt dich weiter zu machen, was verunsichert dich?\n\nIn der folgenden Abbildung 4.4 sehen wir nochmal den Unterschied zwischen der Motivation und der Disziplin dargestellt. Ein Text schreibt sich dann eben nur Wort für Wort und das dann jeden Tag für Tag.\n\n\n\n\n\n\nAbbildung 4.4— Der kleiner Unterschied zwischen der Motivation und der Disziplin. Quelle: @impostercomics\n\n\n\nIn der Abbildung 4.5 siehst du meinen persönlichen Fortschritt des letzten Jahres (September 2022 bis September 2023) auf GitHub. Wie du siehst, habe ich versucht fast jeden Tag etwas zu schreiben oder aber für das Skript zu machen. In den letzten Monaten habe ich meine Rhythmus etwas geändert, da aus privaten Gründen mir täglich nicht mehr so viel Zeit bleibt. Auch ist es im Semester schwierig täglich substanziell zu schreiben. Da bin ich dann ja in der Vorlesung und spreche. So strecke ich dann eben alles Schreiben etwas auf jeden Tag. Ich schreibe eigentlich mittlerweile täglich oder mache etwas was mit dem Schreiben zu tun hat. Ziel ist es immer mindestens ein bedeutendes Commit bei GitHub pro Tag zu machen. Passt mir so mehr in meinen Rhythmus.\n\n\n\n\n\n\nAbbildung 4.5— Mein Schreibprozess am Skript und anderen Lehrprojekten. Es ist nicht immer dieses Skript, ich mache ja noch andere Sachen im bezug auf meine Lehre.\n\n\n\n\n\nArbeitsatmosphäre\n\n“You don’t need special traits, special genes, or special motivation to write a lot. You don’t need to want to write–people rarely feel like doing unpleasant tasks that lack deadlines–so don’t wait until you feel like it. Productive writing comes from harnessing the power of habit, and habits come from repetition” — Paul J. Silvia, How to Write a Lot: A Practical Guide to Productive Academic Writing\n\nKennst du die Orte an der Hochschule wo du Arbeiten kannst, wenn es zu Hause nicht klappt? Vielleicht hilft es dir einfach mal zu Arbeit zu fahren und dich in ein Café oder Bibliothek zu setzen. Besuche doch einfach die Bibliotheken der Hochschule oder die Bibliotheken der Universität Osnabrück - reinsetzen kannst du dich ja überall. Teilweise findest du Lernlandschaften, die du dann auch nutzen kannst. Dann kannst du deine Arbeit von der Freizeit besser trennen. Hier nochmal ein paar Fragen, die dir dabei helfen können dich zu sortieren.\n\nKannst du störungsfrei und konzentriert arbeiten oder wird dein Arbeitsprozess regelmäßig unterbrochen?\nWas längt dich ab? Bist du mehr im Internet oder bei Netflix als bei der Arbeit?\nWann und wo arbeitest du am besten?\nTauscht du dich mit anderen aus oder schreibst du alleine?\nSprichst du mit anderen auch mal über dein Thema und deine Sorgen sowie Herausforderungen?\n\n\n\n\n\n\n\nAbbildung 4.6— Quelle: ruthe.de\n\n\n\nOder du setzte dich einfach in einen Regionalexpress an die Küste und tippst im Zug. Dann isst du ein Eis am Meer und fährst wieder zurück. Mit etwas Glück ist das Internet so schlecht, dass du dich auch nicht so gut ablenken kannst.\n\n\nFähigkeiten & Wissen\n\n“Some lessons have to be experienced to be learned.” — James Clear\n\nHier geht es jetzt eher darum sich nochmal zu sortieren, ob du alles hast was du brauchst oder aber dir nochmal Hilfe suchen möchtest. Teilweise wirst du feststellen, dass du im Prozess der Arbeit noch neue Fähigkeiten und Fertigkeiten brauchst. Das ist nicht schlimm, du kannst dann noch lernen und ergänzen. Wenn du aber gar keine Techniken kennst, dann kannst du diese auch nicht anwenden. Hier hilft wie immer das LearningCenter.\n\nHast du die für deine Arbeit notwendigen Fähigkeiten und Fertigkeiten?\nBist du in der Lage, dir diese anzueignen und vielleicht Unterstützung zu organisieren?\nVerfügst du über das für deine Arbeit notwendige Sachwissen sowie prozedurale Wissen?\nWelche Strategien und Techniken setzt du für dein persönliches Wissens- und Informationsmanagement ein?\nKönnte ein probeweise Strategiewechsel deinen Arbeitsprozess verbessern?\n\nHilfreich ist hier auch sich mit anderen Studierenden, die eine Bachelorarbeit schreiben, zu vernetzen. Du hast die Chance dich nochmal mit anderen auszutauschen und so zu sehen, wie es bei dir läuft. Auch musst du dann nicht alle Techniken selber herausfinden. Gemeinsam geht die Abschlussarbeit dann einfacher.\n\n\n\n\n\n\nWarum nicht mit \\(\\LaTeX\\) und Overleaf?\n\n\n\nWarum nicht etwas Neues lernen und die Abschlussarbeit in \\(\\LaTeX\\) schreiben? \\(\\LaTeX\\) ist ein Textsatzsystem was wirklich wunderbare Dokumente erstellt. Und was noch besser ist, mit Overleaf steht eine kostenlose Onlinevariante zu Verfügung, die du einfach so nutzen kannst ohne viel zu installieren zu müssen. Deshalb schau doch einmal ins Tutorium Creating a document in Overleaf oder aber das Tutorium zum Bibliography management with bibtex. Dann hast du auch gleich eine Möglichkeit einfach deine Referenzen zu sortieren.\n Video ergänzen für Nutzung in Deutsch und Referenzen.\n\n\n\n\nRahmenbedingungen\nIn diesem Block wollen wir gemeinsam raus finden was eigentlich die Rahmenbedingungen für deine Arbeit sind. Vielleicht hast du Kinder oder aber musst dich um wen kümmern. Musst du vielleicht nebenher noch Arbeiten oder hast noch andere ehrenamtliche Verpflichtungen? Dann schränken die Verpflichtungen dich naturgemäß ein. Die Fragen sollen deinen Blick nochmal für die Begleitumstände schärfen.\n\nUnter welchen sozialen und ökonomischen Rahmenbedingungen findet deine Arbeit statt?\nHaste du viel oder wenig Zeit?\nHast du Vorwissen oder musst du dich komplett neu einarbeiten?\nHast du einen Plan B für den Fall der Fälle?\nErfährst du in deinem privaten Umfeld Unterstützung, Bestärkung oder Anregung?\n\nSpreche deshalb auch offen mit dein:r Betreuer:in darüber. Es gibt Themen, die verlangen mehr Einarbeitung und Themen, die sich einfacher durchführen lassen. Sicherlich hat das Thema und die Schwierigkeit eine Auswirkung auf die endgültige Note, aber wenn du mehrere Bälle in der Luft halten musst, dann kann ein einfacheres Thema dich sehr entlasten. Sprich also im Zweifel einmal mit mehreren Betreuer:innen und schaue welche Themen die so anbieten.\n\n\nVerhältnis zu den Betreuer:innen\n\n“Kommunikation ist die Suche nach Übereinstimmung” — anonym\n\n\n\n\n\n\n\nAbbildung 4.7— Quelle: phdcomics.com\n\n\n\nKommen wir jetzt zum Punkt der Betreuung und dabei gilt, dass die Studierenden “eine Informations- und Mitwirkungspflicht” haben. Das heißt aber nicht, dass die Studierenden alles alleine machen sollen und am Ende drückt jemand eine Note auf die Arbeit. Deshalb informiere dich im Vorfeld bei anderen Studierenden wie die Dozenten so arbeiten und wie es so ist bei jemanden eine Abschlussarbeit zu schreiben. Jeder Dozierende ist anders und so findet sich auch jemand, der gut zu dir passt.\n\nHast du ein gutes Verhältnis zu deiner Betreuerin oder deinem Betreuer?\nIst er oder sie erreichbar?\nSprechen wir die gleiche Sprache oder aneinander vorbei?\nGibt es offene oder verborgene Konflikte?\nBekommst du die Unterstützung, die du brauchst?\nKannst du woanders Unterstützung finden?\n\n\n\n\n\n\n\nAbbildung 4.8— Quelle: phdcomics.com\n\n\n\nEs funktioniert einfach mit der Betreuung nicht? Das ist nichts außergewöhnliches und kommt häufiger vor als du glaubst. Du bist da auch nichts besonders, dass kann jedem mal passieren, dass man jemanden nicht kann. Deshalb melde dich zeitnah beim LearningCenter oder deinem zuständigen Studierendensekretariat. Im Zweifel wechselst du dann nochmal die Betreuung.\n\n\nWas ist zu tun?\n\n“I hope that in this year to come, you make mistakes. Because if you are making mistakes…you’re Doing Something.” — Neil Gaiman\n\nNa dann los… Moment… Was wollen wir jetzt eigentlich in welcher Reihenfolge machen? Das ist eine der wichtigsten Frage, deshalb auch hier am Schluss zusammen mit anderen Fragen, die sich mit dem Tun beschäftigen. Diese Fragen musst du dir vermutlich häufiger in deinem Arbeitsprozess stellen, denn meistens überlebt ein Plan den Kontakt mit der Realität nicht.\n\nWeist du, welche Aufgaben du zu bewältigen hast? Kennst du die grobe Reihenfolge der Aufgaben?\nWeist du, wie du diese Aufgaben bewältigen kannst oder wie du diese herausfindest?\nPlanst du deine Arbeiten sinnvoll?\nHast du dir einen groben Zeitplan gemacht und den Zeitplan mit jemanden besprochen?\nWie hoch ist deine Frustrationstoleranz bei Problemen, Fehlern oder Verzögerungen?\n\nNa dann mal los. Du weist ja jetzt, wo du dir Hilfe holen kannst. Und such dir Gleichgesinnte. Wer schreibt denn noch an einer Abschlussarbeit, wissenschaftlichen Veröffentlichung oder einem Bericht? Setzt euch zusammen und sprecht über eure gemeinsamen Erfahrungen und Herausforderungen.\n\n“You may accumulate a vast amount of knowledge but it will be of far less value to you than a much smaller amount if you have not thought it over for yourself; because only through ordering what you know by comparing every truth with every other truth can you take complete possession of your knowledge and get it into your power.” — Arthur Schopenhauer, Essays and Aphorisms\n\nEs ist nicht immer einfach und auch Albrecht Dürer (1471–1528) war sich nicht so sicher, ob er wirklich gut war. Und so schuf Dürer als eines seiner Werke um den Meistertitel zu erlangen den Holzstich Melencolia, zu sehen in Abbildung 4.9. Dürer stellt hier die erste Stufe des Genies dar, mit dem Willen zu Schaffen, aber unfähig zu tun.\n\n\n\n\n\n\nAbbildung 4.9— Die erste Stufe des Genies, mit dem Willen zum Schaffen, aber unfähig, zu tun\n\n\n\n\n\n\n\n\n\nWichtigste Frage am Ende deiner Abschussarbeit…\n\n\n\nWenn du deine Abschlussarbeit nochmal mit deinem jetzigen Wissen von Vorne machen würdest, was würdest du alles anders machen?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Abschlussarbeit</span>"
    ]
  },
  {
    "objectID": "abschlussarbeit.html#der-individuelle-schreibprozess",
    "href": "abschlussarbeit.html#der-individuelle-schreibprozess",
    "title": "4  Abschlussarbeit",
    "section": "Der individuelle Schreibprozess",
    "text": "Der individuelle Schreibprozess\n\n“Dicke Bücher sind deswegen dick, weil der Autor nicht die Zeit hatte, sich kurz zu fassen.” — Walter Moers\n\nSchreiben ist nicht einfach. Aber es folgt einem Schema. Schreiben ist anstrengend und dauert seine Zeit. Ein guter Text wird mehrfach revidiert, umgeschrieben und gelöscht, bis er zu einem guten Text geworden ist. Häufig vergehen mehrere Tage bis man eine Idee so auf das Blatt Papier gebracht hat, dass auch ein Dritter den Text lesen und verstehen kann.\n\n\n\n\n\n\nAbbildung 4.10— Quelle: Dorthe Landschulz / Holzbaum-Verlag\n\n\n\n\n\n\n\n\n\nZentraler Gedanke\n\n\n\nDer erste Entwurf ist für einen selber, ab dem zweiten geht es um den Leser.\n\n\nEs geht also am Anfang erstmal darum, Text auf das weiße Blatt zu kriegen.\n\n“Viele wissenschaftliche Themen kann man erst dann lösen, wenn man alle Aspekte explizit formuliert hat. Das Denken ist dafür insofern nicht genügend vorbereitet, als es immer nur kleine Ausschnitte fokussieren kann. Systematisch denken kann man nur, wenn man schreibt, also die Ergebnisse seines Denkens festhält und mit weiteren Aspekten in Beziehung setzt.” — Otto Kruse, Keine Angst vor dem leeren Blatt\n\nDas ist die Idee vom Schreiben, wie sie von Otto Kruse so schön formuliert wird. Andere merken, wie weit du mit deinen Gedanken gekommen bist. Wir bringen komplexe Gedankengänge in die lineare Form des Textes.\nDer Schreibprozess läuft dabei nach dem Beginn in den nächsten Wochen und Monaten in mehreren Phasen ab. In jeder Phase ist es wichtig, sich ein gutes Umfeld für konzentriertes Arbeiten zu schaffen, gleichzeitig aber die Kommunikation mit den Betreuenden und anderen Mitschaffenden nicht abreißen zu lassen.\n\n\n\n\n\n\nEin paar Videos, die mich motivieren…\n\n\n\nIch schreibe, du schreibst und wir schauen nicht YouTube Videos. Ende. Aber ich habe dann doch noch zwei Videos, die mich sehr inspiriert haben. Zum einen das Video über den Schreibprozess eines Buches 2 Years Of Writing A Book In 30 Minutes und nochmal ein Video über die Strukturierung des Lesens sowie des Schreibens mit Ryan Holiday’s 3-Step System for Reading Like a Pro.\n\n\n\nZeitplan\nHier ist es wichtig was geschrieben werden soll. Eine Bachelor- und Masterarbeit hat einen klaren Zeitplan, der vorab feststehen muss. Bis wann müssen wie viele Wörter geschrieben sein, damit die Arbeit fertig werden kann. Beide Abschlussformen haben ja unterschiedliche Zeitrahmen. Und das ist wirklich wichtig! Wann sollte man fertig sein mit Programmieren, wie lange soll man sich Zeit für Vortragsvorbereitung nehmen, etc. Auch eine wissenschaftliche Veröffentlichung hat einen Zeitplan! Leider – das ist der Primat der Forschung – kann sich der Zeitplan immer wieder ändern, wenn Methoden nicht klappen oder neue Erkenntnisse gewonnen werden. Dennoch muss klar sein, dass in endlicher Zeit – meist ein Jahr – ein Paper eingereicht werden kann. Auf dieses Ziel sollten sich alle Einschwören. Sonst ist eine Promotion in endlicher Zeit nicht machbar.\n\n\nIdeen entwickeln\nMan sammelt Ideen zunächst in der Breite und fokussiert dann, was davon man aufschreiben will. Man liest andere wissenschaftliche Paper, lernt vielleicht noch Grundlagen und Methoden, und macht sich Notizen, was interessant sein könnte, und wo es steht. Natürlich kann man sich bei den Betreuern und Mitschaffenden Hilfe und Anregungen holen, wo man Input herbekommt. Holen heißt aber nicht, dass man gebracht bekommt.\n\n\n\n\n\n\nOCAR Prinzip\n\n\n\nFrage dich, was ist das Opening (der Hintergrund der Arbeit), die Challenge (was ist das Problem, was gelöst werden soll?), die Action (was wirst du tun um dieses Problem zu lösen?) und die Results (Was kam dabei raus oder soll rauskommen?)\n\n\n\n\nStrukturieren\nDas grobe Gerüst ist ja vorgegeben. Eine wissenschaftliche Arbeit folgt dem IMRaD Schema. Erst die Einleitung (Introduction), dann die Methoden (Methods), gefolgt von den Ergebnissen (Results) und der Diskussion (Discussion). Am Ende der Einleitung wird nochmal die Fragestellung benannt. Welche Frage soll in der Arbeit beantwortet werden? Dann fehlt noch die Zusammenfassung am Anfang (abstract) und der Schluss bzw. das Fazit (conclusion). So ist das vorgegebene Schema für die Arbeit, das soll mit Inhalt gefüllt werden. Klingt erstmal einfach und ist es auch. Mit Zwischenüberschriften in den einzelnen Abschnitten kann man sich eine grobe Ordnung vorgeben, was in welcher Reihenfolge aufgeschrieben werden soll. Die Struktur innerhalb von Methodenteilen ist zum Beispiel oft gleich, Beschreibung der Studie, Beschreibung der interessierenden Variablen und ihrer Erhebung, Beschreibung der Auswertungsmethodik. Das kommt aber aufs Thema an. Und unterhalb dieser kann man sich wieder Unter-zwischen-unterüberschriften machen. Es wird sowieso noch alles überarbeitet.\n\n\n\n\n\n\nGrobe Strukturierung nach IMRaD\n\n\n\n\nZusammenfassung\n\n\nEinleitung\n\nStand des Wissens\nForschungsfrage\n\nMaterial und Methoden\nErgebnisse\nDiskussion\n\n\nLiteratur\n\n\n\nWenn es einen Flowchart gibt, so gibt dieser auch die Struktur vor. Ein Flowchart ist nicht final und ändert sich mit der Zeit! Mach den Flowchart am besten auf einem Blatt Papier. Da kannst du schneller was ergänzen.\n\n\n\n\n\n\nZeichne einmal den Ablauf deiner Arbeit auf ein Blattpapier\n\n\n\nZeichne einen Flowchart, der aufzeigt was in der Arbeit passiert!\n\n\n\n\nRohtexten\nDas kann wirklich sloppy sein, in Stichpunkten oder hingerotzt, aber hier soll man sich auch nicht an Details aufhalten, sondern dem Arbeits- und Denkfluss folgen. Gerne auch Denglisch. Lieber erst Text schreiben und dann korrigieren. Wenn das Englische Wort nicht einfällt, das deutsche Hinschreiben. Den Schreibprozess nicht durch im Internet suchen und dann mal was Anderes gucken unterbrechen. Gerade wenn die Arbeit selbst noch im Entstehen ist, schreibt man erst einmal auf, was man tut, was man gemacht, gelernt, oder gelesen hat. Diese Frage stellt sich meist nach den ersten paar Sätzen in einer wissenschaftlichen Arbeit. Worum geht es hier eigentlich? Es könnte alles so einfach sein. Das ist normal. Durch das Aufschreiben werden einem meist die Dinge klarer und uns wird bewusst, wo wir nochmal genauer einhaken müssen.\n\n\nReflektieren\nDie Grundideen sind jetzt schon mal auf dem Papier, jetzt muss man sich überlegen, wie daraus ein Text wird. Gut ist es, sich schon an dieser Stelle Feedback von Betreuern oder Kommilitonen zu holen. Das hilft auch, die eigene Perspektive auf den Text zu ändern und ihn aus mehreren Richtungen zu betrachten – was ist wichtig, was soll viel Platz einnehmen, was fehlt vielleicht noch?\n\n\n\n\n\n\nHinweis\n\n\n\nWas könnte die zentrale Abbildung in den Ergebnissen sein?\n\n\nEin guter Ansatz um einen Fokus zu haben!\n\n\nJetzt schreiben wir! Wie geht es am besten?\nDafür geht gut die Hälfte der Schreibzeit drauf. Am wichtigsten sind Inhalt und Struktur, Korrektheit und Verständlichkeit dürfen aber auch nicht unterschätzt werden: Wir nutzen einfache englische Sprache. Das geschriebene Wort muss nicht schlau klingen, sondern der Inhalt muss schlau sein. Keine umständlichen, gekünstelten Verben verwenden, wenn es ein einfaches Verb auch tut. Wir machen es dem Leser einfach. Später wirst du in einem wissenschaftlichen Paper „we” schreiben, da man selten ein Paper alleine schreibt. Um das jetzt hier gleich am Anfang zu üben, schreibst du keine verschachtelten Passivkonstruktionen, sondern „I do/did something”. Das fühlt sich erst seltsam an, aber wir als Leser danken dir!\n\n\n\n\n\n\nAbbildung 4.11— Das Oxford Komma ist natürlich im englischpsrachigen Raum sehr wichtig.\n\n\n\n\n\n\n\n\n\nSchlecht\n\n\n\nAfter the raw methylation data has been preprocessed, a student t test was used for the differential analysis.\n\n\n\n\n\n\n\n\nGut\n\n\n\nI used the student t test for the differential analysis after the preprocessing of the raw methylation data.\n\n\nWir nutzen auch keine Pronomen, wo man nicht weiß, was diese Pronomen aussagen sollen. Was ist „it” oder „they”?\n\n\n\n\n\n\nAbbildung 4.12— Wer tut was? Was ist mit dies, dieser oder jener gemeint?\n\n\n\n\n\n\n\n\n\nSchlecht\n\n\n\nThe dog and the cat walk into a house. It eats all the cookies.\n\n\n\n\n\n\n\n\nGut\n\n\n\nThe dog and the cat walk into a house. The cat eats all the cookies.\n\n\nWir führen alle Begriffe vorher ein, daher erklären wir diese Begriffe, bevor wir die Begriffe verwenden. Ja, mache Begriffe sind klar, aber welche das sind, ergibt sich manchmal erst auf Nachfrage bei uns und ist für einen Neuling in einem Fachbereich gar nicht zu wissen.\n\n\n\n\n\n\nSchlecht\n\n\n\nI analyzed the NGS data with an ANOVA after checking the residuals with a QQ-plot.\n\n\n\n\n\n\n\n\nGut\n\n\n\nI analyzed the next generation sequencing data (NGS) with an analysis of variance (ANOVA) after plotting the residuals of the model in a quantile-quantile plot (QQ-plot).\n\n\n\n\n\n\n\n\nAbbildung 4.13— Manchmal kann ein falsches Passiv zu Problemem führen…\n\n\n\nIst dir der Begriff nicht klar, erkläre ihn. Später können wir immer noch kürzen. Erkenntnisgewinn durch schreiben ist das Ziel.\n\n\nWer macht was? Die Frage des Lesers an jeden einzelnen Satz!\nKomme in den ersten sieben Wörtern zum Punkt, wer was macht. Subjekt und Prädikat sollen nah beieinander sein und möglichst früh im Satz kommen. Wir schreiben kurze Sätze und vermeiden komplizierte Schachtelsätze.\n\n\n\n\n\n\nSchlecht\n\n\n\nThe differential analysis of whole genome genetic data - like methylation pattern or expression analysis - has a long history of different invented methods and I used different analysis methods to find the best method for the analysis of methylation data with repeated measurements.\n\n\n\n\n\n\n\n\nGut\n\n\n\n[1] A long history of different analysis methods like methylation pattern or expression analysis exists. [2] Hence, many scientist have invented different analysis methods of whole genome.\n\n\n\n\nLöschen von Text\nText löschen macht keine Freude. Es ist immer nervig Teile des Textes, den man so mühsam in die Maschine getippt hat, zu löschen. Lege dir eine neue Datei an, in der du alles was du löschen willst reinkopierst. Bei mir heißt die Datei dump.docx oder dump.R oder auch anders. Auf jeden Fall löschst du so keinen Text, sondern bewahrst ihn erstmal auf. Denk immer daran, es geht nicht darum nur viel Text zu produzieren!\n\n\nZum Schluss kommen\nWie ich jetzt in diesem Augenblick musst du zum Schluss kommen. Kein Text ist perfekt, kein Gedankengang so klar niedergeschrieben, wie er sein könnte. Aber irgendwann muss gut sein. Lass das Perfekte nicht der Feind des Guten sein. Dieser Leitfaden ist good enough und somit muss es reichen. Viel Erfolg!\n\n\n\n\n\n\nErfahrungsbericht von ehemaligen Bachelorstudierenden\n\n\n\n\nkeine neuen (wichtigen) Begriffe verwenden, wenn sie nicht vorher eingeführt wurden\nwenn man einen Begriff nicht richtig versteht oder nicht richtig erklären kann, sollte man ihn nicht verwenden. Daher ist es sehr hilfreich sich gründlich in den Hintergrund des Themas einzulesen.\nman sollte mehr wissen über das Thema, als man eigentlich im Text erklärt (um auf Fragen vorbereitet zu sein)\nAchtgeben auf die Zeitformen\n“it”, “they” und weitere Pronomen vermeiden, d.h. immer klar machen worauf man sich bezieht\nwenn etwas komplizierter scheint, sollte man Beispiele benutzen oder es eventuell graphisch darstellen. Aber: ein Bild nicht verwenden, wenn es keinen inhaltlichen Wert hat! Bringt dieses Bild etwas zum Verständnis des Lesers bei?\nin der Diskussion kein neues “Fass aufmachen”\ndie Limitationen in der Diskussion im Fließtext “verstecken”\ndie eigene Arbeit nicht schlecht reden; “schlechte” Resultate sind auch Resultate\nkeine zu langen Sätze\nes sollte ein roter Faden in jedem Kapitel zu erkennen sein: Was ist das Ziel? Wie erreiche ich das? Was bringt mir diese Methode? Was sagen mir die Resultate (in Bezug auf mein Ziel)?\nund für die eigene Motivation: Es werden oft Schreibblockaden kommen, es wird oft frustrierend sein, weil vielleicht etwas umgeschmissen wird und man von vorne anfangen muss, etc., aber davon soll man sich nicht entmutigen lassen\nDer Text muss nicht gleich beim ersten Hinschreiben perfekt sein, sondern es hilft erstmal etwas drauflos zu schreiben um die Gedanken besser ordnen zu können\nähnlich beim Programmieren: hier hat es mir auch geholfen einfach erstmal anzufangen und Ideen aufzuschreiben, aus welchen sich kann das Programm aufbauen kann\nMan sollte nicht zögern so etwas wie deepl (deepl.com) zu benutzen, wenn man mal mit einer englischen Formulierung nicht weiterkommt\nMan kann vor Beginn eines Tages ein realistisches Tagesziel (oder alternativ ein Wochenziel) festlegen, dann hat man ein Zwischenziel vor Augen und ist zufrieden mit sich, wenn dieses erreicht ist",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Abschlussarbeit</span>"
    ]
  },
  {
    "objectID": "abschlussarbeit.html#sec-bachelorarbeit",
    "href": "abschlussarbeit.html#sec-bachelorarbeit",
    "title": "4  Abschlussarbeit",
    "section": "Bachelorarbeit in der Bio Data Science",
    "text": "Bachelorarbeit in der Bio Data Science\n\n\n\n\n\n\nWichtig - Bitte lesen!\n\n\n\nDer folgende Text und die Anforderungen richten sich an eine Bachelorarbeit bei mir in der Bio Data Science. Keinesfalls handelt es sich hierbei um eine generische Hilfe oder Ratschläge, die sich verallgemeinern lassen. Bitte wende dich dazu vertrauensvoll an dein:e Betreuer:in.\n\n\nHier findest sich eine aktuelle Struktursammlung für die Bachelorarbeit in der Bio Data Science. Hier findest du keine Themen. Dafür musst du mich bitte ansprechen oder eine E-Mail schreiben. Die Themen finden sich dann etwa mit Kooperationspartern oder aber eher methodisch ohne echte Daten. Das müssen wir dann aber Absprechen, die Tendenz ist bei mir aber eindeutig Richtung Methodik und Simulationsstudien als echte Daten auswerten. Eins ist aber sicher, Experimente zur Datengenerierung mache ich keine.\nBitte halte Rücksprache, wenn dir Teile der Regeln für die Bachelorarbeit unklar sind.\n\nIch empfehle die Arbeit in engischer Sprache zu verfassen.\nDie Bachelorarbeit umfasst einen Zeitraum von 12 Wochen. Bei Unsicherheit über das Thema kann einmalig eine 4-wöchige Einarbeitungsphase vereinbart werden. Danach wird das Thema der Bachelorarbeit konkretisiert.\nDer Umfang sollte die 40 Seiten nicht überschreiten.\nDie Arbeit umfasst ca. 30 Referenzen, davon sind die meisten aktuelleren Datums. Internetseiten zählen ausdrücklich nicht als Referenz.\nIm Rahmen der Betreuung finden jede Wochen ein kurzes Zoom-Treffen statt in dem der aktuelle Fortschritt der Arbeit besprochen wird.\nIn der 4-ten Woche wird eine kurze Präsentation der bisherigen erarbeiteten Inhalte gegeben. Diese Präsentation kann in der 8-ten Woche erneut erfolgen.\nMit einem methodischen Thema wird die Arbeit in Quarto in R oder in LaTeX in Overleaf geschreiben.\n\nIm Folgenden siehst du nochmal den groben zeitlichen Ablauf in Abbildung 4.14. Der Ablauf dient der groben Orientierung, damit du auch weißt, wo du etwa stehst. Nach vier Wochen solltest du gut 3500 Wörter geschrieben haben und nach 8 Wochen ca. 7000 Worte. Damit solltest du dann am Ende auf die 30 Seiten mit ca. 10500 Worten kommen.\n\n\n\n\n\n\nAbbildung 4.14— Grober zeitlicher Ablauf einer Bachelorarbeit. Die Präsentation umfasst ca. 10 Slides und folgt ebenfalls dem IMRAD Schema. Ich rechne mit 350 Worten pro deutscher Standardseite.\n\n\n\nBitte beachte auch die Hilfestellungen und die Erfahrungsberichte weiter oben, wo ich nochmal über Writing principles etwas aufgeschrieben habe. Vielleicht hilft dir das dann auch.\n\n\n\n\n\n\nKorrekte Schreibweise von einer Formel\n\n\n\n\n\n\\[\ny \\sim x_1 + x_2\n\\]\nmit\n\n\\(y\\) gleich dem gemessenen Frischgewicht in [kg/ha],\n\\(x_1\\) als der kontinuierliche Einflussvariable 1,\n\\(x_2\\) als der Einflussvariable 2 als Faktor mit den Leveln \\(a\\), \\(b\\) und \\(c\\).\n\n\n\n\n\n\n\n\n\n\nKorrekte Beschriftung und Referenzierung einer Abbildung\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 4.15— Boxplots der Sprungweiten in [cm] getrennt für Hunde- und Katzenflöhe.\n\n\n\n\n\nIn Abbildung 4.15 sind die Sprungweiten in [cm] von Hunde- und Katzenflöhen als Boxplots dargestellt.\n\n\n\n\n\n\n\n\n\nKorrekte Beschriftung und Referenzierung einer Tabelle\n\n\n\n\n\n\n\n\n\nTabelle 4.1— Tabelle der Sprunglängen in [cm] von Hunde- und Katzenflöhen. Es wurden sieben Hundeflöhe und sieben Katzenflöhe gemessen (\\(n= 14\\)).\n\n\n\n\n\n\nanimal\njump_length\n\n\n\n\ndog\n5.7\n\n\ndog\n8.9\n\n\ndog\n11.8\n\n\ndog\n5.6\n\n\ndog\n9.1\n\n\ndog\n8.2\n\n\ndog\n7.6\n\n\ncat\n3.2\n\n\ncat\n2.2\n\n\ncat\n5.4\n\n\ncat\n4.1\n\n\ncat\n4.3\n\n\ncat\n7.9\n\n\ncat\n6.1\n\n\n\n\n\n\n\n\nIn Tabelle 4.1 sind die Sprunglängen in [cm] in der Spalte jump_length von Hunde- und Katzenflöhen in der Spalte animal dargestellt. Insgesamt wurden \\(n = 14\\) Flöhe gemessen davon sieben Hundeflöhe und sieben Katzenflöhe. Wir haben ein balanciertes Design vorliegen.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Abschlussarbeit</span>"
    ]
  },
  {
    "objectID": "abschlussarbeit.html#referenzen",
    "href": "abschlussarbeit.html#referenzen",
    "title": "4  Abschlussarbeit",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 4.1— Anzahl der Referenzen und Seiten in Bachelorarbeiten der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Alle Angaben ohne Gewähr und Doppelungen können enthalten sein. Die Auswertung fand anonym im Rahmen des Moduls Spezielle Statistik und Versuchswesen im Studiengang Pflanzenbiologie statt.\nAbbildung 4.2— Darstellung des heterogenen Zusammenhangs von Seitenzahlen zu der Anzahl an Referenzen in einer Bachelorarbeit. Die rote Linie stellt ein 1:1 Verhältnis von Seitenzahl und der Anzahl an Referenzen dar. Die gestrichelten Linien geben das Minimum und das Maximum, die durchgezogene Linien die mittlere Seitenzahl sowie die mittlere Anzahl an Referenzen wieder. Eingefärbt sind die Punkte nach der Foldchange (abk. FC). Insgesamt haben 27 Arbeiten weniger Referenzen als Seiten zu 16 Arbeiten mit mehr Referenzen als Seiten.\nAbbildung 4.3— Quelle: phdcomics.com\nAbbildung 4.4— Der kleiner Unterschied zwischen der Motivation und der Disziplin. Quelle: @impostercomics\nAbbildung 4.5— Mein Schreibprozess am Skript und anderen Lehrprojekten. Es ist nicht immer dieses Skript, ich mache ja noch andere Sachen im bezug auf meine Lehre.\nAbbildung 4.6— Quelle: ruthe.de\nAbbildung 4.7— Quelle: phdcomics.com\nAbbildung 4.8— Quelle: phdcomics.com\nAbbildung 4.9— Die erste Stufe des Genies, mit dem Willen zum Schaffen, aber unfähig, zu tun\nAbbildung 4.10— Quelle: Dorthe Landschulz / Holzbaum-Verlag\nAbbildung 4.11— Das Oxford Komma ist natürlich im englischpsrachigen Raum sehr wichtig.\nAbbildung 4.12— Wer tut was? Was ist mit dies, dieser oder jener gemeint?\nAbbildung 4.13— Manchmal kann ein falsches Passiv zu Problemem führen…\nAbbildung 4.14— Grober zeitlicher Ablauf einer Bachelorarbeit. Die Präsentation umfasst ca. 10 Slides und folgt ebenfalls dem IMRAD Schema. Ich rechne mit 350 Worten pro deutscher Standardseite.\nAbbildung 4.15— Boxplots der Sprungweiten in [cm] getrennt für Hunde- und Katzenflöhe.\n\n\n\nAdler MJ, Van Doren C. 2014. How to read a book: The classic guide to intelligent reading. Simon; Schuster.\n\n\nBernard V, Michaut M. 2013. Explain bioinformatics to your grandmother! PLOS Computational Biology 9: e1003305.\n\n\nMorgan V. 2006. Bird by Bird: Some Instructions on Writing and Life. Journal of Applied Communications 90: 6.\n\n\nSchechinger K. 2023. What Do You Remember? An Analysis of Information Retention and Recall Through Data Visualization Use in Infographics.\n\n\nSchimel J. 2012. Writing science: how to write papers that get cited and proposals that get funded. OUP USA.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Abschlussarbeit</span>"
    ]
  },
  {
    "objectID": "example-preface.html",
    "href": "example-preface.html",
    "title": "Datenbeispiele",
    "section": "",
    "text": "Letzte Änderung am 24. March 2024 um 16:55:29\n\n“It is possible to commit no mistakes and still lose. That is not a weakness. That is life.” — Jean-Luc Picard, Star Trek: The Next Generation, Peak Performance\n\nWir brauchen am Anfang erstmal ein simples Beispiel. Konkrete Zahlen mit denen wir arbeiten können und Grundlagen aufbauen können. Was liegt da näher als sich einmal am Kopf zu kratzen und zu fragen, was juckt den da? Genau! Flöhe. Wir schauen uns einmal Flöhe auf Hunden und Katzen an. Daran können wir viel über Zahlen und Buchstaben in der Statistik und dann im Programmieren lernen.\nMir ist bewusst, dass du die Unterschiede zwischen Zahlen und Buchstaben kennst. Nur leider ist eine Zahl nicht nur eine Zahl und eine Buchstabenkombination bildet zwar ein Wort aber ein Wort ist nicht immer ein Wort im eigentlichen Sinne. Das hat mit der eingeschränkten Kommunikationsfähigkeit von Computerprogrammen zu tun. R braucht da deine Mithilfe und dein neues Verständnis von Buchstaben und Zahlen. Eben wie ein Computer denkt.",
    "crumbs": [
      "Datenbeispiele"
    ]
  },
  {
    "objectID": "example-fleas-dogs-cats.html",
    "href": "example-fleas-dogs-cats.html",
    "title": "5  Von Flöhen auf Tieren",
    "section": "",
    "text": "5.1 Von Flöhen und Hunden\nIn unserem ersten Beispiel wollen wir uns verschiedene Daten von Hunden und Hundeflöhen anschauen. Unter anderem sind dies die Sprungweite, die Anzahl an Flöhen, die Boniturnoten auf einer Hundemesse sowie der Infektionsstatus. Hier nochmal detailliert, was wir uns im Folgenden immer wieder anschauen wollen. Ich habe dann zu der mathematischen Schreibweise noch die Schreibweise in R mit einem Objekt ergänzt.\nSprungweite in [cm] von verschiedenen Flöhen:\nAnzahl an Flöhen auf verschiedenen Hunden:\nGewicht des gesprungenen Flohes in [mg] auf verschiedenen Hunden:\nBoniturnoten [1 = schwächste bis 9 = stärkste Ausprägung] von verschiedenen Hunden:\nInfektionstatus [0 = gesund, 1 = infiziert] mit Flöhen von verschiedenen Hunden:\nJe nachdem was wir messen, nimmt \\(Y\\) andere Zahlenräume an. Wir sagen, \\(Y\\) folgt einer Verteilung. Die Sprungweite ist normalverteilt, die Anzahl an Flöhen folgt einer Poisson Verteilung, die Boniturnoten sind multinominal/ordinal bzw. kategoriell verteilt. Der Infektionsstatus ist binomial verteilt. Wir werden uns später die Verteilungen anschauen und visualisieren. Das können wir hier aber noch nicht. Wichtig ist, dass du schon mal gehört hast, dass \\(Y\\) unterschiedlich verteilt ist, je nachdem welche Dinge wir messen. Die Tabelle 5.1 zeigt dir die Darstellung der Daten von oben in einer einzigen Tabelle. Bitte beachte, dass genau eine Zeile für eine Beobachtung, in diesem Fall einem Hund, vorgesehen ist.\nTabelle 5.1— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Gewicht der Flöhe, Boniturnote sowie der Infektionsstatus für Hundeflöhe. Die Tabelle ist im Long-Format dargestellt.\n\n\n\n\n\n\nanimal\njump_length\nflea_count\nweight\ngrade\ninfected\n\n\n\n\ndog\n5.7\n18\n2.1\n8\n0\n\n\ndog\n8.9\n22\n2.3\n8\n1\n\n\ndog\n11.8\n17\n2.8\n6\n1\n\n\ndog\n5.6\n12\n2.4\n8\n0\n\n\ndog\n9.1\n23\n1.2\n7\n1\n\n\ndog\n8.2\n18\n4.1\n7\n0\n\n\ndog\n7.6\n21\n3.2\n9\n0",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Von Flöhen auf Tieren</span>"
    ]
  },
  {
    "objectID": "example-fleas-dogs-cats.html#sec-example-1",
    "href": "example-fleas-dogs-cats.html#sec-example-1",
    "title": "5  Von Flöhen auf Tieren",
    "section": "",
    "text": "MathematikR (built-in)\n\n\n\\[\nY_{jump} = \\{5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6\\}.\n\\]\n\n\n\ny_jump_dog &lt;- c(5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6)\ny_jump_dog\n\n[1]  5.7  8.9 11.8  5.6  9.1  8.2  7.6\n\n\n\n\n\n\n\nMathematikR (built-in)\n\n\n\\[\nY_{count} = \\{18, 22, 17, 12, 23, 18, 21\\}.\n\\]\n\n\n\ny_count_dog &lt;- c(18, 22, 17, 12, 23, 18, 21)\ny_count_dog\n\n[1] 18 22 17 12 23 18 21\n\n\n\n\n\n\n\nMathematikR (built-in)\n\n\n\\[\nY_{weight} = \\{2.1, 2.3, 2.8, 2.4, 1.2, 4.1, 3.2\\}.\n\\]\n\n\n\ny_weight_dog &lt;- c(2.1, 2.3, 2.8, 2.4, 1.2, 4.1, 3.2)\ny_weight_dog\n\n[1] 2.1 2.3 2.8 2.4 1.2 4.1 3.2\n\n\n\n\n\n\n\nMathematikR (built-in)\n\n\n\\[\nY_{grade} = \\{8, 8, 6, 8, 7, 7, 9\\}.\n\\]\n\n\n\ny_grade_dog &lt;- c(8, 8, 6, 8, 7, 7, 9)\ny_grade_dog\n\n[1] 8 8 6 8 7 7 9\n\n\n\n\n\n\n\nMathematikR (built-in)\n\n\n\\[\nY_{infected} = \\{0, 1, 1, 0, 1, 0, 0\\}.\n\\]\n\n\n\ny_infected_dog &lt;- c(0, 1, 1, 0, 1, 0, 0)\ny_infected_dog\n\n[1] 0 1 1 0 1 0 0\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatei für von Flöhen und Hunden\n\n\n\nDu findest die Datei flea_dog.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Von Flöhen auf Tieren</span>"
    ]
  },
  {
    "objectID": "example-fleas-dogs-cats.html#sec-example-2",
    "href": "example-fleas-dogs-cats.html#sec-example-2",
    "title": "5  Von Flöhen auf Tieren",
    "section": "5.2 Von Flöhen, Hunden und Katzen",
    "text": "5.2 Von Flöhen, Hunden und Katzen\nWir wollen jetzt das Beispiel von den Hunden und Flöhen um eine Spezies erweitern. Wir nehmen noch die Katzen mit dazu und fragen uns, wie sieht es mit der Sprungfähigkeit von Katzen und Hundeflöhen aus? Konzentrieren wir uns hier einmal auf die Sprungweite. Wir können wie in dem vorherigen Beispiel mit den Hundeflöhen die Sprungweiten [cm] der Katzenflöhe wieder in der gleichen Weise aufschreiben:\n\\[\nY_{jump} = \\{3.2, 2.2, 5.4, 4.1, 4.3, 7.9, 6.1\\}.\n\\]\nWenn wir jetzt die Sprungweiten der Hundeflöhe mit den Katzenflöhen vergleichen wollen haben wir ein Problem. Beide Zahlenvektoren heißen gleich, nämlich \\(Y_{jump}\\). Wir könnten jeweils in die Indizes noch \\(dog\\) und \\(cat\\) schreiben als \\(Y_{jump,\\, dog}\\) und \\(Y_{jump,\\, cat}\\) und erhalten folgende Vektoren.\n\\[\n\\begin{align}\nY_{jump,\\, dog} &= \\{5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6\\}\\\\\nY_{jump,\\, cat} &= \\{3.2, 2.2, 5.4, 4.1, 4.3, 7.9, 6.1\\}\n\\end{align}\n\\]\nDadurch werden die Indizes immer länger und unübersichtlicher. Auch das \\(Y\\) einfach \\(Y_{dog}\\) oder \\(Y_{cat}\\) zu nennen ist keine Lösung - wir wollen uns vielleicht später nicht nur die Sprungweite vergleichen, sondern vielleicht auch die Anzahl an Flöhen oder den Infektionsstatus. Dann stünden wir wieder vor dem Problem die \\(Y\\) für die verschiedenen Outcomes zu unterscheiden. Daher erstellen wir uns die Tabelle 5.2. Wir haben jetzt eine Datentabelle.\n\n\n\n\nTabelle 5.2— Sprunglängen [cm] für Hunde- und Katzenflöhe. Die Tabelle ist im Wide-Format dargestellt.\n\n\n\n\n\n\ndog\ncat\n\n\n\n\n5.7\n3.2\n\n\n8.9\n2.2\n\n\n11.8\n5.4\n\n\n5.6\n4.1\n\n\n9.1\n4.3\n\n\n8.2\n7.9\n\n\n7.6\n6.1\n\n\n\n\n\n\n\n\nIntuitiv ist die obige Tabelle 5.2 übersichtlich und beinhaltet die Informationen die wir wollten. Dennoch haben wir das Problem, das wir in dieser Tabelle 5.2 nicht noch weitere Outcomes angeben können. Wir könnten die Anzahl an Flöhen auf den Hunde und Katzen nicht einfach so in dieser Form darstellen. Als Lösung ändern wir die Tabelle 5.2 in das Long-Format und erhalten die folgende Tabelle 5.3. Jede Beobachtung belegt nun eine Zeile. Dies ist sehr wichtig im Kopf zu behalten, wenn du eigene Daten in z.B. Excel erstellt.\n\n\n\n\nTabelle 5.3— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Gewicht der Flöhe, Boniturnote sowie der Infektionsstatus von Hunde- und Katzenflöhe. Die Tabelle ist im Long-Format dargestellt.\n\n\n\n\n\n\nanimal\njump_length\nflea_count\nweight\ngrade\ninfected\n\n\n\n\ndog\n5.7\n18\n2.1\n8\n0\n\n\ndog\n8.9\n22\n2.3\n8\n1\n\n\ndog\n11.8\n17\n2.8\n6\n1\n\n\ndog\n5.6\n12\n2.4\n8\n0\n\n\ndog\n9.1\n23\n1.2\n7\n1\n\n\ndog\n8.2\n18\n4.1\n7\n0\n\n\ndog\n7.6\n21\n3.2\n9\n0\n\n\ncat\n3.2\n12\n1.1\n7\n1\n\n\ncat\n2.2\n13\n2.1\n5\n0\n\n\ncat\n5.4\n11\n2.4\n7\n0\n\n\ncat\n4.1\n12\n2.1\n6\n0\n\n\ncat\n4.3\n16\n1.5\n6\n1\n\n\ncat\n7.9\n9\n3.7\n6\n0\n\n\ncat\n6.1\n7\n2.9\n5\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatei für von Flöhen, Hunden und Katzen\n\n\n\nDu findest die Datei flea_dog_cat.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Von Flöhen auf Tieren</span>"
    ]
  },
  {
    "objectID": "example-fleas-dogs-cats.html#sec-example-3",
    "href": "example-fleas-dogs-cats.html#sec-example-3",
    "title": "5  Von Flöhen auf Tieren",
    "section": "5.3 Von Flöhen auf Hunde, Katzen und Füchsen",
    "text": "5.3 Von Flöhen auf Hunde, Katzen und Füchsen\nWir wollen jetzt das Beispiel von den Hunde- und Katzenflöhen um eine weitere Spezies erweitern. Warum machen wir das? Später wollen wir uns anschauen, wie sich verschiedene Gruppen oder Behandlungen voneinander unterscheiden. Wir brauchen also mehr Spezies. Wir nehmen noch die Füchse mit dazu und fragen uns, wie sieht es mit der Sprungfähigkeit von Hunde-, Katzen- und Fuchsflöhen aus?\n\n\n\n\nTabelle 5.4— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Gewicht der Flöhe, Boniturnote sowie der Infektionsstatus von Hunde-, Katzen- und Fuchsflöhe.\n\n\n\n\n\n\nanimal\njump_length\nflea_count\nweight\ngrade\ninfected\n\n\n\n\ndog\n5.7\n18\n2.1\n8\n0\n\n\ndog\n8.9\n22\n2.3\n8\n1\n\n\ndog\n11.8\n17\n2.8\n6\n1\n\n\ndog\n5.6\n12\n2.4\n8\n0\n\n\ndog\n9.1\n23\n1.2\n7\n1\n\n\ndog\n8.2\n18\n4.1\n7\n0\n\n\ndog\n7.6\n21\n3.2\n9\n0\n\n\ncat\n3.2\n12\n1.1\n7\n1\n\n\ncat\n2.2\n13\n2.1\n5\n0\n\n\ncat\n5.4\n11\n2.4\n7\n0\n\n\ncat\n4.1\n12\n2.1\n6\n0\n\n\ncat\n4.3\n16\n1.5\n6\n1\n\n\ncat\n7.9\n9\n3.7\n6\n0\n\n\ncat\n6.1\n7\n2.9\n5\n0\n\n\nfox\n7.7\n21\n3.1\n5\n1\n\n\nfox\n8.1\n25\n4.2\n4\n1\n\n\nfox\n9.1\n31\n5.1\n4\n1\n\n\nfox\n9.7\n12\n3.5\n5\n1\n\n\nfox\n10.6\n28\n3.2\n4\n0\n\n\nfox\n8.6\n18\n4.6\n4\n1\n\n\nfox\n10.3\n19\n3.7\n3\n0\n\n\n\n\n\n\n\n\nDer Datensatz in Tabelle 5.4 beginnt schon recht groß zu werden. Deshalb brauchen wir auch die Statistiksoftware R als Werkzeug um große Datensätze auswerten zu können.\n\n\n\n\n\n\nDatei für von Flöhen auf Tieren\n\n\n\nDu findest die Datei flea_dog_cat_fox.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Von Flöhen auf Tieren</span>"
    ]
  },
  {
    "objectID": "example-fleas-dogs-cats.html#sec-example-4",
    "href": "example-fleas-dogs-cats.html#sec-example-4",
    "title": "5  Von Flöhen auf Tieren",
    "section": "5.4 Von Flöhen auf Tieren in Habitaten",
    "text": "5.4 Von Flöhen auf Tieren in Habitaten\nWir schauen uns in diesem Beispiel wiederum drei Tierarten an: Hunde, Katzen und Füchse. Auf diesen Tierarten messen wir die Sprunglänge von jeweils zehn Tieren. Im Vergleich zu dem vorherigen Beispiel erweitern wir die Daten um eine Spalte site in der wir vier verschiedene Messorte protokollieren. Es ergibt sich folgende Tabelle 5.5 und die dazugehörige Abbildung 5.1.\n\n\n\n\nTabelle 5.5— Sprunglängen [cm] für Hunde-, Katzen- und Fuchsflöhe in verschiedenen Habitaten.\n\n\n\n\n\n\nanimal\nsite\nrep\njump_length\n\n\n\n\ncat\ncity\n1\n12.04\n\n\ncat\ncity\n2\n11.98\n\n\ncat\ncity\n3\n16.1\n\n\ncat\ncity\n4\n13.42\n\n\ncat\ncity\n5\n12.37\n\n\ncat\ncity\n6\n16.36\n\n\n…\n…\n…\n…\n\n\nfox\nfield\n5\n16.38\n\n\nfox\nfield\n6\n14.59\n\n\nfox\nfield\n7\n14.03\n\n\nfox\nfield\n8\n13.63\n\n\nfox\nfield\n9\n14.09\n\n\nfox\nfield\n10\n15.52\n\n\n\n\n\n\n\n\nDie Datentabelle ist in dieser Form schon fast nicht mehr überschaubar. Daher hilft hier die explorative Datenanalyse weiter. Wir schauen uns daher die Daten einmal als einen Boxplot in Abbildung 5.1 an. Wir sehen hier, dass wir drei Tierarten an vier Orten die Sprungweite in [cm] gemessen haben.\n\n\n\n\n\n\n\n\nAbbildung 5.1— Boxplot der Sprungweiten [cm] für Hunde-, Katzen- und Fuchsflöhe in verschiedenen Habitaten.\n\n\n\n\n\n\n\n\n\n\n\nDatei für von Flöhen auf Tieren in Habitaten\n\n\n\nDu findest die Datei flea_dog_cat_fox_site.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Von Flöhen auf Tieren</span>"
    ]
  },
  {
    "objectID": "example-fleas-dogs-cats.html#sec-example-5",
    "href": "example-fleas-dogs-cats.html#sec-example-5",
    "title": "5  Von Flöhen auf Tieren",
    "section": "5.5 Von vielen Flöhen auf Hunden und Katzen",
    "text": "5.5 Von vielen Flöhen auf Hunden und Katzen\nWir schauen uns in diesem Beispiel wiederum nur zwei Tierarten an: Hunde und Katzen. Auf diesen Tierarten messen wir wieder die Sprunglänge in [cm] von jeweils 400 Tieren. Im Vergleich zu dem vorherigen Beispiel erweitern wir die Daten um eine Spalte jump_weight in [mg] sowie sex [male, female]. Bei Versuch wurde noch in der Variable hatch_time gemessen, wie lange die Flöhe in Stunden zum Schlümpfen brauchen. Es ergibt sich folgende Tabelle 5.6 mit den ersten zehn Beobachtungen und die dazugehörige Abbildung 5.2.\n\n\n\n\nTabelle 5.6— Sprunglängen [cm], Gewichte [mg], Geschecht [sex] und Schlüpfzeit [h] für Hunde- und Katzenflöhe.\n\n\n\n\n\n\nanimal\nsex\nweight\njump_length\nflea_count\nhatch_time\n\n\n\n\ncat\nmale\n6.02\n15.79\n5\n483.60\n\n\ncat\nmale\n5.99\n18.33\n1\n82.56\n\n\ncat\nmale\n8.05\n17.58\n1\n296.73\n\n\ncat\nmale\n6.71\n14.09\n3\n140.90\n\n\ncat\nmale\n6.19\n18.22\n1\n162.20\n\n\ncat\nmale\n8.18\n13.49\n1\n167.47\n\n\ncat\nmale\n7.46\n16.28\n1\n291.20\n\n\ncat\nmale\n5.58\n14.54\n0\n112.58\n\n\ncat\nmale\n6.19\n16.36\n1\n143.97\n\n\ncat\nmale\n7.53\n15.08\n1\n766.31\n\n\n\n\n\n\n\n\nDie Datentabelle ist in dieser Form schon fast nicht mehr überschaubar. Daher hilft hier die explorative Datenanalyse weiter. Wir schauen uns daher die Daten einmal als einen Scatterplot in Abbildung 5.2 an. Wir sehen hier, dass wir das mit dem Gewicht [mg] der Flöhe auch die Sprungweite in [cm] steigt.\n\n\n\n\n\n\n\n\nAbbildung 5.2— Scatterplot der Sprunglängen [cm] und Gewichte [mg] für Hunde- und Katzenflöhe.\n\n\n\n\n\n\n\n\n\n\n\nDatei für von vielen Flöhen auf Hunden und Katzen\n\n\n\nDu findest die Datei flea_dog_cat_length_weight.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Von Flöhen auf Tieren</span>"
    ]
  },
  {
    "objectID": "example-fleas-dogs-cats.html#referenzen",
    "href": "example-fleas-dogs-cats.html#referenzen",
    "title": "5  Von Flöhen auf Tieren",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 5.1— Boxplot der Sprungweiten [cm] für Hunde-, Katzen- und Fuchsflöhe in verschiedenen Habitaten.\nAbbildung 5.2— Scatterplot der Sprunglängen [cm] und Gewichte [mg] für Hunde- und Katzenflöhe.\n\n\n\nCadiergues M-C, Joubert C, Franc M. 2000. A comparison of jump performances of the dog flea, Ctenocephalides canis (Curtis, 1826) and the cat flea, Ctenocephalides felis felis (Bouché, 1835). Veterinary parasitology 92: 239–241.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Von Flöhen auf Tieren</span>"
    ]
  },
  {
    "objectID": "example-gummi-bears.html",
    "href": "example-gummi-bears.html",
    "title": "6  Von Gummibärchen",
    "section": "",
    "text": "Referenzen",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Von Gummibärchen</span>"
    ]
  },
  {
    "objectID": "example-gummi-bears.html#referenzen",
    "href": "example-gummi-bears.html#referenzen",
    "title": "6  Von Gummibärchen",
    "section": "",
    "text": "Kruppa J, Kiehne B. 2019. Statistik lebendig lehren durch Storytelling und forschungsbasiertes Lernen. Beiträge zu Praxis, Praxisforschung und Forschung 501.\n\n\nKruppa J, Sieg M. 2021. Spielerisch Daten reinigen. Seiten 93–103 in. Zeig mir Health Data Science! Springer.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Von Gummibärchen</span>"
    ]
  },
  {
    "objectID": "example-complex.html",
    "href": "example-complex.html",
    "title": "7  Von komplexeren Daten",
    "section": "",
    "text": "7.1 Von infizierten Ferkeln\nIm Folgenden schauen wir uns den anonymisierten Datensatz zu einer Ferkelinfektion an. Wir haben verschiedene Gesundheitsparameter an den Ferkeln gemessen und wollen an diesen Rückschließen, ob diese Gesundheitsparameter etwas mit der Infektion zu tun haben. Insgesamt haben wir gut \\(400\\) Ferkel an vier verschiedenen Orten in Niedersachsen gemessen.\nTabelle 7.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\ninfected\n\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\n1\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\n1\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\n0\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.15\n19.05\n4.35\n1\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n55.38\n18.44\n5.27\n1\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n60.29\n18.42\n4.78\n1\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\npre-frail\n55.32\n19.75\n3.92\n1\n\n\n56\nmale\nwest\n13.91\n20.8\nfrail\n58.37\n17.28\n7.44\n0\n\n\n57\nmale\nnorthwest\n12.49\n21.95\npre-frail\n56.66\n16.86\n2.44\n1\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\n1\n\n\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\n0\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\n1\nAuch hier haben wir nur eingeschränkte Informationen zu den erhobenen Variablen. Daher müssen wir schauen, dass die Variablen in etwa Sinn ergeben.\nWir nutzen den Datensatz unter anderem in der logistischen Regression in Kapitel 49.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Von komplexeren Daten</span>"
    ]
  },
  {
    "objectID": "example-complex.html#sec-example-pigs",
    "href": "example-complex.html#sec-example-pigs",
    "title": "7  Von komplexeren Daten",
    "section": "",
    "text": "age, das Alter in Lebenstagen der untersuchten Ferkel.\nsex, das bestimmte Geschlecht der Ferkel.\nlocation, anonymisierter Ort der Untersuchung. Wir unterscheiden zwischen Norden, Nordosten, West und Nordwest in Niedersachsen.\nactivity, Minuten an Aktivität pro Stunde. Die Aktivität wurde über eine automatische Bilderkennung bestimmt. Dabei musste die Bewegung ein gewisses Limit übersteigen. Einfach rumgehen hat nicht gereicht um gezählt zu werden.\ncrp, der CRP-Wert in mg/l aus der Blutprobe. Das Ausmaß des CRP-Anstiegs gibt einen Hinweis auf die Schwere der zugrundeliegenden Krankheit.\nfrailty, die visuelle Einordnung des Gesundheitszustandes anhand der Beweglichkeit des Ferkels. Nach einem Punkteschema wurden die Ferkel in die drei Gruppen robust, pre-frail und frail eingeteilt.\nbloodpressure, gemessener Blutdruck der Ferkel.\nweight, das gemessene Gewicht der Ferkel in kg.\ncreatinin, der Creatinin-Wert aus der Blutprobe. Zu hohe Kreatinin-Werte können auf eine Nierenschwäche, Verletzungen der Muskulatur oder eine Entzündung der Haut und Muskulatur hindeuten.\ninfected, der Infektionsstatus zum Zeitpunkt der Untersuchung.\n\n\n\n\n\n\n\n\nDatei von den infizierten Ferkeln\n\n\n\nDu findest die Datei infected_pigs.xlsx auf GitHub jkruppa.github.io/data/ als Excel Datei.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Von komplexeren Daten</span>"
    ]
  },
  {
    "objectID": "example-complex.html#sec-example-longnose",
    "href": "example-complex.html#sec-example-longnose",
    "title": "7  Von komplexeren Daten",
    "section": "7.2 Von langnasigen Hechten",
    "text": "7.2 Von langnasigen Hechten\nIn der folgenden Datentabelle wollen wir uns die Anzahl an Hechten in verschiedenen nordamerikanischen Flüßen anschauen. Jede Zeile des Datensatzes steht für einen Fluss. Wir haben dann in jedem Fluss die Anzahl an Hechten gezählt und weitere Flussparameter erhoben. Wir fragen uns, ob wir anhand der Flussparameter eine Aussage über die Anzahl an Hechten in einem Fluss machen können.\n\n\nDie Daten zu den langnasigen Hechten stammt von Salvatore S. Mangiafico - An R Companion for the Handbook of Biological Statistics.\n\n\n\n\nTabelle 7.2— Auszug aus dem Daten zu den langnasigen Hechten.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstream\nlongnose\narea\ndo2\nmaxdepth\nno3\nso4\ntemp\n\n\n\n\nbasin_run\n13\n2528\n9.6\n80\n2.28\n16.75\n15.3\n\n\nbear_br\n12\n3333\n8.5\n83\n5.34\n7.74\n19.4\n\n\nbear_cr\n54\n19611\n8.3\n96\n0.99\n10.92\n19.5\n\n\nbeaver_dam_cr\n19\n3570\n9.2\n56\n5.44\n16.53\n17\n\n\nbeaver_run\n37\n1722\n8.1\n43\n5.66\n5.91\n19.3\n\n\nbennett_cr\n2\n583\n9.2\n51\n2.26\n8.81\n12.9\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nseneca_cr\n23\n18422\n9.9\n45\n1.58\n8.37\n20.1\n\n\nsouth_br_casselman_r\n2\n6311\n7.6\n46\n0.64\n21.16\n18.5\n\n\nsouth_br_patapsco\n26\n1450\n7.9\n60\n2.96\n8.84\n18.6\n\n\nsouth_fork_linganore_cr\n20\n4106\n10\n96\n2.62\n5.45\n15.4\n\n\ntuscarora_cr\n38\n10274\n9.3\n90\n5.45\n24.76\n15\n\n\nwatts_br\n19\n510\n6.7\n82\n5.25\n14.19\n26.5\n\n\n\n\n\n\n\n\nWie immer haben wir nicht so viele Informationen über die Daten vorliegen. Einiges können wir aber aus den Namen der Spalten in dem Datensatz ableiten. Wir haben in verschiedenen Flüssen die Anzahl an Hechten gezählt und noch weitere Flussparameter gemessen. Ein wenig müssen wir hier auch unsere eigene Geschichte spinnen.\n\nstream, beschreibt den Fluss, wo die Messung der Anzahl an langnasigen Hechten stattgefunden hat.\nlongnose, die Anzahl der Hechte, die in einem Flussarm in einer definierten Zeit gezählet wurden.\narea, erfasste Oberfläche des Flusses in dem gemessenen Gebiet. Die Fläche wurde über Satelietenbilder bestimmt.\ndo2, gemessener Partialdruck von Sauerstoiff \\(O_2\\) im Wasser und damit auch der verfügbarer Sauerstoff (engl. Oxygen-Delivery, DO2) im Wasser.\nmaxdepth, die maximale Tiefe des Flusses über mindestens einen Kilometer. Kürze Tiefen wurden nicht berücksichtigt.\nno3, die gemessene Nitratkonzentration im Wasser.\nso4, die gemessene Schwefelkonzentration im Wasser.\ntemp, gemessene Temperatur in dem Flussarm zur Zeit der Zählung.\n\nWir nutzen den Datensatz unter anderem in der Poisson Regression in Kapitel 47.\n\n\n\n\n\n\nDatei von den langnasigen Hechten\n\n\n\nDu findest die Datei longnose.csv auf GitHub jkruppa.github.io/data/ als Csv Datei.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Von komplexeren Daten</span>"
    ]
  },
  {
    "objectID": "example-complex.html#sec-example-chickpea",
    "href": "example-complex.html#sec-example-chickpea",
    "title": "7  Von komplexeren Daten",
    "section": "7.3 Von den Kichererbsen in Brandenburg",
    "text": "7.3 Von den Kichererbsen in Brandenburg\nIm Folgenden schauen wir uns die Daten eines Pilotprojektes zum Anbau von Kichererbsen in Brandenburg an. Wir haben an verschiedenen anonymisierten Bauernhöfen Kichererbsen angebaut und das Trockengewicht als Endpunkt bestimmt. Darüber hinaus haben wir noch andere Umweltparameter erhoben und wollen schauen, welche dieser Parameter einen Einfluss auf das Trockengewicht hat.\n\n\n\n\nTabelle 7.3— Auszug aus dem Daten zu den Kichererbsen in Brandenburg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntemp\nrained\nlocation\nno3\nfe\nsand\nforest\ndryweight\n\n\n\n\n25.26\nhigh\nnorth\n5.56\n4.43\n63\n&gt;1000m\n253.42\n\n\n21.4\nhigh\nnortheast\n9.15\n2.58\n51.17\n&lt;1000m\n213.88\n\n\n27.84\nhigh\nnortheast\n5.57\n2.19\n55.57\n&gt;1000m\n230.71\n\n\n24.59\nlow\nnorth\n7.97\n1.47\n62.49\n&gt;1000m\n257.74\n\n\n26.51\nlow\nnorth\n6.29\n4.3\n59.09\n&gt;1000m\n242.03\n\n\n22.3\nlow\nnortheast\n6.69\n4.78\n58.72\n&gt;1000m\n236.98\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n25.04\nlow\nnortheast\n5.64\n2.22\n59.47\n&gt;1000m\n240.28\n\n\n28.77\nlow\nwest\n6.55\n2.26\n61.11\n&gt;1000m\n268.39\n\n\n25.47\nlow\nnorth\n6.92\n3.18\n64.55\n&lt;1000m\n268.58\n\n\n29.04\nlow\nnorth\n5.64\n2.87\n53.27\n&gt;1000m\n236.07\n\n\n24.11\nhigh\nnortheast\n4.31\n3.66\n63\n&lt;1000m\n259.82\n\n\n28.88\nlow\nnortheast\n7.92\n2\n65.75\n&gt;1000m\n274.75\n\n\n\n\n\n\n\n\nEs ist ja schon fast Mode, aber auch hier haben wir wenig bis gar keine Informationen zu den erhobenen Variablen. Daher machen wir das Beste aus der Sachlage und überlegen uns was hier passen könnte.\n\ntemp, die mittlere Temperatur über die Wachstumsperiode.\nrained, erfasste Regenmenge im Vergleich zum 10jähigen Mittel.\nlocation, anonymisierter Ort der Untersuchung.\nno3, die gemessene Nitratkonzentration im Boden.\nso4, die gemessene Eisenkonzentration im Boden.\nsand, der Anteil an Sand im Boden.\nforest, der Abstand zum nächsten geschlossenen Waldstück.\ndryweight, das Trockengewicht der Kichererbsen gemittelt über eine Hektar.\n\nWir nutzen den Datensatz unter anderem in der Gaussian Regression in Kapitel 46.\n\n\n\n\n\n\nDatei von den Kichererbsen in Brandenburg\n\n\n\nDu findest die Datei chickpeas.xlsx auf GitHub jkruppa.github.io/data/ als Excel Datei.",
    "crumbs": [
      "Datenbeispiele",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Von komplexeren Daten</span>"
    ]
  },
  {
    "objectID": "programing-preface.html",
    "href": "programing-preface.html",
    "title": "Programmieren in R",
    "section": "",
    "text": "Referenzen",
    "crumbs": [
      "Programmieren in R"
    ]
  },
  {
    "objectID": "programing-preface.html#referenzen",
    "href": "programing-preface.html#referenzen",
    "title": "Programmieren in R",
    "section": "",
    "text": "Abbildung 1— © 2023 Sarah C Andersen, https://sarahcandersenshop.com/\nAbbildung 2— Die menschlichen Computer des Jet Propulsion Laboratory (JPL) der NASA trugen nicht nur zum Start des US-Raumfahrtprogramms bei, sondern bedeuteten auch einen wichtigen Schritt nach vorn für Frauen und andere unterrepräsentierte Personen zu einer Zeit, als ein Großteil der Berufswelt und insbesondere technische Bereiche von weißen Männern dominiert wurden. Janez Lawson (auf diesem Foto von 1953, vordere Reihe, fünfte von links) war die erste Afroamerikanerin, die in einer technischen Position am JPL eingestellt wurde. Quelle: NASA\nAbbildung 3— Diese Szene zeigt eine typische Hierarchie von Kalkulationsarbeitern: in der Mehrzahl sitzende, gering qualifizierte Stanzarbeiter, hinter ihnen ihre unmittelbaren Vorgesetzten, ein paar Sortiermaschinenbediener in der Nähe der Fenster und ihre Vorgesetzten dahinter. Quelle: Computer History Museum – Birth of the Computer.\nAbbildung 4 (a)— Porträt von Ada von der britischen Malerin Margaret Sarah Carpenter (1836)\nAbbildung 4 (b)— Hopper in einem Computerraum in Washington, D.C., 1978, fotografiert von Lynn Gilbert\nAbbildung 4 (c)— Katherine Johnson bei der Arbeit in der NASA im Jahr 1966\nAbbildung 4 (d)— Margaret Hamilton im Jahr 1969, neben dem Stapel der Ausdrucke der Software, die sie und ihr MIT-Team für das Apollo-Projekt entwickelt haben\nAbbildung 5— Im Jahr 1995 waren 37 % der Informatiker Frauen. Heute sind es nur noch 24 %. Der Prozentsatz wird weiter sinken, wenn wir nichts tun. Wir wissen, dass der größte Rückgang von Mädchen in der Informatik im Alter zwischen 13 und 17 Jahren stattfindet. Quelle: Girls who code\n\n\n\nAuel A. 2019. The Mathematics of Grace Murray Hopper. Notices of the American Mathematical Society 66.\n\n\nGrier DA. 2013. When computers were human. Princeton University Press.\n\n\nShetterly M. 2020. Im Kernschatten des Mondes-Die unbekannten Heldinnen der NASA. HarperCollins.\n\n\nWilkinson L. 2008. The future of statistical computing. Technometrics 50: 418–435.",
    "crumbs": [
      "Programmieren in R"
    ]
  },
  {
    "objectID": "programing-letters-numbers.html",
    "href": "programing-letters-numbers.html",
    "title": "8  Buchstaben und Zahlen",
    "section": "",
    "text": "8.1 Daten in R sind tibble()\nIm Folgenden sehen wir die Daten aus der Tabelle 8.1 in R als tibble dargestellt. Was ist nun ein tibble? Ein tibble ist zu aller erst ein Speicher für Daten in R. Das heißt wir lagern unsere Daten in Spalten und Zeilen. Jede Spalte repräsentiert eine Messung oder Variable und die Zeilen jeweils eine Beobachtung.\n# A tibble: 14 × 6\n   animal jump_length flea_count weight grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;   \n 1 dog            5.7         18    2.1     8 FALSE   \n 2 dog            8.9         22    2.3     8 TRUE    \n 3 dog           11.8         17    2.8     6 TRUE    \n 4 dog            5.6         12    2.4     8 FALSE   \n 5 dog            9.1         23    1.2     7 TRUE    \n 6 dog            8.2         18    4.1     7 FALSE   \n 7 dog            7.6         21    3.2     9 FALSE   \n 8 cat            3.2         12    1.1     7 TRUE    \n 9 cat            2.2         13    2.1     5 FALSE   \n10 cat            5.4         11    2.4     7 FALSE   \n11 cat            4.1         12    2.1     6 FALSE   \n12 cat            4.3         16    1.5     6 TRUE    \n13 cat            7.9          9    3.7     6 FALSE   \n14 cat            6.1          7    2.9     5 FALSE\nSchauen wir uns also ein tibble() der obigen Datentabelle einmal näher an. Als erstes erfahren wir, dass wir einen A tibble: 14 x 5 vorliegen haben. Das heißt, wir haben 14 Zeile und 5 Spalten. In einem tibble wird immer in der ersten Zeile angezeigt wie viele Beobachtungen wir in dem Datensatz haben. Wenn das tibble zu groß wird, werden wir nicht mehr das ganze tibble sehen sondern nur noch einen Ausschnitt. Im Weiteren hat jede Spalte noch eine Eigenschaft unter dem Spaltennamen:\nWieso ist das wichtig? Wir sehen hier, was R glaubt in den Spalten an Zahlen und Buchstaben vorzufinden. Es mag aber sein, dass R sich irrt. Dann müssen wir die Eigenschaften der Spalte ändern.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Buchstaben und Zahlen</span>"
    ]
  },
  {
    "objectID": "programing-letters-numbers.html#daten-in-r-sind-tibble",
    "href": "programing-letters-numbers.html#daten-in-r-sind-tibble",
    "title": "8  Buchstaben und Zahlen",
    "section": "",
    "text": "&lt;chr&gt; bedeutet character. Wir haben also hier Worte vorliegen.\n&lt;dbl&gt; bedeutet double. Ein double ist eine Zahl mit Kommastellen.\n&lt;int&gt; bedeutet integer. Ein integer ist eine ganze Zahl ohne Kommastellen.\n&lt;lgl&gt; bedeutet logical oder boolean. Hier gibt es nur die Ausprägung wahr oder falsch. Somit TRUE oder FALSE. Statt den Worten TRUE oder FALSE kann hier auch 0 oder 1 stehen.\n&lt;str&gt; bedeutet string der aus verschiedenen character besteht kann, getrennt durch Leerzeichen.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Buchstaben und Zahlen</span>"
    ]
  },
  {
    "objectID": "programing-letters-numbers.html#faktoren-als-wörter-zu-zahlen",
    "href": "programing-letters-numbers.html#faktoren-als-wörter-zu-zahlen",
    "title": "8  Buchstaben und Zahlen",
    "section": "8.2 Faktoren als Wörter zu Zahlen",
    "text": "8.2 Faktoren als Wörter zu Zahlen\nEin Faktor ist eine Variable mit mehreren Faktorstufen oder Leveln. Für uns sieht der Faktor wie ein Wort aus, hinter jedem Wort steht aber eine Zahl mit der gerechnet werden kann.\nEin Computer und somit auch eine Programmsprache wie R kann keine Buchstaben verrechnen. Ein Programm kann nur mit Zahlen rechnen. Wir haben aber in der Tabelle 8.1 in der Spalte animal Buchstaben stehen. Da wir hier einen Kompromiss eingehen müssen führen wir Faktoren ein. Ein Faktor kombiniert Buchstaben mit Zahlen. Wir als Anwender sehen die Buchstaben, die Wörter bilden. Intern steht aber jedes Wort für eine Zahl, so dass R mit den Zahlen rechnen kann. Klingt ein wenig kryptisch, aber wir schauen uns einen factor einmal in R an.\n\ndata_tbl$animal[1:8]\n\n[1] \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\"\n\n\nÜber das $ Symbol kannst du im Kapitel 9.7 mehr erfahren.\nWas haben wir gemacht? Als erstes haben wir die Spalte animal aus dem Datensatz data_tbl mit dem Dollarzeichen $ herausgezogen. Mit dem $ Zeichen können wir uns eine einzelne Spalte aus dem Datensatz data_tbl raus ziehen. Du kannst dir das $ wie einen Kleiderbügel und das data_tbl als einen Schrank für Kleiderbügel verstellen. An dem Kleiderbügel hängen dann die einzelnen Zahlen und Worte. Wir nehmen aber nicht den ganzen Vektor sondern nur die Zahlen 1 bis 8, dargestellt durch [1:8]. Die Gänsefüßchen \" um dog zeigen uns, dass wir hier Wörter oder charactervorliegen haben. Schauen wir auf das Ergebnis, so erhalten wir sieben Mal dog und einmal cat. Insgesamt die ersten acht Einträge der Datentabelle. Wir wollen diesen Vektor uns nun einmal als Faktor anschauen. Wir nutzen die Funktion as_factor().\nÜber Funktionen kannst du im Kapitel 9.4 mehr erfahren.\n\nas.factor(data_tbl$animal[1:8])\n\n[1] dog dog dog dog dog dog dog cat\nLevels: cat dog\n\n\nIm direkten Vergleich verschwinden die Gänsefüßchen \" um dog und zeigen uns, dass wir hier keine character mehr vorliegen haben. Darüber hinaus sehen wir auch, dass die der Faktor jetzt Levels hat. Exakt zwei Stück. Jeweils einen für dog und einen für cat. Wir werden später Faktoren benötigen, wenn wir zum Beispiel eine einfaktorielle ANOVA rechnen. Hier siehst du schon den Begriff Faktor wieder.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Buchstaben und Zahlen</span>"
    ]
  },
  {
    "objectID": "programing-letters-numbers.html#von-wörtern-und-objekten",
    "href": "programing-letters-numbers.html#von-wörtern-und-objekten",
    "title": "8  Buchstaben und Zahlen",
    "section": "8.3 Von Wörtern und Objekten",
    "text": "8.3 Von Wörtern und Objekten\nDas mag etwas verwirrend sein, denn es gibt in R Wörter string &lt;str&gt; oder character &lt;chr&gt;. Wörter sind was anderes als Objekte. Streng genommen sind beides Wörter, aber in Objekten werden Dinge gespeichert wohin gegen das Wort einfach ein Wort ist. Deshalb kennzeichnen wir Wörter auch mit Gänsefüßchen als \"wort\" und zeigen damit, dass es sich hier um einen String handelt.\nWir tippen \"animal\" in R und erhalten \"animal\" als Wort zurück. Das sehen wir auch an dem Ausdruck mit den Gänsefüßchen.\n\n\"animal\"\n\n[1] \"animal\"\n\n\nÜber den Zuweisungspfeil &lt;- kannst du im Kapitel 9.5 mehr erfahren.\nWir tippen animal ohne die Anführungszeichen in R und erhalten den Inhalt von animal ausgegeben. Dafür müssen wir aber das Objekt animal erst einmal über den Zuweisungspfeil &lt;- erschaffen.\n\nanimal &lt;- c(\"dog\", \"cat\", \"fox\")\nanimal\n\n[1] \"dog\" \"cat\" \"fox\"\n\n\nSollte es das Objekt animal nicht geben, also nicht über den Zuweisungspfeil &lt;- erschaffen worden, dann wird eine Fehlermeldung von R ausgegeben:\nFehler in eval(expr, envir, enclos) : Objekt 'animal' nicht gefunden\nDas Konzept des Objekts als Speicher für Dinge und der Unterschied zum Wort ist hier etwas sehr abstrakt. Wenn wir aber in R anfangen zu arbeiten wird dir das Konzept klarer werden. Hier müssen wir aber einmal Objekte einführen, damit wir auch etwas in R speichern können und nicht immer wieder neu erschaffen müssen.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Buchstaben und Zahlen</span>"
    ]
  },
  {
    "objectID": "programing-letters-numbers.html#zusammenfassung",
    "href": "programing-letters-numbers.html#zusammenfassung",
    "title": "8  Buchstaben und Zahlen",
    "section": "8.4 Zusammenfassung",
    "text": "8.4 Zusammenfassung\nVariablennamen meint hier immer den Namen der Spalte im Datensatz bzw. tibble\nTabelle 8.2 zeigt eine Übersicht wie einzelne Variablennamen und deren zugehörigen Beispielen sowie den Namen in R, der Informatik allgemein, als Skalenniveau und welcher Verteilungsfamilie die Variable angehören würde. Leider ist es so, dass wieder gleiche Dinge unterschiedliche benannt werden. Aber an dieses doppelte Benennen können wir uns in der Statistik schon mal gewöhnen.\n\n\n\n\nTabelle 8.2— Zusammenfassung und Übersicht von Variablennamen und deren Bennung in R, in der Informatik allgemein, als Skalenniveau und die dazugehörige Verteilungsfamilie.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariablenname\nBeispiel\nR\nInfomatik\nSkalenniveau\nVerteilungsfamilie\n\n\n\n\nweight\n12.3, 12.4, 5.4, 21.3, 13.4\nnumeric\ndouble\ncontinuous\nGaussian\n\n\ncount\n5, 0, 12, 23, 1, 4, 21\ninteger\ninteger\ndiscrete\nPoisson\n\n\ndosis\nlow, mid, high\nordered\n\ncategorical / ordinal\nOrdinal\n\n\nfield\nmainz, berlin, kiel\nfactor\n\ncategorical\nMultinomial\n\n\ncancer\n0, 1\nfactor\n\ndichotomous / binary / nominal\nBinomial\n\n\ntreatment\n“placebo”, “aspirin”\ncharacter\ncharacter/string\ndichotomous / binary / nominal\nBinomial\n\n\nbirth\n2001-12-02, 2005-05-23\ndate",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Buchstaben und Zahlen</span>"
    ]
  },
  {
    "objectID": "programing-basics.html",
    "href": "programing-basics.html",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "",
    "text": "9.1 Pakete und library()\nIn der Vanilla-Variante hat R sehr wenige Funktionen. Als Vanilla beschreibt man in der Informatikerwelt ein Programm, was keine zusätzlichen Pakete geladen hat. Also die reinste Form ohne zusätzlichen Geschmack. Ohne zusätzliche Pakete ist R mehr ein sehr potenter Taschenrechner. Leider mit der Funktionalität aus den 90’zigern, was die Programmierumgebung und die Funktionen angeht. Das wollen wir aber nicht. Wir wollen auf den aktuellen Stand der Technik und auch Sprache programmieren. Daher nutzen wir zusätzliche R Pakete.\nIn Abbildung 9.1 wird gezeigt wie du ein zusätzliches Paket installieren kannst. Hierbei ist nochmal wichtig den semantischen Unterschied zu wissen. Es gibt das Paket {tidyverse} was wir viel nutzen. Wir installieren einmalig Pakete der Funktion install.packages() oder eben wie in Abbildung 9.1 gezeigt. Wir nutzen die Funktion library() um ein Paket in R zu laden. Ja, es müsste anders heißen, tut es aber nicht.\n## Das Paket {tidyverse} installieren - einmalig\ninstall.packages(tidyverse)\n\n## Das Paket {tidyverse} laden - jedes Mal\nlibrary(tidyverse)\nNun muss man sich immer merken, ob das Paket schon installiert ist oder man schreibt relativ viele library() untereinander. Das passiert schnell, wenn du viele Pakete laden willst. Dafür erlaubt dir das Paket {pacman} eine Vereinfachung. Die Funktion p_load() installiert Pakete, wenn die Pakete nicht installiert sind. Sollten die Pakete installiert sein, so werden die Pakete geladen. Du musst nur einmal install.packages(pacman) ausführen um das Paket {pacman} zu installieren.\npacman::p_load(tidyverse, magrittr, readxl)",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#sec-R-packages",
    "href": "programing-basics.html#sec-R-packages",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "",
    "text": "Abbildung 9.1— Auf den Reiter Packages klicken und dann Install. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#anordnung-der-fenster-im-rstudio",
    "href": "programing-basics.html#anordnung-der-fenster-im-rstudio",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "9.2 Anordnung der Fenster im RStudio",
    "text": "9.2 Anordnung der Fenster im RStudio\nWie dir sicherlich aufgefallen ist, sind in meinen Videos die einzelnen Kacheln im RStudio anders angeordnet. Der Grund ist einfach. Wir sind die meiste Zeit in dem Skript auf der linken Seite und schicken dann den R Code auf die rechte Seite. Normalerweise sind das Skript und die R Console links untereinander angeordnet. Das finde ich aber disfunktional. In Abbildung 9.2 und Abbildung 9.3 kannst du nachvollziehen, wie du die Anordnung der Kacheln im R Studio ändern kannst.\n\n\n\n\n\n\nAbbildung 9.2— Auf den Reiter Tools klicken und dann Global Options…. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\n\n\n\n\n\n\nAbbildung 9.3— Auf den Reiter Pane Layout klicken und dann die Kacheln so anordnen wie du sie hier siehst. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\n\n\nDu kannst vieles in den Global Options… anpassen - unter anderem auch das Aussehen (eng. Appearance).",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#sec-R-vector",
    "href": "programing-basics.html#sec-R-vector",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "9.3 Einen Vektor bauen c()",
    "text": "9.3 Einen Vektor bauen c()\nWir können mit der Funktion c() Zahlen und Wörter zu einem Vektor kombinieren.\n\nc(\"dog\", \"dog\", \"cat\", \"cat\", \"fox\", \"fox\")\n\n[1] \"dog\" \"dog\" \"cat\" \"cat\" \"fox\" \"fox\"\n\n\nHier werden die Wörter “dog”, “cat” und “fox” miteinader in einen Vektor kombiniert. Wir erinnern uns an das $ Zeichen, was uns erlaubt eine Variable als Vektor aus einem tibble()herauszuziehen.\nWir können auch Zahlen zusammenbauen oder aber ganze Bereiche mit dem : definieren. Wir lesen den : als “von bis”.\n\nc(1, 8, 4, 5)\n\n[1] 1 8 4 5\n\n\nDie Zahlen von 1 bis 5 werden durch den : ausgegeben.\n\nc(1:5)\n\n[1] 1 2 3 4 5",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#sec-R-function",
    "href": "programing-basics.html#sec-R-function",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "9.4 Funktionen",
    "text": "9.4 Funktionen\nWir haben schon einige Funktion nebenbei in R kennengelernt. Zum einen as.factor() um einen Faktor zu erstellen oder aus dem Kapitel 9.1, wo wir die Funktion install.packages() nutzen um ein Paket zu installieren oder aber die Funktion library() um ein Paket in R zu laden.\nFunktionen sehen aus wie Wörter. Haben aber keine Gänsefüßchen und beinhalten auch keine Daten oder Vektoren. Funktionen können mit Daten und Vektoren rechnen und geben das Berechnete dann wieder. Nehmen wir als Beispiel die Funktion mean(), die den Mittelwert von einer Reihe Zahlen berechnet.\n\ny &lt;- c(1.2, 3.4, 2.1, 6, 4.3)\nmean(y)\n\n[1] 3.4\n\n\nWir sehen, dass wir mit der Funktion c() die Zahlen \\(1.2, 3.4, 2.1, 6, 4.3\\) zusammenkleben. Danach speichern wir die Zahlen in den Objekt y als einen Vektor ab. Wir müssen y nicht erst erschaffen, das Erschaffen und Speichern passiert in R in einem Schritt. Wir stecken nun den Vektor y in die Funktion mean() und erhalten den Mittelwert von \\(3.4\\) der Zahlen wiedergegeben.\nWir können auch eigene Funktionen mit dem Befehl function(){} oder noch einfacher mit \\(){} erstellen. Du siehst schon den Unterschied, es sind hier zwei unterschiedlich Klammern. Schauen wir uns die Anwendung einmal im Beispiel an. Im ersten Fall bauen wir uns eine Funktion, die eigentlich nur die Aufgabe hat mehre Zeilen Code zusammenzufassen. Wenn wir my_mean() mit den Werten für x ausführen, erhalten wir immer den gleichen Mittelwert wieder.\n\nmit function(x){}mit \\(x){}\n\n\nHier einmal die lange Version mit dem Namen function in der Langschreibweise. Dann sieht der Code wie folgt aus.\n\nmy_mean &lt;- function(x){\n   res &lt;- c(1.2, 3.4, 2.1, 6, 4.3) |&gt; \n     sum() |&gt; \n     divide_by(5) \n   return(res)\n}\n\nmy_mean(x = c(1.2, 3.4, 2.1, 6, 4.3))\n\n[1] 3.4\n\n\n\n\nWenn du es noch kürzer magst, wie ich es teilweise nutze, dann ersetzt du das Wort function durch einen \\ und schon hast du die Kurzschreibweise.\n\nmy_mean &lt;- \\(x){\n   res &lt;- c(1.2, 3.4, 2.1, 6, 4.3) |&gt; \n     sum() |&gt; \n     divide_by(5) \n   return(res)\n}\n\nmy_mean(x = c(1.2, 3.4, 2.1, 6, 4.3))\n\n[1] 3.4\n\n\n\n\n\nWichtig ist, dass jeder Funktionsblock {} mit einem return() endet in dem das Objekt steht, was von der Finktion zurückgegeben werden soll.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#sec-R-pfeil",
    "href": "programing-basics.html#sec-R-pfeil",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "9.5 Zuweisungspfeil <-",
    "text": "9.5 Zuweisungspfeil &lt;-\nMit dem Zuweisungspfeil speichern wir Dinge in Objekte. Das heißt wir speichern damit intern in R Datensätze und viele andere Sachen, die wir dan später wieder verwenden wollen. Schauen wir uns das einmal im Beispiel an. Schrieben wir nur den Vektor c() mit Hunden und Katzen darin, so erscheint eine Ausgabe in R.\n\nc(\"dog\", \"dog\", \"cat\", \"cat\", \"fox\", \"fox\")\n\n[1] \"dog\" \"dog\" \"cat\" \"cat\" \"fox\" \"fox\"\n\n\nSchreiben wir den gleichen Vektor und nutzen den Zuweisungspfeil, dann wird der Vektor in dem Objekt animal gespeichert. Wenn du Strg Enter drückst, dann erstellt das RStudio automatisch den Zuweisungspfeil &lt;-.\n\nanimal &lt;- c(\"dog\", \"dog\", \"cat\", \"cat\", \"fox\", \"fox\")\n\nWie kommen wir jetzt an die Sachen, die in animal drin sind? Wir können einfach animal in R schreiben und dann wird uns der Inhalt von animal ausgegeben.\n\nanimal\n\n[1] \"dog\" \"dog\" \"cat\" \"cat\" \"fox\" \"fox\"\n\n\nDer Zuweisungspfeil &lt;- ist zentral für die Nutzung von R. Wenn du Strg Enter drückst, dann erstellt das RStudio automatisch den Zuweisungspfeil &lt;-.\nWir nutzen den Zuweisungspfeil &lt;- ist zentral für die Nutzung von R. Wir brauchen den Zuweisungspfeil &lt;- um Objekte in R zu erschaffen und Ergebnisse intern abzuspeichern. Zusammen mit Funktionen nutzen wir nur noch die Pipe |&gt; öfter.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#sec-R-pipe",
    "href": "programing-basics.html#sec-R-pipe",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "9.6 Pipe |> (bzw. %>%)",
    "text": "9.6 Pipe |&gt; (bzw. %&gt;%)\nAls erstes gab es den Pipe-Operator %&gt;% aus dem R Paket {magrittr}. Nun gibt es schon seit einiger Zeit den built-in Operator |&gt; in R. Das heißt, du musst dann kein eigens Paket mehr laden um einen Pipe-Operator zu nutzen. Ich habe daher im Laufe des Jahres 2024 den Pipe-Operator %&gt;% systematisch durch den Pipe-Operator |&gt; hier im Buch ersetzt. Mehr kannst du auch auf StackOverflow und der Frage What are the differences between R’s native pipe |&gt; and the magrittr pipe %&gt;%? erfahren. Leider sind die beiden Operatoren nicht 1 zu 1 austauschbar. Der Blogpost Replacing the Magrittr Pipe With the Native R Pipe geht auch nochmal auf einige Punkte ein. In der folgenden Abbildung zeige ich nochmal wie die den native Pipe-Operator |&gt; aktivierst.\n\n\n\n\n\n\nAbbildung 9.4— Unter den Gloabl Options... findets du unter Code die Möglichkeit den native Pipe-Operator |&gt; im RStudio zu aktivieren.\n\n\n\nIm Weiteren nutzen wir den Pipe Operator also dargestellt als |&gt;. Du kannst dir den Pipe Operator als eine Art Röhre vorstellen in dem die Daten verändert werden und dann an die nächste Funktion weitergeleitet werden. Im folgenden siehst du viele Funktionen, die aneinander über Objekte miteinander verbunden werden. Im Kapitel 11 erfährst du mehr über die Funktionen select()und filter().\n\n1data_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n2animal_1_tbl &lt;- select(data_tbl, animal, jump_length)\n3animal_2_tbl &lt;- filter(animal_1_tbl, jump_length &gt;= 4)\n4sort(animal_2_tbl$jump_length)\n\n\n5data_tbl |&gt;\n  select(animal, jump_length) |&gt; # \n  filter(jump_length &gt;= 4) |&gt; #\n6  pull(jump_length) |&gt;\n  sort() # \n\n\n1\n\nLade den Datensatz flea_dog_cat.xlsx\n\n2\n\nWähle die entsprechenden Spalten animal und jump_length\n\n3\n\nFiltere alle Beobachtungen mit einer Sprunglänge größer/gleich 4\n\n4\n\nSortiere die Sprunglänge nach große der Werte\n\n5\n\nPipe den Datensatz in die folgenden Funktionen\n\n6\n\nExtrahiere die Spalte jump_length als Vektor\n\n\n\n\n [1]  4.1  4.3  5.4  5.6  5.7  6.1  7.6  7.9  8.2  8.9  9.1 11.8\n [1]  4.1  4.3  5.4  5.6  5.7  6.1  7.6  7.9  8.2  8.9  9.1 11.8\n\n\nIm unteren Beispiel siehst du die Nutzung des Pipe Operators |&gt;. Das Ergebnis ist das gleiche, aber der Code ist einfacher zu lesen. Wir nehmen den Datensatz data_tbl leiten den Datensatz in den Funktion select() und wählen die Spalten animal sowie jump_length. Dann filtern wir noch nach jump_lengthgrößer als 4 cm. Dann ziehen wir uns mit der Funktion pull() die Spalte jump_length aus dem Datensatz. Den Vektor leiten wir dann weiter in die Funktion sort() und erhalten die sortierten Sprunglängen zurück.\nIn Abbildung 9.5 und Abbildung 9.6 sehen wir, wie wir den Shortcut für das Erstellen des Pipe Operators umdefinieren. Danach können wir einfach den Shortcut nutzen und müssen nicht immer händisch den Pipe Operator eingeben.\n\n\n\n\n\n\nAbbildung 9.5— Auf den Reiter Modify Keyboard Shortcuts klicken. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\n\n\n\n\n\n\nAbbildung 9.6— Im Suchfeld pipe eingeben und dann in das Feld mit dem Shortcut klicken. Danach Alt und . klicken. Danach wird der Pipe Operator mit dem Shortcut Alt . gesetzt. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\nWas gibt es noch an Pipes? Das R Paket {magrittr} erlaubt noch weitere Pipes zu nutzen. Wir haben noch den %$%-Pipe oder auch Dollar-Pipe genannt. Hier können wir dann die Namen der Spalten weiterleiten. Ab und zu nutze ich noch diesen Dollar-Pipe-Operator.\n\ndata_tbl %$%\n  cor(jump_length, flea_count)\n\n[1] 0.4912994",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#sec-dollar",
    "href": "programing-basics.html#sec-dollar",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "9.7 Spalte extrahieren $",
    "text": "9.7 Spalte extrahieren $\nWir nutzen eigentlich die Funktion pull() um eine Spalte bzw. Vektor aus einem Datensatz zu extrahieren.\n\ndata_tbl |&gt; \n  pull(animal)\n\n [1] \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\" \"cat\" \"cat\" \"cat\" \"cat\"\n[13] \"cat\" \"cat\"\n\n\nManche Funktionen in R, besonders die älteren Funktionen, benötigen keinen Datensatz sondern meist zwei bis drei Vektoren. Das heißt, wir können nicht einfach einen Datensatz in eine Funktion über data = data_tbl stecken sondern müssen der Funktion Vektoren übergeben. Dafür nutzen wir den $ Operator.\n\ndata_tbl$animal\n\n [1] \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\" \"cat\" \"cat\" \"cat\" \"cat\"\n[13] \"cat\" \"cat\"\n\ndata_tbl$jump_length\n\n [1]  5.7  8.9 11.8  5.6  9.1  8.2  7.6  3.2  2.2  5.4  4.1  4.3  7.9  6.1\n\n\nWir werden versuchen diese Schreibweise zu vermeiden, aber manchmal ist es sehr nützlich die Möglichkeit zu haben auf diese Weise eine Spalte zu extrahieren.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#werte-extrahieren-mit-oder-pluck",
    "href": "programing-basics.html#werte-extrahieren-mit-oder-pluck",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "9.8 Werte extrahieren mit [] oder pluck()",
    "text": "9.8 Werte extrahieren mit [] oder pluck()\nWenn wir aus einem Vektor oder eine Matrix Werste extrahieren wollen, dann können wir dazu die eckigen Klammern [] nutzen. Im Folgenden wollen wir uns einmal den vierten Wert des Vektors animal wiedergeben lassen. Wie immer geht auch eine Kombination aus Zahlen, wie c(1, 4, 6) oder aber gleich eine ganze Sequenz mit 1:4.\n\n1animal[4]\n\n\n1\n\nGibt den vierten Wert in dem Vektor animal wieder.\n\n\n\n\n[1] \"cat\"\n\n\nWir können auf diese Art und Weise auch auf Zeilen und Spalten zugreifen, wenn es unbedingt sein muss. Es ist besser über select() Spalten beim Namen zu wählen oder aber direkt über filter() Zeilen auszuschließen. Aber manchmal braucht man dann auch die Brechstange. Somit hier einmal die Brechstange des Daten rausziehen.\n\nZeilen aus einem Datensatz wiedergeben\n\nMit [1, ] wählst du die erste Zeile. Du musst aber unbedingt das Komma hinter der Zahl setzen. Wie immer gehen hier auch beliebige andere Zahlen Kombinationen. Du musst die Zahlen nur mit einem c() zusammenfassen.\n\n\n\ndata_tbl[1, ]\n\n# A tibble: 1 × 6\n  animal jump_length flea_count weight grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dog            5.7         18    2.1     8        0\n\n\n\nSpalten aus einem Datensatz wiedergeben lassen\n\nMit [, 2] lässt du dir die zweite Spalte aus dem Datensatz wiedergeben. Hier wird es dann leider wild. Nimmst du nur eine Spalte erhälst du einen Vektor. Bei mehr als einer Spalte wiederum einen Datensatz. Deshalb nutze select() da kriegst du immer ein tibble wieder.\n\n\n\ndata_tbl[2, ]\n\n# A tibble: 1 × 6\n  animal jump_length flea_count weight grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dog            8.9         22    2.3     8        1\n\n\nDas R Paket {purrr} liefert die Funktion pluck(), die es dir ermöglicht auch aus komplexeren Datenstrukturen wie Listen zuverlässig den richtigen Wert zu finden. Bei Listen wird es sehr schnell wild und pluck hilft dir dabei die Übersicht zu behalten.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#sec-formula",
    "href": "programing-basics.html#sec-formula",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "9.9 Modelle definieren mit formula",
    "text": "9.9 Modelle definieren mit formula\nWir müssen später Modelle in R definieren um zum Beispiel den t Test oder aber eine lineare Regression rechnen zu können. Wir nutzen dazu in R die formula Syntax. Das heißt links von der Tilde ~ steht das \\(y\\), also der Spaltenname aus dem Datensatz data = den wir nutzen, der das Outcome repräsentiert. Rechts von der Tilde ~ stehen alle \\(x_1, ..., x_p\\), also alle Spalten aus dem Datensatz data = den wir nutzen, der die Einflussfaktoren repräsentiert.\nIn unserem Beispiel mit den Hunde- und Katzenflöhen aus Kapitel 5.2 wäre das \\(y\\) die Spalte jump_length und das \\(x\\) der Faktor animal. Wir erstellen mit der Funktion formula() das Modell in R. Wir brauchen später die Funktion formula nur implizit, aber hier ist es gut, das du einmal siehst, wie so eine Formula in R aussieht.\n\nformula(jump_length ~ animal)\n\njump_length ~ animal\n\n\nWenn die Formel sehr lang wird bzw. wir die Namen der Spalten aus anderen Funktionen haben, können wir auch die Funktion reformulate() nutzen. Wir brauchen die Funktion aber eher im Bereich des maschinellen Lernens. Hier ist die Funktion reformulate() aufgeführt, da es inhaltlich passt.\n\n1reformulate(termlabels = c(\"animal\", \"sex\", \"site\"),\n2            response = \"jump_length\",\n3            intercept = TRUE)\n\n\n1\n\nHier steht was auf die rechte Seite der ~ im Modell als Einflussvariablen kommt.\n\n2\n\nHier steht was auf der linken Seite der ~ im Modell als Outcome kommt.\n\n3\n\nWillst du den Intercept mit ins Modell haben TRUE oder soll alles durch den Nullpunkt FALSE.\n\n\n\n\njump_length ~ animal + sex + site\n\n\nWir schon gesagt, die Funktion ist echt was für fortgeschrittene Programmierung, aber ich habe die Funktion jahrelang teilweise schmerzlich vermisst. Deshalb ist die Funktion dann auch hier.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-basics.html#sec-R-help",
    "href": "programing-basics.html#sec-R-help",
    "title": "9  Operatoren, Funktionen und Pakete",
    "section": "9.10 Hilfe mit ?",
    "text": "9.10 Hilfe mit ?\nDas Fragezeichen ? vor einem Funktionsnamen erlaubt die Hilfeseite zu öffnen. Die Hilfeseiten findest du auch in einem der Reiter im RStudio.\n\n\n\n\n\n\nAbbildung 9.7— Neben den Paketen in R findet sich auch der Reiter Help, wo du Hilfe für die einzelnen Funktionen findest.\n\n\n\n\n\n\nAbbildung 9.1— Auf den Reiter Packages klicken und dann Install. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\nAbbildung 9.2— Auf den Reiter Tools klicken und dann Global Options…. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\nAbbildung 9.3— Auf den Reiter Pane Layout klicken und dann die Kacheln so anordnen wie du sie hier siehst. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\nAbbildung 9.4— Unter den Gloabl Options... findets du unter Code die Möglichkeit den native Pipe-Operator |&gt; im RStudio zu aktivieren.\nAbbildung 9.5— Auf den Reiter Modify Keyboard Shortcuts klicken. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\nAbbildung 9.6— Im Suchfeld pipe eingeben und dann in das Feld mit dem Shortcut klicken. Danach Alt und . klicken. Danach wird der Pipe Operator mit dem Shortcut Alt . gesetzt. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\nAbbildung 9.7— Neben den Paketen in R findet sich auch der Reiter Help, wo du Hilfe für die einzelnen Funktionen findest.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Operatoren, Funktionen und Pakete</span>"
    ]
  },
  {
    "objectID": "programing-import.html",
    "href": "programing-import.html",
    "title": "10  Daten einlesen",
    "section": "",
    "text": "10.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, janitor, plyr)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Daten einlesen</span>"
    ]
  },
  {
    "objectID": "programing-import.html#sec-format",
    "href": "programing-import.html#sec-format",
    "title": "10  Daten einlesen",
    "section": "10.2 Dateiformat",
    "text": "10.2 Dateiformat\n\n\nDas Buch Cookbook for R stellt auch Beispiele für die Funktion gather() zu Verfügung für die Umwandlung von Wide zu Long Format: Converting data between wide and long format\nWir unterschieden bei Datenformaten zwischen den Wide Format und dem Long Format. Meistens gibst du die Daten intuitv im Wide Format in Excel ein. Das ist in Excel auch übersichtlicher. R und später die Funktion ggplot() zur Visualisierung der Daten kann aber nur mit dem Long Format arbeiten. Wir können aber mit der Funktion gather() das Wide Format in das Long Format umwandeln.\n\n10.2.1 Wide Format\nIn Tabelle 10.1 sehen wir eine typische Datentabelle in einem Wide Format. Die Spalten egeben jeweils die Tierart wieder und die Einträge in den Spalten sind die Sprungweiten in [cm].\n\n\n\nTabelle 10.1— Eine Datentabelle mit Sprungweiten in [cm] von Hunde- und Katzenflöhen im Wide Format.\n\n\n\n\n\ndog\ncat\n\n\n\n\n5.2\n10.1\n\n\n4.9\n9.4\n\n\n12.1\n11.8\n\n\n8.2\n6.7\n\n\n5.6\n8.2\n\n\n9.1\n9.1\n\n\n7.4\n7.1\n\n\n\n\n\n\nWir können diese Datentablle auch in R erstellen und uns als tibble() wiedergeben lassen.\n\njump_wide_tbl &lt;- tibble(dog = c(5.2, 4.9, 12.1, 8.2, 5.6, 9.1, 7.4),\n                        cat = c(10.1, 9.4, 11.8, 6.7, 8.2, 9.1, 7.1))\njump_wide_tbl\n\n# A tibble: 7 × 2\n    dog   cat\n  &lt;dbl&gt; &lt;dbl&gt;\n1   5.2  10.1\n2   4.9   9.4\n3  12.1  11.8\n4   8.2   6.7\n5   5.6   8.2\n6   9.1   9.1\n7   7.4   7.1\n\n\nWenn du schon Daten hast, dann macht es eventuell mehr Sinn eine neue Exceldatei anzulegen in der du dann die Daten in das Long Format kopierst.\nWir können aber mit einem Wide-Format nicht mit ggplot() die Daten aus der Tabelle 10.1 visualisieren. Deshalb müssen wir entweder das Wide Format in das Long Format umwandeln oder die Daten gleich in Excel im Long Format erstellen.\n\n\n10.2.2 Long Format\nWenn du Daten erstellst ist es wichtig, dass du die Daten in Excel im Long-Format erstellst. Dabei muss eine Beobachtung eine Zeile sein. Du siehst in Abbildung 10.1 ein Beispiel für eine Tabelle in Excel, die dem Long Format folgt.\n\n\n\n\n\n\nAbbildung 10.1— Beispiel für eine Exceldatentabelle in Long Format.\n\n\n\nIm Folgenden sehen wir einmal wie die Funktion gather() das tibble() in Wide Format in ein tibble() in Long Format umwandelt. Wir müssen dafür noch die Spalte benennen mit der Option key = in die die Namen der Spalten aus dem Wide Format geschrieben werden sowie den Spaltennamen für die eigentlichen Messwerte mit der Option value =.\n\njump_tbl &lt;- tibble(dog = c(5.2, 4.9, 12.1, 8.2, 5.6, 9.1, 7.4),\n                   cat = c(10.1, 9.4, 11.8, 6.7, 8.2, 9.1, 7.1)) |&gt;\n  gather(key = \"animal\", value = \"jump_length\")\njump_tbl\n\n# A tibble: 14 × 2\n   animal jump_length\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 dog            5.2\n 2 dog            4.9\n 3 dog           12.1\n 4 dog            8.2\n 5 dog            5.6\n 6 dog            9.1\n 7 dog            7.4\n 8 cat           10.1\n 9 cat            9.4\n10 cat           11.8\n11 cat            6.7\n12 cat            8.2\n13 cat            9.1\n14 cat            7.1\n\n\nWir sehen, dass ein Long Format viel mehr Paltz benötigt. Das ist aber in R kein Problem. Wir sehen die Daten kaum sondern nutzen Funktionen wie ggplot() um die Daten zu visualisieren. Wichtig ist, dass du die Daten in Excel sauber abgelegt hast.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Daten einlesen</span>"
    ]
  },
  {
    "objectID": "programing-import.html#beispiel-in-excel",
    "href": "programing-import.html#beispiel-in-excel",
    "title": "10  Daten einlesen",
    "section": "10.3 Beispiel in Excel…",
    "text": "10.3 Beispiel in Excel…\nSchauen wir uns den Fall nochmal als Beispiel in einer Exceldatei an. Du findest die Beispieldatei germination_data.xlsx auf GitHub zum Herunterladen. Eventuell muss du bei dir den Pfad ändern oder aber die Importfunktion des RStudios nutzen. Dafür siehe einfach den nächsten Abschnitt.\nWir haben in der Beispieldatei germination_data.xlsx zum einen Messwiederholungen, gekenntzeichnet durch die Spalten t1 bis t4 sowie einmal gemessene Spalten wie freshmatter, drymatter, count_small_leaf und count_large_leaf.\n\n10.3.1 …ohne Messwiederholung\nWir schauen uns erstmal die Spalten ohne Messwiederholung an. Wenn du also keine Messwiederholungen hast, also die hast das Frischegewicht nur einmal an einer Pflanze gemessen, dann sieht deine Datei so aus wie in Abbildung 10.2. Ich zeige hier nur die Spalten A und F bis I aus der Datei germination_data.xlsx.\n\n\n\n\n\n\nAbbildung 10.2— Beispiel für eine Exceldatentabelle ohne Messwiederholungen.\n\n\n\nWir können dann die Datei auch über die Funktion read_excel() einlesen. Ich nutze noch die Funktion select() um die Spalten auszuwählen, die wir auch oben in der Abbildung sehen. Durch den Doppelpunkt : kann ich zusammenhängende Spalten auswählen und muss die Namen nicht einzeln eingeben.\n\nread_excel(\"data/germination_data.xlsx\") |&gt; \n  select(treatment, freshmatter:count_large_leaf)\n\n# A tibble: 20 × 5\n   treatment freshmatter drymatter count_small_leaf count_large_leaf\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 control          13.7      0.98                2               12\n 2 control          18.1      1.31                4               12\n 3 control          14.4      1.01                4               12\n 4 control          10.7      0.74                2               12\n 5 control          15.9      1.11                2               12\n 6 low              26.4      1.9                10               18\n 7 low              24.3      1.68                8               14\n 8 low              27.1      1.87                6               18\n 9 low              27.2      1.8                 4               18\n10 low              18.2      1.22                6               18\n11 mid              20.9      1.31                6                8\n12 mid              19.4      1.41                4                8\n13 mid              21.5      1.44                4               14\n14 mid              24        1.56                6               11\n15 mid              25.8      1.77                6               11\n16 high             30.9      2.22                6               24\n17 high             36.3      2.52                2               19\n18 high             25.6      1.82                4               22\n19 high             33.3      2.27                6               22\n20 high             30.4      2.14                6               22\n\n\nWir könnten jetzt die Ausgabe auch in ein Objekt schreiben und dann mit der eingelesenen Datei weiterarbeiten.\n\n\n10.3.2 … mit Messwiederholung\nIn Abbildung 10.3 siehst du ein Datenbeispiel für eine Behandlung mit Messwiederholungen. Das heist wir haben immer noch eine Pflanze pro Zeile, aber die Pflanze wurde zu den Zeitpunkten t1 bis t4 gemessen. In dieser Form ist es viel einfacher aufzuschreiben, aber wir brauchen einen Faktor time_point mit vier Leveln t1 bis t4.\n\n\n\n\n\n\nAbbildung 10.3— Beispiel für eine Exceldatentabelle mit Messwiederholungen.\n\n\n\nWir nutzen die Funktion pivot_longer() um die Spalten t1 bis t4 zusammenzufassen und untereinander zu kleben. Die Spalte treatment wird dann einfach viermal wiederholt. Wir müssen dann noch die Spalte für den Faktor benennen und die Spalte für die eigentlichen Messwerte. Beides machen wir einmal über die Option names_to = und values_to =. Wir haben dann im Anschluss einen Datensatz im Long-Format mit dem wir dann weiterarbeiten können.\n\nread_excel(\"data/germination_data.xlsx\") |&gt; \n  select(treatment, t1:t4) |&gt; \n  pivot_longer(cols = t1:t4, \n               names_to = \"time_point\", \n               values_to = \"weight\")\n\n# A tibble: 80 × 3\n   treatment time_point weight\n   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n 1 control   t1             16\n 2 control   t2             21\n 3 control   t3             23\n 4 control   t4             23\n 5 control   t1             17\n 6 control   t2             19\n 7 control   t3             18\n 8 control   t4             24\n 9 control   t1             16\n10 control   t2             22\n# ℹ 70 more rows\n\n\nWir könnten jetzt die Ausgabe auch in ein Objekt schreiben und dann mit der eingelesenen Datei weiterarbeiten.\n\n\n10.3.3 … mit mehreren Tabellenblättern\nWenn du eine Datei mit mehreren Tabellenblättern hast, dann geht das Einlesen der Datei auch, aber dann müssen die Tabellenblätter wirklich alle für R einlesbar sein. Das heist keine Leerzeilen oder andere Dinge, die stören könnten. Als erstes musst du den Pfad zu deiner Datei angeben. Das kann ganz einfach sein, wenn die Datei in dem gleichen Ordner wie dein R Skript liegt. Dann ist der Pfad wirklich nur ein Punkt in Anführungszeichen path &lt;- \".\". Sicher ist natürlich du gibst den Pfad absolut ein. Hier einmal wie der Pfad in meinem Fall aussehen würde.\n\npath &lt;- file.path(\"data/multiple_sheets.xlsx\")\npath\n\n[1] \"data/multiple_sheets.xlsx\"\n\n\nWir können dann den Pfad zu der Ecxeldatei an die Funktion excel_sheets() pipen, die alle Tabellenblätter in der Datei findet. Dann müssen wir noch die Funktion set_names() verwenden um die Namen der Tabellenblätter zu recyclen. Abschließend können wir alle Exceltabellenblätter in eine Liste laden.\n\ndata_lst &lt;- path |&gt; \n  excel_sheets() |&gt; \n  rlang::set_names() |&gt; \n  map(read_excel, path = path)\ndata_lst\n\n$day_1\n# A tibble: 9 × 3\n  animal jump_length infected\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 dog            8.9        1\n2 dog           11.8        1\n3 dog            8.2        0\n4 cat            4.3        1\n5 cat            7.9        0\n6 cat            6.1        0\n7 fox            7.7        1\n8 fox            8.1        1\n9 fox            9.1        1\n\n$day_3\n# A tibble: 8 × 5\n  animal jump_length flea_count grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 cat            3.2         12     7        1\n2 cat            2.2         13     5        0\n3 cat            5.4         11     7        0\n4 cat            4.1         12     6        0\n5 fox            9.7         12     5        1\n6 fox           10.6         28     4        0\n7 fox            8.6         18     4        1\n8 fox           10.3         19     3        0\n\n$day12\n# A tibble: 4 × 3\n  animal jump_length flea_count\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 dog            5.7         18\n2 dog            8.9         22\n3 dog           11.8         17\n4 dog            8.2         12\n\n\nDas ist ja schonmal gut, aber wie kommen wir jetzt an die einzelnen Tabellenblätter ran? Dafür gibt es die Funktion pluck(), die es erlaubt aus einer Liste nach Namen oder Position das Tabellenblatt herauszuziehen. Wir können dann das Tabellenblatt wieder in einem Objekt speichern und dann weiter mit den Daten arbeiten.\n\npluck(data_lst, \"day_3\")\n\n# A tibble: 8 × 5\n  animal jump_length flea_count grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 cat            3.2         12     7        1\n2 cat            2.2         13     5        0\n3 cat            5.4         11     7        0\n4 cat            4.1         12     6        0\n5 fox            9.7         12     5        1\n6 fox           10.6         28     4        0\n7 fox            8.6         18     4        1\n8 fox           10.3         19     3        0\n\n\n\npluck(data_lst, 2)\n\n# A tibble: 8 × 5\n  animal jump_length flea_count grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 cat            3.2         12     7        1\n2 cat            2.2         13     5        0\n3 cat            5.4         11     7        0\n4 cat            4.1         12     6        0\n5 fox            9.7         12     5        1\n6 fox           10.6         28     4        0\n7 fox            8.6         18     4        1\n8 fox           10.3         19     3        0\n\n\nUnd dann kannst du dir noch über die Funktion ldply() einfach einen Datensatz aus den Listen zusammenbauen. Ich nutze danach noch die Funktion as_tibble() um eine bessere Ausgabe der Daten zu haben. Das Schöne ist hier, dass mir dann noch eine Spalte mit der .id der Listennamen generiert wird. Da habe ich dann wirklich alles zusammen.\n\ndata_lst |&gt; \n  ldply() |&gt; \n  as_tibble()\n\n# A tibble: 21 × 6\n   .id   animal jump_length infected flea_count grade\n   &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 day_1 dog            8.9        1         NA    NA\n 2 day_1 dog           11.8        1         NA    NA\n 3 day_1 dog            8.2        0         NA    NA\n 4 day_1 cat            4.3        1         NA    NA\n 5 day_1 cat            7.9        0         NA    NA\n 6 day_1 cat            6.1        0         NA    NA\n 7 day_1 fox            7.7        1         NA    NA\n 8 day_1 fox            8.1        1         NA    NA\n 9 day_1 fox            9.1        1         NA    NA\n10 day_3 cat            3.2        1         12     7\n# ℹ 11 more rows",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Daten einlesen</span>"
    ]
  },
  {
    "objectID": "programing-import.html#importieren-mit-rstudio",
    "href": "programing-import.html#importieren-mit-rstudio",
    "title": "10  Daten einlesen",
    "section": "10.4 Importieren mit RStudio",
    "text": "10.4 Importieren mit RStudio\nWir können das RStudio nutzen um Daten mit Point-and-Klick rein zuladen und dann den Code wieder in den Editor kopieren. Im Prinzip ist dieser Weg der einfachste um einmal zu sehen, wie ein pfad funktioniert und der Code lautet. Später benötigt man diese ‘Krücke’ nicht mehr. Wir nutzen dann direkt den Pfad zu der Datei. Abbildung 10.4 zeigt einen Ausschnitt, wo wir im RStudio die Import Dataset Funktionalität finden.\n\n\n\n\n\n\nAbbildung 10.4— Auf den Reiter Einviroment klicken und dann Import Dataset. In der deutschen version vom RStudio mögen die Begriffe leicht anders sein.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Daten einlesen</span>"
    ]
  },
  {
    "objectID": "programing-import.html#sec-pfad",
    "href": "programing-import.html#sec-pfad",
    "title": "10  Daten einlesen",
    "section": "10.5 Importieren per Pfad",
    "text": "10.5 Importieren per Pfad\nIn Abbildung 10.5 können wir sehen wie wir den Pfad zu unserer Excel Datei flea_dog_cat.xlsx finden. Natürlich kannst du den Pfad auch anders herausfinden bzw. aus dem Explorer oder Finder kopieren.\n\n\n\n\n\n\nAbbildung 10.5— Durch den Rechts-Klick auf die Eigenschaften einer Datei kann man sich den Pfad zur Datei anzeigen lassen. Achtung! Unter Windows muss der Slash \\ noch in den Backslash / gedreht werden.\n\n\n\nNachdem wir den Pfad gefunden haben, können wir den Pfad in die Funktion read_excel() kopieren und die Datei in das Objekt data_tbl einlesen. Ja, es wird nichts in der R Console ausgegeben, da sich die Daten jetzt in dem Object data_tbl befinden.\n\n## Ganzer Pfad zur Datei flea_dog_cat.xlsx\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n\n\n\n\n\n\n\nUnterschied zwischen \\ in Windows und / in R\n\n\n\nAchte einmal auf den Slash im Pfad in R und einem im Pfsd in Windows. Einmal ist es der Slash \\ im Dateipfad und einmal der Backslash /. Das ist sehr ärgerlich, aber dieses Problem geht zurück in die 80’ziger. Bill hat entschieden für sein Windows / zu nutzen und Steve (und Unix) eben /. Und mit dieser Entscheidung müssen wir jetzt leben…",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Daten einlesen</span>"
    ]
  },
  {
    "objectID": "programing-import.html#sec-umlaute",
    "href": "programing-import.html#sec-umlaute",
    "title": "10  Daten einlesen",
    "section": "10.6 Auf ein englisches Wort in Dateien",
    "text": "10.6 Auf ein englisches Wort in Dateien\nEin großes Problem in Datein sind Umlaute (ä,ö,ü) oder aber andere (Sonder)zeichen (ß, ?, oder #). Als dies sollte vermieden werden. Eine gute Datei für R beinhaltet nur ganze Wörter, Zahlen oder aber leere Felder. Ein leeres Feld ist ein fehlender Wert. Abbildung 10.6 zeigt eine gute Exceldatentablle. Wir schreiben jump_length mit Unterstrich um den Namen besser zu lesen zu können. Sonst ist auch alles in Englisch geschrieben. Wir vermeiden durch die neglische Schreibweise aus versehen einen Umlaut oder anderweitig problematische Zeichen zu verwenden. Später können wir alles noch für Abbildungen anpassen.\n\n\n\n\n\n\nAbbildung 10.6— Beispiel für eine gute (Excel)Datentabelle. Keine Umlaute sind vorhanden und die Spaltennamen haben keine Leerzeichen oder Sonderzeichen.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Daten einlesen</span>"
    ]
  },
  {
    "objectID": "programing-import.html#sec-spalten",
    "href": "programing-import.html#sec-spalten",
    "title": "10  Daten einlesen",
    "section": "10.7 Spaltennamen in der (Excel)-Datei",
    "text": "10.7 Spaltennamen in der (Excel)-Datei\nDie Funktion clean_names() aus dem R Paket {janitor} erlaubt es die Spaltennamen einer eingelesenen Datei in eine für R gute Form zu bringen.\n\nKeine Leerzeichen in den Spaltennamen.\nAlle Spaltennamen sind klein geschrieben.\n\n\ndata_tbl |&gt; \n  clean_names()\n\n# A tibble: 14 × 6\n   animal jump_length flea_count weight grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            5.7         18    2.1     8        0\n 2 dog            8.9         22    2.3     8        1\n 3 dog           11.8         17    2.8     6        1\n 4 dog            5.6         12    2.4     8        0\n 5 dog            9.1         23    1.2     7        1\n 6 dog            8.2         18    4.1     7        0\n 7 dog            7.6         21    3.2     9        0\n 8 cat            3.2         12    1.1     7        1\n 9 cat            2.2         13    2.1     5        0\n10 cat            5.4         11    2.4     7        0\n11 cat            4.1         12    2.1     6        0\n12 cat            4.3         16    1.5     6        1\n13 cat            7.9          9    3.7     6        0\n14 cat            6.1          7    2.9     5        0\n\n\n\n\n\nAbbildung 10.1— Beispiel für eine Exceldatentabelle in Long Format.\nAbbildung 10.2— Beispiel für eine Exceldatentabelle ohne Messwiederholungen.\nAbbildung 10.3— Beispiel für eine Exceldatentabelle mit Messwiederholungen.\nAbbildung 10.4— Auf den Reiter Einviroment klicken und dann Import Dataset. In der deutschen version vom RStudio mögen die Begriffe leicht anders sein.\nAbbildung 10.5— Durch den Rechts-Klick auf die Eigenschaften einer Datei kann man sich den Pfad zur Datei anzeigen lassen. Achtung! Unter Windows muss der Slash \\ noch in den Backslash / gedreht werden.\nAbbildung 10.6— Beispiel für eine gute (Excel)Datentabelle. Keine Umlaute sind vorhanden und die Spaltennamen haben keine Leerzeichen oder Sonderzeichen.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Daten einlesen</span>"
    ]
  },
  {
    "objectID": "programing-dplyr.html",
    "href": "programing-dplyr.html",
    "title": "11  Daten bearbeiten",
    "section": "",
    "text": "11.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, readxl, magrittr, janitor)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Daten bearbeiten</span>"
    ]
  },
  {
    "objectID": "programing-dplyr.html#spalten-wählen-mit-select",
    "href": "programing-dplyr.html#spalten-wählen-mit-select",
    "title": "11  Daten bearbeiten",
    "section": "11.2 Spalten wählen mit select()",
    "text": "11.2 Spalten wählen mit select()\n\nDer Datensatz, den wir im Experiment erschaffen, ist meist riesig. Jetzt könnten wir natürlich eine Exceltabelle mit unterschiedlichen Sheets bzw. Reitern erstellen oder aber die Spalten die wir brauchen in R selektieren. Wir nutzen die Funktion select()um Spalten zu wählen. Im folgenden Codeblock wählen wir die Spalten animal, jump_length und flea_count.\n\ndata_tbl |&gt; \n  select(animal, jump_length, flea_count)\n\n# A tibble: 21 × 3\n   animal jump_length flea_count\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 dog            5.7         18\n 2 dog            8.9         22\n 3 dog           11.8         17\n 4 dog            5.6         12\n 5 dog            9.1         23\n 6 dog            8.2         18\n 7 dog            7.6         21\n 8 cat            3.2         12\n 9 cat            2.2         13\n10 cat            5.4         11\n# ℹ 11 more rows\n\n\nWir können die Spalten beim selektieren auch umbenennen und in eine andere Reihenfolge bringen.\n\ndata_tbl |&gt; \n  select(Sprungweite = jump_length, flea_count, animal)\n\n# A tibble: 21 × 3\n   Sprungweite flea_count animal\n         &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; \n 1         5.7         18 dog   \n 2         8.9         22 dog   \n 3        11.8         17 dog   \n 4         5.6         12 dog   \n 5         9.1         23 dog   \n 6         8.2         18 dog   \n 7         7.6         21 dog   \n 8         3.2         12 cat   \n 9         2.2         13 cat   \n10         5.4         11 cat   \n# ℹ 11 more rows\n\n\nDu findest auf der englischen Hilfeseite für select() noch weitere Beispiele für die Nutzung.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Daten bearbeiten</span>"
    ]
  },
  {
    "objectID": "programing-dplyr.html#zeilen-wählen-mit-filter",
    "href": "programing-dplyr.html#zeilen-wählen-mit-filter",
    "title": "11  Daten bearbeiten",
    "section": "11.3 Zeilen wählen mit filter()",
    "text": "11.3 Zeilen wählen mit filter()\n\nWährend wir die Auswahl an Spalten gut und gerne auch in Excel durchführen können, so ist dies bei der Auswahl der Zeilen nicht so einfach. Wir können in R hier auf die Funktion filter() zurückgreifen. Wir nutzen die Funktion filter() um Zeilen nach Kriterien zu wählen.\nIm folgenden Codeblock wählen wir die Zeilen aus in denen die Worte dog und fox stehen. Wir nutzen dazu den Operator %in% um auszudrücken, dass wir alle Einträge in der Spalte animal wollen die in dem Vektor c(\"dog\", \"fox\") beschrieben sind.\n\ndata_tbl |&gt; \n  filter(animal %in% c(\"dog\", \"fox\"))\n\n# A tibble: 14 × 6\n   animal jump_length flea_count weight grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            5.7         18    2.1     8        0\n 2 dog            8.9         22    2.3     8        1\n 3 dog           11.8         17    2.8     6        1\n 4 dog            5.6         12    2.4     8        0\n 5 dog            9.1         23    1.2     7        1\n 6 dog            8.2         18    4.1     7        0\n 7 dog            7.6         21    3.2     9        0\n 8 fox            7.7         21    3.1     5        1\n 9 fox            8.1         25    4.2     4        1\n10 fox            9.1         31    5.1     4        1\n11 fox            9.7         12    3.5     5        1\n12 fox           10.6         28    3.2     4        0\n13 fox            8.6         18    4.6     4        1\n14 fox           10.3         19    3.7     3        0\n\n\nEs stehen dir Folgende logische Operatoren zu Verfügung wie in Tabelle 11.2 gezeigt. Am Anfang ist es immer etwas schwer sich in den logischen Operatoren zurechtzufinden. Daher kann ich dir nur den Tipp geben einmal die Operatoren selber auszuprobieren und zu schauen, was du da so raus filterst.\n\n\n\nTabelle 11.2— Logische Operatoren und R und deren Beschreibung\n\n\n\n\n\n\n\n\n\nLogischer Operator\nBeschreibung\n\n\n\n\n&lt;\nkleiner als (eng. less than)\n\n\n&lt;=\nkleiner als oder gleich (eng. less than or equal to)\n\n\n&gt;\ngrößer als (eng. greater than)\n\n\n&gt;=\ngrößer als oder gleich (eng. greater than or equal to)\n\n\n==\nexact gleich (eng. exactly equal to)\n\n\n!=\nnicht gleich (eng. not equal to)\n\n\n!x\nnicht (eng. not x)\n\n\nx | y\noder (eng. x or y)\n\n\nx & y\nund (eng. x and y)\n\n\n\n\n\n\nHier ein paar Beispiele. Probiere gerne auch mal Operatoren selber aus. Im folgenden Codeblock wollen wir nur die Zeilen haben, die eine Anzahl an Flöhen größer von 15 haben.\n\ndata_tbl |&gt; \n  filter(flea_count &gt; 15)\n\n# A tibble: 13 × 6\n   animal jump_length flea_count weight grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            5.7         18    2.1     8        0\n 2 dog            8.9         22    2.3     8        1\n 3 dog           11.8         17    2.8     6        1\n 4 dog            9.1         23    1.2     7        1\n 5 dog            8.2         18    4.1     7        0\n 6 dog            7.6         21    3.2     9        0\n 7 cat            4.3         16    1.5     6        1\n 8 fox            7.7         21    3.1     5        1\n 9 fox            8.1         25    4.2     4        1\n10 fox            9.1         31    5.1     4        1\n11 fox           10.6         28    3.2     4        0\n12 fox            8.6         18    4.6     4        1\n13 fox           10.3         19    3.7     3        0\n\n\nWir wollen nur die infizierten Tiere haben.\n\ndata_tbl |&gt; \n  filter(infected == TRUE)\n\n# A tibble: 10 × 6\n   animal jump_length flea_count weight grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            8.9         22    2.3     8        1\n 2 dog           11.8         17    2.8     6        1\n 3 dog            9.1         23    1.2     7        1\n 4 cat            3.2         12    1.1     7        1\n 5 cat            4.3         16    1.5     6        1\n 6 fox            7.7         21    3.1     5        1\n 7 fox            8.1         25    4.2     4        1\n 8 fox            9.1         31    5.1     4        1\n 9 fox            9.7         12    3.5     5        1\n10 fox            8.6         18    4.6     4        1\n\n\nWir wollen nur die infizierten Tiere haben UND die Tiere mit einer Flohanzahl größer als 20.\n\ndata_tbl |&gt; \n  filter(infected == TRUE & flea_count &gt; 20)\n\n# A tibble: 5 × 6\n  animal jump_length flea_count weight grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dog            8.9         22    2.3     8        1\n2 dog            9.1         23    1.2     7        1\n3 fox            7.7         21    3.1     5        1\n4 fox            8.1         25    4.2     4        1\n5 fox            9.1         31    5.1     4        1\n\n\nDu findest auf der englischen Hilfeseite für filter() noch weitere Beispiele für die Nutzung.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Daten bearbeiten</span>"
    ]
  },
  {
    "objectID": "programing-dplyr.html#spalten-ändern-mit-mutate",
    "href": "programing-dplyr.html#spalten-ändern-mit-mutate",
    "title": "11  Daten bearbeiten",
    "section": "11.4 Spalten ändern mit mutate()",
    "text": "11.4 Spalten ändern mit mutate()\n\nNachdem wir die Spalten mit select() udn eventuell die Zeieln mit filter() gewählt haben. müssen wir jetzt noch die Eigenschaften der Spalten ändern. Das Ändern müssen wir nicht immer tun, aber häufig müssen wir noch einen Faktor erschaffen. Wir nutzen noch die Funktion pull() um uns die Spalte animal aus dem Datensatz zu ziehen. Nur so sehen wir die vollen Eigenschaften des Faktors. Später nutzen wir pull seltener und nur um zu kontrollieren, was wir gemacht haben. Wir nutzen die Funktion mutate() um die Eigenschaften von Spalten daher Variablen zu ändern. Die Reihenfolge der Funktionen ist wichtig um unliebsame Effekte zu vermeiden.\n\nErst wählen wir die Spalten mit select()\nDann filtern wir die Zeilen mit filter()\nAbschließend ändern wir die Eigenschaften der Spalten mit mutate()\n\nIm folgenden Codeblock verwandeln wir die Variable animal in einen Faktor durch die Funktion as_factor. Wir sehen, dass die Level des Faktors so sortiert sind, wie das Auftreten in der Spalte animal.\n\ndata_tbl |&gt; \n  mutate(animal = as_factor(animal)) |&gt; \n  pull(animal)\n\n [1] dog dog dog dog dog dog dog cat cat cat cat cat cat cat fox fox fox fox fox\n[20] fox fox\nLevels: dog cat fox\n\n\nWollen wir die Sortierung der Level ändern, können wir die Funktion factor() nutzen. Wir ändern die Sortierung des Faktors zu fox, dog und cat.\n\ndata_tbl |&gt; \n  mutate(animal = factor(animal, levels = c(\"fox\", \"dog\", \"cat\"))) |&gt; \n  pull(animal)\n\n [1] dog dog dog dog dog dog dog cat cat cat cat cat cat cat fox fox fox fox fox\n[20] fox fox\nLevels: fox dog cat\n\n\nWir können auch die Namen (eng. labels) der Level ändern. Hier musst du nur aufpassen wie du die alten Labels überschreibst. Wenn ich gleichzeitig die Level und die Labels ändere komme ich häufig durcheinander. Da muss du eventuell nochmal schauen, ob auch alles so geklappt hat wie du wolltest.\n\ndata_tbl |&gt; \n  mutate(animal = factor(animal, labels = c(\"Hund\", \"Katze\", \"Fuchs\"))) |&gt; \n  pull(animal)\n\n [1] Katze Katze Katze Katze Katze Katze Katze Hund  Hund  Hund  Hund  Hund \n[13] Hund  Hund  Fuchs Fuchs Fuchs Fuchs Fuchs Fuchs Fuchs\nLevels: Hund Katze Fuchs\n\n\nDu findest auf der englischen Hilfeseite für mutate() noch weitere Beispiele für die Nutzung. Insbesondere die Nutzung von mutate() über mehrere Spalten gleichzeitig erlaubt sehr effiezientes Programmieren. Aber das ist für den Anfang etwas viel.\n\n\n\n\n\n\nDie Funktionen select(), filter() und mutate() in R\n\n\n\nBitte schaue dir auch die Hilfeseiten der Funktionen an. In diesem Skript kann ich nicht alle Funktionalitäten der Funktionen zeigen. Oder du kommst in das R Tutorium welches ich anbiete und fragst dort nach den Möglichkeiten Daten in R zu verändern.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Daten bearbeiten</span>"
    ]
  },
  {
    "objectID": "programing-dplyr.html#gruppieren-mit-group_by",
    "href": "programing-dplyr.html#gruppieren-mit-group_by",
    "title": "11  Daten bearbeiten",
    "section": "11.5 Gruppieren mit group_by()",
    "text": "11.5 Gruppieren mit group_by()\nSobald wir einen Faktor erschaffen haben, können wir die Daten in R auch nach dem Faktor gruppieren. Das heißt wir nutzen die Funktion group_by() um R mitzuteilen, dass nun folgende Funktionen getrennt für die einzelen Gruppen erfolgen sollen. Im folgenden Codeblock siehst du die Anwendung.\n\ndata_tbl |&gt; \n  mutate(animal = as_factor(animal)) |&gt; \n  group_by(animal)\n\n# A tibble: 21 × 6\n# Groups:   animal [3]\n   animal jump_length flea_count weight grade infected\n   &lt;fct&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            5.7         18    2.1     8        0\n 2 dog            8.9         22    2.3     8        1\n 3 dog           11.8         17    2.8     6        1\n 4 dog            5.6         12    2.4     8        0\n 5 dog            9.1         23    1.2     7        1\n 6 dog            8.2         18    4.1     7        0\n 7 dog            7.6         21    3.2     9        0\n 8 cat            3.2         12    1.1     7        1\n 9 cat            2.2         13    2.1     5        0\n10 cat            5.4         11    2.4     7        0\n# ℹ 11 more rows\n\n\nAuf den ersten Blick ändert sich nicht viel. Es entsteht aber die Zeile # Groups: animal [3]. Wir wissen nun, dass wir nach der Variable animal mit drei Gruppen die Datentabelle gruppiert haben.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Daten bearbeiten</span>"
    ]
  },
  {
    "objectID": "programing-dplyr.html#mehr-informationen-durch-glimpse-und-str",
    "href": "programing-dplyr.html#mehr-informationen-durch-glimpse-und-str",
    "title": "11  Daten bearbeiten",
    "section": "11.6 Mehr Informationen durch glimpse() und str()",
    "text": "11.6 Mehr Informationen durch glimpse() und str()\nAm Ende noch zwei Funktionen zur Kontrolle, was wir hier eigentlich gerade tun. Mit der Funktion glimpse() können wir uns einen Einblick in die Daten geben lassen. Wir sehen dann nochmal kurz und knapp wieviel Zeieln und Spalten wir haben und welche Inhalte in den Spalten stehen. Die gleichen Informationen erhalten wir auch durch die Funktion str(). Die Funktion str()geht aber noch einen Schritt weiter und nennt uns auch Informationen zu dem Objekt. Daher wir wissen jetzt, dass es sich beim dem Objekt data_tbl um ein tibble() handelt.\n\nglimpse(data_tbl)\n\nRows: 21\nColumns: 6\n$ animal      &lt;chr&gt; \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"cat\", \"c…\n$ jump_length &lt;dbl&gt; 5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6, 3.2, 2.2, 5.4, 4.1, 4.…\n$ flea_count  &lt;dbl&gt; 18, 22, 17, 12, 23, 18, 21, 12, 13, 11, 12, 16, 9, 7, 21, …\n$ weight      &lt;dbl&gt; 2.1, 2.3, 2.8, 2.4, 1.2, 4.1, 3.2, 1.1, 2.1, 2.4, 2.1, 1.5…\n$ grade       &lt;dbl&gt; 8, 8, 6, 8, 7, 7, 9, 7, 5, 7, 6, 6, 6, 5, 5, 4, 4, 5, 4, 4…\n$ infected    &lt;dbl&gt; 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1…\n\nstr(data_tbl)\n\ntibble [21 × 6] (S3: tbl_df/tbl/data.frame)\n $ animal     : chr [1:21] \"dog\" \"dog\" \"dog\" \"dog\" ...\n $ jump_length: num [1:21] 5.7 8.9 11.8 5.6 9.1 8.2 7.6 3.2 2.2 5.4 ...\n $ flea_count : num [1:21] 18 22 17 12 23 18 21 12 13 11 ...\n $ weight     : num [1:21] 2.1 2.3 2.8 2.4 1.2 4.1 3.2 1.1 2.1 2.4 ...\n $ grade      : num [1:21] 8 8 6 8 7 7 9 7 5 7 ...\n $ infected   : num [1:21] 0 1 1 0 1 0 0 1 0 0 ...",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Daten bearbeiten</span>"
    ]
  },
  {
    "objectID": "programing-strings.html",
    "href": "programing-strings.html",
    "title": "12  Reguläre Ausdrücke",
    "section": "",
    "text": "12.1 Das R Paket {stringr}\nDas R Paket {stringr} und das Cheat Sheet zu {stringr} geben eine große Übersicht über die Möglichkeiten ein character zu bearbeiten. Im Folgenden schauen wir uns einmal einen simplen Datensatz an. Wir wollen auf dem Datensatz ein paar Funktionen aus dem R Paket {stringr} anwenden.\nregex_tbl &lt;- tibble(animal = c(\"cat\", \"cat\", \"dog\", \"bird\"),\n                    site = c(\"village\", \"village\", \"town\", \"cities\"),\n                    time = c(\"t1_1\", \"t2_2\", \"t3_3\", \"t3_5\"))\nDie einfachste und am meisten genutzte Funktion ist str_c(). Die Funktion str_c() klebt verschiedene Vektoren zusammen. Wir können auch Zeichen wählen, wie das -, um die Vektoren zu verbinden. Wir bauen uns also eine neue Spalte animal_site in dem wir die Spalten animal und site mit einem - verbinden. Wir können statt dem - auch ein beliebiges anderes Zeichen oder auch Wort nehmen.\nregex_tbl |&gt; \n  mutate(animal_site = str_c(animal, \"-\", site))\n\n# A tibble: 4 × 4\n  animal site    time  animal_site\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      \n1 cat    village t1_1  cat-village\n2 cat    village t2_2  cat-village\n3 dog    town    t3_3  dog-town   \n4 bird   cities  t3_5  bird-cities\nHäufig brauchen wir auch eine ID Variable, die exakt \\(n\\) Zeichen lang ist. Hier können wir die Funktion str_pad() nutzen um Worte auf die Zeichenlänge width = zu verlängern. Wir können auch das Zeichen wählen, was angeklebt wird und die Seite des Wortes wählen an die wir kleben wollen. Wir verlängern also links die Spalte site auf ein Wort mit acht Zeichen und als Füllzeichen nehmen wir die Null.\nregex_tbl |&gt; \n  mutate(village_pad = str_pad(site, pad = \"0\", \n                               width = 8, side = \"left\"))\n\n# A tibble: 4 × 4\n  animal site    time  village_pad\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      \n1 cat    village t1_1  0village   \n2 cat    village t2_2  0village   \n3 dog    town    t3_3  0000town   \n4 bird   cities  t3_5  00cities\nAbschließend können wir auch eine Spalte in zwei Spalten aufteilen. Dafür müssen wir den Separator wählen nachdem die Spalte aufgetrennt werden soll. Wir können eine Spalte auch in mehrere Spalten aufteilen, wenn der Separator eben an zwei oder mehr Stellen steht. Wir haben die Spalte time und trennen die Spalte time an der Stelle _ in zwei Spalten auf.\nregex_tbl |&gt; \n  separate(time, into = c(\"time\", \"rep\"), \n           sep = \"_\", convert = TRUE)\n\n# A tibble: 4 × 4\n  animal site    time    rep\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n1 cat    village t1        1\n2 cat    village t2        2\n3 dog    town    t3        3\n4 bird   cities  t3        5\nEs gibt noch sehr viel mehr Möglichkeiten einen character Vektor zu bearbeiten. Teilweise nutze ich stringr bei der Auswertung von den Beispielen im Anhang. Schau dir da mal um, dort wirst du immer mal wieder die Funktionen aus dem Paket finden.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reguläre Ausdrücke</span>"
    ]
  },
  {
    "objectID": "programing-strings.html#sec-regex",
    "href": "programing-strings.html#sec-regex",
    "title": "12  Reguläre Ausdrücke",
    "section": "12.2 Regular expressions",
    "text": "12.2 Regular expressions\nEin regulärer Ausdruck (eng. regular expression, abk. RegExp oder Regex) ist eine verworrene Zeichenkette, die einer Maschine ein Muster übersetzt. Dieses Muster soll dann auch alle Wörter angewendet werden. Das klingt kryptisch und ist es auch. Reguläre Ausdrücke sind das Nerdthema schlechthin in der Programmierung. Also tauchen wir mal ab in die Welt der seht nützlichen und mächtigen regulären Ausdrücke.\n\n\nWir immer gibt es schöne Tutorials. Einmal Regular expressions in stingr und dann von Hadley Wickham ein Kapitel zu Regular expressions.\nFangen wir mit der grundsätzlichen Idee an. Reguläre Ausdrücke finden bestimmte Zeichen in Wörtern. Ich habe dir hier einmal eine winzige Auswahl an regulären Ausdrücken mitgebracht. Du kannst auch reguläre Ausdrück miteinander verknüpfen. Daher kommt die eigentlich Macht eines regulären Ausdruck. Aber später mehr dazu.\n\n\\d: entspricht einer beliebigen Ziffer. Wir finden also eine Ziffer oder Zahl in einem Wort.\n\\s: entspricht einem beliebigen Leerzeichen (z. B. Leerzeichen, Tabulator, Zeilenumbruch). Wir finden also ein Leerzeichen in einem String.\n[abc]: passt auf a, b oder c. Das ist jetzt wörtlich gemeint. Wir finden die Buchstaben a, b oder c.\n[^abc]: passt auf alles außer a, b oder c. Das ist jetzt ebenfalls wörtlich gemeint.\n\nDenk daran, dass du, um einen regulären Ausdruck zu erstellen, der \\d oder \\s enthält, das \\ für die Zeichenkette ausschließen musst, also gib \\\\d oder \\\\s ein. Das ist eien Besonderheit in R. Wir müssen in R immer ein doppeltes \\\\ schreiben.\nEs gibt eine große Auswahl an möglichen regulären Ausdrücken. Ich nutze meist dann noch ein Cheat Sheet um den Überblick zu bewahren. Aber wie schon oben geschrieben, reguläre Ausdrücke braucht man meist erst, wenn die Daten so große werden, dass wir die Daten nicht mehr händisch bearbeiten können.\n\n\nIch selber nutze immer das Regular Expressions Cheat Sheet by DaveChild um den Überblick zu bewahren. Es ist dann auch einfach zu viel zu merken.\nWir können wieder das R Paket {stringr} nutzen um die regulären Ausdrück in R anzuwenden. Beginnen wir erstmal mit den einfachen Funktionen und arbeiten uns dann vor. Bitte beachte auch, dass du in der Funktion select() auch Helferfunktionen nutzen kannst, die dir das Leben wirklich einfacher machen. Auf diese Helferfunktionen gehen wir später nochmal ein.\n\nstr_detect() gibt TRUE/FALSE wieder, wenn das Wort eine Zeichenkette enthält.\nstr_subset() gibt die Werte wieder in denen die Zeichenkette erkannt wird.\nstr_replace() ersetzt das erste Auftreten der Zeichenkette durch eine andere Zeichenkette. Die Funktion str_replace_all() ersetzt dann jedes Auftreten der Zeichenkette.\n\nFangen wir simple an, wir wollen Wörter finden, die eine Zeichenkette enthalten. Dafür nutzen wir die Funktion str_detect().\n\nc(\"dog\", \"small-dog\", \"doggy\", \"cat\", \"kitty\") |&gt; \n  str_detect(\"dog\")\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE\n\n\nOkay, das hat ja schon mal gut funktioniert. Wenn wir die Worte wiederhaben wollen, dann können wir auch die Funktion str_subset() nutzen. Wir wollen jetzt aber nur die Einträge, die mit der Zeichenkette dog anfangen. Deshalb schreiben wir ein ^ vor die Zeichenkette. Wenn wir nur die Einträge gewollt hätten, die mit dog enden, dann hätten wir dog$ geschrieben. Das $ schaut dann von hinten in die Wörter.\n\nc(\"dog\", \"small-dog\", \"doggy\", \"cat\", \"kitty\") |&gt; \n  str_subset(\"^dog\")\n\n[1] \"dog\"   \"doggy\"\n\n\nNun haben wir uns verschrieben und wollen das small entfernen und mit large ersetzen. Statt large hätten wir auch nichts \"\" hinschreiben können. Dafür können wir die Funktion str_replace() nutzen. Die Funktion entfernt das erste Auftreten einer Zeichenkette. Wenn du alle Zeichenketten entfernen willst, dann musst du die Funktion str_replace_all() verwenden.\n\nc(\"dog\", \"small-dog\", \"doggy\", \"cat\", \"kitty\") |&gt; \n  str_replace(\"small\", \"large\")\n\n[1] \"dog\"       \"large-dog\" \"doggy\"     \"cat\"       \"kitty\"    \n\n\nDamit haben wir die wichtigsten drei Funktionen einmal erklärt. Wir werden dann diese Funktionen immer mal wieder anwenden und dann kannst du sehen, wie die regulären Ausdrücke in den Funktionen funktionieren. Auf der Hilfeseite von stringr gibt es nochmal ein Tutorium zu Regular expressions, wenn du hier mehr erfahren möchtest.\nViel häufiger nutzen wir die Helferfunktionen in select(). Wir haben hier eine große Auswahl, die uns das selektieren von Spalten sehr erleichtert. Im Folgenden einmal die Liste alle möglichen Funktionen. Beachte auch dabei folgende weitere Funktionen im Overview of selection features.\n\nmatches(), wählt alle Spalten die eine Zeichenkette enthalten. Hier können wir reguläre Ausdrücke verwenden.\nall_of(), ist für die strenge Auswahl. Wenn eine der Variablen im Zeichenvektor fehlt, wird ein Fehler ausgegeben.\nany_of(), prüft nicht auf fehlende Variablen. Dies ist besonders nützlich bei der negativen Selektionen, wenn du sicherstellen möchtest, dass eine Variable entfernt wird.\ncontains(), wählt alle Spalten die eine Zeichenkette enthalten. Funktioniert nicht mit regulären Ausdrücken.\nstarts_with(), wählt alle Spalten die mit einer Zeichenkette beginnt. Funktioniert nicht mit regulären Ausdrücken.\nends_with(), wählt alle Spalten die mit einer Zeichenkette endet. Funktioniert nicht mit regulären Ausdrücken.\neverything(), sortiert die restlichen Spalten hinten an. Wir können uns also die wichtigen Spalten namentlich nach vorne holen und dann den Rest mit everything() hinten an kleben.\nlast_col(), wählt die letzte Spalte aus. Besonders wichtig, wenn wir von einer Spalte bis zur letzten Spalte auswählen wollen.\nnum_range(), können wir nutzen, wenn wir über eine Zahl eine Spalte wählen wollen. Das heißt, unsere Spalten haben Zahlen in den Namen und wir könne darüber dann die Spalten wählen.\n\nHier einmal das Beispiel mit matches() wo wir alle Spalten nehmen in denen ein _ als Unterstrich vorkommt.\n\ndata_tbl |&gt; \n  select(matches(\"_\"))\n\n# A tibble: 3 × 2\n  jump_length flea_count\n        &lt;dbl&gt;      &lt;dbl&gt;\n1         5.7         18\n2         8.9         22\n3        11.8         17\n\n\nWir können auch über einen Vektor die Spalten auswählen. Das ist meistens nötig, wenn wir die Namen der Spalten zum Beispiel aus einer anderen Funktion erhalten. Dafür können wir dann die Funktion one_of() nutzen.\n\ndata_tbl |&gt; \n  select(one_of(\"animal\", \"grade\"))\n\n# A tibble: 3 × 2\n  animal grade\n  &lt;chr&gt;  &lt;dbl&gt;\n1 dog        8\n2 dog        8\n3 dog        6\n\n\nEs gibt noch eine Menge anderer Tools zum Nutzen von Regulären Ausdrücken. Aber hier soll es erstmal reichen. Wir gehen dann später in der Anwendung immer mal wieder auf die Funktionen hier ein.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reguläre Ausdrücke</span>"
    ]
  },
  {
    "objectID": "programing-strings.html#sec-time-date",
    "href": "programing-strings.html#sec-time-date",
    "title": "12  Reguläre Ausdrücke",
    "section": "12.3 Zeit und Datum",
    "text": "12.3 Zeit und Datum\nDie Arbeit mit Datumsdaten in R kann frustrierend sein. Die R Befehle für Datumszeiten sind im Allgemeinen nicht intuitiv und ändern sich je nach Art des verwendeten Datumsobjekts. Hier gibt es aber eine Lösung mit dem R Paket {lubridate}, die uns die Arbeit etwas erleichtert.\n\n\n\n\n\n\n\nQuelle: https://xkcd.com/\n\n\nDer beste Tipp ist eigentlich immer, das Datum in ein Format zu bringen und dann dieses Format weiterzuverarbeiten.\n\nWandle immer deine Datumsspalte in eine character Spalte mit as.character() um. Ein Datum besteht immer aus dem Tag, dem Monat und dem Jahr. Dabei ist wichtig, dass der Monat und der Tag immer zweistellig sind. Manchmal muss man dann über str_split() und str_pad() erstmal echt sich einen Wolf splitten und kleben, bis dann das Format so passt. Dann geht es meistens wie folgt weiter.\nVersuche dann mit der Funktion as_date() eine Datumspalte zu erschaffen. Häufig erkennt die Funktion die Spalte richtig und schneidet das Datum korrekt in Jahr/Monat/Tag. Manchmal klappt das aber auch nicht. Dann müssen wir uns weiter strecken.\nWenn die Umwandlung mit as_date() nicht klappt, musst du nochmal über parse_date_time() gehen und angeben, wie dein Datum formatiert ist.\n\n\n\nAuf der Hilfeseite der Funktion parse_date_time() erfährst du dann mehr über User friendly date-time parsing functions\nSchauen wir uns einmal ein Beispiel für Daten an. Ich habe hier die Daten von Deutschen Wetterdienst runtergeladen und möchte die Spalte jjjjmmdd in ein Datum in R umwandeln.\n\ntime_tbl &lt;- read_table(\"data/day_values_osnabrueck.txt\") |&gt; \n  clean_names() |&gt; \n  select(jjjjmmdd) |&gt; \n  print(n = 3)\n\n# A tibble: 501 × 1\n  jjjjmmdd\n     &lt;dbl&gt;\n1 20221030\n2 20221029\n3 20221028\n# ℹ 498 more rows\n\n\nWir nehmen die Datumsspalte, die eine Zahl ist und transformieren die Spalte in einen character. Danach können wir dann die Funktion as_date() nutzen um uns ein Datum wiedergeben zu lassen.\n\ntime_tbl |&gt; \n  mutate(jjjjmmdd = as.character(jjjjmmdd),\n         jjjjmmdd = as_date(jjjjmmdd)) |&gt; \n  print(n = 3)\n\n# A tibble: 501 × 1\n  jjjjmmdd  \n  &lt;date&gt;    \n1 2022-10-30\n2 2022-10-29\n3 2022-10-28\n# ℹ 498 more rows\n\n\nWie wir sehen passt die Umwandlung in diesem Fall hervorragend. Die Funktion as_date() erkennt das Jahr, den Monat und den Tag und baut uns dann die Datumsspalte zusammen. Meistens passt es auch, dann können wir hier enden.\nAls eine Alternative haben wir auch die Möglichkeit die Funktion as.Date() zu nutzen. Hier können wir das Datumformat in einer etwas kryptischen Form angeben. Schauen wir uns erst die Funktion in Arbeit an und dann was wir hier gemacht haben.\n\ntime_tbl |&gt; \n  mutate(jjjjmmdd = as.character(jjjjmmdd),\n         jjjjmmdd = as.Date(jjjjmmdd, \"%Y%m%d\")) |&gt; \n  print(n = 3)\n\n# A tibble: 501 × 1\n  jjjjmmdd  \n  &lt;date&gt;    \n1 2022-10-30\n2 2022-10-29\n3 2022-10-28\n# ℹ 498 more rows\n\n\nWir können der Funktion das Datumsformat mitgeben. Im Folgenden einmal eine Auswahl an Möglichkeiten. Die jeweiligen Prozent/Buchstaben-Kombinationen stehen dann immer für ein Jahr oder eben ein Monat.\n\n%Y: 4-Zeichen Jahr (1982)\n%y: 2-Zeichen Jahr (82)\n%m: 2-Zeichen Monat (01)\n%d: 2-Zeichen Tag des Monats (13)\n%A: Wochentag (Wednesday)\n%a: Abgekürzter Wochentag (Wed)\n%B: Monat (January)\n%b: Abgekürzter Monat (Jan)\n\nNehmen wir einmal an, wir haben das Datum in folgender Form 2012-11-02 vorliegen. Dann können wir dafür als Format %Y-%m-%d schreiben und das Datum wird erkennt. Hier ist es besonders hilfreich, dass wir die Trennzeichen mit angeben können. Sonst müssen wir die Trennzeichen dann über str_replace_all() entfernen und könten dann schauen, ob es über die Funktion as_date() geht.\nAls ein weiteres Beispiel nochmal das Einlesen von einer Datei mit dem Datum und der Uhrzeit in zwei Spalten. Wir wollen die beiden Spalten zusammenführen, so dass wir nur noch eine Spalte mit datetime haben.\n\ndate_time_tbl &lt;- read_excel(\"data/date_time_data.xlsx\") |&gt; \n  clean_names()\ndate_time_tbl\n\n# A tibble: 7 × 5\n  datum               uhrzeit             messw   min   max\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2023-04-11 00:00:00 1899-12-31 13:30:00  22    22    22  \n2 2023-04-11 00:00:00 1899-12-31 14:00:00  19.5  19.5  22.6\n3 2023-04-11 00:00:00 1899-12-31 14:30:00  23.4  19.5  25.8\n4 2023-04-11 00:00:00 1899-12-31 15:00:00  19.2  18.3  28.1\n5 2023-04-11 00:00:00 1899-12-31 15:30:00  17.1  17.1  23.3\n6 2023-04-11 00:00:00 1899-12-31 16:00:00  19.2  15.3  21.9\n7 2023-04-11 00:00:00 1899-12-31 16:30:00  29.2  18.3  31  \n\n\nDazu nutzen wir die Funktion format() die es uns erlaubt die Spalten einmal als Datum ohne Uhrzeit zu formatieren und einmal erlaubt die Uhrzeit ohne das Datum zu bauen. Dann nehmen wir beide Spalten und packen das Datum und die Uhrzeit wieder zusammen.\n\ndate_time_tbl |&gt; \n  mutate(uhrzeit = format(uhrzeit, format = \"%H:%M:%S\"),\n         datum = format(datum, format = \"%Y-%m-%d\"),\n         datum = ymd(datum) + hms(uhrzeit))\n\n# A tibble: 7 × 5\n  datum               uhrzeit  messw   min   max\n  &lt;dttm&gt;              &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2023-04-11 13:30:00 13:30:00  22    22    22  \n2 2023-04-11 14:00:00 14:00:00  19.5  19.5  22.6\n3 2023-04-11 14:30:00 14:30:00  23.4  19.5  25.8\n4 2023-04-11 15:00:00 15:00:00  19.2  18.3  28.1\n5 2023-04-11 15:30:00 15:30:00  17.1  17.1  23.3\n6 2023-04-11 16:00:00 16:00:00  19.2  15.3  21.9\n7 2023-04-11 16:30:00 16:30:00  29.2  18.3  31  \n\n\nIm Weiteren hilft das Tutorium zum R Paket {lubridate} - Make Dealing with Dates a Little Easier und natürlich das weitere Tutorium Dates and times.\n\n\n\nQuelle: https://xkcd.com/",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reguläre Ausdrücke</span>"
    ]
  },
  {
    "objectID": "programing-purrr-furrr.html",
    "href": "programing-purrr-furrr.html",
    "title": "13  Ein purrr Cookbook",
    "section": "",
    "text": "13.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, rstatix, \n               janitor, purrr, furrr, see,\n               readxl, tictoc, multcompView, \n               parameters, scales,\n               conflicted)\nconflicts_prefer(magrittr::extract)\nconflicts_prefer(dplyr::filter)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ein `purrr` Cookbook</span>"
    ]
  },
  {
    "objectID": "programing-purrr-furrr.html#die-daten",
    "href": "programing-purrr-furrr.html#die-daten",
    "title": "13  Ein purrr Cookbook",
    "section": "13.2 Die Daten",
    "text": "13.2 Die Daten\nAls Datenbeispiel schauen wir uns einmal ein einfaktoriellen Datensatz an und suchen uns auch nur acht Zeilen und drei Outcomes raus. Wir könnten die Analyse auch über den vollen Datensatz rechnen, aber dann wird hier alles sehr voll. Es geht ja hier mehr um die Demonstration.\n\nsoil_tbl &lt;- read_excel(\"data/soil_1fac_data.xlsx\") |&gt; \n  mutate(variante = str_c(variante, \"_\", amount),\n         variante = as_factor(variante),\n         across(where(is.numeric), round, 2)) |&gt; \n  select(-amount) |&gt;\n  extract(1:8, 1:4) |&gt; \n  pivot_longer(cols = fe:no3, \n               names_to = \"outcome\",\n               values_to = \"rsp\") \n\nAls zweiten Datensatz nehmen wir noch eine zweifaktorilles Design mit einem Behandlungs- und einem Blockeffekt. Darüberhinaus haben wir dann noch verschiedene Outcomes und diese Outcomes dann auch an zwei Orten, einmal im Blatt und einmal im Stiel, gemessen. Das heißt, wir haben immer eine Outcome/Sample-Kombination vorliegen. Bei drei Outcomes und zwei Messorten macht das dann sechs Kombinationen auf denen wir dann immer unsere zweifaktoriellen Analysen rechnen wollen.\n\nspinach_tbl &lt;- read_excel(\"data/spinach_metal_data.xlsx\") |&gt; \n  mutate(trt = as_factor(trt),\n         sample = as_factor(sample),\n         block = as_factor(block)) |&gt; \n  pivot_longer(cols = fe:zn,\n               names_to = \"outcome\",\n               values_to = \"rsp\") |&gt; \n  mutate(outcome = as_factor(outcome))",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ein `purrr` Cookbook</span>"
    ]
  },
  {
    "objectID": "programing-purrr-furrr.html#daten-aufteilen",
    "href": "programing-purrr-furrr.html#daten-aufteilen",
    "title": "13  Ein purrr Cookbook",
    "section": "13.3 Daten aufteilen…",
    "text": "13.3 Daten aufteilen…\nIn R haben wir zwei Möglichkeiten für map() die Daten aufzuteilen. Klar, wir können die Aufteilung sicherlich auch mit anderen Funktionen machen, aber diese beiden Funktionen sind sehr nützlich.\n\n13.3.1 … mit split()\nMit der Funktion split() können wir einen Datensatz nach einer Faktorspalte in eine Liste aufspalten. Die Liste ist dann auch gleich so benannt wie das Level es Faktors für das wir die Aufteilung gemacht haben. Die Benamung der Liste ist dann praktisch, wenn wir später wieder einen Datensatz aus den Ergebnissen bauen.\n\nsoil_lst &lt;- soil_tbl |&gt;\n  split(~outcome) \n\nsoil_lst\n\n$fe\n# A tibble: 8 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 fe       0.26\n2 Holzfasern_0 fe       0.33\n3 Holzfasern_0 fe       0.27\n4 Holzfasern_0 fe       0.31\n5 Torf_30      fe       0.46\n6 Torf_30      fe       0.37\n7 Torf_30      fe       0.28\n8 Torf_30      fe       0.5 \n\n$k\n# A tibble: 8 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 k        3.7 \n2 Holzfasern_0 k        3.66\n3 Holzfasern_0 k        3.83\n4 Holzfasern_0 k        3.66\n5 Torf_30      k        2.89\n6 Torf_30      k        3.41\n7 Torf_30      k        2.94\n8 Torf_30      k        2.89\n\n$no3\n# A tibble: 8 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 no3      3.28\n2 Holzfasern_0 no3      3.24\n3 Holzfasern_0 no3      3.64\n4 Holzfasern_0 no3      3.24\n5 Torf_30      no3      5.02\n6 Torf_30      no3      9.44\n7 Torf_30      no3      8.71\n8 Torf_30      no3      5.02\n\n\n\n\n13.3.2 … mit nest()\nWenn wir mehr als eine Gruppierungsspalte haben, dann können wir die Funktion nest() nutzen. In unserem Beispiel haben wir die Spalte outcome und sample. Für jede Kombination der beiden Spalte wollen wir dann jeweils ein Modell rechnen. Hier meine ich mit Modell eine lineare Regression und dann eine ANOVA. Als erstes müssen wir unsere Daten gruppieren und dann können wir die Daten nesten. Mit unnest() lässt sich dann die genestete Struktur wieder in einen normalen Datensatz zurückführen.\n\nspinach_nest_tbl &lt;- spinach_tbl |&gt; \n  group_by(sample, outcome) |&gt; \n  nest() \n\nspinach_nest_tbl\n\n# A tibble: 6 × 3\n# Groups:   sample, outcome [6]\n  sample outcome data             \n  &lt;fct&gt;  &lt;fct&gt;   &lt;list&gt;           \n1 leaf   fe      &lt;tibble [28 × 3]&gt;\n2 leaf   cd      &lt;tibble [28 × 3]&gt;\n3 leaf   zn      &lt;tibble [28 × 3]&gt;\n4 stem   fe      &lt;tibble [28 × 3]&gt;\n5 stem   cd      &lt;tibble [28 × 3]&gt;\n6 stem   zn      &lt;tibble [28 × 3]&gt;",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ein `purrr` Cookbook</span>"
    ]
  },
  {
    "objectID": "programing-purrr-furrr.html#sec-purrr",
    "href": "programing-purrr-furrr.html#sec-purrr",
    "title": "13  Ein purrr Cookbook",
    "section": "13.4 Mit {purrr} über Daten",
    "text": "13.4 Mit {purrr} über Daten\nDas R Paket {purrr} erlaubt es sehr effizient immer das Gleiche auf Listeneinträgen oder genereller auf Daten anzuwenden. Wir können uns dabei selber eine Funktion schreiben oder aber schon implementierte Funktionen anwenden. Gehen wir einmal alle Funktionen durch. Wir werden hier nicht alle zeigen, aber es ist gut einmal zu wissen, welche Funktionen es gibt. Schaue auch mal in das Cheat Sheet des R Paketes {purrr} rein: Apply functions with purrr::cheat sheet.\n\n13.4.1 Mit map() auf Listeneinträgen\nWir können die Funktion map() und deren verwandete Funktionen nutzen um zügig über Listen andere Funktionen anzuwenden.\n\nmap() erlaubt über eine Liste von Datensätzen ein Funktion anzuwenden. Dabei können wir dann die einzelnen Listeneinträge über .x an die Funktionen weitergeben. Siehe hierzu auch Basic map functions.\nmap2() erlaubt es über zwei gleichlange Vektoren zu laufen. Wir können hier zwei Optionen in der Form .x, .y an die Funktion weitergeben. Siehe hierzu auch Map with multiple inputs.\npmap() kann nun über eine Liste an Vektoren laufen und somit mehrere Inputoptionen verarbeiten. Damit ist pmap() die Generalisierung der map() Funktion. Siehe hierzu auch Map with multiple inputs.\nwalk() ist ein silent map(). Damit können wir Daten in eine Datei schreiben, ohne ein Output wieder zubekommen.\nimap() können wir nutzen, wenn wir den Index \\(i\\) wieder haben wollen. Das heißt, wir wollen über einen Vektor laufen und brauchen dafür den Index. Hier hilft die imap() Familie.\nmodify()können wir anwenden, wenn wir nur Spalten modifizieren oder mutieren wollen. Wir haben einen Datensatz und wollen alle character Spalten in einen Faktor umwandeln. Siehe hierzu auch Modify elements selectively.\n\nSchauen wir uns die Anwendung von der Funktion map() auf eine Liste an. In folgenden Code entfernen wir einmal in jedem Listeneintrag die Spalte outcome. Dann lassen wir uns von jedem Listeneintrag die erste Zeile wiedergeben.\n\nsoil_lst |&gt;\n  map(select, -outcome) |&gt; \n  map(head, 1)\n\n$fe\n# A tibble: 1 × 2\n  variante       rsp\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Holzfasern_0  0.26\n\n$k\n# A tibble: 1 × 2\n  variante       rsp\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Holzfasern_0   3.7\n\n$no3\n# A tibble: 1 × 2\n  variante       rsp\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Holzfasern_0  3.28\n\n\nWir können zum einen map(head, 1) schreiben oder aber den Listeneintrag als .x direkt in die Funktion weiterleiten. Dann schreiben wir map(~head(.x, 1)) und müssen noch die Tilde ~ vor die Funktion setzen.\n\nsoil_lst |&gt;\n  map(~head(.x, 1))\n\n$fe\n# A tibble: 1 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 fe       0.26\n\n$k\n# A tibble: 1 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 k         3.7\n\n$no3\n# A tibble: 1 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 no3      3.28\n\n\nDie richtige Stärke entwickelt dann map(), wenn wir mehrere Funktionen hineinander schalten. In unserem Fall rechnen wir einen Games-Howell Test und wollen uns dann das compact letter display wiedergeben lassen. Da wir am Ende dann einen Datensatz haben wollen, nutzen wir die Funktion bind_rows() um die Listeneinträge in einen Datensatz zusammen zukleben.\n\nsoil_lst |&gt; \n  map(~games_howell_test(rsp ~ variante, data = .x)) |&gt; \n  map(~mutate(.x, contrast = str_c(.x$group1, \"-\", .x$group2))) |&gt; \n  map(pull, p.adj, contrast) |&gt; \n  map(~multcompLetters(.x)$Letters) |&gt; \n  bind_rows(.id = \"outcome\") \n\n# A tibble: 3 × 3\n  outcome Holzfasern_0 Torf_30\n  &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;  \n1 fe      a            a      \n2 k       a            b      \n3 no3     a            a      \n\n\nWir können auch auch auf genesteten Daten die Funktion map() anwenden. In diesem Fall belieben wir die ganze Zeit in einem tibble. Wir lagern unsere Ergebnisse sozusagen immer in einer Zelle und können auf diese Einträge dann immer wieder zugreifen. Einmal zu Demonstration rechnen wir sechs Mal ein lineares Modell mit der Funktion lm() und speichern das Ergebnis in der Spalte model. Wir haben jetzt dort jeweils &lt;lm&gt; stehen. Damit wissen wir auch, dass wir dort unser Modell drin haben. Wir können jetzt auf der Spalte model weiterechnen und uns neue Spalten mit Ergebnissen erschaffen.\n\nspinach_nest_tbl |&gt;\n  mutate(model = map(data, ~lm(rsp ~ trt + block, data = .x)))\n\n# A tibble: 6 × 4\n# Groups:   sample, outcome [6]\n  sample outcome data              model \n  &lt;fct&gt;  &lt;fct&gt;   &lt;list&gt;            &lt;list&gt;\n1 leaf   fe      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n2 leaf   cd      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n3 leaf   zn      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n4 stem   fe      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n5 stem   cd      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n6 stem   zn      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n\n\nIm Folgenden rechnen wir ein lineares Modell, dann eine ANOVA und lassen uns das Ergebnis der ANOVA mit der Funktion model_parameters() aufhübschen. Wie du siehst, geben wie immer den Spaltennamen eine Funktion weiter. Dann wählen wir noch die Spalten, die wir dann unnesten wollen.\n\nspinach_nest_tbl &lt;- spinach_nest_tbl |&gt; \n  mutate(model = map(data, ~lm(rsp ~ trt + block, data = .x))) |&gt; \n  mutate(anova = map(model, anova)) |&gt; \n  mutate(parameter = map(anova, model_parameters)) |&gt; \n  select(sample, outcome, parameter) \n\nZum Abschluss nutzen wir die Funktion unnest() um uns die aufgehübschten Ergebnisse der ANOVA wiedergeben zu lassen. Dann will ich noch, dass die Namen alle klein geschrieben sind und auch sonst sauber sind. Dafür nutze ich dann die Funktion clean_names(). Abschließend filtere ich und runde ich noch die Ergebnisse. Am Ende will ich dann nur die Kombinationen aus sample und outcome haben sowie den \\(p\\)-Wert aus der ANOVA.\n\nspinach_nest_tbl |&gt;\n  unnest(parameter) |&gt; \n  clean_names() |&gt; \n  mutate(across(where(is.numeric), round, 2)) |&gt; \n  filter(parameter != \"Residuals\") |&gt; \n  select(sample, outcome, parameter, p)\n\n# A tibble: 12 × 4\n# Groups:   sample, outcome [6]\n   sample outcome parameter     p\n   &lt;fct&gt;  &lt;fct&gt;   &lt;chr&gt;     &lt;dbl&gt;\n 1 leaf   fe      trt        0   \n 2 leaf   fe      block      0.05\n 3 leaf   cd      trt        0.05\n 4 leaf   cd      block      0   \n 5 leaf   zn      trt        0.42\n 6 leaf   zn      block      0.29\n 7 stem   fe      trt        0.02\n 8 stem   fe      block      0.02\n 9 stem   cd      trt        0.33\n10 stem   cd      block      0   \n11 stem   zn      trt        0.51\n12 stem   zn      block      0   \n\n\nHier noch ein weiteres Beispiel für split(), nest() und nest_by() zum Ausprobieren und rumspielen. Wir wollen für hier einmal auf ganze vielen Behandlungen den Shapiro-Wilk-Tests für die Abweichung von der Normalverteilung rechnen. Dazu laden wir uns einmal die Daten clove_germ_rate.xlsx.\n\nclove_tbl &lt;- read_excel(\"data/clove_germ_rate.xlsx\") |&gt; \n  mutate(clove_strain = as_factor(clove_strain),\n         germ_rate = as.numeric(germ_rate))\n\nWir rechnen einmal die Shapiro-Wilk-Tests über die Funktion split() und dann einer Liste.\n\nclove_tbl |&gt; \n  split(~clove_strain) |&gt; \n  map(~shapiro.test(.x$germ_rate)) |&gt; \n  map(tidy) |&gt; \n  bind_rows(.id = \"test\") |&gt;\n  select(test, p.value) |&gt; \n  mutate(decision = ifelse(p.value &lt;= 0.05, \"reject normal\", \"normal\"),\n         p.value = pvalue(p.value, accuracy = 0.001))\n\n# A tibble: 20 × 3\n   test          p.value decision     \n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;        \n 1 standard      0.272   normal       \n 2 west_rck_1    0.272   normal       \n 3 south_III_V   0.855   normal       \n 4 west_rck_2_II 0.653   normal       \n 5 comb_001      0.103   normal       \n 6 western_4     0.849   normal       \n 7 north_549     0.855   normal       \n 8 subtype_09    0.983   normal       \n 9 subtype_III_4 0.051   normal       \n10 ctrl_pos      0.992   normal       \n11 ctrl_7        0.683   normal       \n12 trans_09_I    0.001   reject normal\n13 new_xray_9    0.406   normal       \n14 old_09        0.001   reject normal\n15 recon_1       0.100   normal       \n16 recon_3456    0.001   reject normal\n17 east_new      0.907   normal       \n18 east_old      0.161   normal       \n19 south_II_U    0.048   reject normal\n20 west_3_cvl    0.272   normal       \n\n\nDann das selbe nochmal mit der Funktion nest_by(), die jetzt Vektoren generiert.\n\nclove_tbl |&gt; \n  nest_by(clove_strain) |&gt; \n  mutate(shapiro = map(data, ~shapiro.test(.x)),\n         clean = tidy(shapiro)) |&gt; \n  reframe(clean)\n\n# A tibble: 20 × 4\n   clove_strain  statistic p.value method                     \n   &lt;fct&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                      \n 1 standard          0.863 0.272   Shapiro-Wilk normality test\n 2 west_rck_1        0.863 0.272   Shapiro-Wilk normality test\n 3 south_III_V       0.972 0.855   Shapiro-Wilk normality test\n 4 west_rck_2_II     0.940 0.653   Shapiro-Wilk normality test\n 5 comb_001          0.801 0.103   Shapiro-Wilk normality test\n 6 western_4         0.971 0.849   Shapiro-Wilk normality test\n 7 north_549         0.972 0.855   Shapiro-Wilk normality test\n 8 subtype_09        0.995 0.983   Shapiro-Wilk normality test\n 9 subtype_III_4     0.763 0.0511  Shapiro-Wilk normality test\n10 ctrl_pos          0.998 0.992   Shapiro-Wilk normality test\n11 ctrl_7            0.945 0.683   Shapiro-Wilk normality test\n12 trans_09_I        0.630 0.00124 Shapiro-Wilk normality test\n13 new_xray_9        0.895 0.406   Shapiro-Wilk normality test\n14 old_09            0.630 0.00124 Shapiro-Wilk normality test\n15 recon_1           0.799 0.0996  Shapiro-Wilk normality test\n16 recon_3456        0.630 0.00124 Shapiro-Wilk normality test\n17 east_new          0.981 0.907   Shapiro-Wilk normality test\n18 east_old          0.827 0.161   Shapiro-Wilk normality test\n19 south_II_U        0.760 0.0476  Shapiro-Wilk normality test\n20 west_3_cvl        0.863 0.272   Shapiro-Wilk normality test\n\n\nUnd nochmal mit der Pipe von group_by() zu nest(). Die Funktion nest() hate auch eine .by =-Option, so dass wir auch den Schritt mit group_by() weglassen könnten.\n\nclove_tbl |&gt;\n  group_by(clove_strain) |&gt; \n  nest() |&gt; \n  mutate(shapiro = map(data, ~shapiro.test(.x$germ_rate)),\n         clean = map(shapiro, tidy)) |&gt; \n  unnest(clean)\n\n# A tibble: 20 × 6\n# Groups:   clove_strain [20]\n   clove_strain  data             shapiro statistic p.value method              \n   &lt;fct&gt;         &lt;list&gt;           &lt;list&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               \n 1 standard      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.863 0.272   Shapiro-Wilk normal…\n 2 west_rck_1    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.863 0.272   Shapiro-Wilk normal…\n 3 south_III_V   &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.972 0.855   Shapiro-Wilk normal…\n 4 west_rck_2_II &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.940 0.653   Shapiro-Wilk normal…\n 5 comb_001      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.801 0.103   Shapiro-Wilk normal…\n 6 western_4     &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.971 0.849   Shapiro-Wilk normal…\n 7 north_549     &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.972 0.855   Shapiro-Wilk normal…\n 8 subtype_09    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.995 0.983   Shapiro-Wilk normal…\n 9 subtype_III_4 &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.763 0.0511  Shapiro-Wilk normal…\n10 ctrl_pos      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.998 0.992   Shapiro-Wilk normal…\n11 ctrl_7        &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.945 0.683   Shapiro-Wilk normal…\n12 trans_09_I    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.630 0.00124 Shapiro-Wilk normal…\n13 new_xray_9    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.895 0.406   Shapiro-Wilk normal…\n14 old_09        &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.630 0.00124 Shapiro-Wilk normal…\n15 recon_1       &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.799 0.0996  Shapiro-Wilk normal…\n16 recon_3456    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.630 0.00124 Shapiro-Wilk normal…\n17 east_new      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.981 0.907   Shapiro-Wilk normal…\n18 east_old      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.827 0.161   Shapiro-Wilk normal…\n19 south_II_U    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.760 0.0476  Shapiro-Wilk normal…\n20 west_3_cvl    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.863 0.272   Shapiro-Wilk normal…\n\n\n\n\n\n\n\n\nBesseres unnest()\n\n\n\nEs hilft hier auch sich einmal die Funktion unnest_wider() sowie unnest_longer() anzuschauen. Beide Funktionen liefern manchmal gleich das gewünschte Resultat. Besonders hilfreich sind beide Funktionen, wenn es um Listen geht. Aber da musst du dann einmal konkret schauen, was du brauchst.\n\n\n\n\n13.4.2 Weitere Funktionen wie keep, reduce und Co.\nIm Folgenden nochmal eine Sammlung von weiteren Funktionen, die ich im Rahmen meiner Data Science Analysen dann immer mal wieder nutze und gerne vergesse. Da aber die Funktionen so praktisch sind, habe ich mir mal alles hier mit aufgeschrieben. Tja, teilweise ist das Skript hier auch eine große Fundgrube für mich.\n\n\nDas Data Wrangling with dplyr and tidyr - Cheat Sheet ist auch immer eine gute Hilfe. Auch ist das Apply functions with purrr Cheat - Sheet ist auch immer ein Blick wert.\nWenn ich einen Vektor habe mit Namen, der teilweise aus map() herauskommt, dann kann ich den Vektor über enframe() und der Dokumentation zu enframe() in ein tibble umwandeln.\n\nnamed_vec &lt;- c(dog = 2, cat = 4, dog = 3, fox = 5)\nenframe(named_vec)\n\n# A tibble: 4 × 2\n  name  value\n  &lt;chr&gt; &lt;dbl&gt;\n1 dog       2\n2 cat       4\n3 dog       3\n4 fox       5\n\n\nDas ist manchmal extrem praktisch, wenn eine Liste nur einen Eintrag hat. Dann können wir über unlist() einen benamten Vektor bauen und den Vektor dann in einen tibble umwandeln.\n\nlst(id237189 = 4,\n    id7629w0 = 5,\n    id790182 = 3) |&gt; \n  unlist() |&gt; \n  enframe()\n\n# A tibble: 3 × 2\n  name     value\n  &lt;chr&gt;    &lt;dbl&gt;\n1 id237189     4\n2 id7629w0     5\n3 id790182     3\n\n\nIch habe häufig Listen und möchte nach gewissen Merkmalen die Listeneinträge filtern - manchmal auch Datensätze. Hier hilft die Funktion keep() und der Dokumentation zu keep() mit der ich dann Listeneinträge nach einer Funktion, die TRUE oder FALSE ausgibt filtern kann. Das Gange geht dann mit discard() auch in der Verneinung von keep(). Ich will in der Folge nur die Listeneinträge behalten in denen das entsprechende tibble im Listeneintrag mehr als zwei Zeilen hat.\n\ndata_lst &lt;- lst(dog_tbl = tibble(id = c(\"id786\", \"id987\", \"id231\", \"id566\"),\n                                 dog = rep(\"dog\", 4)),\n                cat_tbl = tibble(id = c(\"id786\", \"id566\"),\n                                 cat = rep(\"cat\", 2)),\n                fox_tbl = tibble(id = c(\"id786\", \"id987\", \"id776\", \"id129\", \"id231\", \"id566\"),\n                                 fox = rep(\"fox\", 6)))\ndata_lst |&gt; \n  keep(\\(x) nrow(x) &gt; 2)\n\n$dog_tbl\n# A tibble: 4 × 2\n  id    dog  \n  &lt;chr&gt; &lt;chr&gt;\n1 id786 dog  \n2 id987 dog  \n3 id231 dog  \n4 id566 dog  \n\n$fox_tbl\n# A tibble: 6 × 2\n  id    fox  \n  &lt;chr&gt; &lt;chr&gt;\n1 id786 fox  \n2 id987 fox  \n3 id776 fox  \n4 id129 fox  \n5 id231 fox  \n6 id566 fox  \n\n\nWenn ich in einer Liste Einträge finde, die keinen Werte haben, also NULL sind, dann kann ich über die Funktion compact() die leeren Listeneinträge einfach entfernen.\n\ndata_null_lst &lt;- lst(dog_tbl = tibble(animal = rep(\"dog\", 4)),\n                     cat_tbl = NULL,\n                     fox_tbl = NULL)\ndata_null_lst |&gt; \n  compact()\n\n$dog_tbl\n# A tibble: 4 × 1\n  animal\n  &lt;chr&gt; \n1 dog   \n2 dog   \n3 dog   \n4 dog   \n\n\nDie Funktion reduce und der Dokumentation zu reduce() ermöglicht es mit Listeneinträge zu kombinieren. Wenn ich alle Listeneinträge nur untereinander packen will, dann nutze ich die Funktion bind_rows() oder gleich map_dfr().\n\ndata_lst |&gt; \n  reduce(left_join, by = \"id\")\n\n# A tibble: 4 × 4\n  id    dog   cat   fox  \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 id786 dog   cat   fox  \n2 id987 dog   &lt;NA&gt;  fox  \n3 id231 dog   &lt;NA&gt;  fox  \n4 id566 dog   cat   fox  \n\n\nWenn alle Listeneinträge nach einer Spalte zusammengführt werden sollen, dann Nutze ich die Funktion left_join(). Weiteres zum Zusammenführen (eng. merge) von Datensätzen in der Dokumentation zu join(). Beim Mergen von Datensätzen muss man recht viel Nachdenken und überlegen was man eigentlich will, deshalb kann ich hier keine vollständige Abhandlung liefern.\n\nsort_vec &lt;- c(\"cat\", \"fox\", \"dog\")\ndata_tbl &lt;- tibble(animal = c(\"dog\", \"cat\", \"fox\"),\n       jump_length = c(4.1, 5.8, 6.2))\ndata_tbl\n\n# A tibble: 3 × 2\n  animal jump_length\n  &lt;chr&gt;        &lt;dbl&gt;\n1 dog            4.1\n2 cat            5.8\n3 fox            6.2\n\n\nDann möchte ich meist noch einen Datensatz nach einer externen Spalte sortieren. Dafür nutze ich die Funktion arrange() wie folgt.\n\ndata_tbl |&gt; \n  arrange(factor(animal, levels = sort_vec))\n\n# A tibble: 3 × 2\n  animal jump_length\n  &lt;chr&gt;        &lt;dbl&gt;\n1 cat            5.8\n2 fox            6.2\n3 dog            4.1\n\n\nManchmal möchte man dann auch einem Long-Format wieder ein Wide-Format machen. Zum einen natürlich wieder der Verweis auf die Funktion pivot_wider() und der Dokumentation zu pivot_wider(). Wir nutzen hier aber auch das R Paket {glue} und der Dokumentation zu {glue} um uns die Spaltennamen zu bauen. In unserem Beispiel kriegt jede Kuh zwei Zeilen mit Werten. Wir wollen aber den die Spalte type auflösen und nur noch eine Zeile pro Kuh vorliegen haben.\n\ncow_tbl &lt;- tibble(lom = c(\"276000355496007\", \"276000355496007\"),\n                  type = c(\"val\", \"lal\"),\n                  rz = c(128, 254),\n                  lfd_nr = c(3, 4))\ncow_tbl\n\n# A tibble: 2 × 4\n  lom             type     rz lfd_nr\n  &lt;chr&gt;           &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 276000355496007 val     128      3\n2 276000355496007 lal     254      4\n\n\nWir nehmen also die Namen aus der Spalte type und kleben (eng. glue) erst den Namen der Spalte und dann mit einem Unterstrich getrennt den Namen der beiden anderen Spalten rz und lfd_nr dran. Wir wollen ja auch die Werte von den beiden Spalten dann in das Wide-Format übertragen.\n\ncow_tbl |&gt; \n  pivot_wider(names_from = type,\n              names_glue = \"{type}_{.value}\",\n              values_from = c(rz, lfd_nr))\n\n# A tibble: 1 × 5\n  lom             val_rz lal_rz val_lfd_nr lal_lfd_nr\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 276000355496007    128    254          3          4",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ein `purrr` Cookbook</span>"
    ]
  },
  {
    "objectID": "programing-purrr-furrr.html#sec-furrr",
    "href": "programing-purrr-furrr.html#sec-furrr",
    "title": "13  Ein purrr Cookbook",
    "section": "13.5 Mit {furrr} parallel über Daten",
    "text": "13.5 Mit {furrr} parallel über Daten\nWarum geht es den jetzt hier? Wenn du purrr und die Funktionen map() verstanden hast, dann geht natürlich alles auch in paralleler Berechnung. Die parallele Berechnung ist in dem R Paket {furrr} implementiert. Das heißt wir müssen nur die Funktionsnamen ändern und schon rechnet sich alles in Parallel. Wir nutzen also nicht nur einen Kern von unseren Rechnern sondern eben alles was wir haben.\n\nno_cores &lt;- availableCores() - 1\nno_cores\n\nsystem \n     7 \n\n\nEinmal das ganze in sequenzieller Programmierung. Also alles nacheinander gerechnet.\n\nplan(sequential)\n\ntic()\nnothingness &lt;- future_map(c(2, 2, 2), ~Sys.sleep(.x))\ntoc()\n\n6.035 sec elapsed\n\n\nDer folgende Code sollte ca. 2 Sekunden dauern, wenn der Code parallel läuft. Wir haben einen kleinen Overhead in future_map() durch das Senden von Daten an die einzelnen Kerne. Es gibt auch einmalige Zeitkosten für plan(multisession), um die Kerne einzurichten.\n\nplan(multisession, workers = 3)\n\ntic()\nnothingness &lt;- future_map(c(2, 2, 2), ~Sys.sleep(.x))\ntoc()\n\n2.355 sec elapsed\n\n\nWie du siehst, musst du nur future_ vor die map() Funktion ergänzen und schon kannst du parallel rechnen.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ein `purrr` Cookbook</span>"
    ]
  },
  {
    "objectID": "programing-purrr-furrr.html#progressr-an-introduction",
    "href": "programing-purrr-furrr.html#progressr-an-introduction",
    "title": "13  Ein purrr Cookbook",
    "section": "13.6 progressr: An Introduction",
    "text": "13.6 progressr: An Introduction\nDie Funktion map() hat die Option .progress = TRUE mit der du dir auch einen Fortschritt anzeigen lassen kannst. Also wie lange noch die Funktion braucht um über alle Listeneinträge zu rechnen. Wenn du es noch schöner haben willst, dann schaue dir einmal das R Paket {progressr}: An Introduction an.\n\n\n\nAbbildung 13.1— Quelle: https://www.gocomics.com/garfield/1991/02/24",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ein `purrr` Cookbook</span>"
    ]
  },
  {
    "objectID": "programing-quarto-shiny.html",
    "href": "programing-quarto-shiny.html",
    "title": "14  Quarto und Shiny App",
    "section": "",
    "text": "14.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, readxl, \n               janitor, see, directlabels,\n               owidR, conflicted)\nconflicts_prefer(dplyr::filter)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Quarto und Shiny App</span>"
    ]
  },
  {
    "objectID": "programing-quarto-shiny.html#r-quarto",
    "href": "programing-quarto-shiny.html#r-quarto",
    "title": "14  Quarto und Shiny App",
    "section": "14.2 R Quarto",
    "text": "14.2 R Quarto\nWas du hier siehst ist in R Quarto geschrieben. Damit ist es recht schwer für mich dir hier in diesem Text zu zeigen, wie der Text strukturell aufgebaut ist. Dafür gibt es die tolle Hilfsseite Welcome to Quarto, die dir da sehr viel weiter hilft. In der folgenden Abbildung 14.1 siehst du einmal, wie du ein neues Quarto Dokument erstellst. Wir gehen hier nicht den ganzen Prozess durch, dafür gibt es dann am Ende noch ein YouTube Video.\n\n\n\n\n\n\nAbbildung 14.1— Um eine neues R Quarto Dokument zu erstellen klicken wir einmal auf das ‘+’ oben links und wählen dann Quarto Document... aus.\n\n\n\nIn der Abbildung 14.6 siehst du dann das nächste Fenster, indem du dann den Titel des Dokuments und dich als Autor eintragen kannst. Darüber hinaus auch noch das Ausgabeformat. Richtig, du kannst hier auch gleich MS Word wählen. Ich nehme dann meistens HTML aber auch PDF funktioniert für mich gut. Du kannst dann aber später jederzeit die Ausgabe ändern! Du bist hier nicht an einen Typ gebunden, das ist ja das Schöne. Ein Skript, viele Ausgabenmöglichkeiten.\n\n\n\n\n\n\nAbbildung 14.2— Einmal die Maske für die Erstellung des R Quarto Dokuments. Du kannst später auch zwischen HTML, PDF und Word wechseln.\n\n\n\nIm Folgenden einmal die Links zu den einzelnen Kapiteln der Hilfeseite von Quarto, die dich dann direkt zu den entsprechenden Seiten bringen.\n\nCreating a Document — hier einmal als Einstieg das HTML Dokument, aber du kannst natürlich auch zu den anderen Dokumenttypen leicht wechseln. Ein Dokument kannst du dann als Word erstellen und mit diesem Dokument und den erstellten Tabellen weiterarbeiten. Eine gute Möglichkeit um mal einen Projektbericht zu schreiben.\n\nCreating a Presentation — hier einmal die Möglichkeit um eine Präsentation zu erstellen. Und verrückterweise gehen sogar Power Point Präsentationen. Ich habe selber noch nicht PowerPoint mit R kombiniert. Daher weiß ich auch noch nicht wie sich das dann mit R verhält.\n\nCreating a Website — hier einmal die Erstellung einer Webseite. Muss nicht R zusammen sein. Aber ich finde es ist ein angenehmer Weg um seine Ergebnisse anderen öffentlich zu präsentieren. Es gibt hier sicherlich auch die Möglichkeit, den Zugriff auf die Webseite zu begrenzen, so dass Google und Co. die Webseite nicht finden.\nCreating a Book — hier einmal die Erstellung eines Buches. Das was du hier gerade liest, ist mit dem Template eines Buches in R Quarto erstellt. Ja, du besuchst die Seite über einen Browser, aber die grundlegende Struktur ist die eines Buches. Theoretisch könnte ich den Text hier auch als E-Book exportieren. Dafür ist dann aber leider hier alles zu groß geworden.\nCreating a Dashboard — hier dann einmal die Möglichkeit ein Dashboard mit unterschiedlichen Informationen in Quarto zu erstellen. Dashboard kennst du vermutlich aus den Medien. Berühmtheit hat diese Art der Darstellung in der Coronapandemie gewonnen. Hier kannst du dann relativ einfach deine eigenen Informationsseiten bauen. Vielleicht etwas um in einer Firma Informationen aufzuarbeiten.\nCreating a Typst Document — eine etwas einfachere Variante um ein PDF zu erstellen. Hier brauchst du dann keine tiefer greifenden Kenntnisse in einer anderen Programmiersprache sondern kannst relativ einfach etwas kompliziertere und schönere Dokumente erstellen. Die Dokumente in Quarto sind ja eher simple, die Dokumente in Typst erlauben dir da mehr Gestaltungsspielraum. Oder du besuchst einmal die Webseite typst.app für mehr Informationen.\n\n\n\n\n\n\n\nNur ein Dokument für HTML anstatt noch ein Ordner dazu!\n\n\n\n\n\nWenn du deine HTML Ausgabe als ein Standalone HTML Dokument habe willst, dann setze einfach in den YAML Header am Beginn des Dokuments folgenden Code.\n\nformat:\n  html:\n    embed-resources: true\n\nDann wird aber nicht die Matheumgebung mit eingebunden. Dafür musst du dann den folgenden Code verwenden. Aber da nicht alle Dokumente Matheformeln enthalten, muss es auch nicht immer sein.\n\nformat:\n  html:\n    embed-resources: true\n    self-contained-math: true\n\n\n\n\nDie Stärke von R Quarto kommt natürlich zu tragen, wenn wir die Analysen oder die deskriptive Statistik in einem Dokument zusammen erstellen können. Die deskriptiven Tabellen können wir leicht mit dem R Paket {modelsummary} oder {gtsummary} erstellen. Ich zeige die Erstellung einmal exemplarisch in dem Kapitel zur deskriptiven Statistik in dem Abschnitt zur Automatisierten Tabellenerstellung in R. Neben den beiden vorgestellten gibt es natürlich auch Alternativen zu gtsummary und modelsummary. Da müsstest du dann aber mal selber schauen oder mich direkt Fragen. Wie immer gibt es in R nicht nur die eine Möglichkeit eine Tabelle zu erstellen.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Quarto und Shiny App</span>"
    ]
  },
  {
    "objectID": "programing-quarto-shiny.html#r-shiny-app",
    "href": "programing-quarto-shiny.html#r-shiny-app",
    "title": "14  Quarto und Shiny App",
    "section": "14.3 R Shiny App",
    "text": "14.3 R Shiny App\nVor gar nicht allzu langer Zeit sprach ich mit einer Freundin aus meiner eigenen Studentenzeit, die jetzt bei einer großen Pharmafirma arbeitet, über R und die Anwendung von R bei ihr in der Firma. Wie sich herausstellte ist dort R Shiny sehr beliebt. Die Statistiker oder jetzt Data Scientist und Data Analyst arbeiten dort die Datenmengen in R Shiny Apps auf, so dass dann die Anderen in der Firma dann leichter an den Daten rumschrauben können. Das erspart allen sehr viel Arbeit, den fixierte Berichte müssen bei jeder Änderung ja wieder neu gemacht werden.\nDeshalb kann ich nur sagen: Welcome to Shiny. Wenn du wirklich jemanden mit deiner Analyse von Daten beeindrucken willst, dann nutze R Shiny. Auf der anderen Seite gibt es manchmal Datensätze mit so vielen Subgruppen oder aber anderen Messwerten, dass ich immer gar nicht weiß, was ich alles in einer statischen Abbildung reinpacken soll. Da ist mir dann eine dynamische Abbildung hundertmal lieber. Ich kann auch hier nur die groben Umrisse liefern. Mehr erfährst du dann in dem Video zur Erstellung einer Shiny App. Aber wie immer, einfach mal selber ausprobieren. Du kannst wirklich mit wenig Aufwand richtig gute interaktive Webtools erschaffen.\nEine R Shiny App besteht immer aus zwei Teilen. Du hast immer die ui und einmal den server vorliegen. Das ist wichtig sich zu vergewissern. Gerade für Neulinge ist es dann immer etwas schwer, eine Idee davon zu entwickeln, was diese beiden Konzepte sein sollen. Deshalb hier nochmal zusammengefasst.\n\nui: Hier liegt alles drin, was du an Reglern und Oberfläche brauchst. Die ui beschreibt, was du siehst, wenn du eine Shiny App startest. Hier arbeitest du selber mit der Shiny App. Welche Regler da sind und wie die Shiny App aufgebaut ist, findets du in der ui.\nserver: Hier liegen die Funktionen drin. Der server erstellt die Abbidlungen oder die Tabellen. Keine Angst, es handelt sich um ganz normalen R Code, der aber im Hintergrund die interaktiven Abbildungen erstelt. Du baust also hier das Grundgerüst für die Abbdilungen und Tabellen. Auch deine Bearbeitung der Daten machst du hier.\n\nIn der Abbildung 14.9 siehst du den Zusammenhang zwischen der ui und dem server in einer komplexeren R Shiny App einmal dargestellt. Komplex meint hier aber, dass wir die Shiny App auf zwei Dateien ui.R und server.R aufteilen. Ich habe dir in der Abbildung einmal farblich dargestellt, wie die Verbindungen so sind.\n\n\n\n\n\n\nR Shiny App zu verschiedenen Verteilungen\n\n\n\nWir besuchen gerne die R Shiny App The distribution zoo um mehr über die verschiedenen Verteilungen und deren Parameter zu erfahren.\n\n\n\n14.3.1 Shiny App in RStudio\nDas tolle an der R Shiny App ist, dass wir alles im RStudio bauen können. In der Abbildung 14.3 siehst du einmal, wie du eine Shiny Web App einfach erstellen kannst. Wir gehen hier nicht den ganzen Prozess durch, sondern ich gehe wie immer auf die häufigsten Probleme einmal ein. Hier gibt es aber nicht so viele davon, eher Geschmacksfragen.\n\n\n\n\n\n\nAbbildung 14.3— Um eine neue R Shiny Web App zu erstellen klicken wir einmal auf das ‘+’ oben links und wählen dann Shiny Web App... aus.\n\n\n\nJetzt öffnet sich ein neues Fenster und du kannst den Namen deiner Shiny App eintragen. Hier ist es wichtig, sich schon für einen guten Namen zu entscheiden, später wird der Name auch Teil der Webadresse mit dem du dann die Shiny App aufrufst. Natürlich nur unter der Annahme, dass du deine Shiny App ins Internet stellst. Du kannst die Shiny App auch nur privat auf deinen Rechner laufen lassen. Nochmal, der Name des Ordners bestimmt den Namen deiner Shiny App, nicht der Name der R Datei oder Sonstiges was du angibst.\nJetzt muasst du dich noch entscheiden, ob deine Shiny App eher klein wird, dann nutze die Option Single File (app.R). Dann bauen wir eine einfache App, wie ich sie gleich mal in dem ersten Abschnitt vorstellen werde. Wenn deine Shiny App komplexer wird, dann solltest du Multiple Files (ui.R/server.R) wählen. Du kannst aber auch später leciht aus der einen Datei dann zwei Dateien bauen. Meist fange ich immer mit einer Datei app.R an und schaue wie komplex die Shiny App wird.\n\n\n\n\n\n\nAbbildung 14.4— Wir brauchen jetzte ien Namen für die Shiny Web App Application name und müssen uns entscheiden, ob wir mit einer Datei Single file (app.R) oder mehrere Dateien Multiple File (ui.R/server.R) arbeiten wollen. In beiden Fällen wird ein neuer Ordner mit dem Namem der Shiny Web App erstellt.\n\n\n\nIm Folgenden schauen wir uns also die zwei Fälle einmal an. In dem ersten Fall willst du nur eine simple Shiny App mit wenigen Funktionen und einer simplen Oberfläche für die interaktive Anwendung bauen. Im zweiten Fall haben wir es dann schon mit einem komplexeren Fall zu tun und wir spalten dann die Oberfläche (ui.R) von den Berechnungen (server.R) ab.\nNachdem wir uns dann das Standardbeispiel mit der Shiny App Hello_Shiny angeschaut haben, werde ich dann nochmal ein komplexeres Datenbeispiel mit Daten von Our World in Data zeigen. Du findest dann den Code für alle Shiny Apps auf https://github.com/jkruppa/shiny. Dort lagere ich alle meine Shiny Apps, wenn dich da noch mehr interessiert.\n\n\n\n\n\n\nHier kommt das Video\n\n\n\n\n\n\n\n\n\n\n\n\n14.3.2 Einfache Shiny App\nSchauen wir uns als Erstes einmal eine sehr simple R Shiny App an. Mit simple meine ich, dass wir alles in eine Datei app.R packen. Wir brauchen nur diese eine Datei, da wir die ui und den server in zwei Funktionen auslagern. Beide Funktion sind jedoch weiterhin in der Datei app.R. In der Abbildung 14.5 siehst du den Aufbau einmal dargestellt. Wir haben daher unsere app.R in dem Ordner Hello_Shiny abliegen. Unsere Shiny App heißt dementsprechend nach dem Ordner auch Hello_Shiny. Die beiden Funktionalitäten der ui und des server werden durch Funktionen abgebildet.\n\n\n\n\n\n\nflowchart LR\nsubgraph ide1 [\"~/Documents/GitHub/shiny/Hello_Shiny\"]\n    direction TB\n    A{app.R}:::nodeA --- B(\"ui( )\"):::nodeB \n    A{app.R}:::nodeA --- C(\"server( )\"):::nodeB \n    end\n    classDef nodeA fill:#E69F00,stroke:#333,stroke-width:0.75px\n    classDef nodeB fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    \n\n\n\n\nAbbildung 14.5— Für eine simple Shiny App brauchen wir nur eine Datei. Hier sind die Funktionen für die ui und dem server in einer Datei app.R enthalten.\n\n\n\n\n\nIm Folgenden siehst du den Inhalt der app.R für unsere Shiny App Hello_Shiny. Einfach über die Zahlen am rechten Rand fahren um meine Kommentare zu lesen. Du musst immer das Paket {shiny} laden und natürlich die anderen R Pakete die du dann verwenden willst. Faktisch ist ja eine Shiny App nichts anderes als ein R Skript. Daher hier jetzt kein tieferer Text mehr, den der Code ist ja annotiert.\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\n1ui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n2    sidebarLayout(\n3        sidebarPanel(\n4            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n5        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\n6server &lt;- function(input, output) {\n\n7    output$distPlot &lt;- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    &lt;- faithful[, 2]\n8        bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n9        hist(x, breaks = bins, col = 'darkgray', border = 'white',\n             xlab = 'Waiting time to next eruption (in mins)',\n             main = 'Histogram of waiting times')\n    })\n}\n\n# Run the application \n10shinyApp(ui = ui, server = server)\n\n\n1\n\nFunktion für die Erstellung der ui, der Benutzeroberfläche.\n\n2\n\nStart des Layouts der Shiny App.\n\n3\n\nMit einem Kontrollpanel auf der Seite.\n\n4\n\nMit einem Schieberegeler als Eingabe. Die Eingabe wird in bins gespeichert.\n\n5\n\nIm Hauptpanel ist die Abbildung distplot. Die Abbildung entsteht in der server Funktion.\n\n6\n\nFunktion für die Erstellung des Histogramms mit dem input aus der ui.\n\n7\n\nDie zu erstellende Ausgabe displot wird ins Objekt output geschrieben.\n\n8\n\nHier wird dynamisch der Wert aus dem input$bins verwendet.\n\n9\n\nDas Histogramm wird dynamisch mit sich ändernden Werten für input$bins generiert.\n\n10\n\nDie eigentliche Funktion, um eine Shiny App zu starten.\n\n\n\n\nDie Abbildung 14.6 zeigt dir, wie du die Shiny App starten kannst. Zum einen über Button &gt; Run App direkt aus dem R Studio heraus. Dann öffnet sich ein anderes Fenster und R friert ein. Du kannst während eine Shiny App läuft nicht in R programmieren. Beende dafür erst die Shiny App durch das schließen des Fensters. Die zweite Möglichkeit ist, die Shiny App zu veröffentlichen. Die Veröffentlichung findet auf https://www.shinyapps.io/ statt und du musst dich dort erst anmelden. Dann kannst du dein RStudio über einen Tocken mit der Webseite verbinden. Es gibt einen kostenlosen Account, der eine sehr begrenzte Laufzeit der Apps erlaubt. Daher immer die Shiny Apps auf dem Dashboard von https://www.shinyapps.io/ stoppen und auf Sleeping setzen. Das Schöne ist jetzt, dass du über einen Browser weltweit auf die Shiny App zugreifen kannst.\n\n\n\n\n\n\nAbbildung 14.6— Mit dem Button &gt; Run App starten wir die Shiny App aus dem R Studio heraus. Wenn du auf den Pfeil nach unten klickst, dann kannst du auch andere Optionen für den Start der Shiny App wählen. Als zweite Möglichkeit auf den Button Publish rechts neben &gt; Run App klicken.\n\n\n\nIn der Abbildung 14.7 siehst du jetzt das Ergebnis der gestarteten Shiny App Hello_Shiny. Auf der rechten Seite siehst du den Regler, der die Anzahl an bins definiert. Hier sind die bins die Anzahl an Säulen in dem Histogramm. Alles was du hier siehst, wird in der ui definiert. Der R Code für die dynamische Erstellung des Histogramms ist dann in dem server zu finden. Ja, es ist etwas ironisch, dass ich hier eine dynamische Webseite mit einem Screenshot darstelle.\n\n\n\n\n\n\nAbbildung 14.7— Ausgabe der Hello_Shiny Shiny App. Etwas sinnlos eine dynamische App durch einen Screenschots darzustellen . Aber so sieht dann diese sehr simple App aus.\n\n\n\nUnd damit haben wir unsere R Shiny App auch schon fertig. Das war jetzt das Beispiel für eine Shiny App in einer Datei. Jetzt schauen wir uns das Ganze nochmal an, wenn wir eine komplexere Shiny App bauen würden.\n\n\n14.3.3 Komplexere Shiny App\nIm Folgenden schauen wir uns eine komplexere Shiny App an. Die Shiny App ist selber die gleiche wie eben schon, nur das wir die ui und den server auf zwei Dateien aufteilen sowie den Aufruf der Shiny App dann in eine dritte Datei auslagern. Das macht dann Sinn, wenn die ui sehr groß wird oder aber du noch sehr viel ändern möchtest. Schnell wird dann mal eine Datei sehr unübersichtlich. Das gleiche gilt dann auch für den server schnell hast du so viele Funktionen, da macht es dann Sinn, die Funktionen in eine Extradatei auszulagern. Wenn du die Shiny App aus dem RStudio startest, dann brauchst du die app.R mit der Funktion runApp() nicht, dann starte die Shiny App aus der ui.R Datei. In der Abbildung 14.8 siehst du den Zusammenhang nochmal schematisch dargestellt.\n\n\n\n\n\n\nflowchart LR\n    subgraph ide1 [\"~/Documents/GitHub/shiny/Hello_Shiny\"]\n    direction LR\n    B[(ui.R)]:::nodeB --&gt; A{app.R}:::nodeA \n    C[(server.R)]:::nodeB --&gt; A{app.R}:::nodeA\n    end\n    classDef nodeA fill:#E69F00,stroke:#333,stroke-width:0.75px\n    classDef nodeB fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    \n\n\n\n\nAbbildung 14.8— Für eine komplexere Shiny App brauchen wir drei Dateien. Deshalb ist es immer gut eine Ordnerstruktur zu haben. Hier liegen unsere Dateien ui.R, server.R und app.R im Ordner Hello_Shiny.\n\n\n\n\n\nWir du im Folgenden siehst, besteht die Datei app.R nur noch aus dem Aufruf runApp(). Mit der Funktion runApp() startest du dann die Shiny App, die sich in dem entsprechenden Orderpfad befindet. Damit ist eigentlich schon alles gesagt, die Funktion runApp() sucht sich dann die ui.R und die server.R und startet die Shiny App. Deshalb müssen auch unbedingt die beiden Dateien ui.R und server.R in dem Ordner Hello_Shiny liegen. Hier hast du keine Wahl über den Namen der Datei. Alles andere kann natürlich auch noch im Ordner liegen, wie zum Beispiel zusätzliche Daten oder aber ausgelagerte Funktionen, die erst in der Datei server.R geladen werden.\n\n\n\n\n\n\napp.R\n\n\n\n\n\n\nlibrary(shiny)\n\nrunApp(\"~/Documents/GitHub/shiny/Hello_Shiny\")\n\n\n\n\nDie beiden folgenden Dateien ui.R und server.R unterscheiden sich nicht vom Code in der Datei app.R aus dem obigen Beispiel zur simplen Shiny App. Die Funktionen zur ui und server sind jetzt aufgeteilt in die beiden Dateien. Deshalb schaue nochmal oben, wenn dich die Annotation des R Codes interessiert. Ein anderes Beispiel findest du dann am Ende des Kapitels wo ich dann nochmal mit externen Daten arbeite.\n\n\n\n\n\n\nui.R\n\n\n\n\n\n\nlibrary(shiny)\n\n# Define UI for app that draws a histogram ----\nui &lt;- fluidPage(\n\n  # App title ----\n  titlePanel(\"Hello Shiny!\"),\n\n  # Sidebar layout with input and output definitions ----\n  sidebarLayout(\n\n    # Sidebar panel for inputs ----\n    sidebarPanel(\n\n      # Input: Slider for the number of bins ----\n      sliderInput(inputId = \"bins\",\n                  label = \"Number of bins:\",\n                  min = 1,\n                  max = 50,\n                  value = 30)\n\n    ),\n\n    # Main panel for displaying outputs ----\n    mainPanel(\n\n      # Output: Histogram ----\n      plotOutput(outputId = \"distPlot\")\n\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\n\nserver.R\n\n\n\n\n\n\n# Define server logic required to draw a histogram ----\nserver &lt;- function(input, output) {\n\n  # Histogram of the Old Faithful Geyser Data ----\n  # with requested number of bins\n  # This expression that generates a histogram is wrapped in a call\n  # to renderPlot to indicate that:\n  #\n  # 1. It is \"reactive\" and therefore should be automatically\n  #    re-executed when inputs (input$bins) change\n  # 2. Its output type is a plot\n  output$distPlot &lt;- renderPlot({\n\n    x    &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n    hist(x, breaks = bins, col = \"#007bc2\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\")\n\n    })\n\n}\n\n\n\n\nIn der Abbildung 14.9 ist nochmal die Vernetzung der beiden Dateien ui.R und der server.R dargestellt. Ich stelle mir die Sachen auch gerne als Abbildung dar, wenn ich eine Shiny App plane. Natürlich nicht als fertige App sondern als Skizze auf einem guten, alten Blattpapier. Du siehst hier sehr gut, wie die Option input$bins aus der ui.R in die server.R weitergeleitet werden. Hier ist eigentlich der Witz, sich klar zu machen, was wir wie von der einen Datei in die andere Datei weiterleiten. In Shiny gibt es eine Reihe von Slidern und Möglichkeiten einen Input zu generieren. Da musst du dann immer schauen, dass du den Überblick behälst. Immer einfach anfangen und dann steigern. In der server.R entsteht dann abschließend als Ausgabe das Histogramm, was über den output$distPlot wieder an die ui.R zurückgegebenen wird.\n\n\n\n\n\n\nAbbildung 14.9— Zusammenhang zwischen der ui.R und der server.R. Die ui.R bildet die Oberfläche ab auf der sich die Regler befinden. Die geänderten Reglerinformationen werden über input dann in der server.R Datei verarbeitet und als ouput wieder an die ui.R wiedergeben. Dann ändert sich die Wiedergabe.\n\n\n\n\n\n14.3.4 Best practice\nWie baut man nun am besten eine Shiny App? Da gibt es wie immer viele Wege nach Rom. Ich beschreibe dir einfach meine Gedankengänge bei der Entwicklung einer Shiny App. Je mehr Apps du dann natürlich machst, desto einfacher fällt dir die Geneierung.\n\nBaue immer erst die ui auf und achte darauf, dass du in der Shiny App alles siehst. Du baust dir am besten als erstes das Layout. Ohne eine Verbindung zu dem server. Du kannst ja einfach eine statische Abbildung erstellen lassen. Sonst weiß man immer nicht wo der Fehler herkommt.\nBeginne immer mit einem input und erweitere dann immer Schritt für Schritt die anderen Eingabeparameter. Ich kommentiere mir im Zweifel dann die inputs mit dem # aus, damit ich immer nur an einem Regler und dessen Input arbeite.\nErstelle die Ausgabegrafik oder Tabelle erstmal als statische ggplot Abbildung oder eben als Tabelle. In einer Shiny App etwas zu bauen, ist wirklich sehr anstrengend und zäh.\nSuche dir bei Google oder StackOverflow mit R shiny &lt;suchbegriff&gt; die passende Hilfe. Es gibt eigentlich nichts, was nicht geht, aber wir müssen dann eben die Lösung auch finden.\nEs kann Sinn machen erstmal die Shiny App in nur einer Datei zu bauen und dann später in ui.R und server.R aufzuspalten. Dann hast du beides in einer Datei und musst nicht zwischen den Tabs hin und her wechseln.\n\nAm Ende ist es wie bei jedem Best practice, es kommt auf dich an und wie du programmierst. Lass dich nur nicht frsutrieren, die Verkettung von verschiedenen Funktionen macht natürlich auch eine Shiny App etwas fehleranfälliger bei der Entwicklung.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Quarto und Shiny App</span>"
    ]
  },
  {
    "objectID": "programing-quarto-shiny.html#our-world-in-data",
    "href": "programing-quarto-shiny.html#our-world-in-data",
    "title": "14  Quarto und Shiny App",
    "section": "14.4 Our World in Data",
    "text": "14.4 Our World in Data\nAm Ende wollen wir uns nochmal ein echtes Beispiel mit großen Daten anschauen. Deshalb bin ich zur Webseite Our World in Data gegangen und habe mir als Beispiel die Daten zum Thema Global emissions have not yet peaked heruntergeladen. Die Datei heißt annual-co-emissions-by-region.csv und wurde von mir nur etwas bearbeitet. Oh, was für ein Segen hier dann clean_names() aus dem R Paket {janitor} ist. Wir bauen jetzt hier erstmal die statische Abbildung in ggplot und überlegen, was wir alles machen könnten. Dann geht es weiter mit der Shiny App.\n\nco2_tbl &lt;- read_csv(\"data/annual-co-emissions-by-region.csv\") |&gt; \n  clean_names()\n\nIn der Tabelle 14.1 siehst du nochmal einen Auszug aus der Datei co2_tbl. Die Datei ist wirklich groß und beinhaltet sehr viele Länder und Jahre. Wir werden deshalb nur eine Auswahl an Ländern uns anschauen. Ich wähle relativ zufällig und aus Interesse world, Europe, China und Africa. Du könntest auch alle Länder nehmen, aber dann braucht die Grafik sehr lange und ich erkenne dann auch nichts mehr in der Abbildung. Aber probiere es einfach selber aus.\n\n\n\n\nTabelle 14.1— Auszug aus der Datei zur globalen CO2 Emission für verschiedne Länder.\n\n\n\n\n\n\nentity\ncode\nyear\nannual_co2_emissions\n\n\n\n\nAfghanistan\nAFG\n1949\n14656\n\n\nAfghanistan\nAFG\n1950\n84272\n\n\nAfghanistan\nAFG\n1951\n91600\n\n\nAfghanistan\nAFG\n1952\n91600\n\n\nAfghanistan\nAFG\n1953\n106256\n\n\nAfghanistan\nAFG\n1954\n106256\n\n\nAfghanistan\nAFG\n1955\n153888\n\n\n\n\n\n\n\n\nNun können wir uns in der Abbildung 14.10 einmal das Ergebnis der Visualisierung über die Jahre anschauen. Wir sehen, dass wir eine Bandbreite von 1750 bis 2021 vorliegen haben. Dann habe ich noch die Namen der Länder rechts an die Graphen ergänzt.\n\nco2_tbl |&gt; \n  filter(entity %in% c(\"World\", \"Europe\", \"China\", \"Africa\")) |&gt; \n  ggplot(aes(year, annual_co2_emissions, color = entity)) +\n  theme_minimal() +\n  geom_line() +\n  theme(legend.position = \"none\") +\n  scale_x_continuous(expand = c(0, 30), breaks = c(seq(1750, 2021, by = 25), 2021)) +\n  geom_dl(aes(label = entity), method = list(dl.trans(x = x + 0.2), \"last.points\", cex = 0.8))\n\n\n\n\n\n\n\nAbbildung 14.10— kkkn\n\n\n\n\n\nWas könnten wir jetzt in einer Shiny App dynamisch ändern? Ich habe mir da drei Dinge überlegt.\n\nWir ändern die Auswahl an Jahren. Also die Spannbreite der Jahre, die wir uns auf der x-Achse anschauen wollen. Das können wir über die Funktion filter() in dem server abbilden, indem wir einfach einen Bereich an Jahren filtern. In der ui brauchen wir dann einen Schiebregeler, der uns die Auswahl ermöglicht.\nWir können über die Funktion stat_smooth() noch eine Gerade anpassen, die nicht durch alle Messpunkte läuft. Wir wollen also einen Smoother anpassen. Da gibt es aber mehrere Arten von. Deshalb wäre es schön zwischen lm, loess und gam wählen zu können. Wir brauchen hier also ein Dropdownmenü in der ui, was uns die Auswahl erlaubt.\nWir können in stat_smooth einen Standardfehler als Bereich einblenden lassen. Dafür müssen wir die Option se = TRUE setzen. Da können wir uns dann einen Button in der ui erstellen, der den Standardfehler einblendet oder ausblendet.\n\nIm Folgenden siehst du dann einmal die annotierte app.R, die dir dann die Shiny App in der Abbildung 14.11 erstellt. Das Ganze geht natürlich noch viel schöner und besser, aber hier will ich einmal das Prinzip zeigen.\n\npacman::p_load(shiny, tidyverse, magrittr, readxl,                   \n               janitor, see, directlabels,                          \n               conflicted)\nconflicts_prefer(dplyr::filter)\n\n## read in the need data  \n1co2_tbl &lt;- read_csv(\"annual-co-emissions-by-region.csv\") |&gt;\n  clean_names()\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Global emissions have not yet peaked\"), \n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel( \n2      sliderInput(\"years\",\n                  \"Shown years:\",\n                  min = 1750,\n                  max = 2021,\n                  step = 25,\n                  value = c(1750, 2021)),\n      \n3      selectInput(\"method\", \"Method for line:\",\n                  c(\"Linear\" = \"lm\",\n                    \"Loess\" = \"loess\",\n                    \"Gam\" = \"gam\")),\n      \n4      radioButtons(\"se\",\n                   \"Standard deviation:\",\n                   c(\"Yes\" = \"yes\",\n                     \"No\" = \"no\")),\n      \n    ),\n    # Show a plot of the generated distribution\n    mainPanel(\n5      plotOutput(\"line_plot\")\n    )\n  )\n)\n\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$line_plot &lt;- renderPlot({\n\n6    se_flag &lt;- switch(input$se,\n                      yes = TRUE,\n                      no = FALSE)\n    # ggplot from above with the options from `input`\n    co2_tbl |&gt; \n      filter(entity %in% c(\"World\", \"Europe\", \"China\", \"Africa\")) |&gt; \n7      filter(year &gt;= input$years[1] & year &lt;= input$years[2]) |&gt;\n      ggplot(aes(year, annual_co2_emissions, color = entity)) +\n      theme_minimal() +\n      geom_line() +\n8      stat_smooth(method = input$method, se = se_flag) +\n      theme(legend.position = \"none\") +\n      scale_x_continuous(expand = c(0, 30), \n                         breaks = c(seq(1750, 2021, by = 25), 2021)) +\n      geom_dl(aes(label = entity), \n              method = list(dl.trans(x = x + 0.2), \n                            \"last.points\", cex = 0.8))\n    \n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n1\n\nDie Daten annual-co-emissions-by-region.csv im gleichen Ordner werden geladen.\n\n2\n\nDer Slider für die Jahre wird erstellt. Wir wollen immer in 25 Jahre Schritten weitergehen.\n\n3\n\nDas Dropdownmenü wird gesetzt. Wir haben hier die drei Methoden lm, loess und gam voreingestellt.\n\n4\n\nDer Button für den Standardfehler ja/nein für den Smoother in der Abbildung.\n\n5\n\nUnser Plot aus dem server wird dann line_plot heißen und wird hier wiedergeben.\n\n6\n\nDer input$se wird in TRUE oder FALSE aufgelöst. Brauchen wir gleich bei se = se_flag.\n\n7\n\nWir filtern nach der unteren Grenze des Schiebers input$years[1] und der oberen Grenze es Schiebers input$years[2]\n\n8\n\nWir generieren die Gerade nach der Methode input$method und entscheiden, ob wir einen Standardfehler einzeichnen wollen TRUE/FALSE.\n\n\n\n\nWie schon oben geschrieben, erhalten wir am Ende die Shiny App in der Abbildung 14.11. Ich habe hier die Jahre von 1900 bis 2021 gewählt und mit als Methode für Gerade dann Loess ausgewählt. Ich wollte mir zwar den Standardfehler einzeichnen lassen, aber die Methode Loess gibt keinen Fehler zurück. Deshalb dann hier auch kein Standardfehler. Wenn du dir die Methode Linear anschaust, wirst du dann einen Bereich für den Standardfehler eingezeichnet bekommen.\n\n\n\n\n\n\nAbbildung 14.11— Ausgabe der co2_emissons Shiny App. Etwas sinnlos eine dynamische App durch einen Screenschots darzustellen . Aber so sieht dann diese etwas komplexere App aus.\n\n\n\nUnd jetzt? Naja, es ist so, dass es für R Shiny Apps sehr viele Quellen und Anwendungen gibt. Hier heißt es dann mal selber probieren. Ich finde persönlich, dass mit R Shiny die Flexibilität besteht Laien, die mit dem ganzen Modellieren nichts zu tun haben, eine Möglichkeit zu geben selber eine explorative Datenanalyse durchzuführen. Wenn eine Shiny App gut gemacht ist, dann kannst du mit deinem Team super zusammen in Daten stöbern. Und das ist ja die Stärke von einer dynamischen Abbildung, dass du dann in einem Meeting noch die Abbildung ändern kannst.\n\n\n\nAbbildung 14.1— Um eine neues R Quarto Dokument zu erstellen klicken wir einmal auf das ‘+’ oben links und wählen dann Quarto Document... aus.\nAbbildung 14.2— Einmal die Maske für die Erstellung des R Quarto Dokuments. Du kannst später auch zwischen HTML, PDF und Word wechseln.\nAbbildung 14.3— Um eine neue R Shiny Web App zu erstellen klicken wir einmal auf das ‘+’ oben links und wählen dann Shiny Web App... aus.\nAbbildung 14.4— Wir brauchen jetzte ien Namen für die Shiny Web App Application name und müssen uns entscheiden, ob wir mit einer Datei Single file (app.R) oder mehrere Dateien Multiple File (ui.R/server.R) arbeiten wollen. In beiden Fällen wird ein neuer Ordner mit dem Namem der Shiny Web App erstellt.\nAbbildung 14.6— Mit dem Button &gt; Run App starten wir die Shiny App aus dem R Studio heraus. Wenn du auf den Pfeil nach unten klickst, dann kannst du auch andere Optionen für den Start der Shiny App wählen. Als zweite Möglichkeit auf den Button Publish rechts neben &gt; Run App klicken.\nAbbildung 14.7— Ausgabe der Hello_Shiny Shiny App. Etwas sinnlos eine dynamische App durch einen Screenschots darzustellen . Aber so sieht dann diese sehr simple App aus.\nAbbildung 14.9— Zusammenhang zwischen der ui.R und der server.R. Die ui.R bildet die Oberfläche ab auf der sich die Regler befinden. Die geänderten Reglerinformationen werden über input dann in der server.R Datei verarbeitet und als ouput wieder an die ui.R wiedergeben. Dann ändert sich die Wiedergabe.\nAbbildung 14.10— kkkn\nAbbildung 14.11— Ausgabe der co2_emissons Shiny App. Etwas sinnlos eine dynamische App durch einen Screenschots darzustellen . Aber so sieht dann diese etwas komplexere App aus.",
    "crumbs": [
      "Programmieren in R",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Quarto und Shiny App</span>"
    ]
  },
  {
    "objectID": "eda-preface.html",
    "href": "eda-preface.html",
    "title": "Explorative Datenanalyse",
    "section": "",
    "text": "Letzte Änderung am 24. March 2024 um 17:04:07\n\n“Richtiges Auffassen einer Sache und Missverstehen der gleichen Sache schließen einander nicht vollständig aus.” — Franz Kafka, Vor dem Gesetz\n\nIn dem vorherigen Kapiteln haben wir uns mit dem Programmieren in R beschäftigt. Wir haben gelernt mit einem Computer durch eine Programmiersprache zu kommunizieren. Jetzt haben wir die Daten in R Eingelesen und im Zweifel noch angepasst. Nun wollen wir uns die Daten einmal angucken. Nicht in dem Sinne, dass wir nur auf die Datentabelle schauen. Sondern wir wollen die Daten visualisieren und damit Zusammenhänge aufdecken. Wir erstellen Abbildungen von den Daten und versuchen so mehr über die Daten zu erfahren. In dem Zusammenhang können wir auch Tabellen erstellen, die uns die Daten zusammenfassen. Das führt uns dann schon näher an die eigentlichen Forschungsfragen, als es die puren Daten tun. Sehen wir Zusammenhänge zwischen verschiedenen Variablen bzw. Spalten? Dazu führen wir eine explorative Datenanalyse durch (abk. EDA). Über die explorative Datenanalyse wollen wir uns nun in den folgenden Kapitel einmal Gedanken machen.",
    "crumbs": [
      "Explorative Datenanalyse"
    ]
  },
  {
    "objectID": "eda-descriptive.html",
    "href": "eda-descriptive.html",
    "title": "15  Deskriptive Statistik",
    "section": "",
    "text": "15.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, readxl, gtsummary,\n               janitor, see, patchwork, modelsummary)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#daten",
    "href": "eda-descriptive.html#daten",
    "title": "15  Deskriptive Statistik",
    "section": "15.2 Daten",
    "text": "15.2 Daten\nIn diesem Kapitel wollen wir uns einmal die Daten zu den Sprungweiten verschiedener Hundeflöhe sowie die Eigenschaften der Hunde einmal genauer ansehen. Wir laden also als erstes einmal die Daten und filtern nur die Hunde aus den Daten. Wir wollen jetzt die Spalten oder eben auch Variablen der Daten einmal sinnvoll zusammenfassen und durch wenige statistische Maßzahlen beschreiben.\n\nflea_dog_tbl &lt;- read_excel(\"data/flea_dog.xlsx\") \n\nDie Tabelle 15.2 zeigt nochmal die Daten dargestellt. Zur Erinnerung, wir wollen jetzt die Spalten mit wenigen Zahlen zusammenfassen.\n\n\n\n\nTabelle 15.2— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Boniturnote sowie der Infektionsstatus von Hundeflöhen.\n\n\n\n\n\n\nanimal\njump_length\nflea_count\nweight\ngrade\ninfected\n\n\n\n\ndog\n5.7\n18\n2.1\n8\n0\n\n\ndog\n8.9\n22\n2.3\n8\n1\n\n\ndog\n11.8\n17\n2.8\n6\n1\n\n\ndog\n5.6\n12\n2.4\n8\n0\n\n\ndog\n9.1\n23\n1.2\n7\n1\n\n\ndog\n8.2\n18\n4.1\n7\n0\n\n\ndog\n7.6\n21\n3.2\n9\n0\n\n\n\n\n\n\n\n\nWir fangen jetzt erstmal mit den kontinuierlichen Variablen an und schauen dann später auf die kategorialen Variablen. Nehmen wir also als erstes Beispiel die Sprungweiten in [cm] von Hundeflöhen. Wir messen die Sprungweiten von sieben Hundeflöhen und erhalten dabei folgende Werte in [cm]: 5.7, 8.9, 11.8, 5.6, 9.1, 8.2 und 7.6. Wir schreiben diese Sprungweiten nun als \\(y\\) in einen Vektor in der folgenden Form auf.\n\\[\ny = \\{5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6\\}.\n\\]\nDu findest in Kapitel 9 den Einstieg für die Programmierung in R. Da findest du auch die Erklärung für c() und den Zuweisungspfeil &lt;-.\nIn R würde der Vektor der Zahlen etwas anders aussehen, aber das hat eher was mit der Schreibweise der Mathematik und der Informatik zu tun. Inhaltlich sehen wir hier gleichen Informationen.\n\ny &lt;- c(5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6) \n\nWir wollen nun die Sprungweiten in \\(y\\) beschrieben und durch wenige andere Zahlen zusammenfassen. Einige dieser statistischen Maßzahlen sind dir vermutlich schon bekannt, andere eher neu. Wir gehen jetzt also gemeinsam eine Menge an statistischen Maßzahlen durch. Nicht alle wirst du immer brauchen und dir auch nicht merken müssen. Wichtig ist zu wissen, dass es die folgenden statistischen Maßzahlen gibt und das du dann im Zweifel den Begriff nachschlagen kannst. Ich mache das im Prinzip auch nicht anders.\n\n\n\n\n\n\nWarum \\(y\\) und nicht \\(x\\)?\n\n\n\nWenn du dir andere Statistikbücher anschaust, dann wird meist die Variable, mit der wir den Mittelwert berechnen, mit \\(x\\) bezeichnet. Wir machen das hier nicht denn wir haben später für das \\(x\\) eine andere Verwendung. In einem statistischen Modell schauen wir uns den Zusammenhang von \\(y \\sim x\\) an. Die Sprungweite von Hundeflöhen ist das \\(y\\) und nicht das \\(x\\). Glaube mir, es wird nachher leichter zu verstehen. Dafür nehmen wir jetzt einen etwas schwierigen Weg.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#arithmetisches-mittel",
    "href": "eda-descriptive.html#arithmetisches-mittel",
    "title": "15  Deskriptive Statistik",
    "section": "15.3 Arithmetisches Mittel",
    "text": "15.3 Arithmetisches Mittel\nDer Mittelwert einer Zahlenreihe beschreibt den Schwerpunkt der Zahlen. Was soll das Bedeuten? Was ist der Schwerpunkt von Zahlen? Du kannst dir das wie eine Wippe vorstellen, der Mittelwert liegt so, dass links und rechts des Mittelwerts gleiche Abstände zu den Zahlen vorliegen. Der Mittelwert wird auch als Lageparameter benannt. Der Mittelwert ist damit auch der Lageparameter einer Verteilung. Er beschreibet die numerische Stelle, wo die Verteilungskurve am höchsten ist. Wir schreiben den Mittelwert mit einem Strich über den Vektor, der die Zahlen enthält. Im Folgenden ist die Formel für den Mittelwert der Sprungweite in [cm] der Hunde gezeigt. Der Mittelwert ist in dem Sinne eine künstliche Zahl, da der Mittelwert häufig nicht in den beobachteten Zahlen vorkommt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWir werden immer mal wieder Formeln vereinfachen. Zum Beispiel nur \\(\\sum\\) schreiben anstatt \\(\\sum_i^n\\), wenn wir einen Vektor aufsummieren und uns die Indizes sparen. Ja, das ist nicht schön, aber einfacher.\n\n\n\n\nIm Folgenden sehen wir einmal wie der Mittelwert mathematisch oder schrittweise in R berechnet wird. Beachte hierbei die unterschiedliche Schreibweise. Wir haben hier ja auch zwei unterschiedliche Sprachen abgebildet. Wir haben nämlich eine mathematische Sprache und einmal eine Programmiersprache vorliegen.\n\nMathematikR (Schritt für Schritt)R (Built-in)\n\n\n\\[\n\\bar{y} = \\sum_{i=1}^{n}\\cfrac{y_i}{n} =\n\\cfrac{5.7 + 8.9 + 11.8 + 5.6 + 9.1 + 8.2 + 7.6}{7} = \\cfrac{56.9}{7} = 8.13\n\\]\n\n\nEinmal der Vektor y mit den Zahlen, die wir benutzen wollen.\n\ny &lt;- c(5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6) \ny\n\n[1]  5.7  8.9 11.8  5.6  9.1  8.2  7.6\n\n\nDie Summe von y.\n\nsum(y)\n\n[1] 56.9\n\n\nDie Länge des Vektors y oder eben die Anzahl \\(n\\) gleich der Fallzahl.\n\nlength(y)\n\n[1] 7\n\n\nUnd dann einmal händisch der Mittelwert von y berechnet.\n\nsum(y)/length(y)\n\n[1] 8.128571\n\n\n\n\n\nmean(y)\n\n[1] 8.128571\n\n\n\n\n\nWir sagen also, dass im Durchschnitt oder im Mittel die Hundeflöhe 8.13 cm weit springen. In der Abbildung 15.1 wollen wir die Formel nochmal visualisieren. Vielleicht fällt dir dann der Zusammenhang von dem Index \\(i\\) und der gesamten Fallzahl \\(n\\) leichter.\n\n\n\n\n\n\nAbbildung 15.1— Zusammenhang zwischen \\(y\\) sowie dem Index \\(i\\) in der Formel für den Mittelwert.\n\n\n\nIn R können wir den Mittelwert einfach mit der Funktion mean() berechnen. Dann nutzen wir nochmal den Pipe Operator |&gt; für die Berechnung des Mittelwertes. Wir wollen dann den Mittelwert noch auf die zweite Kommastelle runden. Das machen wir dann mit der Funktion round(). Du findest in Kapitel 9 den Einstieg für die Programmierung in R. Da findest du auch die Erklärung für den Pipe Operator |&gt;.\n\n## mit pipe-Operator\ny |&gt; mean() |&gt; round(2)\n\n[1] 8.13\n\n\nWir erhalten das gleiche Ergebnis wie oben in unserer händischen Rechnung. Die Hundeflöhe springen im Mittel 8.13 cm weit. Ja, für so einfache Beispiele ist es natürlich etwas kompliziert R zu nutzen und Code zu schreiben, später wird uns der Code aber sehr helfen.\nDer Mittelwert ist eine bedeutende statistische Maßzahl der Normalverteilung. Später wirst du noch mehr über Verteilungen von Zahlen erfahren und deshalb hier schon mal die kurze Verbindung vom Mittelwert zur Normalverteilung. Daher merken wir uns hier schon mal, dass wir den Mittelwert brauchen werden. Auch wenn wir darüber nachdenken ob sich zwei Gruppen unterscheiden, so nutzen wir hierzu den Mittelwert. Unterscheiden sich die mittleren Sprungweiten in [cm] von Hunde- und Katzenflöhen?",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#sec-desc-stat-geometric",
    "href": "eda-descriptive.html#sec-desc-stat-geometric",
    "title": "15  Deskriptive Statistik",
    "section": "15.4 Geometrisches Mittel",
    "text": "15.4 Geometrisches Mittel\nIm Gegensatz zum arithmetischen Mittel ist das geometrische Mittel nur für nicht negative Zahlen \\(y\\) definiert. Wir können also für negative Zahlen kein geometrische Mittel berechnen. Das geometrische Mittel ist daher nur für echt positive Zahlen sinnvoll. Wenn eine der Zahlen in \\(y\\) gleich Null ist, ist schon das ganze Produkt der Zahlen gleich Null. Somit ist dann das geometrische Mittel sinnlos. Schauen wir uns einmal die Formel des geometrischen Mittels an.\n\\[\n\\bar{y}_\\mathrm{geom} = \\sqrt[n]{\\prod_{i=1}^n{y_i}} = \\sqrt[n]{y_1 \\cdot y_2 \\dotsm y_n} = (y_1 \\cdot y_2 \\dotsm y_n)^{1/n}\n\\]\nDann rechnen wir doch auch gleich mal ein Beispiel mit den vier Zahlen \\(y = \\{4, 3, 6, 9\\}\\).\n\\[\n\\bar{y}_\\mathrm{geom} =  \\sqrt[4]{4 \\cdot 3 \\cdot 6 \\cdot 9} =\\sqrt[4]{648} = 5.045\n\\]\nUm die Besonderheit des geometrische Mittelwerts zum arithmetischen Mittel zu verstehen, nehmen wir einmal ein Beispiel mit nur den Zahlen \\(y = \\{1,9\\}\\). Bei der geometrischen Mittelwertbildung weichen beide Werte vom Mittelwert um denselben Multiplikator ab. Diese Eigenschaft ist beim arithmetischen Mittel nicht gegeben. So ergibt sich aus den Zahlen 1 und 9 das arithmetische Mittel 5. Dabei ist die 1 vom Mittelwert 5 um Faktor 5 entfernt, während die 9 lediglich um Faktor 1.8 davon entfernt liegt.\nDas geometrische Mittel aus 1 und 9 hingegen ergibt den Mittelwert \\(\\bar{y}_\\mathrm{geom} = 3\\). Sowohl der niedrige Wert 1 wie auch der hohe Wert 9 sind vom Mittelwert 3 um den Faktor 3 entfernt. Der Unterschied zwischen arithmetischem und geometrischem Mittelwert kann beträchtlich sein, was in der Praxis unter Umständen zur Fehlinterpretation von Durchschnittsangaben führt.\nIn R können wir das geometrische Mittel einfach mit der Funktion geometric.mean() aus dem R Paket {psych} berechnen.\n\nc(4, 3, 6, 9) |&gt; \n  psych::geometric.mean()\n\n[1] 5.045378\n\n\nDas geometrische Mittel ist der einzige korrekte Mittelwert, wenn normalisierte Ergebnisse gemittelt werden, daher Zahlen und Ergebnisse, die als Verhältnis zu Referenzwerten dargestellt werden. Eine Anwendung ist hierbei der Wirkungsgrad von zum Beispiel Pflanzenschutzmitteln.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#spannweite",
    "href": "eda-descriptive.html#spannweite",
    "title": "15  Deskriptive Statistik",
    "section": "15.5 Spannweite",
    "text": "15.5 Spannweite\nDie Spannweite erlaubt uns zu überprüfen was die kleinste Zahl und die größte Zahl ist. Also uns das Minimum und das Maximum einer Zahlenreihe anzuschauen. Auf den ersten Blick mag das nicht so sinnig sein, aber wenn wir uns hunderte von Beobachtungen anschauen, wollen wir wissen, ob wir nicht einen Fehler bei Eintragen der Daten gemacht haben. Wir wissen eigentlich, dass z.B keine negativen Zuwachsraten auftreten können.\n\\[\ny_{range} = y_{max} - y_{min} = 12.1 - 4.9 = 7.2\n\\]\nDie Hundeflöhe springen in einer Spannweite von 7.2 cm. Das kommt einem normal vor und wir würden meinen, dass Flöhe in diesem Bereich springen könnten. Die Spannweite ist nicht übertrieben groß. Der minimale Wert ist \\(4.9cm\\) und der maximale Wert ist \\(12.1cm\\) und somit sind beide Zahlen in Ordnung. Keine der beiden Zahlen ist übertrieben klein oder gar negativ. Später musst du dann selber schauen, ob die Zahlen in die biologische Fragestellung passen. Besonders wenn du Datenbanken ausliest, kann es schnell passieren, dass du unplausible Werte wiederfindest. Dann musst du diese Werte meist entfernen.\nIn R können wir die Spannweite mit range() wie folgt berechnen. Wir erhalten den minimalen und maximalen Wert.\n\n## ohne pipe-Operator\nrange(y) \n\n[1]  5.6 11.8\n\n## mit pipe-Operator\ny |&gt; range()\n\n[1]  5.6 11.8\n\n\nWir merken uns, dass die Spannweite eine Maßzahl für die Validität der Daten ist. Hat das Experiment geklappt oder kamen da nur komische Zahlen bei raus, die wir so in der Realität nicht erwarten würden. Zum Beispiel negative Sprungweiten, weil wir einmal auf das Minuszeichen auf der Tastatur beim eingeben der Zahlen gekommen sind.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#varianz",
    "href": "eda-descriptive.html#varianz",
    "title": "15  Deskriptive Statistik",
    "section": "15.6 Varianz",
    "text": "15.6 Varianz\nBis jetzt können wir mit dem Mittelwert \\(\\bar{y}\\) die Lage oder den Mittelpunkt unserer Zahlenreihe \\(y\\) beschreiben. Uns fehlt damit aber die Information über die Streuung der Zahlen. Sind die Zahlen alle eher gleich oder sehr verschieden? Liegen die Zahlen daher alle bei dem Mittelwert oder sind die Zahlen weit um den Mittelwert gestreut. Es macht ja einen bedeutenden Unterschied, ob wir \\(n\\)-mal das “Gleiche” messen oder eben \\(n\\)-mal sehr unterschiedliche Zahlen.\n\n\n\n\n\n\nDie Abweichungsquadrate in der Statistik\n\n\n\nAbweichungsquadrate sind ein wichtiges Konzept in der Statistik. Wenn wir wissen wollen, wie groß eine Abweichung von einer Zahl zu einer anderen ist, dann nutzen wir immer das Quadrat der Abweichung und bilden die Quadratsumme.\n\n\nDie Streuung der Zahlen \\(y\\) um den Mittelwert beschreibt die Varianz. Wir schreiben für die Varianz auch \\(s^2\\). Wir berechnen die Varianz indem wir von jeder Zahl \\(i\\) den Mittelwert aller Zahlen \\(\\bar{y}\\) abziehen und dann das Ergebnis quadrieren. Das machen wir für alle Zahlen und addieren dann die Summe auf. Wir erhalten die Quadratsumme von \\(y\\). Wie immer ist Mathematik in Textform etwas sperrig, deshalb einmal auch hier die Formel. Dann kannst du dir auch noch die Umsetzung in R anschauen. Auch hier die etwas sperrige Form in Schritten, später nutzt du dann die Funktion var().\n\nMathematikR (Schritt für Schritt)R (Built-in)\n\n\n\\[\ns^2 = \\sum_{i=1}^n\\cfrac{(y_i - \\bar{y})^2}{n-1} = \\cfrac{(5.7 -\n8.13)^2 + ... + (7.6 - 8.13)^2}{7-1} = \\cfrac{27.59}{6} = 4.6\n\\]\n\n\nEinmal der Vektor y mit den Zahlen, die wir benutzen wollen.\n\ny &lt;- c(5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6)\ny\n\n[1]  5.7  8.9 11.8  5.6  9.1  8.2  7.6\n\n\nDie Summe von y.\n\nsum(y)\n\n[1] 56.9\n\n\nDie Länge des Vektors y minus 1 oder eben die Anzahl \\(n - 1\\) gleich der Fallzahl.\n\nlength(y) - 1\n\n[1] 6\n\n\nDann einmal die Formel für die Varianz zusammengebaut. Statt \\(8.13\\) kannst du auch sum(y)/length(y) oder eben mean(y) schreiben.\n\nsum((y - 8.13)^2)/(length(y) - 1)\n\n[1] 4.59905\n\n\n\n\n\nvar(y)\n\n[1] 4.599048\n\n\n\n\n\nDie Varianz beschreibt also die Streuung der Zahlen im Quadrat um den Mittelwert. Das heißt in unserem Beispiel, dass die Sprungweite eine Varianz von \\(4.6cm^2\\) hat. Wir können Quadratzentimeter schlecht biologisch interpretieren. Wir leben nicht in einer Welt mit quadratischen Einheiten. Deshalb führen wir gleich die Wurzel der Varianz als die Standardabweichung ein. Die Standardabweichung ist also einfach die Wurzel der Varianz. Da die Standardabweichung aber sehr bedeutend ist, kommen wir auf die Standardabweichung gleich nochmal in einem extra Abschnitt zu sprechen. Wir benötigen die Varianz häufig nur als Zwischenschritt um die Standardabweichung zu berechnen. Das Konzept der Abweichungsquadrate benötigen wir aber in der Varianzanalyse (ANOVA) und für die Beschreibung einer Normalverteilung zusätzlich zum Mittelwert.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#standardabweichung",
    "href": "eda-descriptive.html#standardabweichung",
    "title": "15  Deskriptive Statistik",
    "section": "15.7 Standardabweichung",
    "text": "15.7 Standardabweichung\nDie Standardabweichung \\(s\\) ist die Wurzel der Varianz \\(s^2\\). Wo die Varianz die Abweichung der Sprungweite in [cm\\(^2\\)] beschreibt, beschreibt die Standardabweichung die Streuung der Sprungweite in [cm]. Damit können wir dann die Standardabweichung viel besser interpretieren als die Varianz.\n\nMathematikR (Schritt füt Schritt)R (Built-in)\n\n\n\\[\ns = \\sqrt{s^2} = \\sqrt{4.6} = 2.14\n\\]\n\n\nEinmal der Vektor y mit den Zahlen, die wir benutzen wollen.\n\ny &lt;- c(5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6)\ny\n\n[1]  5.7  8.9 11.8  5.6  9.1  8.2  7.6\n\n\nDie Summe von y.\n\nsum(y)\n\n[1] 56.9\n\n\nDie Länge des Vektors y minus 1 oder eben die Anzahl \\(n - 1\\) gleich der Fallzahl.\n\nlength(y) - 1\n\n[1] 6\n\n\nDann einmal die Formel für die Varianz zusammengebaut. Statt \\(8.13\\) kannst du auch sum(y)/length(y) oder eben mean(y) schreiben.\n\nsum((y - 8.13)^2)/(length(y) - 1)\n\n[1] 4.59905\n\n\nDann noch die Wurzel aus der Varianz um die Standardabweichung zu berechnen. Denn nichts anderes haben wir ja bis jetzt noch nicht berechnet.\n\nsqrt(sum((y - 8.13)^2)/(length(y) - 1))\n\n[1] 2.14454\n\n\n\n\n\nsd(y)\n\n[1] 2.144539\n\n\n\n\n\nWir können also schreiben, dass die Flöhe im Mittel 8.13 \\(\\pm\\) 2.14cm weit springen. Somit haben wir die Lage und die Streuung der Zahlenreihe \\(y\\) der Sprungweite in [cm] mit zwei Zahlen beschrieben. Wichtig ist hierbei zu merken, dass wir den Abstand vom Mittelwert nach “oben” und “unten” jeweils mit dem gleichen Abstand beschreiben. Unsere Zahlen können also nicht mehr in die eine als in die andere Richtung streuen. Merke dir hier nochmal, dass du die Varianz nur als Zwischenschritt für die Standardabweichung berechnest. Später interessiert dich die Standardabweichung, die du dann auch in deinen Berichten oder deiner Abschlussarbeit nutzt.\n\n\n\n\n\n\nSchreibweise des Mittelwerts und der Standardabweichung\n\n\n\nWir schreiben immer den Mittelwert plusminus die Standardabweichung. Also immer in der Form \\(\\bar{y} \\pm s\\). Meistens willst du dann aber nicht eine Tabelle haben, sondern wir nutzen dann den Mittelwert plusminus die Standardabweichung um uns Säulendiagramme zu erstellen.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#mittelwert-und-varianz---eine-herleitung",
    "href": "eda-descriptive.html#mittelwert-und-varianz---eine-herleitung",
    "title": "15  Deskriptive Statistik",
    "section": "15.8 Mittelwert und Varianz - eine Herleitung",
    "text": "15.8 Mittelwert und Varianz - eine Herleitung\nWas ist der Mittelwert und die Varianz genau? Schauen wir uns das einmal in Abbildung 15.2 an. Die graue Linie oder Grade beschreibt den Mittelwert der fünf Beobachtungen. Die fünf Beobachtungen sind als blaue Kreuze dargestellt. Auf der \\(x\\)-Achse ist nur der Index \\(i\\) der Beobachtung. Das heißt \\(y_1\\) ist die erste Beobachtung und somit ist der Index \\(i\\) gleich 1. In der Form stellen wir dann alle Beobachtungen in der Abbildung dar. Die eigentlichen Zahlenwerte sind dann auf der \\(y\\)-Achse.\n\n\n\n\n\n\nAbbildung 15.2— Die graue Linie beschreibt den Mittelwert der genau so durch die blauen Punkte geht, dass die Abstände der Punkte oberhalb und unterhalb zu Null aufaddieren. Die Linie liegt in der Mitte der Punkte. Die quadrierten Abstände sind die Varianz der blauen Punkte. Auf der x-Achse ist der Index des Punktes eingetragen.\n\n\n\nWenn wir die Summe der Abweichungen von \\(y_1\\) bis \\(y_5\\) zu dem Mittelwert bilden, so wird diese Summe 0 sein. Der Mittelwert liegt genau in der Mitte der Punkte. In unserem Beispiel ist der Mittelwert \\(\\bar{y} = 5.8\\). Wir können jetzt die Abstände wie in der folgenden Tabelle 15.3 berechnen.\n\n\n\nTabelle 15.3— Zusammenhang von den Werten von \\(y\\), dem Mittelwert sowie die Abweichung vom Mittelwert \\(\\epsilon\\)\n\n\n\n\n\n\n\n\n\n\n\nIndex \\(i\\)\ny\n\\(\\boldsymbol{\\epsilon_i = y_i - \\bar{y}}\\)\nWert\n\n\n\n\n1\n5.7\n\\(\\epsilon_1 = y_1 - \\bar{y}\\)\n\\(5.7 - 8.13 = -2.43\\)\n\n\n2\n8.9\n\\(\\epsilon_2 = y_2 - \\bar{y}\\)\n\\(8.9 - 8.13 = 0.77\\)\n\n\n3\n11.8\n\\(\\epsilon_3 = y_3 - \\bar{y}\\)\n\\(11.8 - 8.13 = 3.67\\)\n\n\n4\n8.2\n\\(\\epsilon_4 = y_4 - \\bar{y}\\)\n\\(8.2 - 8.13 = 0.07\\)\n\n\n5\n5.6\n\\(\\epsilon_5 = y_5 - \\bar{y}\\)\n\\(5.6 - 8.13 = -2.53\\)\n\n\n6\n9.1\n\\(\\epsilon_6 = y_4 - \\bar{y}\\)\n\\(9.1 - 8.13 = 0.97\\)\n\n\n7\n7.6\n\\(\\epsilon_7 = y_5 - \\bar{y}\\)\n\\(7.6 - 8.13 = -0.53\\)\n\n\n\n\n\n\nWir nennen die Abstände \\(y_i - \\bar{y}\\) nach dem griechischen Buchstaben Epsilon \\(\\epsilon\\). Das \\(\\epsilon\\) soll an das \\(e\\) von Error erinnern. So meint dann Error eben auch Abweichung. Ja, es gibt hier viele Worte für das gleiche Konzept. Aber manche Sachen brauchen wir dann wiederholt und es ist gut, die Worte hier schon mal zu lesen. Oder sich später hier wieder an das Konzept zu erinnern. Wir berechnen einen Mittelwert von den Epsilons mit \\(\\bar{\\epsilon} = 0\\). Ein Mittelwert nahe Null bzw. von Null wundert uns nicht. Wir haben die Gerade ja so gebaut, das nach oben und unten die gleichen Abstände sind.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#standardfehler-oder-standard-error-se",
    "href": "eda-descriptive.html#standardfehler-oder-standard-error-se",
    "title": "15  Deskriptive Statistik",
    "section": "15.9 Standardfehler oder Standard Error (SE)",
    "text": "15.9 Standardfehler oder Standard Error (SE)\nDer Standardfehler ist eine statistische Maßzahl, die wir häufig im Kontext des statistischen Testens benutzen. Wenn wir den Mittelwert der Sprungweiten berichten dann gehört die Standardabweichung der Sprungweiten mit als beschreibendes, statistisches Maß dazu. Wir berichten keinen Mittelwert ohne Standardabweichung. Nun ist es aber so, dass der Mittelwert und die Standardabweichung von der Fallzahl abhängen. Je mehr Fallzahl bzw. Beobachtungen wir haben, desto genauer wird der Mittelwert sein. Oder anders ausgedrückt unserer in der Stichprobe unserer Daten ermittelte Mittelwert \\(\\bar{y}\\) wird sich dem wahren, unbekannten Mittelwert \\(\\mu_y\\) annähern. Das gleiche gilt auch für unsere beobachtete Standardabweichung \\(s_y\\), die sich der wahren, unbekannten Standardabweichung \\(\\sigma_y\\) mit steigender Fallzahl annähert.\nAus diesem Grund brauchen wir noch einen Fehler bzw. eine statistische Maßzahl für die Streuung, die unabhängig von der Fallzahl ist. Damit erhalten wir dann den Standardfehler (abk. \\(SE\\) oder auch eng. standard error of the mean, abk. \\(SEM\\)). Mit der Fallzahl meinen wir hier die Anzahl an Beobachtungen. Wir skalieren also die Standardabweichung mit der Fallzahl indem wir die Standardabweichung durch die Wurzel der Fallzahl teilen.\n\nMathematikR (Schritt für Schritt)R (Built-in)\n\n\n\\[\nSE = \\cfrac{s}{\\sqrt{n}} = \\cfrac{2.14}{\\sqrt{7}} = \\cfrac{2.14}{2.65} = 0.81\n\\]\n\n\nEinmal der Vektor y mit den Zahlen, die wir benutzen wollen.\n\ny &lt;- c(5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6)\ny\n\n[1]  5.7  8.9 11.8  5.6  9.1  8.2  7.6\n\n\nDie Summe von y.\n\nsum(y)\n\n[1] 56.9\n\n\nDie Länge des Vektors y minus 1 oder eben die Anzahl \\(n - 1\\) gleich der Fallzahl.\n\nlength(y) - 1\n\n[1] 6\n\n\nDann einmal die Formel für die Varianz zusammengebaut. Statt \\(8.13\\) kannst du auch sum(y)/length(y) oder eben mean(y) schreiben.\n\nsum((y - 8.13)^2)/(length(y) - 1)\n\n[1] 4.59905\n\n\nDann noch die Wurzel aus der Varianz um die Standardabweichung zu berechnen. Denn nichts anderes haben wir ja bis jetzt noch nicht berechnet.\n\nsqrt(sum((y - 8.13)^2)/(length(y) - 1))\n\n[1] 2.14454\n\n\nUnd dann den ganzen Term nochmal durch die Wurzel der Fallzahl.\n\nsqrt(sum((y - 8.13)^2)/(length(y) - 1))/sqrt(length(y))\n\n[1] 0.8105598\n\n\n\n\n\nse &lt;- sd(y)/sqrt(length(y))\nse\n\n[1] 0.8105596\n\n\n\n\n\nWir müssten ein Paket in R laden um den Standardfehler zu berechnen. Das Laden von zusätzlichen Paketen wollen wir hier aber vermeiden; wir können den Standardfehler auch einfach selber berechnen. Wir erhalten einen Standardfehler von \\(0.81cm\\). Diese Zahl ist in dem Sinne nicht zu interpretieren, da wir hier nur Experimente losgelöst von deren Fallzahl miteinander vergleichen können. Wenn du also zwei Sprungweitenexperimente mit unterschiedlichen Fallzahlen \\(n_1\\) und \\(n_2\\) bezüglich der Streuung in den Experimenten vergleichen willst, dann nutzt du den Standardfehler, da der Standardfehler unabhängig von der Fallzahl der beiden Experimente ist. Auf der anderen Seite können wir ohne die berichtete Fallzahl nicht vom Standardfehler auf die Standardabweichung schließen.\nDas Buch A biologist’s guide to statistical thinking and analysis von Fay und Gerow (2018) liefert in dem dem Abschnitt A quick guide to interpreting different indicators of variation nochmal einen Überblick über die verschiedenen Arten der Variation eines Datensatz und deren Interpretation. Ich habe die Interpretationen abhängig von der Variationsquelle nochmal in der Tabelle 15.4 zusammengefasst. Die Standardabweichung ist eine biologische Interpretation der Variation oder Streuung in den Daten. Die beiden anderen Quellen Standardfehler und Konfidenzintervall ist dagegen statistische Maßzahlen, die anders interpretiert werden können und müssen. Hier ist der Kontext dann ein anderer.\n\n\n\nTabelle 15.4— Verschiedene Quellen der Variation für den Fehlerbalken und die sich dadurch ergebende Interpretation der Fehlerbalken. Für mehr Informationen siehe auch den Abschnitt A quick guide to interpreting different indicators of variation aus Fay und Gerow (2018).\n\n\n\n\n\n\n\n\n\n\nArt des Fehlerbalkens\nÜberschneidung der Fehlerbalken\nNicht überlappende Fehlerbalken\n\n\n\n\nStandardabweichung\nKein Rückschluss möglich\nKein Rückschluss möglich\n\n\nStandardfehler\nKein signifikanter Unterschied zwischen den Mittelwerten\nKein Rückschluss möglich\n\n\nKonfidenzintervall\nKein signifikanter Unterschied zwischen den Mittelwerten\nSignifikanter Unterschied zwischen den Mittelwerten\n\n\n\n\n\n\nIn der Abbildung 15.3 siehst du dann die Aussagen der Tabelle 15.4 nochmal visualisiert. Alle Mittelwerte haben die gleichen Werte für die beiden Behandlungen für den Fall der sich überschneidenden Fehlerbalken sowie für die sich nicht überlappenden Fehlerbalken. Je nach Variationsquelle sehen die Fehlerbalken anders aus und haben auch eine andere Interpretation.\n\n\n\n\n\n\n\n\nAbbildung 15.3— Verschiedene Quellen der Variation für den Fehlerbalken und die sich dadurch ergebende Interpretation der Fehlerbalken nach deren Überlappung oder nicht. Nur die Fehlerbalken, die aus Konfidenzintervallen bestehen, können im Sinne einer vollständigen statistischen Interpretation mit n.s als nicht signifikant und s als signifikant genutzt werden.\n\n\n\n\n\nJe nachdem, was du nun zeigen möchtest, kannst du jetzt entscheiden, welche Variationsquelle du nutzen willst. Wir benötigen den Standardfehler eigentlich nicht zum Berichten mit Fokus von biologischen Ergebnissen. Bei manchen Modellen und Outcomes macht aber der Standardfehler mehr Sinn als die Standardabweichung. Wir können nämlich sonst Werte erhalten die biologisch keinen Sinn ergeben. Der Standardfehler ist nicht als Zahl biologisch interpretierbar und somit eine reine statistische Größe. Tabelle 15.5 zeigt die Zusammenfassung und den Vergleich von Standardabweichung und Standardfehler nochmal aus einer anderen Perspektive. Das Konfidenzintervall ist eine statistische Größe, die wir dann in den folgenden Kapiteln noch behandelt werden.\n\n\n\nTabelle 15.5— Zusammenfassung und Vergleich von Standardabweichung und Standardfehler\n\n\n\n\n\n\n\n\n\nStandardabweichung\nStandardfehler\n\n\n\n\n… ist eine Aussage über die Streuung der erhobenen Werte einer Stichprobe.\n… ist eine Aussage über die Genauigkeit des Mittelwertes einer Stichprobe.\n\n\n… hängt von der biologischen Variabilität ab.\n… abhängig von der Messgenauigkeit\n\n\n… ist ein beschreibendes Maß.\n… ist ein statistisches Maß.\n\n\n… ist nur wenig durch die Größe der Stichprobe beineinflussbar.\n… steht im direkten Verhältnis zur Größe der Stichprobe.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#median",
    "href": "eda-descriptive.html#median",
    "title": "15  Deskriptive Statistik",
    "section": "15.10 Median",
    "text": "15.10 Median\nJetzt haben wir uns eine Möglichkeit den Schwerpunkt von einer Zahlenreihe zu berechnen angeschaut. Nun gibt es aber noch eine weitere Möglichkeit die “Mitte” von Zahlen zu ermitteln. Wir wollen uns jetzt noch eine andere Art der Zusammenfassung von Zahlen anschauen. Anstatt mit den Zahlen zu rechnen und die Zahlen aufzusummieren, sortieren wir jetzt die Zahlen. Wir nehmen also die Zahlen aus dem Vektor \\(y = \\{5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6\\}\\) und sortieren diese Zahlen nach dem Rang. Wir rechnen dann mit den Rängen und nicht mehr mit den Werten der Zahlen. Die kleinste Zahl kriegt den kleinsten Rang, die nächst größere Zahl dann den nächsten Rang. Wir können R über die Funktion sort() nutzen um den Vektor \\(y\\) zu sortieren. Klar das geht auch schnell per Hand, aber wenn die Zahlenvektoren länger werden, dann ist so eien Funktion schon praktisch.\n\ny |&gt; sort()\n\n[1]  5.6  5.7  7.6  8.2  8.9  9.1 11.8\n\n\nDie neue Art die Mitte von dem sortierten Zahlenvektor zu bestimmen, nennen wir dann den Median berechnen. Der Median \\(\\tilde{y}\\) ist die mittlere Zahl eines Zahlenvektors. Wir haben hier sieben Zahlen, also ist der Median die vierte Zahl. Wir müssen hier aber zwischen einer ungeraden Anzahl und einer geraden Anzahl unterscheiden.\n\nUngerade Anzahl\n\nWenn wir eine ungerade Anzahl an Zahlen vorliegen haben, ist der Median die mittlere Zahl des Vektors \\(y\\): \\[\n5.6,  5.7,  7.6,  \\underbrace{8.2,}_{Median}  8.9,  9.1, 11.8\n\\]\n\n\nIn R können wir den Median einfach mit der Funktion median()berechnen.\n\n## ohne pipe-Operator\nmedian(y) \n\n[1] 8.2\n\n## mit pipe-Operator\ny |&gt; median()\n\n[1] 8.2\n\n\n\nGerade Anzahl\n\nWenn wir eine gerade Anzahl von Zahlen vorliegen haben, ist der Median der Mittelwert der beiden mittleren Zahlen des Vektors \\(y\\). Ich habe hier einfach die Zahl 13.1 aus dem Hut gezaubert. Es könnte auch eine beliebige andere Zahl sein, die größer als 11.8 ist. Nur damit wir hier eine gerade Anzahl an Zahlen haben: \\[\n5.6,  5.7,  7.6,  \\underbrace{8.2, 8.9,}_{Median = \\tfrac{8.2+8.9}{2}=8.55} 9.1, 11.8, \\color{blue}{13.1}\n\\]\n\n\nIn R können wir den Median wieder einfach mit der Funktion median()berechnen. Wir müssen nur die Zahl 13.1 zu dem Vektor y mit der Funktion c() hinzufügen.\n\nc(y, 13.1) |&gt; median() \n\n[1] 8.55\n\n\nDer Median ist eine Alternative zu dem Mittelwert. Insbesondere in Fällen, wo es sehr große Zahlen gibt, die den Mittelwert in der Aussage verzerren, kann der Median sinnvoll sein. Wenn der Mittelwert stark von dem Median abweicht, deutet dies auf eine schiefe Verteilung oder aber Ausreißer in den Daten hin. Wir müssen dann in der explorativen Datenanalyse der Sachlage nachgehen.\n\n\n\n\n\n\nMedian versus Mittelwert\n\n\n\nZur Veranschaulichung des Unterschiedes zwischen Median und Mittelwert nehmen wir die Mietpreise in New York. Der mittlere Mietpreis für eine 2-Zimmerwohnung in Manhattan liegt bei 5000$ pro Monat. In den mittleren Mietpreis gehen aber auch die Mieten der Billionaires’ Row mit ein. Der mediane Mietpreis liegt bei 4000$. Die hohen Mieten ziehen den Mittelwert nach rechts.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#quantile-und-quartile",
    "href": "eda-descriptive.html#quantile-und-quartile",
    "title": "15  Deskriptive Statistik",
    "section": "15.11 Quantile und Quartile",
    "text": "15.11 Quantile und Quartile\nBei dem Mittelwert beschreibt die Standardabweichung die Streuung der Daten um den Mittelwert. Bei dem Median sind dies die Quartile. Die Quartile beschreiben die Streuung der Daten um den Median. Damit beschreiben wir also die Streuung der Beobachtungen um den Median anhand der Quartile. Um die Quartile bestimmen zu können, teilen wir die Daten in 100 Quantile. Du kannst dir Quantile wie Prozente vorstellen. Wir schneiden die Daten also in 100 Scheiben. Das geht natürlich erst wirklich, wenn wir hundert Zahlen haben. Deshalb hilft man sich mit Quartilen - von Quarta, ein Viertel - aus. Tabelle 15.6 zeigt dir da nochmal den Zusammenhang.\n\n\n\nTabelle 15.6— Zusammenfassung und Vergleich von Quantilen, Quartilen und Median\n\n\n\n\n\nQuantile\nQuartile\nMedian\n\n\n\n\n25% Quantile\n1\\(^{st}\\) Quartile\n\n\n\n50% Quantile\n2\\(^{nd}\\) Quartile\nMedian\n\n\n75% Quantile\n3\\(^{rd}\\) Quartile\n\n\n\n\n\n\n\nDamit haben wir sogar eine Dreifachbelegung der “mittleren” Zahl. Zum einen handelt es sich um das 50% Quantil, weil 50% der Zahlen größer und 50% der Zahlen kleiner sind. Zum anderen handelt es sich auch um das 2\\(^{nd}\\) Quartile. Da die Mitte einer Zahlenreihe als Median definiert ist, haben wir dann hier auch den Median vorliegen.\nWir bestimmen die Quartile wie den Median. Wir müssen unterscheiden, ob wir eine ungerade Anzahl an Zahlen oder eine gerade Anzahl an Zahlen vorliegen haben.\n\nUngerade Anzahl\n\nBei einer ungeraden Anzahl von Zahlen, ist das 1\\(^{st}\\) Quartile die mittlere Zahl des unteren Mittels und das 3\\(^{rd}\\) Quartile die mittlere Zahl des oberen Mittels des Vektors \\(y\\): \\[\n5.6,  \\underbrace{5.7,}_{1st\\ Quartile}  7.6,  8.2,  8.9,  \\underbrace{9.1,}_{3rd\\ Quartile} 11.8\n\\]\n\nGerade Anzahl\n\nBei einer geraden Anzahl von Zahlen, ist das 1\\(^{st}\\) Quartile der Mittelwert der beiden mittleren Zahl des unteren Mittels und das 3\\(^{rd}\\) Quartile der Mittelwert der beiden mittleren Zahlen des oberen Mittels des Vektors \\(y\\). Ich habe hier einfach die Zahl 13.1 aus dem Hut gezaubert. Es könnte auch eine beliebige andere Zahl sein, die größer als 11.8 ist. Nur damit wir hier eine gerade Anzahl an Zahlen haben: \\[\n5.6,  \\underbrace{5.7, 7.6,}_{1st\\ Quartile = \\tfrac{5.7+7.6}{2}=6.65}    8.2,  8.9,  \\underbrace{9.1, 11.8}_{3rd\\ Quartile = \\tfrac{9.1+11.8}{2}=10.45} \\color{blue}{13.1}\n\\]\n\n\nDas 95% Quantile und das 97.25% Quantile werden wir später nochmal im statistischen Testen brauchen. Auch hier ist die Idee, dass wir die Daten in hundert Teile schneiden und uns dann die extremen Zahlen anschauen.\nIn R können wir den Median einfach mit der Funktion quantile() berechnen. Wir berechnen hier das 25% Quantile also das 1\\(^{st}\\) Quartile sowie das 50% Quantile also den Median und das 75% Quantile also das 3\\(^{rd}\\) Quartile.\n\ny |&gt; quantile(probs = c(0.25, 0.5, 0.75)) |&gt; round(2)\n\n 25%  50%  75% \n6.65 8.20 9.00 \n\nc(y, 13.1) |&gt; quantile(probs = c(0.25, 0.5, 0.75)) |&gt; round(2) \n\n 25%  50%  75% \n7.12 8.55 9.77 \n\n\nWarum unterscheiden sich die händisch berechneten Quartile von den Quartilen aus R? Es gibt verschiedene Arten der Berechnung. In der Klausur nutzen wir die Art und Weise wie die händische Berechnung hier beschrieben ist. Später in der Anwendung nehmen wir die Werte, die R ausgibt. Die Abweichungen sind so marginal, dass wir diese Abweichungen in der praktischen Anwendung ignorieren wollen.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#interquartilesabstand-iqr",
    "href": "eda-descriptive.html#interquartilesabstand-iqr",
    "title": "15  Deskriptive Statistik",
    "section": "15.12 Interquartilesabstand (IQR)",
    "text": "15.12 Interquartilesabstand (IQR)\nDer Interquartilesabstand (IQR) beschreibt den Abstand zwischen dem 1\\(^{st}\\) Quartile und dem 3\\(^{rd}\\) Quartile. Daher ist der Interquartilesabstand (IQR) ähnlich der Spannweite zwischen dem maximalen und minimalen Wert. Wir benötigen das Interquartilesabstand (IQR) in der explorativen Datenanalyse wenn wir einen Boxplot erstellen wollen.\n\\[\nIQR = 3^{rd}\\,\\mbox{Quartile} - 1^{st}\\,\\mbox{Quartile} = 9.1 - 5.7 = 3.4\n\\]\nWir verwenden das IQR als Zahl eher selten.\n\n\n\n\n\n\nParametrik versus Nicht-Parametrik\n\n\n\nWenn wir einen Zahlenvektor wie durch \\(y = \\{5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6\\}\\) beschrieben zusammenfassen wollen, haben wir zwei Möglichkeiten.\n\nDie parametrische Variante indem wir mit den Zahlen rechnen und deskriptive Maßzahlen wie Mittelwert, Varianz und Standardabweichung berechnen. Diese Maßzahlen kommen aber in den Zahlen nicht vor.\nDie nicht-parametrische Variante indem wir die Zahlen in Ränge umwandeln, also sortieren, und mit den Rängen der Zahlen rechnen. Die deskriptiven Maßzahlen wären dann Median, Quantile und Quartile.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#variationskoeffizient",
    "href": "eda-descriptive.html#variationskoeffizient",
    "title": "15  Deskriptive Statistik",
    "section": "15.13 Variationskoeffizient",
    "text": "15.13 Variationskoeffizient\nIm Gegensatz zu der Varianz ist der Variationskoeffizient ein relatives Streuungsmaß, das heißt, der Variationskoeffizient hängt nicht von der Maßeinheit von \\(y\\) ab. Die Motivation für den Variationskoeffizient ist, dass ein \\(y\\) mit großem Mittelwert häufig eine größere Varianz aufweist als eine mit einem kleinen Mittelwert. Da die Varianz und die daraus abgeleitete Standardabweichung nicht normiert sind, kann ohne Kenntnis des Mittelwerts nicht beurteilt werden, ob eine Varianz groß oder klein ist. Der Variationskoeffizient ist eine Normierung der Varianz. Ist die Standardabweichung somit größer als der Mittelwert, so ist der Variationskoeffizient größer 1. Der Variationskoeffizient wird häufig verwendet, um die Variation zwischen zwei verschiedenen Datensätzen zu vergleichen.\nWir können den Variationskoeffizient basierend auf dem Mittelwert und der Standardabweichung berechnen. In unserem Beispiel würden wir als die Standardabweichung \\(s\\) und den Mittelwert \\(\\bar{y}\\) in die Formel einsetzen.\n\nMathematikR (Schritt für Schritt)R (Built-in)\n\n\n\\[\nv = \\cfrac{s}{\\bar{y}} = \\cfrac{2.14}{8.13} = 0.26\n\\]\n\n\nEinmal der Vektor y mit den Zahlen, die wir benutzen wollen.\n\ny &lt;- c(5.7, 8.9, 11.8, 5.6, 9.1, 8.2, 7.6)\ny\n\n[1]  5.7  8.9 11.8  5.6  9.1  8.2  7.6\n\n\nDie Summe von y.\n\nsum(y)\n\n[1] 56.9\n\n\nDie Länge des Vektors y minus 1 oder eben die Anzahl \\(n - 1\\) gleich der Fallzahl.\n\nlength(y) - 1\n\n[1] 6\n\n\nDann einmal die Formel für die Varianz zusammengebaut. Statt \\(8.13\\) kannst du auch sum(y)/length(y) oder eben mean(y) schreiben.\n\nsum((y - 8.13)^2)/(length(y) - 1)\n\n[1] 4.59905\n\n\nDann noch die Wurzel aus der Varianz um die Standardabweichung zu berechnen. Denn nichts anderes haben wir ja bis jetzt noch nicht berechnet.\n\nsqrt(sum((y - 8.13)^2)/(length(y) - 1))\n\n[1] 2.14454\n\n\nDann nochmal den Mittelwert berechnen den wir nochmal brauchen. Aber auch hier getht natürlich auch die Built-in Funktion mean(y).\n\nsum(y)/length(y)\n\n[1] 8.128571\n\n\nDann einmal alles zusammen und wir haben den Variationskoeffizient Schritt für Schritt berechnet.\n\nsqrt(sum((y - 8.13)^2)/(length(y) - 1))/(sum(y)/length(y))\n\n[1] 0.2638274\n\n\n\n\n\nvar_coef &lt;- sd(y)/mean(y)\nvar_coef\n\n[1] 0.2638273\n\n\n\n\n\nWir können den Variationskoeffizient auch basierend auf dem Median berechnen.\n\\[\nv_r=\\cfrac{x_{0.75}-x_{0.25}}{\\tilde{x}_{0.5}}\n\\]\nIn unserem Fall müssen wir hier dann die Quartile und den Median in die Formel einsetzen.\n\\[\nv_r=\\cfrac{9.1 - 5.7}{8.2} = 0.41\n\\]\nIm Bereich der Agrarwissenschaften kommt der Variationskoeffizient eher selten vor. Der Variationskoeffizient ist eben eine statistische Maßzahl des Labors. Aber dennoch kommt der Variationskoeffizient immer mal wieder vor. Ich berechne denn Variationskoeffizient immer in R direkt aus der Standardabweichung und dem Mittelwert oder aber ich baue mir die Formel in R selber. So schwer ist es dann auch nicht.\n\nvar_coef &lt;- function(y) {\n  return(sd(y)/mean(y))\n}\nvar_coef(y)\n\n[1] 0.2638273",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#häufigkeiten",
    "href": "eda-descriptive.html#häufigkeiten",
    "title": "15  Deskriptive Statistik",
    "section": "15.14 Häufigkeiten",
    "text": "15.14 Häufigkeiten\nNeben der Darstellung von kontinuierlichen Zahlen, wollen wir auch häufig kategoriale Zahlen sinnvoll darstellen und zusammenfassen. Zwar können wir auch ohne weiteres die mittlere Note berechnen, aber meistens wollen wir dann doch wissen, wie viele Noten jeweils vergeben worden sind. Auch wenn wir uns mit einer \\(0/1\\) Variable beschäftigen, also zum Beispiel infiziert ja/nein, wollen wir diese Variablen zusammenfassen. Wir wollen hier also Häufigkeiten (eng. frequency) berechnen und darstellen. Dabei gibt es verschiedene Arten von Häufigkeiten, die wir uns jetzt nacheinander einmal anschauen.\nFangen wir also einmal mit der Variable infected an. Wir haben in dieser Variable abgelegt, ob der Hund mit einem Floh infiziert ist 1 oder eben nicht infiziert ist 0. Wie du siehst, kodieren wir das “Ja” mit einer 1 und das “Nein” mit einer 0.\n\\[\ny_{infected} = \\{0, 1, 1, 0, 1, 0, 0\\}.\n\\]\nIn R würde der Vektor der Zahlen etwas anders aussehen, aber das hat eher was mit der Schreibweise der Mathematik und der Informatik zu tun. Inhaltlich sehen wir hier gleichen Informationen.\n\ny_infected &lt;- c(0, 1, 1, 0, 1, 0, 0) \n\nSchauen wir uns als einmal die drei Möglichkeiten die kategorialen Daten als Häufigkeiten abzubilden an. Wir können zum einen die absoluten Häufigkeiten berichten. Wir sagen also ganz einfach, wie viele Hunde sind von der Gesamtheit infiziert.\n\nAbsolute Häufigkeit\n\nWir zählen die Anzahl an infizierten Tieren und berichten die Anzahl zusammen mit der gesamt Anzahl an Tieren. Also haben wir dann als absolute Häufigkeit \\(f_{absolut} = n_{infiziert} / n_{gesamt}\\) zu berichten.\n\n\nDamit könne wir auch die absoluten Häufigkeiten einfach ausrechnen und berichten. Wir haben ja drei infizierte Hunde von sieben Hunden vorliegen.\n\\[\nf_{infected} = 3 / 7\n\\]\nAls nächstes schauen wir uns die relativen Häufigkeiten an. Relative Häufigkeiten werden weit häufiger berichtet. Wir haben ja dann die guten, alten Prozente vorliegen.\n\nRelative Häufigkeit\n\nWir zählen die infizierten Tiere und teilen die Anzahl an infizierten Tiere durch die gesamte Anzahl an Tieren. Wir rechnen also die relative Häufigkeit mit \\(f_{relativ} = n_{infiziert} / n_{gesamt}\\).\n\n\nSieht erstmal aus wie die absolute Häufigkeit, aber hier rechnen wir den Term dann auch aus. Damit teilen wir dann unsere drei infizierten Hunde durch die gesamte Anzahl von sieben Tieren.\n\\[\nf_{infected} = \\cfrac{3}{7} = 0.43\n\\]\nMeistens schreiben wir dann aber einen Hybriden aus absoluter und relativer Häufigkeit mit \\(3 / 7\\; (43\\%)\\). Wir ergänzen also die absoluten Häufigkeiten um die relativen Häufigkeiten. Ist dann eigentlich doppelt gemoppelt, aber wir wollen häufig auch sehen auf welcher Gesamtzahl die relative Häufigkeit sich begründet.\nNeben der klassischen Häufigkeit, die wir durch die prozentuale Angabe kennen, gibt es auch noch die Chance. Die Chance infiziert zu sein, ist was anderes als die Wahrscheinlichkeit infiziert zu sein. Die Wahrscheinlichkeit ist eine relative Häufigkeit. Die Chance berechnet sich dabei etwas anders, den die Chance ist ein Anteil.\n\nChance\n\nDie Chance beschreibt den Anteil der infizierten Tiere an den nicht infizierten Tieren. Oder aber wir schreiben für die Chance \\(f_{chance} = n_{1} : n_{0}\\) mit \\(n_1\\) gleich der Anzahl an infizierten Tieren und \\(n_0\\) gleich der Anzahl der nicht infizierten also gesunden Tiere.\n\n\nWie du im Folgenden siehst, ergibt sich bei der Chance ein anderer Wert als bei der relativen Häufigkeiten. Wir haben also eine andere Chance infiziert zu sein, als die Wahrscheinlichkeit infiziert zu sein. Das ist auch so gewollt, denn wir betrachten ja bei der Chance etwas anders. Wir schauen den Anteil der drei infizierten Tiere im Vergleich zu den vier gesunden Tieren an. Wir erhalten dann eine Chance von \\(75\\%\\) um infiziert zu sein.\n\\[\nf_{infected} = 3 : 4 = 0.75\n\\]\nWenn wir die Häufigkeiten berechnen wollen, dann nutzen wir auch wieder R. Es gibt viele Funktionen in R, die wir nutzen könne. Hier erstellen wir eine Tabelle in R mit der Funktion tabyl() aus dem R Paket {janitor}. Es gibt noch andere Möglichkeiten, die schauen wir uns aber gleich im nächsten Abschnitt an.\nEinmal die sehr simplen Form der Berechnung für die infizierten Hunden. Die Funktion tabyl() liefert neben der absoluten Häufigkeiten dann auch die relativen Häufigkeiten. Deshalb mag ich diese Funktion sehr gerne für das schnelle Nachschauen.\n\ny_infected |&gt; \n  tabyl()\n\n y_infected n   percent\n          0 4 0.5714286\n          1 3 0.4285714\n\n\nOder aber etwas komplexer für die Noten der Hunde. Hier siehst du dann nochmal schöner, wie sich die Tabelle aufbaut.\n\nflea_dog_tbl |&gt;\n  pull(grade) |&gt; \n  tabyl()\n\n pull(flea_dog_tbl, grade) n   percent\n                         6 1 0.1428571\n                         7 2 0.2857143\n                         8 3 0.4285714\n                         9 1 0.1428571\n\n\nDamit sind wir dann auch schon durch mit den Häufigkeiten. Selten berichten wir die reinen Häufigkeiten jeweils separat sondern in einer großen Übersichtstabelle. Um diese großen Übersichtstabelle und deren Erstellung wollen wir uns jetzt im nächsten Abschnitt einmal anschauen.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#sec-automated-table",
    "href": "eda-descriptive.html#sec-automated-table",
    "title": "15  Deskriptive Statistik",
    "section": "15.15 Automatisierung in R",
    "text": "15.15 Automatisierung in R\n\n\n\n\n\n\nUps, das hier ist mir aber zu wild!\n\n\n\nDer folgende Abschnitt ist nicht relevant für eine Klausur. Wir brauchen aber die Automatisierung in R um später sinnvoll Daten in einer Abschlussarbeit oder einem Projekt auswerten zu können. Also keine Angst, bitte überspringt den Abschnitt, wenn du nur für die Klausur lernst.\n\n\nIm Folgenden wollen wir einmal den Datensatz zu den Gummibärchen zusammenfassen und zu beschreiben. Hier können wir nicht einfach so einen Überblick kriegen in dem wir uns einfach nur die Daten anschauen. Die Exceltabelle ist dafür viel zu lang um einen Überblick zu erlangen. Laden wir also einmal den Datensatz und wählen nur ein paar Spalten aus, damit wir hier nicht zu viel Tabellen produzieren.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  |&gt;\n  select(gender, height, age, count_color, \n         most_liked) |&gt; \n  mutate(gender = as_factor(gender))\n\nWir erhalten das Objekt gummi_tbl mit dem Datensatz in Tabelle 17.1 nochmal als Auszug dargestellt. Wir brauchen aber nicht alle Spalten aus dem ursprünglichen Datensatz und somit ist die Tabelle etwas übersichtlicher. Wir schauen uns also dann mal zwei kontinuierlichen und kategorialen Variablen an.\n\n\n\n\nTabelle 15.7— Auszug aus den selektierten Daten zu den Gummibärchendaten mit zwei kontinuierlichen und kategorialen Variablen\n\n\n\n\n\n\ngender\nheight\nage\ncount_color\nmost_liked\n\n\n\n\nm\n193\n35\n3\nlightred\n\n\nw\n159\n21\n5\nyellow\n\n\nw\n159\n21\n6\nwhite\n\n\nw\n180\n36\n5\nwhite\n\n\n…\n…\n…\n…\n…\n\n\nm\n187\n24\n6\ndarkred\n\n\nm\n182\n24\n4\ngreen\n\n\nw\n170\n23\n4\nwhite\n\n\nw\n180\n24\n6\ngreen\n\n\n\n\n\n\n\n\nIch stelle jetzt zwei R Pakete mit {gtsummary} und {modelsummary} vor, die du nutzen kannst um Daten zusammenzufassen. Es gibt aber noch weit mehr Pakete, aber wir konzentrieren uns mal auf diese beiden Pakete.\n\n\n\n\n\n\nWas gibt es noch an Möglichkeiten?\n\n\n\nNeben den beiden vorgestellten gibt es natürlich auch Alternativen zu {gtsummary} und {modelsummary}. Da müsstest du dann aber mal selber schauen oder mich direkt Fragen. Wenn Interesse besteht kann ich auch noch andere Pakete vorstellen.\n\n\n\n15.15.1 Mit dem Paket {gtsummary}\nMeiner Meinung nach ist das Paket {gtsummary} das Paket für die Erstellung von Übersichttabellen. Wie du auf derHilfeseite von gtsummary sehen wirst, gibt es wirklich sehr viele Möglichkeiten eine schöne Tabelle zu bauen. Neben der Zusammenfassung kannst du auch \\(p\\)-Werte aus statistischen Tests ergänzen oder auch die Differenzen zwischen zwei Gruppen. Hier ist wirklich viel möglich in dem Paket.\nFangen wir also einmal an eine Übersichtstabelle zu erstellen. Wir bauen eine simple Tabelle über alle Variablen in dem Datensatz gummi_tbl aufgeteilt nach dem Geschlecht. Das geht recht schnell und ohne viel Schnickschnack.\n\ngummi_tbl |&gt; \n  tbl_summary(by = gender)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nm, N = 3561\nw, N = 3431\n\n\n\n\nheight\n184 (178, 189)\n169 (164, 174)\n\n\nage\n22.0 (21.0, 24.0)\n22.0 (20.0, 25.0)\n\n\ncount_color\n\n\n\n\n\n\n    2\n0 (0%)\n1 (0.3%)\n\n\n    3\n18 (5.1%)\n15 (4.4%)\n\n\n    4\n94 (26%)\n83 (24%)\n\n\n    5\n189 (53%)\n164 (48%)\n\n\n    6\n55 (15%)\n80 (23%)\n\n\nmost_liked\n\n\n\n\n\n\n    darkred\n137 (38%)\n135 (39%)\n\n\n    green\n83 (23%)\n91 (27%)\n\n\n    hellrot\n1 (0.3%)\n0 (0%)\n\n\n    lightred\n36 (10%)\n21 (6.1%)\n\n\n    none\n30 (8.4%)\n14 (4.1%)\n\n\n    orange\n15 (4.2%)\n15 (4.4%)\n\n\n    white\n38 (11%)\n51 (15%)\n\n\n    yellow\n16 (4.5%)\n16 (4.7%)\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n\n\nDas ist schon toll, sich so schnell eine Übersicht über die Daten zu schaffen. Wir haben alle Informationen, die wir brauchen, um mal eine Idee der Daten zu erhalten. Das ist dann immer so eine Sache, meist wollen wir dann die Daten dann doch nicht so exakt zusammenfassen. Aber auch hier hilft {gtsummary} mit sehr vielen Optionen. In der Abbildung 15.4 siehst du nochmal die Optionen in {gtsummary} übersetzt in die ausgegebene Tabelle.\n\n\n\n\n\n\nAbbildung 15.4— Darstellung der {gtsummary} Optionen in die ausgegebene Tabelle.\n\n\n\nHier nochmal ein Beispiel mit mehr Optionen und dann einer anderen Darstellung. Siehe das hier aber nur als Beispiel, es gibt aber noch sehr viel mehr Optionen für die Darstellung. Du kannst auch {mean} ({sd}) durch {mean}+/-{sd} ersetzen um eine andere Schreibweise zu haben.\n\ngummi_tbl |&gt;\n  tbl_summary(\n    by = gender,\n    statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\",\n      all_categorical() ~ \"{n} / {N} ({p}%)\"\n    ),\n    digits = all_continuous() ~ 2,\n    label = count_color ~ \"Anzahl Farben\",\n    missing_text = \"(Missing)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nm, N = 3561\nw, N = 3431\n\n\n\n\nheight\n183.65 (7.80)\n168.84 (7.31)\n\n\nage\n23.12 (4.44)\n23.66 (6.41)\n\n\nAnzahl Farben\n\n\n\n\n\n\n    2\n0 / 356 (0%)\n1 / 343 (0.3%)\n\n\n    3\n18 / 356 (5.1%)\n15 / 343 (4.4%)\n\n\n    4\n94 / 356 (26%)\n83 / 343 (24%)\n\n\n    5\n189 / 356 (53%)\n164 / 343 (48%)\n\n\n    6\n55 / 356 (15%)\n80 / 343 (23%)\n\n\nmost_liked\n\n\n\n\n\n\n    darkred\n137 / 356 (38%)\n135 / 343 (39%)\n\n\n    green\n83 / 356 (23%)\n91 / 343 (27%)\n\n\n    hellrot\n1 / 356 (0.3%)\n0 / 343 (0%)\n\n\n    lightred\n36 / 356 (10%)\n21 / 343 (6.1%)\n\n\n    none\n30 / 356 (8.4%)\n14 / 343 (4.1%)\n\n\n    orange\n15 / 356 (4.2%)\n15 / 343 (4.4%)\n\n\n    white\n38 / 356 (11%)\n51 / 343 (15%)\n\n\n    yellow\n16 / 356 (4.5%)\n16 / 343 (4.7%)\n\n\n\n1 Mean (SD); n / N (%)\n\n\n\n\n\n\n\n\n\nDas ist soweit schon mal gut, aber wir wollen meist die Tabelle dann doch in Word haben. Nicht das du deine Abschlussarbeit auch mit \\(\\LaTeX\\) schreiben könntest. Wir können dafür dann über die Funktion gtsave aus dem R Paket gt unsere Tabelle dann in einem Worddokument speichern. Wie toll ist das denn? Aber Achtung, bitte nicht in dein Hauptdokument abspeichern sondern immer eine Datei für eine Tabelle. Sonst kann es sein, dass du dir aus Versehen mal deine Tabellen überschreibst.\n\ngummi_tbl |&gt; \n  tbl_summary(by = gender) |&gt; \n  as_gt() |&gt;  \n  gt::gtsave(filename = \"example.docx\")\n\nFrag mich gerne, wenn du weitere Informationen brauchst oder schau auch mal das Cheatsheet für gtsummary an. Du findest im Cheatsheet auch nochmal einen Überblick was alles möglich ist. Ich finde da den Überblick wirklich super.\n\n\n15.15.2 Mit dem Paket {modelsummary}\nEine andere Möglichkeit ist das R Paket {modelsummary}. Hier haben wir auch die Möglichkeit neben Modellvergleiche auch Daten in schönen Tabellen zusammenzufassen. Auch hier hilft die Hilfeseite von {modelsummary} um über die vielen Möglichkeiten einen Überblick zu erlangen. Meiner Meinung nach braucht das Paket {modelsummary} etwas mehr Programmiererfahrung für die Bedienung, kann dafür dann aber auch mehr. Du musst dich vorab um fehlende Daten kümmern oder aber die Funktionen entsprechend umdefinieren. Also entfernen wir erstmal grob alle fehlenden Werte mit na.omit(). Das ist jetzt aber die etwas ungünstige Variante, da wir das jetzt über alle Variablen die fehlenden Werte entfernen. Sobald eine Variable einen fehlenden Wert hat, schmeißen wir die ganze Beobachtung raus.\n\ngummi_clean_tbl &lt;- gummi_tbl |&gt; \n  na.omit()\n\nJetzt nutzen wir die Funktion datasummary_skim() um einmal einen schnellen Überblick zu produzieren. Wir müssen die Funktion aber zweimal nutzen. Einmal um uns die kontinuierlichen Variablen type = \"numeric\" anzuschauen und einmal für die kategorialen Variablen type = \"categorical\". Du kriegst zwar im Folgenden die fehlenden Werte angezeigt, aber das ist eben nur in der Funktion datasummary_skim() möglich.\n\ngummi_clean_tbl |&gt; \n  datasummary_skim(type = \"numeric\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnique\nMissing Pct.\nMean\nSD\nMin\nMedian\nMax\n\n\n\n\n\nheight\n53\n0\n176.4\n10.6\n147.0\n176.0\n205.0\n\n\n\n \n\n\n\nage\n35\n0\n23.4\n5.5\n11.0\n22.0\n61.0\n\n\n\n \n\n\n\ncount_color\n5\n0\n4.8\n0.8\n2.0\n5.0\n6.0\n\n\n\n \n\n\n\n\n\n\n\n\n\nDann nochmal die Anzahl und die relative Häufigkeit für die kategorialen Variablen. Ich habe hier jetzt nicht nochmal tiefer die Ausgabe modifiziert.\n\ngummi_clean_tbl |&gt; \n  datasummary_skim(type = \"categorical\")\n\n\n\n\n\n\n\nN\n%\n\n\n\n\ngender\nm\n356\n50.9\n\n\n\nw\n343\n49.1\n\n\nmost_liked\ndarkred\n272\n38.9\n\n\n\ngreen\n174\n24.9\n\n\n\nhellrot\n1\n0.1\n\n\n\nlightred\n57\n8.2\n\n\n\nnone\n44\n6.3\n\n\n\norange\n30\n4.3\n\n\n\nwhite\n89\n12.7\n\n\n\nyellow\n32\n4.6\n\n\n\n\n\n\n\n\nEs gibt aber in dem R Paket {modelsummary} noch wirklich eine Menge mehr Optionen. Bitte schau da nochmal die Hilfeseite datasummary: Crosstabs, frequencies, correlations, balance (a.k.a. “table 1”), and more an. Wir gehen hier dann nicht tiefer auf die Möglichkeiten ein.\nUnd auch hier kannst du ganz einfach deine Tabelle in ein Worddokument speichern. Ich habe mich hier an der Hilfeseite datasummary als *.docx abspeichern orientiert und den Code etwas angepasst. Faktisch ergänzt du einfach die Option output = \"table.docx\". Damit speicherst du dann deine datasumary Tabelle in die Datei table.docx.\n\ndatasummary(height ~ gender * (mean + sd),\n            data = gummi_clean_tbl, \n            output = 'table.docx')",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-descriptive.html#referenzen",
    "href": "eda-descriptive.html#referenzen",
    "title": "15  Deskriptive Statistik",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 15.1— Zusammenhang zwischen \\(y\\) sowie dem Index \\(i\\) in der Formel für den Mittelwert.\nAbbildung 15.2— Die graue Linie beschreibt den Mittelwert der genau so durch die blauen Punkte geht, dass die Abstände der Punkte oberhalb und unterhalb zu Null aufaddieren. Die Linie liegt in der Mitte der Punkte. Die quadrierten Abstände sind die Varianz der blauen Punkte. Auf der x-Achse ist der Index des Punktes eingetragen.\nAbbildung 15.3— Verschiedene Quellen der Variation für den Fehlerbalken und die sich dadurch ergebende Interpretation der Fehlerbalken nach deren Überlappung oder nicht. Nur die Fehlerbalken, die aus Konfidenzintervallen bestehen, können im Sinne einer vollständigen statistischen Interpretation mit n.s als nicht signifikant und s als signifikant genutzt werden.\nAbbildung 15.4— Darstellung der {gtsummary} Optionen in die ausgegebene Tabelle.\n\n\n\nFay DS, Gerow K. 2018. A biologist’s guide to statistical thinking and analysis. WormBook: The Online Review of C. elegans Biology [Internet].",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "eda-ggplot.html",
    "href": "eda-ggplot.html",
    "title": "16  Visualisierung von Daten",
    "section": "",
    "text": "16.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, readxl, ggmosaic, \n               janitor, see, patchwork, latex2exp, ggbeeswarm,\n               ggdist, gghalves, ggbreak, duke, wesanderson, conflicted)\nconflicts_prefer(dplyr::summarise)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(latex2exp::TeX)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Visualisierung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-ggplot.html#grundlagen-in-ggplot",
    "href": "eda-ggplot.html#grundlagen-in-ggplot",
    "title": "16  Visualisierung von Daten",
    "section": "16.2 Grundlagen in ggplot()",
    "text": "16.2 Grundlagen in ggplot()\n\n“Numerical quantities focus on expected values, graphical summaries on unexpected values.” — John Tukey\n\nWir nutzen in R das R Paket {ggplot2} um unsere Daten zu visualisieren. Die zentrale Idee von {ggplot2} ist, dass wir uns eine Abbildung wie ein Sandwich bauen. Zuerst legen wir eine Scheibe Brot hin und legen uns dann Scheibe für Scheibe weitere Schichten übereinander. Oder die Idee eines Bildes, wo wir erst die Leinwand definieren und dann Farbschicht über Farbschicht auftragen. Im Gegensatz zu dem Pipe-Operator |&gt; nutzt {ggplot2} den Operator + um die verschiedenen Funktionen (geom_) miteinander zu verbinden. Das Konzept von {ggplot2}ist schlecht zu beschreiben deshalb habe ich auch noch zwei Videos hierfür gemacht. Um den Prozess von {ggplot2} zu visualisieren - aber wie immer, nutze was du brauchst.\n\n\n\n\n\n\nGrundlagen von ggplot() im Video\n\n\n\nDu findest auf YouTube Einführung in R - Teil 16.0 - Trockenübung ggplot2 simpel und einfach erklärt als Video. Sowie dann auch auf YouTube Einführung in R - Teil 16.1 - Abbildungen mit ggplot in R erstellen. Idee und Konzept von ggplot als Video. Also alles nochmal als Video - vielleicht einfacher nachzuvollziehen als in einem Fließtext.\n\n\nDie Funktion ggplot() ist die zentrale Funktion, die die Leinwand erschafft auf der wir dann verschiedene Schichten aufbringen werden. Diese Schichten heißen geom. Es gibt nicht nur ein geom sondern mehrere. Zum Beispiel das geom_boxplot() für die Erstellung von Boxplots, das geom_histogram() für die Erstellung von Histogrammen. Die Auswahl ist riesig. Die einzelnen Schichten werden dann über den Operator + miteinander verbunden. Soviel erstmal zur Trockenübung. Schauen wir uns das ganze einmal an einem Beispiel an. Dafür müssen wir dann erstmal einen Datensatz laden, damit wir auch etwas zum abbilden haben.\n\n16.2.1 Daten\nWir importieren den Datensatz flea_cat_dog.xlsx und wollen einzelne Variablen visualisieren. Wir kennen den Datensatz schon aus den vorherigen Beispielen. Dennoch nochmal hier der Datensatz in Tabelle 16.1 einmal dargestellt.\n\nflea_dog_cat_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\") |&gt; \n  mutate(animal = as_factor(animal))\n\nIm Folgenden ist es wichtig, dass du dir die Spaltennamen merkst. Wir können nur die exakten, wortwörtlichen Spaltennamen verwenden. Sonst erhalten wir einen Fehler. Deshalb haben wir auch keine Leerzeichen in den Spaltennamen.\n\n\n\n\nTabelle 16.1— Beispieldatensatz für Eigenschaften von Flöhen von zwei Tierarten.\n\n\n\n\n\n\nanimal\njump_length\nflea_count\nweight\ngrade\ninfected\n\n\n\n\ndog\n5.7\n18\n2.1\n8\n0\n\n\ndog\n8.9\n22\n2.3\n8\n1\n\n\ndog\n11.8\n17\n2.8\n6\n1\n\n\ndog\n5.6\n12\n2.4\n8\n0\n\n\ndog\n9.1\n23\n1.2\n7\n1\n\n\ndog\n8.2\n18\n4.1\n7\n0\n\n\ndog\n7.6\n21\n3.2\n9\n0\n\n\ncat\n3.2\n12\n1.1\n7\n1\n\n\ncat\n2.2\n13\n2.1\n5\n0\n\n\ncat\n5.4\n11\n2.4\n7\n0\n\n\ncat\n4.1\n12\n2.1\n6\n0\n\n\ncat\n4.3\n16\n1.5\n6\n1\n\n\ncat\n7.9\n9\n3.7\n6\n0\n\n\ncat\n6.1\n7\n2.9\n5\n0\n\n\n\n\n\n\n\n\nWir brauchen dann ab und an auch nochmal mehr Datenpunkte, daher nehmen wir auch einmal den Gummibärchendatensatz und schauen uns dort die Variablen gender, height und age einmal genauer an. Wie immer nutzen wir die Funktion select() um die Spalten zu selektieren. Abschließend verwandeln wir das Geschlecht gender noch in einen Faktor und entfernen alle fehlenden Werte mit na.omit().\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  |&gt;\n  select(gender, height, age) |&gt; \n  mutate(gender = factor(gender, labels = c(\"männlich\", \"weiblich\"))) |&gt; \n  na.omit()\n\n\n\n16.2.2 Erste Abbildung in ggplot()\nErstellen wir also erstmal unseren erste Visualisierung in ggplot(). Wie immer empfehle ich dir dann auch das entsprechende Video auf YouTube anzuschauen. In Textform ist eine echte Herausforderung zu erklären wie man Plots baut. Der folgende R Code erstellt die Leinwand in der Abbildung 16.2 für die folgende, zusätzliches Schichten (geom). Wir haben also immer erst eine leere Leinwand auf der wir dann zusätzlich geome plotten. Wir bauen uns sozusagen ein Sandwich.\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal , y = jump_length)) +\n  theme_minimal()\n\nWir schauen uns einmal den Code im Detail an.\n\nggplot ruft die Funktion auf. Die Funktion ist dafür da den Plot zu zeichnen.\ndata = flea_dog_cat_tbl benennt den Datensatz aus dem der Plot gebaut werden soll.\naes()ist die Abkürzung für aesthetics und beschreibt, was auf die \\(x\\)-Achse soll, was auf die \\(y\\)-Achse soll sowie ob es noch andere Faktoren in den Daten gibt. Wir können nämlich noch nach anderen Spalten die Abbildung einfärben oder anderweitig ändern.\n\nx braucht den Spaltennamen für die Variable auf der \\(x\\)-Achse.\ny braucht den Spaltennamen für die Variable auf der \\(y\\)-Achse.\n\n+ theme_minimal() setzt das Canvas oder die Leinwand auf schwarz/weiß. Sonst wäre die Leinwand flächig grau.\n\nMit Faktoren meine ich hier andere Gruppenvariablen. Variablen sind ein anderes Wort für Spalten. Also Variablen die wir mit as_factor erschaffen haben und die uns noch mehr über unsere Daten dann verraten können. Hier ist es dann etwas abstrakt, aber es wird dann später in der Anwendung klarer.\n\n\n\n\n\n\n\n\nAbbildung 16.2— Leere ggplot() Leinwand mit den Spalten animal und jump_length aus dem Datensatz flea_dog_cat_tbl.\n\n\n\n\n\nAm Ende sehen wir, dass wir nichts sehen. In der Abbildung 16.2 ist nichts dargestellt. Der Grund ist, dass wir noch kein geom hinzugefügt haben. Das geom beschreibt nun wie die Zahlen in der Datentabelle flea_dog_cat_tbl visualisiert werden sollen. Wir habe eine sehr große Auswahl an geomen, deshalb gibt es gleich einmal eine Auswahl an Abbildungen.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Visualisierung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-ggplot.html#die-häufigsten-abbildungen",
    "href": "eda-ggplot.html#die-häufigsten-abbildungen",
    "title": "16  Visualisierung von Daten",
    "section": "16.3 Die häufigsten Abbildungen",
    "text": "16.3 Die häufigsten Abbildungen\nIm Folgenden gehen wir dann einmal die wichtigsten Abbildungen einmal durch. Viele der Abbildungen kennst du vielleicht schon und dann musst du hier nur noch schauen, wie die Abbildungen in ggplot zu realisieren sind. Ansonsten gilt wie immer, es ist nur ein kleiner Ausschnitt, du findest auf der Hilfeseite von ggplot eine sehr viel größere Übersicht.\n\n\n\n\n\n\nHistogramm, Boxplot, Scatterplot und Mosaicplot im Video\n\n\n\nDu findest auf YouTube Einführung in R - Teil 16.2 - Histogramm, Boxplot, Scatterplot und Mosaicplot mit ggplot in R als Video. Weitere Videos werden dann noch folgen und ergänzt.\n\n\n\n16.3.1 Barplot oder Balkendiagramm oder Säulendiagramm\nDer Barplot oder das Balkendiagramm auch Säulendiagramm ist eigentlich veraltet. Wir haben mit dem Boxplot eine viel bessere Methode um eine Verteilung und gleichzeitig auch die Gruppenunterschiede zu visualisieren. Warum nutzen wir jetzt so viel den Barplot? Das hat damit zu tun, dass früher - oder besser bis vor kurzem - in Excel kein Boxplot möglich war. Daher nutzte jeder der mit Excel seine Daten auswertet den Barplot. Und was der Bauer nicht kennt… deshalb ist hier auch der Barplot dargestellt. Ich persönlich mag den Barplot eher weniger. Der Barplot ist einfach schlechter als der Boxplot. Wir haben nur die Standardabweichung als Maßzahl für die Streuung. Beim Boxplot haben wir das IQR, was uns mehr über die Streuung aussagt. Aber gut, häufig musst du den Barplot in deiner Abschlussarbeit machen. Also dann hier der Barplot. Wie erstellen wir nun einen Barplot in R? Zuerst laden wir die Daten mit der Funktion read_excel() in R, wenn du die Daten als .xlsx Datei vorliegen hast.\n\nflea_dog_cat_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n\nWir müssen jetzt für ggplot() noch den Mittelwert und die Streuung für die Gruppen berechnen. Du kannst als Streuung die Standardabweichung oder den Standardfehler nehmen. Ich würde die Standardabweichung bei kleinen Fallzahlen kleiner als 20 Beobachtungen nehmen.\n\nstat_tbl &lt;- flea_dog_cat_tbl |&gt; \n  group_by(animal) |&gt; \n  summarise(mean = mean(jump_length),\n            sd = sd(jump_length),\n            se = sd/sqrt(n()))\n\nWir nutzen nun das Objekt stat_tbl um den Barplot mit der Funktion ggplot() zu erstellen. Dabei müssen wir zum einen schauen, dass die Balken nicht übereinander angeordnet sind. Nebeneinander angeordnete Balken kriegen wir mit der Option stat = \"identity\" in dem geom_bar(). Dann müssen wir noch die Fehlerbalken ergänzen mit dem geom_errorbar. Hier kann nochmal mit der Option width = an der Länge der Fehlerenden gedreht werden.\n\nggplot(stat_tbl, aes(x = animal, y = mean, fill = animal)) + \n  theme_minimal() +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2)\n\n\n\n\n\n\n\n\nSchau doch auch mal in den ein oder anderen Zerforschenkasten, da findest du dann noch mehr Inspiration aus anderen Abbildungen, die ich nachgebaut habe. Ich bin einmal über den Campus gelaufen und habe geschaut, welche Barplots so auf den Postern verwendet werden. Im Folgenden dann einmal eine Reihe von Barplots zerforscht und in ggplot nachgebaut.\n\n\n\n\n\n\nZerforschen: Einfaktorieller Barplot mit compact letter display\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns einen einfaktoriellen Barplot oder Säulendiagramm anschauen. Daher fangen wir mit der folgenden Abbildung einmal an. Wir haben hier ein Säulendiagramm mit Compact letter display vorliegen. Daher brauchen wir eigentlich gar nicht so viele Zahlen. Für jede der vier Behandlungen jeweils einmal einen Mittelwert für die Höhe der Säule sowie einmal die Standardabweichung. Die Standardabweichung addieren und subtrahieren wir dann jeweils von dem Mittelwert und schon haben wir die Fehlerbalken.\n\n\n\n\n\n\nAbbildung 16.3— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein simples Säulendiagramm mit sehr für Farbblinde ungünstigen Farben. Es sind die Mittelwerte sowie die Standardabweichung durch die Fehlerbalken dargestellt.\n\n\n\nAls erstes brauchen wir die Daten. Die Daten habe ich mir in dem Datensatz zerforschen_barplot_simple.xlsx selber ausgedacht. Ich habe einfach die obige Abbildung genommen und den Mittelwert abgeschätzt. Dann habe ich die vier Werte alle um den Mittelwert streuen lassen. Dabei habe ich darauf geachtet, dass die Streuung dann in der letzten Behandlung am größten ist. Da wir beim Einlesen keine Umlaute oder sonstige Leerzeichen wollen, habe ich alles sehr simple aufgeschrieben und dann in R in der Funktion factor() richtig über die Option levels sortiert und über die Option labels sauber beschrieben. Dann passt auch die Sortierung der \\(x\\)-Achse.\n\nbarplot_tbl &lt;- read_excel(\"data/zerforschen_barplot_simple.xlsx\") |&gt; \n  mutate(trt = factor(trt, \n                      levels = c(\"water\", \"rqflex\", \n                                 \"nitra\", \"laqua\"),\n                      labels = c(\"Wasserdestilation\",\n                                 \"RQflex Nitra\",\n                                 \"Nitrachek\",\n                                 \"Laqua Nitrat\")))\nbarplot_tbl \n\n# A tibble: 16 × 2\n   trt               nitrat\n   &lt;fct&gt;              &lt;dbl&gt;\n 1 Wasserdestilation    135\n 2 Wasserdestilation    130\n 3 Wasserdestilation    145\n 4 Wasserdestilation    135\n 5 RQflex Nitra         120\n 6 RQflex Nitra         130\n 7 RQflex Nitra         135\n 8 RQflex Nitra         135\n 9 Nitrachek            100\n10 Nitrachek            120\n11 Nitrachek            130\n12 Nitrachek            130\n13 Laqua Nitrat         230\n14 Laqua Nitrat         210\n15 Laqua Nitrat         205\n16 Laqua Nitrat         220\n\n\nJetzt brauchen wir noch die Mittelwerte und die Standardabweichung für jede der vier Behandlungen. Den Code kennst du schon von oben wo wir die Barplots für die Sprungweiten der Hunde- und Katzenflöhe gebaut haben. Hier habe ich dann den Code entsprechen der Daten barplot_tbl angepasst. Wir haben ja als Gruppierungsvariabel trt vorliegen und wollen die Mittelwerte und die Standardabweichung für die Variable nitrat berechnen.\n\nstat_tbl &lt;- barplot_tbl |&gt; \n  group_by(trt) |&gt; \n  summarise(mean = mean(nitrat),\n            sd = sd(nitrat))\nstat_tbl\n\n# A tibble: 4 × 3\n  trt                mean    sd\n  &lt;fct&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 Wasserdestilation  136.  6.29\n2 RQflex Nitra       130   7.07\n3 Nitrachek          120  14.1 \n4 Laqua Nitrat       216. 11.1 \n\n\nUnd dann haben wir auch schon die Abbildung 16.4 erstellt. Ja vielleicht passen die Standardabweichungen nicht so richtig, da könnte man nochmal an den Daten spielen und die Werte solange ändern, bis es besser passt. Du hast aber jetzt eine Idee, wie der Aufbau funktioniert.\n\nggplot(data = stat_tbl, aes(x = trt, y = mean,\n                            fill = trt)) +\n  theme_minimal() +\n1  geom_bar(stat = \"identity\") +\n2  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2) +\n  labs(x = \"\", \n       y = \"Nitrat-Konzentration \\n im Tannensaft [mg/L]\") +\n  ylim(0, 250) +\n  theme(legend.position = \"none\") + \n3  scale_fill_okabeito() +\n  annotate(\"text\", \n           x = c(1.05, 2.05, 3.05, 4.05), \n           y = stat_tbl$mean + stat_tbl$sd + 8, \n           label = c(\"b\", \"b\", \"a\", \"c\"))\n\n\n1\n\nHier werden die Säulen des Säulendiagramms erstellt.\n\n2\n\nHier werden die Fehlerbalken erstellt. Die Option width steuert wie breit die Fehlerbalken sind.\n\n3\n\nHier wird eine Farbpalette für farbblinde Personen geladen.\n\n\n\n\n\n\n\n\n\n\nAbbildung 16.4— Die Abbildung des Säulendiagramms in ggplot nachgebaut.\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_barplot.png\", width = 5, height = 3)\n\n\n\n\n\n\n\n\n\n\nZerforschen: Zweifaktorieller Barplot mit compact letter display\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns einen zweifaktoriellen Barplot oder Säulendiagramm anschauen. Wir haben hier ein Säulendiagramm mit compact letter display vorliegen. Daher brauchen wir eigentlich gar nicht so viele Zahlen. Für jede der vier Zeitpunkte und der Kontrolle jeweils einmal einen Mittelwert für die Höhe der Säule sowie einmal die Standardabweichung. Die Standardabweichung addieren und subtrahieren wir dann jeweils von dem Mittelwert und schon haben wir die Fehlerbalken.\n\n\n\n\n\n\nAbbildung 16.5— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit zwei Faktoren Zeit und die Iodine Form.\n\n\n\nAls erstes brauchen wir die Daten. Die Daten habe ich mir in dem Datensatz zerforschen_barplot_2fac_target.xlsx selber ausgedacht. Ich habe einfach die obige Abbildung genommen und den Mittelwert abgeschätzt. Dann habe ich die drei Werte alle um den Mittelwert streuen lassen. Da wir beim Einlesen keine Umlaute oder sonstige Leerzeichen wollen, habe ich alles sehr simple aufgeschrieben und dann in R in der Funktion factor() richtig über die Option levels sortiert und über die Option labels sauber beschrieben. Dann passt auch die Sortierung der \\(x\\)-Achse.\n\nbarplot_tbl &lt;- read_excel(\"data/zerforschen_barplot_2fac_target.xlsx\") |&gt; \n  mutate(time = factor(time, \n                       levels = c(\"ctrl\", \"7\", \"11\", \"15\", \"19\"),\n                       labels = c(\"Contr.\", \"07:00\", \"11:00\", \"15:00\", \"19:00\")),\n         type = as_factor(type))\nbarplot_tbl \n\n# A tibble: 27 × 3\n   time  type  iodine\n   &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1 07:00 KIO3      50\n 2 07:00 KIO3      55\n 3 07:00 KIO3      60\n 4 07:00 KI        97\n 5 07:00 KI        90\n 6 07:00 KI        83\n 7 11:00 KIO3      73\n 8 11:00 KIO3      75\n 9 11:00 KIO3      78\n10 11:00 KI       130\n# ℹ 17 more rows\n\n\nJetzt brauchen wir noch die Mittelwerte und die Standardabweichung für jede der vier Behandlungen. Hier nur kurz, den Code kennst du schon aus anderen Zerforschenbeispielen zu den Barplots.\n\nstat_tbl &lt;- barplot_tbl |&gt; \n  group_by(time, type) |&gt; \n  summarise(mean = mean(iodine),\n            sd = sd(iodine))\nstat_tbl\n\n# A tibble: 9 × 4\n# Groups:   time [5]\n  time   type   mean    sd\n  &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Contr. KIO3    5    3   \n2 07:00  KIO3   55    5   \n3 07:00  KI     90    7   \n4 11:00  KIO3   75.3  2.52\n5 11:00  KI    150   20   \n6 15:00  KIO3   80    7   \n7 15:00  KI    130   20   \n8 19:00  KIO3   90.3 19.5 \n9 19:00  KI     95    6   \n\n\nUnd dann geht es auch schon los. Wir müssen am Anfang einmal scale_x_discrete() setzen, damit wir gleich den Zielbereich ganz hinten zeichnen können. Sonst ist der blaue Bereich im Vordergrund. Dann färben wir auch mal die Balken anders ein. Muss ja auch mal sein. Auch nutzen wir die Funktion geom_text() um das compact letter display gut zu setzten. Die \\(y\\)-Position berechnet sich aus dem Mittelwert plus Standardabweichung innerhalb des geom_text(). Leider haben wir nur einen Balken bei der Kontrolle, deshalb hier nachträglich der Buchstabe mit annotate(). Ich habe mich dann noch entschieden neben dem Barplot noch den Boxplot als Alternative zu erstellen.\n\nBarplotBoxplot\n\n\nEinmal der Barplot wie beschrieben. Am besten löscht du immer mal wieder eine Zeile Code damit du nachvollziehen kannst, was die Zeile Code in der Abbildung macht. Vergleiche auch einmal diese Abbildung der Barplots mit der Abbildung der Boxplots und überlege, welche der beiden Abbildungen dir mehr Informationen liefert.\n\nggplot(data = stat_tbl, aes(x = time, y = mean,\n                               fill = type)) +\n  theme_minimal() +\n  scale_x_discrete() +\n  annotate(\"rect\", xmin = 0.25, xmax = 5.75, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = \"darkblue\") +                        \n  annotate(\"text\", x = 0.5, y = 75, hjust = \"left\", label = \"Target area\") + \n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_manual(name = \"Type\", values = c(\"darkgreen\", \"darkblue\")) + \n  theme(legend.position = c(0.1, 0.8),\n        legend.title = element_blank(), \n        legend.spacing.y = unit(0, \"mm\"), \n        panel.border = element_rect(colour = \"black\", fill=NA),\n        axis.text = element_text(colour = 1, size = 12),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) +\n  labs(x = \"Time of application [time of day]\",\n       y =  expression(Iodine~content~\"[\"*mu*g~I~100*g^'-1'~f*.*m*.*\"]\")) +\n  scale_y_continuous(breaks = c(0, 50, 100, 150, 200),\n                     limits = c(0, 200)) +\n  geom_text(aes(label = c(\"\", \"b\", \"bc\", \"bc\", \"d\", \"bc\", \"d\", \"bc\", \"c\"), \n                y = mean + sd + 2),  \n            position = position_dodge(width = 0.9), vjust = -0.25) + \n  annotate(\"text\", x = 0.77, y = 15, label = \"a\") \n\n\n\n\n\n\n\nAbbildung 16.6— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen.\n\n\n\n\n\n\n\nFür die Boxplost brauchen wir dann noch ein Objekt mehr. Um das compacte letter dislay an die richtige Position zu setzen brauchen wir noch eine \\(y\\)-Position. Ich nehme hier dann das 90% Quantile. Das 90% Quantile sollte dann auf jeden Fall über die Schnurrhaare raus reichen. Wir nutzen dann den Datensatz letter_pos_tbl in dem geom_text() um die Buchstaben richtig zu setzen.\n\nletter_pos_tbl &lt;- barplot_tbl |&gt; \n  group_by(time, type) |&gt; \n  summarise(quant_90 = quantile(iodine, probs = c(0.90)))\nletter_pos_tbl\n\n# A tibble: 9 × 3\n# Groups:   time [5]\n  time   type  quant_90\n  &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;\n1 Contr. KIO3       7.4\n2 07:00  KIO3      59  \n3 07:00  KI        95.6\n4 11:00  KIO3      77.4\n5 11:00  KI       166  \n6 15:00  KIO3      85.6\n7 15:00  KI       146  \n8 19:00  KIO3     106  \n9 19:00  KI        99.8\n\n\nUnd dann müssen wir nur noch das geom_bar() und geom_errorbar() entfernen und durch das geom_boxplot() ersetzen. Dann haben wir auch schon unsere wunderbaren Boxplots. Das Problem sind natürlich die wenigen Beobachtungen, deshalb sehen die Boxplots teilweise etwas wild aus. Beachte auch das wir die Orginaldaten nutzen und nicht die zusammengefassten Daten.\n\nggplot(data = barplot_tbl, aes(x = time, y = iodine,\n                               fill = type)) +\n  theme_minimal() +\n  scale_x_discrete() +\n  annotate(\"rect\", xmin = 0.25, xmax = 5.75, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = \"darkblue\") +                        \n  annotate(\"text\", x = 0.5, y = 75, hjust = \"left\", label = \"Target area\") + \n  geom_boxplot(position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_manual(name = \"Type\", values = c(\"darkgreen\", \"darkblue\")) + \n  theme(legend.position = c(0.1, 0.8),\n        legend.title = element_blank(), \n        legend.spacing.y = unit(0, \"mm\"), \n        panel.border = element_rect(colour = \"black\", fill=NA),\n        axis.text = element_text(colour = 1, size = 12),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) +\n  labs(x = \"Time of application [time of day]\",\n       y =  expression(Iodine~content~\"[\"*mu*g~I~100*g^'-1'~f*.*m*.*\"]\")) +\n  scale_y_continuous(breaks = c(0, 50, 100, 150, 200),\n                     limits = c(0, 200)) +\n  geom_text(data = letter_pos_tbl, \n            aes(label = c(\"\", \"b\", \"bc\", \"bc\", \"d\", \"bc\", \"d\", \"bc\", \"c\"), \n                y = quant_90 + 5),  \n            position = position_dodge(width = 0.9), vjust = -0.25) + \n  annotate(\"text\", x = 0.77, y = 15, label = \"a\") \n\n\n\n\n\n\n\nAbbildung 16.7— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen, dafür müssen wir usn aber nochmal ein Positionsdatensatz bauen.\n\n\n\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_barplot.png\", width = 5, height = 3)\n\n\n\n\n\n\n\n\n\n\nZerforschen: Zweifaktorieller, gekippter Barplot mit Zielbereich\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns einen zweifaktoriellen Barplot oder Balkendiagramm anschauen. Wir haben hier ein echtes Balkendiagramm mit compact letter display vorliegen. Daher brauchen wir eigentlich gar nicht so viele Zahlen. Für jede der vier Zeitpunkte und der Kontrolle jeweils einmal einen Mittelwert für die Länge des Balkens sowie einmal die Standardabweichung. Die Standardabweichung addieren und subtrahieren wir dann jeweils von dem Mittelwert und schon haben wir die Fehlerbalken. Ich habe hier dann jeweils drei Werte für jede Faktorkombination. Die brauche ich dann auch, weil ich später nochmal vergleichend den Boxplot erstellen möchte. Der macht meiner Ansicht nach hier mehr Sinn.\n\n\n\n\n\n\nAbbildung 16.8— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein zweifaktorielles Balkendiagramm mit einem Zielbereich und compact letter display\n\n\n\nAls erstes brauchen wir wieder die Daten. Die Daten habe ich mir in dem Datensatz zerforschen_barplot_2fac.xlsx selber ausgedacht. Ich habe einfach die obige Abbildung genommen und den Mittelwert abgeschätzt. Dann habe ich die drei Werte alle um den Mittelwert streuen lassen. Das war es dann auch schon.\n\nbarplot_tbl &lt;- read_excel(\"data/zerforschen_barplot_2fac_flipped.xlsx\") |&gt; \n  mutate(iod = fct_rev(iod),\n         fruit = fct_rev(fruit))\nbarplot_tbl\n\n# A tibble: 12 × 3\n   iod   fruit iod_yield\n   &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;\n 1 KI    65-75        85\n 2 KI    65-75       100\n 3 KI    65-75        75\n 4 KI    75-80        52\n 5 KI    75-80        70\n 6 KI    75-80        30\n 7 KIO3  65-75        55\n 8 KIO3  65-75        48\n 9 KIO3  65-75        60\n10 KIO3  75-80        45\n11 KIO3  75-80        53\n12 KIO3  75-80        38\n\n\nJetzt brauchen wir noch die Mittelwerte und die Standardabweichung für jede der vier Behandlungen. Hier nur kurz, den Code kennst du schon aus anderen Zerforschenbeispielen zu den Barplots.\n\nstat_tbl &lt;- barplot_tbl |&gt; \n  group_by(iod, fruit) |&gt; \n  summarise(mean = mean(iod_yield),\n            sd = sd(iod_yield))\nstat_tbl\n\n# A tibble: 4 × 4\n# Groups:   iod [2]\n  iod   fruit  mean    sd\n  &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 KIO3  75-80  45.3  7.51\n2 KIO3  65-75  54.3  6.03\n3 KI    75-80  50.7 20.0 \n4 KI    65-75  86.7 12.6 \n\n\nIm Folgenden stelle ich die zusammengefassten Daten stat_tbl als Balkendiagramm dar. Die ursprünglichen Daten barplot_tbl kann ich nutzen um die Boxplots zu erstellen. Hier ist wichtig nochmal zu erinnern, das wir Barplots auf dem Mittelwert und der Standardabweichung darstellen und die Boxplots auf den Originaldaten. Mit der unteren Grenze machen Boxplots mehr Sinn, wenn du wissen willst, ob du einen Zielbereich vollkommen erreicht hast.\n\nBarplotBoxplot\n\n\nZuerst einmal der Barplot, wie wir ihn auch schon oben in der Abbildung sehen. Wir nutzen hier zum einen die Funktion coord_flip() um ganz zum Schluss die Abbildung zu drehen. Deshalb musst du aufpassen, denn vor dem Flippen ist ja alles auf der \\(y\\)-Achse die \\(y\\)-Achse und umgekehrt. Deshalb müssen wir dann auch teilweise die Ordnung der Level in den einzelnen Faktoren drehen, damit wir wieder die richtige Reihenfolge nach dem Flip haben. Wir müssen ganz am Anfang einmal scale_x_discrete() setzen, damit wir den Zielbereich als erstes einzeichnen können. Sonst ist der Zielbereich nicht ganz hinten in der Abbildung und überdeckt die Balken. Deshalb ist das Wort “Zielbereich” auch recht weit hinten im Code, damit es eben im Vordergrund ist. Sonst ist eigentlich vieles gleich. Wir nutzen hier einmal das R Paket latex2exp für die Erstellung der mathematischen Formeln.\n\nggplot(data = stat_tbl, aes(x = iod, y = mean,\n                            fill = fruit)) +\n  theme_minimal() +\n  scale_x_discrete() +\n  annotate(\"rect\", xmin = 0, xmax = 3, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = cbbPalette[6]) + \n  geom_hline(yintercept = c(25, 50, 75, 100), linetype = \"dashed\") +\n  geom_bar(stat = \"identity\", position = position_dodge(0.9)) + \n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd), \n                width = 0.2, position = position_dodge(0.9)) +\n  labs(x = \"Iodform\",\n       y =TeX(r\"(Iodgehalt $[\\mu g\\, l\\, (100g\\, FM)^{-1}]$)\")) +\n  scale_fill_okabeito(name = \"Frucht-\\ngröße [mm]\", breaks=c('65-75', '75-80')) +\n  theme(legend.position = c(0.85, 0.2),\n        legend.box.background = element_rect(color = \"black\"),\n        legend.box.margin = margin(t = 1, l = 1),\n        legend.text.align = 0) +\n  annotate(\"label\", x = 1.5, y = 75, label = \"Zielbereich\", size = 5) +\n  annotate(\"text\", x = c(0.8, 1.25, 1.8, 2.25), y = c(55, 62, 72, 102),\n           label = c(\"a\", \"a\", \"a\", \"b\")) +\n  coord_flip() \n\n\n\n\n\n\n\nAbbildung 16.9— Die Abbildung des Balkendiagramms in ggplot nachgebaut. Ein extra Zielbereich ist definiert sowie die Legende in die Abbildung integriert.\n\n\n\n\n\n\n\nFür die Boxplots müssen wir gar nicht viel tun. Wir müssen nur noch das geom_bar() und geom_errorbar() entfernen und durch das geom_boxplot() ersetzen. Dann haben wir auch schon unsere wunderbaren Boxplots. Das Problem sind natürlich die wenigen Beobachtungen, deshalb sehen die Boxplots teilweise etwas wild aus. Beachte auch das wir die Orginaldaten nutzen und nicht die zusammengefassten Daten. Am Ende machen Boxplots mit einer unteren Grenze mehr Sinn, wenn wir uns Fragen, ob wir einen Zielbereich erreicht haben. Da sind dann doch Balkendiagramme etwas ungeeignet.\n\nggplot(data = barplot_tbl, aes(x = iod, y = iod_yield,\n                               fill = fruit)) +\n  theme_minimal() +\n  geom_hline(yintercept = c(25, 50, 75, 100), linetype = \"dashed\") +\n  geom_boxplot() +\n  labs(x = \"Iodform\",\n       y =TeX(r\"(Iodgehalt $[\\mu g\\, l\\, (100g\\, FM)^{-1}]$)\")) +\n  scale_fill_okabeito(name = \"Frucht-\\ngröße [mm]\", breaks=c('65-75', '75-80')) +\n  theme(legend.position = c(0.85, 0.2),\n        legend.box.background = element_rect(color = \"black\"),\n        legend.box.margin = margin(t = 1, l = 1),\n        legend.text.align = 0) +\n  annotate(\"rect\", xmin = 0, xmax = 3, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = cbbPalette[6]) + \n  annotate(\"label\", x = 1.5, y = 75, label = \"Zielbereich\", size = 5) +\n  annotate(\"text\", x = c(0.8, 1.25, 1.8, 2.25), y = c(55, 62, 72, 102),\n           label = c(\"a\", \"a\", \"a\", \"b\")) +\n  coord_flip() \n\n\n\n\n\n\n\nAbbildung 16.10— Die Abbildung des Balkendiagramms in ggplot nachgebaut. Durch den Boxplot erhalten wir auch untere Grenzen, was bei der Frage, ob wir in einem Zielbereich sind, viel sinnvoller ist, als ein Balkendiagramm. Eine höhere Fallzahl als \\(n=3\\) würde die Boxplots schöner machen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.3.2 Boxplot\nMit dem Boxplot können wir den Median und die Quartile visualisieren. In Abbildung 16.11 sehen wir einen Boxplot, der den Median und die Quartile visualisiert. Die Box wird aus dem IQR gebildet. Der Median wird als Strich in der Box gezeigt. Die Schnurrhaare (eng. Whiskers) sind das 1.5 fache des IQR. Punkte die außerhalb der Schnurrhaare liegen werden als einzelne Punkte dargestellt. Diese einzelnen Punkte werden auch als Ausreißer (eng. Outlier) bezeichnet.\n\n\n\n\n\n\nAbbildung 16.11— Ein Boxplot der die statistischen Maßzahlen Median und Quartile visualisiert. Die Box wird aus dem IQR gebildet. Der Median wird als Strich in der Box gezeigt. Die Schnurrhaare sind das 1.5 fache des IQR. Punkte die außerhalb der Schnurrhaare liegen werden als einzele Punkte dargestellt.\n\n\n\nWie erstellen wir nun einen Boxplot in R? Zuerst laden wir die Daten mit der Funktion read_excel() in R, wenn du die Daten als .xlsx Datei vorliegen hast.\n\nflea_dog_cat_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n\nIn Abbildung 16.12 ist der Boxplot für die Daten aus der Datei flea_dog_cat.xlsx dargestellt. Auf der x-Achse finden wir die Tierart als cat und dog. Auf der y-Achse ist die Sprungweite in [cm] dargestellt. Wir erkennen auf einen Blick, dass die Sprungweite von den Hundeflöhen weiter ist als die Sprungweite der Katzenflöhe. Im Weiteren können wir abschätzen, dass die Streuung etwa gleich groß ist. Die Boxen sind in etwa gleich groß und die Whiskers in etwa gleich lang.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = jump_length,\n                                    fill = animal)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(x = \"Tierart\", y = \"Sprungweite [cm]\") \n\n\n\n\n\n\n\nAbbildung 16.12— An 39 Hunden wurde die Anzahl an Flöhen gezählt.\n\n\n\n\n\nWir neigen dazu die Boxplots und die Effekte zu überschätzen, wenn die Anzahl der Beobachtungen klein ist. Deshalb können wir mit dem geom_jitter() in der Abbildung 16.13 noch die Beobachtungen zu den Boxplot ergänzen. Die Funktion geom_jitter() streut die Punkte zufällig, so dass keine Punkte übereinander liegen. Wir haben hier die Streuweite durch die Option width = 0.25 etwas eingeschränkt. Darüber hinaus habe wir das Aussehen der Punkte mit shape = 1 geändert, so dass wir die Jitter-Punkte von den potenziellen Ausreißer-Punkten unterscheiden können. Du kannst auch andere Zahlen hinter shape eintragen um verschiedene Punktesymbole durchzuprobieren. Eine Übersicht an shapes findest du auch hier unter Cookbook for R &gt; Graphs &gt; Shapes and line types.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = jump_length,\n                                    fill = animal)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.25, shape = 1) +\n  theme_minimal() +\n  labs(x = \"Tierart\", y = \"Sprungweite [cm]\") \n\n\n\n\n\n\n\nAbbildung 16.13— An 39 Hunden wurde die Anzahl an Flöhen gezählt zusammen mit den einzelnen Beobachtungen dargestellt.\n\n\n\n\n\nSchau dir auch hier mal in den Zerforschenkasten an, da findest du dann noch mehr Inspiration aus anderen Abbildungen, die ich nachgebaut habe. Ich bin einmal über den Campus gelaufen und habe geschaut, welche Barplots so auf den Postern verwendet werden und habe hier auch einmal einen Boxplot aus dem Barplot gebaut.\n\n\n\n\n\n\nZerforschen: Zweifaktorieller Barplot oder Boxplot\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns einen zweifaktoriellen Barplot oder Säulendiagramm anschauen. Wir haben hier ein Säulendiagramm mit den Mittelwerten über den Faktor auf der \\(x\\)-Achse vorliegen. Daher brauchen wir eigentlich gar nicht so viele Zahlen. Für jede der drei Fruchtgrößen jeweils einmal einen Mittelwert für die Höhe der Säule sowie einmal die Standardabweichung für die vier Stickstoffangebote. Das addiert sich dann auf, aber es geht noch. Die Standardabweichung addieren und subtrahieren wir dann später jeweils von dem Mittelwert und schon haben wir die Fehlerbalken. Da ich auch hier einmal als Alternative die Boxplots erstellen will, brauche ich hier mehr Werte aus denen ich dann die Mittelwerte und die Standardabweichung berechne.\n\n\n\n\n\n\nAbbildung 16.14— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein zweifaktorieller Barplot mit Mittelwerten über dem Faktor auf der \\(x\\)-Achse.\n\n\n\nAls erstes brauchen wir wieder die Daten. Die Daten habe ich mir in dem Datensatz zerforschen_barplot_2fac.xlsx selber ausgedacht. Ich habe einfach die obige Abbildung genommen und den Mittelwert abgeschätzt. Dann habe ich die drei Werte alle um den Mittelwert streuen lassen. Das war es dann auch schon.\n\nbarplot_tbl &lt;- read_excel(\"data/zerforschen_barplot_2fac.xlsx\") |&gt; \n  mutate(frucht = factor(frucht, \n                         levels = c(\"klein\", \"mittel\", \"groß\"),\n                         labels = c(\"Klein\", \"Mittel\", \"Groß\")),\n         nmin = as_factor(nmin))\nbarplot_tbl \n\n# A tibble: 36 × 3\n   frucht nmin  yield\n   &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt;\n 1 Klein  150     1.5\n 2 Klein  150     2.2\n 3 Klein  150     2.8\n 4 Klein  200     2.4\n 5 Klein  200     1.9\n 6 Klein  200     2.9\n 7 Klein  250     3.7\n 8 Klein  250     3.9\n 9 Klein  250     3.5\n10 Klein  300     2.5\n# ℹ 26 more rows\n\n\nJetzt brauchen wir noch die Mittelwerte und die Standardabweichung für jede der drei Fruchtgrößen und Stickstoffangebote. Hier nur kurz, den Code kennst du schon aus anderen Zerforschenbeispielen zu den Barplots. Das ist soweit erstmal nichts besonderes und ähnelt auch der Erstellung der anderen Barplots.\n\nstat_all_tbl &lt;- barplot_tbl |&gt; \n  group_by(frucht, nmin) |&gt; \n  summarise(mean = mean(yield),\n            sd = sd(yield))\nstat_all_tbl\n\n# A tibble: 12 × 4\n# Groups:   frucht [3]\n   frucht nmin   mean    sd\n   &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Klein  150    2.17 0.651\n 2 Klein  200    2.4  0.5  \n 3 Klein  250    3.7  0.2  \n 4 Klein  300    2.5  0.4  \n 5 Mittel 150    5.03 1.05 \n 6 Mittel 200    7.03 0.252\n 7 Mittel 250    8    0.200\n 8 Mittel 300    7.7  0.200\n 9 Groß   150    6.1  1.95 \n10 Groß   200    8.27 0.702\n11 Groß   250    9.07 1.00 \n12 Groß   300    8.97 1.00 \n\n\nWeil wir dann noch die globalen Mittelwerte der Früchte über alle Stickstofflevel wollen, müssen wir nochmal die Mittelwerte und die Standardabweichung nur für die drei Fruchtgrößen berechnen. Daher haben wir dann zwei Datensätze, die uns eine Zusammenfassung der Daten liefern.\n\nstat_fruit_tbl &lt;- barplot_tbl |&gt; \n  group_by(frucht) |&gt; \n  summarise(mean = mean(yield))\nstat_fruit_tbl\n\n# A tibble: 3 × 2\n  frucht  mean\n  &lt;fct&gt;  &lt;dbl&gt;\n1 Klein   2.69\n2 Mittel  6.94\n3 Groß    8.1 \n\n\nAuch hier möchte ich einmal den Barplot nachbauen und dann als Alternative noch den Barplot anbieten. Am nervigsten war der Zeilenumbruch in der Legendenbeschriftung mit N\\(_min\\). Es hat echt gedauert, bis ich die Funktion atop() gefunden hatte, die in einer expression() einen Zeilenumbruch erzwingt. Meine Güte war das langwierig. Der Rest ist eigentlich wie schon in den anderen Beispielen. Da schaue dann doch nochmal da mit rein.\n\nBarplotBoxplot\n\n\nEinmal der Barplot wie beschrieben. Vergleiche auch einmal diese Abbildung der Barplots mit der Abbildung der Boxplots in dem anderem Tab und überlege, welche der beiden Abbildungen dir mehr Informationen liefert.\n\nggplot(data = stat_all_tbl, aes(x = frucht, y = mean,\n                                fill = nmin)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\", position = position_dodge(0.9)) + \n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd), \n                width = 0.2, position = position_dodge(0.9)) +\n  labs(x = \"Fruchtgröße zum Erntezeitpunkt\", \n       y = expression(atop(\"Gesamtfruchtertrag\", \"[\"*kg~FM~m^\"-2\"*\"]\")),\n       fill = expression(atop(N[min]~\"Angebot\", \"[\"*kg~N~ha^\"-1\"*\"]\"))) +\n  scale_y_continuous(breaks = c(0, 2, 4, 6, 8, 10, 12),\n                     limits = c(0, 12)) +\n  scale_fill_okabeito() +\n  theme(legend.position = \"right\",\n        legend.box.background = element_rect(color = \"black\"),\n        legend.box.margin = margin(t = 1, l = 1),\n        legend.text.align = 0) +\n  annotate(\"text\", x = c(1, 2, 3, 1.45), y = c(6, 10, 11, 3),\n           label = c(expression(bar(y)*\" = \"*2.7),\n                     expression(bar(y)*\" = \"*6.9),\n                     expression(bar(y)*\" = \"*8.1),\n                     \"SD\"))\n\n\n\n\n\n\n\nAbbildung 16.15— Die Abbildung des Säulendiagramms in ggplot nachgebaut.\n\n\n\n\n\n\n\nFür die Boxplots müssen wir gar nicht viel tun. Wir müssen nur noch das geom_bar() und geom_errorbar() entfernen und durch das geom_boxplot() ersetzen. Dann haben wir auch schon unsere wunderbaren Boxplots. Das Problem sind natürlich die wenigen Beobachtungen, deshalb sehen die Boxplots teilweise etwas wild aus. Beachte auch das wir die Orginaldaten nutzen und nicht die zusammengefassten Daten.\n\nggplot(data = barplot_tbl, aes(x = frucht, y = yield,\n                               fill = nmin)) +\n  theme_minimal() +\n  geom_boxplot() +\n  labs(x = \"Fruchtgröße zum Erntezeitpunkt\", \n       y = expression(atop(\"Gesamtfruchtertrag\", \"[\"*kg~FM~m^\"-2\"*\"]\")),\n       fill = expression(atop(N[min]~\"Angebot\", \"[\"*kg~N~ha^\"-1\"*\"]\"))) +\n  scale_y_continuous(breaks = c(0, 2, 4, 6, 8, 10, 12),\n                     limits = c(0, 12)) +\n  scale_fill_okabeito() +\n  theme(legend.position = \"right\",\n        legend.box.background = element_rect(color = \"black\"),\n        legend.box.margin = margin(t = 1, l = 1),\n        legend.text.align = 0) +\n  annotate(\"text\", x = c(1, 2, 3), y = c(6, 10, 11),\n           label = c(expression(bar(y)*\" = \"*2.7),\n                     expression(bar(y)*\" = \"*6.9),\n                     expression(bar(y)*\" = \"*8.1)))\n\n\n\n\n\n\n\nAbbildung 16.16— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut.\n\n\n\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_barplot.png\", width = 5, height = 3)\n\n\n\n\n\n\n\n\n\n\nZerforschen: Boxplot für mehrere Outcomes\n\n\n\n\n\nHier wollen wir einmal eine etwas größere Abbildung 33.13 mit einer Menge Boxplots zerforschen. Da ich den Datensatz dann nicht zu groß machen wollte und Boxplots zu zerforschen manchmal nicht so einfach ist, passen die Ausreißer dann manchmal dann doch nicht. Auch liefern die statistischen Tests dann nicht super genau die gleichen Ergebnisse. Aber das macht vermutlich nicht so viel, hier geht es ja dann eher um den Bau der Boxplots und dem rechnen des statistischen Tests in {emmeans}. Die Reihenfolge des compact letter displays habe ich dann auch nicht angepasst sondern die Buchstaben genommen, die ich dann erhalten habe. Die Sortierung kann man ja selber einfach ändern. Wir haben hier ein einfaktorielles Design mit einem Behandlungsfaktor mit drei Leveln vorliegen. Insgesamt schauen wir uns vier Endpunkte in veränderten Substrat- und Wasserbedingungen an.\n\n\n\n\n\n\nAbbildung 16.17— Ursprüngliche Abbildung, die nachgebaut werden soll. Insgesamt vier Outcomes sollen für zwei Behandlungen ausgewertet werden. Das praktische ist hier, dass wir es nur mit einem einfaktoriellen Design zu tun haben.\n\n\n\nIm Folgenden lade ich einmal den Datensatz, den ich dann per Auge zusammengesetzt habe. Das war jetzt etwas anstrengender als gedacht, da ich nicht weiß wie viele Beobachtungen einen Boxplot bilden. Je mehr Beobachtungen, desto feiner kann man den Boxplot abstimmen. Da ich hier nur mit sieben Beobachtungen ja Gruppe gearbeitet habe, habe ich es nicht geschafft die Ausreißer darzustellen. Das wäre mir dann zu viel Arbeit geworden. Nachdem ich jetzt die Daten geladen habe, muss ich noch über die Funktion pivot_longer() einen Datensatz passenden im Longformat bauen. Abschließend mutiere ich dann noch alle Faktoren richtig und vergebe bessere Namen als labels sonst versteht man ja nicht was die Spalten bedeuten.\n\nboxplot_mult_tbl &lt;- read_excel(\"data/zerforschen_boxplot_mult.xlsx\") |&gt; \n  pivot_longer(cols = fresh_weight:flower_number,\n               values_to = \"rsp\",\n               names_to = \"plant_measure\") |&gt; \n  mutate(trt = as_factor(trt),\n         plant_measure = factor(plant_measure,\n                                levels = c(\"fresh_weight\", \"dry_weight\",\n                                           \"plant_height\", \"flower_number\"),\n                                labels = c(\"Fresh weight (g)\", \"Dry weight (g)\",\n                                           \"Plant height (cm)\", \"Flower number\")),\n         type = factor(type, labels = c(\"Substrate\", \"Water\"))) \n\nIch habe mir dann die beiden Behandlungen substrate und water in die neue Spalte type geschrieben. Die Spaltennamen der Outcomes wandern in die Spalte plant_measure und die entsprechenden Werte in die Spalte rsp. Dann werde ich hier mal alle Outcomes auf einmal auswerten und nutze dafür das R Paket {purrr} mit der Funktion nest() und map(). Ich packe mir als die Daten nach type und plant_measure einmal zusammen. Dann habe ich einen neuen genesteten Datensatz mit nur noch acht Zeilen. Auf jeder Zeile rechne ich dann jeweils mein Modell. Wie du dann siehst ist in der Spalte data dann jeweils ein Datensatz mit der Spalte trt und rsp für die entsprechenden 21 Beobachtungen.\n\nboxplot_mult_nest_tbl &lt;- boxplot_mult_tbl |&gt;\n  group_by(type, plant_measure) |&gt;\n  nest() \n\nboxplot_mult_nest_tbl\n\n# A tibble: 8 × 3\n# Groups:   type, plant_measure [8]\n  type      plant_measure     data             \n  &lt;fct&gt;     &lt;fct&gt;             &lt;list&gt;           \n1 Substrate Fresh weight (g)  &lt;tibble [21 × 2]&gt;\n2 Substrate Dry weight (g)    &lt;tibble [21 × 2]&gt;\n3 Substrate Plant height (cm) &lt;tibble [21 × 2]&gt;\n4 Substrate Flower number     &lt;tibble [21 × 2]&gt;\n5 Water     Fresh weight (g)  &lt;tibble [21 × 2]&gt;\n6 Water     Dry weight (g)    &lt;tibble [21 × 2]&gt;\n7 Water     Plant height (cm) &lt;tibble [21 × 2]&gt;\n8 Water     Flower number     &lt;tibble [21 × 2]&gt;\n\n\nZur Veranschaulichung rechne ich jetzt einmal mit mutate() und map() für jeden der Datensätze in der Spalte data einmal ein lineares Modell mit der Funktion lm(). Ich muss nur darauf achten, dass ich die Daten mit .x einmal an die richtige Stelle in der Funktion lm() übergebe. Dann habe ich alle Modell komprimiert in der Spalte model. Das geht natürlcih auch alles super in einem Rutsch.\n\nboxplot_mult_nest_model_tbl &lt;- boxplot_mult_nest_tbl |&gt;\n  mutate(model = map(data, ~lm(rsp ~ trt, data = .x)))\n\nboxplot_mult_nest_model_tbl \n\n# A tibble: 8 × 4\n# Groups:   type, plant_measure [8]\n  type      plant_measure     data              model \n  &lt;fct&gt;     &lt;fct&gt;             &lt;list&gt;            &lt;list&gt;\n1 Substrate Fresh weight (g)  &lt;tibble [21 × 2]&gt; &lt;lm&gt;  \n2 Substrate Dry weight (g)    &lt;tibble [21 × 2]&gt; &lt;lm&gt;  \n3 Substrate Plant height (cm) &lt;tibble [21 × 2]&gt; &lt;lm&gt;  \n4 Substrate Flower number     &lt;tibble [21 × 2]&gt; &lt;lm&gt;  \n5 Water     Fresh weight (g)  &lt;tibble [21 × 2]&gt; &lt;lm&gt;  \n6 Water     Dry weight (g)    &lt;tibble [21 × 2]&gt; &lt;lm&gt;  \n7 Water     Plant height (cm) &lt;tibble [21 × 2]&gt; &lt;lm&gt;  \n8 Water     Flower number     &lt;tibble [21 × 2]&gt; &lt;lm&gt;  \n\n\nJetzt können wir einmal eskalieren und insgesamt acht mal die ANOVA rechnen. Das sieht jetzt nach viel Code aus, aber am Ende ist es nur eine lange Pipe. Am Ende erhalten wir dann den \\(p\\)-Wert für die einfaktorielle ANOVA für die Behandlung trt wiedergegeben. Wir sehen, dass wir eigentlich nur einen signifikanten Unterschied in der Wassergruppe und der Pflanzenhöhe erwarten sollten. Da der \\(p\\)-Wert für die Wassergruppe und der Blütenanzahl auch sehr nah an dem Signifikanzniveau ist, könnte hier auch etwas sein, wenn wir die Gruppen nochmal getrennt testen.\n\nboxplot_mult_nest_model_tbl  |&gt; \n  mutate(anova = map(model, anova)) |&gt; \n  mutate(parameter = map(anova, model_parameters)) |&gt; \n  select(type, plant_measure, parameter) |&gt; \n  unnest(parameter) |&gt; \n  clean_names() |&gt; \n  filter(parameter != \"Residuals\") |&gt; \n  select(type, plant_measure, parameter, p)\n\n# A tibble: 8 × 4\n# Groups:   type, plant_measure [8]\n  type      plant_measure     parameter        p\n  &lt;fct&gt;     &lt;fct&gt;             &lt;chr&gt;        &lt;dbl&gt;\n1 Substrate Fresh weight (g)  trt       0.813   \n2 Substrate Dry weight (g)    trt       0.381   \n3 Substrate Plant height (cm) trt       0.959   \n4 Substrate Flower number     trt       0.444   \n5 Water     Fresh weight (g)  trt       0.384   \n6 Water     Dry weight (g)    trt       0.843   \n7 Water     Plant height (cm) trt       0.000138\n8 Water     Flower number     trt       0.0600  \n\n\nDann schauen wir uns nochmal das \\(\\eta^2\\) an um zu sehen, wie viel Varianz unsere Behandlung in den Daten erklärt. Leider sieht es in unseren Daten sehr schlecht aus. Nur bei der Wassergruppe und der Pflanzenhöhe scheinen wir durch die Behandlung Varianz zu erklären.\n\nboxplot_mult_nest_model_tbl %&gt;%  \n  mutate(eta = map(model, eta_squared)) %&gt;% \n  unnest(eta) %&gt;% \n  clean_names() %&gt;% \n  select(type, plant_measure, eta2) \n\n# A tibble: 8 × 3\n# Groups:   type, plant_measure [8]\n  type      plant_measure        eta2\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Substrate Fresh weight (g)  0.0228 \n2 Substrate Dry weight (g)    0.102  \n3 Substrate Plant height (cm) 0.00466\n4 Substrate Flower number     0.0863 \n5 Water     Fresh weight (g)  0.101  \n6 Water     Dry weight (g)    0.0188 \n7 Water     Plant height (cm) 0.628  \n8 Water     Flower number     0.268  \n\n\nDann können wir auch schon den Gruppenvergleich mit dem R Paket {emmeans} rechnen. Wir nutzen hier die Option vcov. = sandwich::vcovHAC um heterogene Varianzen zuzulassen. Im Weiteren adjustieren wir nicht für die Anzahl der Vergleiche und lassen uns am Ende das compact letter display wiedergeben.\n\nemm_tbl &lt;- boxplot_mult_nest_model_tbl |&gt; \n  mutate(emm = map(model, emmeans, ~trt, vcov. = sandwich::vcovHAC)) |&gt; \n  mutate(cld = map(emm, cld, Letters = letters, adjust = \"none\")) |&gt; \n  unnest(cld) |&gt; \n  select(type, plant_measure, trt, rsp = emmean, group = .group) |&gt; \n  mutate(group = str_trim(group))\n\nemm_tbl\n\n# A tibble: 24 × 5\n# Groups:   type, plant_measure [8]\n   type      plant_measure     trt     rsp group\n   &lt;fct&gt;     &lt;fct&gt;             &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 Substrate Fresh weight (g)  UWF    95.4 a    \n 2 Substrate Fresh weight (g)  TWF   101   a    \n 3 Substrate Fresh weight (g)  Peat  105   a    \n 4 Substrate Dry weight (g)    UWF    12.3 a    \n 5 Substrate Dry weight (g)    Peat   14.3 ab   \n 6 Substrate Dry weight (g)    TWF    15.1 b    \n 7 Substrate Plant height (cm) TWF    37.9 a    \n 8 Substrate Plant height (cm) Peat   38.0 a    \n 9 Substrate Plant height (cm) UWF    39.1 a    \n10 Substrate Flower number     UWF    14.6 a    \n# ℹ 14 more rows\n\n\nDa die Ausgabe viel zu lang ist, wollen wir ja jetzt einmal unsere Abbildungen in {ggplot} nachbauen. Dazu nutze ich dann einmal zwei Wege. Einmal den etwas schnelleren mit facet_wrap() bei dem wir eigentlich alles automatisch machen lassen. Zum anderen zeige ich dir mit etwas Mehraufwand alle acht Abbildungen einzeln baust und dann über das R Paket {patchwork} wieder zusammenklebst. Der zweite Weg ist der Weg, wenn du mehr Kontrolle über die einzelnen Abbildungen haben willst.\n\nMit facet_wrap()Mit {patchwork}\n\n\nDie Funktion facet_wrap() erlaubt es dir automatisch Subplots nach einem oder mehreren Faktoren zu erstellen. Dabei muss auch ich immer wieder probieren, wie ich die Faktoren anordne. In unserem Fall wollen wir zwei Zeilen und auch den Subplots erlauben eigene \\(x\\)-Achsen und \\(y\\)-Achsen zu haben. Wenn du die Option scale = \"free\" nicht wählst, dann haben alle Plots alle Einteilungen der \\(x\\)_Achse und die \\(y\\)-Achse läuft von dem kleinsten zu größten Wert in den gesamten Daten.\n\nboxplot_mult_tbl |&gt; \n  ggplot(aes(trt, rsp)) +\n  theme_minimal() +\n  stat_boxplot(geom = \"errorbar\", width = 0.25) + \n  geom_boxplot(outlier.shape = 18, outlier.size = 2) +\n  stat_summary(fun.y = mean, geom = \"point\", shape = 23, size = 3, fill = \"red\") +\n  facet_wrap(~ type * plant_measure, nrow = 2, scales = \"free\") +\n  labs(x = \"\", y = \"\") +\n  geom_text(data = emm_tbl, aes(y = rsp, label = group), size = 3, fontface = \"bold\",\n            position = position_nudge(0.2), hjust = 0, vjust = 0, color = \"red\")\n\n\n\n\n\n\n\nAbbildung 16.18— Nachbau der Abbildung mit der Funktion facte_wrap() mit Boxplots und dem Mittelwert. Neben dem Mittelwert finden sich das compact letter display. Auf eine Einfärbung nach der Behandlung wurde verzichtet um die Abbildung nicht noch mehr zu überladen.\n\n\n\n\n\n\n\nJetzt wird es etwas wilder. Wir bauen uns jetzt alle acht Plots einzeln und kleben diese dann mit dem R Paket {patchwork} zusammen. Das hat ein paar Vorteile. Wir können jetzt jeden einzelnen Plot bearbeiten und anpassen wie wir es wollen. Damit wir aber nicht zu viel Redundanz haben bauen wir uns erstmal ein Template für ggplot(). Dann können wir immer noch die Werte für scale_y_continuous() in den einzelnen Plots ändern. Hier also einmal das Template, was beinhaltet was für alle Abbildungen gelten soll.\n\ngg_template &lt;- ggplot() +\n  aes(trt, rsp) +\n  theme_minimal() +\n  stat_boxplot(geom = \"errorbar\", width = 0.25) + \n  geom_boxplot(outlier.shape = 18, outlier.size = 2) +\n  stat_summary(fun.y = mean, geom = \"point\", shape = 23, size = 3, fill = \"red\") +\n  labs(x = \"\")\n\nJa, jetzt geht es los. Wir bauen also jeden Plot einmal nach. Dafür müssen wir dann jeweils den Datensatz filtern, den wir auch brauchen. Dann ergänzen wir noch die korrekte \\(y\\)-Achsenbeschriftung. So können wir dann auch händisch das compact letter display über die Whisker einfach setzen. Im Weiteren habe ich dann auch einmal als Beispiel noch die \\(y\\)-Achseneinteilung mit scale_y_continuous() geändert. Ich habe das einmal für den Plot p1 gemacht, der Rest würde analog dazu funktionieren.\n\np1 &lt;- gg_template %+%\n  filter(boxplot_mult_tbl, type == \"Substrate\" & plant_measure == \"Fresh weight (g)\") +\n  labs(y = \"Fresh weight (g)\") +\n  annotate(\"text\", x = c(1, 2, 3), y = c(170, 120, 135), label = c(\"a\", \"a\", \"a\"), \n           color = \"red\", size = 3, fontface = \"bold\") +\n  scale_y_continuous(breaks = seq(40, 180, 20), limits = c(60, 180))\np2 &lt;- gg_template %+%\n  filter(boxplot_mult_tbl, type == \"Substrate\" & plant_measure == \"Dry weight (g)\") +\n  labs(y = \"Dry weight (g)\")\np3 &lt;- gg_template %+%\n  filter(boxplot_mult_tbl, type == \"Substrate\" & plant_measure == \"Plant height (cm)\") +\n  labs(y = \"Plant height (cm)\")\np4 &lt;- gg_template %+%\n  filter(boxplot_mult_tbl, type == \"Substrate\" & plant_measure == \"Flower number\") +\n  labs(y = \"Flower number\")\np5 &lt;- gg_template %+%\n  filter(boxplot_mult_tbl, type == \"Water\" & plant_measure == \"Fresh weight (g)\") +\n  labs(y = \"Fresh weight (g)\")\np6 &lt;- gg_template %+%\n  filter(boxplot_mult_tbl, type == \"Water\" & plant_measure == \"Dry weight (g)\") +\n  labs(y = \"Dry weight (g)\")\np7 &lt;- gg_template %+%\n  filter(boxplot_mult_tbl, type == \"Water\" & plant_measure == \"Plant height (cm)\") +\n  labs(y = \"Plant height (g)\")\np8 &lt;- gg_template %+%\n  filter(boxplot_mult_tbl, type == \"Water\" & plant_measure == \"Flower number\") +\n  labs(y = \"Flower number\")\n\nDann kleben wir die Abbildungen einfach mit einem + zusammen und dann wählen wir noch aus, dass wir vier Spalten wollen. Dann können wir noch Buchstaben zu den Subplots hinzufügen und noch Titel und anderes wenn wir wollen würden. Dann haben wir auch den Plot schon nachgebaut. Ich verzichte hier händisch überall das compact letter display zu ergänzen. Das macht super viel Arbeit und endlos viel Code und hilft dir dann aber in deiner Abbildung auch nicht weiter. Du musst dann ja bei dir selber die Positionen für die Buchstaben finden.\n\npatch &lt;- p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 +\n  plot_layout(ncol = 4)\npatch + plot_annotation(tag_levels = 'A',\n                        title = 'Ein Zerforschenbeispiel an Boxplots',\n                        subtitle = 'Alle Plots wurden abgelesen und daher sagen Signifikanzen nichts.',\n                        caption = 'Disclaimer: Die Abbildungen sagen nichts aus.')\n\n\n\n\n\n\n\nAbbildung 16.19— Nachbau der Abbildung mit dem R Paket {patchwork} mit Boxplots und dem Mittelwert. Neben dem Mittelwert finden sich das compact letter display bei dem ersten Plot. Auf eine Einfärbung nach der Behanldung wurde verzichtet um die Abbildung nicht noch mehr zu überladen.\n\n\n\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_boxplot.png\", width = 12, height = 6)\n\n\n\n\n\n\n16.3.3 Histogramm\nWir nutzen für die Erstellung eines Histogramms den Datensatz dog_fleas_hist.csv. Wir brauchen für ein anständiges Histogramm, wo du auch was erkennen kannst, mindestens 20 Beobachtung. Am besten mehr noch mhr Beobachtungen. Deshalb schauen wir uns jetzt einmal 39 Hunde an und zählen wieviele Flöhe die Hunde jeweils haben, dargestellt in der Spalteflea_count. Darüber hinaus bestimmen wir auch noch das mittlere Gewicht der Flöhe auf dem jeweiligen Hund, dargestellt in der Spalte flea_weight.\n\ndog_fleas_hist_tbl &lt;- read_csv(\"data/dog_fleas_hist.csv\")\n\n\n\n\n\n\n\nFür die vollständige Datentabelle bitte aufklappen\n\n\n\n\n\n\n\n\n\nTabelle 16.2— Beispieldatensatz für die Anzahl an Flöhen auf 39 Hunden. Gezählt wurde die Anzahl an Flöhen flea_count und das gemittelte Gewicht der Flöhe flea_weight. Hier die ersten sieben Zeilen des Datensatzes.\n\n\n\n\n\n\nflea_count\nflea_weight\n\n\n\n\n0\n0.00\n\n\n1\n7.43\n\n\n4\n21.04\n\n\n2\n20.07\n\n\n1\n21.90\n\n\n0\n0.00\n\n\n2\n24.96\n\n\n1\n27.08\n\n\n5\n16.58\n\n\n1\n19.92\n\n\n0\n0.00\n\n\n0\n0.00\n\n\n2\n24.63\n\n\n4\n21.64\n\n\n3\n20.97\n\n\n1\n23.15\n\n\n0\n0.00\n\n\n3\n14.91\n\n\n1\n19.39\n\n\n2\n17.66\n\n\n1\n19.15\n\n\n1\n25.10\n\n\n2\n26.38\n\n\n2\n19.33\n\n\n2\n13.29\n\n\n1\n17.81\n\n\n0\n0.00\n\n\n2\n23.56\n\n\n1\n18.64\n\n\n1\n15.64\n\n\n3\n19.88\n\n\n1\n18.40\n\n\n1\n25.17\n\n\n0\n0.00\n\n\n0\n0.00\n\n\n\n\n\n\n\n\n\n\n\nDie Tabelle 16.2 zeigt den Datensatz dog_fleas_hist.csv. Du musst oben einmal den blauen Kasten aufklappen, um die vollständige Datentabelle zu sehen. Wir wollen jetzt die Variable flea_count und flea_weight jeweils abbilden. Wir beginnen mit der diskreten Variable flea_count. Im Gegensatz zu der Variable flea_weight haben wir bei der Anzahl gleiche Zahlen vorliegen, die wir dann zusammen darstellen können. Abbildung 16.20 zeigt die Darstellung der Tabelle. Auf der x-Achse ist die Anzahl an Flöhen dargestellt. Auf der y-Achse die Anzahl der jeweiligen Anzahl an Flöhen. Das klingt jetzt etwas schief, aber schauen wir uns die Abbilung näher an.\n\n\n\n\n\n\n\n\nAbbildung 16.20— Die Anzahl von Flöhen auf 39 Hunden. Jeder Punkt entspricht einem Hund und der entsprechenden Anzahl an Flöhen auf dem Hund.\n\n\n\n\n\nWir sehen in Abbildung 16.20 das acht Hunde keine Flöhe hatten - also eine Anzahl an Flöhen von 0. Auf der anderen Seite hatten zwei Hunde vier Flöhe und ein Hund hatte sogar fünf Flöhe. Wir sehen also die Verteilung der Anzahl an Flöhen über alle unsere 39 Hundebeobachtungen.\nWir schauen uns aber die Verteilung der Anzahl an Flöhen meist nicht in der Form von gestapelten Punkten an, sondern in der Form eines Histogramms also einem Balkendiagramm. Abbildung 16.21 zeigt das Histogramm für die Anzahl der Flöhe.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_count)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Anzahl Flöhe\", y = \"Anzahl\") \n\n\n\n\n\n\n\nAbbildung 16.21— Histogramm der Anzahl von Flöhen auf 39 Hunden.\n\n\n\n\n\nWas sehen wir in der Abbildung 16.21? Anstatt von gestapelten Punkten sehen wir jetzt Balken, die die jeweilige Anzahl an Flöhen zusammenfassen. Der Unterschied ist bei einer diskreten Variable wie der Anzahl (eng. count) relativ gering.\nAnders sieht es für kontenuierliche Variablen mit Kommazahlen aus. Schauen wir uns das Gewicht der Flöhe an, so sehen wir, dass es sehr viele Zahlen gibt, die nur einmal vorkomen. Abbildung 16.22 zeigt das Histogramm für das Geicht der Flöhe.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_weight)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Gewicht [mg]\", y = \"Anzahl\") \n\n\n\n\n\n\n\nAbbildung 16.22— Histogramm des Gewichts von Flöhen auf 39 Hunden.\n\n\n\n\n\nWie entsteht nun ein Hisotgramm für konetnierliche Zahlen? Schauen wir uns dafür einmal ein kleineres Datenbeispiel an, in dem wir nur Flöhe mit einem Gewicht größer als 11 und kleiner als 19 wäheln. Wir nutzen dazu die Funktion filter(flea_weight &gt; 11 & flea_weight &lt; 19). Wir erhalten folgende Zahlen und das entsprechende Histogramm.\n\n\n[1] 13.29 14.91 15.64 16.58 17.66 17.81 18.40 18.64\n\n\n\n\n\n\n\n\nAbbildung 16.23— Zusammenhang zwischen den einzelnen Beobachtungen und der Höhe der einzelnen Balken am Beispiel von acht Hunden.\n\n\n\n\n\nAbbildung 16.23 zeigt das Histogramm der reduzierten Daten. Die roten vertikalen Linien zeigen die Position der einzelnen Flohgewichte auf der x-Achse. Die blauen Hilfslinien machen nochmal klarer, wie hoch die einzelnen Balken sind sowie welche Beobachtungen auf der x-Achse in den jeweiligen Balken mit eingehen. Wir sehen, dass wir einen Hund mit Flöhen haben, die zwischen 12.5 und 13.5 wiegen - der entsprechende Balken erhält die Anzahl von eins. Auf der anderen Seite sehen wir, dass es drei Hunde mit Flöhen, die zwischen 17.5 und 18.5 wiegen. Daher wächst der Balken auf eine Anzahl von drei.\nWir können mit der Option binwidth in dem geom_histogram() einstellen, wie breit auf der x-Achse die jeweiligen Balken sein sollen. Hier empfiehlt es sich verschiedene Zahlen für binwidthauszuprobieren.\n\n\n16.3.4 Density Plot\nEine weitere Möglichkeit sich eine Verteilung anzuschauen, ist die Daten nicht als Balkendiagramm sondern als Densityplot - also Dichteverteilung - anzusehen. Im Prinzip verwandeln wir die Balken in eine Kurve. Damit würden wir im Prinzip unterschiedliche Balkenhöhen ausgleichen und eine “glattere” Darstellung erreichen. Wir wir aber gleich sehen werden, benötigen wir dazu eine Menge an Beobachtungen und auch dann ist das Ergebnis eventuell nicht gut zu interpretieren.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_count)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Anzahl Flöhe\", y = \"Anzahl\")\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_count)) +\n  geom_density(fill = \"gray\", color = \"black\") +\n  theme_minimal() +\n  labs(x = \"Anzahl Flöhe\", y = \"Häufigkeit\") \n\n\n\n\n\n\n\n\n\n\n\n(a) Histogramm\n\n\n\n\n\n\n\n\n\n\n\n(b) Densityplot\n\n\n\n\n\n\n\nAbbildung 16.24— Zusammenhang von Histogramm und Densityplot an der Anzahl der Flöhe auf 39 Hunden.\n\n\n\n\nAbbildung 16.24 zeigt auf der linken Seite erneut die Abbildung des Histogramms als Balkendiagramm für die Anzahl der Flöhe auf den 39 Hunden. Auf der rechten Seite die entsprechenden gleichen Daten als Denistyplot. Klar ist die Wellenbewegung des Densityplots zu erkennen. Hier liegen zu wenige Beobachtungen und Kategorien auf der x-Achse vor, so dass der Densityplot nicht zu empfehlen ist.\n\n\n16.3.5 Scatterplot\nDer Scatterplot wird auch \\(xy\\)-Plot genannt. Wir stellen in einem Scatterplot zwei kontinuierliche Variablen dar. Wir haben also auf der \\(x\\)-Achse Zahlen genauso wie auf der \\(y\\)-Achse. Das Ziel eines Scatterplots ist es meist eine Abhängigkeit zwischen den Werten auf der \\(y\\)-Achse und der \\(x\\)-Achse darzustellen. Wenn sich die Werte auf der \\(x\\)-Achse ändern, wie ändern sich dann die Werte auf der \\(y\\)-Achse? Um diesen Zusammenhang zwischen \\(y\\) und \\(x\\) zu visualisieren legen wir eine Linie durch die Punkte. Im Prinzip fragen wir uns, wie hänge die Werte auf der \\(y\\)-Achse von den Werten auf der \\(x\\)-Achse ab? Wenn sich also die Werte auf der \\(x\\)-Achse erhöhen oder kleiner werden, wie verhalten sich dann die Werte auf der \\(y\\)-Achse? In der folgenden Abbildung siehst du einmal den Zusammenhang zwischen der Sprungweite und dem Gewicht von Flöhen aufgetrennt für die beiden Tierarten.\n\nggplot(data = flea_dog_cat_tbl, aes(x = weight, y = jump_length, \n                                    color = animal)) +\n  theme_minimal() +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Gewicht der Flöhe [mg]\", y = \"Sprungweite in [cm]\",\n       color = \"Tierart\") \n\n\n\n\n\n\n\nAbbildung 16.25— Zusammenhang zwischen der Sprungweite in [cm] und Gewicht der Flöhe. Jeder Punkt stellt eine Beobachtung dar.\n\n\n\n\n\nDie Abbildung 16.25 zeigt den Scatterplot für die Spalte weight auf der \\(x\\)-Achse und jump_length auf der \\(y\\)-Achse. Mit der Funktion geom_point() können wir die Punktepaare für jede Beobachtung zeichnen. Wir trennen dann noch die Beobachtungen nach den beiden Tierarten durch die Option fill = animal auf. In unserem Fall zeichnen wir mit der Funktion stat_smooth() noch die entsprechende Grade durch die Punkte. Es handelt sich hierbei um eine simple lineare Regression, da wir eine Gerade durch die Punktewolke zeichnen. Wir du erkennen kannst, hat das Gewicht der Katzenflöhe einen Einfluss auf die Sprungweite. Bei Hundeflöhen ist es egal, wie schwer ein Floh ist, die Flöhe springen immer gleich weit.\nIm Folgenden siehst du einmal eine Geradengleichung mathematisch und die Repräsentation in R. Wir wir methodisch zu den Zahlen kommen, kannst du dann später in dem Kapitel zur linearen Regression nachlesen.\n\nMathematikR (Built-in)\n\n\nDie Geradengleichung aus einer simplen linearen Regression.\n\\[\njump\\_length = 3.314 + 1.289 \\cdot weight\n\\]\n\n\nWir bauen uns hier eine Funktion in R, die die Geradengleichung repräsentiert.\n\njump_func &lt;- \\(x){3.314 + 1.289 * x}\n\n\n\n\nWir können jetzt die Geradengleichung einmal über die Funktion geom_function() zu der Abbildung ergänzen. Ich habe hier einmal die Geradengleichung über die beiden Tierarten zusammengenommen. Später können wir dann auch die beiden Tierarten trennen.\n\nggplot(data = flea_dog_cat_tbl, aes(x = weight, y = jump_length)) +\n  theme_minimal() +\n  geom_point() +\n  geom_function(fun = jump_func, color = \"blue\") +\n  labs(x = \"Gewicht der Flöhe [mg]\", y = \"Sprungweite in [cm]\",\n       color = \"Tierart\") \n\n\n\n\n\n\n\nAbbildung 16.26— Zusammenhang zwischen der Sprungweite in [cm] und Gewicht der Flöhe. Jeder Punkt stellt eine Beobachtung dar. Eine eigene Geradengleichung wurde durch geom_function() ergänzt.\n\n\n\n\n\nWenn du mehr über die Regression lernen willst und wie sich weitere Abbildungen zusammensetzen, dann schaue doch einmal hier in den Zerforschenkasten oder aber ans Ende des Kapitels. Dort findest du dann noch mehr Beispiele, wie du eine Regressionsgerade erstellen kannst.\n\n\n\n\n\n\nZerforschen: Simple lineare Regression mit Bestimmtheitsmaß \\(R^2\\)\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns eine simple lineare Regression in einem Scatterplot anschauen. Das schöne an dem Beispiel ist, dass wir hier zum einen noch einen Pfeil einfügen wollen und dass wir nur einen Bereich darstellen wollen. Die Gerade fürht also nicht durch den gesamten Plot. Sonst haben wir dann noch die Herausforderung, dass wir einmal die Geradengleichung zusammen mit dem Bestimmtheitsmaß \\(R^2\\) ergänzen müssen.\n\n\n\n\n\n\nAbbildung 16.27— Ursprüngliche Abbildung, die nachgebaut werden soll. Eine simple lineare Regression mit Bestimmtheitsmaß \\(R^2\\) für die Gerade durch die Punkte.\n\n\n\nAuch hier mache ich es mir etwas einfacher und erschaffe mir die Daten dann direkt in R. Ich baue mir also einen tibble() mit den ungefähren Werten aus der Abbildung. Das war ein wenig aufwendig, aber nach einigem Hin und Her passte dann auch die Werte zu der Abbildung.\n\nshoot_n_tbl &lt;- tibble(n_in_shoot = c(2.5, 3.5, 3.9, 4.1, 4.2, 4.3 ,4.5, 4.7, 5.1, 5.1, 5.8),\n                      freshmass = c(7.5, 9, 12.5, 11, 18, 16, 12, 17, 16, 20, 21))\n\nDann rechnen wir auch schon die lineare Regression mit der Funktion lm() die uns dann die Koeffizienten der Geraden wiedergibt.\n\nshoot_n_fit &lt;- lm(freshmass ~ n_in_shoot, data = shoot_n_tbl)\n\nDann einmal die Koeffizienten mit der Funktion coef() ausgelesen. Den Rest brauchen wir hier nicht für die Abbildung.\n\nshoot_n_fit |&gt; coef()\n\n(Intercept)  n_in_shoot \n  -4.333333    4.353599 \n\n\nAus den Koeffizienten baue ich mir dann auch die Geradengleichung einmal in R. Ich kann dann die Gleichung mit der Funktion geom_function() gleich in meine nachgebaute Abbildung ergänzen.\n\nshoot_n_func &lt;- \\(x){4.35 * x - 4.33}\n\nIch berechne dann einmal das Bestimmtheitsmaß \\(R^2\\) mit der Funktion r2() aus dem R Paket {performance}. Wir sehen, ich habe die Werte recht gut abgelesen, dass passt ganz gut mit dem Bestimmtheitsmaß \\(R^2\\).\n\nshoot_n_fit |&gt; r2()\n\n# R2 for Linear Regression\n       R2: 0.750\n  adj. R2: 0.722\n\n\nUnd dann bauen wir uns schon die Abbildung 16.28 einmal nach. Der etwas aufwendigere Teil sind die Achsenbeschriftungen sowie die Ergänzung des Pfeils und der Beschriftung. Das verbraucht dann immer etwas mehr Code und Platz. Ich habe dann etwas die Achseneinteilung geändert und stelle nicht den ganzen Bereich dar. Es reicht auch vollkommen nur den Bereich zu visualisieren, der von Interesse ist. Daher beginnt dann meine \\(x\\)-Achse auch bei Zwei und nicht bei Null.\n\nshoot_n_tbl |&gt; \n  ggplot(aes(n_in_shoot, freshmass)) +\n  theme_minimal() +\n  geom_point(size = 4, color = \"red4\") +\n  geom_function(fun = shoot_n_func, color = \"black\", size = 1, xlim = c(2.5, 6)) + \n  labs(x = \"N in shoot [% DM]\", y = TeX(r\"(Shoot fresh mass \\[g plant$^{-1}$\\] - 17 DAP)\")) +\n  scale_x_continuous(breaks = c(2, 3, 4, 5, 6), limits = c(2, 6)) +\n  scale_y_continuous(breaks = c(0, 5, 10, 15, 20, 25), limits = c(0, 25)) +\n  annotate(\"text\", x = 2.5, y = 22, hjust = \"left\", color = \"black\", size = 4, \n           label = TeX(r\"($y = 4.35 \\cdot x - 4.33$)\")) +\n  annotate(\"text\", x = 2.5, y = 20, hjust = \"left\", color = \"black\", size = 4, \n           label = TeX(r\"($R^2 = 0.75$)\")) +\n  geom_curve(x = 3.5, y = 5.1, xend = 2.55, yend = 7,\n             arrow = arrow(length = unit(0.03, \"npc\"), type = \"closed\"),\n             curvature = -0.5) +\n  annotate(\"text\", x = 3.6, y = 4.5, label = \"Trt. with\\n50% cattail\",\n           hjust = \"left\", size = 4)\n\n\n\n\n\n\n\nAbbildung 16.28— Visualisierung der simplen linearen Regression mit einem Pfeil sowie den Informationen zu der Geradengleichung und dem Bestimmtheitsmaß \\(R^2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZerforschen: Zwei simple lineare Regression nebeneinander\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns eine simple lineare Regression in einem Scatterplot anschauen. Das stimmt nicht so ganz, den die Schwierigkeit liegt darin, dass es sich um zwei Scatterplots handelt. Klar, du kannst die beiden Abbildungen einfach getrennt erstellen und dann wäre gut. Ich zeige dir dann aber noch zwei weitere Möglichkeiten. Daher fangen wir mit der folgenden Abbildung einmal an. Wir haben hier zwei Scatterplots mit jeweils einer linearen Regression, dargestellt durch eine Gerade mit Regressionsgleichung, vorliegen. Hier brauchen wir dann mal ein paar mehr Zahlen, die ich mir dann aber so grob aus der Abbildung abgeleitet habe.\n\n\n\n\n\n\nAbbildung 16.29— Ursprüngliche Abbildung, die nachgebaut werden soll. Zwei lineare Regressionen mit den jeweiligen Regressionsgleichungen.\n\n\n\nWir laden als erstes wieder den Datensatz, den ich mir aus der obigen Abbildung erstellt habe. Wie immer beim Zerforschen habe ich nicht so genau drauf geachtet nur das die Zahlen so grob stimmen. Die Erstellung der Daten kann hier recht langwierig sein, aber hier geht es ja mehr um die Nutzung von ggplot. Also mach dir keinen Gedanken, wenn die Punkte nicht so perfekt passen.\n\nregression_tbl &lt;- read_excel(\"data/zerforschen_regression_linear.xlsx\") |&gt; \n  mutate(type = factor(type, labels = c(\"Basil\", \"Oregano\")))\nregression_tbl \n\n# A tibble: 40 × 3\n   type  washed unwashed\n   &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Basil      0        0\n 2 Basil     10       15\n 3 Basil     20       18\n 4 Basil     30       32\n 5 Basil     40       36\n 6 Basil     50       52\n 7 Basil     60       59\n 8 Basil     70       72\n 9 Basil     80       85\n10 Basil    100      105\n# ℹ 30 more rows\n\n\nDen folgenden Teil kannst du überspringen, wenn es dir um die Abbildung geht. Ich möchte in den zwei folgenden Tabs einmal die simple lineare Regression für die Abbildung mit dem Basilikum und einmal für das Oregano rechnen.\n\nLineare Regression für BasilikumLineare Regression für Oregano\n\n\nWir erstellen uns einmal eine simple lineare Regression mit der Funktion lm(). Mehr zu dem Thema und die Maßzahlen der Güte einer linearen Regression wie das Bestimmtheitsmaß \\(R^2\\) findest du im Kapitel zur simplen linearen Regression. Deshalb hier nur die Durchführung und nicht mehr.\n\nfit &lt;- lm(unwashed ~ washed, data = filter(regression_tbl, type == \"Basil\"))\n\nfit |&gt; \n  parameters::model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |        2.10\nwashed      |        1.00\n\nperformance::r2(fit)\n\n# R2 for Linear Regression\n       R2: 0.994\n  adj. R2: 0.994\n\n\nWir nutzen jetzt gleich die Koeffizienten aus der linearen Regression für die Erstellung der Geradengleichung.\n\n\nAuch hier gilt wie im anderen Tab, dass wir uns einmal eine simple lineare Regression mit der Funktion lm() erstellen. Mehr zu dem Thema und die Maßzahlen der Güte einer linearen Regression wie das Bestimmtheitsmaß \\(R^2\\) findest du im Kapitel zur simplen linearen Regression. Deshalb hier nur die Durchführung und nicht mehr.\n\nfit &lt;- lm(unwashed ~ washed, data = filter(regression_tbl, type == \"Oregano\"))\n\nfit |&gt; \n  parameters::model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |        8.17\nwashed      |        0.99\n\nperformance::r2(fit)\n\n# R2 for Linear Regression\n       R2: 0.997\n  adj. R2: 0.996\n\n\nWir nutzen jetzt gleich die Koeffizienten aus der linearen Regression für die Erstellung der Geradengleichung.\n\n\n\nSoweit so gut. In den beiden obigen Tabs haben wir jetzt die Koeffizienten der Regressionsgleichung berechnet. Wir kriegen also aus der Funktion lm() die Steigung und den y-Achsenabschnitt (eng. Intercept). Damit können wir uns dann die beiden Funktionen für die Gerade der Basilikumdaten und der Oreganodaten bauen. Wir werden dann in ggplot mit der Funktion geom_function() die entsprechenden Gerade zeichnen.\n\nbasil_func &lt;- \\(x){2.10 + 1.00 * x}\noregano_func &lt;- \\(x){8.17 + 0.99 * x}\n\nDu hast jetzt im Folgenden die Wahl zwischen drei Lösungen des Problems. Jede dieser Lösungen ist vollkommen in Ordnung und ich zeige dir hier nur die Möglichkeiten. Nimm einfach die Lösung, die dir am besten gefällt und passt. Was machen wir nun? Wir stellen einmal die beiden Abbildungen getrennt voneinander dar. Im Weiteren nutzen wir einmal die Funktion facet_wrap() um nach einem Faktor die Abbildungen aufzutrennen. Am Ende nutzen wir noch das R Paket patchwork um aus zwei Abbildungen dann eine schön annotierte Abbildung zu machen.\n\nZwei AbbildungenMit facet_wrap()Mit patchwork\n\n\nDer Kern der Abbildung 57.2 und Abbildung 57.3 ist die Funktion filter(). Wir bauen uns sozusagen zweimal einen Datensatz und leiten dann den Datensatz in die Funktion ggplot() weiter. Der Trick ist eigentlich, dass wir große Teile des Codes kopieren und dann für das Oregano wieder verwenden. Wenn du dir beide Chunks mal näher anschaust, wirst du große Änlichkeiten sehen. Im Prinzip musst du nur aufpassen, dass du jeweils die richtigen Geradenfunktionen einsetzt.\n\nfilter(regression_tbl, type == \"Basil\") |&gt; \n  ggplot(aes(x = washed, y = unwashed, color = type)) +\n  theme_minimal() +\n  geom_function(fun = basil_func, color = cbbPalette[2], linetype = 'dashed') + \n  geom_point(color = cbbPalette[2]) +\n  scale_x_continuous(name = TeX(r\"(Iodine content in \\textbf{unwashed} herbs $[\\mu g\\, l \\, (100 g\\, FM)^{-1}]$)\"), \n                     breaks = seq(0, 600, 150)) +\n  scale_y_continuous(name = TeX(r\"(Iodine content in \\textbf{washed} herbs $[\\mu g\\, l \\, (100 g\\, FM)^{-1}]$)\"), \n                     breaks = seq(0, 600, 150)) + \n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 150, y = 100, hjust = \"left\", color = cbbPalette[2],  \n           label = TeX(r\"($y = 2.10 + 1.00 \\cdot x;\\; R^2 = 0.99$)\")) \n\n\n\n\n\n\n\nAbbildung 16.30— Einmal die einfache Abbildung der linearen Regression in ggplot für Basilikum nachgebaut. Beachte die Funktion filter(), die den jeweiligen Datensatz für die beiden Kräuter erzeugt.\n\n\n\n\n\nUnd nochmal die simple Regression in dem Scatterplot für das Oregano. Bitte beachte einmal die Beschreibungen im Code und du wirst sehen, dass hier sehr viel gleich zum obigen Codeblock ist. In dem Tab zum R Paket patchwork zeige ich dir dann noch die Möglichkeit ein Template zu erstellen und dann einiges an Zeilen an Code zu sparen. Aber es geht auch so.\n\nfilter(regression_tbl, type == \"Oregano\") |&gt; \n  ggplot(aes(x = washed, y = unwashed, color = type)) +\n  theme_minimal() +\n  geom_function(fun = oregano_func, color = cbbPalette[3], linetype = 'dashed') +\n  geom_point(color = cbbPalette[3]) +\n  scale_x_continuous(name = TeX(r\"(Iodine content in \\textbf{unwashed} herbs $[\\mu g\\, l \\, (100 g\\, FM)^{-1}]$)\"),\n                     breaks = seq(0, 900, 150)) + \n  scale_y_continuous(name = TeX(r\"(Iodine content in \\textbf{washed} herbs $[\\mu g\\, l \\, (100 g\\, FM)^{-1}]$)\"), \n                     breaks = seq(0, 900, 150)) +\n  theme(legend.position = \"none\") +\n  annotate(\"text\", x = 150, y = 100, hjust = \"left\", color = cbbPalette[3],  \n           label = TeX(r\"($y = 8.17 + 0.99 \\cdot x;\\; R^2 = 0.99$)\")) \n\n\n\n\n\n\n\nAbbildung 16.31— Einmal die einfache Abbildung der linearen Regression in ggplot für Oregano nachgebaut. Beachte die Funktion filter(), die den jeweiligen Datensatz für die beiden Kräuter erzeugt.\n\n\n\n\n\n\n\nHier brauchen wir jetzt das R Paket grid damit wir am Anschluss noch unsere Abbildungen mit den Gleichungen beschriften können. Die Idee ist eigentlich recht simple. Wir haben den Faktor type und nutzen die Funktion facet_wrap() um nach diesem Faktor zwei Abbildungen zu bauen. Unser Faktor hat zwei Level Basilikum und Oregano und deshalb erhalten wir auch zwei Subbplots. Wir können dann auch entscheiden, wie die Abbildungen angeordnet werden sollen, aber da bitte einmal bei Hilfeseite von facet_wrap(). Sonst sit alles gleich wie im ersten Tab. Also bitte nochmal da schauen.\n\nggplot(data = regression_tbl, aes(x = washed, y = unwashed,\n                                  color = type)) +\n  theme_minimal() +\n  scale_color_okabeito() +\n  geom_function(data = filter(regression_tbl, type == \"Basil\"),\n                fun = basil_func, color = cbbPalette[2], linetype = 'dashed') + \n  geom_function(data = filter(regression_tbl, type == \"Oregano\"),\n                fun = oregano_func, color = cbbPalette[3], linetype = 'dashed') + \n  geom_point() +\n  facet_wrap(~ type) + \n  scale_x_continuous(name = TeX(r\"(Iodine content in \\textbf{unwashed} herbs $[\\mu g\\, l \\, (100 g\\, FM)^{-1}]$)\"),\n                     breaks = seq(0, 900, 150)) +\n  scale_y_continuous(name = TeX(r\"(Iodine content in \\textbf{washed} herbs $[\\mu g\\, l \\, (100 g\\, FM)^{-1}]$)\"),\n                     breaks = seq(0, 900, 150)) +\n  theme(legend.position = \"none\") \n \ngrid::grid.text(TeX(r\"($y = 2.10 + 1.00 \\cdot x;\\; R^2 = 0.99$)\"),  \n                x = 0.2, y = 0.2, just = \"left\", gp = grid::gpar(col = cbbPalette[2])) \ngrid::grid.text(TeX(r\"($y = 8.17 + 0.99 \\cdot x;\\; R^2 = 0.99$)\"),  \n                x = 0.65, y = 0.2, just = \"left\", gp = grid::gpar(col = cbbPalette[3]))\n\n\n\n\n\n\n\nAbbildung 16.32— Einmal die einfache Abbildung der linearen Regression in ggplot für Oregano nachgebaut. Beachte die Funktion facet_wrap(), die den jeweiligen Datensatz für die beiden Kräuter erzeugt.\n\n\n\n\n\nUnd dann sind wir auch schon fertig. Gut, das ist jetzt mit der Regressionsgleichung etwas fricklig, aber das ist es meistens, wenn du viel auf einmal darstellen willst. Vielleicht ist dann noch die Idee mit dem R Paket patchwork im nächsten Tab hilfreich.\n\n\nJetzt drehen wir nochmal frei und holen alles raus was geht. Wir nutzen zum einen das R Paket patchwork um zwei Abbildungen miteinander zu verbinden. Prinzipiell geht das auch mit dem R Paket grid und der Funktion grid.arrange(), aber dann wird das hier sehr voll. Wir nutzen am Ende nur eine Funktion aus dem Paket grid um wiederum die \\(x\\)-Achse schön hinzukriegen. Als erstes wollen wir uns aber ein Template in ggplot bauen, dass wir dann mit einem neuen Datensatz durch den Operator %+% mit einem neuen Datensatz versehen können.\nIm Folgenden stecken wir den ganzen Datensatz in eine ggplot()-Funktion. Später wählen wir dann mit filter() die beiden Kräuterdatensätze aus. Wir definieren in dem Template alles, was wir auch für die beiden Abbildungen brauchen würden. Das spart dann etwas an Zeilen Code. Manchmal dann aber auch nicht ganz so viel, denn wir müssen für die einzelnen Datensätze dann doch noch einiges anpassen.\n\np_template &lt;- ggplot(regression_tbl, aes(x = washed, y = unwashed,\n                                         color = type)) +\n  theme_minimal() +\n  geom_point() +\n  scale_x_continuous(name = \"\",\n                     breaks = seq(0, 900, 150), limits = c(0, 900)) +\n  scale_y_continuous(name = TeX(r\"(\\textbf{Washed} herbs $[\\mu g\\, l \\, (100 g\\, FM)^{-1}]$)\"),\n                     breaks = seq(0, 900, 150), limits = c(0, 900)) +\n  theme(legend.position = \"none\")\n\nWir nutzen jetzt das p_template und ergänzen den gefilterten Datensatz für das Basilikum mit dem Operator %+%. Dann wählen wir noch die passende Farbe über die Option order = 1 aus und ergänzen die Geradengleichung sowie den Titel für die Abbildung.\n\np_basil &lt;- p_template %+%\n  filter(regression_tbl, type == \"Basil\") +\n  scale_color_okabeito(order = 1) +\n  geom_function(fun = basil_func, color = cbbPalette[2], \n                linetype = 'dashed') +\n  annotate(\"text\", x = 150, y = 100, hjust = \"left\", color = cbbPalette[2], \n           label = TeX(r\"($y = 2.10 + 1.00 \\cdot x;\\; R^2 = 0.99$)\")) +\n  ggtitle(\"Basil\")\n\nDas Ganze dann nochmal für das Oregano, aber hier entfernen wir die \\(y\\)-Achse. Wir brauchen nur eine auf der linken Seite. Das ist auch der Grund warum wir keine \\(x\\)-Achse benannte haben, dass machen wir dann über die beiden Plots zusammen ganz am Ende. Auch hier ergänzen wir dann die Gweradengleichung sowie den Titel der Abbildung.\n\np_oregano &lt;- p_template %+%\n  filter(regression_tbl, type == \"Oregano\") +\n  scale_color_okabeito(order = 2) +\n  geom_function(fun = oregano_func, color = cbbPalette[3], \n                linetype = 'dashed') +\n  theme(axis.title.y = element_blank()) +\n  annotate(\"text\", x = 150, y = 100, hjust = \"left\", color = cbbPalette[3], \n           label = TeX(r\"($y = 8.17 + 0.99 \\cdot x;\\; R^2 = 0.99$)\")) +\n  ggtitle(\"Oregano\")\n\nJetzt geht es los mit dem Zusammenbauen. Wir können dazu einfach das + nutzen. Wenn du mehr wissen willst, was du noch ändern kannst, dann schaue einmal das Tutorium Adding Annotation and Style für das R Paket patchworkan. Da kannst du dann auch die Schriftgröße und weiteres ändern. Wir müssen dann ganz am Ende nochmal mit der Funktion grid.draw() die gemeinsame \\(x\\)-Achse ergänzen. Am Ende habe ich noch die Achsenbeschriftungen gekürzt und die Informationen in den Titel mit der Funktion plot_annotation geschoben. Dann habe ich noch die Subplots mit einem Buchstaben versehen. Und dann sind wir auch schon fertig.\n\np_basil + p_oregano +\n  plot_annotation(title = 'Iodine content in herbs',\n                  subtitle = 'The iodine content is measured in washed and unwashed herbs',\n                  caption = 'Disclaimer: The measurement has been done in freshmatter',\n                  tag_levels = 'A')\ngrid::grid.draw(grid::textGrob(TeX(r\"(\\textbf{Unwashed} herbs $[\\mu g\\, l \\, (100 g\\, FM)^{-1}]$)\"), \n                               y = 0.07))\n\n\n\n\n\n\n\nAbbildung 16.33— Einmal die einfache Abbildung der linearen Regression in ggplot nachgebaut. Die Abbildung A zeigt die Punkte und die Geradengleichung für das Basilikum. Die Abbildung B die entsprechenden Informationen für das Oregano. Die beiden Achsenbeschriftungen wurden gekürzt und die Informationen in den Titel übernommen.\n\n\n\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_simple_regression.png\", width = 5, height = 3)\n\n\n\n\n\n\n16.3.6 Dotplot, Beeswarm und Raincloud Plot\nWenn wir weniger als fünf Beobachtungen haben, dann ist meist ein Boxplot verzerrend. Wir sehen eine Box und glauben, dass wir viele Datenpunkte vorliegen haben. Bei 3 bis 7 Beobachtungen je Gruppe bietet sich der Dotplot als eine Lösung an. Wir stellen hier alle Beobachtungen als einzelne Punkte dar. Wie erstellen wir nun einen Dotplot in R? Wir nutzen dazu die Funktion geom_dotplot() wie folgt.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = grade,\n                                    fill = animal)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  theme_minimal() +\n  labs(x = \"Tierart\", y = \"Boniturnote [1-9]\") \n\n\n\n\n\n\n\nAbbildung 16.34— Der Dotplot für die Anzahl der Flöhe für die beiden Tierarten Hund und Katze.\n\n\n\n\n\nIn Abbildung 16.34 sehen wir den Dotplot aus der Datei flea_dog_cat.xlsx. Auf der x-Achse sind die Level des Faktors animal dargestellt und auf der y-Achse die Notenbewertung grade der einzelnen Hunde und Katzen. Die Funktion geom_dotplot() erschafft das Layer für die Dots bzw. Punkte. Wir müssen in der Funktion noch zwei Dinge angeben, damit der Plot so aussieht, dass wir den Dotplot gut interpretieren können. Zum einen müssen wir die Option binaxis = y wählen, damit die Punkte horizontal geordnet werden. Zum anderen wollen wir auch, dass die Punkte zentriert sind und nutzen dafür die Option stackdir = center.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = grade,\n                            fill = animal)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  stat_summary(fun = median, fun.min = median, fun.max = median,\n               geom = \"crossbar\", width = 0.5) +\n  theme_minimal() +\n  labs(x = \"Tierart\", y = \"Boniturnote [1-9]\") \n\n\n\n\n\n\n\nAbbildung 16.35— Der Dotplot für die Anzahl der Flöhe für die beiden Tierarten Hund und Katze. Die schwarze Linie stelt den Median für die beiden Tierarten dar.\n\n\n\n\n\nNun macht es wenig Sinn bei sehr wenigen Beobachtungen noch statistische Maßzahlen mit in den Plot zu zeichnen. Sonst hätten wir auch gleich einen Boxplot als Visualisierung der Daten wählen können. In Abbildung 16.35 sehen wir die Ergänzung des Medians. Hier müssen wir etwas mehr angeben, aber immerhin haben wir so eine Idee, wo die “meisten” Beobachtungen wären. Aber auch hier ist Vorsicht geboten. Wir haben sehr wenige Beobachtungen, so dass eine Beobachtung mehr oder weniger große Auswirkungen auf den Median und die Interpretation hat.\nDann möchte ich hier den Beeswarm als eine Alternative zu dem Dotplot vorstellen. Insbesondere wenn du sehr viele Beobachtungen hast, dann hat der Beeswarm bessere Eigenschaften als der Dotplot. Es gibt hier auch die tolle Hilfeseite zu Beeswarm plot in ggplot2 with geom_beeswarm() und natürlich noch die Möglichkeit ein Violin Plot zu ergänzen. Auch hier dann mal bei der Hilfeseite Violin plot with data points in ggplot2 schauen. In Abbildung 16.36 siehst du dann einmal das Alter und die Körpergröße für die beiden Geschlechter in den Gummibärchendaten aufgeteilt.\n\nggplot(data = gummi_tbl, aes(x = gender, y = age,\n                             color = gender)) +\n  geom_beeswarm() +\n  theme_minimal() +\n  labs(x = \"Geschlecht\", y = \"Alter in Jahren\") +\n  theme(legend.position = \"none\")\n\nggplot(data = gummi_tbl, aes(x = gender, y = height,\n                             color = gender)) +\n  geom_beeswarm() +\n  theme_minimal() +\n  labs(x = \"Geschlecht\", y = \"Körpergröße in cm\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Alter nach Geschlecht\n\n\n\n\n\n\n\n\n\n\n\n(b) Körpergröße nach Geschlecht\n\n\n\n\n\n\n\nAbbildung 16.36— Der Beeswarm ist ein Dotplot für eine große Anzahl an Beobachtungen. Hier schauen wir uns einmal das Alter und die Körpergröße aufgeteilt nach Geschlecht an.\n\n\n\n\nUnd dann bringen wir in der Abbildung 16.37 mal verschiedene Abbildungen zusammen mit dem R Paket{gghalves}. Wir können mit {gghalves} halbe Plots erstellen und diese dann miteinander kombinieren. Damit packen wir dann in die Mitte Boxplots. Links von den Boxplots zeichnen wir die einzelnen Beobachtungen als Punkte mit stat_dots() und die Verteilung der einzelnen Beobachtungen zeichnen wir mit dem R Paket {ggdist}. Das Tutorium Visualizing Distributions with Raincloud Plots liefert dann noch mehr Anleitungen für noch mehr Varianten. Wie du aber schon am R Code siehst, ist das eine etwas komplexere Abbildung geworden.\n\nggplot(gummi_tbl, aes(x = gender, y = age, color = gender)) +\n  theme_minimal() +\n  stat_halfeye(adjust = 0.5, width = 0.4, .width = 0, \n    justification = -0.3, point_colour = NA) + \n  geom_boxplot(width = 0.15, outlier.shape = NA) +\n  stat_dots(side = \"left\", justification = 1.12, binwidth = .25) +\n  coord_cartesian(xlim = c(1.2, 1.9), clip = \"off\") +\n  labs(x = \"Geschlecht\", y = \"Alter in Jahren\") +\n  scale_color_okabeito() +\n  theme(legend.position = \"none\")\n\nggplot(gummi_tbl, aes(x = gender, y = height, color = gender)) +\n  theme_minimal() +\n  stat_halfeye(adjust = 0.5, width = 0.4, .width = 0, \n    justification = -0.3, point_colour = NA) + \n  geom_boxplot(width = 0.15, outlier.shape = NA) +\n  stat_dots(side = \"left\", justification = 1.12, binwidth = .25) +\n  coord_cartesian(xlim = c(1.2, 1.9), clip = \"off\") +\n  labs(x = \"Geschlecht\", y = \"Körpergröße in cm\") +\n  scale_color_okabeito() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Alter nach Geschlecht\n\n\n\n\n\n\n\n\n\n\n\n(b) Körpergröße nach Geschlecht\n\n\n\n\n\n\n\nAbbildung 16.37— Der {gghalves}-Plot als Kombination vom Dotplot, Boxplot sowie Densityplot. Mit der Art der Abbildung spart man sich dann drei Abbildungen. Hier haben wir dann alle Informationen über die Körpergröße sowie dem Alter in Abhängigkeit vom Geschlecht in einer Abbildung.\n\n\n\n\n\n\n16.3.7 Violinplot\nEine etwas neuere Abbildung, die eigentlich gar so neu ist, ist der Violinplot. Der Violinplot verbindet im Prinzip den Boxplot zusammen mit dem Densityplot. Wir haben am Ende eben eine Verteilung der Daten visualisiert. Wir schauen uns aber nicht wie in einem Histogramm die Werte als Balken an, sondern glätten die Balken zu einer Kurve. Wie immer gibt es auch ein Tutorium mit noch mehr Hilfe unter ggplot2 violin plot : Quick start guide - R software and data visualization. Wir schauen uns jetzt mal die Erstellung von Violinplots in verschiedenen Kombinationen mit anderen Abbildungen an.\nDa ein Violinplot keinen Median oder sonst eine deskriptive Zahl beinhaltet, müssen wir uns eine Funktion erstellen, die den Mittelwert plusminus Standardabweichung wiedergibt. Die Funktion rufen wir dann innerhalb von ggplot() auf und erhalten dann den Mittelwert und Standardabweichung als einen Punkt mit zwei Linien dargestellt.\n\ndata_summary &lt;- function(y) {\n   m &lt;- mean(y)\n   ymin &lt;- m - sd(y)\n   ymax &lt;- m + sd(y)\n   return(c(y = m, ymin = ymin, ymax = ymax))\n}\n\nIn der Abbildung 16.38 siehst du einmal einen Violinplot mit der Funktion geom_violin(). Ich nutze eigentlich immer die Option trim = FALSE damit die Violinplots nicht so abgeschnitten sind. Der Nachteil ist dann, dass eventuell Werte angezeigt werden, die in den Daten nicht vorkommen, aber das ist auch sonst der Fall bei anderen Densityplots. Hier sieht es dann einfach besser aus und deshalb nutze ich es gerne. Durch die Funktion stat_summary() ergänze ich dann noch den Mittelwert und die Standardabweichung.\n\nggplot(data = gummi_tbl, aes(x = gender, y = height,\n                             color = gender)) +\n  theme_minimal() +\n  geom_violin(trim = FALSE) +\n  theme(legend.position = \"none\") +\n  stat_summary(fun.data = data_summary) +\n  labs(x = \"Geschlecht\", y = \"Körpergröße in cm\") +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 16.38— Der Violinplot die Körpergröße aufgeteilt nach Geschlecht als die simpelste Art der Darstellung mit einem Violinplot. Ergänzt noch durch den Mittelwert plusminus der Standardabweichung.\n\n\n\n\n\nIn der nächsten Abbildung 16.39 siehst du dann die Implementierung des Violinplot aus dem R Paket {see} mit der Funktion geom_violindot(). Auch hier trimme ich nicht die Spitzen der Violinplots und vergrößere die Punkte in dem Dotplot. Die Stärke von der Funktion ist der halbe Violinplot zusammen mit einem Dotplot, wir haben dann beides. Zum einen können wir die Werte sehen, wie sie sich in einem Histogramm anordnen würden. Zum anderen haben wir dann auch den Densityplot als geglättete Kurve daneben. Ich habe auch hier den Mittelwert und die Standardabweichung ergänzt, musste aber die Position in der \\(x\\)-Richtung etwas verschieben.\n\nggplot(data = gummi_tbl, aes(x = gender, y = height,\n                             color = gender)) +\n  theme_minimal() +\n  geom_violindot(dots_size = 4, trim = FALSE) +\n  theme(legend.position = \"none\") +\n  stat_summary(fun.data = data_summary, \n               position = position_nudge(x = 0.1)) +\n  labs(x = \"Geschlecht\", y = \"Körpergröße in cm\") +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 16.39— Der Violinplot die Körpergröße aufgeteilt nach Geschlecht als die simpelste Art der Darstellung mit einem Violinplot. Ergänzt noch durch den Mittelwert plusminus der Standardabweichung. Hier müssen wir aber die Darstellung auf der \\(x\\)-Achse um \\(0.1\\) etwas verschieben.\n\n\n\n\n\nDu musst natürlich keine Funktion aus einem anderen Paket nehmen. Der Violinplot lässt sich als ganzer Plot auch mit dem Dotplot kombinieren. Wir plotten als erstes in den Hintergrund den Violinplot, ergänzen dann darüber den Dotplot und zeichnen ganz zum Schluss noch die Mittelwerte und die Standardabweichung ein. So erhalten wir dann die Abbildung 16.40.\n\nggplot(data = gummi_tbl, aes(x = gender, y = height,\n                             fill = gender)) +\n  theme_minimal() +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\",\n               dotsize = 0.5) +\n  stat_summary(fun.data = data_summary, size = 1, linewidth = 2) +\n  theme(legend.position = \"none\") +\n  labs(x = \"Geschlecht\", y = \"Körpergröße in cm\") +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 16.40— Der Violinplot die Körpergröße aufgeteilt nach Geschlecht als die simpelste Art der Darstellung mit einem Violinplot. Ergänzt noch durch den Mittelwert plusminus der Standardabweichung sowie den einzelnen Beobachtungen aus einem Dotplot.\n\n\n\n\n\nIn der Abbildung 16.41 sehen wir dann anstatt von einem Dotplot einen Beeswarm. Wie immer ist es Geschmackssache welcher Plot einem mehr zusagt. Der Beeswarm wirkt immer etwas kompakter und so lässt sich hier auch mehr erkennen. Das Problem ist eher, dass die Punkte sich nicht füllen lassen, so dass wir dann doch ein recht einheitliches Bild kriegen. Hier muss ich dann immer überlegen, was ich dann wie einfärben will.\n\nggplot(data = gummi_tbl, aes(x = gender, y = height,\n                             fill = gender, color = gender)) +\n  theme_minimal() +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_beeswarm() +\n  theme(legend.position = \"none\") +\n  stat_summary(fun.data = data_summary, size = 1, linewidth = 2,\n               color = \"black\") +\n  labs(x = \"Geschlecht\", y = \"Körpergröße in cm\") +\n  scale_fill_okabeito() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 16.41— Der Violinplot die Körpergröße aufgeteilt nach Geschlecht als die simpelste Art der Darstellung mit einem Violinplot. Ergänzt noch durch den Mittelwert plusminus der Standardabweichung sowie den einzelnen Beobachtungen aus einem Beeswarm.\n\n\n\n\n\n\n\n16.3.8 Mosaic Plot\nWenn wir zwei Spalten visualisieren wollen, die aus zwei Faktoren bestehen mit jeweils zwei Leveln, dann nutzen wir den Mosaic Plot. Wir nutzen den Datensatz flea_dog_cat.xlsx mit vierzehn Beobachtungen. Zuerst drehen wir nochmal die Ordnung der Level in dem Faktor animal.\n\nflea_dog_cat_mosaic_tbl &lt;- flea_dog_cat_tbl |&gt; \n  mutate(animal = factor(animal, levels = c(\"dog\", \"cat\"))) \n\nSchauen wir uns jetzt einmal die 2x2 Kreuztabelle der beiden Spalten animal und infected an. Um die 2x2 Tabelle in R in der richtigen Orientierung vorliegen zu haben, müssen wir nochmal einen kleinen Klimmzug über mutate() nehmen. Wir wandeln die Variable infected in einen Faktor um und sortieren die Level entsprechend, so dass wir die richtige Ordnung wie später im Mosaic Plot haben. Dieser Umweg hat nur didaktische Gründe, später plotten wir den Mosaic Plot direkt und schauen uns vorher nicht die 2x2 Tabelle in R an. Hier also die 2x2 Kreuztablle aus R.\n\nflea_dog_cat_mosaic_tbl %&gt;% \n  mutate(infected = factor(infected, levels = c(1, 0))) |&gt; \n  tabyl(infected, animal) \n\n infected dog cat\n        1   3   2\n        0   4   5\n\n\nWir sehen in der Tabelle, dass wir mehr nicht infizierte Tiere (n = 9) als infizierte Tiere haben (n = 5). Die Aufteilung zwischen den beiden Tierarten ist nahezu gleich. Im folgenden wollen wir diese Tabelle durch einen Mosaic Plot einmal visualisieren.\nUm jetzt einen Mosaic Plot zeichnen zu können müssen wir die relativen Anteile pro Spalte bzw. für jedes Level von \\(x\\) berechnen. In unserem Fall ist \\(x\\) die Variable animal und die Level sind dog und cat. In der folgenden 2x2 Kreuztablle werden die relativen Anteile für die Hunde- und Katzenflöhe für den Infektionsstatus berechnet.\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal\n\n\n\n\n\n\nDog\nCat\n\n\n\nInfected\nYes (1)\n\\(\\cfrac{3}{7} = 0.43\\)\n\\(\\cfrac{2}{7} = 0.29\\)\n\\(\\mathbf{5}\\)\n\n\n\nNo (0)\n\\(\\cfrac{4}{7} = 0.57\\)\n\\(\\cfrac{5}{7} = 0.71\\)\n\\(\\mathbf{9}\\)\n\n\n\n\n\\(\\mathbf{7}\\)\n\\(\\mathbf{7}\\)\n\\(n = 14\\)\n\n\n\nWir können jetzt die relativen Anteile in den Mosaic Plot übertragen und erhalten die Abbildung 16.42. Wir müssen also zuerst die absoluten Anteile bestimmen um dann die relativen Anteile für die Spalten berechnen zu können. Abschließend zeichnen wir dann den Mosaic Plot. Wir nutzen dafür das R Paket ggmosaic mit der Funktion geom_mosaic().\n\nggplot(data = flea_dog_cat_mosaic_tbl) +\n  geom_mosaic(aes(x = product(infected, animal), fill = animal)) +\n  annotate(\"text\", x = c(0.25, 0.25, 0.75, 0.75), \n                   y = c(0.25, 0.75, 0.25, 0.85), \n           label = c(\"0.57\", \"0.43\", \"0.71\", \"0.29\"), size = 7) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nAbbildung 16.42— Visualisierung einer 2x2 Tabelle als Mosaic Plot. Die unterschiedlich großen Flächen geben die Verhältnisse per Spalte wieder.\n\n\n\n\n\nAbbildung 16.42 zeigt den Mosaic Plot für die Variable animal and infected. Die untrschiedlich großen Flächen bilden die Verhältnisse der 2x2 Tabelle ab. So sehen wir, dass es mehr uninfizierte Tiere als infizierte Tiere gibt. Am meisten gibt es uninfizierte Katzen. Am wenigstens treten infizierte Katzen auf.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Visualisierung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-ggplot.html#zusätzliche-möglichkeiten",
    "href": "eda-ggplot.html#zusätzliche-möglichkeiten",
    "title": "16  Visualisierung von Daten",
    "section": "16.4 Zusätzliche Möglichkeiten",
    "text": "16.4 Zusätzliche Möglichkeiten\nIm Folgenden dann noch eine Sammlung an nützlichen Optionen und Möglichkeiten, die einem das Leben einfacher machen und die Abbildungen dann noch schöner. Nicht alles musst du in ggplot machen, manchmal geht es dann in PowerPoint dann doch schneller mal eben einen Text zu ergänzen. Sehe das hier deshalb als Ergänzung und meinen privaten Raum, den ich nutze um mir den Code zu merken.\n\n16.4.1 Überschriften, Achsen und Legenden\nWenn du mehr machen willst, also die Überschriften anpassen oder aber die Achsenbeschriftung ändern, dann gibt es hier global Hilfe im ggplot Manual. Die Webseite R Cookbook hat auch spezielle Hilfe für ggplot().\n\nÜberschriften von Abbildungen\nAchsenbeschriftung\nLegende\nFarben\n\nIn Abbildung 16.43 siehst du eine Abbildung mit Titel und veränderten Beschriftungen. Die Möglichkeiten sind nahezu unbegrenzt und sprengen auch hier den Rahmen. Im Zweifel im R Tutorium vorbeischauen oder aber in der Vorlesung fragen.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = jump_length,\n                                    fill = animal)) +\n  geom_boxplot() +\n  labs(title = \"Frischgewicht in Abhängigkeit von der Behandlung\",\n       x = \"Behandlung\", y = \"Frischgewicht in kg/ha\") +\n  scale_x_discrete(labels = c(\"Katze\", \"Hund\")) +\n  scale_fill_discrete(name = \"Behandlung\", labels = c(\"Katze\", \"Hund\")) +\n  theme_minimal() \n\n\n\n\n\n\n\nAbbildung 16.43— Beispielhafte Abbildung mit Titel und geänderter Achsenbeschrittung\n\n\n\n\n\n\n\n16.4.2 Abbildungen abspeichern\nWenn du eine Abbildung abspeichern willst, dann musst du nur nach dem ggplot-Code die Funktion ggsave() setzen. Wie du im hier im Folgenden siehst, speichere ich die Abbildung der Boxplots der Hunde- und Katzenflöhe einmal in der Datei flea_dog_boxplot.png ab. Dabei wähle ich eine Breite width und eine Höhe height von jeweils 5. Du musst dann immer etwas spielen, je größer die Zahlen, desto größer die Abbildung und die Auflösung.\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal, y = jump_length)) +\n  geom_boxplot() \n\n## Abspeichern des obigen ggplots  \nggsave(\"flea_dog_boxplot.png\", width = 5, height = 5)\n\nWie immer hilft auch die Hilfeseite von ggsave() weiter, wenn es um mehr Optionen und Qualität der Abbildungen geht.\n\n\n16.4.3 Mathematische Ausdrücke in den Achsenbeschriftungen\n\n\n\n\n\n\nBesuche auch ggplot2 extensions für weitere tolle Möglichkeiten!\n\n\n\nHäufig wollen wir nicht nur einfache Achsenbeschriftungen haben, sondern auch irgendwie komplexere Einheiten wie Eisendüngergehalt im Boden in \\([kg\\, ha]^{-1}\\) darstellen. Jetzt soll die Einheit auch in dieser Form mit in die Achsenbeschriftung. Wir können dafür zwei Wege wählen. Einmal über das R Paket {latex2exp} und die Funktion TeX() oder aber die Funktion expression(), wofür wir dann kein eigenes R Paket brauchen. Beide Wege haben Vor- und Nachteile. Wir gehen aber beide mal durch. Mehr Informationen durch das Tutorium Using latex2exp oder aber eben der Klassiker mit Plot math expression.\nWir können die Funktion expression() nutzen um uns mathematische Formeln zu bauen. Leider ist das Ganze etwas frickelig und auch ich brauche immer drei Anläufe, bis die Formel dann passt. Im Folgenden aber einmal zwei Beispiel für mathematische Formeln und Ausdrücke. Beachte, dass du jedes Leerzeichen durch eine Tilde ~ abbilden musst. Ich nutze die Funktion expression() sehr selten und nur wenn die Formel wirklich sehr einfach ist. Da wir aber schon mit eckigen Klammern Probleme kriegen und diese so nervig mit \" einklammern müssen, nutze ich dann das Paket {latex2exp} was ich im Folgenden vorstellen werde.\nHier aber erstmal zwei Beispiele für eine Formel mit der Funktion expression(). Wenn du mehr über die Möglichkeiten wissen willst, dann schauen auch einmal auf die Hilfeseite von Plot math oder du googelst dir die Lösung wie ich früher zusammen.\n\nplot(expression(Eisendüngeform~und~-höhe~\"[kg ha]\"^-1), cex = 1.5, main = \"\")\n\n\n\n\n\n\n\nplot(expression(Fe-Gehalt~\"[\"~mg%.%(kg~TM)^-1~\"]\"), cex = 1.5, main = \"\")\n\n\n\n\n\n\n\n\nFür mich ausdrücklich einfacher geht es mit dem R Paket {latex2exp} und der Funktion TeX() sowie die Helferfunktion r\"()\". Ja, hier muss man dann noch eine andere Programmiersprache kennen, aber wie immer, du wirst nur schlauer. Die Informationen zur Matheumgebung in \\(\\LaTeX\\) kommen dann nochmal extra zwischen zwei Dollarzeichen $. Ja, das ist etwas wirr für einen Anfänger, aber wir nutzen hier auch zwei Programmiersprachen zusammen. Zum einen \\(\\LaTeX\\) um die Mathesymbole sauber darzustellen und dann R um die Abbildungen in ggplot() zu bauen. Mehr Informationen zu der Matheumgebung in \\(\\LaTeX\\) findest du einmal in der LaTeX Mathehilfe I sowie der LaTeX Mathehilfe II.\nWie bauen wir uns also unseren mathematischen Ausdruck? Als erstes brauchen wir die Funktion Tex(), die sagt einfach nur aus, dass jetzt \\(\\LaTeX\\)-Code kommt. Dann wollen wir noch einen String brauen in dem der \\(\\LaTeX\\)-Code für unseren mathematischen Ausdruck drin steht. Diesen String bauen wir mit r\"()\". Achtung, hier ist das Gänsefüßchen oben und unten vor und nach der runden Klammer sehr wichtig. In den Ausdruck können wir dann Text schreiben Eisengehalt oder aber einen mathematischen Ausdruck abgrenzt von zwei Dollarzeichen $ wie $[kg\\, ha]^{-1}$. \\(\\LaTeX\\) kann nämlich nicht nur mathematische Ausdrücke sondern ist eigentlich ein Textverarbeitungsprogramm. Deshalb musst du hier nochmal zwischen Text und mathematischen Ausdruck unterscheiden.\nHier nochmal aufgeschlüsselt wie der Code aussieht. Wir schreiben den Code nachher in einer Zeile, aber zum Verständnis ist es besser, wenn wir den Code einmal aufgeklappt sehen.\n\nTeX(\n    r\"(\n      Eisengehalt $[kg\\, ha]^{-1}$\n    )\"\n   )\n\nWir wollen uns das Ergebnis einmal in einem simplen plot() anschauen. Wir nutzen die Funktionalität natürlich später in ggplot, aber hier ist es so einmal einfacher zu sehen.\n\nplot(cex = 2, main = \"\",\n  TeX(r\"(\n         Eisengehalt $[kg\\, ha]^{-1}$\n      )\")\n    )\n\n\n\n\n\n\n\n\nAuch können wir sehr viel komplexere Formeln erstellen. Beachte auch hier, dass wir zwei Matheumgebungen in \\(\\LaTeX\\) vorliegen haben.\n\nplot(cex = 2, main = \"\",\n  TeX(r\"(\n         A $\\LaTeX$ formula: $\\frac{2hc^2}{\\lambda^5}\\frac{1}{e^{\\frac{hc}{\\lambda k_B T}} - 1}$\n      )\")\n  )\n\n\n\n\n\n\n\n\nIn der Abbildung 16.44 dann nochmal die Anwendung in einem ggplot in dem wir die Achsen entsprechend beschriften und dann auch noch eine ausgedachte Regressionsgeleichung zu der Abbildung ergänzen.\n\nggplot(data = flea_dog_cat_tbl, aes(x = flea_count, y = jump_length)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(x = TeX(r\"(Eisengehalt und -höhe $[kg\\, ha]^{-1}$)\"), \n       y = TeX(r\"(Fe-Gehalt $[mg \\cdot (kg TM)^{-1}]$)\")) +\n  annotate(\"text\", x = 10, y = 10,\n           label = TeX(r\"($y = \\beta_0 + \\beta_1 \\cdot x;\\; R^2 = 0.24$)\"))\n\n\n\n\n\n\n\nAbbildung 16.44— Zusammenhang zwischen dem Eisengehalt und -höhe im Boden und dem Eisengehalt in Salat. Zusätzlich ergänzt eine Regressiongleichung und ein ausgedachtes Bestimmtheitsmaß.\n\n\n\n\n\nWenn du dann mal die Funktion Tex() in geom_text() verwenden willst, dann musst du einmal etwas anpassen. Dann klappt es aber auch hier. Das hat mich mal echt Nerven und Zeit gekostet, deshalb lagere ich die Information mal hier für mich.\n\nggplot() +\n  theme_void() +\n  geom_text(aes(0, 0, \n                label = TeX(r'($\\alpha  x^\\alpha$, where $\\alpha \\in 1\\ldots 5$)',\n                                  output = \"character\")), parse = TRUE) \n\n\n\n16.4.4 Inklusive Farbpaletten\n\n\n\n\n\n\nBesuche auch ggplot2 extensions für weitere tolle Möglichkeiten!\n\n\n\nNeben den klassischen Farben im R Paket {ggplot2} gibt es noch weit, weit mehr Farbpaletten. Wir nutzen in der Folge immer wieder die Okabe-Ito Farbpalette aus dem R Paket {see}. Die Okabe-Ito Farbpalette ist speziell so gebaut, dass die Farben sich gut für farbenblinde Personen unterscheiden. Mehr zum R Paket {see} auf der Hilfeseite des Paketes. Der Kontrast zwischen den Farben ist sehr gut. Wenn du eine andere Farbpalette nutzen willst, findest du hier noch andere Color Scales.\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal, y = jump_length,\n           fill = animal)) +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 16.45— Beispielhafte Abbildung der Okabe-Ito Farbpalette für Boxplots.\n\n\n\n\n\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal, y = jump_length,\n           color = animal)) +\n  geom_point() +\n  scale_color_okabeito() +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 16.46— Beispielhafte Abbildung der Okabe-Ito Farbpalette für Punkte.\n\n\n\n\n\nDas Ganze geht dann auch händisch mit dem folgenden Code für die jeweiligen Farben. Anbei einmal die Farbpalette dargestellt.\n\n\n\n\n\n\n\n\nAbbildung 16.47— Farbpalette nach dem Okabe-Ito-Schema ebenfalls für farbblinde Personen erstellt.\n\n\n\n\n\nDie Farben sind dann in der Reihenfolge wie folgt kodiert.\n\ncbbPalette &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \n                \"#CC79A7\", \"#999999\", \"#000000\")\n\nWenn wir Boxplots einfärben wollen dann nehmen wir den folgenden Code.\n\nscale_fill_manual(values = cbPalette)\n\nUnd das hier ist die Ergänzung für Punkte und Linien.\n\nscale_colour_manual(values = cbPalette)\n\nNeben der Okabe-Ito Farbpalette liefert das R Paket {duke} noch eine andere Möglichkeit eine Farbpalette für Farbblinde zu generieren.\n\n\n\n\n\n\n\n\nAbbildung 16.48— Farbpalette nach dem Duke-Schema ebenfalls für farbblinde Personen erstellt.\n\n\n\n\n\nDie Funktionen hier sind scale_duke_color_discrete() sowie scale_duke_continuous() und scale_duke_fill_discrete().\n\n\n16.4.5 Farbpaletten\nManchmal benötigen wir auch Farbverläufe. In R heißen diese Farbverläufe dann Farbpaletten. Eine Einführung liefert das Tutorium Using RColorBrewer palettes. Ich selber nutze gerne das R Paket {wesanderson} welches sehr schöne Farbverläufe hat. Mehr kannst du auf der GitHub Seite Wes Anderson Palettes erfahren. Wir können die Paletten ganz einfach mit der Funktion wes_palette() laden.\n\nwes_palette(\"Zissou1\")\n\n\n\n\n\n\n\n\nDas schöne ist hier, dass wir einfach wie folgt Farbverläufe erstellen können. Wir wollen hier 21 Farbwerte haben und das Ganze dann als kontinuierlichen Verlauf.\n\nwes_palette(\"Zissou1\", 21, type = \"continuous\")\n\n\n\n\n\n\n\n\n\n\n16.4.6 Abbildungen nebeneinander\n\n\n\n\n\n\nBesuche auch ggplot2 extensions für weitere tolle Möglichkeiten!\n\n\n\nDas R Paket {patchwork} erlaubt es mehrere ggplot Abbildungen nebeneinander oder in einem beliebigen Layout miteinander zu verbinden. Das tolle ist, dass die Idee sehr intuitiv ist. Wir nutzen wieder das + um verschiedene Plots miteinander zu verbinden. Im Folgenden erschaffen wir uns zwei ggplots und speichern die Plots in den Objekten p1 und p2. Das ist wie wir es bisher kennen, nur das jetzt keine Abbildung erscheint sondern beide Plots in zwei Objekten gespeichert sind.\n\np1 &lt;- ggplot(data = flea_dog_cat_tbl, \n             aes(x = flea_count, y = jump_length,\n                 color = animal)) +\n  geom_point() +\n  scale_color_okabeito() +\n  theme_minimal()\n\np2 &lt;- ggplot(data = flea_dog_cat_tbl, \n                aes(x = animal, y = jump_length,\n                    color = animal)) +\n  geom_point() +\n  scale_color_okabeito() +\n  theme_minimal()\n\nWie können wir nun die beiden Abbildungen nebeneinander zeichnen? Wir nutzen einfach das + Symbol.\n\np1 + p2\n\n\n\n\n\n\n\nAbbildung 16.49— Beispielhafte Abbildung der zweier Plots nebeneinander.\n\n\n\n\n\nAuf der Seite des R Paket {patchwork} findest du viel mehr Möglichkeiten das Layout anzupassen und auch die einzelnen Subplots zu beschriften.\n\n\n16.4.7 Gebrochene \\(y\\)-Achse\n\n\n\n\n\n\nBesuche auch ggplot2 extensions für weitere tolle Möglichkeiten!\n\n\n\nDas R Paket {ggbreak} erlaubt es dir in die \\(x\\)-Achse oder aber \\(y\\)-Achse Lücken einzusetzen oder aber die Achsen eben gebrochen darzustellen. Zur Demonstration bauen wir uns nochmal den stat_tbl für die Hunde- und Katzenflöhe. Wir berechnen hier dann die Mittelwerte und nicht mehr die Standardabweichung, da es sonst nicht so gut mit der Darstellung mit der gebrochenen \\(y\\)-Achse für dieses Beispiel klappt.\n\nstat_tbl &lt;- flea_dog_cat_tbl |&gt; \n  group_by(animal) |&gt; \n  summarise(mean = mean(jump_length))\n\nIn der Abbildung 16.50 siehst du einmal die Abbildung der Mittelwerte der Sprungweiten der Hunde- und Katzenflöhe als Barplots dargestellt. Ich habe hier einen Bruch auf der \\(y\\)-Achse mit der Funktion scale_y_break() bei 1 bis 4 eingefügt und den Abstand über die Option space etwas visuell vergrößert. Mit der Option scales könntest du dann noch die Skalierung der gebrochenen \\(y\\)-Achse anpassen.\n\nggplot(stat_tbl, aes(x = animal, y = mean, fill = animal)) + \n  theme_minimal() +\n  geom_bar(stat = \"identity\") +\n  scale_y_break(c(1, 4), space = 0.5)\n\n\n\n\n\n\n\nAbbildung 16.50— Beispielhafte Abbildung der Barplots mit gebrochener \\(y\\)-Achse. Die Fehlerbalken wurden aus Gründen der besseren Darstellung der zerbrochenen \\(y\\)-Achse entfernt.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Visualisierung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-ggplot.html#zerforschen",
    "href": "eda-ggplot.html#zerforschen",
    "title": "16  Visualisierung von Daten",
    "section": "16.5 Zerforschen",
    "text": "16.5 Zerforschen\nAm Ende des Kapitels wollen wir hier nochmal Abbildungen von Postern aus der Hochschule Osnabrück nachbauen. Ich habe die Fotos so aufgenommen, dass hier nicht direkt klar wird worum es geht, aber das ist ja auch irgendwie egal. Es geht um den Spaß und das Lernen. Natürlich lassen sich die Poster dann wiederfinden, aber nicht alle Poster bleiben ja auch ewig hängen. Hier geht es also darum mal zu lernen, wie so Abbildungen aufgebaut sind und wie wir die Abbildungen dann in ggplot nachbauen können. Ich finde es immer Augen öffnend hier nochmal die Abbildungen in Excel mit den wirklich schöneren Abbildungen aus R zu vergleichen.\nHier findest du die etwas komplexeren Beispiele, die ich oben nicht einordnen wollte, da es mehr verwirrt als hilft. In den Kapiteln zur Methode hinter den Abbildungen findest du die Beispiele nochmal. Mehr Beispiele findest du dann in den jeweiligen Kapiteln zu den entsprechenden Methoden. Hier also nur noch eine ergänzende Übersicht.\n\n\n\n\n\n\nZerforschen: Nicht-lineare Regression mit Zielbereich\n\n\n\n\n\nIn diesem Zerforschenbeispiel schauen wir uns eine nicht-lineare Regression einmal genauer an. Ganz am Ende geht es dann auch nochmal ganz kurz um die Modellierung. Jetzt wollen wir uns aber erstmal die folgende Abbildung vornehmen und diese in ggplot nachbauen. Das wird eine echte Freude, denn die Abbildung ist wunderbar komplex. Wir haben zum einen die nicht-linearen Regressionsgeraden sowie deren Funktionen im Plot. Darüber hinaus dann noch ein Zielbereich mit einem Pfeil und einer Beschriftung. Am Ende müssen wir dann auch noch die Achsen mit den mathematischen Formeln beschriften. Wir haben also einiges zu tun.\n\n\n\n\n\n\nAbbildung 16.51— Ursprüngliche Abbildung, die nachgebaut werden soll. Zwei nicht-lineare Regession laufen durch Mittelwert plus/minus Standardabweichung. Im Weiteren sind die Regressionsgleichungen noch ergänzt sowie ein Zielbereich farblich hervorgehoben. Am Ende müssen dann die Achsen noch sauber beschriftet werden.\n\n\n\nZuerst habe ich mir einmal einen leeren Plot erstellt in dem ich nur die Regressionsgleichungen abbilden werde. Aus den Gleichungen kann ich mir dann die Mittelwerte von dem content für die entsprechenden iodine-Werte berechnen. Also erstmal ein paar passende \\(x\\)-Werte und die entsprechenden \\(y\\)-Werte. Hierbei sind die \\(y\\)-Werte nicht so wichtig, die werden wir uns ja mit den Regressionsgleichungen berechnen, aber wir brauchen ja die \\(y\\)-Werte für unser Canvas. Die Funktion ggplot() muss ja wissen was auf \\(x\\) und \\(y\\) soll.\n\nfunc_tbl &lt;- tibble(iodine = c(0, 0.25, 0.5, 0.75, 1, 2),\n                   content = c(0, 100, 150, 200, 300, 400))\n\nDann schreibe ich mir noch über das \\(x){} die Funktionen für die Regressionsgleichungen in R auf. Dabei ist \\(x){} die Kurzschreibweise für function(x){}. Ich mache es mir hier nur etwas leichter.\n\nki_func &lt;- \\(x) {-104 * x^2 + 445 * x}\nkio3_func &lt;- \\(x) {-117 * x^2 + 374 * x}\n\nDann schauen wir uns einmal die beiden Funktion mit dem geom_function() einmal in der Abbildung 54.5 einmal an. Das geom_function() nimmt dabei eine definierte Funktion und berechnet dann die entsprechenden \\(y\\)-Werte aus den \\(x\\)-Werten.\n\nggplot(data = func_tbl, aes(x = iodine, y = content)) +\n  theme_minimal() +\n  geom_function(fun = ki_func, color = cbbPalette[2], linetype = 'dashed') +\n  geom_function(fun = kio3_func, color = cbbPalette[3], linetype = 'dashed') \n\n\n\n\n\n\n\nAbbildung 16.52— Darstellung der beiden Regressionsgleichungen für \\(KI\\) und \\(KIO_3\\).\n\n\n\n\n\nAnhand der beiden Funktionen kann ich mir jetzt auch die Mittelwerte berechnen. Die Mittelwerte sind ja die Werte, die sich für einen iodine-Wert auf der entsprechenden Regressionsgeraden ergeben. Hier also einmal alle meine Werte des content auf den beiden Geraden für die entsprechenden iodine-Werte.\n\ntibble(iodine = c(0, 0.25, 0.5, 0.75, 1, 2),\n       ki = ki_func(iodine),\n       kio3 = kio3_func(iodine))\n\n# A tibble: 6 × 3\n  iodine    ki  kio3\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0       0    0  \n2   0.25  105.  86.2\n3   0.5   196. 158. \n4   0.75  275. 215. \n5   1     341  257  \n6   2     474  280  \n\n\nIch habe mir dann die Mittelwerte genommen und für jede iodine/ki-Kombination noch zwei Werte ergänzt um dann noch etwas Streuung zu bekommen. So habe ich dann nur drei Beobachtungen pro Kombination, aber die Anzahl soll hier erstmal reichen. Du findest die Werte in der Datei zerforschen_regression.xlsx. Hier ist die Datei in einem Auszug dann einmal geladen und dargestellt.\n\nregression_tbl &lt;- read_excel(\"data/zerforschen_regression.xlsx\") |&gt; \n  mutate(type = as_factor(type))\nregression_tbl \n\n# A tibble: 36 × 3\n   type  iodine content\n   &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 KI      0       0.1 \n 2 KI      0       0.05\n 3 KI      0       0.15\n 4 KI      0.25  100   \n 5 KI      0.25  105   \n 6 KI      0.25  110   \n 7 KI      0.5   180   \n 8 KI      0.5   190   \n 9 KI      0.5   200   \n10 KI      0.75  275   \n# ℹ 26 more rows\n\n\nWir berechnen wie gewohnt die Mittelwerte und die Standardabweichungen um diese dann als Punkte und Fehlerbalken in der Abbildung darstellen zu können.\n\nstat_tbl &lt;- regression_tbl |&gt; \n  group_by(type, iodine) |&gt; \n  summarise(mean = mean(content),\n            sd = sd(content))\nstat_tbl\n\n# A tibble: 12 × 4\n# Groups:   type [2]\n   type  iodine    mean      sd\n   &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 KI      0      0.1    0.05  \n 2 KI      0.25 105      5     \n 3 KI      0.5  190     10     \n 4 KI      0.75 262.    12.6   \n 5 KI      1    340     30     \n 6 KI      2    470     90     \n 7 KIO3    0      0.117  0.0764\n 8 KIO3    0.25  83.3    7.64  \n 9 KIO3    0.5  142.    10.4   \n10 KIO3    0.75 222.    10.4   \n11 KIO3    1    260     30     \n12 KIO3    2    275     50     \n\n\nSo und nun geht es los. Ich habe dir den folgenden Code einmal annotiert. Klicke dafür dann einfach auf die Nummern um mehr Informationen zu erhalten. Der Code ist sehr umfangreich, aber die Abbildung ist ja auch recht komplex. Beachte im Besonderen den Abschnitt von weiter oben zu den mathematischen Ausdrücken in den Achsenbeschriftungen. Ich mache hier ja auch sehr viel um dann die mathematischen Ausdrücke richtig hinzukriegen.\n\nggplot(data = stat_tbl, aes(x = iodine, y = mean,\n                            color = type)) +\n  theme_minimal() +\n1  geom_function(fun = ki_func, color = cbbPalette[2], linetype = 'dashed') +\n  geom_function(fun = kio3_func, color = cbbPalette[3], linetype = 'dashed') +\n  geom_point() +\n2  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.03) +\n  scale_y_continuous(breaks = c(0, 150, 300, 450, 600), limits = c(0, 600)) +\n3  scale_color_okabeito(labels = c(TeX(r\"($KI$)\"), TeX(r\"($KIO_3$)\"))) +\n  labs(color = \"Iodine\",\n       x = TeX(r\"(Iodine supply $[kg\\, l\\, ha^{-1}]$)\"),\n       y = TeX(r\"(Iodine content $[\\mu g\\, l \\, 100 g^{-1}\\, f.m.]$)\")) +\n4  annotate(\"text\", x = 0.8, y = 500, hjust = \"left\",\n           label = TeX(r\"($y = -98.41 \\cdot x^2 + 432.09 \\cdot x;\\; R^2 = 0.957$)\"),\n           color = cbbPalette[2]) +\n  annotate(\"text\", x = 0.8, y = 200, hjust = \"left\",\n           label = TeX(r\"($y = -117.08 \\cdot x^2 + 372.34 \\cdot x;\\; R^2 = 0.955$)\"),\n           color = cbbPalette[3]) +\n5  annotate(\"rect\", xmin = 0, xmax = 2, ymin = 50, ymax = 100,\n           alpha = 0.2, fill = cbbPalette[6]) +\n  annotate(\"text\", x = 1.525, y = 150, label = \"Target range\", hjust = \"left\",\n           size = 5) +\n  geom_curve(aes(x = 1.5, y = 150, xend = 1.25, yend = 105),\n             colour = \"#555555\",\n             size = 0.5,\n             curvature = 0.2,\n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  theme(legend.position = c(0.11, 0.8),\n        legend.box.background = element_rect(color = \"black\"),\n        legend.box.margin = margin(t = 1, l = 1),\n        legend.text.align = 0) \n\n\n1\n\nHier werden die beiden nicht-linearen Kurven gezeichnet.\n\n2\n\nHier werden die Fehlerbalken erstellt.\n\n3\n\nHier passen wir die Labels in der Legende entsprechend an.\n\n4\n\nHier schreiben wir die Regressionsgleichung für die jeweiligen Kurven hin.\n\n5\n\nHier produzieren wir den blauen Bereich plus den Pfeil sowie die Beschreibung.\n\n\n\n\n\n\n\n\n\n\nAbbildung 16.53— Einmal die komplexe Abbildung der nicht-linearen Regression in ggplot nachgebaut. Am Ende wurde es dann doch noch eine Legende und keine Beschriftung.\n\n\n\n\n\nUnd dann zum Abschluss nochmal die nicht-lineare Regression um zu schauen welche Koeffizienten der nicht-linearen Regression wir erhalten würden, wenn wir unsere selbst ausgedachten Daten nehmen würden. Wir nutzen hier einmal die Funktion nls() um die nicht-lineare Regression anzupassen und dann die Funktion r2() aus dem R Paket performance für unser Bestimmtheitsmaß \\(R^2\\). Das Ganze machen wir dann natürlich für beide Geraden, also einmal für \\(KI\\) und einmal für \\(KIO_3\\).\n\nNicht-lineare Regression für \\(KI\\)Nicht-lineare Regression für \\(KIO_3\\)\n\n\n\nfit &lt;- nls(content ~ b1 * iodine^2 + b2 * iodine, data = filter(regression_tbl, type == \"KI\"), \n           start = c(b1 = 1, b2 = 1)) \n\nfit |&gt; \n  parameters::model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\nb1        |      -98.41\nb2        |      432.09\n\nperformance::r2(fit)\n\n  R2: 0.957\n\n\n\n\n\nfit &lt;- nls(content ~ b1 * iodine^2 + b2 * iodine, data = filter(regression_tbl, type == \"KIO3\"), \n    start = c(b1 = 1, b2 = 1)) \n\nfit |&gt; \n  parameters::model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\nb1        |     -117.08\nb2        |      372.34\n\nperformance::r2(fit)\n\n  R2: 0.955\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZerforschen: Nicht-lineare Regression und zwei \\(\\boldsymbol{y}\\)-Achsen\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns eine nicht-lineare Regression anschauen. Das besondere hier sind die zwei Achsen auf der linken und rechten Seite der Abbildung. Wir brauchen also eine zwei \\(y\\)-Achse. Da wir auch zwei Geraden anpassen wollen, machen wir die Sache etwas komplizierter, aber es geht ja auch draum mal was Neues in ggplot zu lernen. Wir wollen also die folgende Abbildung einmal nachbauen.\n\n\n\n\n\n\nAbbildung 16.54— Ursprüngliche Abbildung, die nachgebaut werden soll. Zwei nicht-lineare Regession mit zwei \\(y\\)-Achsen.\n\n\n\nHier möchte ich die Daten direkt in R erschaffen, dafür baue ich mir direkt ein tibble und nutze dann die Daten weiter. Natürlich kannst du auch hier eine Exceldatei erschaffen und dann die Exceldatei einlesen. Aber hier geht es so etwas schneller, wir haben ja nur eine handvoll von Beobachtungen vorliegen.\n\ntree_tbl &lt;- tibble(ph = c(3.8, 4.3, 4.4, 4.8, 5.2, 5.3, 5.4, 5.6, 5.8),\n                   boden = c(12.1, 6.0, 5.0, 1, 1, 0.8, 0, 0, 0),\n                   nadel = c(12.0, 7.0, 12, 10, 5, 6, 5, 8, 6))\n\nWenn dich nicht interessiert, wie die Geradengleichung entsteht, dann überspringe einfach den nächsten Kasten. Ich nutze hier eine nicht-lineare Regression um auf die Werte der Gleichungen für Nadeln und Blatt zu kommen. Spannenderweise kann ich die Werte in der Orginalabbildung nicht reproduzieren. Aber gut, wir nutzen dann die Koeffizienten, die zu den Daten passen.\n\nNicht-lineare Regression für BodenNicht-lineare Regression für Nadeln\n\n\n\nfit &lt;- nls(boden ~ b1 * exp(ph*b2), data = tree_tbl, \n           start = c(b1 = 1, b2 = 0)) \n\nfit |&gt; \n  parameters::model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\nb1        |    10496.50\nb2        |       -1.77\n\nperformance::r2(fit)\n\n  R2: 0.974\n\n\n\n\n\nfit &lt;- nls(nadel ~ b1 * exp(ph*b2), data = tree_tbl, \n           start = c(b1 = 1, b2 = 0)) \n\nfit |&gt; \n  parameters::model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\nb1        |       49.20\nb2        |       -0.38\n\nperformance::r2(fit)\n\n  R2: 0.543\n\n\n\n\n\nWir erhalten dann die Koeffizienten für die beiden folgenden Funktionen aus der nicht-linearen Regression aus den obigen Tabs. Wir brauchen zwei Geradengleichungen, da wir ja einmal die Nadeln und einmal die Blätter abbilden müssen. Auch hier bauen wir uns dann einmal eine Funktion zusammen. Beide Funktionen nutzen wir dann gleich für die Funktion geom_function().\n\nnadel_func &lt;- \\(x) {49.20 * exp(-0.38 * x)}\nboden_func &lt;- \\(x) {10496.50 * exp(-1.77 * x)}\n\nDer zentrale Punkt hier ist die zweite \\(y\\)-Achse, die wir über sec_axis() definieren. Hier musst du schauen, dass die zweite Achse ein Verhältnis zu der ersten Achse ist. Hier haben wir auf der linken Seite maximal 15 und auf der rechten Seite maximal 600. Daher ist das Verhältnis der Achsen zueinander 600 zu 15. Mehr dazu dann auf der Hilfeseite Specify a secondary axis. Wir fangen auch mit einem leeren Plot an und legen dann alle anderen geom-Funktionen darüber. Das macht uns die Konstruktion etwas einfacher, da wir dann die Punkte und Geraden besser einzeichnen können. Wir nutzen dann auch hier das R Paket latex2exp umd die mathematischen Formeln in der Legende zu erstellen.\n\nggplot() +\n  theme_minimal() +\n  geom_point(data = tree_tbl, aes(x = ph, y = boden), color = cbbPalette[7]) +\n  geom_point(data = tree_tbl, aes(x = ph, y = nadel), color = cbbPalette[4]) +\n  geom_function(fun = boden_func, color = cbbPalette[7]) +\n  geom_function(fun = nadel_func, color = cbbPalette[4]) +\n\n  scale_y_continuous(sec.axis = sec_axis(~ . * 600/15, name = \"Al-Gehalt Nadeln\\n [mg/kg TM]\",\n                                         breaks = c(0, 150, 300, 450, 600)),\n                     limits = c(0, 15),\n                     breaks = c(0, 3, 6, 9, 12, 15),\n                     name = \"Al-Gehalt Boden\\n [mg/kg TM]\") +\n  xlim(3.5, 6) +\n  annotate(\"text\", x = c(4.25, 4.5), y = c(2, 12.5), hjust = \"left\",\n           label = c(\"Boden\", \"Nadeln\"), color = cbbPalette[c(7, 4)], size = 6) +\n  labs(x = \"pH-Wert des Bodens\") +\n  annotate(\"text\", x = 3.75, y = 0.4, hjust = \"left\",\n           label = TeX(r\"($y = 10496.50 \\cdot e^{-1.77\\cdot x}\\; R^2 = 0.974$)\"), \n           color = cbbPalette[7]) + \n  annotate(\"text\", x = 4.25, y = 11, hjust = \"left\", \n           label = TeX(r\"($y = 49.20 \\cdot e^{-0.38\\cdot x}\\; R^2 = 0.543$)\"), \n           color = cbbPalette[4]) \n\n\n\n\n\n\n\nAbbildung 16.55— Einmal die komplexe Abbildung der nicht-linearen Regression in ggplot nachgebaut. Für beide Quellen Nadeln und Boden wurde jewiels eine eigene \\(y\\)-Achse erstellt. Die Regressionsgleichungen aus der Orginalabbildung entsprechen nicht den Werten hier in der Abbildung.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZerforschen: Einfaktorieller Barplot mit emmeans und compact letter display\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns einen einfaktoriellen Barplot oder Säulendiagramm anschauen. Daher fangen wir mit der folgenden Abbildung einmal an. Wir haben hier ein Säulendiagramm mit compact letter display vorliegen. Daher brauchen wir eigentlich gar nicht so viele Zahlen. Für jede der vier Behandlungen jeweils einmal einen Mittelwert für die Höhe der Säule sowie einmal die Standardabweichung. Die Standardabweichung addieren und subtrahieren wir dann jeweils von dem Mittelwert und schon haben wir die Fehlerbalken. Für eine detaillierte Betrachtung der Erstellung der Abbildung schauen einmal in das Kapitel zum Barplot oder Balkendiagramm oder Säulendiagramm.\n\n\n\n\n\n\nAbbildung 16.56— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein simples Säulendiagramm mit sehr für Farbblinde ungünstigen Farben. Es sind die Mittelwerte sowie die Standardabweichung durch die Fehlerbalken dargestellt.\n\n\n\nAls erstes brauchen wir die Daten. Die Daten habe ich mir in dem Datensatz zerforschen_barplot_simple.xlsx selber ausgedacht. Ich habe einfach die obige Abbildung genommen und den Mittelwert abgeschätzt. Dann habe ich die vier Werte alle um den Mittelwert streuen lassen. Dabei habe ich darauf geachtet, dass die Streuung dann in der letzten Behandlung am größten ist.\n\nbarplot_tbl &lt;- read_excel(\"data/zerforschen_barplot_simple.xlsx\") |&gt; \n  mutate(trt = factor(trt, \n                      levels = c(\"water\", \"rqflex\", \"nitra\", \"laqua\"),\n                      labels = c(\"Wasserdestilation\",\n                                 \"RQflex Nitra\",\n                                 \"Nitrachek\",\n                                 \"Laqua Nitrat\")))\nbarplot_tbl \n\n# A tibble: 16 × 2\n   trt               nitrat\n   &lt;fct&gt;              &lt;dbl&gt;\n 1 Wasserdestilation    135\n 2 Wasserdestilation    130\n 3 Wasserdestilation    145\n 4 Wasserdestilation    135\n 5 RQflex Nitra         120\n 6 RQflex Nitra         130\n 7 RQflex Nitra         135\n 8 RQflex Nitra         135\n 9 Nitrachek            100\n10 Nitrachek            120\n11 Nitrachek            130\n12 Nitrachek            130\n13 Laqua Nitrat         230\n14 Laqua Nitrat         210\n15 Laqua Nitrat         205\n16 Laqua Nitrat         220\n\n\nIm Folgenden sparen wir uns den Aufruf mit group_by() den du aus dem Kapitel zum Barplot schon kennst. Wir machen das alles zusammen in der Funktion emmeans() aus dem gleichnamigen R Paket. Der Vorteil ist, dass wir dann auch gleich die Gruppenvergleiche und auch das compact letter display erhalten. Einzig die Standardabweichung \\(s\\) wird uns nicht wiedergegeben sondern der Standardfehler \\(SE\\). Da aber folgernder Zusammenhang vorliegt, können wir gleich den Standardfehler in die Standardabweichung umrechnen.\n\\[\nSE = \\cfrac{s}{\\sqrt{n}}\n\\]\nWir rechnen also gleich einfach den Standardfehler \\(SE\\) mal der \\(\\sqrt{n}\\) um dann die Standardabweichung zu erhalten. In unserem Fall ist \\(n=4\\) nämlich die Anzahl Beobachtungen je Gruppe. Wenn du mal etwas unterschiedliche Anzahlen hast, dann kannst du auch einfach den Mittelwert der Fallzahl der Gruppen nehmen. Da überfahren wir zwar einen statistischen Engel, aber der Genauigkeit ist genüge getan.\nIn den beiden Tabs siehst du jetzt einmal die Modellierung unter der Annahme der Varianzhomogenität mit der Funktion lm() und einmal die Modellierung unter der Annahme der Varianzheterogenität mit der Funktion gls() aus dem R Paket nlme. Wie immer lässt sich an Boxplots visuell überprüfen, ob wir Homogenität oder Heterogenität vorliegen haben. Oder aber du schaust nochmal in das Kapitel Der Pre-Test oder Vortest, wo du mehr erfährst.\n\nVarianzhomogenitätVarianzheterogenität (gls)Varianzheterogenität (vcovHAC)\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim().\n\nemmeans_homogen_tbl &lt;- lm(nitrat ~ trt, data = barplot_tbl) |&gt;\n  emmeans(~ trt) |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(4)) \nemmeans_homogen_tbl\n\n# A tibble: 4 × 8\n  trt               emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 Nitrachek           120   5.08    12     109.     131. a       10.2\n2 RQflex Nitra        130   5.08    12     119.     141. ab      10.2\n3 Wasserdestilation   136.  5.08    12     125.     147. b       10.2\n4 Laqua Nitrat        216.  5.08    12     205.     227. c       10.2\n\n\nIn dem Objekt emmeans_homogen_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. Wie dir vielleicht auffällt sind alle Standardfehler und damit alle Standardabweichungen für alle Gruppen gleich, das war ja auch unsere Annahme mit der Varianzhomogenität.\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket und natürlich in den Abschnitt zu dem Gruppenvergleich unter Varianzheterogenität. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim().\n\nemmeans_hetrogen_gls_tbl &lt;- gls(nitrat ~ trt, data = barplot_tbl, \n                            weights = varIdent(form =  ~ 1 | trt)) |&gt;\n  emmeans(~ trt) |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(4)) \nemmeans_hetrogen_gls_tbl\n\n# A tibble: 4 × 8\n  trt               emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 Nitrachek           120   7.07  3.00     97.5     143. a      14.1 \n2 RQflex Nitra        130   3.54  2.98    119.      141. a       7.07\n3 Wasserdestilation   136.  3.15  2.98    126.      146. a       6.29\n4 Laqua Nitrat        216.  5.54  3.00    199.      234. b      11.1 \n\n\nIn dem Objekt emmeans_hetrogen_gls_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. In diesem Fall hier sind die Standardfehler und damit auch die Standardabweichungen nicht alle gleich, wir haben ja für jede Gruppe eine eigene Standardabweichung angenommen. Die Varianzen sollten ja auch heterogen sein.\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket und natürlich in den Abschnitt zu dem Gruppenvergleich unter Varianzheterogenität. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim().\n\nemmeans_hetrogen_vcov_tbl &lt;- lm(nitrat ~ trt, data = barplot_tbl) |&gt;\n  emmeans(~ trt, vcov. = sandwich::vcovHAC) |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(4)) \nemmeans_hetrogen_vcov_tbl\n\n# A tibble: 4 × 8\n  trt               emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 Nitrachek           120   7.84    12     103.     137. a      15.7 \n2 RQflex Nitra        130   3.92    12     121.     139. a       7.84\n3 Wasserdestilation   136.  2.06    12     132.     141. a       4.12\n4 Laqua Nitrat        216.  4.94    12     205.     227. b       9.88\n\n\nIn dem Objekt emmeans_hetrogen_vcov_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. In diesem Fall hier sind die Standardfehler und damit auch die Standardabweichungen nicht alle gleich, wir haben ja für jede Gruppe eine eigene Standardabweichung angenommen. Die Varianzen sollten ja auch heterogen sein.\n\n\n\nUnd dann haben wir auch schon die Abbildungen hier erstellt. Ja vielleicht passen die Standardabweichungen nicht so richtig, da könnte man nochmal an den Daten spielen und die Werte solange ändern, bis es besser passt. Du hast aber jetzt eine Idee, wie der Aufbau funktioniert. Die beiden Tabs zeigen dir dann die Abbildungen für die beiden Annahmen der Varianzhomogenität oder Varianzheterogenität. Der Code ist der gleiche für die drei Abbildungen, die Daten emmeans_homogen_tbl oder emmeans_hetrogen_gls_tbl ober emmeans_hetrogen_vcov_tbl sind das Ausschlaggebende.\n\nVarianzhomogenitätVarianzheterogenität (gls)Varianzheterogenität (vcovHAC)\n\n\n\nggplot(data = emmeans_homogen_tbl, aes(x = trt, y = emmean, fill = trt)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\") + \n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd), \n                width = 0.2) + \n  labs(x = \"\", \n       y = \"Nitrat-Konzentration \\n im Tannensaft [mg/L]\") +\n  ylim(0, 250) +\n  theme(legend.position = \"none\") + \n  scale_fill_okabeito() + \n  geom_text(aes(label = .group, y = emmean + sd + 10))\n\n\n\n\n\n\n\nAbbildung 16.57— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt.\n\n\n\n\n\n\n\n\nggplot(data = emmeans_hetrogen_gls_tbl, aes(x = trt, y = emmean, fill = trt)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\") + \n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd), \n                width = 0.2) + \n  labs(x = \"\", \n       y = \"Nitrat-Konzentration \\n im Tannensaft [mg/L]\") +\n  ylim(0, 250) +\n  theme(legend.position = \"none\") + \n  scale_fill_okabeito() + \n  geom_text(aes(label = .group, y = emmean + sd + 10))\n\n\n\n\n\n\n\nAbbildung 16.58— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion gls() im obigen Modell berücksichtigt.\n\n\n\n\n\n\n\n\nggplot(data = emmeans_hetrogen_vcov_tbl, aes(x = trt, y = emmean, fill = trt)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\") + \n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd), \n                width = 0.2) + \n  labs(x = \"\", \n       y = \"Nitrat-Konzentration \\n im Tannensaft [mg/L]\") +\n  ylim(0, 250) +\n  theme(legend.position = \"none\") + \n  scale_fill_okabeito() + \n  geom_text(aes(label = .group, y = emmean + sd + 10))\n\n\n\n\n\n\n\nAbbildung 16.59— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion sandwich::vcovHAC im obigen Modell berücksichtigt.\n\n\n\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_barplot.png\", width = 5, height = 3)\n\n\n\n\n\n\n\n\n\n\nZerforschen: Zweifaktorieller Barplot mit emmeans und compact letter display\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns einen zweifaktoriellen Barplot anschauen. Wir haben hier ein Säulendiagramm mit compact letter display vorliegen. Daher brauchen wir eigentlich gar nicht so viele Zahlen. Für jede der vier Zeitpunkte und der Kontrolle jeweils einmal einen Mittelwert für die Höhe der Säule sowie einmal die Standardabweichung. Da wir hier aber noch mit emmeans() eine Gruppenvergleich rechnen wollen, brauchen wir mehr Beobachtungen. Wir erschaffen uns also fünf Beobachtungen je Zeit/Jod-Kombination. Für eine detaillierte Betrachtung der Erstellung der Abbildung schauen einmal in das Kapitel zum Barplot oder Balkendiagramm oder Säulendiagramm.\n\n\n\n\n\n\nAbbildung 16.60— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit den zwei Faktoren Zeit und die Iodine Form.\n\n\n\nAls erstes brauchen wir die Daten. Die Daten habe ich mir in dem Datensatz zerforschen_barplot_2fac_target_emmeans.xlsx selber ausgedacht. Ich habe einfach die obige Abbildung genommen und den Mittelwert abgeschätzt. Dann habe ich die fünf Werte alle um den Mittelwert streuen lassen. Ich brauche hier eigentlich mehr als fünf Werte, sonst kriegen wir bei emmeans() und der Interaktion im gls()-Modell Probleme, aber da gibt es dann bei kleinen Fallzahlen noch ein Workaround. Bitte nicht mit weniger als fünf Beobachtungen versuchen, dann wird es schwierig mit der Konsistenz der Schätzer aus dem Modell.\nAch, und ganz wichtig. Wir entfernen die Kontrolle, da wir die Kontrolle nur mit einer Iodid-Stufe gemessen haben. Dann können wir weder die Interaktion rechnen, noch anständig eine Interpretation durchführen.\n\nbarplot_tbl &lt;- read_excel(\"data/zerforschen_barplot_2fac_target_emmeans.xlsx\")  |&gt;  \n  mutate(time = factor(time, \n                       levels = c(\"ctrl\", \"7\", \"11\", \"15\", \"19\"),\n                       labels = c(\"Contr.\", \"07:00\", \"11:00\", \"15:00\", \"19:00\")),\n         type = as_factor(type)) |&gt; \n  filter(time != \"Contr.\")\nbarplot_tbl \n\n# A tibble: 40 × 3\n   time  type  iodine\n   &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1 07:00 KIO3      50\n 2 07:00 KIO3      55\n 3 07:00 KIO3      60\n 4 07:00 KIO3      52\n 5 07:00 KIO3      62\n 6 07:00 KI        97\n 7 07:00 KI        90\n 8 07:00 KI        83\n 9 07:00 KI        81\n10 07:00 KI        98\n# ℹ 30 more rows\n\n\nIm Folgenden sparen wir uns den Aufruf mit group_by() den du aus dem Kapitel zum Barplot schon kennst. Wir machen das alles zusammen in der Funktion emmeans() aus dem gleichnamigen R Paket. Der Vorteil ist, dass wir dann auch gleich die Gruppenvergleiche und auch das compact letter display erhalten. Einzig die Standardabweichung \\(s\\) wird uns nicht wiedergegeben sondern der Standardfehler \\(SE\\). Da aber folgernder Zusammenhang vorliegt, können wir gleich den Standardfehler in die Standardabweichung umrechnen.\n\\[\nSE = \\cfrac{s}{\\sqrt{n}}\n\\]\nWir rechnen also gleich einfach den Standardfehler \\(SE\\) mal der \\(\\sqrt{n}\\) um dann die Standardabweichung zu erhalten. In unserem Fall ist \\(n=5\\) nämlich die Anzahl Beobachtungen je Gruppe. Wenn du mal etwas unterschiedliche Anzahlen hast, dann kannst du auch einfach den Mittelwert der Fallzahl der Gruppen nehmen. Da überfahren wir zwar einen statistischen Engel, aber der Genauigkeit ist genüge getan.\nIn den beiden Tabs siehst du jetzt einmal die Modellierung unter der Annahme der Varianzhomogenität mit der Funktion lm() und einmal die Modellierung unter der Annahme der Varianzheterogenität mit der Funktion gls() aus dem R Paket nlme. Wie immer lässt sich an Boxplots visuell überprüfen, ob wir Homogenität oder Heterogenität vorliegen haben. Oder aber du schaust nochmal in das Kapitel Der Pre-Test oder Vortest, wo du mehr erfährst.\nWenn du jeden Boxplot miteinander vergleichen willst, dann musst du in dem Code emmeans(~ time * type) setzen. Dann berechnet dir emmeans für jede Faktorkombination einen paarweisen Vergleich.\n\nVarianzhomogenitätVarianzheterogenität\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim(). Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen. Wenn du alles mit allem Vergleichen willst, dann setze bitte emmeans(~ time * type).\n\nemmeans_homogen_tbl &lt;- lm(iodine ~ time + type + time:type, data = barplot_tbl) |&gt;\n  emmeans(~ time | type) |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(5)) \nemmeans_homogen_tbl\n\n# A tibble: 8 × 9\n  time  type  emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 07:00 KIO3    55.8  5.58    32     44.4     67.2 a       12.5\n2 11:00 KIO3    75.2  5.58    32     63.8     86.6 b       12.5\n3 15:00 KIO3    81    5.58    32     69.6     92.4 b       12.5\n4 19:00 KIO3    84.2  5.58    32     72.8     95.6 b       12.5\n5 07:00 KI      89.8  5.58    32     78.4    101.  a       12.5\n6 19:00 KI      90    5.58    32     78.6    101.  a       12.5\n7 15:00 KI     124    5.58    32    113.     135.  b       12.5\n8 11:00 KI     152    5.58    32    141.     163.  c       12.5\n\n\nIn dem Objekt emmeans_homogen_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. Wie dir vielleicht auffällt sind alle Standardfehler und damit alle Standardabweichungen für alle Gruppen gleich, das war ja auch unsere Annahme mit der Varianzhomogenität.\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim(). Da wir hier etwas Probleme mit der Fallzahl haben, nutzen wir die Option mode = \"appx-satterthwaite\" um dennoch ein vollwertiges, angepasstes Modell zu erhalten. Du kannst die Option auch erstmal entfernen und schauen, ob es mit deinen Daten auch so klappt. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen. Wenn du alles mit allem Vergleichen willst, dann setze bitte emmeans(~ time * type).\n\nemmeans_hetrogen_tbl &lt;- gls(iodine ~ time + type + time:type, data = barplot_tbl, \n                            weights = varIdent(form =  ~ 1 | time*type)) |&gt;\n  emmeans(~ time | type, mode = \"appx-satterthwaite\") |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(5)) \nemmeans_hetrogen_tbl\n\n# A tibble: 8 × 9\n  time  type  emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 07:00 KIO3    55.8  2.29  4.02     49.5     62.1 a       5.12\n2 11:00 KIO3    75.2  1.50  3.93     71.0     79.4 b       3.35\n3 15:00 KIO3    81    3.30  3.97     71.8     90.2 b       7.38\n4 19:00 KIO3    84.2  7.88  4.01     62.3    106.  b      17.6 \n5 07:00 KI      89.8  3.48  4.03     80.2     99.4 a       7.79\n6 19:00 KI      90    4.31  3.97     78.0    102.  a       9.64\n7 15:00 KI     124    7.48  4.01    103.     145.  b      16.7 \n8 11:00 KI     152    9.03  3.99    127.     177.  c      20.2 \n\n\nIn dem Objekt emmeans_hetrogen_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. In diesem Fall hier sind die Standardfehler und damit auch die Standardabweichungen nicht alle gleich, wir haben ja für jede Gruppe eine eigene Standardabweichung angenommen. Die Varianzen sollten ja auch heterogen sein.\n\n\n\nDann bauen wir usn auch schon die Abbildung. Wir müssen am Anfang einmal scale_x_discrete() setzen, damit wir gleich den Zielbereich ganz hinten zeichnen können. Sonst ist der blaue Bereich im Vordergrund. Dann färben wir auch mal die Balken anders ein. Muss ja auch mal sein. Auch nutzen wir die Funktion geom_text() um das compact letter display gut zu setzten. Die \\(y\\)-Position berechnet sich aus dem Mittelwert emmean plus Standardabweichung sd innerhalb des geom_text(). Da wir hier die Kontrollgruppe entfernen mussten, habe ich dann nochmal den Zielbereich verschoben und mit einem Pfeil ergänzt. Die beiden Tabs zeigen dir dann die Abbildungen für die beiden Annahmen der Varianzhomogenität oder Varianzheterogenität. Der Code ist der gleiche für beide Abbildungen, die Daten emmeans_homogen_tbl oder emmeans_hetrogen_tbl sind das Ausschlaggebende. Wie du sehen wirst, haben wir hier mal keinen Unterschied vorliegen.\n\nVarianzhomogenitätVarianzheterogenität\n\n\n\nggplot(data = emmeans_homogen_tbl, aes(x = time, y = emmean, fill = type)) +\n  theme_minimal() +\n  scale_x_discrete() +\n  annotate(\"rect\", xmin = 0.25, xmax = 4.75, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = \"darkblue\") +                        \n  annotate(\"text\", x = 0.5, y = 120, hjust = \"left\", label = \"Target area\", \n           size = 5) + \n  geom_curve(aes(x = 1.25, y = 120, xend = 1.7, yend = 105),   \n             colour = \"#555555\",   \n             size = 0.5,   \n             curvature = -0.2,  \n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_manual(name = \"Type\", values = c(\"darkgreen\", \"darkblue\")) + \n  theme(legend.position = c(0.1, 0.8),\n        legend.title = element_blank(), \n        legend.spacing.y = unit(0, \"mm\"), \n        panel.border = element_rect(colour = \"black\", fill=NA),\n        axis.text = element_text(colour = 1, size = 12),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) +\n  labs(x = \"Time of application [time of day]\",\n       y =  expression(Iodine~content~\"[\"*mu*g~I~100*g^'-1'~f*.*m*.*\"]\")) +\n  scale_y_continuous(breaks = c(0, 50, 100, 150, 200),\n                     limits = c(0, 200)) +\n  geom_text(aes(label = .group, y = emmean + sd + 2),  \n            position = position_dodge(width = 0.9), vjust = -0.25) \n\n\n\n\n\n\n\nAbbildung 16.61— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\n\n\n\n\n\n\n\n\nggplot(data = emmeans_hetrogen_tbl, aes(x = time, y = emmean, fill = type)) +\n  theme_minimal() +\n  scale_x_discrete() +\n  annotate(\"rect\", xmin = 0.25, xmax = 4.75, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = \"darkblue\") +                        \n  annotate(\"text\", x = 0.5, y = 120, hjust = \"left\", label = \"Target area\", \n           size = 5) + \n  geom_curve(aes(x = 1.25, y = 120, xend = 1.7, yend = 105),   \n             colour = \"#555555\",   \n             size = 0.5,   \n             curvature = -0.2,  \n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_manual(name = \"Type\", values = c(\"darkgreen\", \"darkblue\")) + \n  theme(legend.position = c(0.1, 0.8),\n        legend.title = element_blank(), \n        legend.spacing.y = unit(0, \"mm\"), \n        panel.border = element_rect(colour = \"black\", fill=NA),\n        axis.text = element_text(colour = 1, size = 12),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) +\n  labs(x = \"Time of application [time of day]\",\n       y =  expression(Iodine~content~\"[\"*mu*g~I~100*g^'-1'~f*.*m*.*\"]\")) +\n  scale_y_continuous(breaks = c(0, 50, 100, 150, 200),\n                     limits = c(0, 200)) +\n  geom_text(aes(label = .group, y = emmean + sd + 2),  \n            position = position_dodge(width = 0.9), vjust = -0.25) \n\n\n\n\n\n\n\nAbbildung 16.62— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\n\n\n\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_barplot.png\", width = 5, height = 3)",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Visualisierung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-ggplot.html#referenzen",
    "href": "eda-ggplot.html#referenzen",
    "title": "16  Visualisierung von Daten",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 16.1— “The reason to avoid pie charts” Quelle: wumo.com\nAbbildung 16.2— Leere ggplot() Leinwand mit den Spalten animal und jump_length aus dem Datensatz flea_dog_cat_tbl.\nAbbildung 16.3— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein simples Säulendiagramm mit sehr für Farbblinde ungünstigen Farben. Es sind die Mittelwerte sowie die Standardabweichung durch die Fehlerbalken dargestellt.\nAbbildung 16.4— Die Abbildung des Säulendiagramms in ggplot nachgebaut.\nAbbildung 16.5— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit zwei Faktoren Zeit und die Iodine Form.\nAbbildung 16.6— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen.\nAbbildung 16.7— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen, dafür müssen wir usn aber nochmal ein Positionsdatensatz bauen.\nAbbildung 16.8— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein zweifaktorielles Balkendiagramm mit einem Zielbereich und compact letter display\nAbbildung 16.9— Die Abbildung des Balkendiagramms in ggplot nachgebaut. Ein extra Zielbereich ist definiert sowie die Legende in die Abbildung integriert.\nAbbildung 16.10— Die Abbildung des Balkendiagramms in ggplot nachgebaut. Durch den Boxplot erhalten wir auch untere Grenzen, was bei der Frage, ob wir in einem Zielbereich sind, viel sinnvoller ist, als ein Balkendiagramm. Eine höhere Fallzahl als \\(n=3\\) würde die Boxplots schöner machen.\nAbbildung 16.11— Ein Boxplot der die statistischen Maßzahlen Median und Quartile visualisiert. Die Box wird aus dem IQR gebildet. Der Median wird als Strich in der Box gezeigt. Die Schnurrhaare sind das 1.5 fache des IQR. Punkte die außerhalb der Schnurrhaare liegen werden als einzele Punkte dargestellt.\nAbbildung 16.12— An 39 Hunden wurde die Anzahl an Flöhen gezählt.\nAbbildung 16.13— An 39 Hunden wurde die Anzahl an Flöhen gezählt zusammen mit den einzelnen Beobachtungen dargestellt.\nAbbildung 16.14— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein zweifaktorieller Barplot mit Mittelwerten über dem Faktor auf der \\(x\\)-Achse.\nAbbildung 16.15— Die Abbildung des Säulendiagramms in ggplot nachgebaut.\nAbbildung 16.16— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut.\nAbbildung 16.17— Ursprüngliche Abbildung, die nachgebaut werden soll. Insgesamt vier Outcomes sollen für zwei Behandlungen ausgewertet werden. Das praktische ist hier, dass wir es nur mit einem einfaktoriellen Design zu tun haben.\nAbbildung 16.18— Nachbau der Abbildung mit der Funktion facte_wrap() mit Boxplots und dem Mittelwert. Neben dem Mittelwert finden sich das compact letter display. Auf eine Einfärbung nach der Behandlung wurde verzichtet um die Abbildung nicht noch mehr zu überladen.\nAbbildung 16.19— Nachbau der Abbildung mit dem R Paket {patchwork} mit Boxplots und dem Mittelwert. Neben dem Mittelwert finden sich das compact letter display bei dem ersten Plot. Auf eine Einfärbung nach der Behanldung wurde verzichtet um die Abbildung nicht noch mehr zu überladen.\nAbbildung 16.20— Die Anzahl von Flöhen auf 39 Hunden. Jeder Punkt entspricht einem Hund und der entsprechenden Anzahl an Flöhen auf dem Hund.\nAbbildung 16.21— Histogramm der Anzahl von Flöhen auf 39 Hunden.\nAbbildung 16.22— Histogramm des Gewichts von Flöhen auf 39 Hunden.\nAbbildung 16.23— Zusammenhang zwischen den einzelnen Beobachtungen und der Höhe der einzelnen Balken am Beispiel von acht Hunden.\nAbbildung 16.24 (a)— Histogramm\nAbbildung 16.24 (b)— Densityplot\nAbbildung 16.25— Zusammenhang zwischen der Sprungweite in [cm] und Gewicht der Flöhe. Jeder Punkt stellt eine Beobachtung dar.\nAbbildung 16.26— Zusammenhang zwischen der Sprungweite in [cm] und Gewicht der Flöhe. Jeder Punkt stellt eine Beobachtung dar. Eine eigene Geradengleichung wurde durch geom_function() ergänzt.\nAbbildung 16.27— Ursprüngliche Abbildung, die nachgebaut werden soll. Eine simple lineare Regression mit Bestimmtheitsmaß \\(R^2\\) für die Gerade durch die Punkte.\nAbbildung 16.28— Visualisierung der simplen linearen Regression mit einem Pfeil sowie den Informationen zu der Geradengleichung und dem Bestimmtheitsmaß \\(R^2\\).\nAbbildung 16.29— Ursprüngliche Abbildung, die nachgebaut werden soll. Zwei lineare Regressionen mit den jeweiligen Regressionsgleichungen.\nAbbildung 16.30— Einmal die einfache Abbildung der linearen Regression in ggplot für Basilikum nachgebaut. Beachte die Funktion filter(), die den jeweiligen Datensatz für die beiden Kräuter erzeugt.\nAbbildung 16.31— Einmal die einfache Abbildung der linearen Regression in ggplot für Oregano nachgebaut. Beachte die Funktion filter(), die den jeweiligen Datensatz für die beiden Kräuter erzeugt.\nAbbildung 16.32— Einmal die einfache Abbildung der linearen Regression in ggplot für Oregano nachgebaut. Beachte die Funktion facet_wrap(), die den jeweiligen Datensatz für die beiden Kräuter erzeugt.\nAbbildung 16.33— Einmal die einfache Abbildung der linearen Regression in ggplot nachgebaut. Die Abbildung A zeigt die Punkte und die Geradengleichung für das Basilikum. Die Abbildung B die entsprechenden Informationen für das Oregano. Die beiden Achsenbeschriftungen wurden gekürzt und die Informationen in den Titel übernommen.\nAbbildung 16.34— Der Dotplot für die Anzahl der Flöhe für die beiden Tierarten Hund und Katze.\nAbbildung 16.35— Der Dotplot für die Anzahl der Flöhe für die beiden Tierarten Hund und Katze. Die schwarze Linie stelt den Median für die beiden Tierarten dar.\nAbbildung 16.36 (a)— Alter nach Geschlecht\nAbbildung 16.36 (b)— Körpergröße nach Geschlecht\nAbbildung 16.37 (a)— Alter nach Geschlecht\nAbbildung 16.37 (b)— Körpergröße nach Geschlecht\nAbbildung 16.38— Der Violinplot die Körpergröße aufgeteilt nach Geschlecht als die simpelste Art der Darstellung mit einem Violinplot. Ergänzt noch durch den Mittelwert plusminus der Standardabweichung.\nAbbildung 16.39— Der Violinplot die Körpergröße aufgeteilt nach Geschlecht als die simpelste Art der Darstellung mit einem Violinplot. Ergänzt noch durch den Mittelwert plusminus der Standardabweichung. Hier müssen wir aber die Darstellung auf der \\(x\\)-Achse um \\(0.1\\) etwas verschieben.\nAbbildung 16.40— Der Violinplot die Körpergröße aufgeteilt nach Geschlecht als die simpelste Art der Darstellung mit einem Violinplot. Ergänzt noch durch den Mittelwert plusminus der Standardabweichung sowie den einzelnen Beobachtungen aus einem Dotplot.\nAbbildung 16.41— Der Violinplot die Körpergröße aufgeteilt nach Geschlecht als die simpelste Art der Darstellung mit einem Violinplot. Ergänzt noch durch den Mittelwert plusminus der Standardabweichung sowie den einzelnen Beobachtungen aus einem Beeswarm.\nAbbildung 16.42— Visualisierung einer 2x2 Tabelle als Mosaic Plot. Die unterschiedlich großen Flächen geben die Verhältnisse per Spalte wieder.\nAbbildung 16.43— Beispielhafte Abbildung mit Titel und geänderter Achsenbeschrittung\nAbbildung 16.44— Zusammenhang zwischen dem Eisengehalt und -höhe im Boden und dem Eisengehalt in Salat. Zusätzlich ergänzt eine Regressiongleichung und ein ausgedachtes Bestimmtheitsmaß.\nAbbildung 16.45— Beispielhafte Abbildung der Okabe-Ito Farbpalette für Boxplots.\nAbbildung 16.46— Beispielhafte Abbildung der Okabe-Ito Farbpalette für Punkte.\nAbbildung 16.47— Farbpalette nach dem Okabe-Ito-Schema ebenfalls für farbblinde Personen erstellt.\nAbbildung 16.48— Farbpalette nach dem Duke-Schema ebenfalls für farbblinde Personen erstellt.\nAbbildung 16.49— Beispielhafte Abbildung der zweier Plots nebeneinander.\nAbbildung 16.50— Beispielhafte Abbildung der Barplots mit gebrochener \\(y\\)-Achse. Die Fehlerbalken wurden aus Gründen der besseren Darstellung der zerbrochenen \\(y\\)-Achse entfernt.\nAbbildung 16.51— Ursprüngliche Abbildung, die nachgebaut werden soll. Zwei nicht-lineare Regession laufen durch Mittelwert plus/minus Standardabweichung. Im Weiteren sind die Regressionsgleichungen noch ergänzt sowie ein Zielbereich farblich hervorgehoben. Am Ende müssen dann die Achsen noch sauber beschriftet werden.\nAbbildung 16.52— Darstellung der beiden Regressionsgleichungen für \\(KI\\) und \\(KIO_3\\).\nAbbildung 16.53— Einmal die komplexe Abbildung der nicht-linearen Regression in ggplot nachgebaut. Am Ende wurde es dann doch noch eine Legende und keine Beschriftung.\nAbbildung 16.54— Ursprüngliche Abbildung, die nachgebaut werden soll. Zwei nicht-lineare Regession mit zwei \\(y\\)-Achsen.\nAbbildung 16.55— Einmal die komplexe Abbildung der nicht-linearen Regression in ggplot nachgebaut. Für beide Quellen Nadeln und Boden wurde jewiels eine eigene \\(y\\)-Achse erstellt. Die Regressionsgleichungen aus der Orginalabbildung entsprechen nicht den Werten hier in der Abbildung.\nAbbildung 16.56— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein simples Säulendiagramm mit sehr für Farbblinde ungünstigen Farben. Es sind die Mittelwerte sowie die Standardabweichung durch die Fehlerbalken dargestellt.\nAbbildung 16.57— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt.\nAbbildung 16.58— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion gls() im obigen Modell berücksichtigt.\nAbbildung 16.59— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion sandwich::vcovHAC im obigen Modell berücksichtigt.\nAbbildung 16.60— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit den zwei Faktoren Zeit und die Iodine Form.\nAbbildung 16.61— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\nAbbildung 16.62— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\n\n\n\nCumming G, Fidler F, Vaux DL. 2007. Error bars in experimental biology. The Journal of cell biology 177: 7–11.\n\n\nRiedel N, Schulz R, Kazezian V, Weissgerber T. 2022. Replacing bar graphs of continuous data with more informative graphics: are we making progress? Clinical Science 136: 1139–1156.\n\n\nTukey JW. 1962. The future of data analysis. The annals of mathematical statistics 33: 1–67.\n\n\nTukey JW, others. 1977. Exploratory data analysis. Reading, MA.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Visualisierung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html",
    "href": "eda-distribution.html",
    "title": "17  Verteilung von Daten",
    "section": "",
    "text": "17.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, see, readxl, ggbeeswarm, conflicted)\nconflicts_prefer(dplyr::filter)\n\n[conflicted] Will prefer dplyr::filter over any other package.\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html#daten",
    "href": "eda-distribution.html#daten",
    "title": "17  Verteilung von Daten",
    "section": "17.2 Daten",
    "text": "17.2 Daten\nDamit wir uns auch eine Verteilung anschauen können brauchen wir viele Beobachtungen. Wir haben das ja schon bei den Histogrammen gesehen, wenn wir ein aussagekräftiges Histogramm erstellen wollen, dann brauchen wir mehr als zwanzig Beobachtungen. Daher nehmen wir für dieses Kapitel einmal den Gummibärchendatensatz und schauen uns dort die Variablen gender, height, age ,count_bears und count_color einmal genauer an. Wie immer nutzen wir die Funktion select() um die Spalten zu selektieren. Abschließend verwandeln wir das Geschlecht gender und das module noch in einen Faktor.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  |&gt;\n  select(year, module, gender, height, age, count_bears, \n         count_color, most_liked ) |&gt; \n  mutate(gender = as_factor(gender),\n         module = as_factor(module)) |&gt; \n  na.omit()\n\nWir erhalten das Objekt gummi_tbl mit dem Datensatz in Tabelle 17.1 nochmal dargestellt. Wir brauchen nicht alle Spalten aus dem ursprünglichen Datensatz und somit ist die Tabelle etwas übersichtlicher.\n\n\n\n\nTabelle 17.1— Auszug aus den selektierten Daten zu den Gummibärchendaten.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmodule\ngender\nheight\nage\ncount_bears\ncount_color\nmost_liked\n\n\n\n\n2018\nFU Berlin\nm\n193\n35\n9\n3\nlightred\n\n\n2018\nFU Berlin\nw\n159\n21\n10\n5\nyellow\n\n\n2018\nFU Berlin\nw\n159\n21\n9\n6\nwhite\n\n\n2018\nFU Berlin\nw\n180\n36\n10\n5\nwhite\n\n\n2018\nFU Berlin\nm\n180\n22\n10\n6\nwhite\n\n\n2018\nFU Berlin\nm\n180\n22\n10\n5\ngreen\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n2024\nBiostatistik\nm\n193\n24\n9\n4\ndarkred\n\n\n2024\nBiostatistik\nm\n189\n27\n9\n5\nwhite\n\n\n2024\nBiostatistik\nm\n187\n24\n11\n6\ndarkred\n\n\n2024\nBiostatistik\nm\n182\n24\n10\n4\ngreen\n\n\n2024\nBiostatistik\nw\n170\n23\n9\n4\nwhite\n\n\n2024\nBiostatistik\nw\n180\n24\n10\n6\ngreen\n\n\n\n\n\n\n\n\nGibt es eigentlich eine typische weibliche Person oder männliche Person in den Daten? Ja, gibt es. Unsere meist vertretende Frau ist 158cm groß, 12 Jahre alt und mag am liebsten Gummibärchen der Farbe dunkelrot. Auf der anderen Seite ist unser meist vertretender Mann 190cm groß, 21 Jahre alt und mag am liebsten Gummibärchen der Farbe grün.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html#sec-normal",
    "href": "eda-distribution.html#sec-normal",
    "title": "17  Verteilung von Daten",
    "section": "17.3 Die Normalverteilung",
    "text": "17.3 Die Normalverteilung\nWir sprechen in der Statistik auch von Verteilungsfamilien. Daher schreiben wir in R auch family = gaussian, wenn wir sagen wollen, dass unsere Daten einer Normalverteilung entstammen.\nWenn wir von de Normalverteilung sprechen, dann schreiben wir ein \\(\\mathcal{N}\\) Symbol - also ein großes N mit Serifen. Die Normalverteilung sieht aus wie eine Glocke, deshalb wird die Normalverteilung auch Glockenkurve genannt. Im englischen Sprachgebrauch und auch in R nutzen wir dagegen die Bezeichnung nach dem “Entdecker” der Normalverteilung, Carl Friedrich Gauß (1777 - 1985). Wir nennen daher die Normalverteilung auch Gaussian-Verteilung.\nParameter sind Zahlen, die eine Verteilungskurve beschreiben.\nEine Normalverteilung wird ruch zwei Verteilungsparameter definiert. Eine Verteilung hat Parameter. Parameter sind die Eigenschaften einer Verteilung, die notwendig sind um eine Verteilung vollständig zu beschreiben. Im Falle der Normalverteilung brauchen wir zum einen den Mittelwert \\(\\bar{y}\\), der den höchsten Punkt unserer Glockenkurve beschreibt. Zum anderen brauchen wir auch die Standardabweichung \\(s^2_y\\), die die Ausbreitung oder Breite der Glockenkurve bestimmt. Wir beschreiben eine Normalverteilung für eine Stichprobe mit \\(\\bar{y}\\) und \\(s^2_y\\) wie folgt.\n\\[\n\\mathcal{N}(\\bar{y}, s^2_y)\n\\]\nOder mit mehr Details in folgender Form. Wir können hier Verallgemeinern und schreiben in der Grundgesamtheit mit \\(\\mu = \\bar{y}\\) und \\(\\sigma^2 = s^2_y\\). Das heißt, wenn wir unendlich viele Beobachtungen vorliegen hätten, dann wüssetn wir auch den wahren Mittelwert \\(\\mu\\) und die wahre Varianz \\(\\sigma^2\\) der Daten.\n\\[\nf(y \\mid\\mu,\\sigma^2)=\\cfrac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\cfrac{(y-\\mu)^2}{2\\sigma^2}}\\quad -\\infty&lt;y&lt;\\infty\n\\]\nIm Falle der Normalverteilung brauchen wir einen Paramter für den höchsten Punkt der Kurve, sowie einen Parameter für die Ausbreitung, also wie weit geht die Kurve nach links und nach rechts. Je nach \\(\\bar{y}\\) und \\(s^2_y\\) können wir verschiedenste Normalverteilungen vorliegen haben. Eine Sammlung von Verteilungen nennen wir auch Familie (eng. family).\nWir haben Varianzhomogenität vorliegen, wenn \\(s^2_{1} = s^2_{2} = s^2_{3}\\) sind. Wir haben Varianzheterogenität vorliegen, wenn \\(s^2_{1} \\neq s^2_{2} \\neq s^2_{3}\\) sind.\nIn Abbildung 17.1 sehen wir verschiedene Normalverteilungen mit unterschiedlichen Mittelwerten. In ?fig-normal-02-1 sehen wir eine Varianzhomogenität vorliegen, da die Varianzen in allen drei Normalverteilungen gleich sind. Wir können auch schreiben, dass \\(s^2_{1} = s^2_{2} = s^2_{3} = 2\\). In ?fig-normal-02-2 haben wir Varianzheterogenität vorliegen, da die Varianzen der Normalverteilungen ungleich sind. Wir können hier dann schreiben, dass \\(s^2_{1} = 6 \\neq s^2_{2} = 1 \\neq s^2_{3} = 3\\) sind. Häufig gehen statistische Verfahren davon aus, dass wir Varianzhomogenität über die Gruppen und daher auch die Normalverteilungen vorliegen haben. Konkret, wenn wir die Sprungweiten in[cm] von Hunde- und Katzenflöhen mit einander vergleichen wollen, dann gehen wir erstmal davon aus, dass die Mittelwerte verschieden sind, aber die Varianzen gleich sind.\n\n\n\n\n\n\n\n\nAbbildung 17.1— Histogramm verschiedener Normalverteilungen mit unterschiedlichen Mittelwerten. (A) Drei Normalverteilungen mit Varianzhomogenität. (B) Drei Normalverteilungen unter Varianzheterogenität.\n\n\n\n\n\nIn einer Normalverteilung liegen 68% der Werte innerhalb \\(\\bar{y}\\pm 1 \\cdot s_y\\) und 95% der Werte innerhalb \\(\\bar{y}\\pm 2 \\cdot s_y\\)\nWenn wir eine Normalverteilung vorliegen haben, dann liegen 68% der Werte plus/minus einer Standardabweichung vom Mittelwert. Ebenso liegen 95% der Werte plus/minus zwei Standabweichungen vom Mittelwert. Über 99% der Werte befinden sich innerhalb von drei Standardabweichungen vom Mittelwert. Diese Eigenschaft einer Normalverteilung können wir später noch nutzen um abzuschätzen, ob wir einen relevanten Gruppenunterschied vorliegen haben oder aber ob unsere Daten unnatürlich breit streuen.\nWir nutzen das Wort approximativ wenn wir sagen wollen, dass ein Outcome näherungsweise normalverteilt ist.\nSchauen wir uns die Normalverteilung einmal am Beispiel unserer Gummibärchendaten und der Körpergröße der Studierenden an. Wir färben das Histogramm nach dem Geschlecht ein. In Abbildung 17.2 sehen wir das Ergebnis einmal als Histogramm und einmal als Densityplot dargestellt. Wir können annehmen, dass die Größe approximativ normalverteilt ist.\n\n\n\n\n\n\n\n\nAbbildung 17.2— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Die Farben repräsentieren die jeweiligen Geschlechter. Die Männer sind blau und die Frauen in lila dargestellt. (A) Histogramm. (B) Densityplot.\n\n\n\n\n\n\n\n\n\n\n\nGibt es einen signifikanten Unterschied für die Körpergröße?\n\n\n\n\n\nWenn wir testen wollen, ob es einen signifikanten Unterschied zwischen der Körpergröße zwischen Männern und Frauen gibt, dann können wir die Funktion t.test() nutzen, die zwei Mittelwerte einer Normalverteilung vergleicht.\n\nt.test(height ~ gender, data = gummi_tbl)\n\n\n    Welch Two Sample t-test\n\ndata:  height by gender\nt = 25.913, df = 696.49, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group m and group w is not equal to 0\n95 percent confidence interval:\n 13.68715 15.93130\nsample estimates:\nmean in group m mean in group w \n       183.6489        168.8397 \n\n\nMehr gibt es dann in dem Kapitel zum t-test. Dort findest du dann auch mehr Informationen zu der Interpretation des Ergebnisses.\n\n\n\nWir können die Funktion rnorm() nutzen um uns zufällige Zahlen aus der Normalverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert mean = und die gewünschte Standardabweichung mit sd = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rnorm() mit zehn Werten.\n\nrnorm(n = 10, mean = 5, sd = 2) |&gt; round(2)\n\n [1] 6.97 6.24 5.59 5.27 6.30 4.91 4.13 4.92 3.67 4.42\n\n\nDu kannst ja mal den Mittelwert und die Standardabweichung der zehn Zahlen ausrechnen. Da wir es hier mit einer Stichprobe mit zehn Beobachtungen zu tun haben, wird der Mittelwert \\(\\bar{y}\\) und die Standardabweichung \\(s_y\\) sich von den vorher definierten Mittelwert \\(\\mu_y = 5\\) und Standardabweichung \\(\\sigma_y = 2\\) der Grundgesamtheit unterscheiden.\nWir können auch aus unseren Gummibärchendaten für die Körpergröße in [cm] jeweils den Mittelwert und die Standardabweichung getrennt für die Geschlechter berechnen und dann die theoretische Normalverteilung zeichenen. In ?fig-normal-03-2 und ?fig-normal-03-4 sehen wir die Verteilung der theoretischen Werte, wenn wir die Mittelwerte und die Standardabweichung aus den Verteilungen in ?fig-normal-03-1 schätzen. Spannderweise bildet sich den zufällig gezogenen Daten auch eine leichte Schulter bei der Verteilung der Körpergrößen. Auch \\(n = 699\\) vollständige Beobachtungen bedeuten nicht, dass wir eine perfekte Normalverteilung erhalten.\n\n\n\n\n\n\n\n\nAbbildung 17.3— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte. Einmal dargestellt als Histogramm und einmal als Densityplot. Die Farben repräsentieren die jeweiligen Geschlechter. Die Männer sind blau und die Frauen in lila dargestellt. (A & C) Verteilung der beobachteten Werte. (B & D) Verteilung der theoretischen Werte.\n\n\n\n\n\nIn der Abbildung 17.4 ist nochmal der Beeswarm für das Alter und die Körpergröße nach Geschlecht dargestellt. Manchmal sieht man in so einem Plot nochmal mehr als in einem reinen Histogramm oder Densityplot. Ich finde es jedenfalls spannend die Teilnehmerinnen vom Girl’s Day klar zu erkennen sowie die Teilnehmer:innen meiner Weiterbildungen. Auch spannend ist die Größenverteilung der Männer, die einen klaren Trend zu der Größe von 1.80 haben, aber dafür dann weniger zur Größe 1.79 oder 1.81 zu haben scheinen. Es ist immer wieder spannend was du dann in den unterschiedlichen Abbildungen erkennen kannst.\n\n\n\n\n\n\n\n\nAbbildung 17.4— Der Beeswarm ist ein Dotplot für eine große Anzahl an Beobachtungen. Hier schauen wir uns einmal das Alter und die Körpergröße aufgeteilt nach Geschlecht an. Bei sehr vielen Beobachtungen kommt dann auch ein Bienenschwarm an die Grenze. (A) Alter nach Geschlecht. (B) Körpergröße nach Geschlecht.\n\n\n\n\n\nDann schauen wir uns einmal in der Abbildung 17.5 die verschiedenen Kurse im Bezug auf die Körpergröße aufgeteilt nach Männern und Frauen an. Gibt es in den einzelnen Veranstaltungen einen Unterschied in der Verteilung? Oder sehen die Verteilungen der Körpergrößen eher gleich aus? Dann habe ich noch die durchschnittlichen Körpergrößen aus dem statistischen Bundesamt mit \\(167.5cm\\) für 20 bis 25 jährige Frauen sowie \\(181.4cm\\) für 20 bis 25 jährige Männer als gestrichelte Linie ergänzt. Die durchgezogene Linie stellt die Körpergrößen in unseren Daten über alle Beobachtungen dar. Wir haben hier anscheinend im Schnitt etwas größere Personen als zu erwarten wäre.\n\n\n\n\n\n\n\n\nAbbildung 17.5— Densityplot der Körpergrößen für Männer und Frauen aufgeteilt nach einer Auswahl von Lehrveranstaltungen. Die schwarze Linie zeigt den aktuellen Durchschnitt der Körpergröße über alle Beobachtungen. Die gestrichelte Linie zeigt die durchschnittiche Körpergröße nach dem statistsischen Bundesamt mit \\(167.5cm\\) für 20 bis 25 jährige Frauen sowie \\(181.4cm\\) für 20 bis 25 jährige Männer.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html#die-standardnormalverteilung",
    "href": "eda-distribution.html#die-standardnormalverteilung",
    "title": "17  Verteilung von Daten",
    "section": "17.4 Die Standardnormalverteilung",
    "text": "17.4 Die Standardnormalverteilung\nEs gibt viele Normalverteilungen. Eiegntlich gibt es unednlich viele Normalverteilunge, da wir für die Parameter Mittelwert \\(\\bar{y}\\) und die Standardabweichung \\(s_y\\) beliebige Zahlen einsetzen können. Aber es gibt eine besondere Normalverteilung, so dass diese Verteilung einen eigenen Namen hat. Wir sprechen von der Standardnormalverteilung, wenn der Mittelwert gleich Null ist und die Standardabweichung gleich Eins. Du siehst hier nochmal die Standardnormalverteilung ausgeschrieben.\n\\[\n\\mathcal{N}(0, 1)\n\\]\nFolgende Eigenschaften sind der Standardnormalverteilung gegeben. Die Standardnormalverteilung hat eine Fläche von \\(A = 1\\) unter der Kurve. Darüber hinaus liegen 95% der Werte zwischen \\(\\approx -2\\) und \\(\\approx 2\\). Die einzelnen Werte einer Standardnormalverteilung nennen wir \\(z\\)-Werte. Wenn wir eine beliebige Normalverteilung in eine Standardnormalverteilung überführen wollen so machen wir die Umwandlung mit der \\(z\\)-Transformation. Und jetzt fahren wir wieder in die Doppeldeutigkeit in R.\n\n\n\n\n\n\nAbbildung 17.6— Darstellung von dem Zusammenhang von pnorm(q = 1.96) und qnorm(p = 0.025). Mit der Option lower.tail bestimmen wir auf welche Seite der Verteilung wir sein wollen.\n\n\n\nIn Abbildung 17.6 haben wir eine Standardnormalverteilung gegeben. Können jetzt verschiedene Werte auf der \\(x\\)-Achse und die Flächen links und rechts von diesen Werten berechnen. Wir nutzen die Funktion pnorm() wenn wir die Fläche rechts oder links von einem Wert \\(q\\) berechnen wollen.\n\npnorm(q = 1.96, mean = 0, sd = 1, lower.tail = FALSE) |&gt; \n  round(3)\n\n[1] 0.025\n\n\nWir berechen die Fläche links von \\(q\\) und damit auch die Wahrscheinlichkeit \\(Pr(X \\leq q)\\) mit lower.tail = TRUE. Warum ist die Fläche jetzt eine Wahrscheinlichkeit? Wir haben unter der Kurve der Standardnormalverteilung eine Fläche von \\(A = 1\\). Damit ist jede Fläche auch gleich einer Wahrscheinlichkeit. Wenn wir an der Fläche rechts von \\(q\\) interessiert sind und damit auch an der Wahrscheinlichkeit \\(Pr(X &gt; q)\\) nutzen wir die Option lower.tail = FALSE. Das ist erstmal immer etwas verwirrend, aber schau dir den Zusammenhang nochmal in der Abbildung 17.6 an. Wir brauchen diese Idee von der Fläche ist auch gleich Wahrscheinlichkeit im Kapitel 20 zum statistischen Testen.\nWir können die Berechnung von \\(q\\) zu \\(p\\) auch umdrehen. Wir geben eine Fläche vor und wollen wissen wie der Wert auf der x-Achse zu der entsprechenden Fläche ist. In diesem Fall will ich die Werte zu den Flächen von \\(p = 0.025\\) und \\(p = 0.05\\). Da wir lower.tail = FALSE ausgewählt haben, sind wir auf der rechten Seite der Verteilung.\n\nqnorm(p = c(0.025, 0.05), mean = 0, sd = 1, lower.tail = FALSE) |&gt; \n  round(3)\n\n[1] 1.960 1.645\n\n\nUnd hier einmal als Gegenprobe mit der Option lower.tail = TRUE. Wir springen dann damit auf die linke Seite der Verteilung und wie zu erwarten erhlaten wir dann auch den negativen Wert für die Fläche von \\(p = 0.05\\).\n\nqnorm(p = 0.05, mean = 0, sd = 1, lower.tail = TRUE) |&gt; \n  round(3)\n\n[1] -1.645\n\n\nDie ganzen Berechnungen funktionieren natürlich auch, wenn wir nicht die Fläche \\(A=1\\) unterhalb der Standardnormalverteilung hätten. Aber wir nutzen hier eben den Zusammenhang von Fläche zu Wahrscheinlichkeit um mit der Verteilung zu rechnen und Wahrscheinlichkeiten abzuschätzen.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html#sec-t-dist",
    "href": "eda-distribution.html#sec-t-dist",
    "title": "17  Verteilung von Daten",
    "section": "17.5 Die t-Verteilung",
    "text": "17.5 Die t-Verteilung\nDie t-Verteilung ist eine Abwandlung der Standardnormalverteilung. Wir haben wieder eine Fläche \\(A = 1\\) unter der Verteilungskurve. Wir benötigen die t-Verteilung, als eine künstliche Verteilung, im Kapitel 27 zum statistischen Testen mit dem t-Test. Wir bezeichnen die t-Verteilung als eine künstliche Verteilung, da wir in der Biologie nichts beobachten können, was t-verteilt ist. Wir nutzen die t-Verteilung nur im statistischen Kontext und in diesem Kontekt nur um uns klar zu machen wie statistisches Testen konzeptionell funktioniert. Anwenden werden wir die Verteilung nicht.\nDer Unterschied ist die Form der t-Verteilung. Wir geben mit der Option df = die Freiheitsgrade der Verteilung an. Hier soll es reichen, dass mit \\(\\lim_{df \\to \\infty}\\) sich die t-Verteilung der Standardnormalverteilung fast gleicht. Bei niedrigeren Freiheitsgraden ist die t-Verteilung nicht mehr so hoch und daher sind die Verteilungsenden weiter nach außen geschoben. Die t-Verteilung ist gestaucht wie wir in Abbildung 17.7 etwas überspitzt gezeichnet sehen. Die Freiheitsgrade hängen direkt an der beobachteten Fallzahl mit \\(df = n_1 + n_2 - 2\\).\n\n\n\n\n\n\nAbbildung 17.7— Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und damit die Fallzahl, desto näher kommt die t-Verteilung einer Normalverteilung nahe.\n\n\n\nWie auch bei der Standardnormalverteilung gilt folgender Zusammenhang, wenn wir die Flächen anhand eines gegebenen t-Wertes berechnen wollen. Wenn wir die Fläche links von dem t-Wert berechnen wollen, also die Wahrscheinlichkeit \\(Pr(X \\leq t)\\), dann nutzen wir die Option lower.tail = TRUE. Wenn wir die Fläche auf der rechten Seite von unserem t-Wert berechnen wollen, dann nutzen wir mit \\(Pr(X &gt; t)\\) die Option lower.tail = FALSE. In der Funktion pt() ist das q= als t= zu lesen. Das macht das Verständnis vielleicht leichter.\n\npt(q = 2.571, df = 5, lower.tail = FALSE) |&gt; \n  round(3)\n\n[1] 0.025\n\n\nNeben der Berechnung der Wahrscheinlichkeit rechts und links eines gegebenen Wertes \\(t\\) können wir auch \\(t\\) berechnen, wenn wir eine Fläche vorgeben. Das kann uns dann die Funktion qt() liefern. Wir sehen, dass mit steigender Fallzahl und damit steigenden Freiheitsgrad sich der berechnete Wert sich dem Wert der Standardnormalverteilung von \\(1.96\\) für \\(p = 0.05\\) annähert.\n\nqt(p = c(0.025), df = c(5, 10, 20, 100, 1000), lower.tail = FALSE) |&gt; \n  round(3)\n\n[1] 2.571 2.228 2.086 1.984 1.962\n\n\nWir haben gelernt, dass der Zusammenhang zwischen der Standardnormalverteilung und der t-Verteilung ziemlich stark ist. Nutzen werden wir die t-Verteilung aber nur im Rahmen des statistischen Testens.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html#sec-poisson",
    "href": "eda-distribution.html#sec-poisson",
    "title": "17  Verteilung von Daten",
    "section": "17.6 Die Poissonverteilung",
    "text": "17.6 Die Poissonverteilung\nEine weitere wichtige Verteilung ist die Poissonverteilung. Die Poissonverteilung ist eine diskrete Verteilung. Daher kommen nur ganze Zahlen vor. Damit bildet die Poissonverteilung die Zähldaten ab. Wenn wir also etwas Zählen, dann ist diese Variable mit den gezählten Ergebnissen poissonverteilt. Im Folgenden sehen wir die Poissonverteilung einmal dargestellt.\n\\[\n\\mathcal{Pois}(\\lambda)\n\\]\nOder mit mehr Details in folgender Form.\n\\[\nP_\\lambda (k) = \\frac{\\lambda^k}{k!}\\, \\mathrm{e}^{-\\lambda}\n\\]\nDie Poisson-Verteilung gibt dann die Wahrscheinlichkeit einer bestimmten Ereignisanzahl \\(k\\) im Einzelfall an, wenn die mittlere Ereignisrate \\(\\lambda\\) bekannt ist. Im Gegensatz zur Normalverteilung hat die Poissonverteilung nur einen Parameter. Den Lageparameter \\(\\lambda\\) ausgedrückt durch den griechischen Buchstaben Lambda. Eine Poissonverteilung mit \\(\\mathcal{Pois}(4)\\) hat den höchsten Punkt bei vier. Nun hat die Poissonverteilung hat mehrere Besonderheiten. Da die Poissonverteilung keinen Streuungsparameter hat, steigt mit dem \\(\\lambda\\) auch die Streuung. Daher haben Poissonverteilungen mit einem großen \\(\\lambda\\) auch eine große Streuung. ie Ausbreitung der Kurve ist eine Funktion von \\(\\lambda\\) und steigt mit \\(\\lambda\\) an. Du kannst diesen Zusammenhang in Abbildung 17.8 beobachten.\nDarüber hinaus kann eine Poissonverteilung nicht negativ werden. Es kann keine kleinere Zahl als die Null geben. Durch die diskreten Zahlen haben wir auch immer mal Lücken zwischen den Balken der Poissonverteilung. Das passiert besonders, wenn wir eine kleine Anzahl an Beobachtungen haben. Abschließend konvergiert die Poissonverteilung bei großen \\(\\lambda\\) hin zu einer Normalverteilung.\n\n\n\n\n\n\n\n\nAbbildung 17.8— Histogramm verschiedener Poissonverteilungen.\n\n\n\n\n\nSchauen wir uns nun einmal die Poissonverteilung im Beispiel an. In Abbildung 17.9 sehen wir die Histogramme der Anzahl an Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte. Da wir es hier mit Zähldaten zu tun haben, könnte es sich um eine Poissonverteilung handeln. Wie müssen uns nun die Frage stellen, ob die Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte wirklich eine zufällige Realistierung sind. Daher eine zufällige Stichprobe der Grundgesamtheit. Wir können diese Annahme überprüfen in dem wir die theoretischen Werte für die beiden Poissonverteilung mit \\(\\mathcal{Pois}(10)\\) und \\(\\mathcal{Pois}(5)\\) genieren.\n\n\n\n\n\n\n\n\nAbbildung 17.9— Histogramme der Anzahl an Gummibärchen und die Anzahl an Farben in einer Tüte. Es gibt nicht mehr als sechs Farben. (A) Anzahl an Bärchen. (B) Anzahl an Farben.\n\n\n\n\n\nWir können die Funktion rpois() nutzen um uns zufällige Zahlen aus der Poissonverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert lambda = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rpois() mit zehn Werten.\n\nrpois(n = 10, lambda = 5)\n\n [1] 3 5 4 5 3 3 9 3 5 5\n\n\nEs gibt neben der Poissonverteilung auch die negative Binomialverteilung sowie die Quasi-Poissonverteilung, die es erlauben einen Streuungsparameter für die Poissonverteilung zu schätzen.\nWir können nun auch aus unseren Gummibärchendaten für die Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte die theoretische Poissonverteilung berechnen. In Abbildung 17.10 sehen wir die Verteilung der beobachteten Werte für Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte und deren theoretischen Verteilung nach dem geschätzen \\(\\lambda = 10\\) und \\(\\lambda = 5\\). Wir sehen ganz klar, dass die beide Variablen keine Zufallsrealisierung sind. Zum einen haben wir das auch nicht erwartet, es gibt nicht mehr als sechs Farben und zum anderen ist zu vermuten, dass Haribo technisch in den Auswahlprozess eingreift. Wir haben auf jeden Fall eine sehr viel kleinere Streuung als bei einer klassischen Poissonverteilung anzunehmen wäre.\n\n\n\n\n\n\n\n\nAbbildung 17.10— Darstellung Anzahl an Bärchen und Anzahl an Farben. Es gibt nicht mehr als sechs Farben. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte. (A) Verteilung der beobachteten Anzahl an Bärchen. (B) Verteilung der theoretischen Anzahl an Bärchen. (C) Verteilung der beobachteten Anzahl an Farben. (D) Verteilung der theoretischen Anzahl an Farben.\n\n\n\n\n\nIn Abbildung 17.11 schauen wir uns nochmal an in wie weit sich die Füllung der Tütchen im Laufe der Jahre entwickelt hat. Die Daten werden ja schon seit 2018 erhoben. Wir schauen uns daher die Densityplot einmal aufgetrennt für die Jahre 2018 bis heute an. Das Jahr 2020 fehlt, da bedingt durch die Coronapandemie keine Präsenslehre stattfand. Wir sehen, dass sich die Verteilung anscheinend in dem Jahr 2022 langsam nach links zu weniger Bärchen in einer Tüte bewegt. Wir bleiben gespannt auf den weiteren Trend.\n\n\n\n\n\n\n\n\nAbbildung 17.11— Densityplot der Anzahl an Bärchen in einer Tüte aufgetrennt nach den Jahren der Erhebung. Das Jahr 2020 fehlt bedingt durch die Coronapandemie.\n\n\n\n\n\nIn Abbildung 17.12 betrachten wir die Verteilung der am meisten gemochten Gummibärchen aufgeteilt nach dem angegebenen Geschlecht im Vergleich zu den Gummibärchen in den Tütchen. Wir sehen, dass Haribo die Tütchen sehr gleichmäßig verteilt und auf die Geschmäcker keinerlei Rücksicht nimmt. Entweder weiß Haribo nichts von den Vorlieben seiner Käufer:innen oder aber es ist dann doch zu viel Aufwand die Produktion anzupassen.\n\n\n\n\n\n\n\n\nAbbildung 17.12— Histogramme der am liebsten gemochten Gummibärchchen im Vergleich zum Inhalt der Tütchen. (A) Anzahl am liebsten gemochten Gummibärchen aufgeteilt nach Geschlecht. (B) Anzahl der Gummibärchen aufgeteilt nach der Farbe.\n\n\n\n\n\n\n\n\n\n\n\nGibt es einen signifikanten Unterschied für weiße Gummibärchen?\n\n\n\n\n\nWenn wir testen wollen, ob es einen signifikanten Unterschied zwischen den Vorlieben zwischen Männern und Frauen für den Geschmack von weißen Gummibärchen gibt, dann können wir die Funktion prop.test() nutzen, die zwei Anteile vergleicht. Wir geben hier nur nicht die Anteile \\(p_1 = 37/86\\) und \\(p2 = 49/86\\) als Wahrscheinlichkeiten sondern eben als absolute Zahlen in die Funktion.\n\nprop.test(x = c(38, 51), n = c(89, 89))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(38, 51) out of c(89, 89)\nX-squared = 3.236, df = 1, p-value = 0.07204\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.30263327  0.01049844\nsample estimates:\n   prop 1    prop 2 \n0.4269663 0.5730337 \n\n\nMehr gibt es dann in dem Kapitel zum Vergleich zweier Anteile \\(p_1\\) und \\(p_2\\). Dort findest du dann auch Möglichkeiten alle Geschmacksrichtungen für die beiden Geschlechter zu verglichen.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html#sec-binom",
    "href": "eda-distribution.html#sec-binom",
    "title": "17  Verteilung von Daten",
    "section": "17.7 Die Binominalverteilung",
    "text": "17.7 Die Binominalverteilung\nDie Binomialverteilung wird uns vor allem später in den logistischen Regression wieder begegnen. An dieser Stelle ist es wichtig zu wissen, dass wir es bei der Binomialverteilung mit binären Ereignissen zu tun haben. Wir haben nur Erfolg oder nicht. Daher haben wir nur das Ergebnis \\(0/1\\) daher Null oder Eins. Dieses Ergebnis ist im Prinzip auch die Beschreibung eines Patienten, ob dieser krank oder nicht krank ist. Deshalb finden wir die Binomialverteilung auch häufig in einem medizinischen Kontext.\n\n\nEs gibt auch ein schönes Tutorial zur Binomial Distribution von David Arnold.\nSchauen wir uns die Formel für die Binomialverteilung einmal genauer an. Wichtig ist, dass wir etwas \\(n\\)-mal wiederholen und uns dann fragen, wie exakt \\(k\\)-oft haben wir Erfolg.\n\\[\nB(k\\mid p,n)=\n\\begin{cases}\n  \\binom nk p^k (1-p)^{n-k} &\\text{falls} \\quad k\\in\\left\\{0,1,\\dots,n\\right\\}\\\\\n  0            & \\text{sonst.}\n  \\end{cases}\n\\]\nmit\n\n\\(n\\) gleich der Anzahl an Versuchen (eng. trails)\n\\(k\\) gleich der Anzahl an Erfolgen\n\\(p\\) gleich der Wahrscheinlichkeit für einen Erfolg.\n\nBevor wir mit dem Beispiel beginnen können brauchen wir noch etwas mehr für die Berechnung der Formel. Wir brauchen noch für die Berechnung der Binomalverteilung den Binomialkoeffizienten \\(\\tbinom {n}{k}\\), den wir wie folgt bestimmen können. Dabei bedeutet das \\(!\\), dass wir eine Zahl aufmultiplizieren. Daher müssen wir für \\(4!\\) dann wie folgt rechnen \\(4! = 1 \\cdot 2 \\cdot 3 \\cdot 4 = 24\\).\n\\[\n\\binom nk = \\cfrac{n!}{k! \\cdot (n-k)!}\n\\]\nNehmen wir dafür einmal ein Beispiel mit 5 über 3 und schauen uns die Rechnung einmal an. Wir erhalten den Binomialkoeffizienten \\(\\tbinom {5}{3}\\) wie folgt.\n\\[\n\\binom 5 3 = \\frac{5!}{3! \\cdot (5-3)!} = \\frac{5!}{3! \\cdot 2!} = \\frac{1\\cdot 2\\cdot 3\\cdot 4\\cdot 5}{(1\\cdot 2\\cdot 3) \\cdot (1\\cdot 2)} = \\frac{4\\cdot 5}{1\\cdot 2} = 10\n\\]\nViele Taschenrechner können den Binomialkoeffizienten flott ausrechnen. Wenn wir keinen Taschenrechner haben, dann können wir auch das Pascalsche (oder Pascal’sche) Dreieck nutzen. Das Pascalsche Dreieck ist eine Form der grafischen Darstellung der Binomialkoeffizienten \\(\\tbinom {n}{k}\\). Wir sehen einmal in Abbildung 17.13 den Zusammenhang mit dem Binomialkoeffizienten dargestellt. Mit dem Pascalsche Dreieck können wir auch ohne Taschenrechner den Binomialkoeffizienten bestimmen.\n\n\n\n\n\n\nAbbildung 17.13— Darstellung des Pascalsche (oder Pascal’sche) Dreieckes im Zusammenhang zum Binomialkoeffizienten.\n\n\n\nSomit können wir auch einmal ein erweitertes Beispiel der Binomialverteilung rechnen. Was ist die Wahrscheinlichkeit bei \\(n = 5\\) Münzwürfen genau dann \\(k = 2\\) Erfolge zu erzielen, wenn die Münze fair ist und damit gilt \\(p = 0.5\\)?\n\\[\n\\begin{aligned}\nPr(Y = 3) &= \\binom {5}{3} 0.5^{3} (1-0.5)^{5-3} \\\\  \n&= 10 \\cdot 0.5^3 \\cdot 0.5^2 \\\\\n&= 0.31\n\\end{aligned}\n\\]\nWir immer können wir die ganze Rechnung dann auch in R durchführen. Dank der Funktion choose() können wir schnell den Binomialkoeffizienten berechnen. Der Rest ist dann nur noch das Einsetzen.\n\nchoose(5,3) * 0.5^3 * 0.5^2\n\n[1] 0.3125\n\n\nAuch hier geht es natürlich auch in R noch einen Schritt schneller. Leider heißt dann wieder alles anders. Wir wollen x = k = 3 Erfolge aus size = n = 5 Versuchen mit einer Erfolgswahrscheinlichkeit von prob = 0.5. Daran muss man sich dann gewöhnen, dass sich die Begrifflichkeiten dann doch immer mal wieder ändern.\n\ndbinom(x = 3, size = 5, prob = 0.5)\n\n[1] 0.3125\n\n\nWas wäre wenn wir jetzt die Wahrscheinlichkeit \\(Pr(Y \\leq 3)\\) berechnen wollen? Also nicht exakt die Wahrscheinlichkeit für \\(k=3\\) Erfolge sondern eben \\(k\\) Erfolge oder weniger \\(k \\leq 3\\). Dann müssen wir die Wahrscheinlichkeiten für \\(Pr(Y = 0)\\), \\(Pr(Y = 1)\\), \\(Pr(Y = 2 )\\) und \\(Pr(Y = 3)\\) berechnen und diese Wahrscheinlichkeiten aufaddieren.\n\ndbinom(0, 5, 0.5) + dbinom(1, 5, 0.5) + dbinom(2, 5, 0.5) + dbinom(3, 5, 0.5)\n\n[1] 0.8125\n\n\nOder wir rechen einfach die Fläche und damit die Wahrscheinlichkeit links von \\(k = 3\\) aus. Dafür haben wir dann die Funktion pbinom(). Es geht dann eben doch etwas flotter. Wie immer können wir dann über die Option lower.tail = entscheiden, auf welche Seite der Verteilung wir schauen wollen.\n\npbinom(3, 5, 0.5, lower.tail = TRUE)\n\n[1] 0.8125\n\n\nAngenommen, eine Münze wird so gewichtet, dass sie in 60 % der Fälle Kopf ergibt. Wie hoch ist die Wahrscheinlichkeit, dass Sie nach 50 Würfen 25 oder mehr Köpfe erhalten? Dafür können wir dann auch die Funktion pbinom() einmal nutzen. Da wir mehr wollen, also “größer als”, müssen wir rechts von dem berechneten Wert schauen, also auswählen, dass lower.tail = FALSE ist.\n\npbinom(25, 50, 0.6, lower.tail = FALSE)\n\n[1] 0.9021926\n\n\nZum Abschluss schauen wir nochmal in unseren Gummibärchendaten, wie dort ein Histogramm einer binären Variable mit nur zwei Ausprägungen aussehen würde. In Abbildung 17.14 sehen wir einmal das Geschlecht als Balkendiagramm dargestellt. Mehr gibt es zu diesem Diagramm erstmal nicht zu berichten. Bei einer Variable bei einem unbekannten \\(p\\) für eine der Kategorien, ist schwer etwas zu bewerten. Wir sehen aber, dass wir eine sehr schöne Gleichverteilung von den Geschlechtern in den Daten haben.\n\n\n\n\n\n\n\n\nAbbildung 17.14— Beispiel für eine Binomialverteilung anhand des Geschlechts. Die fehlenden Angaben wurden entfernt.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html#sec-uniform",
    "href": "eda-distribution.html#sec-uniform",
    "title": "17  Verteilung von Daten",
    "section": "17.8 Die Uniformverteilung",
    "text": "17.8 Die Uniformverteilung\nDie Gleichverteilung oder Uniformverteilung brauchen wir in der Statistik eher selten. Da wir aber hin und wieder mal auf die Gleichverteilung in technischen Prozessen stoßen, wollen wir uns die Gleichverteilung nochmal anschauen. Wenn wir eine Gleichverteilung vorliegen haben, dann sind alle Kategorien gleich häufig vertreten. Es ergibt sich dann folgende Verteilung als Plateau. Das Eintreten jedes Ereignisses ist gleich wahrscheinlich.\n\\[\nf(y)=\n\\begin{cases}\n  \\cfrac 1{b-a} & a \\le y \\le b\\\\\n  0            & \\text{sonst.}\n\\end{cases}\n\\]\n\n\n\n\n\n\nAbbildung 17.15— Darstellung der Uniformverteilung zwischen den beiden Punkten \\(a\\) und \\(b\\).\n\n\n\nDa gibt es auch sonst wenig mehr zu berichten. Nehmen wir daher nochmal ein technisches Beispiel aus unseren Gummibärchendaten. Wir würden je Farbe \\(1218\\) Gummibärchen erwarten. Warum ist das so? Wir haben insgesamt \\(7309\\) ausgezählt. Wenn jede der sechs Kategorien mit der gleichen Wahrscheinlichkeit auftritt, dann erwarten wir jeweils \\(1/6\\) von der Gesamtzahl. Wir erkennen, dass wir etwas zu wenig grüne Bärchen haben. Ebenso sind die hellroten Bärchen unterrepräsentiert. Dafür haben wir dann zwangsweise etwas mehr an gelben und orangen Gummibärchen. Dennoch würde ich hier von einer Gleichverteilung ausgehen.\n\n\n\n\n\n\n\n\nAbbildung 17.16— Beispiel für eine uniforme Verteilung anhand der Anzahl der Gummibärchen pro Tüte nach Farbe. (A) Verteilung der beobachteten Anzahl der Gummibärchen pro Tüte nach Farbe. (B) Verteilung der theoretischen Anzahl der Gummibärchen pro Tüte nach Farbe.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html#weitere-verteilungen",
    "href": "eda-distribution.html#weitere-verteilungen",
    "title": "17  Verteilung von Daten",
    "section": "17.9 Weitere Verteilungen",
    "text": "17.9 Weitere Verteilungen\n\n\nWir besuchen gerne die R Shiny App The distribution zoo um mehr über die verschiedenen Verteilungen und deren Parameter zu erfahren.\nWeitere Beispiele finden sich unter Basic Probability Distributions in R. Im Weiteren liefert Dormann (2013) eine gute Übersicht über verschiedene Verteilungen und deren Repräsentation in R. Das ist nur eine Auswahl an möglichen Verteilungen. Bitte hier nicht ins rabbit hole der Verteilungen gehen. Wir benötigen in unserer täglichen Arbeit nur einen kleinen Teil der Verteilungen. Es reicht, wenn du eine Vorstellungen der Verteilungen in diesem Kapitel hat.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-distribution.html#referenzen",
    "href": "eda-distribution.html#referenzen",
    "title": "17  Verteilung von Daten",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 17.1— Histogramm verschiedener Normalverteilungen mit unterschiedlichen Mittelwerten. (A) Drei Normalverteilungen mit Varianzhomogenität. (B) Drei Normalverteilungen unter Varianzheterogenität.\nAbbildung 17.2— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Die Farben repräsentieren die jeweiligen Geschlechter. Die Männer sind blau und die Frauen in lila dargestellt. (A) Histogramm. (B) Densityplot.\nAbbildung 17.3— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte. Einmal dargestellt als Histogramm und einmal als Densityplot. Die Farben repräsentieren die jeweiligen Geschlechter. Die Männer sind blau und die Frauen in lila dargestellt. (A & C) Verteilung der beobachteten Werte. (B & D) Verteilung der theoretischen Werte.\nAbbildung 17.4— Der Beeswarm ist ein Dotplot für eine große Anzahl an Beobachtungen. Hier schauen wir uns einmal das Alter und die Körpergröße aufgeteilt nach Geschlecht an. Bei sehr vielen Beobachtungen kommt dann auch ein Bienenschwarm an die Grenze. (A) Alter nach Geschlecht. (B) Körpergröße nach Geschlecht.\nAbbildung 17.5— Densityplot der Körpergrößen für Männer und Frauen aufgeteilt nach einer Auswahl von Lehrveranstaltungen. Die schwarze Linie zeigt den aktuellen Durchschnitt der Körpergröße über alle Beobachtungen. Die gestrichelte Linie zeigt die durchschnittiche Körpergröße nach dem statistsischen Bundesamt mit \\(167.5cm\\) für 20 bis 25 jährige Frauen sowie \\(181.4cm\\) für 20 bis 25 jährige Männer.\nAbbildung 17.6— Darstellung von dem Zusammenhang von pnorm(q = 1.96) und qnorm(p = 0.025). Mit der Option lower.tail bestimmen wir auf welche Seite der Verteilung wir sein wollen.\nAbbildung 17.7— Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und damit die Fallzahl, desto näher kommt die t-Verteilung einer Normalverteilung nahe.\nAbbildung 17.8— Histogramm verschiedener Poissonverteilungen.\nAbbildung 17.9— Histogramme der Anzahl an Gummibärchen und die Anzahl an Farben in einer Tüte. Es gibt nicht mehr als sechs Farben. (A) Anzahl an Bärchen. (B) Anzahl an Farben.\nAbbildung 17.10— Darstellung Anzahl an Bärchen und Anzahl an Farben. Es gibt nicht mehr als sechs Farben. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte. (A) Verteilung der beobachteten Anzahl an Bärchen. (B) Verteilung der theoretischen Anzahl an Bärchen. (C) Verteilung der beobachteten Anzahl an Farben. (D) Verteilung der theoretischen Anzahl an Farben.\nAbbildung 17.11— Densityplot der Anzahl an Bärchen in einer Tüte aufgetrennt nach den Jahren der Erhebung. Das Jahr 2020 fehlt bedingt durch die Coronapandemie.\nAbbildung 17.12— Histogramme der am liebsten gemochten Gummibärchchen im Vergleich zum Inhalt der Tütchen. (A) Anzahl am liebsten gemochten Gummibärchen aufgeteilt nach Geschlecht. (B) Anzahl der Gummibärchen aufgeteilt nach der Farbe.\nAbbildung 17.13— Darstellung des Pascalsche (oder Pascal’sche) Dreieckes im Zusammenhang zum Binomialkoeffizienten.\nAbbildung 17.14— Beispiel für eine Binomialverteilung anhand des Geschlechts. Die fehlenden Angaben wurden entfernt.\nAbbildung 17.15— Darstellung der Uniformverteilung zwischen den beiden Punkten \\(a\\) und \\(b\\).\nAbbildung 17.16— Beispiel für eine uniforme Verteilung anhand der Anzahl der Gummibärchen pro Tüte nach Farbe. (A) Verteilung der beobachteten Anzahl der Gummibärchen pro Tüte nach Farbe. (B) Verteilung der theoretischen Anzahl der Gummibärchen pro Tüte nach Farbe.\n\n\n\nDormann CF. 2013. Parametrische Statistik. Springer.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Verteilung von Daten</span>"
    ]
  },
  {
    "objectID": "eda-transform.html",
    "href": "eda-transform.html",
    "title": "18  Transformieren von Daten",
    "section": "",
    "text": "18.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, scales, see, MASS, bestNormalize,\n               conflicted)\nconflicts_prefer(MASS::boxcox)\nconflicts_prefer(dplyr::select)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nIm Folgenden ist das R Paket {dlookr} aus dem Code entfernt, da du das Paket extra installieren musst, Stand Ende 2023. Das führt bei mir zu Schluckauf im Code, so dass ich die Funktionen jetzt nur noch erwähne. Anscheinend funktioniert aber die gängige Installation wieder, da das R Paket {dlookr} Anfang 2024 überarbeitet wurde.\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformieren von Daten</span>"
    ]
  },
  {
    "objectID": "eda-transform.html#genutzte-r-pakete",
    "href": "eda-transform.html#genutzte-r-pakete",
    "title": "18  Transformieren von Daten",
    "section": "",
    "text": "Alternative Installation von {dlookr}\n\n\n\nIst das R Paket {dlookr} mal nicht per Standard aus RStudio zu installieren kannst du das R Paket auch über GitHub installieren. Dafür einmal das R Paket {devtools} installieren und dann folgenden Code ausführen.\n\ndevtools::install_github(\"choonghyunryu/dlookr\")",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformieren von Daten</span>"
    ]
  },
  {
    "objectID": "eda-transform.html#daten",
    "href": "eda-transform.html#daten",
    "title": "18  Transformieren von Daten",
    "section": "18.2 Daten",
    "text": "18.2 Daten\nWir wollen uns in diesem Kapitel mit der normalverteilten Variable jump_length gemessen in [cm] und der nicht-normalverteilten Variable hatch_time gemessen in [h] aus dem Datensatz flea_dog_cat_length_weight.csv\" beschäftigen. Wir wählen über die Funktion select() nur die beiden Spalten aus dem Datensatz, die wir benötigen.\n\ndata_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") |&gt;\n  select(jump_length, hatch_time)\n\nIn der Tabelle 18.1 ist der Datensatz data_tbl nochmal dargestellt. Wir zeigen hier nur die ersten sieben zeilen des Datensatzes.\n\n\n\n\nTabelle 18.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length und der nicht-normalverteilten Variable hatch_time. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\n\n\n\n\n\njump_length\nhatch_time\n\n\n\n\n15.79\n483.60\n\n\n18.33\n82.56\n\n\n17.58\n296.73\n\n\n14.09\n140.90\n\n\n18.22\n162.20\n\n\n13.49\n167.47\n\n\n16.28\n291.20\n\n\n\n\n\n\n\n\nIm Folgenden nutzen wir oft die Funktion mutate(). Schau dir im Zweifel nochmal im Kapitel zu Programmierung die Funktion mutate() an.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformieren von Daten</span>"
    ]
  },
  {
    "objectID": "eda-transform.html#log-transformation",
    "href": "eda-transform.html#log-transformation",
    "title": "18  Transformieren von Daten",
    "section": "18.3 \\(log\\)-Transformation",
    "text": "18.3 \\(log\\)-Transformation\nWir nutzen die \\(log\\)-Transformation, wenn wir aus einem nicht-normalverteiltem Outcome \\(y\\) ein approxomativ normalverteiltes Outcome \\(y\\) machen wollen. Dabei ist wichtig, dass wir natürlich auch die Einheit mit \\(log\\)-transformieren.\nIm Folgenden sehen wir die \\(log\\)-Transformation der Variable hatch_time mit der Funktion log(). Wir erschaffen eine neue Spalte im tibble damit wir die beiden Variable vor und nach der \\(log\\)-Transformation miteinander vergleichen können.\nDas R Paket {dlookr} hat eine große Auswahl an implementierten Funktionen für \\(y\\)-Transformationen.\n\nlog_tbl &lt;- data_tbl |&gt; \n  mutate(log_hatch_time = log(hatch_time))\n\nWir können dann über ein Histogramm die beiden Verteilungen anschauen. In Abbildung 18.1 (a) sehen wir die nicht transformierte, rohe Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Abbildung 18.1 (b) zeigt die \\(log\\)-transmutierten Daten. In diesem Fall sehen wir normalverteilte Daten. Wir haben also ein \\(log\\) normalverteiltes Outcome \\(y\\) mit dem wir jetzt weiterechnen können.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(log\\)-transformierte Daten.\n\n\n\n\n\n\n\nAbbildung 18.1— Histogramm der nicht transfomierten und transformierten Daten.\n\n\n\n\nIn der Abbildung Abbildung 18.2 siehst du nochmal die Anwendung der \\(\\log\\)-Skala in ggplot auf die Daten. Aber Achtung, die ursprünglichen Daten sind nicht transformiert. Du schaust dir hier nur an, wie die Daten transformiert aussehen würden. Es gibt auch hierzu ein kleines Tutorium zu ggplot log scale transformation. Da kannst du dann auch einmal nachschauen, wie die \\(\\log_2\\)-Skala in ggplot funktioniert.\n\nggplot(log_tbl, aes(hatch_time)) +\n  geom_histogram(fill = cbbPalette[2], color = \"black\") +\n  theme_minimal() +\n  labs(x = expression(\"Zeit bis zum Schlüpfen in\" ~ log[10](h)), y = \"Anzahl\") +\n  ## here starts the log scale\n1  scale_x_log10(breaks = trans_breaks(\"log10\", function(x) 10^x),\n2                labels = trans_format(\"log10\", math_format(10^.x))) +\n3  annotation_logticks(sides = \"b\")\n\n\n1\n\nHier wird die \\(x\\)-Achse einmal auf die \\(\\log\\)-Skala gestellt und die breaks entsprechend angepasst.\n\n2\n\nWir ergänzen noch die richtige Schreibweise.\n\n3\n\nDie Ticks werden auch richtig gezeichnet. Du kannst mit l auch auf der \\(y\\)-Achse \\(\\log\\)-Ticks anzeigen lassen.\n\n\n\n\n\n\n\n\n\n\nAbbildung 18.2— Histogramm der nicht transfomierten Daten einmal auf der \\(log_{10}\\)-Skala in ggplot dargestellt. Achtung, hier ist nur die Darstellung transformiert. Die ursprünglichen Daten bleiben von der Transformation in ggplot unberührt.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformieren von Daten</span>"
    ]
  },
  {
    "objectID": "eda-transform.html#quadratwurzel-transformationen",
    "href": "eda-transform.html#quadratwurzel-transformationen",
    "title": "18  Transformieren von Daten",
    "section": "18.4 Quadratwurzel-Transformationen",
    "text": "18.4 Quadratwurzel-Transformationen\nDie Quadratwurzel-Transformationen ist eine etwas seltenere Transformation. Meist wird die Quadratwurzel-Transformationen als die schwächere \\(log\\)-Transformation bezeichnet. Wir sehen in Abbildung 18.3 (b) den Grund dafür. Aber zuerst müssen wir aber über die Funktion sqrt() unsere Daten transformieren. Wir können auch die Funktion transform() aus dem R Paket {dlookr} verwenden und haben eine große Auswahl an möglichen Transformationen. Einfach mal die Hilfeseite von transform() aufrufen und nachschauen.\n\nsqrt_tbl &lt;- data_tbl |&gt; \n  mutate(sqrt_hatch_time = sqrt(hatch_time))\n\nIn Abbildung 18.3 (a) sehen wir die nicht transformierte, rohe Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam nach rechts aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Abbildung 18.3 (b) zeigt die Wurzel-transmutierten Daten. Unser Ziel besser normalverteilte Daten vorliegen zu haben, haben wir aber mit der Quadratwurzel-Transformationen nicht erreicht. Die Daten sind immer noch rechtsschief. Wir würden also die \\(log\\)-Transformation bevorzugen.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n\n\n\n\n\n\n(b) Wurzel-transformierte Daten.\n\n\n\n\n\n\n\nAbbildung 18.3— Histogramm der nicht transfomierten und transformierten Daten.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformieren von Daten</span>"
    ]
  },
  {
    "objectID": "eda-transform.html#box-cox-transformation",
    "href": "eda-transform.html#box-cox-transformation",
    "title": "18  Transformieren von Daten",
    "section": "18.5 Box-Cox Transformation",
    "text": "18.5 Box-Cox Transformation\nDie Box-Cox-Transformation ist ein statistisches Verfahren zur Umwandlung von nicht normalverteilten Daten in eine Normalverteilung. Die Transformation ist nicht so einfach wie die logarithmische Transformation oder die Quadratwurzeltransformation und erfordert etwas mehr Erklärung. Beginnen wir zunächst die Gleichung zu verstehen, die die Transformation beschreibt. Die grundsätzliche Idee ist, dass wir unser \\(y\\) als Outcome mit einem \\(\\lambda\\)-Exponenten transformieren. Die Frage ist jetzt, welches \\(\\lambda\\) produziert die besten normalverteilten Daten? Wenn wir ein \\(\\lambda\\) von Null finden sollten, dann rechnen wir einfach eine \\(\\log\\)-Transformation. Die Idee ist also recht simpel.\n\\[\ny(\\lambda)=\\left\\{\\begin{matrix} \\dfrac{y^{\\lambda}-1} {\\lambda} & \\quad  \\mathrm{f\\ddot ur\\;\\;}\\lambda \\ne 0 \\\\[10pt] \\log(y) &\\quad  \\mathrm{f\\ddot ur\\;\\;}\\lambda = 0\\end{matrix}\\right.\n\\]\nWir werden natürlich jetzt nicht händisch alle möglichen \\(\\lambda\\) durchprobieren bis wir das beste \\(\\lambda\\) gefunden haben. Dafür gibt es die Funktion boxcox aus dem R Paket {MASS}, die ein lineares Modell benötigt. Daher bauen wir usn erst unser lineares Modell und dann stecken wir das Modell in die Funktion boxcox().\n\njump_mod &lt;- lm(jump_length ~ 1, data = data_tbl)\n\nJetzt einmal die Funktion boxcox() ausführen und danach das \\(\\lambda\\) extrahieren. Hier ist es etwas umständlicher, da das \\(\\lambda\\) in der Ausgabe der Funktion etwas vergraben ist. Dafür ist das R Paket {MASS} einfach nicht mehr das jüngste Paket und hat keine so guten Funktionen zum Erhalten von wichtigen Parametern. Ich möchte die Abbildung nicht haben, daher die Option plotit = FALSE.\n\nbc_obj &lt;- boxcox(jump_mod, plotit = FALSE)\nlambda &lt;- bc_obj$x[which.max(bc_obj$y)]\nlambda\n\n[1] 0.9\n\n\nWir erhalten also ein \\(\\lambda\\) von 0.9 wieder. Dieses \\(\\lambda\\) können wir dann nutzen um unsere Daten nach Box-Cox zu transformieren. Dafür übersetzen wir dann die obige Matheformel einmal in R Code.\n\ndata_tbl &lt;- data_tbl |&gt;\n  mutate(jump_boxcox = ((jump_length ^ lambda - 1)/lambda))\n\nIn der Abbildung 18.4 sehen wir dann einmal das Ergebnis der Transformation. Sieht gar nicht mal so schlecht aus und noch besser als die reine \\(\\log\\)-Transformation. Wie immer musst du aber auch hier rumprobieren, was dann am besten einer Normalverteilung folgt.\n\nggplot(data = data_tbl) +\n  theme_minimal() +\n  geom_histogram(aes(x = jump_boxcox), alpha = 0.9,\n                 fill = cbbPalette[3], color = \"black\") +\n  labs(x = 'Box-Cox transformierte Sprungweiten', y = 'Anzahl')\n\n\n\n\n\n\n\nAbbildung 18.4— Histogramm der Box-Cox transfomierten Daten in ggplot dargestellt.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformieren von Daten</span>"
    ]
  },
  {
    "objectID": "eda-transform.html#standardisierung",
    "href": "eda-transform.html#standardisierung",
    "title": "18  Transformieren von Daten",
    "section": "18.6 Standardisierung",
    "text": "18.6 Standardisierung\nDie Standardisierung wird auch \\(z\\)-Transformation genannt. In dem Fall der Standardisierung schieben wir die Daten auf den Ursprung, in dem wir von jedem Datenpunkt \\(y_i\\) den Mittelwert \\(\\bar{y}\\) abziehen. Dann setzen wir noch die Standardabweichung auf Eins in dem wir durch die Standardabweichung \\(y_s\\) teilen. Unser standardisiertes \\(y\\) ist nun standardnormalverteilt mit \\(\\mathcal{N(0,1)}\\). Wir nutzen für die Standardisierung folgende Formel.\n\\[\ny_z = \\cfrac{y_i - \\bar{y}}{s_y}\n\\]\nIn R können wir für die Standardisierung die Funktion scale() verwenden. Wir müssen auch nichts weiter in den Optionen von scale() angeben. Die Standardwerte der Funktion sind so eingestellt, dass eine Standardnormalverteilung berechnet wird. Wir können auch die Funktion transform() aus dem R Paket {dlookr} verwenden. Die Option wäre dann zscore.\n\nscale_tbl &lt;- data_tbl |&gt; \n  mutate(scale_jump_length = scale(jump_length))\n\nIn Abbildung 18.5 (a) sehen wir nochmal die nicht transformierten, rohen Daten. Wir haben in diesem Beispiel die normalvertielte Variable jump_length gewählt. Der Mittelwert von jump_length ist 20.51 und die Standardabweichung ist 3.77. Ziehen wir nun von jedem Wert von jump_length den Mittelwert mit 19.3 ab, so haben wir einen neuen Schwerpunkt bei Null. Teilen wir dann jede Zahl durch 3.36 so haben wir eine reduzierte Spannweite der Verteilung. Es ergibt sich die Abbildung 18.5 (b) als Standardnormalverteilung. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die Sprungweite auf der \\(z\\)-Skala nicht mehr biologisch interpretieren.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(z\\)-transformierte Daten.\n\n\n\n\n\n\n\nAbbildung 18.5— Histogramm der nicht transfomierten und transformierten Daten.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformieren von Daten</span>"
    ]
  },
  {
    "objectID": "eda-transform.html#normalisierung",
    "href": "eda-transform.html#normalisierung",
    "title": "18  Transformieren von Daten",
    "section": "18.7 Normalisierung",
    "text": "18.7 Normalisierung\nAbschließend wollen wir uns nochmal die Normalisierung anschauen. In diesem Fall wollen wir die Daten so transformieren, dass die Daten nur noch in der Spannweite 0 bis 1 vorkommen. Egal wie die Einheiten vorher waren, alle Variablen haben jetzt nur noch eine Ausprägung von 0 bis 1. Das ist besonders wichtig wenn wir viele Variablen haben und anhand der Variablen eine Vorhersage machen wollen. Uns interessieren die Werte in den Variablen an sich nicht, sondern wir wollen ein Outcome vorhersagen. Wir brauchen die Normalisierung später für das maschinelle Lernen und die Klassifikation. Die Formel für die Normalisierung lautet wie folgt.\n\\[\ny_n = \\cfrac{y_i - \\min(y)}{\\max(y) - \\min(y)}\n\\]\nIn R gibt es die Normalisierungsfunktion nicht direkt. Wir könnten hier ein extra Paket laden, aber bei so einer simplen Formel können wir auch gleich die Berechnung in der Funktion mutate() machen. Wir müssen nur etwas mit den Klammern aufpassen. Wir können auch die Funktion transform() aus dem R Paket {dlookr} mit der Option minmax verwenden.\n\nnorm_tbl &lt;- data_tbl |&gt; \n  mutate(norm_jump_length = (jump_length - min(jump_length))/(max(jump_length) - min(jump_length)))\n\nIn Abbildung 18.6 (a) sehen wir nochmal die nicht transformierten, rohen Daten. In Abbildung 18.6 (b) sehen wir die normalisierten Daten. Hier fällt dann auf, dass die normalisierten Sprungweiten nur noch Werte zwischen Null und Eins annehmen. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die normalisierten Sprungweiten nicht mehr biologisch interpretieren.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n\n\n\n\n\n\n(b) Normalisierte Daten\n\n\n\n\n\n\n\nAbbildung 18.6— Histogramm der nicht transfomierten und transformierten Daten.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformieren von Daten</span>"
    ]
  },
  {
    "objectID": "eda-transform.html#automatische-auswahl-mit-bestnormalize",
    "href": "eda-transform.html#automatische-auswahl-mit-bestnormalize",
    "title": "18  Transformieren von Daten",
    "section": "18.8 Automatische Auswahl mit {bestNormalize}",
    "text": "18.8 Automatische Auswahl mit {bestNormalize}\nDann sind wir bis hierher gekommen und fragen uns nun welche ist den jetzt die beste Transformation für unsere Daten um einigermaßen eine Normalverteilung hinzubekommen? Da hilft uns jetzt das R Paket {bestNormalize}. Die Kurzanleitung zum Paket erlaubt es automatisiert auf einem Datenvektor die beste Transformation zu finden um die Daten in eine Normalverteilung zu verwandeln. Wie immer geht noch mehr, aber wir machen hier einmal den schnellen Durchlauf. Wir bauen uns dazu einmal einen Datenvektor x mit 1000 Beobachtungen, die einer Gamma-Verteilung folgen.\n\nx &lt;- rgamma(1000, 1, 1)\n\nJetzt fragst du dich, wie sieht so eine Gamma-Verteilung aus? Zuerst natürlich nicht wie eine Normalverteilung. Ich habe dir in der Abbildung 18.7 einmal das Histogramm der x-Werte dargestellt. Wir haben also eine sehr schiefe Verteilung vorliegen.\n\n\n\n\n\n\n\n\nAbbildung 18.7— Histogramm der Gamma-Verteilung.\n\n\n\n\n\nJetzt können wir die Funktion bestNormalize() nutzen um uns die beste Transformationsmethode wiedergeben zu lassen. Das ist super praktisch in der Anwendung. Wir können uns theoretisch noch verschiedene Gütekriterien aussuchen aber für hier reicht die Standardimplementierung. Wichtig ist noch, dass wir hier eine Kreuzvalidierung durchführen, so dass die Ergebnisse und die Auswahl des Algorithmus robust sein sollte.\n\nbn_obj &lt;- bestNormalize(x)\nbn_obj\n\nBest Normalizing transformation with 1000 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 3.118\n - Box-Cox: 0.9408\n - Center+scale: 5.7976\n - Double Reversed Log_b(x+a): 9.9981\n - Exp(x): 57.0569\n - Log_b(x+a): 2.0557\n - orderNorm (ORQ): 0.9933\n - sqrt(x + a): 1.349\n - Yeo-Johnson: 1.2549\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized Box Cox Transformation with 1000 nonmissing obs.:\n Estimated statistics:\n - lambda = 0.2738115 \n - mean (before standardization) = -0.3779664 \n - sd (before standardization) = 1.007427 \n\n\nLaut dem Algorithmus sollen wir eine Box Cox Transformation durchführen. Okay, wie machen wir das jetzt? Wir hätten zwar die Parameter in der Ausgabe angegeben und die könnten wir dann auch in einer Veröffentlichung angeben, aber wenn es schneller gehen soll, dann können wir die Funktion predict() nutzen, die uns die transformierten x Werte wiedergibt.\n\nx_trans &lt;- predict(bn_obj)\n\nDann nochmal schnell gucken, ob das auch mit der Rücktransformation klappen würde.\n\nx_trans_back &lt;- predict(bn_obj, newdata = x_trans, inverse = TRUE)\n\nEinmal dann der Vergleich ob alle x_trans_back Werte den ursprünglichen x Werten entsprechen. Ja, das sieht gut aus.\n\nall.equal(x_trans_back, x)\n\n[1] TRUE\n\n\nUnd zum Abschluss nochmal eine Abbildung der transformierten x Werte. In der Abbildung 18.8 siehst du dann einmal das entsprechende Histogramm. Das sieht doch sehr gut aus und wir mussten nicht zig verschiedene Algorithmen selber testen.\n\n\n\n\n\n\n\n\nAbbildung 18.8— Histogramm der transformierten x Werte mit der Box-Cos Transformation aus dem R Paket {bestNormalize}.\n\n\n\n\n\n\n\n\nAbbildung 18.1 (a)— Nicht transformierte, rohe Daten\nAbbildung 18.1 (b)— \\(log\\)-transformierte Daten.\nAbbildung 18.2— Histogramm der nicht transfomierten Daten einmal auf der \\(log_{10}\\)-Skala in ggplot dargestellt. Achtung, hier ist nur die Darstellung transformiert. Die ursprünglichen Daten bleiben von der Transformation in ggplot unberührt.\nAbbildung 18.3 (a)— Nicht transformierte, rohe Daten\nAbbildung 18.3 (b)— Wurzel-transformierte Daten.\nAbbildung 18.4— Histogramm der Box-Cox transfomierten Daten in ggplot dargestellt.\nAbbildung 18.5 (a)— Nicht transformierte, rohe Daten\nAbbildung 18.5 (b)— \\(z\\)-transformierte Daten.\nAbbildung 18.6 (a)— Nicht transformierte, rohe Daten\nAbbildung 18.6 (b)— Normalisierte Daten\nAbbildung 18.7— Histogramm der Gamma-Verteilung.\nAbbildung 18.8— Histogramm der transformierten x Werte mit der Box-Cos Transformation aus dem R Paket {bestNormalize}.",
    "crumbs": [
      "Explorative Datenanalyse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Transformieren von Daten</span>"
    ]
  },
  {
    "objectID": "stat-tests-preface-theory.html",
    "href": "stat-tests-preface-theory.html",
    "title": "Testen von Hypothesen",
    "section": "",
    "text": "Referenzen",
    "crumbs": [
      "Testen von Hypothesen"
    ]
  },
  {
    "objectID": "stat-tests-preface-theory.html#referenzen",
    "href": "stat-tests-preface-theory.html#referenzen",
    "title": "Testen von Hypothesen",
    "section": "",
    "text": "Abbildung 1— Abbildung über die Grundgesamtheit und die Stichprobe(n) \\(D_1\\) bis \\(D_j\\). Durch Randomisierung wird Sturkturgleichheit erreicht, die dann einen Rückschluß von der Stichprobe auf die Grundgesamtheit erlaubt. Jede Stichprobe ist anders und nicht jede Randomisierung ist erfolgreich was die Strukturgleicheit betrifft.\n\n\n\nGigerenzer G, Krauss S, Vitouch O. 2004. The null ritual. The Sage handbook of quantitative methodology for the social sciences 391–408.\n\n\nSalsburg D. 2001. The lady tasting tea: How statistics revolutionized science in the twentieth century. Macmillan.\n\n\nWasserstein RL, Schirm AL, Lazar NA. 2019. Moving to a world beyond „p&lt; 0.05“. The American Statistician 73: 1–19.",
    "crumbs": [
      "Testen von Hypothesen"
    ]
  },
  {
    "objectID": "stat-tests-basic.html",
    "href": "stat-tests-basic.html",
    "title": "19  Die Testentscheidung",
    "section": "",
    "text": "19.1 Die Hypothesen\nWir können auf allen Daten einen statistischen Test rechnen und erhalten statistische Maßzahlen wie eine Teststatistik oder einen \\(p\\)-Wert. Nur leider können wir mit diesen statistischen Maßzahlen nicht viel anfangen ohne die Hypothesen zu kennen. Jeder statistische Test testet eine Nullhypothese. Ob diese Hypothese dem Anwender nun bekannt ist oder nicht, ein statistischer Test testet eine Nullhypothese. Daher müssen wir uns immer klar sein, was die entsprechende Nullhypothese zu unserer Fragestellung ist. Wenn du hier stockst, ist das ganz normal. Eine Fragestellung mit einer statistischen Hypothese zu verbinden ist nicht immer so einfach gemacht.\nAls Veranschaulichung nehmen wir das Beispiel aus der unterschiedlichen Sprungweiten in [cm] für Hunde- und Katzenflöhe. Wir formulieren als erstes die Fragestellung. Eine Fragestellung endet mit einem Fragezeichen.\nLiegt ein Unterschied zwischen den Sprungweiten von Hunde- und Katzenflöhen vor?\nWir können die Frage auch anders formulieren.\nSpringen Hunde- und Katzenflöhe unterschiedlich weit?\nWichtig ist, dass wir eine Fragestellung formulieren. Wir können auch mehrere Fragen an einen Datensatz haben. Das ist auch vollkommen normal. Nur hat jede Fragestellung ein eigenes Hypothesenpaar. Wir bleiben aber bei dem simplen Beispiel mit den Sprungweiten von Hunde- und Katzenflöhen.\nWie sieht nun die statistische Hypothese in diesem Beispiel aus? Wir wollen uns die Sprungweite in [cm] anschauen und entscheiden, ob die Sprungweite für Hunde- und Katzenflöhen sich unterscheidet. Eine statistische Hypothese ist eine Aussage über einen Parameter einer Population. Wir entscheiden jetzt, dass wir die mittlere Sprungweite der Hundeflöhe \\(\\bar{y}_{dog}\\) mit der mittleren Sprungweite der Katzenflöhe \\(\\bar{y}_{cat}\\) vergleichen wollen. Es ergibt sich daher folgendes Hypothesenpaar.\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{dog} &= \\bar{y}_{cat} \\\\  \nH_A: \\bar{y}_{dog} &\\neq \\bar{y}_{cat} \\\\   \n\\end{aligned}\n\\]\nEs ist wichtig sich in Erinnerung zu rufen, dass wir nur und ausschließlich Aussagen über die Nullhypothese treffen werden. Das frequentistische Hypothesentesten kann nichts anders. Wir kriegen keine Aussage über die Alternativhypothese sondern nur eine Abschätzung der Wahrscheinlichkeit des Auftretens der Daten im durchgeführten Experiment, wenn die Nullhypothese wahr wäre. Wenn die Nullhypothese war ist, dann liegt kein Effekt oder Unterschied vor.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Die Testentscheidung</span>"
    ]
  },
  {
    "objectID": "stat-tests-basic.html#sec-hypothesen",
    "href": "stat-tests-basic.html#sec-hypothesen",
    "title": "19  Die Testentscheidung",
    "section": "",
    "text": "Die Nullhypothese \\(H_0\\) und die Alternativhypothese \\(H_A\\)\n\n\n\nDie Nullhypothese \\(H_0\\) nennen wir auch die Null oder Gleichheitshypothese. Die Nullhypothese sagt aus, dass zwei Gruppen gleich sind oder aber kein Effekt zu beobachten ist.\n\\[\nH_0: \\bar{y}_{1} = \\bar{y}_{2}\n\\]\nDie Alternativhypothese \\(H_A\\) oder \\(H_1\\) auch Alternative genannt nennen wir auch Unterschiedshypothese. Die Alternativhypothese besagt, dass ein Unterschied vorliegt oder aber ein Effekt vorhanden ist.\n\\[\nH_A: \\bar{y}_{1} \\neq \\bar{y}_{2}\n\\]\n\n\n\n\n\n\n\n\n\nDas Falisifkationsprinzip - wir können nur Ablehnen - kommt hier zusammen mit der frequentistischen Statistik in der wir nur eine Wahrscheinlichkeitsaussage über das Auftreten der Daten \\(D\\) - unter der Annahme \\(H_0\\) gilt - treffen können.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Die Testentscheidung</span>"
    ]
  },
  {
    "objectID": "stat-tests-basic.html#die-testentscheidung",
    "href": "stat-tests-basic.html#die-testentscheidung",
    "title": "19  Die Testentscheidung",
    "section": "19.2 Die Testentscheidung…",
    "text": "19.2 Die Testentscheidung…\nIn den folgenden Kapiteln werden wir verschiedene statistische Tests kennenlernen. Alle statistischen Tests haben gemein, dass ein Test eine Teststatistik \\(T_{calc}\\) berechnet. Darüber hinaus liefert jeder Test auch einen p-Wert (eng. p-value). Manche statistischen Test geben auch ein 95% Konfidenzintervall wieder. Eine Testentscheidung gegen die Nullhypothese \\(H_0\\) kann mit jedem der drei statistischen Maßzahlen - Teststatistik, \\(p\\)-Wert und Konfidenzintervall - durchgeführt werden. Die Regel für die Entscheidung, ob die Nullhypothese \\(H_0\\) abgelehnt werden kann, ist nur jeweils anders. In Tabelle 19.1 sind die Entscheidungsregeln einmal zusammengefasst. Wir wollen in den folgenden Abschnitten die jeweiligen Entscheidungsregeln eines statistisches Tests anhand der Maßzahl Teststatistik, \\(p\\)-Wert und Konfidenzintervall einmal durchgehen.\n\n\n\n\nTabelle 19.1— Zusammenfassung der statistischen Testentscheidung unter der Nutzung der Teststatistik, dem p-Wert und dem 95% Konfidenzintervall. Die Entscheidung nach der Teststatistik ist veraltet und dient nur dem konzeptionellen Verständnisses. In der Forschung angewandt wird der \\(p\\)-Wert und das 95% Konfidenzintervall. Im Fall des 95% Konfidenzintervalls müssen wir noch unterschieden, ob wir einen Mittelwertsunterschied \\(\\Delta_{A-B}\\) oder aber einen Anteilsunterschied \\(\\Delta_{A/B}\\) betrachten.\n\n\n\n\n\n\n\n\n\n\n\n\nTeststatistik\np-Wert\n95% Konfidenzintervall\n\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(\\boldsymbol{KI_{1-\\alpha}}\\)\n\n\nH\\(_0\\) ablehnen\n\\(T_{calc} \\geq T_{\\alpha = 5\\%}\\)\n\\(Pr(\\geq T_{calc}| H_0) \\leq \\alpha\\)\n\\(\\Delta_{A-B}\\): enthält nicht 0\n\n\nH\\(_0\\) ablehnen\n\n\n\\(\\Delta_{A/B}\\): enthält nicht 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStreng genommen gilt die Regel \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) nur für eine Auswahl an statistischen Tests siehe dazu auch Kapitel 19.2.1. Bei manchen statistischen Tests ist die Entscheidung gedreht. Hier lassen wir das aber mal so stehen…\n\n\n\n\n\n19.2.1 … anhand der Teststatistik\n\nWir wollen uns dem frequentistischen Hypothesentesten über die Idee der Teststatistik annähern. Die Teststatistik kannst du einfach anhand einer mathematischen Formel ausrechnen. Dabei hat die Teststatistik den Vorteil, dass sie einheitslos ist. Egal ob du das Gewicht zwischen Elefanten [t] oder Hamstern [g] vergleichst, die Teststatistik wird immer ähnliche numerische Werte annehmen. Du kannst also Teststatistiken über verschiedene Experimente miteinander vergleichen. Im Folgenden sehen wir die Formel für den t-Test, den wir dazu nutzen um zwei Mittelwerte miteinander zu vergleichen. Den t-Test werden wir im Kapitel 27 uns nochmal detaillierter anschauen, hier deshalb nur die Formel mit der wir dann die Teststatistik erarbeiten und verstehen werden. Hier nutzen wir deshalb die vereinfachte Formel des Student t-Test um das Konzept der Teststatistik \\(T\\) zu verstehen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_2}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nmit\n\n\\(\\bar{y}_1\\) dem Mittelwert für die erste Gruppe.\n\\(\\bar{y}_2\\) dem Mittelwert für die zweite Gruppe.\n\\(s_{p}\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{s_1 + s_2}{2}\\).\n\\(n_g\\) der Gruppengröße der gruppen. Wir nehmen an beide Gruppen sind gleich groß.\n\nZum berechnen der Teststatistik \\(T_{calc}\\) benötigen wir also die zwei Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) sowie deren gepoolte Standardabweichung \\(s_p\\) und die Anzahl der Beobachtungen je Gruppe \\(n_g\\). Im Folgenden wenden wir die Formel des t-Tests einmal auf einen kleinen Beispieldatensatz zu den Sprunglängen in [cm] von jeweils \\(n_g = 4\\) Hunde- und Katzenflöhen an. Du siehst in der Formel, dass wir die Einheit [cm] dadurch verlieren, dass wir den Mittelwertsunterschied in [cm] durch die gepoolte Standardabweichung in [cm] teilen. Beide Maßzahlen haben die gleiche Einheit, so dass wir am Ende eine einheitslose Teststatistik \\(T_{calc}\\) vorliegen haben. In Tabelle 19.2 ist das Datenbeispiel gegeben.\n\n\n\n\nTabelle 19.2— Beispiel für die Berechnung von einem Mittelwertseffekt an der Sprunglänge [cm] von Hunde und Katzenflöhen.\n\n\n\n\n\n\nanimal\njump_length\n\n\n\n\ncat\n8.5\n\n\ncat\n9.9\n\n\ncat\n8.9\n\n\ncat\n9.4\n\n\ndog\n8.0\n\n\ndog\n7.2\n\n\ndog\n8.4\n\n\ndog\n7.5\n\n\n\n\n\n\n\n\nNun berechnen wir die Mittelwerte und die Standardabweichungen aus der obigen Datentabelle für die Sprungweiten getrennt für die Hunde- und Katzenflöhe. Die Werte setzen wir dann in die Formel ein und berechnen die Teststatistik \\(T_{calc}\\).\n\\[\nT_{calc}=\\cfrac{9.18cm - 7.78cm}{\\cfrac{(0.61cm + 0.53cm)}{2} \\cdot \\sqrt{2/4}} = 3.47\n\\]\nmit\n\n\\(\\bar{y}_{cat} = 9.18\\) dem Mittelwert für die Gruppe cat.\n\\(\\bar{y}_{dog} = 7.78\\) dem Mittelwert für die Gruppe dog.\n\\(s_p = 0.57\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.61 + 0.53}{2}\\).\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nWir haben nun die Teststatistik \\(T_{calc} = 3.47\\) berechnet. In der ganzen Rechnerei verliert man manchmal den Überblick. Erinnern wir uns, was wir eigentlich wollten. Die Frage war, ob sich die mittleren Sprungweiten der Hunde- und Katzenflöhe unterschieden. Wenn die \\(H_0\\) wahr wäre, dann wäre der Unterschied \\(\\Delta\\) der beiden Mittelwerte der Hunde- und Katzenflöhe gleich Null. Oder nochmal in der Analogie der t-Test Formel, dann wäre im Zähler \\(\\Delta = \\bar{y}_{cat} - \\bar{y}_{dog} = 0\\). Wenn die Mittelwerte der Sprungweite [cm] der Hunde- und Katzenflöhe gleich wäre, dann wäre die berechnete Teststatistik \\(T_{calc} = 0\\), da im Zähler Null stehen würde. Die Differenz von zwei gleichen Zahlen ist Null.\nJe größer die berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die beiden Mittelwerte per Zufall gleich sind. Wie groß muss nun die berechnete Teststatistik \\(T_{calc}\\) werden damit wir die Nullhypothese ablehnen können?\n\n\n\n\n\n\nAbbildung 19.2— Die t-Verteilung aller möglichen \\(T_{calc}\\) wenn die Nullhypothese wahr ist. Der Mittelwert der t-Verteilung ist \\(T=0\\). Wenn wir keinen Effekt erwarten würden dann wären die beiden Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) gleich groß. Die Differenz wäre 0. Je größer der \\(T_{calc}\\) wird desto weniger können wir davon ausgehen, dass die beiden Mittelwerte gleich sind. Liegt der \\(T_{calc}\\) über dem kritischen Wert von \\(T_{\\alpha = 5\\%}\\) dann wir die Nullhypothese abgelehnt.\n\n\n\nIn Abbildung 19.2 ist die Verteilung aller möglichen \\(T_{calc}\\) Werte unter der Annahme, dass die Nullhypothese wahr ist, dargestellt. Wir sehen, dass die t-Verteilung den Gipfel bei \\(T_{calc} = 0\\) hat und niedrigere Werte mit steigenden Werten der Teststatistik annimmt. Wenn \\(T = 0\\) ist, dann sind auch die Mittelwerte gleich. Je größer unsere berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die Nullhypothese gilt.\nDie t-Verteilung ist so gebaut, dass die Fläche \\(A\\) unter der Kurve gleich \\(A=1\\) ist. Wir können nun den kritischen Wert \\(T_{\\alpha = 5\\%}\\) berechnen an dem rechts von dem Wert eine Fläche von 0.05 oder 5% liegt. Somit liegt dann links von dem kritischen Wert die Fläche von 0.95 oder 95%. Den kritischen Wert \\(T_{\\alpha = 5\\%}\\) können wir statistischen Tabellen entnehmen. Oder wir berechnen den kritischen Wert direkt in R mit \\(T_{\\alpha = 5\\%} = 2.78\\).\nKommen wir zurück zu unserem Beispiel. Wir haben in unserem Datenbeispiel für den Vergleich von der Sprungweite in [cm] von Hunde- und Katzenflöhen eine Teststatistik von \\(T_{calc} = 3.47\\) berechnet. Der kritische Wert um die Nullhypothese abzulehnen liegt bei \\(T_{\\alpha = 5\\%} = 2.78\\). Wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. In unserem Fall ist \\(3.47 \\geq 2.78\\). Wir können die Nullhypothese ablehnen. Es gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen.\n\n\n\n\n\n\nWoher kommt die Testverteilung von \\(T\\), wenn \\(H_0\\) gilt?\n\n\n\n\n\nIn diesem Exkurs wollen wir einmal überlegen, woher die Testverteilung \\(T\\) herkommt, wenn die \\(H_0\\) gilt. Wir wollen die Verteilung der Teststatistik einmal in R herleiten. Zuerst gehen wir davon aus, dass die Mittelwerte der Sprungweite der Hunde- und Katzenflöhe gleich sind \\(\\bar{y}_{cat} = \\bar{y}_{dog} = (9.18 + 7.78)/2 = 8.48\\). Daher nehmen wir an, dass die Mittelwerte aus der gleichen Normalverteilung kommen. Wir ziehen also vier Sprungweiten jeweils für die Hunde- und Katzenflöhe aus einer Normalverteilung mit \\(\\mathcal{N}(8.48, 0.57)\\). Wir nutzen dafür die Funktion rnorm(). Anschließend berechnen wir die Teststatistik. Diesen Schritt wiederholen wir eintausend Mal.\n\nset.seed(20201021)\nT_vec &lt;- map_dbl(1:1000, function(...){\n  dog_vec &lt;- rnorm(n = 4, mean = 8.48, sd = 0.57)\n  cat_vec &lt;- rnorm(n = 4, mean = 8.48, sd = 0.57)\n  s_p &lt;- (sd(cat_vec) + sd(dog_vec))/2 \n  T_calc &lt;- (mean(cat_vec) - mean(dog_vec))/(s_p * sqrt(2/4)) \n  return(T_calc)  \n}) |&gt; round(2)\n\nNachdem wir eintausend Mal die Teststatistik unter der \\(H_0\\) berechnet haben, schauen wir uns die sortierten ersten 100 Werte der Teststatistik einmal an. Wir sehen, dass extrem kleine Teststatistiken bis sehr große Teststatistiken zufällig auftreten können, auch wenn die Mittelwerte für das Ziehen der Zahlen gleich waren.\n\nT_vec |&gt; magrittr::extract(1:100) |&gt; sort()  \n\n  [1] -5.19 -3.48 -3.29 -2.65 -2.40 -2.10 -1.48 -1.35 -1.30 -1.29 -1.29 -1.27\n [13] -1.24 -1.22 -1.10 -1.03 -1.02 -1.02 -0.91 -0.87 -0.84 -0.79 -0.79 -0.76\n [25] -0.76 -0.76 -0.73 -0.66 -0.63 -0.63 -0.62 -0.61 -0.57 -0.56 -0.55 -0.52\n [37] -0.52 -0.50 -0.48 -0.48 -0.43 -0.35 -0.33 -0.32 -0.26 -0.26 -0.22 -0.21\n [49] -0.20 -0.18 -0.17 -0.17 -0.14 -0.14 -0.12 -0.12 -0.10 -0.06  0.04  0.10\n [61]  0.14  0.16  0.17  0.31  0.34  0.41  0.45  0.50  0.50  0.51  0.55  0.63\n [73]  0.63  0.68  0.73  0.73  0.77  0.89  0.92  0.95  0.99  1.07  1.07  1.09\n [85]  1.12  1.16  1.22  1.33  1.33  1.76  2.11  2.16  2.51  2.79  2.87  3.24\n [97]  3.48  3.56  3.60  6.56\n\n\nUnsere berechnete Teststatistik war \\(T_{calc} = 3.47\\). Wenn wir diese Zahl mit den ersten einhundert, sortierten Teststatistiken vergleichen, dann sehen wir, dass nur 4 von 100 Zahlen größer sind als unsere berechnete Teststatistik. Wir beobachten also sehr seltene Daten wie in Tabelle 19.2, wenn wir davon ausgehen, dass kein Unterschied zwischen der Sprungweite der Hunde- und Katzenflöhe vorliegt.\nIn Abbildung 19.3 sehen wir die Verteilung der berechneten eintausend Verteilungen nochmal als ein Histogramm dargestellt. Wiederum sehen wir, dass unsere berechnete Teststatistik - dargestellt als rote Linie - sehr weit rechts am Rand der Verteilung liegt.\n\nggplot(as_tibble(T_vec), aes(x = value)) +\n  theme_minimal() +\n  labs(x = \"Teststatistik\", y = \"Anzahl\") +\n  geom_histogram() +\n  geom_vline(xintercept = 3.47, color = \"red\")\n\n\n\n\n\n\n\nAbbildung 19.3— Histogramm der 1000 gerechneten Teststaistiken \\(T_{calc}\\), wenn die \\(H_0\\) war wäre und somit kein Unterschied zwischen den Mittelwerten der Sprungweiten der Hunde- und Katzenflöhe vorliegen würde.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen. Die Aussage ist statistisch falsch. Wir können im frequentistischen Hypothesentesten keine Aussage über die \\(H_A\\) treffen. Im Sinne der Anwendbarkeit soll es hier so stehen bleiben.\n\n\n\n\nNun ist es leider so, dass jeder statistische Test seine eigene Teststatistik \\(T\\) hat. Daher ist es etwas mühselig sich immer neue und andere kritische Werte für jeden Test zu merken. Es hat sich daher eingebürgert, sich nicht die Teststatistik für die Testentscheidung gegen die Nullhypothese zu nutzen sondern den \\(p\\)-Wert. Den \\(p\\)-Wert wollen wir uns in dem folgenden Abschnitt anschauen.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik\n\n\n\nBei der Entscheidung mit der Teststatistik müssen wir zwei Fälle unterschieden.\n\nBei einem t-Test und einem \\(\\mathcal{X}^2\\)-Test gilt, wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nBei einem Wilcoxon-Mann-Whitney-Test gilt, wenn \\(T_{calc} &lt; T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\n\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.\n\n\n\n\n19.2.2 … anhand dem p-Wert\n\nIn dem vorherigen Abschnitt haben wir gelernt, wie wir zu einer Entscheidung gegen die Nullhypothese anhand der Teststatistik kommen. Wir haben einen kritischen Wert \\(T_{\\alpha = 5\\%}\\) definiert bei dem rechts von dem Wert 5% der Werte liegen. Anstatt nun den berechneten Wert \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) zu vergleichen, vergleichen wir jetzt die Flächen rechts von den jeweiligen Werten. Wir machen es uns an dieser Stelle etwas einfacher, denn wir nutzen immer den absoluten Wert der Teststatistik. Wir schreiben \\(\\boldsymbol{Pr}\\) und meinen damit eine Wahrscheinlichkeit (eng. probability). Häufig wird auch nur das \\(P\\) verwendet, aber dann kommen wir wieder mit anderen Konzepten in die Quere.\nIn Abbildung 19.2 sind die Flächen auch eingetragen. Da die gesamte Fläche unter der t-Verteilung mit \\(A = 1\\) ist, können wir die Flächen auch als Wahrscheinlichkeiten lesen. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert genannt. Die gesamte Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%. Wir können also die Teststatistiken oder den p-Wert mit dem \\(\\alpha\\)-Niveau von 5% vergleichen.\n\n\n\nTabelle 19.3— Zusammenhang zwischen der Teststatistik \\(T\\) und der Fläche \\(A\\) rechts von der Teststatistik. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T|H_0)\\) oder \\(p\\)-Wert genannt. Die Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%.\n\n\n\n\n\nTeststatistik \\(T\\)\nFläche \\(A\\)\n\n\n\n\n\\(T_{calc}\\)\n\\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert\n\n\n\\(T_{\\alpha = 5\\%}\\)\n\\(\\alpha\\)\n\n\n\n\n\n\nIn der folgenden Abbildung 19.4 ist dann nochmal der Zusammenhang aus der Tabelle als eine Abbildung visualisiert. Mit dem \\(p\\)-Wert entscheiden wir anhand von Flächen. Wir schauen uns in diesem Fall die beiden Seiten der Testverteilung mit jeweils \\(T_{\\alpha = 2.5\\%}\\) für \\(-T_K\\) und \\(T_K\\) an und vergleichen die Flächen rechts neben der berechneten Teststatistik \\(T_{calc}\\).\n\n\n\n\n\n\n\nAbbildung 19.4— Die Flächen links und rechts von \\(T_{\\alpha = 2.5\\%}\\) nochmal separat dargestellt. Wir vergleichen bei der Entscheidung mit dem \\(p\\)-Wert nicht die berechnete Teststatistik \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) sondern die Flächen rechts von den jeweiligen Teststatistiken mit \\(A_K = 5\\%\\) und \\(A_{calc}\\) als den \\(p\\)-Wert. An dem Flächenvergleich machen wir dann die Testentscheidung fest.\n\n\n\n\nDer p-Wert oder \\(Pr(T|H_0)\\) ist eine Wahrscheinlichkeit. Eine Wahrscheinlichkeit kann die Zahlen von 0 bis 1 annehmen. Dabei sind die Grenzen einfach zu definieren. Eine Wahrscheinlichkeit von \\(Pr(A) = 0\\) bedeutet, dass das Ereignis A nicht auftritt; eine Wahrscheinlichkeit von \\(Pr(A) = 1\\) bedeutet, dass das Ereignis A eintritt. Der Zahlenraum dazwischen stellt jeden von uns schon vor große Herausforderungen. Der Unterschied zwischen 40% und 60% für den Eintritt des Ereignisses A sind nicht so klar zu definieren, wie du auf den ersten Blick meinen magst. Ein frequentistischer Hypothesentest beantwortet die Frage, mit welcher Wahrscheinlichkeit \\(Pr\\) die Teststatistik \\(T\\) aus dem Experiment mit den Daten \\(D\\) zu beobachten wären, wenn es keinen Effekt gäbe (\\(H_0\\) ist wahr).\nIn anderen Büchern liest man an dieser Stelle auch gerne etwas über die Likelihood, nicht so sehr in deutschen Büchern, schon aber in englischen Veröffentlichungen. Im Englischen gibt es die Begrifflichkeiten einer Likelihood und einer Probability. Meist wird beides ins Deutsche ungenau mit Wahrscheinlichkeit übersetzt oder wir nutzen einfach Likelihood. Was aber auch nicht so recht weiterhilft, wenn wir ein Wort mit dem gleichen Wort übersetzen. Es handelt sich hierbei aber um zwei unterschiedliche Konzepte. Deshalb Übersetzen wir Likelihood mit Plausibilität und Probability mit Wahrscheinlichkeit.\nIm Folgenden berechnen wir den \\(p\\)-Wert in R mit der Funktion t.test(). Mehr dazu im Kapitel 27, wo wir den t-Test und deren Anwendung im Detail besprechen. Hier fällt der \\(p\\)-Wert etwas aus den Himmel. Wir wollen aber nicht per Hand Flächen unter einer Kurve berechnen sondern nutzen für die Berechnung von \\(p\\)-Werten statistische Tests in R.\n\n\n# A tibble: 1 × 2\n  statistic p.value\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      3.47  0.0133\n\n\nWir sagen, dass wir ein signifikantes Ergebnis haben, wenn der \\(p\\)-Wert kleiner ist als die Signifikanzschwelle \\(\\alpha\\) von 5%. Wenden wir also das Wissen einmal an. Wir erhalten einen \\(p\\)-Wert von 0.013 und vergleichen diesen Wert zu einem \\(\\alpha\\) von 5%. Ist der \\(p\\)-Wert kleiner als der \\(\\alpha\\)-Wert von 5%, dann können wir die Nullhypothese ablehnen. Da 0.013 kleiner ist als 0.05 können wir die Nullhypothese und damit die Gleichheit der mittleren Sprungweiten in [cm] ablehnen. Wir sagen, dass wir ein signifikantes Ergebnis vorliegen haben.\n\n\n\n\n\n\nEntscheidung mit dem p-Wert\n\n\n\nWenn der p-Wert \\(\\leq \\alpha\\) dann wird die Nullhypothese (H\\(_0\\)) abgelehnt. Das Signifikanzniveau \\(\\alpha\\) wird als Kulturkonstante auf 5% oder 0.05 gesetzt. Die Nullhypothese (H\\(_0\\)) kann auch Gleichheitshypothese gesehen werden. Wenn die H\\(_0\\) gilt, liegt kein Unterschied zwischen z.B. den Behandlungen vor.\n\n\n\n\n19.2.3 … anhand des 95% Konfidenzintervall\n\nEin statistischer Test der eine Teststatistik \\(T\\) berechnet liefert auch immer einen \\(p\\)-Wert. Nicht alle statistischen Tests ermöglichen es ein 95% Konfidenzintervall zu berechnen. Abbildung 19.5 zeigt ein 95% Konfidenzintervall.\n\n\n\n\n\n\nAbbildung 19.5— Ein 95% Konfidenzintervall. Der Punkt in der Mitte entspricht dem Unterschied oder Effekt \\(\\Delta\\).\n\n\n\nMit p-Werten haben wir Wahrscheinlichkeitsaussagen und damit über die Signifikanz. Damit haben wir noch keine Aussage über die Relevanz des beobachteten Effekts. Mit der Teststatistik \\(T\\) und dem damit verbundenen \\(p\\)-Wert haben wir uns Wahrscheinlichkeiten angeschaut und erhalten eine Wahrscheinlichkeitsaussage. Eine Wahrscheinlichkeitsaussage sagt aber nichts über den Effekt \\(\\Delta\\) aus. Also wie groß ist der mittlere Sprungunterschied zwischen Hunde- und Katzenflöhen. Die Idee von 95% Konfidenzintervallen ist es jetzt den Effekt mit der Wahrscheinlichkeitsaussage zusammenzubringen und beides in einer Visualisierung zu kombinieren. Im Folgenden sehen wir die vereinfachte Formel für das 95% Konfidenzintervall eines t-Tests.\n\\[\n\\left[\n(\\bar{y}_1-\\bar{y}_2) -\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}}; \\;\n(\\bar{y}_1-\\bar{y}_2) +\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}}\n\\right]\n\\]\nDie Formel ist ein wenig komplex, aber im Prinzip einfach, wenn du ein wenig die Formel auf dich wirken lässt. Der linke und der rechte Teil neben dem Semikolon sind fast gleich, bis auf das Plus- und Minuszeichen. Abbildung 19.6 visualisiert die Formel einmal. Wir sehen Folgendes in der Formel und dann in der entsprechenden Abbildung:\n\n\\((\\bar{y}_{1}-\\bar{y}_{2})\\) ist der Effekt \\(\\Delta\\). In diesem Fall der Mittelwertsunterschied. Wir finden den Effekt als Punkt in der Mitte des Intervals.\n\\(T_{\\alpha = 5\\%} \\cdot \\frac {s}{\\sqrt{n}}\\) ist der Wert, der die Arme des Intervalls bildet. Wir vereinfachen die Formel mit \\(s_p\\) für die gepoolte Standardabweichung und \\(n_g\\) für die Fallzahl der beiden Gruppen. Wir nehmen an das beide Gruppen die gleiche Fallzahl \\(n_1 = n_2\\) haben.\n\n\n\n\n\n\n\nAbbildung 19.6— Zusammenhang zwischen der vereinfachten Formel für das 95% Konfidenzintervall und der Visualisierung des 95% Konfidenzintervalls. Der Effektschätzer wird als Punkt in der Mitte des Intervalls dargestellt. Der Effektschäter \\(\\Delta\\) kann entweder ein Mittelwertsunterschied sein oder ein Anteilsunterschied. Bei einem Mittelwertsunterschied kann die Nullhypothese abgelehnt werden, wenn die 0 nicht im Konfidenzintervall ist; bei einem Anteilsunterschied wenn die 1 nicht im Konfidenzintervall ist. Die Arme werden länger oder kürzer je nachdem wie sich die statistischen Maßzahlen \\(s\\) und \\(n\\) verändern.\n\n\n\nWir können eine biologische Relevanz definieren, dadurch das ein 95% Konfidenzintervall die Wahrscheinlichkeitsaussage über die Signifikanz, daher ob die Nullhypothese abgelehnt werden kann, mit dem Effekt zusammenbringt. Wo die Signifikanzschwelle klar definiert ist, hängt die Relevanzschwelle von der wissenschaftlichen Fragestellung und weiteren externen Faktoren ab. Die Signifikanzschwelle liegt bei 0, wenn wir Mittelwerte miteinander vergleichen und bei 1, wenn wir Anteile vergleichen. Abbildung 19.7 zeigt fünf 95% Konfidenzintervalle (a-e), die sich anhand der Signifikanz und Relevanz unterscheiden. Bei der Relevanz ist es wichtig zu wissen in welche Richtung der Effekt gehen soll. Erwarten wir einen positiven Effekt wenn wir die Differenz der beiden Gruppen bilden oder einen negativen Effekt?\n\n\n\n\n\n\nAbbildung 19.7— Verschiedene signifikante und relevante Konfidenzintervalle: (a) signifikant und nicht relevant; (b) nicht signifikant und nicht relevant; (c) signifikant und relevant; (d) signifikant und nicht relevant, der Effekt ist zu klein; (e) signifikant und potenziell relevant, Effekt zeigt in eine unerwartete Richtung gegeben der Relevanzschwelle.\n\n\n\nWir wollen uns nun einmal anschauen, wie sich ein 95% Konfidenzintervall berechnet. Wir nehmen dafür die vereinfachte Formel und setzen die berechneten statistischen Maßzahlen ein. In der Anwendung werden wir die Konfidenzintervalle nicht selber berechnen. Wenn ein statistisches Verfahren Konfidenzintervalle berechnen kann, dann liefert die entsprechende Funktion in R das Konfidenzintervall.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWir nutzen hier eine vereinfachte Formel für das Konfidenzintervall um das Konzept zu verstehen. Später berechnen wir das Konfidenzintervall in R.\n\n\n\n\nEs ergibt sich Folgende ausgefüllte Formel für das 95% Konfidenzintervalls eines t-Tests für das Beispiel des Sprungweitenunterschieds [cm] zwischen Hunde- und Katzenflöhen.\n\\[\n\\left[\n(9.18-7.78) -\n2.78 \\cdot \\frac {0.57}{\\sqrt{4}}; \\;\n(9.18-7.78) +\n2.78 \\cdot \\frac {0.57}{\\sqrt{4}};\n\\right]\n\\]\nmit\n\n\\(\\bar{y}_{cat} = 9.18\\) dem Mittelwert für die Gruppe cat.\n\\(\\bar{y}_{dog} = 7.78\\) dem Mittelwert für die Gruppe dog.\n\\(T_{\\alpha = 5\\%} = 2.78\\) dem kritischen Wert.\n\\(s_p = 0.57\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.61 + 0.53}{2}\\).\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nLösen wir die Formel auf, so ergibt sich folgendes 95% Konfidenzintervall des Mittelwertsunterschiedes der Hunde- und Katzenflöhe.\n\\[[0.64; 2.16]\\]\nWir können sagen, dass mit 95% Wahrscheinlichkeit das Konfidenzintervall den wahren Effektunterschied \\(\\Delta\\) überdeckt. Oder etwas mehr in Prosa, dass wir eine Sprungweitenunterschied von 0.64 cm bis 2.16 cm zwischen Hunde- und Katzenflöhen erwarten würden.\nDie Entscheidung gegen die Nullhypothese bei einem Mittelwertsunterschied erfolgt bei einem 95% Konfidenzintervall danach ob die Null mit im Konfidenzintervall liegt oder nicht. In dem Intervall \\([0.64; 2.16]\\) ist die Null nicht enthalten, also können wir die Nullhypothese ablehnen. Es ist mit einem Unterschied zwischen den mittleren Sprungweiten von Hunde- und Katzenflöhen auszugehen.\nIn unserem Beispiel, könnten wir die Relevanzschwelle für den mittleren Sprungweitenunterschied zwischen Hund- und Katzenflöhen auf 2 cm setzen. In dem Fall würden wir entscheiden, dass der mittlere Sprungweitenunterschied nicht relevant ist, da die 2 cm im Konfidenzintervall enthalten sind. Was wäre wenn wir die Relevanzschwelle auf 4 cm setzen? Dann wäre zwar die Relevanzschwelle nicht mehr im Konfidenzintervall, aber wir hätten Fall (d) in der Abbildung 19.7 vorliegen. Der Effekt ist einfach zu klein, dass der Effekt relevant sein könnte.\nWir können dann die 95% Konfidenzintervall des Mittelwertsunterschiedes der Hunde- und Katzenflöhe auch nochmal richtig in R berechnen. Wir haben ja oben eine einfachere Formel für die gepoolte Standardabweichung genutzt. Wenn wir also ganz genau rechnen wollen, dann sind die 95% Konfidenzintervall wie folgt. Wir nutzen auch hier die Funktion t.test(). Mehr dazu im Kapitel 27, wo wir den t-Test und deren Anwendung im Detail besprechen.\n\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.412      2.39\n\n\n\n\n\n\n\n\nEntscheidung mit dem 95% Konfidenzintervall\n\n\n\nBei der Entscheidung mit dem 95% Konfidenzinterval müssen wir zwei Fälle unterscheiden.\n\nEntweder schauen wir uns einen Mittelwertsunterschied (\\(\\Delta_{y_1-y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 0 im 95% Konfidenzinterval ist.\nOder wir schauen uns einen Anteilsunterschied (\\(\\Delta_{y_1/y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 1 im 95% Konfidenzinterval ist.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Die Testentscheidung</span>"
    ]
  },
  {
    "objectID": "stat-tests-basic.html#sec-delta-n-s",
    "href": "stat-tests-basic.html#sec-delta-n-s",
    "title": "19  Die Testentscheidung",
    "section": "19.3 Auswirkung des Effektes, der Streuung und der Fallzahl",
    "text": "19.3 Auswirkung des Effektes, der Streuung und der Fallzahl\nWir wollen einmal den Zusammenhang zwischen dem Effekt \\(\\Delta\\), der Streuung als Standardabweichung \\(s\\) und Fallzahl \\(n\\) uns näher anschauen. Wir können die Formel des t-Tests wie folgt vereinfachen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_1}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nFür die Betrachtung der Zusammenhänge wandeln wir \\(\\sqrt{2/n_g}\\) in \\(1/n\\) um. Dadurch wandert die Fallzahl \\(n\\) in den Zähler. Die Standardabweichung verallgemeinern wir zu \\(s\\) und damit allgemein zur Streuung. Abschließend betrachten wir \\(\\bar{y}_A-\\bar{y}_B\\) als den Effekt \\(\\Delta\\). Es ergibt sich folgende vereinfachte Formel.\n\\[\nT_{calc} = \\cfrac{\\Delta \\cdot n}{s}\n\\]\nWir können uns nun die Frage stellen, wie ändert sich die Teststatistik \\(T_{calc}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\) in den Daten. Die Tabelle 19.4 zeigt die Zusammenhänge auf. Die Aussagen in der Tabelle lassen sich generalisieren. So bedeutet eine steigende Fallzahl meist mehr signifikante Ergebnisse. Eine steigende Streuung reduziert die Signifikanz eines Vergleichs. Ein Ansteigen des Effektes führt zu mehr signifikanten Ergebnissen. Ebenso verschiebt eine Veränderung des Effekt das 95% Konfidenzintervall, eine Erhöhung der Streuung macht das 95% Konfidenzintervall breiter, eine sinkende Streuung macht das 95% Konfidenzintervall schmaler. Bei der Fallzahl verhält es sich umgekehrt. Eine Erhöhung der Fallzahl macht das 95% Konfidenzintervall schmaler und eine sinkende Fallzahl das Konfidenzintervall breiter.\n\n\n\n\nTabelle 19.4— Zusammenhang von der Teststatistik \\(T_{calc}\\) und dem p-Wert \\(Pr(\\geq T_{calc}|H_0)\\) sowie dem \\(KI_{1-\\alpha}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\n\n\n\\(\\Delta \\uparrow\\)\nsteigt\nsinkt\nverschoben\n\\(\\Delta \\downarrow\\)\nsinkt\nsteigt\nverschoben\n\n\n\\(s \\uparrow\\)\nsinkt\nsteigt\nbreiter\n\\(s \\downarrow\\)\nsteigt\nsinkt\nschmaler\n\n\n\\(n \\uparrow\\)\nsteigt\nsinkt\nschmaler\n\\(n \\downarrow\\)\nsinkt\nsteigt\nbreiter",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Die Testentscheidung</span>"
    ]
  },
  {
    "objectID": "stat-tests-basic.html#referenzen",
    "href": "stat-tests-basic.html#referenzen",
    "title": "19  Die Testentscheidung",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 19.1— Auszug aus dem Zusammenfassung (eng. abstract) aus der Arbeit von Cadiergues u. a. (2000). Der Fokus liegt hier auf der Sprungweite von Hunde- und Katzenflöhen.\nAbbildung 19.2— Die t-Verteilung aller möglichen \\(T_{calc}\\) wenn die Nullhypothese wahr ist. Der Mittelwert der t-Verteilung ist \\(T=0\\). Wenn wir keinen Effekt erwarten würden dann wären die beiden Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) gleich groß. Die Differenz wäre 0. Je größer der \\(T_{calc}\\) wird desto weniger können wir davon ausgehen, dass die beiden Mittelwerte gleich sind. Liegt der \\(T_{calc}\\) über dem kritischen Wert von \\(T_{\\alpha = 5\\%}\\) dann wir die Nullhypothese abgelehnt.\nAbbildung 19.3— Histogramm der 1000 gerechneten Teststaistiken \\(T_{calc}\\), wenn die \\(H_0\\) war wäre und somit kein Unterschied zwischen den Mittelwerten der Sprungweiten der Hunde- und Katzenflöhe vorliegen würde.\nAbbildung 19.4— Die Flächen links und rechts von \\(T_{\\alpha = 2.5\\%}\\) nochmal separat dargestellt. Wir vergleichen bei der Entscheidung mit dem \\(p\\)-Wert nicht die berechnete Teststatistik \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) sondern die Flächen rechts von den jeweiligen Teststatistiken mit \\(A_K = 5\\%\\) und \\(A_{calc}\\) als den \\(p\\)-Wert. An dem Flächenvergleich machen wir dann die Testentscheidung fest.\nAbbildung 19.5— Ein 95% Konfidenzintervall. Der Punkt in der Mitte entspricht dem Unterschied oder Effekt \\(\\Delta\\).\nAbbildung 19.6— Zusammenhang zwischen der vereinfachten Formel für das 95% Konfidenzintervall und der Visualisierung des 95% Konfidenzintervalls. Der Effektschätzer wird als Punkt in der Mitte des Intervalls dargestellt. Der Effektschäter \\(\\Delta\\) kann entweder ein Mittelwertsunterschied sein oder ein Anteilsunterschied. Bei einem Mittelwertsunterschied kann die Nullhypothese abgelehnt werden, wenn die 0 nicht im Konfidenzintervall ist; bei einem Anteilsunterschied wenn die 1 nicht im Konfidenzintervall ist. Die Arme werden länger oder kürzer je nachdem wie sich die statistischen Maßzahlen \\(s\\) und \\(n\\) verändern.\nAbbildung 19.7— Verschiedene signifikante und relevante Konfidenzintervalle: (a) signifikant und nicht relevant; (b) nicht signifikant und nicht relevant; (c) signifikant und relevant; (d) signifikant und nicht relevant, der Effekt ist zu klein; (e) signifikant und potenziell relevant, Effekt zeigt in eine unerwartete Richtung gegeben der Relevanzschwelle.\n\n\n\nCadiergues M-C, Joubert C, Franc M. 2000. A comparison of jump performances of the dog flea, Ctenocephalides canis (Curtis, 1826) and the cat flea, Ctenocephalides felis felis (Bouché, 1835). Veterinary parasitology 92: 239–241.\n\n\nGigerenzer G, Krauss S, Vitouch O. 2004. The null ritual. The Sage handbook of quantitative methodology for the social sciences 391–408.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Die Testentscheidung</span>"
    ]
  },
  {
    "objectID": "stat-tests-theorie.html",
    "href": "stat-tests-theorie.html",
    "title": "20  Die Testtheorie",
    "section": "",
    "text": "20.1 Der \\(\\alpha\\)-Fehler und der \\(\\beta\\)-Fehler\nVielleicht ist die Idee der Testtheorie und der Testentscheidung besser mit der Analogie des Rauchmelders zu verstehen. Wir nehmen an, dass der Rauchmelder der statistische Test ist. Der Rauchmelder hängt an der Decke und soll entscheiden, ob es brennt oder nicht. Daher muss der Rauchmelder entscheiden, die Nullhypothese “kein Feuer” abzulehnen oder die Hypothese “kein Feuer” beizubehalten.\n\\[\n\\begin{aligned}\nH_0&: \\mbox{kein Feuer im Haus}  \\\\  \nH_A&: \\mbox{Feuer im Haus}  \\\\   \n\\end{aligned}\n\\]\nWir können jetzt den Rauchmelder so genau einstellen, dass der Rauchmelder bei einer Kerze losgeht. Oder aber wir stellen den Rauchmelder so ein, dass er erst bei einem Stubenbrand ein Piepen von sich gibt. Wie sensibel auf Rauch wollen wir den Rauchmelder einstellen? Soll der Rauchmelder sofort die Nullhypothese ablehnen? Wenn also nur eine Kerze brennt. Soll also der \\(\\alpha\\)-Fehler groß sein? Erinnere dich, mit einem großen \\(\\alpha\\)-Fehler würden wir mehr Nullhypothesen ablehnen oder anders gesprochen leichter die Nullhypothese ablehnen. Wir würden ja den \\(\\alpha\\)-Fehler zum Beispiel von 5% auf 20% setzen können. Das wäre nicht sehr sinnvoll. Die Feuerwehr würde schon bei einer Kerze kommen oder wenn wir mal was anbrennen. Wir dürfen also den \\(\\alpha\\)-Fehler nicht zu groß einstellen.\nIntuitiv würde man meinen, ein sehr kleiner \\(\\alpha\\)-Fehler nun sinnvoll sei. Wenn wir aber den Rauchmelder sehr unsensibel einstellen, also der Rauchmelder erst bei sehr viel Rauch die Nullhypothese ablehnt, könnte das Haus schon unrettbar in Flammen stehen. Dieser Fehler, Haus steht in Flammen und der Rauchmelder geht nicht, wird als \\(\\beta\\)-Fehler bezeichnet. Wie du siehst hängen die beiden Fehler miteinander zusammen. Wichtig hierbei ist immer, dass wir uns einen Zustand vorstellen, das Haus brennt nicht (\\(H_0\\) ist wahr) oder das Haus brennt nicht (\\(H_A\\) ist wahr). An diesem Zustand entscheiden wir dann, wie hoch der Fehler jeweils sein soll diesen Zustand zu übersehen.\nWie sieht nun die Lösung, erstmal für unseren Rauchmelder, aus? Wir müssen Grenzen für den \\(\\alpha\\) und \\(\\beta\\)-Fehler festlegen bei denen der Rauchmelder angeht und wir die Feuerwehr rufen.\nNachdem wir uns die Testentscheidung mit der Analogie des Rauchmelders angesehen haben, wollen wir uns wieder der Statistik zuwenden. Betrachten wir das Problem nochmal von der theoretischen Seite mit den statistischen Fachbegriffen.\nSoweit haben wir es als gegeben angesehen, dass wir eine Testentscheidung durchführen. Entweder mit der Teststatistik, dem \\(p\\)-Wert oder dem 95% Konfidenzintervall. Immer wenn wir eine Entscheidung treffen, können wir auch immer eine falsche Entscheidung treffen. Wie wir wissen hängt die berechnete Teststatistik \\(T_{calc}\\) nicht nur vom Effekt \\(\\Delta\\) ab sondern auch von der Streuung \\(s\\) und der Fallzahl \\(n\\). Auch können wir den falschen Test wählen oder Fehler im Design des Experiments gemacht haben. Schlussendlich gibt es viele Dinge, die unsere simple mathematischen Formeln beeinflussen können, die wir nicht kennen. Ein frequentistischer Hypothesentest gibt immer nur eine Aussage über die Nullhypothese wieder. Also ob wir die Nullhypothese ablehnen können oder nicht.\nAbbildung 20.2 zeigt die theoretische Verteilung der Nullyhypothese und der Alternativehypothese. Wenn die beiden Verteilungen sehr nahe beieinander sind, wird es schwer für den statistischen Test die Hypothesen klar voneinander zu trennen. Die Verteilungen überlappen. Es gibt einen sehr kleinen Unterschied in den Sprungweiten zwischen Hunde- und Katzenflöhen. In dem Beispiel wurde der gesamte \\(\\alpha\\)-Fehler auf die rechte Seite gelegt. Das macht die Darstellung etwas einfacher.\nIn der Regression wird uns auch wieder das \\(\\beta\\) als Symbol begegnen. In der statistischen Testtheorie ist das \\(\\beta\\) ein Fehler; in der Regression ist das \\(\\beta\\) ein Koeffizient der Regression. Hier ist der Kontext wichtig.\nWir können daher bei statistischen Testen zwei Arten von Fehlern machen. Zum einen den \\(\\alpha\\) Fehler oder auch Type I Fehler genannt. Zum anderen den \\(\\beta\\) Fehler oder auch Type II Fehler genannt. Die Grundidee basiert darauf, dass wir eine Testentscheidung gegen die Nullhypothese machen. Diese Entscheidung kann richtig sein, da in Wirklichkeit die Nullhypothese gilt oder aber falsch sein, da in Wirklichkeit die Nullhypothese nicht gilt. In Abbildung 20.3 wird der Zusammenhang in einer 2x2 Tafel veranschaulicht.\nBeide Fehler sind Kulturkonstanten. Das heißt, dass sich diese Zahlen von 5% und 20% so ergeben haben. Es gibt keinen rationalen Grund diese Zahlen so zu nehmen. Prinzipiell schon, aber die Gründe leigen eher in der Anwendbarkeit von feststehenden Tabellen vor der Entwicklung des Computers. Man kann eigentlich sagen, dass die 5% und die 20% eher einem Zufall entsprungen sind, als einer tieferen Rationalen. Wir behalten diese beiden Zahlen bei aus den beiden schlechtesten Gründe überhaupt: i) es wurde schon immer so gemacht und ii) viele machen es so. Die Diskussion über den \\(p\\)-Wert und dem Vergleich mit dem \\(\\alpha\\)-Fehler wird in der Statistik seit 2019 verstärkt diskutiert (Wasserstein u. a. 2019). Das Nullritual wird schon lamge kritisiert (Gigerenzer u. a. 2004). Siehe dazu auch The American Statistician, Volume 73, Issue sup1 (2019).\nEine weitere wichtige statistische Maßzahl im Kontext der Testtheorie ist die \\(Power\\) oder auch \\(1-\\beta\\). Die \\(Power\\) ist die Gegenwahrscheinlichkeit von dem \\(\\beta\\)-Fehler. In der Analogie des Rauchmelders wäre die \\(Power\\) daher Alarm with fire. Das heißt, wie wahrscheinlich ist es einen wahren Effekt - also einen Unterschied - mit dem statistischen Test auch zu finden. Oder anders herum, wenn wir wüssten, dass die Hunde- und Katzenflöhe unterschiedliche weit springen, mit welcher Wahrscheinlichkeit würde diesen Unterschied ein statistsicher Test auch finden? Mit eben der \\(Power\\), also gut 80%. Tabelle 20.1 zeigt die Abhängigkeit der \\(Power\\) vom Effekt \\(\\Delta\\), der Streuung \\(s\\) und der Fallzahl \\(n\\). Die \\(Power\\) ist eine Wahrscheinlichkeit und sagt nichts über die Relevanz des Effektes aus.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Die Testtheorie</span>"
    ]
  },
  {
    "objectID": "stat-tests-theorie.html#sec-alpha-beta",
    "href": "stat-tests-theorie.html#sec-alpha-beta",
    "title": "20  Die Testtheorie",
    "section": "",
    "text": "Der \\(\\alpha\\)-Fehler und \\(\\beta\\)-Fehler als Rauchmelderanalogie\n\n\n\nHäufig verwirrt die etwas theoretische Herangehensweise an den \\(\\alpha\\)-Fehler und \\(\\beta\\)-Fehler. Wir versuchen hier nochmal die Analogie eines Rauchmelders und dem Feuer im Haus.\n\n\n\n\n\n\nAbbildung 20.1— Andere Art der Darstellung des \\(\\alpha\\)-Fehlers als Alarm without fire und dem \\(\\beta\\)-Fehler als Fire without alarm. Je nachdem wie empfindlich wir den Alarm des Rauchmelders (den statistischen Test) über das \\(\\alpha\\) einstellen, desto mehr Alarm bekommen wir ohne das ein Effekt vorhanden wäre. Drehen wir den Alarm zu niedrig, dann kriegen wir kein Feuer mehr angezeigt, den \\(\\beta\\)-Fehler.\n\n\n\n\n\\(\\boldsymbol{\\alpha}\\)-Fehler: Alarm without fire. Der statistische Test schlägt Alarm und wir sollen die \\(H_0\\) ablehnen, obwohl die \\(H_0\\) in Wahrheit gilt und kein Effekt vorhanden ist.\n\\(\\boldsymbol{\\beta}\\)-Fehler: Fire without alarm. Der statistische Test schlägt nicht an und wir sollen die \\(H_0\\) beibehalten, obwohl die \\(H_0\\) in Wahrheit nicht gilt und ein Effekt vorhanden ist.\n\n\n\n\n\nWir setzen den \\(\\alpha\\)-Fehler auf 5%. Somit haben wir in 1 von 20 Fällen das Problem, dass uns der Rauchmelder angeht obwohl gar kein Feuer da ist. Wir lehnen die Nullhypothese ab, obwohl die Nullhypothese gilt.\nAuf der anderen Seite setzen wir den \\(\\beta\\)-Fehler auf 20%. Damit brennt uns die Bude in 1 von 5 Fällen ab ohne das der Rauchmelder einen Pieps von sich gibt. Wir behalten die Nullhypothese bei, obwohl die Nullhypothese nicht gilt.\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 20.2— Darstellung der Null- und Alternativehypothese. Mit steigendem \\(T_{calc}\\) wird die Wahrscheinlichkeit für die \\(H_0\\) immer kleiner. Leider ist uns nichts über \\(H_A\\) und deren Lage bekannt. Sollte die \\(H_A\\) Verteilung zu weit nach links ragen, könnten wir die \\(H_0\\) beibehalten, obwohl die \\(H_A\\) gilt.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 20.3— Zusammenhang zwischen der Testentscheidung gegen die \\(H_0\\) Hypothese sowie dem Beibehalten der \\(H_0\\) Hypothese und der unbekannten Wahrheit in der die \\(H_0\\) falsch sein kann oder die \\(H_0\\) wahr sein kann. Wir können mit unserer Testenstscheidung richtig liegen oder falsch. Mit welcher Wahrscheinlichkeit geben der \\(\\alpha\\) Fehler und \\(\\beta\\) Fehler wieder. Unten rechts ist der Zusammenhang zu der Abbildung 20.2 gezeigt.\n\n\n\n\n\n\n\n\n\nTabelle 20.1— Abhängigkeit der \\(Power (1-\\beta)\\) vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\). Die \\(Power\\) ist eine Wahrscheinlichkeit und sagt nichts über die Relevanz des Effektes aus.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{Power (1-\\beta)}\\)\n\n\\(\\boldsymbol{Power (1-\\beta)}\\)\n\n\n\n\n\\(\\Delta \\uparrow\\)\nsteigt\n\\(\\Delta \\downarrow\\)\nsinkt\n\n\n\\(s \\uparrow\\)\nsinkt\n\\(s \\downarrow\\)\nsteigt\n\n\n\\(n \\uparrow\\)\nsteigt\n\\(n \\downarrow\\)\nsinkt",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Die Testtheorie</span>"
    ]
  },
  {
    "objectID": "stat-tests-theorie.html#sec-einseitig-zweiseitig",
    "href": "stat-tests-theorie.html#sec-einseitig-zweiseitig",
    "title": "20  Die Testtheorie",
    "section": "20.2 Einseitig oder zweiseitig?",
    "text": "20.2 Einseitig oder zweiseitig?\nManchmal kommt die Frage auf, ob wir einseitig oder zweiseitig einen statistischen Test durchführen wollen. Beim Fall des zweiseitigen Testens verteilen wir den \\(\\alpha\\)-Fehler auf beide Seiten der Testverteilung mit jeweils \\(\\cfrac{\\alpha}{2}\\). In dem Fall des einseitigen Tests liegt der gesamte \\(\\alpha\\)-Fehler auf der rechten oder linken Seite der Testverteilung. In Abbildung 20.4 wird der Zusammenhang beispielhaft an der t-Verteilung gezeigt.\n\n\n\n\n\n\n\nAbbildung 20.4— Zusammenhang zwischen dem einseitigen und zweiseitigen Testen. Im Falle des zweiseitigen Testens teilen wir den \\(\\alpha\\)-Fehler auf beide Seiten der beispielhaften t-Verteilung auf. Im Falle des einseitigen Testen leigt der gesamte \\(\\alpha\\)-Fehler auf der rechten oder der linken Seite der t-Verteilung.\n\n\n\n\nIn der Anwendung testen wir immer zweiseitig. Der Grund ist, dass das Vorzeichen von der Teststatik davon abhängt, welche der beiden Gruppen den größeren Mittelwert hat. Da wir die Mittelwerte vor der Auswertung nicht kennen, können wir auch nicht sagen in welche Richtung der Effekt und damit die Teststatistik laufen wird.\nEs gibt theoretisch Gründe, die für ein einseitiges Testen unter bestimmten Bedingungen sprechen, aber wir nutzen in der Anwendung nur das zweiseite Testen. Wir müssen dazu in R auch nichts weiter angeben. Ein durchgeführter statistischer Test in R testet automatisch immer zweiseitig.\n\n\n\n\n\n\nEinseitig oder zweiseitig im Spiegel der Regulierungsbehörden\n\n\n\nIn den allgemeinen Methoden des IQWiG, einer Regulierungsbehörde für klinische Studien, wird grundsätzlich das zweiseitige Testen empfohlen. Wenn einseitig getestet werden sollte, so soll das \\(\\alpha\\)-Niveau halbiert werden. Was wiederum das gleiche wäre wie zweiseitiges Testen - nur mit mehr Arbeit.\nZur besseren Vergleichbarkeit mit 2-seitigen statistischen Verfahren wird in einigen Guidelines für klinische Studien eine Halbierung des üblichen Signifikanzniveaus von 5 % auf 2,5 % gefordert. – Allgemeine Methoden Version 6.1 vom 24.01.2022, p. 180",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Die Testtheorie</span>"
    ]
  },
  {
    "objectID": "stat-tests-theorie.html#sec-statistisches-testen-alpha-adjust",
    "href": "stat-tests-theorie.html#sec-statistisches-testen-alpha-adjust",
    "title": "20  Die Testtheorie",
    "section": "20.3 Adjustierung für multiple Vergleiche",
    "text": "20.3 Adjustierung für multiple Vergleiche\nIm Kapitel 33 werden wir mehrere multiple Gruppenvergleiche durchführen. Das heißt, wir wollen nicht nur die Sprungweite von Hunde- und Katzenflöhen miteinander vergleichen, sondern auch die Sprungweite von Hunde- und Fuchsflöhen sowie Katzen- und Fuchsflöhen. Wir würden also \\(k = 3\\) t-Tests für die Mittelwertsvergleiche rechnen.\nDieses mehrfache Testen führt aber zu einer Inflation des \\(\\alpha\\)-Fehlers oder auch Alphafehler-Kumulierung genannt. Daher ist die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird, nicht mehr durch das Signifikanzniveau \\(\\alpha\\) kontrolliert, sondern kann sehr groß werden.\nGehen wir von einer Situation mit \\(k\\) Null- und Alternativhypothesen aus. Wir rechnen also \\(k\\) statistische Tests und alle Nullhypothesen werden zum lokalen Niveau \\(\\alpha_{local} = 0.05\\) getestet. Im Weiteren nehmen wir an, dass tatsächlich alle Nullhypothesen gültig sind. Wir rechnen also \\(k\\) mal einen t-Test und machen jedes mal einen 5% Fehler Alarm zu geben, obwohl kein Effekt vorhanden ist.\nDie Wahrscheinlichkeit für einen einzelnen Test korrekterweise \\(H_0\\) abzulehnen ist \\((1 − \\alpha)\\). Da die \\(k\\) Tests unabhängig sind, ist die Wahrscheinlichkeit alle \\(k\\) Tests korrekterweise abzulehnen \\((1 − \\alpha)^k\\). Somit ist die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird \\(1-(1-\\alpha)^k\\). In der Tabelle 20.2 wird dieser Zusammenhang nochmal mit Zahlen für verschiedene \\(k\\) deutlich.\n\n\n\nTabelle 20.2— Inflation des \\(\\alpha\\)-Fehlers. Wenn 50 Hypothesen getestet werden, ist die Wahrscheinlichkeit mindestens eine falsche Testentscheidung zu treffen fast sicher.\n\n\n\n\n\nAnzahl Test \\(\\boldsymbol{k}\\)\n\\(\\boldsymbol{1-(1-\\alpha)^k}\\)\n\n\n\n\n1\n0.05\n\n\n2\n0.10\n\n\n10\n0.40\n\n\n50\n0.92\n\n\n\n\n\n\nAus Tabelle 20.3 können wir entnehmen, dass wenn 100 Hypothesen getestet werden, werden 5 Hypothesen im Schnitt fälschlicherweise abgelehnt. Die Tabelle 20.3 ist nochmal die Umkehrung der vorherigen Tabelle 20.2.\n\n\n\nTabelle 20.3— Inflation des \\(\\alpha\\)-Fehlers. Erwartete Anzahl fälschlich abgelehnter Nullhypothesen abhängig von der Anzahl der durchgeführten Tests\n\n\n\n\n\nAnzahl Test \\(\\boldsymbol{k}\\)\n\\(\\boldsymbol{\\alpha \\cdot k}\\)\n\n\n\n\n1\n0.05\n\n\n20\n1\n\n\n100\n5\n\n\n200\n10\n\n\n\n\n\n\nNachdem wir verstanden haben, dass wiederholtes statistisches Testen irgendwann immer ein signifikantes Ergebnis produziert, müssen wir für diese \\(\\alpha\\) Inflation unsere Ergebnisse adjustieren. Ich folgenden stelle ich verschiedene Adjustierungsverfahren vor.\nWie können wir nun die p-Werte in R adjustieren? Zum einen passiert dies teilweise automatisch zum anderen müssen wir aber wissen, wo wir Informationen zu den Adjustierungsmethoden finden. Die Funktion p.adjust() ist hier die zentrale Anlaufstelle. Hier finden sich alle implementierten Adjustierungsmethoden in R.\nIm folgenden Code erschaffen wir uns 50 \\(z\\)-Werte von denen 25 aus einer Normalverteilung \\(\\mathcal{N}(0, 1)\\) und 25 aus einer Normalverteilung mit \\(\\mathcal{N}(3, 1)\\) kommen. Die Fläche unter allen Normalverteilungen ist Eins, da die Standatdabweichung Eins ist. Wir berechnen die \\(p-Wert\\) anhand der Fläche rechts von dem \\(z\\)-Wert. Wir testen zweiseitig, deshalb multiplizieren wir die \\(p\\)-Werte mit Zwei. Diese \\(p\\)-Werte können wir nun im Folgenden für die Adjustierung nutzen.\n\nz &lt;- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))\np &lt;- 2*pnorm(sort(-abs(z)))\n\nÜber die eckigen Klammern [] und das : können wir uns die ersten zehn p-Werte wiedergeben lassen.\n\np[1:10] |&gt; round(5)\n\n [1] 0.00000 0.00001 0.00007 0.00009 0.00027 0.00050 0.00124 0.00251 0.00324\n[10] 0.00442\n\n\nWir sehen, dass die ersten fünf p-Werte hoch signifikant sind. Das würden wir auch erwarten, immerhin haben wir ja auch 25 \\(z\\)-Werte mit einem Mittelwert von Drei. Du kannst dir den \\(z\\)-Wert wie den \\(t\\)-Wert der Teststatistik vorstellen.\n\n20.3.1 Bonferroni Korrektur\nDie Bonferroni Korrektur ist die am weitesten verbreitete Methode zur \\(\\alpha\\) Adjustierung, da die Bonferroni Korrektur einfach durchzuführen ist. Damit die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird beim simultanen Testen von \\(k\\) Hypothesen durch das globale (und multiple) Signifikanzniveau \\(\\alpha = 5\\%\\) kontrolliert ist, werden die Einzelhypothesen zum lokalen Signifikanzniveau \\(\\alpha_{local} = \\tfrac{\\alpha_{5\\%}}{k}\\) getestet.\nDabei ist das Problem der Bonferroni Korrektur, dass die Korrektur sehr konservativ ist. Wir meinen damit, dass das tatsächliche globale (und multiple) \\(\\sum\\alpha_{local}\\) Niveau liegt deutlich unter \\(\\alpha_{5\\%}\\) und somit werden die Nullhypothesen zu oft beibehalten.\n\n\n\n\n\n\nAdjustierung des \\(\\boldsymbol{\\alpha}\\)-Fehlers\n\n\n\n\nDas globale \\(\\alpha\\)-Level wird durch die Anzahl \\(k\\) an durchgeführten statistischen Tests geteilt.\n\\(\\alpha_{local} = \\tfrac{\\alpha}{k}\\) für die Entscheidung \\(p &lt; \\alpha_{local}\\)\n\n\n\n\n\n\n\n\n\nAdjustierung des \\(\\boldsymbol{p}\\)-Wertes\n\n\n\n\nDie p-Werte werden mit der Anzahl an durchgeführten statistischen Tests \\(k\\) multipliziert.\n\\(p_{adjust} = p_{raw} \\cdot k\\) mit \\(k\\) gleich Anzahl der Vergleiche.\nwenn \\(p_{adjust} &gt; 1\\), wird \\(p_{adjust}\\) gleich 1 gesetzt, da \\(p_{adjust}\\) eine Wahrscheinlichkeit ist.\n\n\n\nWir schauen uns die ersten zehn nach Bonferroni adjustierten p-Wert nach der Anwendung der Funktion p.adjust() einmal an.\n\np.adjust(p, \"bonferroni\")[1:10] |&gt; round(3)\n\n [1] 0.000 0.000 0.004 0.005 0.014 0.025 0.062 0.126 0.162 0.221\n\n\nNach der Adjustierung erhalten wir weniger signifikante \\(p\\)-Werte als vor der Adjustierung. Wir sehen aber, dass wir weit weniger signifikante Ergebnisse haben, als wir eventuell erwarten würden. Wir haben immerhin 25 \\(z\\)-Werte mit einem Mittelwert von Drei. Nach der Bonferroni-Adjustierung hgaben wir nur noch sechs signifikante \\(p\\)-Werte.\n\n\n20.3.2 Benjamini-Hochberg\nDie Benjamini-Hochberg Adjustierung für den \\(\\alpha\\)-Fehler wird auch Adjustierung nach der false discovery rate (abk. FDR) bezeichnet. Meistens werden beide Namen synoym verwendet, der Trend geht jedoch hin zur Benennung mit der Abkürzung FDR. In der Tabelle 20.4 sehen wir einmal ein Beispiel für die FDR Adjustierung. Die Idee ist, dass wir uns für jeden der \\(m\\) Vergleiche eine eigene lokale Signifikanzschwelle \\(\\alpha_{local}\\) berechnen. Dafür rangieren wir zuerst unsere Vergleiche nach dem \\(p\\)-Wert. Der kleinste \\(p\\)-Wert kommt zuerst und dann der Rest der anderen \\(p\\)-Werte. Wir berechnen jedes lokale Signifkanzniveau mit \\(\\alpha_{local} =(i/m)\\cdot Q\\). Dabei steht das \\(i\\) für den jeweiligen Rang und das \\(m\\) für unsere Anzahl an Vergleichen. In unserem Beispiel haben wir \\(m = 25\\) Vergleiche. Jetzt kommt der eugentlich spannende Teil. Wir können jetzt \\(Q\\) als unsere false discovery rate selber wählen! In unserem Beispiel setzen wir die FDR auf 25%. Es geht aber auch 20% oder 10%. Wie du möchtest.\nIn der letzten Spalte der Tavbelle siehst du die Entscheidung, ob eine Variable noch signifkant ist oder nicht. Wir entscheiden nach folgender Regel. Der fettgedruckte p-Wert für den fertilizer ist der höchste p-Wert, der auch kleiner mit \\(0.042 &lt; 0.050\\) als der kritische Wert ist. Alle darüber liegenden Werte und damit diejenigen mit niedrigeren p-Werten werden hervorgehoben und als signifikant betrachtet, auch wenn diese p-Werte unter den kritischen Werten liegen. Beispielsweise sind N und sun einzeln nicht signifikant, wenn Sie das Ergebnis mit der letzten Spalte vergleichen. Mit der FDR-Korrektur werden sie jedoch als signifikant angesehen.\n\n\n\nTabelle 20.4— Beispiel für die Benjamini-Hochberg-Prozedur der \\(\\alpha\\)-Fehleradjustierung.\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\\(\\boldsymbol{Pr(D|H_0)}\\)\nRang (\\(\\boldsymbol{i}\\))\n\\(\\boldsymbol{(i/m)\\cdot Q}\\)\n\n\n\n\n\nfe\n0.001\n1\n\\(1/25 \\cdot 0.25 = 0.01\\)\ns.\n\n\nwater\n0.008\n2\n\\(2/25 \\cdot 0.25 = 0.02\\)\ns.\n\n\nN\n0.039\n3\n\\(3/25 \\cdot 0.25 = 0.03\\)\ns.\n\n\nsun\n0.041\n4\n\\(4/25 \\cdot 0.25 = 0.04\\)\ns.\n\n\nfertilizer\n0.042\n5\n\\(5/25 \\cdot 0.25 = 0.05\\)\ns.\n\n\ninfected\n0.060\n6\n\\(6/25 \\cdot 0.25 = 0.06\\)\nn.s.\n\n\nwind\n0.074\n7\n\\(7/25 \\cdot 0.25 = 0.07\\)\nn.s.\n\n\nS\n0.205\n8\n\\(8/25 \\cdot 0.25 = 0.08\\)\nn.s.\n\n\n…\n…\n…\n…\n…\n\n\nblock\n0.915\n25\n\\(25/25 \\cdot 0.25 = 0.25\\)\nn.s.\n\n\n\n\n\n\nDie adjustierten \\(p\\)-Werte nach der Benjamini-Hochberg-Prozedur ist etwas umständlicher, deshalb benutzen wir hier die Funktion p.adjust() in R und erhalten damit die adjustierten \\(p\\)-Werte wieder.\n\np.adjust(p, \"BH\")[1:10] |&gt; round(3)\n\n [1] 0.000 0.000 0.001 0.001 0.003 0.004 0.009 0.016 0.018 0.022\n\n\nMit der Bonferroni Adjustierung und der FDR Adjustierung haben wir zwei sehr gute Möglichkeiten für das multiple Testen zu korrigieren. Die FDR Korrektur wird häufiger in der Genetik bzw. Bioinformatik eingesetzt. In dem Kontext kommt auch die recht ähnliche Benjamini & Yekutieli (abk. BY) Adjustierung vor. Die Benjamini & Yekutieli unterscheidet sich aber nur in Nuancen unter bestimmten Rahmenbedingungen von der FDR Adjustierung. Wir lassen den statistischen Engel hier mal am Straßenrand stehen und wenden uns der letzten Adjustierung zu.\n\n\n20.3.3 Dunn-Sidak\nDie Sidak Korrektur berechnet das lokale Signifikanzniveau \\(\\alpha_{local}\\) auf folgende Art und Weise. Wir haben ein globales \\(\\alpha\\) von 5%. Das \\(\\alpha_{local}\\) berechnen wir indem wir die Anzahl der Vergleiche \\(m\\) wie folgt miteinander verbinden.\n\\[\n\\alpha_{local} = 1 - (1 - \\alpha)^{\\tfrac{1}{m}}\n\\]\nDie Sidak Korrektur ist weniger streng als die Bonferroni-Korrektur, aber das auch nur sehr geringfügig. Zum Beispiel beträgt für \\(\\alpha = 0.05\\) und \\(m = 10\\) das Bonferroni-bereinigte lokale Signifikanzniveau \\(\\alpha_{local} = 0.005\\) und das nach Sidak Korrektur ungefähr \\(0.005116\\). Das ist jetzt auch kein so großer Unterschied. Wir finden die Sidak Korrektur dann im R Paket {emmeans} für die Adjustierung bei den multiplen Vergleichen wieder.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Die Testtheorie</span>"
    ]
  },
  {
    "objectID": "stat-tests-theorie.html#referenzen",
    "href": "stat-tests-theorie.html#referenzen",
    "title": "20  Die Testtheorie",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 20.1— Andere Art der Darstellung des \\(\\alpha\\)-Fehlers als Alarm without fire und dem \\(\\beta\\)-Fehler als Fire without alarm. Je nachdem wie empfindlich wir den Alarm des Rauchmelders (den statistischen Test) über das \\(\\alpha\\) einstellen, desto mehr Alarm bekommen wir ohne das ein Effekt vorhanden wäre. Drehen wir den Alarm zu niedrig, dann kriegen wir kein Feuer mehr angezeigt, den \\(\\beta\\)-Fehler.\nAbbildung 20.2— Darstellung der Null- und Alternativehypothese. Mit steigendem \\(T_{calc}\\) wird die Wahrscheinlichkeit für die \\(H_0\\) immer kleiner. Leider ist uns nichts über \\(H_A\\) und deren Lage bekannt. Sollte die \\(H_A\\) Verteilung zu weit nach links ragen, könnten wir die \\(H_0\\) beibehalten, obwohl die \\(H_A\\) gilt.\nAbbildung 20.3— Zusammenhang zwischen der Testentscheidung gegen die \\(H_0\\) Hypothese sowie dem Beibehalten der \\(H_0\\) Hypothese und der unbekannten Wahrheit in der die \\(H_0\\) falsch sein kann oder die \\(H_0\\) wahr sein kann. Wir können mit unserer Testenstscheidung richtig liegen oder falsch. Mit welcher Wahrscheinlichkeit geben der \\(\\alpha\\) Fehler und \\(\\beta\\) Fehler wieder. Unten rechts ist der Zusammenhang zu der Abbildung 20.2 gezeigt.\nAbbildung 20.4— Zusammenhang zwischen dem einseitigen und zweiseitigen Testen. Im Falle des zweiseitigen Testens teilen wir den \\(\\alpha\\)-Fehler auf beide Seiten der beispielhaften t-Verteilung auf. Im Falle des einseitigen Testen leigt der gesamte \\(\\alpha\\)-Fehler auf der rechten oder der linken Seite der t-Verteilung.\n\n\n\nGigerenzer G, Krauss S, Vitouch O. 2004. The null ritual. The Sage handbook of quantitative methodology for the social sciences 391–408.\n\n\nWasserstein RL, Schirm AL, Lazar NA. 2019. Moving to a world beyond „p&lt; 0.05“. The American Statistician 73: 1–19.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Die Testtheorie</span>"
    ]
  },
  {
    "objectID": "stat-tests-effect.html",
    "href": "stat-tests-effect.html",
    "title": "21  Der Effektschätzer",
    "section": "",
    "text": "21.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, see, scales,\n               effectsize, parameters, broom, readxl,\n               emmeans, vcd, irr, conflicted)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(effectsize::oddsratio)\nconflicts_prefer(magrittr::set_names)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Der Effektschätzer</span>"
    ]
  },
  {
    "objectID": "stat-tests-effect.html#unterschied-zweier-mittelwerte",
    "href": "stat-tests-effect.html#unterschied-zweier-mittelwerte",
    "title": "21  Der Effektschätzer",
    "section": "21.2 Unterschied zweier Mittelwerte",
    "text": "21.2 Unterschied zweier Mittelwerte\nWir berechnen zwei Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\). Wenn wir wissen wollen wie groß der Effekt zwischen den beiden Mittelwerten ist, dann bilden wir die Differenz. Wir berechnen das \\(\\Delta_{y_1-y_2}\\) für \\(y_1\\) und \\(y_2\\) indem wir die beiden Mittelwerte voneinander abziehen.\n\\[\n\\Delta_{y_1-y_2} = \\bar{y}_1 - \\bar{y}_2\n\\]\nWenn es keinen Unterschied zwischen den beiden Mittelwerten \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) gibt, dann ist die Differenz \\(\\Delta_{y_1-y_2} = \\bar{y}_1 - \\bar{y}_2\\) gleich 0. Wir sagen, die Nullhypothese vermutlich gilt, wenn die Differenz klein ist. Warum schreiben wir hier vermutlich? Ein statistischer Test ist eine Funktion von \\(\\Delta\\), \\(s\\) und \\(n\\). Wir können auch mit kleinem \\(\\Delta\\) die Nullhypothese ablehnen, wenn \\(s\\) und \\(n\\) eine passende Teststatistik generieren. Siehe dazu auch das Kapitel 19.3. Was wir besser annehmen können ist, dass die Relevanz klein ist. Effekt mit einem geringen Mittelwertsunterschied sind meistens nicht relevant. Aber diese Einschätzung hängt stark von der Fragestellung ab.\n\\[\nH_0: \\Delta_{y_1-y_2} = \\bar{y}_1 - \\bar{y}_2 = 0\n\\]\nIn Tabelle 21.1 ist nochmal ein sehr simples Datenbeispiel gegeben an dem wir den Zusammenhang nochmal nachvollziehen wollen.\n\n\n\n\nTabelle 21.1— Beispiel für die Berechnung von einem Mittelwertseffekt an der Sprunglänge [cm] von Hunde und Katzenflöhen.\n\n\n\n\n\n\nanimal\njump_length\n\n\n\n\ncat\n8.0\n\n\ncat\n7.9\n\n\ncat\n8.3\n\n\ncat\n9.1\n\n\ndog\n8.0\n\n\ndog\n7.8\n\n\ndog\n9.2\n\n\ndog\n7.7\n\n\n\n\n\n\n\n\nNehmen wir an, wir berechnen für die Sprungweite [cm] der Hundeflöhe einen Mittelwert von \\(\\bar{y}_{dog} = 8.2\\) und für die Sprungweite [cm] der Katzenflöhe einen Mittelwert von \\(\\bar{y}_{cat} =8.3\\). Wie große ist nun der Effekt? Oder anders gesprochen, welchen Unterschied in der Sprungweite macht es aus ein Hund oder eine Katze zu sein? Was ist also der Effekt von animal? Wir rechnen \\(\\bar{y}_{dog} - \\bar{y}_{cat} = 8.2 - 8.3 = -0.1\\). Zum einen wissen wir jetzt “die Richtung”. Da wir ein Minus vor dem Mittelwertsunterschied haben, müssen die Katzenflöhe weiter springen als die Hundeflöhe, nämlich 0.1 cm. Dennoch ist der Effekt sehr klein.\n\n21.2.1 Cohen’s d\nDa der Mittlwertsunterschied alleine nnur eine eingeschränkte Aussage über den Effekt erlaubt, gibt es noch Effektschätzer, die den Mittelwertsunterschied \\(\\Delta_{y_1-y_2}\\) mit der Streuung \\(s^2\\) sowie der Fallzahl zusammenbringt. Der bekannteste Effektschätzer für einen Mittelwertsunterschied bei großer Fallzahl mit mehr als 20 Beobachtungen ist Cohen’s d. Wir können Cohen’s d wie folgt berechnen.\n\\[\n|d| = \\cfrac{\\bar{y}_1-\\bar{y}_2}{\\sqrt{\\cfrac{s_1^2+s_2^2}{2}}}\n\\]\nWenn wir die berechneten Mittelwerte und die Varianz der beiden Gruppen in die Formel einsetzten ergibt sich ein absolutes Cohen’s d von 0.24 für den Gruppenvergleich.\n\\[\n|d| = \\cfrac{8.2 - 8.3}{\\sqrt{(0.5^2+0.3^2) /2}} = \\cfrac{-0.1}{0.41} = \\lvert-0.24\\rvert\n\\]\nWas denn nun Cohen’s d exakt aussagt, kann niemand sagen. Aber wir haben einen Wust an möglichen Grenzen. Hier soll die Grenzen von Cohen (1988) einmal angegeben werden. Cohen (1988) hat in seiner Arbeit folgende Grenzen in Tabelle 21.2 für die Interpretation von \\(d\\) vorgeschlagen. Mehr Informationen zu Cohen’s d gibt es auf der Hilfeseite von effectsize: Interpret standardized differences.\n\n\n\nTabelle 21.2— Interpretation der Effektstärke nach Cohen (1988).\n\n\n\n\n\nCohen’s d\nInterpretation des Effekts\n\n\n\n\n\\(d &lt; 0.2\\)\nSehr klein\n\n\n\\(0.2 \\leq d &lt; 0.5\\)\nKlein\n\n\n\\(0.5 \\leq d &lt; 0.8\\)\nMittel\n\n\n\\(d \\geq 0.8\\)\nStark\n\n\n\n\n\n\nWir können auch über die Funktion cohens_d() Cohen’s d einfach in R berechnen. Die Funktion cohens_d() akzeptiert die Formelschreibweise. Die 95% Konfidenzintervalle sind mit Vorsicht zu interpretieren. Denn die Nullhypothese ist hier nicht so klar formuliert. Wir lassen also die 95% Konfidenzintervalle erstmal hier so stehen.\n\ncohens_d(jump_length ~ animal, data = data_tbl, pooled_sd = TRUE)\n\nCohen's d |        95% CI\n-------------------------\n0.24      | [-1.16, 1.62]\n\n- Estimated using pooled SD.\n\n\nDankenswerterweise gibt es noch die Funktion interpret_cohens_d, die es uns erlaubt auszusuchen nach welche Literturquelle wir den Wert von Cohen’s d interpretieren wollen. Ob dieser Effekt relevant zur Fragestellung ist musst du selber entscheiden.\n\ninterpret_cohens_d(0.24, rules = \"cohen1988\")\n\n[1] \"small\"\n(Rules: cohen1988)\n\n\n\n\n21.2.2 Hedges’ g\nSoweit haben wir uns mit sehr großen Fallzahlen beschäftigt. Cohen’s d ist dafür auch sehr gut geeigent und wenn wir mehr als 20 Beobachtungen haben, können wir Cohen’s d auch gut anwenden. Wenn wir weniger Fallzahl vorliegen haben, dann können wir Hedges’ g nutzen. Hedges’ g bietet eine Verzerrungskorrektur für kleine Stichprobengrößen (\\(N &lt; 20\\)) sowie die Möglichkeit auch für unbalanzierte Gruppengrößen einen Effektschätzeer zu berechnen. Die Formel sieht mit dem Korrekturterm recht mächtig aus.\n\\[\ng = \\cfrac{\\bar{y}_1 - \\bar{y}_2}{s^*} \\cdot \\left(\\cfrac{N-3}{N-2.25}\\right) \\cdot \\sqrt{\\cfrac{N-2}{N}}\n\\]\nmit\n\\[\ns^* = \\sqrt{\\cfrac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\n\\]\nWir können aber einfach die Mittelwerte und die Varianzen aus unserem beispiel einsetzen. Da unsere beiden Gruppen gleich groß sind \\(n_1 = n_2\\) und damit ein balanziertes Design vorliegt, sind Cohen’s d und Hedges’ g numerisch gleich. Wir können dann noch für die geringe Fallzahl korrigieren und erhalten ein händisches \\(g = 0.18\\).\n\\[\ng = \\cfrac{8.2 - 8.3}{0.41} \\cdot \\left(\\cfrac{8-3}{8-2.25}\\right) \\cdot \\sqrt{\\cfrac{8-2}{8}} = \\lvert-0.24\\rvert \\cdot 0.87 \\cdot 0.87 \\approx 0.18\n\\]\nmit\n\\[\ns^* = \\sqrt{\\cfrac{(4-1)\\cdot0.5^2 + (4-1)\\cdot0.3^2}{4+4-2}} = \\sqrt{\\cfrac{0.75 + 0.27}{6}} = 0.41\n\\]\nIn R gibt es die Funktion hedges_g() die uns erlaubt in der Formelschreibweise direkt Hedges’ g zu berechnen. Wir sehen hier eine Abweichung von unserer händischen Rechnung. Das ist aber in soweit nicht ungewöhnlich, da es noch eine Menge Varianten der Anpassung für die geringe Fallzahl gibt. In der Anwendung nutzen wir die Funktion aus dem Paket {effectsize} wie hier durchgeführt.\nWir ignorieren wie auch bei Cohen’s d das 95% Konfidenzintervall, da die Interpretation ohne die Nullhypothese nicht möglich ist. Die Nullhypothese ist in diesem Fall komplexer. Wir lassen daher das 95% Konfidenzintervall erstmal einfach hier so stehen.\n\nhedges_g(jump_length ~ animal, data = data_tbl, pooled_sd = TRUE)\n\nHedges' g |        95% CI\n-------------------------\n0.21      | [-1.01, 1.41]\n\n- Estimated using pooled SD.\n\n\nAuch für Hedges’ g gibt es die Möglichkeit sich über die Funktion interpret_hedges_g() den Wert von \\(g=0.21\\) interpretieren zu lassen. Nach Sawilowsky (2009) haben wir hier einen kleinen Effekt vorliegen. Ob dieser Effekt relevant zur Fragestellung ist musst du selber entscheiden.\n\ninterpret_hedges_g(0.21, rules = \"sawilowsky2009\")\n\n[1] \"small\"\n(Rules: sawilowsky2009)\n\n\nDie Hilfeseite zu dem Paket effectsize bietet eine Liste an möglichen Referenzen für die Wahl der Interpretation der Effektstärke. Du musst dann im Zweifel schauen, welche der Quellen und damit Grenzen du nutzen willst.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Der Effektschätzer</span>"
    ]
  },
  {
    "objectID": "stat-tests-effect.html#unterschied-zweier-anteile",
    "href": "stat-tests-effect.html#unterschied-zweier-anteile",
    "title": "21  Der Effektschätzer",
    "section": "21.3 Unterschied zweier Anteile",
    "text": "21.3 Unterschied zweier Anteile\nNeben den Unterschied zweier Mittelwerte ist auch häufig das Interesse an dem Unterschied zwischen zwei Anteilen. Nun unterscheiden wir zwischen Wahrscheinlichkeiten und Chancen. Beide Maßzahlen, die Wahrscheinlichkeit wie auch die Chance, beschreiben einen Anteil. Hier tritt häufig Verwirrung auf, daher hier zuerst ein Beispiel.\nWir behandelt \\(n = 65\\) Hunde mit dem Antiflohmittel FleaEx. Um die Wirkung von FleaEx auch bestimmen zu können haben wir uns zwei Gruppen von Hunden ausgesucht. Wir haben Hunde, die mit Flöhe infiziert sind und Hunde, die nicht mit Flöhen infiziert sind. Wir schauen nun in wie weit FleaEx gegen Flöhe hilft im Vergleich zu einer Kontrolle.\n\n\n\nTabelle 21.3— Eine 2x2 Tabelle als Beispiel für unterschiedliche Flohinfektionen bei nach einer Behandlung mit FleaEx für die Berechnung von Effektschätzern eines Anteils.\n\n\n\n\n\n\n\nGroup\n\n\n\n\n\nFleaEx\nControl\n\n\nInfected\nYes (1)\n\\(18_{\\;\\Large a}\\)\n\\(23_{\\;\\Large b}\\)\n\n\n\nNo (0)\n\\(14_{\\;\\Large c}\\)\n\\(10_{\\;\\Large d}\\)\n\n\n\n\n\n\nAus der Tabelle 21.3 können wir entnehmen, dass 18 behandelte Hunde mit Flöhen infiziert sind und 14 Hunde keine Infektion aufweisen. Bei den Hunden aus der Kontrolle haben wir 23 infizierte und 10 gesunde Tiere beobachtet.\nWir können nun zwei Arten von Anteilen berechnen um zu beschreiben, wie sich der Anteil an infizierten Hunden verhält. Das bekanntere ist die Frequenz oder Wahrscheinlichkeit oder Risk Ratio (\\(RR\\)). Das andere ist das Chancenverhältnis oder Odds Ratio (\\(OR\\)). Beide kommen in der Statistik vor und sind unterschiedlich zu interpretieren. Es gibt verschiedene Typen von klinischen Studien, also Untersuchungen an Menschen. Einige Studien liefern nur \\(OR\\) wieder andere Studientypen liefern \\(RR\\). George u. a. (2020) liefert eine gute Übersicht über What’s the risk: differentiating risk ratios, odds ratios, and hazard ratios?\nUm die die Odds Ratio und die Risk Ratios auch in R berechnen zu können müssen wir einmal die 2x2 Kreuzabelle in R nachbauen. Wir nutzen dafür die Funktion matrix() und müssen schauen, dass die Zahlen in der 2x2 Kreuztabelle in R dann auch so sind, wie in der Datentabelle. Das ist jetzt ein schöner Codeblock, ist aber dafür da um sicherzustellen, dass wir die Zahlen richtig eintragen.\n\ncross_mat &lt;- matrix(c(18, 23, 14, 10),\n  nrow = 2, byrow = TRUE,\n  dimnames = list(\n    Infected = c(\"Yes\", \"No\"),\n    Group = c(\"FleaEx\", \"Control\")\n  )\n)\n\ncross_mat\n\n        Group\nInfected FleaEx Control\n     Yes     18      23\n     No      14      10\n\n\nSpäter werden wir das \\(OR\\) und \\(RR\\) wieder treffen. Das \\(OR\\) kommt in der logistsichen Regression als Effektschätzer vor. Wir nutzen das \\(RR\\) als Effektschätzer in der Poissonregression.\n\n21.3.1 Wahrscheinlichkeitsverhältnis oder Risk Ratio (RR)\nWir berechnen wir nun das Wahrscheinlichkeitsverhältnis oder Risk Ratio (RR)? Das Risk Ratio ist das Verhältnis von den infizierten Hunden in der Behandlung (\\(a\\)) zu allen infizierten Hunden (\\(a+c\\)) zu dem Verhältnis der gesunden Hunde in der Behandlung (\\(b\\)) zu allen gesunden Hunden (\\(b+d\\)). Das klingt jetzt etwas wirr, deshalb helfen manchmal wirklich Formeln, den Zusammenhang besser zu verstehen. Damit ist \\(Pr(\\mbox{FleaEx}|\\mbox{infected})\\) die Wahrscheinlichkeit infiziert zu sein, wenn der Hund mit FleaEx behandelt wurde.\n\\[\nPr(\\mbox{FleaEx}|\\mbox{infected}) = \\cfrac{a}{a+c} = \\cfrac{18}{18+14} \\approx 0.56\n\\]\nDie Wahrscheinlichkeit \\(Pr(\\mbox{Control}|\\mbox{infected})\\) ist dann die Wahrscheinlichkeit infiziert zu sein, wenn der Hund mit in der Kontrolle war.]{.aside}\n\\[\nPr(\\mbox{Control}|\\mbox{infected}) = \\cfrac{b}{b+d} = \\cfrac{23}{23 + 10} \\approx 0.70\n\\]\nDas Risk Ratio ist mehr oder minder das Verhältnis von der beiden Spalten der Tabelle 21.3 für die Behandlung. Wir erhalten also ein \\(RR\\) von \\(0.76\\). Damit mindert die Gabe von FleaEx die Wahrscheinlichkeit sich mit Flöhen zu infizieren.\n\\[\n\\Delta_{y_1/y_2} = RR = \\cfrac{Pr(\\mbox{FleaEx}|\\mbox{infected})}{Pr(\\mbox{Control}|\\mbox{infected})} =  \\cfrac{0.56}{0.70} \\approx 0.80\n\\]\nWir überprüfen kurz mit der Funktion riskratio() ob wir richtig gerechnet haben. Das 95% Konfidenzintervall können wir interpretieren, dafür brauchen wir aber noch einmal eine Idee was “kein Effekt” bei einem Risk Ratio heist.\n\nriskratio(cross_mat)\n\nRisk ratio |       95% CI\n-------------------------\n0.81       | [0.55, 1.18]\n\n\nWann liegt nun kein Effekt bei einem Anteil wie dem RR vor? Wenn der Anteil in der einen Gruppe genauso groß ist wie der Anteil der anderen Gruppe.\n\\[\nH_0: RR = \\cfrac{Pr(\\mbox{dog}|\\mbox{infected})}{Pr(\\mbox{cat}|\\mbox{infected})} = 1\n\\]\nWir interpretieren das \\(RR\\) nun wie folgt. Unter der Annahme, dass ein kausaler Effekt zwischen der Behandlung und dem Outcome besteht, können die Werte des relativen Risikos auf folgende Art und Weise interpretiert werden:\n\n\\(RR = 1\\) bedeutet, dass die Behandlung keinen Einfluss auf das Outcome hat\n\\(RR &lt; 1\\) bedeutet, dass das Risiko für das Outcome durch die Behandlung verringert wird, was ein “Schutzfaktor” ist\n\\(RR &gt; 1\\) bedeutet, dass das Risiko für das Outcome durch die Behandlung erhöht wird, was ein “Risikofaktor” ist.\n\nDas heist in unserem Fall, dass wir mit einem RR von \\(0.80\\) eine protektive Behandlung vorliegen haben. Die Gabe von FleaEx reduziert das Risiko mit Flöhen infiziert zu werden. Durch das 95% Konfidenzintervall wissen wir auch, dass das \\(RR\\) nicht signifikant ist, da die 1 im 95% Konfidenzintervall enthalten ist.\n\n\n21.3.2 Chancenverhältnis oder Odds Ratio (OR)\nNeben dem Risk Ratio gibt es noch das Odds Ratio. Das Odds Ratio ist ein Chancenverhältnis. Wenn der Mensch an sich schon Probleme hat für sich Wahrscheinlichkeiten richtig einzuordnen, scheitert man allgemein an der Chance vollkommen. Dennoch ist das Odds Ratio eine gute Maßzahl um abzuschätzen wie die Chancen stehen, einen infizierten Hund vorzufinden, wenn der Hund behandelt wurde.\nSchauen wir uns einmal die Formeln an. Im Gegensatz zum Risk Ratio, welches die Spalten miteinander vergleicht, vergleicht das Odds Ratio die Zeilen. Als erstes berechnen wir die Chance unter der Gabe von FleaEx infiziert zu sein wie folgt.\n\\[\nOdds(\\mbox{FleaEx}|\\mbox{infected}) = a:b = 18:23 = \\cfrac{18}{23} = 0.78\n\\]\nDann berechnen wir die Chance in der Kontrollgruppe infiziert zu sein wie folgt.\n\\[\nOdds(\\mbox{Control}|\\mbox{infected}) = c:d = 14:10 = \\cfrac{14}{10} \\approx 1.40\n\\]\nAbschließend bilden wir das Chancenverhältnis der Chance unter der Gabe von FleaEx infiziert zu sein zu der Chance in der Kontrollgruppe infiziert zu sein. Es ergbit sich das Odds Ratio wie folgt.\n\\[\n\\Delta_{y_1/y_2} = OR =  \\cfrac{Odds(\\mbox{Flea}|\\mbox{infected})}{Odds(\\mbox{Control}|\\mbox{infected})} = \\cfrac{a \\cdot d}{b \\cdot c} = \\cfrac{0.78}{1.40} \\approx 0.56\n\\]\nWir überprüfen kurz mit der Funktion oddsratio() ob wir richtig gerechnet haben. Das 95% Konfidenzintervall können wir interpretieren, dafür brauchen wir aber noch einmal eine Idee was “kein Effekt” bei einem Odds Ratio heist.\n\noddsratio(cross_mat)\n\nOdds ratio |       95% CI\n-------------------------\n0.56       | [0.20, 1.55]\n\n\nWann liegt nun kein Effekt bei einem Anteil wie dem OR vor? Wenn der Anteil in der einen Gruppe genauso groß ist wie der Anteil der anderen Gruppe.\n\\[\nH_0: OR =  \\cfrac{Odds(\\mbox{dog}|\\mbox{infected})}{Odds(\\mbox{cat}|\\mbox{infected})} = 1\n\\]\nWir interpretieren das \\(OR\\) nun wie folgt. Unter der Annahme, dass ein kausaler Effekt zwischen der Behandlung und dem Outcome besteht, können die Werte des Odds Ratio auf folgende Art und Weise interpretiert werden:\n\n\\(OR = 1\\) bedeutet, dass die Behandlung keinen Einfluss auf das Outcome hat\n\\(OR &lt; 1\\) bedeutet, dass sich die Chance das Outcome zu bekommen durch die Behandlung verringert wird, was ein “Schutzfaktor” ist\n\\(OR &gt; 1\\) bedeutet, dass sich die Chance das Outcome zu bekommen durch die Behandlung erhöht wird, was ein “Risikofaktor” ist.\n\nDas heist in unserem Fall, dass wir mit einem OR von \\(0.56\\) eine protektive Behandlung vorliegen haben. Die Gabe von FleaEx reduziert die Chance mit Flöhen infiziert zu werden. Durch das 95% Konfidenzintervall wissen wir auch, dass das \\(OR\\) nicht signifikant ist, da die 1 im 95% Konfidenzintervall enthalten ist.\n\n\n21.3.3 Odds Ratio (OR) zu Risk Ratio (RR)\nWenn wir das OR berechnet haben, wollen wir eventuell das \\(OR\\) in dem Sinne eines Riskoverhältnisses berichten. Leider ist es nun so, dass wir das nicht einfach mit einem \\(OR\\) machen können. Ein \\(OR\\) von 3.5 ist ein großes Chancenverhältnis. Aber ist es auch 3.5-mal so wahrscheinlich? Nein so einfach können wir das OR nicht interpretieren. Wir können aber das \\(OR\\) in das \\(RR\\) umrechnen. Dafür brauchen wir aber das \\(p_0\\). Dabei ist das \\(p_0\\) das Basisrisiko also die Wahrscheinlichkeit des Ereignisses ohne die Intervention. Wenn wir nichts tun würden, wie wahrscheinlich wäre dann das Auftreten des Ereignisses? Es ergibt sich dann die folgende Formel für die Umrechnung des \\(OR\\) in das \\(RR\\). Grant (2014) gibt nochmal eine wissenschaftliche Diskussion des Themas zur Konvertierung von OR zu RR.\n\\[\nRR = \\cfrac{OR}{(1 - p_0 + (p_0 \\cdot OR))}\n\\]\nSchauen wir uns das einmal in einem Beispiel an. Wir nutzen für die Umrechnung die Funktion oddsratio_to_riskratio() aus dem R Paket {effectsize}. Wenn wir ein \\(OR\\) von 3.5 haben, so hängt das \\(RR\\) von dem Basisriskio ab. Wenn das Basisirisko für die Erkrankung ohne die Behandlung sehr hoch ist mit \\(p_0 = 0.85\\), dann ist das \\(RR\\) sehr klein.\n\nOR &lt;- 3.5\nbaserate &lt;- 0.85\n\noddsratio_to_riskratio(OR, baserate) %&gt;% round(2)\n\n[1] 1.12\n\n\nAuf der anderen Seite nähert sich das \\(OR\\) dem \\(RR\\) an, wenn das Basisriskio für die Erkrankung mit \\(p_0 = 0.04\\) sehr klein ist.\n\nOR &lt;- 3.5\nbaserate &lt;- 0.04\n\noddsratio_to_riskratio(OR, baserate) %&gt;% round(2)\n\n[1] 3.18\n\n\nWeil wir natürlich das Basisrisiko nur abschätzen können, verbleibt hier eine gewisse Unsicherheit, wie das \\(RR\\) zu einem gegebenen \\(OR\\) aussieht.\n\n\n21.3.4 Foldchange\nDer Foldchange (deu. Zunahmenänderung ungebräuchlich, abk. FC) ist ein statistisches Maß welches uns beschreibt, wie sehr sich eine Menge von einem Anfangs- zu einem Endwert verändert. Dabei ist es wichtig, dass wir es hier gleich mit einem Anteil zu tun haben. Beispielsweise entspricht ein Anfangswert von 30 und ein Endwert von 60 einer 2-fachen Änderung oder eben eines two-fold change. Wir wollen dabei natürlich wissen, wie sich der Wert von Ende zu Beginn geändert hat.\n\\[\nFC = \\cfrac{Endwert}{Anfangswert} = \\cfrac{60}{30} = 2\n\\]\nEine Verringerung von 80 auf 20 wäre ein Foldchange von -0.75, während eine Veränderung von 20 auf 80 ein Foldchange von 3 wäre. Damit sind wir natürlich sehr unsymmetrisch um die 1.\n\\[\nFC = \\cfrac{Endwert}{Anfangswert} = \\cfrac{20}{80} = 0.75\n\\]\nAlle Foldchanges mit einer Verringerung pressen sich in den Zahlenraum \\([0,1]\\) während alle Erhöhungen sich im Zahlenraum \\([1, \\infty]\\) bewegen. Um dies zu umgehen, verwenden wir \\(log2\\) für den Ausdruck der Log Foldchange. Damit du das besser verstehst, dann die Sachlage einmal als ein Beispiel. Wenn du einen FC von 2 vorliegen hast und dann \\(log_2(2)\\) berechnest, dann erhälst du einen \\(log_2FC\\) von 1.\n\nFC &lt;- c(0.1, 0.5, 0.75, 1, 2, 3, 4)\nlog2(FC) |&gt; \n  round(2) |&gt; \n  set_names(FC)\n\n  0.1   0.5  0.75     1     2     3     4 \n-3.32 -1.00 -0.42  0.00  1.00  1.58  2.00 \n\n\nWenn du einen log2-transformierten FC-Wert wieder zurück haben willst, dann nutze einfach die 2-er Potenz mit \\(2^{logFC}\\). Auch hier einmal das schnelle Beispiel.\n\n2^log2(FC)\n\n[1] 0.10 0.50 0.75 1.00 2.00 3.00 4.00\n\n\nWenn dir also einmal ein Foldchange über den Weg läuft, dann geht es eigentlich nur darum eine Veränderung gleichmäßig um die Eins darstellen zu können.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Der Effektschätzer</span>"
    ]
  },
  {
    "objectID": "stat-tests-effect.html#sec-effect-wirkung",
    "href": "stat-tests-effect.html#sec-effect-wirkung",
    "title": "21  Der Effektschätzer",
    "section": "21.4 Wirkungsgrad von Pflanzenschutzmitteln",
    "text": "21.4 Wirkungsgrad von Pflanzenschutzmitteln\nNeben den klassischen Effektmaßzahlen, die sich aus einem Mittelwert oder einem Anteil direkt berechnen, gibt es noch andere Effektmaße. Einer dieser Effektmaße ist der Wirkungsgrad für zum Beispiel ein Pflanzenschutzmittel. Wir können hier aber auch weiter denken und uns überlegen in wie weit wir eine Population von Schaderregern durch eine Behandlung reduzieren können. Unabdingbar ist in diesem Fall eine positive Kontrolle in der nichts gemacht wird sondern nur der normale Befall gemessen wird. Merke also, ohne eine Kontrolle geht bei der Bestimmung des Wirkungsgrades gar nichts.\n\n\n\n\n\n\nÜbersicht über Wirkungsgrade\n\n\n\n\nDie Seite LdP Line by Ehab Bakr liefert nochmal die Formeln und Berechungsmöglichkeiten für andere Wirkungsgrade mit einem praktischen Online-Rechner. Leider ist die Seite sehr alt und die Sicherheitsbestimmungen lassen moderne Browser zweifeln.\nDer Vortrag von Andreas Büchse “Nutzung der Wirkungsgradberechnung nach Abbott und Henderson-Tilton in der angewandten Agrarforschung” gibt auch nochmal weitere Einblicke und ist die Grundlage für die Beispiele hier.\n\n\n\nIn der Tabelle 21.4 sehen wir nochmal eine Übersicht über vier mögliche Arten den Wirkungsgrad zu berechnen. Je nachdem was wir vorliegen haben, entweder einen Befall mit Zähldaten oder aber eine Sterblichkeit bzw. Mortalität, müssen wir eine andere Methode wählen. Auch müssen wir unterscheiden, ob wir eine homogene Population vorliegen haben oder eher eine heterogene Population. Ich kann hier aber schon mal etwas abkürzen, meistens können wir nur den Wirkungsgrad nach Abbott u. a. (1925) berechnen, da unser experimentelles Design nicht mehr hergibt. Der Wirkungsgrad nach Abbott hat nämlich nur die räumliche Komponente und nicht die zeitliche. So lässt sich Abbott häufig einfacher durchführen und ausrechnen. Für eine zeitliche Komponente wie bei Henderson-Tilton müssten wir dann den Befall wiederholt messen.\n\n\n\nTabelle 21.4— Übersicht der möglichen Wirkungsgrade nach Population und gemessen Werten an den Beobachtungen.\n\n\n\n\n\n\nHomogene Population\nHeterogene Population (Vorbefall differenziert)\n\n\nBefall oder lebende Individuen\nAbbott [räumlicher Bezug]\nHenderson-Tilton [zeitlich und räumlicher Bezug]\n\n\nSterblichkeit oder tote Individuen\nSchneider-Orelli\nSun-Shepard\n\n\n\n\n\n\nIm Folgenden gehen wir einmal die vier Wirkungsgrade durch. Neben Abbott stelle ich dann hier nochmal den Wirkungsgrad nach Henderson und Tilton (1955) vor. Die anderen Wirkungsgrade nur als Formeln, da mir aktuell noch keine Anwendungen hier über den Weg gelaufen sind. Der Hauptfokus liegt dann aber am Ende auf den Wirkungsgrad nach Abbott. Den Wirkungsgrad brauchen wir eben dann doch am meisten, da wir nach Abbott nur den gezählten Befall nach der Behandlung bestimmen müssen. Der Wirkungsgrad nach Abbott entspricht meistens eher der experimentellen Wirklichkeit. Das soll dich natürlich nicht davon abschrecken auch die anderen Wirkunsggrade zu nutzen, aber dafür musst du meistens mehr Aufwand betreiben.\n\n21.4.1 Wirkungsgrad nach Henderson und Tilton (1955)\nDer Wirkungsgrad nach Henderson-Tilton hate den großen Vorteil, dass wir auch eine zzeitliche Komponente mit berücksichtigen können. Wie verhält sich der BEfall der Kontrolle und der Behandlung über die Zeit des Versuches? Wir müssen dafür den Befall vor (eng. before, abk. b) und den Befall nach (eng. after, abk. a) der Applikation bestimmen. Dann erhalten wir folgende Formel, die die Anzahlen der Behandlung und Kontrolle räumlich wie zeitlich zusamenfasst. Nämlich einmal zu Beginn des Versuches und einmal zum Ende. Henderson-Tilton erlaubt es damit zusätzlich Unterschiede in der Populationsdynamik zu bewerten.\n\\[\nWG_{HT} = \\left(1 - \\cfrac{T_a}{T_b}\\cdot\\cfrac{C_b}{C_a}\\right)\n\\]\nmit\n\n\\(T_{a}\\) Anzahl lebend in behandelter Parzelle nach Applikation (eng. after)\n\\(T_{b}\\) Anzahl lebend in behandelter Parzelle vor Applikation (eng. before)\n\\(C_{a}\\) Anzahl lebend in Kontrolle nach Applikation (eng. after)\n\\(C_{b}\\) Anzahl lebend in Kontrolle vor Applikation (eng. before)\n\nHier nochmal ein kleines numerisches Beispiel mit \\(T_b = 300\\) sowie \\(T_a = 30\\) und \\(C_b = 500\\) sowie \\(C_a = 1000\\). Wir fangen also in einer Population mit einem Befall von \\(500\\) Trepsen in der Kontrolle an und haben am Ende des Versuches dann \\(1000\\) Trepsen in unserem Versuch vorliegen. In der Behandlung haben wir zu Beginn des Versuches \\(300\\) Trepsen im Feld gezählt und finden nach der Applikation unseres Pflanzenschutzmittels am Ende des Versuches noch \\(30\\) Trepsen wieder. Dann können wir einmal die Zahlen in die Formel einsetzen.\n\\[\nWG_{HT} = \\left(1 - \\cfrac{T_a}{T_b}\\cdot\\cfrac{C_b}{C_a}\\right)  = \\left(1 - \\cfrac{30}{300}\\cdot\\cfrac{500}{1000}\\right) = 1 - (0.1 \\cdot 0.5) = 0.95\n\\]\nWir haben hier nun einen räumlichen und zeitlichen Bezug in unserem Wirkungsgrad abgebildet. Leider haben wir nicht immer einen Befall in unserem Bestand. Der Befall tritt erst während der Applikation auf. Dann müssen wir die Formel nach Abbott nutzen, die sich nur auf die Entwicklung des Befalls am Ende des Versuches konzentriert.\n\n\n21.4.2 Wirkungsgrad nach Abbott u. a. (1925)\nIm Folgenden wollen wir uns auf die Berechnung des Wirkungsgrad nach Abbott konzentrieren. Der Wirkungsgrad wird sehr häufig genutzt, so dass sich hier eine tiefere Betrachtung lohnt. Wir berechnen hier den Wirkungsgrad nach Abbott u. a. (1925) erstmal sehr simpel an einem Beispiel. Dann nutzen wir R um die ganze Berechnung nochmal etwas komplexer durchzuführen. Abschließend schauen wir uns mit der Anpassung von Finner u. a. (1989) noch eine Erweiterung des Wirkungsgrads nach Abbott an. Der Wirkungsgrad \\(WG\\) eines Schutzmittels im Vergleich zur Kontrolle berechnet sich wie folgt.\n\\[\nWG_{Ab} = \\left(\\cfrac{C_a - T_a}{C_a}\\right) = \\left(1-\\cfrac{T_a}{C_a}\\right)\n\\]\nmit\n\n\\(T_a\\) Anzahl lebend in der Behandlung nach Applikation (eng. after)\n\\(C_a\\) Anzahl lebend in der Kontrolle nach Applikation (eng. after)\n\nWir können auch nur den räumlichen Bezug mit dem Wirkungsgrad nach Abbott abbilden. Hier nochmal ein kleines numerisches Beispiel von oben mit \\(T_b = 300\\) sowie \\(T_a = 30\\) und \\(C_b = 500\\) sowie \\(C_a = 1000\\). Wir fangen also in einer Population mit einem Befall von \\(500\\) Trepsen in der Kontrolle an und haben am Ende des Versuches dann \\(1000\\) Trepsen in unserem Versuch vorliegen. In der Behandlung haben wir zu Beginn des Versuches \\(300\\) Trepsen im Feld gezählt und finden nach der Applikation unseres Pflanzenschutzmittels am Ende des Versuches noch \\(30\\) Trepsen wieder. Hier ist es wichtig, dass wir uns nicht den Zusammenhang über die Zeit anschauen können. Also entweder betrachten wir den Zusammenhang von nach unser Applikation oder aber den Vergleich von vorher und nachher von unserer Behandlung.\n\\[\nWG_{Ab} = \\left(1-\\cfrac{T_a}{C_a}\\right) = \\left(1-\\cfrac{30}{1000}\\right) = 0.97\n\\]\nOder wir konzentrieren uns eben nur auf den zeitlichen Bezug in dem wir die Formel nach Abbott etwas modidifzieren und nur auf die Behandlung \\(T\\) schauen.\n\\[\nWG_{Ab^*} = \\left(1-\\cfrac{T_a}{T_b}\\right) = \\left(1-\\cfrac{30}{300}\\right) = 0.90\n\\]\nNatürlich ist die Formel wieder auf sehr einfachen Daten gut zu verstehen. Wir haben aber später dann in der Anwendung natürlich Datensätze, so dass wir uns hier einmal zwei Beispieldaten anschauen wollen. Zuerst schauen wir uns einen Datensatz zu dem Befall mit Trespe, einem Wildkraut, an. Wir haben also Parzellen in denen sich die Trespe ausbreitet und haben verschiedene Behandlungen durchgeführt. Wichtig hierbei, wir haben auch Parzellen wo wir nichts gemacht haben, das ist dann unsere positive Kontrolle (ctrl).\n\n\n\n\nTabelle 21.5— Trespebefall in einem randomisierten Blockdesign mit vier Behandlungsvarianten.\n\n\n\n\n\n\nvariante\nblock_1\nblock_2\nblock_3\nblock_4\n\n\n\n\ncrtl\n16.0\n16.0\n4.8\n11.2\n\n\n2\n4.8\n4.0\n0.0\n2.4\n\n\n3\n10.4\n2.4\n0.0\n0.0\n\n\n4\n8.0\n8.8\n7.2\n4.0\n\n\n\n\n\n\n\n\nIn der Tabelle 21.6 haben wir dann einmal den Wirkungsgrad nach Abbott wg_abbott aus dem mittleren Befall über die vier Blöcke berechnet. Wir lassen hier mal weg, dass wir in einigen Parzellen mehr Befall hatten und deshab negative Wirkunsggrade. Hier hilft dann der Mittelwert.\n\n\n\n\nTabelle 21.6— Trespebefall in einem randomisierten Blockdesign mit vier Behandlungsvarianten ergänzt um den Wirkungsgrad nach Abbott wg_abbott berechnet aus dem mittleren Befall über die Blöcke.\n\n\n\n\n\n\nvariante\nblock_1\nblock_2\nblock_3\nblock_4\nmean\nwg_abbott\n\n\n\n\ncrtl\n16.0\n16.0\n4.8\n11.2\n12.0\n0.00\n\n\n2\n4.8\n4.0\n0.0\n2.4\n2.8\n0.77\n\n\n3\n10.4\n2.4\n0.0\n0.0\n3.2\n0.73\n\n\n4\n8.0\n8.8\n7.2\n4.0\n7.0\n0.42\n\n\n\n\n\n\n\n\nDann das Ganze auch nochmal händisch aufbereitet und berechnet. Wir erhalten die gleichen Werte wie auch oben in der Tabelle.\n\\[\n\\begin{align}\nWG_2 &= \\left(1 - \\cfrac{2.8}{12}\\right) = 0.77 \\\\\nWG_3 &= \\left(1 - \\cfrac{3.2}{12}\\right) = 0.73 \\\\\nWG_4 &= \\left(1 - \\cfrac{7.0}{12}\\right) = 0.42\n\\end{align}\n\\]\nJetzt wollen wir etwas tiefer in die Thematik einsteigen und müssen daher einmal die Daten umwandeln, da unsere Daten nicht im Long-Format vorliegen. Wir müssen die Daten erst noch anpassen und dann die Spalte block in einen Faktor umwandeln. Laden wir einmal den Datensatz in R und verwandeln das Wide-Format in das Long-Format. Dann natürlich wie immer alle Faktoren als Faktoren mutieren.\n\ntrespe_tbl &lt;- read_excel(\"data/raubmilben_data.xlsx\", sheet = \"trespe\") %&gt;% \n  pivot_longer(block_1:block_4,\n               names_to = \"block\",\n               values_to = \"count\") %&gt;% \n  mutate(block = factor(block, labels = 1:4),\n         variante = as_factor(variante))\n\nIn der Abbildung 21.5 sehen wir nochmal die orginalen, untransformierten Daten sowie die \\(log\\)-transformierten Daten. Wir nutzen die Funktion log1p() um die Anzahl künstlich um 1 zu erhöhen, damit wir Nullen in den Zähldaten zum Logarithmieren vermeiden.\n\nggplot(trespe_tbl, aes(variante, count, fill = block)) +\n  theme_minimal() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Behandlungsvariante\", y = \"Anzahl\", fill = \"Block\") +\n  scale_fill_okabeito()\n  \nggplot(trespe_tbl, aes(variante, log1p(count), fill = block)) +\n  theme_minimal() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Behandlungsvariante\", y = \"log(Anzahl)\", fill = \"Block\") +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\n\n\n\n\n(a) Verteilung der Werte auf der originalen Skala.\n\n\n\n\n\n\n\n\n\n\n\n(b) Verteilung der Werte auf der logarithmischen Skala. Beobachtungen mit einer 0 Zählung wurden auf 1 gesetzt.\n\n\n\n\n\n\n\nAbbildung 21.5— Dotplot der Anzahl an Trespen je Sorte und Block.\n\n\n\n\nNun geht es eigentlich ganz fix den Wirkungsgrad nach Abbott mit einer linearen Poisson Regression zu berechnen. Erstmal schätzen wir eine Poissonregression mit der Anzahl der Trespen als Outcome. Es ist wichtig in der Funktion glm() die Option family = poisson zu setzen. Sonst rechnen wir auch keine Poissonregression. Wir haben hier im Prinzip die effizientere Variante vorliegen, da wir durch die Modellierung auch noch andere Einflussfaktoren mit ins Modell nehemn können als nur den Block. Da wir hier aber nichts Weiteres gemessen haben, bleibt es bei dem einfachen Modell.\n\nfit &lt;- glm(count ~ variante + block, data = trespe_tbl, family = poisson)\n\nWir nutzen die Schätzer aus dem Modell um mit der Funktion emmeans() die Raten in jeder Variante gemittelt über alle Blöcke zu berechnen. Dann müssen wir nur noch die Formel nach Abbott nutzen um jede Rate in das Verhältnis zur Rate der Kontrolle rate[1] zu setzen. Wir erhalten dann den Wirkungsgrad nach Abbott für unsere drei Varianten. Wie wir sehen, sind auch hier die Werte gleich.\n\nres_trespe_tbl &lt;- fit %&gt;% \n  emmeans(~variante, type = \"response\") %&gt;% \n  tidy() %&gt;% \n  mutate(WG_abbott = (rate[1] - rate)/rate[1],\n         WG_abbott_per = percent(WG_abbott)) %&gt;% \n  select(variante, rate, WG_abbott, WG_abbott_per)\nres_trespe_tbl\n\n# A tibble: 4 × 4\n  variante  rate WG_abbott WG_abbott_per\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;        \n1 crtl     10.8      0     0.0%         \n2 2         2.52     0.767 76.7%        \n3 3         2.89     0.733 73.3%        \n4 4         6.31     0.417 41.7%        \n\n\nDamit haben wir den Wirkungsgard \\(WG_{abbott}\\) für unser Trepsenbeispiel einmal berechnet. Die Interpretation ist dann eigentlich sehr intuitiv. Wir haben zum Beispiel bei Variante 2 einen Wirkungsgrad von 76.7% der Kontrolle und somit auch eine Reduzierung von 76.7% der Trepsen auf unseren Parzellen im Vergleich zur Kontrolle.\nWir können auch einfach testen, ob sich die Kontrolle von den anderen Varianten sich unterscheidet. Wir nutzen dafür wieder die Funktion emmeans() und vergleichen alle Varianten zu der Kontrolle. Da die Kontrolle in der ersten Zeile steht bzw. das erste Level des Faktors variante ist, müssen wir noch die Referenz mit ref = 1 setzen.\n\nfit %&gt;% \n  emmeans(trt.vs.ctrlk ~ variante, type = \"response\", ref = 1) %&gt;% \n  pluck(\"contrasts\")\n\n contrast ratio     SE  df null z.ratio p.value\n 2 / crtl 0.233 0.0774 Inf    1  -4.385  &lt;.0001\n 3 / crtl 0.267 0.0839 Inf    1  -4.202  0.0001\n 4 / crtl 0.583 0.1387 Inf    1  -2.267  0.0633\n\nResults are averaged over the levels of: block \nP value adjustment: dunnettx method for 3 tests \nTests are performed on the log scale \n\n\nWir wir sehen, sind alle Varianten, bis auf die Variante 4, signifikant unterschiedlich zu der Kontrolle. Nun könnten wir uns auch Fragen in wie weit sich die Wirkungsgrade untereinander unterscheiden. Dafür müssen wir dann die Anteile mit der Funktion pairwise.prop.test() paarweise Vergleichen. Da wir natürlich auf die Kontrolle standardisieren hat der Vergleich mit der Kontrolle nur so halbwegs Sinn, mag aber von Interesse sein.\n\npairwise.prop.test(x = res_trespe_tbl$WG_abbott * 100, \n                   n = c(100, 100, 100, 100),\n                   p.adjust.method = \"none\") \n\n\n    Pairwise comparisons using Pairwise comparison of proportions \n\ndata:  res_trespe_tbl$WG_abbott * 100 out of c(100, 100, 100, 100) \n\n  1       2       3      \n2 &lt; 2e-16 -       -      \n3 &lt; 2e-16 0.7     -      \n4 1.4e-12 1.0e-06 1.2e-05\n\nP value adjustment method: none \n\n\nWir sehen, dass sich alle Varianten in ihrem Wirkungsgrad von der Kontrolle unterscheiden, die Kontrolle hat hier die ID 1. Die Variante 4 unterscheidet sich von allen anderen Varianten in ihrem Wirkungsgrad.\nIm zweiten Beispiel wollen wir uns mit dem geometrischen Mittel \\(WG_{geometric}\\) als Schätzer für den Wirkungsgrad beschäftigen. Hier kochen wir dann einmal die Veröffentlichung von Finner u. a. (1989) nach. Dafür brauchen wir einmal die Daten zu den Raubmilben, die ich schon als Exceldatei aufbereitet habe. Wie immer sind die Rohdaten im Wide-Format, wir müssen aber im Long-Format rechnen. Da bauen wir uns also einmal schnell die Daten um. Dann wollen wir noch die Anzahlen der Raubmilben logarithmieren, so dass wir jede Anzahl um 1 erhöhen um logarithmierte Nullen zu vermeiden. Das ganze machen wir dann in einem Rutsch mit der Funktion log1p().\n\nmite_tbl &lt;- read_excel(\"data/raubmilben_data.xlsx\", sheet = \"mite\") %&gt;% \n  pivot_longer(block_1:block_5,\n               names_to = \"block\",\n               values_to = \"count\") %&gt;% \n  mutate(block = factor(block, labels = 1:5),\n         sorte = as_factor(sorte),\n         log_count = log1p(count))\n\n\n\n\n\nTabelle 21.7— Raubmilbenbefall auf acht Sorten und einer Kontrolle.\n\n\n\n\n\n\nsorte\nblock_1\nblock_2\nblock_3\nblock_4\nblock_5\n\n\n\n\n1\n0\n2\n2\n21\n0\n\n\n2\n302\n108\n64\n23\n49\n\n\n3\n59\n51\n59\n1\n26\n\n\n4\n64\n154\n41\n27\n41\n\n\n5\n45\n141\n51\n70\n37\n\n\n6\n58\n240\n140\n27\n11\n\n\n7\n1\n2\n3\n8\n16\n\n\n8\n4\n1\n0\n0\n1\n\n\nctrl\n46\n32\n62\n90\n20\n\n\n\n\n\n\n\n\nSchauen wir uns einmal die Daten in der Abbildung 21.6 an. Wichtig ist, dass wir usn merken, dass in unseren Daten jetzt die Kontrolle an der neuten Position ist. Das ist eben in diesen Daen so, ich hätte die Sortierung dann anders gemacht. Aber wir arbeiten hier mit dem was wir vorliegen haben.\n\nggplot(mite_tbl, aes(sorte, count, fill = block)) +\n  theme_minimal() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Sorten\", y = \"Anzahl\", fill = \"Block\") +\n  scale_fill_okabeito()\n  \nggplot(mite_tbl, aes(sorte, log1p(count), fill = block)) +\n  theme_minimal() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Sorten\", y = \"log(Anzahl)\", fill = \"Block\") +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\n\n\n\n\n(a) Verteilung der Werte auf der originalen Skala.\n\n\n\n\n\n\n\n\n\n\n\n(b) Verteilung der Werte auf der logarithmischen Skala. Beobachtungen mit einer 0 Zählung wurden auf 1 gesetzt.\n\n\n\n\n\n\n\nAbbildung 21.6— Dotplot der Anzahl an Raubmilden je Sorte und Block.\n\n\n\n\nWir brauchen jetzt eine Helferfunktion, die uns aus \\(Pr\\) die Gegenwahrscheinlichkeit \\(1 - Pr\\) berechnet. Auch wollen wir dann die Prozentangabe der Gegenwahrscheinlichkeit, also die Gegenwahrscheinlichkeit \\(1 - Pr\\) multipliziert mit Einhundert. Dann brauchen wir als Variable noch die Gruppengröße \\(n_g\\), die bei uns ja bei 5 liegt. Wir haben pro Sorte fünf Beobachtungen je Block.\n\nget_q &lt;- function(x){100 * (1 - x)}\nn_group &lt;- 5\n\nWir nutezn jetzt das geometrisches Mittel um den Effekt der Behandlung bzw. Sorte im Verhältnis zur Kontrolle zu berechnen. Hierbei ist es wichtig sich zu erinnern, dass wir nicht alle paarweisen Vergleiche rechnen, sondern nur jede Sorte \\(j\\) zu der Kontrolle \\(ctrl\\) vergleichen. Dabei nutzen wir dann das Verhältnis der geometrisches Mittel um zu Beschreiben um wie viel weniger Befall mit Raubmilden wir in den Sorten \\(y_j\\) im Verhältnis zur Kontrolle \\(y_{ctrl}\\) vorliegen haben.\n\\[\n\\Delta_{geometric} = \\left(\\cfrac{\\prod_{i=1}^n y_j}{\\prod_{i=1}^n y_{ctrl}}\\right)^{1/n_j} \\mbox{ für Sorte } j\n\\]\nBerechnen wir also als erstes einmal das Produkt aller gezählten Raubmilden pro Sorte und speichern das Ergebnis in der Spalte prod.\n\nmite_wg_gemetric_tbl &lt;- mite_tbl %&gt;% \n  mutate(count = count + 1) %&gt;% \n  group_by(sorte) %&gt;% \n  summarise(prod = prod(count)) \nmite_wg_gemetric_tbl\n\n# A tibble: 9 × 2\n  sorte       prod\n  &lt;fct&gt;      &lt;dbl&gt;\n1 1            198\n2 2     2576106000\n3 3       10108800\n4 4      497624400\n5 5      916413472\n6 6      673639344\n7 7           3672\n8 8             20\n9 ctrl   186729543\n\n\nJetzt können wir im nächsten Schritt einmal das \\(\\Delta_{geometric}\\) berechnen und dann den Wirkungsgrad über die Gegenwahrscheinlichkeit. Wichtig ist hier, dass die Kontrolle in der neunten Zeile ist. Daher teilen wir immer durch das Produkt an neunter Position mit prod[9]. Wir sehen ganz klar, das wir ein Delta von 1 für die Kontrolle erhalten, da wir ja die Kontrolle ins Verhältnis zur Kontrolle setzen. Damit sollte der Rest auch geklappt haben. Den Wirkungsgrad der Sorten in Prozent gegen Raubmilbenbefall im Verhältnis zur Kontrolle können wir dann direkt ablesen. Die Funktion percent() berechnet uns aus den Gegenwahrscheinlichkeiten dann gleich die Prozent. Wir brauchen daher hier noch nicht unsere Funktion get_q().\n\nmite_wg_gemetric_tbl %&gt;% \n  mutate(delta_geometric = (prod/prod[9])^(1/n_group),\n         WG_geometric = percent(1 - delta_geometric))\n\n# A tibble: 9 × 4\n  sorte       prod delta_geometric WG_geometric\n  &lt;fct&gt;      &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;       \n1 1            198          0.0638 93.6%       \n2 2     2576106000          1.69   -69.0%      \n3 3       10108800          0.558  44.2%       \n4 4      497624400          1.22   -21.7%      \n5 5      916413472          1.37   -37.5%      \n6 6      673639344          1.29   -29.3%      \n7 7           3672          0.114  88.6%       \n8 8             20          0.0404 96.0%       \n9 ctrl   186729543          1      0.0%        \n\n\nSoweit haben wir erstmal nur eine andere Variante des Wirkungsgrades berechnet. Im Gegensatz zu der Berechnung nach Abbott u. a. (1925) können wir aber bei den geometrischen Wirkungsgrad auch ein einseitiges 95% Konfidenzintervall \\([-\\infty; upper]\\) angeben. Dafür müssen wir erstmal den Exponenten \\(a\\) berechnen und mit diesem dann die obere Konfidenzschranke \\(upper\\). Dafür brauchen wir dann doch ein paar statistische Maßzahlen.\n\\[\n\\begin{align}\na &= \\sqrt{2/n} \\cdot s \\cdot t_{\\alpha=5\\%} + \\ln(1 - \\Delta_{geometric})\\\\\nupper &= 1 - e^a\n\\end{align}\n\\]\nWir brauchen zum einen die Freiheitsgrade der Residuen df.residual sowie den Standardfehler der Residuen sigma oder \\(s\\). Beides erhalten wir aus einem linearen Modell auf den logarithmierten Anzahlen der Raubmilben.\n\nresidual_tbl &lt;- lm(log_count ~ sorte + block, data = mite_tbl) %&gt;% \n  glance() %&gt;% \n  select(df.residual, sigma)\nresidual_tbl\n\n# A tibble: 1 × 2\n  df.residual sigma\n        &lt;int&gt; &lt;dbl&gt;\n1          32 0.962\n\n\nMit den Freiheitsgraden der Residuen können wir jetzt den kritischen Wert \\(t_{\\alpha = 5\\%}\\) aus der \\(t\\)-Verteilung berechnen.\n\nt_quantile &lt;- qt(p = 0.05, df = residual_tbl$df.residual, lower.tail = FALSE)\nt_quantile\n\n[1] 1.693889\n\n\nWir können jetzt unseren Datensatz mite_wg_gemetric_tbl um die Spalte mit den Werten des Exponenten \\(a\\) ergänzen und aus diesem dann die obere Schranke des einseitigen 95% Konfidenzintervall berechnen. Die Werte für die Kontrolle ergeben keinen biologischen Sinn und sind ein mathematisches Artefakt. Wir kriegen halt immer irgendwelche Zahlen raus.\n\nmite_res_tbl &lt;- mite_wg_gemetric_tbl %&gt;% \n  mutate(delta_geometric = (prod/prod[9])^(1/n_group),\n         WG_geometric = get_q(delta_geometric),\n         a = sqrt(2/n_group) * residual_tbl$sigma * t_quantile + log(delta_geometric),\n         upper = get_q(exp(a)))\nmite_res_tbl\n\n# A tibble: 9 × 6\n  sorte       prod delta_geometric WG_geometric      a  upper\n  &lt;fct&gt;      &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1            198          0.0638         93.6 -1.72    82.1\n2 2     2576106000          1.69          -69.0  1.56  -374. \n3 3       10108800          0.558          44.2  0.447  -56.4\n4 4      497624400          1.22          -21.7  1.23  -241. \n5 5      916413472          1.37          -37.5  1.35  -285. \n6 6      673639344          1.29          -29.3  1.29  -262. \n7 7           3672          0.114          88.6 -1.14    67.9\n8 8             20          0.0404         96.0 -2.18    88.7\n9 ctrl   186729543          1               0    1.03  -180. \n\n\nWir räumen nochmal die Ausgabe auf und konzentrieren uns auf die Spalten und runden einmal die Ergebnisse.\n\nmite_res_tbl %&gt;% \n  select(sorte, WG_geometric, upper) %&gt;% \n  mutate(across(where(is.numeric), ~round(.x, 2))) \n\n# A tibble: 9 × 3\n  sorte WG_geometric  upper\n  &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 1             93.6   82.1\n2 2            -69.0 -374. \n3 3             44.2  -56.4\n4 4            -21.7 -241. \n5 5            -37.5 -285. \n6 6            -29.2 -262. \n7 7             88.6   67.9\n8 8             96.0   88.7\n9 ctrl           0   -180. \n\n\nWir können wir jetzt das einseitige 95% Konfidenzintervall interpretieren? In der Sorte 1 erhalten wir einen Wirkungsgrad von 93.6% und mit 95% Sicherheit mindestens einen Wirkungsgrad von 82.1%. Damit haben wir auch eine untere Schranke für unseren Wirkungsgradschätzer. Wir berichten für die Sorte 1 einen Wirkungsgrad von 93.6% [\\(-\\infty\\); 82.1%].\nGenauso können wir aber auch den Wirkungsgrad nach Abbott u. a. (1925) nochmal auf den Daten berechnen. Dafür müssen wir nur eine Poissonregression auf der Anzahl der Raubmilben rechnen. Die angepassten Werte können wir dann verwenden um den \\(WG_{abbott}\\) zu schätzen.\n\nfit &lt;- glm(count ~ sorte + block, data = mite_tbl, family = poisson)\n\nWir nutzen hier dann die Funktion emmeans() um die mittlere Anzahl an Raubmilben über alle Blöcke zu ermitteln. Dann können wir auch schon den \\(WG_{abbott}\\) berechnen. Wichtig ist hier, dass die Referenzkategorie mit der Kontrolle an der neunten Stelle bzw. Zeile steht. Deshalb müssen wir auch hier durch prod[9] teilen. Achtung, die \\(p\\)-Werte haben hier keine tiefere Bedeutung im Bezug auf den Wirkungsgrad und deshalb schauen wir uns diese Werte auch gar nicht tiefer an.\n\nres_mite_tbl &lt;- fit %&gt;% \n  emmeans(~sorte, type = \"response\") %&gt;% \n  tidy() %&gt;% \n  mutate(WG_abbott = (rate[9] - rate)/rate[9],\n         WG_abbott_per = percent(WG_abbott)) %&gt;% \n  select(sorte, WG_abbott, WG_abbott_per, rate)\nres_mite_tbl\n\n# A tibble: 9 × 4\n  sorte WG_abbott WG_abbott_per  rate\n  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 1         0.900 90.0%          4.49\n2 2        -1.18  -118.4%       98.0 \n3 3         0.216 21.6%         35.2 \n4 4        -0.308 -30.8%        58.7 \n5 5        -0.376 -37.6%        61.7 \n6 6        -0.904 -90.4%        85.4 \n7 7         0.880 88.0%          5.38\n8 8         0.976 97.6%          1.08\n9 ctrl      0     0.0%          44.9 \n\n\nWenn du verwirrt bist über die negativen Wirkungsgrade, dann musst du dir klar werden, dass wir immer den Wirkungsgrad im Verhältnis zur Kontrolle berechnen. Wenn du also negative Wirkungsgrade siehst, dann sind die Anzahlen in den Sorten höher als in der Kontrolle. Je nach Fragestellung macht dieses Ergebnis mehr oder weniger Sinn.\nAuch hier können wir noch schnell einen statistischen Test rechnen und die Sorten zu der Kontrolle ref = 9 vergleichen.\n\nfit %&gt;% \n  emmeans(trt.vs.ctrlk ~ sorte, type = \"response\", ref = 9) %&gt;% \n  pluck(\"contrasts\")\n\n contrast ratio      SE  df null z.ratio p.value\n 1 / ctrl 0.100 0.02098 Inf    1 -10.977  &lt;.0001\n 2 / ctrl 2.184 0.16678 Inf    1  10.229  &lt;.0001\n 3 / ctrl 0.784 0.07480 Inf    1  -2.551  0.0686\n 4 / ctrl 1.308 0.10989 Inf    1   3.196  0.0101\n 5 / ctrl 1.376 0.11436 Inf    1   3.841  0.0009\n 6 / ctrl 1.904 0.14872 Inf    1   8.244  &lt;.0001\n 7 / ctrl 0.120 0.02319 Inf    1 -10.973  &lt;.0001\n 8 / ctrl 0.024 0.00991 Inf    1  -9.028  &lt;.0001\n\nResults are averaged over the levels of: block \nP value adjustment: dunnettx method for 8 tests \nTests are performed on the log scale \n\n\nWir wir sehen, sind alle Varianten signifikant unterschiedlich zu der Kontrolle bis auf die Sorte 3.\nAuch hier können wir die Wirkungsgrade miteinander vergleichen. Aber Achtung, wir können nur Wirkungsgrade zwischen 0 und 1 miteinander sinnvoll vergleichen. Deshalb fliegen bei uns einige an Sorten raus. Darüber hinaus musst du jetzt auch einmal schauen, welche Sorte sich hinter der numerischen ID aus dem pairwise.prop.test() verbirgt.\n\nres_mite_tbl %&gt;% \n  filter(WG_abbott &lt; 1 & WG_abbott &gt;= 0) %$% \n  pairwise.prop.test(x = WG_abbott * 100, \n                   n = rep(100, 5),\n                   p.adjust.method = \"none\") \n\n\n    Pairwise comparisons using Pairwise comparison of proportions \n\ndata:  WG_abbott * 100 out of rep(100, 5) \n\n  1       2       3       4      \n2 &lt; 2e-16 -       -       -      \n3 0.821   &lt; 2e-16 -       -      \n4 0.053   &lt; 2e-16 0.019   -      \n5 &lt; 2e-16 2.7e-06 &lt; 2e-16 &lt; 2e-16\n\nP value adjustment method: none \n\n\nDas letzte Beispiel zeigt nochmal schön, dass wir uns immer überlegen müssen, was wir mit den Wirkungsgraden eigentlich zeigen wollen und welche Wirkungsgrade noch eine biologisch sinnvolle Bedeutung haben.\n\n\n21.4.3 Wirkungsgrad nach Sun-Shepard\nDer Wirkungsgrad nach Sun-Shepard bezieht sich auf die Mortalität [%] in unserem Versuch. Wir können müssen hier auch vor der Applikation und nach der Applikation in unserer Kontrolle die Mortalität bestimmen, sonst können wir nicht das \\(C_{\\Delta}\\) in der Kontrolle berechnen. Auf der Webseite von Ehab Bakr, auf die sich irgendwie in einem Zirkelschluss alle meine gefundenen Quellen beziehen, steht dann zwar \\(\\pm\\) aber ich denke, dass hier ausgesagt werden soll, dass \\(C_{\\Delta}\\) negativ oder positiv sein kann. Dann können wir auch auch besser nur Plus schreiben und damit ist dann die Formel auch in sich konsistent. Das passt dann auch zu der Durchführung und dem Beispiel auf der Webseite.\n\\[\nWG_{SS} = \\left(\\cfrac{T + C_{\\Delta}}{1 + C_{\\Delta}}\\right)\n\\]\nund\n\\[\nC_{\\Delta} = \\left(\\cfrac{C_a - C_b}{C_b}\\right)\n\\]\nmit\n\n\\(T\\) Sterberate in der Behandlung\n\\(C_b\\) Anzahl lebend in der Kontrolle nach Applikation (eng. after)\n\\(C_a\\) Anzahl lebend in der Kontrolle vor Applikation (eng. before)\n\n\n\n21.4.4 Wirkungsgrad nach Schneider-Orelli\nDer Wirkungsgrad nach Schneider-Orelli betrachtet nur die Mortalität nach dem Versuch und ist somit mit dem Wirkungsgrad nach Abbott zu vergleichen. Die Formel ist dabei sehr simpel\n\\[\nWG_{SO} = \\left(\\cfrac{T_a - C_a}{1 - C_a}\\right)\n\\]\nmit\n\n\\(T_a\\) Sterberate in der Behandlung nach Applikation (eng. after)\n\\(C_a\\) Sterberate in der Kontrolle nach Applikation (eng. after)",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Der Effektschätzer</span>"
    ]
  },
  {
    "objectID": "stat-tests-effect.html#sec-effect-interrater",
    "href": "stat-tests-effect.html#sec-effect-interrater",
    "title": "21  Der Effektschätzer",
    "section": "21.5 Interrater Reliabilität",
    "text": "21.5 Interrater Reliabilität\nWorum geht es jetzt in diesem Abschnitt? Wir wollen uns anschauen, ob verschiedene Bewerter auf die gleichen Noten oder Einschätzungen kommen. Das ist eher selten im Kontext einer Abschlussarbeit, aber kommt häufiger bei der Bonitur vor. Wir können uns dabei zwei Fälle vorstellen. Zum einen wollen wir wissen ob zwei oder mehr Bewerter auf die gleichen Noten kommen, wenn sie das gleiche Subjekt bewerten. Wir haben also eine Rasenfläche oder ein Schwein vor uns und drei Forschende sollen bestimmen wie krank der Rasen oder das Schwein auf einer Skala von 1 bis 9 ist. Vorher wurden natürlich alle drei auf der Notensakal trainiert. Das würden wir dann die Interrater Reliabilität nennen. Benoten zwei oder mehr Bewerter ein Subjekt nach einem gegebenen Notenschlüssen und Kriterien gleich. Wichtig ist hier, dass es ein Kriterienkatalog gibt. Wir machen machen hier also nicht auf Geratewohl los.\nDie andere Möglichkeit ist, dass wir wissen wollen, ob ein Bewerter selber konstante Benotungen an dem gleichen Subjekt liefert. Das können wir durch Fotos und zufällige Zuordnung machen oder eben eine Stunde oder Zeit dazwischen lassen. Wir sprechen dann von Intrarater Reliabilität. Diese Art der Reliabilität ist etwas seltener und kann auch gleich gelöst werden wie der Vergleich von unterschiedlichen Bewertern. Meistens reicht es dann auch aus und wir müssen die Korrelation durch den gleichen Bewerter eigentlich nicht weiter beachten.\n\nInterrater Reliabilität\n\nUnterscheiden sich die Bewertungen oder Benotungen an einer Beobachtung zwischen zwei oder mehr Bewertern?\n\nIntrarater Reliabilität\n\nUnterscheiden sich die Bewertungen oder Benotungen eines Bewerters an einer Beobachtung über die Zeit?\n\n\nReliabilität ist der Grad an Übereinstimmung einer Messung, wenn diese unter identischen Bedingungen jedoch von unterschiedlichen Bewertern wiederholt wird. Anhand der Interrater Reliabilität wird es somit möglich, den Grad zu ermitteln, in dem sich die erhaltenen Ergebnisse wiederholen lassen. Eine wunderbare Übersicht zur Interrater Reliabilität gibt auf Data Novia mit Introduction to R for Inter-Rater Reliability Analyses. Ebenso liefert das Tutorium Inter-rater-reliability and Intra-class-correlation einen guten Überblick über alle Methoden in R. Wir schauen uns jetzt einmal die häufigsten Maßzahlen für die Reliabilität an. Für das erste beschränken wir uns hier einmal auf Cohen’s Kappan.\n\n21.5.1 … für kategorielle Variablen\nFangen wir mit dem simpelsten Fall an. Wir haben zwei Bewerter \\(R_1\\) und \\(R_2\\) und wir wollen schauen, ob diese beiden Bewerter ein Subjekt richtig in \\(ja\\) oder \\(nein\\) einordnen. So kann die Frage sein, hat die Sonnenblume Mehltau? Oder aber die Frage, ist das Schwein erkrankt? Wir haben hier keine Wahrheit vorliegen, sondern wollen einfach nur wissen, ob unsere beiden Bewerter zum gleichen Schluss kommen. Dafür können wir dann Cohen’s Kappa \\(\\kappa\\) nutzen. Dabei ist die Verwendung von Cohen’s Kappa nicht unumstritten. Maclure und Willett (1987) diskutiert in seiner Arbeit Misinterpretation and misuse of the kappa statistic diesen Sachverhalt. Fangen wir einmal mit einem Beispiel an. Die Tabelle 21.8 zeigt einmal das Ergebnis von siebzig bewerteten Subjekten. Wir haben hier zwei Bewerter vorliegen, die jeweils mit \\(ja\\) oder \\(nein\\) bewerten sollten. In R kannst du dir später so eine Tabelle einfach mit der Funktion xtabs() bauen.\n\n\n\nTabelle 21.8— Eine 2x2 Tabelle als Beispiel für die Bewertung einer Erkrankung an siebzig Beobachtungen von zwei Bewertern. Für die folgenden Berechnungen sind die Zellen mit Buchstaben versehen.\n\n\n\n\n\n\n\nBewerter 1\n\n\n\n\n\nja (1)\nnein (0)\n\n\nBewerter 2\nja (1)\n\\(25_{\\;\\Large a}\\)\n\\(10_{\\;\\Large b}\\)\n\n\n\nnein (0)\n\\(15_{\\;\\Large c}\\)\n\\(20_{\\;\\Large d}\\)\n\n\n\n\n\n\nAls Faustregel für Cohen’s Kappa \\(\\kappa\\) dient dabei, dass die Diagonale von links oben nach rechts unten die Übereinstimmungen zeigt. Die Diagonale von links unten nach rechts oben zeigt dann die Nichtübereinstimmung. Das ist natürlich nur eine grobe Abschätzung im Folgenden einmal die Formel mit der du dann Cohen’s Kappa \\(\\kappa\\) berechnen kannst.\n\\[\n\\kappa = \\cfrac{p_0 - p_e}{1 - p_e}\n\\]\nmit\n\n\\(p_0\\) Anteil der beobachteten Übereinstimmung\n\\(p_e\\) Anteil der veränderten Übereinstimmung\n\nDas ist jetzt natürlich wieder sehr theoretisch. Deshalb wollen wir einmal die beiden Teile \\(p_0\\) und \\(p_e\\) aus den obigen Daten berechnen. Wenn du jetzt denkst, dass dir das doch irgendwie bekannt vorkommt, ja der \\(\\mathcal{X}^2\\)-Test sieht sehr ähnlich aus. Wir können \\(p_0\\) wie folgt aus der 2x2 Kreuztabelle berechnen. Wir schauen hier auf die Übereinstimmungen auf der Diagonalen von obeb links nach unten rechts.\n\\[\np_0 = \\cfrac{a + d}{N} = \\cfrac{25 + 20}{70} = 0.643\n\\]\nFür den Wert von \\(p_e\\) brauchen wir zwei Werte, die wir dann aufaddieren. Einmal berechnen wir den Wert für \\(p_e\\) von \\(ja\\) und einmal den Wert von \\(p_e\\) für \\(nein\\). Das geht dann mit der Formel auch recht schnell. Wir schauen hier im Prinzip auf die Diagonale von unten links nach oben rechts.\n\\[\np_e^{ja} = \\cfrac{a + b}{N} \\cdot \\cfrac{a + c}{N} = \\cfrac{25+10}{70} \\cdot \\cfrac{25+15}{70} = 0.5 \\cdot 0.57 = 0.285\n\\]\n\\[\np_e^{nein} = \\cfrac{c + d}{N} \\cdot \\cfrac{b + d}{N} = \\cfrac{15+20}{70} \\cdot \\cfrac{10+20}{70} = 0.5 \\cdot 0.428 = 0.214\n\\]\nDann addieen wir die beiden Werte für \\(p_e\\) einmal auf.\n\\[\np_e = p_e^{ja} + p_e^{nein} = 0.285 + 0.214 = 0.499\n\\]\nDann können wir auch schon Cohen’s Kappa \\(\\kappa\\) für eine 2x2 Kreuztabelle berechnen. Ehrlicherweise, machen wir das dann natürlich gleich in R.\n\\[\n\\kappa = \\cfrac{0.643 - 0.499}{1 - 0.499} = 0.28\n\\]\nDabei interpretieren wir dann Cohen’s Kappa \\(\\kappa\\) wie folgt. Beachte dabei immer, dass es sich natürlich erstmal nur um eine grobe Abschätzung handelt. Wie gut oder wie schlecht dann diese Zahl von Cohen’s Kappa \\(\\kappa\\) in deinem Bereich ist, liegt dann auch daran wie genau du arbeiten möchtest.\n\nCohen’s Kappa \\(\\kappa\\) interpretieren\n\nCohen’s Kappa kann von -1 (keine Übereinstimmung) bis +1 (perfekte Übereinstimmung) reichen. Wenn \\(\\kappa = 0\\) ist, ist die Übereinstimmung nicht besser als das, was durch Zufall erreicht werden würde. Wenn \\(\\kappa\\) negativ ist, ist die Übereinstimmung geringer als die zufällig erwartete Übereinstimmung. Wenn \\(\\kappa\\) positiv ist, übersteigt die Übereinstimmung der Bewerter die zufällige Übereinstimmung.\n\n\nWas sind jetzt die exakten grenzen für die Cohen’s Kappa Werte? Das kannst du dir fast wieder aussuchen wie du möchtest. Es gibt eine Reihe von Publikationen, die alle verschiedene Grenzwerte liefern. Gerne kannst du dir die Tabellen in dem Tutorium Cohen’s Kappa in R: For Two Categorical Variables einmal näher anschauen. Ich würde dann mal als Quintessenz folgende Regel von McHugh (2012) nehmen.\n\n\n\nTabelle 21.9— Interpretation des Cohen’s Kappa Wert nach McHugh (2012)\n\n\n\n\n\nCohen’s Kappa\nLevel of agreement\n% of data that are reliable\n\n\n\n\n0 - 0.20\nNone\n0 - 4‰\n\n\n0.21 - 0.39\nMinimal\n4 - 15%\n\n\n0.40 - 0.59\nWeak\n15 - 35%\n\n\n0.60 - 0.79\nModerate\n35 - 63%\n\n\n0.80 - 0.90\nStrong\n64 - 81%\n\n\nAbove 0.90\nAlmost Perfect\n82 - 100%\n\n\n\n\n\n\nDas R Paket {vcd} liefert nun die Funktion Kappa() mit der wir auf einer 2x2 Kreuztabelle zum Beispiel aus der Funktion xtabs() dann Cohen’s Kappa \\(\\kappa\\) berechnen können. Die Funktion xtabs() hilft dir aus zwei oder mehr Spalten eine Kreuztabelle zu bauen.\n\nxtab &lt;- as.table(rbind(c(25, 10), c(15, 20)))\n\nDann können wir auch schon Cohen’s Kappa \\(\\kappa\\) in R berechnen.\n\nkappa_res &lt;- Kappa(xtab)\nkappa_res\n\n            value    ASE    z Pr(&gt;|z|)\nUnweighted 0.2857 0.1134 2.52  0.01173\nWeighted   0.2857 0.1134 2.52  0.01173\n\n\nDie ungewichtete Version entspricht dem Cohen’s Kappa, mit dem wir uns in diesem Kapitel beschäftigen. Das gewichtete Kappa sollte nur für ordinale Variablen betrachtet werden und wird weitgehend hier ignoriert. Dann noch die Konfidenzintervalle und alles ist soweit da, was wir brauchen.\n\nkappa_res |&gt; \n  confint()\n\n            \nKappa               lwr      upr\n  Unweighted 0.06352154 0.507907\n  Weighted   0.06352154 0.507907\n\n\nDas Ganze geht natürlich nicht nur für zwei Level (ja/nein) sondern dann in R auch für mehrere Level. Also wenn wir zum Beispiel dann Boniturnoten einmal bewerten und dann für zwei Bewerter vergleichen wollen.\nCohen’s Kappa für mehr als zwei Bewerter liefert das R Paket {irr} mit der Funktion kappam.light() oder kappam.fleiss(), wenn es dann auch noch mehr Kategorien als zwei in den Spalten gibt. Das R Paket ist schon ziemlich alt und hat auch sehr schlechte bis gar keine Hilfeseiten. Wenn man weiß, was man braucht und die Funktion bedeuten, dann kannst du das Paket gut nutzen.\nHier helfen dann auch die passenden Tutorien weiter. Ich habe hier nochmal die beiden anderen Fälle aufgelistet, wenn du eben Noten oder noch andere Kategorien in deinen Daten hast. Im Prinzip änderst du nur deine Funktion die du dann nutzt. Die Bewertung der einzelnen Ausgaben schwankt dann auch leicht. Aber eine Eins bedeutet immer eine perfekte Übereinstimmung.\n\nWeighted Kappa in R: For Two Ordinal Variables mit der Funktion Kappa() aus dem R Paket {vcd}. Wir nutzen dann die Weighted Zeile in der Ausgabe der Funktion.\nFleiss’ Kappa in R: For Multiple Categorical Variables und der Funktion kappam.fleiss() aus dem R Paket {irr}.\n\nDamit wären wir hier soweit erstmal fertig. Im nächsten Abschnitt schauen wir uns dann einmal den Fall an, dass wir kontinuierliche Bewertungen haben. Wir haben nämlich gezählt, was ja auch eine Art der Bewertung ist. Nur eben nicht auf einer Notenskala sondern eben kontinuierlich.\n\n\n21.5.2 … für kontinuierliche Variablen\njetzt betrachten wir einen Datensatz indem Entenküken auf verschiedenen Bildern von Studierenden gezählt werden sollten. Insgesamt haben wir vier Studierende als Bewerter, die die Küken dann in zwei voneinander getrennten Sessions bewertet haben. Wir laden also einmal den Datensatz und schauen uns die Zähldaten einmal an.\n\nduck_tbl &lt;- read_excel(\"data/interrater_duck_count.xlsx\")\n\nIn der Tabelle 21.10 siehst du einmal einen Auszug aus den Entendaten. Dabei steht die id für ein Bild, die Spalte session beschreibt die beiden Termine, wo die Studierenden dann die Bilder ausgezählt haben.\n\n\n\n\nTabelle 21.10— Auszug aus Entenkükendatensatz.\n\n\n\n\n\n\nid\nsession\nrater_1\nrater_2\nrater_3\nrater_4\n\n\n\n\n#385\n1\n106\n115\n100\n82\n\n\n#387\n1\n77\n94\n103\n77\n\n\n#374\n1\n120\n131\n213\n133\n\n\n#381\n1\n115\n127\n208\n135\n\n\n#871\n1\n104\n165\n97\n165\n\n\n…\n…\n…\n…\n…\n…\n\n\n#874\n2\n43\n39\n33\n43\n\n\n#826\n2\n110\n122\n114\n117\n\n\n#832\n2\n104\n99\n89\n106\n\n\n#836\n2\n101\n100\n107\n103\n\n\n#821\n2\n92\n106\n108\n94\n\n\n\n\n\n\n\n\nDa so eine Datentabelle dann imer schwer zu lesen ist, dann hier einmal in der Abbildung 21.7 die Visualisierung der Daten. Wir sehen, dass die zweite Session schonmals ehr viel weniger Varianz zeigt und sich die Bewerter sehr viel ähnlicher sind als bei der ersten Session. Darüber hinaus ist der dritte Bewerter in der ersten Session am schlechtesten, da er die größte Varianz und Abweichung zu den anderen Bewertern zeigt.\n\nduck_tbl |&gt; \n  pivot_longer(cols = rater_1:rater_4,\n               values_to = \"count\",\n               names_to = \"rater\") |&gt; \n  mutate(session = as_factor(session)) |&gt; \n  ggplot(aes(x = rater, y = count, color = session, label = id)) +\n  theme_minimal() +\n  geom_text(position = position_dodge(0.5), show.legend = FALSE) +\n  scale_color_okabeito() +\n  labs(x = \"Bewerter\", y = \"Gezählte Anzahl an Entenküken auf dem Bild\")\n\n\n\n\n\n\n\nAbbildung 21.7— Visualisierung der Zählungen an Entenküken über die beiden Sessions der vier Studierenden. Die Zahlen geben die jeweilige Bildnummer an.\n\n\n\n\n\nWas erwarten wir nun? Zum einen sollte die zweite Session bessere Übereinstimmungen liefern als die erste Session. Immerhin haben ja die Studierenden geübt. Wir nutzen jetzt hier den Intraclass Correlation Coefficient (ICC) in R um auf die Reliabilität von Bewertern zurückzuschließen. Das Tutorium liefert nochmal eine Reihe an mehr Informationen. Ich halte hier die Sachlage relativ kurz. Wir wollen hier mal prinzipiell schauen, wie unsere Bewerter so abschneiden. Als zweite Überlegung wollen wir dann noch wissen, ob unsere Bewerter dann auch konsistent sind. Daher die erste Session genauso wie die zweite Session bewertet haben. Aber auch hier Achtung, wir wissen nicht die Wahrheit. Wir schauen nur, ob die beiden Sessions gleich waren. Ein Bewerter, der in der ersten Session versagt und in der zweiten alles richtig zählt, würde hier also schlechter wegkommen, als ein Bewerter, der nichts gelernt hat und in beiden Fällen schlecht zählt. Wir würden dann nur sehen, dass die Bewerter untereinander nicht passen.\nBetrachten wir also einmal in den beiden folgenden Tabs die beiden Sessions getrennt voneinander über alle Bewerter hinweg und berechnen den Intraclass Correlation Coefficient (ICC). Der ICC-Wert läuft von 0 bis 1, wobei 0 dann gar keine Übereinstimmung und 1 eine perfekte Übereinstimmung bedeutet. Wir würden sagen, dass ab 0.9 eine perfekte Übereinstimmung und unter 0.5 eine schlechte Übereinstimmung vorliegt. Wichtig ist hier, dass die Funktion icc() nur die Spalten mit den Bewertungen will und sonst rein gar nichts.\n\nSession 1Session 2\n\n\n\nduck_tbl |&gt; \n  filter(session == 1) |&gt; \n  select(matches(\"rater\")) |&gt;\n  icc(model = \"twoway\", type = \"agreement\", unit = \"single\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 12 \n     Raters = 4 \n   ICC(A,1) = 0.304\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n F(11,29.2) = 3.32 , p = 0.00462 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.061 &lt; ICC &lt; 0.638\n\n\nWir sehen, dass wir eine relativ schlechte Übereinstimmung haben. Der ICC-Wert ist mit 0.304 sehr nahe an der Null. Auch wenn der Test sagt, dasss wir einen signifkanten Unterschied von der Null haben, geht mir ein ICC-Wert von unter 0.8 nicht als Gleich durch. Das deckt sich auch mit unserer obigen Abbildung.\n\n\n\nduck_tbl |&gt; \n  filter(session == 2) |&gt; \n  select(matches(\"rater\")) |&gt;\n  icc(model = \"twoway\", type = \"agreement\", unit = \"single\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 12 \n     Raters = 4 \n   ICC(A,1) = 0.806\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n F(11,33.4) = 16.3 , p = 2.2e-10 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.61 &lt; ICC &lt; 0.931\n\n\nWir sehen, dass wir eine relativ gute Übereinstimmung haben! Der ICC-Wert ist mit 0.806 nahe an der Eins. Das deckt sich auch mit unserer obigen Abbildung, wo wir sehen, dass die Bewerter in der zweiten Session viel mehr auf einer Linie liegen. Zwar ist 0.8 noch nicht perfekt, aber schonmal eins sehr viel besserer Wert.\n\n\n\nJetzt möchte ich mir nochmal die Bewerter einzeln für die jeweiligen Sessions anschauen. Wie unterscheiden sich den die Bewerter, wenn sie ein Bild erneut sehen? Auch hier nochmal, wir haben keinen Goldstandard, wir schauen nur, ob die Bewerter in der zweiten Session genauso zählen wie in der ersten Session. Wir machen das jetzt im Folgenden auf zwei Arten. Einmal kannst du das händsich über select() machen. Du wählst einfach immer den Bewerter aus, den du möchtest und führst den Code aus. Du ersetzt also rater_1 dann durch rater_2 und dann so weiter. Die zweite Variante ist alles in einem Rutsch mit der Funktion map().\n\nFür einen Rater – händischFür alle Rater – map()\n\n\n\nduck_tbl |&gt; \n  select(id, session, rater_1) |&gt;\n  pivot_wider(names_from = session, values_from = rater_1, names_prefix = \"session_\") |&gt; \n  select(-id) |&gt; \n  icc(model = \"twoway\", type = \"agreement\", unit = \"single\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 12 \n     Raters = 2 \n   ICC(A,1) = 0.985\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n F(11,11.3) = 124 , p = 4.35e-10 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.95 &lt; ICC &lt; 0.996\n\n\nWir sehen, dass der erste Studierende extrem konsistent die zweite Session im Vergleich zu der ersten bewertet hat. Der ICC-Wert ist fast bei Eins und somit fast ein perfektes Ergebnis. Auf einer numerischen Skala ist auch kaum noch mehr Spielraum nach oben.\n\n\nJetzt spielen wir ein wenig. Zuerst bauen wir uns dann für jeden Bewerter einen Datensatz der nur aus den beiden Sessions besteht. Dafür nutzen wir die Funktion pivot_wider(). Dann entfernen wir noch die ID, da die Funktion icc() gleich die ID nicht mag. Wir nutzen die Funktion map() um für jeden Bewerter den Datensatz zu bauen und am Ende in einer Liste zu speichern.\n\nrater_lst &lt;- map(c(\"rater_1\", \"rater_2\", \"rater_3\", \"rater_4\"), \\(x) {\n  duck_tbl |&gt; \n  select(id, session, all_of(x)) |&gt;\n  pivot_wider(names_from = session, \n              values_from = x, \n              names_prefix = \"session_\") |&gt; \n  select(-id)\n})\n\nDa wir alles einer Liste gespeichert haben, können wir jetzt die Liste mit der Funktion map() iterativ bearbeiten. Wir rufen dann für jeden der Listeneinträge dann einmal die Funktion icc() auf und extrahieren uns durch die Funktion pluck() den berechneten ICC Wert. Wenn du die letzte Zeile weglässt, dann kriegst du eben vier mal die Ausgabe der Funktion icc(), das war mir dann hier zu viel Platz.\n\nrater_lst |&gt; \n  map(~icc(ratings = .x, model = \"twoway\", type = \"agreement\", unit = \"single\")) |&gt; \n  map_dbl(pluck(\"value\"))\n\n[1] 0.98511710 0.26459153 0.26335874 0.06509083\n\n\nWir sehen hier einmal als Werte, was wir auch schon in der Abbildung 21.7 beobachten konnten. Je höher die Zahl, desto besser die Übereinstimmung zwischen der ersten und der zweiten Session. Die Reihenfolge ist die Reihenfolge der Bewerter.\n\n\n\nDer erste Studierende ist der konsitenteste Bewerter und der vierte Studierende hat die geringste Übereinstimmung. Hier muss man aber vorsichtig sein, da wir ja sehen, dass die erste Session teilweise sehr mies war. Dann kann auch die Übereinstimmung zwischen der ersten und zweiten Session nur schelcht sein, wenn sich der Studierende sehr verbessert hat. Denn eins musst du immer wissen, die Lernleistung oder Verbesserung kannst du hier nicht abbilden. Ein sehr schelchter Bewerter, der aber zügig lernt, wird hier immer mit einer sehr schelchten Übereinstimmung rauskommen. Dagegen müsstest du dann sehr viele Sessions laufen lassen, denn je mehr Sessions er macht, desto weniger unterscheiden sich diese.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Der Effektschätzer</span>"
    ]
  },
  {
    "objectID": "stat-tests-effect.html#referenzen",
    "href": "stat-tests-effect.html#referenzen",
    "title": "21  Der Effektschätzer",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 21.1— Auszug aus dem Zusammenfassung (eng. abstract) aus der Arbeit von Venables u. a. (2008). Der Fokus liegt hier auf der Steigerung von 17% Fettverbrennungsrate von Placebo zu Grünen Teeextrakt.\nAbbildung 21.2— Auszug aus dem Ergebnis zu der Kohlenhydrat- und Fatverbrennung aus der Arbeit von Venables u. a. (2008). Auch hier liegt der Fokus auf der Veränderung von Placebo zu Grünen Teeextrakt.\nAbbildung 21.3— Die Abbildung 1.A (eng. FIGURE 1.A) aus der Arbeit von mit der ein Unterschied in der Fettverbrennung von 17% von Placebo zu Grünen Teeextrakt gezeigt werden soll. Der Text schreibt von einem Unterschied in der Einheit \\(g/min\\). In der Abbildung werden als Einheit \\(kJ/min\\) angegeben. Entweder ist die Referenz nicht korrekt oder aber die Umrechung nicht nachzuvollziehen. Fett hat einen Energiegehalt von ca. \\(37kj/g\\). Abkürzung: total energy expenditure (TEE).\nAbbildung 21.4— (A) Zerforschung der Abbildung 1.A (eng. FIGURE 1.A) aus der Arbeit von Venables u. a. (2008) mit der ein Unterschied in der Fettverbrennung von 17% von Placebo zu Grünen Teeextrakt gezeigt werden soll. Ich habe hierbei die Werte abgelesen und die Standardabweichung anstatt den Standardfehler für die Fehlerbalken gewählt. Der Verlauf ist gut getroffen und die Fläche unter der Kurve soll berechnet werden. Auf der linken \\(y\\)-Achse findet sich die Fettverbrennungsrate in \\(kJ/min\\) auf der rechten Seite die mit dem Faktor \\(37 kJ/g\\) umgerechnete Einheit in \\(g/min\\). (B) Zerforschung und Visualisierung als Barplots der Fettverbrennungsrate aus der Zusammenfassung (eng. abstract) der Arbeit von Venables u. a. (2008).\nAbbildung 21.5 (a)— Verteilung der Werte auf der originalen Skala.\nAbbildung 21.5 (b)— Verteilung der Werte auf der logarithmischen Skala. Beobachtungen mit einer 0 Zählung wurden auf 1 gesetzt.\nAbbildung 21.6 (a)— Verteilung der Werte auf der originalen Skala.\nAbbildung 21.6 (b)— Verteilung der Werte auf der logarithmischen Skala. Beobachtungen mit einer 0 Zählung wurden auf 1 gesetzt.\nAbbildung 21.7— Visualisierung der Zählungen an Entenküken über die beiden Sessions der vier Studierenden. Die Zahlen geben die jeweilige Bildnummer an.\n\n\n\nAbbott WS, others. 1925. A method of computing the effectiveness of an insecticide. J. econ. Entomol 18: 265–267.\n\n\nFinner H, Kunert J, Sonnemann E. 1989. Über die Berechnung des Wirkungsgrades von Pflanzenschutzmitteln. Nachrichtenblatt des Deutschen Pflanzenschutzdienstes (Braunschweig) 41: 145–149.\n\n\nGeorge A, Stead TS, Ganti L. 2020. What’s the risk: differentiating risk ratios, odds ratios, and hazard ratios? Cureus 12.\n\n\nGrant RL. 2014. Converting an odds ratio to a range of plausible relative risks for better communication of research findings. Bmj 348.\n\n\nHenderson CF, Tilton EW. 1955. Tests with acaricides against the brown wheat mite. Journal of economic entomology 48: 157–161.\n\n\nMaclure M, Willett WC. 1987. Misinterpretation and misuse of the kappa statistic. American journal of epidemiology 126: 161–169.\n\n\nMcHugh ML. 2012. Interrater reliability: the kappa statistic. Biochemia medica 22: 276–282.\n\n\nVenables MC, Hulston CJ, Cox HR, Jeukendrup AE. 2008. Green tea extract ingestion, fat oxidation, and glucose tolerance in healthy humans. The American journal of clinical nutrition 87: 778–784.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Der Effektschätzer</span>"
    ]
  },
  {
    "objectID": "stat-tests-pretest.html",
    "href": "stat-tests-pretest.html",
    "title": "22  Der Pre-Test oder Vortest",
    "section": "",
    "text": "22.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, olsrr,\n               broom, car, performance, \n               see, scales, readxl, nlme,\n               conflicted)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(dplyr::select)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Der Pre-Test oder Vortest</span>"
    ]
  },
  {
    "objectID": "stat-tests-pretest.html#die-flowchart-die-jeder-möchte",
    "href": "stat-tests-pretest.html#die-flowchart-die-jeder-möchte",
    "title": "22  Der Pre-Test oder Vortest",
    "section": "22.2 Die Flowchart, die jeder möchte…",
    "text": "22.2 Die Flowchart, die jeder möchte…\nDann hier mal gleich zu Beginn die Flowchart, die jeder gerne möchte. In der Abbildung 22.1 sehen wir einmal dargestellt, welches statistische Modell wir rechnen können um dann eine ANOVA und ein Posthoc-Test anzuschließen. Wie du auf den ersten Blick siehst, ist es immer besser auf der linken Seite der Flowchart zu sein. Viele Merkmale in den Agrarwissenschaften folgen per se einer Normalverteilung, so dass sich nur die Frage nach der Varianzhomogenität stellt.\n\n\n\n\n\n\nflowchart TD\n    A(\"Normalverteiltes Outcome\n       in jeder Versuchsgruppe\"):::factor --- B(((ja))) --&gt; B1 \n    A(\"Normalverteiltes Outcome\n       in jeder Versuchsgruppe\"):::factor --- F(((nein))) --&gt; B2\n    subgraph B1[\"Mittelwertsvergleiche\"]\n    C(\"Varianzhomogenität\n       über alle Gruppen\"):::factor --- D(((\"ja\"))) --&gt; E(\"lm()\"):::factor\n    C(\"Varianzhomogenität\n       über alle Gruppen\"):::factor --- J(((nein))) --&gt; K(\"gls()\"):::factor \n    end\n    subgraph B2[\"Medianvergleiche, OR oder RR\"]\n    G(\"Varianzhomogenität\n       über alle Gruppen\"):::factor --- H(((ja))) --&gt; I(\"rq()\"):::factor  \n    G(\"Varianzhomogenität\n       über alle Gruppen\"):::factor --- L(((nein))) \n    end\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 22.1— Flowchart für die Entscheidung welches statistische Modell gerechnet werden kann: lm(), lineare Regression, gls(), generalized least squares Regression, rq(), Quantilesregression. Die Funktionen finden sich teilweise in eigenen R Paketen. Bei einem nicht-normalverteilten Outcome mit Varianzheterogenität über die Gruppen müssen wir nochmal gemeinsam in die Daten schauen.\n\n\n\n\n\nDu findest die vorgeschlagenen Funktionen dann in den entsprechenden Kapiteln zur ANOVA und den Posthoc-Tests. Du kannst dir dann dort den Code zusammensuchen. Je nach deiner Datenlage musst du dann nochmal etwas an dem R Code programmieren. Beachte, dass die Funktionen sich teilweise in eigenen R Paketen finden lassen. So ist die Funktion gls() im R Paket {nlme} und die Funktion rq() im R Paket {quantreg} zu finden. Du kannst auch bei Varianzheterogenität das R Paket {sandwich} nutzen und einen entsprechend angepassten Varianzschätzer. Mehr findest du dazu bei den Posthoc-Test in dem Abschnitt zu dem Gruppenvergleich unter Varianzheterogenität oder gleich im ersten Zerforschenbeispiel zum einfaktoriellen Barplot.\nUnd dann auch gleich hier, weil es so schön passt, die Zerforschenbeispiele zu der obigen Flowchart. Im Prinzip kannst du einmal rein schauen und sehen, ob die Abbildung dem entspricht was du möchtest. Mehr zu dem Code findest du dann in den entsprechenden Kapiteln zu den multiplen Vergleichen oder aber der Regressionen.\n\n\n\n\n\n\nZerforschen: Einfaktorieller Barplot mit lm(), gls() oder sandwich\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns einen einfaktoriellen Barplot oder Säulendiagramm anschauen. Daher fangen wir mit der folgenden Abbildung einmal an. Wir haben hier ein Säulendiagramm mit compact letter display vorliegen. Daher brauchen wir eigentlich gar nicht so viele Zahlen. Für jede der vier Behandlungen jeweils einmal einen Mittelwert für die Höhe der Säule sowie einmal die Standardabweichung. Die Standardabweichung addieren und subtrahieren wir dann jeweils von dem Mittelwert und schon haben wir die Fehlerbalken. Für eine detaillierte Betrachtung der Erstellung der Abbildung schauen einmal in das Kapitel zum Barplot oder Balkendiagramm oder Säulendiagramm.\n\n\n\n\n\n\nAbbildung 22.2— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein simples Säulendiagramm mit sehr für Farbblinde ungünstigen Farben. Es sind die Mittelwerte sowie die Standardabweichung durch die Fehlerbalken dargestellt.\n\n\n\nAls erstes brauchen wir die Daten. Die Daten habe ich mir in dem Datensatz zerforschen_barplot_simple.xlsx selber ausgedacht. Ich habe einfach die obige Abbildung genommen und den Mittelwert abgeschätzt. Dann habe ich die vier Werte alle um den Mittelwert streuen lassen. Dabei habe ich darauf geachtet, dass die Streuung dann in der letzten Behandlung am größten ist.\n\nbarplot_tbl &lt;- read_excel(\"data/zerforschen_barplot_simple.xlsx\") |&gt; \n  mutate(trt = factor(trt, \n                      levels = c(\"water\", \"rqflex\", \"nitra\", \"laqua\"),\n                      labels = c(\"Wasserdestilation\",\n                                 \"RQflex Nitra\",\n                                 \"Nitrachek\",\n                                 \"Laqua Nitrat\")))\nbarplot_tbl \n\n# A tibble: 16 × 2\n   trt               nitrat\n   &lt;fct&gt;              &lt;dbl&gt;\n 1 Wasserdestilation    135\n 2 Wasserdestilation    130\n 3 Wasserdestilation    145\n 4 Wasserdestilation    135\n 5 RQflex Nitra         120\n 6 RQflex Nitra         130\n 7 RQflex Nitra         135\n 8 RQflex Nitra         135\n 9 Nitrachek            100\n10 Nitrachek            120\n11 Nitrachek            130\n12 Nitrachek            130\n13 Laqua Nitrat         230\n14 Laqua Nitrat         210\n15 Laqua Nitrat         205\n16 Laqua Nitrat         220\n\n\nIm Folgenden sparen wir uns den Aufruf mit group_by() den du aus dem Kapitel zum Barplot schon kennst. Wir machen das alles zusammen in der Funktion emmeans() aus dem gleichnamigen R Paket. Der Vorteil ist, dass wir dann auch gleich die Gruppenvergleiche und auch das compact letter display erhalten. Einzig die Standardabweichung \\(s\\) wird uns nicht wiedergegeben sondern der Standardfehler \\(SE\\). Da aber folgernder Zusammenhang vorliegt, können wir gleich den Standardfehler in die Standardabweichung umrechnen.\n\\[\nSE = \\cfrac{s}{\\sqrt{n}}\n\\]\nWir rechnen also gleich einfach den Standardfehler \\(SE\\) mal der \\(\\sqrt{n}\\) um dann die Standardabweichung zu erhalten. In unserem Fall ist \\(n=4\\) nämlich die Anzahl Beobachtungen je Gruppe. Wenn du mal etwas unterschiedliche Anzahlen hast, dann kannst du auch einfach den Mittelwert der Fallzahl der Gruppen nehmen. Da überfahren wir zwar einen statistischen Engel, aber der Genauigkeit ist genüge getan.\nIn den beiden Tabs siehst du jetzt einmal die Modellierung unter der Annahme der Varianzhomogenität mit der Funktion lm() und einmal die Modellierung unter der Annahme der Varianzheterogenität mit der Funktion gls() aus dem R Paket nlme. Wie immer lässt sich an Boxplots visuell überprüfen, ob wir Homogenität oder Heterogenität vorliegen haben. Oder aber du schaust nochmal in das Kapitel Der Pre-Test oder Vortest, wo du mehr erfährst.\n\nVarianzhomogenitätVarianzheterogenität (gls)Varianzheterogenität (vcovHAC)\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim().\n\nemmeans_homogen_tbl &lt;- lm(nitrat ~ trt, data = barplot_tbl) |&gt;\n  emmeans(~ trt) |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(4)) \nemmeans_homogen_tbl\n\n# A tibble: 4 × 8\n  trt               emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 Nitrachek           120   5.08    12     109.     131. a       10.2\n2 RQflex Nitra        130   5.08    12     119.     141. ab      10.2\n3 Wasserdestilation   136.  5.08    12     125.     147. b       10.2\n4 Laqua Nitrat        216.  5.08    12     205.     227. c       10.2\n\n\nIn dem Objekt emmeans_homogen_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. Wie dir vielleicht auffällt sind alle Standardfehler und damit alle Standardabweichungen für alle Gruppen gleich, das war ja auch unsere Annahme mit der Varianzhomogenität.\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket und natürlich in den Abschnitt zu dem Gruppenvergleich unter Varianzheterogenität. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim().\n\nemmeans_hetrogen_gls_tbl &lt;- gls(nitrat ~ trt, data = barplot_tbl, \n                            weights = varIdent(form =  ~ 1 | trt)) |&gt;\n  emmeans(~ trt) |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(4)) \nemmeans_hetrogen_gls_tbl\n\n# A tibble: 4 × 8\n  trt               emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 Nitrachek           120   7.07  3.00     97.5     143. a      14.1 \n2 RQflex Nitra        130   3.54  2.98    119.      141. a       7.07\n3 Wasserdestilation   136.  3.15  2.98    126.      146. a       6.29\n4 Laqua Nitrat        216.  5.54  3.00    199.      234. b      11.1 \n\n\nIn dem Objekt emmeans_hetrogen_gls_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. In diesem Fall hier sind die Standardfehler und damit auch die Standardabweichungen nicht alle gleich, wir haben ja für jede Gruppe eine eigene Standardabweichung angenommen. Die Varianzen sollten ja auch heterogen sein.\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket und natürlich in den Abschnitt zu dem Gruppenvergleich unter Varianzheterogenität. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim().\n\nemmeans_hetrogen_vcov_tbl &lt;- lm(nitrat ~ trt, data = barplot_tbl) |&gt;\n  emmeans(~ trt, vcov. = sandwich::vcovHAC) |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(4)) \nemmeans_hetrogen_vcov_tbl\n\n# A tibble: 4 × 8\n  trt               emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 Nitrachek           120   7.84    12     103.     137. a      15.7 \n2 RQflex Nitra        130   3.92    12     121.     139. a       7.84\n3 Wasserdestilation   136.  2.06    12     132.     141. a       4.12\n4 Laqua Nitrat        216.  4.94    12     205.     227. b       9.88\n\n\nIn dem Objekt emmeans_hetrogen_vcov_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. In diesem Fall hier sind die Standardfehler und damit auch die Standardabweichungen nicht alle gleich, wir haben ja für jede Gruppe eine eigene Standardabweichung angenommen. Die Varianzen sollten ja auch heterogen sein.\n\n\n\nUnd dann haben wir auch schon die Abbildungen hier erstellt. Ja vielleicht passen die Standardabweichungen nicht so richtig, da könnte man nochmal an den Daten spielen und die Werte solange ändern, bis es besser passt. Du hast aber jetzt eine Idee, wie der Aufbau funktioniert. Die beiden Tabs zeigen dir dann die Abbildungen für die beiden Annahmen der Varianzhomogenität oder Varianzheterogenität. Der Code ist der gleiche für die drei Abbildungen, die Daten emmeans_homogen_tbl oder emmeans_hetrogen_gls_tbl ober emmeans_hetrogen_vcov_tbl sind das Ausschlaggebende.\n\nVarianzhomogenitätVarianzheterogenität (gls)Varianzheterogenität (vcovHAC)\n\n\n\nggplot(data = emmeans_homogen_tbl, aes(x = trt, y = emmean, fill = trt)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\") + \n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd), \n                width = 0.2) + \n  labs(x = \"\", \n       y = \"Nitrat-Konzentration \\n im Tannensaft [mg/L]\") +\n  ylim(0, 250) +\n  theme(legend.position = \"none\") + \n  scale_fill_okabeito() + \n  geom_text(aes(label = .group, y = emmean + sd + 10))\n\n\n\n\n\n\n\nAbbildung 22.3— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt.\n\n\n\n\n\n\n\n\nggplot(data = emmeans_hetrogen_gls_tbl, aes(x = trt, y = emmean, fill = trt)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\") + \n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd), \n                width = 0.2) + \n  labs(x = \"\", \n       y = \"Nitrat-Konzentration \\n im Tannensaft [mg/L]\") +\n  ylim(0, 250) +\n  theme(legend.position = \"none\") + \n  scale_fill_okabeito() + \n  geom_text(aes(label = .group, y = emmean + sd + 10))\n\n\n\n\n\n\n\nAbbildung 22.4— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion gls() im obigen Modell berücksichtigt.\n\n\n\n\n\n\n\n\nggplot(data = emmeans_hetrogen_vcov_tbl, aes(x = trt, y = emmean, fill = trt)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\") + \n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd), \n                width = 0.2) + \n  labs(x = \"\", \n       y = \"Nitrat-Konzentration \\n im Tannensaft [mg/L]\") +\n  ylim(0, 250) +\n  theme(legend.position = \"none\") + \n  scale_fill_okabeito() + \n  geom_text(aes(label = .group, y = emmean + sd + 10))\n\n\n\n\n\n\n\nAbbildung 22.5— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion sandwich::vcovHAC im obigen Modell berücksichtigt.\n\n\n\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_barplot.png\", width = 5, height = 3)\n\n\n\n\n\n\n\n\n\n\nZerforschen: Zweifaktorieller Barplot mit lm() oder gls()\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns einen zweifaktoriellen Barplot anschauen. Wir haben hier ein Säulendiagramm mit compact letter display vorliegen. Daher brauchen wir eigentlich gar nicht so viele Zahlen. Für jede der vier Zeitpunkte und der Kontrolle jeweils einmal einen Mittelwert für die Höhe der Säule sowie einmal die Standardabweichung. Da wir hier aber noch mit emmeans() eine Gruppenvergleich rechnen wollen, brauchen wir mehr Beobachtungen. Wir erschaffen uns also fünf Beobachtungen je Zeit/Jod-Kombination. Für eine detaillierte Betrachtung der Erstellung der Abbildung schauen einmal in das Kapitel zum Barplot oder Balkendiagramm oder Säulendiagramm.\n\n\n\n\n\n\nAbbildung 22.6— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit den zwei Faktoren Zeit und die Iodine Form.\n\n\n\nAls erstes brauchen wir die Daten. Die Daten habe ich mir in dem Datensatz zerforschen_barplot_2fac_target_emmeans.xlsx selber ausgedacht. Ich habe einfach die obige Abbildung genommen und den Mittelwert abgeschätzt. Dann habe ich die fünf Werte alle um den Mittelwert streuen lassen. Ich brauche hier eigentlich mehr als fünf Werte, sonst kriegen wir bei emmeans() und der Interaktion im gls()-Modell Probleme, aber da gibt es dann bei kleinen Fallzahlen noch ein Workaround. Bitte nicht mit weniger als fünf Beobachtungen versuchen, dann wird es schwierig mit der Konsistenz der Schätzer aus dem Modell.\nAch, und ganz wichtig. Wir entfernen die Kontrolle, da wir die Kontrolle nur mit einer Iodid-Stufe gemessen haben. Dann können wir weder die Interaktion rechnen, noch anständig eine Interpretation durchführen.\n\nbarplot_tbl &lt;- read_excel(\"data/zerforschen_barplot_2fac_target_emmeans.xlsx\")  |&gt;  \n  mutate(time = factor(time, \n                       levels = c(\"ctrl\", \"7\", \"11\", \"15\", \"19\"),\n                       labels = c(\"Contr.\", \"07:00\", \"11:00\", \"15:00\", \"19:00\")),\n         type = as_factor(type)) |&gt; \n  filter(time != \"Contr.\")\nbarplot_tbl \n\n# A tibble: 40 × 3\n   time  type  iodine\n   &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1 07:00 KIO3      50\n 2 07:00 KIO3      55\n 3 07:00 KIO3      60\n 4 07:00 KIO3      52\n 5 07:00 KIO3      62\n 6 07:00 KI        97\n 7 07:00 KI        90\n 8 07:00 KI        83\n 9 07:00 KI        81\n10 07:00 KI        98\n# ℹ 30 more rows\n\n\nIm Folgenden sparen wir uns den Aufruf mit group_by() den du aus dem Kapitel zum Barplot schon kennst. Wir machen das alles zusammen in der Funktion emmeans() aus dem gleichnamigen R Paket. Der Vorteil ist, dass wir dann auch gleich die Gruppenvergleiche und auch das compact letter display erhalten. Einzig die Standardabweichung \\(s\\) wird uns nicht wiedergegeben sondern der Standardfehler \\(SE\\). Da aber folgernder Zusammenhang vorliegt, können wir gleich den Standardfehler in die Standardabweichung umrechnen.\n\\[\nSE = \\cfrac{s}{\\sqrt{n}}\n\\]\nWir rechnen also gleich einfach den Standardfehler \\(SE\\) mal der \\(\\sqrt{n}\\) um dann die Standardabweichung zu erhalten. In unserem Fall ist \\(n=5\\) nämlich die Anzahl Beobachtungen je Gruppe. Wenn du mal etwas unterschiedliche Anzahlen hast, dann kannst du auch einfach den Mittelwert der Fallzahl der Gruppen nehmen. Da überfahren wir zwar einen statistischen Engel, aber der Genauigkeit ist genüge getan.\nIn den beiden Tabs siehst du jetzt einmal die Modellierung unter der Annahme der Varianzhomogenität mit der Funktion lm() und einmal die Modellierung unter der Annahme der Varianzheterogenität mit der Funktion gls() aus dem R Paket nlme. Wie immer lässt sich an Boxplots visuell überprüfen, ob wir Homogenität oder Heterogenität vorliegen haben. Oder aber du schaust nochmal in das Kapitel Der Pre-Test oder Vortest, wo du mehr erfährst.\nWenn du jeden Boxplot miteinander vergleichen willst, dann musst du in dem Code emmeans(~ time * type) setzen. Dann berechnet dir emmeans für jede Faktorkombination einen paarweisen Vergleich.\n\nVarianzhomogenitätVarianzheterogenität\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim(). Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen. Wenn du alles mit allem Vergleichen willst, dann setze bitte emmeans(~ time * type).\n\nemmeans_homogen_tbl &lt;- lm(iodine ~ time + type + time:type, data = barplot_tbl) |&gt;\n  emmeans(~ time | type) |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(5)) \nemmeans_homogen_tbl\n\n# A tibble: 8 × 9\n  time  type  emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 07:00 KIO3    55.8  5.58    32     44.4     67.2 a       12.5\n2 11:00 KIO3    75.2  5.58    32     63.8     86.6 b       12.5\n3 15:00 KIO3    81    5.58    32     69.6     92.4 b       12.5\n4 19:00 KIO3    84.2  5.58    32     72.8     95.6 b       12.5\n5 07:00 KI      89.8  5.58    32     78.4    101.  a       12.5\n6 19:00 KI      90    5.58    32     78.6    101.  a       12.5\n7 15:00 KI     124    5.58    32    113.     135.  b       12.5\n8 11:00 KI     152    5.58    32    141.     163.  c       12.5\n\n\nIn dem Objekt emmeans_homogen_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. Wie dir vielleicht auffällt sind alle Standardfehler und damit alle Standardabweichungen für alle Gruppen gleich, das war ja auch unsere Annahme mit der Varianzhomogenität.\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim(). Da wir hier etwas Probleme mit der Fallzahl haben, nutzen wir die Option mode = \"appx-satterthwaite\" um dennoch ein vollwertiges, angepasstes Modell zu erhalten. Du kannst die Option auch erstmal entfernen und schauen, ob es mit deinen Daten auch so klappt. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen. Wenn du alles mit allem Vergleichen willst, dann setze bitte emmeans(~ time * type).\n\nemmeans_hetrogen_tbl &lt;- gls(iodine ~ time + type + time:type, data = barplot_tbl, \n                            weights = varIdent(form =  ~ 1 | time*type)) |&gt;\n  emmeans(~ time | type, mode = \"appx-satterthwaite\") |&gt;\n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group),\n         sd = SE * sqrt(5)) \nemmeans_hetrogen_tbl\n\n# A tibble: 8 × 9\n  time  type  emmean    SE    df lower.CL upper.CL .group    sd\n  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 07:00 KIO3    55.8  2.29  4.09     49.5     62.1 a       5.12\n2 11:00 KIO3    75.2  1.50  4.05     71.1     79.3 b       3.35\n3 15:00 KIO3    81    3.30  4.00     71.8     90.2 b       7.38\n4 19:00 KIO3    84.2  7.88  4.01     62.3    106.  b      17.6 \n5 07:00 KI      89.8  3.48  4.00     80.1     99.5 a       7.79\n6 19:00 KI      90    4.31  4.02     78.1    102.  a       9.64\n7 15:00 KI     124    7.48  3.98    103.     145.  b      16.7 \n8 11:00 KI     152    9.03  3.99    127.     177.  c      20.2 \n\n\nIn dem Objekt emmeans_hetrogen_tbl ist jetzt alles enthalten für unsere Barplots mit dem compact letter display. In diesem Fall hier sind die Standardfehler und damit auch die Standardabweichungen nicht alle gleich, wir haben ja für jede Gruppe eine eigene Standardabweichung angenommen. Die Varianzen sollten ja auch heterogen sein.\n\n\n\nDann bauen wir usn auch schon die Abbildung. Wir müssen am Anfang einmal scale_x_discrete() setzen, damit wir gleich den Zielbereich ganz hinten zeichnen können. Sonst ist der blaue Bereich im Vordergrund. Dann färben wir auch mal die Balken anders ein. Muss ja auch mal sein. Auch nutzen wir die Funktion geom_text() um das compact letter display gut zu setzten. Die \\(y\\)-Position berechnet sich aus dem Mittelwert emmean plus Standardabweichung sd innerhalb des geom_text(). Da wir hier die Kontrollgruppe entfernen mussten, habe ich dann nochmal den Zielbereich verschoben und mit einem Pfeil ergänzt. Die beiden Tabs zeigen dir dann die Abbildungen für die beiden Annahmen der Varianzhomogenität oder Varianzheterogenität. Der Code ist der gleiche für beide Abbildungen, die Daten emmeans_homogen_tbl oder emmeans_hetrogen_tbl sind das Ausschlaggebende. Wie du sehen wirst, haben wir hier mal keinen Unterschied vorliegen.\n\nVarianzhomogenitätVarianzheterogenität\n\n\n\nggplot(data = emmeans_homogen_tbl, aes(x = time, y = emmean, fill = type)) +\n  theme_minimal() +\n  scale_x_discrete() +\n  annotate(\"rect\", xmin = 0.25, xmax = 4.75, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = \"darkblue\") +                        \n  annotate(\"text\", x = 0.5, y = 120, hjust = \"left\", label = \"Target area\", \n           size = 5) + \n  geom_curve(aes(x = 1.25, y = 120, xend = 1.7, yend = 105),   \n             colour = \"#555555\",   \n             size = 0.5,   \n             curvature = -0.2,  \n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_manual(name = \"Type\", values = c(\"darkgreen\", \"darkblue\")) + \n  theme(legend.position = c(0.1, 0.8),\n        legend.title = element_blank(), \n        legend.spacing.y = unit(0, \"mm\"), \n        panel.border = element_rect(colour = \"black\", fill=NA),\n        axis.text = element_text(colour = 1, size = 12),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) +\n  labs(x = \"Time of application [time of day]\",\n       y =  expression(Iodine~content~\"[\"*mu*g~I~100*g^'-1'~f*.*m*.*\"]\")) +\n  scale_y_continuous(breaks = c(0, 50, 100, 150, 200),\n                     limits = c(0, 200)) +\n  geom_text(aes(label = .group, y = emmean + sd + 2),  \n            position = position_dodge(width = 0.9), vjust = -0.25) \n\n\n\n\n\n\n\nAbbildung 22.7— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\n\n\n\n\n\n\n\n\nggplot(data = emmeans_hetrogen_tbl, aes(x = time, y = emmean, fill = type)) +\n  theme_minimal() +\n  scale_x_discrete() +\n  annotate(\"rect\", xmin = 0.25, xmax = 4.75, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = \"darkblue\") +                        \n  annotate(\"text\", x = 0.5, y = 120, hjust = \"left\", label = \"Target area\", \n           size = 5) + \n  geom_curve(aes(x = 1.25, y = 120, xend = 1.7, yend = 105),   \n             colour = \"#555555\",   \n             size = 0.5,   \n             curvature = -0.2,  \n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_errorbar(aes(ymin = emmean-sd, ymax = emmean+sd),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_manual(name = \"Type\", values = c(\"darkgreen\", \"darkblue\")) + \n  theme(legend.position = c(0.1, 0.8),\n        legend.title = element_blank(), \n        legend.spacing.y = unit(0, \"mm\"), \n        panel.border = element_rect(colour = \"black\", fill=NA),\n        axis.text = element_text(colour = 1, size = 12),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) +\n  labs(x = \"Time of application [time of day]\",\n       y =  expression(Iodine~content~\"[\"*mu*g~I~100*g^'-1'~f*.*m*.*\"]\")) +\n  scale_y_continuous(breaks = c(0, 50, 100, 150, 200),\n                     limits = c(0, 200)) +\n  geom_text(aes(label = .group, y = emmean + sd + 2),  \n            position = position_dodge(width = 0.9), vjust = -0.25) \n\n\n\n\n\n\n\nAbbildung 22.8— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\n\n\n\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_barplot.png\", width = 5, height = 3)\n\n\n\n\n\n\n\n\n\n\nZerforschen: Zweifaktorieller Boxplot mit rq()\n\n\n\n\n\nIn diesem Zerforschenbeispiel wollen wir uns einen zweifaktoriellen Boxplot unter der Annahme nicht normalverteilter Daten anschauen. Dabei ist Iodanteil in den Pflanzen nicht normalverteilt, so dass wir hier eine Quantilesregression rechnen wollen. Die Daten siehst du wieder in der unteren Abbildung. Ich nehme für jede der vier Zeitpunkte jeweils fünf Beobachtungen an. Für die Kontrolle haben wir dann nur drei Beobachtungen in der Gruppe \\(KIO_3\\) vorliegen. Das ist wichtig, denn sonst können wir nicht mit emmeans rechnen. Wir haben einfach zu wenige Beobachtungen vorliegen.\n\n\n\n\n\n\nAbbildung 22.9— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit den zwei Faktoren Zeit und die Iodine Form. Hier soll es dann ein Boxplot werden.\n\n\n\nAls erstes brauchen wir die Daten. Die Daten habe ich mir in dem Datensatz zerforschen_barplot_2fac_target_emmeans.xlsx selber ausgedacht. Ich nehme hier die gleichen Daten wie für den Barplot. Ich habe einfach die obige Abbildung genommen und den Mittelwert abgeschätzt. Dann habe ich die fünf Werte alle um den Mittelwert streuen lassen. Ich brauche hier eigentlich mehr als fünf Werte, sonst kriegen wir bei emmeans() und der Interaktion im gls()-Modell Probleme, aber da gibt es dann bei kleinen Fallzahlen noch ein Workaround. Bitte nicht mit weniger als fünf Beobachtungen versuchen, dann wird es schwierig mit der Konsistenz der Schätzer aus dem Modell.\nAch, und ganz wichtig. Wir entfernen die Kontrolle, da wir die Kontrolle nur mit einer Iodid-Stufe gemessen haben. Dann können wir weder die Interaktion rechnen, noch anständig eine Interpretation durchführen.\n\nboxplot_tbl &lt;- read_excel(\"data/zerforschen_barplot_2fac_target_emmeans.xlsx\") |&gt; \n  mutate(time = factor(time, \n                       levels = c(\"ctrl\", \"7\", \"11\", \"15\", \"19\"),\n                       labels = c(\"Contr.\", \"07:00\", \"11:00\", \"15:00\", \"19:00\")),\n         type = as_factor(type)) |&gt; \n  filter(time != \"Contr.\")\nboxplot_tbl  \n\n# A tibble: 40 × 3\n   time  type  iodine\n   &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1 07:00 KIO3      50\n 2 07:00 KIO3      55\n 3 07:00 KIO3      60\n 4 07:00 KIO3      52\n 5 07:00 KIO3      62\n 6 07:00 KI        97\n 7 07:00 KI        90\n 8 07:00 KI        83\n 9 07:00 KI        81\n10 07:00 KI        98\n# ℹ 30 more rows\n\n\nHier gehen wir nicht weiter auf die Funktionen ein, bitte schaue dann einmal in dem Abschnitt zu Gruppenvergleich mit dem emmeans Paket. Wir entfernen aber noch die Leerzeichen bei den Buchstaben mit der Funktion str_trim(). Wichtig ist hier, dass wir zur Modellierung die Funktion rq() aus dem R Paket quantreg nutzen. Wenn du den Aufbau mit den anderen Zerforschenbeispielen vergleichst, siehst du, dass hier viel ähnlich ist. Achtung, ganz wichtig! Du musst am Ende wieder die Ausgabe aus der cld()-Funktion nach der Zeit time und der Form type sortieren, sonst passt es gleich nicht mit den Beschriftungen der Boxplots.\nDu musst dich nur noch entscheiden, ob du das compact letter display getrennt für die beiden Jodformen type berechnen willst oder aber zusammen. Wenn du das compact letter display für die beiden Jodformen zusammen berechnest, dann berechnest du für jede Faktorkombination einen paarweisen Vergleich.\n\nGetrennt nach typeZusammen über type\n\n\nHier rechnen wir den Vergleich nur innerhalb der jeweiligen Jodform type. Daher vergleichen wir die Boxplots nicht untereinander sondern eben nur in den jeweiligen Leveln \\(KIO_3\\) und \\(KI\\).\n\nemmeans_quant_sep_tbl &lt;- rq(iodine ~ time + type + time:type, data = boxplot_tbl, tau = 0.5) |&gt;\n  emmeans(~ time | type) |&gt; \n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group)) |&gt; \n  arrange(time, type)\nemmeans_quant_sep_tbl\n\n# A tibble: 8 × 8\n  time  type  emmean    SE    df lower.CL upper.CL .group\n  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; \n1 07:00 KIO3      55  3.15    32     48.6     61.4 a     \n2 07:00 KI        90  5.51    32     78.8    101.  a     \n3 11:00 KIO3      75  1.97    32     71.0     79.0 b     \n4 11:00 KI       150 13.8     32    122.     178.  b     \n5 15:00 KIO3      80  4.72    32     70.4     89.6 b     \n6 15:00 KI       120  7.87    32    104.     136.  b     \n7 19:00 KIO3      85  7.48    32     69.8    100.  b     \n8 19:00 KI        90  2.36    32     85.2     94.8 a     \n\n\n\n\nDann einmal die Berechnung über alle Boxplots und damit allen Faktorkombinationen aus Zeit und Jodform. Wir können dann damit jeden Boxplot mit jedem anderen Boxplot vergleichen.\n\nemmeans_quant_comb_tbl &lt;- rq(iodine ~ time + type + time:type, data = boxplot_tbl, tau = 0.5) |&gt;\n  emmeans(~ time*type) |&gt; \n  cld(Letters = letters, adjust = \"none\") |&gt; \n  as_tibble() |&gt; \n  mutate(.group = str_trim(.group)) |&gt; \n  arrange(time, type)\nemmeans_quant_comb_tbl\n\n# A tibble: 8 × 8\n  time  type  emmean    SE    df lower.CL upper.CL .group\n  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; \n1 07:00 KIO3      55  3.15    32     48.6     61.4 a     \n2 07:00 KI        90  5.51    32     78.8    101.  c     \n3 11:00 KIO3      75  1.97    32     71.0     79.0 b     \n4 11:00 KI       150 13.8     32    122.     178.  d     \n5 15:00 KIO3      80  4.72    32     70.4     89.6 bc    \n6 15:00 KI       120  7.87    32    104.     136.  d     \n7 19:00 KIO3      85  7.48    32     69.8    100.  bc    \n8 19:00 KI        90  2.36    32     85.2     94.8 c     \n\n\n\n\n\nFür die Boxplots brauchen wir dann noch ein Objekt mehr. Um das nach den Faktorkombinationen sortierte compacte letter dislay an die richtige Position zu setzen brauchen wir noch eine \\(y\\)-Position. Ich nehme hier dann das 95% Quantile. Das 95% Quantile sollte dann auf jeden Fall über die Schnurrhaare raus reichen. Wir nutzen dann den Datensatz letter_pos_tbl in dem geom_text() um die Buchstaben richtig zu setzen.\n\nletter_pos_tbl &lt;- boxplot_tbl |&gt; \n  group_by(time, type) |&gt; \n  summarise(quant_90 = quantile(iodine, probs = c(0.95)))\nletter_pos_tbl\n\n# A tibble: 8 × 3\n# Groups:   time [4]\n  time  type  quant_90\n  &lt;fct&gt; &lt;fct&gt;    &lt;dbl&gt;\n1 07:00 KIO3      61.6\n2 07:00 KI        97.8\n3 11:00 KIO3      78.8\n4 11:00 KI       174  \n5 15:00 KIO3      89.4\n6 15:00 KI       146  \n7 19:00 KIO3     106  \n8 19:00 KI        99.8\n\n\nDas Problem sind natürlich die wenigen Beobachtungen, deshalb sehen die Boxplots teilweise etwas wild aus. Aber wir wollen hier eben die Mediane darstellen, die wir dann auch in der Quantilesregression berechnet haben. Wenn du mehr Beobachtungen erstellst, dann werden die Boxplots auch besser dargestellt.\n\nGetrennt nach typeZusammen über type\n\n\nHier nehmen wir das compact letter display aus dem Objekt emmeans_quant_sep_tbl. Wichtig ist, dass die Sortierung gleich der Beschriftung der \\(x\\)-Achse ist. Deshalb nutzen wir weiter oben auch die Funktion arrange() zur Sortierung der Buchstaben. Beachte, dass wir hier jeweils die beiden Jodformen getrennt voneinander betrachten.\n\nggplot(data = boxplot_tbl, aes(x = time, y = iodine,\n                               fill = type)) +\n  theme_minimal() +\n  scale_x_discrete() +\n  annotate(\"rect\", xmin = 0.25, xmax = 4.75, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = \"darkblue\") +                        \n  annotate(\"text\", x = 0.5, y = 120, hjust = \"left\", label = \"Target area\") + \n  geom_curve(aes(x = 1.1, y = 120, xend = 1.7, yend = 105),   \n             colour = \"#555555\",   \n             size = 0.5,   \n             curvature = -0.2,  \n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_boxplot(position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_manual(name = \"Type\", values = c(\"darkgreen\", \"darkblue\")) + \n  theme(legend.position = c(0.1, 0.8),\n        legend.title = element_blank(), \n        legend.spacing.y = unit(0, \"mm\"), \n        panel.border = element_rect(colour = \"black\", fill=NA),\n        axis.text = element_text(colour = 1, size = 12),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) +\n  labs(x = \"Time of application [time of day]\",\n       y =  expression(Iodine~content~\"[\"*mu*g~I~100*g^'-1'~f*.*m*.*\"]\")) +\n  scale_y_continuous(breaks = c(0, 50, 100, 150, 200),\n                     limits = c(0, 200)) +\n  geom_text(data = letter_pos_tbl, \n            aes(label = emmeans_quant_sep_tbl$.group, \n                y = quant_90 + 5),  \n            position = position_dodge(width = 0.9), vjust = -0.25) \n\n\n\n\n\n\n\nAbbildung 22.10— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen, dafür müssen wir uns aber nochmal ein Positionsdatensatz bauen. Hier ist das compact letter display getrennt für die beiden Jodformen berechnet.\n\n\n\n\n\n\n\nUnd hier nutzen wir das compact letter display aus dem Objekt emmeans_quant_comb_tbl. Wichtig ist, dass die Sortierung gleich der Beschriftung der \\(x\\)-Achse ist. Deshalb nutzen wir weiter oben auch die Funktion arrange() zur Sortierung der Buchstaben. Hier betrachten wir jeden Boxplot einzelne und du kannst auch jeden Boxplot mit jedem Boxplot vergleichen.\n\nggplot(data = boxplot_tbl, aes(x = time, y = iodine,\n                               fill = type)) +\n  theme_minimal() +\n  scale_x_discrete() +\n  annotate(\"rect\", xmin = 0.25, xmax = 4.75, ymin = 50, ymax = 100, \n           alpha = 0.2, fill = \"darkblue\") +                        \n  annotate(\"text\", x = 0.5, y = 120, hjust = \"left\", label = \"Target area\") + \n  geom_curve(aes(x = 1.1, y = 120, xend = 1.7, yend = 105),   \n             colour = \"#555555\",   \n             size = 0.5,   \n             curvature = -0.2,  \n             arrow = arrow(length = unit(0.03, \"npc\"))) +\n  geom_boxplot(position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_manual(name = \"Type\", values = c(\"darkgreen\", \"darkblue\")) + \n  theme(legend.position = c(0.1, 0.8),\n        legend.title = element_blank(), \n        legend.spacing.y = unit(0, \"mm\"), \n        panel.border = element_rect(colour = \"black\", fill=NA),\n        axis.text = element_text(colour = 1, size = 12),\n        legend.background = element_blank(),\n        legend.box.background = element_rect(colour = \"black\")) +\n  labs(x = \"Time of application [time of day]\",\n       y =  expression(Iodine~content~\"[\"*mu*g~I~100*g^'-1'~f*.*m*.*\"]\")) +\n  scale_y_continuous(breaks = c(0, 50, 100, 150, 200),\n                     limits = c(0, 200)) +\n  geom_text(data = letter_pos_tbl, \n            aes(label = emmeans_quant_comb_tbl$.group, \n                y = quant_90 + 5),  \n            position = position_dodge(width = 0.9), vjust = -0.25) \n\n\n\n\n\n\n\nAbbildung 22.11— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen, dafür müssen wir uns aber nochmal ein Positionsdatensatz bauen. Hier ist das compact letter display für jede einzelne Faktorkombination berechnet.\n\n\n\n\n\n\n\n\nAm Ende kannst du dann folgenden Code noch hinter deinen ggplot Code ausführen um dann deine Abbildung als *.png-Datei zu speichern. Dann hast du die Abbildung super nachgebaut und sie sieht auch wirklich besser aus.\n\nggsave(\"my_ggplot_boxplot.png\", width = 5, height = 3)\n\n\n\n\nBei einem nicht-normalverteilten Outcome mit Varianzheterogenität über die Gruppen müssen wir nochmal gemeinsam in die Daten schauen. Da meldest du dich bitte nochmal bei mir in der statistischen Beratung. Hier öffnet sich nämlich eine Tür in eine ganz eigene Modellwelt und je nach wissenschaftlicher Fragestellung können wir dann eine Lösung finden.\n\n\n\n\n\n\nExkurs: Robuste Schätzung von Standardfehlern, Konfidenzintervallen und p-Werten\n\n\n\n\n\nWenn du noch etwas weiter gehen möchtest, dann kannst du dir noch die Hilfeseite von dem R Paket {performance} Robust Estimation of Standard Errors, Confidence Intervals, and p-values anschauen. Die Idee ist hier, dass wir die Varianz/Kovarianz robuster daher mit der Berücksichtigung von Varianzheterogenität (eng. heteroskedasticity) schätzen.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Der Pre-Test oder Vortest</span>"
    ]
  },
  {
    "objectID": "stat-tests-pretest.html#pre-test-auf-varianzhomogenität",
    "href": "stat-tests-pretest.html#pre-test-auf-varianzhomogenität",
    "title": "22  Der Pre-Test oder Vortest",
    "section": "22.3 Pre-Test auf Varianzhomogenität",
    "text": "22.3 Pre-Test auf Varianzhomogenität\nWas will also der Pre-Test auf Varianzhomogenität? Eigentlich ist der Test vollkommen verquer. Zum einen testet der Test auf Varianzhomogenität gar nicht die Anwesenheit von Homogenität. Wir können dank dem Falisifikationsprinzip nur Ablehnen. Deshalb steht in der Nullhypothese die Gleichheit der Varianzen, also Varianzhomogenität und in der Alternativen dann die Varianzheterogenität, als der Unterschied.\nAb wann sollten wir denn die Varianzhomogenität ablehnen? Wenn wir standardmäßig auf 5% testen, dann werden wir zu selten die Varianzhomogenität ablehnen. Daher ist es ratsam in diesem Fall auf ein Signifikanzniveau von \\(\\alpha\\) gleich 20% zu testen. Aber auch in diesem Fall können wir natürlich eine Varianzhomogenität übersehen oder aber eine Varianzheterogenität fälschlicherweise annehmen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEinigermaßen zuverlässig meint, dass wir dann in 1 von 20 Fällen eine Varianzhomogenität ablehnen, obwohl eine Varianzhomogenität vorliegt. Ebenso können wir in 1 von 5 Fällen die Nullhypothese nicht ablehnen, obwohl die Varianzen heterogen sind (siehe auch Kapitel 20.1).\n\n\n\n\nEs ergeben sich folgende Hypothesen für den Pre-Test auf Varianzhomogenität.\n\\[\n\\begin{aligned}\nH_0: &\\; s^2_A = s^2_B\\\\\nH_A: &\\; s^2_A \\ne s^2_B\\\\\n\\end{aligned}\n\\]\nWir sehen, dass in der Nullhypothese die Gleichheit der Varianzen steht und in der Alternativehypothese der Unterschied, also die Varianzhterogenität.\n\n\n\n\n\n\nEntscheidung zur Varianzhomogenität\n\n\n\nBei der Entscheidung zur Varianzhomogenität gilt folgende Regel. Ist der \\(p\\)-Wert des Pre-Tests auf Varianzhomogenität kleiner als das Signifikanzniveau \\(\\alpha\\) von 20% lehnen wir die Nullhypothese ab. Wir nehmen Varianzheterogenität an.\n\nIst \\(p \\leq \\alpha = 20\\%\\) so nehmen wir Varianzheterogenität an.\nIst \\(p &gt; \\alpha = 20\\%\\) so nehmen wir Varianzhomogenität an.\n\nAuf jeden Fall sollten wir das Ergebnis unseres Pre-Tests auf Varianzhomogenität nochmal visuell bestätigen.\n\n\nWir nutzen zum statistischen Testen den Levene-Test über die Funktion leveneTest() oder den Bartlett-Test über die Funktion bartlett.test(). Beide Tests sind in R implementiert und können über das Paket {car} genutzte werden. Wir werden uns jetzt nicht die Formel anschauen, wir nutzen wenn die beiden Tests nur in R und rechnen nicht selber die Werte nach.\nEinfach ausgedrückt, überprüft der Bartlett-Test die Homogenität der Varianzen auf der Grundlage des Mittelwerts. Dementsprechend ist der Bartlett-Test empfindlicher gegen eine Abweichung von der Normalverteilung der Daten, die er überprüfen soll. Der Levene-Test überprüft die Homogenität der Varianzen auch auf der Grundlage des Mittelwerts. Wir haben aber auch die Wahl, den Median zu nutzen dann ist der Levene-Test robuster gegenüber Ausreißern.\nIm Folgenden wollen wir uns einmal in der Theorie den Levene-Test anschauen. Der Levene-Test ist eigentlich nichts anderes als eine etwas versteckte einfaktorielle ANOVA, aber dazu dann am Ende mehr. Dafür nutzen wir als erstes die folgende Formel um die Teststatistik zu berechnen. Dabei ist \\(W\\) die Teststatistik, die wir zu einer \\(F\\)-Verteilung, die wir schon aus der ANOVA kennen, vergleichen können.\n\\[\nW = \\frac{(N-k)}{(k-1)} \\cdot \\frac{\\sum_{i=1}^k N_i (\\bar{Z}_{i\\cdot}-\\bar{Z}_{\\cdot\\cdot})^2} {\\sum_{i=1}^k \\sum_{j=1}^{N_i} (Z_{ij}-\\bar{Z}_{i\\cdot})^2}\n\\]\nZur Veranschaulichung bauen wir uns einen simplen Datensatz mit \\(N = 14\\) Beobachtungen für \\(k = 2\\) Tierarten mit Hunden und Katzen. Damit hat jede Tierart \\(7\\) Beobachtungen der Sprunglängen der jeweiligen Hunde- und Katzenflöhe.\n\nanimal_tbl &lt;- tibble(dog = c(5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6),\n                     cat = c(3.2, 2.2, 5.4, 4.1, 1.1, 7.9, 8.6))\nanimal_tbl\n\n# A tibble: 7 × 2\n    dog   cat\n  &lt;dbl&gt; &lt;dbl&gt;\n1   5.7   3.2\n2   8.9   2.2\n3  11.8   5.4\n4   8.2   4.1\n5   5.6   1.1\n6   9.1   7.9\n7   7.6   8.6\n\n\nDann berechnen wir uns auch gleich die absoluten Abstände \\(Z_{ij}\\) von jeder Beobachtung zu den jeweiligen Mittelwerten der Gruppe. Wir könnten auch die Abstände zu den jeweiligen Medianen der Gruppe berechnen. Beides ist möglich. Hier sehen wir auch den Unterschied zu der ANOVA, wir berechnen hier nicht die quadratischen Abstände sondern die absoluten Abstände.\n\nz_tbl &lt;- animal_tbl |&gt; \n  mutate(dog_abs = abs(dog - mean(dog)),\n         cat_abs = abs(cat - mean(cat)))\nz_tbl\n\n# A tibble: 7 × 4\n    dog   cat dog_abs cat_abs\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1   5.7   3.2  2.43     1.44 \n2   8.9   2.2  0.771    2.44 \n3  11.8   5.4  3.67     0.757\n4   8.2   4.1  0.0714   0.543\n5   5.6   1.1  2.53     3.54 \n6   9.1   7.9  0.971    3.26 \n7   7.6   8.6  0.529    3.96 \n\n\nIm Folgenden nochmal in formelschreibweise der Unterschied zwischen den beiden Abstandsberechnungen \\(Z_{ij}\\) für jeden Wert. Wir haben die Wahl zwischen den Abständen von jeder Beobachtung zu dem Mittelwert oder dem Median.\n\\[\nZ_{ij} =\n\\begin{cases}\n|Y_{ij} - \\bar{Y}_{i\\cdot}|, & \\bar{Y}_{i\\cdot} \\text{ ist der Mittelwert der } i\\text{-ten Gruppe}, \\\\\n|Y_{ij} - \\tilde{Y}_{i\\cdot}|, & \\tilde{Y}_{i\\cdot} \\text{ ist der Median der } i\\text{-ten Gruppe}.\n\\end{cases}\n\\]\nBerechnen wir nun die Gruppenmittelwerte \\(\\bar{Z}_{i\\cdot}\\) für die Hunde und die Katzen jeweils einmal separat.\n\nmean(z_tbl$dog_abs)\n\n[1] 1.567347\n\n\n\nmean(z_tbl$cat_abs)\n\n[1] 2.277551\n\n\nDann brauchen wir noch den Mittelwert über alle Beobachtungen hinweg \\(\\bar{Z}_{\\cdot\\cdot}\\). Den können wir aus allen Beoachtungen berechnen oder aber einfach den Mittelwert der beiden Gruppenmittelwerte nehmen.\n\n(1.57 + 2.28)/2\n\n[1] 1.925\n\n\nAm Ende fehlt uns dann noch der Nenner mit der Summe der einzelnen quadratischen Abstände \\(Z_{ij}\\) zu den Abstandsmitteln der einzelnen Gruppen \\(\\bar{Z}_{i\\cdot}\\) mit \\(\\sum_{i=1}^k \\sum_{j=1}^{N_i} (Z_{ij}-\\bar{Z}_{i\\cdot})^2\\). Den Wert können wir dann in R direkt einmal berechnen. Wir nehmen also die Vektoren der Einzelwerte und ziehen dort immer den Mittelwert der Abstände der Gruppe ab. Abschließend summieren wir dann einmal auf.\n\nsum((z_tbl$dog_abs - 1.57)^2)\n\n[1] 10.3983\n\nsum((z_tbl$cat_abs - 2.28)^2)\n\n[1] 11.42651\n\n\nWir können dann alle Zahlen einmal zusammenbringen und in die Formel des Levene-Test einsetzen. Nun rechen wir dann wieder die quadratischen Abstände auf den absoluten Abständen. Ja, das ist etwas wirr, wenn man es zum ersten Mal liest.\n\\[\nW =\n\\cfrac{14-2}{2-1}\\cdot\n\\cfrac{7 \\cdot (1.57 - 1.93)^2 + 7 \\cdot (2.28 - 1.93)^2}\n{10.39 + 11.43} =\n\\cfrac{12}{1} \\cdot \\cfrac{1.76}{21.82} =\n\\cfrac{21.12}{21.82} \\approx 0.968\n\\]\nWir erhalten ein \\(W = 0.968\\), was wir direkt als eine F-Statistik interpretieren können. Schauen wir uns das Ergebnis einmal in der R Funktion leveneTest() aus dem R Paket {car} an. Wir brauchen dafür einmal die Werte für die Sprungweiten und müssen dann die Daten in das long-Format umbauen und dann rechnen wir den Levene-Test. Wir erhalten fast die numerisch gleichen Werte. Bei uns haben wir etwas gerundet und dann kommt die Abweichung zustande.\n\nz_tbl |&gt; \n  select(dog, cat) |&gt; \n  gather(key = animal, value = jump_length) %$% \n  leveneTest(jump_length ~ animal, center = \"mean\")\n\nLevene's Test for Homogeneity of Variance (center = \"mean\")\n      Df F value Pr(&gt;F)\ngroup  1  0.9707  0.344\n      12               \n\n\nDer Levene-Test ist eigentlich nichts anderes als eine einfaktorielle ANOVA auf den absoluten Abständen von den einzelnen Werten zu dem Mittelwert oder dem Median. Das können wir hier einmal nachvollziehen indem wir auf den absoluten Werten einmal eine einfaktorielle ANOVA in R rechnen. Wir erhalten den gleichen F-Wert in beiden Fällen. Eigentlich ist die ANOVA sogar etwas genauer, da wir hier auch die Sum of squares wie auch Mean squares erhalten.\n\nz_tbl |&gt; \n  select(dog_abs, cat_abs) |&gt; \n  gather(key = animal, value = jump_length) %$% \n  lm(jump_length ~ animal) |&gt; \n  anova()\n\nAnalysis of Variance Table\n\nResponse: jump_length\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\nanimal     1  1.7654  1.7654  0.9707  0.344\nResiduals 12 21.8247  1.8187               \n\n\nWir wollen uns nun im Folgenden nun zwei Fälle einmal näher anschauen. Zum einen den Fall, dass wir eine niedrige Fallzahl vorliegen haben und Varianzhomogenität sowie den Fall, dass wir eine niedrige Fallzahl und Varianzheterogenität vorliegen haben. Den Fall, dass wir hohe Fallzahl vorliegen haben, betrachten wir jetzt nicht weiter. In dem Fall funktionieren die Tests einigermaßen zuverlässig.\n\n22.3.1 Varianzen sind homogen, Fallzahl niedrig\nWir bauen uns nun einen Datensatz mit zwei Gruppen \\(A\\) und \\(B\\) zu je zehn Beobachtungen. Beide Gruppen kommen aus einer Normalverteilung mit einem Mittelwert von \\(\\bar{y}_A = \\bar{y}_A = 10\\). Darüber hinaus haben wir Varianzhomogenität mit \\(s_A = s_B = 5\\) vorliegen. Ja, wir spezifizieren hier in der Funktion rnorm() die Standardabweichung, aber eine homogene Standardabweichung bedingt eine homogene Varianz und umgekehrt. Abschließend verwandeln wir das Wide-Format noch in das Long-Format um.\n\nset.seed(202209013)\nsmall_homogen_tbl &lt;- tibble(A = rnorm(n = 10, mean = 10, sd = 5),\n                            B = rnorm(n = 10, mean = 10, sd = 5)) |&gt; \n  gather(trt, rsp) |&gt; \n  mutate(trt = as_factor(trt))\n\nIn der Abbildung 22.12 sehen wir die Daten aus dem small_homogen_tbl einmal als Boxplot visualisiert.\n\n\n\n\n\n\n\n\nAbbildung 22.12— Boxplot der beiden Treatment Level A und B. Beide Gruppen haben die gleichen Varianzen. Es liegt Varianzhomogenität vor.\n\n\n\n\n\nWir wollen nun die Varianz auf Homogenität testen. Wir nutzen dafür den leveneTest() sowie den bartlett.test(). Beide Tests bieten sich an. Die Daumenregel ist, dass der Bartlett-Test etwas bessere statistische Eigenschaften hat. Dennoch ist der Levene-Test bekannter und wird häufiger angefragt und genutzt. Wir nutzen die Funktion tidy() aus dem Paket {broom} um die Ausgabe aufzuräumen und selektieren nur den \\(p\\)-Wert.\n\nleveneTest(rsp ~ trt, data = small_homogen_tbl) |&gt; \n  tidy() |&gt; \n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.345\n\nbartlett.test(rsp ~ trt, data = small_homogen_tbl) |&gt; \n  tidy() |&gt; \n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.325\n\n\nWir sehen, dass der \\(p\\)-Wert größer ist als das Signifikanzniveau \\(\\alpha\\) von 20%. Damit können wir die Nullhypothese nicht ablehnen. Wir nehmen Varianzhomogenität an. Überdies sehen wir auch, dass sich die \\(p\\)-Werte nicht groß voneinander unterscheiden.\nWir können auch die Funktion check_homogeneity() aus dem Paket {performance} nutzen. Wir erhalten hier auch gleich eine Entscheidung in englischer Sprache ausgegeben. Die Funktion check_homogeneity() nutzt den Bartlett-Test. Wir können in Funktion auch andere Methoden mit method = c(\"bartlett\", \"fligner\", \"levene\", \"auto\") wählen.\n\nlm(rsp ~ trt, data = small_homogen_tbl) |&gt; \n  check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.325).\n\n\nWir nutzen das Paket {performance} für die Modellgüte im Kapitel 39.\n\n\n22.3.2 Varianzen sind heterogen, Fallzahl niedrig\nNun stellt sich die Frage, wie sieht es aus, wenn wir ungleiche Varianzen vorliegen haben. Wir bauen uns nun einen Datensatz mit zwei Gruppen \\(A\\) und \\(B\\) zu je zehn Beobachtungen. Beide Gruppen kommen aus einer Normalverteilung mit einem Mittelwert von \\(\\bar{y}_A = \\bar{y}_A = 12\\). Darüber hinaus haben wir Varianzheterogenität mit \\(s_A = 10 \\ne s_B = 5\\) vorliegen.\n\nset.seed(202209013)\nsmall_heterogen_tbl &lt;- tibble(A = rnorm(10, 10, 12),\n                              B = rnorm(10, 10, 5)) |&gt; \n  gather(trt, rsp) |&gt; \n  mutate(trt = as_factor(trt))\n\nIn der Abbildung 22.13 sehen wir die Daten aus dem small_heterogen_tbl einmal als Boxplot visualisiert.\n\n\n\n\n\n\n\n\nAbbildung 22.13— Boxplot der beiden Treatment Level A und B. Beide Gruppen haben ungleiche Varianzen. Es liegt Varianzheterogenität vor.\n\n\n\n\n\nWir wollen nun die Varianz auf Homogenität testen. Wir nutzen dafür den levenTest() sowie den bartlett.test(). Wir können nur die Varianzhomogenität testen, da jeder statistischer Test nur eine Aussage über die Nullhypothese erlaubt. Damit können wir hier nur die Varianzhomogenität testen.\n\nleveneTest(rsp ~ trt, data = small_heterogen_tbl) |&gt; \n  tidy() |&gt; \n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0661\n\nbartlett.test(rsp ~ trt, data = small_heterogen_tbl) |&gt; \n  tidy() |&gt; \n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.127\n\n\nWir sehen, dass der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 20%. Damit können wir die Nullhypothese ablehnen. Wir nehmen Varianzheterogenität an. Überdies sehen wir auch, dass sich die \\(p\\)-Werte nicht groß voneinander unterscheiden. Was wir sehen ist, dass wir zu einem Signifikanzniveau von 5% die klare Varianzheterogenität nicht erkannt hätten und immer noch Varianzhomogenität angenommen hätten.\nWir können auch die Funktion check_homogeneity() aus dem Paket {performance} nutzen. Wir erhalten hier auch gleich eine Entscheidung in englischer Sprache ausgegeben. Die Funktion check_homogeneity() nutzt den Bartlett-Test. Wir können in Funktion auch andere Methoden mit method = c(\"bartlett\", \"fligner\", \"levene\", \"auto\") wählen.\n\nlm(rsp ~ trt, data = small_heterogen_tbl) |&gt; \n  check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.127).\n\n\nWir sehen, dass sich die Implementierung des Bartlett-Tests in check_homogeneity() nicht von der Funktion bartlett.test() unterscheidet, aber die Entscheidung gegen die Varianzhomogenität zu einem Signifikanzniveau von 5% gefällt wird. Nicht immer hilft einem der Entscheidungtext einer Funktion.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Der Pre-Test oder Vortest</span>"
    ]
  },
  {
    "objectID": "stat-tests-pretest.html#pre-test-auf-normalverteilung",
    "href": "stat-tests-pretest.html#pre-test-auf-normalverteilung",
    "title": "22  Der Pre-Test oder Vortest",
    "section": "22.4 Pre-Test auf Normalverteilung",
    "text": "22.4 Pre-Test auf Normalverteilung\nWir treffen bei dem Test auf die Normalverteilung auch auf das gleiche Problem wie bei dem Pre-Test zur Varianzhomogenität. Wir haben wieder die Gleichheit, also das unser beobachtetes Outcome gleich einer Normalverteilung ist, in der Nullhypothese stehen. Den Unterschied, also das unser beobachtetes Outcome nicht aus einer Normalverteilung kommmt, in der Alternative.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{y ist gleich normalverteilt}\\\\\nH_A: &\\; \\mbox{y ist nicht gleich normalverteilt}\\\\\n\\end{aligned}\n\\]\nNun ist es aber so, dass es nicht nur zwei Verteilungen gibt. Es gibt mehr als die Normalverteilung und die Nicht-normalverteilung. Wir haben eine große Auswahl an möglichen Verteilungen und seit den 90zigern des letzten Jahrhunderts auch die Möglichkeiten andere Verteilungen des Outcomes \\(y\\) zu modellieren. Leider fällt dieser Fortschritt häufig unter den Tisch und wir bleiben gefangen zwischen der Normalverteilung oder eben keiner Normalverteilung.\n\n\nDer zentrale Grenzwertsatz besagt, dass wenn ein \\(y\\) von vielen Einflussfaktoren \\(x\\) bestimmt wird, man von einem normalverteilten \\(y\\) ausgehen.\nDas Gewicht wird von vielen Einflussfaktoren wie Sport, Kalorienaufnahme oder aber Veranlagung sowie vielem mehr bestimmt. Wir können davon ausgehen, dass das Gewicht normalverteilt ist.\nAbschließend sei noch gesagt, dass es fast unmöglich ist, eine Verteilung mit weniger als zwanzig Beobachtungen überhaupt abzuschätzen. Selbst dann können einzelne Beobachtunge an den Rändern der Verteilung zu einer Ablehnung der Normalverteilung führen, obwohl eine Normalverteilung vorliegt.\nDas R Paket {oslrr} bietet hier noch eine Funktion ols_test_normality(), die es erlaubt mit allen bekannten statistischen Tests auf Normalverteilung zu testen. Wenn der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\), dann können wir die Nullhypothese, dass unsere Daten gleich einer Normalverteilung wären, ablehnen. Die Anwendung kannst du dir in Kapitel 46 anschauen. Um jetzt kurz einen statistischen Engel anzufahren, wir nutzen wenn überhaupt den Shapiro-Wilk-Test oder den Kolmogorov-Smirnov-Test. Für die anderen beiden steigen wir jetzt hier nicht in die Therorie ab.\nAm Ende sei noch auf den QQ-plot verwiesen, mit dem wir auch visuell überprüfen können, ob eine Normalverteilung vorliegt.\n\n\n\n\n\n\nEntscheidung zur Normalverteilung\n\n\n\nBei der Entscheidung zur Normalverteilung gilt folgende Regel. Ist der \\(p\\)-Wert des Pre-Tests auf Normalverteilung kleiner als das Signifikanzniveau \\(\\alpha\\) von 5% lehnen wir die Nullhypothese ab. Wir nehmen eine Nicht-Normalverteilung an.\n\nIst \\(p \\leq \\alpha = 5\\%\\) so nehmen wir Nicht-Normalverteilung von \\(y\\) an.\nIst \\(p &gt; \\alpha = 5\\%\\) so nehmen wir Normalverteilung von \\(y\\) an.\n\nAuf jeden Fall sollten wir das Ergebnis unseres Pre-Tests auf Normalverteilung nochmal visuell bestätigen.\n\n\n\n22.4.1 Approximativ normalverteilt, niedrige Fallzahl\nAuch hier schauen wir uns den Fall mit einer niedrigen Fallzahl an. Dafür bauen wir usn erstmal Daten mit der Funktion rt(). Wir ziehen uns zufällig Beobachtungen aus einer t-Verteilung, die approximativ normalverteilt ist. Je höher die Freiheitsgrade df desto näher kommt die t-Verteilung einer Normalverteilung. Mit einem Freiheitsgrad von df = 30 sind wir sehr nah an einer Normalverteilung dran.\n\nset.seed(202209013)\nlow_normal_tbl &lt;- tibble(A = rt(10, df = 30),\n                         B = rt(10, df = 30)) |&gt; \n  gather(trt, rsp) |&gt; \n  mutate(trt = as_factor(trt))\n\nIn Abbildung 22.14 sehen wir auf der linken Seite den Dotplot der zehn Beobachtungen aus den beiden Gruppen \\(A\\) und \\(B\\). Wir sehen, dass die Verteilung für das Outcome rsp in etwa normalverteilt ist.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Dotplot des Outcomes rsp.\n\n\n\n\n\n\n\n\n\n\n\n(b) Densityplot des Outcomes rsp.\n\n\n\n\n\n\n\nAbbildung 22.14— Verteilung des Outcomes rsp der zehn Beobachtungen aus den Gruppen \\(A\\) und \\(B\\). Beiden Gruppen kommen aus einer t-Verteilung.\n\n\n\n\nWir können den Shapiro-Wilk-Test nutzen um statistisch zu testen, ob eine Abweichung von der Normalverteilung vorliegt. Wir erfahren aber nicht, welche andere Verteilung vorliegt. Wir testen natürlich für die beiden Gruppen getrennt. Die Funktion shapiro.test()kann nur mit einem Vektor von Zahlen arbeiten, daher übergeben wir mit pull die entsprechend gefilterten Werte des Outcomes rsp.\n\nlow_normal_tbl |&gt; \n  filter(trt == \"A\") |&gt; \n  pull(rsp) |&gt; \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  pull(filter(low_normal_tbl, trt == \"A\"), rsp)\nW = 0.9457, p-value = 0.618\n\nlow_normal_tbl |&gt; \n  filter(trt == \"B\") |&gt; \n  pull(rsp) |&gt; \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  pull(filter(low_normal_tbl, trt == \"B\"), rsp)\nW = 0.89291, p-value = 0.1828\n\n\nWir sehen, dass der \\(p\\)-Wert größer ist als das Signifikanzniveau \\(\\alpha\\) von 5% für beide Gruppen. Damit können wir die Nullhypothese nicht ablehnen. Wir nehmen eine Normalverteilung an.\nIn dem folgendem Beispiel sehen wir dann aber, was ich mit in die Ecke testen meine bzw. so lange statistisch zu Testen bis nichts mehr geht.\n\n\n22.4.2 Nicht normalverteilt, niedrige Fallzahl\nSchauen wir uns jetzt den anderen Fall an. Wir haben jetzt wieder eine niedrige Fallzahl mit je 10 Beobachtungen je Gruppe \\(A\\) und \\(B\\). In diesem Fall kommen die Beobachtungen aber aus einer exponentiellen Verteilung. Wir haben also definitiv keine Normalverteilung vorliegen. Wir generieren uns die Daten mit der Funktion rexp().\n\nset.seed(202209013)\nlow_nonnormal_tbl &lt;- tibble(A = rexp(10, 1/1500),\n                            B = rexp(10, 1/1500)) |&gt; \n  gather(trt, rsp) |&gt; \n  mutate(trt = as_factor(trt))\n\nIn Abbildung 22.15 sehen wir auf der linken Seite den Dotplot der zehn Beobachtungen aus den beiden Gruppen \\(A\\) und \\(B\\). Wir sehen, dass die Verteilung für das Outcome für die Behandlung \\(B\\) in etwa normalverteilt ist sowie das das Outcome für die Behandlung \\(A\\) keiner Normalverteilung folgt oder zwei Ausreißer hat. Die Entscheidung was jetzt stimmt ohne zu wissen wie die Daten generiert wurden, ist in der Anwendung meist nicht möglich.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Dotplot des Outcomes rsp.\n\n\n\n\n\n\n\n\n\n\n\n(b) Densityplot des Outcomes rsp.\n\n\n\n\n\n\n\nAbbildung 22.15— Verteilung des Outcomes rsp der zehn Beobachtungen aus den Gruppen \\(A\\) und \\(B\\). Beiden Gruppen kommen aus einer Exponentialverteilung.\n\n\n\n\nWir können wieder den Shapiro-Wilk-Test nutzen um statistisch zu testen, ob eine Abweichung von der Normalverteilung vorliegt. Wir erfahren aber nicht, welche andere Verteilung vorliegt. Wir testen natürlich für die beiden Gruppen getrennt.\n\nlow_nonnormal_tbl |&gt; \n  filter(trt == \"A\") |&gt; \n  pull(rsp) |&gt; \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  pull(filter(low_nonnormal_tbl, trt == \"A\"), rsp)\nW = 0.77114, p-value = 0.006457\n\nlow_nonnormal_tbl |&gt; \n  filter(trt == \"B\") |&gt; \n  pull(rsp) |&gt; \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  pull(filter(low_nonnormal_tbl, trt == \"B\"), rsp)\nW = 0.93316, p-value = 0.4797\n\n\nWir sehen, dass der \\(p\\)-Wert für die Behandlung \\(A\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Damit können wir die Nullhypothese ablehnen. Wir nehmen keine Normalverteilung für Gruppe \\(A\\) an. Auf der anderen Seite sehen wir, dass der \\(p\\)-Wert für die Behandlung \\(B\\) größer ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Damit können wir die Nullhypothese nicht ablehnen. Wir nehmen eine Normalverteilung für Gruppe \\(A\\) an.\nSuper, jetzt haben wir für die eine Gruppe eine Normalverteilung und für die andere nicht. Wir haben uns in die Ecke getestet. Wir können jetzt verschiedene Szenarien vorliegen haben.\n\nWir könnten in der Gruppe \\(A\\) zwei Ausreißer vorliegen haben.\nWir könnten in der Gruppe \\(B\\) zufällig eine Normalverteilung beobachtet haben.\n\nUnd nochmal zum Schluß, einem statistischen Test mit 4 bis 5 Wiederholungen in einer Gruppe zu glauben, ob eine Normalverteilung vorliegt, kannst du auch würfeln…\nLeider wissen wir im echten Leben nicht, aus welcher Verteilung unsere Daten stammen, wir können aber annehmen, dass die Daten einer Normalverteilung folgen oder aber die Daten so transformieren, dass die Daten einer approximativen Normalverteilung folgen. Siehe dazu auch das Kapitel 18 zur Transformation von Daten.\nWenn deine Daten keiner Normalverteilung folgen, dann kann es sein, dass du mit den Effektschätzern ein Problem bekommst. Du erfährst vielleicht, dass du die Nullhypothese ablehnen kannst, aber nicht wie stark der Effekt in der Einheit des gemessenen Outcomes ist.\n\n\n22.4.3 Testen der Normalverteilungsannahme in mehreren Gruppen\nIm folgenden Beispiel zu den Keimungsraten von verschiedenen Nelkensorten wollen wir einmal testen, ob jede Sorte einer Normalverteilung folgt. Da wir hier insgesamt 20 Sorten vorliegen haben, nutzen wir die Funktion map() aus dem R Paket {purrr} um hier schneller voranzukommen. Wie immer laden wir erst die Daten und mutieren die Spalten in dem Sinne wie wir die Spalten brauchen.\n\nclove_tbl &lt;- read_excel(\"data/clove_germ_rate.xlsx\") |&gt; \n  mutate(clove_strain = as_factor(clove_strain),\n         germ_rate = as.numeric(germ_rate))\n\nJetzt können wir die Daten nach der Sorte der Nelken in eine Liste mit der Funktion split() aufspalten und dann auf jedem Listeneintrag einen Shapiro-Wilk-Test rechnen. Dann machen wir uns noch die Ausgabe schöner und erschaffen uns eine decision-Spalte in der wir gleich das Ergebnis für oder gegen die Normalverteilung ablesen können.\n\nclove_tbl |&gt; \n  split(~clove_strain) |&gt; \n  map(~shapiro.test(.x$germ_rate)) |&gt; \n  map(tidy) |&gt; \n  bind_rows(.id = \"test\") |&gt;\n  select(test, p.value) |&gt; \n  mutate(decision = ifelse(p.value &lt;= 0.05, \"reject normal\", \"normal\"),\n         p.value = pvalue(p.value, accuracy = 0.001))\n\n# A tibble: 20 × 3\n   test          p.value decision     \n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;        \n 1 standard      0.272   normal       \n 2 west_rck_1    0.272   normal       \n 3 south_III_V   0.855   normal       \n 4 west_rck_2_II 0.653   normal       \n 5 comb_001      0.103   normal       \n 6 western_4     0.849   normal       \n 7 north_549     0.855   normal       \n 8 subtype_09    0.983   normal       \n 9 subtype_III_4 0.051   normal       \n10 ctrl_pos      0.992   normal       \n11 ctrl_7        0.683   normal       \n12 trans_09_I    0.001   reject normal\n13 new_xray_9    0.406   normal       \n14 old_09        0.001   reject normal\n15 recon_1       0.100   normal       \n16 recon_3456    0.001   reject normal\n17 east_new      0.907   normal       \n18 east_old      0.161   normal       \n19 south_II_U    0.048   reject normal\n20 west_3_cvl    0.272   normal",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Der Pre-Test oder Vortest</span>"
    ]
  },
  {
    "objectID": "stat-tests-pretest.html#visuelle-überprüfung",
    "href": "stat-tests-pretest.html#visuelle-überprüfung",
    "title": "22  Der Pre-Test oder Vortest",
    "section": "22.5 Visuelle Überprüfung",
    "text": "22.5 Visuelle Überprüfung\nAbschließend möchte ich hier nochmal das R Paket {performance} vorstellen. Wir können mit dem Paket auch die Normalverteilungsannahme der Residuen überprüfen sowie die Annahme der Homogenität der Varianzen. Das geht ganz einfach mit der Funktion check_model() in die wir einfach das Objekt mit dem Fit des Modells übergeben.\n\nlm_fit &lt;- lm(germ_rate ~ clove_strain, clove_tbl)\n\nTja und schon haben wir in der Abbildung 22.16 mehr als wir eigentlich wollen. Aber du siehst hier ganz gut, dass wir in diesen Daten Probleme mit der Varianzhomogenität haben. Die Linie in dem Subplot “Homogeneity of Variance” ist nicht flach und horizontal. Deshalb könnten wir die Daten einmal Transformieren oder aber mit der Varianzheterogenität modellieren. Das Modellieren machen wir gleich mal im Anschluss.\n\nlm_fit |&gt; check_model(check = c(\"normality\", \"linearity\", \"homogeneity\", \"qq\"))\n\n\n\n\n\n\n\nAbbildung 22.16— Übersicht der Plots zu der Modellgüte aus der Funktion check_model() nach der Modellierung mit der Funktion lm() und der Annahme der Varianzhomogenität.\n\n\n\n\n\nJetzt modellieren wir einmal die Effekte der clove_strain unter der Annahme der Varianzheterogenität. Die Funktion gls() aus dem R Paket {nlme} passt ein lineares Modell unter Verwendung der verallgemeinerten kleinsten Quadrate an. Die Fehler dürfen dabei aber korreliert oder aber ungleiche Varianzen haben. Damit haben wir ein Modell, was wir nutzen können, wenn wir Varianzheterogenität vorliegen haben.\n\ngls_fit &lt;- gls(germ_rate ~ clove_strain, \n               weights = varIdent(form =  ~ 1 | clove_strain), \n               data = clove_tbl)\n\nWir gehen hier nicht tiefer auf die Funktion ein. Wir müssen nur die weights so angeben, dass die weights die verschiedenen Gruppen in dem Faktor clove_strain berücksichtigen können. Dann haben wir schon das Modell für die Varianzheterogenität. Mehr musst du dann auch nicht machen, dass ist dann manchmal das schöne in R, dass wir dann auch immer wieder auf gewohnte Templates zurückgreifen können.\nIn der Abbildung 22.17 sehen wir das Ergebnis der Modellanpassung. Sieht gut aus, jedenfalls besser als mit dem lm(). Besonders schön sehen wir, dass wir dann auch wieder normalverteilte Residuen vorliegen haben. Damit ist eine wichtige Annahme an das Modell erfüllt. Das wäre jetzt ein gutes Modell um im Anschluss emmeans() zu nutzen. Da gehst du dann aber bitte in das Kapitel zu den Posthoc Tests.\n\ngls_fit |&gt; check_model(check = c(\"normality\", \"linearity\", \"qq\"))\n\n\n\n\n\n\n\nAbbildung 22.17— Übersicht der Plots zu der Modellgüte aus der Funktion check_model() nach der Modellierung mit der Funktion gls() und der Annahme der Varianzheterogenität über die Gruppen von clove_strain.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Der Pre-Test oder Vortest</span>"
    ]
  },
  {
    "objectID": "stat-tests-pretest.html#referenzen",
    "href": "stat-tests-pretest.html#referenzen",
    "title": "22  Der Pre-Test oder Vortest",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 22.2— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein simples Säulendiagramm mit sehr für Farbblinde ungünstigen Farben. Es sind die Mittelwerte sowie die Standardabweichung durch die Fehlerbalken dargestellt.\nAbbildung 22.3— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt.\nAbbildung 22.4— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion gls() im obigen Modell berücksichtigt.\nAbbildung 22.5— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion sandwich::vcovHAC im obigen Modell berücksichtigt.\nAbbildung 22.6— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit den zwei Faktoren Zeit und die Iodine Form.\nAbbildung 22.7— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\nAbbildung 22.8— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\nAbbildung 22.9— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit den zwei Faktoren Zeit und die Iodine Form. Hier soll es dann ein Boxplot werden.\nAbbildung 22.10— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen, dafür müssen wir uns aber nochmal ein Positionsdatensatz bauen. Hier ist das compact letter display getrennt für die beiden Jodformen berechnet.\nAbbildung 22.11— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen, dafür müssen wir uns aber nochmal ein Positionsdatensatz bauen. Hier ist das compact letter display für jede einzelne Faktorkombination berechnet.\nAbbildung 22.12— Boxplot der beiden Treatment Level A und B. Beide Gruppen haben die gleichen Varianzen. Es liegt Varianzhomogenität vor.\nAbbildung 22.13— Boxplot der beiden Treatment Level A und B. Beide Gruppen haben ungleiche Varianzen. Es liegt Varianzheterogenität vor.\nAbbildung 22.14 (a)— Dotplot des Outcomes rsp.\nAbbildung 22.14 (b)— Densityplot des Outcomes rsp.\nAbbildung 22.15 (a)— Dotplot des Outcomes rsp.\nAbbildung 22.15 (b)— Densityplot des Outcomes rsp.\nAbbildung 22.16— Übersicht der Plots zu der Modellgüte aus der Funktion check_model() nach der Modellierung mit der Funktion lm() und der Annahme der Varianzhomogenität.\nAbbildung 22.17— Übersicht der Plots zu der Modellgüte aus der Funktion check_model() nach der Modellierung mit der Funktion gls() und der Annahme der Varianzheterogenität über die Gruppen von clove_strain.\n\n\n\nKozak M, Piepho H-P. 2018. What’s normal anyway? Residual plots are more telling than significance tests when checking ANOVA assumptions. Journal of agronomy and crop science 204: 86–98.\n\n\nZuur AF, Ieno EN, Elphick CS. 2010. A protocol for data exploration to avoid common statistical problems. Methods in ecology and evolution 1: 3–14.",
    "crumbs": [
      "Testen von Hypothesen",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Der Pre-Test oder Vortest</span>"
    ]
  },
  {
    "objectID": "experimental-design-preface.html",
    "href": "experimental-design-preface.html",
    "title": "Experimentelle Designs",
    "section": "",
    "text": "Übersicht der experimentellen Designs\nZuerst empfehle ich dir das folgende Kapitel in dem ich nochmal auf die Erstellung von Versuchsplänen in R eingehe. Ich stelle dort R Pakete vor und zeige auch wie du dir selber dein Design erstellen kannst.\nWir schauen uns in den folgenden Kapiteln einmal eine Auswahl an experimentellen Designs an. Im Laufe der Zeit werden sicherlich noch andere Designs ergänzt werden. Soweit erstmal diese Auswahl hier.\nAls Spezialfälle dann noch folgende Designs, die ich selten in der Beratung habe, aber dennoch nochmal vorstellen möchte.\nIn dem weiteren Kapitel wollen wir uns mit der Erstellung von komplexeren experimentellen Designs beschäftigen.\nDie Liste ist nicht vollständig und kann von mir auch je nach Bedarf erweitert werden. Wir belassen es aber bis auf Weiteres bei diesen sehr häufig genutzten Varianten.",
    "crumbs": [
      "Experimentelle Designs"
    ]
  },
  {
    "objectID": "experimental-design-preface.html#übersicht-der-experimentellen-designs",
    "href": "experimental-design-preface.html#übersicht-der-experimentellen-designs",
    "title": "Experimentelle Designs",
    "section": "",
    "text": "In dem ersten Kapitel zur Versuchsplanung in R zeige ich einmal auf, wie du Experimente in R planen kannst.\n\n\n\nComplete randomized design (CRD). Das complete randomized design ist der Klassiker unter den experimentellen Designs und wird häufig verwendet.\nRandomized complete block design (RCBD). Das randomized vomplete block design ist entweder eine Erweiterung des romplete randomized design und bringt noch eine neuen Faktor für die Wiederholung mit in das Experiment mit ein.\nLatin square design (LSD) Das latin square design liefert eine gleichmäßige Aufteilung der experimentellen Einheiten über ein Feld oder ein Stall.\n\n\n\nAlpha design. Hier handelt es sich um ein incomplete block design. Das heißt wir haben nicht jede Behandlung in jedem Block vorliegen. Da wir hier dann natürlich Fläche einsparen können wir dann mehr Behandlungen oder Sorten testen. Dafür bezahlen wir auch einen preis, das Alpha design hat eine nicht so hohe Aussagekraft.\nAugmented design. Wir nutzen dieses Design, wenn wir in unserem incomplete block design noch Behandlungen haben, die nicht unvollständig sein sollen. Das heißt, wir haben hier den Hybriden aus dem RCBD und eben dem Alpha design. Wir nehmen also immer in jeden unvollständigen Block noch Behandlungen mit auf, die dann eben vollständig randomisiert sind.\n\n\n\nRandomized complete block design (RCBD, 3-faktoriell)\nSplit plot design (3-faktoriell) oder Spaltanlage\nSubsampling\nIncomplete block design (3-faktoriell und 2-faktoriell)\nStrip plot design (3-faktoriell) oder Streifenanlage",
    "crumbs": [
      "Experimentelle Designs"
    ]
  },
  {
    "objectID": "experimental-design-preface.html#grundlagen-eines-statistischen-versuchs",
    "href": "experimental-design-preface.html#grundlagen-eines-statistischen-versuchs",
    "title": "Experimentelle Designs",
    "section": "Grundlagen eines statistischen Versuchs",
    "text": "Grundlagen eines statistischen Versuchs\nWie funktioniert ein statistischer Versuch? Ich könnte auch wissenschaftliches Experiment schreiben, aber ein wissenschaftliches Experiment ist sehr abstrakt. Wir wollen ja einen Versuch durchführen und danach - ja was eigentlich? Was wollen wir nach dem Versuch haben? Meistens eine neue Erkenntnis. Um diese Erkenntnis zu validieren oder aber abzusichern nutzen wir Statistik. Dazu musst du noch wissen, dass wir eine spezielle Form der Statistik nutzen: die frequentistische Statistik.\nDie frequentistische Statistik basiert - wie der Name andeutet - auf Wiederholungen in einem Versuch. Daher der Name frequentistisch. Also eine Frequenz von Beobachtungen. Ist ein wenig gewollt, aber daran gewöhnen wir uns schon mal. Konkret, ein Experiment welches wir frequentistisch Auswerten wollen besteht immer aus biologischen Wiederholungen. Wir müssen also ein Experiment planen in dem wir wiederholt ein Outcome an vielen Tieren, Pflanzen oder Menschen messen. Eine biologische Wiederholung beinhaltet ein neues Tier, Pflanze oder Mensch. Eine technische Wiederholung ist die gleiche Messung an dem gleichen Tier, Pflanze oder Mensch.\n\n\n\n\n\n\nWie gehen wir nun vor, wenn wir ein Experiment durchführen wollen?\n\n\n\n\nWir müssen auf jeden Fall wiederholt ein Outcome an verschiedenen Tieren, Pflanzen oder Menschen messen.\nWir überlegen uns aus welcher Verteilungsfamilie unser Outcome stammt, damit wir dann die entsprechende Verfahren zur Analyse nehmen können.",
    "crumbs": [
      "Experimentelle Designs"
    ]
  },
  {
    "objectID": "experimental-design-preface.html#tipps-tricks",
    "href": "experimental-design-preface.html#tipps-tricks",
    "title": "Experimentelle Designs",
    "section": "Tipps & Tricks",
    "text": "Tipps & Tricks\nHier einmal eine Sammlung an Tipps & Tricks für deinen versuch, der sich in der letzten Zeit so ansammelt. Ich ergänze hier immer fortlaufend, wenn ich was passenden finde oder höre.\n\nWir berechnen meist den Mittelwert von \\(n\\) Pflanzen in einer Parzelle. Wenn wir das nicht tun, könnte es sein, dass du gerade ein Subsampling Experiment durchführst.\nMarkiere dir die Pflanze, die du wiederholt messen willst, zu Beginn des Experiments mit einem farbigen Stock. Du kannst im Gewächshaus die Blumentöpfe mit Sprühfarben markieren.\nNehme dir einen Zollstock mit, wenn du photographierst, sonst hast du keinen passenden Maßstab.\nNehme dir die Zeit für die Fotos, später lassen sich viele Fotos nicht wiederholen. Lege dir einen einfarbigen Untergrund unter die Pflanze. Je nach Art der Pflanze oder des Organs, mag die Farbe der Unterlage entscheident sein.\nSind deine Zettel wasserfest und dein Stift auch? Wenn du im Freiland unterwegs bist, kann es regnen.\n\nIn Tabelle 1 sehen wir einmal mögliche Quellen für die Verwirrung und die Möglichkeiten des experimentellen Design etwas gegen diese Quellen der Verwirrung zu unternehmen. Wir können hier Quelle der Verwirrung auch als Quelle der Varianz deuten. Eine detaillierte Diskussion findet sich in Dormann (2013) und Hurlbert (1984). Wichtig ist hier mitzunehmen, dass wir häufig eine Kontrolle brauchen um überhaupt die Stärke des Effektes messen zu können. Sonst können wir die Frage, ob die Behandlung besser ist nicht quantifizieren.\n\n\n\nTabelle 1— In Dormann (2013) und Hurlbert (1984) finden wir eine Zusammenfassung von Quellen der Verwirrung also eigentlich der Varianz und deren mögliche Lösung um die Varianz zu beherrschen.\n\n\n\n\n\n\n\n\n\n\n\nQuelle der Verwirrung\nMerkmal des experimentellen Designs um die Verwirrung zu reduziert oder aufzulösen\n\n\n\n\n1.\nZeitliche Veränderung\nKontrollgruppe\n\n\n2.\nArtefakte in der Behandlung\nKontrollgruppe\n\n\n3.\nVoreingenommenheit des Forschenden (Bias)\nRandomisierte Zuordnung der Versuchseinheiten zu den Behandlungen; generelle Randomisierung bei allen möglichen Prozessen; Verblindete Prozeduren\n\n\n4.\nVom Forschenden induzierte Variabilität (zufälliger Fehler)\nWiederholungen der Behandlungen (und Kontrolle)\n\n\n5.\nAnfängliche oder beinhaltende Variabilität zwischen den Versuchseinheiten\nWiederholungen der Behandlungen (und Kontrolle); Durchmischen der Behandlungen; Begleitbeobachtungen (Positive Kontrolle)\n\n\n6.\nNicht-dämonische Einflüsse\nWiederholung und Durchmischung der Behandlungen (und Kontrolle)\n\n\n7.\nDämonische Eingriffe\nEwige Wachsamkeit - siehe dazu auch Feynman (1998); Geisteraustreibung; Menschenopfer",
    "crumbs": [
      "Experimentelle Designs"
    ]
  },
  {
    "objectID": "experimental-design-preface.html#referenzen",
    "href": "experimental-design-preface.html#referenzen",
    "title": "Experimentelle Designs",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nBenz B, Kurz S, Hartung J. 2023. Einfluss des Stichprobendesigns und der Messdauer von Liegezeiten bei Milchkühen auf den Stichprobenumfang. Landtechnik 78.\n\n\nCasler MD. 2015. Fundamentals of experimental design: Guidelines for designing successful experiments. Agronomy Journal 107: 692–705.\n\n\nDormann CF. 2013. Parametrische Statistik. Springer.\n\n\nFeynman RP. 1998. Cargo cult science. Seiten 55–61 in. The art and science of analog circuit design. Elsevier.\n\n\nHurlbert SH. 1984. Pseudoreplication and the design of ecological field experiments. Ecological monographs 54: 187–211.\n\n\nPiepho H-P, Gabriel D, Hartung J, Büchse A, Grosse M, Kurz S, Laidig F, Michel V, Proctor I, Sedlmeier JE, others. 2022. One, two, three: Portable sample size in agricultural research. The Journal of Agricultural Science 160: 459–482.",
    "crumbs": [
      "Experimentelle Designs"
    ]
  },
  {
    "objectID": "experimental-design-r.html",
    "href": "experimental-design-r.html",
    "title": "23  Versuchsplanung in R",
    "section": "",
    "text": "23.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\nset.seed(20230812)\npacman::p_load(tidyverse, magrittr, writexl, agricolae,\n               desplot, dae, FielDHub,\n               conflicted)\nconflicts_prefer(magrittr::set_names)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Versuchsplanung in R</span>"
    ]
  },
  {
    "objectID": "experimental-design-r.html#selbermachen-mit-expand_grid",
    "href": "experimental-design-r.html#selbermachen-mit-expand_grid",
    "title": "23  Versuchsplanung in R",
    "section": "23.2 Selbermachen mit expand_grid()",
    "text": "23.2 Selbermachen mit expand_grid()\nWenn wir uns das experimentelle Design selber zusammen programmieren können wir auf das tidyverse Paket zurückgreifen und haben dann ein schöneres Leben. Wir müssen zwar etwas mehr beachten, aber dafür sind wir auch sehr viel mehr flexibel. Deshalb stelle ich für die einfacheren Designs in den folgenden Kapiteln auch immer den Code zum Selbermachen vor. Hier gibt es jetzt einmal die Grundidee am Complete randomized design (CRD). Wir gehen immer von einem balancierten Design aus, dass heißt in jeder Behandlungsgruppe sind gleich viele Beobachtungen.\nNehmen wir folgendes simples Modell. Wir wollen untersuchen, ob das Trockengewicht drymatter von einer Behandlung trt abhängt. Wir werden uns dann gleich noch entscheiden, wie viele Behandlungsgruppen wir wollen und wie viele Wiederholungen wir pro Behandlungsgruppe nehmen.\n\\[\ndrymatter \\sim \\overbrace{trt}^{f_1}\n\\]\nZentral für unsere Überlegungen ist die Funktion expand_grid(), die es uns einfach erlaubt alle Faktorkombinationen aus zwei oder mehr Vektoren zu erstellen. Wir haben im Folgenden im Vektor a die Zahlen 1 bis 3 und in dem Vektor b die Zahlen 1 und 2. Jetzt wollen wir alle Kombinationen von a und b haben und nutzen dafür expand_grid().\n\nexpand_grid(a = 1:3, b = 1:2)\n\n# A tibble: 6 × 2\n      a     b\n  &lt;int&gt; &lt;int&gt;\n1     1     1\n2     1     2\n3     2     1\n4     2     2\n5     3     1\n6     3     2\n\n\nJetzt einmal konkreter auf unser Beispiel angewandt. Wir wollen vier Behandlungsgruppen und in jeder Behandlungsgruppe fünf Wiederholungen rep. Dann benennen wir noch die Behandlungen mit ctrl, A, B und C. Unsere Wiederholungen kriegen die Zahlen von 1 bis 5. Dann ergänzen wir noch eine Pflanzenidentifizierungsnummer pid über die wir dann später randomisieren können. Wenn du willst, kannst du dir dann über select() noch die Spalten sauber sortieren.\n\ncrd_long_tbl &lt;- expand_grid(trt = 1:4, rep = 1:5) |&gt; \n  mutate(trt = factor(trt, labels = c(\"ctrl\", \"A\", \"B\", \"C\")),\n         rep = factor(rep, labels = 1:5),\n         pid = 1:n()) |&gt; \n  select(pid, everything())\ncrd_long_tbl\n\n# A tibble: 20 × 3\n     pid trt   rep  \n   &lt;int&gt; &lt;fct&gt; &lt;fct&gt;\n 1     1 ctrl  1    \n 2     2 ctrl  2    \n 3     3 ctrl  3    \n 4     4 ctrl  4    \n 5     5 ctrl  5    \n 6     6 A     1    \n 7     7 A     2    \n 8     8 A     3    \n 9     9 A     4    \n10    10 A     5    \n11    11 B     1    \n12    12 B     2    \n13    13 B     3    \n14    14 B     4    \n15    15 B     5    \n16    16 C     1    \n17    17 C     2    \n18    18 C     3    \n19    19 C     4    \n20    20 C     5    \n\n\nDann kannst du dir die Datei mit der Funktion write_xlsx() aus dem R Paket writexl raus schreiben und dann entsprechend mit deinen Messwerten für das Trockengewicht ergänzen.\n\ncrd_long_tbl |&gt; \n  write_xlsx(\"template_sheet.xlsx\")\n\nWenn du für deine zwanzig Pflanzen noch ein Randomisierungmuster brauchst, dann empfehle ich dir die Folgende schnelle Art und Weise. Du nimmst die Pflanzen ID’s von 1 bis 20 und mischt die Zahlen einmal mit der Funktion sample() durch. Dann erstellst du dir als dein Grid für deine Pflanzen mit einer \\(4 \\times 5\\)-Matrix und pflanzt nach diesem Grid die Pflanzen ein.\n\ncrd_long_tbl$pid |&gt; \n  sample() |&gt; \n  matrix(nrow = 4, ncol = 5,\n         dimnames = list(str_c(\"Reihe\", 1:4, sep = \"-\"),\n                         str_c(\"Spalte\", 1:5, sep = \"-\")))\n\n        Spalte-1 Spalte-2 Spalte-3 Spalte-4 Spalte-5\nReihe-1        5       16        2       19       12\nReihe-2        3        1       20       13       11\nReihe-3       18        9        6       10        4\nReihe-4       14        8       15        7       17\n\n\nEs gibt natürlich noch andere Möglichkeiten, aber das ist jetzt die schnellste Variante ein gutes Randomisierungsmuster hinzukriegen.\n\n\n\n\n\n\nSpielecke mit expand_grid()\n\n\n\n\n\nHier nochmal für mich ein paar Punkte zum expand_grid(), die ich immer mal wieder brauche, aber gerne mal vergesse.\n\n## Erstellung eines zweifaktoriellen Grids\ntbl &lt;- expand_grid(block = 1:2, trt = 1:4)\n\n## Vollständige Randomisierung\ntbl |&gt; \n  slice_sample(prop = 1)\n\n## Randomisierung im Faktor trt\ntbl |&gt; \n  group_by(trt) |&gt; \n  slice_sample(prop = 1)\n\n## Randomisierung im Faktor trt und Subgruppe id ergänzt \ntbl |&gt; \n  group_by(trt) |&gt; \n  slice_sample(prop = 1) |&gt; \n  expand_grid(id = 1:3)",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Versuchsplanung in R</span>"
    ]
  },
  {
    "objectID": "experimental-design-r.html#das-r-paket-fieldhub",
    "href": "experimental-design-r.html#das-r-paket-fieldhub",
    "title": "23  Versuchsplanung in R",
    "section": "23.3 Das R Paket {FielDHub}",
    "text": "23.3 Das R Paket {FielDHub}\nBevor du alles selber machst, schaue dir unbedingt das tolle R Paket {FielDHub} einmal an. Bei dem Paket handelt es sich um eine Shiny App, so dass du eine super Oberfläche in einem Browser hast. In der Shiny App kannst du dann selber die Designs auswählen und die verschiedenen Parameter ändern. Die Darstellung erfolgt dann teilweise auch in den hier vorgestellten R Paketen wie {desplot}, aber eben schon sehr gut aufgearbeitet. Du erhältst dann auch gleich ein Feldbuch, welches du dir als Exceltabelle exportieren lassen kannst. Das Feldbuch kannst du dann gleich zum randomisieren nutzen und auch deine gemessenen Werte eintragen. Dann wollen wir einmal loslegen, der Code ist nur zwei Zeilen lang und dann startet die Shiny App in deinem Browser.\n\nlibrary(FielDHub)\nrun_app()\n\nHier kannst du dann auch erstmal stoppen, wenn es dir nur um die Anwendung geht. Das R Paket {FielDHub} ist allen anderen Paketen hier was die Anwendbarkeit angeht überlegen. Ich nutze in den folgenden Kapiteln immer die R Implementierung von {FielDHub} und nicht die Shiny App. Es gibt dafür auf der Hilfeseite von {FielDHub} eine Übersicht über alle Funktionen und implementierten experimentellen Designs.\n\n\n\n\n\n\nHier kommt das Video",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Versuchsplanung in R</span>"
    ]
  },
  {
    "objectID": "experimental-design-r.html#das-r-paket-agricolae",
    "href": "experimental-design-r.html#das-r-paket-agricolae",
    "title": "23  Versuchsplanung in R",
    "section": "23.4 Das R Paket {agricolae}",
    "text": "23.4 Das R Paket {agricolae}\nFür die Erstellung von komplexeren experimentellen Designs führt kein Weg an dem R Paket agricolae vorbei. Bei den einfacheren Designs ist es dann so eine Sache, ob du dir mit agricolae einen Gefallen tust oder eher die Sache sehr stark verkomplizierst. Ich würde dir bei den einfacheren Designs empfehlen einfach exoand_grid() wie oben beschrieben zu nutzen. Das ist einfacher und funktioniert auch gut, wenn nicht gar besser. Darüber hinaus ist die Hilfe der Funktionen teilweise etwas sehr mager und die Weiterentwicklung eher fraglich. Für das Paket agricolae gibt es zwe i ziemlich identische Tutorien einmal das Tutorium agricolae als PDF und einmal mit Beispielen von der Webseite Experimental Designs with agricolae. Beide Tutorien sind identisch, dass eine ist ein PDF und das andere eine Webseite.\nSo dann schauen wir uns mal die gängigen Parameter bei der Erstellung des Designs mit den Funktionen von agricolae an:\n\nseries: legt fest wie viele Zeichen plus 1 die Nummerierung der Zeile haben soll. Wenn wir series = 2 setzen, dann zählen wir mit 101, 102, 203 usw. die Zeilen hoch. Im Falle eines vollständig randomisierten Designs ist die Nummerierung fortlaufend.\nseed: der Seed für die Zufallsgenerierung und sein Wert ist eine beliebige Zahl.\nkinds: die Methode der Zufallsgenerierung, standardmäßig “Super-Duper” und interessiert uns hier nicht besonders.\nrandomization: Soll das Design randomisiert werden?\n\nUnd Folgendes kommt dann bei agricolae als Ausgabe raus. Zwar nicht immer, aber das ist der grobe Überblick.\n\nparameters: die Eingabe zur Generierung des Designs. Wir erhalten also nochmal unsere Werte wieder, die wir eingegeben haben. Meistens nicht von Interesse.\nbook: Das Feldbuch indem das wichtige drin steht, nämlich unsere generiertes Faktordesign.\nstatistics: die Informationsstatistiken das Design, die wir noch zusätzlich kriegen. Diese nutzen wir nur für komplexere Designs.\nsketch: Verteilung der Behandlungen im Feld. Wird uns nicht immer wiedergeben und ist als Hilfe für die direkte Anwendung gedacht.\n\nWenn wir Parzellen anlegen, dann erhalten wir auch noch folgende Informationen wieder. Dfür muss dann aber auch das experimentelle Design entsprechende Parzellen haben.\n\nzigzag: ist eine Funktion, die es erlaubt die Verteilung der Beobachtungen entlang der Parzellen zu kontrollieren. Meistens ist es etwas zu viel des Guten, aber gut das du hier noch Änderungen vornehmen kannst.\nfieldbook: Ausgabe des Zickzacks aus der obigen Funktion und das entsprechende Feldbuch mit dem Design.\n\nEs gibt eine weitreichende Anzahl an design.*-Funktionen für sehr viele Designs. Wir schauen uns also jetzt einmal als Beispiel die Funktion design.crd() an um ein complete randomized design zu erstellen. Der Vorteil der Funktion ist hier, dass wir verschiedene Anzahlen von Individuen in die Generierung des Designs nehmen können. Also zum Beispiel drei Pflanzen in der Kontrolle und dann jeweils fünf Pflanzen in der Behandlung. Weil es aber dann meistens nicht auf die zwei Kontrollpflanzen weniger ankommt, machen wir immer ein balanciertes Design. Wenn es aber unbalanciert sein soll, dann ist es natürlich hier einfacher umzusetzen als mit expand_grid().\nIn den Designfunktionen haben wir meistens einmal die Option für die Behandlung trt sowie für die Wiederholungen r. Leider ist es so, dass je nach Design die Option r mal die Wiederholungen oder aber die Blöcke beschreibt. Schau dir da bitte die Beispiele an. Im Weiteren haben wir dann noch die serie Option, die einfach die Nummerierung auf Hunderter setzt.\n\ncrd_obj &lt;- design.crd(trt = c(\"ctrl\", \"A\", \"B\", \"C\"), \n                      r = c(3, 5, 5, 5), serie = 2)\n\nNun ist es so, dass die Funktion den Spaltennamen der Behandlung auf c(\"ctrl\", \"A\", \"B\", \"C\") setzt anstatt auf einen Namen. Dafür müssten wir dann einen Vektor trt übergeben, aber das wird mir dann irgendwann zu wirr. Deshalb nenne ich dann alle Spalten nochmal mit der Funktion set_names() entsprechend um und erschaffe mir einen tibble.\n\ncrd_book &lt;- crd_obj |&gt; \n  pluck(\"book\") |&gt; \n  as_tibble() |&gt; \n  set_names(c(\"plots\", \"r\", \"trt\"))\ncrd_book\n\n# A tibble: 18 × 3\n   plots     r trt  \n   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;\n 1   101     1 C    \n 2   102     2 C    \n 3   103     1 B    \n 4   104     2 B    \n 5   105     1 A    \n 6   106     1 ctrl \n 7   107     2 A    \n 8   108     3 C    \n 9   109     4 C    \n10   110     3 B    \n11   111     2 ctrl \n12   112     3 A    \n13   113     5 C    \n14   114     4 B    \n15   115     4 A    \n16   116     3 ctrl \n17   117     5 B    \n18   118     5 A    \n\n\nWir haben kein sketch aus der Funktion. Daher müssen wir uns selber überlegen, wie wir dann die Pflanzen anordnen würden. Darüber hinaus ist die Ordnung wild, ich sehe da eher weniger Struktur in der Ausgabe, als das mir es hilft. Aber dazu dann mehr in den folgenden Kapiteln. Hier ist es erstmal die stumpfe Durchführung am Beispiel des complete randomized design.\nDas Paket agricolae hat keine interne Möglichkeit sich die Designs zu visualisieren.\n\n\n\n\n\n\nFehlende Einbindung von {agricolae} in andere R Pakete\n\n\n\n\n\nIn der Abbildung 23.3 sehen wir die Problematik mit der fehlenden Einbindung von agricolae in das Paket desplot. Die Abbildung ist einfach suboptimal. Da wir in dem Feldbuch von agricolae keine Zeilen und Spalten der Position wiederbekommen, können wir das Design nicht sauber darstellen. Leider liefert uns die Funktion design.crd() auch keine Verteilung der Behandlungen im Feld, so dass wir hier alles selber bauen müssten. Dann geht es mit expand_grid() schneller.\n\nggdesplot(data = crd_book, flip = TRUE,\n          form = trt ~ r + plots,              \n          text = trt, cex = 1, shorten = \"no\", \n          main = \"Field layout\", show.key = F)    \n\n\n\n\n\n\n\nAbbildung 23.2— Beispiel für die fehlende Einbindung von agricolae durch desplot. Da agricolae für durch die Funktion design.crd() keine Positionen in Zeile und Spalte liefert kann nur ein suboptimaler Plot erstellt werden.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Versuchsplanung in R</span>"
    ]
  },
  {
    "objectID": "experimental-design-r.html#das-r-paket-desplot",
    "href": "experimental-design-r.html#das-r-paket-desplot",
    "title": "23  Versuchsplanung in R",
    "section": "23.5 Das R Paket {desplot}",
    "text": "23.5 Das R Paket {desplot}\nWenn wir uns das Design eines Esperiments abbilden wollen, dann können wir das R Paket desplot nutzen. Die Hilfeseite Plotting field maps with the desplot package liefert nochmal mehr Informationen. Auch finde ich die Beispiele für die Anwendung von desplot von DSFAIR - Designing experiments sehr schön. Wir besprechen aber die einzelnen Abbildungen dann in den separaten Abschnitten in den folgenden Kapiteln.\nWir nehmen wieder unseren eigenes Design für das complete randomized design aus dem Objekt crd_long_tbl. Wir können aber das Design nicht einfach so abbilden, wir brauchen noch die Position der einzelne Behandlungen auf dem Feld. Die Positionen bestimmen wir durch Reihen rows und Spalten cols. Jetzt müssen wir also unser Grid ergänzen auf dem wir unsere Pflanzen stellen wollen. In unserem Fall wollen wir unsere Pflanzen auf vier Zeilen rows und fünf Spalten cols stellen. Das bietet sich bei zwanzig Pflanzen dann ja auch an.\n\ncrd_grid &lt;- expand_grid(rows= 1:4, cols = 1:5)\ncrd_grid  \n\n# A tibble: 20 × 2\n    rows  cols\n   &lt;int&gt; &lt;int&gt;\n 1     1     1\n 2     1     2\n 3     1     3\n 4     1     4\n 5     1     5\n 6     2     1\n 7     2     2\n 8     2     3\n 9     2     4\n10     2     5\n11     3     1\n12     3     2\n13     3     3\n14     3     4\n15     3     5\n16     4     1\n17     4     2\n18     4     3\n19     4     4\n20     4     5\n\n\nUnser Grid der Positionen können wir jetzt mit unserem Design durch die Funktion bind_cols() verbinden. Würden wir das nur so machen, dann hätten wir keine Randomsierung drin. Deshalb durchmischen wir die Daten einmal vollständig mit der Funktion slice_sample(). Dann haben wir unser Objekt crd_plot_tbl ferti und können damit weitermachen.\n\ncrd_plot_tbl &lt;- crd_long_tbl |&gt; \n  slice_sample(prop = 1)  |&gt; \n  bind_cols(crd_grid)\ncrd_plot_tbl  \n\n# A tibble: 20 × 5\n     pid trt   rep    rows  cols\n   &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt;\n 1     1 ctrl  1         1     1\n 2     4 ctrl  4         1     2\n 3    17 C     2         1     3\n 4    10 A     5         1     4\n 5     2 ctrl  2         1     5\n 6     6 A     1         2     1\n 7    12 B     2         2     2\n 8    13 B     3         2     3\n 9    11 B     1         2     4\n10    15 B     5         2     5\n11     3 ctrl  3         3     1\n12     8 A     3         3     2\n13    19 C     4         3     3\n14     7 A     2         3     4\n15    20 C     5         3     5\n16    16 C     1         4     1\n17     9 A     4         4     2\n18    14 B     4         4     3\n19     5 ctrl  5         4     4\n20    18 C     3         4     5\n\n\nIn der Abbildung 23.2 sehen wir einmal unser complete randomized design Design aus dem Objekt crd_plot_tbl dargestellt. Wir sehen, dass wir die vier Behandlungen mit den fünf Wiederholungen zufällig über den ganzen Tisch verteilt haben. Wichtig ist, dass du einmal die Form form angibst, wie sich der Plot aufbaut. Links von der Tilde ~ steht die Behandlung und rechts von der Tilde ~ die Positionsangaben. Je nach Design ändert sich das noch etwas und wir ergänzen noch andere visuelle Parameter. Dann wollen wir alles mit den Labels von der Behandlung trt beschriften. Dafür nutzen wir dann die Option text. Der Rest der Optionen sind dann noch quality of life Funktionen, die du an- oder abstellen kannst.\n\nggdesplot(data = crd_plot_tbl, \n          form = trt ~ cols + rows,\n          text = trt, cex = 1, show.key = FALSE, \n          shorten = \"no\", \n          main = \"Complete randomized design (CRD)\")\n\n\n\n\n\n\n\nAbbildung 23.3— Das complete randomized design Design in desplot dargestellt.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Versuchsplanung in R</span>"
    ]
  },
  {
    "objectID": "experimental-design-r.html#das-r-paket-dae",
    "href": "experimental-design-r.html#das-r-paket-dae",
    "title": "23  Versuchsplanung in R",
    "section": "23.6 Das R Paket {dae}",
    "text": "23.6 Das R Paket {dae}\nEin weiteres R Paket, was es ermöglicht komplexere Experimente zu planen ist das R Paket dae. Über die Hilfeseite dae: Functions Useful in the Design and ANOVA of Experiments kannst du dir das Tutorium anschauen, was ich dir auch empfehlen würde. Wir machen wie immer hier nur einen kleinen Teil, den Rest musst du dann selber nach schauen. Wenn es aber um komplexere experimentelle Designs geht, dann ist das Paket dae auf jeden Fall geeignet.\nDas R Paket dae hat für die Faktorerstellung in Gruppen eine eigne Funktion fac.gen(). Mit der Funktion können wir besonders gut Faktoren generieren. Bei einfachen Beispielen wie dem randomized complete block design (RCBD) bräuchten wir die Funktion eigentlich nicht, aber hier einmal zur Demonstration. Bei komplexeren Beispielen ist die Funktion nicht wegzudenken. Deshalb einmal hier unser Modell mit einem zusätzlichen Block zu unserer Behandlung.\n\\[\ndrymatter \\sim \\overbrace{trt}^{f_1} + \\underbrace{block}_{f_2}\n\\]\nJetzt können wir die Funktion fac.gen() wiederholt nutzen und uns daraus dann einen Datensatz zusammenbauen. Wir sehen auch gleich nochmal eine Alternative für dieses relativ einfache Beispiel. Wir brauchen ja nur ein Positionsgrid und dann die Zuordnung der Behandlungen mit den entsprechenden Gruppenleveln.\n\nrcbd_sys &lt;- cbind(fac.gen(generate = list(rows = 5, cols = 4)),\n                  fac.gen(generate = list(trt = LETTERS[1:4]), times = 5))\n\nWir können auch mit der Funktion expand_grid() von weiter oben relativ einfach das Pflanzengrid nachbauen. Da brauchen wir eigentlich nicht die Funktion fac.gen(). Wie immer, bei einfachen Beispielen reicht auch viel was in R finden, bei komplexeren Designs würde ich immer zu fac.gen() wechseln.\n\nrcbd_sys &lt;- expand_grid(rows = 1:5,\n                        cols = 1:4) |&gt; \n  mutate(trt = factor(cols, labels = LETTERS[1:4]),\n         rows = as_factor(rows),\n         cols = as_factor(cols))\n\nDie Ergebnisse sind in beiden Fällen gleich. Wir erhalten einen Datensatz, der uns die Positionen und die Behandlungen mit den Gruppen wiedergibt. Dann können wir schon die Funktion designRandomize() mit den Informationen füttern. Wir haben hier wieder den Fall, dass wir erst die Positionen in einem Griddatensatz bauen und dann die Randomisierung ergänzen. Folgende Optionen müssen wir dabei in unserem etwas einfacheren Fall nur berücksichtigen.\n\nallocated: Ist meistens der Faktor unserer Behandlung und damit der zugeordnete Faktor. Das heißt, wir wollen diesen Faktor auf das Grid aus Zeilen und Spalten verteilen bzw. zuordnen. In unserem Fall die vier Behandlungen mit trt.\nrecipient: Sind die Faktoren, die den Behandlungsgruppen zugeordnet werden. Damit auch das Grid, was den Behandlungsfaktor empfängt. Wir haben hier also Zeilen rows und Spalten columns in denen unsere Behandlung zufällig verteilt werden soll.\nnested.recipients: Hier wird angegeben, in welcher Abhängigkeitsstruktur das Grid vorliegt. Unsere Blöcke sind in diesem Fall über die Zeilen rows orientiert.\n\nDann rufen wir einmal die Funktion auf und erhalten unseren randomisierten Datensatz wieder. So einfach ist die Funktion leider dann nicht zu verstehen - intuitiv ist was anders. Dennoch liefert dae sehr viele Möglichkeiten. Du musst dich da im Zweifel etwas länger durch die Hilfeseiten arbeiten. Ich werde in den folgenden Kapiteln und Abschnitten versuchen die gängigsten experimentellen Designs in dae abzubilden.\n\nrcbd_lay &lt;- designRandomize(allocated = rcbd_sys[\"trt\"],\n                            recipient = rcbd_sys[c(\"rows\", \"cols\")],\n                            nested.recipients = list(cols = \"rows\"),\n                            seed = 1134)\nrcbd_lay \n\n   rows cols trt\n1     1    1   C\n2     1    2   D\n3     1    3   B\n4     1    4   A\n5     2    1   C\n6     2    2   A\n7     2    3   D\n8     2    4   B\n9     3    1   B\n10    3    2   A\n11    3    3   C\n12    3    4   D\n13    4    1   B\n14    4    2   C\n15    4    3   A\n16    4    4   D\n17    5    1   A\n18    5    2   C\n19    5    3   D\n20    5    4   B\n\n\nIn der Abbildung 23.4 sehen wir das Ergebnis unseres Designs einmal visualisiert. Hier müssen wir wieder den Datensatz übergeben und einmal angeben, wie wir über labels die Fläche beschriften wollen. Dann müssen wir die Spalten in unserem Designdatensatz über row.factors = \"rows\" und column.factors = \"cols\" benennen, in dem die Zeilen und Spalten enthalten sind. Am Ende definieren wir noch die Blöcke von der Spalte 1 bis 4 und dann haben wir das soweit einmal fertig.\n\ndesignGGPlot(rcbd_lay, labels = \"trt\", \n             row.factors = \"rows\", column.factors = \"cols\",\n             cellalpha = 0.75,\n             blockdefinition = cbind(1, 4),\n             blocklinecolour = \"black\")\n\n\n\n\n\n\n\nAbbildung 23.4— Das randomized complete block design Design in designGGPlot() dargestellt.\n\n\n\n\n\nDamit haben wir dann auch eine Möglichkeit das experimentelle Design innerhalb von dem Paket dae zu visualisieren. Leider ist es aber so, dass die Hilfe für die Funktion designGGPlot() wirklich zu wünschen übrig lässt. Ich würde annehmen, dass wenn ich ein Objekt an eine Funktion in einem Paket übergebe, dass diese Funktionen dann miteinander gut kommunizieren. Später sehen wir, dass die Erstellung einer Visualisierung eines Split plot Designs sehr viel Recherche bedarf. Ja, es ist vieles möglich in dae, aber es verlangt auch eine tief ergreifende Lektüre der Design Notes von dae und der dortigen Beispiele. Wie immer habe ich mein Bestes versucht in den folgenden Abschnitten und Kapiteln die Visualisierung der experimentellen Designs in dae zu zeigen. Manchmal war es dann noch einfacher das Paket desplot zu nutzen. Aber das ist ja wie immer eigentliche eine Frage des Geschmacks.\nAm Ende ermöglicht das Paket dae super flexibel experimentelle Design zu erstellen. Ich werde das Paket vermutich auch für Designs im Rahmen der Analyse von linearen gemischten Modellen nutzen. Dafür ist das Paket super. Du musst aber wissen, dass das Paket einiges an Einarbeitung benötigt und du auch die Philosophie hinter den Funktionen verstehen musst. Mal eben schnell, geht leider nicht. Dafür kann das Paket dae mehr als als andere Pakete im Bereich der Erstellung von experimentellen Designs. Wenn du also mal im Bereich der Versuchsplanung arbeiten willst, dann schaue dir auf jeden Fall das Paket nochmal genauer an.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Versuchsplanung in R</span>"
    ]
  },
  {
    "objectID": "experimental-design-r.html#das-r-paket-agridat",
    "href": "experimental-design-r.html#das-r-paket-agridat",
    "title": "23  Versuchsplanung in R",
    "section": "23.7 Das R Paket {agridat}",
    "text": "23.7 Das R Paket {agridat}\nEine wunderbare Sammlung von Datensätzen aus dem Bereich der Agarwissenschaften liefert das R Paket agridat. Über die Hilfeseite agridat: Agricultural Datasets findest du dann einmal einen gesamten Überblick und auch die Informationen über einige ausgewählte Datensätze aus Dutzenden von Datensätzen. Alle Datensätze der wichtigen Bücher zu dem experimentellen Designs sind dort eigentlich enthalten und einmal kuratiert. Hier noch der Link zu agridat - Datensätze mit Abbildungen in desplot. Du musst dann auf die jeweiligen Datensätze in der Liste klicken und dann komsmt du zu dem Datensatz mit mehr Details sowie meistens auch einer Abbildung in desplot.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Versuchsplanung in R</span>"
    ]
  },
  {
    "objectID": "experimental-design-r.html#referenzen",
    "href": "experimental-design-r.html#referenzen",
    "title": "23  Versuchsplanung in R",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 23.2— Beispiel für die fehlende Einbindung von agricolae durch desplot. Da agricolae für durch die Funktion design.crd() keine Positionen in Zeile und Spalte liefert kann nur ein suboptimaler Plot erstellt werden.\nAbbildung 23.3— Das complete randomized design Design in desplot dargestellt.\nAbbildung 23.4— Das randomized complete block design Design in designGGPlot() dargestellt.\n\n\n\nOnofri A, Carbonell E, Piepho H-P, Mortimer A, Cousens R. 2010. Current statistical issues in Weed Research. Weed Research 50: 5–24.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Versuchsplanung in R</span>"
    ]
  },
  {
    "objectID": "experimental-design-basic.html",
    "href": "experimental-design-basic.html",
    "title": "24  Einfache Designs",
    "section": "",
    "text": "24.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\nset.seed(2034)\npacman::p_load(tidyverse, magrittr, dae, agricolae, FielDHub,\n               conflicted)\nconflicts_prefer(magrittr::set_names)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einfache Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-basic.html#sec-crd",
    "href": "experimental-design-basic.html#sec-crd",
    "title": "24  Einfache Designs",
    "section": "24.2 Complete randomized design (CRD)",
    "text": "24.2 Complete randomized design (CRD)\nDas komplett randomisierte Design ist sicherlich das einfachste Design. Aber auch das praktischste, wenn du ein gar nicht so komplexes Experiment hast. Im PRinzip hast du nur eine Behandlung mit unterschiedlichen Gruppen. Oder allgemeiner gesprochen, du hast einen Faktor \\(f_1\\), der deine Behandlungen beinhaltet. Du kannst viele Behandlungen haben, die dann als Level des Faktors dargestellt werden. Wir gehen jetzt das complete randomized design einmal an einem Beispiel durch. Hier einmal für eine Behandlung trt mit vier Leveln oder auch Gruppen ctrl, A, B und C sowie als Outcome das Trockengewicht drymatter. Das Outcome müssen wir gar nicht weiter beachten, es dient nur als Gedankenstütze.\nWir haben also folgendes Modell, was wir dann in einem Design abbilden wollen.\n\\[\ndrymatter \\sim \\overbrace{trt}^{f_1}\n\\]\nmit\n\ndem Faktor Behandlung trt und den vier Behandlungsstufen als Level ctrl, A, B und C.\n\nJetzt ist nur noch die Frage, wie oft wollen wir jede Behandlung wiederholen (eng. replicate). Wir entscheiden uns dafür jeweils 4 Pflanzen pro Behandlungsgruppe zu verwenden. Damit haben wir vier Wiederholungen für vier Behandlungsgruppen und somit insgesamt zwanzig Pflanzen oder auch Beobachtungen.\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nWir rechnen ein simples lineares Modell für die statistische Analyse.\n\nfit &lt;- lm(drymatter ~ trt, data = crd_tbl)\n\n\n\n\n\n\n\n\n\nRandomisierung kurz und schmerzlos\n\n\n\n\n\nWenn du für deine zwanzig Pflanzen nur ein Randomisierungmuster brauchst, dann empfehle ich dir die Folgende schnelle Art und Weise. Du nimmst die Zahlen von 1 bis 20 und mischt die Zahlen einmal mit der Funktion sample() durch. Dann erstellst du dir als dein Grid für deine Pflanzen mit einer \\(4 \\times 5\\)-Matrix und pflanzt nach diesem Grid die Pflanzen ein. Das war es dann und du bist durch.\n\n1:20 |&gt; \n  sample() |&gt; \n  matrix(nrow = 4, ncol = 5,\n         dimnames = list(str_c(\"Reihe\", 1:4, sep = \"-\"),\n                         str_c(\"Spalte\", 1:5, sep = \"-\")))\n\n        Spalte-1 Spalte-2 Spalte-3 Spalte-4 Spalte-5\nReihe-1       20        5        6       18       12\nReihe-2       11        1        8       15       10\nReihe-3        9        3       14       17       16\nReihe-4       13        2        7       19        4\n\n\n\n\n\n\n24.2.1 … mit expand_grid()\nWarum kompliziert, wenn es auch einfach geht. Wir nutzen hier einmal die Funktion expand_grid() um uns das complete randomized design zu erstellen. Die Randomisierung kommt dann über das sample() und der Pflanzenidentifikationsnummer pid. Das geht super einfach und wir brauchen auch gar keine anderen höheren Funktionen von komplexeren R Paketen. Die Spalte trt_rep wird uns später helfen die Beobachtung dann in dem Grid der Pflanzenposition auf dem Tisch wiederzufinden.\n\ncrd_long_tbl &lt;- expand_grid(trt = 1:4, rep = 1:4) |&gt; \n  mutate(trt = factor(trt, labels = c(\"ctrl\", \"A\", \"B\", \"C\")),\n         rep = factor(rep, labels = as.roman(1:4)),\n         trt_rep = str_c(trt, \".\", rep),\n         pid = sample(1:n())) |&gt;  ## Randomsierung über alles\n  select(pid, everything())\ncrd_long_tbl\n\n# A tibble: 16 × 4\n     pid trt   rep   trt_rep \n   &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt;   \n 1     8 ctrl  I     ctrl.I  \n 2     9 ctrl  II    ctrl.II \n 3     7 ctrl  III   ctrl.III\n 4    12 ctrl  IV    ctrl.IV \n 5    10 A     I     A.I     \n 6    16 A     II    A.II    \n 7    14 A     III   A.III   \n 8     4 A     IV    A.IV    \n 9     3 B     I     B.I     \n10    15 B     II    B.II    \n11     1 B     III   B.III   \n12    11 B     IV    B.IV    \n13     5 C     I     C.I     \n14    13 C     II    C.II    \n15     6 C     III   C.III   \n16     2 C     IV    C.IV    \n\n\nDann kannst du dir die Datei mit der Funktion write_xlsx() aus dem R Paket {writexl} raus schreiben und dann entsprechend mit deinen Messwerten für das Trockengewicht ergänzen.\n\ncrd_long_tbl |&gt; \n  write_xlsx(\"template_sheet.xlsx\")\n\nFür die Visualisierung fehlt uns jetzt noch die Positionsangaben auf dem Feld. Daher bauen wir uns einmal ein Grid der Reihen rows und der Spalten cols. Das Grid können wir dann mit den nach den Pflanzen-ID’s sortieren Design zusammenbringen. Damit haben wir dann eigentlich schon fast alles zusammen.\n\ncrd_plot_tbl &lt;- crd_long_tbl |&gt; \n  arrange(pid) |&gt; \n  bind_cols(expand_grid(row = 1:4, col = 1:4))\ncrd_plot_tbl\n\n# A tibble: 16 × 6\n     pid trt   rep   trt_rep    row   col\n   &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt;    &lt;int&gt; &lt;int&gt;\n 1     1 B     III   B.III        1     1\n 2     2 C     IV    C.IV         1     2\n 3     3 B     I     B.I          1     3\n 4     4 A     IV    A.IV         1     4\n 5     5 C     I     C.I          2     1\n 6     6 C     III   C.III        2     2\n 7     7 ctrl  III   ctrl.III     2     3\n 8     8 ctrl  I     ctrl.I       2     4\n 9     9 ctrl  II    ctrl.II      3     1\n10    10 A     I     A.I          3     2\n11    11 B     IV    B.IV         3     3\n12    12 ctrl  IV    ctrl.IV      3     4\n13    13 C     II    C.II         4     1\n14    14 A     III   A.III        4     2\n15    15 B     II    B.II         4     3\n16    16 A     II    A.II         4     4\n\n\nJetzt können wir uns einmal das Grid der Pflanzungen in der Abbildung 24.1 einmal anschauen. Wir nutzen hier die Möglichkeiten des R Pakets {desplot}. Hier wird es dann nochmal klarer wo die Pflanzen eigentlich hin sollen. Du kannst dir dann den Tisch so einteilen, dass eben oben links die vierte Wiederholung der Behandlung B steht. Dann steht rechts daneben die zweite Wiederholung der Behandlung C. Nach der Ernte bestimmst du dann das Trockengewicht und trägst es in die Exceltabelle ein.\n\ndesplot(data = crd_plot_tbl, \n        form = trt ~ col + row,\n        text = trt_rep, cex = 1, show.key = FALSE, \n        shorten = \"no\", flip = TRUE,\n        main = \"Pflanzschema für ein complete randomized design\")\n\n\n\n\n\n\n\nAbbildung 24.1— Schema der Pflanzungen nach den Behandlungen für das complete randomized design.\n\n\n\n\n\nDas war jetzt die schnelle Variante mit expand_grid() und der anschließenden Visualisierung mit einem Plan für die Positionen der jeweiligen Beobachtungen.\n\n\n24.2.2 … mit {FielDHub}\nFür die Erstellung des complete randomized design in der Shiny App schaue dir die Anleitung unter Completely Randomized Design mit {FielDHub} einmal an. Hier einmal der Code für die Erstellung in R. Wir haben jetzt vier Behandlungen vorliegen und wollen jeweils drei Wiederholungen anlegen. Die Wiederholungen können Blöcke, Tische oder Ställe sein. Das hängt dann vom eigentlichen Experiment ab.\n\ncrd_obj &lt;- CRD(t = c(\"ctrl\", \"A\", \"B\", \"C\"),\n               reps = 3, seed = 2023) \n\nDann können wir uns einmal das erstellte Feldbuch wiedergeben lassen, was du dann auch in Excel raus schreiben kannst. Nicht immer brauchst du alle Informationen. Zum Beispiel ist der Ort location, wo das Experiment durchgeführt wird eher zweitrangig. Du könntest dir auch zusätzliche Daten generieren lassen, aber das kannst du dir dann auch selber mit Werten ausdenken.\n\ncrd_obj$fieldBook |&gt; \n  as_tibble()\n\n# A tibble: 12 × 5\n      ID LOCATION  PLOT REP   TREATMENT\n   &lt;int&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;chr&gt;    \n 1     1        1   101 2     C        \n 2     2        1   102 2     ctrl     \n 3     3        1   103 1     C        \n 4     4        1   104 3     A        \n 5     5        1   105 1     ctrl     \n 6     6        1   106 3     C        \n 7     7        1   107 1     A        \n 8     8        1   108 3     B        \n 9     9        1   109 2     A        \n10    10        1   110 1     B        \n11    11        1   111 2     B        \n12    12        1   112 3     ctrl     \n\n\nWir müssen hier mit der Standardfunktion von {desplot} leben und können nicht das Design wählen, was aber vollkommen okay. Dann erstellen wir auch schon die Abbildung 24.2 mit dem complete randomized design. Schneller geht es nun wirklich nicht. Wenn du mehr Kontrolle über die einzelnen Schritte haben willst, dann ist das R Paket {agricolae} noch eine Alternative.\n\ncrd_obj |&gt; plot()\n\n\n\n\n\n\n\nAbbildung 24.2— Schema der Pflanzungen nach den Behandlungen für das complete randomized design.\n\n\n\n\n\n\n\n24.2.3 … mit {agricolae}\nAls Alternative bietet sich dann auch das R Paket {agricolae} an, obwohl ich für so einfaches Design nicht {agricolae} empfehlen kann. Wir haben nämlich so ein paar gewünschte Eigenschaften und damit musst du etwas programmieren um gut mit dem Paket umgehen zu können. Wir nutzen hier die Funktion design.crd() um uns das Design eines complete randomized design zu erstellen. Die Funktion benötigt einen Vektor mit den Namen der Behandlungslevel. Also in unserem Fall eben ctrl, A, B und C. Dann noch einen Vektor r mit den Wiederholungen pro Behandlungslevel. Hier ist Funktion design.crd() etwas besser, denn wir können hier auch unbalancierte Designs abbilden. Der Nachteil ist, dass wir faktisch danach keine andere Funktion mehr aus den anderen Paketen nutzen können. Deshalb ist es sinniger, balanciert zu planen und dann eben die Pflanzen später zu entfernen.\nHier einmal die simple Anwendung der Funktion zum erstellen eines complete randomized design mit vier Behandlungen und fünf Wiederholungen pro Behandlung.\n\ncrd_obj &lt;- design.crd(trt = c(\"ctrl\", \"A\", \"B\", \"C\"), \n                      r = c(5, 5, 5, 5))\n\nJetzt kommt leider der etwas aufwenidigere Teil. Das Paket {agricolae} ist schon etwas älter und damit auch teilweise nicht mehr so kompatibel und auf dem Stand der Zeit. Wir bauen uns daher jetzt noch ein paar Sachen zurecht. Am Ende wollen wie ein sauberes tibble mit anständigen Namen und einer Variable trt_rep, die uns sagt welche Behandlung und Wiederholung wir vorliegen haben.\n\ncrd_book_tbl &lt;- crd_obj |&gt; \n  pluck(\"book\") |&gt; \n  as_tibble() |&gt; \n  set_names(c(\"plots\", \"r\", \"trt\")) |&gt; \n  mutate(trt_rep = str_c(trt, \".\", r))\ncrd_book_tbl\n\n# A tibble: 20 × 4\n   plots     r trt   trt_rep\n   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  \n 1   101     1 C     C.1    \n 2   102     1 B     B.1    \n 3   103     2 C     C.2    \n 4   104     3 C     C.3    \n 5   105     4 C     C.4    \n 6   106     2 B     B.2    \n 7   107     5 C     C.5    \n 8   108     1 ctrl  ctrl.1 \n 9   109     3 B     B.3    \n10   110     2 ctrl  ctrl.2 \n11   111     4 B     B.4    \n12   112     1 A     A.1    \n13   113     5 B     B.5    \n14   114     3 ctrl  ctrl.3 \n15   115     4 ctrl  ctrl.4 \n16   116     2 A     A.2    \n17   117     3 A     A.3    \n18   118     4 A     A.4    \n19   119     5 A     A.5    \n20   120     5 ctrl  ctrl.5 \n\n\nSchön ist was anderes, da hast du recht, aber wir können uns dann auch noch das Pflanzgrid einmal als Abbildung wiedergeben lassen. Da die Ausgabe der Funktion design.crd() schon permutiert ist, müssen wir hier nur die Informationen zu den Zeilen und Spalten ergänzen. Im Prinzip kannst du entscheiden, wie du die Wiederholungen cols oder Behandlungen rows anordnest. Du kannst am Ende aber auch einfach den ausgedruckten Zettel drehen.\n\ncrd_plot_tbl &lt;- crd_book_tbl |&gt;\n  bind_cols(expand_grid(rows = 1:4,\n                        cols = 1:5))\n\nIn der Abbildung 24.3 siehst du dann einmal das Schema für die Pflanzungen eines complete randomized design. Das Schema unterscheidet sich so nicht von dem Eigenbau mit expand_grid(). Da bist du frei zu entscheiden was du nutzen möchtest.\n\ndesplot(trt ~ rows + cols, flip = TRUE,\n        text = trt_rep, cex = 1, shorten = \"no\",\n        data = crd_plot_tbl,\n        main = \"Pflanzschema für ein complete randomized design\", \n        show.key = FALSE) \n\n\n\n\n\n\n\nAbbildung 24.3— Schema der Pflanzungen nach den Behandlungen für das complete randomized design.\n\n\n\n\n\nDamit wären wir auch schon durch. Wir haben in R im Prinzip die Möglichkeiten das complete randomized design selber, mit dem R Paket {FielDHub}oder aber mit dem R Paket {agricolae} zu erstellen.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einfache Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-basic.html#sec-rcbd",
    "href": "experimental-design-basic.html#sec-rcbd",
    "title": "24  Einfache Designs",
    "section": "24.3 Randomized complete block design (RCBD)",
    "text": "24.3 Randomized complete block design (RCBD)\nJetzt kommen wir zum vermutlich meist genutzten Design, dem randomized complete block design. Hier haben wir neben dem Faktor Behandlung trt auch noch einen zusätzlichen Faktor block. Wir haben dann für jede Behandlung/Block-Kombination einen Messwert. Wenn du am Feld stehst, ist es aber meistens so, dass wir nicht eine Pflanze pro Block haben, sondern 20 oder mehr Pflanzen. Wir mitteln aber den Wert für das Trockengewicht über alle 20 Pflanzen pro Block und haben dann wieder einen Messwert für jede Behandlung/Block-Kombination. Das ist manchmal etwas verwirrend.\nWir haben also folgendes Modell vorliegen, wir habend den Faktor Behandlung trt sowie den Faktor Block block. Für jede der Behandlung/Block-Kombination erhalten wir dann einen Wert. Wir schauen uns gleich vier Behandlungen und vier Blöcke an.\n\\[\ndrymatter \\sim \\overbrace{trt}^{f_1} + \\underbrace{block}_{f_2}\n\\]\nmit\n\ndem Faktor Behandlung trt und den vier Behandlungsstufen als Level ctrl, A, B und C.\ndem Faktor Block block und den vier Leveln I bis IV.\n\nWir randomiseren unsere Behandlungen trt in den Blöcken block. Somit ergibt sich Abbildung 24.4 in der unsere Behandlungen in den Blöcken genestet sind.\n\n\n\n\n\n\nflowchart LR\n    A(trt):::factor --- B(((nested))) --&gt; C(block):::factor\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 24.4— Abhängigkeitsstruktur eines randomized complete block design mit einem Faktor trt nested im Faktor block.\n\n\n\n\n\nDamit gilt dann auch, dass jeder Block alle Behandlungslevel enthält und die Behandlungen innerhalb eines Blockes randomsiert werden. Unsere Randomisierungseinheit ist also der Block.\n\\[\n\\underbrace{\\mbox{Blöcke}}_{f_2} \\xrightarrow[]{beinhaltet\\; alle} \\overbrace{\\mbox{Behandlungen}}^{f_1}\n\\]\nTheoretisch könnten wir auch pro Block mehrere einzelne Pflanzen messen, dann sind wir aber im Subsampling drin. Wenn du mehr zum Design erfahren möchtest, dann schau einfach mal bei Abschnitt zum subsampling im nächsten Kapitel rein.\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nWir rechnen ein multiples lineares Modell für die statistische Analyse.\n\nfit &lt;- lm(drymatter ~ trt + block + trt:block, data = rcbd_tbl)\n\n\n\n\n\nHier einmal der Link zu der Auswertung zum Subsampling bei DSFAIR.\n\n24.3.1 … mit expand_grid()\nDas praktische an expand_grid() ist, dass du nochmal eine Vorstellugn über den Aufbau des Experiments erhälst. Wir nehmen als erstes den Block block und ergänzen dann die Behandlungen trt. Damit erschaffen wir uns erst die vier Blöcke und dann pro Block die vier Behandlungen. Dann müssen wir noch unsere Daten etwas mutieren und schöner machen. Wir ergänzen noch einen Identifikator trt_block damit wir auch unsere Positionen für die Pflanzungen später in den Abbildungen wieder finden. Am Ende gruppieren wir die Daten nochmal pro Block damit wir dann innerhalb der Blöcke unsere Behandlungslevel über die Funktion sample() randomsisiern können.\n\nrcbd_long_tbl &lt;- expand_grid(block = 1:4, \n                             trt = 1:4) |&gt; \n  mutate(trt = factor(trt, labels = c(\"ctrl\", \"A\", \"B\", \"C\")),\n         block = factor(block, labels = as.roman(1:4)),\n         trt_block = str_c(trt, \".\", block),\n         pid = 1:n()) |&gt; \n  group_by(block) |&gt; \n  mutate(trt = sample(trt)) ## Randomisierung pro Block\n\n\nrcbd_plot_tbl &lt;- rcbd_long_tbl |&gt; \n  bind_cols(expand_grid(rows = 1:4, cols = 1:4))\n\nDie dicken, schwarzen Linien stellen die Grenzen der Blöcke dar. Im realen Leben kann eine farbige Kachel in der Abbildung ein ganzes Feld sein und die Blockgrenzen sind dann eben Feldwege. Die Abbildung 24.5 stellt ja auch nur eine schematische Darstellung der Pflanzungen dar.\n\ndesplot(data = rcbd_plot_tbl, \n        form = trt ~ rows + cols,\n        out1 = block,\n        main = \"Pflanzschema für ein randomized complete block design\", \n        text = trt_block, cex = 1, show.key = FALSE, shorten = \"no\")\n\n\n\n\n\n\n\nAbbildung 24.5— Schema der Pflanzungen nach den Behandlungen für das randomized complete block design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\n\n\n\n\n\nDann können wir auch schon unseren Versuchsplan als Exceldatei rausschreiben. Wir können dann noch weitere Spalten als Outcome ergänzen und dann mit der Datei unsere Auswertung durchführen.\n\nrcbd_long_tbl |&gt; \n  write_xlsx(\"template_sheet.xlsx\")\n\n\n\n24.3.2 … mit {FielDHub}\nFür die Erstellung des randomized complete block design in der Shiny App schaue dir die Anleitung wie schon weiter oben unter Generates a Randomized Complete Block Design (RCBD) mit {FielDHub} einmal an. Hier einmal der Code für die Erstellung eines randomized complete block design in R mit Code. Wir haben jetzt vier Behandlungen vorliegen und wollen jeweils drei Wiederholungen anlegen, aber das kennst du ja schon von dem obigen Beispiel. Die Wiederholungen können Blöcke, Tische oder Ställe sein. Das hängt dann wiederum vom eigentlichen Experiment ab. Die Funktion ist daher dann auch schon fast selbsterklärend\n\nrcbd_obj &lt;- RCBD(t = c(\"ctrl\", \"A\", \"B\", \"C\"),\n                 reps = 3, seed = 2023) \n\nWir können uns dann auch wieder das Feldbuch wiedergeben lassen und es eben dann auch in Excel als .xlsx exportieren und so dann direkt mit den Daten aus unserem Experiment füllen oder als Randomisierung nutzen.\n\nrcbd_obj$fieldBook |&gt; \n  as_tibble()\n\n# A tibble: 12 × 5\n      ID LOCATION  PLOT   REP TREATMENT\n   &lt;int&gt; &lt;fct&gt;    &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1     1 loc1       101     1 B        \n 2     2 loc1       102     1 ctrl     \n 3     3 loc1       103     1 A        \n 4     4 loc1       104     1 C        \n 5     5 loc1       201     2 B        \n 6     6 loc1       202     2 A        \n 7     7 loc1       203     2 C        \n 8     8 loc1       204     2 ctrl     \n 9     9 loc1       301     3 B        \n10    10 loc1       302     3 A        \n11    11 loc1       303     3 C        \n12    12 loc1       304     3 ctrl     \n\n\nIn der Abbildung 24.6 siehst du einmal das Ergebnis der Randomisierung in {FielDHub}. Hier kannst du dann noch mit den Einstellungen rumspielen und dir die verschiedenen Ergebnisse einmal anschauen. Ehrlich gesagt funktioniert das natürlich in der Shiny App um Längen besser.\n\nrcbd_obj |&gt; plot()\n\n\n\n\n\n\n\nAbbildung 24.6— Schema der Pflanzungen nach den Behandlungen für das randomized complete block design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\n\n\n\n\n\n\n\n24.3.3 … mit {agricolae}\nWir können auch ein randomized complete block design mit {agricolae} erstellen. Hier geht es ähnlich wie vorher schon. Wichtig ist, dass wir hier unsere Blöcke wieder mit r bezeichnen. Damit bauen wir uns also wieder ein Design mit vier Behandlungsleveln und fünf Blöcken.\n\nrcbd_obj &lt;- design.rcbd(trt = c(\"ctrl\", \"A\", \"B\", \"C\"), r = 5)\n\nLeider ist die Ausgabe von design.rcbd() etwas ungünstig für die weitere Darstellung. Deshalb wandeln wir etwas das Feldbuch für unsere Bedürfnisse um. Wir könnten natürlich gleich mit dem Feldbuch weitermachen und die Pflanzen in der Form ausbringen, aber wir wollen ja noch ein Pflanzschema erstellen.\n\nrcbd_book_tbl &lt;- rcbd_obj |&gt; \n  pluck(\"book\") |&gt; \n  as_tibble() |&gt; \n  set_names(c(\"plots\", \"block\", \"trt\"))\nrcbd_book_tbl\n\n# A tibble: 20 × 3\n   plots block trt  \n   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n 1   101 1     A    \n 2   102 1     B    \n 3   103 1     ctrl \n 4   104 1     C    \n 5   201 2     B    \n 6   202 2     A    \n 7   203 2     C    \n 8   204 2     ctrl \n 9   301 3     C    \n10   302 3     B    \n11   303 3     A    \n12   304 3     ctrl \n13   401 4     C    \n14   402 4     A    \n15   403 4     ctrl \n16   404 4     B    \n17   501 5     B    \n18   502 5     A    \n19   503 5     C    \n20   504 5     ctrl \n\n\nJetzt nochmal das Grid für die Pflanzungen ergänzt. Hier müssen wir etwas mehr schauen, wie wir die Zeilen rows und Spalten cols zuordnen. Unsere Zeilen sind die Blöcke, aber innerhalb der Blöcke müssen wir die Anzahl der Behandlungen nummerieren. Deshalb machen wir den Zwischenschritt mit group_by().\n\nrcbd_plot_tbl &lt;- rcbd_book_tbl |&gt;\n  mutate(rows = as.numeric(block),\n         trt_block = str_c(trt, \".\", block)) |&gt; \n  group_by(rows) |&gt; \n  mutate(cols = 1:n())\n\nZur Kontrolle einmal die Ausgabe des skretch. Auch hier steht wieder nichts an den Zeilen und Spalten, so dass wir immer überlegen müssen, was war nun der Block und was war die Behandlung.\n\nrcbd_obj |&gt; \n  pluck(\"sketch\") |&gt; \n  t()\n\n     [,1]   [,2]   [,3]   [,4]   [,5]  \n[1,] \"A\"    \"B\"    \"C\"    \"C\"    \"B\"   \n[2,] \"B\"    \"A\"    \"B\"    \"A\"    \"A\"   \n[3,] \"ctrl\" \"C\"    \"A\"    \"ctrl\" \"C\"   \n[4,] \"C\"    \"ctrl\" \"ctrl\" \"B\"    \"ctrl\"\n\n\nUm es schöner zu haben schauen wir uns einmal die Abbildung 24.7 als Pfalnzschema an. Wir haben hier die gleiche Ausgabe wie auch schon weiter oben. Die Positionen haben sich geändert, wir haben ja auch zweimal randomisiert. Sonst sehen wir aber das Gleiche. Die schwarze Striche sind wieder die Grenzen unserer Blöcke und können im echten Leben dann Feldwege oder Gewächshäuser sein.\n\ndesplot(trt ~ rows + cols, flip = TRUE,\n        text = trt_block, cex = 1, shorten = \"no\",\n        out1 = block,\n        data = rcbd_plot_tbl,\n        main = \"Pflanzschema für ein randomized complete block design\", \n        show.key = FALSE, key.cex = 0.5)\n\n\n\n\n\n\n\nAbbildung 24.7— Schema der Pflanzungen nach den Behandlungen für das randomized complete block design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einfache Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-basic.html#sec-lsd",
    "href": "experimental-design-basic.html#sec-lsd",
    "title": "24  Einfache Designs",
    "section": "24.4 Latin square design (LSD)",
    "text": "24.4 Latin square design (LSD)\nDas latin square design erweist sich dann als besonders sinnvoll, wenn zwei unabhängige Störgradienten vorhanden sind, die orthogonal zueinander ausgerichtet sind. Ein Beispiel hierfür wäre im Boden des Versuchsfeldes ein Feuchtigkeitsgefälle von Nord nach Süd und ein Nährstoffgefälle von Ost nach West. Um diese Störgrößen im Boden zu berücksichtigen, werden die Randomisationseinheiten in Form von Quadraten angeordnet. Diese Anordnung stellt sicher, dass sowohl die Anzahl der Zeilen als auch die Anzahl der Spalten zwangsläufig gleich sind. Durch diese Anordnung ist es möglich, eine Randomisierung durchzuführen, bei der sowohl jede Spalte als auch jede Zeile einen vollständigen Block bildet, ähnlich wie bei einem Sudoku-Raster. Es darf eben in jeder Zeile und Spalte immer nur einmal eine Behandlung auftauchen.\n\n\nDas Bild des ersten Latin Square an einem Hang, angelegt und geplant von Roland Fisher an der Rothamsted Experimental Station.\nAuch findest du im Chapter 2 Designing experiments aus Experimental methods in agriculture noch schöne Beispiele für ein Latin Square Design.\nSchauen wir uns einmal das Modell an nachdem wir unseren Versuchsplan bauen wollen. Wir haben wie immer unsere Behandlung trt sowie den Block, den wir aber in rows und cols aufteilen. Die Regel lautet im latin square design, dass keine Behandlung in der gleichen Zeile und Spalte doppelt auftreten darf.\n\\[\ndrymatter \\sim \\overbrace{trt}^{f_1} + \\underbrace{rows}_{f_2} + \\overbrace{cols}^{f_3}\n\\]\nmit\n\ndem Faktor Behandlung trt und den fünf Behandlungsstufen als Level ctrl, A, B, C und D.\ndem Faktor Reihe rows mit der Anzahl der Level gleich der Level der Behandlung also hier fünf.\ndem Faktor Spalte cols mit der Anzahl der Level gleich der Level der Behandlung also hier fünf.\n\nIn der Abbildung 24.8 sehen wir nochmal den schematischen Aufbau eines latin square design dargestellt. Vermutlich hilft es auch sich einmal das finale Design in der Abbildung 24.10 einmal anzuschauen. Wichtig ist, dass wir den Block gar nicht betrachten. Wir haben hier eher fünfundzwanzig kleine Subblöcke, die wir durch die Position der rows und cols definieren.\n\n\n\n\n\n\nflowchart LR\n    A(trt):::factor --- B(((nested))) --&gt; C(rows):::factor \n    B(((nested))) --&gt; D(cols):::factor \n    C --- F(block)\n    D --- F\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 24.8— Schematische Darstellung der Abhängigkeitsstruktur im latin suare design. Die Behandlungen werden in rows und cols genestet, die einem quadratischen Block mit den Längen der Anzahl der Level der Behandlungen entsprechen.\n\n\n\n\n\nWir schauen uns hier das R Paket FieldHub, das R Paket {agricolae} und das R Paket {dae} an. Selber machen ist hier nicht, da du dir bei der Erstellung sehr schnell Fehler rein holst. Es ist super frickelig darauf zu achten, dass sich wirklich keine Behandlung in irgendeiner Zeilen- oder Spaltenposition überschneidet. Da sind die vorgefertigten Funktionen echt mal besser.\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nWir rechnen ein multiples lineares Modell für die statistische Analyse.\n\nfit &lt;- lm(drymatter ~ trt + cols + rows, data = lsd_tbl)\n\n\n\n\n24.4.1 … mit {FielDHub}\nFür die Erstellung des latin square design in der Shiny App schaue dir die Anleitung wie schon weiter oben unter Generates a Latin Square Design mit {FielDHub} einmal an. Hier einmal der Code für die Erstellung eines latin square design in R mit Code. Wir haben jetzt vier Behandlungen vorliegen, aber das kennst du ja schon von dem obigen Beispiel. Wir können hier einmal zwei Wiederholungen angeben, was dann eigentlich Blöcke wären. Du. kannst dir die Wiederholungen als zusätzliche Felder, Ställe oder Tische vorstellen.\n\nlatin_square_obj &lt;- latin_square(t = 4, reps = 2,\n                                 seed = 2023) \n\nDamit können wir uns auch schon einmal das Feldbuch anschauen. Auch hier gilt, das Feldbuch lässt sich dann einfach nach Excel exportieren. Hier zeige ich nur die ersten zehn Zeilen, da sonst die Ausgabe zu groß wird.\n\nlatin_square_obj$fieldBook |&gt; \n  as_tibble() |&gt; \n  head(10)\n\n# A tibble: 10 × 7\n      ID LOCATION  PLOT SQUARE ROW   COLUMN   TREATMENT\n   &lt;int&gt;    &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;fct&gt; &lt;fct&gt;    &lt;chr&gt;    \n 1     1        1   101      1 Row 1 Column 1 T4       \n 2     2        1   102      1 Row 1 Column 2 T1       \n 3     3        1   103      1 Row 1 Column 3 T3       \n 4     4        1   104      1 Row 1 Column 4 T2       \n 5     5        1   108      1 Row 2 Column 1 T1       \n 6     6        1   107      1 Row 2 Column 2 T4       \n 7     7        1   106      1 Row 2 Column 3 T2       \n 8     8        1   105      1 Row 2 Column 4 T3       \n 9     9        1   109      1 Row 3 Column 1 T3       \n10    10        1   110      1 Row 3 Column 2 T2       \n\n\nIn der folgenden Abbildung 24.9 siehst du dann einmal das latin square design für zwei Wiederholungen. Wie du gleich in den folgenden Beispielen siehst, kann man das latin square design auch nur für eine Wiederholung erstellen. Wichtig ist eben, dass in jeder Zeilen- und Spaltenkombination eine Behandlung vorkommt.\n\nlatin_square_obj |&gt; plot()\n\n\n\n\n\n\n\nAbbildung 24.9— Schema der Pflanzungen nach den Behandlungen für das latin square design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\n\n\n\n\n\n\n\n24.4.2 … mit {agricolae}\nIm latin square design brauchen wir nur einen Behandlungsvektor. Durch die Anzahl der Behandlungen wird auch die Größe des latin square bestimmt. Wir haben hier fünf Behandlungen vorliegen, also kriegen wir dann auch ein \\(5 \\times 5\\)-latin square für unseren Versuchsplan raus.\n\nlsd_obj &lt;- design.lsd(trt = c(\"ctrl\", \"A\", \"B\", \"C\", \"D\"), \n                      seed = 42)\n\nZum Glück kriegen wir hier auch die Zeilen und Spalten direkt von der Funktion design.lsd() geliefert. Leider als Faktoren, so dass wir die Faktoren nochmal in Zahlen umwandeln damit es später mit der Visualisierung klappt.\n\nlsd_book_tbl &lt;- lsd_obj |&gt; \n  pluck(\"book\") |&gt; \n  as_tibble() |&gt; \n  set_names(c(\"plots\", \"rows\", \"cols\", \"trt\")) |&gt; \n  mutate(rows = as.numeric(rows),\n         cols = as.numeric(cols))\nlsd_book_tbl\n\n# A tibble: 25 × 4\n   plots  rows  cols trt  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;\n 1   101     1     1 D    \n 2   102     1     2 C    \n 3   103     1     3 ctrl \n 4   104     1     4 B    \n 5   105     1     5 A    \n 6   201     2     1 C    \n 7   202     2     2 B    \n 8   203     2     3 D    \n 9   204     2     4 A    \n10   205     2     5 ctrl \n# ℹ 15 more rows\n\n\nDann einmal die Ausgabe des sketch. Hier kann man mal nichts falsch machen. Wir kriegen unser Quadrat als latin square und gut ist.\n\nlsd_obj |&gt; \n  pluck(\"sketch\") |&gt; \n  t()\n\n     [,1]   [,2]   [,3]   [,4]   [,5]  \n[1,] \"D\"    \"C\"    \"ctrl\" \"B\"    \"A\"   \n[2,] \"C\"    \"B\"    \"D\"    \"A\"    \"ctrl\"\n[3,] \"ctrl\" \"D\"    \"A\"    \"C\"    \"B\"   \n[4,] \"B\"    \"A\"    \"C\"    \"ctrl\" \"D\"   \n[5,] \"A\"    \"ctrl\" \"B\"    \"D\"    \"C\"   \n\n\nIn der Abbildung 24.10 siehst du dann nochmal das latin square design visualisiert. Die dicken Linien sind wieder die Blockgrenzen und mögen dann die Feldwege oder Wege zwischen den Behandlungens sein. Wie du siehst, gibt es pro Zeile und Spalte immer nur eine Behandlung. Wir haben einmal perfekt über das ganze latin square randomisiert.\n\ndesplot(trt ~ rows + cols, flip = TRUE,\n        out1 = rows, out1.gpar = list(col=\"black\", lwd=3),\n        out2 = cols, out2.gpar = list(col=\"black\", lwd=3),\n        text = trt, cex = 1, shorten = \"no\",\n        data = lsd_book_tbl,\n        main = \"Pflanzschema für ein latin square design\", \n        show.key = FALSE, key.cex = 0.5)\n\n\n\n\n\n\n\nAbbildung 24.10— Schema der Pflanzungen nach den Behandlungen für das latin square design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\n\n\n\n\n\nDann nochmal das latin square design rausschreiben und wir können mit der Datenerfassung beginnen.\n\nlsd_book_tbl |&gt; \n  select(trt, rows, cols) |&gt; \n  write_xlsx(\"template_sheet.xlsx\")\n\n\n\n24.4.3 … mit {dae}\nDas latin square design in dem R Paket {dae} ist etwas verkompliziert herzustellen. Das Paket {dae} ist eher für komplexere Designs gedacht, so dass simple Designs manchmal übermäßig kompliziert wirken. Wir übergeben erstmal unser Positionsgrid, natürlich ein \\(5 \\times 5\\)-Grid, aus Zeilen und Spalten. Dann ergänzen wir unsere fünf Behandlungen in einem latin square design. Das Ganze ist irgendwie dreifach, aber da wir auch komplexeres können, machen wir das mal so und gehen hier nicht tiefer drauf ein.\n\nlsd_sys &lt;- cbind(fac.gen(list(rows = 5, cols = 5)),\n                 trt = factor(designLatinSqrSys(5), labels = LETTERS[1:5]))\n\nWenn wir das grundlegende Objekt lsd_obj mit den Spalten, Zeilen und Behandlungen erstellt haben, können wir die Zuordnung randomisieren. Dabei wollen wir die Behandlungen trt auf die Zeilen rows und Spalten cols randomisieren. Deshalb ist sind die Behandlungen in der Option allocated sowie die Zeilen und Spalten in recipient.\n\nlsd_lay &lt;- designRandomize(allocated = lsd_sys[\"trt\"],\n                           recipient = lsd_sys[c(\"rows\", \"cols\")],\n                           seed = 141)\n\nWir können dann uns auch in Abbildung 24.11 die Ausgabe des latin square design anschauen. Prinzipiell gibt es hier keinen Unterschied zu dem R Paket {agricolae}. Es ist dann eher eine Geschmacksfrage, wie die Darstellung aussehen soll. Meiner Meinung nach ist die Erstellung in {dae} viel zu kompiliziert im Vergleich zu den anderen R Paketen.\n\ndesignGGPlot(lsd_lay, labels = \"trt\", cellalpha = 0.75,\n             row.factors = \"rows\", column.factors = \"cols\",\n             blockdefinition = cbind(1, 1),\n             blocklinecolour = \"black\", \n             title = \"Pflanzschema für ein latin square design\")\n\n\n\n\n\n\n\nAbbildung 24.11— Schema der Pflanzungen nach den Behandlungen für das latin square design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einfache Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-basic.html#spezialfälle",
    "href": "experimental-design-basic.html#spezialfälle",
    "title": "24  Einfache Designs",
    "section": "24.5 Spezialfälle",
    "text": "24.5 Spezialfälle\nIm Folgenden schauen wir uns noch die Generierung von zwei Spezialfällen an. Beide sind im Prinzip wiederum zweifaktorielle Versuchspläne mit einer Behandlung und entsprechenden Wiederholungen in Blöcken. Beide sind aber etwas spezieller beim Aufbau, deshalb hier nur am Rande. Die Auswertung verlangt dann natürlich auch wieder etwas mehr Modellierung.\n\nAlpha design\nAugmented design\n\nWir schauen uns einmal für diese Designs die Implementierung in {agricolae} sowie in {FieldHub} an. Selber machen ist hier etwas aufwendig und hier nutzen wir dann mal wirklich ein R Paket, dass macht es uns dann einfacher im Erstellen.\n\n24.5.1 Alpha design\nDer Alpha-Versuchsplan ist dafür gedacht ein Versuchsplan mit unvollständigen Blöcken abzubilden. Die Blöcke sind dann aber wiederum zu vollständigen Wiederholungen gruppiert. Das heißt, in jeder Wiederholung findest du alle Blöcke wieder. Solche Versuchspläne werden dann als auflösbar bezeichnet, also als berechenbar. Das klingt jetzt etwas komplizierter als es eiigentlich ist. Wie haben auch in diesem Beispiel wieder fünfzehn genetische Linien vorliegen. Wir haben aber in jedem unserer Blöcke nur Platz drei dieser Linien abzubilden. Das heißt also pro Block nehmen wir drei Linien damit kommen wir auf fünf Blöcke um alle Linien abzubilden. Diese fünf Blöcke werden dann in drei Wiederholungen randomisiert, so dass wir auf fünfzehn Blöcke kommen.\nOkay, das klingt jetzt echt wirr. Schauen wir uns dazu einmal das Schema der Abhängigkeitsstruktur in der Abbildung 24.12 an. Wir haben also unsere Behandlung trt_gen mit fünzehn Leveln entsprechend der fünfzehn genetischen Linien. Wir nehmen jetzt zufällig drei Linien heraus und packen diese drei Linien in einen Block inc_block. Das machen wir fünf mal, da wir \\(15/3 = 5\\) Blöcke brauchen. Wir generieren incomplete blocks. Dann kommen alle incomplete blocks in eine Wiederholung rep. In einer Wiederholung sind alle genetischen Linien dann enthalten.\n\n\n\n\n\n\nflowchart LR\n    A(trt_gen):::factor --&gt; B & C & D & E & F\n    B[(\"G_10 \n        G_4\n        G_8\")]:::level -.- G(((nested))) -.-&gt; L(inc_block):::inc_block\n    C[(\"G_3\n        G_15\n        G_9\")]:::level -.- H(((nested))) -.-&gt; L\n    D[(\"G_2 \n        G_11\n        G_1\")]:::level -.- I(((nested))) -.-&gt; L\n    E[(\"G_13\n        G_12\n        G_7\")]:::level -.- J(((nested))) -.-&gt; L\n    F[(\"G_6 \n        G_5\n        G_14\")]:::level -.- K(((nested))) -.-&gt; L\n    L --- M(((nested))) --&gt; N(Wiederholung):::factor\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef level fill:#E69F00,stroke:#333,stroke-width:0.75px\n    classDef inc_block fill:#CC79A7,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 24.12— Schematische Darstellung der Abhängigkeitsstruktur im Alpha-Versuchsplan. Die Level der Behandlung werden zufällig auf incomplete blocks aufgeteilt und diese Blöcke dann vollständig in Wiederholungen randomisiert.\n\n\n\n\n\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nWir rechnen ein multiples lineares Modell für die statistische Analyse.\n\nfit &lt;- lm(drymatter ~ trt + rep + rep:inc_block, data = alpha_tbl)\n\nOder können überlegen ein gemischtes Modell zu rechnen.\n\nfit &lt;- lmer(drymatter ~ trt + rep + (1 | rep:inc_block), data = alpha_tbl)\n\nDie Entscheidung kannst du dann mit der Modellselektion durchführen.\n\n\n\n\nAuch hier hilft die Seite DSFAIR - Alpha design mit einer beispielhaften Auswertung eventuell weiter. Beispielhafte Daten und Auswertung sind dort vorhanden.\nDeshalb brauchen wir erstmal unsere fünfzehn Linien trt_gen und unsere drei Wiederholungen n_rep sowie eine Größe von unseren Blöcken size_block von drei.\n\ntrt_gen &lt;- str_c(\"G\", 1:15)  \nn_trt_gen &lt;- n_distinct(trt_gen)\nn_rep &lt;- 3\nsize_block &lt;- 3\n\nLeider sind die Namen der Optionen etwas unglücklich gewählt. Zwar macht es Sinn die Namen etwas gleich zu halten, aber manchmal macht es die Sache für die Anwendung echt nicht einfacher. Hier ist also k die Größe der Blöcke und r die Anzahl der Wiederholungen. Es funktionieren nicht alle möglichen Kombinationen, im Zweifel bitte vorher hier mal rumspielen, bevor der Versuch angelegt wird oder schon Anzahlen bestimmt werden.\n\nalpha_obj &lt;- design.alpha(trt = trt_gen,\n                          k = size_block,\n                          r = n_rep,\n                          seed = 42)\n\n\nAlpha Design (0,1) - Serie  II \n\nParameters Alpha Design\n=======================\nTreatmeans : 15\nBlock size : 3\nBlocks     : 5\nReplication: 3 \n\nEfficiency factor\n(E ) 0.7 \n\n&lt;&lt;&lt; Book &gt;&gt;&gt;\n\n\nWie immer müssen wir uns noch die Zeilen und Spalten selber erschaffen, damit wir das alpha design auch abbilden können. Hier ist wichtig, dass wir dann noch die Wiederholung haben, die kein Block ist. Pro Wiederholung haben wir eben fünf unvollständige Blöcke mit drei Linien vorliegen.\n\nalpha_book_tbl &lt;- alpha_obj$book |&gt; \n  mutate(replication = str_c(\"Wiederholung \", replication),\n         rows = as.numeric(block),\n         cols = as.numeric(cols))\n\nIn der Abbildung 24.13 siehst du dann das Ergebnis der Versuchsplanung. Wir haben unsere drei Wiederholungen und in den drei Wiederholungen dann jeweils fünf Blöcke randomisiert. Pro Block finden sich dann drei Linien. Unsere Blöcke sind damit unvollständig. Für mich ist das alpha design eins der schwierigeren Designs, weil man hier zwischen den Wiederholungen und den Blöcken unterscheiden muss. Das führt manchmal zu Verwirrung was randomisert wird.\n\ndesplot(block ~ rows + cols | replication, flip = TRUE,\n        out1 = replication,\n        out2 = block, out2.gpar = list(col = \"black\", lty = 3), \n        text = trt_gen, cex = 1, shorten = \"no\",\n        data = alpha_book_tbl, \n        main = \"Pflanzschema für ein alpha design\", \n        show.key = FALSE)\n\n\n\n\n\n\n\nAbbildung 24.13— Schema der Pflanzungen nach den Behandlungen für das alpha design.\n\n\n\n\n\nDann nochmal das alpha design als Feldbuch rausschreiben und wir können mit der Datenerfassung beginnen. Auch hier hiulft es sich nochmal ein Beispieloutcome auszudenken und in die Tabelle als Spalte einzufügen. Dann kannst du schauen, ob es so passen könnte mit der Datenerhebung.\n\nlsd_book_tbl |&gt; \n  select(replication, trt_gen, inc_block = block, rows, cols) |&gt; \n  write_xlsx(\"template_sheet.xlsx\")\n\nFür die Erstellung des alpha design in der Shiny App schaue dir die Anleitung wie schon weiter oben unter Generates an Alpha Design mit {FielDHub} einmal an. Hier fängt es wirklich an sinnvoll zu sein, eine Shiny App zu haben. So kannst du viel besser mit den Parametern des alpha designs rumspielen und sehen was für dich passen würde. Hier einmal der Code für die Erstellung eines alpha design in R mit Code. Wir haben jetzt neun Behandlungen vorliegen, aber das kennst du ja schon von dem obigen Beispiel in {agricolae}. Auch hier haben wir dann drei Blöcke vorliegen und drei Wiederholungen. Damit erklärt sich der Code wir oben auch schon.\n\nalpha_obj &lt;- alpha_lattice(t = 9,\n                           k = 3,\n                           r = 3,\n                           seed = 2023)\n\nDann können wir uns auch hier sehr einfach das Feldbuch rausschrieben. Wir nutzen dann den gleichen Code wie oben für das rausschrieben nach Excel.\n\nalpha_obj$fieldBook |&gt; \n  as_tibble() |&gt; \n  head(10)\n\n# A tibble: 10 × 8\n      ID LOCATION  PLOT   REP IBLOCK  UNIT ENTRY TREATMENT\n   &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1     1 1          101     1      1     1     6 G-6      \n 2     2 1          102     1      1     2     1 G-1      \n 3     3 1          103     1      1     3     8 G-8      \n 4     4 1          104     1      2     1     5 G-5      \n 5     5 1          105     1      2     2     3 G-3      \n 6     6 1          106     1      2     3     7 G-7      \n 7     7 1          107     1      3     1     4 G-4      \n 8     8 1          108     1      3     2     2 G-2      \n 9     9 1          109     1      3     3     9 G-9      \n10    10 1          201     2      1     1     4 G-4      \n\n\nIn der Abbildung 24.14 siehst du dann einmal das alpha design in {FielDHub} dargestellt und hier gefällt mir dann die Abbildung überhaupt nicht mehr. Die Zahlen sind kaum zu lesen. Daher würde ich hier dann das erstellte Feldbuch alpha_obj$fieldBook nochmal selber in {desplot} darstellen und rausschreiben. So ist wirklich schwer zu lesen, was was sein soll. Die Farbwahl ist in {FielDHub} etwas eingeschränkt.\n\nalpha_obj |&gt; plot()\n\n\n\n\n\n\n\nAbbildung 24.14— Schema der Pflanzungen nach den Behandlungen für das alpha design.\n\n\n\n\n\n\n\n24.5.2 Augmented design\nDas augmened design basiert darauf, dass wir neben den eigentlichen Behandlungen noch Standardbehandlungen vorliegen haben. Die Standardbehandlungen randomisieren wir voll auf die Blöcke, wohingegen die anderen Behandlungen unvollständig randomisiert werden. In unserem Beispiel haben wir jetzt fünfzehn genetische Linien G1 bis G15 in dem Behandlungsfaktor trt_gen sowie die Standardbehandlung Std_A und Std_B vorliegen. Wir werden dann drei Blöcke bilden und in jeder der Blöcke wird jeweils die Standardbehandlung Std_A und Std_B sowie eine Auswahl an genetischen Linien enthalten sein.\nKlingt jetzt auch wieder sehr viel wirrer als es am Ende ist. Deshalb hier erstmal das Modell, wir haben auch hier im Prinzip nur zwei Faktoren vorliegen. Wir haben einmal den Faktor Behandlung trt_gen sowie den Faktor Block zu erstellen.\n\\[\ndrymatter \\sim \\overbrace{trt_{gen}}^{f_1} + \\underbrace{inc\\_{block}}_{f_2}\n\\]\nmit\n\ndem Faktor Behandlung trt und den fünzehn genetischen Linien als Level G_1 bis G_15 und die beiden Standardbehandlungen Std_A und Std_B.\ndem Faktor Block inc_block und den vier Leveln I bis III.\n\nIn dem augmened design dreht sich aber alles um die Zuweisung der Behandlungslevel zu den Blöcken. In der Abbildung 24.15 wird die Zuordnung nochmal dargestellt. Die Standardbedingungen werden immer auf die Blöcke randomisiert. Daher haben wir für die Blöcke eine vollständige Randomisierung vorliegen. Die genetischen Linien werden nur teilweise jedem Block zugeordnet. Wir haben für die Level der genetischen Linien ein incomplete block design vorliegen.\n\n\n\n\n\n\nflowchart LR\n    A(trt_gen):::factor --&gt; B & D\n    B[(\"Std_A \n        Std_B\")]:::level --- C(((nested))) --&gt; F(inc_block):::inc_block\n    D[(\"G_1\n        bis\n        G_15\")]:::level -.- E(((nested))) -.-&gt; F\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef level fill:#E69F00,stroke:#333,stroke-width:0.75px\n    classDef inc_block fill:#CC79A7,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 24.15— Schematische Darstellung der Abhängigkeitsstruktur im augmened design. Eine Auswahl der Level der genetischen Linien werden zufällig auf incomplete blocks aufgeteilt. Die Standardbedingungen werden vollständig jedem Block zugeordnet.\n\n\n\n\n\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nWir rechnen ein multiples lineares Modell für die statistische Analyse.\n\nfit &lt;- lm(drymatter ~ trt + inc_block, data = augment_tbl)\n\nOder können überlegen ein gemischtes Modell zu rechnen.\n\nfit &lt;- lmer(drymatter ~ trt + (1 | inc_block), data = augment_tbl)\n\nDie Entscheidung kannst du dann mit der Modellselektion durchführen.\n\n\n\n\nAuch hier hilft die Seite DSFAIR - Augmented design mit einer beispielhaften Auswertung eventuell weiter. Beispielhafte Daten und Auswertung sind dort vorhanden.\nDie Erstellung ist sehr fehleranfällig, wenn wir das selber machen. Deshalb nutzen wir hier das R Paket {agricolae}. Du musst eben sehr genau darauf achten, dass wirklich auch alle genetischen Linien in den Blöcken sind. Das macht aber die Funktion design.dau() gleich für uns. Wir brauchen also einmal einen Faktor für unsere genetischen Linien, dann einmal die Anzahl an Linien sowie die Anzahl an Blöcken. Das definieren wir uns jetzt hier einmal in Objekten vor, dass macht die Arbeit später einfacher.\n\ntrt_gen &lt;- str_c(\"G\", 1:15)   \nn_trt_gen &lt;- n_distinct(trt_gen)\nn_block &lt;- 3\n\nDann noch den Standard erschaffen. Hier ist wichtig, dass der Standard immer in jedem Block sein wird. Das wird nicht der Fall sein für die genetischen Linien. Daher haben wir dann für die Linien ein incomplete block design vorliegen.\n\nstandard &lt;- c(\"Std_A\", \"Std_B\")\n\nWir nutzen hier einal die Funktion design.dau() aus dem R Paket {agricolae} um uns das augmented design ersellen zu lassen. Das macht die Funktion auch einigermaßen gut, wir müssen gleich nur noch die Positionen auf dem Pflanzgrid ergänzen.\n\naugmented_obj &lt;- design.dau(trt1 = standard,\n                            trt2 = trt_gen,\n                            r = n_block,\n                            seed = 42)\n\nJetzt nehmen wir das Feldbuch und ergänzen nochmal die Positionen rows und cols. Dabei müssen wir die rows etwas komplizierter aus der letzten Ziffer der Variable plots durch die Funktion str_sub() extrahieren. Sonst passt es leider nicht ganz mit den jeweiligen Positionen. Die Spalten cols sind unsere Blöcke.\n\naugmented_book_tbl &lt;- augmented_obj$book |&gt; \n  mutate(cols = as.numeric(block),\n         rows = as.numeric(str_sub(plots, 3,3)))\n\nEs ist eine gute Idee sich die Linien und die Standards einmal unterschiedlich einzufärben. Ich nutze für die Linien einen Gradienten von Blau und für die Standardbehandlungen einen Gradienten von Orange. Wir erschaffen uns also einen Vektor mit Farbnamen gleich der Anzahl an Linien plus den zwei Standards.\n\nblue_15 &lt;- colorRampPalette(colors=c(\"#56B4E9\", \"#0072B2\"))(15) \norange_03 &lt;- colorRampPalette(colors=c(\"#E69F00\", \"#D55E00\"))(3) \nblue_15_and_orange_03 &lt;- c(blue_15, orange_03)\n\nDann müssen wir noch den Farbvektor korrekt benennen, so dass die Behandlung im Namen steht und darunter im Vektor dann der Farbwert.\n\nnames(blue_15_and_orange_03) &lt;- augmented_book_tbl |&gt; \n  pull(trt) |&gt; as.character() |&gt; sort() |&gt; unique()\n\nDann können wir schon in der Abbildung 24.16 einmal das augmented design einmal anschauen. Es ist schön zu sehen, das pro Reihe und damit Block beide Standards zufällig auf der Parzelle auftauchen. Auf der anderen Seite sind nicht alle fünfzehn Linien in jedem Block enthalten. Wir sparen mit dem augmented design auf jeden Fall Platz auf dem Feld und können so mehr Linien testen, als wir es mit einem RCBD gekonnt hätten.\n\ndesplot(trt ~ rows + cols, flip = TRUE,\n        out1 = block, out1.gpar = list(col = \"black\", lwd = 2, lty = 3),\n        col.regions = blue_15_and_orange_03,  \n        text = trt, cex = 1, shorten = \"no\",\n        data = augmented_book_tbl,\n        main = \"Pflanzschema für ein augmented design mit zwei Standards.\", \n        show.key = F)\n\n\n\n\n\n\n\nAbbildung 24.16— Schema der Pflanzungen nach den Behandlungen für das augmented design. Die Standardbedingungen sind vollständig in den Blöcken randomisiert, die genetischen Linien nur nicht komplett.\n\n\n\n\n\nFür die Erstellung des augmented design in der Shiny App schaue dir die Anleitung wie schon weiter oben unter Generates an Augmented Randomized Complete Block Design (ARCBD) mit {FielDHub} einmal an. Hier fängt es wirklich an sinnvoll zu sein, eine Shiny App zu haben. So kannst du viel besser mit den Parametern des augmented design rumspielen und sehen was für dich passen würde. Hier einmal der Code für die Erstellung eines augmented design in R mit Code. Wir haben jetzt fünfzehn Linien vorliegen, aber das kennst du ja schon von dem obigen Beispiel in {agricolae}. Auch hier haben wir dann fünf Blöcke vorliegen und zwei Standardbehandlungen. Damit erklärt sich der Code wir oben auch schon.\n\naugmented_obj &lt;- RCBD_augmented(lines = 15,\n                                checks = 2,\n                                b = 5,\n                                seed = 2023)\n\nSpannenderweise müssen wir hier fünf Blöcke wählen und können nicht das Design von oben in {agricolae} nachbauen. Dennoch erstellen wir uns auch schon das letzte Feldbuch in diesem Kapitel und können dann das Feldbuch für die Planung nutzen. Auch hier hilft dann wirklich die Shiny App mehr, dann kannst du besser mit den Werten spielen.\n\naugmented_obj$fieldBook |&gt; \n  as_tibble() |&gt; \n  head(10)\n\n# A tibble: 10 × 11\n      ID EXPT  LOCATION YEAR   PLOT   ROW COLUMN CHECKS BLOCK ENTRY TREATMENT\n   &lt;int&gt; &lt;fct&gt; &lt;fct&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    \n 1     1 Expt1 1        2024    101     1      1      0     1 15    G15      \n 2     2 Expt1 1        2024    102     1      2      1     1 1     CH1      \n 3     3 Expt1 1        2024    103     1      3      0     1 3     G3       \n 4     4 Expt1 1        2024    104     1      4      0     1 9     G9       \n 5     5 Expt1 1        2024    105     1      5      1     1 2     CH2      \n 6     6 Expt1 1        2024    106     2      5      0     2 16    G16      \n 7     7 Expt1 1        2024    107     2      4      1     2 1     CH1      \n 8     8 Expt1 1        2024    108     2      3      1     2 2     CH2      \n 9     9 Expt1 1        2024    109     2      2      0     2 10    G10      \n10    10 Expt1 1        2024    110     2      1      0     2 8     G8       \n\n\nAbschließend haben wir dann die Abbildung 24.17, die sich dann auch von der obigen unterscheidet. Auch hier gefällt mir die Abbildung nicht so sehr und ich würde die andere Abbildung 24.16 mit dem {desplot} Code vorziehen. Ich erkenne mit dem eigenen Farbdesign mehr als in dieser Variante. Aber auch hier musst du dann schauen was du wirklich brauchst. Erstmal das Design in {FielDHub} erstellen und dann das Feldbuch in {desplot} selber hübsch machen.\n\naugmented_obj |&gt; plot()\n\n\n\n\n\n\n\nAbbildung 24.17— Schema der Pflanzungen nach den Behandlungen für das augmented design. Die Standardbedingungen sind vollständig in den Blöcken randomisiert, die genetischen Linien nur nicht komplett.\n\n\n\n\n\n\n\n\nAbbildung 24.1— Schema der Pflanzungen nach den Behandlungen für das complete randomized design.\nAbbildung 24.2— Schema der Pflanzungen nach den Behandlungen für das complete randomized design.\nAbbildung 24.3— Schema der Pflanzungen nach den Behandlungen für das complete randomized design.\nAbbildung 24.5— Schema der Pflanzungen nach den Behandlungen für das randomized complete block design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\nAbbildung 24.6— Schema der Pflanzungen nach den Behandlungen für das randomized complete block design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\nAbbildung 24.7— Schema der Pflanzungen nach den Behandlungen für das randomized complete block design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\nAbbildung 24.9— Schema der Pflanzungen nach den Behandlungen für das latin square design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\nAbbildung 24.10— Schema der Pflanzungen nach den Behandlungen für das latin square design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\nAbbildung 24.11— Schema der Pflanzungen nach den Behandlungen für das latin square design. Beachte die schwarzen Linien als Grenzen für die Blöcke. Im realen Leben können das Gänge zwischen Tischen sein oder Feldwege.\nAbbildung 24.13— Schema der Pflanzungen nach den Behandlungen für das alpha design.\nAbbildung 24.14— Schema der Pflanzungen nach den Behandlungen für das alpha design.\nAbbildung 24.16— Schema der Pflanzungen nach den Behandlungen für das augmented design. Die Standardbedingungen sind vollständig in den Blöcken randomisiert, die genetischen Linien nur nicht komplett.\nAbbildung 24.17— Schema der Pflanzungen nach den Behandlungen für das augmented design. Die Standardbedingungen sind vollständig in den Blöcken randomisiert, die genetischen Linien nur nicht komplett.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Einfache Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-advanced.html",
    "href": "experimental-design-advanced.html",
    "title": "25  Fortgeschrittene Designs",
    "section": "",
    "text": "25.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, agricolae, dae, desplot,\n               janitor, FielDHub,\n               conflicted)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(magrittr::set_names)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Fortgeschrittene Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-advanced.html#sec-rcbd-3fac",
    "href": "experimental-design-advanced.html#sec-rcbd-3fac",
    "title": "25  Fortgeschrittene Designs",
    "section": "25.2 Randomized complete block design (RCBD, 3-faktoriell)",
    "text": "25.2 Randomized complete block design (RCBD, 3-faktoriell)\nDas Maximum des machbaren ist eigentlich ein dreifaktorielles Modell im randomized complete block design. Natürlich geht auch noch mehr, aber der Grundsatz ist eigentlich, dass wir uns maximal zwei Behandlungsfaktoren und dann ein bis zwei Cluster anschauen. Die Cluster sind dann meist einmal der klassische Block plus ein Faktor für verschiedene Lokalisationen. Wir nehmen hier jetzt einmal zwei Behandlungsfaktoren und dann noch einen klassischen Block dazu. Beide Behandlungsfaktoren sind ineinander und dann natürlich im Block genestet. Hier aber erstmal das Modell mit den drei Faktoren für den besseren Überblick. Wir haben einmal die Düngung fert, dann den Faktor Boden soil sowie die verschiedenen Blöcke durch block.\n\\[\ny \\sim \\overbrace{soil}^{f_1} + \\underbrace{fert}_{f_2} + \\overbrace{block}^{f_3}\n\\]\nmit\n\ndem Faktor Dünger fert und den sechs Düngestufen als Level fe_1 bis fe_6.\ndem Faktor Boden soil und den vier Bodenarten als Level soil_1 bis soil_4.\ndem Faktor Block block und den zwei Leveln block_1 und block_2.\n\nDie Struktur der Daten ist wie folgt gegeben. Jedes Level der Düngerstufe fert ist in dem Faktor des Bodens soil enthalten. Die beiden Behandlungen sind dann wiederum jeweils vollständig in den Blöcken block vorhanden. In der Abbildung 25.2 sehen wir einmal den Zusammenhang zwischen den drei Faktoren\n\n\n\n\n\n\nflowchart LR\n    A(fert):::factor --- B(((nestet))) --&gt; E(block):::factor\n    C(soil):::factor --- D(((nestet))) --&gt; E\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 25.2— text\n\n\n\n\n\nWir wollen uns jetzt die Daten einmal selber erstellen und das R Paket agricolae nutzen um uns ein passendes Design zu bauen. Abschließend schauen wir uns den Versuchsplan einmal mit desplot an.\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nWir rechnen ein multiples lineares Modell für die statistische Analyse.\n\nfit &lt;- lm(drymatter ~ soil + fert + soil:fert + block, data = rcbd_3f_tbl)\n\n\n\n\n25.2.1 … mit expand_grid()\nBei dem randomized complete block design haben wir ja Glück, wir müssen nur darauf achten, das die einzelnen Beobachtungen innerhalb der Blöcke vollständig randomisiert sind. Daher Randomisieren wir ganz am Ende einmal die Saplte fert und die Spalte soil durch. Wir hätten uns das mutate() auch sparen können und stattdessen einfach die Funktion slice_sample(prop = 1) nutzen. Die Funktion permutiert dann alles innerhalb der gruppierten Blöcke durch. Mach wie es dir besser gefällt und du es besser nachvollziehen kannst.\n\nthree_fct_long_tbl &lt;- expand_grid(block = 1:2, soil = 1:4, fert = 1:6) |&gt; \n  mutate(block = factor(block, labels = str_c(\"Block \", 1:2)),\n         soil = factor(soil, labels = str_c(\"soil_\", 1:4)),\n         fert = factor(fert, label = str_c(\"fe_\", 1:6))) |&gt; \n  group_by(block) |&gt; \n  mutate(fert = sample(fert), # Randomisierung fert\n         soil = sample(soil)) # Randomisierung soil\nthree_fct_long_tbl\n\n# A tibble: 48 × 3\n# Groups:   block [2]\n   block   soil   fert \n   &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;\n 1 Block 1 soil_1 fe_6 \n 2 Block 1 soil_2 fe_3 \n 3 Block 1 soil_3 fe_1 \n 4 Block 1 soil_2 fe_3 \n 5 Block 1 soil_2 fe_6 \n 6 Block 1 soil_1 fe_2 \n 7 Block 1 soil_3 fe_6 \n 8 Block 1 soil_3 fe_4 \n 9 Block 1 soil_3 fe_5 \n10 Block 1 soil_4 fe_5 \n# ℹ 38 more rows\n\n\nJetzt mache ich es etwas anders bei der Erstellung des Grids der Spalten und Zeilen. Ich will später beim desplot() für die Blöcke durch | block separieren. Daher setzte ich die Spaltennummerierung cols jeweils auf 1 bis 4 und bilde die Blöcke durch ein davor geschaltetes rep ab. Wenn du die Variable rep nicht möchtest, kannst du auch die cols auf 1 bis 8 setzten, darfst dann aber nicht für die Blöcke durch rep separieren.\n\nthree_fct_plot_tbl &lt;- three_fct_long_tbl |&gt; \n  bind_cols(expand_grid(rep = 1:2, cols = 1:4, rows = 1:6))\n\nIn der Abbildung 25.3 sehen wir dann einmal das randomized complete block design mit drei Faktoren einmal dargestellt. Wir sehen gut, wie die zwei Behandlungen vollständig randomisiert wurden. Beachte, dass die Farben den Faktor soil darstellen und die Labels dann den Faktor fert. Du kannst mit der Option show.key = FALSE auch die Legende ausschalten. Bei sehr komplexen Designs mit vielen Faktorstufen ist es dann doch mal ratsam, sich die Legende mit ausgeben zu lassen.\n\ndesplot(block ~ cols + rows | block, flip = TRUE,\n        out1 = rows, out1.gpar = list(col = \"grey\", lty = 1),\n        out2 = cols, out2.gpar = list(col = \"grey\", lty = 1), \n        text = fert, cex = 1, shorten = \"no\", \n        col = soil, \n        data = three_fct_plot_tbl ,\n        main = \"Randomized complete block design (3-faktoriell)\",\n        show.key = TRUE)\n\n\n\n\n\n\n\nAbbildung 25.3— Randomized complete block design (3-faktoriell) für die Faktoren Dünger fert sowie soil vollständig randomisiert in den beiden Blöcken.\n\n\n\n\n\nAuch hier können wir dann den Versuchsplan relativ einfach raus schreiben. Ich entferne noch die Spalte rep, da ich die Spalte nicht weiter brauchen werde. Dann wären wir auch schon mit dem Design fertig.\n\nthree_fct_plot_tbl |&gt; \n  select(-rep) |&gt; \n  write_xlsx(\"template_sheet.xlsx\")\n\n\n\n25.2.2 … mit {FielDHub}\nFür die Erstellung des randomized complete block design in der Shiny App schaue dir die Anleitung unter Generates a Full Factorial Design mit {FielDHub} einmal an. Hier einmal der Code für die Erstellung in R. Wir haben jetzt vier Behandlungen für den Faktor soil sowie sechs Behandlungen für den Faktor fe vorliegen und wollen jeweils drei Blöcke anlegen. Jetzt einmal die generische Funktion full_factorial() in der wir aber keine Namen der Level übergeben können. Ohne die Namen wird die Abbildung maximal wirr. Ich erkenne dann gar nichts mehr, wenn ich nur noch Zahlen stehen habe.\n\nrcbd_fac3_obj &lt;- full_factorial(setfactors = c(4, 6), \n                                reps = 3, l = 1, type = 2)\n\nDeshalb gibt es die Möglichkeit sich ein Grid zu erstellen und anhand der Namen des Grid dann die Parzellen zu benennen. Leider ist das Vorgehen etwas suboptimal was das Einfärben gleich angeht.\n\ndata_factorial &lt;- lst(fct = c(rep(c(\"soil\", \"fe\"), c(4, 6))),\n                      lvl = c(str_c(\"soil_\", 1:4), str_c(\"fe_\", 1:6))) |&gt; \n  as.data.frame()\ndata_factorial\n\n    fct    lvl\n1  soil soil_1\n2  soil soil_2\n3  soil soil_3\n4  soil soil_4\n5    fe   fe_1\n6    fe   fe_2\n7    fe   fe_3\n8    fe   fe_4\n9    fe   fe_5\n10   fe   fe_6\n\n\nWir bauen uns jetzt die Daten mit der Funktion full_factorial() setzen aber die Option setfactors = NULL damit die Funktion die Faktoren und deren Level aus unserem Datensatz data_factorial nimmt. Das ist jetzt auch nur so semi praktisch, aber immerhin könnten wir hier beliebig viele Faktoren prinzipiell nehmen.\n\nrcbd_fac3_obj &lt;- full_factorial(setfactors = NULL, reps = 3, l = 1, type = 2,\n                                data = data_factorial) \n\nDann können wir uns einmal das erstellte Feldbuch wiedergeben lassen, was du dann auch in Excel raus schreiben kannst. Nicht immer brauchst du alle Informationen. Zum Beispiel ist der Ort location, wo das Experiment durchgeführt wird eher zweitrangig. Du könntest dir auch zusätzliche Daten generieren lassen, aber das kannst du dir dann auch selber mit Werten ausdenken.\n\nrcbd_fac3_obj$fieldBook |&gt; \n  as_tibble()\n\n# A tibble: 72 × 7\n      ID LOCATION  PLOT   REP FACTOR_soil FACTOR_fe TRT_COMB   \n   &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;      \n 1     1        1   101     1 soil_4      fe_5      soil_4*fe_5\n 2     2        1   102     1 soil_1      fe_6      soil_1*fe_6\n 3     3        1   103     1 soil_3      fe_2      soil_3*fe_2\n 4     4        1   104     1 soil_2      fe_1      soil_2*fe_1\n 5     5        1   105     1 soil_3      fe_4      soil_3*fe_4\n 6     6        1   106     1 soil_2      fe_6      soil_2*fe_6\n 7     7        1   107     1 soil_1      fe_4      soil_1*fe_4\n 8     8        1   108     1 soil_3      fe_1      soil_3*fe_1\n 9     9        1   109     1 soil_2      fe_3      soil_2*fe_3\n10    10        1   110     1 soil_4      fe_2      soil_4*fe_2\n# ℹ 62 more rows\n\n\nDann erstellen wir auch schon die Abbildung 25.4 mit dem randomized complete block design. Leider ist die Abbildung nur noch an der Grenze zu okay. Wir haben wilde Farben, die jeweils die Faktorkombination wiedergeben. Leider helfen die Farben wenig und auch ist alles sehr unübersichtlich. Das Paket {desplot} kann definitiv mehr. Hier würde ich dann wirklich auf den Plot hier aus {FielDHub} verzichten und stattdessen mir den Plot wie oben in der Abbildung 25.3 aus den den Feldbuch selber bauen.\n\nrcbd_fac3_obj |&gt; plot()\n\n\n\n\n\n\n\nAbbildung 25.4— Schema der Pflanzungen nach den Behandlungen für das randomized complete block design. Die Farbwhal und die Beschriftung ist mehr als gewöhnungsbedürftig. Die Abbildung ist nicht sehr übersichtlich und nur begrenzt als Darstellung geeignet.\n\n\n\n\n\n\n\n25.2.3 … mit {agricolae}\nJetzt bauen wir unser Design nochmal in {agricolae} nach. Hier brauchen wir dann aber alles als Vektor, sonst wird es zu unübersichtlich in den Funktionen. Darüber hinaus ist es dann auch mal was anders und du siehst nochmal eine andere Art den Code zu schreiben. Prinzipiell hätten wir auch im vorherigen Teil alles erstmal in Vektoren lagern können. Aber gut, hier erstmal alle Level der Faktoren vorbereiten und die Anzahl der Blöcke auf vier gesetzt. Dann brauchen wir noch die Länge der Level, also die Anzahl an Düngerstufen und Bodenarten. Dafür nutzen wir die Funktion distinct_n().\n\ntrt_fac1_soil &lt;- str_c(\"soil_\", 1:4)    \nn_trt_fac1_soil &lt;- n_distinct(trt_fac1_soil) \ntrt_fac2_fert &lt;- str_c(\"fe_\", 1:6)  \nn_trt_fac2_fert &lt;- n_distinct(trt_fac2_fert) \nn_block &lt;- 4\n\nDann können wir schon das randomized complete block design mit drei Faktoren über die Funktion design.ab() erstellen. Wie immer heißt die Wiederholung auch hier r, das ist zwar sehr einheitlich aber manchmal auch verwirrend. Auch musst die beiden Vektoren aneinanderkleben damit die Funktion funktioniert.\n\nrcbd_fac3_obj &lt;- design.ab(trt = c(n_trt_fac2_fert, n_trt_fac1_soil), \n                           design = \"rcbd\",\n                           r = n_block, \n                           seed = 42)\n\nDann können wir auch schon unser Ergebnis der Funktion einmal aufarbeiten. Wie immer fehlt das Positionsgrid, so dass wir hier nochmal tätig werden müssen. Dann wollen wir noch die Faktoren wieder umbenennen, so dass auch die Level passen. Am Ende wähle ich noch die Spalten aus, die wir dann später brauchen werden. Meistens müssen wir auch bei den komplexeren Designs und der Nutzung von agricolae im Nachgang sehr viel selber machen. Teilweise lohnt es sich da für mich nicht, für zwei Zeilen Code eine Funktion zu nutzen, der ich dann auch noch sehr viel Nachprogrammieren muss. Man merkt hier eben auch das Alter von dem R Paket agricolae.\n\nrcbd_fac3_book_tbl &lt;- rcbd_fac3_obj$book |&gt;\n  bind_cols(expand.grid(rows = 1:n_trt_fac2_fert,\n                        cols = 1:(n_trt_fac1_soil*n_block))) |&gt; \n  mutate(trt_fac2_fert = str_c(\"fe_\", A),\n         trt_fac1_soil = str_c(\"soil_\", B),\n         block = paste0(\"Block \", block)) |&gt; \n  select(block, fert = trt_fac2_fert, soil = trt_fac1_soil, rows, cols)\n\nNatürlich hat die Funktion auch keinen sketch, wenn man ihn braucht. Das haben wir ja jetzt schon selber mit dem Positionsgrid programmiert. In der Abbildung 25.5 siehst du dann das Ergebnis des Versuchsplans. Wir haben hier nochmal zwei Blöcke zusätzlich zu dem obigen Beispiel genommen, dann siehst du nochmal schöner, wie sich die beiden Behandlungen in den Blöcken randomisieren.\n\ndesplot(block ~ cols + rows | block, flip = TRUE,\n        out1 = rows, out1.gpar = list(col = \"grey\", lty = 1),\n        out2 = cols, out2.gpar = list(col = \"grey\", lty = 1), \n        text = fert, cex = 1, shorten = \"no\", col = soil,\n        data = rcbd_fac3_book_tbl,\n        main = \"Randomized complete block design (3-faktoriell)\", \n        show.key = TRUE, key.cex = 1)\n\n\n\n\n\n\n\nAbbildung 25.5— Randomized complete block design (3-faktoriell) für die Faktoren Dünger fert sowie soil vollständig randomisiert in den vier Blöcken.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Fortgeschrittene Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-advanced.html#sec-split",
    "href": "experimental-design-advanced.html#sec-split",
    "title": "25  Fortgeschrittene Designs",
    "section": "25.3 Split plot design (3-faktoriell)",
    "text": "25.3 Split plot design (3-faktoriell)\nDie Spaltanlage (eng. split plot design) ist eine häufig genutzte Variante, wenn wir eine Behandlungsfaktor nur in einem anderen Level eines zweiten Behandlungsfaktor randomisieren können. Wir können also eine Feldspur nur auf eine Art mechanisch bearbeiten und geben dann aber pro Feldspur verschiedene Dünger auf. Oder wir können in einen Stahl nur auf eine Art Futter zuführen, aber verschiedene Arten von Futter. Das gleiche kannst du dir auch mit einer Klimakammer vorstellen in der wir verschiedene Pflanzenlinien stellen können.\nBeide Behandlungsfaktoren sind ineinander und dann natürlich im Block genestet. Hier aber erstmal das Modell mit den drei Faktoren für den besseren Überblick. Wir haben einmal die Düngung fert, dann den Faktor Boden soil sowie die verschiedenen Blöcke mit block.\n\\[\ny \\sim \\overbrace{soil}^{f_1} + \\underbrace{fert}_{f_2} + \\overbrace{block}^{f_3}\n\\]\nmit\n\ndem Faktor Dünger fert und den sechs Düngestufen als Level fe_1 bis fe_6.\ndem Faktor Boden soil und den vier Bodenarten als Level soil_1 bis soil_4.\ndem Faktor Block block und den zwei Leveln block_1 und block_2.\n\nIn der Abbildung 25.6 sehen wir die Abhängigkeitsstruktur des split plot designs. Wir haben den Faktor fert den wir in den Faktor soil nesten. Den Faktor soil nesten wir dann wiederrum in die Spalten cols des Blocks. Deshalb nennen wir das ja auch eine Spaltanlage. Ein Faktor ist immer in den Spalten angeordnet und der andere Faktor in der Spalte randomisiert. Wie immer wird es vielleicht klarer, wenn du dir dazu die Abbildung 25.7 als Ergebnis des Versuchsdesigns anschaust.\n\n\n\n\n\n\nflowchart LR\n    A(fert):::factor --- B(((nestet))) --&gt; C(soil):::factor --- D(((nestet))) --&gt; E(cols) --- F(block):::factor\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 25.6— Abhängigkeitsstruktur des split plot design. Der Faktor soil ist in den Spalten cols der Blöcke randomisiert und der zweite Faktor fert innerhalb des anderen Faktors und somit auch in den Spalten.\n\n\n\n\n\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nWir rechnen ein komplexeres gemischtes Modell mit allen möglichen zufälligen Effekten.\n\nfit &lt;-  lmer(yield ~ soil + fert + soil:fert + \n               (1|block) + (1|block:fert) + (1|block:soil), \n             data = split_tbl)\n\nOder können überlegen ein gemischtes Modell mit weniger zufälligen Effekten zu rechnen.\n\nfit &lt;-  lmer(yield ~ soil + fert + soil:fert + \n               (1|block), \n             data = split_tbl)\n\nDie Entscheidung kannst du dann mit der Modellselektion durchführen.\n\n\nJetzt haben wir die volle Auswahl an Möglichkeiten. Ich zeige einmal wie man händisch das split plot designs erstellt. Dann schauen wir uns die Erstellung in agricolae einmal an und dann zeige ich noch die Variante in dae, die mich echt einiges an Zeit und Nerven gekostet hat. In sich ist das Paket dae ja logisch, aber die Dokumentation lässt für mich etwas zu wünschen übrig. Dennoch hier einmal die volle Dreifaltigkeit der Versuchsdesignerstellung, wenn ich mir schon die Mühe gemacht habe.\n\n25.3.1 … mit expand_grid()\nJa, selber machen ist hier etwas mühsamer, aber wenn du die Schritte nachvollziehst, dann wird dir vermutlich das split plot design sehr viel klarer. Als erstes erschaffen wir die zwei Blöcke und die vier Bodenarten. Dabei sind in jedem Block die vier Bodenarten genestet. Dann gruppieren wir nach Block und randomiseren einmal die Bodenarten je Block. Im nächsten Schritt erweitern wir dann jede Block/Boden-Kombination um die sechs Düngerstufen. Dann gruppieren wir wieder, aber diesmal für jede Block/Boden-Kombination, um hier dann einmal die Düngestufen zu randomisieren. Dann lösen wir alle Gruppen auf und setzen fürunsere Faktoren dann noch die richtigen Labels.\n\nsplitplot_long_tbl &lt;- expand_grid(block = 1:2, \n                                  soil = 1:4) |&gt; \n  group_by(block) |&gt;             # Gruppieren nach block\n  mutate(soil = sample(soil)) |&gt; # Randomisieren von soil in block\n  expand_grid(fert = 1:6) |&gt;   \n  group_by(block, soil) |&gt;       # Gruppieren nach block und soil\n  mutate(fert = sample(fert)) |&gt; # Randomisieren von fert in block und soil\n  ungroup() |&gt; \n  mutate(fert = factor(fert, label = str_c(\"fe_\", 1:6)),\n         block = factor(block, labels = str_c(\"Block \", 1:2)),\n         soil = factor(soil, labels = str_c(\"soil_\", 1:4))) \nsplitplot_long_tbl\n\n# A tibble: 48 × 3\n   block   soil   fert \n   &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;\n 1 Block 1 soil_4 fe_1 \n 2 Block 1 soil_4 fe_3 \n 3 Block 1 soil_4 fe_4 \n 4 Block 1 soil_4 fe_6 \n 5 Block 1 soil_4 fe_2 \n 6 Block 1 soil_4 fe_5 \n 7 Block 1 soil_2 fe_4 \n 8 Block 1 soil_2 fe_5 \n 9 Block 1 soil_2 fe_2 \n10 Block 1 soil_2 fe_6 \n# ℹ 38 more rows\n\n\nDa ich auch gleich wieder für die Blöcke separieren möchte, baue ich mir für jeden Block rep nochmal vier Spalten, je eine Spalte für eine Bodenbehandlung, und dann nochmal sechs Zeilen, eine für jede Düngestufe. Dann verbinden wir den randomisirten Datensatz mit den Pflanzgrid und schon können wir uns das split plot design einmal anschauen.\n\nsplitplot_plot_tbl &lt;- splitplot_long_tbl |&gt; \n  bind_cols(expand_grid(rep = 1:2, \n                        cols = 1:4, \n                        rows = 1:6))\n\nIn der Abbildung 25.7 siehst du einmal das split plot design dargestellt. Hier wird auch nochmal schön das Randomisierungsmuster klar. Du siehst auch warum das split plot design im Deutschen auch Spaltanlage heißt. Der Dünger ist in Spalten des Bodenfaktores randomisiert. Jeder Block hat also seine eigene Randomiserung der Böden.\n\ndesplot(block ~ cols + rows | block, flip = TRUE,\n        out1 = rows, out1.gpar = list(col = \"grey\", lty = 1),\n        out2 = cols, out2.gpar = list(col = \"grey\", lty = 1), \n        text = fert, cex = 1, shorten = \"no\", col = soil,\n        data = splitplot_plot_tbl ,\n        main = \"Split plot design (3-faktoriell)\",\n        show.key = TRUE)\n\n\n\n\n\n\n\nAbbildung 25.7— Split plot design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den zwei Blöcken. Der Faktor fert ist in dem Faktor soil genestet und randomisiert.\n\n\n\n\n\n\n\n25.3.2 … mit {FielDHub}\nFür die Erstellung des split plot design in der Shiny App schaue dir die Anleitung unter Generates a Split Plot Design mit {FielDHub} einmal an. Hier einmal der Code für die Erstellung in R. Wir haben jetzt vier Behandlungen für den Faktor soil sowie sechs Behandlungen für den Faktor fe vorliegen und wollen jeweils drei Blöcke anlegen. Jetzt einmal die Funktion split_plot() in der wir hier aber die Namen der Level übergeben können. Das ist dann wiederum eine Erleichterung zum dreifaktoriellen Design eben.\n\nsplit_plot_obj &lt;- split_plot(wp = paste0(\"soil_\", 1:4),\n                             sp = paste0(\"fe_\", 1:6),\n                             reps = 3,\n                             seed = 2023)\n\nDann können wir uns einmal das erstellte Feldbuch wiedergeben lassen, was du dann auch in Excel raus schreiben kannst. Wie immer brauchst du nicht alle Informationen. Auch hier würde ich das Feldbuch dann nutzen um mir selber in {desplot} die Abbildung zu erstellen. Die automatisierte Abbildungserstellung in {FielDHub} ist der manuellen weit unterlegen.\n\nsplit_plot_obj$fieldBook |&gt; \n  as_tibble()\n\n# A tibble: 72 × 7\n      ID LOCATION  PLOT   REP WHOLE_PLOT SUB_PLOT TRT_COMB   \n   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;      \n 1     1        1   101     1 soil_3     fe_3     soil_3|fe_3\n 2     2        1   101     1 soil_3     fe_2     soil_3|fe_2\n 3     3        1   101     1 soil_3     fe_1     soil_3|fe_1\n 4     4        1   101     1 soil_3     fe_6     soil_3|fe_6\n 5     5        1   101     1 soil_3     fe_4     soil_3|fe_4\n 6     6        1   101     1 soil_3     fe_5     soil_3|fe_5\n 7     7        1   102     1 soil_1     fe_4     soil_1|fe_4\n 8     8        1   102     1 soil_1     fe_1     soil_1|fe_1\n 9     9        1   102     1 soil_1     fe_6     soil_1|fe_6\n10    10        1   102     1 soil_1     fe_3     soil_1|fe_3\n# ℹ 62 more rows\n\n\nDann erstellen wir auch schon die Abbildung 25.8 mit dem split plot design. Leider ist die Abbildung nur noch okay. Die Einteilung mit dem | ist besser als beim faktoriellen Design, aber so richtig überzeugen tut mich die Darstellung nicht. Wir haben ja die wilden Farben, da wir jeweils die Faktorkombination wiedergeben. Leider helfen die Farben wenig und auch ist alles sehr unübersichtlich. Das Paket {desplot} kann definitiv mehr. Hier würde ich dann wirklich auf den Plot hier aus {FielDHub} verzichten und stattdessen mir den Plot wie oben in der Abbildung 25.7 aus den den Feldbuch selber bauen.\n\nsplit_plot_obj |&gt; plot()\n\n\n\n\n\n\n\nAbbildung 25.8— Split plot design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den zwei Blöcken. Der Faktor fert ist in dem Faktor soil genestet und randomisiert.\n\n\n\n\n\n\n\n25.3.3 … mit {agricolae}\nJetzt machen wir das Ganze nochmal mit dem R Paket {agricolae}. Hier müssen wir wieder die Vektoren mit den Faktoren und Leveln vorab erschaffen. Auch brauchen wir die Anzahl an Leveln für jeden Faktor. Am Ende randomisieren wir hier mal in vier Blöcke, einfach um es etwas anders zu machen.\n\ntrt_fac1_soil &lt;- str_c(\"soil_\", 1:4)    \nn_trt_fac1_soil &lt;- n_distinct(trt_fac1_soil) \ntrt_fac2_fert &lt;- str_c(\"fe_\", 1:6)  \nn_trt_fac2_fert &lt;- n_distinct(trt_fac2_fert) \nn_block &lt;- 4\n\nFür die Erstellung des split plot design nutzen wir die Funktion design.split(). Hier ist es wichtig, unbedingt die serie = 0 als Option zu setzen. Wir brauchen die so entstehende Information in der Spalte plots sonst klappt es im Folgenden nicht die plots gleich den Spalten cols zu setzen. Ja, ich weiß, ist alles super suboptimal, aber so ist es eben in {agricolae}. Du musst noch einges machen, damit die Funktion auch einen visualisierbare Ausgabe wiedergibt.\n\nsplitplot_obj &lt;- design.split(trt1 = trt_fac1_soil,\n                              trt2 = trt_fac2_fert,\n                              r = n_block,\n                              seed = 42, serie = 0)\n\nLeider gibt es kein sketch-Objekt gerade hier wo man eins gebrauchen könnte. Deshalb also nochmal alles umbauen und die Zeilen rows und Spalten cols entsprechend ergänzen. Ich nenne jetzt nochmal ein wenig die Variablen um, einfach damit hier nicht immer das Gleiche passiert. Wichtig ist, dass wir hier immer für die Blöcke separieren. Wir müssen also in desplot definitiv die Option | block setzen, sonst klappt es mit der Darstellung nicht.\n\nsplitsplot_book_tbl &lt;- splitplot_obj$book |&gt; \n  mutate(block = str_c(\"Block \", block),\n         cols = plots, \n         rows = as.numeric(splots)) |&gt; \n  select(block, fert = trt_fac2_fert, soil = trt_fac1_soil, rows, cols)\n\nIn der Abbildung 25.9 siehst du das Ergebnis der Versuchsplanerstellung mit {agricolae}. Wir kriegen faktisch das Gleiche raus, gut die Faktoren sind anders permutiert, aber das wundert uns jetzt nicht. Am Ende musst du entscheiden, was dir besser gefällt. Je komplexer die Randomisierung ist, desto einfacher ist die Nutzung der {agricolae} Funktionen. Beim split plot design ist es so eine Sache, es lohnt sich bei sehr großen Anlagen dann schon.\n\ndesplot(block ~ cols + rows | block, flip = TRUE,\n        out1 = rows, out1.gpar = list(col = \"grey\", lty = 1),\n        out2 = cols, out2.gpar = list(col = \"grey\", lty = 1), \n        text = fert, cex = 1, shorten = \"no\", col = soil,\n        data = splitsplot_book_tbl ,\n        main = \"Split plot design (3-faktoriell)\",\n        show.key = TRUE)\n\n\n\n\n\n\n\nAbbildung 25.9— Split plot design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den vier Blöcken. Der Faktor fert ist in dem Faktor soil genestet und randomisiert.\n\n\n\n\n\n\n\n25.3.4 … mit {dae}\nAuch das R Paket {dae} liefert die Möglichkeit ein split plot design zu erstellen. Die Idee von dae ist ja ein mehr generalisiertes Framework für die Erstellung von Versuchsplänen anzubieten. Deshalb gibt es ja nur die Funktion designRandomize(), die ein gegebenes Design nach feststehenden Regeln randomisiert. Die Idee ist gut, aber dafür musst du dich relativ tief in das Paket dae einarbeiten. Für mich war dieser kurze Code für das split plot design schon recht langwierig aus den Beispielen aus den Notes on the use of dae for design herzuleiten. Prinzipiell ist es ja nicht schwer, aber es ist schon etwas mehr Auswand.\nWie funktioniert nun die Erstellung eines split plot design? Wir bauen uns erstmal unser Design der Positionen mit den Blöcken, Mainplots MPlots sowie den Subplot SubPlots. Dabei ist es wichtig zu wissen, dass die Subplots in den Mainplots genestet sind. Die Mainplots sind dann wiederum in den Blöcken genestet. Ja, hier hast du dann ein Paket spezifisches Naming der Optionen. Im zweiten Schritt packen wir dann den Faktor Boden soil und den Faktor fert zu den Positionen hinzu. Wichtig ist das times = 4 am Ende, was nochmal die Faktorkombinationen von soil und fert jeweils in der Anzahl der Blöcke wiederholt. Wir haben ja hier vier Blöcke vorliegen, dass vergisst man ja mal schnell.\n\nsplit_sys &lt;- cbind(fac.gen(list(Blocks = 4, MPlots = 4, SubPlots = 6)),\n                   fac.gen(list(soil = str_c(\"soil_\", 1:4),\n                                fert = str_c(\"fe_\", 1:6)), times = 4)) \nsplit_sys |&gt; \n  as_tibble()\n\n# A tibble: 96 × 5\n   Blocks MPlots SubPlots soil   fert \n   &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt;\n 1 1      1      1        soil_1 fe_1 \n 2 1      1      2        soil_1 fe_2 \n 3 1      1      3        soil_1 fe_3 \n 4 1      1      4        soil_1 fe_4 \n 5 1      1      5        soil_1 fe_5 \n 6 1      1      6        soil_1 fe_6 \n 7 1      2      1        soil_2 fe_1 \n 8 1      2      2        soil_2 fe_2 \n 9 1      2      3        soil_2 fe_3 \n10 1      2      4        soil_2 fe_4 \n# ℹ 86 more rows\n\n\nAn der Ausgabe siehst du ganz schön, wie die Spalte Mainplots zu dem Faktor soil passt sowie die Spalte Subplots zu dem Faktor fert. Die Herausforderung ist nun, dass das R Paket dae davon ausgeht, dass du weißt, was ein split plot design ist und wie es aufgebaut ist. Es gibt also wenig Hilfestellung innerhalb der Funktionen. Dafür sind die Funktionen super flexibel und du kannst dir deine eigenen kreativen Designs bauen - wovon ich abrate.\nIm nächsten Schritt nutzen wir die Funktion designRandomize() um die Faktoren soil und fert den Positionen in der Spalte Block, MPlots und SubPlots randomisiert zuzuweisen. Deshalb sind ja auch die Faktoren soil und fert der Option allocated (deu. zugeteilt) zugewiesen, da die beiden Faktoren ja den Spalten Block, MPlots und SubPlots als Empfänger (eng. recipient) zugeteilt werden. Soweit so gut, dass sagt aber noch nichts über das Wie der Zuteilung aus. Das machen wir dann mit der Option nestet.recipients. Der Option sagen wir jetzt wer in was genestet ist. Und durch diese Zuordnung erschaffen wir ein split plot design. Die Mainplots sind in den Blöcken genestet und dann die Subplots in den Mainplots und den Blöcken. Ja, da muss man erstmal drauf kommen. Hat bei mir gedauert, bis ich das hingekriegt habe.\n\nsplit_lay &lt;- designRandomize(allocated = split_sys[c(\"soil\", \"fert\")],\n                             recipient = split_sys[c(\"Blocks\", \"MPlots\", \"SubPlots\")],\n                             nestet.recipients = list(MPlots = \"Blocks\",\n                                                      SubPlots = c(\"MPlots\", \"Blocks\")),\n                             seed = 235805)\nsplit_lay |&gt; \n  as_tibble()\n\n# A tibble: 96 × 5\n   Blocks MPlots SubPlots soil   fert \n   &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt;\n 1 1      1      1        soil_4 fe_4 \n 2 1      1      2        soil_4 fe_2 \n 3 1      1      3        soil_4 fe_5 \n 4 1      1      4        soil_4 fe_3 \n 5 1      1      5        soil_4 fe_1 \n 6 1      1      6        soil_4 fe_6 \n 7 1      2      1        soil_2 fe_4 \n 8 1      2      2        soil_2 fe_2 \n 9 1      2      3        soil_2 fe_5 \n10 1      2      4        soil_2 fe_3 \n# ℹ 86 more rows\n\n\nUnd wir haben eine Randomisierung. Leider wissen wir nicht, ob die so geklappt hat, es ist ja wirklich schwer zusehen. Deshalb wollen wir uns die Ausgabe mal visualisieren. Das Gleiche Problem wie schon eben haben wir mit der Visualisierung mit designGGPlot(). Die Hilfeseite ist etwas unterkomplex für die Möglichkeiten der Funktion. So musste ich auch hier recht lange Rumspielen, bis ich die folgende Abbildung 25.10 erstellen konnte. Der Witz ist eigentlich hier, dass wir eine interne Funktion nutzen und auch das Objekt aus designRandomize() übergeben. Dennoch müssen wir uns selber mit den row.factors() und den column.factors() alles zusammenkleben. Ich habe es dann geschafft, aber es war mehr Zufall als Können. Das die Spalten die Mainplots sind, war mir klar, aber das die Zeilen durch die beiden Vektoren Blocks und Subplots so in der Reihenfolge gebildet werden war nur durch probieren zu lösen. Das eine aussagekräftige Legende fehlt, ist dann schon fast nicht mehr von Belang.\n\n\n\n\n\n\n\n\nAbbildung 25.10— Split plot design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den vier Blöcken. Der Faktor fert ist in dem Faktor soil genestet und randomisiert.\n\n\n\n\n\nAm Ende ermöglicht das Paket {dae} super flexibel experimentelle Design zu erstellen. Ich werde das Paket auch für Designs im Rahmen der Analyse von linearen gemischten Modellen nutzen. Dafür ist das Paket super. Du musst aber wissen, dass das Paket einiges an Einarbeitung benötigt und du auch die Philosophie hinter den Funktionen verstehen musst. Mal eben schnell, geht leider nicht. Dafür kann das Paket {dae} mehr als als andere Pakete im Bereich der Erstellung von experimentellen Designs.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Fortgeschrittene Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-advanced.html#sec-subsampling",
    "href": "experimental-design-advanced.html#sec-subsampling",
    "title": "25  Fortgeschrittene Designs",
    "section": "25.4 Subsampling",
    "text": "25.4 Subsampling\nDie zentrale Idee des Subsamplings ist, dass wir nicht über die Pflanzen in unseren Blöcken mitteln. Wir nehmen also alle Pflanzen in unserem Block mit in die Analyse. Oder aber wir mitteln nicht das Gewicht der Ferkel für jede Bucht, sondern wollen die Gewichte individuell mit betrachten. Wir messen die Beobachtungen innerhalb der Behandlung und nehmen diese individuellen Beobachtungen auch mit ins Modell. Damit haben wir eigentlich das gleiche Modell wie auch bei dem randomized complete block design mit zwei Faktoren. Nur das wir eben in den Blöcken und Behandlungen nicht einen Mittelwert über alles haben, sondern eben sechs individuelle Werte für die sechs Beobachtungen in einer Behandlung/Block-Kombination in unserem Beispiel.\n\n\n\n\n\n\nBeispieldaten aus {agridat} zu Subsampling\n\n\n\nWenn dich nochmal Daten zum Subsampling interessiert dann schaue dir doch die Beispieldaten aus dem R Paket {agridat} an. Zum einen wäre der Datensatz Rice RCB with subsamples sowie die Daten Split-plot experiment of rice, with subsamples von Interesse. Du findest hinter dem Link dann auch die Hilfe wie du die Daten in R mit data() lädst.\n\n\nDas generelle Modell unterscheidet sich nicht von einem RCBD. Wir haben zwei Faktoren, einmal die Behandlung und einmal einen Block vorliegen. Damit sind wir noch sehr ähnlich. Den Unterschied macht gleich die Randomisierung und die Betrachtung der einzelnen individuellen Beobachtungen.\n\\[\ny \\sim \\overbrace{trt}^{f_1} + \\underbrace{block}_{f_2}\n\\]\nmit\n\ndem Faktor Behandlung trt und den vier Behandlungsstufen als Level ctrl, A, B und C.\ndem Faktor Block block und den vier Leveln block_1 bis block_4.\n\nEs gibt jetzt zwei Arten, wie wir die Daten randomisieren und betrachten können. Zum einen randomisieren wir die Individuen vollständig auf die Behandlungen und die Behandlungen nesten wir dann in die Blöcke. Dann haben wir die Abhängigkeitssruktur in der Abbildung 25.11 vorliegen. Im Anschluss mitteln wir dann nicht über alle Beobachtungen. Prinzipiell ist es also ein sehr großes randomized complete block design.\n\n\n\n\n\n\nflowchart LR\n    A(trt):::factor --- B(((nestet))) --&gt; C(block):::factor \n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 25.11— Subsampling vollständig innerhalb der Blöcke randomisiert. Die individuellen Beobachtungen sind über alle Behandlungen innerhalb der Blöcke randomisiert\n\n\n\n\n\nAnders sieht es in der Variante zwei aus, hier haben wir die individuellen Beobachtungen in Clustern für eine einzelne Behandlung vorliegen. Die Beobachtungen sind innerhalb der Behandlungen genestet. In unserem Beispiel stehen die individuellen Pflanzen immer in einem Pflanztray. Du kannst dir aber auch Ferkel von verschiedenen Müttern in einer Bucht in einem Stall vorstellen. Es ergibt sich dann die Abbildung 25.12, wo unsere einzelnen Beobachtungen in den Behandlungen und dann in den Blöcken genestet sind.\n\n\n\n\n\n\nflowchart LR\n    A(tray) --- B(((nestet))) --&gt; C(trt):::factor --- D(((nestet))) --&gt; E(cols) --- F(block):::factor\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 25.12— Subsampling vollständig innerhalb der Behandlung randomisiert. Die individuellen Pflanzen stehen in Trays und können nicht innerhalb der Behandlung trt randomisiert werden.\n\n\n\n\n\nFür die abschließende statistische Analyse kommt es darauf an, wie die individuellen Pflanzen in den Behandlungen stehen. Eventuell ist alles durcheinander gemischt, das heißt, die Pflanzen sind in den Behandlungen und Blöcken randomisiert. Dann ist es recht einfach mit einem linearen Modell zu machen.\nWenn du aber die Pflanzen in einem Pflanztray oder irgendwie anders fixierst hast, dann macht es schon einen Unterschied. In dem Fall sind die Pflanzen ja individuell nicht unabhängig voneinander. Die Pflanzen sind ja in einem Tray zusammen. Dann nutzen wir ein lineares gemischtes Modell.\nIm Folgenden generieren wir einmal beide Fälle, wobei der zweite Fall - Pflanzen stehen zusammen - als Subsampling angesehen wird. Häufig kann dies auch bei Tierversuchen passieren, wenn zum Beispiel Ferkel in einer Bucht in einem Stahl stehen. Da aber ab und zu beides vorkommt, hier dann einmal beides. Wir können das Subsampling nur selber mit expand_grid() durchführen, da weder das R Paket {agricolae} noch das R Paket {dae} eine entsprechende Implementierung der Funktionalität zur Generierung eines Datensatzes mit Subsampling aufweist.\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nVollständig im Block randomisiert, wir rechnen ein lineares Modell.\n\nfit &lt;- lm(drymatter ~ trt + block + trt:block, data = subsampling_tbl)\n\nBehandlung im Block randomisiert, dass heißt, dass die Pflanzen zusammen in einem Tray stehen, wir rechnen ein gemischtes Modell. Der Tray-Effekt wird dann im zufälligen Effekt (1 | trt:block) modelliert.\n\nfit &lt;- lmer(drymatter ~ trt + block + (1 | trt:block), data = subsampling_tbl)\n\n\n\nDas Subsampling geht nur im Selbermachen, also machen wir das dann einmal mit der Funktion expand_grid(). Als erstes erstellen wir einmal ein Design mit vier Blöcken und vier Behandlungen. Dann ergänzen wir noch pro Behandlung sechs individuelle Beobachtungen. Am Ende randomisieren wir einmal über die Behandlungen in den Blöcken. Wichtig ist hier, dass wir die Daten nach den Blöcken gruppieren müssen und schon vorher alle individuellen Pflanzen generieren.\n\nsubsampling_long_tbl &lt;- expand_grid(block = 1:4, \n                                    trt = 1:4,\n                                    rep = 1:6) |&gt; \n  mutate(trt = factor(trt, labels = c(\"ctrl\", \"A\", \"B\", \"C\")),\n         block = factor(block, labels = str_c(\"Block \", 1:4)),\n         pid = 1:n(),\n         pid_text = str_pad(1:96, width = 2, pad = \"0\")) |&gt; \n  group_by(block) |&gt; \n  mutate(trt = sample(trt)) ## Randomisierung in den Blöcken\n\nDann bauen wir noch unser Grid auf. Wir wollen insgesamt sechzehn Spalten, dass sind die Blöcke/Behandlungs-Kombinationen. Dann brauchen wir noch jeweils sechs Zeilen für die individuellen Pflanzen. Dann können wir die Daten schon nehmen und in der Abbildung 25.13 (a) visualisieren. Bevor wir die Abbildung besprechen, generieren wir uns nochmal die anderen Daten, wo die Behandlungen in den Blöcken randomisiert werden.\n\nsubsampling_plot_tbl &lt;- subsampling_long_tbl |&gt; \n  bind_cols(expand_grid(cols = 1:16, rows = 1:6))\n\nEine andere Möglichkeit ist, dass unsere Pflanzen in einem Pflanztray stehen. Damit haben wir keine Randomisierung mehr in den Blöcken, sondern wir können nur unsere Behandlungen in den Blöcken randomisieren. Die Positionen in den Trays sind ja fix. Daher Randomisieren wir erst den Faktor der Behandlungen in den gruppierten Blöcken. Dann ergänzen wir noch die sechs individuellen Pflanzen.\n\nsubsampling_tray_long_tbl &lt;- expand_grid(block = 1:4, \n                                        trt = 1:4) |&gt; \n  mutate(trt = factor(trt, labels = trt_fac1_soil),\n         block = factor(block, labels = str_c(\"Block \", 1:4))) |&gt; \n  group_by(block) |&gt; \n  mutate(trt = sample(trt)) |&gt; # Randomisierung der Behandlungen in den Blöcken\n  expand_grid(rep = str_pad(1:6, width = 2, pad = \"0\"))\n\nJetzt bauen wir uns noch das Grid für die Positionen zusammen, da bleibt alles beim alten. Wir brauchen auch wieder sechzehn Spalten, dass sind die Blöcke/Behandlungs-Kombinationen. Dann noch die sechs Zeilen für das Pflanzentray. In der Abbildung 25.13 (b) sehen wir einmal das Ergebnis der Randomisierung. Alle Pflanzen sind jetzt in einer Spalte randomisiert, die eben das Tray der Pflanzung entspricht.\n\nsubsampling_tray_plot_tbl &lt;- subsampling_tray_long_tbl |&gt; \n  bind_cols(expand_grid(cols = 1:16, rows = 1:6))\n\nIn der Abbildung 25.13 sehen wir nochmal die beiden Arten des Subsamplings miteinander vergleichen. In der Abbildung 25.13 (a) sind alle Behandlungen mit ihren sechs Wiederholungen zufällig im Block verteilt. Im anderen Fall sehen wir in der Abbildung 25.13 (b), wenn die Pfanzen zusammen in einem Tray stehen und alle die gleiche Behandlung kriegen. Es können aber auch Schweine in einer Bucht sein und die Blöcke sind dann die einzelnen Ställe.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) … vollständig innerhalb Block randomisiert (subsampling_plot_tbl).\n\n\n\n\n\n\n\n\n\n\n\n(b) … Behandlungen innerhalb Block randomisiert (subsampling_tray_plot_tbl).\n\n\n\n\n\n\n\nAbbildung 25.13— Verschiedene Arten des Subsamplings. Einmal vollständig innerhalb der Blöcke randomsiert und einmal die Behandlung randomisiert.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Fortgeschrittene Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-advanced.html#sec-incomplete",
    "href": "experimental-design-advanced.html#sec-incomplete",
    "title": "25  Fortgeschrittene Designs",
    "section": "25.5 Incomplete block design",
    "text": "25.5 Incomplete block design\nBei dem incomplete block design haben wir eigentlich ein normales Design, wie das 3-faktorielle randomized complete block design vorliegen, aber die Randomiserung ist nicht vollständig über alle Blöcke. Wir nehmen einmal die beiden Spezialfälle Alpha design und Augmented design aus der Betrachtung raus. Wir schauen uns hier nochmal zu Vervollständigung das randomized complete block design mit nicht kompletten Blöcken an. Ich persönlich stehe dem Design etwas zwiegesaplten gegenüber. Ich würde lieber die Anzahl der Gruppen der Behandlungen reduzieren als die Blöcke. Wenn du ein Pilotversuch machst um zu schauen welche Linien wachsen oder aber ob es überhaupt einen Effekt einer Behandlung gibt, dann mag das incomplete block design einen Sinn machen. Dann sparst du wirklich Ressourcen und Zeit.\n\n\n\n\n\n\nVerliere keine Behandlung im incomplete block design\n\n\n\nAchte immer darauf, dass du genug Blöcke hast um auch alle Behandlungslevel aufzunehmen. Schaue lieber doppelt, ob du auch wirklich alle Behandlungen in deinem Versuchsplan drin hast. Es kann schnell passieren, dass du auf einmal nur ein Level der Behandlung oder gar keins mehr in deinem Design vorliegen hast. Das R Paket agricolae versucht das incomplete block design so zu bauen, dass du keine Behandlungen verlierst.\n\n\nWir immer erstmal unser Modell mit den zwei Behandlungsfaktoren soil und fert. Dann kommt noch der nicht komplette Block inc_block hinzu.\n\\[\ny \\sim \\overbrace{soil}^{f_1} + \\underbrace{fert}_{f_2} + \\overbrace{inc\\_block}^{f_3}\n\\]\nmit\n\ndem Faktor Dünger fert und den sechs Düngestufen als Level fe_1 bis fe_6.\ndem Faktor Boden soil und den vier Bodenarten als Level soil_1 bis soil_4.\ndem Faktor Block block und den zwei Leveln block_1 und block_2.\n\nIn der Abbildung 25.14 sehen wir nochmal die Abhängigkeitsstruktur des incomplete block design. Die beiden Faktoren soil und fert sind in den Blöcken genestet. Wir stellen hier die Linien als gestrichelt dar um auszudrücken, dass wir nur eine nicht komplette Zuordnung der Behandlungsfaktoren zu den Blöcken haben.\n\n\n\n\n\n\nflowchart LR\n    A(fert):::factor -.- B(((nestet))) -.-&gt; E(inc_block):::inc_block\n    C(soil):::factor -.- D(((nestet))) -.-&gt; E\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef inc_block fill:#CC79A7,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 25.14— Schema der Abhängigkeitsstruktur des incomplete block design für das 3-faktorielle randomized complete block design. Die gestrichelte Linie stellt die nicht vollständige Zuordnung der Behandlungsfaktoren zu den Blöcken dar. Daher sind die Blöcke inc_block nicht komplett.\n\n\n\n\n\nDann das Ganze nochmal in dem R Paket {agricolae} mit einem 2-faktoriellen randomized complete block design mit dem Fall der nicht kompletten Blöcke. Wir können in der Standardfunktion von agricolae nur ein 2-faktorielles incomplete block design abbilden. Deshalb dieses etwas kleinere Modell mit einem Behandlungsfaktor trt und einem Block block. Wir werden dann die Behandlungsgruppen nicht komplett auf alle Blöcke randomisieren.\n\\[\ny \\sim \\overbrace{trt}^{f_1} + \\underbrace{inc\\_block}_{f_2}\n\\] mit\n\ndem Faktor Behandlung trt und den vier Behandlungsstufen als Level ctrl, A, B und C.\ndem Faktor Block soil und den drei Blöcken mit den Leveln 1, 2, 3.\n\nDann schauen wir uns nochmal die die Abhängigkeitsstruktur des incomplete block design in der Abbildung 25.15 an. Wir sehen, dass Design ist relativ einfach. Die Behandlungen sind in den Blöcken genestet, aber wir haben nicht alle Behandlungen in allen Blöcken vorliegen.\n\n\n\n\n\n\nflowchart LR\n    C(trt):::factor -.- D(((nestet))) -.-&gt; E(inc_block):::inc_block\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef inc_block fill:#CC79A7,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 25.15— Schema der Abhängigkeitsstruktur des incomplete block design für das 2-faktorielle randomized complete block design. Die gestrichelte Linie stellt die nicht vollständige Zuordnung der Behandlungsfaktoren zu den Blöcken dar. Daher sind die Blöcke inc_block nicht komplett.\n\n\n\n\n\nWir oben schon geschrieben, die Verwendung von dem R Paket agricolae schützt dich davor, keine der Behandlungslevel zu verlieren. Bei dem Selbermachen musst du dann immer schauen, ob dir der Versuchsplan so passt und alle Behandlungen einigermaßen vertreten und randomisiert sind.\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nWir rechnen ein multiples lineares Modell für die statistische Analyse. Vermutlich reicht die Fallzahl nicht für alle Interaktionen zwischen allen Faktoren.\n\nfit &lt;- lm(drymatter ~ fert + soil + fert:soil + inc_block, data = inc_block_tbl)\n\nWir rechnen ein multiples lineares Modell für die statistische Analyse.\n\nfit &lt;- lm(drymatter ~ trt + inc_block + trt:inc_block, data = inc_block_tbl)\n\n\n\n\n25.5.1 … mit expand_grid() (3-faktoriell)\nBei dem Selbermachen ist es jetzt eigentlich wie bei dem 3-faktoriellen randomized complete block design. Deshalb schaue einfach nochmal oben nach, wenn dir Teile der Generierung unklar sind. Hier und dort ändert sich ja bei mir immer nur leicht der Code, damit du auch mal was anderes siehst. Es gibt ja viele Wege nach Rom.\n\nincomplete_long_tbl &lt;- expand_grid(block = 1:2, soil = 1:4, fert = 1:6) |&gt; \n  mutate(block = factor(block, labels = str_c(\"Block \", 1:2)),\n         soil = factor(soil, labels = str_c(\"soil_\", 1:4)),\n         fert = factor(fert, label = str_c(\"fe_\", 1:6))) |&gt; \n  group_by(block) |&gt; \n  mutate(fert = sample(fert),\n         soil = sample(soil)) \n\nincomplete_long_tbl\n\n# A tibble: 48 × 3\n# Groups:   block [2]\n   block   soil   fert \n   &lt;fct&gt;   &lt;fct&gt;  &lt;fct&gt;\n 1 Block 1 soil_3 fe_3 \n 2 Block 1 soil_3 fe_3 \n 3 Block 1 soil_1 fe_4 \n 4 Block 1 soil_1 fe_6 \n 5 Block 1 soil_3 fe_2 \n 6 Block 1 soil_3 fe_6 \n 7 Block 1 soil_2 fe_2 \n 8 Block 1 soil_4 fe_3 \n 9 Block 1 soil_1 fe_4 \n10 Block 1 soil_2 fe_2 \n# ℹ 38 more rows\n\n\nNoch haben wir kein incomplete block design vorliegen, dass bauen wir uns jetzt. Wir rasieren nämlich einfach die letzten beiden Zeilen unseres randomized complete block design ab. Damit haben wir dann nur noch ein \\(4\\times4\\)-Block übrig. Sonst wäre es ja ein Block mit sechs Zeilen.\n\nincomplete_plot_tbl &lt;- incomplete_long_tbl |&gt; \n  bind_cols(expand_grid(rep = 1:2, cols = 1:4, rows = 1:6)) |&gt; \n  filter(rows &lt;= 4)\n\nJetzt müssen wir uns die Sachlage einmal anschauen. In der Abbildung 25.16 sehen wir einmal den Versuchsplan für das incomplete block design. Du musst immer schauen, ob wir wirklich alle Level jedes Behandlungsfaktors auch vorliegen haben und ob sich die Level auch einigermaßen gut verteilen. Das incomplete block design ist da etwas frickelig, ob es dann auch wirklich gut ist. Aber du kannst auch nciht alle Kombinationen vorliegen haben, denn sonst wäre es ja auch kein incomplete block design.\n\ndesplot(block ~ cols + rows | block, flip = TRUE,\n        out1 = rows, out1.gpar = list(col = \"grey\", lty = 1),\n        out2 = cols, out2.gpar = list(col = \"grey\", lty = 1), \n        text = fert, cex = 1, shorten = \"no\", col = soil,\n        data = incomplete_plot_tbl,\n        main = \"Incomplete block design (3-faktoriell)\",\n        show.key = TRUE)\n\n\n\n\n\n\n\nAbbildung 25.16— Incomplete block design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den zwei Blöcken.\n\n\n\n\n\n\n\n25.5.2 … mit {FielDHub}\nFür die Erstellung des incomplete block design in der Shiny App schaue dir die Anleitung unter Generates a Resolvable Incomplete Block Design mit {FielDHub} einmal an. Hier einmal der Code für die Erstellung in R. Leider ist das ganz viel rumprobieren, bis es dann mit den Parametern klappt. Dehalb empfhele ich dir hier auf jeden Fall die Shiny App, da kannst du viel einfach rumprobieren und mit den Anzahlen an Behandlungen und Blöcken spielen. Hier generierst du sehr viele Fehlermeldungen und versuchst dann ein Design zu finden was passt, dann musst du es ja noch visualisieren. Die Schritte sind dann in der Shiny App besser miteinander verwoben.\n\nincomplete_block_obj &lt;- incomplete_blocks(t = 6,\n                                          k = 3,\n                                          r = 3,\n                                          seed = 1984)\n\nDann können wir uns einmal das erstellte Feldbuch wiedergeben lassen, was du dann auch in Excel raus schreiben kannst.\n\nincomplete_block_obj$fieldBook |&gt; \n  as_tibble()\n\n# A tibble: 18 × 8\n      ID LOCATION  PLOT   REP IBLOCK  UNIT ENTRY TREATMENT\n   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    \n 1     1        1   101     1      1     1     3 G-3      \n 2     2        1   102     1      1     2     4 G-4      \n 3     3        1   103     1      1     3     6 G-6      \n 4     4        1   104     1      2     1     5 G-5      \n 5     5        1   105     1      2     2     2 G-2      \n 6     6        1   106     1      2     3     1 G-1      \n 7     7        1   201     2      1     1     4 G-4      \n 8     8        1   202     2      1     2     5 G-5      \n 9     9        1   203     2      1     3     6 G-6      \n10    10        1   204     2      2     1     3 G-3      \n11    11        1   205     2      2     2     1 G-1      \n12    12        1   206     2      2     3     2 G-2      \n13    13        1   301     3      1     1     2 G-2      \n14    14        1   302     3      1     2     5 G-5      \n15    15        1   303     3      1     3     6 G-6      \n16    16        1   304     3      2     1     1 G-1      \n17    17        1   305     3      2     2     4 G-4      \n18    18        1   306     3      2     3     3 G-3      \n\n\nDann erstellen wir auch schon die Abbildung 25.17 mit dem split plot design. Da wir hier wieder eine relativ einfache Abbildung haben, geht es dann auch mit der Darstellung in {FielDHub}. Leider sind auch hier die Zahlen teilweise sehr schwer auf den Farben zu erkennen.\n\nincomplete_block_obj |&gt; plot()\n\n\n\n\n\n\n\nAbbildung 25.17— Incomplete block design (2-faktoriell) für den Faktor trt randomisiert in den sechs Blöcken.\n\n\n\n\n\n\n\n25.5.3 … mit {agricolae} (2-faktoriell)\nDer Vorteil von {agricolae} und der Funktion design.bib() ist ganz klar die Effizienz und die optimale Verteilung der Behandlungsfaktoren auf die Blöcke. Wir können hier sicher sein das optimale Ergebnis der Randomisierung zu erhalten und müssen auch keine Sorge haben, das Level der Behandlungen abhanden gekommen sind. Deshalb kann ich hier die Funktion design.bib() in {agricolae} sehr empfehlen.\nDie Namen der Optionen sind nicht die Stärke von {agricolae}, also haben wir hier wieder die alten Probleme. Wir müssen darauf achten, dass wir die Anzahl an Blöcken mit k angeben. Wenn du zu wenig Blöcke bei zu vielen Behandlungslevel wählst, dann gibt dir die Funktion einen Fehler wieder und kein Ergebnis. An der Ausgabe der Funktion kannst du dann gleich deine Effizienz gegenüber einer vollständigen Randomisierung abschätzen.\n\nbib_obj &lt;- design.bib(trt = c(\"ctrl\", \"A\", \"B\", \"C\"),\n                      k = 3, seed = 543, serie = 2)\n\n\nParameters BIB\n==============\nLambda     : 2\ntreatmeans : 4\nBlock size : 3\nBlocks     : 4\nReplication: 3 \n\nEfficiency factor 0.8888889 \n\n&lt;&lt;&lt; Book &gt;&gt;&gt;\n\n\nWichtig ist hier, dass wir auf den Efficiency factor schauen. Wir sind mit unserer Randomisierung des incomplete block design ungefähr zu 89% so gut wie das gleichwertige randomized complete block design. Das ist schonmal gut. Wir sparen Platz auf dem Feld oder dem Stall und sind trotzdem nicht so viel schlechter als ein vollständig randomisiertes Design. Aber Achtung, die Aussagekraft ist hier geringer, für Pilotstudien ist das immer okay, für echte Feldstudien zur Zulassung oder für wissenschaftliche Publikationen eher weniger bis gar nicht.\nWir müssen in diesem Fall nicht so viel an den Daten bereinigen um uns gleich die Visualisierung anzuschauen. Das geht hier relativ einfach. Wir benennen nur die Spalten um und sind schon soweit fertig.\n\nbib_book_tbl &lt;- bib_obj |&gt; \n  pluck(\"book\") |&gt; \n  as_tibble() |&gt; \n  set_names(c(\"plots\", \"block\", \"trt\"))\nbib_book_tbl\n\n# A tibble: 12 × 3\n   plots block trt  \n   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;\n 1   101 1     C    \n 2   102 1     B    \n 3   103 1     A    \n 4   201 2     ctrl \n 5   202 2     C    \n 6   203 2     B    \n 7   301 3     A    \n 8   302 3     ctrl \n 9   303 3     C    \n10   401 4     ctrl \n11   402 4     A    \n12   403 4     B    \n\n\nDann ergänzen wir noch die Informationen zu den Spalten und den Zeilen, so dass wir dann auch gleich desplot() nutzen können. Wir müssen hier nochmal nach den Spalten gruppieren, damit wir die Zeilen sauber zugeordnet kriegen. Wir haben ja nicht für jede Behandlung eine Zeile sondern eben weniger. Ja, man hätte auch mit der Information von k = 3 hier arbeiten können. So finde ich es aber schöner.\n\nbib_plot_tbl &lt;- bib_book_tbl |&gt;\n  mutate(cols = as.numeric(block)) |&gt; \n  group_by(cols) |&gt; \n  mutate(rows = 1:n())\n\nEinmal der Vergleich zum sketch aus der Ausgabe von design.bib(). Siehst gut aus und entspricht unseren Erwartungen. Du siehst hier gut, dass im ersten Block die Kontrolle gar nicht enthalten ist.\n\nbib_obj |&gt; \n  pluck(\"sketch\") |&gt; \n  t()\n\n     [,1] [,2]   [,3]   [,4]  \n[1,] \"C\"  \"ctrl\" \"A\"    \"ctrl\"\n[2,] \"B\"  \"C\"    \"ctrl\" \"A\"   \n[3,] \"A\"  \"B\"    \"C\"    \"B\"   \n\n\nIn der Abbildung 25.18 sehen wir die Visualisierung unseres incomplete block design für den Faktor trt randomisiert in den vier Blöcken. Ich finde die farbliche Hinterlegung sehr schön, da sieht man viel schneller die eigentliche Zuordnung der Behandlungslevel zu den Blöcken. Auch siehst du schön, dass in dem vierten Block die Behandlung C gar nicht vorkommt. Was du dann nicht misst, kann auch nicht in der statistischen Auswertung berücksichtigt werden.\n\ndesplot(trt ~ cols + rows, flip = TRUE,\n        text = trt, cex = 1, shorten = \"no\",\n        out1 = block,\n        data = bib_plot_tbl,\n        main = \"Incomplete block design (2-faktoriell)\", \n        show.key = FALSE)\n\n\n\n\n\n\n\nAbbildung 25.18— Incomplete block design (2-faktoriell) für den Faktor trt randomisiert in den vier Blöcken.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Fortgeschrittene Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-advanced.html#sec-strip-plot",
    "href": "experimental-design-advanced.html#sec-strip-plot",
    "title": "25  Fortgeschrittene Designs",
    "section": "25.6 Strip plot design (3-faktoriell)",
    "text": "25.6 Strip plot design (3-faktoriell)\nDas Streifenparzellenversuch (eng. strip plot design) ist eigentlich ein doppeltes split plot design oder eben eine Erweiterung. Wo das split plot design in Spalten orientiert ist ergänzen wir jetzt durch das strip plot design noch die horizontale Dimension. Das heißt wir haben eine Randomisierung und Abhängigkeitsstruktur in den Spalten sowie in den Zeilen. Dabei orientieren wir uns an dem Strip-plot experiment of rice aus Gomez und Gomez (1984). Du findest dann das Datenbeispiel auch in {agridat} als data(gomez.stripplot). Ich habe versucht jetzt die Daten in der Form nachzubauen und somit kannst du dann auch ähnliche Experimente planen.\nUnser strip plot design hat drei Wiederholungen mit block, eine Sorte trt_gen als horizontaler Streifen und Stickstoffdünger trt_nitro als vertikaler Streifen. Das ganze klingt immer super abstrakt, deshalb gehe gerne gleich zur Auflösung in die Abbildung 25.20. Manchmal ist es besser sich gleich die Visualisierung anzuschauen und dann das folgende Modell.\n\\[\ny \\sim \\overbrace{trt_{gen}}^{f_1} + \\underbrace{trt_{nitro}}_{f_2} + \\overbrace{block}^{f_3}\n\\]\nmit\n\ndem Faktor Behandlung trt_gen und den sechs Linien als Level G_1 bis G_6.\ndem Faktor Dünger trt_nitro und den drei Düngestufen als Level 0, 60 und 120.\ndem Faktor Block block und den drei Leveln 1 bis 3.\n\nIn der Abbildung 25.19 dann einmal die Abhängigkeitsstruktur des split plot design. Wichtig ist hierbei, dass die genetischen Linien trt_gen in den Zeilen rows der Blöcke genestet und randomisiert sind. Wir sehen aber die Zeilen später nicht in der Auswertung. Die Zeilen werden durch den Faktor Block repräsentiert. Genauso ist es mit der Stickstoffbehandlung trt_nitro, die dann in den Spalten cols des Blocks genested und randomisiert wird. Auch hier sehen wir dann in der späteren Auswertung nur den Block.\n\n\n\n\n\n\nflowchart LR\n    A(trt_gen):::factor --- B(((nestet))) ---&gt; C(rows) --- E(block):::factor\n    F(trt_nitro):::factor --- G(((nestet))) ---&gt; H(cols) --- E\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 25.19— Schema der Abhängigkeitsstruktur des split plot design. Die genetischen Linien trt_gen sind in den Zeilen rows der Blöcke genestet und randomisiert. Die Stickstoffbehandlung trt_nitro ist dann in den Spalten cols des Blocks genested und randomisiert.\n\n\n\n\n\n\n\n\n\n\n\nModell zur Auswertung\n\n\n\nWir rechnen hier ein lineares gemischtes Modell für die Auswertung der Daten.\n\nfit &lt;- lmer(yield ~ trt_gen + nitro + trt_gen:nitro + \n              (1 | block) + (1 | block:nitro) + \n              (1 | block:trt_gen), \n            data = stripplot_tbl)\n\n\n\n\n25.6.1 … mit {agricolae}\nDas strip plot design kann ich selber machen, aber hier fängt es dann an wirklich so kompliziert zu werden, dass es sich nicht mehr lohnt. Dafür haben wir dann in {agricolae} die passende Funktion. Die würde ich dann auch immer empfehlen zu nutzen, dass macht das Leben wirklich einfacher. Wir müssen zwar wieder erstmal unsere Faktoren als Vektoren bauen und dann alles in den passenden Objekten ablegen, aber das ist nur wenig Aufwand im Vergleich zum Selbermachen.\n\ntrt_gen &lt;- str_c(\"G\", 1:6)\nn_trt_gen &lt;- n_distinct(trt_gen)\ntrt_nitro &lt;- c(0, 60, 120)\nn_trt_nitro &lt;- n_distinct(trt_nitro)\nn_block &lt;- 3\n\nJetzt kommt der einzige wichtige Teil, damit du später die Zeilen- und Spaltenpositionen einfach zuweisen kannst. Du musst in die Option trt1 = den kürzen Vektor packen. Hier hat der Dünger nur drei Level, also ist der Dünger in der Option trt1 = vertreten. Dann nehmen wir den längeren Vektor für die genetischen Linien in die zweite Option trt2 = mit rein. Nimm also erst den kurzen Vektor dann den langen Vektor, dann klappt es auch mit der späteren Zuweisung der Positionen leichter.\n\nstrip_obj &lt;- design.strip(trt1 = trt_nitro, # kürzerer Vektor (hier der Länge 3) \n                          trt2 = trt_gen,   # längerer Vektor (hier der Länge 6)\n                          r = n_block, serie = 2, seed = 543)\n\nFür die Zuweisung der richtigen Zeilen rows und Spalten cols im Positionsgrid müssen wir dann nicht so viel machen, wenn wir oben darauf geachtet haben, die Behandlungsfaktoren in der richtigen Reihenfolge den Optionen zuzuweisen. Die Zuweisung der cols funktioniert also nur, wenn du in der Funktion design.strip() wirklich den kürzeren vor dem längeren Vektor zuweist.\n\nstrip_plot_tbl &lt;- strip_obj$book |&gt; \n  bind_cols(expand_grid(cols = 1:(n_trt_nitro*n_block), \n                        rows = 1:n_trt_gen))\n\nDann sind wir schon soweit uns einmal in der Abbildung 25.20 das strip plot design anzuschauen. Du kannst sehr schon sehen, wie die genetischen Linien in den Zeilen randomisiert sind und die verschiedenen Stickstoffgaben dann in den Zeilen. Die drei Blöcke werden dann durch die dicken Linien dargestellt. Da wir es bei dem strip plot design häufig mit Großversuchen zu tun haben sind die Blöcke meistens durch Feldwege oder andere bauliche Bedingungen voneinander getrennt.\n\ndesplot(trt_gen ~ cols*rows,\n        out1 = block, \n        out2 = trt_nitro, out2.gpar = list(col = \"grey\", lty = 1), \n        col.regions = cbbPalette, \n        text = trt_nitro, cex = 1,\n        show.key = TRUE, data = strip_plot_tbl, shorten = \"no\",\n        main = \"Strip plot design (3-faktoriell)\")\n\n\n\n\n\n\n\nAbbildung 25.20— Strip plot design (3-faktoriell) für den Faktor trt_gen, in den Zeilen randomisiert sowie dem Faktor trt_nitro in den Saplten randomisiert und in drei Blöcken genestet.\n\n\n\n\n\n\n\n25.6.2 … mit {FielDHub}\nFür die Erstellung des strip plot design in der Shiny App schaue dir die Anleitung unter Strip Plot Design mit {FielDHub} einmal an. Hier einmal der Code für die Erstellung in R. Der Code ist fast so einfach wie auch eben in {agricolae}. Wir definieren wieder unseren horizontalen Parzellen Hplots und dann noch die vertikalen Parzellen Vplots. So wichtig ist die Zuordnung nicht. Du kannst ja auch einfach das Blatt drehen und dann sind die Parzellen anders angeordnet.\n\nstrip_plot_obj &lt;- strip_plot(Hplots = trt_nitro, # kürzerer Vektor (hier der Länge 3) \n                             Vplots = trt_gen,   # längerer Vektor (hier der Länge 6)\n                             b = n_block)\n\nDann können wir uns einmal das erstellte Feldbuch wiedergeben lassen, was du dann auch in Excel raus schreiben kannst.\n\nstrip_plot_obj$fieldBook |&gt; \n  as_tibble()\n\n# A tibble: 54 × 7\n      ID LOCATION  PLOT   REP HSTRIP VSTRIP TRT_COMB\n   &lt;int&gt; &lt;fct&gt;    &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n 1     1 1         1001     1    120 G6     120|G6  \n 2     2 1         1002     1    120 G2     120|G2  \n 3     3 1         1003     1    120 G3     120|G3  \n 4     4 1         1004     1    120 G1     120|G1  \n 5     5 1         1005     1    120 G5     120|G5  \n 6     6 1         1006     1    120 G4     120|G4  \n 7     7 1         1012     1     60 G6     60|G6   \n 8     8 1         1011     1     60 G2     60|G2   \n 9     9 1         1010     1     60 G3     60|G3   \n10    10 1         1009     1     60 G1     60|G1   \n# ℹ 44 more rows\n\n\nDann erstellen wir auch schon die Abbildung 25.21 mit dem strip plot design. Da wir hier wieder eine relativ einfache Abbildung haben, geht es dann auch mit der Darstellung in {FielDHub}. Leider sind auch hier die Zahlen teilweise sehr schwer auf den Farben zu erkennen und ich mag die Darstellung mit dem | nicht. Das geht dann wirklich schöner wie du in der Abbildung 25.20 sehen kannst. Da lohnt sich dann auch die Arbeit mit {desplot} nochmal aus dem Feldbuch was schöneres zu machen. Die Informationen sind ja da.\n\nstrip_plot_obj |&gt; plot()\n\n\n\n\n\n\n\nAbbildung 25.21— Strip plot design (3-faktoriell) für den Faktor trt_gen, in den Zeilen randomisiert sowie dem Faktor trt_nitro in den Saplten randomisiert und in drei Blöcken genestet.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Fortgeschrittene Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-advanced.html#referenzen",
    "href": "experimental-design-advanced.html#referenzen",
    "title": "25  Fortgeschrittene Designs",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 25.1 (a)— Sechs Eisendüngerbehandlung (Text: fe_1 bis fe_6) sowie vier Bodenbehandlung (Farbe: schwarz, rot, orange, grün) in vier Blöcken.\nAbbildung 25.1 (b)— Sechs Eisendüngerbehandlung (Text: fe_1 bis fe_6) sowie vier Bodenbehandlung (Farbe: schwarz, rot, orange, grün) in vier Blöcken.\nAbbildung 25.1 (c)— Vier Bodenbehandlung (Farbe: schwarz, rot, orange, grün) mit den individuellen Beobachtungen in vier Blöcken.\nAbbildung 25.3— Randomized complete block design (3-faktoriell) für die Faktoren Dünger fert sowie soil vollständig randomisiert in den beiden Blöcken.\nAbbildung 25.4— Schema der Pflanzungen nach den Behandlungen für das randomized complete block design. Die Farbwhal und die Beschriftung ist mehr als gewöhnungsbedürftig. Die Abbildung ist nicht sehr übersichtlich und nur begrenzt als Darstellung geeignet.\nAbbildung 25.5— Randomized complete block design (3-faktoriell) für die Faktoren Dünger fert sowie soil vollständig randomisiert in den vier Blöcken.\nAbbildung 25.7— Split plot design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den zwei Blöcken. Der Faktor fert ist in dem Faktor soil genestet und randomisiert.\nAbbildung 25.8— Split plot design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den zwei Blöcken. Der Faktor fert ist in dem Faktor soil genestet und randomisiert.\nAbbildung 25.9— Split plot design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den vier Blöcken. Der Faktor fert ist in dem Faktor soil genestet und randomisiert.\nAbbildung 25.10— Split plot design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den vier Blöcken. Der Faktor fert ist in dem Faktor soil genestet und randomisiert.\nAbbildung 25.13 (a)— … vollständig innerhalb Block randomisiert (subsampling_plot_tbl).\nAbbildung 25.13 (b)— … Behandlungen innerhalb Block randomisiert (subsampling_tray_plot_tbl).\nAbbildung 25.16— Incomplete block design (3-faktoriell) für die Faktoren Dünger fert sowie soil randomisiert in den zwei Blöcken.\nAbbildung 25.17— Incomplete block design (2-faktoriell) für den Faktor trt randomisiert in den sechs Blöcken.\nAbbildung 25.18— Incomplete block design (2-faktoriell) für den Faktor trt randomisiert in den vier Blöcken.\nAbbildung 25.20— Strip plot design (3-faktoriell) für den Faktor trt_gen, in den Zeilen randomisiert sowie dem Faktor trt_nitro in den Saplten randomisiert und in drei Blöcken genestet.\nAbbildung 25.21— Strip plot design (3-faktoriell) für den Faktor trt_gen, in den Zeilen randomisiert sowie dem Faktor trt_nitro in den Saplten randomisiert und in drei Blöcken genestet.\n\n\n\nGomez KA, Gomez AA. 1984. Statistical procedures for agricultural research. John wiley & sons.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Fortgeschrittene Designs</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html",
    "href": "experimental-design-samplesize.html",
    "title": "26  Fallzahlplanung",
    "section": "",
    "text": "26.1 Theoretischer Hintergrund\nManchmal hat man das Gefühl, dass Fallzahlplanung nur ein wildes Gerate ist. Das ist aus der Perspektive eines biologischen Fachlaien auch der Fall. Ich kenne mich sehr wenig in der vielen biologischen Feldern aus. Daher weiß ich wenig darüber was ein großer Effekt ist oder welchen Effekt du überhaupt in deinem Kartoffelexperiment erwarten kannst. Auch ist mir unklar was typische Mittelwertsunterschiede bei Wasserlinsen sind. Du musst so was aber wissen, es ist ja schließlich dein Experiment. Wenn du also eine Fallzahlplanung durchführen willst, dann heißt es zuerst einmal Literatur wälzen oder mit den Fachkollegen sprechen.\nWir kennen ja schon die Formel für den t-Test. Der t-Test vergleicht die Mittelwerte von zwei normalverteilten Outcomes und gewichtet diesen Mittelwertsunterschied bei der Standardabweichung. Da wir in der Formel des t-Tests auch die Fallzahl inkludiert haben, können wir die Formel nach der Fallzahl umstellen.\n\\[\nT = \\cfrac{\\Delta}{s_p \\cdot \\sqrt{\\cfrac{2}{n_g}}}\n\\]\nDabei nutzen wir die Teststatistik etwas anders. Wir zerlegen die Teststatistik \\(T\\) für in den Wert für den \\(\\alpha\\)-Fehler und den \\(\\beta\\)-Fehler. Damit können wir auch die Power \\(1-\\beta\\) mit in unserer Formel berücksichtigen.\n\\[\nn_g = \\cfrac{2\\cdot(T_{\\alpha = 5\\%} + T_{\\beta = 20\\%})^2}{\\left(\\cfrac{\\Delta}{s_p}\\right)^2}\n\\]\nDabei nutzen wir für \\(T_{\\alpha = 5\\%} = 1.96\\) und \\(T_{\\beta = 20\\%} = 0.84\\) und vereinfachen damit die Formel ziemlich. Eigentlich nutzen wir diese Formel dann in der der Klausur oder aber um wirklich mal eben schnell zu schauen, was wir für eine Fallzahl erwarten.\nJetzt könntest du meinen, dass wir jetzt mit verschiedenen Powerleveln spielen könnten. Aber das ist leider nicht der Fall. Wir sind eigentlich ziemlich auf 80% festgelegt. Da gibt es im Rahmen eines Antrags keinen Spielraum. Wir nehmen immer eine Power von 80% an.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#theoretischer-hintergrund",
    "href": "experimental-design-samplesize.html#theoretischer-hintergrund",
    "title": "26  Fallzahlplanung",
    "section": "",
    "text": "Einseitig oder zweiseitig im Spiegel der Regulierungsbehörden\n\n\n\nIn den allgemeinen Methoden des IQWiG, einer Regulierungsbehörde für klinische Studien, wird grundsätzlich das zweiseitige Testen empfohlen. Wenn einseitig getestet werden sollte, so soll das \\(\\alpha\\)-Niveau halbiert werden. Was wiederum das gleiche wäre wie zweiseitiges Testen - nur mit mehr Arbeit.\nZur besseren Vergleichbarkeit mit 2-seitigen statistischen Verfahren wird in einigen Guidelines für klinische Studien eine Halbierung des üblichen Signifikanzniveaus von 5 % auf 2,5 % gefordert. – Allgemeine Methoden Version 6.1 vom 24.01.2022, p. 180\nFazit des Dokumentes ist dann aber, dass wir immmer zu einem Signifikanzniveau \\(\\alpha\\) von 5% und einer Power von 80% testen.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#tierversuchsantrag",
    "href": "experimental-design-samplesize.html#tierversuchsantrag",
    "title": "26  Fallzahlplanung",
    "section": "26.2 Tierversuchsantrag",
    "text": "26.2 Tierversuchsantrag\nWenn du einen Tierversuch durchführen willst, dann bist du natürlich hier falsch. Ich kann dir bei dem Ausfüllen von Dokumenten nicht helfen. Was ich aber kann, ist dir einen Überblick über die Inhalte zu geben, so dass du dann nochmal informiert an anderer Stelle Fragen stellen kanst. Schaue gerne einmal mein Video auf YouTube mit dem Kontext zum Tierversuchsvorhaben. Eine wunderbare Übersicht über den Tierversuchsantrag liefert auch Piper u. a. (2022).\n\n\n\n\n\n\nEinführung in den Kontext zu Tierversuchsvorhaben per Video\n\n\n\nDu findest auf YouTube Kontext zu Tierversuchsvorhaben als Video Reihe. Es handelt sich hierbei um ein reines Lehrvideo mit keinem beratenden Anspruch.\n\n\nIn dem Video habe ich dann alles anhand des Tierversuchsvorhaben am LaGeSo in Berlin besprochen. Das hatte den Grund, dass ich zur Zeit des Videos an der Charité beschäftigt war. Da bei einem Tierversuchsantrag jeweils die Bundesländer zuständig sind, musst du bei deiner jeweiligen Ladesbehörde einmal schauen. In Niedersachsen musst du dir die Wenseite zu Tierversuche vom Laves anschauen. Hier findest du dann andere Dokumente und Ausfüllhilfen. Wenn man als Wissenschaftler viel wechselt, wird man leicht wirr.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#ethikantrag",
    "href": "experimental-design-samplesize.html#ethikantrag",
    "title": "26  Fallzahlplanung",
    "section": "26.3 Ethikantrag",
    "text": "26.3 Ethikantrag\nEben hatten wir uns kurz den Antrag für ein Tierversuchsvorhaben angeschaut. Richtig kompliziert wird es, wenn wir nicht mit Tieren arbeiten sondern Versuche am Menschen durchführen. Ein versuch am Menschen beinhaltet schon das Ausfüllen eines Fragebogens! Daher kannst du auch schnell in die Situation kommen, dass es eventuell eine ethische Komplikation gibt. Ich habe die Inhalte im Kontext einer klinischen Studie einmal in einem YouTube Video dargestellt und allgemein eingeordnet.\n\n\n\n\n\n\nEinführung in den Kontext zum Ethikantrag per Video\n\n\n\nDu findest auf YouTube Kontext zum Ethikantrag als Video Reihe. Es handelt sich hierbei um ein reines Lehrvideo mit keinem beratenden Anspruch.\n\n\nDa ich in meiner Lehre die klinischen Studie nur am Horizont sehe, gibt es hier auch keine weiteren Links zu dem Thema. In dem Video siehst du noch ein paar öffentliche Quellen. Da es sich aber bei einem Ethikantrag meist um einen internen Prozess einer Universitätsklinik handelt, sind die (aktuellen) Dokumente meist nicht öffentlich zugänglich. Im Zweifel bitte an die zuständigen Gremien an deiner Institution wenden.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#genutzte-r-pakete",
    "href": "experimental-design-samplesize.html#genutzte-r-pakete",
    "title": "26  Fallzahlplanung",
    "section": "26.4 Genutzte R Pakete",
    "text": "26.4 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, pwr, readxl, see,\n               effectsize, conflicted)\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#mittelwertsvergleich-für-zwei-gruppen-in-r",
    "href": "experimental-design-samplesize.html#mittelwertsvergleich-für-zwei-gruppen-in-r",
    "title": "26  Fallzahlplanung",
    "section": "26.5 Mittelwertsvergleich für zwei Gruppen in R",
    "text": "26.5 Mittelwertsvergleich für zwei Gruppen in R\nDa wir ja nur die Formel des t-Tests für die Fallzahlberechnung haben, können wir auch immer nur die Fallzahl für den Vergleich zwischen zwei Gruppen rechnen. Das ist immer erstmal wieder ungewohnt. Aber wir machen das jetzt erstmal nur für zwei Gruppen. Später schauen wir uns an, ws passiert, wenn wir mehr Gruppen miteinander vergleichen wollen. Prinzipiell ist der Kern aber immer ein Zweigruppenvergleich, den wir dann etwas anders Aufbauen.\nWenn du für einen Wilcoxon-Test oder einen anderen nicht-parametrischen Test die Fallzahlplanung machen willst, rechne bitte einen t-Test und addiere \\(+15\\%\\) an Fallzahl drauf.\nFür die Berechnung der Fallzahl wollen wir das R Paket {pwr} nutzen. Wir brauchen in diesem Kapitel nur drei Funktion aus dem Paket, aber es gibt auch weit aus mehr. Im Zweifel einfach einmal die Hilfeseite aufrufen und schauen was es dort noch so gibt.\nWir können mit der Funktion pwr.t.test() die Fallzahl für die Effektstärke nach Cohen’s \\(d\\) berechnen. Mehr über Cohen’s \\(d\\) kannst du im Kapitel 21 erfahren. Wir nutzen hier eine relativ harte Abschätzung. Aber hier wird sowieso alles abgeschätzt, da kommt es jetzt auf künstliche Genauigkeit nicht mehr an. Wir berechnen also Cohen’s \\(d\\) vereinfacht für die Fallzahlberechnung wie folgt.\n\\[\nd = \\cfrac{\\Delta}{s_{\\Delta}}\n\\]\nmit\n\n\\(\\Delta\\) als den zu erwartenden Mittelwertsunterschied zwischen den beiden Gruppen. Wir haben den Wert aus der Literatur entnommen.\n\\(s_{\\Delta}\\) als der Standardabweichung des Mittelwertsunterschieds. Wir können hier als Vereinfachung mit der Spannweite der Daten mit \\(\\frac{range}{4}\\) als Schätzer für die Standardabweichung rechnen. Ebenfalls haben wir die Werte aus einer Literaturquelle.\n\nEs gäbe auch die Möglichkeit über die Funktion cohen.ES() die Effekte für verschiedene statistische Tests sich wiedergeben zu lassen, wenn wir definieren, wie stark der Effekt zwischen den Gruppen sein soll. Es steht zur Auswahl small, medium und large. Wir erkennen, dass ist nicht gerade viel Abstufung.\n\ncohen.ES(test = \"t\", size=\"medium\") |&gt; \n  pluck(\"effect.size\")\n\n[1] 0.5\n\n\nDie Fallzahlberechnung geht recht einfach. Wir setzen die Option n = auf NULL, so dass uns die Funktion diese Option berechnet. Wir kriegen also die Fallzahl gegeben von dem Signifikanzniveau, der Power und der Effektstärke wieder. Dann geben wir noch an, dass wir zweiseitig testen. Also eigentlich alles fix, da können wir selber zwar was ändern, aber am Ende wird meist nur die Standardwerte von Dritten akzeptiert.\n\nres_ttest &lt;- pwr.t.test(n = NULL,\n                        sig.level = 0.05, \n                        type = \"two.sample\", \n                        alternative = \"two.sided\", \n                        power = 0.80, \n                        d = 0.8)\nres_ttest\n\n\n     Two-sample t test power calculation \n\n              n = 25.52458\n              d = 0.8\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nBitte immer Aufrunden. Wir brauchen also \\(n_1 = n_2 = 26\\) Beobachtungen je Gruppe, so dass wir für \\(32\\) beobachtungen unseren Versuch planen können. In Abbildung 26.1 sehen die Power abhängig von der verwendeten Fallzahl. Wir sehen, dass wir mit mehr Fallzahl eine höhere Power erhalten würden, aber wir schon sehr nah an der Sättigung sind.\n\nplot(res_ttest) +\n  theme_minimal(base_size = 14) +\n  labs(title = 'Optimierte Fallzahl für den Zweistichproben t-Test.')\n\n\n\n\n\n\n\nAbbildung 26.1— Optimierte Fallzahl für den Zweistichproben t-Test.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#anteilsvergleich-für-zwei-gruppen-in-r",
    "href": "experimental-design-samplesize.html#anteilsvergleich-für-zwei-gruppen-in-r",
    "title": "26  Fallzahlplanung",
    "section": "26.6 Anteilsvergleich für zwei Gruppen in R",
    "text": "26.6 Anteilsvergleich für zwei Gruppen in R\nWann benötigen wir Anteile? Häufig nutzen wir Anteile, wenn wir zum Beispiel infizierte Ferkel untr zwei Behandlungen untersuchen wollen. Wie viel Prozent der Ferkel in der einen Gruppe werden infiziert sein und wieviel Ferkel in der anderen Gruppe. Daher haben wir ein Medikament und wollen schauen, ob sich die Anzahl an infizierten Ferkeln reduziert. Wir nehmen aber nicht die Anzahl als Wert sondern die relative Angabe. Im folgenden Beispiel haben wir \\(95\\%\\) infizierte Ferkel in der einen Gruppe und \\(80\\%\\) infizierte Ferkel in der anderen Gruppe. Wie viel Fallzahl brauchen wir nun, um diesen Untrschied nachzuweisen. Achtung, wir rechnen hier wirklich mit den relativen Zahlen und nicht mit der Differenz. Ist leider so.\nWir können die Funktion ES.h() benutzen um den Effekt zwischen zwei Wahrscheinlichkeiten zu berechnen. Wir geben einfach die beiden Wahrscheinlichkeiten für die zu erwartende Häufigkeit an infizierten Ferkeln ein. Dann berechnen wir den Effekt \\(h\\) und nutzen diesen Wert dann für die Fallzahlberechnung.\n\nES.h(p1 = 0.95, p2 = 0.80) |&gt; \n  round(2)\n\n[1] 0.48\n\n\nHier kommt es dann auch nicht wieder auf die letzte Prozentzahl an. Wir immer kann man hhier spielen. Aber du hast ja deine Zahlen aus der Literatur und passt diese Zahlen dann deinem Setting und anhand deinem biolologischen Wissen an. Es ist immer eine Gradwanderung, wie genau die Zahlen nun seien sollen. Insbesondere, wenn es dann doch nicht so viel Literatur gibt. Wir setzen die Option n = auf NULL, so dass uns die Funktion diese Option berechnet. Wir kriegen also die Fallzahl gegeben von dem Signifikanzniveau, der Power und der Effektstärke wieder. Dann geben wir noch an, dass wir zweiseitig testen.\n\nres_prop &lt;- pwr.p.test(h = 0.48,\n                       n = NULL,\n                       sig.level = 0.05,\n                       power = 0.80,\n                       alternative = \"two.sided\")\nres_prop\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.48\n              n = 34.06623\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nAm Ende erhalten wir eine Fallzahl von \\(n_1 = n_2 = 35\\) Beobachtungen aus der Fallzahlberechnung. Wir wissen also, wie viele Ferkel wir untersuchen müssten um einen Unterschied von \\(95\\%\\) zu \\(80\\%\\) signifikant nachweisen zu können. In Abbildung 26.2 sehen wir nochml die Sättigungskurve für die Power für verschiedene Fallzahlen. Mit unserer berechneten Fallzahl von \\(n=35\\) pro Gruppe sind wir schon recht nah an der Sättigung der Funktion. Wir können damit die Fallzahl beibehalten und uns übrlegen, ob wir überhaupt das Geld und die Ressourcen haben um den Versuch mit dieser Anzahl an Ferkeln durchzuführen.\n\nplot(res_prop) +\n  theme_minimal(base_size = 14) +\n  labs(title = 'Optimierte Fallzahl für zwei Anteile.')\n\n\n\n\n\n\n\nAbbildung 26.2— Optimierte Fallzahl für zwei Anteile.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#anteil-der-erklärten-varianz-in-r",
    "href": "experimental-design-samplesize.html#anteil-der-erklärten-varianz-in-r",
    "title": "26  Fallzahlplanung",
    "section": "26.7 Anteil der erklärten Varianz in R",
    "text": "26.7 Anteil der erklärten Varianz in R\nNun können wir die Fallzahlplanung auch für eine einfaktorielle ANOVA durchführen. Das ist unsere Basis. Wir würden dann überlegen, wie sich dann die Fallzahl mit weiteren Faktoren ändern würde. Auch hier ein Wort der Warnung. Es gibt häufig so starke Randbedingungen, wie Kosten oder Fläche, dass die Berechnung der Fallzahl absolet wird. Wenn du drei Blöcke hat, dann hast du drei Blöcke. nutze die Blöcke dann auch. Wenn du freeie Wahl hättest und viel, viel Geld, dan kann man sicherlich besser die Fallzahl abschätzen und nutzen. Fallzahlberechnung nur so zum Spaß hat dann ja auch wenig Sinn. Also hier nochmal unser Modell was wir uns mit einer einfaktoriellen ANOVA anschauen.\n\\[\ny \\sim f_1\n\\]\nWir immer brauchen wir auch einen Effekt. In dem Fall der ANOVA ist der Effekt Cohen’s \\(f\\). Wir berechnen Cohen’s \\(f\\) wie folgt aus dem \\(\\eta^2\\). Wir können an dieser Stelle schon die Werte für \\(\\eta^2\\) einsetzen und \\(f\\) berechnen.\n\\[\nf = \\sqrt{\\cfrac{\\eta^2}{1- \\eta^2}}\n\\]\nDu erinnerst dich aus der ANOVA, das \\(\\eta^2\\) beschreibt den Anteil an erklärter Varianz durch den Faktor in der ANOVA. Damit ist \\(\\eta^2\\) wie folgt definiert.\n\\[\n\\eta^2 = \\cfrac{SS_{treat}}{SS_{total}}\n\\]\nDas hilft uns nur so begrenzt weiter. Am besten überlegst du dir, wieviel Varianz wohl die Behandlung erklären kann. Damit hast du dann dein \\(\\eta^2\\). Wenn deine Behandlung vermutlich ca. 70% der Varianz in deinen Daten erklären und somit im Ourtcome erklären kann, dann setzt du \\(\\eta^2 = 0.7\\). Dann berechnest du dein \\(f = \\sqrt{\\tfrac{0.7}{0.3}} = 1.53\\) und hast damit einen sehr großen Effekt. Was dir auch die Funktion interpret_eta_squared() aus dem R Paket {effectsize} mitteilt.\n\ninterpret_eta_squared(1.53)\n\n[1] \"large\"\n(Rules: field2013)\n\n\nWir können dann Cohen’s \\(f\\) in die Funktion pwr.anova.test() stecken und die Fallzahl pro Gruppe ausrechnen. Wir haben jetzt mal einen Faktor mit drei Behandlunsgleveln angenommen, deshalb ist auch \\(k = 3\\) in der Funktion.\n\nres_anova &lt;- pwr.anova.test(k = 3,\n                            f = 1.5,\n                            n = NULL,\n                            sig.level = 0.05,\n                            power = 0.80)\nres_anova\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 2.713068\n              f = 1.5\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nTja, mit so einem großen Effekt brauchen wir wirklich wenig Wiederholungen um mindestens einen Unterschied nachzuweisen. Stimmt, wir haben natürlich auch nur global über alle Gruppen geschaut. Ich finde die Fallzahlplanung für eine ANOVA relativ eingeschränkt, aber so ist das eben auch bei der Fallzahlplanung. Meistens ist das was möglich ist sehr eingeschränkt.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#mehr-als-zwei-gruppen",
    "href": "experimental-design-samplesize.html#mehr-als-zwei-gruppen",
    "title": "26  Fallzahlplanung",
    "section": "26.8 Mehr als zwei Gruppen",
    "text": "26.8 Mehr als zwei Gruppen\nWas passiert, wenn wir mehr als zwei Gruppen vorliegen haben? Was eigentlich immer der Fall ist. Also wir haben nicht nur zwei Düngestufen oder zwei Sorten Blumenkohl, die wir miteinander vergleichen wollen, sondern wir haben zehn oder mehr. Wir bauen jetzt nicht so ein großes Beispiel sondern nehmen einmal die Sprungweiten von den Hunde-, Katzen- und Fuchsflöhen.\n\nfleas_tbl &lt;- read_excel(\"data/flea_dog_cat_fox.xlsx\") |&gt; \n  mutate(animal = as_factor(animal))\n\nIn Abbildung 26.3 sehen wir nochmal die Verteilung der Sprungweiten für die drei Tierarten als Boxplots dargestellt.\n\nggplot(fleas_tbl, aes(animal, jump_length, fill = animal)) + \n  theme_minimal() +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 26.3— Boxplot der Sprungweiten für die Hunde-, Katzen- und Fuchsflöhe.\n\n\n\n\n\nWenn wir jetzt für ein neues Experiment die Fallzahl planen wollen würden, dann brauchen wir die Mittelwerte und die Standardabweichung der Sprungweiten. Wir haben ja hier unser Pilotexperiment vorliegen, also können wir auch hier die Mittelwerte und die Standardabweichung getrennt für die Tierarten berechnen.\n\nfleas_tbl |&gt; \n  group_by(animal) |&gt; \n  summarise(mean = mean(jump_length),\n            sd = sd(jump_length)) |&gt; \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 3 × 3\n  animal  mean    sd\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 dog     8.13  2.14\n2 cat     4.74  1.9 \n3 fox     9.16  1.1 \n\n\nNatürlich sind wir nicht an den Mittelwerten sondern an den Unterschieden interessiert. Daher rechnen wir nochmal in Tabelle 26.1 alle Mittelwertsdifferenzen aus.\n\n\n\nTabelle 26.1— Mittelwertsdifferenzen für alle paarweisen Vergleiche.\n\n\n\n\n\n\ncat\nfox\n\n\ndog\n\\(8.13 - 4.74 = 3.39\\)\n\\(8.13 - 9.16 = -1.03\\)\n\n\ncat\n\n\\(4.74 - 9.16 = -4.42\\)\n\n\n\n\n\n\nWenn wir die kleinste Differenz in den Mittelwerten mit einer Power von 80% nachweisen können, dann können wir auch alle anderen größeren Mittelwertsdifferenzen mit einer Power größer als 80% nachweisen. Daher brauchen wir die Fallzahlplanung nur für den kleinsten Mittelwertsunterschied durchführen. Wir berechnen noch Cohen’s d mit \\(d = \\tfrac{1.03}{(2.14 + 1.1)/2} \\approx 0.16\\). Ganz schön kleiner Wert, wie uns die Funktion interpret_cohens_d() aus dem R Paket {effectsize} mitteilt.\n\ninterpret_cohens_d(0.15)\n\n[1] \"very small\"\n(Rules: cohen1988)\n\n\nWeil wir es können berechnen wir auch die Fallzahl und kriegen einen kleinen Schreck. Denn mit einem so kleinen Effekt brauchen wir wirklich viele Flöhe.\n\nres_flea &lt;- pwr.t.test(n = NULL,\n                       sig.level = 0.05, \n                       type = \"two.sample\", \n                       alternative = \"two.sided\", \n                       power = 0.80, \n                       d = 0.16)\nres_flea\n\n\n     Two-sample t test power calculation \n\n              n = 614.1541\n              d = 0.16\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nMit am Ende über \\(2 \\cdot 615 = 1230\\) Flöhen für den Vergleich von Hunde- und Fuchsflöhen sind wir wirklich weit, weit oben was die Fallzahl angeht. Da hilft es dann auch nicht viel, dass wir mit zusätzlich \\(615\\) Katzenflöhen dann auch die anderen paaweisen Vergleichw als signifikant finden würden. Denn Cohen’s d für den Vergleich von den Hunde- und Katrzenflöhen wäre \\(d = \\tfrac{3.39}{(2.14 + 1.9)/2} \\approx 0.42\\). Damit würden wir dann eine Power von \\(0.99999997\\) erhalten. Wir können die Power berechnen indem wir das Feld Power mit NULL belegen und die Fallzahl von \\(n = 615\\) eintragen.\n\nres_flea &lt;- pwr.t.test(n = 615,\n                       sig.level = 0.05, \n                       type = \"two.sample\", \n                       alternative = \"two.sided\", \n                       power = NULL, \n                       d = 0.42)\nres_flea\n\n\n     Two-sample t test power calculation \n\n              n = 615\n              d = 0.42\n      sig.level = 0.05\n          power = 1\n    alternative = two.sided\n\nNOTE: n is number in *each* group",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#gpower-als-alternative",
    "href": "experimental-design-samplesize.html#gpower-als-alternative",
    "title": "26  Fallzahlplanung",
    "section": "26.9 G*Power als Alternative",
    "text": "26.9 G*Power als Alternative\n\n\n\n\n\n\nEinführung in G*Power als Alternative per Video\n\n\n\nDu findest auf YouTube G*Power als Alternative als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nAls Alternative zu R wollen wir uns noch das Standalone Programmm G*Power | Statistical Power Analyses von der Heinrich-Heine-Universität Düsseldorf anschauen. Die Software ist nicht mehr die neuste, wird aber immer noch gewartet und an die aktuellen Versionen von Mac und Windows angepasst. Manchmal ist dann Point und Klick dann doch eine Alternative, wenn man sich ausprobieren will.\nIch werde also im Folgenden ein paar Screenshots zeigen, wie du mi G*Power dir auch die Fallzahl für Mittelwertsunterschiede und Anteilesunterschiede berechnen kannst. Allgemein ist es so das G*Power immer einseitig (eng. Tail(s) one) und zu einer Power von 95% testet. Daher müssen wir immr schauen, dass diese Werte stimmen. Insbeosndere, wenn du viel rumprobierst können auch die Werte mal wieder zurückspringen. Also bitte darauf achten.\nIn Abbildung 26.4 sehen wir die Berechnung der Fallzahl für den t-Test für einen Vergleich zweier Gruppen. Wir müssen darauf achten, dass wir die Testfamilie richtig wählen und dann den korrekten Test auswählen. Du siehst bei der eigenen Verwendung dann, dass es hier eine große Auswahl gibt. Wir nehmen aber den Standard von zwei unabhängigen Gruppen. Wir erhalten dann eine Fallzahl von \\(n = 54\\) für unseren Versuch. Das schöne an G*Power ist, dass du relativ einfach und schnell mit den Zahlen spielen kannst. Das Speichern ist schwerer, so dass ich immer einen Screenshot empfehle. Man vergisst schnell, was alles in den Feldern stand.\n\n\n\n\n\n\n\nAbbildung 26.4— Die Berechnung der Fallzahl für einen t-Test für zwei Gruppen mit einem Mittelwert +/- Standardabweichung von \\(14 \\pm 2\\) in der einen Gruppe und \\(16 \\pm 3\\) in der anderen Gruppe. Es ergibt sich ein Cohens’ d von \\(0.78\\). Wir müssen darauf achten zweiseitig und zu einer Power von 80% zu testen.\n\n\n\n\nIn Abbildung 26.5 sehen wir die Berechnung der Fallzahl für zwei Anteile. Wir haben zwei Gruppen vorliegen und in der ersten Gruppe haben wir 60% infizierte Ferkel, In der anderen Gruppe erwarten wir dann 90% infizierte Ferkel. Um den Unterschied von 30% nachzuweisen, brauchen wir mindestens 180 Ferkel. Leider ist es so, dass wir den Test für Anteile unter dem Reiter Exact finden. Das muss man eben wisen. Achte wieder auf die Power und das zu zweiseitig testen willst.\n\n\n\n\n\n\n\nAbbildung 26.5— Die Berechung der Fallzahl für einen Anteil in zwei Gruppen von \\(0.6\\) in der einen Gruppe und \\(0.8\\) in der anderen Gruppe. Wir müssen darauf achten zweiseitig und zu eienr Power von 80% zu testen.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "experimental-design-samplesize.html#referenzen",
    "href": "experimental-design-samplesize.html#referenzen",
    "title": "26  Fallzahlplanung",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 26.1— Optimierte Fallzahl für den Zweistichproben t-Test.\nAbbildung 26.2— Optimierte Fallzahl für zwei Anteile.\nAbbildung 26.3— Boxplot der Sprungweiten für die Hunde-, Katzen- und Fuchsflöhe.\nAbbildung 26.4— Die Berechnung der Fallzahl für einen t-Test für zwei Gruppen mit einem Mittelwert +/- Standardabweichung von \\(14 \\pm 2\\) in der einen Gruppe und \\(16 \\pm 3\\) in der anderen Gruppe. Es ergibt sich ein Cohens’ d von \\(0.78\\). Wir müssen darauf achten zweiseitig und zu einer Power von 80% zu testen.\nAbbildung 26.5— Die Berechung der Fallzahl für einen Anteil in zwei Gruppen von \\(0.6\\) in der einen Gruppe und \\(0.8\\) in der anderen Gruppe. Wir müssen darauf achten zweiseitig und zu eienr Power von 80% zu testen.\n\n\n\nPiper SK, Zocholl D, Toelch U, Roehle R, Stroux A, Hoessler J, Zinke A, Konietschke F. 2022. Statistical review of animal trials—A guideline. Biometrical Journal.",
    "crumbs": [
      "Experimentelle Designs",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Fallzahlplanung</span>"
    ]
  },
  {
    "objectID": "stat-tests-preface-app.html",
    "href": "stat-tests-preface-app.html",
    "title": "Statistische Gruppenvergleiche",
    "section": "",
    "text": "Welcher Test soll es sein?\nIn der Abbildung 1 siehst du einmal einen simplen Entscheidungsbaum für die Auswahl eines statistischen Test für Gruppenvergleiche dargestellt. Natürlich können nicht alle Eigenheiten eines Datensatzes oder eines experimentellen Designs in so einer Abbildung berücksichtigt werden. Wir gehen deshalb noch auf einige Eigenheiten nochmal in den folgenden Abschnitten ein.\nflowchart TD\n    A(\"Normalverteiltes Outcome y\"):::factor --- B(((ja))) --&gt; B1 %% C(Varianzhomogenität):::factor \n    A(\"Normalverteiltes Outcome y\"):::factor --- F(((nein))) --&gt; B2 %%G(Varianzhomogenität):::factor \n    subgraph B1[\"Mittelwertsvergleiche\"]\n    C(\"Vergleich zweier Gruppen\"):::factor --- D(((\"ja\"))) --&gt; E(\"t-Test\"):::factor\n    C(\"Vergleich zweier Gruppen\"):::factor --- J(((nein))) --&gt; K(\"ANOVA\"):::factor \n    end\n    subgraph B2[\"Medianvergleiche\"]\n    G(\"Vergleich zweier Gruppen\"):::factor --- H(((ja))) --&gt; I(\"Wilcoxon-Mann-\n    Whitney-Test\"):::factor  \n    G(\"Vergleich zweier Gruppen\"):::factor --- L(((nein))) --&gt; M(\"Kruskal-\n    Wallis-Test\"):::factor  \n    end\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n\n\n\n\n\nAbbildung 1— Flowchart für die Entscheidung welcher statistischer Test für einen Gruppenvergleich mit zwei oder mehr Gruppen durchgeführt werden sollte. Zuerst schauen wir jedoch, ob wir ein normalverteiltes Outcome \\(y\\) vorliegen haben.\nWenn du ein normalverteiltes Outcome \\(y\\) vorliegen hast und dann wissen möchtest, ob sich eine Einflussvariable innherlab ihrer Gruppen sich unterscheidet, dann bietet sich der t-Test für den Zweigruppenfall an. Wenn du mehr als zwei Gruppen vorliegen hast, dann nutzt du die ANOVA oder aber ANCOVA, wenn du nicht nur Gruppen vergleichen möchtest.\nManchmal kommt es vor, dass wir kein normalverteiltes Outcome haben. Eine Lösung ist dann die Mediane und nicht die Mittelwerte miteinander zu vergleichen. Eine andere Lösung wäre das Outcome \\(y\\) zu transformieren, so dass wir dann wieder eine approximativ (deu. näherungsweise) Normalverteilung vorliegen haben. Schauen wir uns aber die Standardlösung für die Vergleiche zweier Mediande mit dem Wilcoxon-Mann-Whitney-Test einmal an. Wenn du mehr als zwei Gruppen hast, dann nutzen wir den Kruskal-Wallis-Test.\nSoweit der schmale Überblick. Es geht natürlich weit mehr. Wenn du zum Beispiel einen signifikanten Unterschied bei der ANOVA gefunden hast, möchtest du ja wissen, welche paarweisen Gruppenvergleiche sich nun wirklich unterscheiden. Hier helfen dir dann eine Reihe von Post-hoc Tests weiter. Dann wird die Sachlage schon etwas unübersichtlicher.\nJetzt schauen wir uns noch ein paar Spezialfälle einmal näher an, die du berücksichtigen kannst, aber nicht immer musst.",
    "crumbs": [
      "Statistische Gruppenvergleiche"
    ]
  },
  {
    "objectID": "stat-tests-preface-app.html#welcher-test-soll-es-sein",
    "href": "stat-tests-preface-app.html#welcher-test-soll-es-sein",
    "title": "Statistische Gruppenvergleiche",
    "section": "",
    "text": "Der Student t-Test oder Welch t-Test\n\nDer t-Test ermöglicht es dir zwei normalverteilte Gruppen hinsichtlich ihrem Mittelwertsunterschied zu vergleichen. Oder andersherum, unterscheiden sich die Hunde- und Katzenflöhe in ihrer mittleren Sprungweite? Der t-Test hat unbestreitbare Vorteile in dem Verständnis und der Interpretierbarkeit. Deshalb haben wir auch den t-Test genutzt um einmal das statistische Hypothesentesten zu verstehen.\n\nDie ANOVA und die ANCOVA\n\nDie ANOVA ermöglicht dir mehr als nur zwei Gruppen hinsichtlich ihrem mittleren Unterschied zueinander zu vergleichen. Auch hier gehen wir von einem normalverteilten Outcome aus. Das heißt, unterscheiden sich die mittleren Sprungweiten der Hunde-, Katzen-, und Fuchsflöhe? Die Stärke der ANOVA ist, auch noch einen anderen Faktor mit in das Modell zu nehmen. Wir könnten also noch den Ort der Erfassung mit aufnehmen. Unterschieden sich die mittleren Sprungweiten auch in Dörfern, Städten oder dem Land? Wenn du dann noch das Gewicht der Flöhe mit ins Spiel bringen willst, dann schaue dir einmal die ANCOVA an.\n\n\n\n\nDer Wilcoxon-Mann-Whitney-Test oder auch U-Test\n\nDer Wilcoxon-Mann-Whitney-Test ist im Prinzip das Gleiche wie der t-Test. Wir betrachten hier auch zwei Gruppen, die nicht normalverteilt sein müssen. Wir fragen dann, unterscheiden sich die medianen Sprungweiten der Hunde- und Katzenflöhe? Das Konzept sich einen Unterschied im Median anzuschauen ist möglich hat dann aber den Nachteil, dass wir häufig den medianen Unterschied nicht so schön interpretieren können.\n\nDer Kruskal-Wallis-Test und der Friedman-Test\n\nWenn du merh als zwei Gruppen vorliegen hast, dann kannst du den Kruskal-Wallis-Test nutzen. Auch hier testet du, ob sich ein medianer Unterschied zwischen den nicht normalverteilten Gruppen nachweisen lässt. Unterschieden sich also die medianen Sprungweiten der Hunde-, Katzen- und Fuchsflöhe? Ein weiterer Spezialfall ist der Friedman-Test, wenn du noch einen zusätzlichen Faktor mit ins statistische Modell nehmen willst.",
    "crumbs": [
      "Statistische Gruppenvergleiche"
    ]
  },
  {
    "objectID": "stat-tests-preface-app.html#fallzahl-gleich-oder-ungleich",
    "href": "stat-tests-preface-app.html#fallzahl-gleich-oder-ungleich",
    "title": "Statistische Gruppenvergleiche",
    "section": "Fallzahl gleich oder ungleich?",
    "text": "Fallzahl gleich oder ungleich?\nManche statistische Verfahren haben Probleme, wenn nicht in allen Gruppen gleich viele Beobachtungen vorliegen. Hier wäre jetzt vor allem der Friedman-Test zu nennen. Die anderen Verfahren haben eigentlich keine Probleme mit unterschiedlichen Gruppengrößen. Der Wilcoxon-Mann-Whitney-Test erwartet als Daumenregel immer mehr als drei Beobachtungen pro Gruppe. Wenn du weniger Beobachtungen pro Gruppe hast, dann wird die Sache schwierig eine Signifikanz nachzuweisen. Auch hat der \\(\\mathcal{X}^2\\)-Test seine Limitierungen. Aber das würde dann hier zu weit führen.",
    "crumbs": [
      "Statistische Gruppenvergleiche"
    ]
  },
  {
    "objectID": "stat-tests-preface-app.html#varianzen-gleich-oder-ungleich",
    "href": "stat-tests-preface-app.html#varianzen-gleich-oder-ungleich",
    "title": "Statistische Gruppenvergleiche",
    "section": "Varianzen gleich oder ungleich?",
    "text": "Varianzen gleich oder ungleich?\nAuch die Frage der Varianzhomogenität oder Varianzheterogenität musst du nochmal genauer anschauen. Wenn du ein normalverteiltes Outcome \\(y\\) hast, dann kannst du auch mit Varianzheterogenität umgehen. Dann schließt sich zwar die klassische ANOVA aus, aber das ist nicht unbedingt ein Problem. Die Statistik hat nur noch keine echte Lösung für ein Outcome, das nicht normalverteilt und keine homogenen Varianzen hat, gefunden. Hier müssen wir dann mal über die Eigenschaften des Outcomes \\(y\\) sprechen. Denn nicht normalverteilt bedeutet häufig, dass wir doch die Verteilung erkennen und entsprechend modellieren können. Es gibt neben der Normalverteilung noch eine Reihe von bekannten verteilunge, die hier als Lösung funktionieren können.",
    "crumbs": [
      "Statistische Gruppenvergleiche"
    ]
  },
  {
    "objectID": "stat-tests-preface-app.html#abhängig-oder-unabhängig",
    "href": "stat-tests-preface-app.html#abhängig-oder-unabhängig",
    "title": "Statistische Gruppenvergleiche",
    "section": "Abhängig oder unabhängig?",
    "text": "Abhängig oder unabhängig?\nAm Ende noch die Frage, ob deine Beobachtungen unabhängig oder abhängig sind. Hier stellt sich eigentlich die Frage, ob du eine Beobachtung über die Zeit mehrfach misst. Wenn das der Fall ist, dann musst du komplexere Modelle nutzen als die einfachen Gruppenvergleiche. Jedes der hier vorgestellten Verfahren geht eigentlich von der Unabhängigkeit der Beobachtungen aus. Die Beobachtungen untereinander sind nicht miteinander verwandt oder kausal verflochten. Wir haben also kein Verwandtschaftverhätnisse vorliegen oder messen wiederholt das gleiche Individuum. Ein Ausweg für zwei Zeitpunkte ist der gepaarte t-Test. bei mehreren Zeitpunkten sind wir dann schon bei komplexeren Modellen wie den gemischten linearen Modellen. Aber das führt hier viel zu weit.",
    "crumbs": [
      "Statistische Gruppenvergleiche"
    ]
  },
  {
    "objectID": "stat-tests-ttest.html",
    "href": "stat-tests-ttest.html",
    "title": "27  Der t-Test",
    "section": "",
    "text": "27.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, readxl)\n##\nset.seed(20221206)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Der t-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-ttest.html#daten-und-modell-für-den-t-test",
    "href": "stat-tests-ttest.html#daten-und-modell-für-den-t-test",
    "title": "27  Der t-Test",
    "section": "27.2 Daten und Modell für den t-Test",
    "text": "27.2 Daten und Modell für den t-Test\nWichtig ist, dass wir schon jetzt die Modellschreibweise lernen um die Daten später richtig in R nutzen zu können. Wir werden die Modellschreibweise immer wieder sehen. Die Modellschreibweise ist die Art und Weise wie wir in R eine Abhängigkeit beschreiben. Wir brauchen dieses Konzept in den folgenden Kapiteln. In R heißt y ~ x auch formula, also eine Formelschreibweise. Damit ahben wir die Modellschreibweise \\(y\\) hängt ab von \\(x\\). Das \\(y\\) repräsentiert eine Spalte im Datensatz und das \\(x\\) repräsentiert ebenso eine Spalte im Datensatz. Wir brauchen also zwei Variablen \\(y\\) und \\(x\\), die natürlich nicht so heißen müssen.\n\\[\ny \\sim x\n\\]\nEtwas unbefriedigend, dass der t-Test nur zwei Gruppen miteinander Vergleichen kann. Mehr Gruppen gehen in der ANOVA im Kapitel 28\nWas brauchen wir damit wir den t-Test in R rechnen können? Später in der Anwendung nutzt du ja nur die Implementierung des t-Tests in R. Wir rechnen ja unsere Auswertung nicht per Hand. In R brauchen wir für den t-Test eine Spalte \\(y\\) mit kontinuierlichen Zahlen und einer Spalte \\(x\\) in dem wir einen Faktor mit zwei Leveln finden. Jedes Level steht dann für eine der beiden Gruppen. Das war es schon. Schauen wir uns nochmal den Datensatz flea_dog_cat.xlsx in Tabelle 27.1 an und überlegen, wie wir das realisieren können.\n\n\n\n\nTabelle 27.1— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Boniturnote sowie der Infektionsstatus von Hunden und Katzen.\n\n\n\n\n\n\nanimal\njump_length\nflea_count\nweight\ngrade\ninfected\n\n\n\n\ndog\n5.7\n18\n2.1\n8\n0\n\n\ndog\n8.9\n22\n2.3\n8\n1\n\n\ndog\n11.8\n17\n2.8\n6\n1\n\n\ndog\n5.6\n12\n2.4\n8\n0\n\n\ndog\n9.1\n23\n1.2\n7\n1\n\n\ndog\n8.2\n18\n4.1\n7\n0\n\n\ndog\n7.6\n21\n3.2\n9\n0\n\n\ncat\n3.2\n12\n1.1\n7\n1\n\n\ncat\n2.2\n13\n2.1\n5\n0\n\n\ncat\n5.4\n11\n2.4\n7\n0\n\n\ncat\n4.1\n12\n2.1\n6\n0\n\n\ncat\n4.3\n16\n1.5\n6\n1\n\n\ncat\n7.9\n9\n3.7\n6\n0\n\n\ncat\n6.1\n7\n2.9\n5\n0\n\n\n\n\n\n\n\n\nIn Abbildung 27.1 sehen wir einmal den Zusammenhang zwischen den Schreibweise \\(y \\sim x\\) und den beiden Variablen jump_length als \\(y\\) und animal als \\(x\\) aus dem Datensatz flea_dog_cat.xlsx. Wir haben also die formula Schreibweise in R als jump_length ~ animal.\n\n\n\n\n\n\nAbbildung 27.1— Modellschreibweise bzw. formula-Schreibweise in R. Die Variable \\(y\\) hängt ab von \\(x\\) am Beispiel des Datensatzes flea_dog_cat.xlsx mit den beiden Variablen jump_length als \\(y\\) und animal als \\(x\\).\n\n\n\nWir benötigen für den t-Test ein normalverteiltes \\(y\\) und einen Faktor mit zwei Leveln als \\(x\\). Wir nehmen daher mit select()die Spalte jump_length und animal aus dem Datensatz flea_dog_cat.xlsx. Wichtig ist, dass wir die Spalte animal mit der Funktion as_factor() in einen Faktor umwandeln. Anschließend speichern wir die Auswahl in dem Objekt data_tbl.\n\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\") |&gt; \n  mutate(animal = as_factor(animal)) |&gt; \n  select(animal, jump_length)\n\ndata_tbl\n\n# A tibble: 14 × 2\n   animal jump_length\n   &lt;fct&gt;        &lt;dbl&gt;\n 1 dog            5.7\n 2 dog            8.9\n 3 dog           11.8\n 4 dog            5.6\n 5 dog            9.1\n 6 dog            8.2\n 7 dog            7.6\n 8 cat            3.2\n 9 cat            2.2\n10 cat            5.4\n11 cat            4.1\n12 cat            4.3\n13 cat            7.9\n14 cat            6.1\n\n\nWir haben jetzt die Daten richtig vorbereiten und können uns nun mit dem t-Test beschäftigen. Bevor wir den t-Test jedoch rechnen können, müssen wir uns nochmal überlegen, was der t-Test eigentlich testet und uns die Daten einmal visualisieren.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Der t-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-ttest.html#visualiserung-der-daten",
    "href": "stat-tests-ttest.html#visualiserung-der-daten",
    "title": "27  Der t-Test",
    "section": "27.3 Visualiserung der Daten",
    "text": "27.3 Visualiserung der Daten\nBevor wir einen statistischen Test rechnen, wollen wir uns erstmal die Daten, die dem Test zugrunde liegen, visualisieren. Wir schauen uns in Abbildung 27.2 einmal den Boxplot für die Sprungweiten getrennt nach Hund und Katze an.\nWir sehen, dass sich die Boxen nicht überschneiden, ein Indiz für einen signifikanten Unterschied zwischen den beiden Gruppen. Im Weiteren liegt der Median in etwa in der Mitte der beiden Boxen. Die Whisker sind ungefähr gleich bei Hunden und Katzen. Ebenso sehen wir bei beiden Gruppen keine Ausreißer.\nWir schließen daher nach der Betrachtung der Boxplots auf Folgendes:\n\nDie Sprungweite ist für beide Gruppen ist annähernd bzw. approximativ normalverteilt.\nDie Standardabweichungen und damit die Varianzen \\(s^2_{dog} = s^2_{cat}\\) der beiden Gruppen sind gleich. Es liegt somit Varianzhomogenität vor.\n\n\n\n\n\n\n\n\n\nAbbildung 27.2— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\n\nManchmal ist es etwas verwirrend, dass wir uns in einem Boxplot mit Median und IQR die Daten für einen t-Test anschauen. Immerhin rechnet ja ein t-Test mit den Mittelwerten und der Standardabweichung. Hier vergleichen wir etwas Äpfel mit Birnen. Deshalb in der Abbildung 27.3 der Dotplot mit dem Mittelwert und den entsprechender Standardabweichung als Fehlerbalken.\n\n\n\n\n\n\n\n\nAbbildung 27.3— Dotplot der Sprungweiten [cm] von Hunden und Katzen zusammen mit dem Mittelwert und der Stanardabweichung als Fehlerbalken.\n\n\n\n\n\nWir nutzen aber später häufig den Boxplot zur Visualisierung der einzelnen Gruppen. Über den Boxplot können wir auch gut abschätzen, ob wir eine annährende bzw. approximative Normalverteilung vorliegen haben.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Der t-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-ttest.html#hypothesen-für-den-t-test",
    "href": "stat-tests-ttest.html#hypothesen-für-den-t-test",
    "title": "27  Der t-Test",
    "section": "27.4 Hypothesen für den t-Test",
    "text": "27.4 Hypothesen für den t-Test\nOhne eine Hypothese ist das Ergebnis eines statistischen Tests wie auch der t-Test nicht zu interpretieren. Wir berechenen eine Teststatistik und einen p-Wert. Beide statistischen Maßzahlen machen eine Aussage über die beobachteten Daten \\(D\\) unter der Annahme, das die Nullhypothese \\(H_0\\) gilt.\nWie lautet nun das Hypothesenpaar des t-Tests? Der t-Test vergleicht die Mittelwerte von zwei Gruppen. Die Nullhypothese ist auch die Gleichheitshypothese. Die Alternativehypothese haben wir auch als Unterschiedshypothese bezeichnet.\nDaher ergibt sich für unser Beispiel mit den Sprungweiten für Hunde- und Katzenflöhen folgende Hypothesen. Die Nullhypothese sagt, dass die mittleren Sprungweite für die Hundeflöhe gleich der mittleren Sprungweite der Katzenflöhe ist. Die Alternativehypothese sagt aus, dass sich die mittlere Sprungweite von Hunde- und Katzenflöhen unterscheidet.\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{dog} &= \\bar{y}_{cat} \\\\  \nH_A: \\bar{y}_{dog} &\\neq \\bar{y}_{cat} \\\\   \n\\end{aligned}\n\\]\nWir testen grundsätzlich auf ein zweiseitiges \\(\\alpha\\)-Niveau von 5%.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Der t-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-ttest.html#der-student-t-test",
    "href": "stat-tests-ttest.html#der-student-t-test",
    "title": "27  Der t-Test",
    "section": "27.5 Der Student t-Test",
    "text": "27.5 Der Student t-Test\nLiegt ein normalverteiltes \\(y\\) vor und sind die Varianzen für die beiden zu vergleichenden Gruppen homogen \\(s^2_{cat} = s^2_{dog}\\), können wir einen Student t-Test rechnen. Wir nutzen dazu die folgendeFormel des Student t-Tests.\n\\[\nT_{calc} = \\cfrac{\\bar{y}_{dog}-\\bar{y}_{cat}}{s_{p} \\cdot \\sqrt{\\cfrac{2}{n_{group}}}}\n\\]\nmit der vereinfachten Formel für die gepoolte Standardabweichung \\(s_p\\).\n\\[\ns_{p} = \\cfrac{s_{dog} + s_{cat}}{2}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEigentlich wäre hier folgende Formel mit \\(s_{p} = \\sqrt{\\frac{1}{2} (s^2_{dog} + s^2_{cat})}\\) richtig, aber auch hier erwischen wir einen Statistikengel um es etwas einfacher zu machen.\n\n\n\n\nWir wollen nun die Werte für \\(\\bar{y}_{dog}\\), \\(\\bar{y}_{cat}\\) und \\(s_{p}\\) berechnen. Wir nutzen hierfür R auf die etwas komplizierte Art und Weise. Es gibt in R auch die Funktion t.test(), die für uns alles auf einmal macht, aber hier nochaml zu Fuß.\n\nsum_tbl &lt;- data_tbl |&gt; \n  group_by(animal) |&gt; \n  summarise(mean = round(mean(jump_length), 2), \n            sd = round(sd(jump_length), 2)) \n\nsum_tbl\n\n# A tibble: 2 × 3\n  animal  mean    sd\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 dog     8.13  2.14\n2 cat     4.74  1.9 \n\n\nWir erhalten durch die Funktion group_by() den Mittelwert und die Standardabweichung für die Sprungweite getrennt für die Hunde- und Katzenflöhe. Wir können damit die beiden obigen Formeln füllen.\nWir berechnen \\(s_p\\) wie folgt.\n\\[\ns_{pooled} = \\cfrac{2.14 + 1.9}{2} = 2.02\n\\]\nAnschließend können wir jetzt \\(s_p\\) und die Mittelwerte sowie die Gruppengröße \\(n_g = 7\\) in die Formel für den Student t-Test einsetzen und die Teststatistik \\(T_{calc}\\) berechnen.\n\\[\nT_{calc} = \\cfrac{8.13- 4.74}{2.02 \\cdot \\sqrt{\\cfrac{2}{7}}} = 3.14\n\\]\nWir erhalten eine Teststatistik \\(T_{calc} = 3.14\\) die wir mit dem kritischen Wert \\(T_{\\alpha = 5\\%} = 2.17\\) vergleichen können. Da \\(T_{calc} &gt; T_{\\alpha = 5\\%}\\) ist, können wir die Nullhypothese ablehnen. Wir haben ein signifikanten Unterschied zwischen den mittleren Sprungweiten von Hunde- und Katzenflöhen nachgewiesen.\nSoweit für den Weg zu Fuß. Wir rechnen in der Anwendung keinen Student t-Test per Hand. Wir nutzen die Formel t.test(). Da wir den Student t-Test unter der Annahme der Varianzhomogenität nutzen wollen, müssen wir noch die Option var.equal = TRUE wählen.\nDie Funktion t.test() benötigt erst die das \\(y\\) und \\(x\\) in Modellschreibweise mit den Namen, wie die beiden Variablen auch im Datensatz data_tbl stehen. In unserem Fall ist die Modellschreibweise dann jump_length ~ animal. Im Weiteren müssen wir noch den Datensatz angeben den wir verwenden wollen durch die Option data = data_tbl. Dann können wir die Funktion t.test() ausführen.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  jump_length by animal\nt = 3.1253, df = 12, p-value = 0.008768\nalternative hypothesis: true difference in means between group dog and group cat is not equal to 0\n95 percent confidence interval:\n 1.025339 5.746089\nsample estimates:\nmean in group dog mean in group cat \n         8.128571          4.742857 \n\n\nWir erhalten eine sehr lange Ausgabe, die aucb etwas verwirrend aussieht. Gehen wir die Ausgabe einmal durch. Ich gehe nicht auf alle Punkte ein, sondern konzentriere mich hier auf die wichtigsten Aspekte.\n\nt = 3.12528 ist die berechnete Teststatistik \\(T_{calc}\\). Der Wert unterscheidet sich leicht von unserem berechneten Wert. Der Unterschied war zu erwarten, wir haben ja auch die t-Test Formel vereinfacht.\np-value = 0.0087684 ist der berechnete p-Wert \\(Pr(T_{calc}|H_0)\\) aus der obigen Teststatistik. Daher die Fläche rechts von der Teststatistik.\n95 percent confidence interval: 1.0253394 5.7460892 ist das 95% Konfidenzintervall. Die erste Zahl ist die untere Grenze, die zweite Zahl ist die obere Grenze.\n\nWir erhalten hier dreimal die Möglichkeit eine Aussage über die \\(H_0\\) zu treffen. In dem obigen Output von R fehlt der kritische Wert \\(T_{\\alpha = 5\\%}\\). Daher ist die berechnete Teststatistik für die Testentscheidung nicht verwendbar. Wir nutzen daher den p-Wert und vergleichen den p-Wert mit dem \\(\\alpha\\)-Niveau von 5%. Da der p-Wert kleiner ist als das \\(\\alpha\\)-Niveau können wir wie Nullhypothese ablehnen. Wir haben einen signifikanten Unterschied. Die Entscheidung mit dem Konfidenzintervall benötigt die Signifikanzschwelle. Da wir hier einen Mittelwertsvergleich vorliegen haben ist die Signifikanzschwelle gleich 0. Wenn die 0 im Konfidenzintervall liegt können wir die Nullhypothese nicht ablehnen. In unserem Fall ist das nicht der Fall. Das Konfidenzintervall läfut von 1.025 bis 5.75. Damit ist die 0 nicht im Konfidenzintervall enthalten und wir können die Nullhypothese ablehnen. Wir haben ein signifikantes Konfidenzintervall vorliegen.\nWie wir sehen fehlt der Mittelwertsuntschied als Effekt \\(\\Delta\\) in der Standardausgabe des t-Tests in R. Wir können den Mittelwertsunterschied selber berechnen oder aber die Funktion tidy() aus dem R Paket {broom} nutzen. Da der Funktion tidy() kriegen wir die Informationen besser sortiert und einheitlich wiedergegeben. Da tidy eine Funktion ist, die mit vielen statistischen Tests funktioniert müssen wir wissen was die einzelnen estimate sind. Es hilft in diesme Fall sich die Visualisierung der Daten anzuschauen und die Abbildung mit den berechneten Werten abzugleichen.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = TRUE) |&gt; \n  tidy() \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     3.39      8.13      4.74      3.13 0.00877        12     1.03      5.75\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nWir erkennen als erstes den Mittelwertsunterschied zwischen den beiden Gruppen von 3.39 cm. Danach folgen die einzelnen Mittelwerte der Sprungweiten der Hunde und Katzenflöhe mit jeweils 8.13 cm und 4.74 cm. Darauf folgt noch der p-Wert als p.value mit 0.00891 und die beiden Grenzen des Konfidenzintervalls [1.03; 5.75].",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Der t-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-ttest.html#der-welch-t-test",
    "href": "stat-tests-ttest.html#der-welch-t-test",
    "title": "27  Der t-Test",
    "section": "27.6 Der Welch t-Test",
    "text": "27.6 Der Welch t-Test\nDer t-Test ist auch in der Lage mit Varianzhetrogenität umzugehen. Das heißt, wenn die Varianzen der beiden Gruppen nicht gleich sind. Dadurch ändert sich die Formel für den t-Test wie folgt. Dann nennen wir den statistischen Test Welch t-Test.\n\\[\nT_{calc} = \\cfrac{\\bar{y_1} - \\bar{y_2}}{\\sqrt{\\cfrac{s^2_{y_1}}{n_1} + \\cfrac{s^2_{y_2}}{n_2}}}\n\\]\nWir sehen, dass sich die Formel etwas andert. Da wir nicht mehr annehmen, dass die Varianzen homogen und daher gleich sind, können wir auch keinen gepoolten Varianzschätzer \\(s_p\\) berechnen. Die Varianzen gehen einzeln in die Formel des Welch t-Tests ein. Ebenso müssen die beiden Gruppen nicht mehr gleich groß sein. Statt einen Wert \\(n_g\\) für die Gruppengröße können wir auch die beiden Gruppengrößen separat angeben.\n\n\nHier muss man noch bedenken, dass die Freiheitsgrade anders berechnet werden Die Freiheitsgrade werden wie folgt berechnet.\n\\[\ndf = \\cfrac{\\left(\\cfrac{s^2_{y_1}}{n} +\n    \\cfrac{s^2_{y_2}}{m}\\right)^2}{\\cfrac{\\left(\\cfrac{s^2_{y_1}}{n}\\right)^2}{n-1} + \\cfrac{\\left(\\cfrac{s^2_{y_2}}{m}\\right)^2}{m-1}}\n\\]\nEs ergibt keinen tieferen Sinn die obige Formel nochmal händisch auszurechnen. Die Zahlen ändern sich leicht, aber konzeptionell erhalten wir hier keinen Mehrwert. Deshalb schauen wir uns gleich die Umsetzung in R an. Wir nutzen erneut die Funktion t.test() und zwar diesmal mit der Option var.equal = FALSE. Damit geben wir an, dass die Varianzen heterogen zwischen den beiden Gruppen sind. Wir nutzen in unserem Beispiel die gleichen Zahlen und Daten wie schon im obigen Student t-Test Beispiel.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  jump_length by animal\nt = 3.1253, df = 11.831, p-value = 0.008906\nalternative hypothesis: true difference in means between group dog and group cat is not equal to 0\n95 percent confidence interval:\n 1.021587 5.749842\nsample estimates:\nmean in group dog mean in group cat \n         8.128571          4.742857 \n\n\nWir sehen das viele Zahlen nahezu gleich sind. Das liegt auch daran, dass wir in unserem Daten keine große Abweichung von der Varianzhomogenität haben. Wir erhalten die gleichen Aussagen wie auch schon im Student t-Test.\nSchauen wir uns nochmal die Ausgabe der Funktion tidy() an.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = FALSE) |&gt; \n  tidy() \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     3.39      8.13      4.74      3.13 0.00891      11.8     1.02      5.75\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nWir sehen hier etwas besser, dass es kaum Abweichungen gibt. Alles egal? Nicht unbedingt. Das Problem ist eher das Erkennen von Varianzheterogenität in sehr kleinen Datensätzen. Kleine Datensätze meint Datensätze unter 30 Beobachtungen je Gruppe. Erst aber dieser Anzahl lassen sich unverzerrte Histogramme zeichnen und so aussagekräftige Abschätzungen der Varianzhomogenität oder Varianzheterogenität treffen.\nFür das Erkennen von Normalverteilung und Varianzheterogenität werden häufig so genannte Vortest empfohlen. Aber auch hier gilt, bei kleiner Fallzahl liefern die Vortests keine verlässlichen Ergebnisse. In diesem Fall ist weiterhin die Beurteilung über einen Boxplot sinnvoller. Du findest hier mehr Informationen im Kapitel zum Pre-Test oder Vortest.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Der t-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-ttest.html#der-verbundene-t-test-paired-t-test",
    "href": "stat-tests-ttest.html#der-verbundene-t-test-paired-t-test",
    "title": "27  Der t-Test",
    "section": "27.7 Der verbundene t-Test (Paired t-Test)",
    "text": "27.7 Der verbundene t-Test (Paired t-Test)\nIm folgenden Datenbeispiel in Tabelle 27.2 haben wir eine verbundene Stichprobe. Das heißt wir haben nicht zehn Flöhe gemessen sondern fünf Flöhe. Einmal im ungefütterten Zustand unfed und einmal im gefütterten Zustand fed. Wir wollen nun wissen, ob der Fütterungszustand Auswirkungen auf die Sprungweite in [cm] hat.\n\n\n\n\nTabelle 27.2— Tabelle der Sprunglängen [cm] von fünf Flöhen zu zwei Zeitpunkten. Einmal wurde die Sprungweite ungefüttert und einmal gefüttert bestimmt. Die Daten liegen im Wide Format vor.\n\n\n\n\n\n\nunfed\nfed\ndiff\n\n\n\n\n5.2\n6.1\n0.9\n\n\n4.1\n5.2\n1.1\n\n\n3.5\n3.9\n0.4\n\n\n3.2\n4.1\n0.9\n\n\n4.6\n5.3\n0.7\n\n\n\n\n\n\n\n\nWir nutzen folgende Formel für den paired t-Test für verbundene Stichproben.\n\\[\nT_{calc} = \\sqrt{n}\\cfrac{\\bar{d}}{s_d}\n\\]\nWir können \\(\\bar{d}\\) als Mittelwert der Differenzen der Variablen diff berechnen. Ebenso verfahren wir mit der Standardabweichung der Differenzen \\(s_d\\).\n\\[\nT_{calc} = \\sqrt{10}\\cfrac{0.8}{0.26} = 6.88\n\\]\nUm den die Funktion t.test()in R mit der Option paired = TRUE für den paired t-Test zu nutzen, müssen wir die Daten nochmal über die Funktion gather() in das Long Format umwandeln. Wir wollen nun wissen, ob der Fütterungszustand food_status Auswirkungen auf die Sprungweite in [cm] hat.\n\nt.test(jump_length ~ food_status, \n       data = paired_tbl, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  jump_length by food_status\nt = 6.7612, df = 4, p-value = 0.002496\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.4714866 1.1285134\nsample estimates:\nmean difference \n            0.8 \n\n\nDie Ausgabe des paired t-Test ähnelt stark der Aussage des Student t-Test. Wir erhalten ebenfalls den wichtigen p-Wert mit 0.0025 sowie das 95% Konfidenzintervall mit [0.47; 1.13]. Zum einen ist \\(0.0025 &lt; \\alpha\\) und somit können wir die Nullhypothese ablehnen, zum anderen ist auch die 0 nicht mit in dem Konfidenzintervall, womit wir auch hier die Nullhypothese ablehnen können.\n\nt.test(jump_length ~ food_status, \n       data = paired_tbl, paired = TRUE) |&gt; \n  tidy() \n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      0.8      6.76 0.00250         4    0.471      1.13 Paired t-… two.sided  \n\n\nDie Funktion tidy() gibt uns in diesem Fall keine neuen zusätzlichen Informationen. Wir können die Funktion aber nutzen um uns dann später für viele t-Test eine einheitliche Rückgabe zu erzeugen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Der t-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-ttest.html#freiheitsgrade-im-t-test",
    "href": "stat-tests-ttest.html#freiheitsgrade-im-t-test",
    "title": "27  Der t-Test",
    "section": "27.8 Freiheitsgrade im t-Test",
    "text": "27.8 Freiheitsgrade im t-Test\nDer t-Verteilung der Teststatistiken des t-Tests verhält sich nicht wie eine klassische Normalverteilung, die durch den Mittelwert und die Standardabweichung definiert ist. Die t-Verteilung ist nur durch die Freiheitsgrade definiert. Der Freiheitsgrad (eng. degree of freedom, abk. df) in einem t-Test mit zwei Gruppen mit den Fallzahlen \\(n_1\\) und \\(n_2\\) ist gegeben durch \\(df = n_1 + n_2 -2\\). Damit beschreiben die Freiheitsgrade grob die gesamte Fallzahl in einen Datensatz mit nur zwei Gruppen. Je mehr Fallzahl in den beiden Gruppen desto großer der Freiheitsgrad eines t-Tests.\nDie Abbildung 27.4 visualisiert diesen Zusammenhang von Freiheitsgraden und der Form der t-Verteilung. Je kleiner die Freiheitsgrade und damit die Fallzahl in unseren beiden Gruppen, desto weiter sind die Verteilungsschwänze. Wie du sehen kannst, reicht die rote Verteilung mit einem Freiheitsgrad von \\(df = 10\\) viel weiter nach links und rechts als die anderen Verteilungen mit niedrigeren Freiheitsgraden. Daher benötigen wir auch größere \\(T_{calc}\\) Werte um ein signifikantes Ergebnis zu erhalten. Denn die Fläche unter der t-Verteilung ist immer gleich 1 und somit wandert dann der kritische Wert \\(T_{\\alpha = 5\\%}\\) immer weiter nach außen. Das macht ja auch Sinn, wenn wir wenige Beobachtungen vorliegen haben, dann brauchen wir größere Werte der Teststatistik um an einen signifikanten Effekt glauben zu können.\n\n\n\n\n\n\nAbbildung 27.4— Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und damit die Fallzahl, desto näher kommt die t-Verteilung einer Normalverteilung nahe. Bei einer geringeren Fallzahl, müssen damit größere \\(T_{calc}\\) Werte erreicht werden um eine signifikantes Ergebnis zu erhalten, da mehr Fläche nach außen wandert.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Der t-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-ttest.html#referenzen",
    "href": "stat-tests-ttest.html#referenzen",
    "title": "27  Der t-Test",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 27.1— Modellschreibweise bzw. formula-Schreibweise in R. Die Variable \\(y\\) hängt ab von \\(x\\) am Beispiel des Datensatzes flea_dog_cat.xlsx mit den beiden Variablen jump_length als \\(y\\) und animal als \\(x\\).\nAbbildung 27.2— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\nAbbildung 27.3— Dotplot der Sprungweiten [cm] von Hunden und Katzen zusammen mit dem Mittelwert und der Stanardabweichung als Fehlerbalken.\nAbbildung 27.4— Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und damit die Fallzahl, desto näher kommt die t-Verteilung einer Normalverteilung nahe. Bei einer geringeren Fallzahl, müssen damit größere \\(T_{calc}\\) Werte erreicht werden um eine signifikantes Ergebnis zu erhalten, da mehr Fläche nach außen wandert.\n\n\n\nRasch D, Teuscher F, Guiard V. 2007. How robust are tests for two independent samples? Journal of statistical planning and inference 137: 2706–2720.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Der t-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-anova.html",
    "href": "stat-tests-anova.html",
    "title": "28  Die ANOVA",
    "section": "",
    "text": "28.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, effectsize, ggpubr, \n               see, car, conflicted)\nconflicts_prefer(plyr::mutate)\nconflicts_prefer(dplyr::summarize)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Die ANOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac1",
    "href": "stat-tests-anova.html#sec-fac1",
    "title": "28  Die ANOVA",
    "section": "28.2 Einfaktorielle ANOVA",
    "text": "28.2 Einfaktorielle ANOVA\n\n\nDie einfaktorielle ANOVA ist die simpelste Form der ANOVA. Wir nutzen einen Faktor \\(x\\) mit mehr als zwei Leveln. Im Rahmen der einfaktoriellen ANOVA wollen wir uns auch die ANOVA theoretisch einmal anschauen. Danach wie die einfaktorielle ANOVA in R genutzt wird. Ebenso werden wir die einfaktorielle ANOVA visualisieren. Abschließend müssen wir uns noch überlegen, ob es einen Effektschätzer für die einfaktorielle ANOVA gibt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie einfaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität über den Behandlungsfaktor \\(x\\). Daher alle Level von \\(x\\) sollen die gleiche Varianz haben. Unsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x\\) homogen in den Varianzen sind. Später mehr dazu, wenn wir beides nicht vorliegen haben…\n\n\n\n\n\n28.2.1 Daten für die einfaktorielle ANOVA\nWir wollen uns nun erstmal den einfachsten Fall anschauen mit einem simplen Datensatz. Wir nehmen ein normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Hätten wir nur zwei Level, dann hätten wir auch einen t-Test rechnen können.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac1_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox.csv\") |&gt;\n  select(animal, jump_length) |&gt; \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt fac1_tbl mit dem Datensatz in Tabelle 28.1 nochmal dargestellt.\n\n\n\n\nTabelle 28.1— Selektierter Datensatz für die einfaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln.\n\n\n\n\n\n\nanimal\njump_length\n\n\n\n\ndog\n5.7\n\n\ndog\n8.9\n\n\ndog\n11.8\n\n\ndog\n5.6\n\n\ndog\n9.1\n\n\ndog\n8.2\n\n\ndog\n7.6\n\n\ncat\n3.2\n\n\ncat\n2.2\n\n\ncat\n5.4\n\n\ncat\n4.1\n\n\ncat\n4.3\n\n\ncat\n7.9\n\n\ncat\n6.1\n\n\nfox\n7.7\n\n\nfox\n8.1\n\n\nfox\n9.1\n\n\nfox\n9.7\n\n\nfox\n10.6\n\n\nfox\n8.6\n\n\nfox\n10.3\n\n\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt fac1_tbl folgendes Modell für die spätere Analyse in R.\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der einfaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n\n28.2.2 Hypothesen für die einfaktorielle ANOVA\nDie ANOVA betrachtet die Mittelwerte und nutzt die Varianzen um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mittelwerte jedes Levels des Faktors animal gleich sind.\n\\[\nH_0: \\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Mittelwerten unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mittelwerte unterschieden oder eben nur ein Paar. Wenn eine ANOVA die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mittelwerte sich unterscheiden.\n\\[\n\\begin{aligned}\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nWir schauen uns jetzt einmal die ANOVA theoretisch an bevor wir uns mit der Anwendung der ANOVA in R beschäftigen.\n\n\n28.2.3 Einfaktoriellen ANOVA theoretisch\nKommen wir zurück zu den Daten in Tabelle 28.1. Wenn wir die ANOVA per Hand rechnen wollen, dann ist nicht das Long Format die beste Wahl sondern das Wide Format. Wir haben ein balanciertes Design vorliegen, dass heißt in jeder Level sind die gleiche Anzahl Beobachtungen. Wir schauen uns jeweils sieben Flöhe von jeder Tierart an. Für eine ANOVA ist aber ein balanciertes Design nicht notwendig, wir können auch mit ungleichen Gruppengrößen eine ANOVA rechnen. Statt einer einfaktoriellen ANOVA könnten wir auch gleich einen pairwise.t.test()rechnen. Historisch betrachtet ist die einfaktorielle ANOVA die Visualisierung des paarweisen t-Tests.\nEine einfaktorielle ANOVA macht eigentlich keinen großen Sinn, wenn wir anschließend sowieso paarweise Vergleich, wie in Kapitel 33 beschrieben, rechnen. Aus der Historie stellte sich die Frage, ob es sich lohnt die ganze Arbeit für die paarweisen t-Tests per Hand zu rechnen. Daher wurde die ANOVA davor geschaltet. War die ANOVA nicht signifikant, dann konnte man sich dann auch die Rechnerei für die paarweisen t-Tests sparen.\nIn Tabelle 28.2 sehen wir die Daten einmal als Wide-Format dargestellt.\n\n\n\nTabelle 28.2— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal.\n\n\n\n\n\nj\ndog\ncat\nfox\n\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\n\n\n\nWir können jetzt für jedes der Level den Mittelwert über all \\(j=7\\) Beobachtungen berechnen.\n\\[\n\\begin{aligned}\n\\bar{y}_{dog} &= 8.13 \\\\\n\\bar{y}_{cat} &= 4.74 \\\\\n\\bar{y}_{fox} &= 9.16 \\\\\n\\end{aligned}\n\\]\nWir tun jetzt für einen Moment so, als gebe es den Faktor animal nicht in den Daten und schauen uns die Verteilung der einzelnen Beobachtungen in Abbildung 28.1 (a) einmal an. Wir sehen das sich die Beobachtungen von ca. 2.2cm bis 11 cm streuen. Woher kommt nun diese Streuung bzw. Varianz? Was ist die Quelle der Varianz? In Abbildung 28.1 (b) haben wir die Punkte einmal nach dem Faktor animal eingefärbt. Wir sehen, dass die blauen Beobachtungen eher weitere Sprunglängen haben als die grünen Beobachtungen. Wir gruppieren die Beobachtungen in Abbildung 28.1 (c) nach dem Faktor animal und sehen, dass ein Teil der Varianz der Daten von dem Faktor animal ausgelöst wird.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Die Sprungweite in [cm] ohne den Faktor animal betrachtet.\n\n\n\n\n\n\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal eingefärbt.\n\n\n\n\n\n\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal eingefärbt und gruppiert.\n\n\n\n\n\n\n\nAbbildung 28.1— Die Spungweite in [cm] in Abhängigkeit von dem Faktor animal dargestellt.\n\n\n\n\nGehen wir einen Schritt weiter und zeichnen einmal das globale Mittel in die Abbildung 28.2 (a) von \\(\\bar{y}_{..} = 7.34\\) und lassen die Beobachtungen gruppiert nach dem Faktor animal. Wir sehen, dass die Level des Faktors animal um das globale Mittel streuen. Was ja auch bei einem Mittelwert zu erwarten ist. Wir können jetzt in Abbildung 28.2 (b) die lokalen Mittel für die einzelnen Level dog, catund fox ergänzen. Und abschließend in Abbildung 28.2 (c) die Abweichungen \\(\\\\beta_i\\) zwischen dem globalen Mittel \\(\\bar{y}_{..} = 7.34\\) und den einzelnen lokalen Mittel berechnen. Die Summe der Abweichungen \\(\\\\beta_i\\) ist \\(0.79 + (-2.6) + 1.81 \\approx 0\\). Das ist auch zu erwarten, den das globale Mittel muss ja per Definition als Mittelwert gleich großen Abstand “nach oben” wie “nach unten” haben.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Die Sprungweite in [cm] mit den Faktor animal gruppiert und das globale Mittel \\(\\bar{y}_{..} = 7.34\\) ergänzt.\n\n\n\n\n\n\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die lokalen Mittel \\(\\bar{y}_{i.}\\) für jedes Level ergänzt.\n\n\n\n\n\n\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die Abweichungen \\(\\beta_i\\) ergänzt.\n\n\n\n\n\n\n\nAbbildung 28.2— Dotplot der Spungweite in [cm] in Abhängigkeit von dem Faktor animal.\n\n\n\n\nWir tragen die Werte der lokalen Mittelwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\) noch in die Tabelle 28.3 ein. Wir sehen in diesem Beispiel warum das Wide Format besser zum Verstehen der ANOVA ist, weil wir ja die lokalen Mittelwerte und die Abweichungen per Hand berechnen. Da wir in der Anwendung aber nie die ANOVA per Hand rechnen, liegen unsere Daten immer in R als Long-Format vor. Es handelt sich hier nur um die Veranschaulichung des Konzepts der ANOVA.\n\n\n\nTabelle 28.3— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal. Wir ergänzen die lokalen Mittlwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\).\n\n\n\n\n\nj\ndog\ncat\nfox\n\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\\(\\bar{y}_{i.}\\)\n\\(8.13\\)\n\\(4.74\\)\n\\(9.16\\)\n\n\n\\(\\beta_i\\)\n\\(-2.6\\)\n\\(0.79\\)\n\\(1.81\\)\n\n\n\n\n\n\nWie kriegen wir nun die ANOVA rechnerisch auf die Straße? Schauen wir uns dazu einmal die Abbildung 28.3 an. Auf der linken Seiten sehen wir vier Gruppen, die keinen Effekt haben. Die Gruppen liegen alle auf der gleichen Höhe. Es ist mit keinem Unterschied zwischen den Gruppen zu rechnen. Alle Gruppenmittel liegen auf dem globalen Mittel. Die Abweichungen der einzelnen Gruppenmittel zum globalen Mittel ist damit gleich null. Auf der rechten Seite sehen wir vier Gruppen mit einem Effekt. Die Gruppen unterscheiden sich in ihren Gruppenmitteln. Dadurch unterscheide sich aber auch die Gruppenmittel von dem globalen Mittel.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Kein Effekt\n\n\n\n\n\n\n\n\n\n\n\n(b) Leichter bis mittlerer Effekt\n\n\n\n\n\n\n\nAbbildung 28.3— Darstellung von keinem Effekt und leichtem bis mittleren Effekt in einer einfaktoriellen ANOVA mit einem Faktor mit vier Leveln A - D.\n\n\n\n\nWir können daher wie in Tabelle 28.4 geschrieben die Funktionsweise der ANOVA zusammenfassen. Wir vergleichen die Mittelwerte indem wir die Varianzen nutzen.\n\n\n\nTabelle 28.4— Zusammenfassung der ANOVA Funktionsweise.\n\n\n\n\n\n\n\n\n\n\nAll level means are equal.\n=\nThe differences between level means and the total mean are small.\n\n\n\n\n\n\nNun kommen wir zum eigentlichen Schwenk und warum eigentlich die ANOVA meist etwas verwirrt. Wir wollen eine Aussage über die Mittelwerte machen. Die Nullhypothese lautet, dass alle Mittelwerte gleich sind. Wie wir in Tabelle 28.4 sagen, heißt alle Mittelwerte gleich auch, dass die Abweichungen von den Gruppenmitteln zum globalen Mittel klein ist.\nWie weit die Gruppenmittel von dem globalen Mittel weg sind, dazu nutzt die ANOVA die Varianz. Die ANOVA vergleicht somit\n\ndie Varianz der einzelnen Mittelwerte der (Gruppen)Level zum globalen Mittel (eng. variability between levels)\nund die Varianz der Beobachtungen zu den einzelnen Mittelwerten der Level (eng. variability within one level)\n\nWir berechnen also wie die Beobachtungen jeweils um das globale Mittel streuen (\\(SS_{total}\\)), die einzelnen Beobachtungen um die einzelnen Gruppenmittel \\(SS_{error}\\) und die Streuung der Gruppenmittel um das globale Mittel (\\(SS_{animal}\\)). Wir nennen die Streuung Abstandquadrate (eng. sum of squares) und damit sind die Sum of Square \\((SS)\\) nichts anderes als die Varianz. Die Tabelle 28.5 zeigt die Berechnung des Anteils jeder einzelnen Beobachtung an den jeweiligen Sum of Squares.\n\n\n\n\nTabelle 28.5— Berechnung der \\(SS_{animal}\\), \\(SS_{error}\\) und \\(SS_{total}\\) anhand der einzelnen gemessenen Werte \\(y\\) für durch die jeweiligen Gruppenmittel \\(\\bar{y}_{i.}\\) und dem globalen Mittel \\(\\bar{y}_{..}\\) über alle Beobachtungen\n\n\n\n\n\n\n\n\n\n\n\n\n\nanimal (x)\njump_length (y)\n\\(\\boldsymbol{\\bar{y}_{i.}}\\)\nSS\\(_{\\boldsymbol{animal}}\\)\nSS\\(_{\\boldsymbol{error}}\\)\nSS\\(_{\\boldsymbol{total}}\\)\n\n\n\n\ndog\n\\(5.7\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.7 - 8.13)^2 = 5.90\\)\n\\((5.7 - 7.34)^2 = 2.69\\)\n\n\ndog\n\\(8.9\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.9 - 8.13)^2 = 0.59\\)\n\\((8.9 - 7.34)^2 = 2.43\\)\n\n\ndog\n\\(11.8\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((11.8 - 8.13)^2 = 13.47\\)\n\\((11.8 - 7.34)^2 = 19.89\\)\n\n\ndog\n\\(8.2\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.2 - 8.13)^2 = 0.00\\)\n\\((8.2 - 7.34)^2 = 0.74\\)\n\n\ndog\n\\(5.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.6 - 8.13)^2 = 6.40\\)\n\\((5.6 - 7.34)^2 = 3.03\\)\n\n\ndog\n\\(9.1\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((9.1 - 8.13)^2 = 0.94\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\ndog\n\\(7.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((7.6 - 8.13)^2 = 0.28\\)\n\\((7.6 - 7.34)^2 = 0.07\\)\n\n\ncat\n\\(3.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((3.2 - 4.74)^2 = 2.37\\)\n\\((3.2 - 7.34)^2 = 17.14\\)\n\n\ncat\n\\(2.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((2.2 - 4.74)^2 = 6.45\\)\n\\((2.2 - 7.34)^2 = 26.42\\)\n\n\ncat\n\\(5.4\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((5.4 - 4.74)^2 = 0.44\\)\n\\((5.4 - 7.34)^2 = 3.76\\)\n\n\ncat\n\\(4.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.1 - 4.74)^2 = 0.41\\)\n\\((4.1 - 7.34)^2 = 10.50\\)\n\n\ncat\n\\(4.3\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.3 - 4.74)^2 = 0.19\\)\n\\((4.3 - 7.34)^2 = 9.24\\)\n\n\ncat\n\\(7.9\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((7.9 - 4.74)^2 = 9.99\\)\n\\((7.9 - 7.34)^2 = 0.31\\)\n\n\ncat\n\\(6.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((6.1 - 4.74)^2 = 1.85\\)\n\\((6.1 - 7.34)^2 = 1.54\\)\n\n\nfox\n\\(7.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((7.7 - 9.16)^2 = 2.13\\)\n\\((7.7 - 7.34)^2 = 0.13\\)\n\n\nfox\n\\(8.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.1 - 9.16)^2 = 1.12\\)\n\\((8.1 - 7.34)^2 = 0.58\\)\n\n\nfox\n\\(9.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.1 - 9.16)^2 = 0.00\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\nfox\n\\(9.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.7 - 9.16)^2 = 0.29\\)\n\\((9.7 - 7.34)^2 = 5.57\\)\n\n\nfox\n\\(10.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.6 - 9.16)^2 = 2.07\\)\n\\((10.6 - 7.34)^2 = 10.63\\)\n\n\nfox\n\\(8.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.6 - 9.16)^2 = 0.31\\)\n\\((8.6 - 7.34)^2 = 1.59\\)\n\n\nfox\n\\(10.3\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.3 - 9.16)^2 = 1.30\\)\n\\((10.3 - 7.34)^2 = 8.76\\)\n\n\n\n\n\n\\(74.68\\)\n\\(56.53\\)\n\\(131.21\\)\n\n\n\n\n\n\n\nDie ANOVA wird deshalb auch Varianzzerlegung genannt, da die ANOVA versucht den Abstand der Beobachtungen auf die Variablen im Modell zu zerlegen. Also wie viel der Streuung von den Beobachtungen kann von dem Faktor animal erklärt werden? Genau der Abstand von den Gruppenmitteln zu dem globalen Mittelwert.\nDu kannst dir das ungefähr als eine Reise von globalen Mittelwert zu der einzelnen Beobachtung vorstellen. Nehmen wir als Beispiel die kleinste Sprungweite eines Katzenflohs von 2.2 cm und visualisieren wir uns die Reise wie in Abbildung 28.4 zu sehen. Wie kommen wir jetzt numerisch vom globalen Mittel mit \\(7.34\\) zu der Beobachtung? Wir können zum einen den direkten Abstand mit \\(2.2 - 7.34\\) gleich \\(-5.14\\) cm berechnen. Das wäre der total Abstand. Wie sieht es nun aus, wenn wir das Gruppenmittel mit beachten? In dem Fall gehen wir vom globalen Mittel zum Gruppenmittel cat mit \\(\\bar{y}_{cat} - \\bar{y}_{..} = 4.74 -7.34\\) gleich \\(\\beta_{cat} = -2.6\\) cm. Jetzt sind wir aber noch nicht bei der Beobachtung. Wir haben noch einen Rest von \\(y_{cat,2} - \\bar{y}_{cat} = 2.2 - 4.74\\) gleich \\(\\epsilon_{cat, 2} = -2.54\\) cm, die wir noch zurücklegen müssen. Das heißt, wir können einen Teil der Strecke mit dem Gruppenmittelwert erklären. Oder anders herum, wir können die Strecke vom globalen Mittelwert zu der Beobachtung in einen Teil für das Gruppenmittel und einen unerklärten Rest zerlegen.\n\n\n\n\n\n\n\nAbbildung 28.4— Visualisierung der Varianzzerlegung des Weges vom globalen Mittel zu der einzelnen Beoabchtung. Um zu einer einzelnen Beobachtung zu kommen legen wir den Weg vom globalen Mittelwert über den Abstand vom globalen Mittel zum Gruppenmittel \\(\\beta\\) zurück. Dann fehlt noch der Rest oder Fehler oder Residuum \\(\\epsilon\\).\n\n\n\n\nWir rechnen also eine ganze Menge an Abständen und quadrieren dann diese Abstände zu den Sum of Squares. Oder eben der Varianz. Dann fragen wir uns, ob der Faktor in unserem Modell einen Teil der Abstände erklären kann. Wir bauen uns dafür eine ANOVA Tabelle. Tabelle 28.6 zeigt eine theoretische, einfaktorielle ANOVA Tabelle. Wir berechnen zuerst die Abstände als \\(SS\\). Nun ist es aber so, dass wenn wir in einer Gruppe viele Level und/oder Beobachtungen haben, wir auch größere Sum of Squares bekommen. Wir müssen also die Sum of Squares in mittlere Abweichungsquadrate (eng. mean squares) mitteln. Abschließend können wir die F Statistik berechnen, indem wir die \\(MS\\) des Faktors durch die \\(MS\\) des Fehlers teilen. Das Verhältnis von erklärter Varianz vom Faktor zu dem unerklärten Rest.\n\n\n\n\nTabelle 28.6— Einfaktorielle ANOVA in der theoretischen Darstellung. Die sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(k-1\\)\n\\(SS_{animal} = \\sum_{i=1}^{k}n_i(\\bar{y}_{i.} - \\bar{y}_{..})^2\\)\n\\(MS_{animal} = \\cfrac{SS_{animal}}{k-1}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nerror\n\\(n-k\\)\n\\(SS_{error} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{i.})^2\\)\n\\(MS_{error} = \\cfrac{SS_{error}}{N-k}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{..})^2\\)\n\n\n\n\n\n\n\n\n\nWir füllen jetzt die Tabelle 28.7 einmal mit den Werten aus. Nachdem wir das getan haben oder aber die Tabelle in R ausgegeben bekommen haben, können wir die Zahlen interpretieren.\n\n\n\n\nTabelle 28.7— Einfaktorielle ANOVA mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dann direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 74.68\\)\n\\(MS_{animal} = \\cfrac{74.68}{3-1} = 37.34\\)\n\\(F_{calc} = \\cfrac{37.34}{3.14} = 11.89\\)\n\n\nerror\n\\(21-3\\)\n\\(SS_{error} = 56.53\\)\n\\(MS_{error} = \\cfrac{56.53}{18} = 3.14\\)\n\n\n\ntotal\n\\(21-1\\)\n\\(SS_{total} = 131.21\\)\n\n\n\n\n\n\n\n\n\nZu erst ist die berechnete F Statistik \\(F_{calc}\\) von Interesse. Wir haben hier eine \\(F_{calc}\\) von 11.89. Wir vergleichen wieder die berechnete F Statistik mit einem kritischen Wert. Der kritische F Wert \\(F_{\\alpha = 5\\%}\\) lautet für die einfaktorielle ANOVA in diesem konkreten Beispiel mit \\(F_{\\alpha = 5\\%} = 3.55\\). Die Entscheidungsregel nach der F Teststatistik lautet, die \\(H_0\\) abzulehnen, wenn \\(F_{calc} &gt; F_{\\alpha = 5\\%}\\).\nWir können also die Nullhypothese \\(H_0\\) in unserem Beispiel ablehnen. Es liegt ein signifikanter Unterschied zwischen den Tiergruppen vor. Mindestens ein Mittelwertsunterschied in den Sprungweiten liegt vor.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(F_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(F_{calc}\\) gilt, wenn \\(F_{calc} \\geq F_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr. Wir nutzen in der praktischen Anwendung den \\(p\\)-Wert.\n\n\nAbschließend noch ein Wort zu der Annahme an eine ANOVA. Wir wollen ja für eine ANOVA ein normalverteiltes Outcome \\(y\\) und die gleichen Varianzen über alle Gruppen. Wir wollen Varianzhomogenität vorliegen haben. In Abbildung 28.5 siehst du vier Behandlungsgruppen sowie deren lokale Mittel. Die Gruppe D hat eine sehr viel größere Varianz. Auch ist fraglich, ob die Gruppe D einer Normalverteilung folgt. Dadurch passieren zwei Dinge. Erstens ist der lokale Mittelwert von der Gruppe D viel näher am globalen Mittel als er eigentlich wäre, dadurch wird die Summe aus \\(MS_{treatment}\\) kleiner. Zweitens sind die Beobachtungen sehr weit um das lokale Mittel gestreut, dadurch wird die Summe aus \\(MS_{error}\\) viel größer. Wir erhalten dadurch eine sehr viel kleinere F Statistik. Im Prinzip vergleichst du mit einer ANOVA ein Set an gleich aussehenden Normalverteilungen. Wenn alles schief und krumm ist, dann ist es vermutlich sogar besser, als wenn wir eine total verkorkste Gruppenverteilung haben, wie hier in Beispiel D.\n\n\n\n\n\n\nAbbildung 28.5— Visualisierung der Annahmen an eine ANOVA. Wir berechnen den \\(MS_{treatment}\\) in dem wir alle Abweichungen vom gloablen Mittel \\(\\bar{y}_{..}\\) aufaddieren. Ebenso berechnen wir den \\(MS_{error}\\) indem wir die einzelen Abweichungen von den Bobachtungen \\(\\epsilon\\) zu den lokalen Mitteln berechnen. Hat jetzt eine Gruppe, wie die Gruppe D, eine abweichende Streuung, wird das \\(MS_{error}\\) überproportional groß. Damit wird dann die F Statistik klein.\n\n\n\nWir können das einmal numerisch mit einem kritischen Wert für \\(F_{\\alpha = 5\\%} = 3.55\\) durchspielen. Nehmen wir folgende Summe für \\(MS_{treatment}\\) aus der Abbildung 28.5 mit den einzelnen \\(MS\\) einmal wie folgt an. Wir nehmen an das \\(MS_A\\) gleich 10, \\(MS_B\\) gleich 20, \\(MS_C\\) gleich 8 und \\(MS_D\\) gleich 7 ist. Dann können wir durch das Summieren das \\(MS_{treatment}\\) berechnen.\n\\[\nMS_{treatment} = 10 + 20 + 8 + 7 = 45\n\\]\nWenn wir jetzt die Summe für \\(MS_{error}\\) berechnen, wird diese Summe sehr große, da die Gruppe D eine sehr große Varianz mit einbringt.\n\\[\nMS_{error} = 5 + 6 + 4 + 20 = 35\n\\]\nDas führt am Ende dazu, dass wir eine sehr kleine F Statistik erhalten. Auch wenn sich vielleicht Gruppe B von Gruppe C unterscheidet, können wir den Unterschied wegen der großen Varianz aus Gruppe D nicht nachweisen.\n\\[\nF = \\cfrac{MS_{treatment}}{MS_{error}} = \\cfrac{45}{35} = 1.28 \\leq 3.55\n\\]\nDu siehst hier wie wichtig es ist, ich die Daten einmal zu visualisieren um zu sehen vorher mögliche Probleme herrühren können. Auf der anderen Seite können wir die ANOVA auf ein Recht breites Spektrum an Daten anwenden, solange wir wissen, das die Verteilungen in etwa für jede Gruppe gleich aussehen. Wir müssen nur mit dem \\(\\eta^2\\) aufpassen, wenn wir zu schiefe Verteilungen haben. Dann gibt uns \\(\\eta^2\\) keine valide Aussage mehr über den Anteil der erklärten Varianz.\n\n\n28.2.4 Einfaktoriellen ANOVA in R\nWir rechnen keine ANOVA per Hand sondern nutzen R. Dazu müssen wir als erstes das Modell definieren. Das ist im Falle der einfaktoriellen ANOVA relativ einfach. Wir haben unseren Datensatz fac1_tbl mit einer kontinuierlichen Variable jump_lemgth als \\(y\\) vorliegen sowie einen Faktor animal mit mehr als zwei Leveln als \\(x\\). Wir definieren das Modell in R in der Form jump_length ~ animal. Um das Modell zu rechnen nutzen wir die Funktion lm() - die Abkürzung für linear model. Danach pipen wir die Ausgabe vom lm() direkt in die Funktion anova(). Die Funktion anova() berechnet uns dann die eigentliche einfaktorielle ANOVA. Wir speichern die Ausgabe der ANOVA in fit_1. Schauen wir uns die ANOVA Ausgabe einmal an.\n\nfit_1 &lt;-  lm(jump_length ~ animal, data = fac1_tbl) |&gt; \n  anova()\n\nfit_1\n\nAnalysis of Variance Table\n\nResponse: jump_length\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nanimal     2 74.683  37.341   11.89 0.0005113 ***\nResiduals 18 56.529   3.140                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten die Information was wir gerechnet haben, eine Varianzanalyse. Darunter steht, was das \\(y\\) war nämlich die jump_length. Wir erhalten eine Zeile für den Faktor animal und damit die \\(SS_{animal}\\) und eine Zeile für den Fehler und damit den \\(SS_{error}\\). In R heißen die \\(SS_{error}\\) dann Residuals. Die Zeile für die \\(SS_{total}\\) fehlt.\nNeben der berechneten F Statistik \\(F_{calc}\\) von \\(11.89\\) erhalten wir auch den p-Wert mit \\(0.005\\). Wir ignorieren die F Statistik, da wir in der Anwendung nur den p-Wert berücksichtigen. Die Entscheidung gegen die Nulhypothese lautet, dass wenn der p-Wert kleiner ist als das Signifkanzniveau \\(\\alpha\\) von 5% wir die Nullhypothese ablehnen.\nWir haben hier ein signifikantes Ergebnis vorliegen. Mindestens ein Gruppenmittelerstunterschied ist signifikant. Abbildung 28.6 zeigt nochmal die Daten fac1_tbl als Boxplot. Wir überprüfen visuell, ob das Ergebnis der ANOVA stimmen kann. Ja, die Boxplots und das Ergebnis der ANOVA stimmen überein. Die Boxplots liegen nicht alle auf einer Ebene, so dass hier auch ein signifikanter Unterschied zu erwarten war.\n\n\n\n\n\n\n\n\nAbbildung 28.6— Boxplot der Sprungweiten [cm] von Hunden-, Katzen- und Fuchsflöhen.\n\n\n\n\n\nAbschließend können wir noch die Funktion eta_squared() aus dem R Paket {effectsize} nutzen um einen Effektschätzer für die einfaktorielle ANOVA zu berechnen. Wir können mit \\(\\eta^2\\) abschätzen, welchen Anteil der Faktor animal an der gesamten Varianz erklärt.\n\nfit_1 |&gt; eta_squared()\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nanimal    | 0.57 | [0.27, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDas \\(\\eta^2\\) können wir auch einfach händisch berechnen.\n\\[\n\\eta^2 = \\cfrac{SS_{animal}}{SS_{total}} = \\cfrac{74.68}{131.21} = 0.57 = 57\\%\n\\]\nWir haben nun die Information, das 57% der Varianz der Beobachtungen durch den Faktor animal rklärt wird. Je nach Anwendungsgebiet kann die Relevanz sehr stark variieren. Im Bereich der Züchtung mögen erklärte Varianzen von unter 10% noch sehr relevant sein. Im Bereich des Feldexperiments erwarten wir schon höhere Werte für \\(\\eta^2\\). Immerhin sollte ja unsere Behandlung maßgeblich für die z.B. größeren oder kleineren Pflanzen gesorgt haben.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Die ANOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac2",
    "href": "stat-tests-anova.html#sec-fac2",
    "title": "28  Die ANOVA",
    "section": "28.3 Zweifaktorielle ANOVA",
    "text": "28.3 Zweifaktorielle ANOVA\n\nDie zweifaktorielle ANOVA ist eine wunderbare Methode um herauszufinden, ob zwei Faktoren einen Einfluss auf ein normalverteiltes \\(y\\) haben. Die Stärke der zweifaktoriellen ANOVA ist hierbei, dass die ANOVA beide Effekte der Faktoren auf das \\(y\\) simultan modelliert. Darüber hinaus können wir auch noch einen Interaktionsterm mit in das Modell aufnehmen um zu schauen, ob die beiden Faktoren untereinander auch interagieren. Somit haben wir mit der zweifaktoriellen ANOVA die Auswertungsmehode für ein randomiziertes Blockdesign vorliegen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie zweifaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität jeweils separat über beide Behandlungsfaktor \\(x_1\\) und \\(x_2\\). Daher alle Level von \\(x_1\\) sollen die gleiche Varianz haben. Ebenso sollen alle Level von \\(x_2\\) die gleiche Varianz haben. Unsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x_1\\) und \\(x_2\\) jeweils für sich homogen in den Varianzen sind.\n\n\n\n\n\n28.3.1 Daten für die zweifaktorielle ANOVA\nWir wollen uns nun einen etwas komplexes Modell anschauen mit einem etwas komplizierteren Datensatz flea_dog_cat_fox_site.csv. Wir brauchen hierfür ein normalverteiltes \\(y\\) und sowie zwei Faktoren. Das macht auch soweit Sinn, denn wir wollen ja auch eine zweifaktorielle ANOVA rechnen.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal sowie die Spalte site als \\(x\\). Danach müssen wir noch die Variable animal sowie die Variable site in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac2_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox_site.csv\") |&gt; \n  select(animal, site, jump_length) |&gt; \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 28.8 nochmal dargestellt.\n\n\n\n\nTabelle 28.8— Selektierter Datensatz für die zweifaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\n\n\n\n\n\nanimal\nsite\njump_length\n\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.10\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\ncat\ncity\n14.91\n\n\n\n\n\n\n\n\nDie Beispieldaten sind in Abbildung 28.7 abgebildet. Wir sehen auf der x-Achse den Faktor animal mit den drei Leveln dog, cat und fox. Jeder dieser Faktorlevel hat nochmal einen Faktor in sich. Dieser Faktor lautet site und stellt dar, wo die Flöhe gesammelt wurden. Die vier Level des Faktors site sind city, smalltown, village und field.\n\n\n\n\n\n\n\n\nAbbildung 28.7— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\n\nWir bauen dann mit den beiden Variablen bzw. Faktoren animal und site aus dem Objekt fac2_tbl folgendes Modell für die zweifaktorielle ANOVA:\n\\[\njump\\_length \\sim animal + site\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der zweifaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n\n28.3.2 Hypothesen für die zweifaktorielle ANOVA\nWir haben für jeden Faktor der zweifaktoriellen ANOVA ein Hypothesenpaar. Im Folgenden sehen wir die jeweiligen Hypothesenpaare. Einmal für animal, als Haupteffekt. Wir nennen einen Faktor den Hauptfaktor, weil wir an diesem Faktor am meisten interessiert sind. Wenn wir später einen Posthoc Test durchführen würden, dann würden wir diesen Faktor nehmen. Wir sind primär an dem Unterschied der Sprungweiten in [cm] in Gruppen Hund, Katze und Fuchs interessiert.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für site, als Nebeneffekt oder Blockeffekt oder Clustereffekt. Meist eine Variable, die wir auch erhoben haben und vermutlich auch einen Effekt auf das \\(y\\) haben wird. Oder aber wir haben durch das exprimentelle Design noch eine Aufteilungsvariable wie Block vorliegen. In unserem Beispiel ist es site oder der Ort, wo wir die Hunde-, Katzen, und Fuchsflöhe gefunden haben.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{city} = \\bar{y}_{smalltown} = \\bar{y}_{village} = \\bar{y}_{field}\\\\\nH_A: &\\; \\bar{y}_{city} \\ne \\bar{y}_{smalltown}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{village} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für die Interaktion animal:site - die eigentliche Stärke der zweifaktoriellen ANOVA. Wir können uns anschauen, ob die beiden Faktoren miteinander interagieren. Das heißt, ob eine Interaktion zwischen dem Faktor animal und dem Faktor site vorliegt.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{keine Interaktion}\\\\\nH_A: &\\; \\mbox{eine Interaktion zwischen animal und site}\n\\end{aligned}\n\\]\nWir haben also jetzt die verschiedenen Hypothesenpaare definiert und schauen uns jetzt die ANOVA in R einmal in der Anwendung an.\n\n\n28.3.3 Zweifaktoriellen ANOVA in R\nBei der einfaktoriellen ANOVA haben wir die Berechnungen der Sum of squares nochmal nachvollzogen. Im Falle der zweifaktoriellen ANOVA verzichten wir darauf. Das Prinzip ist das gleiche. Wir haben nur mehr Mitelwerte und mehr Abweichungen von diesen Mittelwerten, da wir ja nicht nur einen Faktor animal vorliegen haben sondern auch noch den Faktor site. Da wir aber die ANOVA nur Anwenden und dazu R nutzen, müssen wir jetzt nicht per Hand die zweifaktorielle ANOVA rechnen. Du musst aber die R Ausgabe der ANOVA verstehen. Und diese Ausgabe schauen wir uns jetzt einmal ohne und dann mit Interaktionsterm an.\nWir wollen nun einmal die zweifaktorielle ANOVA ohne Interaktionsterm rechnen die in Tabelle 28.9 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen die Funktion anova() in R.\n\n\n\n\nTabelle 28.9— Zweifaktorielle ANOVA ohne Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nerror\n\\(n-(a-1)(b-1)\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\n\n\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 37 erfahren.\nIm Folgenden sehen wir nochmal das Modell ohne Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel.\n\\[\njump\\_length \\sim animal + site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm().\n\nfit_2 &lt;-  lm(jump_length ~ animal + site, data = fac2_tbl) |&gt; \n  anova()\n\nfit_2\n\nAnalysis of Variance Table\n\nResponse: jump_length\n           Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nanimal      2 180.03  90.017 19.8808 3.92e-08 ***\nsite        3   9.13   3.042  0.6718    0.571    \nResiduals 114 516.17   4.528                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten wiederum die ANOVA Ergebnistabelle. Anstatt nur die Zeile animal für den Effekt des Faktors animal sehen wir jetzt auch noch die Zeile site für den Effekt des Faktors site. Zuerst ist weiterhin der Faktor animal signifikant, da der \\(p\\)-Wert mit \\(0.000000039196\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können von mindestens einem Gurppenunterschied im Faktor animal ausgehen. Im Weiteren ist der Faktor site nicht signifikant. Es scheint keinen Unterschied zwischend den einzelnen Orten und der Sprunglänge von den Hunde-, Katzen- und Fuchsflöhen zu geben.\nNeben der Standausgabe von R können wir auch die tidy Variante uns ausgeben lassen. In dem Fall sieht die Ausgabe etwas mehr aufgeräumt aus.\n\nfit_2 |&gt; tidy()\n\n# A tibble: 3 × 6\n  term         df  sumsq meansq statistic       p.value\n  &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 animal        2 180.    90.0     19.9    0.0000000392\n2 site          3   9.13   3.04     0.672  0.571       \n3 Residuals   114 516.     4.53    NA     NA           \n\n\nAbschließend können wir uns übr \\(\\eta^2\\) auch die erklärten Anteile der Varianz wiedergeben lassen.\n\nfit_2 |&gt; eta_squared()\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\nanimal    |           0.26 | [0.15, 1.00]\nsite      |           0.02 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 26%. Für den Faktor site haben wir nur einen Anteil von 2% der erklärten Varianz. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 28.10 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\n\nTabelle 28.10— Zweifaktorielle Anova ohne Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dann direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{4.53} = 19.88\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{4.53} = 0.67\\)\n\n\nerror\n\\(120-(3-1)(4-1)\\)\n\\(SS_{error} = 516.17\\)\n\\(MS_{error} = 4.53\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.33\\)\n\n\n\n\n\n\n\n\n\nDie eigentlich Stärke der zweifaktoriellen ANOVA ist die Nutzung des Interaktionsterm. Also die Berücksichtigung der Interaktion zwischen den beiden Faktoren in der ANOVA. Wir wollen nun noch einmal die zweifaktorielle ANOVA mit Interaktionsterm rechnen, die in Tabelle 28.11 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen wie immer die Funktion anova() in R.\n\n\n\n\nTabelle 28.11— Zweifaktorielle ANOVA mit Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nanimal \\(\\times\\) site\n\\((a-1)(b-1)\\)\n\\(SS_{animal \\times site}\\)\n\\(MS_{animal \\times site}\\)\n\\(F_{calc} = \\cfrac{MS_{animal \\times site}}{MS_{error}}\\)\n\n\nerror\n\\(n-ab\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\n\n\nIm Folgenden sehen wir nochmal das Modell mit Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel. Einen Interaktionsterm bilden wir durch das : in R ab. Wir können theoretisch auch noch weitere Interaktionsterme bilden, also auch x:y:z. Ich würde aber davon abraten, da diese Interaktionsterme schwer zu interpretieren sind.\n\\[\njump\\_length \\sim animal + site + animal:site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Es wieder das gleich wie schon zuvor. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm(). Auch die Interaktion müssen wir nicht extra in der ANOVA Funktion angeben. Alles wird im Modell des lm() abgebildet.\nDie visuelle Regel zur Überprüfung der Interaktion lautet nun wie folgt. Abbildung 28.8 zeigt die entsprechende Vislualisierung. Wir haben keine Interaktion vorliegen, wenn die Geraden parallel zueinander laufen und die Abstände bei bei jedem Faktorlevel gleich sind. Wir schauen uns im Prinzip die erste Faktorstufe auf der x-Achse an. Wir sehen den Abstand von der roten zu blauen Linie sowie das die blaue Gerade über der roten Gerade liegt. Dieses Muster erwarten wir jetzt auch an dem Faktorlevel B und C. Eine leichte bis mittlere Interaktion liegt vor, wenn sich die Abstände von dem zweiten Faktor über die Faktorstufen des ersten Faktors ändern. Eine starke Interaktion liegt vor, wenn sich die Geraden schneiden.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Keine Interaktion\n\n\n\n\n\n\n\n\n\n\n\n(b) Leichte bis mittlere Intraktion\n\n\n\n\n\n\n\n\n\n\n\n(c) Starke Interaktion\n\n\n\n\n\n\n\nAbbildung 28.8— Darstellung von keiner Interaktion, leichter bis mittler Interaktion und starker Interaktion in einer zweifaktoriellen ANOVA mit einem Faktor mit drei Leveln A, B und C sowie einem Faktor mit zwei Leveln (rot und blau).\n\n\n\n\nIn der Abbildung 28.9 sehen wir den Interaktionsplot für unser Beispiel. Auf der y-Achse ist die Sprunglänge abgebildet und auf der x-Achse der Faktor animal. Die einzelnen Farben stellen die Level des Faktor site dar.\n\nggplot(fac2_tbl, aes(x = animal, y = jump_length,\n                     color = site, group = site)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 28.9— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\n\nWenn sich die Geraden in einem Interaktionsplot schneiden, haben wir eine Interaktion zwischen den beiden Faktoren vorliegen. Wir schauen zur visuellen Überprüfung auf den Faktor animal und das erste level cat. Wir sehen die Ordnung des zweiten Faktors site mit field, village, smalltown und city. Diese Ordnung und die Abstände sind bei zweiten Faktorlevel dog schon nicht mehr gegeben. Die Geraden schneiden sich. Auch liegt bei dem Level fox eine andere Ordnung vor. Daher sehen wir hier eine starke Interaktion zwischen den beiden Faktoren animal und site. Du kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 37 erfahren.\nWir nehmen jetzt auf jeden Fall den Interaktionsterm animal:site mit in unser Modell und schauen uns einmal das Ergebnis der ANOVA an. Das lineare Modell der ANOVA wird erneut über die Funktion lm() berechnet und anschließend in die Funktion anova() gepipt.\n\nfit_3 &lt;-  lm(jump_length ~ animal + site + animal:site, data = fac2_tbl) |&gt; \n  anova()\n\nfit_3\n\nAnalysis of Variance Table\n\nResponse: jump_length\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nanimal        2 180.03  90.017 30.2807 3.63e-11 ***\nsite          3   9.13   3.042  1.0233   0.3854    \nanimal:site   6 195.11  32.519 10.9391 1.71e-09 ***\nResiduals   108 321.05   2.973                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDie Ergebnistabelle der ANOVA wiederholt sich. Wir sehen, dass der Faktor animal signifikant ist, da der p-Wert mit \\(0.000000000036\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese ablehnen. Mindestens ein Mittelwertsvergleich unterschiedet sich zwischen den Levels des Faktors animal. Im Weiteren sehen wir, dass der Faktor site nicht signifkant ist, da der p-Wert mit \\(0.39\\) größer ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese nicht ablehnen. Abschließend finden wir die Interaktion zwischen dem Faktor animalund site las signifikant vor. Wenn wir eine signifikante Interaktion vorliegen haben, dann müssen wir den Faktor animal getrennt für jedes Levels des Faktors site auswerten. Wir können keine Aussage über die Sprungweite von Hunde-, Katzen- und Fuchsflöhen unabhängig von der Herkunft site der Flöhe machen.\nWir können wie immer die etwas aufgeräumte Variante der ANOVA Ausgabe mit der Funktion tidy() uns ausgeben lassen.\n\nfit_3 |&gt; tidy()\n\n# A tibble: 4 × 6\n  term           df  sumsq meansq statistic   p.value\n  &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 animal          2 180.    90.0      30.3   3.63e-11\n2 site            3   9.13   3.04      1.02  3.85e- 1\n3 animal:site     6 195.    32.5      10.9   1.71e- 9\n4 Residuals     108 321.     2.97     NA    NA       \n\n\nIm Folgenden können wir noch die \\(\\eta^2\\) für die ANOVA als Effektschätzer berechnen lassen.\n\nfit_3 |&gt; eta_squared()\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Eta2 (partial) |       95% CI\n-------------------------------------------\nanimal      |           0.36 | [0.24, 1.00]\nsite        |           0.03 | [0.00, 1.00]\nanimal:site |           0.38 | [0.24, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 36%. Für den Faktor site haben wir nur einen Anteil von 3% der erklärten Varianz. Die Interaktion zwischen animal und site erklärt 38% der beobachteten Varianz udn ist somit auch vom Effekt her nicht zu ignorieren. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 28.12 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\n\nTabelle 28.12— Zweifaktorielle Anova mit Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dan direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{2.97} = 30.28\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{2.97} = 1.02\\)\n\n\nanimal \\(\\times\\) site\n\\((3-1)(4-1)\\)\n\\(SS_{animal \\times site} = 195.12\\)\n\\(MS_{animal \\times site} = 32.52\\)\n\\(F_{calc} = \\cfrac{32.52}{2.97} = 10.94\\)\n\n\nerror\n\\(120 - (3 \\cdot 4)\\)\n\\(SS_{error} = 321.06\\)\n\\(MS_{error} = 2.97\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.34\\)\n\n\n\n\n\n\n\n\n\nWenn wir eine zweifaktorielle ANOVA rechnen, dann können wir verschiedene Typen von ANOVAs rechnen. Dabei unterscheiden wir zwischen den Typen I, II und III. In dem folgenden Kasten gehe ich einmal auf die drei Typen ein. Wichtig ist, dass wir nur zwischen den drei Typen unterscheiden müssen, wenn wir ein unbalanciertes Design vorliegen haben. Ein unbalanciertes Design haben wir vorliegen, wenn wir in den einzelnen Behandlungsgrupen nicht die gleiche Anzahl an Beobachtungen vorliegen haben. Aber auch hier kommt es dann auf eine Beobachtung Unterschied wieder nicht so an.\n\n\n\n\n\n\nDie ANOVA Typ I, II und III\n\n\n\nNochmal ganz wichtig, die Ergebnisse der ANOVA-Typen unterscheiden sich vor allem bei unbalancierten Datensätzen. Bei balancierten Datensätzen liefern alle drei Typen in der Regel ähnliche Ergebnisse. Wenn du aber nicht die gleichen Anzahlen an Beobachtungen pro Gruppe hast, dann gilt Folgendes:\n\nDie Ergebnisse von Typ I hängen von der Reihenfolge der Faktoren ab, die du ins Modell nimmst. Es macht also einen Unterschied, ob du erst \\(f_1\\) und dann \\(f_2\\) ins Modell aufnimmst oder umgekehrt.\nDie Ergebnisse von Typ II und III bleiben unabhängig von der Reihenfolge der Terme gleich, unterscheiden sich aber von den Ergebnissen von Typ I.\nBei Typ III werden die Haupteffekte von \\(f_1\\) und \\(f_2\\) um die Interaktion \\(f_1:f_2\\) bereinigt, was bei Modellen mit Interaktionsterm zu anderen Ergebnissen führt als bei Typ II.\n\nWir können die verschiedenen ANOVA Typen dann mit dem R Paket {car} rechnen. Das ist relativ einfach, die Funktion lauetet dann nur Anova() mit einem großen A am Anfang. Du ersetzt also einfach die Funktion anova() durch die Funktion Anova(), wenn du ein unbalanciertes Design vorliegen hast. Ich würde dir per default die ANOVA Type III empfehlen, wenn du auch einen Interaktionsterm mit in deinem Modell hast. Dann bist du auf der sicheren Seite.\n\n# ANOVA Type I\nfit |&gt; anova()\n\n# ANOVA Type II\nfit |&gt; Anova(type = \"II\")\n\n# ANOVA Type III\nfit |&gt; Anova(type = \"III\")\n\nEinen tieferen Einblick gibt es natürlich wie immer auch unter Anova – Type I/II/III SS explained.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Die ANOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-anova.html#und-weiter",
    "href": "stat-tests-anova.html#und-weiter",
    "title": "28  Die ANOVA",
    "section": "28.4 Und weiter?",
    "text": "28.4 Und weiter?\nNach einer berechneten ANOVA können wir zwei Fälle vorliegen haben.\n\nWir habe eine nicht signifikante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) nicht ablehnen und die Mittelwerte über den Faktor sind vermutlich alle gleich. Wir enden hier mit unserer statistischen Analyse.\nWir haben eine signifikante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) ablehnen und mindestens ein Gruppenvergleich über mindestens einen Faktor ist vermutlich unterschiedlich. Wir können dann in Kapitel 33 eine Posthoc Analyse rechnen.\n\nWenn du in deinem Experiment keine signifikanten Ergebnisse findest, ist das nicht schlimm. Du kannst deine Daten immer noch mit der explorativen Datenanalyse auswerten wie in Kapitel 16 beschrieben. ƒƒ\n\n\n\nAbbildung 28.1 (a)— Die Sprungweite in [cm] ohne den Faktor animal betrachtet.\nAbbildung 28.1 (b)— Die Sprungweite in [cm] mit den Faktor animal eingefärbt.\nAbbildung 28.1 (c)— Die Sprungweite in [cm] mit den Faktor animal eingefärbt und gruppiert.\nAbbildung 28.2 (a)— Die Sprungweite in [cm] mit den Faktor animal gruppiert und das globale Mittel \\(\\bar{y}_{..} = 7.34\\) ergänzt.\nAbbildung 28.2 (b)— Die Sprungweite in [cm] mit den Faktor animal gruppiert und die lokalen Mittel \\(\\bar{y}_{i.}\\) für jedes Level ergänzt.\nAbbildung 28.2 (c)— Die Sprungweite in [cm] mit den Faktor animal gruppiert und die Abweichungen \\(\\beta_i\\) ergänzt.\nAbbildung 28.3 (a)— Kein Effekt\nAbbildung 28.3 (b)— Leichter bis mittlerer Effekt\nAbbildung 28.4— Visualisierung der Varianzzerlegung des Weges vom globalen Mittel zu der einzelnen Beoabchtung. Um zu einer einzelnen Beobachtung zu kommen legen wir den Weg vom globalen Mittelwert über den Abstand vom globalen Mittel zum Gruppenmittel \\(\\beta\\) zurück. Dann fehlt noch der Rest oder Fehler oder Residuum \\(\\epsilon\\).\nAbbildung 28.5— Visualisierung der Annahmen an eine ANOVA. Wir berechnen den \\(MS_{treatment}\\) in dem wir alle Abweichungen vom gloablen Mittel \\(\\bar{y}_{..}\\) aufaddieren. Ebenso berechnen wir den \\(MS_{error}\\) indem wir die einzelen Abweichungen von den Bobachtungen \\(\\epsilon\\) zu den lokalen Mitteln berechnen. Hat jetzt eine Gruppe, wie die Gruppe D, eine abweichende Streuung, wird das \\(MS_{error}\\) überproportional groß. Damit wird dann die F Statistik klein.\nAbbildung 28.6— Boxplot der Sprungweiten [cm] von Hunden-, Katzen- und Fuchsflöhen.\nAbbildung 28.7— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\nAbbildung 28.8 (a)— Keine Interaktion\nAbbildung 28.8 (b)— Leichte bis mittlere Intraktion\nAbbildung 28.8 (c)— Starke Interaktion\nAbbildung 28.9— Boxplot der Sprungweiten [cm] von Hunden und Katzen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Die ANOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-ancova.html",
    "href": "stat-tests-ancova.html",
    "title": "29  Die ANCOVA",
    "section": "",
    "text": "29.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, quantreg,\n               see, performance, emmeans, multcomp, janitor,\n               parameters, conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Die ANCOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-ancova.html#daten",
    "href": "stat-tests-ancova.html#daten",
    "title": "29  Die ANCOVA",
    "section": "29.2 Daten",
    "text": "29.2 Daten\nFür unser Beispiel nutzen wir die Daten der Sprungweite in [cm] von Flöhen auf Hunde-, Katzen- und Füchsen. Damit haben wir den ersten Faktor animal mit drei Leveln. Als Kovariate schauen wir uns das Gewicht als numerische Variable an. Schlussendlich brauchen wir noch das Outcome jump_length als \\(y\\). Für die zweifaktorielle ANCOVA nehmen wir noch den Faktor sex mit zwei Leveln hinzu.\n\nancova_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") |&gt;\n  select(animal, sex, jump_length, weight) |&gt; \n  mutate(animal = as_factor(animal))\n\nIn der Tabelle 55.1 ist der Datensatz ancova_tbl nochmal dargestellt.\n\n\n\n\nTabelle 29.1— Datensatz zu der Sprunglänge in [cm] von Flöhen auf Hunde-, Katzen- und Füchsen.\n\n\n\n\n\n\nanimal\nsex\njump_length\nweight\n\n\n\n\ncat\nmale\n15.79\n6.02\n\n\ncat\nmale\n18.33\n5.99\n\n\ncat\nmale\n17.58\n8.05\n\n\ncat\nmale\n14.09\n6.71\n\n\ncat\nmale\n18.22\n6.19\n\n\ncat\nmale\n13.49\n8.18\n\n\n…\n…\n…\n…\n\n\nfox\nfemale\n27.81\n8.04\n\n\nfox\nfemale\n24.02\n9.03\n\n\nfox\nfemale\n24.53\n7.42\n\n\nfox\nfemale\n24.35\n9.26\n\n\nfox\nfemale\n24.36\n8.85\n\n\nfox\nfemale\n22.13\n7.89\n\n\n\n\n\n\n\n\nUnser zweiter Datensatz ist ein Anwendungsdatensatz aus dem Gemüsebau. Wir schauen uns das Wachstum von drei Gurkensorten über siebzehn Wochen an. Die Gurkensorten sind hier unsere Versuchsgruppen. Da wir es hier mit echten Daten zu tun haben, müssen wir uns etwas strecken damit die Daten dann auch passen. Wir wollen das Wachstum der drei Gurkensorten über die Zeit betrachten - also faktisch den Verlauf des Wachstums.\n\ngurke_raw_tbl &lt;- read_excel(\"data/wachstum_gurke.xlsx\") |&gt; \n  clean_names() |&gt; \n  select(-pfl, -erntegewicht) |&gt; \n  mutate(versuchsgruppe = as_factor(versuchsgruppe)) \n\nIn der Tabelle 55.2 sehen wir einmal die rohen Daten dargestellt.\n\n\n\n\nTabelle 29.2— Datensatz zu dem Längen- und Dickenwachstum von Gurken.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nversuchsgruppe\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nt8\nt9\nt10\nt11\nt12\nt13\nt14\nt15\nt16\nt17\n\n\n\n\nProloog L\n5.5\n6.1\n7.4\n8.9\n9.9\n12\n14.4\n17\n19.8\n21.2\n23.2\n24\n29.7\n32.8\nNA\nNA\nNA\n\n\nProloog L\n4.6\n5.1\n6.4\n5.7\n5.5\n5.2\n5\n5\n4.5\n0\n0\n0\n0\n0\nNA\nNA\nNA\n\n\nProloog L\n5.3\n5.8\n6.8\n8.3\n9\n10\n12.3\n14.6\n17.6\n19.3\n23.1\n23.8\n31.7\n32.3\nNA\nNA\nNA\n\n\nProloog L\n5.4\n5.7\n6.9\n8.2\n8.6\n10\n12.1\n14.5\n16.2\n17.1\n19.3\n21.6\n28.5\n30\nNA\nNA\nNA\n\n\nProloog L\n5\n5.5\n6.3\n7.5\n8.3\n10\n12.2\n14.4\n16.5\n19.9\n21\n22.9\n30.4\n31\nNA\nNA\nNA\n\n\nProloog L\n4.2\n4.6\n5.4\n5.5\n5.2\n5.3\n6.1\n6.5\n8\n9.3\n11\n12.5\n22.3\n24.2\nNA\nNA\nNA\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nKatrina D\n0.62\n0.64\n0.65\n0.65\n0.74\n0.9\n1.07\n1.09\n1.18\n1.09\n1.13\n1.14\n1.3\n1.48\n1.75\n2.02\n2.97\n\n\nKatrina D\n0.55\n0.59\n0.64\n0.62\n0.64\n0.7\n0.68\n0.66\n0.66\n0.7\n0.6\n0.6\n0.57\n0\nNA\nNA\nNA\n\n\nKatrina D\n0.64\n0.73\n0.77\n0.8\n0.86\n1.2\n1.25\n1.42\n1.75\n1.92\n2.24\n2.52\n4.02\n3.9\nNA\nNA\nNA\n\n\nKatrina D\n0.86\n0.87\n0.9\n1.1\n1.12\n1.4\n1.7\n2.1\n2.35\n2.4\n2.51\n2.57\n4.07\n4.3\nNA\nNA\nNA\n\n\nKatrina D\n0.62\n0.67\n0.7\n0.71\n0.78\n0.95\n1.1\n1.24\n1.41\n1.66\n2.02\n2.25\n3.85\n4.07\nNA\nNA\nNA\n\n\nTageslänge\n13.98\n14.05\n14.12\n14.18\n14.25\n14.3\n14.37\n14.43\n14.5\n14.57\n14.62\n14.68\n14.75\n14.8\n15.1\n15.15\n15.32\n\n\n\n\n\n\n\n\nWir haben zwei Typen von Daten für das Gurkenwachstum. Einmal messen wir den Durchmesser für jede Sorte (D im Namen der Versuchsgruppe) oder aber die Länge (L im Namen der Versuchsgruppe). Wir betrachten hier nur das Längenwachstum und deshalb filtern wir erstmal nach allen Versuchsgruppen mit einem L im Namen. Dann müssen wir die Daten noch in Long-Format bringen. Da wir dann auch noch auf zwei Arten die Daten über die Zeit darstellen wollen, brauchen wir einmal die Zeit als Faktor time_fct und einmal als numerisch time_num. Leider haben wir auch Gurken mit einer Länge von 0 cm. Diese Gurken schmeißen wir am Ende mal raus. Auch haben wir ab Woche 14 keine Messungen mehr in der Versuchsgruppe Prolong, also nehmen wir auch nur die Daten bis zur vierzehnten Woche.\n\ngurke_time_len_tbl &lt;- gurke_raw_tbl |&gt; \n  filter(str_detect(versuchsgruppe, \"L$\")) |&gt; \n  mutate(versuchsgruppe = factor(versuchsgruppe, \n                                 labels = c(\"Proloog\", \"Quarto\", \"Katrina\"))) |&gt; \n  pivot_longer(cols = t1:t17,\n               values_to = \"length\",\n               names_to = \"time\") |&gt; \n  mutate(time_fct = as_factor(time),\n         time_num = as.numeric(time_fct)) |&gt; \n  filter(length != 0) |&gt; \n  filter(time_num &lt;= 14)",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Die ANCOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-ancova.html#hypothesen-für-die-ancova",
    "href": "stat-tests-ancova.html#hypothesen-für-die-ancova",
    "title": "29  Die ANCOVA",
    "section": "29.3 Hypothesen für die ANCOVA",
    "text": "29.3 Hypothesen für die ANCOVA\nWir haben für jeden Faktor der ANCOVA ein Hypothesenpaar sowie ein Hypothesenpaar für die Kovariate. Im Folgenden sehen wir die jeweiligen Hypothesenpaare.\nEinmal für animal, als Haupteffekt. Wir nennen einen Faktor den Hauptfaktor, weil wir an diesem Faktor am meisten interessiert sind. Wenn wir später einen Posthoc Test durchführen würden, dann würden wir diesen Faktor nehmen. Wir sind primär an dem Unterschied der Sprungweiten in [cm] in Gruppen Hund, Katze und Fuchs interessiert.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nDu kannst mehr über Geraden sowei lineare Modelle und deren Eigenschaften im Kapitel 37 erfahren.\nFür die Kovariate testen wir anders. Die Kovariate ist ja eine numerische Variable. Daher ist die Frage, wann gibt es keinen Effekt von weight auf die Sprunglänge? Wenn wir eine parallele Linie hätten. Das heißt, wenn sich der Wert von weight ändert, ändert sich der Wert von jump_length nicht. Wir schreiben, dass sich die Steigung der Geraden nicht ändert. Wir bezeichnen die Steigung einer Graden mit \\(\\beta\\). Wenn kein Effekt vorliegt und die Nullhpyothese gilt, dann ist die Steigung der Geraden \\(\\beta_{weight} = 0\\).\n\\[\n\\begin{aligned}\nH_0: &\\; \\beta_{weight} = 0\\\\\nH_A: &\\; \\beta_{weight} \\neq 0\n\\end{aligned}\n\\]\nDu kannst dir überlegen, ob due die Interaktion zwischen dem Faktor und der Kovariate mit ins Modell nehmen willst. Eigentlich schauen wir uns immer nur die Interaktion zwischen den Faktoren an. Generell schreiben wir eine Interaktionshypothese immer in Prosa.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{keine Interaktion}\\\\\nH_A: &\\; \\mbox{eine Interaktion zwischen animal und site}\n\\end{aligned}\n\\]\nWir haben also jetzt die verschiedenen Hypothesenpaare definiert und schauen uns jetzt die ANCOVA in R einmal in der Anwendung an.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Die ANCOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-ancova.html#die-einfaktorielle-ancova-in-r",
    "href": "stat-tests-ancova.html#die-einfaktorielle-ancova-in-r",
    "title": "29  Die ANCOVA",
    "section": "29.4 Die einfaktorielle ANCOVA in R",
    "text": "29.4 Die einfaktorielle ANCOVA in R\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 37 erfahren.\nWir können die ANCOVA ganz klassisch mit dem linaren Modell fitten. Wir nutzen die Funktion lm() um die Koeffizienten des linearen Modellls zu erhalten. Wir erinnern uns, wir haben haben einen Faktor \\(f_1\\) und eine Kovariate bezwiehungsweise ein numerisches \\(c_1\\). In unserem Beispiel sieht dann der Fit des Modells wie folgt aus.\n\nfit_1 &lt;- lm(jump_length ~ animal + weight + animal:weight, data = ancova_tbl)\n\nNachdem wir das Modell in dem Objekt fit_1 gespeichert haben können wir dann das Modell in die Funktion anova() pipen. Die Funktion erkennt, das wir eine ANCOVA rechnen wollen, da wir in unserem Modell einen Faktor und eine Kovariate mit enthalten haben.\n\nfit_1 |&gt; anova ()\n\nAnalysis of Variance Table\n\nResponse: jump_length\n               Df Sum Sq Mean Sq  F value Pr(&gt;F)    \nanimal          2 2693.8 1346.88 204.2764 &lt;2e-16 ***\nweight          1 1918.0 1917.99 290.8961 &lt;2e-16 ***\nanimal:weight   2    0.3    0.14   0.0214 0.9788    \nResiduals     594 3916.5    6.59                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn der ANCOVA erkennne wir nun, dass der Faktor animal signifikant ist. Der \\(p\\)-Wert ist mit \\(&lt;0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Ebenso ist die Kovariate weight signifikant. Der \\(p\\)-Wert ist ebenfalls mit \\(&lt;0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können also schlussfolgern, dass sich mindestens eine Gruppenvergleich der Level des Faktors animal voneinander unterscheidet. Wir wissen auch, dass mit der Zunahme des Gewichts, die Sprunglänge sich ändert.\nDie ANCOVA liefert keine Informationen zu der Größe oder der Richtung des Effekts der Kovariate.\nWas wir nicht wissen, ist die Richtung. Wir wissen nicht, ob mit ansteigenden Gewicht sich die Sprunglänge erhöht oder vermindert. Ebenso wenig wissen wir etwas über den Betrag des Effekts. Wieviel weiter springen denn nun Flöhe mit 1 mg Gewicht mehr? Wir haben aber die Möglichkeit, den Sachverhalt uns einmal in einer Abbildung zu visualisieren. In Abbildung 29.1 sehen wir die Daten einmal als Scatterplot dargestellt.\n\nggplot(ancova_tbl, aes(weight, jump_length, color = animal)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_okabeito() +\n  theme_minimal() +\n  geom_point() +\n  labs(color  = \"Tierart\", shape = \"Geschlecht\")  \n\n\n\n\n\n\n\nAbbildung 29.1— Scatterplot der Daten zur einfaktoriellen ANCOVA.\n\n\n\n\n\nDer Abbildung 29.1 können wir jetzt die positive Steigung entnehmen sowie die Reihenfolge der Tierarten nach Sprungweiten. Die ANCOVA sollte immer visualisiert werden, da sich hier die Stärke der Methode mit der Visualiserung verbindet.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Die ANCOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-ancova.html#die-zweifaktorielle-ancova-in-r",
    "href": "stat-tests-ancova.html#die-zweifaktorielle-ancova-in-r",
    "title": "29  Die ANCOVA",
    "section": "29.5 Die zweifaktorielle ANCOVA in R",
    "text": "29.5 Die zweifaktorielle ANCOVA in R\nDie zweifaktorielle ANCOVA erweitert die einfaktorielle ANCOVA um einen weiteren Faktor. Das ist manchmal etwas verwirrend, da wir auf einmal drei oder mehr Terme in einem Modell haben. Klassischerweise haben wir nun zwei Faktoren \\(f_1\\) und \\(f_2\\) in dem Modell. Weiterhin haben wir nur eine Kovariate \\(c_1\\). Damit sehe das Modell wie folgt aus.\n\\[\ny \\sim f_1 + f_2 + c_1\n\\]\nWir können das Modell dann in R übertragen und ergänzen noch den Interaktionsterm für die Faktoren animal und sex in dem Modell. Das Modell wird klassisch in der Funktion lm() gefittet.\n\nfit_2 &lt;- lm(jump_length ~ animal + sex + weight + animal:sex, data = ancova_tbl)\n\nNach dem Fit können wir das Modell in dem Obkjekt fit_2 in die Funktion anova() pipen. Die Funktion erkennt die Struktur des Modells und gibt uns eine ANCOVA Ausgabe wieder.\n\nfit_2 |&gt; anova() \n\nAnalysis of Variance Table\n\nResponse: jump_length\n            Df Sum Sq Mean Sq  F value Pr(&gt;F)    \nanimal       2 2693.8  1346.9 359.0568 &lt;2e-16 ***\nsex          1 3608.1  3608.1 961.8568 &lt;2e-16 ***\nweight       1    0.0     0.0   0.0053 0.9422    \nanimal:sex   2    2.2     1.1   0.2981 0.7424    \nResiduals  593 2224.4     3.8                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn der ANCOVA erkennne wir nun, dass der Faktor animal signifikant ist. Der \\(p\\)-Wert ist mit \\(&lt;0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Ebenso ist der Faktor sex signifikant. Der \\(p\\)-Wert ist mit \\(&lt;0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Die Kovariate weight ist nicht mehr signifikant. Der \\(p\\)-Wert ist mit \\(0.94\\) größer das das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können also schlussfolgern, dass sich mindestens eine Gruppenvergleich der Level des Faktors animal voneinander unterscheidet. Ebenso wie können wir schlussfolgern, dass sich mindestens eine Gruppenvergleich der Level des Faktors site voneinander unterscheidet. Da wir nur zwei Level in dem Faktor sex haben, wissenwir nun, dass sich die beiden Geschlechter der Flöhe in der Sprungweite unterscheiden. Wir wissen auch, dass mit der Zunahme des Gewichts, sich die Sprunglänge nicht ändert.\nIn Abbildung 29.2 sehen wir nochmal den Zusammenhang dargestellt. Wenn wir die Daten getrennt für den Faktor sex anschauen, dann sehen wir, dass das Gewicht keinen Einfluss mehr auf die Sprungweite hat.\n\nggplot(ancova_tbl, aes(weight, jump_length, color = animal)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_okabeito() +\n  theme_minimal() +\n  geom_point() +\n  labs(color  = \"Tierart\", shape = \"Geschlecht\") +\n  facet_wrap(~ sex, scales = \"free_x\")\n\n\n\n\n\n\n\nAbbildung 29.2— Scatterplot der Daten zur einfaktoriellen ANCOVA aufgetelt nach dem Geschlecht der Flöhe.\n\n\n\n\n\nNach einer berechneten ANCOVA können wir zwei Fälle vorliegen haben.\nWenn du in deinem Experiment keine signifikanten Ergebnisse findest, ist das nicht schlimm. Du kannst deine Daten immer noch mit der explorativen Datenanalyse auswerten wie in Kapitel 16 beschrieben.\n\nWir habe eine nicht signifkante ANCOVA berechnet. Wir können die Nullhypothese \\(H_0\\) nicht ablehnen und die Mittelwerte über den Faktor sind vermutlich alle gleich. Wir enden hier mit unserer statistischen Analyse.\nWir haben eine signifikante ANCOVA berechnet. Wir können die Nullhypothese \\(H_0\\) ablehnen und mindestens ein Gruppenvergleich über mindestens einen Faktor ist vermutlich unterschiedlich. Wir können dann in Kapitel 33 eine Posthoc Analyse rechnen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Die ANCOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-ancova.html#linearer-trendtest",
    "href": "stat-tests-ancova.html#linearer-trendtest",
    "title": "29  Die ANCOVA",
    "section": "29.6 Linearer Trendtest",
    "text": "29.6 Linearer Trendtest\n\n“Make use of time, let not advantage slip.” — William Shakespeare\n\nIm Folgenden schauen wir uns dann die Auswertung der Gurkendaten einmal genauer an. Für mehr Informationen zu dem Paket {emmeans} und den entsprechenden Funktionen dann bitte einmal in das Kapitel Kapitel 33 schauen. In der Abbildung 29.3 sehen wir die Daten einmal als Scatterplot. Die durchgezogene Gerade stellt den Verlauf der Mittelwerte über die Versuchsgruppen dar. Die gestrichelte Linie zeigt den Median über die Gruppen. Wir wollen jetzt der Frage nachgehen, ob es einen Unterschied zwischen den Gurkensorten versuchsgruppen über den zeitlichen Verlauf der vierzehn Wochen gibt.\n\nggplot(gurke_time_len_tbl, aes(time_num, length, color = versuchsgruppe)) +\n  theme_minimal() +\n  geom_point() +\n  stat_summary(fun = \"mean\", geom = \"line\") +\n  stat_summary(fun = \"median\", geom = \"line\", linetype = 2) +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 29.3— Scatterplot des Längenwachstums der drei Gurkensorten über vierzehn Wochen. Die gestrichtelten Linien stellen den Median und die durchgezogene Line den Mittelwert der Gruppen dar.\n\n\n\n\n\nUnser erstes Modell was wir uns anschauen wollen ist nochmal eine klassische zweifaktorielle ANOVA mit einem Interaktionsterm. Wir wollen raus finden ob die Länge von den Gurken von den Sorten (\\(f_1\\)), dem zeitlichen Verlauf (\\(f_2\\)) und der Interaktion zwischen der Sorten und der Zeit abhängt (\\(f_1:f_2\\)). Wir stellen nun folgendes lineares Modell für die zweifaktorielle ANOVA auf.\n\\[\nlength \\sim \\overbrace{versuchsgruppe}^{f_1} + \\underbrace{time\\_fct}_{f_2} + \\overbrace{versuchsgruppe:time\\_fct}^{f_1:f_2}\n\\]\nDieses Modell können wir dann auch in R einmal über die Funktion lm() abbilden.\n\ntime_fct_fit &lt;- lm(length ~ versuchsgruppe + time_fct + versuchsgruppe:time_fct, \n                   gurke_time_len_tbl)\n\nNun wollen wir auch überprüfen, ob es eine Interaktion zwischen den Versuchsgruppen und dem zeitlichen Verlauf gibt. Das ganze schauen wir uns neben einer ANOVA auch einmal graphisch mit der Funktion emmip() an. Wenn wir keine signifikante Interaktion erwaten würden, dann müssten die drei Versuchgruppe über den zeitlichen Verlauf gleichmäßig ansteigen. Wir sehen in der Abbildung 29.4, dass dies nicht der Fall ist. Wir nehmen daher eine Interaktion zwischen den Versuchsgruppen und dem zeitlichen Verlauf an.\n\nemmip(time_fct_fit, versuchsgruppe ~ time_fct, CIs = TRUE) +\n  theme_minimal() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 29.4— Interaktionsplot über den zeitlichen Verlauf für alle drei Sorten.\n\n\n\n\n\nNochmal kurz mit der ANOVA überprüft und wir sehen eine signifikante Interaktion.\n\ntime_fct_fit |&gt; anova() |&gt; model_parameters()\n\nParameter               | Sum_Squares |  df | Mean_Square |      F |      p\n---------------------------------------------------------------------------\nversuchsgruppe          |     3296.84 |   2 |     1648.42 | 169.67 | &lt; .001\ntime_fct                |     4184.15 |  13 |      321.86 |  33.13 | &lt; .001\nversuchsgruppe:time_fct |     1628.12 |  26 |       62.62 |   6.45 | &lt; .001\nResiduals               |     1981.94 | 204 |        9.72 |        |       \n\nAnova Table (Type 1 tests)\n\n\nWir haben eben einmal die Zeit als Faktor mit ins Modell genommen, da wir so für jeden Zeitpunkt einen Mittelwert schätzen können. Wenn wir eine einfaktorielle ANCOVA rechnen, dann geht die Zeit als numerische Kovariate (\\(c_1\\)) linear in das Modell ein. Wir haben also von jedem Zeitpunkt zum nächsten den gleichen Anstieg. Wir modellieren ja auch einen linearen Zusammenhang. Hier einmal das Modell für die ANCOVA.\n\\[\nlength \\sim \\overbrace{versuchsgruppe}^{f_1} + \\underbrace{time\\_num}_{c_1} + \\overbrace{versuchsgruppe:time\\_num}^{f_1:c_1}\n\\]\nWir fitten wieder das Modell in R mit der Funktion lm().\n\ntime_num_fit &lt;- lm(length ~ versuchsgruppe + time_num + versuchsgruppe:time_num, gurke_time_len_tbl)\n\nAuch hier einmal die Überprüfung auf eine Interaktion mit der ANCOVA. Wir sehen, dass wir eine signifikante Interaktion zwischen den Versuchsgruppen und dem zeitlichen verlauf vorliegen haben.\n\ntime_num_fit |&gt; anova() |&gt; model_parameters()\n\nParameter               | Sum_Squares |  df | Mean_Square |      F |      p\n---------------------------------------------------------------------------\nversuchsgruppe          |     3296.84 |   2 |     1648.42 | 155.05 | &lt; .001\ntime_num                |     3816.42 |   1 |     3816.42 | 358.98 | &lt; .001\nversuchsgruppe:time_num |     1426.30 |   2 |      713.15 |  67.08 | &lt; .001\nResiduals               |     2551.50 | 240 |       10.63 |        |       \n\nAnova Table (Type 1 tests)\n\n\nIn der Abbildung 29.5 sehen wir dann nochmal das Modell im Interaktionsplot. In beiden Abbildungen sehe wir den linearen Zusammenhang. Die Interaktion drückt sich in der unterschiedlichen Steigung der Versuchsgruppen über den zeitlichen Verlauf aus. Wir können durch die Option at = entscheiden für welche Zeitpunkte wir uns die 95% Konfidenzintervalle anzeigen lassen wollen, wie in der linken Abbildung exemplarisch für die Zeitpunkte 1 Woche, 7 Wochen und 14 Wochen. Oder aber wir lassen uns alle durch die Option cov.reduce = FALSE anzeigen, wie in der rechten Abbildung gezeigt.\n\nemmip(time_num_fit, versuchsgruppe ~ time_num, CIs = TRUE, \n      at = list(time_num = c(1, 7, 14))) +\n  theme_minimal() +\n  scale_x_continuous(breaks = 1:14) +\n  scale_color_okabeito()\n\nemmip(time_num_fit, versuchsgruppe ~ time_num, CIs = TRUE, \n      cov.reduce = FALSE) +\n  theme_minimal() +\n  scale_x_continuous(breaks = 1:14) +\n  scale_color_okabeito()\n\n\n\n\n\n\n\n\n\n\n\n(a) An drei Zeitpunkten.\n\n\n\n\n\n\n\n\n\n\n\n(b) Über alle Zeitpunkte.\n\n\n\n\n\n\n\nAbbildung 29.5— Interaktionsplot über den zeitlichen Verlauf für alle drei Sorten.\n\n\n\n\nJetzt kommt eigentlich der spannende Teil, wir wollen jetzt über den zeitlichen Verlauf einen statistischen Test rechnen, ob wir einen Trend in den verschiedenen Versuchsgruppen haben. Wir rechnen also einen Trendtest über die Kovariate \\(c_1\\) für die drei Gruppen getrennt. Das können wir mit der Funktion emtrends() durchführen. Hier musst du angeben welche deine Kovariate var ist. In unserem Fall ist die Kovariate natürlich time_num.\n\nemtrends(time_num_fit, ~ versuchsgruppe, var = \"time_num\", infer = TRUE)\n\n versuchsgruppe time_num.trend     SE  df lower.CL upper.CL t.ratio p.value\n Proloog                 1.857 0.0923 240    1.675    2.039  20.116  &lt;.0001\n Quarto                  0.465 0.0883 240    0.291    0.639   5.267  &lt;.0001\n Katrina                 0.699 0.0897 240    0.522    0.876   7.794  &lt;.0001\n\nConfidence level used: 0.95 \n\n\nDie Spalte time_num.trend zeigt uns jetzt den linearen Anstieg über die Zeit für die jeweilige Versuchsgruppe. Das heißt jede Woche wächst die Sorte Proloog um \\(1.857cm\\) an Länge. In der gleichen Art können wir auch die anderen Werte in der Spalte interpretieren. Jetzt stellt sich natürlich die Frage, ob dieses Längenwachstum untereinander unterschiedlich ist. Dafür könne wir dann leicht den Code abändern und setzen das Wort pairwise vor die Tilde. Dann testet die Funktion auch alle paarweisen Vergleiche. Wir nutzen jetzt noch die Funktion model_parameters() um eine schönere Ausgabe zu erhalten.\n\nemtrends(time_num_fit, pairwise ~ versuchsgruppe, var = \"time_num\", infer = TRUE) |&gt; \n  model_parameters()\n\n# emtrends\n\nParameter | Coefficient |   SE |       95% CI | t(240) |      p\n---------------------------------------------------------------\nProloog   |        1.86 | 0.09 | [1.68, 2.04] |  20.12 | &lt; .001\nQuarto    |        0.46 | 0.09 | [0.29, 0.64] |   5.27 | &lt; .001\nKatrina   |        0.70 | 0.09 | [0.52, 0.88] |   7.79 | &lt; .001\n\n# Contrasts\n\nParameter         | Coefficient |   SE |        95% CI | t(240) |      p\n------------------------------------------------------------------------\nProloog - Quarto  |        1.39 | 0.13 | [ 1.14, 1.64] |  10.90 | &lt; .001\nProloog - Katrina |        1.16 | 0.13 | [ 0.90, 1.41] |   9.00 | &lt; .001\nQuarto - Katrina  |       -0.23 | 0.13 | [-0.48, 0.01] |  -1.86 | 0.153 \n\n\nWir sehen wieder erst die Trends, die kennen wir schon. Dann sehen wir die Kontraste oder auch paarweisen Vergleiche. Das kannst du schnelle nachrechnen, der Unterschied von Proloog zu Quarto ist \\(1.48 - 0.45 = 1.03\\). Damit können wir dann auch über den gesamten zeitlichen Verlauf testen, ob ein Unterschied zwischen den Sorten vorliegt.\nWir sind jetzt schon sehr weit gekommen, aber wir könnten auch einen nicht-linearen Zusammenhang zwsichen der Zeit und dem Längenwachstum von Gurken annehmen. Das würde auch biologisch etwas mehr Sinn ergeben. Deshalb modellieren wir den Einfluss der Zeit time_num durch einen Exponenten hoch drei. Daher schreiben wir mathematisch \\((time\\_num)^3\\).\n\\[\nlength \\sim \\overbrace{versuchsgruppe}^{f_1} + \\underbrace{(time\\_num)^3}_{c_1} + \\overbrace{versuchsgruppe:(time\\_num)^3}^{f_1:c_1}\n\\]\nDen Exponenten schreiben wir dann entsprechend in R mit poly(time_num, 3) in die Formel. Die Funktion poly() nutzen wir innerhalb von Formelaufrufen um einen Exponenten einzufügen.\n\ntime_num_poly_fit &lt;- lm(length ~ versuchsgruppe * poly(time_num, 3), gurke_time_len_tbl)\n\nIn der Abbildung 29.6 sehen wir dann nochmal die Anpassung des Modells und wir sehen, dass unser Modell besser zu den Daten passt. Das sieht schon sehr viel sauberer aus, als der brutale lineare Zusammenhang, den wir vorher hatten. Du musst hier etwas mit den Exponenten spielen und ausprobieren, welcher da am besten passt.\n\nemmip(time_num_poly_fit, versuchsgruppe ~ time_num, CIs = TRUE, \n      cov.reduce = FALSE) +\n  theme_minimal() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 29.6— Interaktionsplot über den zeitlichen Verlauf für alle drei Sorten mit einer kubischen Anpassung der Regression.\n\n\n\n\n\nJetzt wollen wir die Analyse nochmal auf die Spitze treiben. Wir schauen uns jetzt an den Zeitpunkten 1 Woche, 7 Wochen und 14 Wochen den Unterschied zwischen den Sorten einmal an. Das machen wir indem wir zu der Funktion emmeans() die Optione at = list(time_num = c(1, 7, 14)) ergänzen. Am Ende lassen wir uns noch das compact letter display wiedergeben.\n\nemmeans(time_num_poly_fit, pairwise ~ versuchsgruppe | time_num, \n        at = list(time_num = c(1, 7, 14))) |&gt; \n  cld(Letters = letters) \n\ntime_num =  1:\n versuchsgruppe emmean    SE  df lower.CL upper.CL .group\n Quarto           2.20 1.008 234    0.209     4.18  a    \n Katrina          3.07 1.009 234    1.077     5.05  a    \n Proloog          5.10 1.011 234    3.106     7.09  a    \n\ntime_num =  7:\n versuchsgruppe emmean    SE  df lower.CL upper.CL .group\n Quarto           3.90 0.492 234    2.932     4.87  a    \n Katrina          5.32 0.492 234    4.348     6.29  a    \n Proloog         10.40 0.496 234    9.421    11.38   b   \n\ntime_num = 14:\n versuchsgruppe emmean    SE  df lower.CL upper.CL .group\n Quarto           8.82 1.008 234    6.832    10.80  a    \n Katrina         13.15 1.073 234   11.033    15.26   b   \n Proloog         30.86 1.097 234   28.698    33.02    c  \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 3 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWas erkennen wir? Wir sehen, dass sich in der ersten Woche die Sorten noch nicht voneinander unterscheiden. Erst in der Woche sieben sehen wir einen Unterschied von Proloog zu dem Rest der Sorten. In der letzten Woche unterscheiden sich dann alle Sorten voneinander. Wie stark, kannst du aus der Spalte emmean entnehmen, dort steht der Mittelwert für die jeweilige Sorte zu dem jeweiligen Zeitpunkt.\nAbschließend wollen wir nochmal schauen, wie sich der Trend in den verschiedenen Modellierungen der Exponenten zeigen würde. Wir müssen dafür bei der Funktion emtrends() angeben bis zu welchen maximalen Exponenten, bei uns hoch Drei, die Funktion rechnen soll. Deshalb setzen wir max.degree = 3. Dann noch pairwise vor die Tilde gesetzt, damit wir dann auch die paarweisen Vergleiche für die Sorten und Verläufe angezeigt kriegen. Achtung, jetzt kommt eine lange Ausgabe.\n\nemtrends(time_num_poly_fit, pairwise ~ versuchsgruppe, var = \"time_num\", infer = TRUE,\n         max.degree = 3) \n\n$emtrends\ndegree = linear:\n versuchsgruppe time_num.trend      SE  df lower.CL upper.CL t.ratio p.value\n Proloog               1.64460 0.21158 234  1.22775   2.0615   7.773  &lt;.0001\n Quarto                0.35912 0.20333 234 -0.04148   0.7597   1.766  0.0787\n Katrina               0.54881 0.20467 234  0.14559   0.9520   2.682  0.0079\n\ndegree = quadratic:\n versuchsgruppe time_num.trend      SE  df lower.CL upper.CL t.ratio p.value\n Proloog               0.15795 0.02308 234  0.11249   0.2034   6.845  &lt;.0001\n Quarto                0.03267 0.02245 234 -0.01156   0.0769   1.455  0.1469\n Katrina               0.05790 0.02285 234  0.01289   0.1029   2.534  0.0119\n\ndegree = cubic:\n versuchsgruppe time_num.trend      SE  df lower.CL upper.CL t.ratio p.value\n Proloog               0.00714 0.00671 234 -0.00608   0.0204   1.064  0.2883\n Quarto                0.00338 0.00644 234 -0.00931   0.0161   0.525  0.5999\n Katrina               0.00505 0.00657 234 -0.00789   0.0180   0.770  0.4424\n\nConfidence level used: 0.95 \n\n$contrasts\ndegree = linear:\n contrast          estimate      SE  df lower.CL upper.CL t.ratio p.value\n Proloog - Quarto   1.28548 0.29345 234   0.5933   1.9776   4.381  0.0001\n Proloog - Katrina  1.09579 0.29437 234   0.4014   1.7901   3.722  0.0007\n Quarto - Katrina  -0.18969 0.28850 234  -0.8702   0.4908  -0.658  0.7883\n\ndegree = quadratic:\n contrast          estimate      SE  df lower.CL upper.CL t.ratio p.value\n Proloog - Quarto   0.12527 0.03220 234   0.0493   0.2012   3.891  0.0004\n Proloog - Katrina  0.10005 0.03247 234   0.0235   0.1766   3.081  0.0065\n Quarto - Katrina  -0.02522 0.03203 234  -0.1008   0.0503  -0.787  0.7111\n\ndegree = cubic:\n contrast          estimate      SE  df lower.CL upper.CL t.ratio p.value\n Proloog - Quarto   0.00375 0.00930 234  -0.0182   0.0257   0.404  0.9141\n Proloog - Katrina  0.00208 0.00939 234  -0.0201   0.0242   0.222  0.9732\n Quarto - Katrina  -0.00167 0.00920 234  -0.0234   0.0200  -0.182  0.9820\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nWas sehen wir in diesem Fall? In der linearen Modellierung der Zeit haben wir sehr viele signifikante Ergebnisse. Leider entspricht der lineare verlauf über die Zeit nicht so den beobachteten Daten. Bei der kubischen Modellierung, also hoch Drei, haben wir dann in der Abbildung eine bessere Modellierung. Die Effekte reichen dann aber nicht aus um über den gesamten zeitlichen Verlauf, Betonung liegt auf dem gesamten zeitlichen Verlauf, einen Unterschied zeigen zu können.\nDaher müsste man hier einmal überlegen, ob man nicht die frühen Wochen aus den Daten entfernt. Hier sind die Gurlen noch sehr ähnlich, so dass wir hier eigentlich auch keinen Unterschied erwarten. Sonst könntest du nochmal mit einer \\(\\log\\)-Transformation der Länge spielen, dann verlierst du zwar die direkte biologische Interpretierbarkeit der Effektschätzer, aber dafür könnte der Verlauf über die Zeit besser aufgesplittet werden. Das Problem sind ja hier sehr kleine Werte zu Anfang und sehr große Werte zum Ende.\n\n\n\nAbbildung 29.1— Scatterplot der Daten zur einfaktoriellen ANCOVA.\nAbbildung 29.2— Scatterplot der Daten zur einfaktoriellen ANCOVA aufgetelt nach dem Geschlecht der Flöhe.\nAbbildung 29.3— Scatterplot des Längenwachstums der drei Gurkensorten über vierzehn Wochen. Die gestrichtelten Linien stellen den Median und die durchgezogene Line den Mittelwert der Gruppen dar.\nAbbildung 29.4— Interaktionsplot über den zeitlichen Verlauf für alle drei Sorten.\nAbbildung 29.5 (a)— An drei Zeitpunkten.\nAbbildung 29.5 (b)— Über alle Zeitpunkte.\nAbbildung 29.6— Interaktionsplot über den zeitlichen Verlauf für alle drei Sorten mit einer kubischen Anpassung der Regression.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Die ANCOVA</span>"
    ]
  },
  {
    "objectID": "stat-tests-utest.html",
    "href": "stat-tests-utest.html",
    "title": "30  Der Wilcoxon-Mann-Whitney-Test",
    "section": "",
    "text": "30.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, coin)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Der Wilcoxon-Mann-Whitney-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-utest.html#daten-für-den-wilcoxon-mann-whitney-test",
    "href": "stat-tests-utest.html#daten-für-den-wilcoxon-mann-whitney-test",
    "title": "30  Der Wilcoxon-Mann-Whitney-Test",
    "section": "30.2 Daten für den Wilcoxon-Mann-Whitney-Test",
    "text": "30.2 Daten für den Wilcoxon-Mann-Whitney-Test\nBindungen (eng. ties) in den Daten sind ein Problem und müssen beachtet werden. Das heißt, wenn es gleiche Zahlen in den Gruppen gibt.\nFür die Veranschaulichung des Wilcoxon-Mann-Whitney-Test nehmen wir ein simples Beispiel. Wir nehmen ein nicht normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Wir nehmen hierbei an, dass die Sprunglänge jetzt mal nicht normalverteilt ist. Später sind es Boniturnoten, die definitiv nicht normalverteilt sind. Aber mit der Sprunglänge ist das Beispiel einfacher nachzuvollziehen. Darüber hinaus haben wir so keine Bindungen in den Daten. Bindungen (eng. ties) heißt, dass wir die numerisch gleichen Zahlen in beiden Gruppen haben.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln. Wir nehmen in diesem Beispiel an, dass die Variable jump_length nicht normalverteilt ist.\n\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\") |&gt; \n  select(animal, jump_length, grade) |&gt; \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt data_tbl mit dem Datensatz in Tabelle 30.1 nochmal dargestellt.\n\n\n\n\nTabelle 30.1— Selektierter Datensatz für den Wilcoxon-Mann-Whitney-Test mit einer nicht-normalverteilten Variable jump_length und einem Faktor animal mit zwei Leveln.\n\n\n\n\n\n\nanimal\njump_length\ngrade\n\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n5.6\n8\n\n\ndog\n9.1\n7\n\n\ndog\n8.2\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt data_tbl folgendes Modell für später:\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wie immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in dem Wilcoxon-Mann-Whitney-Test aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Der Wilcoxon-Mann-Whitney-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-utest.html#hypothesen-für-den-wilcoxon-mann-whitney-test",
    "href": "stat-tests-utest.html#hypothesen-für-den-wilcoxon-mann-whitney-test",
    "title": "30  Der Wilcoxon-Mann-Whitney-Test",
    "section": "30.3 Hypothesen für den Wilcoxon-Mann-Whitney-Test",
    "text": "30.3 Hypothesen für den Wilcoxon-Mann-Whitney-Test\nDer Wilcoxon-Mann-Whitney-Test betrachtet die Mediane und Ränge um einen Unterschied nachzuweisen. Daher haben wir die Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mediane der beiden Levels des Faktors animal gleich sind. Wir vergleichen im Wilcoxon-Mann-Whitney-Test nur zwei Gruppen.\n\\[\nH_0: \\; \\widetilde{y}_{cat} = \\widetilde{y}_{dog}\n\\]\nDie Alternative lautet, dass sich die beiden Gruppen im Median unterscheiden. Wir können uns über die Boxplots oder aber die berechneten Mediane dann den Unterschied bewerten.\n\\[\nH_A: \\; \\widetilde{y}_{cat} \\ne \\widetilde{y}_{dog}\n\\]\nWir schauen uns jetzt einmal den Wilcoxon-Mann-Whitney-Test theoretisch an bevor wir uns mit der Anwendung des Wilcoxon-Mann-Whitney-Test in R beschäftigen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Der Wilcoxon-Mann-Whitney-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-utest.html#wilcoxon-mann-whitney-test-theoretisch",
    "href": "stat-tests-utest.html#wilcoxon-mann-whitney-test-theoretisch",
    "title": "30  Der Wilcoxon-Mann-Whitney-Test",
    "section": "30.4 Wilcoxon-Mann-Whitney-Test theoretisch",
    "text": "30.4 Wilcoxon-Mann-Whitney-Test theoretisch\nDer Wilcoxon-Mann-Whitney-Test berechnet die U Teststatistik auf den Rängend der Daten. Es gibt genau soviele Ränge wie es Beobachtungen im Datensatz gibt. Wir haben \\(n = 14\\) Beobachtungen in unseren Daten zu der Sprungweite in [cm] von den Hunde- und Katzenflöhen. Somit müssen wir auch vierzehn Ränge vergeben.\nDie Tabelle 30.2 zeigt das Vorgehen der Rangvergabe. Wir sortieren als erstes das \\(y\\) aufsteigend. In unserem Fall ist das \\(y\\) die Sprunglänge. Dann vergeben wir die Ränge jweiles zugehörig zu der Position der Sprunglänge und der Tierart. Abschließend addieren wir die Rangsummmen für cat und dog zu den Rangsummen \\(R_{cat}\\) und \\(R_{dog}\\).\n\n\n\nTabelle 30.2— Datentablle absteigend sortiert nach der Sprunglänge in [cm]. Die Level cat und dog haben jeweils die entsprechenden Ränge zugeordnet bekommen und die Rangsummen wurden berechnet\n\n\n\n\n\nRank\nanimal\njump_length\nRänge “cat”\nRänge “dog”\n\n\n\n\n1\ncat\n2.2\n1\n\n\n\n2\ncat\n3.2\n2\n\n\n\n3\ncat\n4.1\n3\n\n\n\n4\ncat\n4.3\n4\n\n\n\n5\ncat\n5.4\n5\n\n\n\n6\ndog\n5.6\n\n6\n\n\n7\ndog\n5.7\n\n7\n\n\n8\ncat\n6.1\n8\n\n\n\n9\ndog\n7.6\n\n9\n\n\n10\ncat\n7.9\n10\n\n\n\n11\ndog\n8.2\n\n11\n\n\n12\ndog\n8.9\n\n12\n\n\n13\ndog\n9.1\n\n13\n\n\n14\ndog\n11.8\n\n14\n\n\n\n\nRangsummen\n\\(R_{cat} = 33\\)\n\\(R_{dog} = 72\\)\n\n\n\n\nGruppengröße\n7\n7\n\n\n\n\n\n\nDie Formel für die U Statistik sieht ein wenig wild aus, aber wir können eigentlich relativ einfach alle Zahlen einsetzen. Dann musst du dich etwas konzentrieren bei der Rechnung.\n\\[\nU_{calc} = n_1n_2 + \\cfrac{n_1(n_1+1)}{2}-R_1\n\\]\nmit\n\n\\(R_1\\) der größeren der beiden Rangsummen,\n\\(n_1\\) die Fallzahl der größeren der beiden Rangsummen\n\\(n_2\\) die Fallzahl der kleineren der beiden Rangsummen\n\nWir setzen nun die Zahlen ein. Da wir ein balanciertes Design vorliegen haben sind die Fallzahlen \\(n_1 = n_2 = 7\\) gleich. Wir müssen nur schauen, dass wir mit \\(R_1\\) die passende Rangsumme wählen. In unserem Fall ist \\(R_1 = R_{dog} = 72\\).\n\\[\nU_{calc} = 7 \\cdot 7 + \\cfrac{7(7+1)}{2}-72 = 5\n\\]\nDer kritische Wert für die U Statistik ist \\(U_{\\alpha = 5\\%} = 8\\) für \\(n_1 = 7\\) und \\(n_2 = 7\\). Bei der Entscheidung mit der berechneten Teststatistik \\(U_{calc}\\) gilt, wenn \\(U_{calc} \\leq U_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. Da in unserem Fall das \\(U_{calc}\\) mit \\(5\\) kleiner ist als das \\(U_{\\alpha = 5\\%} = 8\\) können wir die Nullhypothese ablehnen. Wir haben ein signifkianten Unterschied in den Medianen zwischen den beiden Tierarten im Bezug auf die Sprungweite in [cm] von Flöhen.\nBei grosser Stichprobe, wenn \\(n_1 + n_2 &gt; 30\\) ist, können wir die U Statistik auch standariseren und damit in den z-Wert transformieren.\n\\[\nz_{calc} = \\cfrac{U_{calc} - \\bar{U}}{s_U} = \\cfrac{U_{calc} - \\cfrac{n_1 \\cdot n_2}{2}}{\\sqrt{\\cfrac{n_1 \\cdot n_2 (n_1 + n_2 +1)}{12}}}\n\\]\nmit\n\n\\(\\bar{U}\\) dem Mittelwert der U-Verteilung ohne Unterschied zwischen den Gruppen\n\\(s_U\\) Standardfehler des U-Wertes\n\\(n_1\\) Stichprobengrösse der Gruppe mit der grösseren Rangsumme\n\\(n_2\\) Stichprobengrösse der Gruppe mit der kleineren Rangsumme\n\nWir setzen dafür ebenfalls die berechnete U Statistik ein und müssen dann wieder konzentriert rechnen.\n\\[\nz_{calc} = \\cfrac{5 - \\cfrac{7 \\cdot 7}{2}}{\\sqrt{\\cfrac{7 \\cdot 7 (7 + 7 +1)}{12}}} = \\cfrac{-19.5}{7.83} = |-2.46|\n\\]\nDer kritische Wert für die z-Statistik ist \\(z_{\\alpha = 5\\%} = 1.96\\). Bei der Entscheidung mit der berechneten Teststatistik \\(z_{calc}\\) gilt, wenn \\(z_{calc} \\geq z_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. Wir haben eine berechnete z Statistik von \\(z_{calc} = 2.46\\). Damit ist \\(z_{calc}\\) größer als \\(z_{\\alpha = 5\\%} = 1.96\\) und wir können die Nullhypothese ablehnen. Wir haben einen signifkanten Unterschied zwischen den Medianen der beiden Floharten im Bezug auf die Sprunglänge in [cm].\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(U_{\\boldsymbol{calc}}\\) oder der Teststatistik \\(z_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(U_{calc}\\) gilt, wenn \\(U_{calc} \\leq U_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nBei der Entscheidung mit der berechneten Teststatistik \\(z_{calc}\\) gilt, wenn \\(z_{calc} \\geq z_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Der Wilcoxon-Mann-Whitney-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-utest.html#wilcoxon-mann-whitney-test-in-r",
    "href": "stat-tests-utest.html#wilcoxon-mann-whitney-test-in-r",
    "title": "30  Der Wilcoxon-Mann-Whitney-Test",
    "section": "30.5 Wilcoxon-Mann-Whitney-Test in R",
    "text": "30.5 Wilcoxon-Mann-Whitney-Test in R\nDie Nutzung des Wilcoxon-Mann-Whitney-Test in R ist relativ einfach mit der Funktion wilxoc.test(). Wir müssen zum einen entscheiden, ob Bindungen in den Daten vorliegen. Sollte Bindungen vorliegen, warnt uns R und wir nutzen dann die Funktion wilcox_test() aus dem R Paket {coin}.\n\n30.5.1 Ohne Bindungen\nOhne Bindungen können wir die Funktion wilxoc.test() nutzen. Die Funktion benötigt das Modell in formula Syntax in der Form jump_length ~ animal. Wir geben noch an, dass wir die 95% Konfidenzintervalle wiedergegeben haben wollen.\n\nwilcox.test(jump_length ~ animal, data = data_tbl, \n            conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  jump_length by animal\nW = 44, p-value = 0.01107\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 1.0 5.9\nsample estimates:\ndifference in location \n                   3.5 \n\n\nWir sehen das der Wilcoxon-Mann-Whitney-Test ein signifikantes Ergebnis liefert, da der \\(p\\)-Wert mit 0.011 kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Die Nullhypothese kann daher abgelehnt werden. Wir haben einen medianen Unterschied in den Sprungweiten von 3.5 cm [1.0; 5.9] zwischen Hunde- und Katzenflöhen.\n\n\n30.5.2 Mit Bindungen\nMit Bindungen können wir die Funktion wilxoc_test() aus dem R Paket {coin} nutzen. Wir nutzen hier als \\(y\\) die Boniturnoten grade der Hunde und Katzen. Die Funktion benötigt das Modell in formula Syntax in der Form grade ~ animal. Wir geben noch an, dass wir die 95% Konfidenzintervalle wiedergegeben haben wollen. Wenn du die Funktion wilcox.test() nutzen würdest, würde dir R eine Warnung ausgeben: Warning: cannot compute exact p-value with ties. Du wüsstest dann, dass du die Funktion wechseln musst.\n\nwilcox_test(grade ~ animal, data = data_tbl, \n            conf.int = TRUE) \n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  grade by animal (dog, cat)\nZ = 2.4973, p-value = 0.01251\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n 1 3\nsample estimates:\ndifference in location \n                     2 \n\n\nWir sehen das der Wilcoxon-Mann-Whitney-Test ein signifikantes Ergebnis liefert, da der \\(p\\)-Wert mit 0.015 kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Die Nullhypothese kann daher abgelehnt werden. Wir haben einen medianen Unterschied in den Boniturnoten von 2 [0; 3] zwischen Hunde und Katzen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Der Wilcoxon-Mann-Whitney-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-utest.html#minimale-fallzahl-je-gruppe",
    "href": "stat-tests-utest.html#minimale-fallzahl-je-gruppe",
    "title": "30  Der Wilcoxon-Mann-Whitney-Test",
    "section": "30.6 Minimale Fallzahl je Gruppe",
    "text": "30.6 Minimale Fallzahl je Gruppe\nHäufig wird auch der Wilcoxon-Mann-Whitney-Test eingesetzt, wenn wenig Beobachtungen vorliegen. Es gibt aber eine untere Grenze der Signifikanz. Das heißt unter einer Fallzahl von \\(n_1 = 3\\) und \\(n_2 = 3\\) wird ein Wilcoxon-Mann-Whitney-Test nicht mehr signifikant. Egal wie groß der Unterschied ist, ein Wilcoxon-Mann-Whitney-Test wird dann die Nulhypothese nicht ablehnen können. Schauen wir das Datenbeispiel in Tabelle 30.3 einmal an.\n\n\n\n\nTabelle 30.3— Kleiner Datensatz mit jeweils nur drei Beobachtungen pro Gruppe.\n\n\n\n\n\n\nanimal\njump_length\n\n\n\n\ndog\n1.2\n\n\ndog\n5.6\n\n\ndog\n3.2\n\n\ncat\n100.3\n\n\ncat\n111.2\n\n\ncat\n98.5\n\n\n\n\n\n\n\n\nWir sehen jeweils drei Beobachtunge für Hunde- und Katzensprungweiten. Der Unterschied ist numerisch riesig. Wir können uns den Unterschied nochmal in Abbildung 30.1 visualisieren.\n\n\n\n\n\n\n\n\nAbbildung 30.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\n\nWir sehen, der Unterschied ist riesig. Der Wilcoxon-Mann-Whitney-Test findet jedoch nur einen p-Wert von 0.1 und kann damit die Nullhypothese nicht ablehnen. Wir haben keinen signifkanten Unterschied.\n\nwilcox.test(jump_length ~ animal, data = small_tbl)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  jump_length by animal\nW = 9, p-value = 0.1\nalternative hypothesis: true location shift is not equal to 0\n\n\nWir sehen hier ein schönes Beispiel für die Begrenztheit von Algorithmen und mathematischen Formeln. Es gibt einen Unterschied, aber der Wilcoxon-Mann-Whitney-Test ist technisch nicht in der Lage einen Unterschied nochzuweisen. Daher solltest du immer versuchen die Ergebnisse eines Testes mit einer Abbildung zu überprüfen.\n\n\n\nAbbildung 30.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Der Wilcoxon-Mann-Whitney-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-kruskal.html",
    "href": "stat-tests-kruskal.html",
    "title": "31  Der Kruskal-Wallis-Test",
    "section": "",
    "text": "31.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, rstatix)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Der Kruskal-Wallis-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-kruskal.html#daten-für-den-kruskal-wallis-test",
    "href": "stat-tests-kruskal.html#daten-für-den-kruskal-wallis-test",
    "title": "31  Der Kruskal-Wallis-Test",
    "section": "31.2 Daten für den Kruskal-Wallis-Test",
    "text": "31.2 Daten für den Kruskal-Wallis-Test\nBindungen (eng. ties) in den Daten sind ein Problem und müssen beachtet werden. Das heißt, wenn es gleiche Zahlen in den Gruppen gibt.\nWir wollen uns nun erstmal den einfachsten Fall anschauen mit einem simplen Datensatz. Wir nehmen ein nicht-normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Hätten wir nur zwei Level, dann können wir auch einen Wilcoxon-Mann-Whitney-Test rechnen können.\nWir nehmen in diesem Abschnitt an, dass die Sprunglänge jetzt mal nicht normalverteilt ist. Später sind es Boniturnoten, die definitiv nicht normalverteilt sind. Aber mit der Sprunglänge ist das Beispiel einfacher nachzuvollziehen. Darüber hinaus haben wir so keine Bindungen in den Daten. Bindungen (eng. ties) heißt, dass wir die numerisch gleichen Zahlen in beiden Gruppen haben.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln. Wir nehmen in diesem Beispiel an, dass die Variable jump_length nicht normalverteilt ist.\n\nfac1_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox.csv\") |&gt;\n  select(animal, jump_length, grade) |&gt; \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt fac1_tbl mit dem Datensatz in Tabelle 31.1 nochmal dargestellt.\n\n\n\n\nTabelle 31.1— Selektierter Datensatz für den Kruskal-Wallis-Test mit einer nicht-normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln.\n\n\n\n\n\n\nanimal\njump_length\ngrade\n\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n5.6\n8\n\n\ndog\n9.1\n7\n\n\ndog\n8.2\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\nfox\n7.7\n5\n\n\nfox\n8.1\n4\n\n\nfox\n9.1\n4\n\n\nfox\n9.7\n5\n\n\nfox\n10.6\n4\n\n\nfox\n8.6\n4\n\n\nfox\n10.3\n3\n\n\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt fac1_tbl folgendes Modell für später:\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wie immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in dem Kruskal-Wallis-Test aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Der Kruskal-Wallis-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-kruskal.html#hypothesen-für-den-kruskal-wallis-test",
    "href": "stat-tests-kruskal.html#hypothesen-für-den-kruskal-wallis-test",
    "title": "31  Der Kruskal-Wallis-Test",
    "section": "31.3 Hypothesen für den Kruskal-Wallis-Test",
    "text": "31.3 Hypothesen für den Kruskal-Wallis-Test\nDer Kruskal-Wallis-Test betrachtet die Mediane und Ränge um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mediane jedes Levels des Faktors animal gleich sind.\n\\[\nH_0: \\; \\widetilde{y}_{cat} = \\widetilde{y}_{dog} = \\widetilde{y}_{fox}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Medianen unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mediane unterschieden oder eben nur ein Paar. Wenn ein Kruskal-Wallis-Test die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mediane sich unterscheiden.\n\\[\n\\begin{aligned}\nH_A: &\\; \\widetilde{y}_{cat} \\ne \\widetilde{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{cat} \\ne \\widetilde{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{dog} \\ne \\widetilde{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nWir schauen uns jetzt einmal den Kruskal-Wallis-Test theoretisch an bevor wir uns mit der Anwendung des Kruskal-Wallis-Test in R beschäftigen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Der Kruskal-Wallis-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-kruskal.html#kruskal-wallis-test-theoretisch",
    "href": "stat-tests-kruskal.html#kruskal-wallis-test-theoretisch",
    "title": "31  Der Kruskal-Wallis-Test",
    "section": "31.4 Kruskal-Wallis-Test theoretisch",
    "text": "31.4 Kruskal-Wallis-Test theoretisch\nDer Kruskal-Wallis-Test berechnet die H Teststatistik auf den Rängend der Daten. Es gibt genau soviele Ränge wie es Beobachtungen im Datensatz gibt. Wir haben \\(n = 21\\) Beobachtungen in unseren Daten zu der Sprungweite in [cm] von den Hunde-, Katzen- und Fuchsflöhen. Somit müssen wir auch einundzwanzig Ränge vergeben.\nDie Tabelle 31.2 zeigt das Vorgehen der Rangvergabe. Wir sortieren als erstes das \\(y\\) aufsteigend. In unserem Fall ist das \\(y\\) die Sprunglänge. Dann vergeben wir die Ränge jweiles zugehörig zu der Position der Sprunglänge und der Tierart. Abschließend addieren wir die Rangsummmen für cat, dog und fox zu den Rangsummen \\(R_{cat}\\), \\(R_{dog}\\) und \\(R_{fox}\\).\n\n\n\n\nTabelle 31.2— Datentablle absteigend sortiert nach der Sprunglänge in [cm]. Die Level cat, dog und fox haben jeweils die entsprechenden Ränge zugeordnet bekommen und die Rangsummen wurden berechnet\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nanimal\njump_length\nRänge “cat”\nRänge “dog”\nRänge “fox”\n\n\n\n\n1\ncat\n2.2\n1\n\n\n\n\n2\ncat\n3.2\n2\n\n\n\n\n3\ncat\n4.1\n3\n\n\n\n\n4\ncat\n4.3\n4\n\n\n\n\n5\ncat\n5.4\n5\n\n\n\n\n6\ndog\n5.6\n\n6\n\n\n\n7\ndog\n5.7\n\n7\n\n\n\n8\ncat\n6.1\n8\n\n\n\n\n9\ndog\n7.6\n\n9\n\n\n\n10\nfox\n7.7\n\n\n10\n\n\n11\ncat\n7.9\n11\n\n\n\n\n12\nfox\n8.1\n\n\n12\n\n\n13\ndog\n8.2\n\n13\n\n\n\n14\nfox\n8.6\n\n\n14\n\n\n15\ndog\n8.9\n\n15\n\n\n\n16\ndog\n9.1\n\n16\n\n\n\n17\nfox\n9.1\n\n\n17\n\n\n18\nfox\n9.7\n\n\n18\n\n\n19\nfox\n10.3\n\n\n19\n\n\n20\nfox\n10.6\n\n\n20\n\n\n21\ndog\n11.8\n\n21\n\n\n\n\n\nRangsummen\n\\(R_{cat} = 34\\)\n\\(R_{dog} = 87\\)\n\\(R_{fox} = 110\\)\n\n\n\n\nGruppengröße\n7\n7\n7\n\n\n\n\n\n\n\nDie Summe aller Ränge ist \\(1+2+3+...+21 = 231\\). Wir überprüfen nochmal die Summe der Rangsummen als Gegenprobe \\(R_{cat} + R_{dog} + R_{fox} = 231\\). Das ist identisch, wir haben keinen Fehler bei der Rangaufteilung und der Summierung gemacht.\nDie Formel für die H Statistik sieht wie die U Statistik ein wenig wild aus, aber wir können eigentlich relativ einfach alle Zahlen einsetzen. Dann musst du dich etwas konzentrieren bei der Rechnung.\n\\[\nH = \\cfrac{12}{N(N+1)}\\sum_{i=1}^k\\cfrac{R_i^2}{n_i}-3(N+1)\n\\]\nmit\n\n\\(R_i\\) der Rangsummen für jede Gruppe mit insgesamt \\(k\\) Gruppen\n\\(n_i\\) der Fallzahl in jeder Gruppe\n\\(N\\) der Gesamtzahl an Beobachtungen also die gesamte Fallzahl\n\nWir setzen nun die Zahlen ein. Da wir ein balanciertes Design vorliegen haben sind die Fallzahlen \\(n_1 = n_2 = n_3 = 7\\) gleich.\n\\[\nH_{calc} = \\cfrac{12}{21(21+1)}\\left(\\cfrac{34^2}{7}+\\cfrac{87^2}{7}+\\cfrac{110^2}{7}\\right)-3(21+1) = 11.27\n\\]\nDer kritische Wert für die H Statistik ist \\(H_{\\alpha = 5\\%} = 5.99\\). Bei der Entscheidung mit der berechneten Teststatistik \\(H_{calc}\\) gilt, wenn \\(H_{calc} \\geq U_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. Da in unserem Fall das \\(H_{calc}\\) mit \\(11.27\\) größer ist als das \\(H_{\\alpha = 5\\%} = 5.99\\) können wir die Nullhypothese ablehnen. Wir haben ein signifkianten Unterschied in den Medianen zwischen den beiden Tierarten im Bezug auf die Sprungweite in [cm] von Flöhen.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(F_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(H_{calc}\\) gilt, wenn \\(H_{calc} \\geq H_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Der Kruskal-Wallis-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-kruskal.html#kruskal-wallis-test-in-r",
    "href": "stat-tests-kruskal.html#kruskal-wallis-test-in-r",
    "title": "31  Der Kruskal-Wallis-Test",
    "section": "31.5 Kruskal-Wallis-Test in R",
    "text": "31.5 Kruskal-Wallis-Test in R\nDie Nutzung des Kruskal-Wallis-Test in R ist relativ einfach mit der Funktion kruskal.test(). Wir nutzen die formual Syntax um das Modell zu definieren und können dann schon die Funktion nutzen.\n\nkruskal.test(jump_length ~ animal, data = fac1_tbl) \n\n\n    Kruskal-Wallis rank sum test\n\ndata:  jump_length by animal\nKruskal-Wallis chi-squared = 11.197, df = 2, p-value = 0.003704\n\n\nMit einem p-Wert von \\(0.0037\\) können wir die Nullhypothese ablehnen, da der p-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir haben mindestens einen medianen Unterschied zwischen den Sprungweiten der Hunde-, Katzen- und Fuchsflöhen.\nFür die Betrachtung der Effektgröße in einem Kruskal-Wallis-Test nutzen wir das R Paket {rstatix} und die darin enthaltende Funktion kruskal_effsize(). Wir berechnen hierbei analog zu einfaktoriellen ANOVA den \\(\\eta^2\\) Wert.\n\nfac1_tbl |&gt; kruskal_effsize(jump_length ~ animal)\n\n# A tibble: 1 × 5\n  .y.             n effsize method  magnitude\n* &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;ord&gt;    \n1 jump_length    21   0.511 eta2[H] large    \n\n\nDas \\(\\eta^2\\) nimmt Werte von 0 bis 1 an und gibt, multipliziert mit 100, den Prozentsatz der Varianz der durch die \\(x\\) Variable erklärt wird. In unserem Beispiel wird 51.1% der Varianz in de Daten durch den Faktor animal erklärt.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Der Kruskal-Wallis-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-friedman.html",
    "href": "stat-tests-friedman.html",
    "title": "32  Der Friedman Test",
    "section": "",
    "text": "32.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, rstatix, coin,\n               effectsize, PMCMRplus, rcompanion)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Der Friedman Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-friedman.html#daten-für-den-friedman-test",
    "href": "stat-tests-friedman.html#daten-für-den-friedman-test",
    "title": "32  Der Friedman Test",
    "section": "32.2 Daten für den Friedman Test",
    "text": "32.2 Daten für den Friedman Test\nFür das Datenbeispiel bauen wir uns den Datensatz schnell selber zusammen. Wir wollen uns fünf Weizensorten \\(A\\) bis \\(E\\) anschauen und bonitieren, wie die Qualität der Pflanzen nach einer Dürreperiode aussieht. Daher vergeben wir einen Score auf der Likertskala von \\(1\\) bis \\(9\\), wobei \\(9\\) bedeutet, dass die Weizenpflanze vollkommen intakt ist. Wir haben unseren Versuch in vier Blöcken angelegt.\n\ngrade_tbl &lt;- tibble(block = 1:4,\n                    A = c(2,3,4,3),\n                    B = c(7,9,8,9),\n                    C = c(6,5,4,7),\n                    D = c(2,3,1,2),\n                    E = c(4,5,7,6)) |&gt;\n  gather(key = variety, value = grade, A:E) |&gt; \n  mutate(block =  as_factor(block))\n\nSchauen wir uns nochmal unseren Datensatz an. Wir haben den Datensatz jetzt im Long-Format vorliegen und können dann gleich mit dem Datensatz weiterarbeiten.\n\ngrade_tbl\n\n# A tibble: 20 × 3\n   block variety grade\n   &lt;fct&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 1     A           2\n 2 2     A           3\n 3 3     A           4\n 4 4     A           3\n 5 1     B           7\n 6 2     B           9\n 7 3     B           8\n 8 4     B           9\n 9 1     C           6\n10 2     C           5\n11 3     C           4\n12 4     C           7\n13 1     D           2\n14 2     D           3\n15 3     D           1\n16 4     D           2\n17 1     E           4\n18 2     E           5\n19 3     E           7\n20 4     E           6\n\n\nIn Abbildung 32.1 sehen wir nochmal die Visualisierung der Boniturnoten über die fünf Weizensorten und den vier Blöcken. Bei einer so kleinen Fallzahl von nur vier Blöcken pro Sorte entscheiden wir uns dann für einen Dotplot um uns die Daten einmal anzuschauen.\n\nggplot(grade_tbl, aes(variety, grade, fill = block)) +\n  theme_minimal() +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.3)) +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9))\n\n\n\n\n\n\n\nAbbildung 32.1— Dotplot des Datenbeispiels für die Bonitur von fünf Weizensorten.\n\n\n\n\n\nIm Folgenden schauen wir usn nun einmal die Hypothesen des Friedman Tests an und rechen dann den Friedman Test in R einmal durch.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Der Friedman Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-friedman.html#hypothesen-für-den-friedman-test",
    "href": "stat-tests-friedman.html#hypothesen-für-den-friedman-test",
    "title": "32  Der Friedman Test",
    "section": "32.3 Hypothesen für den Friedman Test",
    "text": "32.3 Hypothesen für den Friedman Test\nDer Friedman Test betrachtet wie auch der Kruskal-Wallis-Test die Mediane \\(\\widetilde{y}\\) und Ränge um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mediane jedes Levels des Faktors variety gleich sind.\n\\[\nH_0: \\; \\widetilde{y}_{A} = \\widetilde{y}_{B} = \\widetilde{y}_{C} = \\widetilde{y}_{D} = \\widetilde{y}_{E}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Medianen unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mediane unterschieden oder eben nur ein Paar. Wenn ein Friedman Test die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mediane sich unterscheiden. Bein unseren fünf Weizensorten kommt da eine ganze Menge an Vergleichen zusammen. In der Folge nur ein Ausschnitt aller Alternativehypothesen.\n\\[\n\\begin{aligned}\nH_A: &\\; \\widetilde{y}_{A} \\ne \\widetilde{y}_{B}\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{A} \\ne \\widetilde{y}_{C}\\\\\n\\phantom{H_A:} &\\; ...\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{D} \\ne \\widetilde{y}_{E}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\] Damit kommen uns die Hypothesen nicht so unbekannt vor. Die Hypothesenpaare sind die gleichen wie in einer ANOVA bzw. dem Kruskal-Wallis-Test.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Der Friedman Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-friedman.html#friedman-test-in-r",
    "href": "stat-tests-friedman.html#friedman-test-in-r",
    "title": "32  Der Friedman Test",
    "section": "32.4 Friedman Test in R",
    "text": "32.4 Friedman Test in R\nDer Friedman Test ist in R in verschiedenen Paketen implementiert. Wir nehmen die Standardfunktion friedman.test(). Wichtig ist wie wir in der Funktion das Modell definieren. Wir nutzen das Symbol | um die Behandlung von dem Block zu trennen. Vor dem | Symbol steht die Behandlung, hinter dem | steht der Block. Damit sieht das Modell etwas anders aus, aber im Prinzip ist die Definition des Modells einfach.\n\n\nDas Tutorial von Salvatore S. Mangiafico zum Friedman Test liefert eine sehr ausführliche Anwendung über mehrere R Pakete hinweg.\n\nfriedman.test(grade ~ variety | block, data = grade_tbl)\n\n\n    Friedman rank sum test\n\ndata:  grade and variety and block\nFriedman chi-squared = 14.684, df = 4, p-value = 0.005403\n\n\nNachdem wir die Funktion aufgerufen haben, erhalten wir auch gleich den \\(p\\)-Wert von \\(0.005\\) wieder. Da der \\(p\\)-Wert kleienr ist als das Signifikanzniveau \\(\\alpha\\) von 5% können wir die Nullhypothese ablehnen. Es gibt mindestens einen paarweisen Unterschied. Das war auch anhand des Doplots zu erwarten. Nun stellt sich noch die Frage, wie groß der Effekt ist. Das können wir mit Kendalls \\(W\\) bestimmen. Auch die Funktionalität ist in der R Paket {effectsize} implementiert. Wir müssen nur wieder das ganze Modell in die Funktion kendalls_w() stecken.\n\nkendalls_w(grade ~ variety | block, data = grade_tbl)\n\nKendall's W |       95% CI\n--------------------------\n0.92        | [0.90, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nNun kriegen wir ein Kendalls \\(W\\) von 0.92 raus. Wir immer ist die Frage nach der Interpretation. Dankenswerterweise gibt es auch die Funktion interpret_kendalls_w(), die uns mit Quelle ausgibt, wie stark der Effekt ist. Einfach nochmal für die Referenz in die Hilfeseite von der Funktion schauen.\n\ninterpret_kendalls_w(0.92)\n\n[1] \"almost perfect agreement\"\n(Rules: landis1977)\n\n\nWir haben also herausgefunden, dass wir einen starken Effekt haben und sich die Mediane der Gruppen unterscheiden.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Der Friedman Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-friedman.html#posthoc-test",
    "href": "stat-tests-friedman.html#posthoc-test",
    "title": "32  Der Friedman Test",
    "section": "32.5 Posthoc Test",
    "text": "32.5 Posthoc Test\nNachdem wir jetzt festgestellt haben, dass sich mindestens ein Gruppenunterschied zwischen den Weizensorten finden lassen muss, wollen wir noch feststellen wo dieser Unterschied liegt. Wir nutzen dafür den Siegel- und Castellan-Vergleichstests für alle Paare durch. Der Test ist mit der Funktion frdAllPairsSiegelTest() in dem R Paket {PMCMRplus} implementiert und einfach nutzbar. In dem Paket sind noch eine weitere Reihe an statistischen Test für paarweise nicht-parametrische Test enthalten. Leider sind das Paket und die Funktion schon älter, so dass wir nicht einmal die Formelschreibweise zu Verfügung haben. Wir müssen alle Spalten aus unserem Datensatz mit dem $ einzeln selektieren und den Optionen zuordnen.\n\nsiegel_test_res &lt;-  frdAllPairsSiegelTest(y = grade_tbl$grade,\n                                          groups = grade_tbl$variety,\n                                          blocks = grade_tbl$block,\n                                          p.adjust.method = \"none\")\nsiegel_test_res\n\n  A      B      C      D     \nB 0.0052 -      -      -     \nC 0.1461 0.1797 -      -     \nD 0.5762 0.0008 0.0442 -     \nE 0.1797 0.1461 0.9110 0.0573\n\n\nIch empfehle ja immer eine Adjustierung für multiple Vergleiche, aber du kannst das selber entscheiden. Wenn du für die multiplen Vergleiche adjustieren willst, dann nutze gerne die Option p.adjust.method = \"bonferroni\" oder eben statt bonferroni die Adjustierungsmethode fdr.\nDamit wir auch das compact letter display aus unseren \\(p\\)-Werte berechnen können, müssen wir unser Ergebnisobjekt nochmal in eine Tabelle umwandeln. Dafür gibt es dann auch eine passende Funktion mit PMCMRTable. Wir sehen, es ist alles etwas altbacken.\n\nsiegel_test_tab &lt;- PMCMRTable(siegel_test_res )\n\nsiegel_test_tab\n\n   Comparison  p.value\n1   B - A = 0  0.00519\n2   C - A = 0    0.146\n3   D - A = 0    0.576\n4   E - A = 0     0.18\n5   C - B = 0     0.18\n6   D - B = 0 0.000796\n7   E - B = 0    0.146\n8   D - C = 0   0.0442\n9   E - C = 0    0.911\n10  E - D = 0   0.0573\n\n\nNachdem wir die Tabellenschreibweise der \\(p\\)-Werte vorliegen haben, können wir dann das compact letter display uns über die Funktion cldList() anzeigen lassen. Leider gibt es keine Möglichkeit die 95% Konfidenzintervalle zu erhalten. Wir auch beim Kruskal-Wallis-Test erhalten wir bei dem Friedman Test keine 95% Konfidenzintervalle und müssen im Zweifel mit dieser Einschränkung leben.\n\ncldList(p.value ~ Comparison, data = siegel_test_tab)\n\n  Group Letter MonoLetter\n1     B      a        a  \n2     C     ab        ab \n3     D      c          c\n4     E    abc        abc\n5     A     bc         bc\n\n\nAm compact letter display sehen wir im Prinzip das gleiche Muster wie in den \\(p\\)-Werten. Nur nochmal etwas anders dargestellt und mit dem Fokus auf die nicht Unterschiede. Wenn du mehr über das compact letter display wissen willst dann lese gerne nochmal im Kapitel 33.7 nach.\n\n\n\nAbbildung 32.1— Dotplot des Datenbeispiels für die Bonitur von fünf Weizensorten.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Der Friedman Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-posthoc.html",
    "href": "stat-tests-posthoc.html",
    "title": "33  Multiple Vergleiche & Post-hoc Tests",
    "section": "",
    "text": "33.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, nlme, \n               multcomp, emmeans, ggpubr, multcompView,\n               rstatix, conflicted, see, rcompanion)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Multiple Vergleiche & Post-hoc Tests</span>"
    ]
  },
  {
    "objectID": "stat-tests-posthoc.html#daten",
    "href": "stat-tests-posthoc.html#daten",
    "title": "33  Multiple Vergleiche & Post-hoc Tests",
    "section": "33.2 Daten",
    "text": "33.2 Daten\nWir nutzen in diesem Kapitel den Datensatz aus dem Beispiel in Kapitel 5.3. Wir haben als Outcome die Sprunglänge in [cm] von Flöhen. Die Sprunglänge haben wir an Flöhen von Hunde, Katzen und Füchsen gemessen. Der Datensatz ist also recht übeerschaubar. Wir haben ein normalverteiltes \\(y\\) mit jump_length sowie einen multinomialverteiltes \\(y\\) mit grade und einen Faktor animal mit drei Leveln. Im Folgenden laden wir den Datensatz flea_dog_cat_fox.csv und selektieren mit der Funktion select() die benötigten Spalten. Abschließend müssen wir die Spalte animalnoch in einen Faktor umwandeln. Damit ist unsere Vorbereitung des Datensatzes abgeschlossen.\n\nfac1_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox.csv\") |&gt;\n  select(animal, jump_length, grade) |&gt; \n  mutate(animal = as_factor(animal))\n\nIn der Tabelle 33.1 ist der Datensatz fac1_tbl nochmal dargestellt. Wir werden nun den Datensatz fac1_tbl in den folgenden Abschnitten immer wieder nutzen.\n\n\n\n\n\n\nFür die vollständige Datentabelle bitte aufklappen\n\n\n\n\n\n\n\n\n\nTabelle 33.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length sowie der multinominalverteilten Variable grade und einem Faktor animal mit drei Leveln.\n\n\n\n\n\n\nanimal\njump_length\ngrade\n\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n5.6\n8\n\n\ndog\n9.1\n7\n\n\ndog\n8.2\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\nfox\n7.7\n5\n\n\nfox\n8.1\n4\n\n\nfox\n9.1\n4\n\n\nfox\n9.7\n5\n\n\nfox\n10.6\n4\n\n\nfox\n8.6\n4\n\n\nfox\n10.3\n3\n\n\n\n\n\n\n\n\n\n\n\n\n33.2.1 Hypothesen für multiple Vergleiche\nAls wir eine ANOVA gerechnet hatten, hatten wir nur eine Nullhypothese und eine Alternativehypothese. Wenn wir Nullhypothese abgelehnt hatten, wussten wir nur, dass sich mindestens ein paarweiser Vergleich unterschiedet. Multiple Vergleich lösen nun dieses Problem und führen ein Hypothesenpaar für jeden paarweisen Vergleich ein. Zum einen rechnen wir damit \\(k\\) Tests und haben damit auch \\(k\\) Hypothesenpaare (siehe auch Kapitel 20.3 zur Problematik des wiederholten Testens). Wenn wir zum Beispiel alle Level des Faktors animal miteinander Vergleichen wollen, dann rechnen wir \\(k=3\\) paarweise Vergleiche. Im Folgenden sind alle drei Hypothesenpaare dargestellt.\n\\[\n\\begin{aligned}\nH_{01}: &\\; \\bar{y}_{cat} = \\bar{y}_{dog}\\\\\nH_{A1}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{02}: &\\; \\bar{y}_{cat} = \\bar{y}_{fox}\\\\\nH_{A2}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{03}: &\\; \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_{A3}: &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\nWenn wir drei Vergleiche rechnen, dann haben wir eine \\(\\alpha\\) Inflation vorliegen. Wir sagen, dass wir für das multiple Testen adjustieren müssen. In R gibt es eine Reihe von Adjustierungsverfahren. Wir nehmen meist Bonferroni oder das Verfahren, was in der jeweiligen Funktion als Standard (eng. default) gesetzt ist. Wir adjustieren grundsätzlich die \\(p\\)-Werte und erhalten adjustierte \\(p\\)-Werte aus den jeweiligen Funktionen in R. Die adjustierten p-Werte können wir dann mit dem Signifikanzniveau von \\(\\alpha\\) gleich 5% vergleichen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Multiple Vergleiche & Post-hoc Tests</span>"
    ]
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "href": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "title": "33  Multiple Vergleiche & Post-hoc Tests",
    "section": "33.3 Simple Gruppenvergleiche",
    "text": "33.3 Simple Gruppenvergleiche\nWenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, dann können wir die Funktion pairwise.*.test() nutzen. Der Stern * steht entweder als Platzhalter für t für den t-Test oder aber für wilcox für den Wilcoxon Test. Die Funktion ist relativ einfach zu nutzen und liefert auch sofort die entsprechenden p-Werte. Die Funktion pairwise.*.test() ist in dem Sinne veraltet, da wir keine 95% Konfidenzintervalle generieren können. Da die Funktion aber immer mal wieder angefragt wird, ist die Funktion hier nochmal aufgeführt. Die Funktion pairwise.*.test() ist veraltet, wir nutzen das R Paket {emmeans} oder das R Paket {multcomp}.\n\n33.3.1 Paarweiser t Test\nWir nutzen den paarweisen t-Test,\n\nwenn wir ein normalverteiltes \\(y\\) vorliegen haben, wie jump_length.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\nwenn wir Varianzhomogenität über alle Level vorliegen haben, dann nutzen wir die Option pool.sd = TRUE. Wenn wir Varianzheterogenität über alle Level des Faktors vorliegen haben, dann nutzen wir pool.sd = FALSE.\n\nDie Funktion pairwise.t.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und daher auch fehleranfällig. Wir können auch den %$%-Operator aus dem R Paket {magrittr} nutzen um die Problematik zu umgehen. Weiter unten siehst du dann einmal beide Anwendungen.\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein sauberes tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.t.test(fac1_tbl$jump_length, fac1_tbl$animal,\n                p.adjust.method = \"bonferroni\",\n                pool.sd = TRUE) |&gt; \n  tidy() |&gt; \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 × 3\n  group1 group2 p.value\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 cat    dog      0.007\n2 fox    dog      0.876\n3 fox    cat      0.001\n\n\nUnd das ganze nochmal mit dem %$%-Operator aus dem R Paket {magrittr}. In diesem Fall können wir uns das $ sparen und greifen direkt auf die Spalten des Datensatzes zu.\n\nfac1_tbl %$%\n  pairwise.t.test(jump_length, animal,\n                  p.adjust.method = \"bonferroni\",\n                  pool.sd = TRUE) |&gt; \n  tidy() |&gt; \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 × 3\n  group1 group2 p.value\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 cat    dog      0.007\n2 fox    dog      0.876\n3 fox    cat      0.001\n\n\nWir erhalten in einem Tibble die adjustierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wenn du keine adjustierten p-Werte möchtest, dann nutze die Option p.adjust.method = \"none\". Wir sehen, dass der Gruppenvergleich cat - dog signifikant ist, der Gruppenvergleich fox - dog nicht signifikant ist und der Gruppenvergleich fox - cat wiederum signifikant ist. Leider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket {ggpubr} anschauen. Die Hilfeseite des R Pakets {ggpubr} liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\). Um die Abbildung 33.16 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen t-Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert einer einfaktoriellen ANOVA als globalen Test ergänzen.\n\nstat_test_obj &lt;- compare_means(\n jump_length ~ animal, data = fac1_tbl,\n method = \"t.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np &lt;- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"jump_length\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(13, 16, 19)) +\n  stat_compare_means(label.y = 20, method = \"anova\")    \n\n\n\n\n\n\n\nAbbildung 33.16— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem t-Test und den Bonferroni adjustierten p-Werten.\n\n\n\n\n\n\n\n33.3.2 Paarweiser Wilcoxon Test\nWir nutzen den paarweisen Wilxocon Test,\n\nwenn wir ein nicht-normalverteiltes \\(y\\) vorliegen haben, wie grade.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\n\nDie Funktion pairwise.wilcox.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und daher auch fehleranfällig.\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein sauberes tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.wilcox.test(fac1_tbl$grade, fac1_tbl$animal,\n                     p.adjust.method = \"bonferroni\") |&gt; \n  tidy() |&gt; \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 × 3\n  group1 group2 p.value\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 cat    dog      0.045\n2 fox    dog      0.005\n3 fox    cat      0.011\n\n\nWir erhalten in einem Tibble die adujstierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wir sehen, dass der Gruppenvergleich cat - dog knapp signifikant ist, der Gruppenvergleich fox - dog ebenfalls signifikant ist und der Gruppenvergleich fox - cat auch signifikant ist. Leider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket {ggpubr} anschauen. Die Hilfeseite des R Pakets {ggpubr} liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\). Um die Abbildung 33.17 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen Wilcoxon Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert eines Kruskal Wallis als globalen Test ergänzen.\n\nstat_test_obj &lt;- compare_means(\n grade ~ animal, data = fac1_tbl,\n method = \"wilcox.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np &lt;- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"grade\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(10, 13, 16)) +\n  stat_compare_means(label.y = 20, method = \"kruskal.test\")    \n\n\n\n\n\n\n\nAbbildung 33.17— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem Wilcoxon Test und den Bonferroni adjustierten p-Werten.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Multiple Vergleiche & Post-hoc Tests</span>"
    ]
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "href": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "title": "33  Multiple Vergleiche & Post-hoc Tests",
    "section": "33.4 Gruppenvergleich mit {emmeans}",
    "text": "33.4 Gruppenvergleich mit {emmeans}\nIm Folgenden wollen wir uns mit einem anderen R Paket beschäftigen was auch multiple Vergleiche rechnen kann. In diesem Kapitel nutzen wir das R Paket {emmeans}. Im Prinzip kann {emmeans} das Gleiche wir das R Paket {multcomp}. Beide Pakete rechnen dir einen multiplen Vergleich. Das Paket {emmeans} kann noch mit nested comparisons umgehen. Deshalb hier nochmal die Vorstellung von {emmeans}. Du kannst aber für eine simple Auswertung mit nur einem Faktor beide Pakete verwenden.\nWir können hier nicht alles erklären und im Detail durchgehen. Hier gibt es noch ein aufwendiges Tutorium zu emmeans: Getting started with emmeans. Daneben gibt es auch noch die Einführung mit Theorie auf der Seite des R Paktes Es gibt hier auch ein weiteres englischsprachiges Tutorium Don’t Ignore Interactions - Unleash the Full Power of Models with {emmeans} R-package oder {emmeans} Game-Changing R-package Squeezes Hidden Knowledge out of Models!. DISCLAIMER: Der Text ist gut, die Bebilderung des entsprechenden Videos geht so leider mal gar nicht…\n\n33.4.1 Gruppenvergleiche mit emmeans in R\nUm den multiplen Vergleich in emmeans durchführen zu können brauchen wir zuerst ein lineares Modell, was uns die notwenidgen Parameter wie Mittelwerte und Standardabweichungen liefert. Wir nutzen in unserem simplen Beispiel ein lineares Modell mit einer Einflussvariable \\(x\\) und nehmen an, dass unser Outcome \\(y\\) normalverteilt ist. Achtung, hier muss natürlich das \\(x\\) ein Faktor sein. Dann können wir ganz einfach die Funktion lm() nutzen. Im Folgenden fitten wir das Modell fit_2 was wir dann auch weiter nutzen werden.\n\nfit_2 &lt;- lm(jump_length ~ animal, data = fac1_tbl)\n\nDer multiple Vergleich in emmeans ist mehrschrittig. Wir pipen unser Modell aus fit_2 in die Funktion emmeans(). Wir geben mit ~ animal an, dass wir über die Level des Faktors animal einen Vergleich rechnen wollen. Wir adjustieren die \\(p\\)-Werte nach Bonferroni. Danach pipen wir weiter in die Funktion contrast() wo der eigentliche Vergleich festgelegt wird. In unserem Fall wollen wir einen many-to-one Vergleich rechnen. Alle Gruppen zu der Gruppe fox. Du kannst mit ref = auch ein anderes Level deines Faktors wählen.\n\ncomp_2_obj &lt;- fit_2 |&gt; \n  emmeans(~ animal) |&gt; \n  contrast(method = \"trt.vs.ctrl\", ref = \"fox\", adjust = \"bonferroni\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - fox    -1.03 0.947 18  -1.086  0.5837\n cat - fox    -4.41 0.947 18  -4.660  0.0004\n\nP value adjustment: bonferroni method for 2 tests \n\n\nWir können auch einen anderen Kontrast wählen. Wir überschreiben jetzt das Objekt comp_2_obj mit dem Kontrast all-pair, der alle möglichen Vergleiche rechnet. In emmeans heißt der all-pair Kontrast pairwise. Die Ausgabe von emmeans können über die Funktion tidy() aufgeräumt werden. Mehr dazu unter der Hilfeseite von tidy() zu emmeans.\n\ncomp_2_obj &lt;- fit_2 |&gt; \n  emmeans(~ animal) |&gt; \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - cat     3.39 0.947 18   3.574  0.0065\n dog - fox    -1.03 0.947 18  -1.086  0.8756\n cat - fox    -4.41 0.947 18  -4.660  0.0006\n\nP value adjustment: bonferroni method for 3 tests \n\n\nWir können das Ergebnis auch noch mit der Funktion tidy() weiter aufräumen und dann die Spalten selektieren, die wir brauchen. Häufig benötigen wir nicht alle Spalten, die eine Funktion wiedergibt.\n\nres_2_obj &lt;- comp_2_obj |&gt; \n  tidy(conf.int = TRUE) |&gt; \n  select(contrast, estimate, adj.p.value, conf.low, conf.high) |&gt; \n  mutate(across(where(is.numeric), round, 4))\n\nres_2_obj\n\n# A tibble: 3 × 5\n  contrast  estimate adj.p.value conf.low conf.high\n  &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 dog - cat     3.39      0.0065    0.886      5.89\n2 dog - fox    -1.03      0.876    -3.53       1.47\n3 cat - fox    -4.41      0.0006   -6.91      -1.91\n\n\nAbschließend wollen wir noch die 95% Konfidenzintervalle in Abbildung 33.18 abbilden. Hier ist es bei emmeans genauso wie bei multcomp. Wir können das Objekt res_2_obj direkt in ggplot() weiterverwenden und uns die 95% Konfidenzintervalle einmal plotten.\n\n  ggplot(res_2_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\n\n\n\n\nAbbildung 33.18— Die 95% Konfidenzintervalle für den allpair-Vergleich des simplen Datensatzes.\n\n\n\n\n\nIn Abbildung 33.19 sehen wir die Ergebnisse des multiplen Vergleiches nochmal anders als Pairwise P-value plot dargestellt. Wir haben auf der y-Achse zu Abwechselung mal die Gruppen dargestellt und auf der x-Achse die \\(p\\)-Werte. In den Kästchen sind die Effekte der Gruppen nochmal gezeigt. In unserem Fall die Mittelwerte der Sprungweiten für die drei Gruppen. Wir sehen jetzt immer den \\(p\\)-Wert für den jeweiligen Vergleich durch eine farbige Linie miteinander verbunden. So können wir nochmal eine andere Übersicht über das Ergebnis des multiplen Vergleich kriegen.\n\nfit_2 |&gt; \n  emmeans(~ animal) |&gt; \n  pwpp(adjust = \"bonferroni\") +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 33.19— Visualisierung der Ergebnisse im Pairwise P-value plot.\n\n\n\n\n\nAuch haben wir die Möglichkeit un die \\(p\\)-Werte mit der Funktion pwpm() als eine Matrix ausgeben zu lassen. Wir erhalten in dem oberen Triangel die \\(p\\)-Wert für den jeweiligen Vergleich. In dem unteren Triangel die geschätzten Mittelwertsunterschiede. Auf der Diagonalen dann die geschätzten Mittelwerte für die jeweilige Gruppe. So haben wir nochmal alles sehr kompakt zusammen dargestellt.\n\nfit_2 |&gt; \n  emmeans(~ animal) |&gt; \n  pwpm(adjust = \"bonferroni\")\n\n       dog    cat    fox\ndog [8.13] 0.0065 0.8756\ncat   3.39 [4.74] 0.0006\nfox  -1.03  -4.41 [9.16]\n\nRow and column labels: animal\nUpper triangle: P values   adjust = \"bonferroni\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later\n\n\nWir wollen uns noch einen etwas komplizierteren Fall anschauen, indem sich emmeans von multcomp in der Anwendung unterscheidet. Wir laden den Datensatz flea_dog_cat_fox_site.csv in dem wir zwei Faktoren haben. Damit können wir dann ein Modell mit einem Interaktionsterm bauen. Wir erinnern uns, dass wir in der zweifaktoriellen ANOAV eine signifikante Interaktion zwischen den beiden Faktoren animal und site festgestelt hatten.\n\nfac2_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox_site.csv\") |&gt; \n  select(animal, site, jump_length) |&gt; \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 33.2 nochmal dargestellt.\n\n\n\n\nTabelle 33.2— Selektierter Datensatz mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\n\n\n\n\n\nanimal\nsite\njump_length\n\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.1\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\n…\n…\n…\n\n\nfox\nfield\n16.38\n\n\nfox\nfield\n14.59\n\n\nfox\nfield\n14.03\n\n\nfox\nfield\n13.63\n\n\nfox\nfield\n14.09\n\n\nfox\nfield\n15.52\n\n\n\n\n\n\n\n\nIn Abbildung 33.20 sehen wir nochmal die Daten visualisiert. Wichtig ist hier, dass wir zwei Faktoren vorliegen haben. Den Faktor animal und den Faktor site. Dabei ist der Faktor animal in dem Faktor site genested. Wir messen jedes Level des Faktors animal jeweils in jedem Level des Faktors site.\n\n\n\n\n\n\n\n\nAbbildung 33.20— Boxplot der Sprungweiten [cm] von Hunden und Katzen gemessen an verschiedenen Orten.\n\n\n\n\n\nWir rechnen ein multiples lineares Modell mit einem Interaktionsterm. Daher packen wir beide Faktoren in das Modell sowie die Intraktion zwischen den beiden Faktoren. Wir erhalten nach dem fitten des Modells das Objekt fit_3.\n\nfit_3 &lt;- lm(jump_length ~ animal + site + animal:site, data = fac2_tbl)\n\nDer Unterschied zu unserem vorherigen multiplen Vergleich ist nun, dass wir auch einen multiplen Vergleich für animal nested in site rechnen können. Dafür müssen wir den Vergleich in der Form animal | site schreiben. Wir erhalten dann die Vergleiche der Level des faktors animal getrennt für die Level es Faktors site.\n\ncomp_3_obj &lt;- fit_3 |&gt; \n  emmeans(~ animal | site) |&gt; \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\ncomp_3_obj\n\nsite = city:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -3.101 0.771 108  -4.022  0.0003\n cat - fox   -6.538 0.771 108  -8.479  &lt;.0001\n dog - fox   -3.437 0.771 108  -4.457  0.0001\n\nsite = smalltown:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -4.308 0.771 108  -5.587  &lt;.0001\n cat - fox   -4.064 0.771 108  -5.271  &lt;.0001\n dog - fox    0.244 0.771 108   0.316  1.0000\n\nsite = village:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -1.316 0.771 108  -1.707  0.2722\n cat - fox   -1.729 0.771 108  -2.242  0.0809\n dog - fox   -0.413 0.771 108  -0.536  1.0000\n\nsite = field:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -0.982 0.771 108  -1.274  0.6167\n cat - fox    1.366 0.771 108   1.772  0.2379\n dog - fox    2.348 0.771 108   3.045  0.0088\n\nP value adjustment: bonferroni method for 3 tests \n\n\nWir können uns das Ergebnis auch etwas schöner ausgeben lassen. Wir nutzen hier noch die Funktion format.pval() um die \\(p\\)-Werte besser zu formatieren. Die \\(p\\)-Wert, die kleiner sind als 0.001 werden als &lt;0.001 ausgegeben und die anderen \\(p\\)-Werte auf zwei Nachstellen nach dem Komma gerundet.\n\ncomp_3_obj |&gt; \n  summary() |&gt; \n  as_tibble() |&gt; \n  select(contrast, site, p.value) |&gt; \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2))\n\n# A tibble: 12 × 3\n   contrast  site      p.value\n   &lt;fct&gt;     &lt;fct&gt;     &lt;chr&gt;  \n 1 cat - dog city      &lt;0.001 \n 2 cat - fox city      &lt;0.001 \n 3 dog - fox city      &lt;0.001 \n 4 cat - dog smalltown &lt;0.001 \n 5 cat - fox smalltown &lt;0.001 \n 6 dog - fox smalltown 1.00   \n 7 cat - dog village   0.27   \n 8 cat - fox village   0.08   \n 9 dog - fox village   1.00   \n10 cat - dog field     0.62   \n11 cat - fox field     0.24   \n12 dog - fox field     0.01   \n\n\nIn der Ausgabe können wir erkennen, dass die Vergleich in der Stadt alle signifkant sind. Jedoch erkennen wir keine signifikanten Ergebnisse mehr in dem Dorf und im Feld ist nur der Vergleich dog - fox signifkant. Hier solltest du nochmal beachten, warum wir die Analyse getrennt machen. In der zweifaktoriellen ANOVA haben wir gesehen, dass ein signifkanter Interaktionsterm zwischen den beiden Faktoren animal und site vorliegt. Wir wollen uns noch über die Funktion confint() die 95% Konfidenzintervalle wiedergeben lassen.\n\nres_3_obj &lt;- comp_3_obj |&gt; \n  confint() |&gt; \n  as_tibble() |&gt; \n  select(contrast, site, estimate, conf.low = lower.CL, conf.high = upper.CL) \n\nres_3_obj\n\n# A tibble: 12 × 5\n   contrast  site      estimate conf.low conf.high\n   &lt;fct&gt;     &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 cat - dog city        -3.10    -4.98     -1.23 \n 2 cat - fox city        -6.54    -8.41     -4.66 \n 3 dog - fox city        -3.44    -5.31     -1.56 \n 4 cat - dog smalltown   -4.31    -6.18     -2.43 \n 5 cat - fox smalltown   -4.06    -5.94     -2.19 \n 6 dog - fox smalltown    0.244   -1.63      2.12 \n 7 cat - dog village     -1.32    -3.19      0.559\n 8 cat - fox village     -1.73    -3.60      0.146\n 9 dog - fox village     -0.413   -2.29      1.46 \n10 cat - dog field       -0.982   -2.86      0.893\n11 cat - fox field        1.37    -0.509     3.24 \n12 dog - fox field        2.35     0.473     4.22 \n\n\nBesonders mit den 95% Konfiendezintervallen sehen wir nochmal den Interaktionseffekt zwischen den beiden Faktoren animal und site. So dreht sich der Effekt von zum Beispiel dog - fox von \\(-3.44\\) in dem Level city zu \\(+2.35\\) in dem Level field. Wir haben eine Interaktion vorliegen und deshalb die Analyse getrennt für jeden Level des Faktors site durchgeführt. Abbildung 33.21 zeigt die entsprechenden 95% Konfidenzintervalle. Wir müssen hier etwas mit der position spielen, so dass die Punkte und der geom_errorbar richtig liegen.\n\n  ggplot(res_3_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high,\n                        color = site, group = site)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    scale_color_okabeito() +\n    coord_flip() +\n    theme_classic()\n\n\n\n\n\n\n\nAbbildung 33.21— Die 95% Konfidenzintervalle für den allpair-Vergleich des Models mit Interaktionseffekt.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Multiple Vergleiche & Post-hoc Tests</span>"
    ]
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "href": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "title": "33  Multiple Vergleiche & Post-hoc Tests",
    "section": "33.5 Gruppenvergleich mit {multcomp}",
    "text": "33.5 Gruppenvergleich mit {multcomp}\nWir drehen hier einmal die Erklärung um. Wir machen erst die Anwendung in R und sollte dich dann noch mehr über die statistischen Hintergründe der Funktionen interessieren, folgt ein Abschnitt noch zur Theorie. Du wirst die Funktionen aus multcomp vermutlich in deiner Abschlussarbeit brauchen. Häufig werden multiple Gruppenvergleiche in Abschlussarbeiten gerechnet.\n\n33.5.1 Gruppenvergleiche mit multcomp in R\nAls erstes brauchen wir ein lineares Modell für die Verwendung von multcomp. Normalerweise verenden wir das gleiche Modell, was wir schon in der ANOVA verwendet haben. Wir nutzen hier ein simples lineares Modell mit nur einem Faktor. Im Prinzip kann das Modell auch größer sein, davon schauen wir uns dann später im Verlauf des Kapitels noch welche an.\n\nfit_1 &lt;- lm(jump_length ~ animal, data = fac1_tbl)\n\nWir haben das Objekt fit_1 mit der Funktion lm() erstellt. Im Modell sind jetzt alle Mittelwerte und die entsprechenden Varianzen geschätzt worden. Mit summary(fit_1) kannst du dir gerne das Modell auch nochmal anschauen. Wenn wir keinen all-pair Vergleich rechnen wollen, dann können wir auch einen many-to-one Vergleich mit dem Dunnett Kontrast rechnen.\nIm Anschluss nutzen wir die Funktion glht() um den multiplen Vergleich zu rechnen. Als erstes musst du wissen, dass wenn wir alle Vergleiche rechnen wollen, wir einen all-pair Vergleich rechnen. In der Statistik heißt dieser Typ von Vergleich Tukey. Wir wollen jetzt als für den Faktor animal einen multiplen Tukey-Vergleich rechnen. Nichts anders sagt mcp(animal = \"Tukey\") aus, dabei steht mcp für multiple comparison procedure. Mit dem hinteren Teil der Funktion weiß jetzt die Funktion glht() was gerechnet werden soll. Wir müssen jetzt der Funktion nur noch mitgeben auf was der multiple Vergleich gerechnet werden soll, mit dem Objekt fit_1. Wir speichern die Ausgabe der Funktion in comp_1_obj.\n\ncomp_1_obj &lt;- glht(fit_1, linfct = mcp(animal = \"Tukey\")) \n\nMit dem Objekt comp_1_fit können wir noch nicht soviel anfangen. Der Inhalt ist etwas durcheinander und wir wollen noch die Konfidenzintervalle haben. Daher pipen wir comp_1_fit erstmal in die Funktion tidy() und lassen mit der Option conf.int = TRUE die simultanen 95% Konfidenzintervalle berechnen. Dann nutzen wir die Funktion select() um die wichtigen Spalten zu selektieren. Abschließend mutieren wir noch alle numerischen Spalten in dem wir auf die dritte Kommastelle runden. Wir speichern alles in das Objekt res_1_obj.\n\nres_1_obj &lt;- comp_1_obj |&gt; \n  tidy(conf.int = TRUE) |&gt; \n  select(contrast, estimate, adj.p.value, \n         conf.low, conf.high) |&gt; \n  mutate_if(is.numeric, round, 3)\n\nWir lassen uns dann den Inhalt von dem Objekt res_1_obj ausgeben.\n\nres_1_obj\n\n# A tibble: 3 × 5\n  contrast  estimate adj.p.value conf.low conf.high\n  &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 cat - dog    -3.39       0.006    -5.80    -0.968\n2 fox - dog     1.03       0.535    -1.39     3.45 \n3 fox - cat     4.41       0.001     2.00     6.83 \n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir cat - dog rechnen. Also wirklich der Mittelwert der Sprungweite der Katzenflöhe minus den Mittelwert der Sprungweite der Hundeflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleichecat - dog mit einem p-Wert von \\(0.006\\) sowie für den Vergleich \\(fox - cat\\) mit einem p-Wert von \\(0.001\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nWenn es die unadjustierten \\(p\\)-Werte sein sollen, dann müssen wir nochmal das glht-Objekt in die Funktion summary() stecken und dann die Option test = adjusted(\"none\") wählen. Dann können wir weiter machen wie gewohnt.\n\nsummary(comp_1_obj, test = adjusted(\"none\")) |&gt; \n  tidy() \n\n# A tibble: 3 × 7\n  term   contrast  null.value estimate std.error statistic  p.value\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 animal cat - dog          0    -3.39     0.947     -3.57 0.00217 \n2 animal fox - dog          0     1.03     0.947      1.09 0.292   \n3 animal fox - cat          0     4.41     0.947      4.66 0.000195\n\n\nIn Abbildung 33.22 sind die simultanen und damit adjustierten 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab. Mit der Funktion factor() kannst du die Sortierung der Level einem Faktor ändern und somit auch Position auf den Achsen.\n\n  ggplot(res_1_obj, aes(contrast, y=estimate, \n                        ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\n\n\n\n\nAbbildung 33.22— Simultane 95% Konfidenzintervalle für den paarweisen Vergleich der Sprungweiten in [cm] der Hunde-, Katzen- und Fuchsflöhe.\n\n\n\n\n\nDie Entscheidung gegen die Nullhypothese anhand der simultanen 95% Konfidenzintervalle ist inhaltlich gleich, wie die Entscheidung anhand der p-Werte. Wir entscheiden gegen die Nullhypothese, wenn die 0 nicht mit im Konfindenzintervall enthalten ist. Wir wählen hier die 0 zur Entscheidung gegen die Nullhypothese, weil wir einen Mittelwertsvergleich rechnen.\nFür den Vergleich fox -dog ist die 0 im 95% Konfidenzintervall, wir können daher die Nullhypothese nicht ablehnen. Das 95% Konfidenzintervall ist nicht signifikant. Bei dem Vergleich fox - cat sowie dem Vergleich cat - dog ist jeweils die 0 nicht im 95% Konfidenzintervall enthalten. Beide 95% Konfidenzintervalle sind signifikant, wir können die Nullhypothese ablehnen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Multiple Vergleiche & Post-hoc Tests</span>"
    ]
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-var-heterogen",
    "href": "stat-tests-posthoc.html#sec-posthoc-var-heterogen",
    "title": "33  Multiple Vergleiche & Post-hoc Tests",
    "section": "33.6 Gruppenvergleich mit Varianzheterogenität",
    "text": "33.6 Gruppenvergleich mit Varianzheterogenität\nBis jetzt sind wir von Varianzhomogenität über alle Gruppen in unserer Behandlung ausgegangen. Oder aber konkret auf unser Beispiel, die Tierarten haben alle die gleiche Varianz. Jetzt gibt es aber häufiger mal den Fall, da können oder wollen wir nicht an Varianzhomogenität über alle Gruppen glauben. Deshalb gibt es hier noch Alternativen, von denen ich ein paar Vorstellen werde.\n\n33.6.1 Games-Howell-Test\nDer Games-Howell-Test ist eine Alternative zu dem Paket {multcomp} und dem Paket {emmeans} für ein einfaktorielles Design. Wir nutzen den Games-Howell-Test, wenn die Annahme der Homogenität der Varianzen, der zum Vergleich aller möglichen Kombinationen von Gruppenunterschieden verwendet wird, verletzt ist. Dieser Post-Hoc-Test liefert Konfidenzintervalle für die Unterschiede zwischen den Gruppenmitteln und zeigt, ob die Unterschiede statistisch signifikant sind. Der Test basiert auf der Welch’schen Freiheitsgradkorrektur und adjustiert die \\(p\\)-Werte. Der Test vergleicht also die Differenz zwischen den einzelnen Mittelwertpaaren mit einer Adjustierung für den Mehrfachtest. Es besteht also keine Notwendigkeit, zusätzliche p-Wert-Korrekturen vorzunehmen. Mit dem Games-Howell-Test ist nur ein all-pair Vergleich möglich.\nFür den Games-Howell-Test aus dem Paket {rstatix} müssen wir kein lineares Modell fitten. Wir schreiben einfach die wie in einem t-Test das Outcome und den Faktor mit den Gruppenleveln in die Funktion games_howell_test(). Wir erhalten dann direkt das Ergebnis des Games-Howell-Test. Wir nutzen in diesem Beispiel die Daten aus dem Objekt fac1_tbl zu sehen in Tabelle 33.1.\n\nfit_4 &lt;- games_howell_test(jump_length ~ animal, data = fac1_tbl) \n\nWir wollen aber nicht mit der Ausgabe arbeiten sondern machen uns noch ein wenig Arbeit und passen die Ausgabe an. Zum einen brauchen wir noch die Kontraste und wir wollen die \\(p\\)-Werte auch ansprechend formatieren. Wir erhalten das Objekt res_4_obj und geben uns die Ausgabe wieder.\n\nres_4_obj &lt;- fit_4 |&gt; \n  as_tibble() |&gt; \n  mutate(contrast = str_c(group1, \"-\", group2)) |&gt; \n  select(contrast, estimate, p.adj, conf.low, conf.high) |&gt; \n  mutate(p.adj = format.pval(p.adj, eps = 0.001, digits = 2))\n\nres_4_obj\n\n# A tibble: 3 × 5\n  contrast estimate p.adj conf.low conf.high\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 dog-cat     -3.39 0.02     -6.28    -0.490\n2 dog-fox      1.03 0.52     -1.52     3.57 \n3 cat-fox      4.41 0.00      2.12     6.71 \n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt, den haben wir uns selber mit der Funktion mutate() und str_c() aus den Spalten group1 und group2 gebaut. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir dog-cat rechnen. Also wirklich den Mittelwert der Sprungweite der Hundeflöhe minus den Mittelwert der Sprungweite der Katzenflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleiche dog - cat mit einem p-Wert von \\(0.02\\) sowie für den Vergleich \\(cat - fox\\) mit einem p-Wert von \\(0.00\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nIn Abbildung 33.23 sind die simultanen 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab.\n\n  ggplot(res_4_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    coord_flip() +\n    theme_classic()\n\n\n\n\n\n\n\nAbbildung 33.23— Die 95% Konfidenzintervalle für den allpair-Vergleich des Games-Howell-Test.\n\n\n\n\n\nDie Entscheidungen nach den 95% Konfidenzintervallen sind die gleichen wie nach dem \\(p\\)-Wert. Da wir hier es mit einem Mittelwertsvergleich zu tun haben, ist die Entscheidung gegen die Nullhypothese zu treffen wenn die 0 im Konfidenzintervall ist.\n\n\n33.6.2 Generalized Least Squares\nNeben dem Games-Howell-Test gibt es auch die Möglichkeit Generalized Least Squares zu nutzen. Hier haben wir dann auch die Möglichkeit mehrfaktorielle Modelle zu schätzen und dann auch wieder emmeans zu nutzen. Das ist natürlich super, weil wir dann wieder in dem emmeans Framework sind und alle Funktionen von oben nutzen können. Deshalb hier auch nur die Modellanpassung, den Rest kopierst du dir dann von oben dazu.\nDie Funktion gls() aus dem R Paket {nlme} passt ein lineares Modell unter Verwendung der verallgemeinerten kleinsten Quadrate an. Die Fehler dürfen dabei aber korreliert oder aber ungleiche Varianzen haben. Damit haben wir ein Modell, was wir nutzen können, wenn wir Varianzheterogenität vorliegen haben. Hier einmal das simple Beispiel für die Tierarten. Wir nehmen wieder die ganz normale Formelschreiweise. Wir können jetzt aber über die Option weights = angeben, wie wir die Varianz modellieren wollen. Die Schreibweise mag etwas ungewohnt sein, aber es gibt wirklich viele Arten die Varainz zu modellieren. Hier machen wir uns es einfach und nutzen die Helferfunktion varIdent und modellieren dann für jedes Tier eine eigene Gruppenvarianz.\n\ngls_fac1_fit &lt;- gls(jump_length ~ animal, \n                    weights = varIdent(form =  ~ 1 | animal), \n                    data = fac1_tbl)\n\nDann können wir die Modellanpasssung auch schon in emmeans() weiterleiten und schauen uns mal das Ergebnis gleich an. Wir achten jetzt auf die Spalte SE, die uns ja den Fehler der Mittelwerte für jede Gruppe wiedergibt.\n\ngls_fac1_fit |&gt; \n  emmeans(~ animal)\n\n animal emmean    SE   df lower.CL upper.CL\n dog      8.13 0.811 6.01     6.15     10.1\n cat      4.74 0.719 6.01     2.98      6.5\n fox      9.16 0.415 6.00     8.14     10.2\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\nWir sehen, dass wir für jedes Tier eine eigene Varianz über den Standardfehler SE geschätzt haben. Das war es ja auch was wir wollten. Bis hierhin wäre es auch mit dem Games-Howell-Test gegangen. Was ist aber, wenn wir ein zweifaktorielles design mit Interaktion schätzen wollen? Das können wir analog wie eben machen. Wir erweitern einfach das Modell um die Terme site für den zweiten Faktor und den Interaktionsterm animal:site.\n\ngls_fac2_fit &lt;- gls(jump_length ~ animal + site + animal:site, \n                    weights = varIdent(form =  ~ 1 | animal), \n                    data = fac2_tbl)\n\nDann können wir uns wieder die multiplen Vergleiche getrennt für die Interaktionsterme wiedergeben lassen. Blöderweise haben jetzt alle Messorte site die gleiche Varianz für jede Tierart, aber auch da können wir noch ran.\n\ngls_fac2_fit |&gt; \n  emmeans(~ animal | site)\n\nsite = city:\n animal emmean    SE   df lower.CL upper.CL\n cat      13.6 0.603 36.0     12.4     14.8\n dog      16.7 0.581 36.1     15.5     17.9\n fox      20.1 0.437 36.0     19.2     21.0\n\nsite = smalltown:\n animal emmean    SE   df lower.CL upper.CL\n cat      13.7 0.603 36.0     12.5     15.0\n dog      18.0 0.581 36.1     16.9     19.2\n fox      17.8 0.437 36.0     16.9     18.7\n\nsite = village:\n animal emmean    SE   df lower.CL upper.CL\n cat      15.2 0.603 36.0     14.0     16.5\n dog      16.6 0.581 36.1     15.4     17.7\n fox      17.0 0.437 36.0     16.1     17.9\n\nsite = field:\n animal emmean    SE   df lower.CL upper.CL\n cat      16.2 0.603 36.0     15.0     17.4\n dog      17.2 0.581 36.1     16.0     18.3\n fox      14.8 0.437 36.0     13.9     15.7\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\nDie eigentliche Stärke von gls() kommt eigentlich erst zu tragen, wenn wir auch noch erlauben, dass wir die Varianz über alle Tierarten, Messsorte und Interaktionen variieren kann. Das machen wir, in dem wir einfach animal*site zu der varIdent() Funktion ergänzen.\n\ngls_fac2_fit &lt;- gls(jump_length ~ animal + site + animal:site, \n                    weights = varIdent(form =  ~ 1 | animal*site), \n                    data = fac2_tbl)\n\nDann noch schnell in emmeans() gesteckt und sich das Ergebnis angeschaut.\n\ngls_fac2_fit |&gt; \n  emmeans(~ animal | site)\n\nsite = city:\n animal emmean    SE   df lower.CL upper.CL\n cat      13.6 0.594 7.87     12.2     15.0\n dog      16.7 0.713 8.86     15.1     18.3\n fox      20.1 0.365 9.00     19.3     20.9\n\nsite = smalltown:\n animal emmean    SE   df lower.CL upper.CL\n cat      13.7 0.433 8.99     12.8     14.7\n dog      18.0 0.614 9.53     16.7     19.4\n fox      17.8 0.433 8.98     16.8     18.8\n\nsite = village:\n animal emmean    SE   df lower.CL upper.CL\n cat      15.2 0.743 8.93     13.6     16.9\n dog      16.6 0.384 9.00     15.7     17.4\n fox      17.0 0.548 8.71     15.7     18.2\n\nsite = field:\n animal emmean    SE   df lower.CL upper.CL\n cat      16.2 0.601 6.66     14.8     17.6\n dog      17.2 0.563 8.38     15.9     18.5\n fox      14.8 0.378 9.00     14.0     15.7\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\nWie du jetzt siehst schätzen wir die Varianz für jede Tierart und jeden Messort separat. Wir haben also wirklich jede Varianz einzeln zugeordnet. Die Frage ist immer, ob das notwendig ist, denn wir brauchen auch eine gewisse Fallzahl, damit die Modelle funktionieren. Aber das kommt jetzt sehr auf deine Fragestellung an und müssten wir nochmal konkret besprechen.\n\n\n\n\n\n\nWeitere Literatur\n\n\n\nAuf der Seite DSFAIR gibt es noch einen Artikel zu emmeans und der Frage Why are the StdErr all the same? und dazu dann auch passend die Publikation Analyzing designed experiments: Should we report standard deviations or standard errors of the mean or standard errors of the difference or what?\n\n\n\n\n33.6.3 Robuste Schätzung von Standardfehlern\nWir immer gibt es noch eine Möglichkeit die Varianzheterogenität zu behandeln. Wir nutzen jetzt hier einmal die Funktionen aus dem R Paket {sandwich}, die es uns ermöglichen Varianzheterogenität (eng. heteroskedasticity) zu modellieren. Es ist eigentlich super einfach, wir müssen als erstes wieder unser Modell anpassen. Hier einmal mit einer simplen linearen Regression.\n\nlm_fac1_fit &lt;- lm(jump_length ~ animal, data = fac1_tbl)\n\nDann können wir auch schon für einen Gruppenvergleich direkt in der Funktion emmeans() für die Varianzheterogenität adjustieren. Es gibt verschiedene mögliche Sandwich-Schätzer, aber wir nehmen jetzt mal einen der häufigsten mit vcovHC. Wie immer gilt, es gibt ja nach Datenlage bessere und schlechtere Schätzer. Wir laden jetzt nicht das ganze Paket, sondern nur die Funktion mit sandwich::vcovHAC. Achtung, hinter der Option vcov. ist ein Punkt. Ohne den Aufruf vcov. = funktioniert die Funktion dann nicht.\n\nem_obj &lt;- lm_fac1_fit |&gt; \n  emmeans(~ animal, method = \"pairwise\", vcov. = sandwich::vcovHAC)\nem_obj\n\n animal emmean    SE df lower.CL upper.CL\n dog      8.13 0.427 18     7.23     9.03\n cat      4.74 0.810 18     3.04     6.44\n fox      9.16 0.468 18     8.17    10.14\n\nConfidence level used: 0.95 \n\n\nWir du siehst, die Standardfehler sind jetzt nicht mehr über alle Gruppen gleich. Dann können wir uns auch die paarweisen Vergleiche ausgeben lassen.\n\ncontr_obj &lt;- em_obj |&gt; \n  contrast(method = \"pairwise\", adjust = \"none\")\n\nOder aber wir lassen uns das compact letter display wiedergeben.\n\ncld_obj &lt;- em_obj |&gt;\n  cld(Letters = letters, adjust = \"none\")\n\n\n\n\n\n\n\nExkurs: Robuste Schätzung von Standardfehlern, Konfidenzintervallen und p-Werten\n\n\n\n\n\nWenn du noch etwas weiter gehen möchtest, dann kannst du dir noch die Hilfeseite von dem R Paket {performance} Robust Estimation of Standard Errors, Confidence Intervals, and p-values anschauen. Die Idee ist hier, dass wir die Varianz/Kovarianz robuster daher mit der Berücksichtigung von Varianzheterogenität (eng. heteroskedasticity) schätzen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Multiple Vergleiche & Post-hoc Tests</span>"
    ]
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-compact-letter",
    "href": "stat-tests-posthoc.html#sec-compact-letter",
    "title": "33  Multiple Vergleiche & Post-hoc Tests",
    "section": "33.7 Compact letter display",
    "text": "33.7 Compact letter display\nIn der Pflanzenernährung ist es nicht unüblich sehr viele Substrate miteinander zu vergleichen. Oder andersherum, wenn wir sehr viele Gruppen haben, dann kann die Darstellung in einem all-pair Vergleich sehr schnell sehr unübersichltich werden. Deshalb wure das compact letter display entwickelt. Das compact letter display zeigt an, bei welchen Vergleichen der Behandlungen die Nullhypothese gilt. Daher werden die nicht signifikanten Ergebnisse visualisiert. Gut zu lesen ist auch die Vignette Re-engineering CLDs der Hilfeseite zu {emmeans} sowie die Hilfeseite Compact Letter Display (CLD) - What is it?.\nSchauen wir uns aber zuerst einmal ein größeres Beispiel mit neun Behandlungen mit jeweils zwanzig Beobachtungen an. Wir erstellen uns den Datensatz in der Form, dass sich die Mittelwerte für die Behandlungen teilweise unterscheiden.\n\nset.seed(20220914)\ndata_tbl &lt;- tibble(trt = gl(n = 9, k = 20, \n                            labels = c(\"pos_ctrl\", \"neg_ctrl\", \"treat_A\", \"treat_B\", \n                                       \"treat_C\", \"treat_D\", \"treat_E\", \"treat_F\", \n                                       \"treat_G\")),\n                   rsp = c(rnorm(20, 10, 5), rnorm(20, 20, 5), rnorm(20, 22, 5), rnorm(20, 24, 5),\n                           rnorm(20, 35, 5), rnorm(20, 37, 5), rnorm(20, 40, 5), rnorm(20, 43, 5),\n                           rnorm(20, 50, 5)))\n\nIn der Abbildung 33.24 ist der Datensatz data_tbl nochmal als Boxplot dargestellt.\n\n\n\n\n\n\n\n\nAbbildung 33.24— Boxplot der Beispieldaten.\n\n\n\n\n\nWir sehen, dass sich die positive Kontrolle von dem Rest der Behandlungen unterscheidet. Danach haben wir ein Plateau mit der negativen Kontrolle und der Behanldung A und der Behandlung B. Nach diesem Plateau haben wir einen Sprung und sehen einen leicht linearen Anstieg der Mittelwerte der Behandlungen.\nSchauen wir uns zuerst einmal an, wie ein compact letter display aussehen würde, wenn kein Effekt vorliegen würde. Daher die Nullhypothese ist wahr und die Mittelwerte der Gruppen unterscheiden sich nicht. Wir nutzen hier einmal ein kleineres Beispiel mit den Behandlungslevels ctrl, treat_A und treat_B. Alle drei Behandlungslevel haben einen Mittelwert von 10. Es gilt die Nullhypothese und wir erhalten folgendes compact letter display in Tabelle 33.3.\n\n\n\nTabelle 33.3— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt, es gibt keinen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\\(\\phantom{a}\\)\n\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n10\na\n\n\n\n\ntreat_B\n10\na\n\n\n\n\n\n\n\n\nDas Gegenteil sehen wir in der Tabelle 33.4. Hier haben wir ein compact letter display wo sich alle drei Mittelwerte mit 10, 15 und 20 voneinander klar unterscheiden. Die Nullhypothese gilt für keinen der möglichen paarweisen Vergleiche.\n\n\n\nTabelle 33.4— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt nicht, es gibt einen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n15\n\nb\n\n\n\ntreat_B\n20\n\\(\\phantom{a}\\)\n\nc\n\n\n\n\n\n\nSchauen wir uns nun die Implementierung des compact letter display für die verschiedenen Möglichkeiten der Multiplen Vergleiche einmal an.\n\n33.7.1 … für pairwise.*.test()\nWenn wir für die Funktionen pairwise.*.test() das compact letter display berechnen wollen, dann müssen wir etwas ausholen. Denn wir müssen dafür die Funktion multcompLetters() nutzen. Diese Funktion braucht die \\(p\\)-Werte als Matrix und diese Matrix der \\(p\\)-Werte kriegen wir über die Funktion fullPTable(). Am Ende haben wir aber dann das was wir wollten. Ich habe hier nochmal das einfache Beispiel mit den Sprungweiten von oben genommen.\n\npairwise.t.test(fac1_tbl$jump_length, fac1_tbl$animal,\n                p.adjust.method = \"bonferroni\") |&gt; \n  extract2(\"p.value\") |&gt; \n  fullPTable() |&gt; \n  multcompLetters()\n\ndog cat fox \n\"a\" \"b\" \"a\" \n\n\nAls Ergebnis erhalten wir, dass Hund- und Fuchsflöhe gleich weit springen, beide teilen sich den gleichen Buchstaben. Katzenflöhe springen unterschiedlich zu Hunden- und Fuchsflöhen. Das Vorgehen ändert sich dann nicht, wenn wir eine andere Funktion wie pairwise.wilcox.test() nehmen.\n\n\n33.7.2 … für das Paket {emmeans}\nIn dem Paket {emmeans} ist das compact letter display ebenfalls implementiert und wir müssen nicht die Funktion multcompLetters() nutzen. Durch die direkte Implementierung ist es etwas einfacher sich das compact letter display anzeigen zu lassen. Das Problem ist dann später sich die Buchstaben zu extrahieren um die Abbildung 33.25 zu ergänzen. Wir nutzen in {emmeans} die Funktion cld() um das compact letter display zu erstellen.\n\nemmeans_cld &lt;- lm(rsp  ~ trt, data = data_tbl) |&gt;\n  emmeans(~ trt) |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\")\n\nWir erhalten dann die etwas besser sortierte Ausgabe für die Behandlungen wieder.\n\nemmeans_cld \n\n trt      emmean   SE  df lower.CL upper.CL .group \n pos_ctrl   9.67 1.12 171     6.51     12.8  a     \n neg_ctrl  20.02 1.12 171    16.86     23.2   b    \n treat_A   20.97 1.12 171    17.81     24.1   b    \n treat_B   23.35 1.12 171    20.19     26.5   b    \n treat_C   34.96 1.12 171    31.80     38.1    c   \n treat_D   37.46 1.12 171    34.30     40.6    cd  \n treat_E   40.17 1.12 171    37.01     43.3     de \n treat_F   43.23 1.12 171    40.07     46.4      e \n treat_G   50.51 1.12 171    47.35     53.7       f\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 9 estimates \nP value adjustment: bonferroni method for 36 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWie die Ausgabe von cld() richtig anmerkt, können compact letter display irreführend sein weil sie eben Nicht-Unterschiede anstatt von signifikanten Unterschieden anzeigen. Zum Anderen sehen wir aber auch, dass wir 36 statistische Tests gerechnet haben und somit zu einem Signifikanzniveau von \\(\\cfrac{\\alpha}{k} = \\cfrac{0.05}{36} \\approx 0.0014\\) testen. Wir brauchen also schon sehr große Unterschiede oder aber eine sehr kleine Streuung um hier signifikante Effekte nachweisen zu können.\nIn Tabelle 33.5 sehen wir das Ergebnis des compact letter display nochmal mit den Mittelwerten der Behandlungslevel zusammen dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. Wir gehen so alle Vergleiche einmal durch.\n\n\n\nTabelle 33.5— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus emmeans. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\n\n\n\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\n\ntreat_E\n40\n\n\n\nd\ne\n\n\n\ntreat_F\n43\n\n\n\n\ne\n\n\n\ntreat_G\n45\n\n\n\n\n\nf\n\n\n\n\n\n\nAbschließend können wir die Buchstaben aus dem compact letter display noch in die Abbildung 33.25 ergänzen. Hier müssen wir etwas mehr machen um die Buchstaben aus dem Objekt emmeans_cld zu bekommen. Du kannst dann noch die y-Position anpassen wenn du möchtest.\n\nletters_tbl &lt;- emmeans_cld |&gt; \n  tidy() |&gt; \n  select(trt, label = .group) |&gt; \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_minimal() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nAbbildung 33.25— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n\n\n\n\n\n\n\nDas compact letter display anders gebaut…\n\n\n\nIn dem kurzen Kapitel zu Re-engineering CLDs in der Vingette des R Pakets {emmeans} geht es darum, wie das compact letter display auch anders gebaut werden könnte. Nämlich einmal als wirkliches Anzeigen von Gleichheit über die Option delta oder aber über das Anzeigen von Signifikanz über die Option signif. Damit haben dann auch wirklich die Information, die wir dann wollen. Also eigentlich was Schönes. Wie machen wir das nun?\nWenn wir das compact letter display in dem Sinne der Gleichheit der Behandlungen interpretieren wollen, also das der gleiche Buchstabe bedeutet, dass sich die Behandlungen nicht unterscheiden, dann müssen wir ein delta setzen in deren Bandbreite oder Intervall die Behandlungen gleich sind. Ich habe hier mal ein delta von 20 gewählt und wir adjustieren bei einem Test auf Gleichheit nicht.\n\nemmeans_cld &lt;- lm(rsp  ~ trt, data = data_tbl) |&gt;\n  emmeans(~ trt) |&gt;\n  cld(Letters = letters, adjust = \"none\", delta = 20)\nemmeans_cld \n\n trt      emmean   SE  df lower.CL upper.CL .equiv.set\n pos_ctrl   9.67 1.12 171     7.45     11.9  a        \n neg_ctrl  20.02 1.12 171    17.80     22.2  ab       \n treat_A   20.97 1.12 171    18.75     23.2  abc      \n treat_B   23.35 1.12 171    21.13     25.6  abcd     \n treat_C   34.96 1.12 171    32.74     37.2   bcde    \n treat_D   37.46 1.12 171    35.24     39.7    cde    \n treat_E   40.17 1.12 171    37.94     42.4     de    \n treat_F   43.23 1.12 171    41.01     45.4      e    \n treat_G   50.51 1.12 171    48.29     52.7      e    \n\nConfidence level used: 0.95 \nStatistics are tests of equivalence with a threshold of 20 \nP values are left-tailed \nsignificance level used: alpha = 0.05 \nEstimates sharing the same symbol test as equivalent \n\n\nWie wir jetzt sehen, bedeuten gleiche Buchstaben, dass die Behandlungen gleich sind. In dem Sinne gleich sind, dass die Behandlungen im Mittel nicht weiter als der Wert in delta auseinanderliegen. Wie du jetzt das delta bestimmst ist eine biologische und keine statistische Frage.\nKönnen wir uns auch signifikante Unterschiede anzeigen lassen? Ja, können wir auch, aber das ist meiner Meinung nach die schlechtere der beiden Anpassungen. Wir haben ja im Kopf, dass das compact letter display eben mit gleichen Buchstaben das Gleiche anzeigt. Jetzt würden wir diese Eigenschaften zu Unterschied ändern. Schauen wir uns das einmal an einem kleineren Datensatz einmal an.\n\nsmall_fit &lt;- data_tbl |&gt; \n  filter(trt %in% c(\"pos_ctrl\", \"neg_ctrl\", \"treat_A\", \"treat_G\")) |&gt; \n  lm(rsp  ~ trt, data = _) \n\nDann können wir uns hier einmal das Problem mit den Buchstaben anschauen. Wir kriegen jetzt mit der Option signif = TRUE wiedergeben Estimates sharing the same symbol are significantly different und das ist irgendwie auch wirr, dass gleiche Buchstaben einen Unterschied darstellen. Das compact letter display wird eben als Gleichheit gelesen, so dass wir hier mehr verwirrt werden als es hilft.\n\nsmall_fit |&gt; \n  emmeans(~ trt) |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\", signif = TRUE)\n\n trt      emmean   SE df lower.CL upper.CL .signif.set\n pos_ctrl   9.67 1.08 76      6.9     12.4  ab        \n neg_ctrl  20.02 1.08 76     17.2     22.8  a         \n treat_A   20.97 1.08 76     18.2     23.7   b        \n treat_G   50.51 1.08 76     47.7     53.3  ab        \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \nP value adjustment: bonferroni method for 6 tests \nsignificance level used: alpha = 0.05 \nEstimates sharing the same symbol are significantly different \n\n\nDeshalb würde ich auf die Buchstaben verzichten und die Zahlen angeben, damit hier nicht noch mehr Verwirrung aufkommt. Wir nehmen also die Option für die Buchstaben einmal aus dem cld() Aufruf raus. Dann haben wir Zahlen als Gruppenzuweisung, was schon mal was anderes ist als die Buchstaben aus dem compact letter display.\n\nsmall_fit |&gt; \n  emmeans(~ trt) |&gt;\n  cld(adjust = \"bonferroni\", signif = TRUE)\n\n trt      emmean   SE df lower.CL upper.CL .signif.set\n pos_ctrl   9.67 1.08 76      6.9     12.4  12        \n neg_ctrl  20.02 1.08 76     17.2     22.8  1         \n treat_A   20.97 1.08 76     18.2     23.7   2        \n treat_G   50.51 1.08 76     47.7     53.3  12        \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 4 estimates \nP value adjustment: bonferroni method for 6 tests \nsignificance level used: alpha = 0.05 \nEstimates sharing the same symbol are significantly different \n\n\nInnerhalb des selben Konzepts dann zwei Arten von Interpretation des compact letter display zu haben, ist dann auch nicht zielführend. Den auch die gleichen Zahlen bedeuten jetzt einen Unterschied. Irgendwie mag ich persönlich nicht diese sehr verdrehte Interpretation nicht. Deshalb lieber ein delta einführen und auf echte Gleichheit testen.\n\n\n\n\n33.7.3 … für das Paket {multcomp}\nWir schauen uns zuerst einmal die Implementierung des compact letter display in dem Paket {multcomp} an. Wir nutzen die Funktion multcompLetters() aus dem Paket {multcompView} um uns das compact letter display wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Das heist wir müssen aus der Funktion glht() die adjustierten \\(p\\)-Werte extrahieren und dann einen Vektor der Vergleiche bzw. Kontraste in der Form A-B bauen. Also ohne Leerzeichen und in der Beschreibung der Level der Behandlung trt. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nmultcomp_cld &lt;- lm(rsp  ~ trt, data = data_tbl) |&gt;\n  glht(linfct = mcp(trt = \"Tukey\")) |&gt; \n  tidy() |&gt; \n  mutate(contrast = str_replace_all(contrast, \"\\\\s\", \"\")) |&gt; \n  pull(adj.p.value, contrast) |&gt; \n  multcompLetters() \n\nWir erhalten dann folgendes compact letter display für die paarweisen Vergleiche aus multcomp.\n\nmultcomp_cld \n\nneg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F  treat_G \n     \"a\"      \"a\"      \"a\"      \"b\"     \"bc\"     \"cd\"      \"d\"      \"e\" \npos_ctrl \n     \"f\" \n\n\nLeider sind diese Buchstaben in dieser Form schwer zu verstehen. Deshalb gibt es noch die Funktion plot() in dem Paket {multcompView} um uns die Buchstaben mit den Leveln der Behandlung einmal ausgeben zu lassen. Wir erhalten dann folgende Abbildung.\n\nmultcomp_cld |&gt; plot()\n\n\n\n\n\n\n\n\nIn dem compact letter display bedeuten gleiche Buchstaben, dass die Behandlungen gleich sind. Es gilt die Nullhypothese für diesen Vergleich. Was sehen wir also hier? Kombinieren wir einmal das compact letter display mit den Leveln der Behandlung und den Mittelwerten der Behandlungen in einer Tabelle 33.6. Wenn die Mittelwerte gleich sind, dann erhalten die Behandlungslevel den gleichen Buchstaben. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein a. Ebenso sind die MIttelwerte von treat_C und treat_D nahezu gleich, dehalb erhalten beide ein b. Das machen wir immer so weiter und konzentrieren uns also auf die nicht signifikanten Ergebnisse. Denn gleiche Buchstaben bedeuten, dass die Behandlungen gleich sind. Wir sehen hier also, bei welchen Vergleichen die Nullhypothese gilt.\n\n\n\nTabelle 33.6— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus multcomp. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\n\n\n\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\n\nneg_ctrl\n20\na\n\n\n\n\n\n\n\ntreat_A\n22\na\n\n\n\n\n\n\n\ntreat_B\n24\na\n\n\n\n\n\n\n\ntreat_C\n35\n\nb\n\n\n\n\n\n\ntreat_D\n37\n\nb\nc\n\n\n\n\n\ntreat_E\n40\n\n\nc\nd\n\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\npos_crtl\n10\n\n\n\n\n\nf\n\n\n\n\n\n\nWir können dann die Buchstaben auch in den Boxplot ergaänzen. Die y-Position kann je nach Belieben dann noch angepasst werden. zum Beispiel könnten hier auch die Mittelwerte aus einer summarise() Funktion ergänzt werden und so die y-Position angepasst werden.\n\nletters_tbl &lt;- multcomp_cld$Letters |&gt; \n  enframe(\"trt\", \"label\") |&gt; \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_minimal() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nAbbildung 33.26— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n\n\n\n33.7.4 … für den Games-Howell-Test\nAbschließend wollen wir uns die Implementierung des compact letter display für den Games-Howell-Test einmal anschauen. Es gilt vieles von dem in diesem Abschnitt schon gesagtes. Wir nutzen die Funktion multcompLetters() aus dem Paket {multcompView} um uns das compact letter display aus dem Games-Howell-Test wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nght_cld &lt;- games_howell_test(rsp ~ trt, data = data_tbl) |&gt; \n  mutate(contrast = str_c(group1, \"-\", group2)) |&gt; \n  pull(p.adj, contrast) |&gt; \n  multcompLetters() \n\nDas compact letter display kennen wir schon aus der obigen Beschreibung.\n\nght_cld\n\npos_ctrl neg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F \n     \"a\"      \"b\"      \"b\"      \"b\"      \"c\"     \"cd\"      \"d\"      \"d\" \n treat_G \n     \"e\" \n\n\nWir können uns dann auch das compact letter display als übersichtlicheren Plot wiedergeben lassen.\n\nght_cld |&gt; plot()\n\n\n\n\n\n\n\n\nUm die Zusammenhänge besser zu verstehen ist in Tabelle 33.7 nochmal die Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. In der Form können wir alle Vergleiche einmal durchgehen.\n\n\n\nTabelle 33.7— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus dem Games-Howell-Test. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\n\n\n\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\ntreat_E\n40\n\n\n\nd\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\n\n\n\nWir können dann auch in Abbildung 33.27 sehen, wie das compact letter display mit den Boxplots verbunden wird.\n\nletters_tbl &lt;- ght_cld$Letters |&gt; \n  enframe(\"trt\", \"label\") |&gt; \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_minimal() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nAbbildung 33.27— Boxplot der Beispieldaten zusammen mit den compact letter display.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Multiple Vergleiche & Post-hoc Tests</span>"
    ]
  },
  {
    "objectID": "stat-tests-posthoc.html#referenzen",
    "href": "stat-tests-posthoc.html#referenzen",
    "title": "33  Multiple Vergleiche & Post-hoc Tests",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 33.1— “Total soluble solids (TSS) in ‘Alicia’ papaya propagated by seed, grafting, and in vitro (n = 12 ± s.e.), evaluated from April 2020 to May 2021 at the maturation of 100% yellow skin color. The line marks the 10 °Brix value, commonly referred to as the minimum for papaya commercialization. Different letters on the same date indicate statistically significant differences between propagation procedures (Tukey’s test p &lt; 0.05).” Quelle: Salinas u. a. (2023)\nAbbildung 33.2— “Panicles with commercial fruit (%) at 2019 and 2020 harvest. In the same year, different letters indicate significant differences among treatments (Tukey’s test p&lt;0.05).” Quelle: Sánchez u. a. (2022)\nAbbildung 33.3— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein simples Säulendiagramm mit sehr für Farbblinde ungünstigen Farben. Es sind die Mittelwerte sowie die Standardabweichung durch die Fehlerbalken dargestellt.\nAbbildung 33.4— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt.\nAbbildung 33.5— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion gls() im obigen Modell berücksichtigt.\nAbbildung 33.6— Die Abbildung des Säulendiagramms in ggplot nachgebaut mit den Informationen aus der Funktion emmeans(). Das compact letter display wird dann über das geom_text() gesetzt. Die Varianzheterogenität nach der Funktion sandwich::vcovHAC im obigen Modell berücksichtigt.\nAbbildung 33.7— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit den zwei Faktoren Zeit und die Iodine Form.\nAbbildung 33.8— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\nAbbildung 33.9— Die Abbildung des Säulendiagramms in ggplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen. Die Kontrolle wurde entfernt, sonst hätten wir hier nicht emmeans in der einfachen Form nutzen können. Wir rechnen hier die Vergleiche getrennt für die beiden Jodformen.\nAbbildung 33.10— Ursprüngliche Abbildung, die nachgebaut werden soll. Ein Barplot mit den zwei Faktoren Zeit und die Iodine Form. Hier soll es dann ein Boxplot werden.\nAbbildung 33.11— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen, dafür müssen wir uns aber nochmal ein Positionsdatensatz bauen. Hier ist das compact letter display getrennt für die beiden Jodformen berechnet.\nAbbildung 33.12— Die Abbildung des Säulendiagramms in ggplot als Boxplot nachgebaut. Wir nutzen das geom_text() um noch besser unser compact letter display zu setzen, dafür müssen wir uns aber nochmal ein Positionsdatensatz bauen. Hier ist das compact letter display für jede einzelne Faktorkombination berechnet.\nAbbildung 33.13— Ursprüngliche Abbildung, die nachgebaut werden soll. Insgesamt vier Outcomes sollen für zwei Behandlungen ausgewertet werden. Das praktische ist hier, dass wir es nur mit einem einfaktoriellen Design zu tun haben.\nAbbildung 33.14— Nachbau der Abbildung mit der Funktion facte_wrap() mit Boxplots und dem Mittelwert. Neben dem Mittelwert finden sich das compact letter display. Auf eine Einfärbung nach der Behandlung wurde verzichtet um die Abbildung nicht noch mehr zu überladen.\nAbbildung 33.15— Nachbau der Abbildung mit dem R Paket {patchwork} mit Boxplots und dem Mittelwert. Neben dem Mittelwert finden sich das compact letter display bei dem ersten Plot. Auf eine Einfärbung nach der Behanldung wurde verzichtet um die Abbildung nicht noch mehr zu überladen.\nAbbildung 33.16— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem t-Test und den Bonferroni adjustierten p-Werten.\nAbbildung 33.17— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem Wilcoxon Test und den Bonferroni adjustierten p-Werten.\nAbbildung 33.18— Die 95% Konfidenzintervalle für den allpair-Vergleich des simplen Datensatzes.\nAbbildung 33.19— Visualisierung der Ergebnisse im Pairwise P-value plot.\nAbbildung 33.20— Boxplot der Sprungweiten [cm] von Hunden und Katzen gemessen an verschiedenen Orten.\nAbbildung 33.21— Die 95% Konfidenzintervalle für den allpair-Vergleich des Models mit Interaktionseffekt.\nAbbildung 33.22— Simultane 95% Konfidenzintervalle für den paarweisen Vergleich der Sprungweiten in [cm] der Hunde-, Katzen- und Fuchsflöhe.\nAbbildung 33.23— Die 95% Konfidenzintervalle für den allpair-Vergleich des Games-Howell-Test.\nAbbildung 33.24— Boxplot der Beispieldaten.\nAbbildung 33.25— Boxplot der Beispieldaten zusammen mit den compact letter display.\nAbbildung 33.26— Boxplot der Beispieldaten zusammen mit den compact letter display.\nAbbildung 33.27— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\nCumming G, Fidler F, Vaux DL. 2007. Error bars in experimental biology. The Journal of cell biology 177: 7–11.\n\n\nSalinas I, Hueso JJ, Força Baroni D, Cuevas J. 2023. Plant Growth, Yield, and Fruit Size Improvements in ‘Alicia’Papaya Multiplied by Grafting. Plants 12: 1189.\n\n\nSánchez M, Velásquez Y, González M, Cuevas J. 2022. Hoverfly pollination enhances yield and fruit quality in mango under protected cultivation. Scientia Horticulturae 304: 111320.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Multiple Vergleiche & Post-hoc Tests</span>"
    ]
  },
  {
    "objectID": "stat-tests-chi-test.html",
    "href": "stat-tests-chi-test.html",
    "title": "34  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "",
    "text": "34.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, effectsize, rstatix,\n               scales, parameters, conflicted)\nconflicts_prefer(stats::chisq.test)\nconflicts_prefer(stats::fisher.test)\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Der $\\mathcal{X}^2$-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-chi-test.html#daten-für-den-mathcalx2-test",
    "href": "stat-tests-chi-test.html#daten-für-den-mathcalx2-test",
    "title": "34  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "34.2 Daten für den \\(\\mathcal{X}^2\\)-Test",
    "text": "34.2 Daten für den \\(\\mathcal{X}^2\\)-Test\nWie eben schon benannt schauen wir uns für den \\(\\mathcal{X}^2\\)-Test eine Vierfeldertafel oder aber 2x2 Kreuztabelle an. In Tabelle 34.1 sehen wir eine solche 2x2 Kreuztabelle. Da wir eine Mindestanzahl an Zellbelegung brauchen um überhaupt mit dem \\(\\mathcal{X}^2\\)-Test rechnen zu können, nutzen wir hier gleich aggrigierte Beispieldaten. Wir brauchen mindestens fünf Beobachtungen je Zelle, dass heißt mindestens 20 Tiere. Da wir dann aber immer noch sehr wenig haben, ist die Daumenregel, dass wir etwa 30 bis 40 Beobachtungen brauchen. In unserem Beispiel schauen wir uns 65 Tiere an.\n\n\n\nTabelle 34.1— Eine 2x2 Tabelle als Beispiel für unterschiedliche Flohinfektionen bei Hunden und Katzen. Dargestellt sind die beobachteten Werte.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfected\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\nAnimal\nDog\n\\(23_{\\;\\Large a}\\)\n\\(10_{\\;\\Large b}\\)\n\\(\\mathbf{a+b = 33}\\)\n\n\n\nCat\n\\(18_{\\;\\Large c}\\)\n\\(14_{\\;\\Large d}\\)\n\\(\\mathbf{c+d = 32}\\)\n\n\n\n\n\\(\\mathbf{a+c = 41}\\)\n\\(\\mathbf{b+d = 24}\\)\n\\(n = 65\\)\n\n\n\n\n\n\nIn der Tabelle sehen wir, dass in den zeieln die Level des Faktors animal angegeben sind und in den Spalten die Level des Faktors infected. Wir haben somit \\(23\\) Hunde, die mit Flöhen infiziert sind, dann \\(10\\) Hunde, die nicht mit Flöhen infiziert sind. Auf der Seite der Katzen haben wir \\(18\\) Katzen, die infiziert sind und \\(14\\) Katzen, die keine Flöhe haben. An den Rändern stehen die Randsummen. Wir haben \\(33\\) Hunde und \\(32\\) Katzen sowie \\(41\\) infizierte Tiere und \\(24\\) nicht infizierte Tiere. Somit haben wir dann in Summe \\(n = 65\\) Tiere. Diese Form der Tabelle wird uns immer wieder begegnen.\nBevor wir jetzt diese 2x2 Kreuztabelle verwenden, müssen wir uns nochmal überlegen, welchen Schluss wir eigentlich über die Nullhypothese machen. Wie immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in dem \\(\\mathcal{X}^2\\)-Test aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Der $\\mathcal{X}^2$-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-chi-test.html#hypothesen-für-den-mathcalx2-test",
    "href": "stat-tests-chi-test.html#hypothesen-für-den-mathcalx2-test",
    "title": "34  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "34.3 Hypothesen für den \\(\\mathcal{X}^2\\)-Test",
    "text": "34.3 Hypothesen für den \\(\\mathcal{X}^2\\)-Test\nDer \\(\\mathcal{X}^2\\)-Test betrachtet die Zellbelegung gegeben den Randsummen um einen Unterschied nachzuweisen. Daher haben wir die Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Zahlen in den Zellen gegeben der Randsummen gleich sind. Wir betrachten hier nur die Hypothesen in Prosa und die mathematischen Hypothesen. Es ist vollkommen ausreichend, wenn du die Nullhypothese des \\(\\mathcal{X}^2\\)-Test nur in Prosa kennst.\n\\[\nH_0: \\; \\mbox{Zellbelegung sind gleichverteilt gegeben der Randsummen}\n\\]\nDie Alternative lautet, dass sich die Zahlen in den Zellen gegeben der Randsummen unterscheiden.\n\\[\nH_A: \\; \\mbox{Zellbelegung sind nicht gleichverteilt gegeben der Randsummen}\n\\]\nWir schauen uns jetzt einmal den \\(\\mathcal{X}^2\\)-Test theoretisch an bevor wir uns mit der Anwendung des \\(\\mathcal{X}^2\\)-Test in R beschäftigen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Der $\\mathcal{X}^2$-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-chi-test.html#mathcalx2-test-theoretisch",
    "href": "stat-tests-chi-test.html#mathcalx2-test-theoretisch",
    "title": "34  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "34.4 \\(\\mathcal{X}^2\\)-Test theoretisch",
    "text": "34.4 \\(\\mathcal{X}^2\\)-Test theoretisch\nIn Tabelle 34.1 von oben hatten wir die beobachteten Werte. Das sind die Zahlen, die wir in unserem Experiment erhoben und gemessen haben. Der \\(\\mathcal{X}^2\\)-Test vergleicht nun die beobachteten Werte mit den anhand der Randsummen zu erwartenden Werte. Daher ist die Formel für den \\(\\mathcal{X}^2\\)-Test wie folgt.\n\\[\n\\chi^2_{calc} = \\sum\\cfrac{(O - E)^2}{E}\n\\]\nmit\n\n\\(O\\) für die beobachteten Werte\n\\(E\\) für die nach den Randsummen zu erwartenden Werte\n\nIn Tabelle 34.2 kannst du sehen wie wir anhand der Randsummen die erwartenden Zellbelegungen berechnen. Hierbei können auch krumme Zahlen rauskommen. Wir würden keinen Unterschied zwischen Hunde und Katzen gegeben deren Infektionsstatus erwarten, wenn die Abweichungen zwischen den beobachteten Werten und den zu erwartenden Werten klein wären. Wir berechnen nun die zu erwartenden Werte indem wir die Randsummen der entsprechenden Zelle multiplizieren und durch die Gesamtanzahl teilen.\n\n\n\nTabelle 34.2— Eine 2x2 Tabelle als Beispiel für unterschiedliche Flohinfektionen bei Hunden und Katzen. Dargestellt sind die zu erwartenden Werte.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfected\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\nAnimal\nDog\n\\(\\cfrac{41 \\cdot 33}{65} = 20.82\\)\n\\(\\cfrac{24 \\cdot 33}{65} = 12.18\\)\n\\(\\mathbf{33}\\)\n\n\n\nCat\n\\(\\cfrac{41 \\cdot 32}{65} = 20.18\\)\n\\(\\cfrac{24 \\cdot 32}{65} = 11.82\\)\n\\(\\mathbf{32}\\)\n\n\n\n\n\\(\\mathbf{41}\\)\n\\(\\mathbf{24}\\)\n\\(n = 65\\)\n\n\n\n\n\n\nWir können dann die Formel für den \\(\\mathcal{X}^2\\)-Test entsprechend ausfüllen. Dabei ist wichtig, dass die Abstände quadriert werden. Das ist ein Kernkonzept der Statistik, Abstände bzw. Abweichungen werden immer quadriert.\n\\[\\begin{aligned}\n\\chi^2_{calc} &= \\cfrac{(23 - 20.82)^2}{20.82} + \\cfrac{(10 - 12.18)^2}{12.18} + \\\\\n&\\phantom{=}\\;\\; \\cfrac{(18 - 20.18)^2}{20.18} + \\cfrac{(14 - 11.82)^2}{11.82} = 1.25\n\\end{aligned}\\]\nEs ergibt sich ein \\(\\chi^2_{calc}\\) von \\(1.25\\) mit der Regel, dass wenn \\(\\chi^2_{calc} \\geq \\chi^2_{\\alpha=5\\%}\\) die Nullhypothese abgelehnt werden kann. Mit einem \\(\\chi^2_{\\alpha=5\\%} = 3.84\\) können wir die Nullhypothese nicht ablehnen. Es besteht kein Zusammenhang zwischen den Befall mit Flöhen und der Tierart. Oder anders herum, Hunde und Katzen werden gleich stark mit Flöhen infiziert.\nWie stark ist nun der beobachtete Effekt? Wir konnten zwar die Nullhypothese nicht ablehnen, aber es wäre auch von Interesse für die zukünftige Versuchsplanung, wie stark sich die Hunde und Katzen im Befall mit Flöhen unterscheiden. Wir haben nun die Wahl zwischen zwei statistischen Maßzahlen für die Beschreibung eines Effektes bei einem \\(\\mathcal{X}^2\\)-Test.\nZum einen Cramers \\(V\\), das wir in etwa interpretieren wie eine Korrelation und somit auch einheitslos ist. Wenn Cramers \\(V\\) gleich 0 ist, dann haben wir keinen Unterschied zwischen den Hunden und Katzen gegeben dem Flohbefall. Bei einem Cramers \\(V\\) von 0.5 haben wir einen sehr starken Unterschied zwischen dem Flohbefall zwischen den beiden Tierarten. Der Vorteil von Cramers V ist, dass wir Cramers \\(V\\) auch auf einer beliebig großen Kreuztabelle berechnen können. Die Formel ist nicht sehr komplex.\n\\[\nV = \\sqrt{\\cfrac{\\mathcal{X}^2/n}{\\min(c-1, r-1)}}\n\\]\nmit\n\n\\(\\mathcal{X}^2\\) gleich der Chi-Quadrat-Statistik\n\\(n\\) gleich der Gesamtstichprobengröße\n\\(r\\): Anzahl der Reihen\n\\(c\\): Anzahl der Spalten\n\n\\[\nV = \\sqrt{\\cfrac{1.26/65}{1}} = 0.14\n\\]\nWir setzen also den Wert \\(\\chi^2_{calc}\\) direkt in die Formel ein. Wir wissen ja auch, dass wir \\(n = 65\\) Tiere untersucht haben. Da wir eine 2x2 Kreuztabelle vorliegen haben, haben wir \\(r = 2\\) und \\(c = 2\\) und somit ist das Minimum von \\(r-1\\) und \\(c-1\\) gleich \\(1\\). Wir erhalten ein Cramers \\(V\\) von \\(0.14\\) was für einen schwachen Effekt spricht. Wir können uns auch grob an der folgenden Tabelle der Effektstärken für Carmers \\(V\\) orientieren.\n\n\n\n\nschwach\nmittel\nstark\n\n\n\n\nCramers \\(V\\)\n0.1\n0.3\n0.5\n\n\n\nZum anderen haben wir noch die Möglichkeit die Odds Ratios oder das Chancenverhältnis zu berechnen. Die Odds Ratios lassen sich direkter als Effekt interpretieren als Cramers \\(V\\) haben aber den Nachteil, dass wir die Odds Ratios nur auf einer 2x2 Kreuztabelle berechnen können. Wichtig bei der Berechnung der Odds Ratios und der anschließenden Interpretation ist die obere Zeile der 2x2 Kreuztabelle. Die obere Zeile ist der Bezug für die Interpretation. Wir nutzen folgende Formel für die Berechnung der Odds Ratios.\n\\[\n\\mbox{Odds Ratio} = OR = \\cfrac{a\\cdot d}{b \\cdot c} = \\cfrac{23\\cdot 14}{10 \\cdot 18} = 1.79\n\\]\nEs ist zwingend notwendig für die folgenden Interpretation der Odds Ratios, dass in den Spalten links die \\(ja\\) Spalte steht und rechts die \\(nein\\) Spalte. Ebenso interpretieren wie die Odds Ratios im Bezug zur oberen Zeile. In unserem Fall ist also die Chance sich mit Flöhen zu infizieren bei Hunden 1.79 mal größer als bei Katzen. Diese Interpretation ist nur korrekt, wenn die 2x2 Kreuztabelle wie beschrieben erstellt ist!",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Der $\\mathcal{X}^2$-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-chi-test.html#mathcalx2-test-in-r",
    "href": "stat-tests-chi-test.html#mathcalx2-test-in-r",
    "title": "34  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "34.5 \\(\\mathcal{X}^2\\)-Test in R",
    "text": "34.5 \\(\\mathcal{X}^2\\)-Test in R\nDer \\(\\mathcal{X}^2\\)-Test wird meist in anderen Funktionen in R verwendet und nicht direkt. Wenn du Fragen dazu hast, schreib mir einfach eine Mail.\nWenn wir den \\(\\mathcal{X}^2\\)-Test in R rechnen wollen nutzen wir die Funktion chisq.test(), die eine Matrix von Zahlen verlangt. Dies ist etwas umständlich. Wir müssen nur beachten, dass wir die Matrix so bauen, wie wir die Matrix auch brauchen. Deshalb immer mal doppelt schauen, ob deine Matrix auch deinen beobachteten Werten entspricht.\n\nmat &lt;- matrix(c(23, 10, 18, 14), \n              byrow = TRUE, nrow = 2,\n              dimnames = list(animal = c(\"dog\", \"cat\"),\n                              infected = c(\"yes\", \"no\")))\nchisq.test(mat, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  mat\nX-squared = 1.2613, df = 1, p-value = 0.2614\n\n\nAls ein mögliches Effektmaß können wir Cramers \\(V\\) berechnen. Wir nutzen hierzu die Funktion cramers_v(). Auf einer reinen 2x2 Kreuztabelle wäre aber Pearsons \\(\\phi\\) durch die Funktion phi() vorzuziehen. Siehe dazu auch \\(\\phi\\) and Other Contingency Tables Correlations auf der Hilfeseite des R Paketes {effectsize}. Wir bleiben hier dann aber bei Cramers \\(V\\).\n\ncramers_v(mat) \n\nCramer's V (adj.) |       95% CI\n--------------------------------\n0.06              | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass der Effekt mit einem \\(V = 0.06\\) schwach ist. Ein Wert von 0 bedeutet keine Assoziation und ein Wert von 1 einen maximalen Zusammenhang. Wir können die Werte von \\(V\\) wie eine Korrelation interpretieren. Der in R berechnete Wert unterscheidet sich von unseren händsichen berechneten Wert, da wir hier mit dem adjustierten Cramers \\(V\\) rechnen. Wir würden auch in der Anwendung den adjustierten Wert verwenden, aber für das Verständnis reicht der händisch berechnete Wert.\nHaben wir eine geringe Zellbelegung von unter 5 in einer der Zellen der 2x2 Kreuztabelle, dann verwenden wir den Fisher Exakt Test. Der Fisher Exakt Test hat auch den Vorteil, dass wir direkt die Odds Ratios wiedergegeben bekommen. Wir können auch den Fisher Exakt Test rechnen, wenn wir viele Beobachtungen pro Zelle haben und einfach an die Odds Ratios rankommen wollen. Der Unterschied zwischen dem klassischen \\(\\mathcal{X}^2\\)-Test und dem Fisher Exakt Test ist in der praktischen Anwendung nicht so groß.\n\nfisher.test(mat)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  mat\np-value = 0.3102\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.5756546 5.6378438\nsample estimates:\nodds ratio \n   1.77279 \n\n\nWir sehen auch hier den nicht signifikanten \\(p\\)-Wert sowie eine Odds Ratio von 1.77. Hunde haben aso eine um 1.77 höhere Chance sich mit Flöhen zu infizieren.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Der $\\mathcal{X}^2$-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-chi-test.html#test-auf-gleiche-oder-gegebene-anteile",
    "href": "stat-tests-chi-test.html#test-auf-gleiche-oder-gegebene-anteile",
    "title": "34  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "34.6 Test auf gleiche oder gegebene Anteile",
    "text": "34.6 Test auf gleiche oder gegebene Anteile\nNeben dem Test auf absolute Anteile, wie der \\(\\mathcal{X}^2\\)-Test es rechnet, wollen wir manchmal auch relative Anteile testen. Also haben wir nicht 8 kranke Erdbeeren gezählt sondern 8 kranke von 12 Erdbeeren. Damit sind dann 66% bzw. 0.66 kranke Erdbeeren vorhanden. Wir rechnen also mit Wahrscheinlichkeiten, also eng. proportions. Deshalb können wir hier auch die R Funktion prop.test() nutzen. Wichtig ist zu wissen, dass wir trotz allem erst die Anzahl an beschädigten Erdbeeren \\(x\\) sowie die absolute Anzahl an Erdbeeren \\(n\\) brauchen. Das wird jetzt aber gleich in dem Beispiel etwas klarer. Im Prinzip ist der prop.test() also eine andere Art den \\(\\mathcal{X}^2\\)-Test zu rechnen.\n\n34.6.1 Vergleich eines Anteils \\(p_1\\) gegen einen gegebenen Anteil \\(p_0\\)\nNehmen wir ein etwas konstruiertes Beispiel zur Erdbeerernte. Wir haben einen neuen Roboter entwickelt, der Erdbeeren erntet. Nun stellen wir fest, dass von 100 Erdbeeren 76 heile sind. Jetzt lesen wir im Handbuch, dass der Ernteroboter eigentlich 84% der Erdbeeren heile ernten sollte. Sind jetzt 76 von 100, also 76%, signifikant unterschiedlich von 84%? Oder können wir die Nullhypothese der Gleichheit zwischen den beiden Wahrscheinlichkeiten nicht ablehnen? Damit können wir den Test auf Anteile nutzen, wenn wir eine beobachtete Wahrscheinlichkeit oder Anteil \\(p_1\\) gegen eine gegebene Wahrscheinlichkeit oder Anteil \\(p_0\\) vergleichen wollen.\nIn unserem Fall haben wir mit \\(p_1\\) die Wahrscheinlichkeit vorliegen, dass die Erdbeeren in unserem Experiment heile sind. Wir wissen also, dass \\(p_1 = \\cfrac{x}{n} = \\cfrac{76}{100} = 0.76 = 76\\%\\) ist. Damit ist die Wahrscheinlichkeit \\(p_1\\) auch die beobachte Wahrscheinlichkeit. Wir erwarten auf der anderen Seite die Wahrscheinlichkeit \\(p_0 = 0.84 = 84\\%\\). Die Roboterfirma hat uns aj zugesagt, dass 84% der Erdbeeren heile bleiben sollen.\nWir berechnen jetzt einmal die beobachten Werte. Zum einen haben wir \\(x=76\\) heile Erdbeeren von \\(n=100\\) Erdbeeren gezählt. Damit ergeben sich dann \\(x = 76\\) heile Erdbeeren und \\(24\\) beschädigte Erdbeeren. Die Summe muss ja am Ende wieder 100 Erdbeeren ergeben.\n\\[\n\\begin{aligned}\nO &=\n\\begin{pmatrix}\nx &|& n - x\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n76 &|& 100 - 76\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\n76 &|& 24\n\\end{pmatrix}\n\\end{aligned}\n\\]\nDann müssen wir uns noch die Frage stellen, welche Anzahl an heilen Erdbeeren hätten wir erwartet? In diesem Fall ja 84% heile Erdbeeren. Das macht dann bei 100 Erdbeeren \\(0.84 \\cdot 100 = 84\\) heile Erdbeeren und \\((1 - 0.84) \\cdot 100 = 16\\) beschädigte Erdbeeren.\n\\[\n\\begin{aligned}\nE &=\n\\begin{pmatrix}\nn \\cdot p &|& n \\cdot (1 - p)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n100 \\cdot 0.84 &|& 100 \\cdot (1 - 0.84)\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\n84 &|& 16\n\\end{pmatrix}\n\\end{aligned}\n\\]\nJetzt müssen wir nur noch die beobachteten Anteile mit den zu erwarteten Anteilen durch die \\(\\mathcal{X}^2\\)-Formel in Kontext setzten. Wir subtrahieren von jedem beobachten Anteil den zu erwartenden Anteil, quadrieren und addieren auf. Dann erhalten wir die \\(\\mathcal{X}^2\\)-Statistik.\n\\[\n\\chi^2_{calc} = \\sum\\cfrac{(O-E)^2}{E} = \\cfrac{(76 - 84)^2}{84} + \\cfrac{(24 - 16)^2}{16} = 4.76\n\\]\nWir können diese einfache Berechnung dann auch schnell nochmal in R durchführen. Wir setzten dafür einfach x, n und p fest und berechnen dann die beobachten Anteile \\(O\\) sowwie die zu erwartenden Anteile \\(E\\). Wir berechnen dann hier auch den gleichen Wert für \\(\\mathcal{X}^2\\)-Statistik.\n\nx &lt;- 76\nn &lt;- 100\np &lt;- 0.84\nO &lt;- cbind(x, n - x)\nE &lt;- cbind(n * p, n * (1 - p))\nsum((abs(O - E))^2/E)\n\n[1] 4.761905\n\n\nUnd wie immer gibt es auch eine Funktion prop.test(), die uns ermöglicht einen beobachteten Anteil x/n zu einem erwarteten Anteil p zu vergleichen. Auch hier sehen wir, dass sich die \\(\\mathcal{X}^2\\)-Statistik aus der R Funktion nicht von unser berechneten \\(\\mathcal{X}^2\\)-Statistik unterscheidet.\n\nprop.test(x = 76, n = 100, p = 0.84, correct = FALSE) \n\n\n    1-sample proportions test without continuity correction\n\ndata:  76 out of 100, null probability 0.84\nX-squared = 4.7619, df = 1, p-value = 0.0291\nalternative hypothesis: true p is not equal to 0.84\n95 percent confidence interval:\n 0.6676766 0.8330867\nsample estimates:\n   p \n0.76 \n\n\n\n\nWas ist die Yates Korrektur, die wir mit correct = TRUE auswählen? Die Korrektur von Yates subtrahiert von jedem Summanden des Chi-Quadrat-Tests 0.5, so dass die Chi-Quadrat-Statistik geringer ausfällt.\n\n\n34.6.2 Vergleich zweier Anteile \\(p_1\\) und \\(p_2\\)\nNun haben wir nicht einen Ernteroboter sondern zwei brandneue Robotertypen. Einmal die Marke Picky und einmal den Roboter Colly. Wir wollen jetzt wieder bestimmen, wie viel Erdbeeren bei der Ernte beschädigt werden. Erdbeeren sind ja auch ein sehr weiches Obst. Wir vergleichen hier also wieder zwei Anteile miteinander. Der Roboter Picky beschädigt 76 von 100 Erdbeeren und der Roboter Colly detscht 91 von 100 Erdbeeren an. Das sind ganz schön meise Werte, aber was will man machen, wir haben jetzt nur die beiden Roboter vorliegen. Die Frage ist nun, ob sich die beiden Roboter in der Häufigkeit der beschädigten Erdbeeren unterscheiden. Wir können hier eine 2x2 Kreuztabelle in der Tabelle 34.5 aufmachen und die jeweiligen Anteile berechnen. Picky beschädigt 76% der Erdbeeren und Colly ganze 91%.\n\n\n\nTabelle 34.3— Eine 2x2 Tabelle für die zwei Ernteroboter und der beschädigten Erdbeeren. Dargestellt sind die beobachteten Werte.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDamaged\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\nRobot\nPicky\n\\(76\\)\n\\(24\\)\n\\(\\mathbf{100}\\)\n\n\n\nColly\n\\(91\\)\n\\(9\\)\n\\(\\mathbf{100}\\)\n\n\n\n\n\\(\\mathbf{167}\\)\n\\(\\mathbf{33}\\)\n\\(n = 200\\)\n\n\n\n\n\n\nWir können die erwarteten Anteile jetzt wie schon bekannt berechnen oder aber wir nutzen folgende Formel um \\(\\hat{p}\\), die Wahrscheinlichkeit für ein Ereignis, zu berechnen. Wir nutzen dann \\(\\hat{p}\\) um die erwarteten Werte \\(E\\) aus zurechnen. Dafür addieren wir alle beobachteten \\(x\\) zusammen und teilen diese Summe durch die gesammte Anzahl an Beobachtungen.\n\\[\n\\hat{p} = \\cfrac{\\sum x}{\\sum n} = \\cfrac{79 + 91}{100 + 100} = \\cfrac{170}{200} = 0.85\n\\]\nIm Folgenden die Rechenschritte nochmal in R aufgedröselt zum besseren nachvollziehen. Wie auch schon im obigen Beispiel berechnen wir erst die beobachten Anteil \\(O\\) sowie die erwartenden Anteile \\(E\\). Dann nutzen wir die Formel des \\(\\mathcal{X}^2\\)-Test um die \\(\\mathcal{X}^2\\)-Statistik zu berechnen.\n\nx &lt;- c(76, 91)\nn &lt;- c(100, 100)\np &lt;- sum(x)/sum(n)\nO &lt;- cbind(x, n - x)\nE &lt;- cbind(n * p, n * (1 - p))\nsum((abs(O - E))^2/E)\n\n[1] 8.165487\n\n\nAuch hier vergleichen wir nochmal unser händisches Ergebnis mit dem Ergebnis der R Funktion prop.test(). Der Funktion übergeben wir dann einmal die beobachteten Anteile \\(x\\) sowie dann die jeweils Gesamtanzahlen \\(n\\). Wichtig ist hier, dass wir als 95% Konfidenzintervall die Differenz der beiden Wahrscheinlichkeiten erhalten.\n\nprop.test(x = c(76, 91), n = c(100, 100), correct = FALSE) \n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(76, 91) out of c(100, 100)\nX-squared = 8.1655, df = 1, p-value = 0.00427\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.25076198 -0.04923802\nsample estimates:\nprop 1 prop 2 \n  0.76   0.91 \n\n\nWir wir sehen unterscheiden sich die beiden Anteil von \\(76/100\\) gleich 76% von \\(91/100\\) gleich 91%. Damit sollten wir den Roboter Picky nehmen, denn da werden prozentual weniger Erdbeeren zermanscht. Ob das jetzt gut oder schlecht ist 76% Erdbeeren zu zerstören ist aber wieder eine Frage, die die Statistik an dieser Stelle nicht beantworten kann.\n\n\n34.6.3 Vergleich mehrerer Anteile \\(p_1\\), \\(p_2\\) bis \\(p_k\\)\nIm Folgenden haben wir jetzt nicht mehr zwei Gruppen, die wir miteinander vergleichen wollen, sondern mehrere Behandlungen als Gruppen. Wir haben uns in einem Experiment die beschädigten Erdbeeren nach vier neue Erntearten, A bis D, angeschaut. Beide Vektoren können wir dann in die Funktion prop.test() stecken.\n\ndamaged  &lt;- c(A = 105, B = 100, C = 139, D = 96)\nberries &lt;- c(106, 113, 156, 102)\nprop.test(damaged, berries)\n\n\n    4-sample test for equality of proportions without continuity correction\n\ndata:  damaged out of berries\nX-squared = 11.747, df = 3, p-value = 0.008303\nalternative hypothesis: two.sided\nsample estimates:\n   prop 1    prop 2    prop 3    prop 4 \n0.9905660 0.8849558 0.8910256 0.9411765 \n\n\nWir erhalten dann einen \\(p\\)-Wert für die Signifikanz von 0.0056 wieder. Was testen wir hier eigentlich? Unsere Nullhypothese ist, dass alle paarweisen Wahrscheinlichkeiten zwischen den Gruppen gleich sind.\n\\[\nH_0: \\; \\mbox{Der Anteil der beschädigten Erdbeeren ist in den vier Gruppen ähnlich hoch}\n\\]\n\\[\nH_A: \\; \\mbox{Der Anteil der beschädigten Erdbeeren in mindestens einer der Behandlungen unterschiedlich ist.}\n\\]\nWie wir sehen ist das nicht der Fall. Wir haben hier vier Wahrscheinlichkeiten vorliegen und mindestens zwei unterscheiden sich. Welche das sind, ist wieder die Frage. Hierzu nutzen wir dann gleich die Funktion pairwise.prop.test(). Wir immer geht die Ausgabe auch schöner und aufgeräumter.\n\nprop.test(damaged, berries) |&gt; \n  model_parameters()\n\n4-sample test for equality of proportions without continuity correction\n\nProportion                        | Chi2(3) |     p\n---------------------------------------------------\n99.06% / 88.50% / 89.10% / 94.12% |   11.75 | 0.008\n\nAlternative hypothesis: two.sided\n\n\nWarum ist ein Test auf Anteile ein \\(\\mathcal{X}^2\\)-Test? Hierfür brauchen wir noch die Informationen zu den nicht beschädigten Erdbeeren.\n\nnon_damaged &lt;- berries - damaged \nnon_damaged\n\n A  B  C  D \n 1 13 17  6 \n\n\nNun können wir uns erstmal eine Tabelle bauen auf der wir dann den \\(\\mathcal{X}^2\\)-Test und den prop.test() rechnen können. Der \\(\\mathcal{X}^2\\)-Test ist nicht nur auf eine 2x2 Kreuztabelle beschränkt. Wir können in einem \\(\\mathcal{X}^2\\)-Test auch andere \\(n \\times m\\) Tabellen testen. Auf der anderen Seite ist der prop.test() auf eine \\(n \\times 2\\) Tabelle beschränkt. Es müssen also immer zwei Spalten sein.\n\ndamaged_tab &lt;- cbind(damaged, non_damaged) |&gt; \n  as.table()\ndamaged_tab\n\n  damaged non_damaged\nA     105           1\nB     100          13\nC     139          17\nD      96           6\n\n\nWir erhalten die Tabelle 34.4 mit den beobachteten Werten sowie die Tabelle 34.5 mit den erwarteten Werten. Die Berechnung der erwarteten Werte kennen wir schon aus dem klassischen \\(\\mathcal{X}^2\\)-Test. Hier machen wir die Berechnungen nur auf einer größeren Tabelle.\n\n\n\nTabelle 34.4— Eine 4x2 Tabelle für die vier Erntearten und der beschädigten Erdbeeren. Dargestellt sind die beobachteten Werte.\n\n\n\n\n\n\n\nDamaged\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\n\n\ndamaged\nnon_damaged\nberries\n\n\nGroup\nA\n\\(105\\)\n\\(1\\)\n\\(\\mathbf{106}\\)\n\n\n\nB\n\\(100\\)\n\\(13\\)\n\\(\\mathbf{113}\\)\n\n\n\nC\n\\(139\\)\n\\(17\\)\n\\(\\mathbf{156}\\)\n\n\n\nD\n\\(96\\)\n\\(6\\)\n\\(\\mathbf{102}\\)\n\n\n\n\n\\(\\mathbf{440}\\)\n\\(\\mathbf{37}\\)\n\\(n = 477\\)\n\n\n\n\n\n\nJetzt können wir auch die Anteile der beschädigten Erdbeeren von allen Erdbeeren berechnen pro Ernteart berechnen.\n\\[\n\\begin{aligned}\nPr(A) &= \\cfrac{105}{106} = 0.9906 = 99.06\\%\\\\\nPr(B) &= \\cfrac{100}{113} = 0.8850 = 88.50\\%\\\\\nPr(C) &= \\cfrac{139}{156} = 0.8910 = 89.10\\%\\\\\nPr(D) &= \\cfrac{96}{102} = 0.9411 = 94.12\\%\n\\end{aligned}\n\\]\n\n\n\nTabelle 34.5— Eine 4x2 Tabelle für die vier Erntearten und der beschädigten Erdbeeren. Dargestellt sind die zu erwartenden Werte, die sich aus den Randsummen ergeben würden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDamaged\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\n\n\ndamaged\nnon_damaged\nberries\n\n\nGroup\nA\n\\(\\cfrac{440 \\cdot 106}{477} = 97.78\\)\n\\(\\cfrac{37 \\cdot 106}{477} = 8.22\\)\n\\(\\mathbf{106}\\)\n\n\n\nB\n\\(\\cfrac{440 \\cdot 113}{477} = 104.23\\)\n\\(\\cfrac{37 \\cdot 113}{477} = 8.77\\)\n\\(\\mathbf{113}\\)\n\n\n\nC\n\\(\\cfrac{440 \\cdot 156}{477} = 143.90\\)\n\\(\\cfrac{37 \\cdot 156}{477} = 12.10\\)\n\\(\\mathbf{156}\\)\n\n\n\nD\n\\(\\cfrac{440 \\cdot 102}{477} = 94.09\\)\n\\(\\cfrac{37 \\cdot 102}{477} = 7.91\\)\n\\(\\mathbf{102}\\)\n\n\n\n\n\\(\\mathbf{440}\\)\n\\(\\mathbf{37}\\)\n\\(n = 477\\)\n\n\n\n\n\n\nSchauen wir uns nun an, ob es einen Unterschied zwischen den vier Erntearten A bis D für die Erdbeeren gibt. Einmal nutzen wir hierfür die Funktion chisq.test() und einmal die Funktion prop.test().\n\nchisq.test(damaged_tab) |&gt; \n  model_parameters()\n\nPearson's Chi-squared test\n\nChi2(3) |     p\n---------------\n11.75   | 0.008\n\nprop.test(damaged_tab) |&gt; \n  model_parameters()\n\n4-sample test for equality of proportions without continuity correction\n\nProportion                        | Chi2(3) |     p\n---------------------------------------------------\n99.06% / 88.50% / 89.10% / 94.12% |   11.75 | 0.008\n\nAlternative hypothesis: two.sided\n\n\nNachdem wir beide Funktionen gerechnet haben, sehen wir, dass beide Tests auf der \\(\\mathcal{X}^2\\) Statistik basieren. Das macht ja auch Sinn, denn wir rechnen ja die Proportions indem wir die beobachteten Werte durch die Gesamtzahl berechnen. Hier haben wir im Prinzip die gleiche Idee wie schon in den beiden obigen Beispielen umgesetzt. Wir können daher den \\(\\mathcal{X}^2\\)-Test auch einmal per Hand rechnen und kommen auf fast die gleiche \\(\\mathcal{X}^2\\)-Statistik. Wir haben eine leichte andere Statistik, da wir hier mehr runden.\n\\[\n\\begin{aligned}\n\\chi^2_{calc} &= \\cfrac{(105 - 97.78)^2}{97.78} + \\cfrac{(1 - 8.22)^2}{8.22} + \\\\\n&\\phantom{=}\\;\\; \\cfrac{(100 - 104.23)^2}{104.23} + \\cfrac{(13 - 8.77)^2}{8.77} + \\\\\n&\\phantom{=}\\;\\; \\cfrac{(139 - 143.90)^2}{143.90} + \\cfrac{(17 - 12.10)^2}{12.10} + \\\\\n&\\phantom{=}\\;\\; \\cfrac{(96 - 94.09)^2}{94.09} + \\cfrac{(6 - 7.91)^2}{7.91} = 11.74 \\approx 10.04\n\\end{aligned}\n\\]\nWir wissen nun, dass es mindestens einen paarweisen Unterschied zwischen den Wahrscheinlichkeiten für eine Beschädigung der Erdbeeren der vier Behandlungen gibt.\nFühren wir den Test nochmal detaillierter mit der Funktion prop_test() aus dem R Paket {rstatix} durch. Es handelt sich hier um eine Alternative zu der Standardfunktion prop.test(). Das Ergebnis ist das Gleiche, aber die Aufarbeitung der Ausgabe ist anders und manchmal etwas besser weiterverarbeiteten.\nDer Zugang ist etwas anders, deshalb bauen wir uns erstmal eine Tabelle mit den beschädigten und nicht beschädigten Erdbeeren. Dann benennen wir uns noch die Tabelle etwas um, damit haben wir dann einen besseren Überblick. Eigentlich unnötig, aber wir wollen uns hier ja auch mal mit der Programmierung beschäftigen.\n\ndamaged_tab &lt;- rbind(damaged, non_damaged) |&gt; \n  as.table()\ndimnames(damaged_tab) &lt;- list(\n  Infected = c(\"yes\", \"no\"),\n  Groups = c(\"A\", \"B\", \"C\", \"D\")\n)\ndamaged_tab \n\n        Groups\nInfected   A   B   C   D\n     yes 105 100 139  96\n     no    1  13  17   6\n\n\nDann können wir die Funktion prop_test() nutzen um den Test zu rechnen. Wir erhalten hier viele Informationen und müssen dann schauen, was wir dann brauchen. Dafür nutzen wir dann die Funktion select() und mutieren dann die Variablen in der Form, wie wir die Variablen haben wollen.\n\nprop_test(damaged_tab, detailed = TRUE) |&gt; \n  select(matches(\"estimate\"), p) |&gt; \n  mutate(p = pvalue(p)) |&gt; \n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 1 × 5\n  estimate1 estimate2 estimate3 estimate4 p    \n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;\n1      0.99      0.88      0.89      0.94 0.008\n\n\nWir erhalten auch hier das gleiche Ergebnis. War auch zu erwarten, denn im Kern sind die beiden Funktionen prop.test() und prop_test() gleich. Nun können wir uns einmal den paarweisen Vergleich anschauen. Wir wollen dann im Anschluß noch das Compact letter display für die Darstellung der paarweisen Vergleiche berechnen. Dafür brauchen wir dann noch folgende R Pakete.\n\n\nDu kannst mehr über das Compact letter display in dem Kapitel Multiple Vergleiche oder Post-hoc Tests erfahren.\n\npacman::p_load(rcompanion, multcompView, multcomp)\n\nDie Standardfunktion für die paarweisen Vergleiche von Anteilen in R ist die Funktion pairwise.prop.test(). Wir wollen hier einmal die \\(p\\)-Werte nicht adjustieren, deshalb schreiben wir auch p.adjust.method = \"none\".\n\npairwise.prop.test(damaged, berries, \n                   p.adjust.method = \"none\") \n\n\n    Pairwise comparisons using Pairwise comparison of proportions \n\ndata:  damaged out of berries \n\n  A      B      C     \nB 0.0035 -      -     \nC 0.0040 1.0000 -     \nD 0.1118 0.2264 0.2466\n\nP value adjustment method: none \n\n\nWir können auch die Funktion pairwise_prop_test() aus dem R Paket {rstatix} nutzen. Hier haben wir dann eine andere Ausgabe der Vergleiche. Manchmal ist diese Art der Ausgabe der Ergebnisse etwas übersichlicher. Wir nutzen dann noch die Funktion pvalue() um die \\(p\\)-Werte einmal besser zu formatieren.\n\npairwise_prop_test(damaged_tab, \n                   p.adjust.method = \"none\") |&gt; \n  mutate(p = pvalue(p.adj))\n\n# A tibble: 6 × 5\n  group1 group2 p        p.adj p.adj.signif\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 A      B      0.004  0.00354 **          \n2 A      C      0.004  0.00401 **          \n3 B      C      &gt;0.999 1       ns          \n4 A      D      0.112  0.112   ns          \n5 B      D      0.226  0.226   ns          \n6 C      D      0.247  0.247   ns          \n\n\nDann benutzen wir noch die Funktion multcompLetters() um uns das Compact letter display wiedergeben zu lassen.\n\npairwise.prop.test(damaged, berries, \n                   p.adjust.method = \"none\") |&gt; \n  pluck(\"p.value\") |&gt; \n  fullPTable() |&gt; \n  multcompLetters() |&gt; \n  pluck(\"Letters\")\n\n   A    B    C    D \n \"a\"  \"b\"  \"b\" \"ab\" \n\n\nHier sehen wir dann auch den Unterschied zwischen den beiden Funktionen. Wir können für die Funktion pairwise_prop_test() etwas einfacher das Compact letter display berechnen lassen. Wir müssen uns nur eine neue Spalte contrast mit den Vergleichen bauen.\n\npairwise_prop_test(damaged_tab, \n                   p.adjust.method = \"none\") |&gt; \n  mutate(contrast = str_c(group1, \"-\", group2)) |&gt; \n  pull(p, contrast) |&gt; \n  multcompLetters() \n\n   A    B    C    D \n \"a\"  \"b\"  \"b\" \"ab\" \n\n\nAm Ende sehen wir, dass sich die Behandlung \\(A\\) von den Behandlungen \\(B\\) und \\(C\\) unterscheidet, da sich die Behandlungen nicht den gleichen Buchstaben teilen. Die Behandlung \\(A\\) unterscheidet sich aber nicht von der Behandlung \\(D\\). In dieser Art und Weise können wir dann alle Behandlungen durchgehen. Du kannst mehr über das Compact letter display in dem Kapitel Multiple Vergleiche oder Post-hoc Tests erfahren.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Der $\\mathcal{X}^2$-Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-diagnostic.html",
    "href": "stat-tests-diagnostic.html",
    "title": "35  Der diagnostische Test",
    "section": "",
    "text": "35.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, \n               pROC, readxl)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Der diagnostische Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-diagnostic.html#die-daten-für-das-diagnostische-testen",
    "href": "stat-tests-diagnostic.html#die-daten-für-das-diagnostische-testen",
    "title": "35  Der diagnostische Test",
    "section": "35.2 Die Daten für das diagnostische Testen",
    "text": "35.2 Die Daten für das diagnostische Testen\nDie Datentabelle für das diagnostische Testen basiert wie der \\(\\chi^2\\)-Test auf der 2x2 Kreuztabelle oder Verfeldertafel. Es ist dabei unabdingbar, dass oben links in der 2x2 Kreuztabelle immer die \\(T^+\\) und \\(K^+\\) Werte stehen. Sonst funktionieren alle Formeln in diesem Kapitel nicht. Wir schauen uns auch immer das Schlechte an. Daher wollen wir immer wissen, ist der Hund krank? Ist der Hund tot? Ist der Hund weggelaufen? Beide Voraussetzung sind wichtig, damit wir mit der 2x2 Kreuztabelle wie in Tabelle 35.1 gezeigt rechnen können.\n\n\n\nTabelle 35.1— Eine 2x2 Tabelle oder Vierfeldertafel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKrank\n\n\n\n\n\n\n\\(K^+\\) (1)\n\\(K^-\\) (0)\n\n\n\nTest\n\\(T^+\\) (1)\n\\(TP_{\\;\\Large a}\\)\n\\(FN_{\\;\\Large b}\\)\n\\(\\mathbf{a+b}\\)\n\n\n\n\\(T^-\\) (0)\n\\(FP_{\\;\\Large c}\\)\n\\(TN_{\\;\\Large d}\\)\n\\(\\mathbf{c+d}\\)\n\n\n\n\n\\(\\mathbf{a+c}\\)\n\\(\\mathbf{b+d}\\)\n\\(\\mathbf{n}\\)\n\n\n\n\n\n\nWir wollen die Tabelle 35.1 mit einem Beispiel von Tollwut an Hauskatzen in ländlicher Umgebung. Die Katzen haben also Auslauf und können sich auch mit Tollwut infizieren. Wir wollen einen neuen, nicht invasiven Labortesten Tollda darauf überprüfen, wie gut der diagnostische Test Tollwut bei Katzen im Frühstadium erkennt.\nWir haben jetzt folgende Informationen erhalten:\n\nDer diagnostische Test TollDa ist positiv \\(T^+\\), wenn Tollwut vorliegt \\(K^+\\) , in 80% der Fälle.\nDer diagnostische Test TollDa ist positiv \\(T^+\\), wenn keine Tollwut vorliegt \\(K^-\\), in 9.5% der Fälle.\nAbschließend haben noch 2% der Katzen in ländlicher Umgebung Tollwut. Wir haben eine Prävalenz der Erkrankung in der betrachteten Population von 2%.\n\nDie Halterin einer Katze möchte nun wissen, wie groß ist die Wahrscheinlichkeit bei einem positiven Testergebnis, dass meine Katze Tollwut hat. Also die bedingte Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests.\n\n\n\n\n\n\nAbbildung 35.1— Visualisierung der Informationen zur Tollwutdiagnostik in einem Doppelbaum. Gefragt ist nach der Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests.\n\n\n\nAbbildung 35.1 visualisiert unsere Frage in einem Doppelbaum. Wir haben \\(10000\\) Katzen vorliegen. Ja wir waren fleißig und wir können damit besser rechnen. Du siehst aber auch, für die Diagnostik brauchen wir eine Menge Beobachtungen.\nVon den \\(10000\\) Katzen sind 2% also \\(200\\) wirklich mit Tollwut infiziert, also haben den Status \\(K^+\\). Damit sind \\(9800\\) Katzen gesund oder nicht krank und haben den Status \\(K^-\\). Wir wissen jetzt, dass 80% der \\(K^+\\) Katzen als positiv vom Test erkannt werden. Damit werden \\(200 \\cdot 0.8 = 160\\) Katzen \\(T^+\\). Im Umkehrschluss sind die anderen \\(40\\) Katzen dann \\(T^-\\). Von den \\(9800\\) gesunden Katzen werden 9.5% falsch als krank erkannt, also \\(9800 \\cdot 0.095 = 931\\) Katzen. Wiederum im Umkehrschluss sind dann \\(9069\\) richtig als gesunde Tiere erkannt.\nWir können nun den Doppelbaum nach unten ergänzen. Wir haben damit \\(1091 = 160 + 931\\) positiv getestete Katzen \\(T^+\\) sowie \\(9109 = 40 + 9069\\) negativ getestete Katzen \\(T^-\\).\nWie groß ist nun die Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests? Wir können die Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) direkt im Baum ablesen. Wir haben \\(160\\) kranke und positive Tier. Insgesamt sind \\(1091\\) Tiere positiv getestet. Daher Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests \\(\\tfrac{160}{1091} = 0.147\\) oder nur 14%.\nSammeln wir unsere Informationen um die Tabelle 35.1 zu füllen. Wir haben insgesamt \\(n = 10000\\) Katzen untersucht. In Tabelle 35.2 sehen wir die Ergebnisse unseres Tests auf Tollwut zusammengefasst. Wir haben \\(160\\) Katzen die Tollwut haben und vom Test als krank erkannt wurden. Dann haben wir \\(931\\) Katzen, die keine Tollwut hatten und vom Test als positiv erkannt wurden. Darüber hinaus haben wir \\(40\\) Katzen, die Tollwut haben, aber ein negatives Testergebnis. Sowie \\(8109\\) gesunde Katzen mit einem negativen Testergebnis.\n\n\n\nTabelle 35.2— Eine 2x2 Tabelle oder Vierfeldertafel gefüllt mit dem Beispiel aus dem Doppelbaum.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKrank\n\n\n\n\n\n\n\\(K^+\\) (1)\n\\(K^-\\) (0)\n\n\n\nTest\n\\(T^+\\) (1)\n160\n931\n\\(\\mathbf{a+b} = 1091\\)\n\n\n\n\\(T^-\\) (0)\n40\n9069\n\\(\\mathbf{c+d} = 9109\\)\n\n\n\n\n\\(\\mathbf{a+c} = 200\\)\n\\(\\mathbf{b+d} = 9800\\)\n\\(\\mathbf{n} = 10000\\)\n\n\n\n\n\n\nWie du sehen kannst ist die mittlere Reihe des Doppelbaums nichts anderes als die ausgefüllte 2x2 Kreuztabelle. In Abbildung 35.2 sehen wir die letzte Visualisierung des Zusammenhangs von Testscore auf der x-Achse und der Verteilung der tollwütigen Katzen \\(K^+\\) und der gesunden Katzen \\(K^-\\). Links und rechts von der Testenstscheidung werden Katzen falsch klassifiziert. Das heißt, die Katzen werden als krank oder gesund von dem Test erkannt, obwohl die Katzen diesen Status nicht haben. Ein idealer Test würde zwei Verteilungen der kranken und gesunden Tiere hervorbringen, die sich perfekt separieren lassen. Es gibt anhand des Testscores keine Überlappung der beiden Verteilungen\n\n\n\n\n\n\nAbbildung 35.2— Visualisierung des Zusammenhangs zwischen Testscore und den Verteilungen der kranken und gesunden Katzen sowie der Testenstscheidung. Links und rechts von der Testenstscheidung werden Katzen falsch klassifiziert.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Der diagnostische Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-diagnostic.html#confusion-matrix-deu.-fehlermatrix",
    "href": "stat-tests-diagnostic.html#confusion-matrix-deu.-fehlermatrix",
    "title": "35  Der diagnostische Test",
    "section": "35.3 Confusion matrix (deu. Fehlermatrix)",
    "text": "35.3 Confusion matrix (deu. Fehlermatrix)\nIn der Statistik beziehungsweise der Epidemiologie gibt es eine Vielzahl an statistischen Maßzahlen für die 2x2 Kreuztabelle. In der Fachsprache wird die 2x2 Kreuztabelle auch Confusion matrix genannt. Auf der Confusion Matrix können viele Maßzahlen berechnet werden wir konzentrieren uns hier erstmal auf die zwei wichtigsten Maßzahlen, nämlich der Spezifität und der Sensitivität.\nIn der wissenschaftlichen Fachsprache hat ein diagnostischer Test oder eine Methode, die erkrankte Personen sehr zuverlässig als krank (\\(K^+\\)) erkennt, eine hohe Sensitivität. Das heißt, sie übersieht kaum erkrankte Personen. Ein Test, der gesunde Personen zuverlässig als gesund (\\(K^-\\)) einstuft, hat eine hohe Spezifität. Wir können die Spezifität und die Sensitivität auf der 2x2 Kreuztabelle wir folgt berechnen.\n\\[\n\\mbox{Sensitivität} = \\mbox{sens} = \\cfrac{TP}{TP + FN} = \\cfrac{160}{160 + 931} = 0.147\n\\]\n\\[\n\\mbox{Spezifität} = \\mbox{spec} = \\cfrac{TN}{TN + FP} = \\cfrac{8869}{8869 + 40} = 0.995\n\\]\nBeide statistischen Maßzahlen benötigen wir im Besonderen bei der Erstellung der Reciever Operator Curve (ROC) im folgenden Abschnitt.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Der diagnostische Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-diagnostic.html#receiver-operating-characteristic-roc",
    "href": "stat-tests-diagnostic.html#receiver-operating-characteristic-roc",
    "title": "35  Der diagnostische Test",
    "section": "35.4 Receiver Operating Characteristic (ROC)",
    "text": "35.4 Receiver Operating Characteristic (ROC)\nDie Reciever Operator Curve (ROC) ist die Abbildung wenn es darum geht darzustellen wie gut eine Klassifikation funktioniert hat. Wir werden die Abbildung später im maschinellen Lernen und der Klssifikation wiedertreffen. Da die ROC Kurve aber ursprünglich zum diagnostischen Testen gehört, kommt die ROC Kurve hier auch rein.\n\n35.4.1 Daten für die Receiver Operating Characteristic (ROC)\nSchauen wir uns erstmal ein Beispiel für die ROC Kurve an. In Tabelle 35.3 sehen wir Beispieldaten von vierzehn Hunden. Von den vierzehn Hunden haben sieben eine verdeckte Flohinfektion im Anfangsstadium und sieben Hunde sind flohfrei und gesund. Daher haben wir sieben kranke Hunde (\\(K^+\\) oder Infektionsstatus ist \\(1\\)) und sieben gesunde Hunde (\\(K^-\\) oder Infektionsstatus ist \\(0\\)). Wir testen jeden der Hunde mit dem Flohindikatormittel FleaDa. Das Flohindikatormittel gibt einen Farbwert als test_score zwischen \\(0\\) und \\(1\\) wieder. Wir wollen nun herausfinden, ob wir mit dem test_score die vierzehn Hunde korrekt nach ihrem Krankheitszustand klassifizieren können.\n\n\n\n\nTabelle 35.3— Datensatz für vierzehn Hunde mit Flohinfektion und keiner Flohinfektion sowie dem Testscore des Flohindikatormittels fleaDa.\n\n\n\n\n\n\ninfected\ntest_score\n\n\n\n\n1\n0.100\n\n\n1\n0.150\n\n\n1\n0.250\n\n\n1\n0.300\n\n\n1\n0.400\n\n\n1\n0.500\n\n\n1\n0.600\n\n\n0\n0.350\n\n\n0\n0.425\n\n\n0\n0.470\n\n\n0\n0.550\n\n\n0\n0.700\n\n\n0\n0.800\n\n\n0\n0.820\n\n\n\n\n\n\n\n\nWir sehen die Daten visualisiert in Abbildung 35.3. Auf der \\(y\\)-Achse ist der binäre Endpunkt \\(infected\\) und auf der \\(x\\)-Achse der Testscore des Flohindikators FleaDa.\n\n\n\n\n\n\n\n\nAbbildung 35.3— Aufteilung der kranken und gesunden Hunde nach Infektionsstatus und dem Testscore.\n\n\n\n\n\nDie Idee der ROC Kurve ist nun für jeden möglichen Wert des Testscores die Spezifität und Sensitivität zu berechnen. Wir müssen das aber nicht für alle Werte des Testscores machen, sondern nur für die Wert bei denen sich der Status einer Beobachtung anhand des Testscores ändern würde. Klingt ein wenig schräg, schauen wir es uns einmal an. Zuerst brauchen wir jeweils die Grenzen an denen wir für den Testscore eine Klassifikation machen. In Abbildung 35.4 sehen wir die Grenzen als gelbe Linie.\n\n\n\n\n\n\n\n\nAbbildung 35.4— Aufteilung der kranken und gesunden Hunde nach Infektionsstatus und dem Testscore.\n\n\n\n\n\nWir können jetzt für jede gelbe Linie als Threshold des Testscore die Spezifität und die Sensitivität berechen. berechnen Tabelle 35.4 sind die Werte von Spezifität und Sensitivität für jede gelbe Linie ausgegeben. Wenn wir als Entscheidungsgrenze einen Testscore von 0.12 nehmen würden, dann würden wir 1 Hund als krank und 13 als gesund klassifizieren. Wir hätten 7 TN, 1 TP, 6 FN und 0 FP. Aus diesen Zahlen, die eine 2x2 Kreuztabelle entsprechen, können wir dann die Spezifität und die Sensitivität berechnen. Wir berechnen dann die Spezifität und die Sensitivität für jeden Threshold.\n\n\n\n\nTabelle 35.4— Die Spezifität und die Sensitivität berechnet für jeden Threshold.\n\n\n\n\n\n\n\nthreshold\nspecificity\nsensitivity\ntn\ntp\nfn\nfp\n\n\n\n\n2\n0.12\n1.00\n0.14\n7\n1\n6\n0\n\n\n3\n0.20\n1.00\n0.29\n7\n2\n5\n0\n\n\n4\n0.28\n1.00\n0.43\n7\n3\n4\n0\n\n\n5\n0.32\n1.00\n0.57\n7\n4\n3\n0\n\n\n6\n0.38\n0.86\n0.57\n6\n4\n3\n1\n\n\n7\n0.41\n0.86\n0.71\n6\n5\n2\n1\n\n\n8\n0.45\n0.71\n0.71\n5\n5\n2\n2\n\n\n9\n0.48\n0.57\n0.71\n4\n5\n2\n3\n\n\n10\n0.52\n0.57\n0.86\n4\n6\n1\n3\n\n\n11\n0.58\n0.43\n0.86\n3\n6\n1\n4\n\n\n12\n0.65\n0.43\n1.00\n3\n7\n0\n4\n\n\n13\n0.75\n0.29\n1.00\n2\n7\n0\n5\n\n\n14\n0.81\n0.14\n1.00\n1\n7\n0\n6\n\n\n\n\n\n\n\n\nWir können jetzt für jeden Threshold und damit jedes verbundene Spezifität und Sensitivität Paar einen Punkt in einen Plot einzeichnen. Wir lassen damit Stück für Stück die ROC Kurve wachsen.\nIn der Abbildung 35.5 sehen wir die ROC Kurve mit dem Threshold von 0.32 vorliegen. Wir haben damit 7 TN, 4 TP, 3 FN und 0 FP klassifizierte Beobchtungen. Darauf ergibt sich eine Spezifität von 1 und eine Sensitivität von 0.57. In der ROC Kurve tragen wir auf die \\(x\\)-Achse die 1 - Spezifitätswerte auf. Damit haben wir eine schön ansteigende Kurve.\n\n\n\n\n\n\nAbbildung 35.5— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.32 und damit 7 TN, 4 TP, 3 FN und 0 FP.\n\n\n\nIn der Abbildung 35.6 istr die ROC Kurve schon nach rechts gewachsen, da wir die ersten falsch positiv (FP) klassifizierten Beobachtungen bei einem Threshold von 0.48 erhalten.\n\n\n\n\n\n\nAbbildung 35.6— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.48 und damit 4 TN, 5 TP, 2 FN und 3 FP.\n\n\n\nIn Abbildung 35.7 sind wir mit einem Threshold von 0.65 schon fast am rechten Rand der Verteilung des Testscores angekommen. Wir haben nur noch 3 richtig negative (TN) Beobachtungen, die jetzt mit steigenden Threshold alle zu falsch positiven (FP) klassifiziert werden.\n\n\n\n\n\n\nAbbildung 35.7— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.65 und damit 3 TN, 7 TP, 0 FN und 4 FP.\n\n\n\nDie ROC Kurve endet dann oben rechts in der Abbildung und startet immer unten links. Wir können noch die Fläche unter der ROC Kurve berechnen. Wir nennen diese Fläche unter der Kurve AUC (eng. area under the curce) und wir brauchen diese Maßzahl, wenn wir verschiedene Testscores und die entsprechenden ROC Kurven miteinander vergleichen wollen. Eine AUC von 0.5 bedeutet, dass unser Testscore nicht in der Lage war die kranken von den gesunden Hunden zu trennen. Ein AUC von 1 bedeutet eine perfekte Trennung der beiden Gruppen durch den Testscore. Ein AUC von 0.5 bedeutet somit auch keine Trennung der beiden Gruppen durch den Testscore.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Der diagnostische Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-diagnostic.html#reciever-operator-curve-roc-in-r",
    "href": "stat-tests-diagnostic.html#reciever-operator-curve-roc-in-r",
    "title": "35  Der diagnostische Test",
    "section": "35.5 Reciever Operator Curve (ROC) in R",
    "text": "35.5 Reciever Operator Curve (ROC) in R\nWir berechnen die ROC Kurve nicht per Hand. Das wäre eine sehr große Fleißarbeit für jeden Threshold zwischen den Beobachtungen die jeweilige Spezifität und Sensitivität Paare zu berechnen. Wir nutzen daher das R Paket {pROC}. Das Pakt erlaubt es uns später auch recht einfach ROC Kurven miteinander zu vergleichen und einen statistischen Test zu rechnen.\nDie Daten von den obigen Beispiel sind in der Datei roc_data.xlsx gespeichert und können dann einfach verwendet werden. Die Funktion roc() akzeptiert ein binäres \\(y\\) wo bei die \\(1\\) das Schlechte bedeutet und die \\(0\\) die Abwesenheit von dem Schlechten. Also eben krank oder gesund wie oben beschrieben. Wir wollen dann noch die AUC und die 95% Konfidenzintervalle für das AUC wiedergegeben haben.\n\nroc_tbl &lt;- read_excel(\"data/roc_data.xlsx\") \n\nroc_obj &lt;- roc(infected ~ test_score, roc_tbl, auc = TRUE, ci = TRUE)\n\nroc_obj\n\n\nCall:\nroc.formula(formula = infected ~ test_score, data = roc_tbl,     auc = TRUE, ci = TRUE)\n\nData: test_score in 7 controls (infected 0) &gt; 7 cases (infected 1).\nArea under the curve: 0.8367\n95% CI: 0.6176-1 (DeLong)\n\n\nWir sehen, dass wir eine AUC von 0.837 haben, was auf eine gute Trennschärfe des Tests hindeutet. Wäre die AUC näher an 0.5 dann würden wir sagen, dass der Test die kranken Hunde nicht von den gesunden Hunden unterscheiden kann.\nIm nächsten Schritt extrahieren wir noch alle wichtigen Informationen aus dem Objekt roc_obj damit wir die ROC Kurve in {ggplot} zeichnen können. Das Paket {pROC} hat auch eine eigne Plotfunktion. Es gibt eine Reihe von Screenshots vom {pROC} Paket, wo du noch andere Möglichkeiten siehst. Einfach mal ausprobieren.\n\nroc_res_tbl &lt;- roc_obj |&gt; \n  coords(ret = \"all\") |&gt; \n  select(specificity, sensitivity)\n\nDas Objekt roc_obj nutzen wir jetzt um die ROC Kurve einmal darzustellen. Achte drauf, dass auf der \\(x\\)-Achse die 1-Spezifität Werte stehen. Wir erhalten die ROC Kurve wie in Abbildung 35.8 dargestellt. Wie zeichnen noch die Diagonale ein. Wenn die ROC nahe an der Diagonalen verläuft, dann ist der Testscore nicht in der Lage die Kranken von den Gesunden zu trennen. Eine perfekte ROC Kurve läuft senkrecht nach oben und dann waagerecht nach rechts und es ergibt sich eine AUC von 1.\n\nggplot(roc_res_tbl, aes(1-specificity, sensitivity)) +\n  geom_path() +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.75, color = \"red\") +\n  theme_minimal() +\n  labs(x = \"1 - Spezifität\", y = \"Sensitivität\")\n\n\n\n\n\n\n\nAbbildung 35.8— Abbildung der ROC Kurve.\n\n\n\n\n\nIn späteren Kapiteln zur Klassifikation werden wir auch verschiedene ROC Kurven zusammen darstellen und miteinander Vergleichen. Dazu nutzen wir dann auch die ROC Kurve und die Möglichkeit auch einen roc.test() zu rechnen. Mehr dazu gibt es soweit erstmal auf der Hilfeseite des R Paketes {pROC} mit vielen Screenshots vom {pROC} Paket. Für dieses Kapitel soll die Darstellung einer ROC Kurve genügen.\n\n\n\nAbbildung 35.1— Visualisierung der Informationen zur Tollwutdiagnostik in einem Doppelbaum. Gefragt ist nach der Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests.\nAbbildung 35.2— Visualisierung des Zusammenhangs zwischen Testscore und den Verteilungen der kranken und gesunden Katzen sowie der Testenstscheidung. Links und rechts von der Testenstscheidung werden Katzen falsch klassifiziert.\nAbbildung 35.3— Aufteilung der kranken und gesunden Hunde nach Infektionsstatus und dem Testscore.\nAbbildung 35.4— Aufteilung der kranken und gesunden Hunde nach Infektionsstatus und dem Testscore.\nAbbildung 35.5— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.32 und damit 7 TN, 4 TP, 3 FN und 0 FP.\nAbbildung 35.6— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.48 und damit 4 TN, 5 TP, 2 FN und 3 FP.\nAbbildung 35.7— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.65 und damit 3 TN, 7 TP, 0 FN und 4 FP.\nAbbildung 35.8— Abbildung der ROC Kurve.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Der diagnostische Test</span>"
    ]
  },
  {
    "objectID": "stat-tests-permutationstest.html",
    "href": "stat-tests-permutationstest.html",
    "title": "36  Permutationstest & Bootstraping",
    "section": "",
    "text": "36.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, conflicted, broom)\nconflicts_prefer(dplyr::filter)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Permutationstest & Bootstraping</span>"
    ]
  },
  {
    "objectID": "stat-tests-permutationstest.html#daten",
    "href": "stat-tests-permutationstest.html#daten",
    "title": "36  Permutationstest & Bootstraping",
    "section": "36.2 Daten",
    "text": "36.2 Daten\nFür unsere Demonstration des Permutationstest nutzen wir den Datensatz flea_dog_cat_length_weight.xlsx. Zum einen wollen wir einen Gruppenvergleich zwischen den Sprungweiten der Hunde- und Katzenflöhe rechnen. Also einen klassischen t-Test für einen Gruppenvergleich. Nur eben hier als einen Permutationstest. Als zweites wollen wir einen \\(p\\)-Wert für das Bestimmtheitsmaß \\(R^2\\) abschätzen. Per se gibt es keinen \\(p\\)-Wert für das Bestimmtheitsmaß \\(R^2\\), aber der Permutationstest liefert hier eine Lösung für das Problem. Daher schauen wir uns in einer simplen linearen Regression den Zusammenhang zwischen einem \\(y\\) und einem \\(x_1\\) an. Daher wählen wir aus dem Datensatz die beiden Spalten jump_length und weight. Wir wollen nun feststellen, ob es einen Zusammenhang zwischen der Sprungweite in [cm] und dem Flohgewicht in [mg] gibt. In dem Datensatz finden wir 400 Flöhe wir wählen aber nur zufällig 20 Tiere aus.\n\nmodel_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") |&gt;\n  select(animal, jump_length, weight) |&gt; \n  filter(animal %in% c(\"dog\", \"cat\")) |&gt; \n  slice_sample(n = 20)\n\nNeben dem einfachen Datensatz zu den Hunde- und Katzenflöhen haben wir noch Daten zu dem Wachstum von Wasserlinsen. Wir haben einmal händisch die Dichte bestimmt duckweeds_density und einmal mit einem Sensor gemessen. Dabei sind die Einheiten der Sensorwerte erstmal egal, wir wollen aber später eben nur mit einem Sensor messen und dann auf den Wasserlinsengehalt zurückschließen. Wir haben hier eher eine Sätigungskurve vorliegen, denn die Dichte der Wasserlinsen ist ja von der Oberfläche begrenzt. Auch können sich die Wasserlinsen nicht beliebig teilen, es gibt ja nur eine begrenzte Anzahl an Ressourcen.\n\nduckweeds_tbl &lt;- read_excel(\"data/duckweeds_density.xlsx\")\n\nIn der Tabelle 54.2 siehst du dann einmal einen Auszug aus den Daten zu den Wasserlinsen. Es ist ein sehr einfacher Datensatz mit nur zwei Spalten. Wie du siehst, scheint sich hierbei um eine nicht lineare Regression zu handeln. Einen linearen Zusammenhang würde ich hier nicht annehmen.\n\n\n\n\nTabelle 36.1— Auszug aus Wasserlinsendatensatz.\n\n\n\n\n\n\nduckweeds_density\nsensor\n\n\n\n\n4.8\n0.4303\n\n\n4.8\n0.4763\n\n\n4.8\n0.4954\n\n\n…\n…\n\n\n53.2\n2.1187\n\n\n53.2\n2.1296\n\n\n53.2\n2.1246\n\n\n\n\n\n\n\n\nAuch die Wasserlinsendaten wollen wir uns erstmal in einer Abbildung anschauen und dann sehen, ob wir eine Kurve durch die Punkte gelegt kriegen. Für die Visualisierung der Wasserlinsendaten in der Abbildung 54.10 verzichte ich einmal auf die logarithmische Darstellung. Auffällig ist erstmal, dass wir sehr viel weniger Beobachtungen und auch Dichtemesspunkte auf der \\(x\\)-Achse haben. Wir haben dann zu den jeweiligen Wasserlinsendichten dann drei Sensormessungen. Das könnte noch etwas herausfordernd bei der Modellierung werden.\n\nggplot(duckweeds_tbl, aes(duckweeds_density, sensor)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Gemessene Dichte der Wasserlinsen\", y = \"Sensorwert\")\n\n\n\n\n\n\n\nAbbildung 36.1— Visualisierung der Sensorwerte nach Wasserlinsendichte. Pro Dichtewert liegen drei Sensormessungen vor.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Permutationstest & Bootstraping</span>"
    ]
  },
  {
    "objectID": "stat-tests-permutationstest.html#einfacher-mittelwertsvergleich",
    "href": "stat-tests-permutationstest.html#einfacher-mittelwertsvergleich",
    "title": "36  Permutationstest & Bootstraping",
    "section": "36.3 Einfacher Mittelwertsvergleich",
    "text": "36.3 Einfacher Mittelwertsvergleich\nJetzt wollen wir uns den Permutationstest einmal ganz einfach anschauen. Wir wollen einen t-Test für den Mittelwertsvergleich einmal händisch mit der Funktion sample() nachbauen. Dann schauen wir uns die Sachlage einmal in dem R Paket {infer} an.\n\n36.3.1 …mit sample()\nWir wollen zuerst einmal mit einem einfachen Mittelwertsvergleich anfangen. Im Prinzip bauen wir hier kompliziert einen t-Test nach. Der t-Test testet, ob es einen signifikanten Mittelwertsunterschied gibt. Anstatt jetzt den t-Test zu rechnen, berechnen wir erstmal das \\(\\Delta\\) und damit den Mittelwertsunterschied der Sprungweiten der Hunde- und Katzenflöhe.\n\nmodel_tbl |&gt;\n  group_by(animal) |&gt; \n  summarise(mean_jump = mean(jump_length)) |&gt; \n  pull(mean_jump) |&gt; \n  diff()\n\n[1] 3.376154\n\n\nWir sehen, dass die Hunde- und Katzenflöhe im Mittel einen Unterschied in der Sprungweite von \\(3.38cm\\) haben. Das ist der Mittelwertsunterschied in unseren beobachteten Daten.\nJetzt wollen wir einmal einen Permutationstest rechnen. Die wichtigste Funktion hierfür ist die Funktion sample(). Die Funktion sample() durchmischt zufällig einen Vektor. Einmal ein Beispiel für die Zahlen 1 bis 10, die wir in die Funktion sample() pipen.\n\nc(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) |&gt; \n  sample()\n\n [1]  4  5  8  1  3  9  2  7  6 10\n\n\nDas gleiche Durchmischen findet auch in der Funktion mutate() statt. Wir durchwirblen einmal die Zuordnung der Level des Faktors animal zu den jeweiligen Speungweiten. Dann berechnen wir die Mittelwertsdifferenz für diesen neuen Datensatz. Das machen wir dann \\(n\\_sim\\) gleich 1000 Mal.\n\nn_sim &lt;- 1000\n\nmean_perm_tbl &lt;- map_dfr(1:n_sim, \\(x) {\n  mean_diff &lt;- model_tbl |&gt;\n    ## Permutation Start\n    mutate(animal = sample(animal)) |&gt; \n    ## Permutation Ende\n    group_by(animal) |&gt; \n    summarise(mean_jump = mean(jump_length)) |&gt; \n    pull(mean_jump) |&gt; \n    diff()\n  return(tibble(mean_diff))\n})\n\nIn der Abbildung 36.2 sehen wir die Verteilung aller Mittelwertsdifferenzen, die aus unserem permutierten Datensätzen herausgekommem sind.\n\n\n\n\n\n\n\n\nAbbildung 36.2— Histogramm der permutierten Mittelwertsdifferenzen\n\n\n\n\n\n\nsum(mean_perm_tbl$mean_diff &gt;= 2.6119)/n_sim\n\n[1] 0.054\n\n\nIst das Gleiche als wenn wir dann den Mittelwert berechnen.\n\nmean(mean_perm_tbl$mean_diff &gt;= 2.6119) \n\n[1] 0.054\n\n\n\n\nTeilweise wird diskutiert, ob der \\(p\\)-Wert hier noch mal 2 genommen werden muss, da wir ja eigentlich zweiseitig Testen wollen, aber da gehen die Meinungen auseinander. Ich belasse es so wie hier.\nDann das ganze nochmal mit einem Student t-Test verglichen und wir sehen, dass wir dem \\(p\\)-Wert aus einem Student t-Test sehr nahe kommen. Wenn du jetzt noch die Anzahl an Simulationen erhöhen würdest, dann würde sich der \\(p_{perm}\\) dem \\(p_{t-Test}\\) immer weiter annähern.\n\nt.test(jump_length ~ animal, data = model_tbl) |&gt; \n  pluck(\"p.value\") |&gt; \n  round(3)\n\n[1] 0.049\n\n\nAm Ende bleibt dann die Frage, wie viele Permutationen sollen es denn sein? Auch hier sehen wir dann, dass der t-Test signifikant ist, aber der Permutationstest noch nicht. Vielleicht helfen da mehr Permutationen? Oder aber der Effekt ist dann doch zu gering. Hier musst du dann immer überlegen, ob du nicht zu sehr an dem Signifikanzniveau von 5% klebst. Am Ende muss dann der permutierte \\(p\\)-Wert zudammen mit dem Effekt im Kontext der Fragestellung diskutiert werden.\n\n\n36.3.2 … mit {infer}\nHier wollen wir noch das R Paket {infer} vorstellen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Permutationstest & Bootstraping</span>"
    ]
  },
  {
    "objectID": "stat-tests-permutationstest.html#bestimmtheitsmaß-r2",
    "href": "stat-tests-permutationstest.html#bestimmtheitsmaß-r2",
    "title": "36  Permutationstest & Bootstraping",
    "section": "36.4 Bestimmtheitsmaß \\(R^2\\)",
    "text": "36.4 Bestimmtheitsmaß \\(R^2\\)\nDas vorherige Beispiel mit dem Mittelwertsvergleich war im Prinzip nur eine Fingerübung für den Ablauf. Wir können auch einfach einen t-Test rechnen und dann ist gut. Anders sieht es aus, wenn wir keinen \\(p\\)-Wert geliefert bekommen und auch keinen \\(H_0\\) Testverteilung bekannt ist um einen \\(p\\)-Wert zu bestimmen. Das ist der Fall bei dem Bestimmtheitsmaß \\(R^2\\). Wir haben hier keine Teststatistik und dadurch einen \\(p\\)-Wert vorliegen. Dagegen können wir dann mit einem Permutationstest was tun. Bei dem 95% Konfidenzintervall wird es dann schwieriger, hier müssen wir dann etwas anders permutieren. Wir nutzen dann die Bootstrap Konfidenzintervalle im nächsten Abschnitt.\n\nmodel_tbl %$%\n  lm(jump_length ~ weight) |&gt; \n  glance() |&gt; \n  pull(r.squared)\n\n[1] 0.3000498\n\n\nDamit haben wir erstmal das Bestimmtheitsmaß aus unseren Daten berechnet. Jetzt stellt sich natürlich die Frage, wie wahrscheinlich ist es den dieses Bestimmtheitsmaß von 0.30 zu beobachten? Dafür lassen wir jetzt einen Permutationstest laufen in dem wir die Daten einmal durchmischen.\n\nn_sim &lt;- 1000\n\nr2_perm_tbl &lt;- map_dfr(1:n_sim, \\(x) {\n  r2 &lt;- model_tbl |&gt;\n    ## Permutation Start\n    mutate(weight = sample(weight)) %$%\n    ## Permutation Ende    \n    lm(jump_length ~ weight) |&gt; \n    glance() |&gt; \n    pull(r.squared)\n  return(tibble(r2))\n})\n\nIn der Abbildung 36.3 sehen wir die Verteilung aller Bestimmtheitsmaße \\(R^2\\), die aus unserem permutierten Datensätzen herausgekommem sind. Wir erkennen sofort, dass es wenig zufällig bessere Datensätze gibt, die ein höheres Bestimmtheitsmaße \\(R^2\\) erzeugen.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nAbbildung 36.3— Histogramm der permutierten Bestimmtheitsmaße \\(R^2\\)\n\n\n\n\n\nJetzt wollen wir einmal bestimmen wie viele Bestimmtheitsmaße \\(R^2\\) größer sind als unser Bestimmtheitsmaß \\(R^2 = 0.29\\) aus den Daten.\n\nsum(r2_perm_tbl$r2 &gt;= 0.291164)/n_sim \n\n[1] 0.013\n\n\nDie Berechnung ist das Gleiche, als wenn wir den Mittelwert aus der logischen Abfrage berechnen würden.\n\nmean(r2_perm_tbl$r2 &gt;= 0.291164) \n\n[1] 0.013\n\n\nTeilweise wird diskutiert, ob der \\(p\\)-Wert hier noch mal 2 genommen werden muss, da wir ja eigentlich zweiseitig Testen wollen, aber da gehen die Meinungen auseinander. Ich belasse es so wie hier. Damit haben wir unseren \\(p\\)-Wert für das Bestimmtheitsmaß \\(R^2\\) mit \\(0.013\\). Das ist was wir wollten und somit können wir dann auch sagen, dass wir einen signifikantes Bestimmtheitsmaß \\(R^2\\) vorliegen haben. Was noch fehlt ist das 95% Konfidenzintervall, was wir Mithilfe des Bootstrapverfahrens berechnen wollen.",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Permutationstest & Bootstraping</span>"
    ]
  },
  {
    "objectID": "stat-tests-permutationstest.html#bootstrap",
    "href": "stat-tests-permutationstest.html#bootstrap",
    "title": "36  Permutationstest & Bootstraping",
    "section": "36.5 Bootstrap",
    "text": "36.5 Bootstrap\nBootstrap oder Bootstraping ist eine mächtige Methode um Vertrauensbereiche zu bestimmen. Wir wollen mehr oder minder wissen, in welchen Bereich unsere Beobachtungen fallen könnten, wenn wir unser Experiment wiederholen. Wir haben ja meistens nur einen sehr kleinen Datensatz, der nur einen Ausschnitt repräsentiert. Durch Bootstrap können wir jetzt auch ohne eine geschlossene mathematische Formel für die Konfidenzintervalle oder Prädiktionsintervalle die jeweiligen Intervalle berechnen. Dennoch musst du beachten, dass wir ein paar Beobachtungen brauchen. Wenn du nur fünf Beobachtungen pro Gruppe hast, wird es meistens sehr schnell sehr eng mit dem Bootstrapverfahren.\n\n36.5.1 95% Konfidenzintervalle\nWir können die Methode des Bootstraping nutzen um uns die 95% Konfidenzintervalle über eine Simulation bzw. Permutation berechnen zu lassen. Haben wir in dem Permutatiosntest noch die Spalten permutiert so permutieren wir bei dem Bootstrap-Verfahren die Zeilen. Da wir aber keinen neuen Datensatz erhalten würden, wenn wir nur die Zeilen permutieren, ziehen wir einen kleineren Datensatz mit zurücklegen. Das heißt, dass wir auch Beobachtungen mehrfach in unseren gezogenen Bootstrapdatensatz haben können. Wir nutzen in R die Funktion slice_sample() in der wir dann auswählen, dass 80% der Beobachtungen mit zurücklegen gezogen werden sollen. Das Zurücklegen können wir mit der Option replace = TRUE einstellen. Wir führen das Bootstraping dann insgesamt 1000 mal durch.\n\nn_boot &lt;- 1000\n\nr2_boot_tbl &lt;- map_dfr(1:n_boot, \\(x) {\n  r2 &lt;- model_tbl |&gt;\n    ## Bootstrap Start\n    slice_sample(prop = 0.8, replace = TRUE) %$%\n    ## Bootstrap Ende\n    lm(jump_length ~ weight) |&gt; \n    glance() |&gt; \n    pull(r.squared)\n  return(tibble(r2))\n})\n\nNachdem wir nun unser Bootstrap gerechnet haben und eintausend Bestimmtheitsmaße bestimmt haben, können wir einfach das \\(2.5\\%\\) und \\(97.5\\%\\) Quantile bestimmen um unser 95% Konfidenzintervall zu bestimmen. Zwischen \\(2.5\\%\\) und \\(97.5\\%\\) liegen ja auch 95% der Werte der eintausend Bestimmtheitsmaße.\n\nr2_boot_tbl %$% \n  quantile(r2, probs = c(0.025, 0.975)) |&gt; \n  round(3)\n\n 2.5% 97.5% \n0.016 0.620 \n\n\nWir hatten ein Bestimmtheitsmaß \\(R^2\\) von \\(0.30\\) berechnet und können dann die untere und obere 95% Konfidenzgrenze ergänzen. Wir erhaltend dann \\(0.300\\, [0.016; 0.620]\\), somit liegt unser beobachtetes Bestimmtheitsmaß \\(R^2\\) mit 95% Sicherheit zwischen \\(0.016\\) und \\(0.620\\).\n\n\n36.5.2 Prädiktionsintervall\nIdee vom Botstraping und Resample Working with resampling sets\nBootstrap confidence intervals für nicht-lineare Daten für unseren Datensatz Tabelle 54.2\nDas R Paket {resample}\nR Paket {workboots}\nUnderstanding Prediction Intervals\nSimulating Prediction Intervals\nQuantile Regression Forests for Prediction Intervals\nP-values for prediction intervals machen keinen Sinn\nR Paket {spin}",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Permutationstest & Bootstraping</span>"
    ]
  },
  {
    "objectID": "stat-tests-permutationstest.html#referenzen",
    "href": "stat-tests-permutationstest.html#referenzen",
    "title": "36  Permutationstest & Bootstraping",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 36.1— Visualisierung der Sensorwerte nach Wasserlinsendichte. Pro Dichtewert liegen drei Sensormessungen vor.\nAbbildung 36.2— Histogramm der permutierten Mittelwertsdifferenzen\nAbbildung 36.3— Histogramm der permutierten Bestimmtheitsmaße \\(R^2\\)\n\n\n\nFay DS, Gerow K. 2018. A biologist’s guide to statistical thinking and analysis. WormBook: The Online Review of C. elegans Biology [Internet].",
    "crumbs": [
      "Statistische Gruppenvergleiche",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Permutationstest & Bootstraping</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-preface.html",
    "href": "stat-linear-reg-preface.html",
    "title": "Grundlagen des Modellierens",
    "section": "",
    "text": "Letzte Änderung am 15. February 2024 um 14:48:38\n\n“Curiosity is the beginning of knowledge. Action is the beginning of change.” — James Clear\n\nIn diesem Teil des Buches und den folgenden Kapiteln wollen wir uns mit der Modellierung von Daten beschäftigen. Wir fangen erstmal mit einem simplen linearen Modell an und erweitern dann dieses Modell zu einem multiplen Modell. Das heißt, wir haben ein Outcome \\(y\\), was normalverteilt ist, sowie eine Einflussvariable \\(x_1\\), die eine kontinuierliche Variable ist. Wir wollen jetzt herausfinden, welchen Einfluss oder Effekt das \\(x_1\\) auf das \\(y\\) hat. Sehr simple Gesprochen legen wir eine Gerade durch eine Punktewolke. Im Weiteren konzentrieren wir uns hierbei aber erst einmal auf ein normalverteiltes Outcome mit einer Gaussian linearen Regression.\nDabei hat ein simples lineares Modell nur eine Einflussvariable \\(x_1\\).\n\\[\ny \\sim x_1\n\\]\nEin multiples lineares Modell hat mehrere Einflussvariablen \\(x_1, ..., x_p\\).\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nViele der Konzepte in diesem Teil des Buches brauchen wir in den folgenden Kapiteln des Teils zum statistischen Modellieren. Wir lernen hier also eher die Aufwärmübungen und Konzepte um dann multiple lineare Regressionen mit anderen Verteilungsfamilien als der Normalverteilung rechnen zu können. In den seltensten Fällen reichen simple lineare Modell aus, um die Realität abzubilden. Fangen wir also mit den Grundlagen an und bauen dann systematisch die Konzepte der simplen linearen Regression auf.",
    "crumbs": [
      "Grundlagen des Modellierens"
    ]
  },
  {
    "objectID": "stat-linear-reg-basic.html",
    "href": "stat-linear-reg-basic.html",
    "title": "37  Simple lineare Regression",
    "section": "",
    "text": "37.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom,\n               readxl, conflicted)\nconflicts_prefer(magrittr::set_names)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Simple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-basic.html#daten",
    "href": "stat-linear-reg-basic.html#daten",
    "title": "37  Simple lineare Regression",
    "section": "37.2 Daten",
    "text": "37.2 Daten\nWir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die lineare Regression auf sehr großen Datensätzen anwenden, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit \\(n=7\\) Beobachtungen. In der Tabelle 37.1 ist der Datensatz simplel_tbl dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in [cm] und dem Gewicht in [mg] für sieben Beobachtungen modellieren.\n\nsimple_tbl &lt;- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),\n                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))\n\n\n\n\n\nTabelle 37.1— Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight.\n\n\n\n\n\n\njump_length\nweight\n\n\n\n\n1.2\n0.8\n\n\n1.8\n1.0\n\n\n1.3\n1.2\n\n\n1.7\n1.9\n\n\n2.6\n2.0\n\n\n1.8\n2.7\n\n\n2.7\n2.8\n\n\n\n\n\n\n\n\nIn Abbildung 37.2 sehen wir die Visualisierung der Daten simple_tbl in einem Scatterplot mit einer geschätzen Gerade.\n\n\n\n\n\n\n\n\nAbbildung 37.2— Scatterplot der Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Die Gerade verläuft mittig durch die Punkte.\n\n\n\n\n\nWir schauen uns in diesem Kapitel nur eine simple lineare Regression mit einem \\(x_1\\) an. In unserem Fall ist das \\(x_1\\) gleich dem weight. Später schauen wir dann multiple lineare Regressionen mit mehreren \\(x_1,..., x_p\\) an.\nBevor wir mit dem Modellieren anfangen können, müssen wir verstehen, wie ein simples Modell theoretisch aufgebaut ist. Danach können wir uns das lineare Modell in R anschauen.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Simple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-basic.html#simple-lineare-regression-theoretisch",
    "href": "stat-linear-reg-basic.html#simple-lineare-regression-theoretisch",
    "title": "37  Simple lineare Regression",
    "section": "37.3 Simple lineare Regression theoretisch",
    "text": "37.3 Simple lineare Regression theoretisch\nWir haben nun die ersten sieben Beobachtungen in dem Objekt simple_tbl vorliegen. Wie sieht nun theoretisch eine lineare Regression aus? Wir wollen eine Grade durch Punkte legen, wie wie wir es in Abbildung 37.2 sehen. Die blaue Gerade wir durch eine Geradengleichung beschreiben. Du kenst vermutlich noch die Form \\(y = mx + b\\). In der Statistik beschreiben wir eine solche Gerade aber wie folgt.\n\\[\ny \\sim \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nmit\n\n\\(\\beta_0\\) als den y-Achsenabschnitt.\n\\(\\beta_1\\) als der Steigung der Geraden.\n\\(\\epsilon\\) als Residuen oder die Abweichungen von den \\(y\\)-Werten auf Geraden zu den einzelnen \\(y\\)-Werten der Beobachtungen.\n\nIn Tabelle 37.2 siehst du nochmal in einer Tabelle den Vergleich von der Schreibweise der linearen Regression in der Schule und in der Statistik. Darüber hinaus sind die deutschen Begriffe den englischen Begriffen gegenüber gestellt. Warum schreiben wir die Gleichung in der Form? Damit wir später noch weitere \\(\\beta_px_p\\)-Paare ergänzen könen und so multiple Modelle bauen können.\n\n\n\nTabelle 37.2— Vergleich und Übersicht der schulischen vs. statistischen Begriffe in den linearen Regression sowie die deutschen und englischen Begriffe.\n\n\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{y = mx +b}\\)\n\\(\\boldsymbol{y \\sim \\beta_0 + \\beta_1 x_1 + \\epsilon}\\)\nDeutsch\nEnglisch\n\n\n\n\n\\(m\\)\n\\(\\beta_1\\)\nSteigung\nSlope\n\n\n\\(x\\)\n\\(x_1\\)\nEinflussvariable\nRisk factor\n\n\n\\(b\\)\n\\(\\beta_0\\)\ny-Achsenabschnitt\nIntercept\n\n\n\n\\(\\epsilon\\)\nResiduen\nResidual\n\n\n\n\n\n\nIn Abbildung 37.3 sehen wir die Visualisierung der Gleichung in einer Abbildung. Die Gerade läuft durch die Punktewolke und wird durch die statistischen Maßzahlen bzw. Parameter \\(\\beta_0\\), \\(\\beta_1\\) sowie den \\(\\epsilon\\) beschrieben. Wir sehen, dass das \\(\\beta_0\\) den Intercept darstellt und das \\(\\beta_1\\) die Steigung der Geraden. Wenn wir \\(x\\) um 1 Einheit erhöhen \\(x+1\\), dann steigt der \\(y\\) Wert um den Wert von \\(\\beta_1\\). Die einzelnen Abweichungen der beobachteten \\(y\\)-Wert zu den \\(y\\)-Werten auf der Gerade (\\(\\hat{y}\\)) werden als Residuen oder auch \\(\\epsilon\\) bezeichnet.\n\n\n\n\n\n\nAbbildung 37.3— Visualisierung der linearen Regression. Wir legen eine Gerade durch eine Punktewolke. Die Gerade wird durch die statistischen Maßzahlen bzw. Parameter \\(\\beta_0\\), \\(\\beta_1\\) sowie den \\(\\epsilon\\) beschrieben.\n\n\n\nSchauen wir uns einmal den Zusammenhang von \\(y\\), den beobachteten Werten, und \\(\\hat{y}\\), den geschätzen Werten auf der Gerade in unserem Beispiel an. In Tabelle 37.3 sehen wir die Berechnung der einzelnen Residuen für die Gerade aus der Abbildung 37.2. Wir nehmen jedes beobachtete \\(y\\) und ziehen den Wert von \\(y\\) auf der Gerade, bezeichnet als \\(\\hat{y}\\), ab. Diesen Schritt machen wir für jedes Wertepaar \\((y_i; \\hat{y}_i)\\). In R werden die \\(\\hat{y}\\) auch fitted values genannt. Die \\(\\epsilon\\) Werte werden dann residuals bezeichnet.\n\n\n\nTabelle 37.3— Zusammenhang zwischen den \\(y\\), den beobachteten Werten, und \\(\\hat{y}\\), den geschätzen Werten auf der Gerade. Wir nennen den Abstand \\(y_i - \\hat{y}_i\\) auch Residuum oder Epsilon \\(\\epsilon\\).\n\n\n\n\n\n\n\n\n\n\n\n\nx\ny\n\\(\\boldsymbol{\\hat{y}}\\)\nResiduen (\\(\\boldsymbol{\\epsilon}\\))\nWert\n\n\n\n\n0.8\n1.2\n1.38\n\\(\\epsilon_1 = y_1 - \\hat{y}_1\\)\n\\(\\epsilon_1 = 1.2 - 1.38 = -0.18\\)\n\n\n1.0\n1.8\n1.48\n\\(\\epsilon_2 = y_2 - \\hat{y}_2\\)\n\\(\\epsilon_2 = 1.8 - 1.48 = +0.32\\)\n\n\n1.2\n1.3\n1.58\n\\(\\epsilon_3 = y_3 - \\hat{y}_3\\)\n\\(\\epsilon_3 = 1.3 - 1.58 = -0.28\\)\n\n\n1.9\n1.7\n1.94\n\\(\\epsilon_4 = y_4 - \\hat{y}_4\\)\n\\(\\epsilon_4 = 1.7 - 1.94 = -0.24\\)\n\n\n2.0\n2.6\n1.99\n\\(\\epsilon_5 = y_5 - \\hat{y}_5\\)\n\\(\\epsilon_5 = 2.6 - 1.99 = +0.61\\)\n\n\n2.7\n1.8\n2.34\n\\(\\epsilon_6 = y_6 - \\hat{y}_6\\)\n\\(\\epsilon_6 = 1.8 - 2.34 = -0.54\\)\n\n\n2.8\n2.7\n2.40\n\\(\\epsilon_7 = y_7 - \\hat{y}_7\\)\n\\(\\epsilon_7 = 2.7 - 2.40 = +0.30\\)\n\n\n\n\n\n\nDie Abweichungen \\(\\epsilon\\) oder auch Residuen genannt haben einen Mittelwert von \\(\\bar{\\epsilon} = 0\\) und eine Varianz von \\(s^2_{\\epsilon} = 0.17\\). Wir schreiben, dass die Residuen normalverteilt sind mit \\(\\epsilon \\sim \\mathcal{N}(0, s^2_{\\epsilon})\\). Wir zeichnen die Gerade also so durch die Punktewolke, dass die Abstände zu den Punkten, die Residuen, im Mittel null sind. Die Optimierung erreichen wir in dem wir die Varianz der Residuuen minimieren. Folglich modellieren wir die Varianz.\n\n\n\n\n\n\nSimple lineare Regression händisch\n\n\n\nGut, das war jetzt die theoretische Abhandlung ohne eine mathematische Formel. Es geht natürlich auch mit den nackten Zahlen. In der Tabelle 37.4 siehst du einmal sieben Beobachtungen mit dem Körpergewicht als \\(y\\) sowie der Körpergröße als \\(x\\). Wir wollen jetzt einmal die Regressionsgleichung bestimmen. Wir sehen also unsere Werte für \\(\\beta_0\\) und \\(\\beta_1\\) aus?\n\n\n\n\nTabelle 37.4— Sieben Messungen der Körpergröße \\(x\\) und dem zugehörigen Körpergewicht \\(y\\).\n\n\n\n\n\n\nheight\nweight\n\n\n\n\n167\n70\n\n\n188\n83\n\n\n176\n81\n\n\n186\n90\n\n\n192\n94\n\n\n205\n100\n\n\n198\n106\n\n\n\n\n\n\n\n\nDamit wir einmal wissen, was wir als Lösung erhalten würden, hier einmal die lineare Regression mit der Funktion \\(lm()\\) und die entsprechenden Werte für den (Intercept) und der Steigung.\n\nlm(weight ~ height, data = by_hand_tbl) |&gt; \n  coef()\n\n(Intercept)      height \n -75.390377    0.877845 \n\n\nWir suchen dann damit die folgende Regressionsgleichung mit der Körpergröße als \\(x\\) und dem zugehörigen Körpergewicht als \\(y\\).\n\\[\nweight = \\beta_0 + \\beta_1 \\cdot height\n\\]\nDa es dann immer etwas schwer ist, sich den Zusammenhang zwischen Körpergewicht und Körpergröße vorzustellen, habe ich nochmal in der Abbildung 37.4 den Scatterplot der Daten erstellt. Die rote Gerade stellt die Regressiongleichung dar. Wir erhalten ein y-Achsenabschnitt mit \\(\\beta_0\\) von \\(-75.39\\) sowie eine Steigung mit \\(\\beta_1\\) von \\(0.88\\) aus unseren Daten.\n\nggplot(by_hand_tbl, aes(height, weight)) +\n  theme_minimal() +\n  geom_point() +\n  geom_function(fun = \\(x) -75.39 + 0.88 * x, color = \"red\")\n\n\n\n\n\n\n\nAbbildung 37.4— Scatterplot der sieben Messungen der Körpergröße \\(x\\) und dem zugehörigen Körpergewicht \\(y\\) sowie der Regressionsgerade mit \\(y = -75.39 + 0.88 \\cdot x\\). Die gerade verlauf wie erwartet mittig durch die Punktewolke.\n\n\n\n\n\nJetzt stellt sich die Frage, wie wir händisch die Werte für den y-Achsenabschnitt mit \\(\\beta_0\\) sowie der Steigung mit \\(\\beta_1\\) berechnen. Dafür gibt es jeweils eine Formel. Hier müssen wir dann sehr viele Summen berechnen, was ich dann gleich einmal in einer Tabelle zusammenfasse.\n\nFormel für y-Achsenabschnitt mit \\(\\beta_0\\)\n\n\\[\n\\beta_0 = \\cfrac{(\\Sigma Y)(\\Sigma X^2) - (\\Sigma X)(\\Sigma XY)}{n(\\Sigma X^2) - (\\Sigma X)^2}\n\\]\n\nFormel für Steigung mit \\(\\beta_1\\)\n\n\\[\n\\beta_1 = \\cfrac{n(\\Sigma XY) - (\\Sigma X)(\\Sigma Y)}{n(\\Sigma X^2) - (\\Sigma X)^2}\n\\]\n\n\nIn der Tabelle 37.5 siehst du nochmal die originalen Datenpunkte und dann die entsprechenden Werte für das Produkt von weight und height mit \\(XY\\) und dann die jeweiligen Quadrate der beiden mit \\(X^2\\) und \\(Y^2\\). Wir brauchen dann aber nicht diese Werte sondern die Summen der Werte. Das Summieren lagere ich dann nochmal in eine weitere Tabelle aus.\n\n\n\n\nTabelle 37.5— Berechnungen des Produkts von \\(X\\) und \\(Y\\) sowie deren Quadrate mit \\(X^2\\) und \\(Y^2\\).\n\n\n\n\n\n\nheight\nweight\n\\(XY\\)\n\\(X^2\\)\n\\(Y^2\\)\n\n\n\n\n167\n70\n11690\n27889\n4900\n\n\n188\n83\n15604\n35344\n6889\n\n\n176\n81\n14256\n30976\n6561\n\n\n186\n90\n16740\n34596\n8100\n\n\n192\n94\n18048\n36864\n8836\n\n\n205\n100\n20500\n42025\n10000\n\n\n198\n106\n20988\n39204\n11236\n\n\n\n\n\n\n\n\nIn der abschließenden Tabelle findest du dann einmal die Summen der beobachteten Werte \\(X\\) und \\(Y\\) sowie des Produkts von \\(X\\) und \\(Y\\) sowie deren Quadrate mit \\(X^2\\) und \\(Y^2\\). Damit haben wir dann alles zusammen um die Formel oben zu füllen.\n\n\n\n\nTabelle 37.6— Summe der Datenpunkte für \\(X\\) und \\(Y\\) sowie des Produkts von \\(X\\) und \\(Y\\) sowie deren Quadrate mit \\(X^2\\) und \\(Y^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight \\((\\Sigma X)\\)\nweight \\((\\Sigma Y)\\)\n\\(\\Sigma XY\\)\n\\(\\Sigma X^2\\)\n\\(\\Sigma Y^2\\)\n\n\n\n\n1312\n624\n117826\n246898\n56522\n\n\n\n\n\n\n\n\nIch habe dann die ganzen Summen einmal händisch berechnet und dann in den Formeln von oben eingesetzt. Wir erhalten dann für den y-Achsenabschnitt \\(\\beta_0\\) folgenden Wert.\n\\[\n\\beta_0 = \\cfrac{624 \\cdot 246898 - 1312 \\cdot 117826}{7\\cdot 246898 - 1312^2} = -75.39038\n\\]\nDie ganze Berechnung habe ich dann auch einmal für die Steigung \\(\\beta_1\\) ebenfalls einmal durchgeführt.\n\\[\nb_1 = \\cfrac{7\\cdot 117826 - 1312\\cdot624}{7\\cdot246898 - 1312^2} = 0.877845\n\\]\nWir sehen, es kommen die gleichen Werte für den y-Achsenabschnitt \\(\\beta_0\\) und die Steigung \\(\\beta_1\\) raus. Das hat ja schonmal sehr gut geklappt. Eine andere Art die gleiche Werte effizienter zu berechnen ist die Matrixberechnung der Koeffizienten der linearen Regression. Wir könnten dann auch komplexere Modelle mit mehr als nur einem \\(x\\) und einem \\(\\beta_1\\) berechnen. Die grundlegende Formel siehst du einmal im Folgenden dargestellt.\n\\[\n\\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{pmatrix}\n= \\mathbf{(X^T X)^{−1}(X^T Y)}\n\\]\nWir brauchen jetzt einiges an Matrixrechnung um die jeweiligen Formelteile zu berechnen. Ich habe dir in den folgenden Tabs einmal Schritt für Schritt die einzelnen Teile berechnet. Wir immer machen wir das eigentlich nicht so richtig per Hand, sondern nutzen einen Computer. Prinzipiell wäre eine händische Lösung natürlich möglich.\n\n\\(X\\)\\(Y\\)\\(X^T\\)\\(X^T X\\)\\(X^T Y\\)\\((X^T X)^{−1}\\)\n\n\n\nX &lt;- as.matrix(c(167, 188, 176, 186, 192, 205, 198))\nX &lt;- cbind(rep(1, 7), X) \nX \n\n     [,1] [,2]\n[1,]    1  167\n[2,]    1  188\n[3,]    1  176\n[4,]    1  186\n[5,]    1  192\n[6,]    1  205\n[7,]    1  198\n\n\n\n\n\nY &lt;- as.matrix(c(70, 83, 81, 90, 94, 100, 106))\nY\n\n     [,1]\n[1,]   70\n[2,]   83\n[3,]   81\n[4,]   90\n[5,]   94\n[6,]  100\n[7,]  106\n\n\n\n\n\nXt &lt;- t(X) \nXt\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]    1    1    1    1    1    1    1\n[2,]  167  188  176  186  192  205  198\n\n\n\n\n\nXtX &lt;- Xt %*% X\nXtX\n\n     [,1]   [,2]\n[1,]    7   1312\n[2,] 1312 246898\n\n\n\n\n\nXtY &lt;- Xt %*% Y\nXtY\n\n       [,1]\n[1,]    624\n[2,] 117826\n\n\n\n\n\nXtXinv &lt;- solve(XtX)\nXtXinv\n\n           [,1]         [,2]\n[1,] 35.5658312 -0.188994526\n[2,] -0.1889945  0.001008355\n\n\n\n\n\nAm Ende müssen wir dann alle Teile in der Form \\(\\mathbf{(X^T X)^{−1}(X^T Y)}\\) einmal zusammenbringen. Das siehst dann in R wie folgt aus. Wir erhalten dann eine Matrix wieder wobei die erste Zeile der y-Achsenabschnitt \\(\\beta_0\\) und die zweite Zeile die Steigung \\(\\beta_1\\) ist. Wir erhalten fast die gleichen Werte wie auch schon oben.\n\nXtXinv %*% Xt %*% Y\n\n           [,1]\n[1,] -75.390377\n[2,]   0.877845\n\n\nWenn du dich tiefer in die Thematik einlesen willst, dann sind hier weitere Quellen zu der Thematik unter den folgenden Links und Tutorien.\n\nHands-On Machine Learning with R | Linear Regression\nManual linear regression analysis using R\nLinear Regression by Hand\nHow to Perform Linear Regression by Hand\nMatrix Approach to Simple Linear Regression in R",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Simple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-basic.html#simples-lineare-regression-in-r",
    "href": "stat-linear-reg-basic.html#simples-lineare-regression-in-r",
    "title": "37  Simple lineare Regression",
    "section": "37.4 Simples lineare Regression in R",
    "text": "37.4 Simples lineare Regression in R\nIm Allgemeinen können wir ein Modell in R wie folgt schreiben. Wir brauchen ein y auf der linken Seite und in der simplen linearen Regressione ein \\(x\\) auf der rechten Seite der Gleichung. Wir brauchen also zwei Variablen \\(y\\) und \\(x\\), die natürlich nicht im Datensatz in R so heißen müssen. Im Folgenen dann einmal die Modellschreibweise für \\(y\\) hängt ab von \\(x\\). Das \\(y\\) repräsentiert eine Spalte im Datensatz und das \\(x\\) repräsentiert ebenso eine Spalte im Datensatz.\n\\[\n\\Large y \\sim x\n\\]\nKonkret würden wir in unserem Beispiel das Modell wie folgt benennen. Das \\(y\\) wird zu jump_length und das \\(x\\) wird zu weight. Wir haben dann das Modell in der simpelsten Form definiert. Im Folgenden siehst du dann eimal die Modellschreibweise in R.\n\\[\n\\Large\\overbrace{\\mbox{jump\\_length}}^{\\Large y} \\sim \\overbrace{\\mbox{weight}}^{\\Large x}\n\\]\nNachdem wir das Modell definiert haben, setzen wir dieses Modell jump_length ~ weight in die Funktion lm() ein um das lineare Modell zu rechnen. Wie immer müssen wir auch festlegen aus welcher Datei die Spalten genommen werden sollen. Das machen wir mit der Option data = simple_tbl. Wir speichern dann die Ausgabe der Funktion lm() in dem Objekt fit_1 damit wir die Ausgabe noch in andere Funktionen pipen können.\n\nfit_1 &lt;- lm(jump_length ~ weight, data = simple_tbl)\n\nWir können jetzt mir dem Modell mehr oder minder drei Dinge tun. Abhängig von der Fragestellung liefert uns natürlich jedes der drei Möglichkeiten eine andere Antwort. Wir können auch mehrere dieser Fragen gleichzeitig beantworten, was aber meistens zu einer konfusen Analyse führt. Im Folgenden einmal eine Flowchart als grobe Übersicht deiner Möglichkeiten nach einem beispielhaften lm()-Modell als simple Regression.\n\n\n\n\n\n\nflowchart TD\n    A(\"Lineares Modell\n       zum Beispiel mit lm()\"):::factor --&gt; B1\n    A(\"Lineares Modell\n       zum Beispiel mit lm()\"):::factor --&gt; B2  \n    A(\"Lineares Modell zum Beispiel mit lm()\"):::factor --&gt; B3\n    subgraph B1[\"Inferenzmodell\"]\n    B[/\"anova()\"\\]:::factor --&gt; E[\\\"emmeans()\"/]:::factor\n    end\n    subgraph B2[\"Kausales Modell\"]\n    C(\"summary()\"):::factor --&gt; F(\"augment()\"):::eval\n    C(\"summary()\"):::factor --&gt; G(\"glance()\"):::eval\n    end   \n    subgraph B3[\"Prädiktives Modell\"]\n    D(\"predict()\"):::factor --&gt; H(\"conf_mat()\"):::eval\n    end \n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef eval fill:#E69F00,stroke:#333,stroke-width:0.75px\n    \n\n\n\n\nAbbildung 37.5— Flowchart der Möglichkeiten nach einer Modellierung mit der Funktion lm() als eine simple lineare Regression. Es gibt natürlich noch andere Modellierungen und damit Funktion. Der generelle Ablauf bleibt jedoch gleich. Die orangen Kacheln stellen optionale Funktionen zur Güte der Regression dar.\n\n\n\n\n\nHier nochmal die Möglichkeiten aus der Abbildung 37.5 mit mehr Details zu der obigen Flowchart. Für mehr Informationen musst du dann die entsprechenden Kapitel besuchen, hier passt dann alles wirklich nicht hin für ein so komplexen Thema wie die Modellierung von Daten.\n\nWir rechnen mit dem Fit des Modells einen Gruppenvergleich oder eine Inferenzstatistik (siehe Kapitel 28 zu der ANOVA oder Kapitel 33 zu multiplen Vergleichen) – wir wollen also einen Vergleich zwischen Gruppen rechnen. Wir haben damit dann kein kontinuierliches \\(x\\) vorliegen sondern einen Faktor mit verschiedenen Leveln als Gruppen.\nWir rechnen ein kausales Modell, uns interessieren die Effekte (siehe Kapitel 37.4.2) – wir sind an den Effekten von verschiedenen Einflussvariablen interessiert. Dabei geht es dann weniger um eine Behandlungsgruppe sondern um viele verschiedene Einflussvariablen, die verstanden werden wollen in ihrem Einfluss auf das Outcome \\(y\\). Wir können uns dann die Ergebnisse durch die Funktionen augment() und glance() aus dem R Paket {broom} näher anschauen.\nWir rechnen ein prädiktives Modell, uns interessiert der Wert neuer Werte (siehe Kapitel 37.4.3) – wir wollen dann also eine Vorhersage rechnen. Wir bauen uns ein komplexes Modell und wollen mit diesem Modell zukünftige Werte vom Outcome \\(y\\) vorhersagen. Wenn wir also neue Einflussvariablen \\(x\\) messen, können wir dann anhand des Modells unbekannte Outcomes \\(y\\) bestimmen. Wir können uns dann mehr Informationen zu der Güte der Vorhersage mit der Funktion conf_mat() aus dem R Paket {tidymodels} anschauen.\n\n\n37.4.1 Inferenzmodell\nIn diesem Kapitel betrachten wir nicht die Analyse von Gruppenvergleichen oder der Inferenzstatistik. Das machen wir dann in eigenen Kapiteln. Bitte besuche das Kapitel 28 zu mehr Informationen zu der einfaktoriellen und zweifaktoriellen ANOVA oder Kapitel 33 zu multiplen Vergleichen. Es würde dieses Kapitel sprengen, wenn wir uns dann hier die Sachlage nochmal hier aufrollen würde.\n\n\n37.4.2 Kausales Modell\nIm Folgenden rechnen wir ein kausales Modell, da wir an dem Effekt des \\(x\\) interessiert sind. Wenn also das \\(x_1\\) um eine Einheit ansteigt, um wie viel verändert sich dann das \\(y\\)? Der Schätzer \\(\\beta_1\\) gibt uns also den Einfluss oder den kausalen Zusammenhang zwischen \\(y\\) und \\(x_1\\) wieder. Im ersten Schritt schauen wir uns die Ausgabe der Funktion lm() in der Funktion summary() an. Daher pipen wir das Objekt fit_1 in die Funktion summary().\n\nfit_1 |&gt; summary\n\nWir erhalten folgende Ausgabe dargestellt in Abbildung 37.6.\n\n\n\n\n\n\nAbbildung 37.6— Die summary() Ausgabe des Modells fit_1. Hier nicht erschrecken, wir kriegen hier sehr viele Informationen aufeinmal, die wir teilweise nicht alle benötigen. Es hängt dann eben sehr stark von der Fragestelung ab.\n\n\n\nWas sehen wir in der Ausgabe der summary() Funktion? Als erstes werden uns die Residuen wiedergegeben. Wenn wir nur wenige Beobachtungen haben, dann werden uns die Residuen direkt wiedergegeben, sonst die Verteilung der Residuen. Mit der Funktion augment() aus dem R Paket {broom} können wir uns die Residuen wiedergeben lassen. Die Residuen schauen wir uns aber nochmal im Kapitel 39 genauer an.\n\nfit_1 |&gt; augment()\n\n# A tibble: 7 × 8\n  jump_length weight .fitted .resid  .hat .sigma .cooksd .std.resid\n        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1         1.2    0.8    1.38 -0.176 0.388  0.496  0.0778     -0.496\n2         1.8    1      1.48  0.322 0.297  0.471  0.151       0.844\n3         1.3    1.2    1.58 -0.280 0.228  0.483  0.0725     -0.701\n4         1.7    1.9    1.94 -0.237 0.147  0.492  0.0275     -0.564\n5         2.6    2      1.99  0.612 0.156  0.384  0.199       1.47 \n6         1.8    2.7    2.34 -0.545 0.367  0.376  0.656      -1.51 \n7         2.7    2.8    2.40  0.304 0.417  0.467  0.276       0.877\n\n\nIm zweiten Block erhalten wir die Koeffizienten (eng. coefficients) der linearen Regression. Das heißt, wir kriegen dort \\(\\beta_0\\) als y-Achsenabschnitt sowie die Steigung \\(\\beta_1\\) für das Gewicht. Dabei ist wichtig zu wissen, dass immer als erstes der y-Achsenabschnitt (Intercept) auftaucht. Dann die Steigungen der einzelnen \\(x\\) in dem Modell. Wir haben nur ein kontinuierliches \\(x\\), daher ist die Interpretation der Ausgabe einfach. Wir können die Gradengleichung wie folgt formulieren.\n\\[\njump\\_length \\sim 0.97 + 0.51 \\cdot weight\n\\]\nWas heißt die Gleichung nun? Wenn wir das \\(x\\) um eine Einheit erhöhen dann verändert sich das \\(y\\) um den Wert von \\(\\beta_1\\). Wir haben hier eine Steigung von \\(0.51\\) vorliegen. Ohne Einheit keine Interpretation! Wir wissen, dass das Gewicht in [mg] gemessen wurde und die Sprungweite in [cm]. Damit können wir aussagen, dass wenn ein Floh 1 mg mehr wiegt der Floh 0.51 cm weiter springen würde.\nSchauen wir nochmal in die saubere Ausgabe der tidy() Funktion. Wir sehen nämlich noch einen \\(p\\)-Wert für den Intercept und die Steigung von weight.\n\nfit_1 |&gt; tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    0.969     0.445      2.18  0.0813\n2 weight         0.510     0.232      2.20  0.0790\n\n\nWenn wir einen \\(p\\)-Wert sehen, dann brauchen wir eine Nullhypothese, die wir dann eventuell mit der Entscheidung am Signifikanzniveau \\(\\alpha\\) von 5% ablehnen können. Die Nullhypothese ist die Gleichheitshypothese. Wenn es also keinen Effekt von dem Gewicht auf die Sprungweite gebe, wie groß wäre dann \\(\\beta_1\\)? Wir hätten dann keine Steigung und die Grade würde parallel zur x-Achse laufen. Das \\(\\beta_1\\) wäre dann gleich null.\n\\[\n\\begin{align*}\nH_0: \\beta_i &= 0\\\\  \nH_A: \\beta_i &\\neq 0 \\\\   \n\\end{align*}\n\\]\nWir haben für jedes \\(\\beta_i\\) ein eigenes Hypothesenpaar. Meistens interessiert uns der Intercept nicht. Ob der Intercept nun durch die Null geht oder nicht ist eher von geringem Interessen.\nSpannder ist aber wie sich der \\(p\\)-Wert berechnet. Der \\(p\\)-Wert basiert auf einer t-Statistik, also auf dem t-Test. Wir rechnen für jeden Koeffizienten \\(\\beta_i\\) einen t-Test. Das machen wir in dem wir den Koeffizienten estimate durch den Fehler des Koeffizienten std.error teilen.\n\\[\n\\begin{align*}\nT_{(Intercept)} &= \\cfrac{\\mbox{estimate}}{\\mbox{std.error}}  = \\cfrac{0.969}{0.445} = 2.18\\\\  \nT_{weight} &= \\cfrac{\\mbox{estimate}}{\\mbox{std.error}}  = \\cfrac{0.510}{0.232} = 2.20\\\\   \n\\end{align*}\n\\]\nWir sehen in diesem Fall, dass weder der Intercept noch die Steigung von weight signifikant ist, da die \\(p\\)-Werte mit \\(0.081\\) und \\(0.079\\) leicht über dem Signifikanzniveau von \\(\\alpha\\) gleich 5% liegen. Wir haben aber einen starkes Indiz gegen die Nullhypothese, da die Wahrscheinlichkeit die Daten zu beobachten sehr gering ist unter der Annahme das die Nullhypothese gilt.\nZun Abschluß noch die Funktion glance() ebenfalls aus dem R Paket {broom}, die uns erlaubt noch die Qualitätsmaße der linearen Regression zu erhalten. Wir müssen nämlich noch schauen, ob die Regression auch funktioniert hat. Die Überprüfung geht mit einem \\(x\\) sehr einfach. Wir können uns die Grade ja anschauen. Das geht dann mit einem Model mit mehreren \\(x\\) nicht mehr und wir brauchen andere statistische Maßzahlen.\n\nfit_1 |&gt; glance() \n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.492         0.391 0.455      4.84  0.0790     1  -3.24  12.5  12.3\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n37.4.3 Prädiktives Modell\nNeben dem kausalen Modell gibt es auch die Möglichkeit ein prädiktives Modell zu rechnen. Im Prinzip ist die Sprache hier etwas ungenau. Wir verwenden das gefittete Modell nur anders. Anstatt das Modell fit_1 in die Funktion summary() zu pipen, pipen wir die das Modell in die Funktion predict(). Die Funktion predict() kann dann für neue Daten über die Option newdata = das \\(y\\) vorhersagen.\nIn unserem Fall müssen wir uns deshalb ein tibble mit einer Spalte bauen. Wir haben ja oben im Modell auch nur ein \\(x_1\\) mit aufgenommen. Später können wir natürlich auch für multiple Modelle die Vorhersage machen. Wichtig ist, dass die Namen gleich sind. Das heißt in dem neuen Datensatz müssen die Spalten exakt so heißen wir in dem alten Datensatz in dem das Modell gefittet wurde.\n\nsimple_new_tbl &lt;- tibble(weight = c(1.7, 1.4, 2.1, 3.0)) \n\npredict(fit_1, newdata = simple_new_tbl) |&gt; round(2)\n\n   1    2    3    4 \n1.84 1.68 2.04 2.50 \n\n\nWie wir sehen ist die Anwendung recht einfach. Wir haben die vier jump_length Werte vorhergesagt bekommen, die sich mit dem Fit des Modells mit den neuen weight Werten ergeben. In Abbildung 37.7 sehen wir die Visualisierung der vier vorhergesagten Werte. Die Werte müssen auf der Geraden liegen.\n\n\n\n\n\n\n\n\nAbbildung 37.7— Scatterplot der alten Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Sowie der neuen vorhergesagten Beobachtungen auf der Geraden.\n\n\n\n\n\nWir werden später in der Klassifikation, der Vorhersage von \\(0/1\\)-Werten, sowie in der multiplen Regression noch andere Prädktionen und deren Maßzahlen kennen lernen. Im Rahmen der simplen Regression soll dies aber erstmal hier genügen.\n\n\n\nAbbildung 37.1— Scatterplot von fünfundzwanzig Beobachtungen die sich aus den paarweisen Werten für \\(x_1\\) und \\(y\\) ergeben. Das Ziel einer linearen Regression ist es die rote Gerade zu bestimmen.\nAbbildung 37.2— Scatterplot der Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Die Gerade verläuft mittig durch die Punkte.\nAbbildung 37.3— Visualisierung der linearen Regression. Wir legen eine Gerade durch eine Punktewolke. Die Gerade wird durch die statistischen Maßzahlen bzw. Parameter \\(\\beta_0\\), \\(\\beta_1\\) sowie den \\(\\epsilon\\) beschrieben.\nAbbildung 37.4— Scatterplot der sieben Messungen der Körpergröße \\(x\\) und dem zugehörigen Körpergewicht \\(y\\) sowie der Regressionsgerade mit \\(y = -75.39 + 0.88 \\cdot x\\). Die gerade verlauf wie erwartet mittig durch die Punktewolke.\nAbbildung 37.6— Die summary() Ausgabe des Modells fit_1. Hier nicht erschrecken, wir kriegen hier sehr viele Informationen aufeinmal, die wir teilweise nicht alle benötigen. Es hängt dann eben sehr stark von der Fragestelung ab.\nAbbildung 37.7— Scatterplot der alten Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Sowie der neuen vorhergesagten Beobachtungen auf der Geraden.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Simple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-basic.html",
    "href": "stat-modeling-basic.html",
    "title": "38  Multiple lineare Regression",
    "section": "",
    "text": "38.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom,\n               see, performance, car, parameters,\n               conflicted)\nconflicts_prefer(magrittr::set_names)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-basic.html#sec-modell-matrix",
    "href": "stat-modeling-basic.html#sec-modell-matrix",
    "title": "38  Multiple lineare Regression",
    "section": "38.2 Die Modellmatrix in R",
    "text": "38.2 Die Modellmatrix in R\nAls erstes wollen wir verstehen, wie ein Modell in R aussieht. Dann können wir auch besser verstehen, wie die eigentlichen Koeffizienten aus dem Modell entstehen. Wozu brauche ich das? Eine gute Frage. Du brauchst das Verständnis der Modellmatrix, wenn du verstehen willst wie R in einer linearen Regression zu den Koeffizienten kommt. Wir bauen uns dafür ein sehr simplen Datensatz. Wir bauen uns einen Datensatz mit Schlangen.\n\nsnake_tbl &lt;- tibble(\n  svl = c(40, 45, 39, 51, 52, 57, 58, 49),\n  mass = c(6, 8, 5, 7, 9, 11, 12, 10),\n  region = as_factor(c(\"west\",\"west\", \"west\", \"nord\",\"nord\",\"nord\",\"nord\",\"nord\")),\n  color = as_factor(c(\"schwarz\", \"schwarz\", \"rot\", \"rot\", \"rot\", \"blau\", \"blau\", \"blau\"))\n) \n\nIn der Tabelle 38.1 ist der Datensatz snake_tbl nochmal dargestellt. Wir haben die Schlangenlänge svl als Outcome \\(y\\) sowie das Gewicht der Schlangen mass, die Sammelregion region und die Farbe der Schlangen color. Dabei ist mass eine kontinuierliche Variable, region eine kategorielle Variable als Faktor mit zwei Leveln und color eine kategorielle Variable als Faktor mit drei Leveln.\n\n\n\n\nTabelle 38.1— Datensatz zu Schlangen ist entlehnt und modifiiert nach Kéry (2010, p. 77)\n\n\n\n\n\n\nsvl\nmass\nregion\ncolor\n\n\n\n\n40\n6\nwest\nschwarz\n\n\n45\n8\nwest\nschwarz\n\n\n39\n5\nwest\nrot\n\n\n51\n7\nnord\nrot\n\n\n52\n9\nnord\nrot\n\n\n57\n11\nnord\nblau\n\n\n58\n12\nnord\nblau\n\n\n49\n10\nnord\nblau\n\n\n\n\n\n\n\n\nWir wollen uns nun einmal anschauen, wie ein Modell in R sich zusammensetzt. Je nachdem welche Spalte \\(x\\) wir verwenden um den Zusammenhang zum \\(y\\) aufzuzeigen.\n\n38.2.1 Kontinuierliches \\(x\\)\nIm ersten Schritt wollen wir uns einmal das Modell mit einem kontinuierlichen \\(x\\) anschauen. Daher bauen wir uns ein lineares Modell mit der Variable mass. Wir erinnern uns, dass mass eine kontinuierliche Variable ist, da wir hier nur Zahlen in der Spalte finden. Die Funktion model.matrix() gibt uns die Modellmatrix wieder.\n\nmodel.matrix(svl ~ mass, data = snake_tbl) |&gt; as_tibble()\n\n# A tibble: 8 × 2\n  `(Intercept)`  mass\n          &lt;dbl&gt; &lt;dbl&gt;\n1             1     6\n2             1     8\n3             1     5\n4             1     7\n5             1     9\n6             1    11\n7             1    12\n8             1    10\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte mass als kontinuierliche Variable.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 6 \\\\\n  1 & 8 \\\\\n  1 & 5 \\\\\n  1 & 7 \\\\\n  1 & 9 \\\\\n  1 & 11\\\\\n  1 & 12\\\\\n  1 & 10\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_{mass}\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt mit der Funktion lm() fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten aus dem Objekt fit_1 wiedergeben zu lassen.\n\nfit_1 &lt;- lm(svl ~ mass, data = snake_tbl) \nfit_1 |&gt; coef() |&gt; round(2)\n\n(Intercept)        mass \n      26.71        2.61 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_1 wieder.\n\nfit_1 |&gt; residuals() |&gt; round(2)\n\n    1     2     3     4     5     6     7     8 \n-2.36 -2.57 -0.75  6.04  1.82  1.61  0.00 -3.79 \n\n\nWir können jetzt die Koeffizienten in die Modellmatrix ergänzen. Wir haben den Intercept mit \\(\\beta_0 = 26.71\\) geschätzt. Weiter ergänzen wir die Koeffizienten aus dem linearen Modell für mass mit \\(\\beta_{mass}=2.61\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein. Wir erhalten dann folgende ausgefüllte Gleichung mit den Matrixen.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  26.71 & \\phantom{0}6 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}8 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}5 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}7 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}9 \\cdot 2.61\\\\\n  26.71 & 11\\cdot 2.61\\\\\n  26.71 & 12\\cdot 2.61\\\\\n  26.71 & 10\\cdot 2.61\\\\\n\\end{pmatrix}\n  +\n  \\begin{pmatrix}\n  -2.36\\\\\n  -2.57\\\\\n  -0.75 \\\\\n  +6.04\\\\\n  +1.82\\\\\n  +1.61\\\\\n  \\phantom{+}0.00\\\\\n  -3.79\\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis. Wie du siehst ergänzen wir hier noch eine Reihe von \\(+\\) um den Intercept mit der Steigung zu verbinden. Steht ja auch so in der Gleichung des linearen Modells drin, alles wird mit einem \\(+\\) miteinander verbunden.\n\nc(26.71 +  6*2.61 - 2.36,\n  26.71 +  8*2.61 - 2.57,\n  26.71 +  5*2.61 - 0.75,\n  26.71 +  7*2.61 + 6.04,\n  26.71 +  9*2.61 + 1.82,\n  26.71 + 11*2.61 + 1.61,\n  26.71 + 12*2.61 + 0.00,\n  26.71 + 10*2.61 - 3.79) |&gt; round() \n\n[1] 40 45 39 51 52 57 58 49\n\n\nOh ha! Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat. Das heißt, die ganze Sache hat funktioniert.\n\n\n38.2.2 Kategorielles \\(x\\) mit 2 Leveln\nIm diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen \\(x\\) mit 2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable region``. Die Funktionmodel.matrix()` gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ region, data = snake_tbl) |&gt; as_tibble()\n\n# A tibble: 8 × 2\n  `(Intercept)` regionnord\n          &lt;dbl&gt;      &lt;dbl&gt;\n1             1          0\n2             1          0\n3             1          0\n4             1          1\n5             1          1\n6             1          1\n7             1          1\n8             1          1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte regionnord. In dieser Spalte steht die Dummykodierung für die Variable region. Die ersten drei Schlangen kommen nicht aus der Region nord und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region nord und erhalten daher eine 1 in der Spalte.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 &  0  \\\\\n  1 &  0 \\\\\n  1 &  0\\\\\n  1 &  1\\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta^{region}_{nord} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen.\n\nfit_2 &lt;- lm(svl ~ region, data = snake_tbl) \nfit_2 |&gt; coef() |&gt; round(2)\n\n(Intercept)  regionnord \n      41.33       12.07 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_2 wieder.\n\nfit_2 |&gt; residuals() |&gt; round(2)\n\n    1     2     3     4     5     6     7     8 \n-1.33  3.67 -2.33 -2.40 -1.40  3.60  4.60 -4.40 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 41.33\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für die Region und das Level nord mit \\(\\beta^{region}_{nord} = 12.07\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  -1.33\\\\\n  +3.67 \\\\\n  -2.33 \\\\\n  -2.40 \\\\\n  -1.40 \\\\\n  +3.60 \\\\\n  +4.60 \\\\\n  -4.40 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(41.33 + 0*12.07 - 1.33,\n  41.33 + 0*12.07 + 3.67,\n  41.33 + 0*12.07 - 2.33,\n  41.33 + 1*12.07 - 2.40,\n  41.33 + 1*12.07 - 1.40,\n  41.33 + 1*12.07 + 3.60,\n  41.33 + 1*12.07 + 4.60,\n  41.33 + 1*12.07 - 4.40) |&gt; round() \n\n[1] 40 45 39 51 52 57 58 49\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat.\n\n\n38.2.3 Kategorielles \\(x\\) mit &gt;2 Leveln\nIm diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen \\(x\\) mit &gt;2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable color. Die Funktion model.matrix() gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ color, data = snake_tbl) |&gt; as_tibble()\n\n# A tibble: 8 × 3\n  `(Intercept)` colorrot colorblau\n          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1             1        0         0\n2             1        0         0\n3             1        1         0\n4             1        1         0\n5             1        1         0\n6             1        0         1\n7             1        0         1\n8             1        0         1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalten für color. Die Spalten colorrot und colorblau geben jeweils an, ob die Schlange das Level rot hat oder blau oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 0 & 0 \\\\\n  1 & 0 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 0 & 1\\\\\n  1 & 0 & 1\\\\\n  1 & 0 & 1\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta^{color}_{rot} \\\\\n  \\beta^{color}_{blau} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen.\n\nfit_3 &lt;- lm(svl ~ color, data = snake_tbl) \nfit_3 |&gt; coef() |&gt; round(2)\n\n(Intercept)    colorrot   colorblau \n      42.50        4.83       12.17 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_3 wieder.\n\nfit_3 |&gt; residuals() |&gt; round(2)\n\n    1     2     3     4     5     6     7     8 \n-2.50  2.50 -8.33  3.67  4.67  2.33  3.33 -5.67 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 25\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für die Farbe und das Level rot mit \\(\\beta^{color}_{rot} = 4.83\\) und für die Farbe und das Level blau mit \\(\\beta^{color}_{blau} = 12.17\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  42.50 & 0 \\cdot 4.83& 0 \\cdot 12.17 \\\\\n  42.50 & 0 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  -2.50 \\\\\n  +2.50 \\\\\n  -8.33 \\\\\n  +3.67 \\\\\n  +4.67 \\\\\n  +2.33 \\\\\n  +3.33 \\\\\n  -5.67 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(42.50 + 0*4.83 + 0*-12.17 - 2.50,\n  42.50 + 0*4.83 + 0*-12.17 + 2.50,\n  42.50 + 1*4.83 + 0*-12.17 - 8.33,\n  42.50 + 1*4.83 + 0*-12.17 + 3.67,\n  42.50 + 1*4.83 + 0*-12.17 + 4.67,\n  42.50 + 0*4.83 + 1*-12.17 + 2.33,\n  42.50 + 0*4.83 + 1*-12.17 + 3.33,\n  42.50 + 0*4.83 + 1*-12.17 - 5.67) |&gt; round()\n\n[1] 40 45 39 51 52 33 34 25\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat.\n\n\n38.2.4 Das volle Modell\nIm letzten Schritt wollen wir uns einmal das volle Modell anschauen. Wir bauen uns ein Modell mit allen Variablen in dem Datensatz snake_tbl. Die Funktion model.matrix() gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ mass + region + color, data = snake_tbl) |&gt; as_tibble() \n\n# A tibble: 8 × 5\n  `(Intercept)`  mass regionnord colorrot colorblau\n          &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1             1     6          0        0         0\n2             1     8          0        0         0\n3             1     5          0        1         0\n4             1     7          1        1         0\n5             1     9          1        1         0\n6             1    11          1        0         1\n7             1    12          1        0         1\n8             1    10          1        0         1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte mass als kontenuierliche Variable. In der Spalte regionnord steht die Dummykodierung für die Variable region. Die ersten drei Schlangen kommen nicht aus der Region nord und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region nord und erhalten daher eine 1 in der Spalte. Die nächsten beiden Spalten sind etwas komplizierter. Die Spalten colorrot und colorblau geben jeweils an, ob die Schlange das Level rot hat oder blau oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 6 & 0 & 0 & 0 \\\\\n  1 & 8 & 0 & 0 & 0\\\\\n  1 & 5 & 0 & 1 & 0\\\\\n  1 & 7 & 1 & 1 & 0\\\\\n  1 & 9 & 1 & 1 & 0\\\\\n  1 & 11& 1 & 0 & 1\\\\\n  1 & 12& 1 & 0 & 1\\\\\n  1 & 10& 1 & 0 & 1\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_{mass} \\\\\n  \\beta^{region}_{nord} \\\\\n  \\beta^{color}_{rot} \\\\\n  \\beta^{color}_{blau} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen.\n\nfit_4 &lt;- lm(svl ~ mass + region + color, data = snake_tbl) \nfit_4 |&gt; coef() |&gt; round(2)\n\n(Intercept)        mass  regionnord    colorrot   colorblau \n      25.00        2.50        5.00        1.50       -2.83 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_4 wieder.\n\nfit_4 |&gt; residuals() |&gt; round(2)\n\n    1     2     3     4     5     6     7     8 \n 0.00  0.00  0.00  2.00 -2.00  2.33  0.83 -3.17 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 25\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für mass mit \\(\\beta_{mass}=2.5\\), für Region und das Level nord mit \\(\\beta^{region}_{nord} = 5\\), für die Farbe und das Level rot mit \\(\\beta^{color}_{rot} = 1.5\\) und für die Farbe und das Level blau mit \\(\\beta^{color}_{blau} = -2.83\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  25 & \\phantom{0}6 \\cdot 2.5 & 0 \\cdot 5 & 0 \\cdot 1.5& 0 \\cdot -2.83 \\\\\n  25 & \\phantom{0}8 \\cdot 2.5 & 0 \\cdot 5 & 0 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}5 \\cdot 2.5 & 0 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}7 \\cdot 2.5 & 1 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}9 \\cdot 2.5 & 1 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & 11\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n  25 & 12\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n  25 & 10\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\phantom{+}0.00 \\\\\n  \\phantom{+}0.00 \\\\\n  \\phantom{+}0.00 \\\\\n  +2.00 \\\\\n  -2.00 \\\\\n  +2.33 \\\\\n  +0.83 \\\\\n  -3.17 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(25 +  6*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,\n  25 +  8*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,\n  25 +  5*2.5 + 0*5 + 1*1.5 + 0*-2.83 + 0.00,\n  25 +  7*2.5 + 1*5 + 1*1.5 + 0*-2.83 + 2.00,\n  25 +  9*2.5 + 1*5 + 1*1.5 + 0*-2.83 - 2.00,\n  25 + 11*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 2.33,\n  25 + 12*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 0.83,\n  25 + 10*2.5 + 1*5 + 0*1.5 + 1*-2.83 - 3.17) \n\n[1] 40 45 39 51 52 57 58 49\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat. Was haben wir gelernt?\n\nIn einem Modell gibt es immer ein Faktorlevel weniger als ein Faktor Level hat. Die Information des alphanumerisch ersten Levels steckt dann mit in dem Intercept.\nIn einem Modell geht eine kontinuierliche Variable als eine Spalte mit ein.\nIn einem Modell gibt es immer nur eine Spalte für die Residuen und damit nur eine Residue für jede Beobachtung, egal wie viele Variablen ein Modell hat.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-basic.html#sec-interpret-x",
    "href": "stat-modeling-basic.html#sec-interpret-x",
    "title": "38  Multiple lineare Regression",
    "section": "38.3 Interpretation von \\(x\\)",
    "text": "38.3 Interpretation von \\(x\\)\nWi interpretiee wir nun das Ergebnis einer linearen Regression? Zum einen nutzen wir häufig das Modell nur um das Modell dann weiter in einem multiplen Vergleich zu nutzen. Hier wollen wir uns jetzt aber wirklich die Koeffizienten aus einer multiplen linearen Regression anschauen udn diese Zahlen einmal interpretieren. Wichtig ist, dass wir ein normalverteiltes \\(y\\) vorliegen haben und uns verschiedene Formen des \\(x\\) anschauen. Wir betrachten ein kontinuierliches \\(x\\), ein kategorielles \\(x\\) mit zwei Leveln und ein kategorielles \\(x\\) mit drei oder mehr Leveln.\n\n38.3.1 Kontinuierliches \\(x\\)\nBauen wir uns also einmal einen Datensatz mit einem kontinuierlichen \\(x\\) und einem normalverteilten \\(y\\). Unser \\(x\\) soll von 1 bis 7 laufen. Wir erschaffen uns das \\(y\\) indem wir das \\(x\\) mit 1.5 multiplizieren, den \\(y\\)-Achsenabschnitt von 5 addieren und einen zufälligen Fehler aus einer Normalverteilung mit \\(\\mathcal{N}(0, 1)\\) aufaddieren. Wir haben also eine klassische Regressionsgleichung mit \\(y = 5 + 1.5 \\cdot x\\).\n\nset.seed(20137937)\ncont_tbl &lt;- tibble(x = seq(from = 1, to = 7, by = 1),\n                   y = 5 + 1.5 * x + rnorm(length(x), 0, 1))\ncont_tbl\n\n# A tibble: 7 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1  5.93\n2     2  8.06\n3     3  8.95\n4     4 12.1 \n5     5 11.9 \n6     6 14.9 \n7     7 16.3 \n\n\nWenn wir keinen Fehler addieren würden, dann hätten wir auch eine Linie von Punkten wie an einer Perlschnur aufgereiht. Dann wäre das \\(R^2 = 1\\) und wir hätten keine Varianz in den Daten. Das ist aber in einem biologischen Setting nicht realistisch, dass unser \\(y\\) vollständig von \\(x\\) erklärt wird.\nSchauen wir uns nochmal die Modellmatrix an. Hier erwartet uns aber keine Überraschung. Wir schätzen den Intercept und dann kommt der Wert für jedes \\(x\\) in der zweiten Spalte.\n\nmodel.matrix(y ~ x, data = cont_tbl)\n\n  (Intercept) x\n1           1 1\n2           1 2\n3           1 3\n4           1 4\n5           1 5\n6           1 6\n7           1 7\nattr(,\"assign\")\n[1] 0 1\n\n\nIm Folgenden schätzen wir jetzt das lineare Modell um die Koeffizienten der Geraden zu erhalten. Wir nutzen die Funktion model_parameter() aus dem R Paket {parameters} für die Ausgabe der Koeffizienten.\n\nlm(y ~ x, data = cont_tbl) |&gt; \n  model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |        4.32\nx           |        1.71\n\n\nWir erhalten einen Intercept von \\(4.32\\) und eine Steigung von \\(x\\) mit \\(1.71\\). Wichtig nochmal, wir haben uns hier zufällige Zahlen erstellen lassen. Wenn du oben den Fehler rausnimmst, dann erhälst du auch die exakten Zahlen für den Intercept und die Steigung von \\(x\\) wieder. Wir sehen also, wenn wir ein kontinuierliches \\(x\\) haben, dann können wir das \\(x\\) in dem Sinne einr Steigung interpretieren. Steigt das \\(x\\) um 1 Einheit an, so erhöht sich das \\(y\\) um den Wert der Steigung von \\(x\\).\nIn Abbildung 38.1 sehen wir den Zusammenhang nochmal graphisch dargestellt. Wir sehen, dass wir die voreingestellten Parameter von \\(\\beta_0 = 5\\) und \\(\\beta_1 = 1.5\\) fast treffen.\n\n\n\n\n\n\n\n\nAbbildung 38.1— Graphische Darstellung der Interpretation von einem kontinuierlichen \\(x\\).\n\n\n\n\n\n\n\n38.3.2 Kategorielles \\(x\\) mit 2 Leveln\nEtwas anders wird der Fall wenn wir ein kategorielles \\(x\\) mit 2 Leveln vorliegen haben. Wir bauen faktisch zwei Punktetürme an zwei \\(x\\) Positionen auf. Dennoch können wir durch diese Punkte eine Gerade zeichnen. Bauen wir uns erst die Daten mit der Funktion rnorm(). Wir haben zwei Gruppen vorliegen, die Gruppe A hat sieben Beobachtungen und einen Mittelwert von 10. Die Gruppe B hat ebenfalls sieben Beobchatungen und einen Mittelwert von 15. Der Effekt zwischen den beiden Gruppen A und B ist die Mittelwertsdifferenz \\(\\Delta_{A-B}\\) ist somit 5. Wir erhlalten dann folgenden Datensatz wobei die Werte der Gruppe A um die 10 streuen und die Werte der Gruppe B um die 15 streuen.\n\nset.seed(20339537)\ncat_two_tbl &lt;- tibble(A = rnorm(n = 7, mean = 10, sd = 1),\n                      B = rnorm(n = 7, mean = 15, sd = 1)) |&gt; \n  gather(key = x, value = y) |&gt; \n  mutate(x = as_factor(x))\ncat_two_tbl\n\n# A tibble: 14 × 2\n   x         y\n   &lt;fct&gt; &lt;dbl&gt;\n 1 A     10.0 \n 2 A     10.8 \n 3 A     10.7 \n 4 A     11.0 \n 5 A      9.25\n 6 A      8.98\n 7 A      9.71\n 8 B     15.1 \n 9 B     16.0 \n10 B     14.5 \n11 B     15.1 \n12 B     14.2 \n13 B     16.8 \n14 B     15.6 \n\n\nWir wollen uns wieder die Modellmatrix einmal anschauen. Wir sehen hier schon einen Unterschied. Zum einen sehen wir, dass der Intercept für alle Beobachtungen geschätzt wird und in der zweiten Spalte nur xB steht. Somit werden in der zweiten Spalte nur die Beobachtungen in der Gruppe B berücksichtigt.\n\nmodel.matrix(y ~ x, data = cat_two_tbl)\n\n   (Intercept) xB\n1            1  0\n2            1  0\n3            1  0\n4            1  0\n5            1  0\n6            1  0\n7            1  0\n8            1  1\n9            1  1\n10           1  1\n11           1  1\n12           1  1\n13           1  1\n14           1  1\nattr(,\"assign\")\n[1] 0 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$x\n[1] \"contr.treatment\"\n\n\nWir rechnen einmal das lineare Modell und lassen uns überraschen was als Ergebnis herauskommt. Wir nutzen die Funktion model_parameter() aus dem R Paket {parameters} für die Ausgabe der Koeffizienten.\n\nlm(y ~ x, data = cat_two_tbl) |&gt; \n  model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |       10.07\nxB          |        5.27\n\n\nNun erhalten wir als den Intercept 10.07 und die Steigung xB mit 5.27 zurück. Wir sehen, der Intercept ist der Mittelwert der Gruppe A und die das xB ist die Änderung von dem Mittelwert A zu dem Mittelwert B. Wir erhalten die Mittelwertsdifferenz \\(\\Delta_{A-B}\\) von 5.27 zurück. In Tabelle 38.2 siehst du den Zusammenhang von den Faktorleveln A und B, den jeweiligen Mittelwerte der Level sowie die Differenz zum Mittel von dem ersten Level A.\n\n\n\n\nTabelle 38.2— Zusammenhang von den Mittelwerten der Level des Faktores \\(x\\) und deren Differenz zu Level A.\n\n\n\n\n\n\nFactor x\nMean of level\nDifference to level A\n\n\n\n\nA\n10.07\n0.00\n\n\nB\n15.33\n5.27\n\n\n\n\n\n\n\n\nIn Abbildung 38.2 kannst du nochmal den visuellen Zusammenhang zwischen den einzelnen Beobachtungen und der sich ergebenen Geraden sehen. Die Gerade geht durch die beiden Mittelwerte der Gruppe A und B. Daher ist die Steigung der Mittlwertsunterschied zwischen der Gruppe A und der Gruppe B. Steigt das \\(x\\) um 1 Einheit an, also springt von Gruppe A zu Gruppe B, so erhöht sich das \\(y\\) um den Mittelwertsunterschied \\(\\Delta_{A-B}\\).\n\n\n\n\n\n\n\n\nAbbildung 38.2— Graphische Darstellung der Interpretation von einem kategoriellen \\(x\\) mit 2 Leveln.\n\n\n\n\n\n\n\n38.3.3 Kategorielles \\(x\\) mit &gt;2 Leveln\nNachdem wir das Problem der Interpretation von einem kategoriellen \\(x\\) mit zwei Leveln verstanden haben, werden wir uns jetzt den Fall für ein kategoriellen \\(x\\) mit drei oder mehr Leveln anschauen. Wir nutzen hier nur drei Level, da es für vier oder fünf Level gleich abläuft. Du kannst das Datenbeispiel gerne auch noch um eine vierte Gruppe erweitern und schauen was sich da ändert.\nUnser Datensatz besteht aus drei Gruppen A, B und C mit den Mittelwerten von 10, 15 und 3. Hierbei ist wichtig, dass wir in der Gruppe C nur einen Mittelwert von 3 haben. Also wir haben keinen linearen Anstieg über die drei Gruppen.\n\nset.seed(20339537)\ncat_three_tbl &lt;- tibble(A = rnorm(n = 7, mean = 10, sd = 1),\n                        B = rnorm(n = 7, mean = 15, sd = 1),\n                        C = rnorm(n = 7, mean = 3, sd = 1)) |&gt; \n  gather(key = x, value = y) |&gt; \n  mutate(x = as_factor(x))\ncat_three_tbl\n\n# A tibble: 21 × 2\n   x         y\n   &lt;fct&gt; &lt;dbl&gt;\n 1 A     10.0 \n 2 A     10.8 \n 3 A     10.7 \n 4 A     11.0 \n 5 A      9.25\n 6 A      8.98\n 7 A      9.71\n 8 B     15.1 \n 9 B     16.0 \n10 B     14.5 \n# ℹ 11 more rows\n\n\nSchauen wir uns einmal die Modellmatrix an. Wir sehen wieder, dass der Intercept über alle Gruppen geschätzt wird und wir Koeffizienten für die Gruppen B und C erhalten. Mit der Modellmatrix wird dann auch das lineare Modell geschätzt.\n\nmodel.matrix(y ~ x, data = cat_three_tbl)\n\n   (Intercept) xB xC\n1            1  0  0\n2            1  0  0\n3            1  0  0\n4            1  0  0\n5            1  0  0\n6            1  0  0\n7            1  0  0\n8            1  1  0\n9            1  1  0\n10           1  1  0\n11           1  1  0\n12           1  1  0\n13           1  1  0\n14           1  1  0\n15           1  0  1\n16           1  0  1\n17           1  0  1\n18           1  0  1\n19           1  0  1\n20           1  0  1\n21           1  0  1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$x\n[1] \"contr.treatment\"\n\n\nWir rechnen wieder das Modell mit der Funktion lm(). Wir nutzen die Funktion model_parameter() aus dem R Paket {parameters} für die Ausgabe der Koeffizienten zu erhalten.\n\nlm(y ~ x, data = cat_three_tbl) |&gt; \n  model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |       10.07\nxB          |        5.27\nxC          |       -7.41\n\n\nWir sehen wieder, dass der Intercept der Mittelwert der Gruppe A ist. Wir hatten einen Mittelwert von 10 für die Gruppe A eingestellt und wir erhalten diesen Wert wieder. Die anderen Koeffizienten sind die Änderung zum Mittelwert von der Gruppe A. In Tabelle 38.3 ist der Zusammenhang nochmal für alle Level des Faktors \\(x\\) dargestellt. Wir haben für die Gruppe B einen Mittelwert von 15 und für die Gruppe C einen Mittelwert von 3 eingestellt. Wir erhalten diese Mittelwerte wieder, wenn wir die Differenzen zu dem Intercept bilden.\n\n\n\n\nTabelle 38.3— Zusammenhang von den Mittelwerten der Level des Faktores \\(x\\) und deren Differenz zu Level A.\n\n\n\n\n\n\nFactor x\nMean of level\nDifference to level A\n\n\n\n\nA\n10.07\n0.00\n\n\nB\n15.33\n5.27\n\n\nC\n2.66\n-7.41\n\n\n\n\n\n\n\n\nWir sehen den Zusammenhang nochmal in der Abbildung 38.3 visualisiert. Wir sehen die Änderung on der Gruppe A zu der Gruppe B sowie die Änderung von der Gruppe A zu der Gruppe C. Die Abbildung ist etwas verwirrend da wir nicht das \\(x\\) um 2 Einheiten erhöhen um auf den Mittelwert von C zu kommen. Wir rechnen sozusagen ausgehend von A die Änderung zu B und C aus. Dabei nehmen wir jedesmal an, dass die Gruppe B und die Gruppe C nur eine Einheit von \\(x\\) von A entfernt ist.\n\n\nWarning in geom_segment(aes(x = 1, y = 10.1, xend = 2, yend = 10.1 + 5.27), : All aesthetics have length 1, but the data has 21 rows.\nℹ Did you mean to use `annotate()`?\n\n\nWarning in geom_segment(aes(x = 1, y = 10.1, xend = 3, yend = 10.1 - 7.41), : All aesthetics have length 1, but the data has 21 rows.\nℹ Did you mean to use `annotate()`?\n\n\n\n\n\n\n\n\nAbbildung 38.3— Graphische Darstellung der Interpretation von einem kategoriellen \\(x\\) mit 3 Leveln.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-basic.html#sec-confounder",
    "href": "stat-modeling-basic.html#sec-confounder",
    "title": "38  Multiple lineare Regression",
    "section": "38.4 Adjustierung für Confounder",
    "text": "38.4 Adjustierung für Confounder\nIm folgenden Abschnitt wollen wir einmal auf Confounder eingehen. Was sind Confounder? Zum einen gibt es kein gutes deutsches Wort für Confounder. Du könntest Confounder in etwa mit Verzerrer oder Störfakor übersetzen. Zum Anderen können Confounder nur zuammen mit anderen Vriabln in einer multiplen Regression auftreten. Das heißt, wir brauchen mindestens zwei Variablen in einer Regression. Ein Confounder verursacht einen Effekt, den wir eigentlich nicht so erwartet hätten. Wir wollen eigentlich einen Effekt schätzen, aber der Effekt ist viel größer oder kleiner, da der Effekt eigentlich von einder anderen Variable verursacht oder aber verdeckt wird. Wir können einen Confounder in beide Richtungen haben. Wichtig ist hierbei, das wir eigentlich nicht an dem Effekt des Confounders interessiert sind. Wir wollen uns zum Beispiel den Effekt einer Düngung auf das Trockengewicht anschauen, aber der Effekt den wir beobachten wird durch die unterschiedlichen Pflanzorte verursacht.\nSchauen wir uns das ganze mal für das Beispiel des Zusammenhangs von dem Flohgewicht mit der Sprungweite an. Dafür benötigen wir den Datensatz flea_dog_cat_length_weight.csv.\n\nmodel_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") |&gt;\n  select(animal, sex, weight, jump_length) |&gt; \n  mutate(animal = as_factor(animal),\n         sex = as_factor(sex))\n\nIn der Tabelle 38.4 ist der Datensatz model_tbl nochmal dargestellt.\n\n\n\n\nTabelle 38.4— Datensatz für die Confounder Adjustierung. Die Variable jump_length ist das \\(y\\) und die Variable weight das \\(x\\) von Interesse.\n\n\n\n\n\n\nanimal\nsex\nweight\njump_length\n\n\n\n\ncat\nmale\n6.02\n15.79\n\n\ncat\nmale\n5.99\n18.33\n\n\ncat\nmale\n8.05\n17.58\n\n\ncat\nmale\n6.71\n14.09\n\n\ncat\nmale\n6.19\n18.22\n\n\ncat\nmale\n8.18\n13.49\n\n\n…\n…\n…\n…\n\n\nfox\nfemale\n8.04\n27.81\n\n\nfox\nfemale\n9.03\n24.02\n\n\nfox\nfemale\n7.42\n24.53\n\n\nfox\nfemale\n9.26\n24.35\n\n\nfox\nfemale\n8.85\n24.36\n\n\nfox\nfemale\n7.89\n22.13\n\n\n\n\n\n\n\n\nWir können uns jetzt drei verschiedene Modelle anschauen.\n\nDas Modell jump_length ~ weight. Wir modellieren die Abhängigkeit von der Sprungweite von dem Gewicht der Flöhe über alle anderen Variablen hinweg.\nDas Modell jump_length ~ weight + animal. Wir modellieren die Abhängigkeit von der Sprungweite von dem Gewicht der Flöhe und berücksichtigen die Tierart des Flohes. Ignorieren aber das Geschlecht des Flohes.\nDas Modell jump_length ~ weight + animal + sex. Wir modellieren die Abhängigkeit von der Sprungweite von dem Gewicht der Flöhe und berücksichtigen alle Variablen, die wir gemessen haben.\n\nHierbei ist wichtig, dass es natürlich auch Confounder geben kann, die wir gar nicht in den Daten erhoben haben. Also, dass sich Gruppen finden lassen, die wir gar nicht als Variable mit in den Daten erfasst haben. Hier könnte eine Hauptkomponentenanalyse helfen.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) jump_length ~ weight\n\n\n\n\n\n\n\n\n\n\n\n(b) jump_length ~ weight + animal\n\n\n\n\n\n\n\n\n\n\n\n(c) jump_length ~ weight + animal + sex\n\n\n\n\n\n\n\nAbbildung 38.4— Darstellung des counfounder Effekts anhand des Zusammenhangs der Sprungweite in [cm] und dem Gewicht von Flöhen [mg].\n\n\n\n\nIn Abbildung 38.4 (a) sehen wir die blaue Linie als die Ausgabe von dem Modell jump_length ~ weight. Wir sehen, dass mit dem Anstieg des Gewichtes der Flöhe auch die Sprungweite sich erhöht. Wir würden annehmen, dass wir hier einen signifikanten Unterschied vorliegen haben. Schauen wir uns die Koeffizienten des Modells aus dem lm() einmal an.\n\nlm(jump_length ~ weight, data = model_tbl) |&gt; \n  model_parameters()\n\nParameter   | Coefficient |   SE |        95% CI | t(598) |      p\n------------------------------------------------------------------\n(Intercept) |        9.79 | 0.77 | [8.28, 11.30] |  12.73 | &lt; .001\nweight      |        1.34 | 0.09 | [1.16,  1.53] |  14.16 | &lt; .001\n\n\nWir sehen, dass wir einen Effekt des Gewichts auf die Sprungweite von \\(1.34\\) vorliegen haben. Auch ist der Effekt und damit die Steigung signifikant.\nErweitern wir nun das Modell um die Tierart und erhalten die Abbildung 38.4 (b). Zum einen sehen wir, dass der globale Effekt nicht so ganz stimmen kann. Die Tierarten haben alle einen unterschiedlichen starken Effekt von dem Gewicht auf die Sprungweite. Auch hier fitten wir einmal das lineare Modell und schauen uns die Koeffizienten an.\n\nlm(jump_length ~ weight + animal, data = model_tbl) |&gt; \n  model_parameters()\n\nParameter    | Coefficient |   SE |       95% CI | t(596) |      p\n------------------------------------------------------------------\n(Intercept)  |        7.82 | 0.62 | [6.60, 9.03] |  12.64 | &lt; .001\nweight       |        1.27 | 0.07 | [1.13, 1.42] |  17.08 | &lt; .001\nanimal [dog] |        2.62 | 0.26 | [2.12, 3.13] |  10.23 | &lt; .001\nanimal [fox] |        4.97 | 0.26 | [4.47, 5.48] |  19.38 | &lt; .001\n\n\nDer Effekt des Gewichtes ist hier in etwa gleich geblieben. Wir sehen aber auch, dass die Sprungweite anscheinend bei Hunden und Füchsen höher ist. Daher haben wir hier einen Effekt on der Tierart. Wir adjustieren für den Confoundereffekt durch die Tierart und erhalten einen besseren Effektschätzer für das Gewicht.\nIn der letzten Abbildung 38.4 (c) wollen wir nochmal das Geschlecht der Flöhe mit in das Modell nehmen. Wir sehen, dass in diesem Fall, das Gewicht gar keinen Einfluss mehr auf die Sprungweite hat. Den Effekt des Gewichtes war nur der Effekt des unterschiedlichen Gewichtes der weiblichen und männlichen Flöhe. Schauen wir auch hier nochmal in des Modell.\n\nlm(jump_length ~ weight + animal + sex, data = model_tbl) |&gt; \n  model_parameters() |&gt; \n  mutate(Coefficient = round(Coefficient, 2))\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | t(595) |      p\n-------------------------------------------------------------------\n(Intercept) |       15.42 | 0.59 | [14.27, 16.58] |  26.23 | &lt; .001\nweight      |        0.01 | 0.08 | [-0.16,  0.17] |   0.07 | 0.942 \nanimaldog   |        2.61 | 0.19 | [ 2.23,  2.99] |  13.50 | &lt; .001\nanimalfox   |        5.19 | 0.19 | [ 4.81,  5.57] |  26.75 | &lt; .001\nsexfemale   |        4.89 | 0.23 | [ 4.44,  5.34] |  21.25 | &lt; .001\n\n\nNun können wir sehen, dass von unserem ursprünglichen Effekt von dem Gewicht auf die Sprungweite nichts mehr übrigbleibt. Wir haben gar keinen Effekt von dem Gewicht auf die Sprungweite vorliegen. Was wir gesehen haben, war der Effekt des Gewichtes der unterschiedlichen Geschlechter. Wenn wir unser Modell für die Confounder animal und sex adjustieren, haben wir einen unverzerrten Schätzer für weight. Wichtig ist nochmal, dafür müssen wir natürlich auch alle variablen erhoben haben. Hätten wir das Geschlecht der Flöhe nicht bestimt, hätten wir hier eventuell einen falschen Schluß getroffen.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-basic.html#sec-vif",
    "href": "stat-modeling-basic.html#sec-vif",
    "title": "38  Multiple lineare Regression",
    "section": "38.5 Variance inflation factor (VIF)",
    "text": "38.5 Variance inflation factor (VIF)\nWenn wir sehr viele Daten erheben, dann kann es sein, dass die Variablen stark miteinander korrelieren. Wir können aber stark mieinander korrelierte Variablen nicht zusammen in ein Modell nehmen. Die Effekte der beiden Variablen würden sich gegenseitig aufheben. Wir hätten eigentlich zwei getrennt signifikante Variablen. Nehmen wir aber beide Variablen mit ins Modell, sind beide Variablen nicht mehr signifikant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWir kürzen hier stark ab. Wenn du mehr über Variablen in einem Modell wissen willst, gibt es dann in den Kapitel zur Variablen Selektion mehr Informationen. Wenn du nur Outcomes und Blöcke vorliegen hast, dann ist das VIF für dich uninteressant.\n\n\n\n\nUm Variablen zu finden, die sich sehr ähnlich verhalten, können wir den Variance inflation factor (VIF) nutzen. Wir können den VIF jedoch nicht für kategoriale Daten verwenden. Statistisch gesehen würde es keinen Sinn machen. Die Anwendung ist recht einfach. Wir fitten als erstes unser Modell und dann können wir die Funktion vif() aus dem R Paket {car} verwenden.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")\nmodel &lt;- lm(height ~ semester + age + count_color + count_bears, data = gummi_tbl)\n\nWir sehen im Folgenden das Eregbnis der Funktion vif(). Wenn der berechnete Wert für den VIF größer als 5 ist, dann liegt mit der Variable ein Problem vor. Wir vermuten dann, dass eine andere variable sehr stark mit dieser Variable korreliert. Wir sehen an den Werten keine Aufälligkeiten.\n\nvif(model)\n\n   semester         age count_color count_bears \n   1.039437    1.030421    1.095417    1.140199 \n\n\nWir können uns mit der Funcktion check_model() aus dem R Paket {performance} auch die Unsicherheit mit angeben lassen. In unserem Beispiel hieft dies gerade nicht sehr viel weiter. Wir bleiben bei den geschätzen Werten und ignorieren das Intervall.\n\n\n\n\n\n\n\n\nAbbildung 38.5— Graphische Darstellung des VIF mit der Funktion check_model().\n\n\n\n\n\nIn Abbildung 38.5 sehen wir nochmal die Visualisierung. Wie immer, manchmal helfen solche Abbildungen, manchmal verwirren die Abbildungen mehr. Wir konzentrieren uns hier auf die Werte des VIF’s und ignorieren die Streuung.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-basic.html#sec-model-basic-compare",
    "href": "stat-modeling-basic.html#sec-model-basic-compare",
    "title": "38  Multiple lineare Regression",
    "section": "38.6 Vergleich von Modellen",
    "text": "38.6 Vergleich von Modellen\nIm Folgenden wollen wir einmal verschiedene Modelle miteinander Vergleichen und uns statistisch wiedergeben lassen, was das beste Modell ist. Und hier holen wir auch einmal kurz Luft, denn wir entschieden nur was das statistisch beste Modell ist. Es kann sein, dass ein Modell biologisch mehr Sinn macht und nicht auf Platz 1 der statistischen Maßzahlen steht. Das ist vollkommen in Ordnung. Du musst abweägen, was für sich das beste Modell ist. Im Zweifel komme ruhig nochmal in meine statistische Beratung oder schreibe mir eine Mail.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWir kürzen hier stark ab bzw. gehen nicht im Detail auf jedes Gütekriterium ein. Wichtig ist, dass du Modelle statistisch vergleichen kannst. Bedenke immmer, dass die statistische Bewertung nicht immer ausreicht! Auch was die Biologie über das Modell sagt ist wichtig.\n\n\n\n\nWir bauchen uns jetzt fünf Modelle von fit_1 bis fit_5. Jedes dieser Modelle hat andere \\(x\\) aber häufig das gleiche Outcome y. In dem Beispiel am Ende des Kapitels nutzen wir auch verschiedene \\(y\\) dafür aber dann gleiche \\(x\\) in dem Modell. Im Weiteren sortieren wir die Modelle von einfach nach komplex. Ich versuche immmer das einfachste Modell fit_1 zu nennen bzw. eher die niedrige Nummer zu geben. Im Idealfall benennst du die Modellobjekte nach den Modellen, die in en Objekten gespeichert sind. Oft sind die Modelle aber sehr groß und die Objekte der Fits haben dann sehr lange Namen.\n\nfit_1 &lt;- lm(jump_length ~ animal, data = model_tbl)\nfit_2 &lt;- lm(jump_length ~ animal + sex, data = model_tbl)\nfit_3 &lt;- lm(jump_length ~ animal + sex + weight, data = model_tbl)\nfit_4 &lt;- lm(jump_length ~ animal + sex + sex:weight, data = model_tbl)\nfit_5 &lt;- lm(log(jump_length) ~ animal + sex, data = model_tbl)\n\nAls Ergänzung zum Bestimmtheitsmaß \\(R^2\\) wollen wir uns noch das Akaike information criterion (abk. \\(AIC\\)) anschauen. Du kannst auch das \\(R^2\\) bzw. das \\(R^2_{adj}\\) für die Modellauswahl nehmen. Das \\(AIC\\) ist neuer und auch für komplexere Modelle geeignet. Es gilt hierbei, je kleiner das \\(AIC\\) ist, desto besser ist das \\(AIC\\). Wir wollen also Modelle haben, die ein kleines \\(AIC\\) haben. Wir gehen jetzt nicht auf die Berechnung der \\(AIC\\)’s für jedes Modell ein. Wir erhalten nur ein \\(AIC\\) für jedes Modell. Die einzelnen Werte des \\(AIC\\)’s sagen nichts aus. Ein \\(AIC\\) ist ein mathematisches Konstrukt. Wir können aber verwandte Modelle mit dem \\(AIC\\) untereinander vergleichen. Daher berechnen wir ein \\(\\Delta\\) über die \\(AIC\\). Dafür nehmen wir das Modell mit dem niedrigsten \\(AIC\\) und berechnen die jeweiligen Differenzen zu den anderen \\(i\\) Modellen. In unserem Beispiel ist \\(i\\) dann gleich fünf, da wir fünf Modelle haben.\n\\[\n\\Delta_i = AIC_i - AIC_{min}\n\\]\n\nwenn \\(\\Delta_i &lt; 2\\), gibt es keinen Unterschied zwischen den Modellen. Das \\(i\\)-te Modell ist genauso gut wie das Modell mit dem \\(AIC_{min}\\).\nwenn \\(2 &lt; \\Delta_i &lt; 4\\), dann gibt es eine starke Unterstützung für das \\(i\\)-te Modell. Das \\(i\\)-te Modell ist immer noch ähnlich gut wie das \\(AIC_{min}\\).\nwenn \\(4 &lt; \\Delta_i &lt; 7\\), dann gibt es deutlich weniger Unterstützung für das \\(i\\)-te Modell;\nModelle mit \\(\\Delta_i &gt; 10\\) sind im Vergleich zu dem besten \\(AIC\\) Modell nicht zu verwenden.\n\nNehmen wir ein \\(AIC_1 = AIC_{min} = 100\\) und \\(AIC_2\\) ist \\(100,7\\) an. Dann ist \\(\\Delta_2=0,7&lt;2\\), so dass es keinen wesentlichen Unterschied zwischen den Modellen gibt. Wir können uns entscheiden, welches der beiden Modelle wir nehmen. Hier ist dann wichtig, was auch die Biologie sagt oder eben andere Kriterien, wie Kosten und Nutzen. Wenn wir ein \\(AIC_1 = AIC_{min} = 100000\\) und \\(AIC_2\\) ist \\(100700\\) vorliegen haben, dann ist \\(\\Delta_2 = 700 \\gg 10\\), also gibt es keinen Grund für das \\(2\\)-te Modell. Das \\(2\\)-te Modell ist substantiell schlechter als das erste Modell. Mehr dazu kannst du unter Multimodel Inference: Understanding AIC and BIC in Model Selection nachlesen.\nWir können das \\(\\Delta_i\\) auch in eine Wahrscheinlichkeit umrechnen. Wir können \\(p_i\\) berechnen und damit die relative (im Vergleich zu \\(AIC_{min}\\)) Wahrscheinlichkeit, dass das \\(i\\)-te Modell den AIC minimiert.\n\\[\np_i = \\exp\\left(\\cfrac{-\\Delta_i}{2}\\right)\n\\]\nZum Beispiel entspricht \\(\\Delta_i = 1.5\\) einem \\(p_i\\) von \\(0.47\\) (ziemlich hoch) und ein \\(\\Delta_i = 15\\) entspricht einem \\(p_i =0.0005\\) (ziemlich niedrig). Im ersten Fall besteht eine Wahrscheinlichkeit von 47%, dass das \\(i\\)-te Modell tatsächlich eine bessere Beschreibung ist als das Modell, das \\(AIC_{min}\\) ergibt, und im zweiten Fall beträgt diese Wahrscheinlichkeit nur 0,05%. Wir können so einmal nachrechnen, ob sich eine Entscheidung für ein anderes Modell lohnen würde. Neben dem \\(AIC\\) gibt es auch das Bayesian information criterion (\\(BIC\\)). Auch beim \\(BIC\\) gilt, je kleiner das BIC ist, desto besser ist das BIC.\nDu siehst schon, es gibt eine Reihe von Möglichkeiten sich mit der Güte oder Qualität eines Modells zu beschäftigen. Wir nutzen die Funktion model_performance() um uns die Informationen über die Güte eines Modells wiedergeben zu lassen. Im folgenden Codeblock habe ich mich nur auf das \\(AIC\\) und das \\(BIC\\) konzentriert.\n\nmodel_performance(fit_1) |&gt; \n  as_tibble() |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1 3076. 3093.\n\n\nGut soweit. Du kannst jetzt für jedes der Modelle das \\(AIC\\) berechnen und dann dir die Modelle entsprechend ordnen. Wir müssen das aber nicht tun. Wir können uns auch die Funktion compare_performance() zu nutze machen. Die Funktion gibt uns die \\(R^2\\)-Werte wieder wie auch die \\(AIC\\) sowie die \\(s^2_{\\epsilon}\\) als sigma wieder. Wir haben also alles zusammen was wir brauchen. Darüber hinaus kann die Funktion auch die Modelle rangieren. Das nutzen wir natürlich gerne.\n\ncomp_res &lt;- compare_performance(fit_1, fit_2, fit_3, fit_4, fit_5, rank = TRUE)\n\ncomp_res\n\n# Comparison of Model Performance Indices\n\nName  | Model |    R2 | R2 (adj.) |  RMSE | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n----------------------------------------------------------------------------------------------------------------\nfit_2 |    lm | 0.739 |     0.738 | 1.926 | 1.933 |       0.644 |        0.650 |       0.959 |            82.69%\nfit_5 |    lm | 0.731 |     0.729 | 0.099 | 0.099 |    2.58e-09 |     2.61e-09 |    3.84e-09 |            56.58%\nfit_3 |    lm | 0.739 |     0.737 | 1.926 | 1.935 |       0.237 |        0.235 |       0.039 |            50.83%\nfit_4 |    lm | 0.739 |     0.737 | 1.925 | 1.935 |       0.119 |        0.115 |       0.002 |            45.01%\nfit_1 |    lm | 0.316 |     0.314 | 3.118 | 3.126 |   5.42e-126 |    5.57e-126 |   7.27e-125 |             0.00%\n\n\nAnhand der Ausgabe der Funktion compare_performance() sehen wir, dass unser Modell fit_2 das beste Modell ist. Zwar ist die Streuung der Residuen nicht die Kleinste aller Modelle (Sigma = 1.933) aber wir haben ein hohes \\(R^2_{adj}\\) und auch ein kleines \\(AIC\\). Wir würden damit sagen, dass das Modell fit_2 mit den Variablen animal und sex für \\(x\\) das Outcome jump_length am besten statistisch beschreibt.\nIn Abbildung 38.6 sehen wir die Ausgabe der Funktion compare_performance() nochmal visualisiert. Wir können dann die einzelnen Modelle nochmal besser vergleichen. Auch siehst du hier, ob ein Modell in einem Bereich sehr schlecht ist oder aber nur in einem Bereich sehr gut.\n\n\n\n\n\n\n\n\nAbbildung 38.6— Graphische Darstellung der Funktion compare_performance() Wir sehen hier die einzelnen Gütekriterien in einer Übersicht dargestellt.\n\n\n\n\n\nZum Ende stellt sich die Frage nach dem statistischen Test. Können wir auch statistisch Testen, ob das Modell fit_1 signifikant unterschiedlich ist? Ja wir können die Funktion test_vuong() nutzen um ein Model zu den anderen Modellen zu vergleichen. Wenn du mehrere Modell miteinander vergleichen möchtest, dann muss du die Funktion mehrfach ausführen.\n\ntest_vuong(fit_1, fit_2, fit_3, fit_4, fit_5)\n\nName  | Model | Omega2 | p (Omega2) |      LR | p (LR)\n------------------------------------------------------\nfit_1 |    lm |        |            |         |       \nfit_2 |    lm |   0.41 |     &lt; .001 |  -18.46 | &lt; .001\nfit_3 |    lm |   0.41 |     &lt; .001 |  -18.45 | &lt; .001\nfit_4 |    lm |   0.41 |     &lt; .001 |  -18.48 | &lt; .001\nfit_5 |    lm |   0.47 |     &lt; .001 | -123.94 | &lt; .001\nEach model is compared to fit_1.\n\n\nNutzen wir die Modellselektion auch einmal an einem konkreten Beispiel. War die Transformation sinnvoll? Wir haben also ein Outcome \\(y\\) vorliegen und wollen wissen, ob wir nicht lieber das Modell mit einem \\(y\\) mit einer \\(log\\)-Transformation rechnen sollten. Wir nutzen dazu den Datensatz mit den Hunde- und Katzenflöhen und die Schlüpfdauer als Outcome. Also wie lange brauchen die Flöhe bis die Flöhe aus den Eiern geschlüpft sind.\n\nmodel_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") |&gt;\n  mutate(log_hatch_time = round(log(hatch_time), 2))\n\nWir bauen uns jetzt zwei Modelle. Zum einen das Modell fit_raw mit der hatch_time und den Variablen ainmal und sex. Das zweite Modell enthält die log(hatch_time) also das Outcome \\(y\\) als \\(log\\)-Transformation. Wiederum sind die \\(x\\) variablen die gleichen wie im untransformierten Modell. Wir fitten beide Modelle und speichern die Objekte entsprechend ab.\n\nfit_raw &lt;- lm(hatch_time ~ animal + sex, data = model_tbl)\nfit_log &lt;- lm(log_hatch_time ~ animal + sex, data = model_tbl)\n\nDie Funktion compare_performance() erlaubt uns wieder die beiden Modelle fit_raw und fit_log miteinander zu vergleichen. Wir erhalten die folgende Ausgabe der Funktion.\n\ncomp_res &lt;- compare_performance(fit_raw, fit_log, rank = TRUE)\n\ncomp_res\n\n# Comparison of Model Performance Indices\n\nName    | Model |    R2 | R2 (adj.) |    RMSE |   Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n----------------------------------------------------------------------------------------------------------------------\nfit_log |    lm | 0.006 |     0.001 |   0.992 |   0.995 |        1.00 |         1.00 |        1.00 |            71.43%\nfit_raw |    lm | 0.008 |     0.003 | 789.441 | 792.086 |    0.00e+00 |     0.00e+00 |    0.00e+00 |            28.57%\n\n\nIn diesem Beispiel wäre das \\(log\\)-transformierte Modell das bessere statistische Modell. Wir müssen jetzt überlegen, ob wir den Preis dafür bezahlen wollen. Wenn wir nämlich alles mit der \\(log\\)-Transformation rechnen, dann erhalten wir auch alle Koeffizienten auf der \\(log\\)-Skala (eng. log scale). Hier kannst du nur von Fall zu Fall entscheiden.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-basic.html#referenzen",
    "href": "stat-modeling-basic.html#referenzen",
    "title": "38  Multiple lineare Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 38.1— Graphische Darstellung der Interpretation von einem kontinuierlichen \\(x\\).\nAbbildung 38.2— Graphische Darstellung der Interpretation von einem kategoriellen \\(x\\) mit 2 Leveln.\nAbbildung 38.3— Graphische Darstellung der Interpretation von einem kategoriellen \\(x\\) mit 3 Leveln.\nAbbildung 38.4 (a)— jump_length ~ weight\nAbbildung 38.4 (b)— jump_length ~ weight + animal\nAbbildung 38.4 (c)— jump_length ~ weight + animal + sex\nAbbildung 38.5— Graphische Darstellung des VIF mit der Funktion check_model().\nAbbildung 38.6— Graphische Darstellung der Funktion compare_performance() Wir sehen hier die einzelnen Gütekriterien in einer Übersicht dargestellt.\n\n\n\nKéry M. 2010. Introduction to WinBUGS for ecologists: Bayesian approach to regression, ANOVA, mixed models and related analyses. Academic Press.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-quality.html",
    "href": "stat-linear-reg-quality.html",
    "title": "39  Modelgüte",
    "section": "",
    "text": "39.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance, ggResidpanel, ggdist)\n\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Modelgüte</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-quality.html#daten",
    "href": "stat-linear-reg-quality.html#daten",
    "title": "39  Modelgüte",
    "section": "39.2 Daten",
    "text": "39.2 Daten\nNachdem wir uns im vorherigen Kapitel mit einem sehr kleinen Datensatz beschäftigt haben, nehmen wir einen großen Datensatz. Bleiben aber bei einem simplen Modell. Wir brauchen dafür den Datensatz flea_dog_cat_length_weight.xlsx. In einer simplen linearen Regression schauen wir uns den Zusammenhang zwischen einem \\(y\\) und einem \\(x_1\\) an. Daher wählen wir aus dem Datensatz die beiden Spalten jump_length und weight. Wir wollen nun feststellen, ob es einen Zusammenhang zwischen der Sprungweite in [cm] und dem Flohgewicht in [mg] gibt. In dem Datensatz finden wir 400 Flöhe von Hunden und Katzen.\n\nmodel_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") |&gt;\n  select(animal, jump_length, weight)\n\nIn der Tabelle 55.1 ist der Datensatz model_tbl nochmal dargestellt.\n\n\n\n\nTabelle 39.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\n\n\n\n\n\nanimal\njump_length\nweight\n\n\n\n\ncat\n15.79\n6.02\n\n\ncat\n18.33\n5.99\n\n\ncat\n17.58\n8.05\n\n\ncat\n14.09\n6.71\n\n\ncat\n18.22\n6.19\n\n\ncat\n13.49\n8.18\n\n\ncat\n16.28\n7.46\n\n\n\n\n\n\n\n\nIm Folgenden ignorieren wir, dass die Sprungweiten und die Gewichte der Flöhe auch noch von den Hunden oder Katzen sowie dem unterschiedlichen Geschlecht der Flöhe abhängen könnten. Wir schmeißen alles in einen Pott und schauen nur auf den Zusammenhang von Sprungweite und Gewicht.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Modelgüte</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-quality.html#simples-lineares-modell",
    "href": "stat-linear-reg-quality.html#simples-lineares-modell",
    "title": "39  Modelgüte",
    "section": "39.3 Simples lineares Modell",
    "text": "39.3 Simples lineares Modell\nFür dieses Kapitel nehmen wir nur ein simples lineares Modell mit nur einem Einflussfaktor weight auf die Sprunglänge jump_length. Später kannst du dann noch komplexere Modelle rechnen mit mehr Einflussfaktoren \\(x\\) oder aber einer anderen Verteilungsfamilie für \\(y\\). Wir erhalten dann das Objekt fit_1 aus einer simplen linearen Gaussianregression was wir dann im Weiteren nutzen werden.\n\nfit_1 &lt;- lm(jump_length ~ weight, data = model_tbl)\n\nWir nutzen jetzt dieses simple lineare Modell für die weiteren Gütekritierien, da wir es uns hier erstmal etwas einfacher machen wollen. Dann erhalten wir folgende zusammenfassende Ausgabe von dem Modell, was wir uns jetzt im Bezug auf die Residuals und dem R-squared näher anschauen wollen. Alle anderen Werte haben wir ja schon in dem Kapitel zur simplen linearen Regression besprochen.\n\nfit_1 |&gt; \n  summary()\n\n\nCall:\nlm(formula = jump_length ~ weight, data = model_tbl)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3240 -2.2511  0.0488  2.3147  9.3958 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.79212    0.76901   12.73   &lt;2e-16 ***\nweight       1.34378    0.09491   14.16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.268 on 598 degrees of freedom\nMultiple R-squared:  0.2511,    Adjusted R-squared:  0.2498 \nF-statistic: 200.5 on 1 and 598 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Modelgüte</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-quality.html#sec-linreg-residual",
    "href": "stat-linear-reg-quality.html#sec-linreg-residual",
    "title": "39  Modelgüte",
    "section": "39.4 Residualplot",
    "text": "39.4 Residualplot\nDie erste Frage ist natürlich, geht die Gerade aus dem lm()-Fit mittig durch die Punkte? Liegen also alle Punkte gleichmaäßig um die Gerade verteilt. Das ist ja die Idee der linearen Regression, wir legen eine Gerade durch eine Punktewolke und wollen das die Abstände von den Beobachtungen unterhalb und oberhalb der Gerade in etwa gleich sind. Die Abstände von den Beobachtungen nennen wir auch \\(\\epsilon\\) als Fehler oder aber eben Residuen. Das R Paket {olsrr} mit der Hilfeseite Residual Diagnostics liefert noch mehr Möglichkeiten der Diagnose der Modellgüte über die Residuen als hier vorgestellt. Das R Paket {olsrr} funktioniert aber nur für einen normalverteiltes Outcome \\(y\\).\nWir wollen nun mit dem Residualplot die Frage beantworten, ob die Gerade mittig durch die Punktewolke läuft. Die Residuen \\(\\epsilon\\) sollen normalverteilt sein mit einem Mittelwert von Null und einer Varianz von \\(s^2_{\\epsilon}\\) und somit gilt \\(\\epsilon \\sim \\mathcal{N}(0, s^2_{\\epsilon})\\). In R wird in Modellausgaben die Standardabweichung der Residuen \\(s_{\\epsilon}\\) häufig als sigma bezeichnet. Wir können auch hier die Funktion augment() aus dem R Paket {broom} nutzen um uns die Residuen wiedergeben zu lassen. Wir erhalten somit die Residuen resid und die angepassten Werte .fitted auf der Geraden über die Funktion augment(). Die Funktion augment() gibt noch mehr Informationen wieder, aber wir wollen uns jetzt erstmal auf die Residuen und deren Derivate konzentrieren.\n\nresid_plot_tbl &lt;- fit_1 |&gt; \n  augment() |&gt; \n  select(-.hat, -.cooksd)\nresid_plot_tbl\n\n# A tibble: 600 × 6\n   jump_length weight .fitted .resid .sigma .std.resid\n         &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1        15.8   6.02    17.9 -2.09    3.27    -0.642 \n 2        18.3   5.99    17.8  0.489   3.27     0.150 \n 3        17.6   8.05    20.6 -3.03    3.27    -0.928 \n 4        14.1   6.71    18.8 -4.72    3.27    -1.45  \n 5        18.2   6.19    18.1  0.110   3.27     0.0337\n 6        13.5   8.18    20.8 -7.29    3.26    -2.23  \n 7        16.3   7.46    19.8 -3.54    3.27    -1.08  \n 8        14.5   5.58    17.3 -2.75    3.27    -0.844 \n 9        16.4   6.19    18.1 -1.75    3.27    -0.537 \n10        15.1   7.53    19.9 -4.83    3.26    -1.48  \n# ℹ 590 more rows\n\n\nWir erhalten jetzt folgende Information über unser Modell aus der Ausgabe der Funktion augment() wiedergeben. Ich habe hier einmal sehr kurz die Informationen erklärt.\n\n.fitted sind die vorhergesagten Werte auf der Geraden. Wir bezeichnen diese Werte auch die \\(\\hat{y}\\) Werte.\n.resid sind die Residuen oder auch \\(\\epsilon\\). Daher der Abstand zwischen den beobachteten \\(y\\)-Werten in der Spalte jump_length und den \\(\\hat{y}\\) Werten auf der Geraden und somit der Spalte .fitted.\n.sigma beschreibt die geschätzte \\(s^2_{\\epsilon}\\) , wenn die entsprechende Beobachtung aus dem Modell herausgenommen wird.\nstd.resid sind die standardisierten Residuen. Dabei werden die Residuen durch die Standardabweichung der Residuen \\(s_{\\epsilon}\\) geteilt. Wir rechnen hier mehr oder minder den Quotienten der Spalten .resid/.sigma. Die standardisierten Residuen folgen dann einer Standardnormalverteilung. Dazu dann aber gleich noch mehr.\n\nDie Daten selber interessieren uns nicht einer Tabelle. Stattdessen zeichnen wir einmal den Residualplot. Bei dem Residualplot tragen wir die Werte der Residuen .resid auf die \\(y\\)-Achse auf und die angepassten y-Werte auf der Geraden .fitted auf die \\(x\\)-Achse. Wir kippen im Prinzip die angepasste Gerade so, dass die Gerade parallel zu \\(x\\)-Achse läuft. Dann kommen wir auch schon zum berühmten Zitat, wie ein Residualplot aussehen soll.\n\n“The residual plot should look like the sky at night, with no pattern of any sort.”\n\nIn Abbildung 39.1 sehen wir den Residualplot von unseren Beispieldaten. Wir sehen, dass wir keine Struktur in der Punktewolke erkennen. Auch sind die Punkte gleichmäßig um die Gerade verteilt. Wir haben zwar einen Punkt, der sehr weit von der Gerade weg ist, das können wir aber ignorieren. Später können wir uns noch in dem Kapitel zu den Ausreißern überlegen, ob wir einen Ausreißer (eng. outlier) vorliegen haben.\n\nggplot(resid_plot_tbl, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"#CC79A7\") +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 39.1— Residualplot der Residuen des Models fit_1. Die rote Linie stellt die geschätzte Gerade da. Die Punkte sollen gleichmäßig und ohne eine Struktur um die Gerade verteilt sein.\n\n\n\n\n\nEs gibt viele Möglichkeiten sich die Residuen anzuschauen. Wir haben hier einmal den klassischen Residualplot nachgebaut. Wenn du dir andere Pakete anschaust, dann kriegts du immer eine Reihe von Abbildungen und nicht nur einen Scatterplot als Residualplot. Das R Paket {ggResidpanel} und die Webseite liefert eine Kombination von verschiedenen Abbildungen mit dem R Paket plotly, so dass wir hier mit der Funktion resid_interact() eine interaktive Abbildung vorliegen haben. Die statische Abbildung kannst du dir über die Funktion resid_panel() erstellen. Über den QQ-Plot erfährst du dann im nächsten Abschnitt mehr.\n\nfit_1 |&gt; \n  resid_interact()\n\n\n\n\n\n\n\nAbbildung 39.2— Residualpanel der Residuen des Models fit_1. Verschiedene Abbildungen geben Informationen über die Residuen und deren Eigenschaften wieder. Durch die Kombination mit dem R Paket {plotly} können wir direkt Informationen aus der Abbildung ablesen.\n\n\n\n\n\n\n\n\n\n\nVerschiedene Arten von Residuen\n\n\n\nResiduen beschreiben je den Anteil des Modells, den wir nicht erklären können. Neben den klassischen Residuen (eng. ordinary), die wir aus einer Regression erhalten, gibt es noch andere Varianten von Residuen. Hier sind die beiden bekanntesten Residuen mit standardisierte Residuen und den studentisierten Residuen. Beide Arten sind sehr nützlich, wenn wir verschiedene Modelle untereinander vergleichen wollen. Hier dann nochmal die Definition.\n\nStandardisierte Residuen (eng. standardized residuals)\n\n… sind Residuen, die durch eine Schätzung ihrer Standardabweichung geteilt werden, mit dem Ergebnis, dass sie unabhängig von der Skala des Ergebnisses eine Standardabweichung sehr nahe bei 1 haben. Wir erhalten die standardisierte Residuen mit der R Funktion rstandard().\n\nStudentisierte Residuen (eng. studentized residuals)\n\n… sind den standardisierten Residuen ähnlich, außer dass für jeden Fall das Residuum durch die Standardabweichung geteilt wird, die aus der Regression ohne diese Beobachtung geschätzt wurde. Wir erhalten die studentisierte Residuen mit der R Funktion rstudent().\n\n\nBeide Arten von Residuen können wir mit der Standardnormalverteilung vergleichen. Ein standardisiertes Residuum von 2 entspricht zum Beispiel einem Punkt, der 2 Standardabweichungen über der Regressionslinie liegt. Damit haben wir dann eine Idee, wie weit weg einzelne Beobachtungen von der Geraden liegen. Beide Arten der Residuen haben ja keine Einheit mehr und so lässt sich hier recht einfach eine allgemeine Aussage machen. Beobachtungen, die ein Residuum von 2 oder mehr haben, liegen damit sehr weit von der Geraden entfernt. Im Folgenden nochmal der Vergleich aller drei Residuen Arten für unser Modell fit_1.\n\nresid_tbl &lt;- tibble(Ordinary = resid(fit_1),\n                    Standard = rstandard(fit_1),\n                    Student = rstudent(fit_1)) |&gt; \n  gather()\n\nDann können wir auch einmal die Mittelwerte und die Standardabweichung berechnen. Bei der relativ großen Fallzahl in unseren Daten sind die standardisierten und die studentisierten Residuen sehr ähnlich von den Werten.\n\nresid_tbl |&gt; \n  group_by(key) |&gt; \n  summarise(mean(value), sd(value)) |&gt; \n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 3 × 3\n  key      `mean(value)` `sd(value)`\n  &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n1 Ordinary             0        3.27\n2 Standard             0        1   \n3 Student              0        1   \n\n\nIm Folgenden nutzen wir einmal das R Paket {ggdist} um uns in der Abbildung 39.3 die drei Arten der Residuen einmal zu visualisieren. Wo die klassischen Residuen noch eine sehr weite Verteilung haben sind die beiden einheitslosen Arten der Residuen sehr ähnlich und kompakt. Wir können daher sagen, das Beobachtungen mit einem Residuum größer als 2 schon sehr auffällige Werte haben und sehr weit von der angepassten Gerade liegen.\n\nggplot(resid_tbl, aes(value, fct_rev(key), fill = key)) +\n  theme_minimal() +\n  stat_halfeye() +\n  scale_fill_okabeito() +\n  labs(y = \"\", x = \"Residuen\")\n\n\n\n\n\n\n\nAbbildung 39.3— Vergleich der klassischen Residuen mit den standardisierten und den studentisierten Residuen. Da die beiden letzteren einheitslos sind lassen sich die Residuen auch in dem Sinne einer Standardnormalverteilung interpretieren.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Modelgüte</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-quality.html#sec-linreg-qq",
    "href": "stat-linear-reg-quality.html#sec-linreg-qq",
    "title": "39  Modelgüte",
    "section": "39.5 QQ-Plot",
    "text": "39.5 QQ-Plot\nMit dem Quantile-Quantile Plot oder kurz QQ-Plot können wir überprüfen, ob unser \\(y\\) aus einer Normalverteilung stammt. Oder andersherum, ob unser \\(y\\) approximativ normalverteilt ist. Der QQ-Plot ist ein visuelles Tool. Daher musst du immer schauen, ob dir das Ergebnis passt oder die Abweichungen zu groß sind. Es hilft dann manchmal die Daten zum Beispiel einmal zu \\(log\\)-Transformieren und dann die beiden QQ-Plots miteinander zu vergleichen.\nWir brauchen für einen QQ-Plot eigentlich viele Beobachtungen. Das heißt, wir brauchen auf jeden Fall mehr als 20 Beobachtungen. Dann ist es auch häufig schwierig den QQ-Plot zu bewerten, wenn es viele Behandlungsgruppen oder Blöcke gibt. Am Ende haben wir dann zwar mehr als 20 Beobachtungen aber pro Kombination Behandlung und Block nur vier Wiederholungen. Und vier Wiederholungen sind zu wenig für eine sinnvolle Interpretation eines QQ-Plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDas klingt hier alles etwas wage… Ja, das stimmt. Aber wir wenden hier den QQ-Plot erstmal an und schauen uns dann im Anschluss nochmal genauer an, wie der QQ-Plot entsteht. Das kannst du dann einmal unten im Kasten nachlesen.\n\n\n\n\nGrob gesprochen vergleicht der QQ Plot die Quantile der vorliegenden Beobachtungen, in unserem Fall der Variablen jump_length, mir den Quantilen einer theoretischen Normalverteilung, die sich aus den Daten mit dem Mittelwert und der Standardabweichung von jump_length ergeben würden. Wir können die Annahme der Normalverteilung recht einfach in ggplot überprüfen. Wir sehen in Abbildung 39.4 den QQ-Plot für die Variable jump_length. Die Punkte sollten alle auf einer Diagonalen liegen. Hier dargestellt durch die rote Linie. Häufig weichen die Punkte am Anfang und Ende der Spannweite der Beobachtungen etwas ab.\n\nggplot(model_tbl, aes(sample = jump_length)) + \n  stat_qq() + \n  stat_qq_line(color = \"#CC79A7\") +\n  labs(x = \"Theoretischen Quantile der Standardnormalverteilung\",\n       y = \"Werte der beobachteten Stichprobe\") + \n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 39.4— QQ-Plot der Sprungweite in [cm]. Die Gerade geht einmal durch die Mitte der Punkte und die Punkte liegen nicht exakt auf der Geraden. Eine leichte Abweichung von der Normalverteilung könnte vorliegen.\n\n\n\n\n\nWir werden uns später auch noch häufig die Residuen aus den Modellen anschauen. Die Residuen müssen nach dem Fit des Modells einer Normalverteilung folgen. Wir können diese Annahme an die Residuen mit einem QQ-Plot überprüfen. In Abbildung 39.5 sehen wir die Residuen aus dem Modell fit_1 in einem QQ-Plot. Wir würden sagen, dass die Residuen approximativ normalverteilt sind. Die Punkte liegen fast alle auf der roten Diagonalen.\n\nggplot(resid_plot_tbl, aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line(color = \"#CC79A7\") +\n  theme_minimal() +\n  labs(x = \"Theoretischen Quantile der Standardnormalverteilung\",\n       y = \"Residuen des Modells\")\n\n\n\n\n\n\n\nAbbildung 39.5— QQ-Plot der Residuen aus dem Modell fit_1. Die Residuen müssen einer approximativen Normalverteilung folgen, sonst hat der Fit des Modelles nicht funktioniert.\n\n\n\n\n\n\n\n\n\n\n\nDen QQ-Plot per Hand erstellen und verstehen\n\n\n\nGut das war jetzt die Anwendung und wie bauen wir uns jetzt einen QQ-Plot per Hand selber? Manchmal versteht man dann ja einen Algorithmus besser, wenn man ihn selber gebaut hat. Nehmen wir also einfach mal eine Reihe von Zahlen \\(x\\) und schauen, ob diese Zahlen einer Normalverteilung folgen. Wie oben schon gezeigt, sind die meist die Residuen einer linearen Regression. Wenn die Residuen einer Normalverteilung folgen, dann folgen auch unsere Daten einer Normalverteilung.\n\nx &lt;- c(7.19, 6.31, 5.89, 4.5, 3.77, 4.25, 5.19, 5.79, 6.79)\n\nDie Idee ist jetzt, dass wir eine Normalverteilung in \\(n\\) Blöcke aufteilen. Dabei ist \\(n\\) die Anzahl unserer Beobachtungen. In unserem Fall wäre \\(n\\) gleich 9. Wenn unsere Residuen normalverteilt wären, dann wären die \\(x\\)-Werte gleichmäßig um einen Mittelwert einer Normalverteilung verteilt. Jetzt ist natürlich die Frage, wie kriegen wir die Aufteilung hin?\n\nn &lt;- length(x)\n\nDafür rangieren wir einmal unsere Daten mit der Funktion rank(). Das klappt in diesem Beispiel nur, da wir keine Bindungen in den Daten \\(x\\) vorliegen haben. Mit Bindungen sind gleiche Werte gemeint.\n\nr &lt;- rank(x)\nr\n\n[1] 9 7 6 3 1 2 4 5 8\n\n\nDann berechnen wir noch die Wahrscheinlichkeit des Auftreten des Ranges mit \\(p=(r − 0.5)/n\\). Die Formel fällt jetzt mal aus dem Himmel, ist aber nichts anderes als eine Transformation von einem Rang zu einer Wahrscheinlichkeit.\n\np &lt;- (r - 0.5) / n\np |&gt; round(2)\n\n[1] 0.94 0.72 0.61 0.28 0.06 0.17 0.39 0.50 0.83\n\n\nJetzt können wir mit der Funktion qnorm() jeder Wahrscheinlichkeit p den entsprechenden \\(z\\)-Wert einer Normalverteilung zuordnen. Wir berechnen nichts anderes als die Werte auf der \\(x\\)-Achse einer Normalverteilung. Bei einer Normalverteilung heißen ja die \\(x\\)-Werte eben \\(z\\)-Werte.\n\nz &lt;- qnorm(p)           \nz\n\n[1]  1.5932188  0.5894558  0.2822161 -0.5894558 -1.5932188 -0.9674216 -0.2822161\n[8]  0.0000000  0.9674216\n\n\nIn der Abbildung 39.6 habe ich dir einmal die neun \\(z\\)-Werte unserer entsprechenden \\(x\\)-Werte in einer Normalverteilung dargestellt. Die Fläche zwischen den \\(z\\)-Werten ist dabei immer gleich. Du siehst, dass die Werte entsprechend gleichmäßig über eine Normalverteilung verteilt sind.\n\n\n\n\n\n\n\n\nAbbildung 39.6— Darstellung der \\(z\\)-Werte von unseren neun Datenpunkten \\(x\\) nach der Rangierung und Umwandlung in eine Wahrscheinlichkeit.\n\n\n\n\n\nDann können wir auch schon unseren QQ-Plot in der Abbildung 39.7 erstellen. Wir haben hier auf der \\(x\\)-Achse wieder die theoretischen Quantile der Standardnormalverteilung oder auch unsere \\(z\\)-Werte. Auf der \\(y\\)-Achse sind dann die Werte der beobachteten Stichprobe \\(x\\) aufgetragen. Wenn die Werte alle auf einer Geraden liegen, dann haben wir eine zumindestens approximative Normalverteilung vorliegen.\n\nggplot(tibble(x, z), aes(z, x)) +\n  theme_minimal() +\n  geom_point(alpha = 0.5) +\n  geom_text(aes(label = x), vjust = 0, hjust = 0.5) +\n  labs(x = \"Theoretischen Quantile der Standardnormalverteilung\",\n       y = \"Werte der beobachteten Stichprobe\")\n\n\n\n\n\n\n\nAbbildung 39.7— QQ-Plot unserer Beobachtungen \\(x\\). Die berechneten, theoretischen \\(z\\)-Werte sind auf der \\(x\\)-Achse dargestellt, die Werte unserer Beobachtungen \\(x\\) auf der \\(y\\)-Achse. Die Zahlen repräsentieren die Werte der Beobachtungen wieder.\n\n\n\n\n\nJetzt müssen wir nur noch die Gerade in dem QQ-Plot nachbauen. Hier nutzen wir das \\(1^{st}\\) und \\(3^{rd}\\) Quartile der beobachten Werte \\(x\\) sowie der theoretischen Normalverteilung. Dann können wir uns den unteren Punkt p1 sowie den oberen Punkt p2 berechnen und durch diese Punkte dann die Gerade zeichnen.\n\nref_prob &lt;- c(.25, .75)               \np1 &lt;- qnorm(ref_prob) \np2 &lt;- quantile(x, ref_prob)            \n\nIn der Abbildung 39.8 siehst du dann einmal eine partielle QQ-Linie durch die Punkte des \\(1^{st}\\) und \\(3^{rd}\\) Quartiles. In der echten Anwendung wird die gerade dann noch weitergezeichnet, aber hier kann man dann gut sehen, wie die Gerade bestimmt wird. Hier also nur eine Demonstration der QQ-Linie.\n\nggplot(tibble(x, z), aes(z, x)) +\n  theme_minimal() +\n  geom_point() +\n  geom_segment(aes(p1[1], p2[1], xend = p1[2], yend = p2[2]), color = \"#CC79A7\") +\n  annotate(\"text\", x = c(-0.8, 0.8), y = c(4.5, 6.31), color = \"#CC79A7\",\n           label = c(expression(1^{st}), expression(3^{rd}))) +\n  labs(x = \"Theoretischen Quantile der Standardnormalverteilung\",\n       y = \"Werte der beobachteten Stichprobe\")\n\nWarning in geom_segment(aes(p1[1], p2[1], xend = p1[2], yend = p2[2]), color = \"#CC79A7\"): All aesthetics have length 1, but the data has 9 rows.\nℹ Did you mean to use `annotate()`?\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\nAbbildung 39.8— Partielle QQ-Linie durch den QQ-Plot. Die QQ-Linie basiert auf den \\(1^{st}\\) und \\(3^{rd}\\) Quartil der beobachten Werte \\(x\\) sowie der theoretischen Normalverteilung.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Modelgüte</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-quality.html#sec-linreg-bestimmt",
    "href": "stat-linear-reg-quality.html#sec-linreg-bestimmt",
    "title": "39  Modelgüte",
    "section": "39.6 Bestimmtheitsmaß \\(R^2\\)",
    "text": "39.6 Bestimmtheitsmaß \\(R^2\\)\nNachdem wir nun wissen wie gut die Gerade durch die Punkte läuft, wollen wir noch bestimmen wie genau die Punkte auf der Geraden liegen. Das heißt wir wollen mit dem Bestimmtheitsmaß \\(R^2\\) ausdrücken wie stark die Punkte um die Gerade variieren. Wir können folgende Aussage über das Bestimmtheitsmaß \\(R^2\\) treffen. Die Abbildung 39.9 visualisiert nochmal den Zusammenhang.\n\nwenn alle Punkte auf der Geraden liegen, dann ist das Bestimmtheitsmaß \\(R^2\\) gleich 1.\nwenn alle Punkte sehr stark um die Gerade streuen, dann läuft das Bestimmtheitsmaß \\(R^2\\) gegen 0.\n\n\n\n\n\n\n\nAbbildung 39.9— Visualisierung des Bestimmtheitsmaßes \\(R^2\\). Auf der linken Seite sehen wir eine perfekte Übereinstimmung der Punkte und der geschätzten Gerade. Wir haben ein \\(R^2\\) von 1 vorliegen. Sind die Punkte und die geschätzte Gerade nicht deckungsgleich, so läuft das \\(R^2\\) gegen 0.\n\n\n\nDa die Streuung um die Gerade auch gleichzeitig die Varianz widerspiegelt, können wir auch sagen, dass wenn alle Punkte auf der Geraden liegen, die Varianz gleich Null ist. Die Einflussvariable \\(x_1\\) erklärt die gesamte Varianz, die durch die Beobachtungen verursacht wurde. Damit beschreibt das Bestimmtheitsmaß \\(R^2\\) auch den Anteil der Varianz, der durch die lineare Regression, daher der Graden, erklärt wird. Wenn wir ein Bestimmtheitsmaß \\(R^2\\) von Eins haben, wird die gesamte Varianz von unserem Modell erklärt. Haben wir ein Bestimmtheitsmaß \\(R^2\\) von Null, wird gar keine Varianz von unserem Modell erklärt. Damit ist ein niedriges Bestimmtheitsmaß \\(R^2\\) schlecht.\nIm Folgenden können wir uns noch einmal die Formel des Bestimmtheitsmaß \\(R^2\\) anschauen um etwas besser zu verstehen, wie die Zusammenhänge mathematisch sind.\n\\[\n\\mathit{R}^2 =\n\\cfrac{\\sum_{i=1}^N \\left(\\hat{y}_i- \\bar{y}\\right)^2}{\\sum_{i=1}^N \\left(y_i - \\bar{y}\\right)^2}\n\\]\nIn der Abbildung 39.10 sehen wir den Zusammenhang nochmal visualisiert. Wenn die Abstände von dem Mittelwert zu den einzelnen Punkten mit \\(y_i - \\bar{y}\\) gleich dem Abstand der Mittelwerte zu den Punkten auf der Geraden mit \\(\\hat{y}_i- \\bar{y}\\) ist, dann haben wir einen perfekten Zusammenhang.\n\n\n\n\n\n\nAbbildung 39.10— Auf der linken Seite sehen wir eine Gerade die nicht perfekt durch die Punkte läuft. Wir nehmen ein Bestimmtheitsmaß \\(R^2\\) von ca. 0.7 an. Die Abstände der einzelnen Beobachtungen \\(y_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\) ist nicht gleich den Werten auf der Geraden \\(\\hat{y}_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\). Dieser Zusammenhang wird in der rechten Abbildung mit einem Bestimmtheitsmaß \\(R^2\\) von 1 nochmal deutlich.\n\n\n\nWir können die Funktion glance() nutzen um uns das r.squared und das adj.r.squared wiedergeben zu lassen.\n\nfit_1 |&gt; \n  glance() |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.251         0.250\n\n\nWir nutzen grundsätzlich das adjustierte \\(R^2\\)adj.r.squared in der Anwendung. Wir haben wir ein \\(R^2\\) von \\(0.31\\) vorliegen. Damit erklärt unser Modell bzw. die Gerade 31% der Varianz. Das ist jetzt nicht viel, aber wundert uns auch erstmal nicht. Wir haben ja die Faktoren animal und sex ignoriert. Beide Faktoren könnten ja auch einen Teil der Varianz erklären. Dafür müssten wir aber eine multiple lineare Regression mit mehren \\(x\\) rechnen.\nWenn wir eine multiple Regression rechnen, dann nutzen wir das adjustierte \\(R^2\\) in der Anwendung. Das hat den Grund, dass das \\(R^2\\) automatisch ansteigt je mehr Variablen wir in das Modell nehmen. Jede neue Variable wird immer etwas erklären. Um dieses Überanpassen (eng. overfitting) zu vermeiden nutzen wir das adjustierte \\(R^2\\). Im Falle des adjustierte \\(R^2\\) wird ein Strafterm eingeführt, der das adjustierte \\(R^2\\) kleiner macht je mehr Einflussvariablen in das Modell aufgenommen werdenn.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Modelgüte</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-quality.html#das-r-paket-performance",
    "href": "stat-linear-reg-quality.html#das-r-paket-performance",
    "title": "39  Modelgüte",
    "section": "39.7 Das R Paket {performance}",
    "text": "39.7 Das R Paket {performance}\nAbschließend möchte ich hier nochmal das R Paket {performance} vorstellen. Wir können mit dem Paket auch die Normalverteilungsannahme der Residuen überprüfen. Das geht ganz einfach mit der Funktion check_normality() in die wir einfach das Objekt mit dem Fit des Modells übergeben.\n\ncheck_normality(fit_1)\n\nOK: residuals appear as normally distributed (p = 0.555).\n\n\nWir haben auch die Möglichkeit uns einen Plot der Modellgüte anzeigen zu lassen. In Abbildung 39.11 sehen wir die Übersicht von bis zu sechs Abbildungen, die uns Informationen zu der Modellgüte liefern. Wir müssen nur den Fit unseres Modells an die Funktion check_model() übergeben. Das Schöne an der Funktion ist, dass jeder Subplot eine Beschreibung in Englisch hat, wie der Plot auszusehen hat, wenn alles gut mit dem Modellieren funktioniert hat. Wir kommen dann in der multiplen linearen Regression nochmal auf das Paket {performance} zurück. Für dieses Kapitel reicht dieser kurze Abriss.\n\ncheck_model(fit_1, colors = cbbPalette[6:8])\n\n\n\n\n\n\n\nAbbildung 39.11— Übersicht der Plots zu der Modellgüte aus der Funktion check_model().\n\n\n\n\n\n\n\n\nAbbildung 39.1— Residualplot der Residuen des Models fit_1. Die rote Linie stellt die geschätzte Gerade da. Die Punkte sollen gleichmäßig und ohne eine Struktur um die Gerade verteilt sein.\nAbbildung 39.3— Vergleich der klassischen Residuen mit den standardisierten und den studentisierten Residuen. Da die beiden letzteren einheitslos sind lassen sich die Residuen auch in dem Sinne einer Standardnormalverteilung interpretieren.\nAbbildung 39.4— QQ-Plot der Sprungweite in [cm]. Die Gerade geht einmal durch die Mitte der Punkte und die Punkte liegen nicht exakt auf der Geraden. Eine leichte Abweichung von der Normalverteilung könnte vorliegen.\nAbbildung 39.5— QQ-Plot der Residuen aus dem Modell fit_1. Die Residuen müssen einer approximativen Normalverteilung folgen, sonst hat der Fit des Modelles nicht funktioniert.\nAbbildung 39.6— Darstellung der \\(z\\)-Werte von unseren neun Datenpunkten \\(x\\) nach der Rangierung und Umwandlung in eine Wahrscheinlichkeit.\nAbbildung 39.7— QQ-Plot unserer Beobachtungen \\(x\\). Die berechneten, theoretischen \\(z\\)-Werte sind auf der \\(x\\)-Achse dargestellt, die Werte unserer Beobachtungen \\(x\\) auf der \\(y\\)-Achse. Die Zahlen repräsentieren die Werte der Beobachtungen wieder.\nAbbildung 39.8— Partielle QQ-Linie durch den QQ-Plot. Die QQ-Linie basiert auf den \\(1^{st}\\) und \\(3^{rd}\\) Quartil der beobachten Werte \\(x\\) sowie der theoretischen Normalverteilung.\nAbbildung 39.9— Visualisierung des Bestimmtheitsmaßes \\(R^2\\). Auf der linken Seite sehen wir eine perfekte Übereinstimmung der Punkte und der geschätzten Gerade. Wir haben ein \\(R^2\\) von 1 vorliegen. Sind die Punkte und die geschätzte Gerade nicht deckungsgleich, so läuft das \\(R^2\\) gegen 0.\nAbbildung 39.10— Auf der linken Seite sehen wir eine Gerade die nicht perfekt durch die Punkte läuft. Wir nehmen ein Bestimmtheitsmaß \\(R^2\\) von ca. 0.7 an. Die Abstände der einzelnen Beobachtungen \\(y_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\) ist nicht gleich den Werten auf der Geraden \\(\\hat{y}_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\). Dieser Zusammenhang wird in der rechten Abbildung mit einem Bestimmtheitsmaß \\(R^2\\) von 1 nochmal deutlich.\nAbbildung 39.11— Übersicht der Plots zu der Modellgüte aus der Funktion check_model().",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Modelgüte</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-corr.html",
    "href": "stat-linear-reg-corr.html",
    "title": "40  Korrelation",
    "section": "",
    "text": "40.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, readxl, janitor,\n               corrplot, GGally, ggraph, correlation, see,\n               conflicted)\nconflicts_prefer(dplyr::filter)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-corr.html#daten",
    "href": "stat-linear-reg-corr.html#daten",
    "title": "40  Korrelation",
    "section": "40.2 Daten",
    "text": "40.2 Daten\nWir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die Korrelation auf sehr großen Datensätzen berechnen, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit \\(n=7\\) Beobachtungen. Wir nutzen den Datensatz ist gleich für die händischen Berechnungen, da brauchen wir nicht viele Datenpunkte.\n\nsimple_tbl &lt;- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),\n                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))\n\nIn der Tabelle 40.1 ist der Datensatz simplel_tbl dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in [cm] und dem Gewicht in [mg] für sieben Beobachtungen modellieren. Oder anders ausgedrückt, wir wollen einfach die Korrelation berechnen.\n\n\n\n\nTabelle 40.1— Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight.\n\n\n\n\n\n\njump_length\nweight\n\n\n\n\n1.2\n0.8\n\n\n1.8\n1.0\n\n\n1.3\n1.2\n\n\n1.7\n1.9\n\n\n2.6\n2.0\n\n\n1.8\n2.7\n\n\n2.7\n2.8\n\n\n\n\n\n\n\n\nIn Abbildung 40.2 sehen wir die Visualisierung der Daten simple_tbl in einem Scatterplot mit einer geschätzten Gerade. Wir wollen jetzt mit der Korrelation die Steigung der Geraden unabhängig von der Einheit beschreiben. Oder wir wollen die Steigung der Geraden standardisieren auf -1 bis 1.\n\nggplot(simple_tbl, aes(weight, jump_length)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE, fullrange = TRUE) +\n  theme_minimal() +\n  xlim(0, 3.5) + ylim(0, 3.5)\n\n\n\n\n\n\n\nAbbildung 40.2— Scatterplot der Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Die Gerade verläuft mittig durch die Punkte.\n\n\n\n\n\nAls nächstes wollen wir uns ein schönes Beispiel aus der Publikation von Matthews (2000) mit dem Titel Storks Deliver Babies (p = 0.008) anschauen. In der Arbeit wird der Frage nachgegangen, ob tatsächlich mehr Störche storks_pairs in einem Gebiet für mehr Babies birth_rate sorgen. Dafür wurden dann neben den Informationen über das Land country noch die Fläche area_km2 sowie die Bevölkerungsdichte humans_millions erhoben. Eine wunderbare Fragestellung, die wir hier dann nochmal detailiert betrachten wollen.\n\nstork_tbl &lt;- read_excel(\"data/storks_deliver_babys.xlsx\") |&gt;\n  clean_names() |&gt; \n  rename(birth_rate = birth_rate_1000_yr)\n\nIn der Tabelle 40.2 siehst du dann nochmal einen Auszug aus den Daten. Insgesamt haben wir uns siebzehn etwas willkürich gewählte Länder angeschaut zu denen wir die Informationen gefunden haben.\n\n\n\n\nTabelle 40.2— Auszug du dem Storchdatensatz aus siebzehn verschiedenen Ländern mit der Geburtenrate und der Anzahl an Storchenpaaren sowie weiteren Informationen zu den Ländern.\n\n\n\n\n\n\ncountry\narea_km2\nstorks_pairs\nhumans_millions\nbirth_rate\n\n\n\n\nAlbania\n28750\n100\n3.2\n83\n\n\nAustria\n83860\n300\n7.6\n87\n\n\nBelgium\n30520\n1\n9.9\n118\n\n\nBulgaria\n111000\n5000\n9\n117\n\n\n…\n…\n…\n…\n…\n\n\nRomania\n237500\n5000\n23\n367\n\n\nSpain\n504750\n8000\n39\n439\n\n\nSwitzerland\n41290\n150\n6.7\n82\n\n\nTurkey\n779450\n25000\n56\n1576\n\n\n\n\n\n\n\n\nAls weiteren Datensatz nutzen wir die Gummibärchendaten und zwar alle numerischen Spalten von count_bears bis zum semester. Dann nehme ich nur die Teilnehmerinnen an der Umfrage, da ich sonst Probleme mit der Körpergröße kriege. Männer und Frauen sind unterschiedlich groß und dann würden wir immer eine Art Effekt von dieser Sachlage bekommen, wenn wir Männer und Frauen nicht getrennt betrachten. Wenn du keine numerischen Daten sondern Worte vorliegen hast, dann kannst du über die Funktion as.factor() dir erst einen Faktor erschaffen und dann über as.numeric() aus dem Faktor eine Zahl machen. Ich zeige dir das hier einmal an der Spalte most_liked.\n\ncorr_gummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") |&gt; \n  filter(gender == \"w\") |&gt; \n  select(count_bears:semester) |&gt; \n  mutate(most_liked = as.numeric(as.factor(most_liked))) |&gt; \n  select_if(is.numeric)\n\nDann in der Tabelle 40.3 nochmal ein Auszug aus den Daten. Wir betrachten hier nur die numerischen Spalten, da wir nur über diese eine Korrelation berechnen können. Du siehst wie wir die Worte in der Spalte most_liked dann in einer Zahl umgewandelt haben. Jede der Zahlen steht dann für eine der sechs Farben der Gummibärchen plus die “keine Präferenz”-Kategorie. Da wir die eigentliche Übersetzung nicht brauchen um die Korrelation zu interpretieren, passt das hier ganz gut.\n\n\n\n\nTabelle 40.3— foo.\n\n\n\n\n\n\ncount_bears\ncount_color\nmost_liked\nage\nheight\nsemester\n\n\n\n\n10\n5\n7\n21\n159\n6\n\n\n9\n6\n6\n21\n159\n6\n\n\n10\n5\n6\n36\n180\n10\n\n\n13\n5\n2\n21\n163\n3\n\n\n…\n…\n…\n…\n…\n…\n\n\n9\n4\n7\n22\n176\n2\n\n\n8\n4\n7\n21\n180\n2\n\n\n9\n4\n6\n23\n170\n2\n\n\n10\n6\n2\n24\n180\n2",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-corr.html#korrelation-theoretisch",
    "href": "stat-linear-reg-corr.html#korrelation-theoretisch",
    "title": "40  Korrelation",
    "section": "40.3 Korrelation theoretisch",
    "text": "40.3 Korrelation theoretisch\nWir schauen uns hier die Korrelation nach Pearson an. Die Korrelation nach Pearson nimmt an, dass beide zu korrelierende Variablen einer Normalverteilung entstammen. Wenn wir keine Normalverteilung vorliegen haben, dann nutzen wir die Korrelation nach Spearman. Die Korrelation nach Spearman basiert auf den Rängen der Daten und ist ein nicht-parametrisches Verfahren. Die Korrelation nach Pearson ist die parametrische Variante. Wir bezeichnen die Korrelation entweder mit \\(r\\) oder dem griechischen Buchstaben \\(\\rho\\) als rho gesprochen.\nWas macht nun die Korrelation? Die Korrelation gibt die Richtung der Geraden an. Oder noch konkreter die Steigung der Geraden normiert auf -1 bis 1. Die Abbildung 40.3 zeigt die Visualisierung der Korrelation für drei Ausprägungen. Eine Korrelation von \\(r = -1\\) bedeutet eine maximale negative Korrelation. Die Gerade fällt in einem 45° Winkel. Eine Korrelation von \\(r = +1\\) bedeutet eine maximale positive Korrelation. Die gerade steigt in einem 45° Winkel. Eine Korrelation von \\(r = 0\\) bedeutet, dass keine Korrelation vorliegt. Die Grade verläuft parallel zur \\(x\\)-Achse.\n\n\n\n\n\n\nAbbildung 40.3— Visualisierung der Korrelation für drei Ausprägungen des Korrelationskoeffizient.\n\n\n\nIm Folgenden sehen wir die Formel für den Korrelationskoeffizient nach Pearson. Es gibt natürlich auch noch andere Korrelationskoeffizienten aber hier geht es dann einmal darum das Prinzip zu verstehen und das können wir am Korrelationskoeffizient nach Pearson am einfachsten. Wichtig hierbei, die beiden Variablen, die wir korrelieren wollen, müssen bei dem Korrelationskoeffizienten nach Pearson normalverteilt sein.\n\\[\n\\rho = r_{x,y} = \\cfrac{s^2_{x,y}}{s_x \\cdot s_y}\n\\]\nWir berechnen die Korrelation immer zwischen zwei Variablen \\(x\\) und \\(y\\). Es gibt keine multiple Korrelation über mehr als zwei Variablen. Im Zähler der Formel zur Korrelation steht die Kovarianz von \\(x\\) und \\(y\\).\nWir können mit folgender Formel die Kovarianzen zwischen den beiden Variablen \\(x\\) und \\(y\\) berechnen.\n\\[\ns^2_{x,y} = \\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\]\nDie folgende Formel berechnet die quadrierten Abweichung der Beobachtungen von \\(x\\) zum Mittelwert \\(\\bar{x}\\).\n\\[\ns_x = \\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nDie folgende Formel berechnet die quadrierten Abweichung der Beobachtungen von \\(y\\) zum Mittelwert \\(\\bar{y}\\).\n\\[\ns_y = \\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n\\]\nIn Tabelle 40.4 ist der Zusammenhang nochmal Schritt für Schritt aufgeschlüsselt. Wir berechnen erst die Abweichungsquadrate von \\(x\\) und die Abweichungsquadrate von \\(y\\). Dann noch die Quadrate der Abstände von \\(x\\) zu \\(y\\). Abschließend summieren wir alles und ziehen noch die Wurzel für die Abweichungsquadrate von \\(x\\) und \\(y\\).\n\n\n\nTabelle 40.4— Tabelle zur Berechnung des Korrelationskoeffizient\n\n\n\n\n\n\n\n\n\n\n\n\njump_length \\(\\boldsymbol{y}\\)\nweight \\(\\boldsymbol{x}\\)\n\\(\\boldsymbol{(y_i-\\bar{y})^2}\\)\n\\(\\boldsymbol{(x_i-\\bar{x})^2}\\)\n\\(\\boldsymbol{(x_i-\\bar{x})(y_i-\\bar{y})}\\)\n\n\n\n\n1.2\n0.8\n0.45\n0.94\n0.65\n\n\n1.8\n1.0\n0.01\n0.60\n0.06\n\n\n1.3\n1.2\n0.33\n0.33\n0.33\n\n\n1.7\n1.9\n0.03\n0.02\n-0.02\n\n\n2.6\n2.0\n0.53\n0.05\n0.17\n\n\n1.8\n2.7\n0.03\n0.86\n-0.07\n\n\n2.7\n2.8\n0.69\n1.06\n0.85\n\n\n\n\\(\\sum\\)\n2.05\n3.86\n1.97\n\n\n\n\\(\\sqrt{\\sum}\\)\n1.43\n1.96\n\n\n\n\n\n\n\nWir können die Zahlen dann aus der Tabelle in die Formel der Korrelation nach Pearson einsetzen. Wir erhalten eine Korrelation von 0.70 und haben damit eine recht starke positve Korrelation vorliegen.\n\\[\n\\rho = r_{x,y} = \\cfrac{1.97}{1.96 \\cdot 1.43} = 0.70\n\\]\nWir können mit der Funktion cor() in R die Korrelation zwischen zwei Spalten in einem Datensatz berechnen. Wir überprüfen kurz unsere Berechnung und stellen fest, dass wir richtig gerechnet haben.\n\ncor(simple_tbl$jump_length, simple_tbl$weight)\n\n[1] 0.7014985\n\n\nWenn du dich noch an das Eingangszitat erinnerst, dann wirst du dich Fragen, aber wie modellieren wir den jetzt die Korrelation zwischen zwei Variablen \\(x\\) und \\(y\\), wenn es noch eine andere versteckte Variable \\(z\\) gibt, die eigentlich die Korrelation ausmacht. Wir haben ja die Korrelation zwischen Eisverkauf und Waldbränden. Aber die beiden Variablen korrelieren gar nicht miteinander sondern werden sozusagen über die Korrelation von Eis zu Sommer und Sommer zu Waldbränden miteinander verknüpft. Wir haben es hier mit einer partielle Korrelationen (eng. partial correlation) zu tun. Dazu dann mehr in dem folgenden Kasten, da die partielle Korrelationen schon etwas spezieller ist. In R können wir die partielle Korrelationen dann im R Paket {correlation} einfach berechnen und visualisieren.\n\n\n\n\n\n\nPartielle Korrelation in a Nutshell\n\n\n\nWas ist die partielle Korrelationen? Partielle Korrelationen \\(r_{xy, z}\\) beschreiben die Korrelation zwischen zwei Variablen \\(x\\) und \\(y\\), wenn für alle Variablen im Datensatz, im einfachtsen Fall für eien weitere Variable \\(z\\), kontrolliert wird. Ein wesentlicher Vorteil der partiellen Korrelationen besteht nun darin, dass unerwünschte Korrelationen vermieden werden. Wir können uns die Sachlage einmal schematisch in der Abbildung 40.4 anschauen. Wir haben dort die schematische Darstellung der partielle Korrelationen \\(r_{xy, z}\\) zwischen den Variablen \\(x\\) und \\(y\\), die durch die Korrelation zwischen \\(x\\) und \\(z\\) sowie \\(y\\) und \\(z\\) beeinflusst wird. Wie stark der Effekt von \\(z\\) ist, hängt vom konkreten Beispiel ab. Die Variable \\(z\\) kann im extremsten Fall die gesamte Korrelation zwischen \\(x\\) und \\(y\\) erklären. In Wirklichkeit liegt dann gar kein Korrelation zwischen \\(x\\) und \\(y\\) vor obwohl wir eine Messen. Wir haben eine Scheinkorrelation vorliegen.\n\n\n\n\n\n\nflowchart LR\n    A(\"x\"):::factor &lt;-. r&lt;sub&gt;xz&lt;/sub&gt; ..-&gt; B(\"z\"):::confound;\n    A(\"x\"):::factor &lt;-- r&lt;sub&gt;xy&lt;/sub&gt; --&gt; C(\"y\"):::factor;\n    B(\"z\"):::confound &lt;-. r&lt;sub&gt;yz&lt;/sub&gt; ..-&gt; C(\"y\"):::factor;\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef confound fill:#CC79A7,stroke:#333,stroke-width:0.75px\n\n\n\n\n\nAbbildung 40.4— Schematische Darstellung der partielle Korrelationen \\(r_{xy, z}\\) zwischen den Variablen \\(x\\) und \\(y\\), die durch die Korrelation zwischen \\(x\\) und \\(z\\) sowie \\(y\\) und \\(z\\) beeinflusst wird. Wie stark der Effekt ist, hängt vom konkreten Beispiel ab. Die Variable \\(z\\) kann im extremsten Fall die gesamte Korrelation zwischen \\(x\\) und \\(y\\) erklären. In Wirklichkeit liegt dann gar keine Korrelation zwischen \\(x\\) und \\(y\\) vor.\n\n\n\n\n\nWie berechnen wir die partielle Korrelation? Wir nutzen dafür folgende einfache Formel. Es gibt noch andere Formeln, die besonders bei mehr als drei Variablen bessere Ergebnisse erzielen. Für usn soll diese einfache Formel ausreichen.\n\\[\nr_{xy, z} = \\cfrac{r_{xy} - r_{xz} \\cdot r_{yz}}{\\sqrt{(1-r^2_{xz}) \\cdot (1-r^2_{yz})}}\n\\]\nmit\n\n\\(r_{xy}\\) der Korrelation zwischen den Variablen \\(x\\) und \\(y\\)\n\\(r_{xz}\\) der Korrelation zwischen den Variablen \\(x\\) und \\(z\\)\n\\(r_{yz}\\) der Korrelation zwischen den Variablen \\(y\\) und \\(z\\)\n\nSchauen wir uns die Formel einmal in Aktion an dem Beispiel von Matthews (2000) in der Arbeit Storks Deliver Babies (p = 0.008) an. In der Arbeit fanden wir eine Korrelation \\(r_{xy}\\) von \\(0.62\\) zwischen \\(x\\) gleich Anzahl der brütenden Störche und \\(y\\) gleich der Anzahl an Babies in den entsprechenden untersuchten Gebieten. Im Folgenden ein Zitat vom Autor, der korrekterweise anmerkt, dass hier etwas vermutlich nicht stimmig ist.\n\nThe most plausible explanation of the observed correlation is, of course, the existence of a confounding variable: some factor common to both birth rates and the number of breeding pairs of storks which […] can lead to a statistical correlation between two variables which are not directly linked themselves. One candidate for a potential confounding variable is land area […] — Matthews (2000)\n\nDafür müssen wir jetzt einmal die Korrelation zwischen allen Variablen in dem Storchendatensatz berechnen. Ich habe jetzt mal nur die drei Variablen von Interesse ausgewählt, damit wir hier nicht so lange Tabellen erhalten. Dann können wir schauen, ob wir eine partielle Korrelation in den Daten vorliegen haben.\n\nstork_tbl |&gt; \n  select(area_km2, storks_pairs, birth_rate) |&gt; \n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |   Parameter2 |    r |       95% CI | t(15) |         p\n---------------------------------------------------------------------\narea_km2     | storks_pairs | 0.58 | [0.14, 0.83] |  2.75 | 0.016*   \narea_km2     |   birth_rate | 0.92 | [0.79, 0.97] |  9.26 | &lt; .001***\nstorks_pairs |   birth_rate | 0.62 | [0.20, 0.85] |  3.06 | 0.016*   \n\np-value adjustment method: Holm (1979)\nObservations: 17\n\n\nDann bauen wir uns den Sachverhalt einmal in der Abbildung Abbildung 40.5 nach. Wir können dort immer die direkten paarweisen Korrelationen eintragen. Wenn wir das gemacht haben, können wir einmal die partielle Korrelation \\(r_{xy, z}\\) berechnen. Wie ändert sich also die paarweise Korrelation zwischen \\(x\\) und \\(y\\), wenn wir die paarweisen Korrelationen zu \\(z\\) bereücksichtigen?\n\n\n\n\n\n\nflowchart LR\n    A(\"stork_pairs\"):::factor &lt;-. 0.58 ..-&gt; B(\"birth_rate\"):::confound;\n    A(\"stork_pairs\"):::factor &lt;-- 0.62 --&gt; C(\"area_km2\"):::factor;\n    B(\"area_km2\"):::confound &lt;-. 0.81 ..-&gt; C(\"birth_rate\"):::factor;\n    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef confound fill:#CC79A7,stroke:#333,stroke-width:0.75px\n\n\n\n\n\nAbbildung 40.5— Schematische Darstellung der Korrelationen zwischen den Variablen stork_pairs und birth_rate sowie zu der Variable area_km2.\n\n\n\n\n\nWir können dann alles einmal in die Formel für die partielle Korrelation einsetzen.\n\\[\nr_{xy, z} = \\cfrac{0.62 - 0.58 \\cdot 0.81}{\\sqrt{(1-0.58^2) \\cdot (1-0.81^2)}} = \\cfrac{0.15}{0.48} = 0.31\n\\]\nDamit erhalten wir eine partielle Korrelation von nur \\(0.31\\) anstatt einer direkten Korrelation von \\(0.62\\). Wie du siehst, haben wir hier einen Effekt von der Landfläche auf beide Variablen der Geburtsrate sowie der Storchenanzahl. Wenn wir die Landfläche nicht berücksichtigen könnte man meinen wir hätten da einen Effekt von Störchen auf die Geburtsrate. Du kannst einfach in der Funktion correlation() einstellen, dass du die partiellen Korrelationen berechnet haben willst. Wie du siehst kommt eine ähnliche Zahl raus, da der Algorithmus etwas anders ist als unsere sehr einfache Formel weiter oben. Auch sehen wir, dass der Zusammenhang zwischen Landfläche und Geburtenrate signifikant ist und eine sehr hohe Korrelation von \\(0.88\\) hat.\n\nstork_tbl |&gt; \n  select(area_km2, storks_pairs, birth_rate) |&gt; \n  correlation(partial = TRUE, p_adjust = \"none\")\n\n# Correlation Matrix (pearson-method)\n\nParameter1   |   Parameter2 |    r |        95% CI | t(15) |         p\n----------------------------------------------------------------------\narea_km2     | storks_pairs | 0.02 | [-0.46, 0.50] |  0.09 | 0.929    \narea_km2     |   birth_rate | 0.88 | [ 0.69, 0.96] |  7.21 | &lt; .001***\nstorks_pairs |   birth_rate | 0.27 | [-0.24, 0.67] |  1.10 | 0.289    \n\np-value adjustment method: none\nObservations: 17",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-corr.html#korrelation-in-r",
    "href": "stat-linear-reg-corr.html#korrelation-in-r",
    "title": "40  Korrelation",
    "section": "40.4 Korrelation in R",
    "text": "40.4 Korrelation in R\nIn R haben wir dann die Möglichkeit die Korrelation mit der Standardfunktion cor() ganz klassisch zu bestimmen oder aber auch das R Paket {correlation} zu nutzen. Das R Paket kann dabei vollumfänglich auf das {tidyverse} zugreifen und ist von mir bevorzugt. Wenn es schnell gehen muss oder ich mir nur aml als Demonstration eine Korrelation berechnen will, dann ist die klassiche Variante in R gut, aber für größere Datensätze nutze ich nur noch das R Paket {correaltion} und deren tollen Funktionen.\n\n40.4.1 Klassisch\nWir nutzen die Korrelation in R selten nur für zwei Variablen. Meistens schauen wir uns alle numerischen Variablen gemeinsam in einer Abbildung an. Wir nennen diese Abbildung auch Korrelationsplot. Faktoren sind keine numerischen Variablen. Daher kann es sein, dass für dein Experiment kein Korrelationsplot in Frage kommt. Wir schauen uns jetzt nochmal einen die Berechnung für den Datensatz simple_tbl an. Wir müssen für die Korrelation zwischen zwei Variablen diese Variablen mit dem $-Zeichen aus dem Datensatz extrahieren. Die Funktion cor() kann nur mit Vektoren oder ganzen numerischen Datensätzen arbeiten.\nWir können den Korrelationskoeffizienten nach Pearson mit der Option method = \"pearson\" auswählen, wenn wir normalverteilte Daten vorliegen haben. Das heißt, dass alle unsere Spalten, mit denen wir die paarweisen Korrelationen berechnen wollen, einer Normalverteilung folgen müssen.\n\ncor(simple_tbl$jump_length, simple_tbl$weight, method = \"pearson\")\n\n[1] 0.7014985\n\n\nJe mehr Variablen du dann hast, desto unwahrscheinlicher wird es natürlich, dass alle einer Normalverteilung folgen. Dann können wir die nicht-parametrische Variante des Korrelationskoeffizienten nach Spearman berechnen. Wir nutzen dazu die Option method = \"spearman\".\n\ncor(simple_tbl$jump_length, simple_tbl$weight, method = \"spearman\")\n\n[1] 0.792825\n\n\nBei stetigen Daten wird dann meist statt des Korrelationskoeffizienten nach Spearman gerne der nach Kendall berechnet. Aber das sind dann schon die Feinheiten. Wir nutzen dazu die Option method = \"kendall\".\n\ncor(simple_tbl$jump_length, simple_tbl$weight, method = \"kendall\")\n\n[1] 0.6831301\n\n\nWir können auch einen statistischen Test für die Korrelation rechnen. Die Nullhypothese \\(H_0\\) wäre hierbei, dass die Korrelation \\(r = 0\\) ist. Die Funktion cor.test() liefert den entsprechenden \\(p\\)-Wert für die Entscheidung gegen die Nullhypothese.\n\ncor.test(simple_tbl$jump_length, simple_tbl$weight, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  simple_tbl$jump_length and simple_tbl$weight\nt = 2.201, df = 5, p-value = 0.07899\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1092988  0.9517673\nsample estimates:\n      cor \n0.7014985 \n\n\nAus dem Test erhalten wir den \\(p\\)-Wert von \\(0.079\\). Damit liegt der \\(p\\)-Wert über den Signifikanzniveau von \\(\\alpha\\) gleich 5%. Wir können somit die Nullhypothese nicht ablehnen. Wir sehen hier, die Problematik der kleinen Fallzahl. Obwohl unsere Korrelation mit \\(0.7\\) groß ist erhalten wir einen \\(p\\)-Wert, der nicht die Grenze von 5% unterschreitet. Wir sehen, dass die starre Grenze von \\(\\alpha\\) auch Probleme bereitet.\n\n\n40.4.2 Mit {correlation}\nDas R Paket {correlation} erlaubt es, die Korrelation in R zu berechnen und hilft wirklich bei der Anwendung. Die ursprünglichen Funktion in R passen überhaupt nicht zum {tidyverse} und so ist es gut, dass wir hier eine andere Möglichkeit haben, die auch noch viel mehr kann als die klassischen Funktion in R. Die zentrale Funktion ist dabei die Funktion correlation(). Wir können eine Vielzahl an Korrelationskoeffizienten auswählen. Wenn du keine Methode über die Option method = eingibst, dann wird der Korrelationskoeffizient nach Pearson unter der Annahme von normalverteilten Daten gerechnet. Die kannst es dir auch einfacher machen und die Funktion über die Option method = \"auto\" selber wählen lassen. Ich bevorzuge aber den Korrelationskoeffizient nach Spearman, wenn ich sehr viele Variablen miteinander vergleichen möchte. Im Normalfall werden die \\(p\\)-Werte adjustiert, dass stelle ich hier aber einmal ab. Sonst nutze gerne die Option p_adjust = \"bonferroni\", wenn du für sehr viele Vergleiche adjustieren willst.\nWenn du nun die Funktion aufrufst, dann erhälst du alle paarweisen Korrelationen in deinem Datensatz über alle Variablen. Daher baue ich mir immer erstmal über select() einen Datensatz zusammen, den ich dann analysieren will. Sonst hast du da Spalten in deiner Auswertung, die dich gar nicht interessieren. Die Spalte rho gibt dir dann den Korrelationskoeffizient wieder. Dann erhälst du noch die 95% Konfidenzintervalle sowie den \\(p\\)-Wert für den Korrelationskoeffizient. Hier testen wir, ob der Korrelationskoeffizient unterschiedlich von der Null ist.\n\ncor_stork_res &lt;- stork_tbl |&gt; \n  correlation(method = \"spearman\", p_adjust = \"none\")\ncor_stork_res\n\n# Correlation Matrix (spearman-method)\n\nParameter1      |      Parameter2 |  rho |        95% CI |      S |         p\n-----------------------------------------------------------------------------\narea_km2        |    storks_pairs | 0.63 | [ 0.21, 0.86] | 298.73 | 0.006**  \narea_km2        | humans_millions | 0.84 | [ 0.60, 0.94] | 127.16 | &lt; .001***\narea_km2        |      birth_rate | 0.83 | [ 0.57, 0.94] | 138.00 | &lt; .001***\nstorks_pairs    | humans_millions | 0.33 | [-0.19, 0.71] | 544.00 | 0.191    \nstorks_pairs    |      birth_rate | 0.42 | [-0.09, 0.75] | 475.16 | 0.095    \nhumans_millions |      birth_rate | 0.96 | [ 0.88, 0.98] |  36.04 | &lt; .001***\n\np-value adjustment method: none\nObservations: 17\n\n\nWir haben jetzt einige signifikante Korrelationskoeffizienten. Jetzt ist es wichtig, dass wir nicht nur auf die Signifikanz schauen, sondern auch fragen, ist der Korrelationskoeffizient relevant? Das heißt, ist der Korrelationskoeffizient weit genug weg von der Null um für uns auch eine Aussagekraft zu haben. Hier können wir uns für die bessere Übersicht auch einmal die Korrelationsmatrix wiedergeben lassen.\n\ncor_stork_res |&gt; \n  summary(redundant = TRUE)\n\n# Correlation Matrix (spearman-method)\n\nParameter       | area_km2 | storks_pairs | humans_millions | birth_rate\n------------------------------------------------------------------------\narea_km2        |          |       0.63** |         0.84*** |    0.83***\nstorks_pairs    |   0.63** |              |            0.33 |       0.42\nhumans_millions |  0.84*** |         0.33 |                 |    0.96***\nbirth_rate      |  0.83*** |         0.42 |         0.96*** |           \n\np-value adjustment method: none\n\n\nDas sieht dann schon relevant aus. Viele der signifikanten Korrelationskoeffizienten sind irgendwie bei knapp \\(0.8\\) was dann auch signifikant unterschiedlich von Null ist und auch eine sehr große Korrelation beschreibt. Ich würde hier schon von einem Zusammenhang ausgehen. In der Abbildung 40.6 siehst du dann die Matrix nochmal visualisiert. Die Farben entsprechen dann der Richtung der Korrelation. Je kräftiger der Farbton, desto größer ist die Korrelation.\n\ncor_stork_res |&gt;\n  summary(redundant = TRUE) |&gt;\n  plot()\n\n\n\n\n\n\n\nAbbildung 40.6— Darstellung der Korrelation zwischen den Variablen in dem Storchendatensatz. Eine rote Einfärbung deutet auf eine positive Korrelation und eine blaue Einfärbung auf eine negative Korrelation hin.\n\n\n\n\n\nEine angenehme Funktion in dem R Paket {correlation} ist die Funktion cor_test(). Wir haben hier zum einen die Möglichkeit direkt Spaltennamen in die Funktion zu pipen. Auf der anderen Seite können wir die Funktion auch gleich in der Funktion plot() visualisieren. Das ist super praktisch, wenn du dir die Daten einmal anschauen willst und sehen willst, ob der Korrelationskoeffizient gepasst hat. Ich nutze die Funktion gerne in meiner Arbeit als schnelle Visualisierung einer Korrelation in einem Scatterplot. Sonst sind es schon ein paar mehr Zeilen Code mit {ggplot}.\nWir sehen dann in der Abbildung 40.7 einmal den Zusammenhang. Die Korrelation beschreibt ja die einheistlose Steigung. Die Korrelation \\(r\\) mit \\(0.62\\) ist signifikant mit einem \\(p\\)-Wert von \\(0.008\\) welcher ja unter dem Signifikanzniveau liegt. Einzig die beiden Beobachtungen rechts mit knapp 30000 sowie 25000 Storchenpaaren ziehen die Gerade sehr nach oben. Hier müsste man sich nochmal eine partielle Korrelation anschauen um zu sehen, ob es hier nicht noch andere Effekt einer anderen Variable in den Daten gibt.\n\nstork_tbl |&gt; \n  cor_test(\"storks_pairs\", \"birth_rate\") |&gt; \n  plot() +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 40.7— Scatterplot zwischen der Anzahl an Storchpaaren storks_pairs und der Geburtenrate birth_rate. Wie man gut sieht, wird die gerade von nur ein paar Punkten getrieben. Hier müsste nochmal geschaut werden, ob nicht eine partielle Korrelation mehr Aussagekraft hätte.\n\n\n\n\n\nAls letztes erlaubt das R Paket noch die Darstellung der Daten in einem gaußsches grafisches Modell (eng. Gaussian Graphical Model). Wir visualisieren uns damit die partielle Korrelationen (eng. partial correlation). Ein gaußsches grafisches Modell besteht dabei aus einer Reihe von Variablen, die durch Kreise dargestellt werden, und einer Reihe von Linien, die die Beziehungen zwischen den Variablen visualisieren. Die Dicke dieser Linien stellt die Stärke der Beziehungen zwischen den Variablen dar. Daher kannst du davon ausgehen, dass ein Fehlen einer Linie keine oder nur sehr schwache Beziehungen zwischen den relevanten Variablen darstellt. Insbesondere erfassen diese Linien im Gaußschen Grafikmodell partielle Korrelationen. Partielle Korrelationen beschreiben die Korrelation zwischen zwei Variablen, wenn für alle Variablen im Datensatz kontrolliert wird. Ein wesentlicher Vorteil der partiellen Korrelationen besteht nun darin, dass unerwünschte Korrelationen vermieden werden. Mehr zu dem Thema findest du auch bei Bhushan u. a. (2019) in deren Arbeit Using a Gaussian Graphical Model to Explore Relationships Between Items and Variables in Environmental Psychology Research.\nIm Folgenden nutzen wir einmal die Funktion correlation() mit der Option partial = TRUE. Zuerst haben wir dann nur noch die Anzahl an Beobachtungen in den Daten für die wir auch überall einen Wert haben. Sonst hätten wir ja noch die Möglichkeit, dass bei einzelnen Variablenpaaren mehr Beobachtungen übrig bleiben. Dann können wir auch schon unsere partielle Korrelation einmal in der Abbildung 40.8 abbilden. Wir setzen dann noch andere Farben mit der Funktion scale_edge_color_continuous() für wenig partielle Korrelation zu viel Korrelation. Wir sehen auch hier ganz klar, dass wir eine partielle Korrelation zwischen der Landfläche und der Geburtenrate haben und weniger zwischen den Storchpaaren und der Geburtenrate. Für solche Darstellungen und Untersuchungen sind die graphischen Modelle super.\n\nstork_tbl |&gt; \n  correlation(partial = TRUE) |&gt; \n  plot() +\n  scale_edge_color_continuous(low = \"#000004FF\", high = \"#FCFDBFFF\")\n\n\n\n\n\n\n\nAbbildung 40.8— Partielle Korrelation in einem Gaussian Graphical Model. Je dicker die Linien desto stärker ist der Zusammenhang zwischen den Variablen desto sträker ist dann auch die partielle Korrelation.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-corr.html#sec-linear-corr-visu",
    "href": "stat-linear-reg-corr.html#sec-linear-corr-visu",
    "title": "40  Korrelation",
    "section": "40.5 Komplexere Visualisierungen",
    "text": "40.5 Komplexere Visualisierungen\nAbschließend wollen wir uns noch die Funktion corrplot() aus dem gleichnamigen R Paket {corrplot} anschauen. Die Hilfeseite zum Paket ist sehr ausführlich und bietet noch eine Reihe an anderen Optionen. Wir benötigen dafür einen etwas größeren Datensatz mit mehreren numerischen Variablen. Wir nutzen daher den Gummibärchendatensatz und selektieren die Spalten count_bears bis semester aus. Wir brauchen für die Funktion corrplot() eine Matrix mit den paarweisen Korrelationen. Wir können diese Matrix wiederum mit der Funktion cor() erstellen. Wir müssen dazu aber erstmal alle numerischen Variablen mit select_if() selektieren und dann alle fehlenden Werte über na.omit() entfernen.\n\ncor_mat &lt;- corr_gummi_tbl |&gt; \n  select_if(is.numeric) |&gt; \n  na.omit() |&gt; \n  cor()\n\ncor_mat |&gt; round(3)\n\n            count_bears count_color most_liked    age height semester\ncount_bears       1.000       0.266      0.077  0.228  0.063    0.131\ncount_color       0.266       1.000     -0.014  0.003  0.022   -0.006\nmost_liked        0.077      -0.014      1.000 -0.022 -0.096    0.087\nage               0.228       0.003     -0.022  1.000  0.016    0.039\nheight            0.063       0.022     -0.096  0.016  1.000   -0.065\nsemester          0.131      -0.006      0.087  0.039 -0.065    1.000\n\n\nWir sehen das in der Korrelationsmatrix jeweils über und unterhalb der Diagonalen die gespiegelten Zahlen stehen. Wir können jetzt die Matrix cor_mat in die Funktion corrplot() stecken und uns den Korrelationsplot in Abbildung 40.9 einmal anschauen.\n\ncorrplot(cor_mat)\n\n\n\n\n\n\n\nAbbildung 40.9— Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke.\n\n\n\n\n\nWir sehen in Abbildung 40.9, dass wir eine schwache positive Korrelation zwischen count_color und count_bears haben, angezeigt durch den schwach blauen Kreis. Der Rest der Korrelation ist nahe Null, tendiert aber eher ins negative. Nun ist in dem Plot natürlich eine der beiden Seiten überflüssig. Wir können daher die Funktion corrplot.mixed() nutzen um in das untere Feld die Zahlenwerte der Korrelation darzustellen.\n\ncorrplot.mixed(cor_mat)\n\n\n\n\n\n\n\nAbbildung 40.10— Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke. In das untere Feld werden die Werte der Korrelation angegeben.\n\n\n\n\n\nEs gibt noch eine Vielzahl an weiteren Möglichkeiten in den Optionen von der Funktion corr.mixed(). Hier hilft dann die Hilfeseite der Funktion oder aber die Hilfeseite zum Paket. Eine weitere Möglichkeit kontinuierliche Daten darzustellen ist das R Paket {GGally} mit der Funktion ggpairs(). Hier können wir die paarweisen Zusammenhänge von Variablen, also den Spalten, darstellen. Prinzipiell geht es auch mit kategorialen Variablen, aber wir konzentrieren uns hier nur auf die numerischen. Im Folgenden wählen wir also nur die numerischen Spalten in unseren Gummibärchendaten einmal aus und nutzen die selektierten Daten dann einmal in der Funktion ggpairs().\n\ncorr_gummi_tbl &lt;- corr_gummi_tbl |&gt; \n  select_if(is.numeric)\n\nDer ggpairs-Plot baut sich als eine Matrix auf, in der jede Variable mit jeder anderen Variable verglichen wird. Damit ergibt sich auf der Diagonalen ein Selbstvergleich und die obere Hälfte und untere Hälfte der Matrix beinhalten die gleichen Informationen. Hier setzt dann ggpairs() an und erlaubt in jede der drei Bereiche, obere Hälfte (upper), der Diagonalen (diag) sowie der unteren Hälfte (lower), eigene Abbildungen oder Maßzahlen für die Vergleiche der Variablen zu verwenden. In der Abbildung 40.11 siehst du die Standardausgabe der Funktion ggpairs() auf einen Datensatz. Auf der unteren Hälfte ist der Scatterplot mit den einzelnen Beobachtungen, in der Diagonalen die Dichte der Variablen sowie im oberen Bereich die Korrelation zwischen den Variablen angegeben. Die Korrelation wurde auch noch einen statistischen Test unterworfen, so dass wir hier auch Sternchen für die Signifikanz bekommen.\n\nggpairs(corr_gummi_tbl) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 40.11— Unmodifizierte Abbildung aus ggpairs() mit allen paarweisen Vergleichen der numerischen Variablen. Auf der unteren Hälfte ist der Scatterplot, in der Diagonalen die Dichte der Variablen sowie im oberen Bereich die Korrelation zwischen den Variablen angegeben.\n\n\n\n\n\nDie Standardabbildung ist okay, wenn du mal in die Daten schauen willst. Aber eigentlich sind wir an einer schöneren Abbildung interessiert. Wie immer, was ist schon schön, aber ich zeige dir einmal, wie du die Abbildungen in den jeweiligen Bereichen ändern kannst. Bei {GGally} hilft mir eigentlich immer am besten den konkreten Sachverhalt zu googlen, den ich ändern will. Wenn es zu viel wird, dann hilft es mehr sich die Abbildungen dann doch selber zu bauen und über {patachwork} zusammenzukleben. Es geht halt nicht beides, schnell und flexibel. In den folgenden Tabs findest du jeweils eine Funktion, die den oberen, diagonalen und unteren Bereich modifiziert. Die Funktion rufen wir dann in der Funktion ggpairs() auf.\n\nUpperDiagLower\n\n\nWir wollen in den oberen Bereich die Korrelation haben, aber ohen die Sternchen und ohne das Wort Corr:. Deshalb müssen wir uns hier nochmal die Korrelationsfunktion selber nachbauen.\n\ncor_func &lt;- function(data, mapping, method, symbol, ...){\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n  corr &lt;- cor(x, y, method=method, use='complete.obs')\n  ggally_text(\n    label = paste(symbol, as.character(round(corr, 2))), \n    mapping = aes(),\n    xP = 0.5, yP = 0.5,\n    color = 'black'\n  ) \n}\n\n\n\nAuf der Diagonalen wollen wir die Desnityplots haben. Die sind auch so da, aber ich färbe die Plots hier nochmal rot ein. Einfach damot du siehst, was man machen kann.\n\ndiag_fun &lt;- function(data, mapping) {\n  ggplot(data = data, mapping = mapping) +\n    geom_density(fill = \"red\", alpha = 0.5)\n}\n\n\n\nIn dem unteren Bereich wollen wir die Punkte etwas kleiner haben, deshalb das feom_point2() aus dem R Paket {see}. Dann möchte ich noch die Regressionsgrade einmal zeichnen. Auch hier geht dann mehr, wenn du loess oder aber den Standardfehler sehen willst.\n\nlower_fun &lt;- function(data, mapping) {\n  ggplot(data = data, mapping = mapping) +\n    geom_point2() + \n    geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE)\n}\n\n\n\n\nUnd dann sammeln wir alles ein und bauen uns die Abbildung 40.12. Wir machen uns es hier etwas einfacher und schreiben gleich das Symbol \\(\\rho\\) als ASCII-Zeichen, da sparen wir etwas nerven. Ansonsten siehst du wie durch die Optionen upper =, diag = und lower = die obigen Funktionen zugewiesen werden und damit dann die einzelnen Bereiche individuell gebaut werden. Wichtig finde ich noch die Möglichkeit, die Seitennamen der Abbildung dann hier in der Funktion über columnLabels = sauber zu benennen.\n\nggpairs(corr_gummi_tbl, \n        upper = list(continuous = wrap(cor_func, method = 'pearson', symbol = expression('\\u03C1 ='))),\n        diag = list(continuous = wrap(diag_fun)),\n        lower = list(continuous = wrap(lower_fun)),\n        columnLabels = c(\"Anzahl Bärchen\", \"Anzahl Farben\", \"Meist gemocht\", \n                         \"Alter\", \"Größe\", \"Semester\")) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 40.12— Modifizierte Abbildung aus ggpairs() mit allen paarweisen Vergleichen der numerischen Variablen. Auf der unteren Hälfte ist der Scatterplot zusammen mit der Regressionsgrade aus stat_smooth(), in der Diagonalen die eingefärbte Dichte der Variablen sowie im oberen Bereich die Korrelation zwischen den Variablen ohne die Signifikanz und der Überschrift Corr: sondern mit \\(\\rho\\) angegeben.\n\n\n\n\n\nHier hilft es dann auch mal mit den Themes theme_minimal() oder theme_void(). In der Abbildung 40.13 habe ich die Labels durch die Funktion axisLabels = \"internal\" auf die Diagonale gesetzt. Dann musst du entweder die Namen kürzer machen oder aber den Plot größer. Ich habe mich hier für kürzere Namen entschieden. Dementsprechenden spiele einfach mal mit den Möglichkeiten, bis du eine gute Abbildung für dich gefunden hast.\n\nggpairs(corr_gummi_tbl, \n        lower = list(continuous = wrap(lower_fun)),\n        columnLabels = c(\"Bärchen\", \"Farben\", \"Gemocht\",\n                         \"Alter\", \"Größe\", \"Semester\"),\n        axisLabels = \"internal\") +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 40.13— Modifizierte Abbildung aus ggpairs() mit allen paarweisen Vergleichen der numerischen Variablen. Hier einmal mit internen Achsenbeschrfitungen und zur Abwechselung dem Theme theme_minimal().\n\n\n\n\n\n\n\n\n\n\n\nAnwendungsbeispiel: Die Korrelation als Vergleich zweier Steigungen\n\n\n\nIn der Abbildung 40.14 können wir uns noch einmal den Vorteil der Korrelation als ein einheitsloses Maß anschauen. Wenn wir uns nur die Steigung der beiden Gerade betrachten würden, dann wäre die Steigung \\(\\beta_{kopfgewicht} = 0.021\\) und die Steigung \\(\\beta_{strunkdurchmesser} = 5.15\\). Man könnte meinen, das es keinen Zusammenhang zwischen der Boniturnote und dem Kopfgewicht gäbe wohl aber einen starken Zusammenhang zwischen der Boniturnote und dem Durchmesser. Die Steigung der Geraden wird aber stark von den unterschiedlich skalierten Einheiten von Kopfgewicht in [g] und dem Strunkdurchmesserdurchmesser in [cm] beeinflusst.\n\n\n\n\n\n\nAbbildung 40.14— Der Zusammenhang von Hohlstrunk Boniturnote und Kopfgewicht sowie Strunkdurchmesser. In dem Beispiel ist gut der Zusammenhang zwischen der Steigung \\(\\beta_1\\) von \\(x\\) und der Einheit von \\(x\\) zu erkennen.\n\n\n\nWir wollen den Zusammenhang nochmal mit der Korrelation überprüfen, da die Korrelation nicht durch die Einheiten von \\(y\\) und \\(x\\), in diesem Fall den Einheiten von Kopfgewicht in [g] und dem Durchmesser in [cm], beeinflusst wird. Wir bauen uns zuerst einen künstlichen Datensatz in dem wir die Informationen aus der Geradengleichung nutzen. Dann addieren wir mit der Funktion rnorm() noch einen kleinen Fehler auf jede Beobachtung drauf.\n\nstrunk_tbl &lt;- tibble(durchmesser = seq(3.5, 4.5, by = 0.05),\n                     bonitur = 5.15 * durchmesser - 16.65 + rnorm(length(durchmesser), 0, 1))\nkopf_tbl &lt;- tibble(gewicht = seq(410, 700, by = 2),\n                   bonitur = 0.021 * gewicht - 8.42 + rnorm(length(gewicht), 0, 1))\n\nWir können und jetzt einmal die Korrelation aus den Daten berechnen. Die Koeffizienten der Geraden sind die gleichen Koeffizienten wie in der Abbildung 40.14. Was wir aber sehen, ist das sich die Korrelation für beide Gerade sehr ähnelt oder fast gleich ist.\n\nstrunk_tbl %$% \n  cor(durchmesser, bonitur, method = \"spearman\")\n\n[1] 0.8688312\n\nkopf_tbl %$% \n  cor(gewicht, bonitur, method = \"spearman\")\n\n[1] 0.8807708\n\n\nWie wir sehen, können wir mit der Korrelation sehr gut verschiedene Zusammenhänge vergleichen. Insbesondere wenn die Gerade zwar das gleiche Outcome haben aber eben verschiedene Einheiten auf der \\(x\\)-Achse. Prinzipiell geht es natürlich auch für die Einheiten auf der \\(y\\)-Achse, aber meistens ist das Outcome der konstante Modellteil.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "stat-linear-reg-corr.html#referenzen",
    "href": "stat-linear-reg-corr.html#referenzen",
    "title": "40  Korrelation",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 40.1— Klassische Darstellung der Korrelation zwischen mehreren Variablen in einer Matrixtabelle. Die Sterne geben eine signifikante Korrelation. Hier ist eine Korrelation signifikant, wenn die Korrelation von 0 unterschiedlich ist.\nAbbildung 40.2— Scatterplot der Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Die Gerade verläuft mittig durch die Punkte.\nAbbildung 40.3— Visualisierung der Korrelation für drei Ausprägungen des Korrelationskoeffizient.\nAbbildung 40.6— Darstellung der Korrelation zwischen den Variablen in dem Storchendatensatz. Eine rote Einfärbung deutet auf eine positive Korrelation und eine blaue Einfärbung auf eine negative Korrelation hin.\nAbbildung 40.7— Scatterplot zwischen der Anzahl an Storchpaaren storks_pairs und der Geburtenrate birth_rate. Wie man gut sieht, wird die gerade von nur ein paar Punkten getrieben. Hier müsste nochmal geschaut werden, ob nicht eine partielle Korrelation mehr Aussagekraft hätte.\nAbbildung 40.8— Partielle Korrelation in einem Gaussian Graphical Model. Je dicker die Linien desto stärker ist der Zusammenhang zwischen den Variablen desto sträker ist dann auch die partielle Korrelation.\nAbbildung 40.9— Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke.\nAbbildung 40.10— Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke. In das untere Feld werden die Werte der Korrelation angegeben.\nAbbildung 40.11— Unmodifizierte Abbildung aus ggpairs() mit allen paarweisen Vergleichen der numerischen Variablen. Auf der unteren Hälfte ist der Scatterplot, in der Diagonalen die Dichte der Variablen sowie im oberen Bereich die Korrelation zwischen den Variablen angegeben.\nAbbildung 40.12— Modifizierte Abbildung aus ggpairs() mit allen paarweisen Vergleichen der numerischen Variablen. Auf der unteren Hälfte ist der Scatterplot zusammen mit der Regressionsgrade aus stat_smooth(), in der Diagonalen die eingefärbte Dichte der Variablen sowie im oberen Bereich die Korrelation zwischen den Variablen ohne die Signifikanz und der Überschrift Corr: sondern mit \\(\\rho\\) angegeben.\nAbbildung 40.13— Modifizierte Abbildung aus ggpairs() mit allen paarweisen Vergleichen der numerischen Variablen. Hier einmal mit internen Achsenbeschrfitungen und zur Abwechselung dem Theme theme_minimal().\nAbbildung 40.14— Der Zusammenhang von Hohlstrunk Boniturnote und Kopfgewicht sowie Strunkdurchmesser. In dem Beispiel ist gut der Zusammenhang zwischen der Steigung \\(\\beta_1\\) von \\(x\\) und der Einheit von \\(x\\) zu erkennen.\n\n\n\nAkoglu H. 2018. User’s guide to correlation coefficients. Turkish journal of emergency medicine 18: 91–93.\n\n\nAsuero AG, Sayago A, González A. 2006. The correlation coefficient: An overview. Critical reviews in analytical chemistry 36: 41–59.\n\n\nBewick V, Cheek L, Ball J. 2003. Statistics review 7: Correlation and regression. Critical care 7: 1–9.\n\n\nBhushan N, Mohnert F, Sloot D, Jans L, Albers C, Steg L. 2019. Using a Gaussian graphical model to explore relationships between items and variables in environmental psychology research. Frontiers in psychology 10: 1050.\n\n\nMatthews R. 2000. Storks deliver babies (p= 0.008). Teaching Statistics 22: 36–38.\n\n\nShao K, Elahi Shirvan M, Alamer A. 2022. How accurate is your correlation? Different methods derive different results and different interpretations. Frontiers in Psychology 13: 901412.\n\n\nTaylor J, Bates T. 2013. A discussion on the significance associated with Pearson’s correlation in precision agriculture studies. Precision Agriculture 14: 558–564.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "stat-modeling-outlier.html",
    "href": "stat-modeling-outlier.html",
    "title": "41  Ausreißer",
    "section": "",
    "text": "41.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\nset.seed(20240116)\npacman::p_load(tidyverse, magrittr, broom, readxl,\n               see, performance, ggbeeswarm, olsrr,\n               outliers, \n               conflicted)\nconflicts_prefer(dplyr::filter)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Ausreißer</span>"
    ]
  },
  {
    "objectID": "stat-modeling-outlier.html#genutzte-r-pakete",
    "href": "stat-modeling-outlier.html#genutzte-r-pakete",
    "title": "41  Ausreißer",
    "section": "",
    "text": "Friedhof der R Pakete\n\n\n\nIn diesem Kapitel ist etwas passiert, was ich noch nie in einem anderem Thema erlebt habe. Ich fand R Pakete bei meiner Recherche, die in den letzten Jahren von CRAN runtergenommen wurden, da die Pakete nicht mehr gepflegt werden. Damit sterben natürlich auch einige der Tutorien weg, dir ich so gefunden hatte. Hier also der Friedhof der R Pakete zu der Ausreißerbestimmung mit dem R Paket {dlookr} (in 2023, jetzt wieder mit Unterstützung), {OutlierDetection} sowie {DMwR}. All diese R Pakete werden nicht mehr unterstützt. Einer Nutzung ist dem normalen Anwender abzuraten.\nDas R Paket {mvoutlier} funktioniert zwar noch, aber es gibt nur ein einziges Tutorium Anomaly Detection aus dem Jahr 2017. Darüber hinaus ist das Paket weder mit {tidyverse} noch mit {ggplot} kompatibel. Deshalb stelle ich das Paket auch nicht weiter vor.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Ausreißer</span>"
    ]
  },
  {
    "objectID": "stat-modeling-outlier.html#daten",
    "href": "stat-modeling-outlier.html#daten",
    "title": "41  Ausreißer",
    "section": "41.2 Daten",
    "text": "41.2 Daten\nUm die Detektion von Ausreißern besser zu verstehen, schauen wir uns zwei Beispieldaten an. Als erstes einen Datensatz zu dem Wachstum von Wasserlinsen und der Messung derselbigen durch einen Sensor. Wir messen also einmal die Dichte duckweeds_density konventionell und wollen dann sehen, ob unser Sensor die Messungen dann widerspiegelt. Hier können wir vermutlich erstmal von einem linearen Wachstum ausgehen, dann aber vermutlich von einer Sättigung. Wir haben un mal nur begrenzt Platz für immer neue sich teilende Wasserlinsen.\n\nduckweeds_tbl &lt;- read_excel(\"data/duckweeds_density.xlsx\")\n\nIn der Tabelle 54.2 siehst du dann einmal einen Auszug aus den Daten zu den Wasserlinsen. Es ist ein sehr einfacher Datensatz mit nur zwei Spalten. Wir haben es eigentlich mit einem nicht linearen Zusammenhang zu tun, aber wir schauen uns hier mal an, was die Algorithmen uns so wiedergeben.\n\n\n\n\nTabelle 41.1— Auszug aus Wasserlinsendatensatz.\n\n\n\n\n\n\nduckweeds_density\nsensor\n\n\n\n\n4.8\n0.4303\n\n\n4.8\n0.4763\n\n\n4.8\n0.4954\n\n\n…\n…\n\n\n53.2\n2.1187\n\n\n53.2\n2.1296\n\n\n53.2\n2.1246\n\n\n\n\n\n\n\n\nIm Weiteren betrachten wir noch das Beispiel der Gummibärchendaten. Auch hier haben wir echte Daten vorliegen, so dass wir eventuell Ausreißer entdecken könnten. Da wir hier fehlende Werte in den Daten haben, entfernen wir alle fehlenden Werte mit der Funktion na.omit(). Damit löschen wir jede Zeile in den Daten, wo mindestens ein fehlender Wert auftritt.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  |&gt;\n  select(module, gender, age, height) |&gt; \n  mutate(gender = factor(gender, labels = c(\"männlich\", \"weiblich\"))) |&gt; \n  na.omit()\n\nIn der Tabelle 41.2 ist der Datensatz gummi_tbl nochmal als Auszug dargestellt dargestellt. Nun haben wir hier in dem Datensatz zu den Gummibärchen auch keine fehlenden Werte mehr.\n\n\n\n\nTabelle 41.2— Auszug aus den Gummibärchendaten für das Modul, dem Geschlecht, dem Alter und der Körpergröße.\n\n\n\n\n\n\nmodule\ngender\nage\nheight\n\n\n\n\nFU Berlin\nmännlich\n35\n193\n\n\nFU Berlin\nweiblich\n21\n159\n\n\nFU Berlin\nweiblich\n21\n159\n\n\nFU Berlin\nweiblich\n36\n180\n\n\n…\n…\n…\n…\n\n\nBiostatistik\nmännlich\n24\n187\n\n\nBiostatistik\nmännlich\n24\n182\n\n\nBiostatistik\nweiblich\n23\n170\n\n\nBiostatistik\nweiblich\n24\n180\n\n\n\n\n\n\n\n\nNun wollen wir uns aber erstmal den simpelsten Fall von Ausreißern und die Problematik dahinter visualisieren. Dann arbeiten wir uns zu komplexeren Paketen vor.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Ausreißer</span>"
    ]
  },
  {
    "objectID": "stat-modeling-outlier.html#visualisierung",
    "href": "stat-modeling-outlier.html#visualisierung",
    "title": "41  Ausreißer",
    "section": "41.3 Visualisierung",
    "text": "41.3 Visualisierung\nBetrachten wir also als erstes einmal die Daten der Wasserlinsen. Wenn wir von einem linearen Zusammenhang ausgehen würden, dann sehen die Daten so aus, als würde es Ausreißer zu Beginn und zum Ende der Messreihe geben. Manchmal lässt uns aber das Auge trügen und wir haben gar keinen linearen Zusammenhang in unseren Daten. Wenn wir dann eine log-Transformation durchführen, sehen die Daten schon sehr viel linearer aus. Diesen Weg schlägt auch Michel u. a. (2020) vor, wenn wir Ausreißer an den Rändern beobachten.\n\nBefore identifying outliers, authors should consider the possibility that the data come from a lognormal distribution, which may make a value look as an outlier on a linear but not on a logarithmic scale. — Michel u. a. (2020), p. 139\n\nMachen wir das doch einmal in den Daten für unsere Wasserlinsen. In der Abbildung 41.1 siehst du einmal die Daten, wie wir sie gemessen haben und einmal auf der log-Skala. Ich habe dann noch eine Gerade hinzugefügt. Wir sehen, dass sich die log-Transformation schon bemerkbar macht. Klare Ausreißer können wir jetzt schon nicht mehr klar erkennen.\n\nggplot(duckweeds_tbl, aes(duckweeds_density, sensor)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Gemessene Dichte der Wasserlinsen\", y = \"Sensorwert\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n\nggplot(duckweeds_tbl, aes(duckweeds_density, sensor)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Gemessene Dichte der Wasserlinsen\", y = \"Sensorwert\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  annotation_logticks(sides = \"bl\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Originale Daten. Aureißer liegen entlang der Geraden.\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(log\\) transformierte Daten. Die Bobachtungen folgen mehr der Geraden.\n\n\n\n\n\n\n\nAbbildung 41.1— Visualisierung der Sensorwerte nach Wasserlinsendichte. Pro Dichtewert liegen drei Sensormessungen vor. Es werden einmal die originalen Daten sowie die \\(log\\)-transformierten Daten betrachtet.\n\n\n\n\nIn der Abbildung 41.2 sehen wir einmal unsere Gummibärchendaten für das Alter in Jahren und die Körpergröße dargestellt. Als potenzielle Ausreißer haben wir hier die Teilnehmerinnnen des Girls Day in den Daten. Zum einen waren es nur Mädchen und zum anderen sehr junge und damit auch kleine Mädchen. Der Großteil der Daten machen ja Studierenden Anfang Zwanzig aus. Im Weiteren habe ich die Gummibärchen auch auf Weiterbildungen gezählt, so dass auch hier Ausreißer im Sinne eines hohen Alters in den Daten sind. Daher haben wir hier schön ein paar Ausreißer in den Daten. Schauen wir mal, ob die Algorithmen diese Ausreißer auch finden.\n\ngummi_tbl |&gt; \n  ggplot(aes(x = gender, y = age, color = gender)) +\n  geom_beeswarm(size = 1) +\n  theme_minimal() +\n  scale_color_okabeito(order = c(2, 7)) +\n  labs(x = \"Geschlecht\", y = \"Alter in Jahren\") +\n  theme(legend.position = \"none\")\n\ngummi_tbl |&gt; \n  ggplot(aes(x = gender, y = height, color = gender)) +\n  geom_beeswarm(size = 1) +\n  theme_minimal() +\n  scale_color_okabeito(order = c(2, 7)) +\n  labs(x = \"Geschlecht\", y = \"Körpergröße in [cm]\") +\n  theme(legend.position = \"none\") \n\n\n\n\n\n\n\n\n\n\n\n(a) Alter nach Geschlecht\n\n\n\n\n\n\n\n\n\n\n\n(b) Körpergröße nach Geschlecht\n\n\n\n\n\n\n\nAbbildung 41.2— Beeswarmplot als ein Dotplot für eine große Anzahl an Beobachtungen. Hier schauen wir uns einmal das Alter und die Körpergröße aufgeteilt nach Geschlecht an. Wir sehen klar die Gruppe der Ausreißer bei den weiblichen Beobachtungen. Hierbei handelt es sich um die Teilnehmerinnen des Girls Day. Auch ahben wir noch einige ältere Beobachtungen in den Daten.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Ausreißer</span>"
    ]
  },
  {
    "objectID": "stat-modeling-outlier.html#bekannte-kriterien-für-ausreißer",
    "href": "stat-modeling-outlier.html#bekannte-kriterien-für-ausreißer",
    "title": "41  Ausreißer",
    "section": "41.4 Bekannte Kriterien für Ausreißer",
    "text": "41.4 Bekannte Kriterien für Ausreißer\nIm Folgenden schauen wir uns einmal eine Auswahl an Kriterien für die Detektion von Ausreißern an. Ich nutze dazu dann gleich einmal einen Dummy-Datensatz – also einen Beispieldatensatz, wo wir wissen was los ist – in dem ich mir einfach mal zwei Ausreißer hineingelegt habe. Wir schauen dann mal, was die Algorithmen so wiedergeben und ob wir was erkennen können. Du findest noch mehr Möglichkeiten in dem Tutorium Outliers detection in R. Wir wollen uns hier aber auf ein paar der gängigsten Varianten konzentrieren. In den folgenden Abschnitten danach, schauen wir dann nochmal spezifisch in einzelne R Paket hinein.\n\nAusreißer mit Cook`s Abstand\nAusreißer mit leverage (deu. Hebelwirkung)\nAusreißer mit \\(\\sigma\\)-Filter\nAusreißer mit Hampel Filter\nAusreißer mit einem statistischen Test\n\nVeranschaulichen wir uns einmal den Zusammenhang an zwei Beispieldaten in der Tabelle 41.3 – einmal einen Datensatz ohne Ausreißer und dann einen Datensatz mit Ausreißern. Ich nehme dazu einfach das Gewicht in [mg] von zehn Flöhen und die jeweilige Sprunglänge in [cm]. Als erstes einmal einen Datensatz no_out_tbl ohne Ausreißer und dann einen Datensatz out_tbl mit den Sprungweiten von \\(56\\)cm und \\(2\\)cm als Ausreißer. Im Folgenden wollen wir uns natürlich nur den Datensatz mit den zwei Ausreißern anschauen.\n\n\n\nTabelle 41.3— Zwei Datentabellen zum Vergleich der Detektion von Ausreißern nach Cook’s Abstand.\n\n\n\n\n\n\n\n(a) Keine Ausreißer\n\n\n\n\n\nweight\njump_length\n\n\n\n\n1.1\n22\n\n\n2.3\n23\n\n\n2.1\n24\n\n\n3.7\n23\n\n\n4.1\n20\n\n\n5.4\n23\n\n\n7.6\n20\n\n\n4.3\n25\n\n\n5.8\n24\n\n\n8.1\n22\n\n\n\n\n\n\n\n\n\n\n\n(b) Zwei Ausreißer\n\n\n\n\n\nweight\njump_length\n\n\n\n\n1.1\n56\n\n\n2.3\n23\n\n\n2.1\n24\n\n\n3.7\n23\n\n\n4.1\n20\n\n\n5.4\n23\n\n\n7.6\n2\n\n\n4.3\n25\n\n\n5.8\n24\n\n\n8.1\n18\n\n\n\n\n\n\n\n\n\n\n\nJetzt schauen wir uns die Daten der obigen Tabellen auch als Visualisierung in Abbildung 41.3 an. Wir sehen die starken Ausreißer in der Visualisierung. Das ist auch so gewollt, wir haben die Ausreißer extra sehr extrem gewählt. Ich habe jetzt bewusst den letzten Ausreißer etwas weniger extrem ausgewählt. Weniger in dem Sinne, dass der Abstand zu den anderen Punkten nicht so groß ist, aber dennoch deutlich. Auch ist die Position herausfordernd für die Algorithmen. Ausreißer an den Rändern können schwer zu erkennen sein. Fangen wir also einmal an die Daten zu untersuchen.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scatterplot ohne Ausreißer\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatterplot mit Ausreißer\n\n\n\n\n\n\n\nAbbildung 41.3— Scatterplots der Datentabelle zum Vergleich der Detektion von Ausreißern nach verschiedenen Algorithmen.\n\n\n\n\n\n41.4.1 Ausreißer mit Cook`s Abstand\nMit der Cook’schen Distanz können wir herausfinden, ob eine einzelne Beobachtung ein Ausreißer im Zusammenhang zu den anderen Beobachtungen ist. Die Cook’sche Distanz misst, wie stark sich alle geschätzten Werte im Modell ändern, wenn der \\(i\\)-te Datenpunkt gelöscht wird. So einfach und auch so simple. Das Ganze machen wir natürlich nicht selber, sondern nutzen dafür die Funktion augment() aus dem R Paket {broom}. Die Funktion augment() braucht aber die Ausgabe einer linearen Regression, damit die Cook’sche Distanz berechnet werden kann. Also eigentlich vom Prozes her ganz einfach. Im Folgenden rechnen wir also eine simple Gaussian lineare Regression auf den Daten und schauen einmal, was wir dann über die einzelnen Beobachtungen erfahren und ob wir die eingestellten Ausreißer wiederfinden.\n\njump_fit &lt;- lm(jump_length ~ weight, data = out_tbl)\n\nWir können nun die Funktion augment() nutzen um die Cook’sche Distanz als .cooksd aus dem linearen Modellfit zu berechnen. Wir lassen uns noch die Variablen weight und jump_length wiedergeben um uns später dann die Visualisierung zu erleichtern.\n\ncook_tbl &lt;- jump_fit |&gt; \n  augment() |&gt; \n  select(weight, jump_length, .cooksd)\ncook_tbl\n\n# A tibble: 10 × 3\n   weight jump_length  .cooksd\n    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1    1.1          56 1.35    \n 2    2.3          23 0.143   \n 3    2.1          24 0.158   \n 4    3.7          23 0.0109  \n 5    4.1          20 0.0180  \n 6    5.4          23 0.00743 \n 7    7.6           2 0.268   \n 8    4.3          25 0.000225\n 9    5.8          24 0.0311  \n10    8.1          18 0.412   \n\n\nZuerst sehen wir, dass die \\(1\\)-ste und die \\(10\\)-te Beobachtung relativ hohe Werte der Cook’schen Distanz haben. Das heißt hier ist irgendwas nicht in Ordnung, es könnte sich also um Ausreißer handeln. Das haben wir ja auch so erwartet. Die beiden Beobachtungen sind ja auch unsere erschaffene Ausreißer. Nun brauchen wir noch einen Threshold oder Grenzwert um obkjektiv zu entscheiden ab wann wir eine Beobachtung als Ausreißer definieren. Es hat sich als “Kulturkonstante” der Wert von \\(4/n\\) als Threshold etabliert (Hardin und Hilbe 2007).\n\nGrenzwert für Cook’s Abstand .cooksd\n\nHardin und Hilbe (2007) bezeichnen Werte, die über \\(4/n\\) liegen als problematisch. Dabei ist dann \\(n\\) ist hierbei die Stichprobengröße. Damit haben wir dann auch die entsprechende Literaturquelle.\n\n\nBerechnen wir also einmal den Threshold für unseren Datensatz indem wir \\(4\\) durch \\(n = 10\\) teilen und erhalten einen Grenzwert von \\(0.4\\) für mögliche Ausreißer. In Abbildung 41.4 haben wir den Threshold einmal als rote Linie eingezeichnet. Auf der \\(x\\)-Achse ist das Gewicht weight der Flöhe eingezeichnet, damit sich die Punkte etwas verteilen. Ich habe als Label die Werte für die Sprungweite jump_length vergeben. Damit siehst du dann auch welche \\(y\\)-Werte aus den Sprungweiten problematisch sein könnten. Und wir sehen auch sofort was spannendes. Wir entfernen eher unsere Sprungweite \\(18\\) als unsere Sprungweite \\(2\\). Gut, die Sprungweite \\(56\\) ist auf jeden Fall ein Ausreißer, aber die Sprungweite \\(2\\) erwischen wir nicht, sondern entfernen die “richtige” Sprungweite \\(18\\). Deshalb immer Augen auf bei der automatsichen Entfernung von Ausreißern. Algorithmen können sich auch irren.\n\nggplot(cook_tbl, aes(weight, .cooksd)) +\n  geom_hline(yintercept = 0.4, color = \"red\") +\n  geom_label(aes(label = jump_length)) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 41.4— Visualisierung der Ausreißer nach der Cook’schen Distanz. Die Grenze als rote Linie ist mit \\(4/n = 0.4\\) berechnet worden.\n\n\n\n\n\nWas wir jetzt mit den Ausreißern machen, müssen wir uns überlegen. Im Prinzip haben wir zwei Möglichkeiten. Entweder entfernen wir die Beobachtungen aus unserem Datensatz oder aber wir setzen die Werte der Ausreißer auf NA oder eine andere passendere Zahl. Ich würde immer empfehlen, die Ausreißer einfach zu entfernen. Das kannst du einfach in deinem Excelfile machen oder etwas komplizierter wie hier über die Funktionen which() oder eben filter(). Weil es hier am Anfang noch relativ einfach sein soll, entfernen wir einfach die beiden Ausreißer aus unseren Daten. Wir erhalten dann einen kleineren Datensatz mit \\(n = 8\\) Beobachtungen. Leider ist jetzt unser Ausreißer \\(2\\) noch drin.\n\n… über which()… über augment()\n\n\nWir können jetzt mit der Funktion which() bestimmen welche Beobachtungen wir als Ausreißer identifiziert haben.\n\nremove_weight_id &lt;- which(cook_tbl$.cooksd &gt; 0.4)\n\nDann können wir einfach die Beobachtungen entfernen. Wir entfernen hier einfach die Zeilen, die im Objekt remove_weight_id hinterlegt sind.\n\nout_tbl[-remove_weight_id, ]\n\n# A tibble: 8 × 2\n  weight jump_length\n   &lt;dbl&gt;       &lt;dbl&gt;\n1    2.3          23\n2    2.1          24\n3    3.7          23\n4    4.1          20\n5    5.4          23\n6    7.6           2\n7    4.3          25\n8    5.8          24\n\n\n\n\nOder wir nutzen die Funktion filter() auf der Ausgabe der Funktion augment(). Ich persönlich mag ja diese Variante hier mehr, aber es kommt immer auf den Kontext an.\n\njump_fit |&gt; \n  augment() |&gt; \n  select(weight, jump_length, .cooksd) |&gt; \n  filter(.cooksd &lt;= 0.4)\n\n# A tibble: 8 × 3\n  weight jump_length  .cooksd\n   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1    2.3          23 0.143   \n2    2.1          24 0.158   \n3    3.7          23 0.0109  \n4    4.1          20 0.0180  \n5    5.4          23 0.00743 \n6    7.6           2 0.268   \n7    4.3          25 0.000225\n8    5.8          24 0.0311  \n\n\n\n\n\nDie nächste Möglichkeit wäre auf dem kleineren Datensatz nochmal eine Ausreißeranalyse zu rechnen und zu hoffen, dass wir jetzt alle Ausreißer erwischen. Dann stellt sich aber die Frage wie lange wir das machen wollen. Am Ende haben wir dann ja kaum noch Daten über, wenn es schlecht läuft.\nDu siehst, dieser Zugang über die Cook’sche Distanz an die Detektion von Ausreißern ist sehr simple. Wir schauen einfach auf die Cook’sche Distanz und haben so einen schnellen Überblick. Ich empfehle auch gerne dieses Vorgehen um einmal einen Überblick über die Daten zu erhalten. Leider liefern nicht alle Modelle eine Cook’sche Distanz, daher müssen wir uns jetzt etwas strecken und noch andere Verfahren einmal ausprobieren. Und am Ende müssen wir natürlich auch schauen, ob wir die richtigen Ausreißer entfernt haben.\n\n\n41.4.2 Ausreißer mit leverage\nEine weitere Möglichkeit Ausreißer zu finden ist mit der Hebelwirkung (eng. leverage) von einzelnen Beobachtungen. Die Frage ist hier, haben einzelne Punkte einen besonderen Einfluss auf den Verlauf der Geraden haben. Beeinflusst also ein einzelner Punkt den Verlauf der Geraden besonders? Punkte, die an den Rändern liegen haben meist ein größeren Einfluss auf den Verlauf der Geraden als Punkte in der Wolke. Wir können usn auch über die Funktion augment() die Werte für die Hebelwirkung der einzelnen Beobachtungen als .hat-Werte wiedergeben lassen.\n\nleverage_tbl &lt;- jump_fit |&gt; \n  augment() |&gt; \n  select(weight, jump_length, .hat)\n\nWenn wir nur eine Einflussvaribale vorliegen haben, dann ist der Grenzwert der leverage faktisch identisch mit dem Grenzwert für Cook’sche Distanz. Wenn du aber mehr \\(x\\) in deinem Modell hast, dann musst den Grenzwert anpassen.\n\nGrenzwert für Leverage .hat\n\nIm Allgemeinen sollte ein Punkt mit einer Hebelwirkung von mehr als \\((2k+2)/n\\) sorgfältig geprüft werden, wobei \\(k\\) die Anzahl der Einflussvariablen und \\(n\\) die Anzahl der Beobachtungen ist.\n\n\nIn unserem Fall wäre dann \\(k = 1\\) sowie \\(n = 10\\), so dass wir auf einen Grenzwert von \\((2\\cdot 1 +2)/10 = 0.4\\) kommen, wie auch schon bei Cook’s Distanz. Das ist eigentlich schon alles. Wir können uns dann in der Abbildung 41.5 einmal die Werte für leverage anschauen. Ich habe wieder den Grenzwert als rote Linie ergänzt und in den Kästchen stehen dann immer die Messwerte der Sprungweite. Hier sehen wir dann spannenderweise, dass wir gar keine Ausreißer finden. Ja, auch das kann passieren. Wir sehen zwar, dass sich drei Messwerte ungünstig verhalten und hohe .hat-Werte haben, aber noch würden diese Werte unter dem Grenzwert von \\(0.4\\) liegen. Mit mehr Beobachtungen und damit auch Fallzahl würde der Grenzwert auch niedriger liegen. So sieht man wieder, für gewisse Methoden brauchst du dann einfach auch Fallzahl.\n\nggplot(leverage_tbl, aes(weight, .hat)) +\n  geom_hline(yintercept = 0.4, color = \"red\") +\n  geom_label(aes(label = jump_length)) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 41.5— Visualisierung der Ausreißer nach der leverage. Die Grenze als rote Linie ist mit 0.4 berechnet worden.\n\n\n\n\n\n\n\n41.4.3 Ausreißer mit \\(\\boldsymbol{\\sigma}\\) Filter\nEine andere Möglichkeit besondere Werte oder eben auch Ausreißer zu finden ist die Sigmamethode (sym. \\(\\sigma\\)). Dabei machen wir uns zu nutzen, dass unter der Annahme der Normalverteilung, unsere Werte symmetrisch nach der Standardabweichung \\(\\sigma\\) um unseren Mittelwert verteilt sind. Damit haben wir auch gleich eine grundlegende Annahme an unsere Daten. Die Sigmamethode funktioniert nur, wenn wir normalverteilte Daten vorliegen haben. Haben wir das, dann können wir den Mittelwert berechnen und über die entsprechende Standardabweichung dann abschätzen wie häufig unsere einzelnen Beobachtungen aufgetreten wären. In der Tabelle 41.4 habe ich dir einmal die Wahrscheinlichkeiten des Auftretens einer Beobachtung in dem Intervall \\(\\mu \\pm k \\cdot \\sigma\\) für verschiedene \\(k\\)-Werte dargestellt. Haben wir zum Beispiel eine mittlere Körpergröße von \\(180cm\\) bei Männern mit einer Standardabweichung \\(\\sigma\\) von \\(10cm\\) in unseren Gummibärchendaten vorliegen, dann sollten \\(68\\%\\) unserer Männer im Intervall \\([170; 190]\\) liegen. Es wäre also auch damit sehr unwahrscheinlich einen Mann mit der Größe von \\(220cm\\) anzutreffen, da dieser schon im Bereich von \\(4\\sigma\\) liegen würde. Damit wäre nur ein Mann von ca. 16000 so groß.\n\n\n\nTabelle 41.4— Wahrscheinlichkeiten für das Auftreten einer Beobachtung in dem Intervall \\(\\mu \\pm k \\cdot \\sigma\\) für normalverteilte Daten.\n\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{\\sigma}\\)\nWahrscheinlichkeit des Auftretens\nChance des Auftretens\n\n\n\n\n\\(\\mu \\pm 1 \\cdot \\sigma\\)\n\\(\\mathbf{68.26}00000\\%\\)\n\n\n\n\\(\\mu \\pm 2 \\cdot \\sigma\\)\n\\(\\mathbf{95.46}00000\\%\\)\n\\(1:20\\)\n\n\n\\(\\mu \\pm 3 \\cdot \\sigma\\)\n\\(\\mathbf{99.73}00000\\%\\)\n\\(1:370\\)\n\n\n\\(\\mu \\pm 4 \\cdot \\sigma\\)\n\\(\\mathbf{99.9937}000\\%\\)\n\\(1:15873\\)\n\n\n\\(\\mu \\pm 5 \\cdot \\sigma\\)\n\\(\\mathbf{99.999943}0\\%\\)\n\\(1:1.7 \\cdot 10^6\\)\n\n\n\\(\\mu \\pm 6 \\cdot \\sigma\\)\n\\(\\mathbf{99.9999996}\\%\\)\n\\(1:2.5 \\cdot 10^8\\)\n\n\n\n\n\n\nWir können wir nun die Sigmaregel an unseren Daten anwenden? Wir haben dazu wieder verschiedene Möglichkeiten. Zuerst wollen wir einmal die Sigmaregel auf die Residuen unser Regression anwenden. Die Residuen beschreiben ja den Abstand der einzelnen Punkte zu der Geraden und sind von sich normalverteilt. Wir erhalten die Residuen .resid mit der Funktion augment() aus dem R Paket {broom}. Die Werte .fitted beschreiben unsere Sprungweiten auf der Geraden.\n\nresid_tbl &lt;- jump_fit |&gt; \n  augment() |&gt; \n  select(weight, jump_length, .fitted, .resid) \nresid_tbl\n\n# A tibble: 10 × 4\n   weight jump_length .fitted .resid\n    &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1    1.1          56   37.4  18.6  \n 2    2.3          23   32.5  -9.54 \n 3    2.1          24   33.3  -9.35 \n 4    3.7          23   26.8  -3.85 \n 5    4.1          20   25.2  -5.22 \n 6    5.4          23   19.9   3.06 \n 7    7.6           2   11.0  -9.00 \n 8    4.3          25   24.4   0.591\n 9    5.8          24   18.3   5.68 \n10    8.1          18    8.97  9.03 \n\n\nWir können jetzt für die Residuen die Standardabweichung bestimmen und schauen in wie weit die einzelnen Beobachtungen um die Gerade streuen unter den jeweiligen Sigmabereichen. Die Standardabweichung haben wir ja zusammen mit den Ausreißern berechnet. Wenn also unsere Gerade sehr schief ist, dann werden die Ausreißer ja auch irgendwie mit in dem Bereich von einem vielfachen von Sigma liegen. Die Gerade wird ja so optimiert, dass möglichst die Residuen zu allen Punkten klein ist.\n\nsd(resid_tbl$.resid)\n\n[1] 9.252808\n\n\nJetzt können wir uns in der Abbildung 41.6 einmal die Gerade mit der Standardabweichung der Residuen für verschiedene \\(k\\) anschauen. Wie du siehst liegen alle Beobachtungen noch in der Bandbreite von \\(2\\sigma\\). Da Problem ist eben, dass die beiden Ausreißer die Gerade so verschieben, dass dann die beiden Ausreißer doch nicht so schlimm sind, wenn wir die Sachlage von der Geraden aus betrachten.\n\nresid_tbl |&gt; \n  ggplot(aes(weight, jump_length)) +\n  theme_minimal() +\n  geom_ribbon(aes(ymin = .fitted - 2 * sd(.resid),\n                  ymax = .fitted + 2 * sd(.resid),\n                  fill = \"2sd\"),\n              alpha = 0.2) +\n  geom_ribbon(aes(ymin = .fitted - 1 * sd(.resid),\n                  ymax = .fitted + 1 * sd(.resid),\n                  fill = \"1sd\"),\n              alpha = 0.2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_fill_okabeito(name = \"\") +\n  geom_point() \n\n\n\n\n\n\n\nAbbildung 41.6— Visualisierung der Sigmaregel auf den Residuen der Geraden.\n\n\n\n\n\nHier müssten wir im Prinzip erst die potenziellen Ausreißer entfernen und dann die Gerade rechnen und schauen, ob unsere potenziellen Ausreißer noch im Intervall wären. Damit säumen wir das Pferd aber von hinten auf. Das wollen wir dann einmal machen. Wir entscheiden jetzt, dass die Werte der Sprunglänge von \\(56\\) und \\(2\\) potenzielle Ausreißer sind.\n\nsd_out_tbl &lt;- out_tbl |&gt; \n  mutate(potential_outlier = c(1, 0, 0, 0, 0, 0, 1, 0, 0, 0))\nsd_out_tbl\n\n# A tibble: 10 × 3\n   weight jump_length potential_outlier\n    &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1    1.1          56                 1\n 2    2.3          23                 0\n 3    2.1          24                 0\n 4    3.7          23                 0\n 5    4.1          20                 0\n 6    5.4          23                 0\n 7    7.6           2                 1\n 8    4.3          25                 0\n 9    5.8          24                 0\n10    8.1          18                 0\n\n\nDann filtern wir einmal die potenziellen Ausreißer aus unseren Daten raus und rechnen den Mittelwert und die Standardabweichung der Sprungweite aus. Hier sehen wir schon, dass sich die Werte unterscheiden. Dadurch, dass wir die beiden potenziellen Ausreißer entfernt haben fällt unsere Standardabweichung \\(\\sigma\\) von \\(9.25\\) auf nur \\(2.33\\). Ein starkes Indiz, dass wir es hier mit Ausreißern zu tun haben. Oder zumindestens mit so extremen Werten, dass wir ein sehr viel größere Standardabweichung mit den Werte vorliegen haben als ohne die Werte.\n\nsd_out_tbl |&gt; \n  filter(!potential_outlier) |&gt; \n  summarise(Mittelwert = mean(jump_length), \n            Standardabweichung = sd(jump_length))\n\n# A tibble: 1 × 2\n  Mittelwert Standardabweichung\n       &lt;dbl&gt;              &lt;dbl&gt;\n1       22.5               2.33\n\n\nIn der Abbildung 41.7 sehen wir dann einmal das Ergebnis unseres Vorgehens. Die \\(\\circ\\) stellen dabei die Beobachtungen dar, die wir nicht als Ausreißer deklariert haben. Die \\(\\bullet\\) repräsentieren dann unsere potenziellen Ausreißer. Dann konnte ich mit den \\(\\circ\\)-Werten einmal den Mittelwert berechnen und die Standardabweichungen für \\(k\\) von 1 bis 3 einfärben. Wir sehen klar, dass die potenziellen Ausreißer nicht in den Sigmaintervallen liegen. Daher wären die beiden potenziellen Ausreißer schon ungewöhnliche Repräsentation den der anderen Werte. Wie immer, liegt hier natürlich das Problem darin, dass wir irgendwie vorab definieren müssen, welche der Werte denn als potenzielle Ausreißer in Frage kommen. Aber meistens kannst du das dann doch, so dass die Sigmaregel eine gute Methode ist einfach und schnell Ausreißer zu definieren.\n\nsd_out_tbl |&gt; \n  ggplot(aes(weight, jump_length)) +\n  theme_minimal() +\n  geom_ribbon(aes(ymin = 22.5 - 3 * 2.33,\n                  ymax = 22.5 + 3 * 2.33,\n                  fill = \"3sd\"), alpha = 0.2) +\n  geom_ribbon(aes(ymin = 22.5 - 2 * 2.33,\n                  ymax = 22.5 + 2 * 2.33,\n                  fill = \"2sd\"), alpha = 0.2) +\n  geom_ribbon(aes(ymin = 22.5 - 1 * 2.33,\n                  ymax = 22.5 + 1 * 2.33,\n                  fill = \"1sd\"), alpha = 0.2) +\n  scale_fill_okabeito(name = \"\") +\n  geom_point(aes(shape = as_factor(potential_outlier))) +\n  scale_shape_manual(name = \"Ausreißer\", values = c(1, 19)) \n\n\n\n\n\n\n\nAbbildung 41.7— Darstellung der Sigmaregel mir vorher deklarierten Ausreißern. Die Sigmaintervalle wurden auf den \\(\\circ\\)-Werten berechnet und die zwei potenziellen Ausreißer als \\(\\bullet\\)-Werte vorab definiert und somit aus der Berechnung herausgenommen.\n\n\n\n\n\n\n\n41.4.4 Ausreißer mit Hampelfilter\nJetzt kann man sich natürlich fragen, wenn es eine Sigmaregel gibt in der wir den Mittelwert plusminus \\(k\\)-mal die Standardabweichung rechnen, geht das denn nicht auch mit dem Median \\(\\tilde{y}\\) anstatt dem Mittelwert? Ja geht auch. Und deshalb heißt dieser Filter dann Hampelfilter. Ja, das ist dann immer in der Statistik so, dass auf einmal die Sachen wieder nach den Erfinder heißen und man sich das nicht am Namen merken kann, was der Filter oder die Methode macht. Der Hampelfilter gibt uns aber auch in Intervall \\(I\\) wieder, in dem wir sagen würden, dass wir alle Beobachtungen wiederfinden sollten. Wenn wir Beobachtungen außerhalb des Intervalls \\(I\\) vorliegen haben, dann sind diese Beobachtungen Ausreißer. Wir berechnen das Intervall \\(I\\) für den Hampelfilter wie folgt.\n\\[\nI = [\\tilde{y} - 3 \\cdot MAD;\\; \\tilde{y} + 3 \\cdot MAD]\n\\]\nWir nehmen einfach den Median \\(\\tilde{y}\\) und addieren und subtrahieren das dreifache des \\(MAD\\)-Wertes. Den Wert für \\(MAD\\) mit der Median absolute deviation (deu. Mittlere absolute Abweichung vom Median, abk. MAD) können wir dann ebenfalls recht einfach wie folgt berechnen. Der MAD ist damit der Median der absoluten Abweichungen aller Beobachtungen vom Median. Sozusagen der Median vom Median.\n\\[\nMAD = \\tilde{d}\\; \\mbox{mit}\\; d = y_i - |\\tilde{y}|\n\\]\nWeil das sich meistens dann doch wirrer anhört als es ist, hier einmal ein Zahlenbeispiel mit sieben willkürlich gewählten Zahlen in dem Vektor \\(y\\).\n\ny &lt;- c(1, 2, 4, 6, 9, 12, 15)\n\nDann berechnen wir einfach einmal den Median. Da ich die Zahlenreihe schon vorab sortiert habe, sieht man gleich, dass der Median \\(6\\) ist.\n\nmedian(y)\n\n[1] 6\n\n\nJetzt berechnen wir noch den absoluten Abstand von jeder Beobachtung \\(y\\) zu dem Median \\(\\tilde{y}\\) gleich 6 und sortieren die Werte wieder. Denn dann können wir wieder den mittleren Wert als Median für unseren \\(MAD\\)-Wert bestimmen.\n\n(y - median(y)) |&gt; abs() |&gt; sort()\n\n[1] 0 2 3 4 5 6 9\n\n\nDamit wäre der \\(MAD\\) in unserem Beispiel auch \\(4\\) und das können wir auch mit der Funktion \\(mad()\\) einmal überprüfen. Wir müssen hier noch als constant gleich 1 wählen, da wir sonst noch einen Korrekturterm in der Formel erhalten, die wir hier nicht brauchen.\n\nmad(y, constant = 1)\n\n[1] 4\n\n\nDann können wir auch schon das Intervall für unser Beispiel aufschreiben.\n\\[\nI = [6 - 3 \\cdot 4;\\; 6 + 3 \\cdot 4] = [-6; 18]\n\\]\nDann können wir den Hampelfilter einmal auf unserem Beispiel der Sprungweitendaten anwenden. Ich nehme hier den vollen Datensatz und deklarieren nicht vorab potenzielle Ausreißer wie bei der Sigmaregel. Das ist eigentlich ein Vorteil des Hampelfilters, wir müssen hier nicht so viel vorher als potenzielle Ausreißer definieren.\n\nout_tbl |&gt; \n  summarise(Median = median(jump_length), \n            MAD = mad(jump_length, constant = 1))\n\n# A tibble: 1 × 2\n  Median   MAD\n   &lt;dbl&gt; &lt;dbl&gt;\n1     23   1.5\n\n\nDamit ahben wir einen Median von \\(23\\) und einen MAD-Wert von \\(1.5\\) vorliegen. Somit kann ich mir dann im geom_ribbon() das Intervall in der Abbildung 41.8 einmal visualisieren. Das hat ziemlich gut funktioniert. Wir sehen, dass unsere beiden Ausreißer außerhalb des Intervalls des Hampelfilters liegen. Die letzte Beobachtung liegt ziemlich genau auf der Grenze, was nicht weiter schlimm ist. Hier würde ich dann ein Auge zudrücken.\n\nsd_out_tbl |&gt; \n  ggplot(aes(weight, jump_length)) +\n  theme_minimal() +\n  geom_ribbon(aes(ymin = 23 - 3 * 1.5,\n                  ymax = 23 + 3 * 1.5,\n                  fill = \"Hampelfilter\"), alpha = 0.2) +\n  scale_fill_okabeito(name = \"\") +\n  geom_point(aes(shape = as_factor(potential_outlier))) +\n  scale_shape_manual(name = \"Ausreißer\", values = c(1, 19)) \n\n\n\n\n\n\n\nAbbildung 41.8— Darstellung des Hampelfilters mir vorher deklarierten Ausreißern. Der Hampelfilter wurde auf allen Werten berechnet und gibt die beiden Ausreißer (\\(\\bullet\\)-Wert) wieder.\n\n\n\n\n\n\n\n41.4.5 Ausreißer mit einem statistischen Test\nNeben den einfachen Regeln, gibt es auch die Möglichkeit einen statistischen Test für Ausreißer zu rechnen. Alle Folgenden drei Tests funktionieren nur auf annähernd normalverteilten Daten. Das ist schon mal die erste wichtige Einschränkung. Wir schauen also, ob wirklich alle Beobachtungen zu einer Normalverteilung passen. Die Beobachtungen, die dann nicht passen, schmeißen wir raus. Also im Prinzip testen wir uns eine Normalverteilung zusammen. Neben dieser Einschränkung gibt es dann bei allen drei Tests noch weitere. Ich selber mag die statistischen Tests auf einen Ausreißer nicht. Zum einen sehr viel Arbeit diese zu Programmieren und durchzuführen und zum anderen nutzen mir die Informationen aus dem Testen wenig. Für die Ausführung in R besuche auch gerne das Tutorium Outliers detection in R - Statistical tests.\n\nDer Grubbs’s Test lässt sich in R mit der Funktion grubbs.test() in dem R Paket {outliers} finden. Neben der Annahme, dass unsere Daten normalvertelt sind, testet der Grubb’s Test immer nur eine einzige Beobachtung, ob diese Beobachtung ein Ausreißer ist. Das macht die Sache schon sehr aufwendig und es wird auch immer nur der kleinste oder der größte Wert betrachtet.\nDer Dixon’s Test ist die Grubb’s Variante für eine kleine Stichprobe von weniger als \\(n = 25\\) Beobachtungen. Aber auch hier Testen wir nur eine Beobachtung, ob diese Beobachtung ein Ausreißer ist. Darüber hinaus betrachtet der Dixon’s Test auch nur den kleinsten oder größten Wert. Die Funktion dixon.test() findest du ebenfalls in dem R Paket {outliers}.\nDer Rosner’s Test mit der R Funktion rosnerTest() aus dem R Paket {EnvStats} erlaubt das Testen von mehreren Ausreißern auf einmal. Hier brauchen wir aber eine Fallzahl von mehr als 20 Beobachtungen, sonst funktioniert der Test nicht robust. Der größte Nachteil hier ist, dass du vorher sagen musst, wie viele Ausreißer \\(k\\) den in deinen Daten drin sind. Wenn du zu wenig angibst, dann übersiehst du welche und bei einer zu großem \\(k\\) kommt der Algorithmus nicht mit.\n\nWir anfangs schon erwähnt, bin ich kein Fan der statistischen Tests auf Ausreißer. Am Ende testet und schneidet man sich eine Normalverteilung aus den Daten, auch wenn ursprünglich gar keine Normalverteilung vorlag. Plus du hast die ganzen Probleme des statistischen Testens mit Fehlerraten. Aber auf der anderen Seite ist es ja hier auch ein Nachschlagewerk.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Ausreißer</span>"
    ]
  },
  {
    "objectID": "stat-modeling-outlier.html#ausreißer-mit-performance",
    "href": "stat-modeling-outlier.html#ausreißer-mit-performance",
    "title": "41  Ausreißer",
    "section": "41.5 Ausreißer mit {performance}",
    "text": "41.5 Ausreißer mit {performance}\nNun wollen wir uns den echten Daten zuwenden und dort einmal schauen, ob wir Ausreißer finden können. Wir nutzen hierzu einmal die Funktion check_outliers() aus dem R Paket {performance}. Die Funktion check_outliers() rechnet nicht eine statistische Maßzahl für die Bestimmung eines Ausreißers sondern eine ganze Reihe an Maßzahlen und gewichtet diese Maßzahlen. Am Ende trifft die Funktion check_outliers() dann eine Entscheidung welche Beobachtungen Ausreißer sind. Dabei werden alle Variablen betrachtet. Es gibt keinen Unterschied zwischen \\(y\\) oder \\(x\\). Wir nutzen den ganzen Datensatz. Mehr zu der Funktion auf der Hilfeseite zu Outliers detection (check for influential observations). Wie immer kann die Funktion weit mehr, als das ich hier diskutieren kann. Wir wollen jetzt einmal für den Gummibärchendatensatz schauen, ob wir unsere Girl’s Day Teilnehmerinnen als Ausrißer finden oder aber die etwas älteren Semester meiner Statistikweiterbildungen.\n\nout_performance_obj &lt;- check_outliers(gummi_tbl)\nout_performance_obj\n\n12 outliers detected: cases 113, 115, 116, 117, 122, 134, 138, 141, 296,\n  297, 298, 299.\n- Based on the following method and threshold: mahalanobis (13.816).\n- For variables: age, height.\n\n\nWir finden also zwölf Ausreißer in unseren Daten. Wir können diese Beobachtungen einmal mit der Funktion filter() rausziehen und uns anschauen. Wir sehen, dass wir hier die älteren Semester in den Daten als Ausreißer wiederfinden, jedoch nicht die Teilnehmerinnen des Girl’s Day.\n\ngummi_tbl |&gt; \n  filter(out_performance_obj)\n\n# A tibble: 12 × 4\n   module                    gender     age height\n   &lt;chr&gt;                     &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1 FU Berlin                 weiblich    57    173\n 2 FU Berlin                 männlich    45    198\n 3 FU Berlin                 männlich    58    173\n 4 FU Berlin                 männlich    61    182\n 5 FU Berlin                 weiblich    45    163\n 6 FU Berlin                 weiblich    47    178\n 7 FU Berlin                 weiblich    54    163\n 8 FU Berlin                 weiblich    53    167\n 9 Fakultaetsinformationstag weiblich    54    171\n10 Fakultaetsinformationstag weiblich    46    165\n11 Fakultaetsinformationstag weiblich    55    177\n12 Fakultaetsinformationstag weiblich    46    173\n\n\nDann bauen wir uns einmal einen Datensatz mit einer zusätzlichen Spalte outlier in der wir uns markieren, welche Beobachtung ein Ausreißer nach der Funktion check_outliers() ist. Dann können wir den Datensatz nutzen um uns einmal die Ausreißer in den gesammten Daten zu visualisieren.\n\ngummi_performance_out_tbl &lt;- gummi_tbl |&gt; \n  mutate(outlier = out_performance_obj)\n\nIn der Abbildung 41.9 siehst du einmal die Ausreißer als \\(\\bullet\\)-Werte und die normalen Beobachtungen als \\(\\circ\\)-Werte. Wie du ziemlich gut erkennen kannst, werden die älteren Personen als Ausreißer gefunden nicht jedoch die jungen Teilnehmerinnen des Girl’s Day. Die Ausreißer beziehen sich auch auf das Alter und nicht auf die Körpergröße. Durch die Körpergröße finden wir keine zusätzlichen Ausreißer in den Daten.\n\ngummi_performance_out_tbl |&gt; \n  ggplot(aes(x = gender, y = age, color = gender, shape = outlier)) +\n  geom_beeswarm(size = 1) +\n  theme_minimal() +\n  scale_color_okabeito(order = c(2, 7), guide = \"none\") +\n  scale_shape_manual(name = \"Ausreißer\", values = c(1, 19)) +\n  labs(x = \"Geschlecht\", y = \"Alter in [Jahren]\") \n\ngummi_performance_out_tbl |&gt; \n  ggplot(aes(x = gender, y = height, color = gender, shape = outlier)) +\n  geom_beeswarm(size = 1) +\n  theme_minimal() +\n  scale_color_okabeito(order = c(2, 7), guide = \"none\") +\n  scale_shape_manual(name = \"Ausreißer\", values = c(1, 19)) +\n  labs(x = \"Geschlecht\", y = \"Körpergröße in [cm]\") \n\n\n\n\n\n\n\n\n\n\n\n(a) Alter nach Geschlecht\n\n\n\n\n\n\n\n\n\n\n\n(b) Körpergröße nach Geschlecht\n\n\n\n\n\n\n\nAbbildung 41.9— Der Beeswarm ist ein Dotplot für eine große Anzahl an Beobachtungen. Hier schauen wir uns einmal das Alter und die Körpergröße aufgeteilt nach Geschlecht an. Die gefundenen Ausreißer sind mit \\(\\bullet\\) markiert, die nromalen Beobachtungen als \\(\\circ\\) dargestellt.\n\n\n\n\nJetzt müssten wir eigentlich einmal mit den Optionen und Methoden in der Funktion check_outlier() spielen und verschiedene Einstellungen testen. Das übersteigt dann aber natürlich dieses Kapitel. So gibt es auch eine plot() Funktion, aber diese stößt bei der Menge an Beobachtungen in den Gummibärchendaten an die Grenze. Wir sehen dann einfach in der Abbildung nichts mehr. Wir können auch Ausreißer anhand eines Modells ermitteln, aber auch hier hilft dann die Hilfeseite der Funktion check_outlier() unter Outliers detection (check for influential observations) weiter.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Ausreißer</span>"
    ]
  },
  {
    "objectID": "stat-modeling-outlier.html#ausreißer-mit-olsrr",
    "href": "stat-modeling-outlier.html#ausreißer-mit-olsrr",
    "title": "41  Ausreißer",
    "section": "41.6 Ausreißer mit {olsrr}",
    "text": "41.6 Ausreißer mit {olsrr}\nEine weitere Möglichkeit Ausreißer zu finden bietet das R Paket {olsrr}, wenn wir uns nur auf normalverteilte Daten konzentrieren. Die grundlegende Idee ist dabei, dass wir ein Modell mit lm() gerechnet haben und jetzt anhand des Modellfits schauen wollen, ob wir auffällige Beobachtungen gegeben dem Modell in unseren Daten haben. Im einfachsten Fall schauen wir dabei, ob wir dann Beobachtungen haben, die weiter weg von der Geraden liegen. Das R Paket {olsrr} hat eine Vielzahl an Funktionen, die wir nutzen können um auffällige Beobachtungen zu finden. Schau dir einfach einmal das R Paket und die Webseite näher an. Nervig ist vor allem, dass wir keine reinen {ggplot} Objekte wiederkriegen, so dass wir dann nicht mal eben mit + das Layout oder andere Dinge ändern können. Das macht uns dann hässliche Abbildungen und ich mag keine hässlichen Abbildungen. Daher sind die Abbildungen eher was für den Anhang als in der Arbeit selber. Auch ist das Extrahieren der Ausreißer aus dem {olsrr} Objekt dann nicht so richtig einfach. Da hätte ich mir eigentlich mehr erhofft. Nur die Abbildung ist zwar schön, aber ich will ja auch die Beobachtungen als Objekt in R wieder haben. Sonst muss ich mir das ja alles mühselig wieder eintippen.\nIn der Abbildung 41.10 siehst du dann einmal die Ausgabe der Diagnoseabbildung für die Ausreißer sowie der leverage. Hier finden wir dann die erste Beobachtung als klaren Ausreißer, die andere Beobachtung jedoch nicht. Da die Abbildung dann auch mit den echten Werten von \\(x\\) und \\(y\\) zu tun hat, muss man immer etwas genauer schauen, welche Beobachtung nun für welche Werte steht. Der Grenzwert für die Ausreißer ist wie oben schon erklärt auf \\(0.4\\) gesetzt.\n\njump_fit |&gt; \n  ols_plot_resid_lev() \n\n\n\n\n\n\n\nAbbildung 41.10— Die Ausreißer und leverage Diagnoseabbildung für unsere Sprungweitendaten. Der Ausreißer an der ersten Position wird erkannt, der andere Ausreißer hingegen nicht.\n\n\n\n\n\nSchauen wir uns dann nochmal ein größeres Beispiel mit den Gummibärchendaten an. Hier bilden wir erstmal das Modell in dem Sinne, dass wir rausfinden wollen, ob das Alter von dem Geschlecht und der Körpergröße abhängt. Dann schauen wir, ob wir Beobachtungen haben, die nicht richtig passen. Auch hier hoffe ich die Teilnehmerinnen vom Girl’s Day oder aber meine älteren Teilnehmer von einem Workshop wiederzufinden.\n\ngummi_fit &lt;- lm(age ~ gender + height, data = gummi_tbl)\n\nDann schauen wir uns einmal in der Abbildung 41.11 einmal die Diagnoseabbildung für die Ausreißer sowie der leverage für die Gummibärchendaten an. Da ist man doch sehr überrascht, dass so viele Ausreißer in den Daten erkannt werden. Meine Güte sind das viele Beobachtungen, die wir rauschmeißen würden. Auch hier hilft es dann nur, sich die Ausreißer einmal in den echten Daten anzuschauen, damit wir wissen, was wir da entfernen würden.\n\nols_plot_resid_lev(gummi_fit)\n\n\n\n\n\n\n\nAbbildung 41.11— Die Ausreißer und leverage Diagnoseabbildung für unsere Gummibärchendaten. Hier werden reichlich Ausreißer in unseren Daten erkannt.\n\n\n\n\n\nLeider müssen wir uns jetzt etwas umnständlich die Daten aus dem Plot ziehen. Allgemein ist die Arbeit mit den Objekten der Abbildungen nicht so super gelöst. Jedenfalls habe ich da keine guten Funktionen auf die Schnelle gefunden. Wir können uns aber die Daten data aus dem Plotobjekt rausziehen. Wir müssen aber mit print_plot = FALSE verhindern, dass uns der Plot angezeigt wird, sonst haben wir immer alles voll mit den Abbildungen. In dem Datensatz steht dann in der Spalte color, welche Beobachtung ein Ausreißer ist. Ja, ist dämlich, aber was will man machen.\n\noutlier_tbl &lt;- ols_plot_resid_lev(gummi_fit, print_plot = TRUE) |&gt; \n  pluck(\"data\") |&gt; \n  as_tibble()\n\n\n\n\n\n\n\noutlier_tbl\n\n# A tibble: 699 × 6\n     obs leverage rstudent color   fct_color   txt\n   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;ord&gt;     &lt;int&gt;\n 1     1  0.00500   2.21   outlier outlier       1\n 2     2  0.00534  -0.522  normal  normal       NA\n 3     3  0.00534  -0.522  normal  normal       NA\n 4     4  0.00604   2.30   outlier outlier       4\n 5     5  0.00314  -0.216  normal  normal       NA\n 6     6  0.00314  -0.216  normal  normal       NA\n 7     7  0.00377  -0.507  normal  normal       NA\n 8     8  0.00748  -0.254  normal  normal       NA\n 9     9  0.00428  -0.0490 normal  normal       NA\n10    10  0.00314  -0.581  normal  normal       NA\n# ℹ 689 more rows\n\n\nJetzt können wir anfangen zu filtern. Ich empfehle dir die Spalte color zu nehmen und nur nach Beobachtungen mit normal zu filtern. Dann entfernst du alle Ausreißer aus den Daten. Wir haben aber nicht nur Ausreißer mit dem Label outlier in den Daten sondern auch welche mit dem Label leverage oder aber nach beiden Kriterien. Deshalb musst du schauen, welche Beobachtungen du dann rauschmeißen willst bzw. behalten willst. Ich filtere hier mal nach normal. In der Abbildung 41.12 siehst du dann einmal auf der linken Seite die Daten mit den Labels für die jeweiligen Ausreißer. Immerhin finden wir hier auch wieder alle älteren Semester plus einen Teil der jüngeren Teilnehmerinnen des Girl’s Day. Wir entfernen aber eine Menge an Datenpunkten. Insbesondere das Entfernen nach der leverage muss kritisch hinterfragt werden. Die angeblichen Ausreißer nach leverage liegen ja direkt in den Punktewolken. Das Entfernen würde ich also hinterfragen und dann komplexer filtern.\n\ngummi_tbl |&gt; \n  bind_cols(outlier_tbl) |&gt; \n  ggplot(aes(x = gender, y = age, color = gender, shape = color)) +\n  geom_beeswarm(size = 1) +\n  theme_minimal() +\n  scale_color_okabeito(order = c(2, 7), guide = \"none\") +\n  scale_shape_manual(name = \"Ausreißer\", values = c(19, 1, 3, 4)) +\n  labs(x = \"Geschlecht\", y = \"Alter in [Jahren]\") \n\ngummi_tbl |&gt; \n  bind_cols(outlier_tbl) |&gt; \n  filter(color == \"normal\") |&gt; \n  ggplot(aes(x = gender, y = age, color = gender, shape = color)) +\n  geom_beeswarm(size = 1) +\n  theme_minimal() +\n  scale_color_okabeito(order = c(2, 7), guide = \"none\") +\n  scale_shape_manual(name = \"Ausreißer\", values = c(1)) +\n  labs(x = \"Geschlecht\", y = \"Alter in [Jahren]\") +\n  ylim(10, 60)\n\n\n\n\n\n\n\n\n\n\n\n(a) Orginaldaten mit markierten Ausreißern.\n\n\n\n\n\n\n\n\n\n\n\n(b) Bereinigte Daten ohne Ausreißer\n\n\n\n\n\n\n\nAbbildung 41.12— Die Gummibärchendaten einmal mit markierten Ausreißern und einmal gefiltert nach normal in der Saplte color. Das Filtern über die Spalte color ist natürlich sehr unintuitiv. Wir sehen, dass wir eine Menge an Beobachtungen aus den Daten entfernen.\n\n\n\n\nWie du siehst ist eine Detektion von Ausreißern nicht so einfach. Zum einen brauchen wir dazu Daten, damit wir auch Ausreißer finden können. Zu irgendwas müssen wir ja die einzelnen Beobachtungen vergleichen. Zum anderen können wir auch extrem viele Beobachtungen entfernen und haben uns dann eine Normalverteilung zusammengeschnitten. Es bleibt ein Drahtseilakt.\n\n\n\n\n\n\nAnwendungsbeispiel: Ausreißer in einer simplen Regression\n\n\n\nWie auch in den anderen Kapiteln gibt es hier nochmal ein Anwendungsbeispiel für die Messung vom Keimfähigkeit durch ein Standardverfahren standard sowie ein Färbeverfahren colorink der Samen. Durch das Einfärben soll die Keimfähigkeit sehr viel früher erkannt werden, als es mit den konventionellen Verfahren möglich ist. Hier geht es jetzt aber erstmal darum, ob wir technische Gleichheit vorliegen haben. Damit wir aber überhaupt schauen können, ob die beiden Verfahren das Gleiche produzieren müssen wir sicher sein, dass wir keine auffälligen Werte in den Daten haben. Es kann ja sein, dass bei den biologischen Proben der eine oder andere Samen defekt ist oder aber andere Probleme wie Parasitenbefall vorlagen und dadurch die Keimfähigkeit verändert ist.\n\nstandard_colorink_tbl &lt;- read_excel(\"data/standard_colorink.xlsx\")\n\nWir rechnen jetzt einmal ein lineare Regression mit der Funktion lm(). Wenn sich die beiden Verfahren nicht unterscheiden würden, dann lägen alle Beobachtungen auf einer Geraden.\n\nfit &lt;- lm(standard ~ colorink, standard_colorink_tbl)\n\nDann schauen wir uns einmal die Daten in der Abbildung 41.13 einmal an. Ich habe die Regression als Gerade einmal durch die Punktewolke gelegt. Wir sehen, dass es hier mindestens drei Beobachtungen gibt, die aussehen als wären sie potenzielle Ausreißer. Die Gerade folgt auch nicht den Punkten und die Abbildung sieht also nicht sehr schlüssig aus. Daher wollen wir einmal schauen, ob wir Ausreißer hier finden können.\n\nstandard_colorink_tbl |&gt; \n  ggplot(aes(colorink, standard)) +\n  theme_minimal() +\n  ylim(50, 100) + xlim(50, 100) +\n  geom_point() +\n  geom_line(aes(y = predict(fit)), color = \"red\")\n\n\n\n\n\n\n\nAbbildung 41.13— Scatterplot der Standardbehandlung zu der Färbebehandung mit drei potenziellen Ausreißern und einer Regressiongeraden. Es ist klar zu erkennen, dass die potenziellen Ausreißer die Gerade stark beeinflussen und somit die Gerade nicht die Punktewolke repräsentiert.\n\n\n\n\n\nHier nochmal kurz das Bestimmtheitsmaß \\(R^2\\) als Maßzahl wie gut die Punkte auf der Geraden liegen. Wie du sehen kannst ist der Wert sehr schlecht. Wir würden bei einer perfekten Übereinstimmung faktisch ein Bestimmtheitsmaß \\(R^2\\) von 1 erwarten. Bei einem Bestimmtheitsmaß \\(R^2\\) von 1 würden die Punkte alle auf einer geraden liegen. Es gebe keine Abweichung von den Punkten zu der Geraden.\n\nfit |&gt; r2()\n\n# R2 for Linear Regression\n       R2: 0.073\n  adj. R2: 0.015\n\n\nMit der Funktion augment() aus dem R Paket {broom} können wir uns einfach die Hebelwirkung (eng. leverage) sowie den cook’schen Abstand wiedergeben lassen. Damit haben wir schon zwei sehr gute Maßzahlen um herauszufinden, ob wir Ausreißer in den Daten vorliegen haben. Damit ist dann nämlich \\(k\\) gleich 1.\n\nfit |&gt; augment()\n\n# A tibble: 18 × 8\n   standard colorink .fitted .resid   .hat .sigma  .cooksd .std.resid\n      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1       85       84    81.7   3.33 0.0587  11.9  0.00276      0.297 \n 2       87       86    82.2   4.77 0.0654  11.9  0.00638      0.427 \n 3       92       95    84.8   7.23 0.143   11.8  0.0381       0.676 \n 4       76       78    80.0  -3.97 0.0612  11.9  0.00410     -0.355 \n 5       85       83    81.4   3.62 0.0567  11.9  0.00312      0.322 \n 6       98       96    85.1  12.9  0.156   11.4  0.138        1.22  \n 7       88       86    82.2   5.77 0.0654  11.8  0.00934      0.517 \n 8       89       62    75.5  13.5  0.235   11.2  0.276        1.34  \n 9       76       74    78.8  -2.84 0.0819  11.9  0.00294     -0.257 \n10       82       89    83.1  -1.08 0.0827  11.9  0.000427    -0.0973\n11       92       95    84.8   7.23 0.143   11.8  0.0381       0.676 \n12       55       93    84.2 -29.2  0.119    8.82 0.490       -2.69  \n13       93       64    76.0  17.0  0.200   10.9  0.338        1.64  \n14       86       91    83.6   2.36 0.0989  11.9  0.00254      0.215 \n15       70       75    79.1  -9.13 0.0753  11.7  0.0275      -0.821 \n16       72       80    80.5  -8.54 0.0565  11.7  0.0173      -0.761 \n17       68       70    77.7  -9.71 0.118   11.6  0.0535      -0.895 \n18       63       65    76.3 -13.3  0.184   11.3  0.183       -1.27  \n\n\nHier nochmal von oben kopiert die Regeln für die Hebenwirkung und den cook’schen Abstand. Wir rechnen hier bei beiden Grenzwerten die gleiche Zahl aus, da wir nur eine \\(x\\)-Variable mit colorink vorliegen haben. P\n\nLeverage .hat\n\nIm Allgemeinen sollte ein Punkt mit einer Hebelwirkung von mehr als \\((2k+2)/n\\) sorgfältig geprüft werden, wobei \\(k\\) die Anzahl der Prädiktorvariablen und n die Anzahl der Beobachtungen ist. Unsere Grenze wäre damit bei \\((2 \\cdot 1 + 2)/18 = 0.22\\)\n\nCook’s Abstand .cooksd\n\nHardin und Hilbe (2007) bezeichnen Werte, die über \\(4/n\\) liegen als problematisch. \\(n\\) ist hierbei die Stichprobengröße. Unsere Grenze wäre damit bei \\(4/18 = 0.22\\)\n\n\nDann können wir auch schon einen Filter setzen und entscheiden hier einmal nach dem cook’schen Abstand. Wenn eine Beobachtung einen Wert größer als \\(0.22\\) in der Spalte .cooksd hat, dann setzen wir diese Beobachtung als Ausreißer. Prinzipiell kannst du natürlich auch mit den anderen vorgestellten Methoden rechnen, aber hier zeige ich einmal den cook’schen Abstand.\n\nclean_tbl &lt;- fit |&gt; \n  augment() |&gt; \n  mutate(outlier = ifelse(.cooksd &gt; 0.22, \"ja\", \"nein\"))\n\nIn der Abbildung 41.14 siehst du einmal das Ergebnis der Ausreißerbestimmung. Wir finden bei der Anzahl an Beobachtungen dann auch unsere drei auffälligen Werte als Ausreißer wieder. Wenn wir die Ausreißer entfernen, dann erhalten wir eine sehr gute Gerade, die auch gut durch die Punkte läuft. Wir würden dann die Beobachtungen aus der Exceldatei entfernen und dann nur noch mit den gereinigten Daten weitermachen. Aber Achtung, bitte nicht die Orginaldaten löschen, eventuell musst du ja später nochmal die Abbildungen mit den Ausreißern erstellen. Ich empfehle da immer einen neuen Tab in der Exceldatei anzulegen.\n\nclean_tbl |&gt; \n  ggplot(aes(colorink, standard, shape = outlier)) +\n  theme_minimal() +\n  ylim(50, 100) + xlim(50, 100) +\n  geom_point() +\n  geom_smooth(data = filter(clean_tbl, outlier != \"ja\"), method = \"lm\", color = \"red\", se = FALSE) +\n  scale_shape_manual(name = \"Ausreißer\", values = c(1, 19)) \n\n\n\n\n\n\n\nAbbildung 41.14— Scatterplot der Standardbehandlung zu der Färbebehandung mit drei potenziellen Ausreißern und einer Regressiongeraden. Die Ausreißer nach dem cook’schen Abstand sind durch \\(\\circ\\)-Werte dargestellt. Die normalen Beobachtungen sind durch \\(\\bullet\\)-Werte abgebildet.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Ausreißer</span>"
    ]
  },
  {
    "objectID": "stat-modeling-outlier.html#referenzen",
    "href": "stat-modeling-outlier.html#referenzen",
    "title": "41  Ausreißer",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 41.1 (a)— Originale Daten. Aureißer liegen entlang der Geraden.\nAbbildung 41.1 (b)— \\(log\\) transformierte Daten. Die Bobachtungen folgen mehr der Geraden.\nAbbildung 41.2 (a)— Alter nach Geschlecht\nAbbildung 41.2 (b)— Körpergröße nach Geschlecht\nAbbildung 41.3 (a)— Scatterplot ohne Ausreißer\nAbbildung 41.3 (b)— Scatterplot mit Ausreißer\nAbbildung 41.4— Visualisierung der Ausreißer nach der Cook’schen Distanz. Die Grenze als rote Linie ist mit \\(4/n = 0.4\\) berechnet worden.\nAbbildung 41.5— Visualisierung der Ausreißer nach der leverage. Die Grenze als rote Linie ist mit 0.4 berechnet worden.\nAbbildung 41.6— Visualisierung der Sigmaregel auf den Residuen der Geraden.\nAbbildung 41.7— Darstellung der Sigmaregel mir vorher deklarierten Ausreißern. Die Sigmaintervalle wurden auf den \\(\\circ\\)-Werten berechnet und die zwei potenziellen Ausreißer als \\(\\bullet\\)-Werte vorab definiert und somit aus der Berechnung herausgenommen.\nAbbildung 41.8— Darstellung des Hampelfilters mir vorher deklarierten Ausreißern. Der Hampelfilter wurde auf allen Werten berechnet und gibt die beiden Ausreißer (\\(\\bullet\\)-Wert) wieder.\nAbbildung 41.9 (a)— Alter nach Geschlecht\nAbbildung 41.9 (b)— Körpergröße nach Geschlecht\nAbbildung 41.10— Die Ausreißer und leverage Diagnoseabbildung für unsere Sprungweitendaten. Der Ausreißer an der ersten Position wird erkannt, der andere Ausreißer hingegen nicht.\nAbbildung 41.11— Die Ausreißer und leverage Diagnoseabbildung für unsere Gummibärchendaten. Hier werden reichlich Ausreißer in unseren Daten erkannt.\nAbbildung 41.12 (a)— Orginaldaten mit markierten Ausreißern.\nAbbildung 41.12 (b)— Bereinigte Daten ohne Ausreißer\nAbbildung 41.13— Scatterplot der Standardbehandlung zu der Färbebehandung mit drei potenziellen Ausreißern und einer Regressiongeraden. Es ist klar zu erkennen, dass die potenziellen Ausreißer die Gerade stark beeinflussen und somit die Gerade nicht die Punktewolke repräsentiert.\nAbbildung 41.14— Scatterplot der Standardbehandlung zu der Färbebehandung mit drei potenziellen Ausreißern und einer Regressiongeraden. Die Ausreißer nach dem cook’schen Abstand sind durch \\(\\circ\\)-Werte dargestellt. Die normalen Beobachtungen sind durch \\(\\bullet\\)-Werte abgebildet.\n\n\n\nBoukerche A, Zheng L, Alfandi O. 2020. Outlier detection: Methods, models, and classification. ACM Computing Surveys (CSUR) 53: 1–37.\n\n\nCook CN, Freeman AR, Liao JC, Mangiamele LA. 2021. The philosophy of outliers: reintegrating rare events into biological science. Integrative and Comparative Biology 61: 2191–2198.\n\n\nHardin JW, Hilbe JM. 2007. Generalized linear models and extensions. Stata press.\n\n\nLyons L. 2013. Discovering the Significance of 5 sigma. arXiv preprint arXiv:1310.1284.\n\n\nMichel MC, Murphy T, Motulsky HJ. 2020. New author guidelines for displaying data and reporting data analysis and statistical methods in experimental biology. Journal of Pharmacology and Experimental Therapeutics 372: 136–147.\n\n\nSejr JH, Schneider-Kamp A. 2021. Explainable outlier detection: What, for Whom and Why? Machine Learning with Applications 6: 100172.\n\n\nZimek A, Filzmoser P. 2018. There and back again: Outlier detection between statistical reasoning and data mining algorithms. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 8: e1280.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Ausreißer</span>"
    ]
  },
  {
    "objectID": "stat-modeling-variable-selection.html",
    "href": "stat-modeling-variable-selection.html",
    "title": "42  Variablenselektion",
    "section": "",
    "text": "42.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, dlookr, \n               MASS, ranger, Boruta, broom, \n               scales, olsrr, gtsummary, parameters,\n               conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Variablenselektion</span>"
    ]
  },
  {
    "objectID": "stat-modeling-variable-selection.html#daten",
    "href": "stat-modeling-variable-selection.html#daten",
    "title": "42  Variablenselektion",
    "section": "42.2 Daten",
    "text": "42.2 Daten\nUm die Variablenselektion einmal durchzuführen nurtzen wir zwei Datensätze. Zum einen den Datensatz zu den Kichererbsen in Brandenburg mit einem normalverteilten Outcome \\(y\\) mit dryweight. Wir laden wieder den Datensatz in R und schauen uns einmal die Daten in Tabelle 42.1 als Auszug aus dem Tabellenblatt an.\nWir du schon siehst, wir brauchen Fallzahl um hier überhaupt was zu machen. Bitte keine Variablenselektion im niedrigen zweistelligen Bereich an Beobachtungen.\n\nchickpea_tbl &lt;- read_excel(\"data/chickpeas.xlsx\") \n\nWir sehen, dass wir sehr viele Variablen vorleigen haben. Sind denn jetzt alle Variablen notwendig? Oder können auch ein paar Variablen raus aus dem Modell. So viele Beobachtungen haben wir mit \\(n = 95\\) ja nicht vorliegen. Daher wollen wir an diesem Datensatz die Variablenselektion unter der Annahme eines normalverteilten \\(y\\) durchgehen.\n\n\n\n\nTabelle 42.1— Auszug aus dem Daten zu den Kichererbsen in Brandenburg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntemp\nrained\nlocation\nno3\nfe\nsand\nforest\ndryweight\n\n\n\n\n25.26\nhigh\nnorth\n5.56\n4.43\n63\n&gt;1000m\n253.42\n\n\n21.4\nhigh\nnortheast\n9.15\n2.58\n51.17\n&lt;1000m\n213.88\n\n\n27.84\nhigh\nnortheast\n5.57\n2.19\n55.57\n&gt;1000m\n230.71\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n29.04\nlow\nnorth\n5.64\n2.87\n53.27\n&gt;1000m\n236.07\n\n\n24.11\nhigh\nnortheast\n4.31\n3.66\n63\n&lt;1000m\n259.82\n\n\n28.88\nlow\nnortheast\n7.92\n2\n65.75\n&gt;1000m\n274.75\n\n\n\n\n\n\n\n\nWas wir auch noch wissen, ist wie die Effekte in den Daten wirklich sind. Die Daten wurden ja künstlich erstellt, deshalb hier die Ordnung der Effektstärke für jede Variable. Im Prinzip müsste diese Reihenfolge auch bei der Variablenselektion rauskommen. Schauen wir mal, was wir erhalten.\n\\[\ny \\sim \\beta_0 + 3 * sand + 2 * temp + 1.5 * rained - 1.2 * forest + 1.1 * no3  \n\\]\nViele Beispiele laufen immer unter der Annahme der Normalverteilung. Deshalb als zweites Beispiel nochmal die Daten von den infizierten Ferkeln mit einem binomialverteilten Outcome \\(y\\) mit infected. Auch hier können wir uns den Auszug der Daten in Tabelle 42.2 anschauen.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") \n\nDas schöne an diesem Datensatz ist jetzt, dass wir mit \\(n = 412\\) Beobachtungen sehr viele Daten vorliegen haben. Daher können wir auch alle Methoden gut verwenden und haben nicht das Problem einer zu geringen Fallzahl.\n\n\n\n\nTabelle 42.2— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\ninfected\n\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\n1\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\n1\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\n1\n\n\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\n0\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\n1\n\n\n\n\n\n\n\n\nAuch in diesem Beispiel wurden die Daten von mir mit folgenden Effekten generiert. Schauen wir mal, was die Variablenselektion bei der hohen Fallzahl mit den Variablen macht bzw. welche Sortierung am Ende rauskommt.\n\\[\ny \\sim \\beta_0 + 2 * crp + 0.5 * sex + 0.5 * frailty + 0.2 * bloodpressure + 0.05 * creatinin +  0.01 * weight\n\\]\nDamit haben wir unsere beiden Beispiel und jetzt gehen wir mal eine Auswahl an Methoden zur Variablenselektion durch. Besonders hier, haltet den statistsichen Engel nah bei euch. Es wird leider etwas ruppig für den statistischen Engel.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Variablenselektion</span>"
    ]
  },
  {
    "objectID": "stat-modeling-variable-selection.html#methoden-der-variablenselektion",
    "href": "stat-modeling-variable-selection.html#methoden-der-variablenselektion",
    "title": "42  Variablenselektion",
    "section": "42.3 Methoden der Variablenselektion",
    "text": "42.3 Methoden der Variablenselektion\nIn den folgenden Abschnitten wollen wir uns eine Reihe an Methoden anschauen um eine Variablenselektion durchzuführen. Dabei gehen wir von einfach nach komplex. Wobei das komplex eher die Methode und nicht die Anwendung meint. Wir nutzen R Pakete und gehen nicht sehr ins Detail wie der Algorithmus jetzt die Auswahl trifft. Für den Hintergrund sind dann die Verweise auf die anderen Kapitel.\n\n42.3.1 Per Hand\nManchmal ist der Anfang auch das Ende. Wir müssen ja gar keinen Algorithmus auf unsere Daten loslassen um eine Variablenselektion durchzuführen. Deshalb möchte ich gleich den ersten Abschnitt mit einem Zitat von Heinze und Dunkler (2017) beginnen.\n“Oft gibt es keinen wissenschaftlichen Grund, eine (algorithmische) Variablenauswahl durchzuführen. Insbesondere erfordern Methoden der (algorithmische) Variablenselektion einen viel größeren Stichprobenumfang als die Schätzung eines multiplen Modells mit einem festen Satz von Prädiktoren auf der Grundlage (klinischer) Erfahrung.” (Übersetzt und ergänzt nach Heinze und Dunkler 2017, p. 9)\nFazit dieses kurzen Abschnitts. Wir können auf alles Folgende einfach verzichten und uns überlegen welche Variablen sinnvollerweise mit ins Modell sollen und das mit unserem Expertenwissen begründen. Gut, und was ist, wenn ich kein Experte bin? Oder wir aber wirklich Neuland betreten? Dann können wir eine Reihe anderer Verfahren nutzen um uns algortimisch einer Wahrheit anzunähern.\n\n\n42.3.2 Univariate Vorselektion\nUnd weiter geht es mit Zitaten aus Heinze und Dunkler (2017) zu der Variablenselektion. Dazu musst du wissen, dass die univariate Vorselektion sehr beliebt war und auch noch ist. Denn die univariate Vorselektion ist einfach durchzuführen und eben auch gut darzustellen.\n“Obwohl die univariable Vorfilterung nachvollziehbar und mit Standardsoftware leicht durchführbar ist, sollte man sie besser ganz vergessen, da sie für die Erstellung multivariabler Modelle weder Voraussetzung noch von Nutzen ist.” (Übersetzt nach Heinze und Dunkler 2017, p. 8)\nIch sage immer, auch mit einem Hammer kann man Scheiben putzen. Halt nur einmal… Deshalb auch hier die univariate Variante der Vorselektion.\nWir sehen also, eigentlich ist die univariate Variablensleketion nicht so das gelbe vom Ei, aber vielleicht musst die Variablenselektion durchführen, so dass her die Lösung in R einmal dargestellt ist. Wir nutzen einmal die gaussian lineare Regression für den Kichererbsendatensatz. Es ist eine ganze Reihe an Code, das hat aber eher damit zu tun, dass wir die Modellausgabe noch filtern und anpassen wollen. Die eigentliche Idee ist simple. Wir nehmen unseren Datensatz und pipen den Datensatz in select und entfernen unser Outcome drymatter. Nun iterieren wir für jede Variable .x im Datensatz mit der Funktion map() und rechnen in jeder Iteration eine gaussian lineare Regression. Dann entferne wir noch den Intercept und sortieren nach den \\(p\\)-Werten.\n\nchickpea_tbl |&gt;\n  select(-dryweight) |&gt;                   \n  map(~glm(dryweight ~ .x, data = chickpea_tbl, family = gaussian)) |&gt;    \n  map(tidy) |&gt;                          \n  map(filter, term != \"(Intercept)\") |&gt;       \n  map(select, -term, -std.error, -statistic) |&gt;                        \n  bind_rows(.id=\"term\") |&gt; \n  arrange(p.value) |&gt; \n  mutate(p.value = pvalue(p.value),\n         estimate = round(estimate, 2))\n\n# A tibble: 8 × 3\n  term     estimate p.value\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  \n1 sand         2.9  &lt;0.001 \n2 temp         1.65 0.011  \n3 no3          2.51 0.037  \n4 fe           2.61 0.091  \n5 location    -7.27 0.130  \n6 rained       4.95 0.165  \n7 forest      -4.39 0.197  \n8 location    -2.87 0.434  \n\n\nWürden wir nur nach dem Signifikanzniveau von 5% gehen, dann hätten wir die Variablen sand und location selektiert. Bei der selektion mit dem \\(p\\)-Wert wird aber eher eine Schwelle von 15.7% vorgeschlagen (Heinze und Dunkler 2017, p. 9). Daher würden wir auch noch no3 und temp mit Selektieren und in unser Modell nehmen.\nEs gibt ja immer zwei Wege nach Rom. Deshalb hier auch nochmal die Funktion tbl_uvregression() aus dem R Paket {gtsummary}, die es erlaubt die univariaten Regressionen über alle Variablen laufen zu lassen. Wir kriegen dann auch eine schöne Tabelle 42.3 wieder.\n\n\nDas R Paket {gtsummary} erlaubt es Ergebnisse der Regression in dem Tutorial: tbl_regression gut darzustellen.\n\nchickpea_tbl |&gt;\n  tbl_uvregression(\n    method = glm,\n    y = dryweight,\n    method.args = list(family = gaussian),\n    pvalue_fun = ~style_pvalue(.x, digits = 2)\n  ) |&gt;\n  add_global_p() |&gt;  # add global p-value \n  add_q() |&gt;         # adjusts global p-values for multiple testing\n  bold_p() |&gt;        # bold p-values under a given threshold (default 0.05)\n  bold_p(t = 0.10, q = TRUE) |&gt; # now bold q-values under the threshold of 0.10\n  bold_labels()\n\n\n\nTabelle 42.3— Univariate Regression mit der Funktion tbl_uvregression().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nBeta\n95% CI1\np-value\nq-value2\n\n\n\n\ntemp\n95\n1.6\n0.41, 2.9\n0.009\n0.032\n\n\nrained\n95\n\n\n\n\n0.16\n0.23\n\n\n    high\n\n\n—\n—\n\n\n\n\n\n\n    low\n\n\n4.9\n-2.0, 12\n\n\n\n\n\n\nlocation\n95\n\n\n\n\n0.31\n0.31\n\n\n    north\n\n\n—\n—\n\n\n\n\n\n\n    northeast\n\n\n-2.9\n-10, 4.3\n\n\n\n\n\n\n    west\n\n\n-7.3\n-17, 2.1\n\n\n\n\n\n\nno3\n95\n2.5\n0.18, 4.8\n0.035\n0.081\n\n\nfe\n95\n2.6\n-0.38, 5.6\n0.087\n0.15\n\n\nsand\n95\n2.9\n2.5, 3.3\n&lt;0.001\n&lt;0.001\n\n\nforest\n95\n\n\n\n\n0.19\n0.23\n\n\n    &lt;1000m\n\n\n—\n—\n\n\n\n\n\n\n    &gt;1000m\n\n\n-4.4\n-11, 2.2\n\n\n\n\n\n\n\n1 CI = Confidence Interval\n\n\n2 False discovery rate correction for multiple testing\n\n\n\n\n\n\n\n\n\n\n\n\nNun führen wir die univariate Regression erneut auf den Ferkeldaten aus. Hier ändern wir nur die family = binomial, da wir hier jetzt eine logistische lineare Regression rechnen müssen. Unser Outcome infected ist ja \\(0/1\\) codiert. Sonst ändert sich der Code nicht.\n\npig_tbl |&gt;\n  select(-infected) |&gt;                   \n  map(~glm(infected ~ .x, data = pig_tbl, family = binomial)) |&gt;    \n  map(tidy) |&gt;                          \n  map(filter, term != \"(Intercept)\") |&gt;       \n  map(select, -term, -std.error, -statistic) |&gt;                        \n  bind_rows(.id=\"term\") |&gt; \n  arrange(p.value) |&gt; \n  mutate(p.value = pvalue(p.value),\n         estimate = round(estimate, 2))\n\n# A tibble: 12 × 3\n   term          estimate p.value\n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  \n 1 crp               0.96 &lt;0.001 \n 2 bloodpressure     0.08 0.013  \n 3 creatinin         0.11 0.133  \n 4 location         -0.4  0.141  \n 5 sex              -0.29 0.188  \n 6 location         -0.24 0.435  \n 7 frailty           0.16 0.645  \n 8 activity          0.02 0.698  \n 9 frailty          -0.12 0.699  \n10 location          0.12 0.712  \n11 weight           -0.02 0.765  \n12 age               0    0.877  \n\n\nIn diesem Fall reicht die Schwelle von 15.7% nur für zwei Variablen (Heinze und Dunkler 2017, p. 9). Wir erhalten die Variablen crp und bloodpressure für das Modell selektiert.\nIn der Tabelle 42.4 sehen wir dann nochmal die Anwendung der Funktion tbl_uvregression() auf den Ferkeldatensatz. Ich musste hier die Option pvalue_fun = ~style_pvalue(.x, digits = 2) entfernen, da sonst die Variable crp keinen \\(p\\)-Wert erhält. Leider sehe ich den \\(p\\)-Wert mit \\(&lt;0.001\\) in meiner Ausgabe in R aber wie du siehst, wird die Tabelle auf der Webseite nicht korrekt angezeigt. Das Problem von automatischen Tabellen. Ein Fluch und Segen zugleich. Du musst immer wieder überprüfen, ob die Optionen dann auch für sich und deine Analyse passen.\n\npig_tbl |&gt;\n  tbl_uvregression(\n    method = glm,\n    y = infected,\n    method.args = list(family = binomial),\n    exponentiate = TRUE\n  ) |&gt;\n  add_global_p() |&gt;  # add global p-value \n  add_nevent() |&gt;    # add number of events of the outcome\n  add_q() |&gt;         # adjusts global p-values for multiple testing\n  bold_p() |&gt;        # bold p-values under a given threshold (default 0.05)\n  bold_p(t = 0.10, q = TRUE) |&gt; # now bold q-values under the threshold of 0.10\n  bold_labels()\n\n\n\nTabelle 42.4— Univariate Regression mit der Funktion tbl_uvregression().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN\nEvent N\nOR1\n95% CI1\np-value\nq-value2\n\n\n\n\nage\n412\n276\n1.00\n0.95, 1.04\n0.9\n0.9\n\n\nsex\n412\n276\n\n\n\n\n0.2\n0.4\n\n\n    female\n\n\n\n\n—\n—\n\n\n\n\n\n\n    male\n\n\n\n\n0.75\n0.49, 1.15\n\n\n\n\n\n\nlocation\n412\n276\n\n\n\n\n0.3\n0.5\n\n\n    north\n\n\n\n\n—\n—\n\n\n\n\n\n\n    northeast\n\n\n\n\n1.12\n0.61, 2.10\n\n\n\n\n\n\n    northwest\n\n\n\n\n0.67\n0.39, 1.14\n\n\n\n\n\n\n    west\n\n\n\n\n0.79\n0.43, 1.44\n\n\n\n\n\n\nactivity\n412\n276\n1.02\n0.90, 1.16\n0.7\n0.9\n\n\ncrp\n412\n276\n2.62\n2.14, 3.27\n&lt;0.001\n&lt;0.001\n\n\nfrailty\n412\n276\n\n\n\n\n0.5\n0.7\n\n\n    frail\n\n\n\n\n—\n—\n\n\n\n\n\n\n    pre-frail\n\n\n\n\n1.17\n0.59, 2.27\n\n\n\n\n\n\n    robust\n\n\n\n\n0.88\n0.46, 1.64\n\n\n\n\n\n\nbloodpressure\n412\n276\n1.08\n1.02, 1.15\n0.012\n0.053\n\n\nweight\n412\n276\n0.98\n0.86, 1.12\n0.8\n0.9\n\n\ncreatinin\n412\n276\n1.12\n0.97, 1.30\n0.13\n0.4\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n2 False discovery rate correction for multiple testing\n\n\n\n\n\n\n\n\n\n\n\n\nNeben der Berechnung von univariaten logistischen Regressionen ist auch die Darstellung der Daten in einer Tabelle 42.5 bei Medizinern sehr beliebt. Deshalb an dieser Stelle auch die Tabelle 1 (eng. table 1) für die Zusammenfasung der Daten getrennt nach dem Infektionsstatus zusammen mit dem \\(p\\)-Wert. Ich nutze hier die Funktion tbl_summary() aus dem R Paket {gtsummary}.\n\npig_tbl |&gt; tbl_summary(by = infected) |&gt; add_p()\n\n\n\nTabelle 42.5— Zusammenfasung der Daten getrennt nach dem Infektionsstatus zusammen mit dem \\(p\\)-Wert.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n0, N = 1361\n1, N = 2761\np-value2\n\n\n\n\nage\n59.5 (57.0, 63.0)\n60.0 (57.0, 63.0)\n0.9\n\n\nsex\n\n\n\n\n0.2\n\n\n    female\n47 (35%)\n114 (41%)\n\n\n\n\n    male\n89 (65%)\n162 (59%)\n\n\n\n\nlocation\n\n\n\n\n0.3\n\n\n    north\n36 (26%)\n85 (31%)\n\n\n\n\n    northeast\n23 (17%)\n61 (22%)\n\n\n\n\n    northwest\n48 (35%)\n76 (28%)\n\n\n\n\n    west\n29 (21%)\n54 (20%)\n\n\n\n\nactivity\n13.40 (12.25, 14.34)\n13.24 (12.28, 14.54)\n0.8\n\n\ncrp\n19.12 (18.13, 19.83)\n20.57 (19.77, 21.46)\n&lt;0.001\n\n\nfrailty\n\n\n\n\n0.5\n\n\n    frail\n18 (13%)\n37 (13%)\n\n\n\n\n    pre-frail\n42 (31%)\n101 (37%)\n\n\n\n\n    robust\n76 (56%)\n138 (50%)\n\n\n\n\nbloodpressure\n56.2 (54.3, 58.5)\n57.2 (55.1, 59.6)\n0.021\n\n\nweight\n18.61 (17.34, 19.41)\n18.32 (17.19, 19.60)\n0.8\n\n\ncreatinin\n4.85 (3.67, 5.93)\n4.86 (4.06, 5.85)\n0.3\n\n\n\n1 Median (IQR); n (%)\n\n\n2 Wilcoxon rank sum test; Pearson’s Chi-squared test\n\n\n\n\n\n\n\n\n\n\n\n\nTja, auch hier ist dann die Frage, wie sortiere ich Variablen. Da es sich bei dem table 1-Stil um eine Übersichtstabelle handelt, ist die Tabelle nach den Variablen sortiert. Auch hier finden wir dann die Variablen crp und bloodpressure wieder. Das Problem hierbei ist natürlich, dass sich die \\(p\\)-Werte unterscheiden. Das muss ja auch so sein, denn eine logitische Regression ist nun mal kein Wilcoxon rank sum test oder ein Pearson’s Chi-squared test.\nFassen wir als Fazit dieses Abschnitts zusammen wie unsere Modelle nach der Variablenslektion aussehen würde. In unserem Beispiel für die Kichererbsen im sandigen Brandenburg würden wir dann folgendes Modell nehmen.\n\\[\ny \\sim \\beta_0 + sand + location + no3 + temp\n\\]\nUnsere infizierten Ferkel würden dann folgendes selektiertes Modell erhalten.\n\\[\ny \\sim \\beta_0 + crp + bloodpressure\n\\]\nSchauen wir mal, was die anderen Algorithmen noch so finden.\n\n\n42.3.3 Sonderfall Gaussian linear Regression\nFür die gaussian lineare Regression gibt es mit dem R Paket {oslrr} eine große Auswahl an Variable Selection Methods. Einfach mal die Möglichkeiten anschauen, die dort angeboten werden. Wir nutzen jetzt nicht alles was din oslrr möglich ist, sondern nur eien Auswahl. Zuerst müssen wir wieder unser Modell fitten. Wir nehmen alle Variablen mit rein und nutzen die Funktion lm() für ein lineares Modell mit einem normalverteilten Outcome \\(y\\) mit dryweight.\n\nchickenpea_fit &lt;- lm(dryweight ~ temp + rained + location + no3 + fe + sand + forest, \n                     data = chickpea_tbl)\n\nNun gibt es wirklich viele Lösungen in dem R Paket {oslrr}. Ich möchte einmal die Variante mit ols_step_all_possible präsentieren. In dem Fall rechnen wir alle Modelle die gehen. Und damit meine ich wirklich alle Modelle. Deshalb filtern wir noch nach dem \\(R^2_{adj}\\) um nicht von dem Angebot erschlagen zu werden. Darüber hinaus möchte ich nur Modelle sehen, die maximal vier Variablen mit in dem Modell haben. Das ist zufällig von mir gewählt… ich will ja ein kleineres Modell haben.\n\nols_step_all_possible(chickenpea_fit)$result |&gt;\n  as_tibble() |&gt;\n  arrange(desc(adjr)) |&gt;\n  filter(n &lt;= 4) |&gt; \n  select(predictors, adjr, aic) \n\n# A tibble: 98 × 3\n   predictors                 adjr   aic\n   &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt;\n 1 temp no3 sand forest      0.893  587.\n 2 temp rained no3 sand      0.886  592.\n 3 temp no3 fe sand          0.886  593.\n 4 temp no3 sand             0.884  593.\n 5 temp location no3 sand    0.882  596.\n 6 temp rained sand forest   0.881  597.\n 7 temp sand forest          0.877  599.\n 8 temp fe sand forest       0.876  600.\n 9 temp location sand forest 0.875  602.\n10 temp rained fe sand       0.872  603.\n# ℹ 88 more rows\n\n\nEine Alternative ist die Funktion ols_step_backward_aic(), die es erlaubt die Selektion anhand dem \\(AIC\\)-Wert zu machen. Der \\(AIC\\)-Wert beschreibt die Güte eines Modells und je kleiner der \\(AIC\\)-Wert ist, desto besser ist das Modell im Vergleich zu anderen Modellen gleicher Art. Da der \\(AIC\\)-Wert von den Daten abhängt in denen der \\(AIC\\)-Wert geschätzt wurde, können verschiedene \\(AIC\\)-Werte nicht übergreifend vergleichen werden.\n\nchick_step_aic &lt;- ols_step_backward_aic(chickenpea_fit)\n\nIn Abbildung 42.1 sehen wir einmal den Verlauf der \\(AIC\\)-Wert durch die Entfernung der jeweiligen Variable. Wenn du auf die y-Achse schaust, ist der Effekt numerisch nicht sehr groß. Davon darf man sich aber nicht beeindrucken lassen., Wir erhalten ein besseres Modell, wenn wir Variablen entfernen. Darüber hinaus sehen wir auch eine Sättigung.\n\nplot(chick_step_aic) \n\n\n\n\n\n\n\nAbbildung 42.1— Visualisierung der ols_step_backward_aic mit der Reduktion des AIC-Wertes.\n\n\n\n\n\nGut, und wie sieht nun unser finales Modell aus? Dafür müssen wir usn aus dem Objekt chick_step_aic das model raus ziehen. Dafür nutzen wir die Funktion pluck(). Dann noch die Ausgabe in die Funktion model_parameters() gepipt und schon haben wir das finale Modell nach \\(AIC\\)-Werten in einer gaussian linearen Regression.\n\npluck(chick_step_aic, \"model\") |&gt; \n  model_parameters()\n\nParameter       | Coefficient |   SE |          95% CI | t(89) |      p\n-----------------------------------------------------------------------\n(Intercept)     |       -2.09 | 9.44 | [-20.85, 16.67] | -0.22 | 0.825 \ntemp            |        2.37 | 0.22 | [  1.94,  2.81] | 10.88 | &lt; .001\nrained [low]    |        1.79 | 1.18 | [ -0.56,  4.15] |  1.51 | 0.133 \nno3             |        1.42 | 0.40 | [  0.62,  2.22] |  3.53 | &lt; .001\nsand            |        3.03 | 0.12 | [  2.80,  3.26] | 26.12 | &lt; .001\nforest [&gt;1000m] |       -3.17 | 1.11 | [ -5.38, -0.95] | -2.84 | 0.006 \n\n\nSpannenderweise ist location nicht mehr im finalen Modell plus die Variable location flog auch sehr früh raus. Das passt auch besser zu den Daten. Ich hatte die Daten so gebaut, dass der Ort eigentlich keinen Effekt haben sollte. Wir sehen, dass je nach Verfahren was anderes herauskommt. Aber Achtung, das schrittweise Verfahren ist der Auswahl nach \\(p\\)-Werten auf jeden Fall vorzuziehen!\n\n\n42.3.4 Schrittweise mit stepAIC\nWas das R Paket {oslrr} für die gaussian linear Regression kann, kann das R Paket {MASS} mit der Funktion stepAIC für den Rest der möglichen Verteilungen. Da wir mit dem Fekerldatensatz ein binominales Outcome \\(y\\) mit infected vorliegen haben nutzen wir un die Funktion stepAIC(). Wir hätten auch den Kichererbsendatensatz mit der Funktion bearbeiten können, aber im Falle der Normalverteilung stehen uns dann eben noch andere Algorithmen zu Verfügung. Wie immer müssen wir zuerst das volle Modell mit der Funktion glm() fitten. Wir geben noch die Verteilungsfamilie mit family = binomial noch mit an und definieren so eine logistische lineare Regression.\n\nfit &lt;- glm(infected ~ age + sex + location + activity + crp + frailty + bloodpressure + weight + creatinin, \n           data = pig_tbl, family = binomial)\n\nNachdem wir das Modell gefittet haben, können wir das Modell direkt in die Funktion stepAIC stecken. Wir nutzen noch die Option direction = \"backward\" um eine Rückwärtsselektion durchzuführen.\n\nfit_step &lt;- stepAIC(fit, direction = \"backward\")\n\nStart:  AIC=421.22\ninfected ~ age + sex + location + activity + crp + frailty + \n    bloodpressure + weight + creatinin\n\n                Df Deviance    AIC\n- location       3   398.49 418.49\n- frailty        2   396.61 418.61\n- weight         1   395.22 419.22\n- age            1   395.24 419.24\n- activity       1   395.47 419.47\n- sex            1   395.93 419.93\n- creatinin      1   396.74 420.74\n&lt;none&gt;               395.22 421.22\n- bloodpressure  1   399.67 423.67\n- crp            1   504.90 528.90\n\nStep:  AIC=418.49\ninfected ~ age + sex + activity + crp + frailty + bloodpressure + \n    weight + creatinin\n\n                Df Deviance    AIC\n- frailty        2   399.68 415.68\n- weight         1   398.50 416.50\n- age            1   398.50 416.50\n- activity       1   398.83 416.83\n- sex            1   399.01 417.01\n- creatinin      1   400.10 418.10\n&lt;none&gt;               398.49 418.49\n- bloodpressure  1   403.24 421.24\n- crp            1   509.50 527.50\n\nStep:  AIC=415.68\ninfected ~ age + sex + activity + crp + bloodpressure + weight + \n    creatinin\n\n                Df Deviance    AIC\n- weight         1   399.69 413.69\n- age            1   399.73 413.73\n- activity       1   400.06 414.06\n- sex            1   400.30 414.30\n- creatinin      1   401.21 415.21\n&lt;none&gt;               399.68 415.68\n- bloodpressure  1   404.17 418.17\n- crp            1   511.29 525.29\n\nStep:  AIC=413.69\ninfected ~ age + sex + activity + crp + bloodpressure + creatinin\n\n                Df Deviance    AIC\n- age            1   399.74 411.74\n- activity       1   400.09 412.09\n- sex            1   400.40 412.40\n- creatinin      1   401.22 413.22\n&lt;none&gt;               399.69 413.69\n- bloodpressure  1   404.18 416.18\n- crp            1   512.26 524.26\n\nStep:  AIC=411.74\ninfected ~ sex + activity + crp + bloodpressure + creatinin\n\n                Df Deviance    AIC\n- activity       1   400.11 410.11\n- sex            1   400.45 410.45\n- creatinin      1   401.40 411.40\n&lt;none&gt;               399.74 411.74\n- bloodpressure  1   404.20 414.20\n- crp            1   512.28 522.28\n\nStep:  AIC=410.11\ninfected ~ sex + crp + bloodpressure + creatinin\n\n                Df Deviance    AIC\n- sex            1   400.47 408.47\n- creatinin      1   401.86 409.86\n&lt;none&gt;               400.11 410.11\n- bloodpressure  1   404.72 412.72\n- crp            1   513.77 521.77\n\nStep:  AIC=408.47\ninfected ~ crp + bloodpressure + creatinin\n\n                Df Deviance    AIC\n- creatinin      1   402.21 408.21\n&lt;none&gt;               400.47 408.47\n- bloodpressure  1   406.37 412.37\n- crp            1   514.11 520.11\n\nStep:  AIC=408.21\ninfected ~ crp + bloodpressure\n\n                Df Deviance    AIC\n&lt;none&gt;               402.21 408.21\n- bloodpressure  1   408.12 412.12\n- crp            1   516.27 520.27\n\n\nJetzt ist die Selektion durchgelaufen und wir sehen in jeden Schritt welche Variable jeweils entfernt wurde und wie sich dann der \\(AIC\\)-Wert ändert. Wir starten mit einem \\(AIC = 425.65\\) und enden bei einem \\(AIC=415.01\\). Schauen wir uns nochmal das finale Modell an.\n\nfit_step |&gt; \n  model_parameters()\n\nParameter     | Log-Odds |   SE |           95% CI |     z |      p\n-------------------------------------------------------------------\n(Intercept)   |   -23.54 | 3.14 | [-29.96, -17.61] | -7.49 | &lt; .001\ncrp           |     0.97 | 0.11 | [  0.76,   1.20] |  8.83 | &lt; .001\nbloodpressure |     0.09 | 0.04 | [  0.02,   0.16] |  2.39 | 0.017 \n\n\nHier erscheint jetzt noch die Variable sex mit in der Auswahl. Das hat natürlich auch weitreichende Auswirkungen! Es macht schon einen gewaltigen Unterschied, ob wir annehmen das, dass Geschelcht der Ferkel keinen Einfluss auf die Infektion hat oder eben doch. Wir sehen auch hier, dass wir Aufpassen müssen wenn wir eine Variablenselektion durchführen. Aber Achtung, das schrittweise Verfahren ist der Auswahl nach \\(p\\)-Werten auf jeden Fall vorzuziehen!\n\n\n42.3.5 Feature Selektion mit ranger\nIn diesem Abschnitt wollen wir die Variablenselektion mit einem maschinellen Lernverfahren durchführen. Im Bereich des maschinellen Lernens heist die Variablenselektion dann aber Feature Selektion. Wir versuchen jetzt die Selektion auf den Orginaldaten durchzuführen. Eigentlich wird empfohlen die Daten vorher zu normalisieren und dann mit den maschinellen Lernverfahren zu nutzen.\n\n\n\n\n\n\nStandardisieren oder Normalisieren von Daten\n\n\n\nEine Herausforderung für maschinelle Lernverfahren sind nicht normalisierte Daten. Das heist, dass wir Variablen haben, die kategorial oder kontinuierlich sein können oder aber sehr unterschiedlich von den Einheiten sind. Deshalb wird empfohlen die Daten vorher zu Standardisieren oder zu Normalisieren. In dem Kapitel 18 kannst du mehr über das Transformieren von Daten nachlesen.\n\n\nWir nutzen als erstes einen Random Forest Algorithmus wie er in Kapitel 70 beschrieben ist. Es bietet sich hier die Implementation im R Paket {ranger} an. Bevor wir aber einen Random Forest auf unsere Daten laufen lassen, Standardisieren wir unsere Daten nochmal. Überall wo wir einen numerischen Wert als Variableneintrag haben rechnen wir eine \\(z\\)-Transformation. Wir erhalten dann die standardisierten Daten zurück.\n\npig_norm_tbl &lt;- pig_tbl |&gt; \n  mutate(across(where(is.character), as_factor),\n         across(where(is.numeric), dlookr::transform, \"zscore\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), dlookr::transform, \"zscore\")`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\npig_norm_tbl\n\n# A tibble: 412 × 10\n   age        sex    location  activity    crp        frailty   bloodpressure\n   &lt;transfrm&gt; &lt;fct&gt;  &lt;fct&gt;     &lt;transfrm&gt;  &lt;transfrm&gt; &lt;fct&gt;     &lt;transfrm&gt;   \n 1  0.2156014 male   northeast  1.23906607  1.6154076 robust     1.6152124   \n 2 -1.5521155 male   northwest -0.15495130 -0.9942285 robust    -0.7895759   \n 3  1.3204244 female northeast -1.18531196 -0.9104969 robust     0.3274677   \n 4 -0.2263278 female north      0.03899895 -0.4848611 robust    -0.2085934   \n 5  0.6575306 male   northwest  0.87540937  1.0502190 robust    -0.4391895   \n 6 -1.1101862 male   northwest  1.54211333  0.9664873 robust     1.0312351   \n 7 -2.4359739 male   west       0.32386336 -0.7569889 pre-frail -0.6488224   \n 8 -1.5521155 male   northwest  0.13597407 -0.7569889 robust    -0.4122367   \n 9 -0.4472924 female west      -0.87014282  1.2735034 robust     0.6838436   \n10 -0.6682570 male   northwest  0.46326510  0.6245832 robust    -0.3343731   \n# ℹ 402 more rows\n# ℹ 3 more variables: weight &lt;transfrm&gt;, creatinin &lt;transfrm&gt;,\n#   infected &lt;transfrm&gt;\n\n\nDen Random Forest rechnen wir mit der Funktion ranger(). Dafür müssen wir wieder unser vollständiges Modell definieren und können dann die Funktion starten. Damit wir eine Variablenwichtigkeit (eng. variable importance) wiederbekommen, müssen wir noch die Option importance = \"permutation\" verwenden.\n\nfit_raw &lt;- ranger(infected ~ age + sex + location + activity + crp + frailty + bloodpressure + weight + creatinin, \n                  data = pig_tbl, ntree = 1000, importance = \"permutation\")\n\npluck(fit_raw, \"variable.importance\") |&gt; \n  sort(decreasing = TRUE) |&gt; \n  round(3)\n\n          crp     creatinin bloodpressure      location      activity \n        0.095         0.006         0.003         0.002         0.000 \n      frailty           age           sex        weight \n       -0.001        -0.001        -0.001        -0.002 \n\n\nWir sehen, dass wir als wichtigste Variable wiederum crp zurückbekommen. Danach wird es schon etwas schwieriger, da die Werte sehr schnell kleiner werden und auch ein Art Plateau bilden. Daher würde man hier nur annehmen, dass crp bedeutend für das Modell ist. Es kann aber auch sein, dass hier eine kontinuierliche Variable sehr vom Algorithmus bevorzugt wurde. Daher schauen wir uns die Sachlage einmal mit den standardisierten Daten an.\n\nfit_norm &lt;- ranger(infected ~ age + sex + location + activity + crp + frailty + bloodpressure + weight + creatinin, \n                  data = pig_norm_tbl, ntree = 1000, importance = \"permutation\")\n\npluck(fit_norm , \"variable.importance\") |&gt; \n  sort(decreasing = TRUE) |&gt; \n  round(3)\n\n          crp     creatinin bloodpressure       frailty      location \n        0.415         0.024         0.013         0.001        -0.002 \n          sex        weight      activity           age \n       -0.003        -0.008        -0.010        -0.013 \n\n\nAuch hier erhalten wir ein ähnliches Bild. Audf jeden Fall ist crp bedeutend für den Infektionsstatus. Danach werden die Werte etwas zufällig. Wir können Werte für die variable importance nicht unter Datensätzen vergleichen. Jeder Datensatz hat seine eigene variable importance, die von den Werten in dem Datensatz abhängt.\nWir ziehen als Fazit, dass wir nur crp als bedeutenden Wert für die Klassifikation des Infektionsstatus ansehen würden. Hier stehen wir wirklich etwas wie das Schwein vor dem Uhrwerk, denn was nun richtiger ist, stepAIC oder ranger lässt sich so einfach nicht bewerten. Zum einen wollen wir ja eigentlich mit Random Forest eine Klassifikation durchführen und mit linearen Regressionsmodellen eher kausale Modelle schätzen. Am Ende musst du selber abschätzen, was in das finale Modell soll. Ich kann ja auch den Threshold für den Variablenausschluss selber wählen. Wähle ich einen Threshold von \\(0.008\\), dann hätte ich crp, weight, bloodpressure und sex mit in dem finalen Modell.\n\n\n42.3.6 Feature Selektion mit boruta\nMit dem Boruta Algorithmus steht uns noch eine andere Implemnetierung des Random Forest Algorithmus zu verfügung um Feature Selektion zu betreiben. Wir nutzen wieder den Boruta Algorithmus in seine einfachen Form und gehen nicht tiefer auf alle Optionen ein. Wir nehmen wieder als Beispiel den Datensatz zu den infizierten Ferkeln und nutzen in diesem Fall auch nur die rohen Daten. Über eine Standardisierung könnte man wiederum nachdenken.\n\n\nIch empfehle noch das Tutorium Feature Selection in R with the Boruta R Package. Wir gehen hier nicht tiefer auf die Funktionalität von Boruta ein.\n\n\n\n\n\n\nStandardisieren oder Normalisieren von Daten\n\n\n\nEine Herausforderung für maschinelle Lernverfahren sind nicht normalisierte Daten. Das heist, dass wir Variablen haben, die kategorial oder kontinuierlich sein können oder aber sehr unterschiedlich von den Einheiten sind. Deshalb wird empfohlen die Daten vorher zu Standardisieren oder zu Normalisieren. In dem Kapitel 18 kannst du mehr über das Transformieren von Daten nachlesen.\n\n\nUm die Funktion Boruta() zu nutzen brauchen wir wieder das Modell und den Datensatz. Sonst ist erstmal nichts weiter anzugeben. Die Funktion läuft dann durch und gibt auch gleich den gewollten Informationshappen. Wichtig ist hierbei, dass wir natürlich noch andere Optionen mit angeben können. Wir können die Anzahl an Iterationen erhöhen und andere Tuning Parameter eingeben. Hier muss man immer schauen was am besten passt. Da würde ich auch immer rumprobieren und auf der Hilfeseite der Funktion ?Boruta einmal nachlesen.\n\nset.seed(20221031)\nboruta_output &lt;- Boruta(infected ~ age + sex + location + activity + crp + frailty + bloodpressure + weight + creatinin,  \n                        data = pig_tbl)  \n\nboruta_output\n\nBoruta performed 99 iterations in 4.861275 secs.\n 1 attributes confirmed important: crp;\n 6 attributes confirmed unimportant: activity, age, frailty, location,\nsex and 1 more;\n 2 tentative attributes left: bloodpressure, creatinin;\n\n\nManchmal ist es super praktisch, wenn eine Funktion einem die Antwort auf die Frage welche Variable bedeutend ist, gleich liefert. Wir erhalten die Information, dass die Variable crp als bedeutsam angesehen wird. Wir können uns den Zusammenhang auch in der Abbildung 42.2 auch einmal anschauen. Die grünen Variablen sind die bedeutenden Variablen.\n\nplot(boruta_output, cex.axis=.7, las=2, xlab=\"\", main=\"Variable Importance\")  \n\n\n\n\n\n\n\nAbbildung 42.2— Visualisierung der Boruta Ausgabe.\n\n\n\n\n\nAm Ende finden wir auch hier die Variable crp als einziges als bedeutend wieder. Wenn wir noch Variablen haben die verdächtig oder vorläufig bedeutend sind, angezeigt durch 2 tentative attributes left: bloodpressure, sex, dann können wir noch die Funktion TentativeRoughFix() nutzen. Die Funktion TentativeRoughFix() rechnet die Variablen nochmal nach und versucht alle Variablen in bedeutend oder nicht bedeutend zu klassifizieren. Wir haben ja zwei tentative Variablen in unseren Fall vorliegen, also nutzen wir noch kurz die Funktion um uns auch hier Klarheit zu schaffen.\n\nTentativeRoughFix(boruta_output)\n\nBoruta performed 99 iterations in 4.861275 secs.\nTentatives roughfixed over the last 99 iterations.\n 2 attributes confirmed important: creatinin, crp;\n 7 attributes confirmed unimportant: activity, age, bloodpressure,\nfrailty, location and 2 more;\n\n\nAm Ende ist die klare Aussage einer Funktion auch immer ein zweischneidiges Schwert. Wir verlieren jetzt noch die beiden tentative Variablen. Wo wir bei ranger die Qual der Wahl haben, werden wir bei Boruta eher vor vollendete Tatsachen gestellt. Meistens neigt man nach einer Boruta-Analyse nicht dazu noch eine zusätzliche Variable mit ins Modell zu nehmen. Dafür ist dann die Ausgabe zu bestimmt, obwohl die Entscheidung am Ende auch genau so unsicher ist wie von ranger und den anderen Modellen.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Variablenselektion</span>"
    ]
  },
  {
    "objectID": "stat-modeling-variable-selection.html#referenzen",
    "href": "stat-modeling-variable-selection.html#referenzen",
    "title": "42  Variablenselektion",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 42.1— Visualisierung der ols_step_backward_aic mit der Reduktion des AIC-Wertes.\nAbbildung 42.2— Visualisierung der Boruta Ausgabe.\n\n\n\nHeinze G, Dunkler D. 2017. Five myths about variable selection. Transplant International 30: 6–10.\n\n\nHeinze G, Wallisch C, Dunkler D. 2018. Variable selection–a review and recommendations for the practicing statistician. Biometrical journal 60: 431–449.\n\n\nTalbot D, Massamba VK. 2019. A descriptive review of variable selection methods in four epidemiologic journals: there is still room for improvement. European journal of epidemiology 34: 725–730.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Variablenselektion</span>"
    ]
  },
  {
    "objectID": "stat-modeling-missing.html",
    "href": "stat-modeling-missing.html",
    "title": "43  Fehlende Werte",
    "section": "",
    "text": "43.1 Was sind fehlende Werte?\nWir beschränken uns hier auf drei Arten von fehlenden Daten. Es gibt noch mehr Abstufungen, aber für den Einstieg reicht es, wenn wir nach drei Typen von fehlenden Daten unterscheiden. Die anderen Typen sind Mischtypen bzw. nicht so von Belang für die Anwendung.\nWie schon angemerkt. Die Struktur der fehlenden Werte lässt sich meist schwer vorhersagen bzw. bestimmen. Wir müssen eine Annahme treffen und diese dann auch in unseren statistischen Berichte oder Abschlussarbeit niederschreiben. Es gibt dann häufig auch Mischformen: MCAR, MAR, MNAR können ineinander verwoben sein. Häufig glauben wir daran, dass unsere Daten der MCAR genügen. Unter der Annahme, dass es sich bei den Daten um MCAR handelt, können auch zu viele fehlende Daten ein Problem darstellen. In der Regel liegt die sichere Obergrenze bei großen Datensätzen bei 5% der Gesamtmenge. Wenn die fehlenden Daten für ein bestimmtes Merkmal oder eine Stichprobe mehr als 5% betragen, sollten Sie dieses Merkmal oder diese Stichprobe wahrscheinlich weglassen. Wir prüfen daher im folgenden Abschnitten, ob in den Merkmalen (Spalten) und Stichproben (Zeilen) mehr als 5% der Daten fehlen. Auch hier gibt es dann Möglichkeiten erstmal die Daten zu visualiseren und dann zu schauen, welches Verfahren zur Imputation geeignet ist.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Fehlende Werte</span>"
    ]
  },
  {
    "objectID": "stat-modeling-missing.html#was-sind-fehlende-werte",
    "href": "stat-modeling-missing.html#was-sind-fehlende-werte",
    "title": "43  Fehlende Werte",
    "section": "",
    "text": "MCAR (eng. missing completely at random): völlig zufällig fehlende Daten. Dies ist das wünschenswerte Szenario im Falle fehlender Daten. Fehlende Werte werden als missing completely at random bezeichnet, wenn die Wahrscheinlichkeit für das Fehlen eines Wertes weder von erfassten noch unerfassten Merkmalen abhängt. Daher kann man sagen, dass MCAR-Werte die Fallzahl reduzieren, aber das Studienergebnis nicht verzerren.\nMAR (eng. missing at random): Fehlende Werte werden als missing at random bezeichnet, wenn die Wahrscheinlichkeit für das Fehlen eines Wertes von einem anderen Merkmal abhängt aber nicht von der Ausprägung des fehlenden Merkmals selbst. MAR-Werte reduzieren die Fallzahl und verzerren möglicherweise das Studienergebnis.\nMNAR (eng. missing not at random): nicht zufällig fehlende Daten. Fehlende, nicht zufällige Daten sind ein schwerwiegenderes Problem, und in diesem Fall kann es ratsam sein, den Datenerhebungsprozess weiter zu überprüfen und zu versuchen zu verstehen, warum die Informationen fehlen. Wenn zum Beispiel die meisten Teilnehmer einer Umfrage eine bestimmte Frage nicht beantwortet haben, warum haben sie das getan? War die Frage unklar? Daher werden fehlende Werte als missing not at random bezeichnet, wenn die Wahrscheinlichkeit für das Fehlen eines Wertes von der Ausprägung des fehlenden Merkmals selbst abhängt. MNAR-Werte reduzieren die Fallzahl und verzerren das Studienergebnis. MNAR sind Non-ignorable missings und müssen auch berichtet werden.\n\n\n\n\n\n\n\n\nSensitivitätsanalysen nach der Imputation von fehlenden Werten\n\n\n\nNachdem wir neue Daten bzw. Beobachtungen in unseren Daten erschaffen haben, ist es üblich noch eine Sensitivitätsanalysen durchzuführen. Wir Vergleich dann die Imputation mit der complete-case Analyse. Oder wir wollen die Frage beantworten, was hat eigentlich meine Imputation am Ergebnis geändert? Das machen wir dann gesammelt in dem Kapitel 44 zu den Sensitivitätsanalysen.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Fehlende Werte</span>"
    ]
  },
  {
    "objectID": "stat-modeling-missing.html#univariat-vs.-multivariate-imputation",
    "href": "stat-modeling-missing.html#univariat-vs.-multivariate-imputation",
    "title": "43  Fehlende Werte",
    "section": "43.2 Univariat vs. multivariate Imputation",
    "text": "43.2 Univariat vs. multivariate Imputation\nWas soll jetzt an dieser Stelle univariat und multivariat bedeuten? Wir haben uns die beiden Begriffe aufgehoben und nutzen diese Begriffe hier in dem Kontext der Imputation. Wir sprechen von einer univariaten Imputation, wenn wir nur eine Variable \\(x\\) imputieren. Das heißt, wir ignorieren die Zusammenhänge der Variable \\(x\\) zu irgendwelchen anderen Variablen in dem Datensatz. Das macht zum Beispiel für die Körpergröße in unserem Gummibärchendatensatz nicht so viel Sinn, denn wir haben ja Frauen und Männer befragt. Wir müssen die Körpergröße getrennt für die Variable Geschlecht imputieren. Wenn wir also Variablen mit Bezug zu anderen Variablen imputieren, dann nennen wir diese Verfahren multivariate Imputationsverfahren. In den folgenden Abschnitten werde ich einmal die gängisten univariaten Verfahren vorstellen und zwei sehr gut funktionierende multivariate Verfahren.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Fehlende Werte</span>"
    ]
  },
  {
    "objectID": "stat-modeling-missing.html#genutzte-r-pakete",
    "href": "stat-modeling-missing.html#genutzte-r-pakete",
    "title": "43  Fehlende Werte",
    "section": "43.3 Genutzte R Pakete",
    "text": "43.3 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, see, readxl, \n               mice, naniar, missForest, missRanger,\n               dlookr, parameters, recipes)\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Fehlende Werte</span>"
    ]
  },
  {
    "objectID": "stat-modeling-missing.html#daten",
    "href": "stat-modeling-missing.html#daten",
    "title": "43  Fehlende Werte",
    "section": "43.4 Daten",
    "text": "43.4 Daten\nIn diesem Kapitel nutzen wir zwei Datensätze. Zum einen den echten Datensatz mit den Gummibärchen aus dem Kapitel 6 und dem Datensatz zu dem Infketionsstatus von Ferkeln aus dem Kapitel 7.1. Der Ferkeldatzensatz hat keine fehlenden Werte und deshalb müssen wir da noch einmal nachhelfen und künstlich fehlende Werte erschaffen. Schauen wir uns nochmal den Gummibärchendatensatz an und wählen nur die Spalten gender, age, height, semester, count_bears, count_color und most_liked. Die anderen Spalten haben keine fehlenden Werte bzw. wenn eine Farbe der Bärchen nicht in der Tüte vorgekommen ist, dann war keine drin. Das sind dann keine fehlenden Werte.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  |&gt;\n  select(gender, age, height, semester, count_bears, count_color,\n         most_liked) |&gt; \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked),\n         count_color = as_factor(count_color)) \n\nZum anderen laden wir nochmal den Ferkeldatensatz mit unseren \\(n = 412\\) Ferkeln.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") \n\nWir haben haben aber in dem Ferkeldatensatz keine fehlenden Werte vorliegen. Deshalb nutzen wir die Funktion generateNA() aus dem R Paket {missRanger}. Wir können in der Funktion missRanger() angeben wieviel fehlende Werte global in dem Datensatz erschaffen werden sollen oder aber per Spalte. Wir erschaffen die fehlenden Werte bei der Spalte, denn die zehnte Spalte ist unser Outcome infected und da wollen wir ja keine fehlenden Werte imputieren. Fehlende Werte in der Outcomespalte bedeutet dann ja, dass die Beobachtung aus den Daten entfernt wird. Das brauchen wir aber hier nicht. Wie du sehen kannst erschaffen wir in jeder Spalte ein unterschiedliches Verhältnis von fehlenden Daten.\n\npig_miss_tbl &lt;- pig_tbl |&gt; \n  generateNA(c(0.1, 0.15, 0.05, 0.3, 0.1, 0.05, 0.15, 0.01, 0.05, 0))\n\nNun haben wir zwei Datensätze vorliegen an denen wir einmal schauen können, wie wir fehlende Daten imputieren können. Ein fehlender Wert wird in R als NA (eng. not availabe) bezeichnet.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Fehlende Werte</span>"
    ]
  },
  {
    "objectID": "stat-modeling-missing.html#numerischer-überblick",
    "href": "stat-modeling-missing.html#numerischer-überblick",
    "title": "43  Fehlende Werte",
    "section": "43.5 Numerischer Überblick",
    "text": "43.5 Numerischer Überblick\nManchmal soll es nur ein schneller numerischer Überblick über die fehlenden Werte sein. Dafür hilft dann auch das R Paket {naniar}. Wir erhalten relativ schnell die Anzahl und die prozentuale Häufigkeit der fehlenden Werte mit den Funktionen n_miss() und n_complete(). In den Schweinedaten haben wir dann eine Quote von 10.6% fehlenden Werten.\n\nn_miss(pig_miss_tbl)/n_complete(pig_miss_tbl)\n\n[1] 0.1066344\n\n\nÜber die Funktion miss_case_summary() kannst du dir dann noch mehr Informationen zu den einzelnen Beobachtungen wiedergeben lassen. Über die Funktion print() kannst du dir dann noch mehr Zeilen als die normalen zehn Zeilen ausgeben lassen.\n\npig_miss_tbl |&gt; \n  miss_case_summary() |&gt; \n  print(n = 12)\n\n# A tibble: 412 × 3\n    case n_miss pct_miss\n   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1     1      4       40\n 2    39      4       40\n 3   108      4       40\n 4   219      4       40\n 5   249      4       40\n 6   251      4       40\n 7   385      4       40\n 8    13      3       30\n 9    40      3       30\n10    91      3       30\n11   117      3       30\n12   126      3       30\n# ℹ 400 more rows\n\n\nSchauen wir uns mal die Schweine vier fehlenden Werten einmal an. Wir nutzen dazu die Funktion slice(), die es uns erlaubt die Zeilen zu extrahieren, welche für die Beobachtungen stehen.\n\npig_miss_tbl |&gt; \n  slice(c(1, 84, 227, 397))\n\n# A tibble: 4 × 10\n    age sex    location  activity   crp frailty   bloodpressure weight creatinin\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1    61 &lt;NA&gt;   northeast     15.3  NA   robust             NA     19.0     NA   \n2    64 male   northwest     15.5  20.0 frail              52.3   NA        3.58\n3    66 female west          NA    20.0 robust             58.6   18.8      6.21\n4    62 female northwest     10.2  18.5 pre-frail          58.0   21.5      4.73\n# ℹ 1 more variable: infected &lt;dbl&gt;\n\n\nDamit haben wir eigentlich die wichtigsten Funktionen zusammen. Es gibt noch einige mehr, die du nutzen kannst um zu erfahren wie viele fehlende Werte du eigentlich in den Daten hast. Meistens willst du aber die Häufigkeit wissen und welche Beobachtungen besonders viele fehlende Werte haben. Dann kannst du zum Beispiel dann wieder in deine Exceldatei zurückgehen und schauen, ob da ein Fehler vorliegt oder aber ob du noch die Daten irgendwo hast. Je nach Aufwand kannst du dann die Beobachtung in Excel entfernen. Sonst eben mit ! in der Form slice(!c(1, 84, 227, 397)) in der Funktion slice(). Damit schmeißt du dann die Beobachtungen aus dem Datensatz raus.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Fehlende Werte</span>"
    ]
  },
  {
    "objectID": "stat-modeling-missing.html#visualisierung",
    "href": "stat-modeling-missing.html#visualisierung",
    "title": "43  Fehlende Werte",
    "section": "43.6 Visualisierung",
    "text": "43.6 Visualisierung\nWie immer ist es sehr wichtig, sich die Struktur der fehlenden Daten einmal zu veranschaulichen. Wir nutzen dazu zwei drei Funktion aus zwei R Paketen. Zum einen die Funktion vis_miss() und gg_miss_upset() aus dem R Paket {naniar} sowie die Funktion plot_na_pareto() aus dem R Paket {dlookr}. Wir schauen uns mit den Funktionen einmal die Daten an und entscheiden, ob wir wirklich MCAR als Struktur der fehlenden Daten vorliegen haben. Wie immer haben beide R Pakete noch eine Reihe von weiteren Funktionen zu bieten. Bei {naniar} sind es weitreichende Visualisierungen zu fehlenden Werten. Bei dlookr sind es eine ergiebige Auswahl an Funktionen zur Diagnose, Report und Explorative Datenanalyse.\nBetrachten wir zunächst die Gummibärchendaten in der Abbildung 43.1. Zuerst sehen wir in Abbildung 43.1 (a), dass sich einige Blöcke in der Variable Semester gebildet haben. Das kann auf einen nicht zufälliges fehlenden der Daten deuten. Auch scheint die Angabe von dem Lieblingsgeschmack in den älteren Beobachtungen besser eingetragen worden zu sein. Trotzdem können wir hier auf ein relativ zufälliges Fehlen der Daten tippen. Wir nehmen mit etwas Bauchschmerzen hier MCAR an und machen dann mit der Imputation weiter. Du könntest auch überlegen, alle fehlenden Wert zu entfernen. Es scheint, dass Beobachtungen häufig über Geschlecht, Alter, Körpergröße und Semester fehlen. Dieser Zusammenhang wird dann in Abbildung 43.1 (b) nochmal deutlich. Wir haben viele NA’s im Semester. Häufig sind dann diese fehlenden Werte aber auch gleichseitig mit fehlenden Werten in der Variable Körpergröße, Alter und Geschlecht verknüpft. In Abbildung 43.1 (c) sehen wir nochmal die Anteile an den fehlenden Werten pro Spalte.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Anzahl der fehlenden Werte zu den vorhandenen Werten mit der Funktion vismiss()\n\n\n\n\n\n\n\n\n\n\n\n(b) Anzahl der fehlenden Werte zu den vorhandenen Werten und deren Überlappung mit anderen Variablen mit der Funktion gg_miss_upset()\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Anzahl der fehlenden Wert in absoluter und relativer Angabe und deren kumulativer Verlauf. Die Funktion plot_na_pareto() gibt auch eine Wertung wieder.\n\n\n\n\n\n\n\nAbbildung 43.1— Visualisierung der fehlenden Werte im Gummibärchendatensatz. Alle drei Abbildungen sind etwas wiederholend, liefern aber ein gutes Gesamtbild.\n\n\n\n\nNachdem wir uns nun echte fehlende Werte in den Gummibärchendaten angesehen haben, wollen wir uns die zufällig erstellten fehlenden Daten in der Abbildung 43.2 einmal anschauen. In Abbildung 43.2 (a) sehen wir jetzt die zufällige Verteilung der fehlenden Werte nach der vorgegebenen Häufigkeit. Das passt dann auch gut zu der Abbildung 43.2 (c) in der wir die Anteile jeweils in absoluten und relativen Häufigkeiten sehen. Auch sind in Abbildung 43.2 (b) die Verbindungen der fehlenden Werte über die verschiedenen Variablen sehr zufällig. Wir haben kaum Blöcke von mehr als zwei Variablen, die gleichzeitig fehlen.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Anzahl der fehlenden Werte zu den vorhandenen Werten mit der Funktion vismiss()\n\n\n\n\n\n\n\n\n\n\n\n(b) Anzahl der fehlenden Werte zu den vorhandenen Werten und deren Überlappung mit anderen Variablen mit der Funktion gg_miss_upset()\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Anzahl der fehlenden Wert in absoluter und relativer Angabe und deren kumulativer Verlauf. Die Funktion plot_na_pareto() gibt auch eine Wertung wieder.\n\n\n\n\n\n\n\nAbbildung 43.2— Visualisierung der fehlenden Werte im Ferkeldatensatz. Alle drei Abbildungen sind etwas wiederholend, liefern aber ein gutes Gesamtbild.\n\n\n\n\nNachdem wir uns beide Datensätze nochmal in der Visualisierung der fehlenden Werte angeschaut haben, stellen wir natürlich fest, dass der Gummibärchendatensatz weniger zufällig fehlende Werte hat als der künstlich erschaffene Datensatz zu den Ferkeln. Dennoch wollen wir mit beiden Datensätzen einmal weitermachen und schauen, wie wir jetzt die fehlenden Werte oder auch NA’s in R imputieren können.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Fehlende Werte</span>"
    ]
  },
  {
    "objectID": "stat-modeling-missing.html#univariate-imputation",
    "href": "stat-modeling-missing.html#univariate-imputation",
    "title": "43  Fehlende Werte",
    "section": "43.7 Univariate Imputation",
    "text": "43.7 Univariate Imputation\nFür die univariate Imputation von fehlenden Werten nutzen wir die Funktion impute_na() aus dem R Paket {dlookr}. Das R Paket {dlookr} hat eine große Auswahl an simplen Imputationsalgorithmen. Die univariate Variante der Imputation von fehlenden Werten ist eigentlich nur anzuraten, wenn wir eine Spalte vorliegen haben, wo fehlende Daten drin sind. Darüber hinaus haben die anderen Spalten keine Verbindung zu dieser Spalte. Dann mag es sinnvoll sein eine univariate Imputation zu nutzen. Ich selber nutze die univariate Imputation nur, wenn es schnell gehen soll und die Daten wenig Spalten haben. Passiert sehr selten.\nBei der univariaten Imputation müssen wir unterscheiden, welche Art die Spalte bzw. Variable ist, die wir imputieren wollen. Wenn die Spalte numerisch ist, daher ein double &lt;dbl&gt; oder integer &lt;int&gt; können wir folgende Optionen der Funktion impute_na() wählen. Wie immer hilft hier auch die Hilfeseite der Funkion ?impute_na() weiter.\n\nmean: Die Variable wird mit dem Mittelwert in der Variablenspalte imputiert. Daher werden alle fehlenden Werte mit dem Mittlwert der Variable ersetzt. Ja, das heist jetzt steht sehr häufig der Mittelwert in der Spalte.\nmedian: Die Variable wird mit dem Median in der Variablenspalte imputiert. Daher werden alle fehlenden Werte mit dem Median der Variable ersetzt. Ja, das heist jetzt auch hier, da steht sehr häufig der Median in der Spalte.\nmode: Dem Modus beziehungsweise den häufigsten Wert in der Variablenspalte können wir auch wählen um die fehlenden Werte zu ersetzen. Mit kontinuierlichen Werten ist diese Methoe nach dem Modus aber nicht anzuraten. Mit Kommastellen in der Variable gibt es schnell keinen oder nur einen Wert mit der absoluten Häufigkeit von zwei oder mehr.\n\nAnders sieht es aus, wenn die Spalte kategorisch ist, daher ein factor &lt;fct&gt; oder character &lt;chr&gt; können wir folgende Optionen der Funktion impute_na() wählen.\n\nmode: Wir imputieren mit dem Modus beziehungsweise den häufigsten Wert in der Variablenspalte und erstetzen damit jeden Wert mit dem häufigsten Wert in der Spalte.\nrpart: Wir können auch Recursive Partitioning and Regression Trees nutzen um eine kategorielle Variable zu imputieren, aber das geht hier dann zu weit. Siehe dazu dann auch das Kapitel 70.\n\nDamit haben wir alle Optionen einmal zur Hand. Damit sich das Kapitel nicht in die Unendlichkeit ausdehnt, wollen wir einmal die Funktion impute_na() an der Spalte age in dem Gummibärchendatensatz ausprobieren. Auch hier nutzen wir nur die mean- und median-Imputation. Du kannst dann gerne noch die anderen Optionen ausprobieren. Im Folgenden also der Code zusammen mit der Funktion mutate().\n\nimp_age_tbl &lt;- gummi_tbl |&gt; \n  mutate(mean_age_imp = imputate_na(gummi_tbl, semester, method = \"mean\"),\n         median_age_imp = imputate_na(gummi_tbl, semester, method = \"median\"),\n         mode_age_imp = imputate_na(gummi_tbl, semester, method = \"mode\"))\n\nWir haben uns also das neue Objekt imp_age_tbl erschaffen in dem die beiden neuen imputierten Spalten drin sind. Wenn du dir die Spalten einmal in R anschaust, wirst du sehen, dass viele Zahlen gleich sind. Die Zahlen sind gleich, weil sie eben den Mittelwert oder den Median entsprechen. In Abbildung 43.3 siehst du nochmal den Vergleich von den Werten vor der Imputation (orginal) und nach der Imputation (imputation). Wenn du die Spalte in die Funktion plot() steckst erkennt die Funktion, dass es sich um importierte Werte handelt und plotted daher die Werte getrennt. Das funktioniert natürlich nur nach der Nutzung der Funktion impute_na().\n\nplot(imp_age_tbl$mean_age_imp)\nplot(imp_age_tbl$median_age_imp)\nplot(imp_age_tbl$mode_age_imp)\n\n\n\n\n\n\n\n\n\n\n\n(a) Imputation mit dem Mittelwert.\n\n\n\n\n\n\n\n\n\n\n\n(b) Imputation mit dem Median.\n\n\n\n\n\n\n\n\n\n\n\n(c) Imputation mit dem Modus.\n\n\n\n\n\n\n\nAbbildung 43.3— Densityplot der Verteilungen vor und nach der Imputation.\n\n\n\n\nWir wir in der Abbildung erkennen können, funktioniert die Methode ganz gut. Wir erhalten aber sehr viel mehr Werte auf die Schwerpunkte der Verteilung. Daher kriegen wir eine sehr viel stärkere bimodale Verteilung heraus als wir vorher hatten. Insbesondere der Modus zeigt hier eine sehr verzerrte Imputation. Ob eine bimodale Verteilung so beim Alter passt ist schwer zu sagen. Bei der Körpergröße wäre es richtiger. Daher ist eine univariate Imputation immer mit Vorsicht zu genießen.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Fehlende Werte</span>"
    ]
  },
  {
    "objectID": "stat-modeling-missing.html#multivariate-imputation",
    "href": "stat-modeling-missing.html#multivariate-imputation",
    "title": "43  Fehlende Werte",
    "section": "43.8 Multivariate Imputation",
    "text": "43.8 Multivariate Imputation\nIm Folgenden schauen wir uns zwei multivariate Verfahren an um fehlende Werte zu imputieren. In beiden Fällen entbindet uns, dass multivariat nicht davon nochmal zu schauen, ob unsere Daten einigermaßen konsistent imputiert wurden. Beide Verfahren haben ihre Vor und Nachteile.\n\nZum einen nutzen wir das R Paket {mice} in Kapitel 43.8.1. Wir müssen in mice für jede Spalte angeben, welcher Verteilung die Spalte folgt bzw. mit welche fortgeschrittenen Methode die Spalte imputiert werden soll. Die Imputation findet dann \\(m\\)-Mal über alle Variablen statt. Danach können wir dann die \\(m\\)-mal imputierten Datensätze weiter benutzen.\nWir nutzen als Alternative noch das R Paket {missRanger} in Kapitel 43.8.2. Wir müssen in dem R Paket {missRanger} nicht angeben welcher Verteilung die Spalten folgen. Daher ist missRanger etwas einfacher zu bedienen, aber auf der anderen Seite auch mehr eine Blackbox. Wir stecken Daten rein und erhalten einen imputierte Daten wieder. Das mag vielleicht auch ein Vorteil sein.\n\nBeide Verfahren liefern uns dann die imputierten Datensätze wieder und wir müssen dann in den entsprechenden Visualisierungen schauen, ob wir so mit der Imputation zufrieden sind.\n\n43.8.1 Imputation mit mice\nBeginnen wir also mit der Imputation unter der Verwendung von dem R Paket {mice}. Die Funktion, die die Imputation durchführt heist ebenfalls mice() was für Multivariate Imputation by Chained Equations steht. Wir nutzen aber nur die Abkürzung mice(). Bei der Nutzung von mice durchlaufen wir mehrere Schritte. Zuerst müssen wir der Funktion mitteilen, welche Eigenschaften die zu imputierenden Spalten haben. Auch hier gilt, die Hilfeseite von ?mice() hilft bei der Entscheidung welche Variante für die jeweilige Spalte in den Daten passt. Wenn wir eine Spalte gar nicht imputieren wollen, dann lassen wir den Eintrag in dem benamten Vektor einfach leer. Im Folgenden der benamte Vektor mit den Variablennamen und wie die einzelnen Variablen dann imputiert werden sollen.\n\nimp_method &lt;- c(gender = \"logreg\", \n                age = \"pmm\", \n                height = \"pmm\", \n                semester = \"pmm\", \n                count_bears = \"\", \n                count_color = \"polyreg\", \n                most_liked = \"polyreg\")\n\nDen Vektor imp_method nutzen wir jetzt in der Funktion mice() für die Option method = imp_method. Nun weis mice() wie die Daten für jede Spalte über alle anderen Spalten imputiert werden soll. Wichtig ist noch anzugeben, wie viele \\(m\\) imputierte Datensätze erschaffen werden sollen. Wir nehmen hier mal \\(m = 5\\) und wiederholen den Prozess nur \\(maxit = 3\\) Mal. Je höher maxit ist, desto genauer wird mice() aber desto mehr Iterationen müssen gerechnet werden. Jede Iteration dauert auch so seine Zeit.\n\nimp_gummi_tbl &lt;- mice(data = gummi_tbl, m = 5, maxit = 3, \n                      method = imp_method)\n\n\n iter imp variable\n  1   1  gender  age  height  semester  most_liked\n  1   2  gender  age  height  semester  most_liked\n  1   3  gender  age  height  semester  most_liked\n  1   4  gender  age  height  semester  most_liked\n  1   5  gender  age  height  semester  most_liked\n  2   1  gender  age  height  semester  most_liked\n  2   2  gender  age  height  semester  most_liked\n  2   3  gender  age  height  semester  most_liked\n  2   4  gender  age  height  semester  most_liked\n  2   5  gender  age  height  semester  most_liked\n  3   1  gender  age  height  semester  most_liked\n  3   2  gender  age  height  semester  most_liked\n  3   3  gender  age  height  semester  most_liked\n  3   4  gender  age  height  semester  most_liked\n  3   5  gender  age  height  semester  most_liked\n\n\nSchauen wir jetzt einmal nach, ob auch die Imputation geklappt hat. In Abbildung 43.4 sehen wir nochmal die Daten visualisiert und sehen, dass es keinen fehlenden Werte mehr gibt. Die Überprüfung ist sinnvoll, da wir manchmal Spalten nicht imputieren wollen und dann müssen wir schauen, ob auch das so geklappt hat.\n\ncomplete(imp_gummi_tbl) |&gt; vis_miss()\n\n\n\n\n\n\n\nAbbildung 43.4— Überprüfung der Imputation mit mice(). Wie erhofft gibt es keine fehlenden Werte mehr in den Daten.\n\n\n\n\n\nNachdem wir mit der Imputation durch sind können wir uns für die kontinuierlichen Variablen einmal die ursprüngliche Verteilung der Daten mit den fehlenden Weren im vergleich zu den \\(m=5\\) imputierten Verteilungen anschauen. Die Funktion densityplot() erlaubt hier eine einfache und schnelle Darstellung in Abbildung 43.5. Wir sehen, dass die Imputation nicht immer sehr gut geklappt hat, aber dadurch das wir die Imputation fünfmal gemacht haben, mittelt sich der Effekt einer einzelen Imputation wieder raus.\n\ndensityplot(imp_gummi_tbl)\n\n\n\n\n\n\n\nAbbildung 43.5— Densityplot der Verteilungen der ursprünglichen kontinuierlichen Daten im Vergleich zu den \\(m=5\\) imputierten Datensätzen.\n\n\n\n\n\nLeider wird es jetzt etwas schwerer mit den imputierten Daten zu arbeiten. Wir müssen ja jetzt die fünf imputierten Datensätze irgendwie analysieren. Die Analyse der fünf Datensätze wird getrennt gemacht und dann mit der Funktion pool() die Effektschätzer und \\(p\\)-Werte aller fünf Datensätze kombiniert. Ein weiteres leider ist, dass wir nicht für jedes Modell in R eine pool Funktion haben. Somit haben wir im Zweifel hier ein Problem, wenn es darum geht die Datensätze weiter zuverwenden. Die meisten glm()-Regressionen können aber so genutzt werden.\n\npooled_res &lt;- imp_gummi_tbl |&gt;\n  mice::complete(\"all\") |&gt;\n  map(lm, formula = height ~ age + semester + gender) |&gt;\n  pool()\n\nWir nutzen dann die Funktion model_parameters() um uns die Ausgabe des Poolings besser anzeigen zu lassen. Wir sehen, dass sich das Ergebnis nicht sonderlich von den Ergbenissen einer normalen linearen Regression unterscheidet. Wir könnten dann mit dem gepoolten Modell auch weiter in einen Gruppenvergleich oder eine ANOVA gehen. Sobald wir durch ein Modell und der pool() Funktion ein Objekt haben, können wir mit dem Objekt weiterarbeiten.\n\npooled_res |&gt; model_parameters()\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |           95% CI | Statistic |     df |      p\n---------------------------------------------------------------------------------\n(Intercept) |      184.96 | 1.37 | [182.21, 187.70] |    134.52 |  67.73 | &lt; .001\nage         |       -0.03 | 0.05 | [ -0.13,   0.08] |     -0.46 | 124.18 | 0.646 \nsemester    |       -0.27 | 0.12 | [ -0.51,  -0.02] |     -2.15 |  88.01 | 0.034 \ngenderw     |      -14.94 | 0.59 | [-16.10, -13.78] |    -25.47 | 141.23 | &lt; .001\n\n\nLeider ist es so, dass wir nicht immer mit pool() arbeiten können, da wir für unsere Funktion, die wir nutzen wollen keine Anwendung in pool() finden. Salopp gesagt, wir erhalten einen Fehler, wenn wir das Modell oder die Funktion poolen wollen. In diesem Fall hilft die Funktion complete() mit der Option action = \"long\" und include = TRUE etwas weiter. Wir erhlaten damit die fünf imputierten Datensätze und den ursprünglichen Datensatz als Long-Format wiedergegeben. Damit können wir dann weiterarbeiten. Das ist aber dann das Thema für ein anderes Kapitel.\n\nimp_all_gummi_tbl &lt;- imp_gummi_tbl |&gt; \n  complete(action = \"long\", include = TRUE) |&gt; \n  select(-.id, imp_run = .imp) |&gt; \n  mutate(imp_run = as_factor(imp_run)) |&gt; \n  as_tibble()\n\nimp_all_gummi_tbl\n\n# A tibble: 4,698 × 8\n   imp_run gender   age height semester count_bears count_color most_liked\n   &lt;fct&gt;   &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;     \n 1 0       m         35    193       10           9 3           lightred  \n 2 0       w         21    159        6          10 5           yellow    \n 3 0       w         21    159        6           9 6           white     \n 4 0       w         36    180       10          10 5           white     \n 5 0       m         22    180        3          10 6           white     \n 6 0       &lt;NA&gt;      NA     NA       NA          10 5           white     \n 7 0       m         22    180        3          10 5           green     \n 8 0       w         21    163        3          13 5           green     \n 9 0       m         22    170        3           9 5           green     \n10 0       m         23    176        3           9 5           white     \n# ℹ 4,688 more rows\n\n\nDie Spalte imp_run gibt uns dann die Imputation \\(m\\) wieder. Die \\(0\\)-te Imputation sind dabei die Orginaldaten. Wie du schon sehen kannst, wird das alles sehr schnell sehr groß und wir müssen hier mit fortgeschrittner Programmierung ran, wenn die Funktion pool() nicht will. In Abbildung 43.6 sehen wir die Anwendung des Objektes imp_all_gummi_tbl für die Visualisierung der beiden kategorialen Variablen gender und most_liked.\n\nggplot(imp_all_gummi_tbl, aes(gender, fill = imp_run)) +\n  theme_minimal() +\n  geom_bar(position = position_dodge(preserve = \"single\")) +\n  scale_fill_okabeito()\n\nggplot(imp_all_gummi_tbl, aes(most_liked, fill = imp_run)) +\n  theme_minimal() +\n  geom_bar(position = position_dodge(preserve = \"single\")) +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\n\n\n\n\n(a) Imputation von gender.\n\n\n\n\n\n\n\n\n\n\n\n(b) Imputation von most_liked.\n\n\n\n\n\n\n\nAbbildung 43.6— Barplots der kategorialen Variablen getrennt nach den nicht-imputierten und fünf imputierten Datensätzen.\n\n\n\n\n\n\n43.8.2 Imputation mit missRanger\nIn diesem letzten Abschnitt wollen wir die Funktion missRanger() aus dem gleichnamigen R Paket {missRanger} nutzen um die fehlenden Werte in unseren Gummibärchendaten zu imputieren. Das Paket basiert auf sogenannten Decision tree, die wir im Kapitel 70 näher betrachten. Das tiefere Verständnis ist aber für die Anwendung nicht notwendig. Wir können die Funktionalität des R Pakets recht einfach nutzen.\nAls erstes brauchen wir den Datensatz und danach die Formel mit der Imputiert werden soll. Die Fomel ist ganz einfach aufgebaut. Links stehen die Variablen, die imputiert werden sollen und rechts stehen die Variablen, die zur Imputation verwendet werden sollen. Wenn wir einfach alles imputieren wollen und dafür alle Variablen nutzen wollen, dann schreiben wir einfach . ~ . auf. Also wir schreiben einen Punkt . links und rechts von der ~. Dann weis die Funktion, dass alles unter der zu Hilfenahme von allen Variablen imputiert werden soll. Wir müssen noch angeben, wie oft die Imputation laufen soll. Wir haben wir per default \\(500\\) Bäume oder Widerholungen angegeben. Wir wollen keine Ausgabe während der Funktion läuft und setzen deshalb verbose = 0.\n\ngummi_imputet_tbl &lt;- missRanger(\n  gummi_tbl, \n  formula = . ~ . ,\n  num.trees = 500, \n  verbose = 0)\n\nWie wir sehen, ist die Funktion sehr viel einfacher zu handhaben. Am Ende erhalten wir per default nur einen Datensatz von der Funktion zurück. Die Funktion missRanger() poolt für uns die Daten, so dass wir dann mit dem einen Datensatz weitermachen können. Das ist natürlich besonders sinnvoll, wenn wir im Anschluss an die Imputation eben keine Regression sondern etwa maschinelle Lernverfahren nutzen wollen.\nIn Abbildung 43.7 sehen wir nochmal die Überprüfung der nicht-imputierten und imputierten Daten. Anscheinend hat die Impiutation gut geklappt. Für den kontinuierlichen Fall liegen die imputierten Beobachtungen gut gestreut in den nicht-imputierten Daten. Auch für die kategoriale Variable gender passen die Verhältnisse. Wir können die Überprüfung jetzt für alle kontinuierlichen und alle kategorialen Variablen fortsetzen. Das müssen wir auch tun! Nur hier ist dann auch bald mal der Platz zu Ende, so dass wir es bei den beiden Abbildungen belassen.\n\nggplot()+\n  geom_point(data = gummi_imputet_tbl, aes(age, height), \n             color = \"red\")+\n  geom_point(data = gummi_tbl, aes(age, height))+\n  theme_minimal() +\n  scale_color_okabeito()\n\nggplot()+\n  geom_bar(data = gummi_tbl, aes(gender), width = 0.3) +\n  geom_bar(data = gummi_imputet_tbl, aes(gender), fill = \"red\",\n           position = position_nudge(x = 0.25), width = 0.3) +\n  theme_minimal() + \n  scale_color_okabeito()\n\n\n\n\n\n\n\n\n\n\n\n(a) Überprüfung zweier kontinuierliche Variablen age und height.\n\n\n\n\n\n\n\n\n\n\n\n(b) Überprüfung einer kategorialen Variable gender.\n\n\n\n\n\n\n\nAbbildung 43.7— Scatterplot und Barplot der imputierten Variablen getrennt nach nicht-imputierten und imputierten. Die roten Punkte stellen die imputierten Beobachtungen da.\n\n\n\n\nAbschlißend können wir auch nur Teile der Daten imputieren. Wenn wir nur die Spalten age und semester imputieren wollen, dann würden wir age + semester ~ . schreiben. Damit würden wir die beiden Spalten Alter und Semester dann durch die Informationen in all den anderen Spalten imputieren. Wir können die beiden Spalten auch nur durch spezifische andere Spalten imputieren lassen. Im folgenden Beispiel imputieren wir die Spalten age und semester durch die Informationen in den Spalten height und gender. Es dürfen natürlich auch gleiche Spalten auf beiden Seiten der Formel stehen.\n\ngummi_imputet_tbl &lt;- missRanger(\n  gummi_tbl, \n  formula = age + semester ~ height + gender ,\n  num.trees = 500,\n  verbose = 0)\n\nWas war jetzt besser? Das ist eine gute Frage. In einer parametrischen Regressionsanalyse bietet sich der Ablauf mit dem R Paket {mice} an. Wir haben in dem Fall der Regression Zugriff auf die Funktion pool() und können damit die Ergebnisse der \\(m\\) Imputationen zusammenfassen. Wenn wir das nicht können, also es keine Möglichkeit gibt unsere Methode der Wahl mit pool() zu nutzen, dann empfiehlt es sich das R Paket {missRanger} zu nutzen.\n\n\n43.8.3 Imputation mit recipes\nWenn wir später in dem Kapitel 65 zu maschinellen Lernverfahren etwas lernen, dann nutzen wir dafür {tidymodels}. Das R Paket {tidymodels} ist eine Zusammenfassung von mehreren wichtigen und sinnvollen R Paketen zur Klassifikation. Wir nutzen darüber hinaus das R Paket {recipes} um uns ein Rezept zu bauen, was wir dann nutzen. Du kannst mehr über den Aufbau von Rezepten in R dann im Kapitel 65 erfahren. Hier nur eine kurze Abhandlung dazu. Um die Rezepte in R nutzen zu können laden wir das Paket {tidymodels}. In dem Paket ist das R Paket {recipes} schon mit enthalten.\n\npacman::p_load(tidymodels)\n\nWir definieren nun unser Rezept nachdem wir imputieren wollen. Im Gegensatz zu missRanger müssen wir hier ein Outcome \\(y\\) angeben und auf der rechten Seite die Variablen, die mit in das Modell sollen. Das ist meistens auch kein Problem in der Klassifikation, da ja sehr häufig das Outcome \\(y\\) binär und bekannt ist. Wir nutzen also einfach unseren Datensatz zu den infizierten Ferkeln und bauen uns unser Rezept. Wir wollen alle anderen Variablen außer die Variable infected mit ins Modell nehmen. Deshalb schreiben wir rechts von der Tilde einfach nur einen . hin.\n\nrec &lt;- recipe(infected ~ ., data = pig_miss_tbl)\n\nNachdem wir unser Rezept haben, also wissen was das Outcome ist und was die Prädiktoren, können wir wir die Funktion step_impute_bag() nutzen um den Algorithmus für die Imputation zu spezifizieren. Es gibt noch zahlreiche andere Möglichkeiten die Variablen zu imputieren, aber wir haben ja wieder eine Mischung aus kontinuierlichen und kategoriellen Variablen, so dass sich hier wieder ein Decision tree Algorithmus anbietet. Das R Paket {recipes} hat folgende Rezepte für die Imputation implementiert.\n\nimpute_rec &lt;- rec |&gt; step_impute_bag(all_predictors())\n\nWir haben jetzt unseren Imputationsalgorithmus mit dem Rezept verbunden und können nun über die Funktionenprep() und bake() die Imputation durchführen. Im ersten Schritt prep() bereiten wir die Imputation vor. Im nächsten Schritt bake() führen wir dann die Imputation auf den Daten von pig_miss_tbl aus. Das mag jetzt etwas von hinten durch die Brust sein, aber da wir durch recipes in dem Kapitel 65 zu maschinellen Lernverfahren besser verschiedene Verfahren aneinander kleben können, sein es hier nochmal so gezeigt.\n\nimputed_tbl &lt;- prep(impute_rec, training = pig_miss_tbl) |&gt; \n  bake(new_data = pig_miss_tbl)\n\nWir finden dann in dem Objekt imputed_tbl einen imputierten Datensatz ohne fehlende Werte wieder. Mit diesem Datensatz können wir dann weiterarbeiten. Die Vingette zu {recipes} zeigt die Imputation mit k-NN an einem etwas komplexeren Beispiel.\n\n\n\nAbbildung 43.1 (a)— Anzahl der fehlenden Werte zu den vorhandenen Werten mit der Funktion vismiss()\nAbbildung 43.1 (b)— Anzahl der fehlenden Werte zu den vorhandenen Werten und deren Überlappung mit anderen Variablen mit der Funktion gg_miss_upset()\nAbbildung 43.1 (c)— Anzahl der fehlenden Wert in absoluter und relativer Angabe und deren kumulativer Verlauf. Die Funktion plot_na_pareto() gibt auch eine Wertung wieder.\nAbbildung 43.2 (a)— Anzahl der fehlenden Werte zu den vorhandenen Werten mit der Funktion vismiss()\nAbbildung 43.2 (b)— Anzahl der fehlenden Werte zu den vorhandenen Werten und deren Überlappung mit anderen Variablen mit der Funktion gg_miss_upset()\nAbbildung 43.2 (c)— Anzahl der fehlenden Wert in absoluter und relativer Angabe und deren kumulativer Verlauf. Die Funktion plot_na_pareto() gibt auch eine Wertung wieder.\nAbbildung 43.3 (a)— Imputation mit dem Mittelwert.\nAbbildung 43.3 (b)— Imputation mit dem Median.\nAbbildung 43.3 (c)— Imputation mit dem Modus.\nAbbildung 43.4— Überprüfung der Imputation mit mice(). Wie erhofft gibt es keine fehlenden Werte mehr in den Daten.\nAbbildung 43.5— Densityplot der Verteilungen der ursprünglichen kontinuierlichen Daten im Vergleich zu den \\(m=5\\) imputierten Datensätzen.\nAbbildung 43.6 (a)— Imputation von gender.\nAbbildung 43.6 (b)— Imputation von most_liked.\nAbbildung 43.7 (a)— Überprüfung zweier kontinuierliche Variablen age und height.\nAbbildung 43.7 (b)— Überprüfung einer kategorialen Variable gender.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Fehlende Werte</span>"
    ]
  },
  {
    "objectID": "stat-modeling-sensitivity.html",
    "href": "stat-modeling-sensitivity.html",
    "title": "44  Sensitivitätsanalyse",
    "section": "",
    "text": "44.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, dlookr, broom, modelsummary,\n               see, performance, ggpubr, factoextra, FactoMineR,\n               conflicted)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Sensitivitätsanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-sensitivity.html#daten",
    "href": "stat-modeling-sensitivity.html#daten",
    "title": "44  Sensitivitätsanalyse",
    "section": "44.2 Daten",
    "text": "44.2 Daten\nIn diesem Beispiel betrachten wir wieder die Gummibärchendaten. Auch hier haben wir echte Daten vorliegen, so dass wir Ausreißer entdecken könnten. Da wir hier auch fehlende Werte in den Daten haben, können wir diese fehlenden Werte auch einfach imputieren und uns dann die Effekte anschauen. Das heißt wir haben also einen idealen Datensatz für unsere Sensitivitätsanalysen.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  |&gt;\n  select(gender, age, height, semester) |&gt; \n  mutate(gender = as_factor(gender)) \n\nIn der Tabelle 41.2 ist der Datensatz gummi_tbl nochmal für die ersten sieben Zeilen dargestellt. Wir werden später sehen, wie sich die Fallzahl von \\(n = 783\\) immer wieder ändert, je nachdem wie wir mit den fehlenden Daten und den Variablen umgehen.\n\n\n\n\nTabelle 44.1— Auszug aus dem Datensatz gummi_tbl. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\n\n\n\n\n\ngender\nage\nheight\nsemester\n\n\n\n\nm\n35\n193\n10\n\n\nw\n21\n159\n6\n\n\nw\n21\n159\n6\n\n\nw\n36\n180\n10\n\n\nm\n22\n180\n3\n\n\nNA\nNA\nNA\nNA\n\n\nm\n22\n180\n3",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Sensitivitätsanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-sensitivity.html#das-modell",
    "href": "stat-modeling-sensitivity.html#das-modell",
    "title": "44  Sensitivitätsanalyse",
    "section": "44.3 Das Modell",
    "text": "44.3 Das Modell\nWir wollen jetzt als erstes das volle Modell schätzen. Das heißt wir packen alle Variablen in das Modell und rechnen dann die lineare Regression. Wir wollen herausfinden in wie weit das Alter, das Geschlecht und das Semester einen Einfluss auf die Körpergröße von Studierenden hat.\n\\[\nheight \\sim gender + age + semester\n\\]\nWir haben nichts an den Daten geändert und somit dient unser volles Modell als Benchmark für die anderen. Wenn sich einige Werte der Modellgüten im Vergleich zum vollen Modell ändern, dann wissen wir, dass etwas nicht stimmt.\n\nfit_full &lt;- lm(height ~ gender + age + semester, data = gummi_tbl)\n\nNeben dem vollen Modell rechnen wir auch noch das Nullmodel. Das Nullmodell beinhaltet nur den Intercept und sonst keine Einflussvariable. Wir wollen schauen, ob es überhaupt was bringt eine unserer Variablen in das Modell zu nehmen oder ob wir es auch gleich lassen können. Im Prinzip unsere Kontrolle für das Modellieren.\n\\[\nheight \\sim 1\n\\]\nIn R fitten wir das Nullmodell in dem wir keine Variablen mit in das Modell nehmen sondern nur eine 1 schreiben. Wir haben dann nur den Intercept mit in dem Modell und sonst nichts. Was wir schon aus den anderen Kapiteln wissen ist, dass das Nullmodell ein schlechtes Modell sein wird.\n\nfit_null &lt;- lm(height ~ 1, data = gummi_tbl)\n\nWir schauen uns die Modelle hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Sensitivitätsanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-detektion-von-ausreißer",
    "href": "stat-modeling-sensitivity.html#nach-der-detektion-von-ausreißer",
    "title": "44  Sensitivitätsanalyse",
    "section": "44.4 … nach der Detektion von Ausreißer",
    "text": "44.4 … nach der Detektion von Ausreißer\nTeilweise können wir eine Überprüfung auf Ausreißer nur auf einen Datensatz ohne fehlende Werte durchführen. Hier beißt sich dann die Katze in den Schwanz. Deshalb nutzen wir die Funktion diagnose_outlier(), die intern die fehlenden Werte entfernt. Das ist natürlich kein richtiges Vorgehen! Aber wir nutzen ja diesen Abschnitt nur als Beispiel. Du findest die Detektion von Ausreißern im Kapitel 41 beschrieben.\n\ndiagnose_outlier(gummi_tbl) \n\n# A tibble: 3 × 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean without_mean\n  &lt;chr&gt;            &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 age                 41           5.24         35.7      23.4         22.6 \n2 height               0           0           NaN       176.         176.  \n3 semester            67           8.56          8.78      2.55         1.89\n\n\nWir sehen, dass wir in der Variable age und semester nach der Funktion zu urteilen Ausreißer gefunden haben. Deshalb werden wir jetzt diese Ausreißer durch die Funktion imputate_outlier() entsprechend ersetzen. Mal schauen, ob wir damit eine substanzielle Änderung in der Modellierung erhalten.\n\ngummi_out_imp_tbl &lt;- gummi_tbl |&gt; \n  mutate(age = imputate_outlier(gummi_tbl, age, method = \"capping\"),\n         semester = imputate_outlier(gummi_tbl, semester, method = \"capping\"))\n\nNun modellieren wir noch mit unseren ersetzten und angepassten Daten die Körpergröße und erhalten den Modellfit zurück. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.\n\nfit_outlier &lt;- lm(height ~ gender + age + semester, data = gummi_out_imp_tbl)\n\nWir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Sensitivitätsanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-imputation-von-fehlenden-werten",
    "href": "stat-modeling-sensitivity.html#nach-der-imputation-von-fehlenden-werten",
    "title": "44  Sensitivitätsanalyse",
    "section": "44.5 … nach der Imputation von fehlenden Werten",
    "text": "44.5 … nach der Imputation von fehlenden Werten\nNehmen wir wieder den Gummibärechendatensatz von neuen und imputieren diesmal die fehlenden Werte mit einer univariaten Imputation. Wir machen uns hier nicht die Mühe ein multivariates Verfahren zu nutzen. Das könnte man tun, aber wir wollen hier ja nur den Weg aufzeigen, wie wir den Vergleich der Modelle zur Sensitivitätsanalyse durchführen. Du findest die Imputation von fehlenden Werten im Kapitel 43 beschrieben.\nIn unserem Fall imputieren wir alle numerischen Variablen mit dem Mittelwert und die kategoriale Variable mit der Methode rpart. Damit haben wir dann keine fehlenden Werte mehr in den Daten und somit sollte das jetzt auch unserer größter Datensatz für die lineare Regression sein. Nicht vergessen, sobald wir einen fehlenden Wert bei einer Variable in einem Modell haben, fällt die ganze Beobachtung aus dem Modell heraus.\n\ngummi_imp_tbl &lt;- gummi_tbl |&gt; \n  mutate(age = imputate_na(gummi_tbl, age, method = \"mean\"),\n         gender = imputate_na(gummi_tbl, gender, method = \"rpart\"),\n         height = imputate_na(gummi_tbl, height, method = \"median\"),\n         semester = imputate_na(gummi_tbl, semester, method = \"mode\"))\n\nDann rechnen wir noch schnell das Modell für die imputierten Daten. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.\n\nfit_imp &lt;- lm(height ~ gender + age + semester, data = gummi_imp_tbl)\n\nWir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Sensitivitätsanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-variablen-selektion",
    "href": "stat-modeling-sensitivity.html#nach-der-variablen-selektion",
    "title": "44  Sensitivitätsanalyse",
    "section": "44.6 … nach der Variablen Selektion",
    "text": "44.6 … nach der Variablen Selektion\nFür die Variablenselektion machen wir es uns sehr einfach. Wir müssen ja nur eine Spalte aus den Daten werfen, mehr ist ja Variablenselektion auch nicht. Wir machen dort nur eine algorithmengetriebene Auswahl. In diesem Fall entscheide ich einfach zufällig welche Variable aus dem Modell muss. Du findest die Variablen Selektion im Kapitel 42 beschrieben. Somit nehmen wir an, wir hätten eine Variablenselektion durchgeführt und die Variable semester aus dem Modell entfernt.\n\nfit_var_select &lt;- lm(height ~ gender + age, data = gummi_tbl)\n\nAuch dieses Modell schauen wir nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Sensitivitätsanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-sensitivity.html#modellvergleich",
    "href": "stat-modeling-sensitivity.html#modellvergleich",
    "title": "44  Sensitivitätsanalyse",
    "section": "44.7 Modellvergleich",
    "text": "44.7 Modellvergleich\nKommen wir zu dem eigentlichen Modellvergleich. In Tabelle 44.2 sehen wir den Modellvergleich aller fünf Modelle aus diesem Kapitel. Dazu nutzen wir die Funktion modelsummary() aus dem R Paket {modelsummary}. Wir vergleichen die Modelle untereinander aber vor allem mit dem vollen Modell. Das volle Modell basiert ja auf den ursprünglichen nicht veränderten Daten. Den Intercept können wir erstmal ignorieren. Spannend ist, dass sich der Effekt von gender auf die Körpergröße durch die Imputation um eine Einheit ändert. Der Effekt des Alters verfünffacht sich durch die Outlieranpassung und verdoppelt sich durch die Imputation. Durch die Imputation wird der Effekt des Semesters abgeschwächt.\nWenn wir auf das \\(R^2_{adj}\\) schauen, dann haben wir eine Verschlechterung durch die Imputation. Sonst bleibt der Wert mehr oder minder konstant. Das ist ein gutes Zeichen, dass wir unser Modell nicht vollkommen an die Wand gefahren haben durch unsere Änderung der Daten. Das \\(AIC\\) wird folglich für die Imputationsdaten sehr viel schlechter und nähert sich dem Nullmodell an. Das ist wirklcih kein gutes Zeichen für die Imputation. Da haben wir mehr kaputt als heile gemacht. Wir sehen keinen Efdekt bei dem Fehler \\(RMSE\\), der noch nach dem Fit des Modell übrig bleibt. Aber das kann passieren. Nicht jede Maßzahl muss sich auch ändern. Deshalb haben wir ja mehrere Maßzahlen vorliegen.\n\nmodelsummary(lst(\"Null Modell\" = fit_null,\n                 \"Volles Modell\" = fit_full,\n                 \"Outlier\" = fit_outlier,\n                 \"Imputation\" = fit_imp,\n                 \"Variablen Selektion\" = fit_var_select),\n             estimate  = \"{estimate}\",\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"))\n\n\n\nTabelle 44.2— Modellvergleich mit den fünf Modellen. Wir schauen in wie weit sich die Koeffizienten und Modelgüten für die einzelnen Modelle im direkten Vergleich zum vollen Modell verändert haben.\n\n\n\n\n\n\n\n\nNull Modell\n Volles Modell\nOutlier\n Imputation\n Variablen Selektion\n\n\n\n\n(Intercept)\n176.382\n184.858\n187.503\n183.285\n184.529\n\n\n\n[175.596, 177.168]\n[182.349, 187.367]\n[183.878, 191.128]\n[180.821, 185.749]\n[182.034, 187.024]\n\n\n\ns.e. = 0.400\ns.e. = 1.278\ns.e. = 1.846\ns.e. = 1.255\ns.e. = 1.271\n\n\n\nt = 440.643\nt = 144.646\nt = 101.557\nt = 145.997\nt = 145.226\n\n\n\np = &lt;0.001\np = &lt;0.001\np = &lt;0.001\np = &lt;0.001\np = &lt;0.001\n\n\ngenderw\n\n−14.825\n−14.796\n−13.317\n−14.788\n\n\n\n\n[−15.950, −13.700]\n[−15.918, −13.674]\n[−14.379, −12.255]\n[−15.913, −13.663]\n\n\n\n\ns.e. = 0.573\ns.e. = 0.571\ns.e. = 0.541\ns.e. = 0.573\n\n\n\n\nt = −25.875\nt = −25.892\nt = −24.616\nt = −25.804\n\n\n\n\np = &lt;0.001\np = &lt;0.001\np = &lt;0.001\np = &lt;0.001\n\n\nage\n\n−0.023\n−0.141\n−0.042\n−0.038\n\n\n\n\n[−0.126, 0.080]\n[−0.299, 0.018]\n[−0.144, 0.060]\n[−0.140, 0.064]\n\n\n\n\ns.e. = 0.052\ns.e. = 0.081\ns.e. = 0.052\ns.e. = 0.052\n\n\n\n\nt = −0.439\nt = −1.738\nt = −0.802\nt = −0.730\n\n\n\n\np = 0.661\np = 0.083\np = 0.423\np = 0.466\n\n\nsemester\n\n−0.260\n−0.253\n−0.057\n\n\n\n\n\n[−0.490, −0.030]\n[−0.513, 0.008]\n[−0.280, 0.167]\n\n\n\n\n\ns.e. = 0.117\ns.e. = 0.133\ns.e. = 0.114\n\n\n\n\n\nt = −2.223\nt = −1.905\nt = −0.496\n\n\n\n\n\np = 0.027\np = 0.057\np = 0.620\n\n\n\nNum.Obs.\n699\n697\n697\n783\n699\n\n\nR2\n0.000\n0.494\n0.496\n0.440\n0.490\n\n\nR2 Adj.\n0.000\n0.492\n0.494\n0.438\n0.489\n\n\nAIC\n5284.9\n4801.8\n4798.7\n5382.8\n4817.6\n\n\nBIC\n5294.0\n4824.6\n4821.4\n5406.1\n4835.8\n\n\nLog.Lik.\n−2640.449\n−2395.911\n−2394.337\n−2686.378\n−2404.795\n\n\nF\n\n\n\n204.030\n334.978\n\n\nRMSE\n10.58\n7.53\n7.51\n7.48\n7.55\n\n\n\n\n\n\n\n\n\n\n\nDas vergleichen von Modellen, die auf unterschiedlichen Daten basieren ist nicht anzuraten. Wir erhalten auch die passende Warnung von der Funktion compare_performance() aus dem R Paket {performance}. Dennoch hier einmal der Vergleich. Wir sehen, dass die Modelle mit der Ersetzung der Ausreißer und das volle Modell sich stark ähneln. Das selektierte Modell und das imputierte Modell fallen dagegen ab. Da wir ja hier nicht zeigen wollen, dass sich die Modelle unterscheiden, ist das Ergebnis ähnlich zu der Übersicht. Die Imputation hat so nicht funktioniert.\n\n\n# Comparison of Model Performance Indices\n\nName           | Model |    R2 | R2 (adj.) |   RMSE |  Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n---------------------------------------------------------------------------------------------------------------------------\nfit_outlier    |    lm | 0.496 |     0.494 |  7.510 |  7.532 |       0.828 |        0.828 |       0.828 |            99.69%\nfit_full       |    lm | 0.494 |     0.492 |  7.527 |  7.549 |       0.172 |        0.172 |       0.171 |            65.42%\nfit_var_select |    lm | 0.490 |     0.489 |  7.549 |  7.565 |    6.47e-05 |     6.56e-05 |    6.24e-04 |            56.21%\nfit_imp        |    lm | 0.440 |     0.438 |  7.478 |  7.497 |   1.22e-127 |    1.23e-127 |   9.11e-128 |            53.91%\nfit_null       |    lm | 0.000 |     0.000 | 10.575 | 10.583 |   2.17e-106 |    2.24e-106 |   1.98e-103 |        3.42e-102%\n\n\nWas ist das Fazit aus der Sensitivitätsanalyse für Arme? Nun wir konnten einmal sehen, dass wir auch mit einfachen Werkzeugen Modelle deskriptiv miteinander vergleichen können und dann einen Schluss über die Güte der Detektion von Ausreißern, der Imputation von fehlenden Werten oder aber der Variablenselektion treffen können. Denk immer dran, die Sensitivitätsanalyse findet nach einer sauberen Detektion, Imputation oder Selektion statt und soll nochmal sicherstellen, dass wir nicht künstliche Effekte der Algorithmen modellieren sondern die Effekte in den Daten sehen.",
    "crumbs": [
      "Grundlagen des Modellierens",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Sensitivitätsanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html",
    "href": "stat-modeling-preface.html",
    "title": "Statistisches Modellieren",
    "section": "",
    "text": "Verteilungsfamilien\nWenn wir gleich in R unsere Modelle rechnen, dann müssen wir uns für die Funktion lm() oder glm() entscheiden. Wir nutzen die Funktion lm(), wenn unser Outcome \\(y\\) einer Normalverteilung genügt und die Funktion glm() mit der Option family, wenn wir eine andere Verteilungsfamilie für unser Outcome benötigen. So werden zum Beispiel Zähldaten mit der Funktion glm() und der Option family = poisson für die Poissonverteilung ausgewertet.",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#das-regressionskreuz",
    "href": "stat-modeling-preface.html#das-regressionskreuz",
    "title": "Statistisches Modellieren",
    "section": "",
    "text": "Das zu betrachtende \\(x\\) ist eine Variable mit kontinuierlichen Zahlen (siehe 38.3.1 Kontinuierliches x)\nDas zu betrachtende \\(x\\) ist eine Variablen mit einem Faktor mit zwei Leveln (siehe 38.3.2 Kategorielles x mit 2 Leveln).\nDas zu betrachtende \\(x\\) ist eine Variablen mit einem Faktor mit mehr als zwei Leveln (siehe 38.3.3 Kategorielles x mit &gt;2 Leveln).\nDas zu betrachtende \\(x\\) ist eine Variable, die einen Block oder Cluster beschreibt (siehe 52  Lineare gemischte Modelle).\n\n\n\n\nDas zu betrachtende \\(y\\) folgt einer Normalverteilung bzw. entstammt einer Gaussian Vertreilungsfamilie. Wir wollen dann eine multiple Gaussian Regression rechen.\nDas zu betrachtende \\(y\\) folgt einer Poissonverteilung bzw. entstammt einer Poisson Vertreilungsfamilie. Wir wollen dann eine multiple Poisson Regression rechen.\nDas zu betrachtende \\(y\\) folgt einer Ordinalen- oder Multinominalenverteilung bzw. entstammt einer Ordinalen- oder Multinominalen Vertreilungsfamilie. Wir wollen dann eine multiple ordinale oder multinominale Regression rechen.\nDas zu betrachtende \\(y\\) folgt einer Binomialverteilung bzw. entstammt einer Binomialen Vertreilungsfamilie. Wir wollen dann eine multiple logistische Regression rechen.",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#gaussian",
    "href": "stat-modeling-preface.html#gaussian",
    "title": "Statistisches Modellieren",
    "section": "Gaussian",
    "text": "Gaussian\n\n\n\n\n\n\n\n\nAbbildung 2— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Normalverteilung (Gaussian) im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#poisson",
    "href": "stat-modeling-preface.html#poisson",
    "title": "Statistisches Modellieren",
    "section": "Poisson",
    "text": "Poisson\n\n\n\n\n\n\n\n\nAbbildung 2— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Poissonverteilung zu Zähldaten im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#beta",
    "href": "stat-modeling-preface.html#beta",
    "title": "Statistisches Modellieren",
    "section": "Beta",
    "text": "Beta\n\n\n\n\n\n\n\n\nAbbildung 3— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Betaverteilung zu Häufigkeiten im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#ordinal",
    "href": "stat-modeling-preface.html#ordinal",
    "title": "Statistisches Modellieren",
    "section": "Ordinal",
    "text": "Ordinal\n\n\n\n\n\n\n\n\nAbbildung 4— Visueller Zusammenhang eines geordneten, kategoriellen Outcomes (\\(y\\)) aus einer Ordinalverteilung wie Noten auf der Likert-Skala im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#binomial",
    "href": "stat-modeling-preface.html#binomial",
    "title": "Statistisches Modellieren",
    "section": "Binomial",
    "text": "Binomial\n\n\n\n\n\n\n\n\nAbbildung 5— Visueller Zusammenhang eines kategoriellen, binären Outcomes (\\(y\\)) aus einer Binomialverteilung im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 6— Visueller Zusammenhang eines kategoriellen, binären Outcomes (\\(y\\)) aus einer Binomialverteilung im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)) modelliert mit einem Probability model wie eine normalverteiltes Outcome. Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "stat-modeling-R.html",
    "href": "stat-modeling-R.html",
    "title": "45  Modellieren in R",
    "section": "",
    "text": "45.1 Marginal Effect Models\nR Paket {ggeffects}\nR Paket {modelbased}\nR Paket {marginaleffects}",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#modellieren-in-r",
    "href": "stat-modeling-R.html#modellieren-in-r",
    "title": "45  Modellieren in R",
    "section": "45.2 Modellieren in R",
    "text": "45.2 Modellieren in R\nDas R Paket {tidymodels}\nTidy Modeling with R\nVeraltet aber manchmal ganz nützlich das R Paket {modelr}\nR Paket {parsnip}",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#visualisierung",
    "href": "stat-modeling-R.html#visualisierung",
    "title": "45  Modellieren in R",
    "section": "45.3 Visualisierung",
    "text": "45.3 Visualisierung\nR Paket {ggeffects}\nR Paket {gtsummary}\nDisplay univariate regression model results in table",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#genutzte-r-pakete",
    "href": "stat-modeling-R.html#genutzte-r-pakete",
    "title": "45  Modellieren in R",
    "section": "45.4 Genutzte R Pakete",
    "text": "45.4 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, emmeans, multcomp, conflicted)\nconflicts_prefer(dplyr::select)\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#daten",
    "href": "stat-modeling-R.html#daten",
    "title": "45  Modellieren in R",
    "section": "45.5 Daten",
    "text": "45.5 Daten\nWir",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#wilde-beispiele",
    "href": "stat-modeling-R.html#wilde-beispiele",
    "title": "45  Modellieren in R",
    "section": "45.6 Wilde Beispiele",
    "text": "45.6 Wilde Beispiele\n\ncutting_tbl &lt;- read_excel(\"data/multiple_outcomes.xlsx\") |&gt; \n  mutate(trt = as_factor(trt),\n         block = as_factor(block)) |&gt; \n  mutate_if(is.numeric, round, 2)\n\n\n\n\n\nTabelle 45.1— Auszug aus den Daten .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrt\nblock\nshoot\nleaf\nflower\nfruit\nca\ndrymatter\nfreshweight\nheight\n\n\n\n\ncontrol\n1\n0\n136\n33\n10\n0.41\n11.59\n85.37\n25\n\n\ncontrol\n2\n0\n157\n33\n9\n0.41\n10.43\n79.78\n22\n\n\ncontrol\n1\n1\n65\n10\n3\n0.4\n11.97\n78.35\n22\n\n\ncontrol\n2\n0\n88\n16\n4\n0.39\n11.24\n85.55\n27\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nnodium_5th\n2\n0\n137\n20\n3\n0.47\n11.11\n86.93\n28\n\n\nnodium_5th\n1\n0\n137\n23\n3\n0.43\n11.95\n89.68\n27\n\n\nnodium_5th\n2\n1\n114\n17\n9\n0.39\n11.21\n70.82\n24\n\n\nnodium_5th\n2\n0\n192\n42\n11\n0.42\n11.29\n66.23\n22\n\n\n\n\n\n\n\n\n\nfamily_lst &lt;- lst(shoot = binomial(),\n                  leaf = quasipoisson(),\n                  flower = quasipoisson(),\n                  fruit = quasipoisson(),\n                  ca = gaussian(), \n                  drymatter = gaussian(), \n                  freshweight = gaussian(), \n                  height = gaussian())\n\n\ncutting_long_tbl &lt;- cutting_tbl |&gt;\n  pivot_longer(cols = shoot:last_col(),\n               names_to = \"outcome\",\n               values_to = \"rsp\") |&gt;\n  arrange(outcome, trt, block)\n\n\ncutting_lst &lt;- cutting_long_tbl |&gt;\n  split(~outcome)\n\n\nglm_lst &lt;- cutting_lst %&gt;% \n  map2(family_lst, \n       ~glm(rsp ~ trt + block + trt:block, \n            data = .x, family = .y))  \n  \nglm_lst %&gt;% \n  map(pluck, \"family\")\n\n$ca\n\nFamily: binomial \nLink function: logit \n\n\n$drymatter\n\nFamily: quasipoisson \nLink function: log \n\n\n$flower\n\nFamily: quasipoisson \nLink function: log \n\n\n$freshweight\n\nFamily: quasipoisson \nLink function: log \n\n\n$fruit\n\nFamily: gaussian \nLink function: identity \n\n\n$height\n\nFamily: gaussian \nLink function: identity \n\n\n$leaf\n\nFamily: gaussian \nLink function: identity \n\n\n$shoot\n\nFamily: gaussian \nLink function: identity \n\nglm_lst %&gt;% \n  map(car::Anova)\n\n$ca\nAnalysis of Deviance Table (Type II tests)\n\nResponse: rsp\n          LR Chisq Df Pr(&gt;Chisq)\ntrt       0.222384  3     0.9739\nblock     0.003979  1     0.9497\ntrt:block 0.005906  3     0.9999\n\n$drymatter\nAnalysis of Deviance Table (Type II tests)\n\nResponse: rsp\n          LR Chisq Df Pr(&gt;Chisq)\ntrt         5.1410  3     0.1618\nblock       0.2084  1     0.6480\ntrt:block   2.0862  3     0.5547\n\n$flower\nAnalysis of Deviance Table (Type II tests)\n\nResponse: rsp\n          LR Chisq Df Pr(&gt;Chisq)    \ntrt        19.3416  3  0.0002323 ***\nblock       0.5678  1  0.4511477    \ntrt:block   1.7967  3  0.6156522    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$freshweight\nAnalysis of Deviance Table (Type II tests)\n\nResponse: rsp\n          LR Chisq Df Pr(&gt;Chisq)  \ntrt         3.6454  3     0.3024  \nblock       4.0375  1     0.0445 *\ntrt:block   4.9674  3     0.1742  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$fruit\nAnalysis of Deviance Table (Type II tests)\n\nResponse: rsp\n          LR Chisq Df Pr(&gt;Chisq)  \ntrt         9.9860  3    0.01869 *\nblock       0.8118  1    0.36759  \ntrt:block   1.7163  3    0.63332  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$height\nAnalysis of Deviance Table (Type II tests)\n\nResponse: rsp\n          LR Chisq Df Pr(&gt;Chisq)\ntrt        2.83465  3     0.4178\nblock      0.12598  1     0.7226\ntrt:block  0.94488  3     0.8146\n\n$leaf\nAnalysis of Deviance Table (Type II tests)\n\nResponse: rsp\n          LR Chisq Df Pr(&gt;Chisq)   \ntrt        15.5183  3   0.001423 **\nblock       1.2623  1   0.261207   \ntrt:block   1.6400  3   0.650353   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$shoot\nAnalysis of Deviance Table (Type II tests)\n\nResponse: rsp\n          LR Chisq Df Pr(&gt;Chisq)    \ntrt             17  3  0.0007067 ***\nblock            0  1  1.0000000    \ntrt:block        2  3  0.5724067    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nemm_lst &lt;- glm_lst %&gt;% \n  map(~emmeans(.x, specs = ~ trt, type = \"response\")) \n\nemm_lst %&gt;% \n  map(~contrast(.x, method = \"pairwise\", adjust = \"bonferroni\")) %&gt;% \n  map(as_tibble) %&gt;% \n  bind_rows(.id = \"outcome\")\n\n# A tibble: 48 × 11\n   outcome   contrast       odds.ratio     SE    df  null z.ratio p.value  ratio\n   &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 ca        control / str…      0.829 0.978    Inf     1 -0.159    1     NA    \n 2 ca        control / nod…      1.30  1.59     Inf     1  0.219    1     NA    \n 3 ca        control / nod…      0.780 0.918    Inf     1 -0.211    1     NA    \n 4 ca        strong / nodi…      1.57  1.90     Inf     1  0.376    1     NA    \n 5 ca        strong / nodi…      0.941 1.10     Inf     1 -0.0523   1     NA    \n 6 ca        nodium_3rd / …      0.598 0.719    Inf     1 -0.428    1     NA    \n 7 drymatter control / str…     NA     0.0436   Inf     1 -0.425    1      0.981\n 8 drymatter control / nod…     NA     0.0479   Inf     1  1.24     1      1.06 \n 9 drymatter control / nod…     NA     0.0424   Inf     1 -0.930    1      0.960\n10 drymatter strong / nodi…     NA     0.0486   Inf     1  1.67     0.571  1.08 \n# ℹ 38 more rows\n# ℹ 2 more variables: estimate &lt;dbl&gt;, t.ratio &lt;dbl&gt;\n\nemm_lst %&gt;% \n  map(~cld(.x, Letters = letters, adjust = \"bonferroni\")) %&gt;% \n  map(as_tibble) %&gt;% \n  bind_rows(.id = \"outcome\") %&gt;% \n  select(outcome, trt, .group)  %&gt;% \n  print(n = 28)\n\n# A tibble: 32 × 3\n   outcome     trt        .group\n   &lt;chr&gt;       &lt;fct&gt;      &lt;chr&gt; \n 1 ca          nodium_3rd \" a\"  \n 2 ca          control    \" a\"  \n 3 ca          strong     \" a\"  \n 4 ca          nodium_5th \" a\"  \n 5 drymatter   nodium_3rd \" a\"  \n 6 drymatter   control    \" a\"  \n 7 drymatter   strong     \" a\"  \n 8 drymatter   nodium_5th \" a\"  \n 9 flower      strong     \" a \" \n10 flower      nodium_3rd \" ab\" \n11 flower      nodium_5th \"  b\" \n12 flower      control    \"  b\" \n13 freshweight strong     \" a\"  \n14 freshweight control    \" a\"  \n15 freshweight nodium_3rd \" a\"  \n16 freshweight nodium_5th \" a\"  \n17 fruit       strong     \" a\"  \n18 fruit       control    \" a\"  \n19 fruit       nodium_3rd \" a\"  \n20 fruit       nodium_5th \" a\"  \n21 height      strong     \" a\"  \n22 height      control    \" a\"  \n23 height      nodium_3rd \" a\"  \n24 height      nodium_5th \" a\"  \n25 leaf        strong     \" a \" \n26 leaf        nodium_3rd \" ab\" \n27 leaf        nodium_5th \" ab\" \n28 leaf        control    \"  b\" \n# ℹ 4 more rows",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#weitere-ideen",
    "href": "stat-modeling-R.html#weitere-ideen",
    "title": "45  Modellieren in R",
    "section": "45.7 Weitere Ideen",
    "text": "45.7 Weitere Ideen\nmap() für mehrere Endpunkte wie bei den Spinatdaten in Application gezeigt.\nmap2() mit Familie für glm zeigen, wenn wir unterschiedliche Outcomes haben.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#genutzte-r-pakete-1",
    "href": "stat-modeling-R.html#genutzte-r-pakete-1",
    "title": "45  Modellieren in R",
    "section": "45.8 Genutzte R Pakete",
    "text": "45.8 Genutzte R Pakete\nNeben den R Paketen, die wir in den jeweiligen Kapiteln brauchen, kommen noch folgende R Pakete immer wieder dran. Deshalb sind die R Pakete hier schon mal mit den jeweiligen Internetseiten aufgeführt.\n\nDas Buch Tidy Modeling with R gibt nochmal einen tieferen Einblick in das Modellieren in R. Wir immer, es ist ein Vorschlag aber kein Muss.\nDas R Paket {parameters} nutzen wir um die Parameter eines Modells aus den Fits der Modelle zu extrahieren. Teilweise sind die Standardausgaben der Funktionen sehr unübersichtich. Hier hilft das R Paket.\nDas R Paket {performance} hilft uns zu verstehen, ob die Modelle, die wir gefittet haben, auch funktioniert haben. In einen mathematischen Algorithmus können wir alles reinstecken, fast immer kommt eine Zahl wieder raus.\nDas R Paket {tidymodels} nutzen wir als das R Paket um mit Modellen umgehen zu können und eine Vorhersage neuer Daten zu berechnen. Das Paket {tidymodels} ist wie das Paket {tidyverse} eine Sammlung an anderen R Paketen, die wir brauchen werden.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#konfidenzintervall",
    "href": "stat-modeling-R.html#konfidenzintervall",
    "title": "45  Modellieren in R",
    "section": "45.9 95% Konfidenzintervall",
    "text": "45.9 95% Konfidenzintervall",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#prädiktionsintervall",
    "href": "stat-modeling-R.html#prädiktionsintervall",
    "title": "45  Modellieren in R",
    "section": "45.10 Prädiktionsintervall",
    "text": "45.10 Prädiktionsintervall\nQuantile Regression Forests for Prediction Intervals\nThe difference between prediction intervals and confidence intervals\nP-values for prediction intervals machen keinen Sinn\nHow NASA didn’t discover the hole in the ozone layer",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-R.html#generalisierung-von-lm-zu-glm-und-glmer",
    "href": "stat-modeling-R.html#generalisierung-von-lm-zu-glm-und-glmer",
    "title": "45  Modellieren in R",
    "section": "45.11 Generalisierung von lm() zu glm() und [g]lmer()",
    "text": "45.11 Generalisierung von lm() zu glm() und [g]lmer()\n\nDie Funktion lm() nutzen wir, wenn das Outcome \\(y\\) einer Normalverteilung folgt.\nDie Funktion glm() nutzen wir, wenn das Outcome \\(y\\) einer andere Verteilung folgt.\nDie Funktion lmer() nutzen wir, wenn das Outcome \\(y\\) einer Normalverteilung folgt und wir noch einen Block- oder Clusterfaktor vorliegen haben.\nDie Funktion glmer() nutzen wir, wenn das Outcome \\(y\\) einer andere Verteilung folgt und wir noch einen Block- oder Clusterfaktor vorliegen haben.\n\n\n\n\n\n\n\nAbbildung 45.2— Übersicht der Namen der Funktionen in R für das lm(), glm() und glmer().\n\n\n\nIn Abbildung 45.3 sehen wir wie wir den Namen einer Regression bilden. Zuerst entscheiden wir, ob wir nur ein \\(x\\) haben oder mehrere. Mit einem \\(x\\) sprechen wir von einem simplen Modell, wenn wir mehrere \\(x\\) haben wir ein multiples Modell. Im nächsten Schritt benennen wir die Verteilung für das Outcome \\(y\\). Dann müssen wir noch entscheiden, ob wir ein gemischtes Modell vorliegen haben, dann schreiben wir das hin. Sonst lassen wir den Punkt leer. Anschließend kommt noch lineares Modell hinten ran.\n\n\n\n\n\n\nAbbildung 45.3— Wie bilde ich den Namen einer Regression? Erst beschreiben wir das \\(x\\), dann das \\(y\\). Am Ende müssen wir noch sagen, ob wir ein gemischtes Modell vorliegen haben oder nicht.\n\n\n\n\n\n\nAbbildung 45.1— Verschiedene Ziele und Möglichkeiten des statistischen Modellierens. Grob können die Möglichkeiten in drei große thematische Zusammenhänge eingeteilt werden. (A) Kausales Modell – Wie verändert sich \\(y\\), wenn sich \\(x_1\\) ändert? Wie ist der numerische Zusammenhang zwischen \\(y\\) und \\(x\\)? (B) Prädikitives Modell – Wenn \\(y\\) und \\(x\\) gemessen wurden, wie sehen dann die Werte von \\(y\\) für neue \\(x\\)-Werte aus? Können wir mit \\(x\\) die Werte in \\(y\\) vorhersagen? (C) Clusteranalyse – Haben wir in unseren Variablen \\(x_1\\) und \\(x_2\\) eine oder mehrere unbekannte Cluster vorliegen? Wie gehören die Beobachtungen gegeben \\(x_1\\) und \\(x_2\\) zusammen? [Zum Vergrößern anklicken]\nAbbildung 45.2— Übersicht der Namen der Funktionen in R für das lm(), glm() und glmer().\nAbbildung 45.3— Wie bilde ich den Namen einer Regression? Erst beschreiben wir das \\(x\\), dann das \\(y\\). Am Ende müssen wir noch sagen, ob wir ein gemischtes Modell vorliegen haben oder nicht.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Modellieren in R</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gaussian.html",
    "href": "stat-modeling-gaussian.html",
    "title": "46  Gaussian Regression",
    "section": "",
    "text": "46.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom,\n               see, performance, scales, parameters,\n               olsrr, readxl, car, gtsummary, emmeans,\n               multcomp, conflicted)\nconflicts_prefer(dplyr::select)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Gaussian Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gaussian.html#annahmen-an-die-daten",
    "href": "stat-modeling-gaussian.html#annahmen-an-die-daten",
    "title": "46  Gaussian Regression",
    "section": "",
    "text": "Wenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 43 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 41 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 42 bei der Variablenselektion.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Gaussian Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gaussian.html#genutzte-r-pakete",
    "href": "stat-modeling-gaussian.html#genutzte-r-pakete",
    "title": "46  Gaussian Regression",
    "section": "46.2 Genutzte R Pakete",
    "text": "46.2 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom,\n               see, performance, scales, parameters,\n               olsrr, readxl, car, gtsummary, emmeans,\n               multcomp, conflicted)\nconflicts_prefer(dplyr::select)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Gaussian Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gaussian.html#daten",
    "href": "stat-modeling-gaussian.html#daten",
    "title": "46  Gaussian Regression",
    "section": "46.2 Daten",
    "text": "46.2 Daten\nIm Folgenden schauen wir uns die Daten eines Pilotprojektes zum Anbau von Kichererbsen in Brandenburg an. Wir haben an verschiedenen anonymisierten Bauernhöfen Kichererbsen angebaut und das Trockengewicht als Endpunkt bestimmt. Darüber hinaus haben wir noch andere Umweltparameter erhoben und wollen schauen, welche dieser Parameter einen Einfluss auf das Trockengewicht hat. In Kapitel 7.3 findest du nochmal mehr Informationen zu den Daten.\n\nchickpea_tbl &lt;- read_excel(\"data/chickpeas.xlsx\") \n\nIn der Tabelle 46.1 ist der Datensatz chickenpea_tbl nochmal als Ausschnitt dargestellt. Insgesamt haben wir \\(n = 95\\) Messungen durchgeführt. Wir sehen, dass wir verschiedene Variablen gemessen haben. Unter anderem, ob es geregent hat oder an welcher Stelle in Brandenburg die Messungen stattgefunden haben. Ebenso haben wir geschaut, ob ein Wald in der Nähe der Messung war oder nicht. Wir nehmen als Outcome \\(y\\) das normalverteilte Trockengewicht dryweight.\n\n\n\n\nTabelle 46.1— Auszug aus dem Daten zu den Kichererbsen in Brandenburg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntemp\nrained\nlocation\nno3\nfe\nsand\nforest\ndryweight\n\n\n\n\n25.26\nhigh\nnorth\n5.56\n4.43\n63\n&gt;1000m\n253.42\n\n\n21.4\nhigh\nnortheast\n9.15\n2.58\n51.17\n&lt;1000m\n213.88\n\n\n27.84\nhigh\nnortheast\n5.57\n2.19\n55.57\n&gt;1000m\n230.71\n\n\n24.59\nlow\nnorth\n7.97\n1.47\n62.49\n&gt;1000m\n257.74\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n25.47\nlow\nnorth\n6.92\n3.18\n64.55\n&lt;1000m\n268.58\n\n\n29.04\nlow\nnorth\n5.64\n2.87\n53.27\n&gt;1000m\n236.07\n\n\n24.11\nhigh\nnortheast\n4.31\n3.66\n63\n&lt;1000m\n259.82\n\n\n28.88\nlow\nnortheast\n7.92\n2\n65.75\n&gt;1000m\n274.75\n\n\n\n\n\n\n\n\nIm Folgenden werden wir die Daten nur für das Fitten eines Modells verwenden. In den anderen oben genannten Kapiteln nutzen wir die Daten dann anders.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Gaussian Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gaussian.html#fit-des-modells",
    "href": "stat-modeling-gaussian.html#fit-des-modells",
    "title": "46  Gaussian Regression",
    "section": "46.3 Fit des Modells",
    "text": "46.3 Fit des Modells\nWir rechnen jetzt den Fit für das vollständige Modell mit allen Variablen in dem Datensatz. Wir sortieren dafür einmal das \\(y\\) mit dryweight auf die linke Seite und dann die anderen Variablen auf die rechte Seite des ~. Wir haben damit unser Modell chickenpea_fit wie folgt vorliegen.\n\nchickenpea_fit &lt;- lm(dryweight ~ temp + rained + location + no3 + fe + sand + forest, \n                   data = chickpea_tbl)\n\nSoweit so gut. Wir können uns zwar das Modell mit der Funktion summary() anschauen, aber es gibt schönere Funktionen, die uns erlauben einmal die Performance des Modells abzuschätzen. Also zu klären, ob soweit alles geklappt hat und wir mit dem Modell weitermachen können.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Gaussian Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gaussian.html#performance-des-modells",
    "href": "stat-modeling-gaussian.html#performance-des-modells",
    "title": "46  Gaussian Regression",
    "section": "46.4 Performance des Modells",
    "text": "46.4 Performance des Modells\nDa ich die Daten selber gebaut habe, ist mir bekannt, dass das Outcome dryweight normalverteilt ist. Immerhin habe ich die Daten aus einer Normalverteilung gezogen. Manchmal will man dann doch Testen, ob das Outcome \\(y\\) einer Normalverteilung folgt. Das R Paket {oslrr} bietet hier eine Funktion ols_test_normality(), die es erlaubt mit allen bekannten statistischen Tests auf Normalverteilung zu testen. Wenn der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\), dann können wir die Nullhypothese, dass unsere Daten gleich einer Normalverteilung wären, ablehnen. Darüber hinaus bietet das R Paket {olsrr} eine weitreichende Diagnostik auf einem normalverteilten Outcome \\(y\\).\n\nols_test_normality(chickenpea_fit) \n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.9809         0.1806 \nKolmogorov-Smirnov        0.088          0.4287 \nCramer-von Mises          8.2878         0.0000 \nAnderson-Darling          0.5182         0.1837 \n-----------------------------------------------\n\n\nWir sehen, testen wir viel, dann kommt immer was signifikantes raus. Um jetzt kurz einen statistischen Engel anzufahren, wir nutzen wenn überhaupt den Shapiro-Wilk-Test oder den Kolmogorov-Smirnov-Test. Für die anderen beiden steigen wir jetzt hier nicht in die Theorie ab.\nNachdem wir die Normalverteilung nochmal überprüft haben wenden wir uns nun dem Wichtigen zu. Wir schauen jetzt auf die Varianz des Modells. Um zu überprüfen, ob das Modell funktioniert können wir uns den Anteil der erklärten Varianz anschauen. Wie viel erklären unsere \\(x\\) von der Varianz des Outcomes \\(y\\)? Wir betrachten dafür das Bestimmtheitsmaß \\(R^2\\). Da wir mehr als ein \\(x\\) vorliegen haben, nutzen wir das adjustierte \\(R^2\\). Das \\(R^2\\) hat die Eigenschaft immer größer und damit besser zu werden je mehr Variablen in das Modell genommen werden. Wir können dagegen Adjustieren und daher das \\(R^2_{adj}\\) nehmen.\n\nr2(chickenpea_fit)\n\n# R2 for Linear Regression\n       R2: 0.903\n  adj. R2: 0.894\n\n\nWir erhalten ein \\(R^2_{adj}\\) von \\(0.87\\) und damit erklärt unser Modell ca 87% der Varianz von \\(y\\) also unserem Trockengewicht. Das ist ein sehr gutes Modell. Je nach Anwendung sind 60% bis 70% erklärte Varianz schon sehr viel.\nIm nächsten Schritt wollen wir nochmal überprüfen, ob die Varianzen der Residuen auch homogen sind. Das ist eine weitere Annahme an ein gutes Modell. Im Prinzip überprüfen wir hier, ob unser Ourtcome auch wirklcih normalveteilt ist bzw. der Annahme der Normalverteilung genügt. Wir nutzen dafür die Funktion check_heteroscedasticity() aus dem R Paket {performance}.\n\ncheck_heteroscedasticity(chickenpea_fit)\n\nOK: Error variance appears to be homoscedastic (p = 0.512).\n\n\nAuch können wir uns einmal numerisch die VIF-Werte anschauen um zu sehen, ob Variablen mit anderen Variablen ungünstig stark korrelieren. Wir wollen ja nur die Korrelation des Modells, also die \\(x\\), mit dem Outcome \\(y\\) modellieren. Untereinander sollen die Variablen \\(x\\) alle unabhängig sein. Für können uns die VIF-Werte für alle kontinuierlichen Variablen berechnen lassen.\n\nvif(chickenpea_fit)\n\n             GVIF Df GVIF^(1/(2*Df))\ntemp     1.067042  1        1.032977\nrained   1.067845  1        1.033366\nlocation 1.119450  2        1.028611\nno3      1.062076  1        1.030571\nfe       1.135119  1        1.065420\nsand     1.072182  1        1.035462\nforest   1.070645  1        1.034720\n\n\nAlle VIF-Werte sind unter dem Threshold von 5 und damit haben wir hier keine Auffälligkeiten vorliegen.\nDamit haben wir auch überprüft, ob unsere Varianzen homogen sind. Also unsere Residuen annähend normalverteilt sind. Da unsere Daten groß genug sind, können wir das auch ohne weiteres Anwenden. Wenn wir einen kleineren Datensatz hätten, dann wäre die Überprüfung schon fraglicher. bei kleinen Fallzahlen funktioniert der Test auf Varianzheterogenität nicht mehr so zuverlässig.\nIn Abbildung 46.3 sehen wir nochmal die Visualisierung verschiedener Modellgütekriterien. Wir sehen, dass unsere beobachte Verteilung des Trockengewichts mit der vorhergesagten übereinstimmt. Ebenso ist der Residualplot gleichmäßig und ohne Struktur. Wir haben auch keine Ausreißer, da alle unsere Beobachtungen in dem gestrichelten, blauen Trichter bleiben. Ebenso zeigt der QQ-Plot auch eine approximative Normalverteilung der Residuen. Wir haben zwar leichte Abweichungen, aber die sind nicht so schlimm. Der Großteil der Punkte liegt auf der Diagonalen. Ebenso gibt es auch keine Variable, die einen hohen VIF-Wert hat und somit ungünstig mit anderen Variablen korreliert.\n\ncheck_model(chickenpea_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\", \"pp_check\", \"homogeneity\", \"vif\")) \n\n\n\n\n\n\n\nAbbildung 46.3— Ausgabe ausgewählter Modelgüteplots der Funktion check_model().",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Gaussian Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gaussian.html#interpretation-des-modells",
    "href": "stat-modeling-gaussian.html#interpretation-des-modells",
    "title": "46  Gaussian Regression",
    "section": "46.5 Interpretation des Modells",
    "text": "46.5 Interpretation des Modells\nNachdem wir nun sicher sind, dass das Modell unseren statistischen Ansprüchen genügt, können wir jetzt die Ergebnisse des Fits des Modells einmal interpretieren. Wir erhalten die Modellparameter über die Funktion model_parameters() aus dem R Paket {parameters}.\n\nchickenpea_fit |&gt; \n  model_parameters()\n\nParameter            | Coefficient |   SE |          95% CI | t(86) |      p\n----------------------------------------------------------------------------\n(Intercept)          |       -5.11 | 9.74 | [-24.48, 14.26] | -0.52 | 0.601 \ntemp                 |        2.37 | 0.22 | [  1.93,  2.80] | 10.80 | &lt; .001\nrained [low]         |        1.78 | 1.19 | [ -0.60,  4.15] |  1.49 | 0.140 \nlocation [northeast] |       -0.10 | 1.20 | [ -2.50,  2.29] | -0.09 | 0.931 \nlocation [west]      |        1.96 | 1.63 | [ -1.29,  5.20] |  1.20 | 0.234 \nno3                  |        1.49 | 0.41 | [  0.68,  2.29] |  3.67 | &lt; .001\nfe                   |        0.66 | 0.53 | [ -0.40,  1.72] |  1.24 | 0.220 \nsand                 |        3.04 | 0.12 | [  2.80,  3.27] | 25.63 | &lt; .001\nforest [&gt;1000m]      |       -3.02 | 1.14 | [ -5.29, -0.75] | -2.64 | 0.010 \n\n\nSchauen wir uns die einzelnen Zeilen aus der Ausgabe einmal in Ruhe an. Wir sind eigentlich nur an den Spalten Coefficient für das \\(\\beta\\) als Effekt der Variablen sowie der Spalte p als \\(p\\)-Wert für die Variablen interessiert. Wir testen immer als Nullhypothese, ob sich der Parameter von 0 unterscheidet.\n\n(Intercept) beschreibt den den \\(y\\)-Achsenabschnitt. Wir brauen den Intercept selten in der Interpretation. Wir nehmen hier erstmal hin, dass wir einen von 0 signifikant unterschiedlichen Intercept haben. Meist löschen wir den Intrcept auch aus der finalen Tabelle raus.\ntemp beschreibt den Effekt der Temperatur. Wenn die Temperatur um ein Grad ansteigt, dann erhalten wir \\(1.75\\) mehr Trockengewicht als Ertrag. Darüber hinaus ist der Effekt der Temperatur signifikant.\nrained [low] beschreibt den Effekt des Levels low des Faktors rained im Vergleich zum Level high. Daher haben wir bei wenig Regen einen um \\(1.33\\) höheren Ertrag als bei viel Regen.\nlocation [northeast] beschreibt den Effekt des Levels northeast zu dem Level north des Faktors location. Wir haben also einen \\(-1.38\\) kleineren Ertrag an Kichererbsen als im Norden von Brandenburg. Wenn du hier eine andere Sortierung willst, dann musst du mit der Funktion factor() die Level anders sortieren.\nlocation [west] beschreibt den Effekt des Levels west zu dem Level north des Faktors location. Wir haben also einen \\(-2.40\\) kleineren Ertrag an Kichererbsen als im Norden von Brandenburg. Wenn du hier eine andere Sortierung willst, dann musst du mit der Funktion factor() die Level anders sortieren.\nno3 beschreibt den Effekt von Nitrat im Boden. Wenn wir die Nitratkonzentration um eine Einheit erhöhen dann steigt der Ertrag um \\(1.11\\) an. Wir haben hier einen \\(p\\)-Wert von \\(0.012\\) vorliegen und können hier von einem signifkianten Effekt sprechen.\nfe beschreibt den Effekt des Eisens im Boden auf den Ertrag an Kichererbsen. Wenn die Konzentration von Eisen um eine Einheit ansteigt, so sinkt der Ertrag von Kichererbsen um \\(-0.72\\) ab.\nsand beschreibt den Anteil an Sand in der Bodenprobe. Wenn der Anteil an Sand um eine Einheit ansteigt, so steigt der Ertrag an Kichererbsen um \\(3.03\\) an. Dieser Effekt ist auch hoch signifikant. Kichererbsen scheinen sandigen Boden zu bevorzugen.\nforest [&gt;1000m] beschreibt die Nähe des nächsten Waldstückes als Faktor mit zwei Leveln. Daher haben wir hier einen höheren Ertrag von \\(0.67\\) an Kichererbsen, wenn wir weiter weg vom Wald messen &gt;1000 als Nahe an dem Wald &lt;1000.\n\nDas war eine Wand an Text für die Interpretation des Modells. Was können wir zusammenfassend mitnehmen? Wir haben drei signifikante Einflussfaktoren auf den Ertrag an Kichererbsen gefunden. Zum einen ist weniger Regen signifikant besser als viel Regen. Wir brauchen mehr Nitrat im Boden. Im Weiteren ist ein sandiger Boden besser als ein fetter Boden. Am Ende müssen wir noch schauen, was die nicht signifikanten Ergebnisse uns für Hinweise geben. Der Ort der Messung ist relativ unbedeutend. Es scheint aber so zu sein, dass im Norden mehr Ertrag zu erhalten ist. Hier müsste man einmal schauen, welche Betriebe hier vorliegen und wie die Bodenbeschaffenheit dort war. Im Weiteren sehen wir, dass anscheinend ein Abstand zum Wald vorteilhaft für den Ertrag ist. Hier könnte Wildfraß ein Problem gewesen sein oder aber zu viel Schatten. Auch hier muss man nochmal auf das Feld und schauen, was das konkrete Problem sein könnte. Hier endet die Statistik dann.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Gaussian Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gaussian.html#sec-mult-comp-gaussian-reg",
    "href": "stat-modeling-gaussian.html#sec-mult-comp-gaussian-reg",
    "title": "46  Gaussian Regression",
    "section": "46.6 Gruppenvergleich",
    "text": "46.6 Gruppenvergleich\nHäufig ist es ja so, dass wir das Modell für die Gaussian Regression nur schätzen um dann einen Gruppenvergleich zu rechnen. Das heißt, dass es uns interessiert, ob es einen Unterschied zwischen den Leveln eines Faktors gegeben dem Outcome \\(y\\) gibt. Wir nehmen hier einen ausgedachten Datensatz zu Katzen-, Hunde- und Fuchsflöhen. Dabei erstellen wir uns einen Datensatz mit mittleren Gewicht an Flöhen pro Tierart. Ich habe jetzt ein mittleres Gewicht von \\(20mg\\) bei den Katzenflöhen, eine mittleres Gewicht von \\(30mg\\) bei den Hundeflöhen und ein mittleres Gewicht von \\(10mg\\) bei den Fuchsflöhen gewählt. Wir generieren uns jeweils \\(20\\) Beobachtungen je Tierart. Damit haben wir dann einen Datensatz zusammen, den wir nutzen können um einmal die Ergebnisse eines Gruppenvergleiches zu verstehen zu können.\n\nset.seed(20231202)\nn_rep &lt;- 20\nflea_weight_tbl &lt;- tibble(animal = gl(3, n_rep, labels = c(\"cat\", \"dog\", \"fox\")),\n                         weight = c(rnorm(n_rep, 20, 1), \n                                         rnorm(n_rep, 30, 1), \n                                         rnorm(n_rep, 10, 1)))\n\nWenn du gerade hierher gesprungen bist, nochmal das simple Modell für unseren Gruppenvergleich unter einer Gaussian Regression. Wir haben hier nur einen Faktor animal mit in dem Modell. Am Ende des Abschnitts findest du dann noch ein Beispiel mit drei Faktoren zu Gewicht von Brokkoli.\n\nlm_fit &lt;- lm(weight ~ animal, data = flea_weight_tbl) \n\nEigentlich ist es recht einfach, wie wir anfangen. Wir rechnen jetzt als erstes die ANOVA. Hier müssen wir dann einmal den Test angeben, der gerechnet werden soll um die p-Werte zu erhalten. Dann nutze ich noch die Funktion model_parameters() um eine schönere Ausgabe zu erhalten.\n\nlm_fit |&gt; \n  anova() |&gt; \n  model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |       F |      p\n-------------------------------------------------------------\nanimal    |     3960.78 |  2 |     1980.39 | 2525.37 | &lt; .001\nResiduals |       44.70 | 57 |        0.78 |         |       \n\nAnova Table (Type 1 tests)\n\n\nIm Folgenden nutzen wir das R Paket {emmeans} um die mittleren Gewichte der Flöhe zu berechnen.\n\nemm_obj &lt;- lm_fit |&gt; \n  emmeans(~ animal)\nemm_obj\n\n animal emmean    SE df lower.CL upper.CL\n cat      20.2 0.198 57    19.77     20.6\n dog      30.1 0.198 57    29.66     30.5\n fox      10.2 0.198 57     9.76     10.6\n\nConfidence level used: 0.95 \n\n\nWir rechnen jetzt den paarweisen Vergleich für alle Tierarten und schauen uns dann an, was wir erhalten haben. Wie du gleich siehst, erhalten wir die Differenzen der Mittelwerte der Flohgewichte für die verschiedenen Tierarten. Hier also einmal die paarweisen Vergleiche, darunter dann gleich das compact letter display.\n\nemm_obj |&gt; \n  pairs(adjust = \"bonferroni\")\n\n contrast  estimate   SE df t.ratio p.value\n cat - dog     -9.9 0.28 57 -35.336  &lt;.0001\n cat - fox     10.0 0.28 57  35.732  &lt;.0001\n dog - fox     19.9 0.28 57  71.068  &lt;.0001\n\nP value adjustment: bonferroni method for 3 tests \n\n\nUnd fast am Ende können wir uns auch das compact letter display erstellen. Auch hier nutzen wir wieder die Funktion cld() aus dem R Paket {multcomp}. Hier erhälst du dann die Information über die mittleren Flohgewichte der jeweiligen Tierarten und ob sich die mittleren Flohgewichte unterscheidet. Ich nutze dann die Ausgabe von emmeans() um mir dann direkt das Säulendiagramm mit den Fehlerbalken und dem compact letter display zu erstellen. Mehr dazu dann im Kasten weiter unten zu dem Beispiel zu den Gewichten des Brokkoli.\n\nemm_obj |&gt;\n  cld(Letters = letters, adjust = \"none\")\n\n animal emmean    SE df lower.CL upper.CL .group\n fox      10.2 0.198 57     9.76     10.6  a    \n cat      20.2 0.198 57    19.77     20.6   b   \n dog      30.1 0.198 57    29.66     30.5    c  \n\nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nAuch hier sehen wir, dass sich alle drei Gruppen signifikant unterschieden, keine der Tierarten teilt sich einen Buchstaben, so dass wir hier von einem Unterschied zwischen den mittleren Flohgewichten der drei Tierarten ausgehen können.\nDamit sind wir einmal mit unserem Gruppenvergleich für die Gaussian Regression auf normalverteilte Daten durch. In dem Kapitel zu den Multiple Vergleichen oder Post-hoc Tests findest du dann noch mehr Inspirationen für die Nutzung von {emmeans}. Hier war es dann die Anwendung auf normalverteilte Dateb zusammen mit einem Faktor. Wenn du dir das Ganze nochmal an einem Beispiel für zwei Faktoren anschauen möchtest, dann findest du im folgenden Kasten ein Beispiel für die Auswertung von Brokkoli nach Gabe verschiedener Dosen eines Düngers und Zeitpunkten.\n\n\n\n\n\n\nAnwendungsbeispiel: Dreifaktorieller Vergleich für das Erntegewicht\n\n\n\nIm folgenden Beispiel schauen wir uns nochmal ein praktische Auswertung von einem agrarwissenschaftlichen Beispiel mit Brokkoli an. Wir haben uns in diesem Experiment verschiedene Dosen fert_amount von einem Dünger aufgebracht sowie verschiedene Zeitpunkte der Düngung fert_time berücksichtigt. Auch hier haben wir einige Besonderheiten in den Daten, da nicht jede Faktorkombination vorliegt. Wir ignorieren aber diese Probleme und rechnen einfach stumpf unseren Gruppenvergleich.\n\nbroc_tbl &lt;- read_excel(\"data/broccoli_weight.xlsx\") |&gt; \n  mutate(fert_time = factor(fert_time, levels = c(\"none\", \"early\", \"late\")),\n         fert_amount = as_factor(fert_amount),\n         block = as_factor(block)) |&gt;\n  select(-stem_hollowness) \n\nDann können wir auch schon die Gaussian Regression mit lm() rechnen.\n\nlm_fit &lt;- lm(weight ~ fert_time + fert_amount + fert_time:fert_amount + block, \n             data = broc_tbl) \n\nJetzt rechnen wir in den beiden folgenden Tabs einmal die ANOVA und dann auch den multiplen Gruppenvergleich mit {emmeans}. Da wir hier normalveteilte Daten haben, können wir dann einfach die Standardverfahren nehmen. Eventuell müssten wir bei dem Gruppenvergleich mit emmeans() nochmal für Varianzheterogenität adjustieren, aber da erfährst du dann mehr in dem Kapitel zu den Multiple Vergleichen oder Post-hoc Tests.\n\nANOVA mit anova()Gruppenvergleich mit emmeans()\n\n\nWir rechnen hier einmal die ANOVA und nutzen den \\(\\mathcal{X}^2\\)-Test für die Ermittelung der p-Werte. Wir müssen hier einen Test auswählen, da per Standardeinstellung kein Test gerechnet wird. Wir machen dann die Ausgabe nochmal schöner und fertig sind wir.\n\nlm_fit |&gt; \n  anova() |&gt; \n  model_parameters()\n\nParameter             | Sum_Squares |   df | Mean_Square |     F |      p\n-------------------------------------------------------------------------\nfert_time             |    2.44e+06 |    2 |    1.22e+06 | 44.12 | &lt; .001\nfert_amount           |    2.11e+06 |    2 |    1.05e+06 | 38.06 | &lt; .001\nblock                 |    7.21e+06 |    3 |    2.40e+06 | 86.73 | &lt; .001\nfert_time:fert_amount |    26973.84 |    2 |    13486.92 |  0.49 | 0.615 \nResiduals             |    4.28e+07 | 1544 |    27692.44 |       |       \n\nAnova Table (Type 1 tests)\n\n\nWir sehen, dass der Effekt der Düngerzeit und die Menge des Düngers signifikant ist, jedoch wir keinen signifikanten Einfluss durch die Interaktion haben. Wir haben aber also keine Interaktion vorliegen. Leider ist auch der Block signifikant, so dass wir eigentlich nicht über den Block mitteln sollten. Wir rechnen trotzdem die Analyse gemittelt über die Blöcke. Wenn du hier mehr erfahren möchtest, dann schaue dir das Beispiel hier nochmal im Kapitel zu dem linearen gemischten Modellen an.\n\n\nIm Folgenden rechnen wir einmal über alle Faktorkombinationen von fert_time und fert_amount einen Gruppenvergleich. Dafür nutzen wir die Option fert_time * fert_amount. Wenn du die Analyse getrennt für die Menge und den Zeitpunkt durchführen willst, dann nutze die Option fert_time | fert_amount. Dann adjustieren wir noch nach Bonferroni und sind fertig.\n\nemm_obj &lt;- lm_fit |&gt; \n  emmeans(~ fert_time * fert_amount) |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\")\nemm_obj\n\n fert_time fert_amount emmean    SE   df lower.CL upper.CL .group\n none      0              169 33.97 1544     77.1      260  a    \n late      150            400 11.25 1544    369.8      430   b   \n early     150            432 11.05 1544    401.8      461   bc  \n late      225            467 11.20 1544    436.9      497    cd \n early     225            497  7.97 1544    475.2      518     de\n late      300            506 11.40 1544    475.1      537     de\n early     300            517 11.38 1544    486.8      548      e\n early     0           nonEst    NA   NA       NA       NA       \n late      0           nonEst    NA   NA       NA       NA       \n none      150         nonEst    NA   NA       NA       NA       \n none      225         nonEst    NA   NA       NA       NA       \n none      300         nonEst    NA   NA       NA       NA       \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 7 estimates \nP value adjustment: bonferroni method for 21 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nDas emm_obj Objekt werden wir dann gleich einmal in {ggplot} visualisieren. Die emmean stellt den mittleren Gewicht des Brokkoli je Faktorkombination dar gemittelt über alle Blöcke. Das Mitteln über die Blöcke ist eher fragwürdig, da wir ja einen Effekt der Blöcke in der ANOVA gefunden hatten. Hier schauen wir dann nochmal auf das Beispiel im Kapitel zu den linearen gemischten Modellen. Dann können wir zum Abschluss auch das compact letter display anhand der Abbildung interpretieren.\n\n\n\nIn der Abbildung 52.17 siehst du das Ergebnis der Auswertung in einem Säulendiagramm. Wir sehen einen klaren Effekt der Düngezeitpunkte sowie der Düngermenge auf das Gewicht von Brokkoli. Wenn wir ein mittleres Gewicht von \\(500g\\) für den Handel erreichen wollen, dann erhalten wir das Zielgewicht nur bei einer Düngemenge von \\(300mg/l\\). Hier stellt sich dann die Frage, ob wir bei \\(225mg/l\\) und einem frühen Zeitpunkt der Düngung nicht auch genug Brokkoli erhalten. Das Ziel ist es ja eigentlich in einen Zielbereich zu kommen. Die Köpfe sollen ja nicht zu schwer und auch nicht zu leicht sein. Aber diese Frage und andere Fragen der biologischen Anwendung lassen wir dann hier einmal offen, denn das Beispiel soll ja nur ein Beispiel sein.\n\nemm_obj |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = fert_time, y = emmean, fill = fert_amount)) +\n  theme_minimal() + \n  labs(y = \"Mittleres Gewicht [g] des Brokkoli\", x = \"Düngezeitpunkt\",\n       fill = \"Düngemenge [mg/l]\") +\n  scale_y_continuous(breaks = seq(0, 500, by = 100)) +\n  geom_hline(yintercept = 500, size = 0.75, linetype = 2) +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_text(aes(label = .group, y = emmean + SE + 0.01),  \n            position = position_dodge(width = 0.9), vjust = -0.25) +\n  geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 46.4— Säulendigramm der mittleren Brokkoligewichte aus einer Gaussian Regression. Das lm()-Modell berechnet das mittler Gewicht des Brokkoli in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre. Wir haben noch ein Mindestgewicht von 500g ergänzt.\n\n\n\n\n\n\n\n\n\n\nAbbildung 46.1— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Normalverteilung (Gaussian) im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\nAbbildung 46.2— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Normalverteilung (Gaussian) im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\nAbbildung 46.3— Ausgabe ausgewählter Modelgüteplots der Funktion check_model().\nAbbildung 46.4— Säulendigramm der mittleren Brokkoligewichte aus einer Gaussian Regression. Das lm()-Modell berechnet das mittler Gewicht des Brokkoli in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre. Wir haben noch ein Mindestgewicht von 500g ergänzt.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Gaussian Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html",
    "href": "stat-modeling-poisson.html",
    "title": "47  Poisson Regression",
    "section": "",
    "text": "47.1 Annahmen an die Daten\nIm folgenden Kapitel zu der multiplen Poisson linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Poissonverteilung. Das ist hier sehr wichtig, denn wir wollen ja eine multiple Poisson lineare Regression rechnen.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 33 nochmal nachlesen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html#annahmen-an-die-daten",
    "href": "stat-modeling-poisson.html#annahmen-an-die-daten",
    "title": "47  Poisson Regression",
    "section": "",
    "text": "Unser gemessenes Outcome \\(y\\) folgt einer Poissonverteilung.\n\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 43 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 41 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 42 bei der Variablenselektion.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html#genutzte-r-pakete",
    "href": "stat-modeling-poisson.html#genutzte-r-pakete",
    "title": "47  Poisson Regression",
    "section": "47.2 Genutzte R Pakete",
    "text": "47.2 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom,\n               parameters, performance, MASS, pscl, see,\n               modelsummary, scales, emmeans, multcomp,\n               conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html#daten",
    "href": "stat-modeling-poisson.html#daten",
    "title": "47  Poisson Regression",
    "section": "47.3 Daten",
    "text": "47.3 Daten\nIm folgenden schauen wir uns ein Datenbeispiel mit Hechten an. Es handelt sich um langnasige Hechte in nordamerikanischen Flüssen. Wir haben uns insgesamt \\(n = 68\\) Flüsse einmal angesehen und dort die Anzahl an Hechten gezählt. Im Weiteren haben wir dann noch andere Flussparameter erhoben und fragen uns nun, welche dieser Parameter einen Einfluss auf die Anzahl an Hechten in den Flussarmen haben. In Kapitel 7.2 findest du nochmal mehr Informationen zu den Daten. Wir entfernen hier die Informationen zu den Flüssen, die brauchen wir in dieser Analyse nicht. Die Daten zu den langnasigen Hechten stammt von Salvatore S. Mangiafico - An R Companion for the Handbook of Biological Statistics. Besuche gerne mal seine Webseite, dort findest du auch andere tolle Beispiele zu statistischen Analysen.\n\nlongnose_tbl &lt;- read_csv2(\"data/longnose.csv\") |&gt; \n  select(-stream)\n\n\n\n\n\nTabelle 47.1— Auszug aus dem Daten zu den langnasigen Hechten.\n\n\n\n\n\n\nlongnose\narea\ndo2\nmaxdepth\nno3\nso4\ntemp\n\n\n\n\n13\n2528\n9.6\n80\n2.28\n16.75\n15.3\n\n\n12\n3333\n8.5\n83\n5.34\n7.74\n19.4\n\n\n54\n19611\n8.3\n96\n0.99\n10.92\n19.5\n\n\n19\n3570\n9.2\n56\n5.44\n16.53\n17\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n26\n1450\n7.9\n60\n2.96\n8.84\n18.6\n\n\n20\n4106\n10\n96\n2.62\n5.45\n15.4\n\n\n38\n10274\n9.3\n90\n5.45\n24.76\n15\n\n\n19\n510\n6.7\n82\n5.25\n14.19\n26.5\n\n\n\n\n\n\n\n\nIm Folgenden werden wir die Daten nur für das Fitten eines Modells verwenden. In den anderen oben genannten Kapiteln nutzen wir die Daten dann anders. In Abbildung 47.1 sehen wir nochmal die Verteilung der Anzahl der Hechte in den Flüssen.\n\nggplot(longnose_tbl, aes(longnose)) +\n  theme_minimal() +\n  geom_histogram()\n\n\n\n\n\n\n\nAbbildung 47.1— Histogramm der Verteilung der Hechte in den beobachteten Flüssen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html#fit-des-modells",
    "href": "stat-modeling-poisson.html#fit-des-modells",
    "title": "47  Poisson Regression",
    "section": "47.4 Fit des Modells",
    "text": "47.4 Fit des Modells\nIn diesem Abschnitt wollen wir verschiedene Modelle für Zähldaten schätzen. Die Poissonverteilung hat keinen eignen Parameter für die Streung wie die Normalverteilung. Die Poissonverteilung ist mit \\(\\mathcal{Pois}(\\lambda)\\) definiert und hat somit die Eigenschaft das die Varianz eins zu eins mit dem Mittelwert \\(\\lambda\\) der Poissonverteilung ansteigt. Es kann aber sein, dass wir in den Daten nicht diesen ein zu eins Zusammenhang von Mittelwert und Varianz vrliegen haben. Häufig ist die Varianz viel größer und steigt schneller an. Wenn die Varianz in Wirklichkeit sehr viel größer ist, dann würden wir die Varianz in unseren Modell unterschätzen.\n\nEin klassisches Poissonmodell glm(..., familiy = poisson) mit der Annahme keiner Overdisperison.\nEin Quasi-Poissonmodell glm(..., family = quasipoisson) mit der Möglichkeit der Berücksichtigung einer Overdispersion.\nEin negative Binomialmodell glm.nb(...) ebenfalls mit der Berücksichtigung einer Overdispersion.\n\nBeginnen wollen wir aber mit einer klassischen Poissonregression ohne die Annahme von einer Overdispersion in den Daten. Wir nutzen dafür die Funktion glm() und spezifizieren die Verteilungsfamilie als poisson. Wir nehmen wieder alle Variablen in das Modell auf der rechten Seite des ~. Auf der linken Seite des ~ kommt dann unser Outcome longnose was die Anzahl an Hechten erhält.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\nWir müssen für die Possionregression noch beachten, dass die Zähldaten von \\(0\\) bis \\(+\\infty\\) laufen. Damit wir normalverteilte Residuen erhalten und einen lineren Zusammenhang, werden wir das Modell auf dem \\(\\log\\)-scale fitten. Das heißt, wir werden den Zusammenhang von \\(y\\) und \\(x\\) logarithmieren. Wichtig ist hierbei der Zusammenhang. Wir transformieren nicht einfach \\(y\\) und lassen den Rest unberührt. Das führt dazu, dass wir am Ende die Koeffizienten der Poissonregression exponieren müssen. Das können die gängigen Funktionen, wir müssen das Exponieren aber aktiv durchführen. Deshalb hier schon mal erwähnt.\n\npoisson_fit &lt;- glm(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                    longnose_tbl, family = poisson)\n\nWir schauen uns die Ausgabe des Modells einmal mit der summary() Funktion an, da wir hier einmal händisch schauen wollen, ob eine Overdispersion vorliegt. Sonst könnten wir auch die Funktion model_parameters() nehmen. Die nutzen wir später für die Interpretation des Modells, hier wollen wir erstmal sehen, ob alles geklappt hat.\n\npoisson_fit |&gt; summary()\n\n\nCall:\nglm(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, family = poisson, data = longnose_tbl)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.234  -4.086  -1.662   1.771  14.362  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.564e+00  2.818e-01  -5.551 2.83e-08 ***\narea         3.843e-05  2.079e-06  18.480  &lt; 2e-16 ***\ndo2          2.259e-01  2.126e-02  10.626  &lt; 2e-16 ***\nmaxdepth     1.155e-02  6.688e-04  17.270  &lt; 2e-16 ***\nno3          1.813e-01  1.068e-02  16.974  &lt; 2e-16 ***\nso4         -6.810e-03  3.622e-03  -1.880   0.0601 .  \ntemp         7.854e-02  6.530e-03  12.028  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2766.9  on 67  degrees of freedom\nResidual deviance: 1590.0  on 61  degrees of freedom\nAIC: 1936.9\n\nNumber of Fisher Scoring iterations: 5\n\n\nWir schauen in die Summary-Ausgabe des Poissonmodells und sehen, dass dort steht, dass Dispersion parameter for poisson family taken to be 1. Wir modellieren also einen eins zu eins Zusammenhang von Mittelwert und Varianz. Wenn dieser Zusammenhang nicht in unseren Daten existiert, dann haben wir eine Overdispersion vorliegen.\nWir können die Overdispersion mit abschätzen indem wir die Residual deviance durch die Freiheitsgrade der Residual deviance teilen. Daher erhalten wir eine Overdispersion von \\(\\cfrac{1590.04}{61} \\approx 26.1\\). Damit haben wir eine eindeutige Overdispersion vorliegen. Damit steigt die Varianz in einem Verhältnis von ca. 1 zu 26. Wir können auch die Funktion check_overdispersion() aus dem R Paket {performance} nutzen um die Overdispersion zu berechnen. Die Funktion kann das schneller und ist auch in der Abfolge einer Analyse besser geeignet.\n\npoisson_fit |&gt; check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =   29.403\n  Pearson's Chi-Squared = 1793.599\n                p-value =  &lt; 0.001\n\n\nOverdispersion detected.\n\n\nWenn wir Overdispersion vorliegen haben und damit die Varianz zu niedrig schätzen, dann erhalten wir viel mehr signifikante Ergebnisse als es in den Daten zu erwarten wäre. Schauen wir uns nochmal die Parameter der Poissonverteilung und die \\(p\\)-Werte einmal an.\n\npoisson_fit |&gt; model_parameters()\n\nParameter   |  Log-Mean |       SE |         95% CI |     z |      p\n--------------------------------------------------------------------\n(Intercept) |     -1.56 |     0.28 | [-2.12, -1.01] | -5.55 | &lt; .001\narea        |  3.84e-05 | 2.08e-06 | [ 0.00,  0.00] | 18.48 | &lt; .001\ndo2         |      0.23 |     0.02 | [ 0.18,  0.27] | 10.63 | &lt; .001\nmaxdepth    |      0.01 | 6.69e-04 | [ 0.01,  0.01] | 17.27 | &lt; .001\nno3         |      0.18 |     0.01 | [ 0.16,  0.20] | 16.97 | &lt; .001\nso4         | -6.81e-03 | 3.62e-03 | [-0.01,  0.00] | -1.88 | 0.060 \ntemp        |      0.08 | 6.53e-03 | [ 0.07,  0.09] | 12.03 | &lt; .001\n\n\nIn der Spalte p finden wir die \\(p\\)-Werte für alle Variablen. Wir sehen, dass fast alle Variablen signifikant sind und das wir eine sehr niedrige Varianz in der Spalte SE sehen. Das heißt unser geschätzer Fehler ist sehr gering. Das ahnten wir ja schon, immerhin haben wir eine Overdisperson vorliegen. Das Modell ist somit falsch. Wir müssen uns ein neues Modell suchen, was Overdispersion berückscihtigen und modellieren kann.\nDie Quasi-Poisson Verteilung hat einen zusätzlichen, unabhänigen Parameter um die Varianz der Verteilung zu schätzen. Daher können wir die Overdispersion mit einer Quasi-Poisson Verteilung berückscihtigen. Wir können eine Quasi-Poisson Verteilung auch mit der Funktion glm() schätzen nur müssen wir als Verteilungsfamilie quasipoisson angeben.\n\nquasipoisson_fit &lt;- glm(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                        data = longnose_tbl, family = quasipoisson)\n\nNach dem Modellti können wir nochmal in der summary() Funktion schauen, ob wir die Overdispersion richtig berücksichtigt haben.\n\nquasipoisson_fit |&gt; summary()\n\n\nCall:\nglm(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, family = quasipoisson, data = longnose_tbl)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.234  -4.086  -1.662   1.771  14.362  \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -1.564e+00  1.528e+00  -1.024  0.30999   \narea         3.843e-05  1.128e-05   3.408  0.00116 **\ndo2          2.259e-01  1.153e-01   1.960  0.05460 . \nmaxdepth     1.155e-02  3.626e-03   3.185  0.00228 **\nno3          1.813e-01  5.792e-02   3.130  0.00268 **\nso4         -6.810e-03  1.964e-02  -0.347  0.73001   \ntemp         7.854e-02  3.541e-02   2.218  0.03027 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 29.40332)\n\n    Null deviance: 2766.9  on 67  degrees of freedom\nResidual deviance: 1590.0  on 61  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nAn der Zeile Dispersion parameter for quasipoisson family taken to be 29.403319 in der Summary-Ausgabe sehen wir, dass das Modell der Quasi-Possion Verteilung die Overdispersion korrekt berücksichtigt hat. Wir können uns nun einmal die Modellparameter anschauen. Die Interpretation machen wir am Ende des Kapitels.\n\nquasipoisson_fit |&gt; model_parameters()\n\nParameter   |  Log-Mean |       SE |        95% CI | t(61) |      p\n-------------------------------------------------------------------\n(Intercept) |     -1.56 |     1.53 | [-4.57, 1.41] | -1.02 | 0.306 \narea        |  3.84e-05 | 1.13e-05 | [ 0.00, 0.00] |  3.41 | &lt; .001\ndo2         |      0.23 |     0.12 | [ 0.00, 0.45] |  1.96 | 0.050 \nmaxdepth    |      0.01 | 3.63e-03 | [ 0.00, 0.02] |  3.18 | 0.001 \nno3         |      0.18 |     0.06 | [ 0.07, 0.29] |  3.13 | 0.002 \nso4         | -6.81e-03 |     0.02 | [-0.05, 0.03] | -0.35 | 0.729 \ntemp        |      0.08 |     0.04 | [ 0.01, 0.15] |  2.22 | 0.027 \n\n\nJetzt sieht unser Modell und die \\(p\\)-Werte zusammen mit dem Standardfehler SE schon sehr viel besser aus. Wir können also diesem Modell erstmal von der Seite der Overdispersion vertrauen.\nAm Ende wollen wir nochmal das Modell mit der negativen Binomialverteilung rechnen. Die negativen Binomialverteilung erlaubt auch eine Unabhängigkeit von dem Mittelwert zu der Varianz. Wir können hier auch für die Overdispersion adjustieren. Wir rechnen die negativen Binomialregression mit der Funktion glm.nb() aus dem R Paket {MASS}. Wir müssen keine Verteilungsfamilie angeben, die Funktion glm.nb() kann nur die negative Binomialverteilung modellieren.\n\nnegativebinomial_fit &lt;- glm.nb(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                               data = longnose_tbl)\n\nAuch hier schauen wir mit der Funktion summary() einmal, ob die Overdisprsion richtig geschätzt wurde oder ob hier auch eine Unterschätzung des Zusammenhangs des Mittelwerts und der Varianz vorliegt.\n\nnegativebinomial_fit |&gt; summary()\n\n\nCall:\nglm.nb(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, data = longnose_tbl, init.theta = 1.666933879, link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4601  -0.9876  -0.4426   0.4825   2.2776  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.946e+00  1.305e+00  -2.256 0.024041 *  \narea         4.651e-05  1.300e-05   3.577 0.000347 ***\ndo2          3.419e-01  1.050e-01   3.256 0.001130 ** \nmaxdepth     9.538e-03  3.465e-03   2.752 0.005919 ** \nno3          2.072e-01  5.627e-02   3.683 0.000230 ***\nso4         -2.157e-03  1.517e-02  -0.142 0.886875    \ntemp         9.460e-02  3.315e-02   2.854 0.004323 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.6669) family taken to be 1)\n\n    Null deviance: 127.670  on 67  degrees of freedom\nResidual deviance:  73.648  on 61  degrees of freedom\nAIC: 610.18\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.667 \n          Std. Err.:  0.289 \n\n 2 x log-likelihood:  -594.175 \n\n\nAuch hier sehen wir, dass die Overdispersion mit dem Parameter \\(\\theta\\) berücksichtigt wird. Wir können die Zahl \\(1.67\\) nicht direkt mit der Overdispersion aus einer Poissonregression verglechen, aber wir sehen dass das Verhältnis von Residual deviance zu den Freiheitsgraden mit \\(\\cfrac{73.65}{61} \\approx 1.20\\) fast bei 1:1 liegt. Wir könnten also auch eine negative Binomialverteilung für das Modellieren nutzen.\n\nnegativebinomial_fit |&gt; model_parameters()\n\nParameter   |  Log-Mean |       SE |         95% CI |     z |      p\n--------------------------------------------------------------------\n(Intercept) |     -2.95 |     1.31 | [-5.85, -0.10] | -2.26 | 0.024 \narea        |  4.65e-05 | 1.30e-05 | [ 0.00,  0.00] |  3.58 | &lt; .001\ndo2         |      0.34 |     0.11 | [ 0.11,  0.58] |  3.26 | 0.001 \nmaxdepth    |  9.54e-03 | 3.47e-03 | [ 0.00,  0.02] |  2.75 | 0.006 \nno3         |      0.21 |     0.06 | [ 0.10,  0.32] |  3.68 | &lt; .001\nso4         | -2.16e-03 |     0.02 | [-0.03,  0.03] | -0.14 | 0.887 \ntemp        |      0.09 |     0.03 | [ 0.03,  0.16] |  2.85 | 0.004 \n\n\n\n\nWie immer gibt es reichlich Tipps & Tricks welches Modell du nun nehmen solltest. How to deal with overdispersion in Poisson regression: quasi-likelihood, negative binomial GLM, or subject-level random effect? und das Tutorial Modeling Count Data. Auch ich mus immer wieder schauen, was am besten konkret in der Anwendung passen könnte und würde.\nWelches Modell nun das beste Modell ist, ist schwer zu sagen. Wenn du Overdisperion vorliegen hast, dann ist natürlich nur das Quasi-Poissonmodell oder das negative Binomialmodell möglich. Welche der beiden dann das bessere ist, hängt wieder von der Fragestellung ab. Allgemein gesprochen ist das Quasi-Poissonmodell besser wenn dich die Zusammenhänge von \\(y\\) zu \\(x\\) am meisten interessieren. Und das ist in unserem Fall hier die Sachlage. Daher gehen wir mit den Quasi-Poissonmdell dann weiter.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html#performance-des-modells",
    "href": "stat-modeling-poisson.html#performance-des-modells",
    "title": "47  Poisson Regression",
    "section": "47.5 Performance des Modells",
    "text": "47.5 Performance des Modells\nIn diesem kurzen Abschnitt wollen wir uns einmal anschauen, ob das Modell neben der Overdispersion auch sonst aus statistischer Sicht in Ordnung ist. Wir wollen ja mit dem Modell aus dem Fit quasipoisson_fit weitermachen. Also schauen wir uns einmal das pseudo-\\(R^2\\) für die Poissonregression an. Da wir es mit einem GLM zu tun haben, ist das \\(R^2\\) mit Vorsicht zu genießen. In einer Gaussianregression können wir das \\(R^2\\) als Anteil der erklärten Varianz durch das Modell interpretieren. Im Falle von GLM’s müssen wir hier vorsichtiger sein. In GLM’s gibt es ja keine Varianz sondern eine Deviance.\n\nr2_efron(quasipoisson_fit)\n\n[1] 0.3257711\n\n\nMit einem pseudo-\\(R^2\\) von \\(0.33\\) erklären wir ca. 33% der Varianz in der Anzahl der Hechte. Das ist zwar keine super gute Zahl, aber dafür, dass wir nur eine handvoll von Parametern erfasst haben, ist es dann auch wieder nicht so schlecht. Die Anzahl an Hechten wird sicherlich an ganz vielen Parametern hängen, wir konnten immerhin einige wichtige Stellschrauben vermutlich finden.\nIn Abbildung 47.2 schauen wir uns nochmal die Daten in den Modelgüteplots an. Wir sehen vorallem, dass wir vielelicht doch einen Ausreißer mit der Beobachtung 17 vorliegen haben. Auch ist der Fit nicht so super, wie wir an dem QQ-Plot sehen. Die Beobachtungen fallen in dem QQ-Plot nicht alle auf eine Linie. Auch sehen wir dieses Muster in dem Residualplot. Hiererwarten wir eine gerade blaue Linie und auch hier haben wir eventuell Ausreißer mit in den Daten.\n\ncheck_model(quasipoisson_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\", \"pp_check\", \"homogeneity\")) \n\n\n\n\n\n\n\nAbbildung 47.2— Ausgabe ausgewählter Modelgüteplots der Funktion check_model().",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html#interpretation-des-modells",
    "href": "stat-modeling-poisson.html#interpretation-des-modells",
    "title": "47  Poisson Regression",
    "section": "47.6 Interpretation des Modells",
    "text": "47.6 Interpretation des Modells\nUm die Effektschätzer einer Poissonregression oder aber einer Quasipoisson-Regression interpretieren zu können müssen wir uns einmal einen Beispieldatensatz mit bekannten Effekten zwischen den Gruppen bauen. Im Folgenden bauen wir uns einen Datensatz mit zwei Gruppen. Einmal einer Kontrollgruppe mit einer mittleren Anzahl an \\(15\\) und einer Behandlungsgruppe mit einer um \\(\\beta_1 = 10\\) höheren Anzahl. Wir haben also in der Kontrolle im Mittel eine Anzahl von \\(15\\) und in der Behandlungsgruppe eine mittlere Anzahl von \\(25\\).\n\nsample_size &lt;- 100\nlongnose_small_tbl &lt;- tibble(grp = rep(c(0, 1), each = sample_size),\n                             count = 15 + 10 * grp + rnorm(2 * sample_size, 0, 1)) |&gt;\n  mutate(count = round(count),\n         grp = factor(grp, labels = c(\"ctrl\", \"trt\")))\n\nIn Tabelle 47.2 sehen wir nochmal die Daten als Ausschnitt dargestellt.\n\n\n\n\nTabelle 47.2— How much is the fish? Der Datensatz über \\(n = 1000\\) Beobachtungen an dem wir überlegen wollen wie wir die Effektschätzer einer Poissonregression zu interpretieren haben.\n\n\n\n\n\n\ngrp\ncount\n\n\n\n\nctrl\n14\n\n\nctrl\n14\n\n\nctrl\n14\n\n\nctrl\n16\n\n\n…\n…\n\n\ntrt\n23\n\n\ntrt\n25\n\n\ntrt\n26\n\n\ntrt\n24\n\n\n\n\n\n\n\n\nDa sich die Tabelle schlecht liest hier nochmal der Boxplot in Abbildung 47.3. Wir sehen den Grupenunterschied von \\(10\\) sowie die unterschiedlichen mittleren Anzahlen für die Kontrolle und die Behandlung.\n\nggplot(longnose_small_tbl, aes(x = grp, y = count, fill = grp)) +\n  theme_minimal() +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\nggplot(data = longnose_small_tbl, aes(x = count, fill = grp)) +\n  theme_minimal() +\n  geom_density(alpha = 0.75) +\n  labs(x = \"\", y = \"\", fill = \"Gruppe\") +\n  scale_fill_okabeito() +\n  scale_x_continuous(breaks = seq(10, 30, by = 5), limits = c(10, 30)) \n\n\n\n\n\n\n\n\n\n\n\n(a) Verteilung der Werte als Boxplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Verteilung der Werte als Densityplot.\n\n\n\n\n\n\n\nAbbildung 47.3— How much is the fish? Der Boxplot über \\(n = 1000\\) Beobachtungen an dem wir überlegen wollen wie wir die Effektschätzer einer Poissonregression zu interpretieren haben.\n\n\n\n\nJetzt fitten wir einmal das simple Poissonmodell mit der Anzahl als Outcome und der Gruppe mit den zwei Leveln als \\(x\\). Wir pipen dann das Ergebnis des Fittes gleich in die Funktion model_parameters() weiter um die Ergebnisse des Modellierens zu erhalten.\n\nglm(count ~ grp, data = longnose_small_tbl, family = poisson) |&gt;\n  model_parameters(exponentiate = TRUE)\n\nParameter   |   IRR |   SE |         95% CI |      z |      p\n-------------------------------------------------------------\n(Intercept) | 14.91 | 0.39 | [14.17, 15.68] | 104.33 | &lt; .001\ngrp [trt]   |  1.68 | 0.06 | [ 1.58,  1.80] |  15.93 | &lt; .001\n\n\nAls erstes fällt auf, dass wir die Ausgabe des Modells exponieren müssen. Um einen linearen Zusamenhang hinzukriegen bedient sich die Poissonregression den Trick, das der Zusammenhang zwischen dem \\(y\\) und dem \\(x\\) transformiert wird. Wir rechnen unsere Regression nicht auf den echten Daten sondern auf dem \\(\\log\\)-scale. Daher müssen wir die Koeffizienten der Poissonregression wieder zurücktransfomieren, wenn wir die Koeffizienten interpretieren wollen. Das können wir mit der Option exponentiate = TRUE durchführen.\nGut soweit, aber was heißen den jetzt die Zahlen? Wir haben einen Intercept von \\(14.99\\) das entspricht der mittleren Anzahl in der Kontrollgruppe. Und was sagt jetzt die \\(1.67\\) vom Level trt des Faktors grp? Wenn wir \\(14.99 \\cdot 1.67\\) rechnen, dann erhalten wir als Ergebnis \\(25.03\\), also die mittlere Anzahl in der Behandlungsgruppe. Was sagt uns das jetzt aus? Wir erhalten aus der Poissonregression eine Wahrscheinlichkeit oder aber ein Risk Ratio. Wir können sagen, dass die Anzahl in der Behandlungsgruppe \\(1.67\\)-mal so groß ist wie in der Kontrollgruppe.\nSchauen wir uns nochmal das volle Modell an und interpretieren die Effekte der einzelnen Variablen.\n\nquasipoisson_fit |&gt; \n  model_parameters(exponentiate = TRUE) \n\nParameter   |  IRR |       SE |       95% CI | t(61) |      p\n-------------------------------------------------------------\n(Intercept) | 0.21 |     0.32 | [0.01, 4.11] | -1.02 | 0.306 \narea        | 1.00 | 1.13e-05 | [1.00, 1.00] |  3.41 | &lt; .001\ndo2         | 1.25 |     0.14 | [1.00, 1.57] |  1.96 | 0.050 \nmaxdepth    | 1.01 | 3.67e-03 | [1.00, 1.02] |  3.18 | 0.001 \nno3         | 1.20 |     0.07 | [1.07, 1.34] |  3.13 | 0.002 \nso4         | 0.99 |     0.02 | [0.95, 1.03] | -0.35 | 0.729 \ntemp        | 1.08 |     0.04 | [1.01, 1.16] |  2.22 | 0.027 \n\n\nSo schön auch die Funktion model_parameters() ist, so haben wir aber hier das Problem, dass wir den Effekt von area nicht mehr richtig sehen. Wir kriegen hier eine zu starke Rundung auf zwei Nachkommastellen. Wir nutzen jetzt mal die Funktion tidy() um hier Abhilfe zu leisten. Ich muss hier noch die Spalte estimate mit num(..., digits = 5) anpassen, damit du in der Ausgabe auf der Webseite auch die Nachkommastellen siehst.\n\nquasipoisson_fit |&gt; \n  tidy(exponentiate = TRUE, digits = 5) |&gt; \n  select(term, estimate, p.value) |&gt; \n  mutate(p.value = pvalue(p.value),\n         estimate = num(estimate, digits = 5))\n\n# A tibble: 7 × 3\n  term         estimate p.value\n  &lt;chr&gt;       &lt;num:.5!&gt; &lt;chr&gt;  \n1 (Intercept)   0.20922 0.310  \n2 area          1.00004 0.001  \n3 do2           1.25342 0.055  \n4 maxdepth      1.01162 0.002  \n5 no3           1.19879 0.003  \n6 so4           0.99321 0.730  \n7 temp          1.08171 0.030  \n\n\nSchauen wir uns die Effekte der Poissonregression einmal an und versuchen die Ergebnisse zu interpretieren. Dabei ist wichtig sich zu erinnern, dass kein Effekt eine 1 bedeutet. Wir schauen hier auf einen Faktor. Wenn wir eine Anzahl mal Faktor 1 nehmen, dann ändert sich nichts an der Anzahl.\n\n(Intercept) beschreibt den Intercept der Poissonregression. Wenn wir mehr als eine simple Regression vorliegen haben, wie in diesem Fall, dann ist der Intercept schwer zu interpretieren. Wir konzentrieren uns auf die Effekte der anderen Variablen.\narea, beschreibt den Effekt der Fläche. Steigt die Fläche um ein Quadratmeter an, so erhöht sich die Anzahl an Fischen um den \\(1.00001\\). Daher würde man hier eher sagen, erhöht sich die Fläche um jeweils 1000qm so erhöht sich die Anzahl an Fischen um den Faktor \\(1.1\\). Dann haben wir auch einen besser zu interpretierenden Effektschätzer. Die Signifikanz bleibt hier davon unbetroffen.\ndo2, beschreibt den Partzialdruck des Sauerstoffs. Steigt dieser um eine Einheit an, so sehen wie eine Erhöhung der Anzahl an Fischen um den Faktor \\(1.25\\). Der Effekt ist gerade nicht signifikant.\nmaxdepth, beschreibt die maximale Tiefe. Je tiefer ein Fluss, desto mehr Hechte werden wir beobachten. Der Effekt von \\(1.01\\) pro Meter Tiefe ist signifikant.\nno3, beschreibt den Anteil an Nitrat in den Flüssen. Je mehr Nitrat desto signifiant mehr Hechte werden wir beobachten. Hier steigt der Faktor auch um \\(1.20\\).\nso4, beschreibt den Schwefelgehalt und mit steigenden Schwefelgehalt nimmt die Anzahl an Fischen leicht ab. Der Effekt ist aber überhaupt nicht signifikant.\ntemp, beschreibt die Temperatur der Flüsse. Mit steigender Temperatur erwarten wir mehr Hechte zu beobachten. Der Effekt von \\(1.08\\) Fischen pro Grad Erhöhung ist signifikant.\n\nWas nehmen wir aus der Poissonregression zu den langnasigen Hechten mit? Zum einen haben die Fläche, die Tiefe und der Nitratgehalt einen signifikanten Einfluss auf die Anzahl an Hechten. Auch führt eine höhere Temperatur zu mehr gefundenen Hechten. Die erhöhte Temperatur steht etwas im Widerspuch zu dem Sauerstoffpartizaldruck. Denn je höher die Temperatur desto weniger Sauerstoff wird in dem Wasser gelöst sein. Auch scheint die Oberfläche mit der Tiefe korreliert. Allgemein scheinen Hechte große Flüsse zu mögen. Hier bietet sich also noch eine Variablenselektion oder eine Untersuchung auf Ausreißer an um solche Effekte nochmal gesondert zu betrachten.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html#zeroinflation",
    "href": "stat-modeling-poisson.html#zeroinflation",
    "title": "47  Poisson Regression",
    "section": "47.7 Zeroinflation",
    "text": "47.7 Zeroinflation\nSo eine Poissonregression hat schon einiges an Eigenheiten. Neben dem Problem der Overdispersion gibt es aber noch eine weitere Sache, die wir beachten müssen. Wir können bei einer Poissonregression auch eine Zeroinflation vorliegen haben. Das heißt, wir beobachten viel mehr Nullen in den Daen, als wir aus der Poissonverteilung erwarten würden. Es gibt also einen biologischen oder künstlichn Prozess, der uns Nullen produziert. Häufig wissen wir nicht, ob wir den Prozess, der uns die Nullen in den Daten produziert, auch abbilden. Das heißt, es kann sein, dass wir einfach nichts Zählen, weil dort nichts ist oder aber es gibt dafür einen Grund. Diesen Grund müssten wir dann irgendwie in unseren Daten erfasst haben, aber meistens haben wir das nicht.\nSchauen wir usn dafür einmal ein Datenbeispiel von Eidechsen in der Lüneburgerheide an. Wir haben Eidechsen lizard in zwei verschiedenen Habitaten grp gezählt. Einmal, ob die Eidechsen eher im offenen Gelände oder eher im bedeckten Gelände zu finden waren. Im Weiteren haben wir geschaut, ob der Boden keinen Regen erhalten hatte, trocken war oder gar feucht. Mit trocken ist hier eine gewisse Restfeuchte gemeint. Am Ende haben wir noch bestimmt, ob wir eher nah an einer Siedlung waren oder eher weiter entfernt. Du kannst dir den Daten satz in der Datei lizards.csv nochmal anschauen. In Tabelle 47.3 sind die Daten nochmal dargestellt.\n\n\n\n\nTabelle 47.3— Ausschnitt aus den Eidechsendaten für die zwei Habitate unter verschiedenen Feuchtigkeitsbedingungen und Nähe zur nächsten Siedlung.\n\n\n\n\n\n\ngrp\nrain\npop\nlizard\n\n\n\n\nopen\nno\nnear\n0\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n0\n\n\nopen\nno\nfar\n2\n\n\nopen\nno\nfar\n4\n\n\n\n\n\n\n\n\nIn Abbildung 47.4 sehen wir die Zähldaten der Eidechsen nochmal als Histogramm dargestellt. Wenn wir an einem Punkt keine Eidechsen gefunden haben, dann haben wir keine fehlenden Werte eingetragen, sondern eben, dass wir keine Eidechsen gezählt haben. Wir sehen das wir sehr viele Nullen in unseren Daten haben. Ein Indiz für eine Inflation an Nullen oder eben einer Zeroinflation.\n\nggplot(lizard_zero_tbl, aes(lizard)) +\n  theme_minimal() +\n  geom_histogram() +\n  labs(x = \"Anzahl der gefundenen Eidechsen\", y = \"Anzahl\") +\n  scale_x_continuous(breaks = 0:7)\n\n\n\n\n\n\n\nAbbildung 47.4— Histogramm der Verteilung der Hechte in den beobachteten Flüssen.\n\n\n\n\n\nUm zu überprüfen, ob wir eine Zeroinflation in den Daten vorliegen haben, werden wir erstmal eine ganz normale Poissonregression auf den Daten rechnen. Wir ignorieren auch eine potenzielle Overdispersion. Das schauen wir uns dann in den Daten später nochmal an.\n\nlizard_fit &lt;- glm(lizard ~ grp + rain + pop, data = lizard_zero_tbl,\n                  family = poisson)\n\nWie immer nutzen wir die Funktion model_parameters() um uns die exponierten Koeffizienten aus dem Modell wiedergeben zu lassen. Das Modell dient uns jetzt nur als Ausgangsmodell und wir werden das Poissonmodell jetzt nicht weiter tiefer verwenden.\n\nlizard_fit |&gt; model_parameters(exponentiate = TRUE)\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.29 | [0.60, 1.77] |  0.20 | 0.840 \ngrp [cover] | 1.88 | 0.46 | [1.18, 3.07] |  2.61 | 0.009 \nrain [dry]  | 0.31 | 0.09 | [0.17, 0.53] | -4.12 | &lt; .001\nrain [wet]  | 0.13 | 0.05 | [0.06, 0.28] | -4.98 | &lt; .001\npop [far]   | 2.41 | 0.61 | [1.49, 4.04] |  3.47 | &lt; .001\n\n\nWir sehen, dass wir in der Variable rain eine starke Reduzierung der Anzahl an Eidechsen sehen. Vielleicht ist dies eine Variable, die zu viele Nullen produziert. Auch hat die Variable pop, die für die Nähe an einer Siedlung kodiert, einen starken positiven Effekt auf unsere Anzahl an Eidechsen. Hier wollen wir also einmal auf eine Zeroinflation überprüfen. Wir nutzen dazu die Funktion check_zeroinflation() aus dem R Paket {performance}. Die Funktion läuft nur auf einem Modellfit.\n\ncheck_zeroinflation(lizard_fit)\n\n# Check for zero-inflation\n\n   Observed zeros: 31\n  Predicted zeros: 27\n            Ratio: 0.87\n\n\nDie Funktion gibt uns wieder, dass wir vermutlich eine Zeroinflation vorliegen haben. Das können wir aber Modellieren. Um eine Zeroinflation ohne Overdispersion zu modellieren nutzen wir die Funktion zeroinfl() aus dem R Paket {pscl}. Der erste Teil der Funktion ist leicht erkläret. Wir bauen uns wieder unswer Model zusammen, was wir fitten wollen. Dann kommt aber ein | und mit diesem Symbol | definieren wir, ob wir wissen, woher die Nullen kommen oder aber ob wir die Nullen mit einem zufälligen Prozess modellieren wollen.\nWenn wir das Modell in der Form y ~ f1 + f2 | 1 schreiben, dann nehmen wir an, dass das Übermaß an Nullen in unseren Daten rein zufällig entstanden sind. Wir haben keine Spalte in de Daten, die uns eine Erklärung für die zusätzlichen Nullen liefern würde.\nWir können auch y ~ f1 + f2 | x3 schreiben. Dann haben wir eine Variable x3 in den Daten von der wir glauben ein Großteil der Nullen herrührt. Wir könnten also in unseren Daten annehmen, dass wir den Überschuss an Nullen durch den Regen erhalten haben und damit über die Spalte rain den Exzess an Nullen modellieren.\nMan sollte immer mit dem einfachsten Modell anfangen, deshalb werden wir jetzt einmal ein Modell fitten, dass annimmt, dass die Nullen durch einen uns unbekannten Zufallsprozess entstanden sind.\n\nlizard_zero_infl_intercept_fit &lt;- zeroinfl(lizard ~ grp + pop + rain | 1, \n                                           data = lizard_zero_tbl) \n\nWir schauen uns das Modell dann wieder einmal an und sehen eine Zweiteilung der Ausgabe. In dem oberen Teil der Ausgabe wird unsere Anzahl an Eidechsen modelliert. In dem unteren Teil wird der Anteil der Nullen in den Daten modelliert. Daher können wir über Variablen in dem Zero-Inflation Block keine Aussagen über die Anzahl an Eidechsen treffen. Variablen tauchen nämlich nur in einem der beiden Blöcke auf.\n\nlizard_zero_infl_intercept_fit |&gt; \n  model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.31 | [0.60, 1.87] |  0.22 | 0.830 \ngrp [cover] | 2.03 | 0.51 | [1.25, 3.31] |  2.84 | 0.005 \npop [far]   | 2.59 | 0.67 | [1.56, 4.31] |  3.67 | &lt; .001\nrain [dry]  | 0.31 | 0.10 | [0.17, 0.56] | -3.82 | &lt; .001\nrain [wet]  | 0.14 | 0.06 | [0.06, 0.31] | -4.73 | &lt; .001\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |   SE |       95% CI |     z |     p\n--------------------------------------------------------------\n(Intercept) |       0.11 | 0.11 | [0.02, 0.74] | -2.26 | 0.024\n\n\nAls erstes beobachten wir einen größeren Effekt der Variable grp. Das ist schon mal ein spannender Effekt. An der Signifikanz hat scih nicht viel geändert. Wir werden am Ende des Kapitels einmal alle Modell für die Modellierung der Zeroinflation vergleichen.\nNun könnte es auch sein, dass der Effekt der vielen Nullen in unserer Variable rain verborgen liegt. Wenn es also regnet, dann werden wir viel weniger Eidechsen beoabchten. Nehmen wir also rain als ursächliche Variable mit in das Modell für die Zeroinflation.\n\nlizard_zero_infl_rain_fit &lt;- zeroinfl(lizard ~ grp + pop | rain, \n                                      data = lizard_zero_tbl)\n\nWieder schauen wir uns einmal die Ausgabe des Modells einmal genauer an.\n\nlizard_zero_infl_rain_fit |&gt; model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |    z |     p\n-------------------------------------------------------\n(Intercept) | 1.13 | 0.34 | [0.63, 2.03] | 0.42 | 0.677\ngrp [cover] | 1.60 | 0.42 | [0.95, 2.67] | 1.77 | 0.077\npop [far]   | 1.84 | 0.51 | [1.07, 3.18] | 2.20 | 0.028\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |     SE |          95% CI |     z |     p\n-------------------------------------------------------------------\n(Intercept) |       0.04 |   0.08 | [0.00,    2.09] | -1.60 | 0.109\nrain [dry]  |      27.93 |  59.03 | [0.44, 1758.19] |  1.58 | 0.115\nrain [wet]  |      83.69 | 178.35 | [1.28, 5452.98] |  2.08 | 0.038\n\n\nEs ändert sich einiges. Zum einen erfahren wir, dass der Regen anscheined doch viele Nullen in den Daten produziert. Wir haben ein extrem hohes \\(OR\\) für die Variable rain. Die Signifikanz ist jedoch eher gering. Wir haben nämlich auch eine sehr hohe Streuung mit den großen \\(OR\\) vorliegen. Au der anderen Seite verlieren wir jetzt auch die Signifikanz von unseren Habitaten und dem Standort der Population. Nur so mäßig super dieses Modell.\nWir können jetzt natürlich auch noch den Standort der Population mit in den Prozess für die Entstehung der Nullen hineinnehmen. Wir schauen uns dieses Modell aber nicht mehr im Detail an, sondern dann nur im Vergleich zu den anderen Modellen.\n\nlizard_zero_infl_rain_pop_fit &lt;- zeroinfl(lizard ~ grp | rain + pop, \n                                          data = lizard_zero_tbl)\n\nDie Gefahr besteht immer, das man sich an die Wand modelliert und vor lauter Modellen die Übersicht verliert. Neben der Zeroinflation müssen wir ja auch schauen, ob wir eventuell eine Overdispersion in den Daten vorliegen haben. Wenn das der Fall ist, dann müsen wir nochmal überlegen, was wir dann machen. Wir testen nun auf Ovrdisprsion in unserem ursprünglichen Poissonmodell mit der Funktion check_overdispersion().\n\ncheck_overdispersion(lizard_fit)\n\n# Overdispersion test\n\n       dispersion ratio =  1.359\n  Pearson's Chi-Squared = 74.743\n                p-value =  0.039\n\n\nTja, und so erfahren wir, dass wir auch noch Overdispersion in unseren Daten vorliegen haben. Wir müsen also beides Modellieren. Einmal modellieren wir die Zeroinflation und einmal die Overdispersion. Wir können beides in einem negativen binominalen Modell fitten. Auch hier hilft die Funktion zeroinfl() mit der Option dist = negbin. Mit der Option geben wir an, dass wir eine negative binominal Verteilungsfamilie wählen. Damit können wir dann auch die Ovrdispersion in unseren Daten modellieren.\n\nlizard_zero_nb_intercept_fit &lt;- zeroinfl(lizard ~ grp + rain + pop | 1, \n                                         dist = \"negbin\", data = lizard_zero_tbl)\n\nDann schauen wir usn einmal das Modell an. Zum einen sehen wir, dass der Effekt ähnlich groß ist, wie bei dem Intercept Modell der Funktion zeroinfl. Auch bleiben die Signifikanzen ähnlich.\n\nlizard_zero_nb_intercept_fit |&gt; model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.31 | [0.60, 1.87] |  0.22 | 0.830 \ngrp [cover] | 2.03 | 0.51 | [1.25, 3.31] |  2.84 | 0.005 \nrain [dry]  | 0.31 | 0.10 | [0.17, 0.56] | -3.82 | &lt; .001\nrain [wet]  | 0.14 | 0.06 | [0.06, 0.31] | -4.73 | &lt; .001\npop [far]   | 2.59 | 0.67 | [1.56, 4.31] |  3.67 | &lt; .001\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |   SE |       95% CI |     z |     p\n--------------------------------------------------------------\n(Intercept) |       0.11 | 0.11 | [0.02, 0.74] | -2.26 | 0.024\n\n\nNun haben wir vier Modelle geschätzt und wolen jetzt wissen, was ist das beste Modell. Dafür hilft usn dann eine Gegenüberstellung der Modelle mit der Funktion modelsummary(). Wir könnten die Modelle auch gegeneinander statistsich Testen, aber hier behalten wir uns einmal den beschreibenden Vergleich vor. In Tabelle 47.4 sehen wir einmal die vier Modelle nebeneinander gestellt. Für eine bessere Übrsicht, habe ich aus allen Modellen den Intercept entfernt.\n\nmodelsummary(lst(\"ZeroInfl Intercept\" = lizard_zero_infl_intercept_fit,\n                 \"ZeroInfl rain\" = lizard_zero_infl_rain_fit,\n                 \"ZeroInfl rain+pop\" = lizard_zero_infl_rain_pop_fit,\n                 \"NegBinom intercept\" = lizard_zero_nb_intercept_fit),\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"),\n             coef_omit = \"Intercept\", \n             exponentiate = TRUE)\n\n\n\nTabelle 47.4— Modellvergleich mit den vier Modellen. Wir schauen in wie weit sich die Koeffizienten und Modelgüten für die einzelnen Modelle im direkten Vergleich zum vollen Modell verändert haben.\n\n\n\n\n\n\n\n\n ZeroInfl Intercept\n ZeroInfl rain\n ZeroInfl rain+pop\nNegBinom intercept\n\n\n\n\ncount_grpcover\n2.031\n1.595\n1.611\n2.031\n\n\n\n[1.245, 3.313]\n[0.951, 2.675]\n[0.912, 2.845000e+00]\n[1.245, 3.313]\n\n\n\ns.e. = 0.507\ns.e. = 0.421\ns.e. = 0.468\ns.e. = 0.507\n\n\n\nt = 2.839\nt = 1.771\nt = 1.642\nt = 2.839\n\n\n\np = 0.005\np = 0.077\np = 0.101\np = 0.005\n\n\ncount_popfar\n2.591\n1.844\n\n2.591\n\n\n\n[1.558, 4.310]\n[1.069, 3.183]\n\n[1.558, 4.311]\n\n\n\ns.e. = 0.673\ns.e. = 0.513\n\ns.e. = 0.673\n\n\n\nt = 3.667\nt = 2.199\n\nt = 3.667\n\n\n\np = &lt;0.001\np = 0.028\n\np = &lt;0.001\n\n\ncount_raindry\n0.308\n\n\n0.308\n\n\n\n[0.168, 0.564]\n\n\n[0.168, 0.564]\n\n\n\ns.e. = 0.095\n\n\ns.e. = 0.095\n\n\n\nt = −3.816\n\n\nt = −3.816\n\n\n\np = &lt;0.001\n\n\np = &lt;0.001\n\n\ncount_rainwet\n0.135\n\n\n0.135\n\n\n\n[0.059, 0.310]\n\n\n[0.059, 0.310]\n\n\n\ns.e. = 0.057\n\n\ns.e. = 0.057\n\n\n\nt = −4.726\n\n\nt = −4.726\n\n\n\np = &lt;0.001\n\n\np = &lt;0.001\n\n\nzero_raindry\n\n27.933\n98.197\n\n\n\n\n\n[0.444, 1758.190]\n[0.000, 7.612076e+07]\n\n\n\n\n\ns.e. = 59.034\ns.e. = 679.419\n\n\n\n\n\nt = 1.576\nt = 0.663\n\n\n\n\n\np = 0.115\np = 0.507\n\n\n\nzero_rainwet\n\n83.692\n402.413\n\n\n\n\n\n[1.285, 5452.976]\n[0.000, 4.188183e+08]\n\n\n\n\n\ns.e. = 178.352\ns.e. = 2844.758\n\n\n\n\n\nt = 2.077\nt = 0.848\n\n\n\n\n\np = 0.038\np = 0.396\n\n\n\nzero_popfar\n\n\n0.148\n\n\n\n\n\n\n[0.022, 1.000000e+00]\n\n\n\n\n\n\ns.e. = 0.144\n\n\n\n\n\n\nt = −1.960\n\n\n\n\n\n\np = 0.050\n\n\n\nNum.Obs.\n60\n60\n60\n60\n\n\nR2\n0.620\n0.477\n0.454\n0.620\n\n\nR2 Adj.\n0.585\n0.449\n0.435\n0.585\n\n\nAIC\n157.3\n167.2\n167.4\n159.3\n\n\nBIC\n169.8\n179.8\n180.0\n173.9\n\n\nRMSE\n1.27\n1.27\n1.32\n1.27\n\n\n\n\n\n\n\n\n\n\n\nDie beiden Intercept Modelle haben die kleinsten \\(AIC\\)-Werte der vier Modelle. Darüber hinaus haben dann beide Modelle auch die höchsten \\(R^2_{adj}\\) Werte. Beide Modelle erklären also im Verhältnis viel Varianz mit 58.5%. Auch ist der \\(RMSE\\) Wert als Fehler bei beiden Modellen am kleinsten. Damit haben wir die Qual der Wahl, welches Modell wir nehmen. Ich würde das negative binominal Modell nehmen. Wir haben ins unseren Daten vermutlich eine Zeroinflation sowie eine Overdispersion vorliegen. Daher bietest es sich an, beides in einer negativen binominalen Regression zu berücksichtigen. Zwar sind die beiden Intercept Modelle in diesem Beispielfall von den Koeffizienten fast numerisch gleich, aber das hat eher mit dem reduzierten Beispiel zu tun, als mit dem eigentlichen Modell. In unserem Fall ist die Overdispersion nicht so extrem.\nWie sehe den unser negative binominal Modell aus, wenn wir mit dem Modell einmal die zu erwartenden Eidechsen vorhersagen würden? Auch das kann helfen um abzuschätzen, ob das Modelle einigermaßen funktioniert hat. Wir haben ja hier den Vorteil, dass wir nur mit kategorialen Daten arbeiten. Wir haben keine kontiniuerlichen Variablen vorliegen und darüber hinaus auch nicht so viele Variablen insgesamt.\nDaher bauen wir uns mit expand_grid() erstmal einen Datensatz, der nur aus den Faktorkombinationen besteht. Wir haben also nur eine Beobachtung je Faktorkombination. Danach nutzen wir die Daten einmal in der Funktion predict() um uns die vorhergesagten Eidechsen nach dem gefitten Modell wiedergeben zu lassen.\n\nnewdata_tbl &lt;- expand_grid(grp = factor(1:2, labels = c(\"open\", \"cover\")),\n                           rain = factor(1:3, labels = c(\"no\", \"dry\", \"wet\")),\n                           pop = factor(1:2, labels = c(\"near\", \"far\")))\n\npred_lizards &lt;- predict(lizard_zero_nb_intercept_fit, newdata = newdata_tbl) \n  \nnewdata_tbl &lt;- newdata_tbl |&gt; \n  mutate(lizard = pred_lizards)\n\nNachdem wir in dem Datensatz newdata_tbl nun die vorhergesagten Eidechsen haben, können wir uns jetzt in der Abbildung 47.5 die Zusammenhänge nochmal anschauen.\n\nggplot(newdata_tbl, aes(x = rain, y = lizard, colour = grp, group = grp)) +\n  theme_minimal() +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ pop) +\n  labs(x = \"Feuchtigkeit nach Regen\", y = \"Anzahl der gezählten Eidechsen\",\n       color = \"Gruppe\") +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 47.5— Scatterplot der vorhergesagten Eidechsen in den Habitaten (grp), der Feuchtigkeit des Bodens nach Regen und dem Abstand zur nächsten Ortschaft.\n\n\n\n\n\nWir erkennen, dass mit der Erhöhung der Feuchtigkeit die Anzahl an aufgefundenen Eidechsen sinkt. Der Effekt ist nicht mehr so stark, wenn es schon einmal geregnet hat. Ebenso macht es einen Unterschied, ob wir nahe einer Siedlung sind oder nicht. Grundsätzlich finden wir immer mehr Eidechsen in geschützten Habitaten als in offenen Habitaten.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html#sec-mult-comp-pois-reg",
    "href": "stat-modeling-poisson.html#sec-mult-comp-pois-reg",
    "title": "47  Poisson Regression",
    "section": "47.8 Gruppenvergleich",
    "text": "47.8 Gruppenvergleich\nHäufig ist es ja so, dass wir das Modell für die Poisson Regression nur schätzen um dann einen Gruppenvergleich zu rechnen. Das heißt, dass es uns interessiert, ob es einen Unterschied zwischen den Leveln eines Faktors gegeben dem Outcome \\(y\\) gibt. Da wir hier in unserem Beispiel zu den Flusshechten keine Gruppe drin haben, zeige ich dir das Prinzip einmal an einem ausgedachten Datensatz. Wir nehmen hier einen Datensatz zu Katzen-, Hunde- und Fuchsflöhen. Dabei erstellen wir uns einen Datensatz mit mittleren Anzahlen an Flöhen pro Tierart. Ich habe jetzt eine mittlere Flohanzahl von \\(10\\) Flöhen bei den Katzen, eine mittlere Anzahl von \\(30\\) Flöhen bei den Hunden und eine mittlere Anzahl von \\(10\\) Flöhen bei den Füchsen gewählt. Wir generieren uns jeweils \\(20\\) Beobachtungen je Tierart. Damit haben wir dann einen Datensatz zusammen, den wir nutzen können um einmal die Ergebnisse eines Gruppenvergleiches mit Zähldaten zu verstehen.\n\nset.seed(20231202)\nn_rep &lt;- 20\nflea_count_tbl &lt;- tibble(animal = gl(3, n_rep, labels = c(\"cat\", \"dog\", \"fox\")),\n                         count = round(c(rnorm(n_rep, 20, 1), \n                                         rnorm(n_rep, 30, 1), \n                                         rnorm(n_rep, 10, 1))))\n\nWenn du gerade hierher gesprungen bist, nochmal das simple Modell für unseren Gruppenvergleich unter einer Poisson Regression. Wir haben hier nur einen Faktor animal mit in dem Modell. Am Ende des Abschnitts findest du dann noch ein Beispiel mit zwei Faktoren zu Thripsen auf Apfelbäumen. Wir modellieren hier eigentlich nicht die Zähldaten an sich, sondern die Raten. Du wirst das aber gleich bei der Ausgabe von emmeans() sehen.\n\npois_fit &lt;- glm(count ~ animal, data = flea_count_tbl, family = \"poisson\") \n\nEigentlich ist es recht einfach, wie wir anfangen. Wir rechnen jetzt als erstes die ANOVA. Hier müssen wir dann einmal den Test angeben, der gerechnet werden soll um die p-Werte zu erhalten. Dann nutze ich noch die Funktion model_parameters() um eine schönere Ausgabe zu erhalten.\n\npois_fit |&gt; \n  anova(test = \"Chisq\") |&gt; \n  model_parameters(drop = \"NULL\")\n\nParameter | df | Deviance | df (error) | Deviance (error) |      p\n------------------------------------------------------------------\nanimal    |  2 |   205.42 |         57 |             3.20 | &lt; .001\n\nAnova Table (Type 1 tests)\n\n\nIm Folgenden nutzen wir das R Paket {emmeans} wie folgt. Wenn wir die mittleren Anzahlen benötigen, dann müssen wir die Option type = \"response\" verwenden. Sonst würdest du Werte auf der Link-Skala wiederbekommen, die dir hier nicht helfen. Ich nutze später die emmeans Ausgabe um ein Säulendiagramm mit den mittleren Anzahlen zu erstellen. Wir du gleich sehen wirst, werden wir aber nicht die Anzahlen vergleichen sondern die Raten in den jeweiligen Gruppen.\n\nemm_obj &lt;- pois_fit |&gt; \n  emmeans(~ animal, type = \"response\")\nemm_obj\n\n animal rate    SE  df asymp.LCL asymp.UCL\n cat    20.1 1.002 Inf     18.23      22.2\n dog    30.1 1.227 Inf     27.79      32.6\n fox    10.2 0.714 Inf      8.89      11.7\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n\nJetzt steht hier zwar rate aber was ist das denn nun? Dafür berechnen wir mal die Mittelwerte der Anzahlen für die drei Tierarten über die Funktion summarise(). Dann vergleichen wir einmal die Ausgaben und schauen, was wir in emmeans() berechnet haben.\n\nflea_count_tbl |&gt; \n  group_by(animal) |&gt; \n  summarise(mean_count = mean(count))\n\n# A tibble: 3 × 2\n  animal mean_count\n  &lt;fct&gt;       &lt;dbl&gt;\n1 cat          20.1\n2 dog          30.1\n3 fox          10.2\n\n\nWunderbar, es sind die Mittelwerte der Anzahlen, die in der Spalte rate in der Ausgabe von emmeans() stehen. Damit können wir dann arbeiten und die Ausgabe nutzen um ein Säulendigramm zu erstellen. Bitte schaue für die Umsetzung in das Beispiel am Ende des Abschnitts. Da zeige ich dir dann wie du ein Säulendigramm zu den Anzahlen von Thripsen auf Apfelbäumen erstellst.\nWenn du die mittleren Anzahlen pro Tierart jetzt für jede Tierart testen willst, dann erhälst du aber keinen Mittelwertsunterschied der Anzahlen, sondern eine Rate. Rechnen wir aber erstmal den paarweisen Vergleich und schauen uns dann an, was wir erhalten haben. Wir kriegen am Ende wiederum einen Vergleich von Wahrscheinlichkeiten, aber eben hier von Raten. Wie immer, es kommt darauf an, was du willst. Hier einmal die paarweisen Vergleiche, darunter dann gleich das compact letter display.\n\nemm_obj |&gt; \n  pairs(adjust = \"bonferroni\")\n\n contrast  ratio    SE  df null z.ratio p.value\n cat / dog 0.668 0.043 Inf    1  -6.269  &lt;.0001\n cat / fox 1.971 0.169 Inf    1   7.891  &lt;.0001\n dog / fox 2.951 0.239 Inf    1  13.358  &lt;.0001\n\nP value adjustment: bonferroni method for 3 tests \nTests are performed on the log scale \n\n\nOkay, jetzt haben wir statt Raten also eine Ratio, also ein Verhältnis von Raten. Das schauen wir uns doch gleich mal in dem Beispiel an. Wir rechnen also einmal die mittlere Anzahl für jede Tierart durch die entsprechende andere mittlere Anzahl der anderen Tierart. Wir erhalten also folgende Ratios der mittleren Anzahlen.\n\\[\n\\cfrac{cat}{dog} = \\cfrac{20.1}{30.1} = 0.67\n\\]\n\\[\n\\cfrac{cat}{fox} = \\cfrac{20.1}{10.2} = 1.97\n\\]\n\\[\n\\cfrac{dog}{fox} = \\cfrac{30.1}{10.2} = 2.95\n\\]\nWie interpretieren wir jetzt die Ratios? Eigentlich ist das intuitiver als man auf den ersten Blick denkt. Wir haben zum Beispiel fast doppelt so viele Flöhe bei Katzen wie bei Füchsen. Die Anzahl von Hundeflöhen ist fast dreimal so hoch wie die Anzahl an Fuchsflöhen. Wir haben fast nur halb so viele Flöhe auf Katzen gefunden als bei den Hunden. Damit können wir dann schon arbeiten und eine Aussage treffen.\nUnd fast am Ende können wir uns auch das compact letter display erstellen. Auch hier nutzen wir wieder die Funktion cld() aus dem R Paket {multcomp}. Hier erhälst du dann die Information über die mittlere Anzahl der jeweiligen Tierarten und ob sich die mittlere Anzahl unterscheidet. Ich nutze dann die Ausgabe von emmeans() um mir dann direkt das Säulendiagramm mit den Fehlerbalken und dem compact letter display zu erstellen. Mehr dazu dann im Kasten weiter unten zu dem Beispiel zu Thripsenanzahl auf Apfelbäumen.\n\nemm_obj |&gt;\n  cld(Letters = letters, adjust = \"none\")\n\n animal rate    SE  df asymp.LCL asymp.UCL .group\n fox    10.2 0.714 Inf      8.89      11.7  a    \n cat    20.1 1.002 Inf     18.23      22.2   b   \n dog    30.1 1.227 Inf     27.79      32.6    c  \n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \nTests are performed on the log scale \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nAuch hier sehen wir, dass sich alle drei Gruppen signifikant unterschieden, keine der Tierarten teilt sich einen Buchstaben, so dass wir hier von einem Unterschied zwischen den mittleren Anzahlen an Flöhen auf den drei Tierarten ausgehen können.\n\n\n\n\n\n\nOffset in einer Poisson Regression\n\n\n\nBei einem geplanten Experiment zählen wir meist in festgelegten Zeiteinheiten oder aber unser \\(n\\) ist vorgeben. Jetzt kann es aber sein, dass wir nicht mit festen Zeitintervallen oder aber einer Fallzahl zählen. Beispielsweise können wir die Anzahl der Baumarten in einem Wald zählen: Ereignisse (eng. event) wären Baumbeobachtungen, die Exposition (eng. exposure) wäre eine Flächeneinheit und die Rate wäre die Anzahl der Arten pro Flächeneinheit. Wir können die Sterberaten in geografischen Gebieten als die Anzahl der Todesfälle geteilt durch die Personenjahre modellieren. Allgemeiner ausgedrückt können Ereignisraten als Ereignisse pro Zeiteinheit berechnet werden, wobei das Beobachtungsfenster für jede Einheit variieren kann. In diesen Beispielen ist die Exposition jeweils eine Einheit Fläche, Personenjahre und Zeiteinheit. So sollten beispielsweise sechs Fälle innerhalb eines Jahres nicht den gleichen Wert haben wie sechs Fälle innerhalb von 10 Jahren. In der Poisson-Regression wird dies als Offset behandelt. Mehr dazu dann gerne im Wikipediartikel zu “Exposure” and offset oder aber die Antwort When to use an offset in a Poisson regression?.\nIn {emmeans} können wir Modelle mit Offset relativ einfach modellieren. Dazu nutzen wir die Funktion offset() in unserem glm() Modell. Aber zuerst einmal ein Spieldatensatz. Wir haben wir unseren Faktor Tierart und dann aber noch das Alter als Faktor mit zwei Stufen. Wir haben jetzt aber nicht immer die gleiche Anzahl an Hunden und Katzen sowie Füchsen ausgezählt. Unsere Anzahl an Flöhen basiert also auf einer anderen \\(n\\) Anzahl. Die Anzahl \\(n\\) modellieren wir dann als offset().\n\ntoy_count_tbl &lt;- tibble(n = c(500, 1200, 100, 400, 500, 300),\n                        animal = factor(rep(1:3,2), labels = c(\"cat\",\"dog\",\"fox\")),\n                        age = gl(2, 3),\n                        count = c(42, 37, 1, 101, 73, 14))\n\nWir nehmen die Anzahl an ausgezählten Tieren dann als Offset mit Logarithmus in das Modell. Deshalb steht dann da auch log(n) im Offset.\n\ntoy_count_fit &lt;- glm(count ~ animal + age + offset(log(n)), \n                     data = toy_count_tbl, family = \"poisson\")\n\nDann können wir uns einmal das Modell anschauen, wie {emmeans} den Vergleich rechnen würde.\n\nref_grid(toy_count_fit)\n\n'emmGrid' object with variables:\n    animal = cat, dog, fox\n    age = 1, 2\n    n = 500\nTransformation: \"log\" \n\n\nDer Offset ist also grob \\(log(500) \\approx 5.97\\). Das stimmt numerisch nicht hundertprozentig, hat aber noch mit einer internen Korrektur zu tun, die uns hier aber nicht weiter interessiert. Das Modell können wir dann wie gewohnt in emmeans() stecken und dann weiter rechnen. In dem Modell wird dann berücksichtigt, dass sich die Anzahlen für das Exposure, also dem Nenner der Raten, unterscheiden.\n\n\nDamit sind wir einmal mit unserem Gruppenvergleich für die Poisson Regression auf Zähldaten durch. In dem Kapitel zu den Multiple Vergleichen oder Post-hoc Tests findest du dann noch mehr Inspirationen für die Nutzung von {emmeans}. Hier war es dann die Anwendung auf Zähldaten zusammen mit einem Faktor. Wenn du dir das Ganze nochmal an einem Beispiel für zwei Faktoren anschauen möchtest, dann findest du im folgenden Kasten ein Beispiel für die Auswertung von Thripsen auf Apfelbäumen nach Gabe verschiedener Dosen eines Insektizids und Zeitpunkten.\n\n\n\n\n\n\nAnwendungsbeispiel: Zweifaktorieller Gruppenvergleich für Thripsenbefall\n\n\n\nIm folgenden Beispiel schauen wir uns nochmal ein praktische Auswertung von einem agrarwissenschaftlichen Beispiel mit jungen Apfelbäumen an. Wir haben uns in diesem Experiment verschiedene Dosen trt von einem Insektizid aufgebracht sowie verschiedene Startanzahlen von Raubmilben als biologische Alternative untersucht. Dann haben wir noch fünf Zeitpunkte bestimmt, an denen wir die Anzahl an Thripsen auf den Blättern gezählt haben. Wir haben nicht die Blätter per se gezählt sondern Fallen waagerecht aufgestellt. Dann haben wir geschaut, wie viele Thripsen wir über above und unter below von den Fallen gefunden haben. In unserem Fall beschränken wir uns auf die obere Anzahl an Thripsen und schauen uns auch nur die Behandlung mit dem Insektizid an.\n\ninsects_tbl &lt;- read_excel(\"data/insects_count.xlsx\") |&gt; \n  mutate(timepoint = factor(timepoint, labels = c(\"1 Tag\", \"4 Tag\", \"7 Tag\", \"11 Tag\", \"14 Tag\")),\n         rep = as_factor(rep),\n         trt = as_factor(trt)) |&gt;\n  select(timepoint, trt, thripse = thripse_above) |&gt; \n  filter(trt %in% c(\"10ml\", \"30ml\", \"60ml\"))\n\nDann können wir auch schon die Poisson Regression mit glm() rechnen. Auch hier wieder darauf achten, dass wir dann als Option family = poisson oder family = quasipoisson wählen. Es hängt jetzt davon ab, ob du in deinen Daten Overdispersion vorliegen hast oder nicht. In den beiden folgenden Tabs, rechne ich dann mal beide Modelle.\n\nfamily = poissonfamily = quasipoisson\n\n\nAls Erstes rechnen wir eine normale Poisson Regression und schauen einmal, ob wir Overdispersion vorliegen haben. Wenn wir Overdispersion vorliegen haben, dann können wir keine Poisson Regression rechnen, sondern müssen auf eine Quasipoisson Regression ausweichen. Das ist aber sehr einfach, wie du im anderen Tab sehen wirst.\n\ninsects_poisson_fit &lt;- glm(thripse ~ trt + timepoint + trt:timepoint, \n                           data = insects_tbl, \n                           family = poisson) \n\nBevor wir uns das Modell mit summary() überhaupt anschauen, wollen wir erstmal überprüfen, ob wir überhaupt Overdispersion vorliegen haben. Wenn ja, dann können wir uns die summary() hier gleich sparen. Also einmal geguckt, was die Overdispersion macht.\n\ninsects_poisson_fit |&gt; check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =   23.498\n  Pearson's Chi-Squared = 3172.179\n                p-value =  &lt; 0.001\n\n\nOverdispersion detected.\n\n\nWir haben sehr starke Overdispersion vorliegen und gehen daher rüber in den anderen Tab und rechnen eine Quasipoisson Regression. Nur wenn du keine Overdispersion vorliegen hast, dann kannst du eine eine Poisson Regression rechnen.\n\n\nEntweder hast du in deinen Daten eine Overdispersion gefunden oder aber du meinst, es wäre besser gleich eine Quasipoisson zu rechnen. Beides ist vollkommen in Ordnung. Ich rechne meistens immer eine Quasipoisson und schaue dann nur, ob die Overdispersion sehr groß war. In den seltensten Fällen hast du eine Overdispersion vorliegen, die eher klein ist. Daher mache ich erst die Lösung und schaue, ob das Problem dann da war.\n\ninsects_quasipoisson_fit &lt;- glm(thripse ~ trt + timepoint + trt:timepoint, \n                                data = insects_tbl, \n                                family = quasipoisson) \n\nDu kannst in der summary() Ausgabe direkt sehen, ob du Overdispersion vorliegen hast. Du musst nur relativ weit unten schauen, was zu dem Dispersion parameter in den Klammern geschrieben ist. Wenn da eine Zahl größer als 1 drin steht, dann hast du Overdispersion.\n\ninsects_quasipoisson_fit |&gt; \n  summary()\n\n\nCall:\nglm(formula = thripse ~ trt + timepoint + trt:timepoint, family = quasipoisson, \n    data = insects_tbl)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-9.7421  -4.1556  -0.5495   1.9488  14.0290  \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              3.861686   0.211964  18.219   &lt;2e-16 ***\ntrt30ml                 -0.565849   0.376336  -1.504   0.1350    \ntrt60ml                 -1.058326   0.432830  -2.445   0.0158 *  \ntimepoint4 Tag          -0.448440   0.339528  -1.321   0.1888    \ntimepoint7 Tag          -0.001914   0.299907  -0.006   0.9949    \ntimepoint11 Tag         -0.121854   0.309323  -0.394   0.6942    \ntimepoint14 Tag         -0.177363   0.313970  -0.565   0.5731    \ntrt30ml:timepoint4 Tag   0.476840   0.553141   0.862   0.3902    \ntrt60ml:timepoint4 Tag  -0.579968   0.809916  -0.716   0.4752    \ntrt30ml:timepoint7 Tag   0.154299   0.519303   0.297   0.7668    \ntrt60ml:timepoint7 Tag   0.252555   0.585830   0.431   0.6671    \ntrt30ml:timepoint11 Tag  0.121854   0.537661   0.227   0.8210    \ntrt60ml:timepoint11 Tag  0.091083   0.620448   0.147   0.8835    \ntrt30ml:timepoint14 Tag -0.162407   0.575416  -0.282   0.7782    \ntrt60ml:timepoint14 Tag -0.201195   0.670025  -0.300   0.7644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 23.49807)\n\n    Null deviance: 4263.4  on 149  degrees of freedom\nResidual deviance: 3345.0  on 135  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nWir haben hier auf jeden Fall Overdispersion vorliegen. Daher nutze ich dann auch das Modell hier mit der Annahme an eine Quasipoissonverteilung. Dann stimmt es auch mit unseren Varianzen und wir produzieren nicht zufällig zu viele signifikante Ergebnisse, die es dann gar nicht gibt.\n\n\n\nIch habe mich gerade in den obigen Tabs für eine Quasipoisson Regression entschieden, da wir Overdispersion vorliegen haben. Damit mache ich dann mit dem insects_quasipoisson_fit Modell weiter. In den beiden folgenden Tabs findest du dann einmal das Ergebnis für die ANOVA und einmal für den Gruppenvergleich mit dem R Paket {emmeans}. Bitte beachte, dass die ANOVA für ein glm()-Objekt nicht ganz gleich wie für ein lm()-Objekt ist. Du kannst aber die ANOVA erstmal ganz normal interpretieren, nur haben wir hier nicht die Möglichkeit ein \\(\\eta^2\\) zu bestimmen. Dann nutzen wir {emmeans} für den Gruppenvergleich. Nochmal, weil wir Overdispersion festgestellt haben, nutzen wir das Objekt insects_quasipoisson_fit mit der Berücksichtigung der Overdispersion.\n\nANOVA mit anova()Gruppenvergleich mit emmeans()\n\n\nWir rechnen hier einmal die ANOVA und nutzen den \\(\\mathcal{X}^2\\)-Test für die Ermittelung der p-Werte. Wir müssen hier einen Test auswählen, da per Standardeinstellung kein Test gerechnet wird. Wir machen dann die Ausageb nochmal schöner und fertig sind wir.\n\ninsects_quasipoisson_fit |&gt; \n  anova(test = \"Chisq\") |&gt; \n  model_parameters(drop = \"NULL\")\n\nParameter     | df | Deviance | df (error) | Deviance (error) |      p\n----------------------------------------------------------------------\ntrt           |  2 |   730.20 |        147 |          3533.22 | &lt; .001\ntimepoint     |  4 |   112.27 |        143 |          3420.95 | 0.311 \ntrt:timepoint |  8 |    75.93 |        135 |          3345.03 | 0.919 \n\nAnova Table (Type 1 tests)\n\n\nWir sehen, dass der Effekt für die Behandlung signifikant ist, jedoch die Zeit und die Interaktion keinen signifikanten Einfluss haben. Wir haben aber also keine Interaktion vorliegen. Daher können wir dann die Analyse gemeinsam über alle Zeitpunkte rechnen.\n\n\nIm Folgenden rechnen wir einmal über alle Faktorkombinationen von trt und timepoint einen Gruppenvergleich. Dafür nutzen wir die Opition trt * timepoint. Wenn du die Analyse getrennt für die Zeitpunkte durchführen willst, dann nutze die Option trt | timepoint. Wir wollen die Wahrscheinlichkeiten für das Auftreten einer Beschädigung von wiedergegeben bekommen, deshalb die Option regrid = \"response. Dann adjustieren wir noch nach Bonferroni und sind fertig.\n\nemm_obj &lt;- insects_quasipoisson_fit |&gt; \n  emmeans(~ trt * timepoint, regrid = \"response\") |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\")\nemm_obj\n\n trt  timepoint rate    SE  df asymp.LCL asymp.UCL .group\n 60ml 4 Tag      5.9  3.72 Inf    -5.029      16.8  a    \n 60ml 14 Tag    11.3  5.15 Inf    -3.825      26.4  ab   \n 60ml 11 Tag    16.0  6.13 Inf    -1.998      34.0  ab   \n 60ml 1 Tag     16.5  6.23 Inf    -1.777      34.8  ab   \n 30ml 14 Tag    19.2  7.08 Inf    -1.572      40.0  ab   \n 60ml 7 Tag     21.2  7.06 Inf     0.483      41.9  ab   \n 30ml 1 Tag     27.0  8.40 Inf     2.356      51.6  ab   \n 30ml 11 Tag    27.0  8.40 Inf     2.356      51.6  ab   \n 30ml 4 Tag     27.8  8.52 Inf     2.782      52.8  ab   \n 10ml 4 Tag     30.4  8.05 Inf     6.725      54.0  ab   \n 30ml 7 Tag     31.4  9.06 Inf     4.849      58.0  ab   \n 10ml 14 Tag    39.8  9.22 Inf    12.748      66.9  ab   \n 10ml 11 Tag    42.1  9.48 Inf    14.258      69.9   b   \n 10ml 7 Tag     47.5 10.07 Inf    17.902      77.0   b   \n 10ml 1 Tag     47.5 10.08 Inf    17.965      77.1   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 15 estimates \nP value adjustment: bonferroni method for 105 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nDas emm_obj Objekt werden wir dann gleich einmal in {ggplot} visualisieren. Die rate stellt die mittlere Anzahl an Thripsen je Faktorkombination dar. Dann können wir auch das compact letter display anhand der Abbildung interpretieren.\n\n\n\nIn der Abbildung 52.19 siehst du das Ergebnis der Auswertung in einem Säulendiagramm. Hier unbedingt SE als den Standardfehler für die Fehlerbalken nutzen, da wir sonst Fehlerbalken größer und kleiner als \\(0\\) erhalten, wenn wir die Standardabweichung nutzen würden. Das ist in unserem Fall nicht so das Problem, aber wenn du eher kleine Anzahlen zählst, kann das schnell zu Werten kleiner Null führen. Wir sehen einen klaren Effekt der Behandlung 60ml. Die Zeit hat keinen Effekt, was ja schon aus der ANOVA klar war, die Säulen sehen für jeden Zeitpunkt vollkommen gleich aus. Gut etwas Unterschied ist ja immer.\n\nemm_obj |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = timepoint, y = rate, fill = trt)) +\n  theme_minimal() + \n  labs(y = \"Mittlere Anzahl an Thripsen\", x = \"Messzeitpunkte der Zählungen\",\n       fill = \"Dosis\") +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_text(aes(label = .group, y = rate + SE + 0.01),  \n            position = position_dodge(width = 0.9), vjust = -0.25) +\n  geom_errorbar(aes(ymin = rate-SE, ymax = rate+SE),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 47.6— Säulendigramm der mitleren Zahl der Thripsen aus einer Poisson Regression. Das glm()-Modell berechnet die mittlere Anzahl in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-poisson.html#referenzen",
    "href": "stat-modeling-poisson.html#referenzen",
    "title": "47  Poisson Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 47.1— Histogramm der Verteilung der Hechte in den beobachteten Flüssen.\nAbbildung 47.2— Ausgabe ausgewählter Modelgüteplots der Funktion check_model().\nAbbildung 47.3 (a)— Verteilung der Werte als Boxplot.\nAbbildung 47.3 (b)— Verteilung der Werte als Densityplot.\nAbbildung 47.4— Histogramm der Verteilung der Hechte in den beobachteten Flüssen.\nAbbildung 47.5— Scatterplot der vorhergesagten Eidechsen in den Habitaten (grp), der Feuchtigkeit des Bodens nach Regen und dem Abstand zur nächsten Ortschaft.\nAbbildung 47.6— Säulendigramm der mitleren Zahl der Thripsen aus einer Poisson Regression. Das glm()-Modell berechnet die mittlere Anzahl in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert.\n\n\n\nDormann CF. 2013. Parametrische Statistik. Springer.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Poisson Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-multinom.html",
    "href": "stat-modeling-multinom.html",
    "title": "48  Multinomiale / Ordinale Regression",
    "section": "",
    "text": "48.1 Annahmen an die Daten\nIm folgenden Kapitel zu der multinomialen / ordinalen logistischen linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Multinomialverteilung. Damit finden wir im Outcome im Falle der multinomialen logistischen linearen Regression ungeordnete Kategorien und im Falle der ordinalen logistischen linearen Regression geordnete Kategorien.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 33 nochmal nachlesen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multinomiale / Ordinale Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-multinom.html#annahmen-an-die-daten",
    "href": "stat-modeling-multinom.html#annahmen-an-die-daten",
    "title": "48  Multinomiale / Ordinale Regression",
    "section": "",
    "text": "Wenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 43 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 41 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 42 bei der Variablenselektion.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multinomiale / Ordinale Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-multinom.html#genutzte-r-pakete",
    "href": "stat-modeling-multinom.html#genutzte-r-pakete",
    "title": "48  Multinomiale / Ordinale Regression",
    "section": "48.2 Genutzte R Pakete",
    "text": "48.2 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom,\n               parameters, performance, gtsummary,\n               ordinal, janitor, MASS, nnet, flextable,\n               emmeans, multcomp, ordinal, see, scales,\n               janitor, conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(magrittr::extract)\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multinomiale / Ordinale Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-multinom.html#daten",
    "href": "stat-modeling-multinom.html#daten",
    "title": "48  Multinomiale / Ordinale Regression",
    "section": "48.3 Daten",
    "text": "48.3 Daten\nIm Folgenden wollen wir uns die Daten von den infizierten Ferkeln noch einmal anschauen. Wir nehmen als Outcome die Spalte frailty und damit die Gebrechlichkeit der Ferkel. Die Spalte ordnen wir einmal nach robust, pre-frail und frail. Wobei robust ein gesundes Ferkel beschreibt und frail ein gebrechliches Ferkel. Damit wir später die Richtung des Effekts richtig interpretieren können, müssen wir von gut nach schlecht sortieren. Das brauchen wir nicht, wenn wir Boniturnoten haben, dazu mehr in einem eigenen Abschnitt. Wir bauen uns dann noch einen Faktor mit ebenfalls der Spalte frailty in der wir so tun, als gebe es diese Ordnung nicht. Wir werden dann die ordinale Regression mit dem Outcome frailty_ord rechnen und die multinominale Regression dann mit dem Outcome frailty_fac durchführen.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") |&gt;\n  mutate(frailty_ord = ordered(frailty, levels = c(\"robust\", \"pre-frail\", \"frail\")),\n         frailty_fac = as_factor(frailty)) |&gt; \n  select(-infected)\n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der Tabelle 48.1 an.\n\n\n\n\nTabelle 48.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\nfrailty_ord\nfrailty_fac\n\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\nrobust\nrobust\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\nrobust\nrobust\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\nrobust\nrobust\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.15\n19.05\n4.35\nrobust\nrobust\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n55.38\n18.44\n5.27\nrobust\nrobust\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n60.29\n18.42\n4.78\nrobust\nrobust\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\npre-frail\n55.32\n19.75\n3.92\npre-frail\npre-frail\n\n\n56\nmale\nwest\n13.91\n20.8\nfrail\n58.37\n17.28\n7.44\nfrail\nfrail\n\n\n57\nmale\nnorthwest\n12.49\n21.95\npre-frail\n56.66\n16.86\n2.44\npre-frail\npre-frail\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\nrobust\nrobust\n\n\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\nrobust\nrobust\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\nrobust\nrobust\n\n\n\n\n\n\n\n\nDas wären dann die Daten, die wir für unsere Modelle dann brauchen. Schauen wir mal was wir jetzt bei der ordinalen Regression herausbekommen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multinomiale / Ordinale Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-multinom.html#sec-ordinal",
    "href": "stat-modeling-multinom.html#sec-ordinal",
    "title": "48  Multinomiale / Ordinale Regression",
    "section": "48.4 Ordinale logistische Regression",
    "text": "48.4 Ordinale logistische Regression\nEs gibt sicherlich einiges an Paketen in R um eine ordinale Regression durchzuführen. Ich nutze gerne die Funktion polr() aus dem R Paket {MASS}. Daneben gibt es auch noch das R Paket {ordinal} mit der Funktion clm(), die wir dann noch im Anschluss besprechen werden. Ich nutze jetzt erstmal die Funktion polr(), da wir hier noch eine externe Referenz haben, die uns noch detailliertere Informationen liefern kann.\n\n\nIch verweise gerne hier auf das tolle Tutorium Ordinal Logistic Regression | R Data Analysis Examples. Hier erfährst du noch mehr über die Analyse der ordinalen logistischen Regression.\nWir schon erwähnt sparen wir usn die mathematischen Details und utzen gleich die Funktion polr auf unserem Outcome frailty. Wir müssen keine Verteilungsfamilie extra angeben, dass haben wir schon mit der Auswahl der Funktion getan. Die Funktion polr kann nur eine ordinale Regression rechnen und wird einen Fehler ausgeben, wenn das Outcome \\(y\\) nicht passt.\n\nologit_fit &lt;- polr(frailty_ord ~ age + sex + location + activity + crp + \n                     bloodpressure + weight + creatinin, \n                   data = pig_tbl)\n\nSchauen wir uns einmal die Ausgabe des Modellfits der ordinalen Regression mit der Funktion summary() an. Wir sehen eine Menge Zahlen und das wichtigste für uns ist ja, dass wir zum einen Wissen, dass wir auch die ordinale Regression auf der \\(link\\)-Funktion rechnen. Wir erhalten also wieder eine Transformation des Zusammenhangs zurück, wie wir es schon bei der Poisson Regression sowie bei der logistischen Regression hatten.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\n\nologit_fit |&gt; summary()\n\nCall:\npolr(formula = frailty_ord ~ age + sex + location + activity + \n    crp + bloodpressure + weight + creatinin, data = pig_tbl)\n\nCoefficients:\n                     Value Std. Error t value\nage                0.03191    0.02168  1.4718\nsexmale            0.06970    0.26447  0.2635\nlocationnortheast -0.27291    0.28063 -0.9725\nlocationnorthwest  0.07598    0.24883  0.3054\nlocationwest       0.24400    0.27285  0.8942\nactivity          -0.06970    0.07267 -0.9591\ncrp                0.06499    0.06820  0.9529\nbloodpressure     -0.05051    0.03037 -1.6632\nweight             0.06190    0.06649  0.9310\ncreatinin         -0.01100    0.06854 -0.1605\n\nIntercepts:\n                 Value   Std. Error t value\nrobust|pre-frail  0.6494  3.0697     0.2116\npre-frail|frail   2.4716  3.0724     0.8044\n\nResidual Deviance: 794.6938 \nAIC: 818.6938 \n\n\nUnsere Ausgabe teilt sich in zwei Teile auf. In dem oberen Teil sehen wir die Koeffizienten des Modells zusammen mit dem Fehler und der Teststatistik. Was wir nicht sehen, ist ein \\(p\\)-Wert. Die Funktion rechnet uns keinen Signifikanztest aus. Das können wir aber gleich selber machen. In dem Abschnitt Intercepts finden wir die Werte für die Gruppeneinteilung auf der link-Funktion wieder. Wir transformieren ja unsere drei Outcomekategorien in einen kontinuierliche Zahlenzusammenhang. Trotzdem müssen ja die drei Gruppen auch wieder auftauchen. In dem Abschnitt Intercepts finden wir die Grenzen für die drei Gruppen auf der link-Funktion.\n\n\nWir gibt auch ein Tutorial für How do I interpret the coefficients in an ordinal logistic regression in R?\nBerechnen wir jetzt einmal die \\(p\\)-Werte per Hand. Dafür brauchen wir die absoluten Werte aus der t value Spalte aus der summary des Modellobjekts. Leider ist die Spalte nicht schön formatiert und so müssen wir uns etwas strecken um die Koeffizienten sauber aufzuarbeiten. Wir erhalten dann das Objekt coef_tbl wieder.\n\ncoef_tbl &lt;- summary(ologit_fit) |&gt; \n  coef() |&gt; \n  as_tibble(rownames = \"term\") |&gt; \n  clean_names() |&gt; \n  mutate(t_value = abs(t_value))\n\ncoef_tbl\n\n# A tibble: 12 × 4\n   term                value std_error t_value\n   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 age                0.0319    0.0217   1.47 \n 2 sexmale            0.0697    0.264    0.264\n 3 locationnortheast -0.273     0.281    0.973\n 4 locationnorthwest  0.0760    0.249    0.305\n 5 locationwest       0.244     0.273    0.894\n 6 activity          -0.0697    0.0727   0.959\n 7 crp                0.0650    0.0682   0.953\n 8 bloodpressure     -0.0505    0.0304   1.66 \n 9 weight             0.0619    0.0665   0.931\n10 creatinin         -0.0110    0.0685   0.161\n11 robust|pre-frail   0.649     3.07     0.212\n12 pre-frail|frail    2.47      3.07     0.804\n\n\nUm die Fläche rechts von dem \\(t\\)-Wert zu berechnen, können wir zwei Funktionen nutzen. Die Funktion pnorm() nimmt eine Standradnormalverteilung an und die Funktion pt() vergleicht zu einer \\(t\\)-Verteilung. Wenn wir rechts von der Verteilung schauen wollen, dann müssen wir die Option lower.tail = FALSE wählen. Da wir auch zweiseitig statistisch Testen, müssen wir den ausgerechneten \\(p\\)-Wert mal zwei nehmen. Hier einmal als Beispiel für den \\(t\\)-Wert von \\(1.96\\). Mit pnorm(1.96, lower.tail = FALSE) * 2 erhalten wir \\(0.05\\) als Ausgabe. Das ist unser \\(p\\)-Wert. Was uns ja nicht weiter überrascht. Denn rechts neben dem Wert von \\(1.96\\) in einer Standardnormalverteilung ist ja \\(0.05\\). Wenn wir einen \\(t\\)-Test rechnen würden, dann müssten wir noch die Freiheitsgrade df mit angeben. Mit steigendem \\(n\\) nähert sich die \\(t\\)-Verteilung der Standardnormalverteilung an. Wir haben mehr als \\(n = 400\\) Beobachtungen, daher können wir auch df = 400 setzen. Da kommt es auf eine Zahl nicht an. Wir erhalten mit pt(1.96, lower.tail = FALSE, df = 400) * 2 dann eine Ausgabe von \\(0.0507\\). Also fast den gleichen \\(p\\)-Wert.\nIm Folgenden setzte ich die Freiheitsgrade df = 3 dammit wir was sehen. Bei so hohen Fallzahlen wir in unserem beispiel würden wir sonst keine Unterschiede sehen.\n\ncoef_tbl |&gt; \n  mutate(p_n = pnorm(t_value, lower.tail = FALSE) * 2,\n         p_t = pt(t_value, lower.tail = FALSE, df = 3) * 2) |&gt; \n  mutate(across(where(is.numeric), round, 3))\n\n# A tibble: 12 × 6\n   term               value std_error t_value   p_n   p_t\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 age                0.032     0.022   1.47  0.141 0.237\n 2 sexmale            0.07      0.264   0.264 0.792 0.809\n 3 locationnortheast -0.273     0.281   0.973 0.331 0.403\n 4 locationnorthwest  0.076     0.249   0.305 0.76  0.78 \n 5 locationwest       0.244     0.273   0.894 0.371 0.437\n 6 activity          -0.07      0.073   0.959 0.338 0.408\n 7 crp                0.065     0.068   0.953 0.341 0.411\n 8 bloodpressure     -0.051     0.03    1.66  0.096 0.195\n 9 weight             0.062     0.066   0.931 0.352 0.421\n10 creatinin         -0.011     0.069   0.161 0.872 0.883\n11 robust|pre-frail   0.649     3.07    0.212 0.832 0.846\n12 pre-frail|frail    2.47      3.07    0.804 0.421 0.48 \n\n\nDamit haben wir einmal händisch uns die \\(p\\)-Werte ausgerechnet. Jetzt könnte man sagen, dass ist ja etwas mühselig. Gibt es da nicht auch einen einfacheren Weg? Ja wir können zum einen die Funktion tidy() nutzen um die 95% Konfidenzintervalle und die exponierten Effektschätzer aus der ordinalen Regresssion zu erhalten. Wir erhalten aber wieder keine \\(p\\)-Werte sondern müssten uns diese \\(p\\)- Werte dann wieder selber berechnen.\n\nologit_fit |&gt; \n  tidy(conf.int = TRUE, exponentiate = TRUE) |&gt; \n  select(-coef.type)\n\n# A tibble: 12 × 6\n   term              estimate std.error statistic conf.low conf.high\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 age                  1.03     0.0217     1.47     0.990      1.08\n 2 sexmale              1.07     0.264      0.264    0.639      1.80\n 3 locationnortheast    0.761    0.281     -0.973    0.437      1.32\n 4 locationnorthwest    1.08     0.249      0.305    0.662      1.76\n 5 locationwest         1.28     0.273      0.894    0.747      2.18\n 6 activity             0.933    0.0727    -0.959    0.808      1.08\n 7 crp                  1.07     0.0682     0.953    0.934      1.22\n 8 bloodpressure        0.951    0.0304    -1.66     0.896      1.01\n 9 weight               1.06     0.0665     0.931    0.934      1.21\n10 creatinin            0.989    0.0685    -0.161    0.865      1.13\n11 robust|pre-frail     1.91     3.07       0.212   NA         NA   \n12 pre-frail|frail     11.8      3.07       0.804   NA         NA   \n\n\nUm all dieses Berechnen zu umgehen, können wir dann auch die Funktion model_parameters() nutzen. Hier berechnen wir dann die \\(p\\)-Wert mit \\(df = 400\\) aus einer \\(t\\)-Verteilung. Damit umgehen wir das Problem, dass unser Modellfit keine \\(p\\)-Werte liefert.\n\nologit_fit |&gt; \n  model_parameters() \n\n# alpha\n\nParameter        | Log-Odds |   SE |        95% CI | t(400) |     p\n-------------------------------------------------------------------\nrobust|pre-frail |     0.65 | 3.07 | [-5.39, 6.68] |   0.21 | 0.833\npre-frail|frail  |     2.47 | 3.07 | [-3.57, 8.51] |   0.80 | 0.422\n\n# beta\n\nParameter            | Log-Odds |   SE |        95% CI | t(400) |     p\n-----------------------------------------------------------------------\nage                  |     0.03 | 0.02 | [-0.01, 0.07] |   1.47 | 0.142\nsex [male]           |     0.07 | 0.26 | [-0.45, 0.59] |   0.26 | 0.792\nlocation [northeast] |    -0.27 | 0.28 | [-0.83, 0.27] |  -0.97 | 0.331\nlocation [northwest] |     0.08 | 0.25 | [-0.41, 0.56] |   0.31 | 0.760\nlocation [west]      |     0.24 | 0.27 | [-0.29, 0.78] |   0.89 | 0.372\nactivity             |    -0.07 | 0.07 | [-0.21, 0.07] |  -0.96 | 0.338\ncrp                  |     0.06 | 0.07 | [-0.07, 0.20] |   0.95 | 0.341\nbloodpressure        |    -0.05 | 0.03 | [-0.11, 0.01] |  -1.66 | 0.097\nweight               |     0.06 | 0.07 | [-0.07, 0.19] |   0.93 | 0.352\ncreatinin            |    -0.01 | 0.07 | [-0.15, 0.12] |  -0.16 | 0.873\n\n\nIn Tabelle 48.2 sehen wir nochmal die Ergebnisse der ordinalen Regression einmal anders aufgearbeitet. Wir aber schon bei der Funktion tidy() fehlen in der Tabelle die \\(p\\)-Werte. Wir können aber natürlich auch eine Entscheidung über die 95% Konfidenzintervalle treffen. Wenn die 1 mit im 95% Konfidenzintervall ist, dann können wir die Nullhypothese nicht ablehnen.\n\nologit_fit |&gt; \n  tbl_regression(exponentiate = TRUE) |&gt; \n  as_flex_table()\n\n\n\nTabelle 48.2— Tabelle der Ergebnisse der ordinalen Regression.\n\n\n\nCharacteristicOR195% CI1age1.030.99, 1.08sexfemale——male1.070.64, 1.80locationnorth——northeast0.760.44, 1.32northwest1.080.66, 1.76west1.280.75, 2.18activity0.930.81, 1.08crp1.070.93, 1.22bloodpressure0.950.90, 1.01weight1.060.93, 1.21creatinin0.990.86, 1.131OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nWi es im gazen Kapitel schon durchscheint, die Interpreation der \\(OR\\) aus einer ordinalen Regression ist nicht einfach, geschweige den intuitiv. Was wir haben ist der Trend. Wir haben unser Outcome von robust zu frail sortiert und damit von gut nach schlecht. Wir können so die Richtung der Variablen in unserem Modell interpretieren. Das heißt, dass männliche Ferkel eher von einer Gebrechlichkeit betroffen sind als weibliche Ferkel. Oder wir sagen, dass ein ansteigender CRP Wert führt zu weniger Gebrechlichkeit. Auf diesem Niveau lassen sich die \\(OR\\) einer ordinalen Regression gut interpretieren.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multinomiale / Ordinale Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-multinom.html#sec-mult-comp-ord-reg",
    "href": "stat-modeling-multinom.html#sec-mult-comp-ord-reg",
    "title": "48  Multinomiale / Ordinale Regression",
    "section": "48.5 Gruppenvergleich",
    "text": "48.5 Gruppenvergleich\nIn diesem Abschnitt wollen wir Gruppenvergleich mit dem Cumulative Link Models (CLM) für ordinale Daten rechnen. Oder andersherum, wir haben Boniturdaten vorliegen und wollen hierfür einen multipen Vergleich rechnen. Mehr zu dem Modell findest du im Tutorium zu Introduction to Cumulative Link Models (CLM) for Ordinal Data. Wir konzentrieren uns hier direkt auf die Auswertung an einem Spieldatensatz. Daran können wir dann einfacher erkennen, was bei einem Vergleich rauskommt, wenn wir wissen was wir reingesteckt haben. Wenn du mehr zu dem Thema lesen willst, dann hilft eventuell auch die Hilfeseite zu Ordinal models with {emmeans}. Ich muss aber sagen, dass die Seite etwas theoretisch ist.\nWir bauen uns jetzt einen Spieldatensatz mit zwanzig Boniturnoten für drei Tierarten. Ich nehme hier nur fünf Notenschritte, da sonst die Sachlage sehr unübersichtlich wird. Im Prinzip generieren wir uns mit der Funktion sample nach vorgegeben Wahrscheinlichkeiten für jede Note zwanzig Boniturnoten für jede Tierart. Dabei haben Katzen eine niedrigere Note als Hunde und die Hunde haben schlechtere Noten als die Füchse. Am Ende brauchen wir dann noch einen geordneten Faktor likert_ord damit wir die ordinale Regression rechnen können. Die ursprünglichen Noten behalte ich als numerisch um die Daten besser in {ggplot} abbilden zu können.\n\nset.seed(20231201)\nn_grp &lt;- 20\ngrade_tbl &lt;- tibble(trt = gl(3, n_grp, labels = c(\"cat\", \"dog\", \"fox\")),\n                    likert = c(sample(1:5, size = n_grp, replace = TRUE, prob = c(0.2, 0.5, 0.2, 0.1, 0.0)),\n                               sample(1:5, size = n_grp, replace = TRUE, prob = c(0.1, 0.2, 0.5, 0.2, 0.0)),\n                               sample(1:5, size = n_grp, replace = TRUE, prob = c(0.0, 0.0, 0.2, 0.5, 0.3)))) |&gt; \n  mutate(likert_ord = ordered(likert))\n\nDann berechnen wir einmal die mittleren Boniturnoten für die drei Tierarten. Wir sehen, dass wir ziemlich gerade Durchschnittsnoten haben.\n\ngrade_tbl |&gt; \n  group_by(trt) |&gt; \n  summarise(mean(likert))\n\n# A tibble: 3 × 2\n  trt   `mean(likert)`\n  &lt;fct&gt;          &lt;dbl&gt;\n1 cat              2  \n2 dog              3  \n3 fox              4.1\n\n\nJetzt können wir die Daten auch schon in die ordinale Regression mit clm() stecken. Da die Noten alle den gleichen Abstand zueinander haben, nutzen wir die Option threshold = \"symmetric\". Wir brauchen hier die Ausgabe der Moelldierung gar nicht weiter im Detail. Wir nutzen den Fit dann gleich in der ANOVA und {emmeans} für den Gruppenvergleich der mittleren Boniturnoten.\n\nclm_fit &lt;- clm(likert_ord ~ trt, data = grade_tbl,\n               threshold = \"symmetric\")\n\nDie ANOVA funktioniert wie gewohnt. Wir müssen hier auch nichts anpassen, wir kriegen einfach so unsere p-Werte geliefert. Wir haben einen signifikanten Effekt der Tierarten, was ja auch zu erwarten war. Die Boniturnoten unterscheiden sich zwischen Katzen, Hunden und Füchsen.\n\nanova(clm_fit)\n\nType I Analysis of Deviance Table with Wald chi-square tests\n\n    Df  Chisq Pr(&gt;Chisq)    \ntrt  2 33.628  4.986e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDann können wir auch schon den multipen Gruppenvergleich rechnen. Da wir die mittleren Boniturnoten wiedergegeben haben wollen, nutzen wir die Option mode = \"mean.class\". Sonst würden wir andere Werte erhalten, die dann wirklich schwer zu interpretieren sind. Wie du siehst, sind es die gleichen Werte die wir auch oben für unsere Mittelwerte über die Boniturnoten berechnet haben.\n\nemm_obj &lt;- clm_fit |&gt; \n  emmeans(~ trt, mode = \"mean.class\")\n\nDann können wir uns auch schon das compact letter display wiedergeben lassen. Auch hier gibt es keine Überraschung, die drei Tierarten unterscheiden sich. Mit der Ausgabe könnten wir dann auch gleich ein Säulendigramm erstellen, aber das zeige ich gleich mal in dem zweiten Beispiel für zwei Faktoren und der Bonitur von Weizen.\n\nemm_obj |&gt; \n  cld(Letters = letters)\n\n trt mean.class    SE  df asymp.LCL asymp.UCL .group\n cat       1.98 0.169 Inf      1.65      2.31  a    \n dog       3.00 0.162 Inf      2.68      3.32   b   \n fox       4.09 0.162 Inf      3.77      4.40    c  \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 3 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nIn unserem zweiten Beispiel schauen wir uns einen zweifaktoriellen Datensatz einmal an. Wir haben Weizen angepflanzt und bonitieren die Weizenpflanzen nach der Likert Skala. Dabei bedeutet dann eine 1 ein schlechte Note und eine 9 die bestmögliche Note. Wir hätten natürlich hier auch einen Kurskal-Wallis-Test rechnen können und dann im Anschluss einen paarweisen Wilcoxon Test. Nun modellieren wir hier aber die Boniturnoten mal mit einer ordinalen Regression und rechnen den anschließenden Gruppenvergleich dann mit dem R Paket {emmeans}.\nUnser Datensatz grade_tbl enthält den Faktor block mit drei Levels sowie den Faktor variety mit fünf Leveln. Jedes Level repräsentiert dabei eine Weizensorte. Wichtig ist hier, dass wir die Noten als geordneten Faktor mit der Funktion ordered erstellen. Nur dann haben die Noten eine Ordnung und die folgenden Funktionen erkennen dann auch die Spalte grade_ord als eine ordinale Spalte mit Noten.\n\ngrade_tbl &lt;- tibble(block = rep(c(\"I\", \"II\", \"III\"), each = 3),\n                    A = c(2, 3, 4, 3, 3, 2, 4, 2, 1),\n                    B = c(7, 9, 8, 9, 7, 8, 9, 6, 7),\n                    C = c(6, 5, 5, 7, 5, 6, 4, 7, 6),\n                    D = c(2, 3, 1, 2, 1, 1, 2, 2, 1),\n                    E = c(4, 3, 7, 5, 6, 4, 5, 7, 5)) |&gt;\n  gather(key = variety, value = grade, A:E) |&gt; \n  mutate(grade_ord = ordered(grade))\n\nWir schauen uns nochmal den Datensatz an und sehen, dass wir einmal die Spalte grade als numerische Spalte vorliegen haben und einmal als geordneten Faktor. Wir brauchen die numerische Spalte um die Daten besser in ggplot() darstellen zu können.\n\ngrade_tbl |&gt; head(4)\n\n# A tibble: 4 × 4\n  block variety grade grade_ord\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;ord&gt;    \n1 I     A           2 2        \n2 I     A           3 3        \n3 I     A           4 4        \n4 II    A           3 3        \n\n\nIn Abbildung 48.1 sehen wir einmal die Daten als Dotplot dargestellt. Auf der x-Achse sind die Weizensorten und auf der y-Achse die Boniturnoten. Ich habe noch die zusätzlichen Linien für jede einzelne Note mit eingezeichnet.\n\nggplot(grade_tbl, aes(variety, grade, fill = block)) +\n  theme_minimal() +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.6), dotsize = 0.75) +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9)) +\n  scale_fill_okabeito() \n\n\n\n\n\n\n\nAbbildung 48.1— Dotplot des Datenbeispiels für die Bonitur von fünf Weizensorten.\n\n\n\n\n\nJetzt können wir schon die Funktion clm() aus dem R Paket {ordinal} verwenden um die ordinale Regression zu rechnen. Wir haben in dem R Paket {ordinal} noch weitere Modelle zu Verfügung mit denen wir auch komplexere Designs bis hin zu linearen gemischten Modellen für eine ordinale Regresssion rechnen können. Da wir mit Boniturnoten als Outcome arbeiten setzen wir auch die Option threshold = \"symmetric\". Damit teilen wir der Funktion clm() mit, dass wir es mit einer symmetrischen Notenskala zu tun haben. Wenn du das nicht hast, dass kannst du die Option auch auf \"flexible\" stellen. Dann wird eine nicht symmetrische Verteilung des Outcomes angenommen.\n\nclm_fit &lt;- clm(grade_ord ~ variety + block + variety:block, data = grade_tbl,\n               threshold = \"symmetric\")\n\nEs ist auch möglich auf dem Modellfit eine ANOVA zu rechnen. Wir machen das hier einmal, aber wir erwarten natürlich einen signifikanten Effekt von der Sorte. Die Signifikanz konnten wir ja schon oben im Dotplot sehen.\n\nanova(clm_fit)\n\nType I Analysis of Deviance Table with Wald chi-square tests\n\n              Df   Chisq Pr(&gt;Chisq)    \nvariety        4 38.7587  7.813e-08 ***\nblock          2  0.0987     0.9518    \nvariety:block  8  4.9452     0.7634    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nJetzt nutzen wir wieder den Modellfit für unseren Gruppenvergleich in {emmeans}. Wir nutzen dafür wieder die Funktion emmeans() und lassen uns das compact letter display über die Funktion cld() wiedergeben. Da wir hier eigentlich keinen signifikanten Effekt der Blöcke vorliegen haben, könnten wir auch einfach die Option ~ variety nutzen und den Block weglassen. Dann wäre auch die Ausgabe mit den compact letter display etwas übersichtlicher. Ich habe dann noch die Ausgabe einmal nach den Sorten sortiert und nicht nach dem compact letter display um hier etwas mehr Übersicht zu erhalten.\n\nemm_obj &lt;- clm_fit |&gt; \n  emmeans(~ variety * block, mode = \"mean.class\") |&gt; \n  cld(Letters = letters) |&gt; \n  arrange(variety)\nemm_obj\n\n variety block mean.class    SE  df asymp.LCL asymp.UCL .group \n A       III         2.23 0.600 Inf     1.054      3.41  abc   \n A       II          2.68 0.484 Inf     1.737      3.63  abc   \n A       I           3.02 0.540 Inf     1.960      4.08  abcd  \n B       III         7.17 0.623 Inf     5.949      8.39      ef\n B       I           7.97 0.502 Inf     6.985      8.95       f\n B       II          7.97 0.502 Inf     6.985      8.95       f\n C       I           5.32 0.550 Inf     4.238      6.40     de \n C       III         5.76 0.672 Inf     4.441      7.08     def\n C       II          5.98 0.582 Inf     4.835      7.12      ef\n D       II          1.39 0.349 Inf     0.709      2.08  a     \n D       III         1.76 0.417 Inf     0.938      2.57  ab    \n D       I           2.03 0.502 Inf     1.048      3.02  ab    \n E       I           4.34 0.739 Inf     2.888      5.78   bcde \n E       II          5.00 0.597 Inf     3.829      6.17    cde \n E       III         5.58 0.616 Inf     4.369      6.78     def\n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 15 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nIn der Abbildung 48.2 siehst du dann einmal die Ausgabe des multipen Gruppenvergleichs visualisiert. Hier wäre es dann schon fast sinnvoll über die Blöcke zu mitteln und daher den Block aus dem Modell zu nehmen. Der Block hat keinen Effekt und sorgt aber für noch mehr Vergleiche, die gerechnet werden müssen. Du kannst dann ja oben nochmal die Funktion emmeans() anpassen und gemittelt über alle Blöcke mit der Option ~ variety rechnen. Die Funktion str_trim() entfernt dann noch die Leerzeichen von dem compact letter display und zentriert damit alles schön.\n\nemm_obj |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = variety, y = mean.class, fill = block)) +\n  theme_minimal() + \n  labs(y = \"Mittlere Boniturnote\", x = \"Sorte\",\n       fill = \"Block\") +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_text(aes(label = str_trim(.group), y = mean.class + SE + 0.01),  \n            position = position_dodge(width = 0.9), vjust = -0.25) +\n  geom_errorbar(aes(ymin = mean.class-SE, ymax = mean.class+SE),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 48.2— Säulendigramm der mittleren Boniturnote des Weizen aus einer ordinalen Regression. Das compact letter display wird dann in {emmeans} generiert. Teilweise kommt das compact letter display an seine visuellen Grenzen. Hier wäre es dann schon fast sinnvoll über die Blöcke zu mitteln und daher den Block aus dem Modell zu nehmen.\n\n\n\n\n\n\n\n\n\n\n\nAnwendungsbeispiel: Gruppenvergleich für eine Bonitur\n\n\n\nIm folgenden Beispiel schauen wir uns nochmal ein praktische Auswertung von einem agrarwissenschaftlichen Beispiel mit Brokkoli an. Wir haben uns in diesem Experiment verschiedene Dosen fert_amount von einem Dünger aufgebracht sowie verschiedene Zeitpunkte der Düngung fert_time berücksichtigt. Ziel ist es die Boniturnoten für den Stamm von Brokkoli miteinander zu vergleichen. Auch hier haben wir einige Besonderheiten in den Daten, da nicht jede Faktorkombination vorliegt. Wir ignorieren aber diese Probleme und rechnen einfach stumpf unseren Gruppenvergleich. Wir müssen aber ein paar Anpassungen durchführen. Unsere Noten müssen ein geordneter Faktor sein, daher nutzen wir die Funktion ordered. Darüber hinaus schmeißen wir die Düngerzeit early aus den Daten, da wir zu dem Zeitpunkt keine hohlen Stämme bonitiert haben.\n\nbroc_tbl &lt;- read_excel(\"data/broccoli_weight.xlsx\") |&gt; \n  mutate(fert_time = factor(fert_time, levels = c(\"none\", \"early\", \"late\")),\n         fert_amount = as_factor(fert_amount),\n         block = as_factor(block),\n         stem_hollowness_num = stem_hollowness,\n         stem_hollowness = ordered(stem_hollowness)) |&gt;\n  filter(fert_time == \"early\") |&gt; \n  select(fert_time, fert_amount, block, stem_hollowness, stem_hollowness_num) |&gt; \n  droplevels() |&gt; \n  na.omit()\n\nIm Folgenden einmal die Tabelle mit tabyl aus dem R Paket {janitor}. Mehr dazu auf der Hilfeseite zu tabyls: a tidy, fully-featured approach to counting things. Wir sehen, dass wir kaum schlechte Noten erhalten. Fast alle Brokkoliköpfe haben eine 1 erhalten, was für intakte Köpfe ohne einen hohlen Stamm spricht.\n\nbroc_tbl |&gt; \n  tabyl(stem_hollowness, fert_amount) \n\n stem_hollowness 150 225 300\n               1 213 329 154\n               2   6  47  15\n               3   4  32  22\n               4   2  14  17\n               5   1  10   4\n               6   0   3   2\n               8   0   1   0\n\n\nDie Tabelle sehen wir dann auch nochmal in der Abbildung 48.3 als Violinplot visualisiert. Wir sehen hier nochmal sehr drastisch, dass wir kaum Noten großer als Eins in den Daten vorliegen haben.\n\nggplot(broc_tbl, aes(fert_amount, stem_hollowness_num, fill = fert_amount)) +\n  theme_minimal() +\n  geom_violin() +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9)) +\n  scale_fill_okabeito() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nAbbildung 48.3— Violinplot der Boniturnoten der Brokkolistämme. Klar ist die sehr schiefe Verteilung der Boniturnoten zu erkennen.\n\n\n\n\n\nWir rechnen jetzt natürlich trotzdem eine ordinale Regression mit der Funktion clm(). Wir setzen die Option theshold = \"symmetric\" da wir davon ausgehen, dass unsere Noten alle den gleichen Abstand zueinander haben.\n\nclm_fit &lt;- clm(stem_hollowness ~ fert_amount, \n               data = broc_tbl, threshold = \"symmetric\")\n\nJetzt rechnen wir in den beiden folgenden Tabs einmal die ANOVA und dann auch den multiplen Gruppenvergleich mit {emmeans}. Da wir hier ordinale Daten haben, können wir dann nicht einfach die Standardverfahren nehmen. Wir entscheiden uns dann für den Standardfehler bei der Darstellung.\n\nANOVA mit anova()Gruppenvergleich mit emmeans()\n\n\nWir rechnen hier einmal die ANOVA und nutzen den \\(\\mathcal{X}^2\\)-Test für die Ermittelung der p-Werte. Wir müssen hier einen Test auswählen, da per Standardeinstellung kein Test gerechnet wird. Wir machen dann die Ausgabe nochmal schöner und fertig sind wir.\n\nclm_fit |&gt; \n  anova() |&gt; \n  model_parameters()\n\nParameter   | Chi2(2) |      p\n------------------------------\nfert_amount |   35.82 | &lt; .001\n\nAnova Table (Type 1 tests)\n\n\nWir sehen, dass der die Menge des Düngers signifikant ist. Wir haben nicht mehr Faktoren in dem Modell gehabt, so dass wir hier auch keine weiteren Aussagen tätigen können.\n\n\nIm Folgenden rechnen wir einmal für den Faktor fert_amount einen Gruppenvergleich. Wir setzen hier die Option mean.class damit wir dann die mittleren Noten wiedergegeben bekommen. Mit den mittleren Noten können wir dann ein Säulendiagramm erstellen. Dann adjustieren wir noch nach Bonferroni und sind fertig.\n\nemm_obj &lt;- clm_fit |&gt; \n  emmeans(~ fert_amount, mode = \"mean.class\") |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\")\nemm_obj\n\n fert_amount mean.class     SE  df asymp.LCL asymp.UCL .group\n 150               1.11 0.0308 Inf      1.04      1.18  a    \n 225               1.50 0.0485 Inf      1.39      1.62   b   \n 300               1.61 0.0753 Inf      1.43      1.79   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: bonferroni method for 3 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nDas emm_obj Objekt werden wir dann gleich einmal in {ggplot} visualisieren. Die mean.class stellt den mittleren Noten des Brokkoli für die Menge der Düngung dar. Wir nutzen dann auch hier die Standardfehler für die Abbildungen, da wir sonst schnelle Werte kleiner 0 und größer 9 erhalten. Dann können wir zum Abschluss auch das compact letter display anhand der Abbildung interpretieren.\nGerade in diesem Beispiel bietet es sich an, dass wir explizit mindestens einen Notenpunktunterschied vorliegen haben wollen, damit wir von einem relevanten Unterschied sprechen können. Die Funktion cld() erlaubt es ein \\(\\Delta\\) zu definieren, dass mindestens überschritten sein muss, damit wir einen Unterschied feststellen. Alles was in dem Bereich \\(\\pm\\Delta\\) liegt, gilt dann als gleich auch wenn es sonst signifikant wäre. Deshalb setzen wir die Option delta = 1 um sicherzustellen, dass nur relevante Unterschiede auch als solche angezeigt werden. Daher haben wir nach der Anpassung auch keine signifikanten Unterschiede mehr. Alle Gruppen sind gleich.\n\nclm_fit |&gt; \n  emmeans(~ fert_amount, mode = \"mean.class\") |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\", delta = 1)\n\n fert_amount mean.class     SE  df asymp.LCL asymp.UCL .equiv.set\n 150               1.11 0.0308 Inf      1.04      1.18  a        \n 225               1.50 0.0485 Inf      1.39      1.62  a        \n 300               1.61 0.0753 Inf      1.43      1.79  a        \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: bonferroni method for 3 tests \nStatistics are tests of equivalence with a threshold of 1 \nP values are left-tailed \nsignificance level used: alpha = 0.05 \nEstimates sharing the same symbol test as equivalent \n\n\n\n\n\nIn der Abbildung 52.22 siehst du das Ergebnis der Auswertung in einem Säulendiagramm. Zwar sind die mittleren Boniturnoten signifikant unterschiedlich, aber ist der Effekt auf der vollen Boniturnotenskala kaum zu sehen. Wir haben es hier mit knapp einer Notenstufe Unterschied zu tun und das ist wirklich wenig. Daher wäre hier mal der Fall, dass wir einen signifikanten aber nicht relevanten Unterschied vorliegen haben.\n\nemm_obj |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = fert_amount, y = mean.class, fill = fert_amount)) +\n  theme_minimal() + \n  labs(y = \"Mittlere Boniturnote\", x = \"Düngemenge [mg/l]\",\n       fill = \"Düngemenge [mg/l]\") +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = str_trim(.group), y = mean.class + SE + 0.01), vjust = -0.25) +\n  geom_errorbar(aes(ymin = mean.class-SE, ymax = mean.class+SE),\n                width = 0.2) +\n  scale_fill_okabeito() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(breaks = c(0, 0.5, 1.0, 1.5, 2.0), limits = c(0, 2))\n\nemm_obj |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = fert_amount, y = mean.class, fill = fert_amount)) +\n  theme_minimal() + \n  labs(y = \"Mittlere Boniturnote\", x = \"Düngemenge [mg/l]\",\n       fill = \"Düngemenge [mg/l]\") +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = str_trim(.group), y = mean.class + SE + 0.01), vjust = -0.25) +\n  geom_errorbar(aes(ymin = mean.class-SE, ymax = mean.class+SE),\n                width = 0.2) +\n  scale_fill_okabeito() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(breaks = 0:9, limits = c(0,9))\n\n\n\n\n\n\n\n\n\n\n\n(a) Reduzierte Boniturskala.\n\n\n\n\n\n\n\n\n\n\n\n(b) Volle Boniturskala.\n\n\n\n\n\n\n\nAbbildung 48.4— Säulendigramm der mittleren Boniturnote des Brokkoli aus einer ordinalen Regression. Das clm()-Modell berechnet das mittlere Boniturnote für den Faktor fert_amount. Das compact letter display wird dann in {emmeans} generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multinomiale / Ordinale Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-multinom.html#sec-multinom",
    "href": "stat-modeling-multinom.html#sec-multinom",
    "title": "48  Multinomiale / Ordinale Regression",
    "section": "48.6 Multinomiale logistische Regression",
    "text": "48.6 Multinomiale logistische Regression\nWas machen wir in eine multinomialen logistische Regression? Im Gegensatz zu der ordinalen Regression haben wir in der multinominalen Regression keine Ordnung in unserem Outcome. Das macht die Sache dann schon eine Nummer komplizierter. Und wir lösen dieses Problem indem wir ein Level des Outcomes oder eben eine Kategorie des Outcomes als Referenz definieren. Dann haben wir wieder unsere Ordnung drin. Und die Definition der Referenz ist auch manchmal das schwerste Unterfangen. Wenn ich keine Ordnung in meinem Outcome habe, wie soll ich dann die Referenz bestimmen? Aber das ist dann immer eine Frage an den konkreten Datensatz. Hier basteln wir uns ja die Fragestellung so hin, dass es passt.\n\n\nIch verweise gerne hier auf das tolle Tutorium Multinomial Logistic Regression | R Data Analysis Examples. Hier erfährst du noch mehr über die Analyse der multinominale logistischen Regression.\nUm eine Referenz in dem Outcome zu definieren nutzen wir die Funktion relevel() und setzen als unsere Referenz das Level frail aus unserem Outcome frailty. Wir hätten auch jedes andere Level als Referenz nehmen können. Zu dieser Referenz werden wir jetzt unser Modell anpassen. Ich nehme immer als Referenz das schlechteste im Sinne von nicht gut. In unserem Fall ist das eben das Level frail.\n\npig_tbl &lt;- pig_tbl |&gt; \n  mutate(frailty_fac = relevel(frailty_fac, ref = \"frail\"))\n\nNachdem wir unsere Referenz definiert haben, können wir wieder recht einfach mit der Funktion multinom() aus dem Paket {nnet} die multinominalen Regression rechnen. Ich mache keinen Hehl daraus. Ich mag die Funktion nicht, da die Ausgabe der Funktion sehr unsortiert ist und uns nicht gerade die Arbeit erleichtert. Auch schweigt die Funktion nicht, sondern muss immer eine Ausgabe wiedergeben. Finde ich sehr unschön.\n\nmultinom_fit &lt;- multinom(frailty_fac ~ age + sex + location + activity + crp + bloodpressure + weight + creatinin, \n                         data = pig_tbl)\n\n# weights:  36 (22 variable)\ninitial  value 452.628263 \niter  10 value 403.652821\niter  20 value 392.117661\niter  30 value 391.553171\nfinal  value 391.549041 \nconverged\n\n\nDie Standardausgabe von multinom() hat wiederum keine \\(p\\)-Werte und wir könnten uns über die Funktion pnorm() wiederum aus den \\(t\\)-Werten unsere \\(p\\)-Werte berechnen. Leider erspart sich multinom() selbst den Schritt die \\(t\\)-Werte zu berechnen, so dass wir die \\(t\\)-Werte selber berechnen müssen. Nicht das es ein Problem wäre, aber schön ist das alles nicht. Im Folgenden siehst du dann einmal die Berechnung der \\(p\\)-Werte über die Berechnung der Teststatistik.\n\nz_mat &lt;- summary(multinom_fit)$coefficients/summary(multinom_fit)$standard.errors\np_n &lt;- (1 - pnorm(abs(z_mat), 0, 1)) * 2\np_n\n\n          (Intercept)       age    sexmale locationnortheast locationnorthwest\nrobust      0.9330415 0.8633078 0.16908717         0.4269151         0.6194343\npre-frail   0.7114684 0.1360834 0.03831569         0.7078459         0.6316654\n          locationwest   activity       crp bloodpressure    weight creatinin\nrobust       0.6632781 0.09080926 0.3594687     0.1421612 0.1134502 0.7118954\npre-frail    0.7383270 0.06115011 0.5643200     0.4895252 0.0683936 0.6178310\n\n\nJetzt müssten wir diese \\(pp\\)-Werte aus der Matrix noch mit unseren Koeffizienten verbauen und da hört es dann bei mir auf. Insbesondere da wir ja mit model_parameters() eine Funktion haben, die uns in diesem Fall wirklich gut helfen kann. Wir nehmen hier zwar die \\(t\\)-Verteilung an und haben damit leicht höre \\(p\\)-Werte, aber da wir eine so große Anzahl an Beobachtungen haben, fällt dieser Unterschied nicht ins Gewicht.\n\nmultinom_fit |&gt; model_parameters(exponentiate = TRUE)\n\n# Response level: robust\n\nParameter            | Odds Ratio |   SE |           95% CI |     z |     p\n---------------------------------------------------------------------------\n(Intercept)          |       1.50 | 7.27 | [0.00, 19843.63] |  0.08 | 0.933\nage                  |       0.99 | 0.03 | [0.93,     1.06] | -0.17 | 0.863\nsex [male]           |       0.54 | 0.24 | [0.23,     1.29] | -1.38 | 0.169\nlocation [northeast] |       1.45 | 0.67 | [0.58,     3.60] |  0.79 | 0.427\nlocation [northwest] |       0.82 | 0.32 | [0.38,     1.77] | -0.50 | 0.619\nlocation [west]      |       0.82 | 0.37 | [0.34,     1.98] | -0.44 | 0.663\nactivity             |       1.22 | 0.14 | [0.97,     1.54] |  1.69 | 0.091\ncrp                  |       0.91 | 0.10 | [0.73,     1.12] | -0.92 | 0.359\nbloodpressure        |       1.07 | 0.05 | [0.98,     1.18] |  1.47 | 0.142\nweight               |       0.84 | 0.09 | [0.68,     1.04] | -1.58 | 0.113\ncreatinin            |       1.04 | 0.11 | [0.84,     1.29] |  0.37 | 0.712\n\n# Response level: pre-frail\n\nParameter            | Odds Ratio |   SE |          95% CI |     z |     p\n--------------------------------------------------------------------------\n(Intercept)          |       0.15 | 0.77 | [0.00, 3351.94] | -0.37 | 0.711\nage                  |       1.06 | 0.04 | [0.98,    1.13] |  1.49 | 0.136\nsex [male]           |       0.38 | 0.18 | [0.16,    0.95] | -2.07 | 0.038\nlocation [northeast] |       1.20 | 0.60 | [0.46,    3.17] |  0.37 | 0.708\nlocation [northwest] |       0.82 | 0.34 | [0.37,    1.84] | -0.48 | 0.632\nlocation [west]      |       1.17 | 0.54 | [0.47,    2.90] |  0.33 | 0.738\nactivity             |       1.26 | 0.16 | [0.99,    1.61] |  1.87 | 0.061\ncrp                  |       0.94 | 0.11 | [0.75,    1.17] | -0.58 | 0.564\nbloodpressure        |       1.04 | 0.05 | [0.94,    1.14] |  0.69 | 0.490\nweight               |       0.81 | 0.09 | [0.65,    1.02] | -1.82 | 0.068\ncreatinin            |       1.06 | 0.12 | [0.85,    1.33] |  0.50 | 0.618\n\n\nWas sehen wir? Zuerst haben wir etwas Glück. Den unsere Referenzlevel macht dann doch Sinn. Wir vergleichen ja das Outcomelevel robust zu frail und das Outcomelevel pre-frail zu frail. Dann haben wir noch das Glück, dass durch unsere Ordnung dann auch frail das schlechtere Outcome ist, so dass wir die \\(OR\\) als Risiko oder als protektiv interpretieren können. Nehmen wir als Beispiel einmal die Variable crp. Der CRP Wert höht das Risiko für frail. Das macht schonmal so Sinn. Und zum anderen ist der Effekt bei dem Vergleich von pre-frail zu frail mit \\(1.16\\) nicht so große wie bei robust zu frail mit \\(1.26\\). Das macht auch Sinn. Deshalb passt es hier einigermaßen.\nIn Tabelle 48.3 sehen wir nochmal die Ausgabe von einer multinominalen Regression durch die Funktion tbl_regression() aufgearbeitet.\n\nmultinom_fit |&gt; \n  tbl_regression(exponentiate = TRUE) |&gt; \n  as_flex_table()\n\n\n\nTabelle 48.3— Tabelle der Ergebnisse der multinominalen Regression.\n\n\n\nOutcomeCharacteristicOR195% CI1p-valuerobustage0.990.93, 1.060.9sexfemale——male0.540.23, 1.290.2locationnorth——northeast1.450.58, 3.600.4northwest0.820.38, 1.770.6west0.820.34, 1.980.7activity1.220.97, 1.540.091crp0.910.73, 1.120.4bloodpressure1.070.98, 1.180.14weight0.840.68, 1.040.11creatinin1.040.84, 1.290.7pre-frailage1.060.98, 1.130.14sexfemale——male0.380.16, 0.950.038locationnorth——northeast1.200.46, 3.170.7northwest0.820.37, 1.840.6west1.170.47, 2.900.7activity1.260.99, 1.610.061crp0.940.75, 1.170.6bloodpressure1.040.94, 1.140.5weight0.810.65, 1.020.068creatinin1.060.85, 1.330.61OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nLeider wird die Sache mit einer multinominalen Regression sehr unangenehm, wenn wir wirklich nicht sortierbare Level im Outcome haben. Dann haben wir aber noch ein Möglichkeit der multinominalen Regression zu entkommen. Wir rechnen einfach separate logistische Regressionen. Die logistischen Regressionen können wir dann ja separat gut interpretieren.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multinomiale / Ordinale Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-multinom.html#logistische-regression-als-ausweg",
    "href": "stat-modeling-multinom.html#logistische-regression-als-ausweg",
    "title": "48  Multinomiale / Ordinale Regression",
    "section": "48.7 Logistische Regression als Ausweg",
    "text": "48.7 Logistische Regression als Ausweg\n\n\n\n\n\n\nBitte Beachten bei der Berechung über separate logistische Regressionen\n\n\n\nDurch die Verwendung von separaten logistischen Regressionen vermindern wir die Fallzahl je gerechneter Regression, so dass wir größere \\(p\\)-Werte erhalten werden als in einer multinominalen Regression. Oder andersherum, durch die verminderte Fallzahl in den separaten logistischen Regressionen haben wir eine geringere Power einen signifikanten Unterschied nachzuweisen.\n\n\nEs gibt den einen Ring um sich zu knechten. Und das ist die logistische Regression. Gut die logistische Regression hilft jetzt nicht, wenn es mit Boniturnoten zu tun hast, aber wenn wir wenige Level im Outcome haben. In unserem Fall haben wir ja drei Level vorliegen, da können wir dann jeweils ein Level rausschmeißen und haben dann nur noch ein binäres Outcome. Das ist auch die zentrale Idee. Wir entfernen immer alle Level bis wir nur noch zwei Level in unserem Outcome haben und rechnen für diese beiden Level dann eine logistische Regression.\nSchauen wir uns erstmal an, wie sich die Daten über die drei Kategorien in unserem Outcome verteilen. Wenn wir eine Kategorie im Outcome kaum vorliegen haben, könnten wir diese Daten vielleicht mit einer anderen Kategorie zusammenlegen oder aber müssen von unserer Idee hier Abstand nehmen.\n\npig_tbl$frailty_fac |&gt; tabyl()\n\n pig_tbl$frailty_fac   n   percent\n               frail  55 0.1334951\n              robust 214 0.5194175\n           pre-frail 143 0.3470874\n\n\nWir haben nicht so viele Beobachtungen in der Kategorie frail. Wir könnten also auch die beiden Faktorlevel pre-frail und frail zusammenlegen. Das R Paket {forcats} liefert sehr viele Funktion, die dir helfen Faktoren zu kodieren und zu ändern.\n\npig_tbl$frailty_fac |&gt; \n  fct_recode(frail_pre_frail = \"frail\", frail_pre_frail = \"pre-frail\") |&gt; \n  tabyl()\n\n fct_recode(pig_tbl$frailty_fac, frail_pre_frail = \"frail\", frail_pre_frail = \"pre-frail\")\n                                                                           frail_pre_frail\n                                                                                    robust\n   n   percent\n 198 0.4805825\n 214 0.5194175\n\n\nDas ist jetzt aber nur eine Demonstration für die Zusammenlegung. Wir wollen jetzt trotzdem unsere drei logistischen Regressionen rechnen. Warum drei? Wir haben ja drei Level in unserem Outcome und wir werden jetzt uns drei Datensätze so bauen, dass in jdem Datensatz unser Outcome immer nur zwei Level hat. Die einzelnen Datensätze speichern wir dann in einer Liste.\n\npig_lst &lt;- list(robust_prefrail = filter(pig_tbl, frailty_fac %in% c(\"robust\", \"pre-frail\")),\n                robust_frail = filter(pig_tbl, frailty_fac %in% c(\"robust\", \"frail\")),\n                prefrail_frail = filter(pig_tbl, frailty_fac %in% c(\"pre-frail\", \"frail\")))\n\nWir können das auch fancy. Und das demonstriere ich dann mal hier. Wenn wir die Funktion combn() nutzen erhalten wir eine Liste mit allen zweier Kombinationen wieder. Diese Liste können wir dann in die Funktion map() stecken, die dann über die Liste unserer Kombinationen iteriert. Pro Liste filtern map() dann den Datensatz für uns heraus. Ja, ist ein wenig over the top, aber ich wollte das mal für mich mit map() ausprobieren und es passte hier so schön.\n\npig_fancy_lst &lt;- combn(c(\"robust\", \"pre-frail\", \"frail\"), 2, simplify = FALSE) |&gt; \n  map(~filter(pig_tbl, frailty_fac %in% .x)) \n\nEgal wie du auf die Liste gekommen bist, wir müssen noch die überflüssigen Level droppen. Keine Ahnung was das deutsche Wort ist. Vermutlich ist das deutsche Wort dann entfernen. Dann können wir für jeden der Listeneinträge die logistische Regression rechnen. Am Ende lassen wir uns noch die exponierten Modellfits ausgeben. In der letzten Zeile entferne ich noch den Intercept von der Ausgabe des Modells. Den Intercept brauchen wir nun wirklich nicht.\n\npig_lst |&gt; \n  map(~mutate(.x, frailty_fac = fct_drop(frailty_fac))) |&gt; \n  map(~glm(frailty_fac ~ age + sex + location + activity + crp + bloodpressure + weight + creatinin, \n           data = .x, family = binomial)) |&gt; \n  map(model_parameters, exponentiate = TRUE) |&gt; \n  map(extract, -1, )\n\n$robust_prefrail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.06 | 0.03 | [1.01, 1.12] |  2.40 | 0.017\nsex [male]           |       0.70 | 0.21 | [0.39, 1.27] | -1.16 | 0.246\nlocation [northeast] |       0.80 | 0.26 | [0.42, 1.49] | -0.70 | 0.484\nlocation [northwest] |       0.97 | 0.28 | [0.55, 1.72] | -0.09 | 0.929\nlocation [west]      |       1.38 | 0.44 | [0.75, 2.57] |  1.03 | 0.302\nactivity             |       1.04 | 0.09 | [0.88, 1.23] |  0.48 | 0.631\ncrp                  |       1.03 | 0.08 | [0.88, 1.20] |  0.33 | 0.743\nbloodpressure        |       0.96 | 0.03 | [0.90, 1.03] | -1.04 | 0.300\nweight               |       0.97 | 0.08 | [0.83, 1.13] | -0.39 | 0.695\ncreatinin            |       1.02 | 0.08 | [0.87, 1.19] |  0.21 | 0.834\n\n$robust_frail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.00 | 0.04 | [0.93, 1.07] |  0.03 | 0.973\nsex [male]           |       0.52 | 0.23 | [0.22, 1.24] | -1.46 | 0.145\nlocation [northeast] |       1.45 | 0.68 | [0.59, 3.79] |  0.79 | 0.430\nlocation [northwest] |       0.85 | 0.33 | [0.39, 1.82] | -0.42 | 0.674\nlocation [west]      |       0.88 | 0.40 | [0.37, 2.18] | -0.28 | 0.780\nactivity             |       1.23 | 0.15 | [0.97, 1.58] |  1.69 | 0.090\ncrp                  |       0.92 | 0.10 | [0.75, 1.14] | -0.75 | 0.453\nbloodpressure        |       1.07 | 0.05 | [0.97, 1.18] |  1.36 | 0.172\nweight               |       0.84 | 0.10 | [0.67, 1.04] | -1.57 | 0.116\ncreatinin            |       1.04 | 0.11 | [0.84, 1.29] |  0.36 | 0.717\n\n$prefrail_frail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.05 | 0.04 | [0.98, 1.12] |  1.26 | 0.208\nsex [male]           |       0.40 | 0.19 | [0.15, 0.99] | -1.94 | 0.053\nlocation [northeast] |       1.12 | 0.57 | [0.42, 3.11] |  0.22 | 0.825\nlocation [northwest] |       0.77 | 0.33 | [0.33, 1.78] | -0.60 | 0.548\nlocation [west]      |       1.05 | 0.51 | [0.41, 2.74] |  0.11 | 0.914\nactivity             |       1.22 | 0.15 | [0.97, 1.55] |  1.69 | 0.092\ncrp                  |       0.95 | 0.11 | [0.74, 1.20] | -0.46 | 0.649\nbloodpressure        |       1.03 | 0.05 | [0.93, 1.14] |  0.57 | 0.566\nweight               |       0.84 | 0.09 | [0.67, 1.03] | -1.65 | 0.100\ncreatinin            |       1.04 | 0.13 | [0.82, 1.33] |  0.34 | 0.734\n\n\nEine Sache ist super wichtig zu wissen. Wie oben schon geschrieben, durch die Verwendung von separaten logistischen Regressionen vermindern wir die Fallzahl je Regression, so dass wir größere \\(p\\)-Werte erhalten werden, als in einer multinominalen Regression. Das ist der Preis, den wir dafür bezahlen müssen, dass wir besser zu interpretierende Koeffizienten erhalten. Und das ist auch vollkommen in Ordnung. Ich selber habe lieber Koeffizienten, die ich interpretieren kann, als unklare Effekte mit niedrigen \\(p\\)-Werten.\nSchauen wir einmal auf unseren Goldstandard, der Variable für den CRP-Wert. Die Variable haben wir ja jetzt immer mal wieder in diesem Kapitel interpretiert und uns angeschaut. Die Variable crp passt von dem Effekt jedenfalls gut in den Kontext mit rein. Die Effekte sind ähnlich wie in der multinominalen Regression. Wir haben eben nur größere \\(p\\)-Werte. Jetzt müssen wir entscheiden, wir können vermutlich die getrennten logistischen Regressionen besser beschreiben und interpretieren. Das ist besonders der Fall, wenn wir wirklich Probleme haben eine Referenz in der multinominalen Regression festzulegen. Dann würde ich immer zu den getrennten logistischen Regressionen greifen als eine schief interpretierte multinominale Regression.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multinomiale / Ordinale Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-multinom.html#referenzen",
    "href": "stat-modeling-multinom.html#referenzen",
    "title": "48  Multinomiale / Ordinale Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 48.1— Dotplot des Datenbeispiels für die Bonitur von fünf Weizensorten.\nAbbildung 48.2— Säulendigramm der mittleren Boniturnote des Weizen aus einer ordinalen Regression. Das compact letter display wird dann in {emmeans} generiert. Teilweise kommt das compact letter display an seine visuellen Grenzen. Hier wäre es dann schon fast sinnvoll über die Blöcke zu mitteln und daher den Block aus dem Modell zu nehmen.\nAbbildung 48.3— Violinplot der Boniturnoten der Brokkolistämme. Klar ist die sehr schiefe Verteilung der Boniturnoten zu erkennen.\nAbbildung 48.4 (a)— Reduzierte Boniturskala.\nAbbildung 48.4 (b)— Volle Boniturskala.\n\n\n\nDormann CF. 2013. Parametrische Statistik. Springer.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Multinomiale / Ordinale Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html",
    "href": "stat-modeling-logistic.html",
    "title": "49  Logistische Regression",
    "section": "",
    "text": "49.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Binomialverteilung. Damit finden wir im Outcome nur \\(0\\) oder \\(1\\) Werte. Im folgenden Kapitel zu der multiplen logistischen linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Binomailverteilung. Damit finden wir im Outcome nur \\(0\\) oder \\(1\\) Werte. Das ist hier sehr wichtig, denn wir wollen ja eine multiple logistische lineare Regression rechnen. In R nutzen wir dazu die Funktion glm() mit der Option family = binomial.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nDann würde das Modell in R wie folgt aussehen.\nglm(y ~ x_1 + x_2, ..., family = binomial)\nNeben der Möglichkeit, dass wir \\(0/1\\) im Outcome \\(y\\) haben, könnten wir auch unser Outcome anders beschreiben. Wir nutzen dann dafür das Wilkinson-Rogers Format welches dann den Anteil an Erfolgen an Fehlschlägen beschreibt. Wir schreiben aber in das Modell die konkrete Anzahl an Erfolgen und Fehlschlägen.\n\\[\n(Success|Failure) \\sim x_1 + x_2 + ... + x_p\n\\]\nIn R würden wir dann die zwei Spalten mit der Anzahl an Erfolgen und Fehlschlägen mit cbind() zusammenfassen und in glm() ergänzen. Daher würden wir dann in R wie folgt schreiben.\nglm(cbind(success, failure) ~ x_1 + x_2, ..., family = binomial)\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier am Anfang des Kapitels nicht um einen reinen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 49.8 nochmal nachlesen, wir du dann das Modell weiterverwendest.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#annahmen-an-die-daten",
    "href": "stat-modeling-logistic.html#annahmen-an-die-daten",
    "title": "49  Logistische Regression",
    "section": "",
    "text": "Wenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 43 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffällige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 41 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 42 bei der Variablenselektion.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#genutzte-r-pakete",
    "href": "stat-modeling-logistic.html#genutzte-r-pakete",
    "title": "49  Logistische Regression",
    "section": "49.2 Genutzte R Pakete",
    "text": "49.2 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, gtsummary, see,\n               tidymodels, cutpointr, emmeans, multcomp,\n               conflicted)\nconflicts_prefer(yardstick::accuracy)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(magrittr::extract)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#daten",
    "href": "stat-modeling-logistic.html#daten",
    "title": "49  Logistische Regression",
    "section": "49.3 Daten",
    "text": "49.3 Daten\nIn diesem Kapitel nutzen wir die infizierten Ferkel als Beispieldatensatz. Wir haben in dem Datensatz über vierhundert Ferkel untersucht und festgehalten, ob die Ferkel infiziert sind (\\(1\\), ja) oder nicht infiziert (\\(0\\), nein). Wir haben daneben noch eine ganze Reihe von Risikofaktoren erhoben. Hier sieht man mal wieder wie wirr die Sprache der Statistik ist. Weil wir rausfinden wollen welche Variable das Risiko für die Infektion erhöht, nennen wir diese Variablen Risikofaktoren. Obwohl die Variablen gar keine kategorialen Spalten sind bzw. nicht alle. So ist das dann in der Statistik, ein verwirrender Begriff jagt den Nächsten.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") \n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der Tabelle 49.1 an.\n\n\n\n\nTabelle 49.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\ninfected\n\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\n1\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\n1\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\n0\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.15\n19.05\n4.35\n1\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n55.38\n18.44\n5.27\n1\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n60.29\n18.42\n4.78\n1\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\npre-frail\n55.32\n19.75\n3.92\n1\n\n\n56\nmale\nwest\n13.91\n20.8\nfrail\n58.37\n17.28\n7.44\n0\n\n\n57\nmale\nnorthwest\n12.49\n21.95\npre-frail\n56.66\n16.86\n2.44\n1\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\n1\n\n\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\n0\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\n1\n\n\n\n\n\n\n\n\nIn dem nächsten Abschnitt werden wir die Daten nutzen um rauszufinden welche Variablen einen Einfluss auf den Infektionsstatus der Ferkel hat.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#theoretischer-hintergrund",
    "href": "stat-modeling-logistic.html#theoretischer-hintergrund",
    "title": "49  Logistische Regression",
    "section": "49.4 Theoretischer Hintergrund",
    "text": "49.4 Theoretischer Hintergrund\nWir schaffen wir es, durch einen \\(0/1\\) Outcome auf der y-Achse eine gerade Linie durch die Punkte zu zeichnen und die Koeffiziente dieser Gerade zu bestimmen? Immerhin gibt es ja gar keine Werte zwischen \\(0\\) und \\(1\\). In Abbildung 49.1 sehen wir beispielhaft den Zusammenhang zwischen dem Infektionsstatus und der Aktivität der Ferkel. Wir haben zwei horizontale Linien. Wie zeichen wir jetzt da eine Gerade durch?\n\nggplot(pig_tbl, aes(x = activity, y = infected)) +\n  theme_minimal() +\n  geom_point() +\n  scale_y_continuous(breaks = c(0, 1))\n\n\n\n\n\n\n\nAbbildung 49.1— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und der Aktivität der Ferkel.\n\n\n\n\n\nIn der folgenden Abbildung 49.2 sehen wir nochmal die gleiche Darstellung der Daten aus dem R Paket {see} und der entsprechenden Funktion geom_binomdensity(). Leider ist die Nutzung nicht so, wie du {ggplot} gewohnt bist, aber für die Darstellung hier ist es sehr angenehm die Verteilung der Beobachtungen besser zu sehen. Du siehst hier einmal ganz gut, wie sich die \\(0/1\\)-Werte für die infizierten Schweine über die Werte der Aktivität anordnen. In der obigen Abbdilung sind dann eben die Punkte alle übereinander angeordnet.\n\nggplot() +\n  theme_minimal() +\n  geom_binomdensity(pig_tbl, x = \"activity\", y = \"infected\") +\n  scale_y_continuous(breaks = c(0, 1))\n\n\n\n\n\n\n\nAbbildung 49.2— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und der Aktivität der Ferkel.\n\n\n\n\n\nDer Trick hierbei ist wieder die Transformation des Zusammenhangs von \\(y \\sim x\\) auf einen \\(\\log\\)-scale. Das heißt wir Rechnen nicht mit den \\(0/1\\) Werten sondern transformieren den gesamten Zusammenhang. Das ist wichtig, den es gibt einen Unterschied zwischen der Transformation von \\(y\\) und der Transformation die hier gemeint ist. Wir halten fest, wir rechnen also nicht auf der ursprünglichen Skala der Daten sondern auf der \\(\\log\\)-scale. Allgemeiner wird auch von der link-Funktion gesprochen, da wir ja verschiedene Möglichkeiten der Transformation des Zusammenhangs haben.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\nWir gehen wir also vor. Zuerst Modellieren wir die Wahrscheinlichkeit für den Eintritt des Ereignisses. Wir machen also aus unseren binären \\(0/1\\) Daten eine Wahrscheinlichkeit für den Eintritt von 1.\n\\[\nY \\rightarrow Pr(Y = 1)\n\\]\nDamit haben wir schon was erreicht den \\(Pr(Y = 1)\\) liegt zwischen \\(0\\) und \\(1\\). Damit haben wir also schon Werte dazwischen. Wenn wir aber normalverteilte Residuen haben wollen, dann müssen unsere Werte von \\(-\\infty\\) bis \\(+\\infty\\) laufen können. Daher rechnen wir im Weiteren die Chance.\n\\[\n\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\n\\] Die Chance (eng. Odds) für das Eintreten von \\(Y=1\\) ist eben die Wahrscheinlichkeit für das Eintreten geteilt durch die Gegenwahrscheinlichkeit. Das ist schon besser, denn damit liegen unsere transformierten Werte für den Zusammenhang schon zwischen \\(0\\) und \\(+\\infty\\). Wenn wir jetzt noch den \\(\\log\\) von den Chancen rechnen, dann haben wir schon fast alles was wir brauchen.\n\\[\n\\log\\left(\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\\right)\n\\]\nDer Logarithmus der Chance liegt dann zwischen \\(-\\infty\\) und \\(+\\infty\\). Deshalb spricht man auch von den \\(\\log\\)-Odds einer logistischen Regression. Auch sieht man hier woher das logistisch kommt. Wir beschreiben im Namen auch gleich die Transformation mit. Am ende kommen wir somit dann auf folgendes Modell.\n\\[\n\\log\\left(\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\\right) = \\beta_0 + \\beta_1 x_1 + ...  + \\beta_p x_p + \\epsilon\n\\] Vielleicht ist dir der Begriff Wahrscheinlichkeit und der Unterschied zur Chance nicht mehr so präsent. Deshalb hier nochmal als Wiederholung oder Auffrischung.\n\nEine Wahrscheinlichkeit beschreibt dem Anteil an Allen. Zum Beispiel den Anteil Gewinner an allen Teilnehmern. Den Anteil Personen mit Therapieerfolg an allen Studienteilnehmern.\nEine Chance oder (eng. Odds) beschreibt ein Verhältnis. Somit das Verhältnis Gewinner zu Nichtgewinner. Oder das Verhältnis Personen mit Therapieerfolg zu Personen ohne Therapieerfolg\n\nNochmal an einem Zahlenbeispiel. Wenn wir ein Glücksspiel haben, in dem es 2 Kombinationen gibt die gewinnen und drei 3 Kombinationen die verlieren, dann haben wir eine Wahrscheinlichkeit zu gewinnen von \\(2 / 5 = 0.40 = 40\\%\\). Wenn wir die Chance zu gewinnen ausrechnen erhalten wir \\(2:3 = 0.67 = 67\\%\\). Wir sehen es gibt einen deutlichen Unterschied zwischen Chance und Wahrscheinlichkeit. Wenn wir große Fallzahl haben bzw. kleine Wahrscheinlichkeiten, dann ist der Unterschied nicht mehr so drastisch. Aber von einer Gleichkeit von Wahrscheinlichkeit und Chance zu sprechen kann nicht ausgegangen werden.\nWas ist nun das Problem? Wir erhalten aus einer logistischen Regression \\(\\log\\)-Odds wieder. Der Effektchätzer ist also eine Chance. Wir werden aber das Ergebnis wie eine Wahrscheinlichkeit interpretieren. Diese Diskrepanz ist wenigen bekannt und ein Grund, warum wir in der Medizin immer uns daran erinnern müssen, was wir eigentlich mit der logistischen Regression aussagen können.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#modellierung",
    "href": "stat-modeling-logistic.html#modellierung",
    "title": "49  Logistische Regression",
    "section": "49.5 Modellierung",
    "text": "49.5 Modellierung\nDie Modellerierung der logistischen Regression ist sehr einfach. Wir nutzen wieder die Formelschreibweise im glm() um unsere Variablen zu definieren. Wenn unser Outcome nicht binär ist, dann jammert R und gibt uns einen Fehler aus. Ich kann hier nur dringlichst raten, das Outcome in \\(0/1\\) zu kodieren mit dem Schlechten als \\(1\\).\nDas glm() muss dann noch wissen, dass es eine logistische Regression rechnen soll. Das machen wir in dem wir als Verteilungsfamilie die Binomialverteilung auswählen. Wir geben also an family = binomial und schon können wir das volle Modell fitten.\n\nlog_fit &lt;- glm(infected ~ age + sex + location + activity + crp + \n                 frailty + bloodpressure + weight + creatinin, \n               data = pig_tbl, family = binomial)\n\nDas war extrem kurz und scherzlos. Also können wir dann auch ganz kurz schauen, ob das Modell einigermaßen funktioniert hat.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#performance-des-modells",
    "href": "stat-modeling-logistic.html#performance-des-modells",
    "title": "49  Logistische Regression",
    "section": "49.6 Performance des Modells",
    "text": "49.6 Performance des Modells\nNachdem wir das Modell gefittet haben, wollen wir uns nochmal das \\(R^2\\) wiedergeben lassen um zu entscheiden, ob unser Modell einigermaßen funktioniert hat. Dieser Abschnitt ist sehr kurz. Wir haben leider nur sehr wenige Möglichkeiten um ein logistischen Modell zu bewerten.\n\nr2(log_fit)\n\n# R2 for Logistic Regression\n  Tjur's R2: 0.285\n\n\nJa, so viel Varianz erklären wir nicht, aber wenn du ein wenig im Internet suchst, dann wirst du feststellen, dass das Bestimmtheitsmaß so eine Sache in glm()’s ist. Wir sind aber einigermaßen zufrieden. Eventuell würde eine Variablenselektion hier helfen, aber das ist nicht Inhalt dieses Kapitels.\nIn Abbildung 49.3 schauen wir nochmal auf die Residuen und die möglichen Ausreißer. Wieder sehen beide Plots einigermaßen in Ordnung aus. Die Abbildungen sind jetzt nicht die Besten, aber ich würde hier auch anhand der Diagnoseplots nicht die Modellierung verwerfen.\n\ncheck_model(log_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\")) \n\n\n\n\n\n\n\nAbbildung 49.3— Ausgabe ausgewählter Modelgüteplots der Funktion check_model().",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#interpretation-des-modells",
    "href": "stat-modeling-logistic.html#interpretation-des-modells",
    "title": "49  Logistische Regression",
    "section": "49.7 Interpretation des Modells",
    "text": "49.7 Interpretation des Modells\nZu Interpretation schauen wir uns wie immer nicht die rohe Ausgabe an, sondern lassen uns die Ausgabe mit der Funktion model_parameters() aus dem R Paket {parameters} wiedergeben. Wir müssen noch die Option exponentiate = TRUE wählen, damit unsere Koeffizienten nicht als \\(\\log\\)-Odds sondern als Odds wiedergeben werden. Korrekterweise erhalten wir die Odds ratio wieder was wir auch als \\(OR\\) angegeben.\n\nmodel_parameters(log_fit, exponentiate = TRUE)\n\nParameter            | Odds Ratio |       SE |       95% CI |     z |      p\n----------------------------------------------------------------------------\n(Intercept)          |   2.91e-11 | 1.24e-10 | [0.00, 0.00] | -5.69 | &lt; .001\nage                  |       1.00 |     0.03 | [0.95, 1.06] |  0.16 | 0.872 \nsex [male]           |       0.74 |     0.26 | [0.37, 1.48] | -0.84 | 0.398 \nlocation [northeast] |       1.07 |     0.40 | [0.52, 2.22] |  0.19 | 0.852 \nlocation [northwest] |       0.62 |     0.20 | [0.33, 1.17] | -1.47 | 0.142 \nlocation [west]      |       0.76 |     0.28 | [0.37, 1.56] | -0.75 | 0.450 \nactivity             |       1.05 |     0.10 | [0.87, 1.27] |  0.51 | 0.612 \ncrp                  |       2.64 |     0.29 | [2.14, 3.32] |  8.70 | &lt; .001\nfrailty [pre-frail]  |       1.12 |     0.46 | [0.49, 2.48] |  0.27 | 0.788 \nfrailty [robust]     |       0.81 |     0.31 | [0.37, 1.72] | -0.54 | 0.588 \nbloodpressure        |       1.09 |     0.04 | [1.01, 1.17] |  2.09 | 0.037 \nweight               |       1.00 |     0.09 | [0.85, 1.19] |  0.06 | 0.955 \ncreatinin            |       1.12 |     0.10 | [0.94, 1.33] |  1.23 | 0.218 \n\n\nWie interpretieren wir nun das \\(OR\\) einer logistischen Regression? Wenn wir darauf gechtet haben, dass wir mit \\(1\\) das Schlechte meinen, dann können wir wir folgt mit dem \\(OR\\) sprechen. Wenn wir ein \\(OR &gt; 1\\) haben, dann haben wir ein Risiko vorliegen. Die Variable mit einem \\(OR\\) größer als \\(1\\) wird die Chance auf den Eintritt des schlechten Ereignisses erhöhen. Wenn wir ein \\(OR &lt; 1\\) haben, dann sprechen wir von einem protektiven Faktor. Die Variable mit einem \\(OR\\) kleiner \\(1\\) wird vor dem Eintreten des schlechten Ereignisses schützen. Schauen wir uns den Zusammenhang mal im Detail für die Ferkeldaten an.\n\n(intercept) beschreibt den Intercept der logistischen Regression. Wenn wir mehr als eine simple Regression vorliegen haben, wie in diesem Fall, dann ist der Intercept schwer zu interpretieren. Wir konzentrieren uns auf die Effekte der anderen Variablen.\nsex beschreibt den Effekt der männlichen Ferkel zu den weiblichen Ferkeln. Daher haben männliche Ferkel eine \\(2.75\\) höhere Chance infiziert zu werden als weibliche Ferkel.\nlocation [northeast], location [northwest] und location [west] beschreibt den Unterschied zur location [north]. Alle Orte haben eine geringere Chance für eine Infektion zum Vergleich der Bauernhöfe im Norden. Zwar ist keiner der Effekte signifikant, aber ein interessantes Ergebnis ist es allemal.\nactivity beschreibt den Effekt der Aktivität der Ferkel. Wenn sich die Ferkel mehr bewegen, dann ist die Chance für eine Infektion gemindert.\ncrp beschreibt den Effekt des CRP-Wertes auf den Infektionsgrad. Pro Einheit CRP steigt die Chance einer Infektion um \\(2.97\\) an. Das ist schon ein beachtlicher Wert.\nfrailty beschreibt die Gebrechlichkeit der Ferkel. Hier müssen wir wieder schauen, zu welchem Level von frailty wir vergleichen. Hier vergleichen wir zu frail. Also dem höchsten Gebrechlichkeitgrad. Ferkel die weniger gebrechlich sind, haben eine niedrigere Chance zu erkranken.\nbloodpressure, weight und creatinin sind alles Variablen, mit einem \\(OR\\) größer als \\(1\\) und somit alles Riskovariablen. Hier sind zwar die \\(OR\\) relativ klein, aber das muss erstmal nichts heißen, da die \\(OR\\) ja hier die Änderung für eine Einheit von \\(x\\) beschreiben. Deshalb musst du immer schauen, wie die Einheiten von kontinuierlichen kodiert Variablen sind.\n\nKommen wir nochmal zu den gänigen Tabellen für die Zusammenfassung eines Ergebnisses einer logistischen Regression. Teilweise sind diese Tabellen so generisch und häufiog verwendet, dass wir schon einen Begriff für diese Tabellen haben. In Tabelle 49.2 siehst du die table 1 für die Übersicht aller Risikovariablen aufgeteilt nach dem Infektionsstatus. Diese Art der Tabellendarstellung ist so grundlegend für eine medizinische Veröffentlichung, dass sich eben der Begriff table 1 etabliert hat. Fast jede medizinische Veröffentlichung hat als erste Tabelle diese Art von Tabelle angegeben. Hierbei ist wichtig, dass die \\(p\\)-Werte alle nur aus einem einfachen statistischen Test stammen. Die \\(p\\)-Werte einer multiplen logistischen Regression werden daher immer anders sein.\n\npig_tbl |&gt; tbl_summary(by = infected) |&gt; add_p() |&gt; as_flex_table()\n\n\n\nTabelle 49.2— Ausgabe der Daten in einer Summary Table oder auch Table 1 genannt. In medizinischen Veröffentlichungen immer die erste Tabelle für die Zusammenfassung der Patienten (hier Ferkel) für jede erhobende Risikovariable.\n\n\n\nCharacteristic0, N = 13611, N = 2761p-value2age59.5 (57.0, 63.0)60.0 (57.0, 63.0)0.9sex0.2female47 (35%)114 (41%)male89 (65%)162 (59%)location0.3north36 (26%)85 (31%)northeast23 (17%)61 (22%)northwest48 (35%)76 (28%)west29 (21%)54 (20%)activity13.40 (12.25, 14.34)13.24 (12.28, 14.54)0.8crp19.12 (18.13, 19.83)20.57 (19.77, 21.46)&lt;0.001frailty0.5frail18 (13%)37 (13%)pre-frail42 (31%)101 (37%)robust76 (56%)138 (50%)bloodpressure56.2 (54.3, 58.5)57.2 (55.1, 59.6)0.021weight18.61 (17.34, 19.41)18.32 (17.19, 19.60)0.8creatinin4.85 (3.67, 5.93)4.86 (4.06, 5.85)0.31Median (IQR); n (%)2Wilcoxon rank sum test; Pearson's Chi-squared test\n\n\n\n\n\nIn Tabelle 49.3 siehst du nochmal für eine Auswahl an Variablen die simplen logistischen Regressionen gerechnet. Du müsst also nicht jede simple logistische Regression selber rechnen, sondern kannst auch die Funktion tbl_uvregression() verwenden. Das R Paket {tbl_summary} erlaubt weitreichende Formatierungsmöglichkeiten. Am bestes schaust du einmal im Tutorial Tutorial: tbl_regression selber nach was du brauchst oder anpassen willst.\n\npig_tbl|&gt;\n  select(infected, age, crp, bloodpressure) |&gt;\n  tbl_uvregression(\n    method = glm,\n    y = infected,\n    method.args = list(family = binomial),\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 2)\n  ) |&gt; as_flex_table()\n\n\n\nTabelle 49.3— Simple logistische Regression für eine Auswahl an Einflussvariablen. Für jede Einflussvariable wurde eine simple logistische Regression gerechnet.\n\n\n\nCharacteristicNOR195% CI1p-valueage4121.000.95, 1.040.88crp4122.622.14, 3.27&lt;0.001bloodpressure4121.081.02, 1.150.0131OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nnun gibt es viele Möglichkeiten sich die logistische Regression wiedergeben zu lassen In Tabelle 49.4 siehst du nochmal die Möglichkeit, die dir das R Paket {tbl_summary} bietet. Am Ende ist es dann eine reine Geschmacksfrage, wie wir die Daten dann aufarbeiten wollen.\n\nlog_fit |&gt; tbl_regression(exponentiate = TRUE) |&gt; as_flex_table()\n\n\n\nTabelle 49.4— Ausgabe der multiplen logistischen Regression durch die Funktion tbl_regression().\n\n\n\nCharacteristicOR195% CI1p-valueage1.000.95, 1.060.9sexfemale——male0.740.37, 1.480.4locationnorth——northeast1.070.52, 2.220.9northwest0.620.33, 1.170.14west0.760.37, 1.560.5activity1.050.87, 1.270.6crp2.642.14, 3.32&lt;0.001frailtyfrail——pre-frail1.120.49, 2.480.8robust0.810.37, 1.720.6bloodpressure1.091.01, 1.170.037weight1.000.85, 1.19&gt;0.9creatinin1.120.94, 1.330.21OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nZum Abschluss wollen wir uns einmal die Ergebnisse des Modellfits als logistischen Gerade für eine simple lineare Regression mit dem Modell \\(infected \\sim crp\\) anschauen. Wie immer können wir uns den Zusammenhang nur in einem simplen Modell anschauen. Im Fall einer multiplen linearen Regresion können wir nicht so viele Dimensionen in einer Grpahik darstellen. Wir fitten also das Modell log_fit_crp wie im folgenden dargestellt.\n\nlog_fit_crp &lt;- glm(infected ~ crp, data = pig_tbl, family = binomial)\n\nNun können wir uns mit der Funktion predict() die Wert auf der Geraden wiedergeben lassen. Wenn wir predict() nur so aufrufen, dann erhalten wir die Werte für \\(y\\) auf der transformierten \\(link\\)-Scale wieder. Das hilft uns aber nicht weiter, wir haben ja nur 0 und 1 Werte für \\(y\\) vorliegen.\n\npredict(log_fit_crp, type = \"link\") |&gt; \n  extract(1:10) |&gt; \n  round(2)\n\n    1     2     3     4     5     6     7     8     9    10 \n 3.19 -0.41 -0.29  0.29  2.41  2.30 -0.08 -0.08  2.72  1.83 \n\n\nDa wir die Werte für die Wahrscheinlichkeit das ein Ferkel infiziert ist, also die Wahrscheinlichkeit \\(Pr(infected = 1)\\), müssen wir noch die Option type = reponse wählen. So erhalten wir die Wahrscheinlichkeiten wiedergegeben.\n\npredict(log_fit_crp, type = \"response\") |&gt; \n  extract(1:10) |&gt; \n  round(2)\n\n   1    2    3    4    5    6    7    8    9   10 \n0.96 0.40 0.43 0.57 0.92 0.91 0.48 0.48 0.94 0.86 \n\n\nAbschließend können wir uns die Gerade auch in der Abbildung 49.4 visualisieren lassen. Auf der x-Achse sehen wir die crp-Werte und auf der y-Achse den Infektionsstatus. Auf der \\(reponse\\)-scale sehen wir eine S-Kurve. Auf der \\(link\\)-scale würden wir eine Gerade sehen.\n\nggplot(pig_tbl, aes(x = crp, y = infected)) +\n  theme_minimal() +\n  geom_point() +\n  geom_line(aes(y = predict(log_fit_crp, type = \"response\")), color = \"red\") \n\n\n\n\n\n\n\nAbbildung 49.4— Visualisierung der logistischen Gerade in einer simplen logistischen Regression mit der Variable crp.\n\n\n\n\n\nNun haben wir das Kapitel zur logistischen Regression fast abgeschlossen. Was noch fehlt ist die Besonderheit der Prädiktion im Kontext des maschinellen Lernens. Das machen wir jetzt im folgenden Abschnitt. Wenn dich die logistische Regression nur interessiert hat um einen kausalen Zusammenhang zwischen Einflussvariablen und dem binären Outcome zu modellieren, dann sind wir hier fertig.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#sec-mult-comp-log-reg",
    "href": "stat-modeling-logistic.html#sec-mult-comp-log-reg",
    "title": "49  Logistische Regression",
    "section": "49.8 Gruppenvergleich",
    "text": "49.8 Gruppenvergleich\nHäufig ist es ja so, dass wir das Modell nur schätzen um dann einen Gruppenvergleich zu rechnen. Das heißt, dass es uns interessiert, ob es einen Unterschied zwischen den Leveln eines Faktors gegeben dem Outcome \\(y\\) gibt. Wir machen den Gruppenvergleich jetzt einmal an der Gebrechlichkeit frailty durch. Wir habe die drei Gruppen frail, pre-frail und robust vorliegen. Wir wollen jetzt wissen, ob es einen Unterschied zwischen den Gruppen hinsichtlich dem Infektionsstatus von den Ferkeln gibt.\nWenn du gerade hierher gesprungen bist, nochmal das simple Modell für unseren Gruppenvergleich. Wir haben hier nur einen Faktor frailty mit in dem Modell. Am Ende des Abschnitts findest du dann noch ein Beispiel mit zwei Faktoren zu beschädigten Mais nach der Ernte.\n\nlog_fit &lt;- glm(infected ~ frailty, data = pig_tbl, family = binomial)\n\nEigentlich ist es recht einfach. Wir rechnen jetzt als erstes die ANOVA. Hier müssen wir dann einmal den Test angeben, der gerechnet werden soll um die p-Werte zu erhalten. Dann nutze ich noch die Funktion model_parameters() um eine schönere Ausgabe zu erhalten.\n\nlog_fit |&gt; \n  anova(test = \"Chisq\") |&gt; \n  model_parameters(drop = \"NULL\")\n\nParameter | df | Deviance | df (error) | Deviance (error) |     p\n-----------------------------------------------------------------\nfrailty   |  2 |     1.48 |        409 |           521.14 | 0.478\n\nAnova Table (Type 1 tests)\n\n\nWir sehen, dass die Gebrechlichkeit der Ferkel keinen signifikanten Einfluss auf den Infektionsstatus hat. Dennoch rechnen wir einmal den Gruppenvergleich. Immerhin geht es hier ja auch um die Demonstration.\nIm folgenden nutzen wir das R Paket {emmeans} wie folgt. Wenn wir die Wahrscheinlichkeiten wiedergeben haben wollen, dann nutzen wir die Option regrid = \"response\". In unserem emmeans-Objekt stehen jetzt die Wahrscheinlichkeiten infiziert zu sein für das jeweilige Level von frailty. Wenn wir aber später die Odds ratio benötigen, dann müssen wir die Option type = \"response\" verwenden. Erstere Option nutze ich später um direkt aus der emmeans Ausgabe ein Säulendiagramm zu erstellen. Auf dem ersten Blick sind beide Ausgaben gleich, aber im Hintergrund werden andere Optionen gesetzt, so dass in folgenden Berechnungen dann was anderes herauskommt.\n\nOption regrid = \"response\"Option type = \"response\"\n\n\nMit dieser Option erhalten wir dann ein emmeans-Objekt in dem die Wahrscheinlichkeiten prob hinterlegt sind. Diese Art der Berechnung eignet sich besonders, wenn später ein compact letter dislay visualisiert werden soll.\n\nem_prob_obj &lt;- log_fit |&gt; \n  emmeans(~ frailty, regrid = \"response\")\nem_prob_obj\n\n frailty    prob     SE  df asymp.LCL asymp.UCL\n frail     0.673 0.0633 Inf     0.549     0.797\n pre-frail 0.706 0.0381 Inf     0.632     0.781\n robust    0.645 0.0327 Inf     0.581     0.709\n\nConfidence level used: 0.95 \n\n\n\n\nMit dieser Option bereiten wir dann die Berechnung von Odds ratios vor. Wir wollen dann auf der ursprünglichen Skala rechnen und nicht auf der Linkfunktion. Wenn du also Odds ratios haben willst, dann musst du diese Option wählen.\n\nem_odds_obj &lt;- log_fit |&gt; \n  emmeans(~ frailty, type = \"response\")\nem_odds_obj\n\n frailty    prob     SE  df asymp.LCL asymp.UCL\n frail     0.673 0.0633 Inf     0.539     0.783\n pre-frail 0.706 0.0381 Inf     0.627     0.775\n robust    0.645 0.0327 Inf     0.578     0.706\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nAchtung, das Objekt sieht jetzt zwar sehr ähnlich aus, aber für die weitere Berechnung macht es dann doch einen Unterschied welchen type du nutzt.\n\n\n\nWenn du un an den Odds ratios und den p-Werten interessiert bist, dann kannst du den Gruppenvergleich mit der Funktion contrast() rechnen. Wir wollen hier alle paarweisen Vergleiche rechnen. Wie immer kannst du auch die Adjustierung anpassen, wenn du möchtest.\n\nem_odds_obj |&gt; \n  contrast(method = \"pairwise\", adjust = \"bonferroni\")\n\n contrast             odds.ratio    SE  df null z.ratio p.value\n frail / (pre-frail)       0.855 0.291 Inf    1  -0.460  1.0000\n frail / robust            1.132 0.363 Inf    1   0.386  1.0000\n (pre-frail) / robust      1.324 0.308 Inf    1   1.208  0.6815\n\nP value adjustment: bonferroni method for 3 tests \nTests are performed on the log odds ratio scale \n\n\nWir sehen also, dass wir auch hier keine signifikanten Unterschiede vorliegen haben. Jetzt lassen wir uns nochmal das unadjustierte compact letter display wiedergeben. Aber auch in dem unadjustierten Fall finden wir keine signifikanten Unterschiede.\n\nem_odds_obj |&gt;\n  cld(Letters = letters, adjust = \"none\")\n\n frailty    prob     SE  df asymp.LCL asymp.UCL .group\n robust    0.645 0.0327 Inf     0.578     0.706  a    \n frail     0.673 0.0633 Inf     0.539     0.783  a    \n pre-frail 0.706 0.0381 Inf     0.627     0.775  a    \n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \nTests are performed on the log odds ratio scale \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nAm Ende möchte ich hier nochmal einen Spieldatensatz infected_tbl erstellen indem ich wiederum drei Gruppen miteinander vergleiche. Ich tue mal so als würden wir uns hier zwei Pestizide und eine Kontrolle anschauen. Unser Outcome ist dann, ob wir eine Infektion vorliegen haben oder das Pestizid alles umgebracht hat. Damit haben wir dann unser Outcome infected definiert. Wir bauen uns die Daten so, dass \\(80%\\) der Beobachtungen in der Kontrolle infiziert sind. In den beiden Behandlungsgruppen sind jeweils \\(50%\\) und \\(30%\\) der Beobachtungen nach der Behandlung noch infiziert. Wir haben jeweils zwanzig Pflanzen n_grp beobachtet. Das sind wirklich wenige Beobachtungen für einen \\(0/1\\) Endpunkt, aber es sollte hier reichen. Dann habe ich noch einen Seed gesetzt, damit auch immer die gleichen Zahlen generiert werden.\n\nset.seed(20231201)\nn_grp &lt;- 20\ninfected_tbl &lt;- tibble(trt = gl(3, n_grp, labels = c(\"ctrl\", \"roundUp\", \"killAll\")),\n                       infected = c(rbinom(n_grp, 1, 0.8), \n                                    rbinom(n_grp, 1, 0.5), \n                                    rbinom(n_grp, 1, 0.2)))\n\nJetzt bauen wir uns wieder unser logistisches Modell zusammen, dass kennst du ja schon. Vorher wollen wir aber nochmal in die Daten schauen. Unser Datensatz hat ja nur ein Outcome und eine Behandlung als Faktor mit drei Leveln bzw. Gruppen. Wir können jetzt einmal nachvollziehen woher die Werte in einer {emmeans}-Ausgabe eigentlich kommen. Schauen wir uns dazu erstmal die Tabelle mit den Infektionen zugeordnet zu den Behandlungen an.\n\ninfected_tbl %$% \n  table(trt, infected) \n\n         infected\ntrt        0  1\n  ctrl     5 15\n  roundUp 12  8\n  killAll 16  4\n\n\nJetzt berechnen wir einmal die Anteile der Infektionen in den jeweiligen Behandlungen. Wir würden ja erwarten, dass wir \\(80%\\), \\(50%\\) und \\(20%\\) vorfinden, aber da wir nur ein kleinen Fallzahl simuliert haben, ergibt sich natürlich eine Abweichung. Wir finden nämlich in den Daten für ctrl gleich \\(15/20 = 0.75\\), für roundUp gleich \\(8/20 = 0.4\\) und für killAll gleich \\(4/20 = 0.2\\) anteilig Infektionen. Finden wir diese Zahlen auch in {emmeans} wieder?\nAls erstes bauen wir uns wieder ein Modell der logistischen Regression.\n\ninfected_log_fit &lt;- glm(infected ~ trt, data = infected_tbl, family = \"binomial\")\n\nDann stecken wir das Modell einmal in emmeans() und schauen uns die Ausgabe an und nutzen die Option regrid = \"response\" um uns Wahrscheinlichkeiten wiedergeben zu lassen.\n\ninfected_log_fit  |&gt; \n  emmeans(~ trt, regrid = \"response\") \n\n trt     prob     SE  df asymp.LCL asymp.UCL\n ctrl    0.75 0.0968 Inf    0.5602     0.940\n roundUp 0.40 0.1095 Inf    0.1853     0.615\n killAll 0.20 0.0894 Inf    0.0247     0.375\n\nConfidence level used: 0.95 \n\n\nUnd tatsächlich, wir finden die gleichen Anteile in der Ausgabe wieder, wie wir auch in unseren Daten vorab aus der Tabelle berechnet haben. Wir erhalten also den Anteil der 1-sen in einem Outcome aus der Funktion emmeans() wieder. Der Anteil an 1-sen ist ja auch nichts anderes als der Mittelwert des Outcomes für die Spalte.\nIm nächsten Schritt wollen wir einmal die Odds ratios verstehen. Dafür erstmal den Aufruf in emmeans() und dann schauen wir einmal in die Ausgabe. Wir wollen jetzt die Odds ratios einmal händisch berechnen.\n\ninfected_log_fit |&gt; \n  emmeans(~ trt, type = \"response\") |&gt; \n  pairs()\n\n contrast          odds.ratio   SE  df null z.ratio p.value\n ctrl / roundUp          4.50 3.10 Inf    1   2.182  0.0742\n ctrl / killAll         12.00 9.13 Inf    1   3.265  0.0031\n roundUp / killAll       2.67 1.92 Inf    1   1.359  0.3626\n\nP value adjustment: tukey method for comparing a family of 3 estimates \nTests are performed on the log odds ratio scale \n\n\nWir wissen ja die Anteile der 1-sen in der Kontrollgruppe ctrl mit \\(0.75\\) sowie den Anteil an 1-sen in der Behandlungsgruppe roundUp mit \\(0.4\\). Wir haben die Werte ja oben in der Tabelle bestimmt. Jetzt können wir die Odds mit \\(Odds = p/(1-p)\\) für die beiden Gruppen berechnen und dann den Quotienten als Odds ratio berechnen.\n\\[\nctrl/roundUp = \\cfrac{0.75}{0.25}/\\cfrac{0.4}{0.6} = 4.5\n\\]\nWie wir sehen, passen die Zahlen. Wir können jetzt auch für die Behandlungsgruppe killAll mit einem Anteil an 1-sen von \\(0.2\\) weitermachen. Auch hier berechnen wir erst die Odds und dann den Quotienten aus den beiden Odds.\n\\[\nctrl/killAll = \\cfrac{0.75}{0.25}/\\cfrac{0.2}{0.8} = 12\n\\]\nDann nochmal als Überprüfung den letzten Wert für den Vergleich von roundUp zu killAll.\n\\[\nroundUp/killAll = \\cfrac{0.4}{0.6}/\\cfrac{0.2}{0.8} = 2.67\n\\]\nNun könntest du einwenden, dass Odds ratios nicht so einfach zu interpretieren sind und du eigentlich nur wissen willst um welchen Faktor mehr oder weniger Infizierte es in einer Gruppe gibt. Also kein Chancenverhältnis sondern eben einfach ein Wahrscheinlichkeitsverhältnis. Du willst also sagen, dass in der Kontrollgruppe x-fach mehr oder weniger infizierte Beobachtungen auftauchen. Damit willst du die Wahrscheinlichkeiten aus der Tabelle weiter oben in ein Verhätnis setzen. Daher willst du folgende Zusammenhänge berechnen.\n\\[\n\\cfrac{ctrl}{roundUp} = \\cfrac{0.75}{0.4} = 1.88\n\\]\n\\[\n\\cfrac{ctrl}{killAll} = \\cfrac{0.75}{0.2} = 3.75\n\\]\n\\[\n\\cfrac{roundUp}{killAll} = \\cfrac{0.4}{0.2} = 2.00\n\\]\nWir können also sagen, dass wir in der Kontrolle ungefähr 1.88 mal mehr Infizierte haben als in der Behandlung roundUp. Oder aber, dass wir in der Kontrolle 3.75 mal mehr Infizierte haben als in der Behandlung killAll Um solche Zahlen zu berechnen, nutzen wir die Poisson Regression auf unseren \\(0/1\\)-Daten. Dafür müssen wir einmal die Option family = \"poisson\" setzen.\n\ninfected_pois_fit &lt;- glm(infected ~ trt, data = infected_tbl, family = \"poisson\")\n\nDann können wir auch schon die Funktion emmeans() anwenden, um uns einmal die Wahrscheinlichkeitsverhältnisse als ratio wiedergeben zu lassen. Wie immer kannst du natürlich den Fit auch erstmal in ein eigenes Objekt stecken und dann noch andere Funktionen in {emmeans} nutzen. Wie du gleich siehst, haben wir aber auch hier keine signifikanten Unterschiede zwischen den Gruppen.\n\ninfected_pois_fit |&gt; \n  emmeans(~ trt, type = \"response\") |&gt; \n  pairs()\n\n contrast          ratio    SE  df null z.ratio p.value\n ctrl / roundUp     1.88 0.821 Inf    1   1.436  0.3224\n ctrl / killAll     3.75 2.110 Inf    1   2.349  0.0493\n roundUp / killAll  2.00 1.225 Inf    1   1.132  0.4943\n\nP value adjustment: tukey method for comparing a family of 3 estimates \nTests are performed on the log scale \n\n\nDamit sind wir einmal mit unserem Gruppenvergleich für die logistische Regression auf einem \\(0/1\\) Outcome durch. In dem Kapitel zu den Multiple Vergleichen oder Post-hoc Tests findest du dann noch mehr Inspirationen für die Nutzung von {emmeans}. Hier war es dann die Anwendung auf binäre Outcomes zusammen mit einem Faktor. Wenn du dir das Ganze nochmal an einem Beispiel für zwei Faktoren anschauen möchtest, dann findest du im folgenden Kasten ein Beispiel für die Auswertung von Beschädigungen an Mais nach verschiedenen Ernteverfahren und Zeitpunkten.\n\n\n\n\n\n\nAnwendungsbeispiel: Zweifaktorieller Gruppenvergleich für Maisernte\n\n\n\nIm folgenden Beispiel schauen wir uns nochmal ein praktische Auswertung von einem agrarwissenschaftlichen Beispiel mit Mais an. Wir haben uns in diesem Experiment verschiedene Arten trt von Ernteverfahren von Mais angeschaut. Dann haben wir nach vier Zeitpunkten bestimmt, ob der Mais durch das Ernteverfahren nachträglich beschädigt war. Die Beschädigung selber wurde dann etwas komplizierter mit einem Labortest festgestellt, aber wir schauen uns nur die Ausprägung ja/nein also \\(1/0\\) als Outcome an. Durch einen Fehler im Labor müssen wir eine Kombination für den letzten Tag und der dritten Behandlung entfernen.\n\nmaize_tbl &lt;- read_excel(\"data/maize_rate.xlsx\") |&gt; \n   mutate(damaged = ifelse(time == \"5d\" & trt == 3, NA, damaged),\n          trt = factor(trt, labels = c(\"wenig\", \"mittel\", \"viel\")))\n\nDann können wir auch schon die logistische Regression mit glm() rechnen. Auch hier wieder darauf achten, dass wir dann als Option family = binomial wählen und unser Outcome infected als Faktor kodiert ist.\n\nmaize_fit &lt;- glm(damaged ~ trt + time + trt:time, data = maize_tbl, family = binomial) \n\nIn den beiden folgenden Tabs findest du dann einmal das Ergebnis für die ANOVA und einmal für den Gruppenvergleich mit dem R Paket {emmeans}. Bitte beachte, dass die ANOVA für ein glm()-Objekt nicht ganz gleich wie für ein lm()-Objekt ist. Du kannst aber die ANOVA erstmal ganz normal interpretieren, nur haben wir hier nicht die Möglichkeit ein \\(\\eta^2\\) zu bestimmen. Dann nutzen wir {emmeans} für den Gruppenvergleich.\n\nANOVA mit anova()Gruppenvergleich mit emmeans()\n\n\nWir rechnen hier einmal die ANOVA und nutzen den \\(\\mathcal{X}^2\\)-Test für die Ermittelung der p-Werte. Wir müssen hier einen Test auswählen, da per Standardeinstellung kein Test gerechnet wird. Wir machen dann die Ausageb nochmal schöner und fertig sind wir.\n\nmaize_fit |&gt; \n  anova(test = \"Chisq\") |&gt; \n  model_parameters(drop = \"NULL\")\n\nParameter | df | Deviance | df (error) | Deviance (error) |      p\n------------------------------------------------------------------\ntrt       |  2 |    38.74 |        162 |           136.89 | &lt; .001\ntime      |  3 |    27.46 |        159 |           109.43 | &lt; .001\ntrt:time  |  5 |     4.00 |        154 |           105.43 | 0.550 \n\nAnova Table (Type 1 tests)\n\n\nWir sehen, dass der Effekt für die Behandlung und die Zeit signifikant sind. Wir haben aber keine Interaktion vorliegen. Daher können wir dann die Analyse gemeinsam über alle Zeitpunkte rechnen.\n\n\nIm Folgenden rechnen wir einmal über alle Faktorkombinationen von trt und time einen Gruppenvergleich. Dafür nutzen wir die Opition trt * time. Wenn du die Analyse getrennt für die Zeitpunkte durchführen willst, dann nutze die Option trt | time. Wir wollen die Wahrscheinlichkeiten für das Auftreten einer Beschädigung von wiedergegeben bekommen, deshalb die Option regrid = \"response. Dann adjustieren wir noch nach Bonferroni und sind fertig.\n\nemm_obj &lt;- maize_fit |&gt; \n  emmeans(~ trt * time, regrid = \"response\") |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\")\nemm_obj\n\n trt    time     prob         SE  df  asymp.LCL asymp.UCL .group\n viel   2d   0.133333 0.08777075 Inf -0.1157247  0.382391  a    \n viel   1d   0.400000 0.12649111 Inf  0.0410692  0.758931  ab   \n mittel 5d   0.600000 0.12649111 Inf  0.2410692  0.958931  abc  \n mittel 2d   0.800000 0.10327956 Inf  0.5069343  1.093066   bc  \n viel   1h   0.866667 0.08777075 Inf  0.6176087  1.115725   bc  \n wenig  5d   0.866667 0.08777075 Inf  0.6176087  1.115725   bc  \n mittel 1h   0.933333 0.06440612 Inf  0.7505747  1.116092    c  \n mittel 1d   0.933333 0.06440612 Inf  0.7505747  1.116092    c  \n wenig  1d   1.000000 0.00000883 Inf  0.9999749  1.000025    c  \n wenig  1h   1.000000 0.00000883 Inf  0.9999749  1.000025    c  \n wenig  2d   1.000000 0.00000883 Inf  0.9999749  1.000025    c  \n viel   5d     nonEst         NA  NA         NA        NA       \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 11 estimates \nP value adjustment: bonferroni method for 55 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nDas emm_obj Objekt werden wir dann gleich einmal in {ggplot} visualisieren. Dann können wir auch das compact letter display anhand der Abbildung interpretieren.\n\n\n\nIn der Abbildung 49.5 siehst du das Ergebnis der Auswertung in einem Säulendiagramm. Hier unbedingt SE als den Standardfehler für die Fehlerbalken nutzen, da wir sonst Fehlerbalken größer und kleiner als \\(0/1\\) erhalten, wenn wir die Standardabweichung nutzen würden. Wir sehen einen klaren Effekt der Behandlung viel. Schade, dass wir dann nach 5 Tagen leider keine Auswertung für die dritte Behandlung vorliegen haben. Aber sowas passiert dann unter echten Bedingungen mal schnell.\n\nemm_obj |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = time, y = prob, fill = trt)) +\n  theme_minimal() + \n  labs(y = \"Anteil beschädigter Mais\", x = \"Stunden nach Ernte\",\n       fill = \"Behandlung\") +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_text(aes(label = .group, y = prob + SE + 0.01),  \n            position = position_dodge(width = 0.9), vjust = -0.25) +\n  geom_errorbar(aes(ymin = prob-SE, ymax = prob+SE),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 49.5— Säulendigramm der Anteile des beschädigten Mais aus einer logistischen Regression. Das glm()-Modell berechnet die Wahrscheinlichkeiten in jeder Faktorkombination, was dann die Anteile des beschädigten Mais entspricht. Das compact letter display wird dann in {emmeans} generiert.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#dichotomisierung",
    "href": "stat-modeling-logistic.html#dichotomisierung",
    "title": "49  Logistische Regression",
    "section": "49.9 Dichotomisierung",
    "text": "49.9 Dichotomisierung\nManchmal ist es so, dass wir eine logistsiche Regression rechnen wollen. Wir fragen nicht, wie ist unser \\(y\\) verteilt und was für eine Regression können wir dann rechnen? Sondern wir wollen mit der logistischen Regression durch die Wand. Wenn wir das wollen, dann können wir unser \\(y\\) dichotomisieren. Das heißt, wir machen aus einer Variable, die mehr als zwei Level hat einen Faktor mit zwei Leveln. Dafür stehen uns verschiedene Möglichkeiten offen.\nIn dem R Paket {dplyr} haben wir mit der Funktion recode() die Möglichkeit eine Variable von alt = neu umzukodieren. Dabei müssen wir natürlich darauf achten, dass wir die alten Level der Variable richtig schreiben und bei der neuen Level nur zwei Namen eintragen. Dann sind wir auch schon durch mit der Umbenennung.\n\npig_tbl |&gt; \n  mutate(frailty = recode(frailty, \n                          \"robust\" = \"robust\", \n                          \"pre-frail\" = \"frail_prefrail\", \n                          \"frail\" = \"frail_prefrail\")) |&gt; \n  pull(frailty) |&gt; extract(1:20)\n\n [1] \"robust\"         \"robust\"         \"robust\"         \"robust\"        \n [5] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n [9] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n[13] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n[17] \"frail_prefrail\" \"frail_prefrail\" \"robust\"         \"frail_prefrail\"\n\n\nIch finde die Funktion case_when() etwas übersichtlicher. Das ist aber eigentlich nur eine Geschmacksfrage. Am Ende kommt jedenfalls das Gleiche heraus.\n\npig_tbl |&gt; \nmutate(frailty = case_when(frailty == \"robust\" ~ \"robust\",\n                           frailty == \"pre-frail\" ~ \"frail\",\n                           frailty == \"frail\" ~ \"frail\")) |&gt; \n  pull(frailty) |&gt; extract(1:20)\n\n [1] \"robust\" \"robust\" \"robust\" \"robust\" \"robust\" \"robust\" \"frail\"  \"robust\"\n [9] \"robust\" \"robust\" \"frail\"  \"robust\" \"robust\" \"robust\" \"frail\"  \"robust\"\n[17] \"frail\"  \"frail\"  \"robust\" \"frail\" \n\n\nHäufig haben wir auch den Fall, dass wir keine kontinuierlichen \\(x\\) in unseren Daten wollen. Alles soll sich in Faktoren verwandeln, so dass wir immer eine 2x2 Tafel haben. Wenn es sein muss, liefert hier cutpointr() die Lösung für dieses Problem. Wir müssen dafür zum einen unser kontinuierliches \\(x\\) angeben und dann mit class unser binäres \\(y\\). Wir erhalten dann für unser \\(y\\) den bestmöglichen Split für unser \\(x\\). Im Beispiel wollen wir einmal die Variable crp für unser Outcome infected in zwei Gruppen aufteilen. Wir wollen eigentlich immer zwei Gruppen, da wir dann in dem Setting eines \\(\\mathcal{X}^2\\)-Test und einer einfacheren Interpretation von dem \\(OR\\) sind.\nWir immer haben wir eine große Bandbreite an Optionen, wie wir den besten Split unseres \\(x\\) kriegen wollen. Ich gehe hier mit den Default-Werten. Damit kommt man eigentlich recht weit. Ich möchte gerne die Summe der Sensivität und der Spezifität sum_sens_spec über alle möglichen Cutpoints maximieren maximize_metric. Der Cutpoint mit der maximalen Summe an Sensivität und der Spezifität wird mir dann wiedergegeben. Natürlich hat das R Paket {cutpoint} noch viel mehr Optionen. Mehr gibt es in An introduction to cutpointr.\n\ncp_crp &lt;- cutpointr(data = pig_tbl,\n                    x = crp,\n                    class = infected,\n                    method = maximize_metric, \n                    metric = sum_sens_spec) \n\ncp_crp\n\n# A tibble: 1 × 16\n  direction optimal_cutpoint method          sum_sens_spec      acc sensitivity\n  &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 &gt;=                   19.71 maximize_metric       1.48870 0.752427    0.768116\n  specificity      AUC pos_class neg_class prevalence outcome  predictor\n        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    \n1    0.720588 0.806119         1         0   0.669903 infected crp      \n  data               roc_curve             boot \n  &lt;list&gt;             &lt;list&gt;                &lt;lgl&gt;\n1 &lt;tibble [412 × 2]&gt; &lt;rc_ctpnt [282 × 10]&gt; NA   \n\n\nIn Abbildung 49.6 sehe wir die Ausgabe der Funktion cutpointr() nochmal visualisiert. Wir sehen, dass der Split einigermaßen die crp-Werte im Sinne von unserem Outcome aufteilt.\n\nplot(cp_crp)\n\n\n\n\n\n\n\nAbbildung 49.6— Visualisierung des Ergebnisses der Funktion cutpointr für die Variable crp.\n\n\n\n\n\nWir können uns jetzt noch den optimalen Cutpoint aus der Ausgabe herausziehen, wenn wir den Punkt nicht aus der Ausgabe ablesen wollen.\n\npluck(cp_crp, \"optimal_cutpoint\")\n\n[1] 19.71\n\n\nAm Ende können wir dann über case_when() uns ein binären CRP-Wert zusammenbauen. Wir müssen dann natürlich entscheiden welche Variable wir mit ins Modell nehme, aber meistens machen wir uns ja die Mühen um dann die neue Variable zu verwenden.\n\npig_tbl |&gt; \nmutate(crp_bin = case_when(crp &gt;= 19.84 ~ \"high\",\n                           crp &lt; 19.84 ~ \"low\")) |&gt; \nselect(crp, crp_bin)  \n\n# A tibble: 412 × 2\n     crp crp_bin\n   &lt;dbl&gt; &lt;chr&gt;  \n 1  22.4 high   \n 2  18.6 low    \n 3  18.8 low    \n 4  19.4 low    \n 5  21.6 high   \n 6  21.4 high   \n 7  19.0 low    \n 8  19.0 low    \n 9  21.9 high   \n10  21.0 high   \n# ℹ 402 more rows\n\n\nDamit haben wir uns dann auch mit dem Problem der Dichotomisierung in der logististischen Regression einmal beschäftigt. Somit bleibt dann noch die Prädiktion übrig.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#prädiktion",
    "href": "stat-modeling-logistic.html#prädiktion",
    "title": "49  Logistische Regression",
    "section": "49.10 Prädiktion",
    "text": "49.10 Prädiktion\nDa wir später in dem Kapitel 65 die logistische Regression auch als Vergleich zu maschinellen Lernverfahren in der Klassifikation nutzen werden gehen wir hier auch die Prädiktion einmal für die logistische Regression durch. Wir wollen also eine Klassifikation, also eine Vorhersage, für das Outcome infected mit einer logistischen Regression rechnen. Wir nutzen dazu die Möglichkeiten des R Pakets {tidymodels} wodurch wir einfacher ein Modell bauen und eine Klassifikation rechnen können. Unsere Fragestellung ist, ob wir mit unseren Einflussvariablen den Infektionsstatus vorhersagen können. Das heißt wir wollen ein Modell bauen mit dem wir zukünftige Ferkel als potenziell krank oder gesund anhand unser erhobenen Daten einordnen bzw. klassifizieren können.\n\n\nMehr zu Rezepten (eng. recipes) kannst du im Kapitel 65 zu den Grundlagen des maschinellen Lernens erfahren.\nDer erste Schritt einer Klassifikation ist immer sicherzustellen, dass unser Outcome auch wirklich aus Kategorien besteht. In R nutzen wir dafür einen Faktor und setzen dann auch gleich die Ordnung fest.\n\npig_tbl &lt;- pig_tbl |&gt; \n  mutate(infected = factor(infected, levels = c(0, 1)))\n\nNun bauen wir uns ein einfaches Rezept mit der Funktion recipe(). Dafür legen wir das Modell, was wir rechnen wollen einmal fest. Wir nehmen infected als Outcome und den Rest der Vairbalen . aus dem Datensatz pig_tbl als die \\(x\\) Variablen. Dann wollen wir noch alle Variablen, die ein Faktor sind in eine Dummyvariable umwandeln.\n\npig_rec &lt;- recipe(infected ~ ., data = pig_tbl) |&gt; \n  step_dummy(all_nominal_predictors())\n\nWir wollen jetzt unser Modell definieren. Wir rechnen eine logistsiche Regression und deshalb nutzen wir die Funktion logistic_reg(). Da wir wirklich viele Möglichkeiten hätten die logistische Regression zu rechnen, müssen wir noch den Algorithmus wählen. Das tuen wir mit der Funktion set_engine(). Wir nutzen hier den simplen glm() Algorithmus. Es gebe aber auch andere Implementierungen.\n\nlogreg_mod &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\n\nJetzt müssen wir noch einen Workflow definieren. Wir wollen ein Modell rechnen und zwar mit den Informationen in unserem Rezept. Das bauen wir einmal zusammen und schauen uns die Ausgabe an.\n\npig_wflow &lt;- workflow() |&gt; \n  add_model(logreg_mod) |&gt; \n  add_recipe(pig_rec)\n\npig_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nDas passt alles soweit. Ja, es ist etwas kompliziert und das ginge sicherlich auch einfacher. Wir werden dann aber noch sehen, dass wir es uns mit dem Ablauf sehr viel einfacher machen, wenn wir kompliziertere Modelle schätzen wollen. Mehr dazu findest du dann im Kapitel 65 zu den maschinellen Lernverfahren.\nJetzt können wir den Workflow nutzen um den Fit zu rechnen. Bis jetzt haben wir nur Informationen gesammelt. Dadurch das wir jetzt das Objekt pig_workflow in die Funktion fit() pipen rechnen wir das Modell.\n\npig_fit &lt;- pig_wflow |&gt; \n  fit(data = pig_tbl)\n\nDas erhaltende Modell könne wir dann in die Funktion predict() stecken um uns den Inektionsstatus vorhersagen zu lassen.\n\npredict(pig_fit, new_data = pig_tbl)\n\n# A tibble: 412 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 1          \n 2 0          \n 3 0          \n 4 1          \n 5 1          \n 6 1          \n 7 0          \n 8 0          \n 9 1          \n10 1          \n# ℹ 402 more rows\n\n\nIn der Spalte .pred_class finden wir dann die vorhergesagten Werte des Infektionsstatus anhand unseres gefitteten Modells. Eigentlich würden wir ja gerne die vorhergesagten Werte mit unseren Orginalwerten vergleichen. Hier hilft uns die Funktion augment(). Dank der Funktion augment() erhalten wir nicht nur die vorhergesagten Klassen sondern auch die Wahrscheinlichkeit für die Klassenzugehörigkeiten. Daneben dann aber auch die Originalwerte für den Infektionsstatus in der Spalte infected.\n\npig_aug &lt;- augment(pig_fit, new_data = pig_tbl) |&gt; \n  select(infected, matches(\"^\\\\.\"))\n\npig_aug\n\n# A tibble: 412 × 4\n   infected .pred_class .pred_0 .pred_1\n   &lt;fct&gt;    &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 1        1            0.0229   0.977\n 2 1        0            0.787    0.213\n 3 0        0            0.540    0.460\n 4 1        1            0.391    0.609\n 5 1        1            0.128    0.872\n 6 1        1            0.102    0.898\n 7 1        0            0.559    0.441\n 8 0        0            0.687    0.313\n 9 1        1            0.0556   0.944\n10 1        1            0.193    0.807\n# ℹ 402 more rows\n\n\nWir können dann die Werte aus dem Objekt pig_aug nutzen um uns die ROC Kurve als Güte der Vorhersage wiedergeben zu lassen. Wir nutzen hier die schnelle Variante der Ploterstellung. In dem Kapitel 68.6 zum Vergleich von Algorithmen gehe ich noch näher auf die möglichen Optionen bei der Erstellung einer ROC Kurve ein. Hier fällt die ROC Kurve dann mehr oder minder vom Himmel. Ich musste noch der Funktion mitgeben, dass das Event bei uns das zweite Level des Faktors infected ist. Sonst ist unsere ROC Kurve einmal an der Diagonalen gespiegelt.\nIn dem Kapitel 35 erfährst du mehr darüber was eine ROC Kurve ist und wie du die ROC Kurve interpretieren kannst.\n\npig_aug |&gt; \n  roc_curve(truth = infected, .pred_1, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\nAbbildung 49.7— ROC Kurve für die Vorhersage des Infektionsstatus der Ferkel anhand der erhobenen Daten.\n\n\n\n\n\nNa das hat doch mal gut funktioniert. Die ROC Kurve verläuft zwar nicht ideal aber immerhin ist die ROC Kurve weit von der Diagnolen entfernt. Unser Modell ist also in der Lage den Infektionsstatus der Ferkel einigermaßen solide vorherzusagen. Schauen wir uns noch die area under the curve (abk. AUC) an.\n\npig_aug |&gt; \n  roc_auc(truth = infected, .pred_1, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.819\n\n\nDer beste Wert wäre hier eine AUC von \\(1\\) und damit eine perfekte Vorhersage. Der schlechteste Wert wäre eine AUC von \\(0.5\\) und damit eine nahezu zufällige Zuordnung des Infeketionsstatus zu den Ferkeln von unserem Modell. Mit einer AUC von \\(0.83\\) können wir aber schon gut leben. Immerhin haben wir kaum am Modell rumgeschraubt bzw. ein Tuning betrieben. Wenn du mehr über Tuning und der Optimierung von Modellen zu Klassifikation wissen willst, dan musst du im Kapitel 65 zu den maschinellen Lernverfahren anfangen zu lesen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-logistic.html#referenzen",
    "href": "stat-modeling-logistic.html#referenzen",
    "title": "49  Logistische Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 49.1— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und der Aktivität der Ferkel.\nAbbildung 49.2— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und der Aktivität der Ferkel.\nAbbildung 49.3— Ausgabe ausgewählter Modelgüteplots der Funktion check_model().\nAbbildung 49.4— Visualisierung der logistischen Gerade in einer simplen logistischen Regression mit der Variable crp.\nAbbildung 49.5— Säulendigramm der Anteile des beschädigten Mais aus einer logistischen Regression. Das glm()-Modell berechnet die Wahrscheinlichkeiten in jeder Faktorkombination, was dann die Anteile des beschädigten Mais entspricht. Das compact letter display wird dann in {emmeans} generiert.\nAbbildung 49.6— Visualisierung des Ergebnisses der Funktion cutpointr für die Variable crp.\nAbbildung 49.7— ROC Kurve für die Vorhersage des Infektionsstatus der Ferkel anhand der erhobenen Daten.\n\n\n\nDormann CF. 2013. Parametrische Statistik. Springer.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Logistische Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-prob-model.html",
    "href": "stat-modeling-prob-model.html",
    "title": "51  Linear Probability Model",
    "section": "",
    "text": "51.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, sandwich, lmtest, \n               emmeans, multcomp, see,\n               performance, broom, conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(magrittr::extract)\nconflicts_prefer(magrittr::set_names)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "stat-modeling-prob-model.html#daten",
    "href": "stat-modeling-prob-model.html#daten",
    "title": "51  Linear Probability Model",
    "section": "51.2 Daten",
    "text": "51.2 Daten\nIn diesem Kapitel nutzen wir die infizierten Ferkel als Beispieldatensatz. Wir haben in dem Datensatz über vierhundert Ferkel untersucht und festgehalten, ob die Ferkel infiziert sind (\\(1\\), ja) oder nicht infiziert (\\(0\\), nein). Wir haben daneben noch eine ganze Reihe von Risikofaktoren erhoben. Hier sieht man mal wieder wie wirr die Sprache der Statistik ist. Weil wir rausfinden wollen welche Variable das Risiko für die Infektion erhöht, nennen wir diese Variablen Risikofaktoren. Wir nehmen hier jetzt aber nicht alle Variablen mit, sondern nur die Variablen für den Entzündungswert crp, das Geschlecht sex, dem Alter age und der Gebrechlichkeitskategorie frailty.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") |&gt; \n  select(infected, crp, sex, age, frailty) \n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der Tabelle 51.1 an.\n\n\n\n\nTabelle 51.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\ninfected\ncrp\nsex\nage\nfrailty\n\n\n\n\n1\n22.38\nmale\n61\nrobust\n\n\n1\n18.64\nmale\n53\nrobust\n\n\n0\n18.76\nfemale\n66\nrobust\n\n\n1\n19.37\nfemale\n59\nrobust\n\n\n…\n…\n…\n…\n…\n\n\n1\n21.95\nmale\n57\npre-frail\n\n\n1\n23.1\nmale\n61\nrobust\n\n\n0\n20.23\nfemale\n59\nrobust\n\n\n1\n19.89\nfemale\n63\nrobust\n\n\n\n\n\n\n\n\nIm Folgenden wollen wir einmal modellieren, ob es einen Zusammenhang von den Variablen crp, sex, age und frailty auf das \\(0/1\\)-Outcome infected gibt. Welche der Variablen erniedrigen oder erhöhen also das Risiko einer Ferkelinfektion?",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "stat-modeling-prob-model.html#theoretischer-hintergrund",
    "href": "stat-modeling-prob-model.html#theoretischer-hintergrund",
    "title": "51  Linear Probability Model",
    "section": "51.3 Theoretischer Hintergrund",
    "text": "51.3 Theoretischer Hintergrund\nDen theoretischen Hintergrund belassen wir hier nur kurz. Die Idee sehen wir dann einmal in der Abbildung 51.1. Wir haben hier dann nur die Variable crp und das Outcome infected dargestellt. Wir sehe die klaren zwei Ebenen. Wir haben ja bei dem Infektionsstatus auch nur zwei mögliche Ausprägungen. Entweder ist unser Ferkel infiziert oder eben nicht. Da wir aber die Entzündungswerte kontinuierlich messen ergeben sich die beiden Ebenen.\n\nggplot(pig_tbl, aes(x = crp, y = infected)) +\n  theme_minimal() +\n  geom_point() \n\n\n\n\n\n\n\nAbbildung 51.1— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und den Entzündungswerten der Ferkel.\n\n\n\n\n\nTja und dann rechnen wir einfach eine Gaussian linear Regression mit der Funktion lm(). Es ergibt sich dann eine gerade Linie, wie wir sie in der Abbildung 51.2 sehen. Da wir aber mit unserem Infektionsstatus auf \\(0/1\\) begrenzt sind, aber eine Gerade nicht, haben wir das Problem, dass wir in diesem Fall den Infektionsstatus für CRP-Werte größer als 22 überschätzen. Ich meine mit überschätzen dann auch wirklich Werte zu erhalten, die es dann gar nicht geben kann. Es kann keinen Infektionsstatus über ja geben.\n\nggplot(pig_tbl, aes(x = crp, y = infected)) +\n  theme_minimal() +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\nAbbildung 51.2— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und den Entzündungswerten der Ferkel ergänzt um Gerade aus der Gaussian linearen Regression.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "stat-modeling-prob-model.html#modellierung",
    "href": "stat-modeling-prob-model.html#modellierung",
    "title": "51  Linear Probability Model",
    "section": "51.4 Modellierung",
    "text": "51.4 Modellierung\nDann können wir schon das probability model anpassen. Dazu nehmen wir die Funktion lm(), die wir auch für unsere Gaussian linearen Regression unter der Annahme eines normalverteilten Outcomes \\(y\\) nutzen. Wichtig ist hier, dass wir auf keinen Fall unseren Infektionsstatus infected als einen Faktor übergeben. Der Infektionsstatus infected muss numerisch sein.\n\nlm_fit &lt;- lm(infected ~ crp + age + sex + frailty, data = pig_tbl)\n\nSchauen wir uns aber gleich mal die Modellausgabe an. Wie immer, du kannst alle Zahlen und Spalten in eine Funktion stecken und am Ende kommt dann was raus. Woher soll auch die Funktion wissen, dass es sich um einen Faktor handelt oder eine numerische Variable?\n\nlm_fit |&gt; \n  summary()\n\n\nCall:\nlm(formula = infected ~ crp + age + sex + frailty, data = pig_tbl)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9413 -0.3454  0.1220  0.3052  0.7996 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.648693   0.411294  -6.440 3.38e-10 ***\ncrp               0.162087   0.014160  11.447  &lt; 2e-16 ***\nage               0.001468   0.004522   0.325    0.746    \nsexmale          -0.047673   0.041556  -1.147    0.252    \nfrailtypre-frail  0.038275   0.065363   0.586    0.558    \nfrailtyrobust    -0.011544   0.062065  -0.186    0.853    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4103 on 406 degrees of freedom\nMultiple R-squared:  0.2499,    Adjusted R-squared:  0.2406 \nF-statistic: 27.05 on 5 and 406 DF,  p-value: &lt; 2.2e-16\n\n\nNeben der sehr schlechten Modellgüte, die wir am Bestimmtheitsmaß \\(R^2\\) mit 0.24 erkennen, sind aber die Residuen nach den deskriptiven Maßzahlen einigermaßen okay. Wir werden aber gleich noch sehen, dass die Maßzahlen hier auch trügen können. Was ist den nun das Tolle am probability model? Wir können die Effektschätzer Estimate direkt als prozentuale Veränderung interpretieren. Das heißt, wir können sagen, dass pro Einheit crp die Wahrscheinlichkeit infiziert zu sein um 16.2087% ansteigt. Das ist natürlich eine sehr schöne Eigenschaft. Nur leider gibt es da meistens dann doch ein Problem. Dafür schauen wir uns einmal die Spannweite der vorhergesagten Werte an.\n\nlm_fit |&gt; \n  predict() |&gt; \n  range() |&gt; \n  round(2)\n\n[1] -0.04  1.27\n\n\nWie du siehst, kriegen wir Werte größer als Eins und kleiner als Null aus dem Modell raus. Das macht jetzt aber recht wenig Sinn. Wir können die Werte aus predict() als Wahrscheinlichkeit infiziert zu sein interpretieren. Da unsere Ferkel aber nur gesund oder krank sein können, machen negative Werte der Wahrscheinlichkeit infiziert zu sein keinen Sinn. Auch Werte größer als Eins können wir nur sehr schwer interpretieren.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "stat-modeling-prob-model.html#varianzheterogenität",
    "href": "stat-modeling-prob-model.html#varianzheterogenität",
    "title": "51  Linear Probability Model",
    "section": "51.5 Varianzheterogenität",
    "text": "51.5 Varianzheterogenität\nWenn du das probability model durchführst, dann hast du in den meisten Fällen das Problem der Heteroskedastizität, auch Varianzheterogenität genannt. Oder in anderen Worten, die Residuen als Fehler unseres Modell sind nicht gleichmäßig mit gleich großen Werten verteilt um die Gerade. Gut, dass klingt jetzt etwas sperrig, hier einmal die Abbildung 51.3 um es besser zu verstehen.\n\n\n\n\n\n\n\n\nAbbildung 51.3— Varianzheterogenität oder Heteroskedastizität in den Daten. Die Abstände der Punkte zu der Geraden, die Residuen, werden immer größer. Wir haben keinen konstanten oder homogenen Fehler.\n\n\n\n\n\nFür unser Modell lm_fit von oben können wir auch gleich die Funktion check_heteroscedasticity() aus dem R Paket {performance} nutzen um zu Überprüfen ob wir Varianzheterogenität vorliegen haben. Aber Achtung, ich wäre hier sehr vorsichtig, wenn die Funktion sagt, dass wir keine Varianzheterogenität vorliegen haben.\n\ncheck_heteroscedasticity(lm_fit)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nNeben der Funktion check_heteroscedasticity() gibt es auch die Möglichkeit über check_model() sich die Varianzen und damit die Residuen einmal anzuschauen. Die visuelle Überprüfung ist auf jeden Fall Pflicht. Und wie du in der Abbildung 51.4 siehst, sind die Varianzen weder homogen noch irgendwie normalverteilt. Wir gehen also von Varianzheterogenität aus. Damit liegen wir in Linie mit der Funktion check_heteroscedasticity(), aber das muss nicht immer unbedingt sein. Besonders bei kleiner Fallzahl, kann es vorkommen, dass check_heteroscedasticity() eine Varianzheterogenität übersieht.\n\ncheck_model(lm_fit, check = c(\"homogeneity\", \"normality\"))\n\n\n\n\n\n\n\nAbbildung 51.4— Überprüfung der Varianzhomogeniutät des Modells lm_fit mit der Funktion check_model(). Wir sehen hier eine klare Varianzheterogenität in dem Modell.\n\n\n\n\n\nJetzt kann man sich fragen, warum sind denn da so Bögen drin? Das kommt von den Abständen der Punkte auf den beiden Ebenen. Die Gerade läuft ja durch einen Bereich in dem keine Beobachtungen sind. Daher ist am Anfang der Abstand zu einer der beiden Ebenen, entweder der Null oder der Eins, minimal und erhöht sich dann langsam. Weil ja nicht alle Beobachtungen alle bei Null sind springen die Abstände von klein zu groß. In Abbildung 51.5 siehst du nochmal den Zusammenhang. Unsere angepasste Gerade steigt ja an, wie du in Abbildung 51.2 siehst. Daher sind die Abstände zu den Null Werten am Anfang sehr klein und stiegen dann an. Sobald die erste Beobachtung mit einem Infektionsstatus von Eins auftaucht, springt der Abstand natürlich sofort nach oben. Werte größer als Eins dürfte es auf der x-Achse gar nicht geben, den dort werden dann Werte größer als Eins geschätzt.\n\nlm_fit |&gt; \n  augment() |&gt; \n  ggplot(aes(x = .fitted, y = .resid^2, color = as_factor(infected))) +\n  theme_minimal() +\n  geom_point() +\n  labs(x = \"Angepasste Werte\", y = \"Residuen\", color = \"Infected\") +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 51.5— Etwas andere Darstellung der angepassten Werte .fitted und der Residuen .resid aus dem Modell lm_fit. Die Punkte sind nach dem Infektionsstatus eingefärbt.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "stat-modeling-prob-model.html#interpretation-des-modells",
    "href": "stat-modeling-prob-model.html#interpretation-des-modells",
    "title": "51  Linear Probability Model",
    "section": "51.6 Interpretation des Modells",
    "text": "51.6 Interpretation des Modells\nWir haben ja schon einmal weiter oben in das Modell geschaut und eine Interpretation vorgenommen. Wir erinnern uns, wir können die Effektschätzer Estimate aus einem probability model direkt als prozentuale Veränderung interpretieren. Das heißt, wir können sagen, dass pro Einheit crp die Wahrscheinlichkeit infiziert zu sein um 16.2087% ansteigt. Das ist natürlich eine sehr schöne Eigenschaft. Dann haben wir auch noch gleich die Richtung mit drin, wenn wir also negative Effekte haben, dann senkt die Variable das Risiko pro Einheit um den prozentualen Wert. Bei kategorialen Variablen haben wir dann den Unterschied zu der nicht vorhandenen Gruppe. Daher sind männliche Ferkel um 5% weniger infiziert als weibliche Ferkel. Leider geht der t-Test, der die \\(p\\)-Werte produziert, von homogenen Varianzen aus. Die haben wir aber nicht vorliegen.\n\nlm_fit |&gt; \n  summary() |&gt; \n  pluck(\"coefficients\") |&gt; \n  round(4)\n\n                 Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)       -2.6487     0.4113 -6.4399   0.0000\ncrp                0.1621     0.0142 11.4467   0.0000\nage                0.0015     0.0045  0.3246   0.7457\nsexmale           -0.0477     0.0416 -1.1472   0.2520\nfrailtypre-frail   0.0383     0.0654  0.5856   0.5585\nfrailtyrobust     -0.0115     0.0621 -0.1860   0.8525\n\n\nDeshalb müssen wir nochmal ran. Wir können die Funktion coeftest() aus dem R Paket {lmtest} zusammen mit den R Paket {sandwich} nutzen um unsere Modellanpassung für die Varianzheterogenität zu adjustieren. Wir ändern also die Spalte Strd. Error. Es gibt aber sehr viele Möglichkeiten type die Varianz anzupassen. Das ist ein eigenes Kapitel worum wir uns hier nicht scheren. Wir nehmen mehr oder minder den Standard mit HC1.\n\nlm_fit |&gt; \n  coeftest(vcov. = vcovHC, type = \"HC1\") |&gt; \n  round(4) |&gt; \n  tidy()\n\n# A tibble: 6 × 5\n  term             estimate std.error statistic p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)       -2.65      0.392     -6.75    0    \n2 crp                0.162     0.0121    13.4     0    \n3 age                0.0015    0.0045     0.326   0.745\n4 sexmale           -0.0477    0.0411    -1.16    0.247\n5 frailtypre-frail   0.0383    0.0598     0.640   0.522\n6 frailtyrobust     -0.0115    0.0559    -0.206   0.836\n\n\nWir schauen also als erstes auf den Standardfehler und sehen, dass unsere Gaussian lineare Regression (OLS) den Standardfehler als zu hoch geschätzt hat. Größer Standardfehler bedeutet kleinere Teststatistik und damit dann auch weniger signifikante \\(p\\)-Werte. In der Tabelle 51.2 siehst du nochmal die beiden Spalten der Standardfehler nebeneinander. Unser Sandwich-Schätzer (HC1) liefert da die besseren Fehlerterme, die eher der Realität der Varianzheterogenität entsprechen. Wir brauchen die adjustierten Standardfehler aber nur, wenn wir eine statistischen Test rechnen wollen und den \\(p\\)-Wert für die Bewertung der Signifikanz brauchen.\n\n\n\n\nTabelle 51.2— Vergleich der Standardfehler der Gaussian linear Regression mit der Annahme der homogenen Varianzen (OLS) und die Adjusterung der Fehler mit dem Sandwich-Schätzer (HC1).\n\n\n\n\n\n\n\nOLS\nHC1\n\n\n\n\n(Intercept)\n0.4113\n0.3924\n\n\ncrp\n0.0142\n0.0121\n\n\nage\n0.0045\n0.0045\n\n\nsexmale\n0.0416\n0.0411\n\n\nfrailtypre-frail\n0.0654\n0.0598\n\n\nfrailtyrobust\n0.0621\n0.0559",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "stat-modeling-prob-model.html#gruppenvergleich",
    "href": "stat-modeling-prob-model.html#gruppenvergleich",
    "title": "51  Linear Probability Model",
    "section": "51.7 Gruppenvergleich",
    "text": "51.7 Gruppenvergleich\nHäufig ist es ja so, dass wir das Modell nur schätzen um dann einen Gruppenvergleich zu rechnen. Das heißt, dass es uns interessiert, ob es einen Unterschied zwischen den Leveln eines Faktors gegeben dem Outcome \\(y\\) gibt. Wir machen den Gruppenvergleich jetzt einmal an der Gebrechlichkeit frailty einmal durch. Wir habe die drei Gruppen frail, pre-frail und robust vorliegen. Danach schauen wir uns nochmal die prinzipielle Idee des Gruppenvergleichs auf mittleren Wahrscheinlichkeiten infiziert zu sein an.\nEigentlich ist es recht einfach. Wir nehmen wieder unser lineares Modell, was wir oben schon angepasst haben. Wir schicken dann das Modell in die Funktion emmeans() um die Gruppenvergleiche zu rechnen. Jetzt müssen wir nur zwei Dinge noch machen. Zum einen wollen wir alle paarweisen Vergleiche zwischen den drei Leveln von dem Faktor frailty rechnen, deshalb setzen wir method = \"pairwise\". Dann müssen wir noch dafür sorgen, dass wir nicht homogene Varianzen schätzen. Deshalb setzen wir die Option vcov. = sandwich::vcovHAC. Damit wählen wir aus dem Paket {sandwich} den Sandwichschätzer vcovHAC und berücksichtigen damit die Varianzheterogenität in den Daten. Wenn du das Paket {sandwich} schon geladen hast, dann musst du das Paket nicht mit Doppelpunkt vor die Funktion des Sandwich-Schätzers setzen.\n\nem_obj &lt;- lm_fit |&gt; \n  emmeans(~ frailty, method = \"pairwise\", vcov. = sandwich::vcovHAC)\nem_obj\n\n frailty   emmean     SE  df lower.CL upper.CL\n frail      0.668 0.0482 406    0.573    0.763\n pre-frail  0.706 0.0355 406    0.636    0.776\n robust     0.656 0.0288 406    0.600    0.713\n\nResults are averaged over the levels of: sex \nConfidence level used: 0.95 \n\n\nDann können wir auch schon uns die Kontraste und damit die Mittelwertsvergleiche wiedergeben lassen. Was heißt hier Mittelwertsvergleiche? Die Mittelwerte sind hier natürlich die mittlere Wahrscheinlichkeit infiziert zu sein. Wir adjustieren hier einmal die \\(p\\)-Werte für multiple Vergleiche nach Bonferroni, damit du auch mal die Optionen siehst.\n\nem_obj |&gt; \n  contrast(method = \"pairwise\", adjust = \"bonferroni\")\n\n contrast             estimate     SE  df t.ratio p.value\n frail - (pre-frail)   -0.0383 0.0597 406  -0.641  1.0000\n frail - robust         0.0115 0.0559 406   0.206  1.0000\n (pre-frail) - robust   0.0498 0.0458 406   1.088  0.8313\n\nResults are averaged over the levels of: sex \nP value adjustment: bonferroni method for 3 tests \n\n\nWir sehen also, dass es einen prozentualen Unterschied zwischen frail - (pre-frail) von -3% gibt. Daher ist die mittlere Wahrscheinlichkeit von pre-frail größer als die von frail. Den Zusammenhang sehen wir auch weiter oben in der Ausgabe von emmeans. Dort haben wir eine mittlere Infektionswahrscheinlichkeit von 66.8% für frail und eine mittlere Infektionswahrscheinlichkeit von 70.6% für pre-frail. Keiner der Vergleiche ist signifikant. Beachte, dass jeder Vergleich immer einen unterschiedlichen Standardfehler zugewiesen bekommt um die Varianzheterogenität zu berücksichtigen.\nJetzt lassen wir uns nochmal das unadjustierte compact letter display wiedergeben. Aber auch in dem unadjustierten Fall finden wir keine signifikanten Unterschiede.\n\nem_obj |&gt;\n  cld(Letters = letters, adjust = \"none\")\n\n frailty   emmean     SE  df lower.CL upper.CL .group\n robust     0.656 0.0288 406    0.600    0.713  a    \n frail      0.668 0.0482 406    0.573    0.763  a    \n pre-frail  0.706 0.0355 406    0.636    0.776  a    \n\nResults are averaged over the levels of: sex \nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nAm Ende möchte ich hier nochmal einen Spieldatensatz toy_tbl erstellen indem ich wiederum drei Gruppen miteinander vergleiche. Ich tue mal so als würden wir uns hier zwei Pestizide und eine Kontrolle anschauen. Unser Outcome ist dann, ob wir eine Infektion vorliegen haben oder das Pestizid alles umgebracht hat. Damit haben wir dann unser Outcome infected definiert. Wir bauen uns die Daten so, dass 80% der Beobachtungen in der Kontrolle infiziert sind. In den beiden Behandlungsgruppen sind jeweils 50% und 30% der Beobachtungen nach der Behandlung noch infiziert. Wir haben jeweils zwölf Pflanzen n_grp beobachtet. Das sind wirklich wenige Beobachtungen für einen \\(0/1\\) Endpunkt.\n\nn_grp &lt;- 12\ntoy_tbl &lt;- tibble(trt = gl(3, n_grp, labels = c(\"ctrl\", \"roundUp\", \"killAll\")),\n                  infected = c(rbinom(n_grp, 1, 0.8), rbinom(n_grp, 1, 0.5), rbinom(n_grp, 1, 0.2)))\n\nJetzt bauen wir uns wieder unser Modell zusammen.\n\ntoy_fit &lt;- lm(infected ~ trt, data = toy_tbl)\ntoy_fit\n\n\nCall:\nlm(formula = infected ~ trt, data = toy_tbl)\n\nCoefficients:\n(Intercept)   trtroundUp   trtkillAll  \n     0.8333      -0.4167      -0.5833  \n\n\nWie du sehen kannst, treffen wir die voreingestellten Infektionswahrscheinlichkeiten nur einigermaßen. Wir wollen für die Kontrolle 80% und erhalten 83.3%. Für roundUp haben wir 50% gewählt und erhalten \\(83.3 - 41.67 = 41.63\\). Auch bei killAll sieht es ähnlich aus, wir wollen 20% und erhalten $83.3 - 58.33 = 24.97$. Wir haben aber auch echt wenige Beobachtungen. Auf der anderen Seite ist es dann für ein agrarwissenschaftliches Experiment gra nicht mal so wenig.\nUnd hier sehen wir dann auch gleich das Problem mit der Funktion check_heteroscedasticity(). Wegen der geringen Fallzahl sagt die Funktion, dass alles okay ist mit den Varianzen und wir keine Varianzheterogenität vorliegen haben.\n\ncheck_heteroscedasticity(toy_fit)\n\nOK: Error variance appears to be homoscedastic (p = 0.527).\n\n\nWenn wir uns aber mal die Abbildung 51.6 anschauen sehen wir, dass wir auf keinen Fall Varianzhomogenität vorliegen haben. Die geringe Fallzahl von zwölf Beobachtungen je Gruppe ist zu klein, damit die Funktion check_heteroscedasticity() eine signifikanten Abweichung finden kann. Deshalb schaue ich mir immer die Abbildungen an.\n\ncheck_model(toy_fit, check = c(\"homogeneity\", \"normality\"))\n\n\n\n\n\n\n\nAbbildung 51.6— Überprüfung der Varianzhomogeniutät des Modells toy_fit mit der Funktion check_model(). Wir sehen hier eine klare Varianzheterogenität in dem Modell.\n\n\n\n\n\nWir können hier auch den coeftest() für Varianzheterogenität rechnen, aber wir sind ja hier an den Gruppenvergleichen interessiert, so dass wir dann gleich zu emmeans weitergehen.\n\ncoeftest(toy_fit, vcov. = vcovHC, type = \"HC1\")\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  0.83333    0.11237  7.4162 1.606e-08 ***\ntrtroundUp  -0.41667    0.18634 -2.2361  0.032226 *  \ntrtkillAll  -0.58333    0.17225 -3.3865  0.001844 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWie schon oben gezeigt, können wir dann einfach emmeans() nutzen um die Gruppenvergleiche zu rechnen. Auch hier müssen wir einmal angeben, dass wir einen paarweisen Vergleich rechnen wollen. Wir wollen alle Gruppen miteinander vergleichen. Dann noch die Option vcov. = sandwich::vcovHAC gewählt um für heterogene Varianzen zu adjustieren.\n\nem_obj &lt;- toy_fit |&gt; \n  emmeans(~ trt, method = \"pairwise\", vcov. = sandwich::vcovHAC)\nem_obj \n\n trt     emmean    SE df lower.CL upper.CL\n ctrl     0.833 0.106 33   0.6181    1.049\n roundUp  0.417 0.173 33   0.0641    0.769\n killAll  0.250 0.133 33  -0.0203    0.520\n\nConfidence level used: 0.95 \n\n\nAuch hier sehen wir die mittlere Wahrscheinlichkeit infiziert zu sein in der Spalte emmean. Die Standardfehler SE sind für jede Gruppe unterschiedlich, die Adjustierung für die Varianzheterogenität hat geklappt. Dann kannst du noch die paarweisen Gruppenvergleiche über einen Kontrasttest dir wiedergeben lassen.\n\nem_obj |&gt; \n  contrast(method = \"pairwise\", adjust = \"none\")\n\n contrast          estimate    SE df t.ratio p.value\n ctrl - roundUp       0.417 0.200 33   2.084  0.0449\n ctrl - killAll       0.583 0.170 33   3.435  0.0016\n roundUp - killAll    0.167 0.225 33   0.740  0.4645\n\n\nOder aber du nutzt das compact letter display.\n\nem_obj |&gt;\n  cld(Letters = letters, adjust = \"none\")\n\n trt     emmean    SE df lower.CL upper.CL .group\n killAll  0.250 0.133 33  -0.0203    0.520  a    \n roundUp  0.417 0.173 33   0.0641    0.769  a    \n ctrl     0.833 0.106 33   0.6181    1.049   b   \n\nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nDa die Fallzahl sehr gering ist, können wir am Ende nur zeigen, dass sich die Kontrolle von der Behandlung killAll unterscheidet. Hätten wir mehr Fallzahl, dann könnten wir sicherlich auch zeigen, dass der Unterschied zwischen der Kontrolle zu der Behandlung roundUp in eine signifikante Richtung geht. So klein ist der Unterschied zwischen Kontrolle und roundUp mit 41.7% ja nicht.\n\n\n\n\n\n\nAnwendungsbeispiel: Beschädigter Mais nach der Ernte\n\n\n\nIm folgenden Beispiel schauen wir uns nochmal ein praktische Auswertung von einem agrarwissenschaftlichen Beispiel mit Mais an. Wir haben uns in diesem Experiment verschiedene Arten trt von Ernteverfahren von Mais angeschaut. Dann haben wir nach vier Zeitpunkten bestimmt, ob der Mais durch das Ernetverfahren nachträglich beschädigt war. Die Beschädigung selber wurde dann etwas komplizierter mit einem Labortest festgestellt, aber wir schauen uns nur die Ausprägung ja/nein also \\(1/0\\) als Outcome an. Durch einen Fehler im Labor müssen wir eine Kombination für den letzten Tag und der dritten Behandlung entfernen.\n\nmaize_tbl &lt;- read_excel(\"data/maize_rate.xlsx\") |&gt; \n   mutate(damaged = ifelse(time == \"5d\" & trt == 3, NA, damaged),\n          trt = factor(trt, labels = c(\"wenig\", \"mittel\", \"viel\")))\n\nDann rechnen wir auch schon das lm() Modell und nutzen {emmeans} für den Gruppenvergleich. Hier unbedingt SE als den Standardfehler für die Fehlerbalken nutzen, da wir sonst Fehlerbalken größer und kleiner als \\(0/1\\) erhalten, wenn wir die Standardabweichung nutzen würden. Du solltest auch immer von Varianzheterogenität ausgehen, deshalb nutze ich hier auch die Option vcov. = sandwich::vcovHAC in emmeans(). In der Abbildung 51.7 siehst du das Ergebnis der Auswertung in einem Säulendiagramm. Wir sehen einen klaren Effekt der Behandlung viel. Schade, dass wir dann nach 5 Tagen leider keine Auswertung für die dritte Behandlung vorliegen haben. Aber sowas passiert dann unter echten Bedingungen mal schnell.\n\nlm(damaged ~ trt + time + trt:time, data = maize_tbl) |&gt; \n  emmeans(~ trt * time, vcov. = sandwich::vcovHAC) |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\") |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = time, y = emmean, fill = trt)) +\n  theme_minimal() + \n  labs(y = \"Anteil beschädigter Mais\", x = \"Stunden nach Ernte\",\n       fill = \"Behandlung\") +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_text(aes(label = .group, y = emmean + SE + 0.01),  \n            position = position_dodge(width = 0.9), vjust = -0.25) +\n  geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 51.7— Säulendigramm der Anteile des beschädigten Mais aus einem linearen Modell. Das lm()-Modell berechnet die Mittelwerte in jeder Faktorkombination, was dann die Anteile des beschädigten Mais entspricht. Das compact letter display wird dann in {emmeans} generiert.\n\n\n\n\n\n\n\n\n\n\nAbbildung 51.1— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und den Entzündungswerten der Ferkel.\nAbbildung 51.2— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und den Entzündungswerten der Ferkel ergänzt um Gerade aus der Gaussian linearen Regression.\nAbbildung 51.3— Varianzheterogenität oder Heteroskedastizität in den Daten. Die Abstände der Punkte zu der Geraden, die Residuen, werden immer größer. Wir haben keinen konstanten oder homogenen Fehler.\nAbbildung 51.4— Überprüfung der Varianzhomogeniutät des Modells lm_fit mit der Funktion check_model(). Wir sehen hier eine klare Varianzheterogenität in dem Modell.\nAbbildung 51.5— Etwas andere Darstellung der angepassten Werte .fitted und der Residuen .resid aus dem Modell lm_fit. Die Punkte sind nach dem Infektionsstatus eingefärbt.\nAbbildung 51.6— Überprüfung der Varianzhomogeniutät des Modells toy_fit mit der Funktion check_model(). Wir sehen hier eine klare Varianzheterogenität in dem Modell.\nAbbildung 51.7— Säulendigramm der Anteile des beschädigten Mais aus einem linearen Modell. Das lm()-Modell berechnet die Mittelwerte in jeder Faktorkombination, was dann die Anteile des beschädigten Mais entspricht. Das compact letter display wird dann in {emmeans} generiert.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Linear Probability Model</span>"
    ]
  },
  {
    "objectID": "stat-modeling-beta.html",
    "href": "stat-modeling-beta.html",
    "title": "51  Beta Regression",
    "section": "",
    "text": "51.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, betareg, car,\n               see, performance, parameters, agridat, mfp,\n               emmeans, multcomp, rcompanion, ggbeeswarm,\n               marginaleffects, nls.multstart, conflicted)\nconflicts_prefer(dplyr::summarise)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\ncb_pal &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n            \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Beta Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-beta.html#daten",
    "href": "stat-modeling-beta.html#daten",
    "title": "51  Beta Regression",
    "section": "51.2 Daten",
    "text": "51.2 Daten\nWie immer schauen wir uns verschiedene Datensätze an, Visualisieren die Zusammenhänge und rechnen dann verschiedene Modelle, die passen könnten. Beginnen möchte ich mit einem Datensatz zu dem Jagederfolg in [%] von Schneefüchsen in verschiedenen Habitaten. Die Daten sind etwas gekürzt, wir haben nur den Jagederfolg und keine Informationen zu den Habitaten. Des Weiteren wollen wir schauen, ob der Jagderfolg der Eisfüche von der standardisierten Schneehöhe in [cm] abhängt. Wir haben hier mehr oder minder die Schneehöhe in dem Habit gemittelt. Der Eisfuchs jagt ja nicht immer an der perfekt gleichen Stelle, wo wir die Schneehöhe kennen.\n\nhunting_tbl &lt;- read_excel(\"data/hunting_fox.xlsx\") |&gt; \n  mutate(proportion = round(success/attempts, 2))\n\nIn der Tabelle 51.1 siehst du einen Auszug aus den Daten. Wir haben die Schneehöhe gemessen und geschaut von wie vielen Anläufen attempts eine Maus unter dem Schnee zu fangen erfolgreich war success oder eben ein Fehlschlag fail. Daraud haben wir dann die Erfolgsrate proportion berechnet. Wir haben einfach den Anteil der Erfolge eine Maus zu fangen an den gesamten Versuchen berechnet.\n\n\n\n\nTabelle 51.1— Auszug aus den Daten zu dem Jagderfolg in [%] von Eisfüchsen in abhängigkeit von der Schneehöhe in [cm].\n\n\n\n\n\n\nsnow_height\nattempts\nsuccess\nfail\nproportion\n\n\n\n\n30\n18\n16\n2\n0.89\n\n\n33\n20\n15\n5\n0.75\n\n\n39\n20\n18\n2\n0.9\n\n\n55\n24\n18\n6\n0.75\n\n\n…\n…\n…\n…\n…\n\n\n65\n22\n10\n12\n0.45\n\n\n63\n16\n2\n14\n0.12\n\n\n70\n15\n1\n14\n0.07\n\n\n18\n21\n20\n1\n0.95\n\n\n\n\n\n\n\n\nIm Weiteren schauen wir uns einen Datensatz zu Brokkoli an. Wir wollen hier einmal schauen, ob wir das Zielgewicht von \\(500g\\) erreichen. Wir sind aber daran interessiert die Rate von untergewichtigen Brokkoli möglichst klein zu halten. Deshalb schauen wir uns in dieser Auswertung den Anteil von Brokkoli unter der Zielmarke von \\(500g\\) für zwei Düngezeitpunkte sowie drei Düngestufen an. Wir müssen hier jetzt die Daten etwas mehr aufbereiten, da wir mehr Informationen in den Daten haben als wir wirklich brauchen.\n\nbroc_tbl &lt;- read_excel(\"data/broccoli_weight.xlsx\") |&gt; \n  filter(fert_time %in% c(\"early\", \"late\")) |&gt; \n  mutate(fert_time = factor(fert_time, levels = c(\"early\", \"late\")),\n         fert_amount = as_factor(fert_amount),\n         block = as_factor(block)) |&gt;\n  select(fert_time, fert_amount, block, weight) |&gt; \n  filter(weight &lt; 500) |&gt; \n  mutate(proportion = weight/500) |&gt; \n  select(-weight)\n\nIn der Tabelle 51.2 siehst du einmal den Auszug aus den Brokkolidaten. Wir wollen jetzt sehen, ob wir in den Behandlungsfaktoren einen Unterschied bezüglich der Anteile der untergewichtigen Brokkoliköpfe finden. Tendenziell wollen wir eine Kombination finden, die uns natürlich möglichst schwere Köpfe beschert.\n\n\n\n\nTabelle 51.2— Auszug aus dem Daten zu den Zielgewichten von Brokkoli zu zwei Düngezeitpunkten und drei Düngestufen.\n\n\n\n\n\n\nfert_time\nfert_amount\nblock\nproportion\n\n\n\n\nearly\n150\n1\n0.82586\n\n\nearly\n150\n1\n0.55294\n\n\nearly\n150\n1\n0.98314\n\n\nearly\n150\n1\n0.68964\n\n\n…\n…\n…\n…\n\n\nearly\n225\n4\n0.43816\n\n\nearly\n225\n4\n0.86136\n\n\nearly\n225\n4\n0.547\n\n\nearly\n225\n4\n0.80844\n\n\n\n\n\n\n\n\nAbschließend schauen wir nochmal in das R Paket {agridat} und nehmen von dort den Datensatz salmon.bunt welcher eine Pilzinfektion von Weizenlinien beschreibt. Mehr dazu dann auf der Hilfeseite Fungus infection in varieties of wheat in der Vignette zum R Paket. Ich möchte später die Faktoren gen für die genetischen Linien und die Pilzarten bunt für die Anteile der Pilzinfektionen sortiert haben. Das mache ich dann einmal mit der Funktion fct_reorder() welche mir erlaubt einen Faktor nach einer anderen Variable zu sortieren. Wir haben zwei Wiederholungen rep, die auch so nicht helfen. Deshalb mittlere ich mit summarise() über die beiden Wiederholungen die Prozente der Pilzinfektionen des Weizen.\n\ndata(salmon.bunt)\nfungi_tbl &lt;- salmon.bunt |&gt; \n  as_tibble() |&gt; \n  select(gen, bunt, rep, percent = pct) |&gt; \n  mutate(gen = fct_reorder(gen, percent),\n         bunt = fct_reorder(bunt, percent),\n         percent = percent/100 + 0.001) \n\nIn der Tabelle 51.3 siehst du dann einmal den Auszug aus unseren Weizendaten mit einer Pilzinfektion. Wir haben 10 genetische Linien sowie 20 Pilzarten vorliegen. Daher ist der Datensatz ziemlich groß, was die Möglichkeiten der Faktorkombinationen angeht.\n\n\n\n\nTabelle 51.3— Auszug aus dem Daten zu den zehn Weizenlinien und den zwanzig Pilzarten. Es wurde der Anteil an infizierten Weizen [%] gemessen.\n\n\n\n\n\n\ngen\nbunt\nrep\npercent\n\n\n\n\nHybrid128\nB1\nR1\n0.867\n\n\nHybrid128\nB2\nR1\n0.765\n\n\nHybrid128\nB3\nR1\n0.95\n\n\nHybrid128\nB4\nR1\n0.911\n\n\n…\n…\n…\n…\n\n\nHussar\nB32\nR2\n0.034\n\n\nHussar\nB51\nR2\n0.001\n\n\nHussar\nB157\nR2\n0.377\n\n\nHussar\nB189\nR2\n0.703\n\n\n\n\n\n\n\n\nDamit habe wir dann einige spannende Datensätze vorliegen, die wir nutzen können um die verschiedenen Aspekte der Beta Regression anzuschauen. Nicht immer muss es ja eine Beta Regression sein, wir haben auch die Möglichkeit unsere Fragestellung mit anderen Modellen eventuell anders oder gar besser zu beantworten.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Beta Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-beta.html#visualisierung",
    "href": "stat-modeling-beta.html#visualisierung",
    "title": "51  Beta Regression",
    "section": "51.3 Visualisierung",
    "text": "51.3 Visualisierung\nAuch hier beginnen wir einmal mit der Visualisierung der Daten. Zuerst schauen wir uns einmal die Daten zu dem Jagderfolg der Eisfüchse in verschiedenen Habitaten in Abhängigkeit zu der Schneehöhe an. Wenn der Schnee zu hoch liegt, werden die Füchse weniger Jagderfolg haben. In der Abbildung 51.1 sehen wir einmal den Jagerfolg von der Schneehöhe aufgetragen. Wir erkennen, dass wir zwar anfänglich eher einen linearen Zusammenhang haben könnten, aber bei höheren Schneedichten dann sehr schnell einen Abfall des Jagderfolges beobachten. Wir schauen uns dann gleich mal verschiedene Modelle an um eine Kurve durch die Punkte zu legen.\n\nhunting_tbl |&gt; \n  ggplot(aes(snow_height, proportion)) +\n  theme_minimal() +\n  geom_point() +\n  labs(y = \"Jagderfolg [%]\", x = \"Standardisierte Schneehöhe [cm]\") +\n  ylim(0, 1)\n\n\n\n\n\n\n\nAbbildung 51.1— Zusammenhang zwischen dem Jagederfolg von Eisfüchsen und der Schneehöhe in den jeweiligen beobachteten Habitaten.\n\n\n\n\n\nIn der Abbildung 51.2 sehen wir einmal die Verteilung unser untergewichtigen Brokkoli für die beiden Düngezeitpunkte und Düngemengen. Wir könnten annehmen, dass wir tendenziell bei einer höheren Düngemenge einen größeren Anteil an erreichtem Zielgewicht erhalten. Global betrachtet scheint es aber nicht so große Effekte zu geben. Wir schauen uns diese Beispiel dann einmal für den Gruppenvergleich an.\n\nbroc_tbl |&gt; \n  ggplot(aes(x = fert_amount, y = proportion, color = fert_time)) +\n  theme_minimal() + \n  labs(y = \"[%] erreichtes Zielgewicht\", x = \"Düngemenge [mg/l]\",\n       color = \"Düngezeitpunkt\") +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +\n  geom_beeswarm(dodge.width = 0.8) +\n  theme(legend.position = \"top\") +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 51.2— Zusammenhang zwischen erreichten Zielgewicht von \\(500g\\) bei Brokkoli [%] und der Düngermenge sowiw dem Düngezeitpunkt.\n\n\n\n\n\nAbschließend schauen wir uns in der Abbildung 51.3 die Heatmap der genetischen Linien und der Art des Pilzes an. Ich habe die Heatmap so erstellt, dass eine viel Infektion rot dargestellt wird und wenig Infektion blau. Durch die Sortierung der Faktoren nach dem Infektionsgrad können wir sehr schön die Linien voneinander unterscheiden. Teilweise werden einige Linien von dem Pilz förmlich aufgefressen während andere Weizenlinien kaum befallen werden. Auch scheinen einige Arten des Pilzen mehr Weizenlinien befallen zu können als andere Pilzarten. So ist die Pilzart B189 extrem erfolgreich bei einer großen Anzahl an Linien. Die Art B1 hingegen kann mehr oder minder nur zwei Weizensorten befallen.\n\nfungi_tbl |&gt; \n  group_by(bunt, gen) |&gt; \n  summarise(percent = mean(percent)) |&gt; \n  ggplot(aes(x = gen, y = bunt, fill = percent)) +\n  theme_minimal() +\n  geom_tile() +\n  scale_fill_gradientn(colors = c(\"#375997\", \"gray\", \"firebrick\"),\n                       breaks = seq(0, 1, 0.1), \n                       limits = c(0, 1)) + \n  labs(y = \"Art des Pilzes\", x = \"Genetische Linie des Weizens\",\n       fill = \"[%] infiziert\")\n\n\n\n\n\n\n\nAbbildung 51.3— Sortierte Heatmap der genetischen Linien und der Art des Pilzes. Farbig dargestellt sind die Anteile an infizierten Weizen in [%]. Nach dem Grad der Infektion wurden die Faktoren sortiert um eine bessere Übersicht zu erhalten.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Beta Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-beta.html#fit-des-modells",
    "href": "stat-modeling-beta.html#fit-des-modells",
    "title": "51  Beta Regression",
    "section": "51.4 Fit des Modells",
    "text": "51.4 Fit des Modells\nDann haben wir jetzt unsere Daten und wissen auch grob was in den Daten stecken könnte. Jetzt wollen wir einmal die verschieden Datensätze auswerten. Je nach Fragestellung können wir da verschiedene Modelle nutzen. Wie immer stelle ich auch Alternativen zu der Beta Regression vor. Wir schauen uns zum einen eine einfache Gaussian Regression an, die passt zwar nicht so richtig zu dem Outcome, aber unter bestimmten Voraussetzungen kann die Gaussian Regression Sinn machen. Wenn wir Erfolg/Misserfolg in unseren Daten als Outcome haben, dann können wir auch eine logistische Regression rechnen. Dementsprechend zeige ich die Anwendung auch einmal auf den Eisfuchsdaten, wo wir ja wissen wie oft ein Erfolg und ein Fehlschlag vorgekommen ist.\n\n51.4.1 … mit der Gaussian Regression\nFangen wir also einmal mit der etwas groben Variante an. Wir rechnen einfach eine lineare Regression unter der Annahme das unser Outcome normalverteilt ist. Das stimmt zwar nur begrenzt für eine Wahrscheinlichkeit, die zwischen 0 und 1 liegt, aber rechnen können wir ja erstmal viel. Besonders wenn du Wahrscheinlichkeiten berechnet hast, die nicht sehr viele Nullen und Einsen beinhalten sondern mehr um die 0.5 streuen, dann kann deine Auswertung auch mit einer Gaussian Regression funktionieren. Wie immer kommt es dann auf den Einzelfall an, aber die Gaussian Regression liefert dann eben auch einen sehr gut zu verstehenden und interpretierenden Effektschätzer.\nWir wenden jetzt also einfach mal die Gaussian Regression mit der Funktion lm() auf unsere Daten zu dem Jagderfolg der Eisfüchse an. Wenn du dir nochmal die Abbildung 51.1 anschaust, dann siehst du, dass wir nicht so viele Beobachtungen nah der Eins und der Null haben. Darüber hinaus ist eine wage Linearität zu erkennen. Oder anderherum, die Daten sehen jetzt nicht so schlimm aus, dass wir nicht eine Gerade durch die Punkte legen könnten.\n\nhunting_lm_fit &lt;- lm(proportion ~ snow_height, data = hunting_tbl)\n\nDann schauen wir uns einmal die Koeffizienten des Modells einmal an. Wir haben einen Intercept von über Eins, was natürlich keinen Sinn ergibt. Wir können keinen Jagederfolg von über Eins bei einer Schneehöhe von Null haben. Hier sieht man schon, dass das Modell nicht so gut für die Randbereiche funktioniert. Dennoch haben wir einen Abfall durch die Steigung der Schneehöhe vorliegen. Wir erkennen, dass pro Zentimeter mehr Schnee der Jagderfolg um \\(0.0156\\) oder eben \\(1.56\\%\\) signifikant zurückgeht. Das ist ein Ergebnis mit dem wir leben könnten.\n\nhunting_lm_fit \n\n\nCall:\nlm(formula = proportion ~ snow_height, data = hunting_tbl)\n\nCoefficients:\n(Intercept)  snow_height  \n    1.36883     -0.01564  \n\n\nWir können uns auch den Effekt der Schneehöhe auf den Jagderfolg einmal mit der Funktion avg_slopes() aus dem R Paket {marginaleffects} wiedergeben lassen. Für eine so simple Gaussion Regression ist der marginale Effekt gleich der Steigung, aber wenn wir gleich ein anderes Modell nehmen, dann wird es schon komplizierter. Mehr dazu dann auf der Hilfeseite zu Marginal Effects Zoo - Slopes wo du dann auch mehr über marginal effects lernen kannst. Wie du dann siehst, erhalten wir hier den gleichen Wert für die Steigung.\n\navg_slopes(hunting_lm_fit)\n\n\n        Term Estimate Std. Error     z Pr(&gt;|z|)    S   2.5 %  97.5 %\n snow_height  -0.0156    0.00203 -7.72   &lt;0.001 46.3 -0.0196 -0.0117\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nSchauen wir einmal wie das Bestimmtheitsmaß \\(R^2\\) aussieht. Hier haben wir einen Wert von \\(0.758\\) und damit können wir durch die Gerade gut \\(75\\%\\) der Varianz erklären. Das ist jetzt nicht der beste Werte und wir schauen uns am Ende nochmal in der Abbildung 51.5 wie die Gerade durch die Punkte läuft.\n\nhunting_lm_fit |&gt; r2()\n\n# R2 for Linear Regression\n       R2: 0.758\n  adj. R2: 0.746\n\n\nDas einmal als sehr schneller und kurzer Einwurf der Gaussian Regression auf einem Outcome mit Prozenten. Es ist nicht ideal und weit weg von der Empfehlung. Aber wenn du einen statistischen Engel anfahren willst und mit der Interpretation ganz gut leben kannst, dann ist eine lineare Modellierung nicht so dramatisch. Achtung eben an den Rändern. Du erhälst eben auch schnell mal vorhergesagte Werte außerhalb von den Grenzen einer Wahrscheinlichkeit.\n\n\n51.4.2 … mit einer logistischen Regression\nJa, auch dieses Problem können wir mit einer logistischen Regression angehen. Wenn du auch mal in den anderen Kapiteln geschaut hast, dann wundert es dich vermutlich nicht mehr, dass die logistische Regression außerhalb der Agarwissenschaften zu einer der beliebtesten Analysewerkzeugen gehört. Hier müssen wir aber unser Outcome etwas anders der Funktion glm() für die logistische Regression übergeben. Wir nutzen nämlich dafür das Wilkinson-Rogers Format welches dann den Anteil an Erfolgen an Fehlschlägen beschreibt. Wir schreiben aber in das Modell die konkrete Anzahl an Erfolgen und Fehlschlägen.\n\\[\n(Success|Failure) \\sim x_1 + x_2 + ... + x_p\n\\]\nIn R würden wir dann die zwei Spalten mit der Anzahl an Erfolgen und Fehlschlägen mit cbind() zusammenfassen und in glm() ergänzen. Dieses Format haben wir dann auch bei unseren Eisfüchsen vorliegen. In unseren Daten haben wir ja die Spalte success, welche die Anzahl Jagderfolge beschreibt sowie die Anzahl der Fehlschläge in der Spalte fail. Wichtig ist hier, dass wir wirklich die beiden Spalten mit den jeweiligen Anzahlen haben. Daher würden wir dann in R wie folgt schreiben.\n\nhunting_log_fit &lt;- glm(cbind(success, fail) ~ snow_height, \n                       data = hunting_tbl, family = binomial)\nhunting_log_fit\n\n\nCall:  glm(formula = cbind(success, fail) ~ snow_height, family = binomial, \n    data = hunting_tbl)\n\nCoefficients:\n(Intercept)  snow_height  \n    4.80027     -0.08326  \n\nDegrees of Freedom: 20 Total (i.e. Null);  19 Residual\nNull Deviance:      146.7 \nResidual Deviance: 31.9     AIC: 99.35\n\n\nDamit haben wir dann auch unseren Koeffizienten des Intercept und der Steigung. Da die logistische Regression auf dem Logit-Link rechnet, können wir die Steigung von \\(-0.08326\\) nicht direkt interpretieren. Daher müssen wir die Steigung wieder von dem Logit-Link auf unsere ursprüngliche Skala (eng. response) zurückrechnen. Dafür gibt es einen einfachen Trick mit der “Teile durch Vier” Regel oder aber eine entsprechende Funktion im R Paket {marginaleffects}. Wichtig ist auch hier, uns interessieren nicht die Odds Ratios aus einer logistischen Regression, da wir an dem Effekt der Steigung interessiert sind. Wir wollen ja wissen, wie sich der Jagderfolg in [%] durch die steigende Schneeschöhe verändert.\n\n\n\n\n\n\n“Teile durch Vier” Regel\n\n\n\nDie “Teile durch Vier” Regel (eng. Divide by 4 Rule) erlaubt uns von dem Koeffizienten aus der logistischen Regression auf die wahre Steigung der Geraden zu schließen. Da die logistische Regression auf dem Logit-Link rechnet, ist unsere \\(-0.08\\) nicht die Steigung der Geraden. Es gibt hier die Daumenregel, den Koeffizienten durch Vier zu teilen und so einen annähernd korrekten Wert zu erhalten. In unserem Fall also \\(-0.08326/4 = -0.021\\). Damit haben wir nicht den exakten Wert der Steigung, aber eine recht guten Wert.\n\n\nMit der Funktion avg_slopes() können wir uns aus dem Modell die Steigung der Gerade berechnen lassen. Wir haben dann nicht mehr die Logit-Skala vorliegen sondern sind wieder auf unserer ursprünglichen Skala der Prozente. Im Gegensatz zur “Teile durch Vier” Regel ist der Wert von \\(-0.15\\) natürlich genauer. Wir sehen, dass wir fast einen ähnlichen Wert für die Reduzierung des Jagderfolges der Eisfüchse wie bei der Gaussian Regression erhalten.\n\navg_slopes(hunting_log_fit)\n\n\n        Term Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 % 97.5 %\n snow_height   -0.015    0.00101 -14.9   &lt;0.001 163.8 -0.017 -0.013\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nLeider gibt es für eine logistische Regression mit einem \\((Success/Failure)\\)-Outcome kein kein echtes Bestimmtheitsmaß \\(R^2\\). Hier greifen wir auf das R Paket {rcompanion} zurück. Wir rechnen auch dabei auch hier kein echtes Bestimmtheitsmaß \\(R^2\\) aus, sondern den Vergleich zu einem Null-Modell in dem wir gar keine Variable als Einfluss mit ins Modell nehmen. Das folgende Bestimmtheitsmaß \\(R^2\\) beantwortet also eher die Frage, ob wir besser mit unserem Modell mit der Schneehöhe sind, also mit einem Modell ohne die Schneehöhe.\n\nhunting_log_fit |&gt; \n  nagelkerke() |&gt; \n  pluck(\"Pseudo.R.squared.for.model.vs.null\")\n\n                             Pseudo.R.squared\nMcFadden                             0.546227\nCox and Snell (ML)                   0.995769\nNagelkerke (Cragg and Uhler)         0.995814\n\n\nHm, am Ende würde ich es lassen. Keine der Bestimmtheitsmaße \\(R^2\\) ist wirklich sinnig. Das erste Bestimmtheitsmaß ist viel zu niedrig, dafür das wir so nahe an den Koeffizienten der Gaussian Regression sind. Dafür sind die anderen beiden Bestimmtheitsmaße viel zu optimistisch, wie wir gleich in der Abbildung 51.5 sehen werden. Aber damit haben wir auch gesehen, dass wir die Analyse auch mit einer logistischen Regression rechnen können.\n\n\n51.4.3 … mit dem R Paket {betareg}\nJetzt haben wir uns durch andere Modellierungen durchgearbeitet und wollen uns jetzt einmal die Beta Regression anschauen. Die Beta Regression ist in dem R Paket {betareg} implementiert und kann über die Funktion betareg() angewendet werden. Wir haben hier noch den Sonderfall, dass wir das Modell mit einem | schreiben können. Wir können nämlich bei der Beta Regression auch die Varianz global schätzen oder aber für eine Variable adjustieren. Wir wählen hier einmal beide Schreibweisen in den folgenden Tabs und schauen was dann herauskommt. Sonst müssen wir bei der Beta Regression erstmal nichts beachten - wie immer schauen wir dann nochmal, ob unser Modell auch gut funktioniert hat.\n\nMit ~ snow_heightMit ~ snow_height | snow_height\n\n\nDas simpleste Modell wäre hier, dass wir einfach den Jagderfolg in [%] durch die Schneehöhe modellieren. Wir nehmen dann mehr oder minder Varianzhomogenität an und erlauben keinen gesonderten Varianzterm. Das kann gut funktionieren, aber meistens variiert die Varianz der \\(y\\)-Werte über die \\(x\\)-Werte. Wir haben meist eine größere Varianz bei größeren \\(x\\)-Werten.\n\nhunting_beta_fit &lt;- betareg(proportion ~ snow_height, data = hunting_tbl)\nhunting_beta_fit\n\n\nCall:\nbetareg(formula = proportion ~ snow_height, data = hunting_tbl)\n\nCoefficients (mean model with logit link):\n(Intercept)  snow_height  \n    4.51312     -0.08082  \n\nPhi coefficients (precision model with identity link):\n(phi)  \n12.53  \n\n\nWir sehen, dass wir unseren Koeffizienten erhalten, der fast unserer logistischen Regression entspricht. Jedenfalls numerisch. Aber auch hier dürfen wir uns nicht blenden lassen, wir rechnen auch in einer Beta Regression auf dem Logit-Link. Deshalb müssen wir die Steigung der Schneehöhe dann erst wieder zurückrechnen. Auch hier hilft die Funktion avg_slopes() aus dem R Paket {marginaleffects}.\n\navg_slopes(hunting_beta_fit)\n\n\n        Term Estimate Std. Error     z Pr(&gt;|z|)     S  2.5 %  97.5 %\n snow_height  -0.0149    0.00109 -13.6   &lt;0.001 137.4 -0.017 -0.0127\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nWir sehen, dass mit jedem Zentimeter mehr Schnee der Jagderfolg um \\(-0.0149\\) oder \\(-1.49\\%\\) zurückgeht. Das sind ähnliche Zahlen wie auch schon bei der logistischen Regression sowie der Gaussian Regression.\n\n\nWenn wir erlauben, dass sich die Varianz der \\(y\\)-Werte über den Verlauf der \\(x\\)-Werte ändern darf, dann schreiben wir nochmal die Variable, die die Varianz verursacht hinter das Symbol |. Wir haben hier nur die Schneehöhe vorliegen, so dass unser Modell sich dann von alleine ergibt. Meistens macht es mehr Sinn die Varianz auch über eine Variable mit zu modellieren. Schauen wir mal, ob wir mit der separaten Modellierung der Varianz der Schneehöhe mehr erreichen.\n\nhunting_beta_phi_fit &lt;- betareg(proportion ~ snow_height | snow_height, data = hunting_tbl)\nhunting_beta_phi_fit\n\n\nCall:\nbetareg(formula = proportion ~ snow_height | snow_height, data = hunting_tbl)\n\nCoefficients (mean model with logit link):\n(Intercept)  snow_height  \n    3.99212     -0.06798  \n\nPhi coefficients (precision model with log link):\n(Intercept)  snow_height  \n    6.42223     -0.07271  \n\n\nAber auch hier dürfen wir uns nicht blenden lassen, wir rechnen auch in einer Beta Regression auf dem Logit-Link. Deshalb müssen wir die Steigung der Schneehöhe dann erst wieder zurückrechnen. Auch hier hilft die Funktion avg_slopes() aus dem R Paket {marginaleffects}.\n\navg_slopes(hunting_beta_phi_fit)\n\n\n        Term Estimate Std. Error     z Pr(&gt;|z|)     S   2.5 % 97.5 %\n snow_height  -0.0132    0.00108 -12.2   &lt;0.001 110.7 -0.0153 -0.011\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nSpannenderweise ist hier der Effekt der Schneehöhe noch einen Tick geringer mit \\(-0.0132\\) pro Zentimeter mehr Schnee. Dann müssen wir uns gleich einmal anschauen welches Modell den niedrigeren AIC-Wert hat.\n\n\n\nDann berechnen wir einmal die AIC-Werte für die beiden Modelle. Ein niedriger AIC-Wert ist besser. Je kleiner oder negativer ein AIC-Wert eines Modells ist, desto besser ist das Modell im Vergleich zu einem anderen Modell. Das AIC ist ein Maß dafür, wie gut die Daten durch das Modell erklärt werden, korrigiert um die Komplexität des Modells. Wir berechnen mit der Funktion AIC() einmal die AIC-Werte der beiden Beta Regressionsmodellen.\n\nAIC(hunting_beta_fit)\n\n[1] -27.58488\n\nAIC(hunting_beta_phi_fit)\n\n[1] -33.9114\n\n\nDie absoluten Zahlen vom AIC sind nicht von Bedeutung. Erstmal sehen wir, dass unser Modell mit der Berücksichtigung der Varianz durch die Schneehöhe hunting_beta_phi_fit besser ist als das reine Modell ohne Berücksichtigung. Wir wollen aber immer die Differenzen von AIC betrachten. Du findest in der wissenschaftlichen Veröffentlichung Multimodel Inference: Understanding AIC and BIC in Model Selection mehr Informationen zu den Entscheidungen (Burnham und Anderson 2004, pp. 270-271). Wichtig ist hier, wenn die Differenz größer ist als 2, dann haben wir einen signifikanten Unterschied zwischen den Modellen und wir sollten das Modell mit dem niedrigeren AIC nehmen.\n\nAIC(hunting_beta_fit) - AIC(hunting_beta_phi_fit)\n\n[1] 6.326516\n\n\nWir nehmen dann mal auf jeden Fall das Modell mit der Berücksichtigung der Varianz durch die Schneehöhe hunting_beta_phi_fit. Damit würden wir einen Effekt der Schneehöhe von \\(-0.0132\\) berichten. Berechnen wir jetzt nochmal als Vergleich das Bestimmtheitsmaß \\(R^2\\) und sehen, dass der Wert schon besser ist. Damit können wir dann auf jeden Fall leben.\n\nhunting_beta_phi_fit |&gt; r2()\n\n# R2 for Beta Regression\n  Pseudo R2: 0.796\n\n\nIn der Abbildung 51.4 sehen wir nochmal die Abbildungen der Modelldiagnostik. Wie du sehen kannst, sind die Ergebnisse zufriedenstellend. Der Residualplot sieht aus, wie wir ihn erwarten würden. Unsere Beobachtungen streuen um die Gerade. Auch haben wir keine Ausreißer vorliegen. Unsere Werte der Cook’s distance sind okay. Der Grenzwert wäre hier \\(4/n = 4/21 = 0.19\\). Unsere erste Beobachtung würde den Grenzwert reißen, aber das ist noch okay so. Das passt auch zu der Abbildung der Residuen. Auch die Generalized leverage deutet auf einen Ausreißer hin. Wir haben hier den Grenzwert von \\(3(k+1)/n\\) mit \\(k\\) gleich der Anzahl an Variablen im Modell. Somit liegt unser Grenzwert bei \\(3(1+1)/21 = 0.29\\). Auch der abschließende Residualplot sieht so aus, wie wir ihn erwarten würden. Als Fazit lässt sich ziehen, das wir zwar eine Beobachtung drin haben, die etwas am Rand liegt, was hier aber für mich noch nicht fürs Entfernen spricht.\n\nplot(hunting_beta_phi_fit)\n\n\n\n\n\n\n\n\n\n\n\n(a) Residualplot I\n\n\n\n\n\n\n\n\n\n\n\n(b) Cooks distance\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Generalized leverage\n\n\n\n\n\n\n\n\n\n\n\n(d) Residualplot II\n\n\n\n\n\n\n\nAbbildung 51.4— Ausgabe ausgewählter Modelgüteplots der Funktion plot(). Im Gegensatz zu der Funktion check_model() musst du wissen, was du erwarten würdest.\n\n\n\n\nDamit hätten wir höchstens einen Ausreißer, aber das kann bei echten Daten schon mal vorkommen, dass nicht alle Datenpunkte perfekt zu einem Modell passen. In der Abbildung 51.5 siehst du nochmal die Daten der Jagderfolge der Eisfüche im Zusammenhang mit der Schneehöhe dargestellt. Ich sehe da keinen eindeutigen Ausreißer und deshalb lasse ich alle Beobachtungen im Modell. Im Weiteren siehst du einmal die Modelle, die ich gerechnet habe, jeweils als Kurve dargestellt. Das beste Modell ist das betareg_phi Modell, was auch ziemlich gut durch die Punkte läuft. Du siehst besonders gut, wie das lineare Modell lm leider die Punkte nur sehr unzureichend an den Rändern trifft. Kann ja das Modell auch nicht anders, es muss ja eine Linie sein. Beachte aber vor allem die Unterschiede in den Effekten. Das lineare Modell mit der Gaussian Regression hat einen viel größeren Effekt berechnet als das beste Modell mit der Beta Regression.\n\nhunting_tbl |&gt; \n  ggplot(aes(snow_height, proportion)) +\n  theme_minimal() +\n  geom_point() +\n  geom_line(aes(y = predict(hunting_log_fit, type = \"response\"), color = \"log\")) +\n  geom_line(aes(y = predict(hunting_lm_fit, type = \"response\"), color = \"lm\")) +\n  geom_line(aes(y = predict(hunting_beta_fit, type = \"response\"), color = \"betareg\")) +\n  geom_line(aes(y = predict(hunting_beta_phi_fit, type = \"response\"), color = \"betareg_phi\")) +\n  scale_color_manual(name = \"Modell\", values = cb_pal[2:5])\n\n\n\n\n\n\n\nAbbildung 51.5— Zusammenhang zwischen dem Jagederfolg von Eisfüchsen und der Schneehöhe in den jeweiligen beobachteten Habitaten zusammen mit den vier vorgestellten Modellen. Das Modell betareg_phi ist dabei das Modell, was die Daten am besten beschreibt.\n\n\n\n\n\n\n\n\n\n\n\nWo ist die mathematische Formel?\n\n\n\nJetzt haben wir zwar schön das Modell mit der Beta Regression geschätzt aber leider keine mathematische Formel erhalten. Das ist jetzt ja eigentlich auch eine andere Fragestellung. Daher nutzen wir für die Erstellung der mathematischen Formel auch nicht die Beta Regression sondern die nicht linear Regression. Prinzipiell ginge natürlich auch die lineare Regression, aber da wissen wir ja schon, dass die nicht so super funktioniert. Hier jetzt also der Weg um die Koeffizienten einer nicht lineare Regression zu bestimmen, die durch die Punkte eine Kurve legt.\nAuch hier haben wir die Wahl zwischen der Funktion nls() aus dem Standardpaket in R oder aber der Funktion mfp() aus dem gleichnamigen R Paket {mfp}. In den beiden Tabs zeige ich dir einmal die schnelle Anwendung. Wenn du mehr lesen willst dann kannst du nochmal in dem Kapitel zur nicht linearen Regression reinschauen. Wie immer führe ich hier den Code mehr aus als ihn dann zu erklären.\n\nMit nls()Mit mfp()\n\n\nDie Funktion nls() hat die Herausforderung, dass wir Startwerte für die Koeffizienten unserer mathematischen Formel übergeben müssen. Darüber hinaus müssen wir auch in der Funktion nls() eine Formel vordefinieren, die dann eben mit den Werten der Koeffizienten gefüllt wird. Wenn es dir auch eher schwer fällt in einer Punktewolke eine mathematische Funktion zu sehen, dann ist die Funktion nls() eine Herausforderung. Es gibt zwar den einen oder anderen Trick, aber am Ende müssen wir schauen, ob es dann passt mit dem Ergebnis.\nWir brauchen erstmal Startwerte für unsere mathematische Formel \\(y = a - x^b\\). Ein Trick ist, erstmal eine lineare Regression mit lm() zu rechnen und die Koeffizienten dann als Startwerte in nls() zu nutzen.\n\nlm(log(proportion) ~ snow_height, hunting_tbl)\n\n\nCall:\nlm(formula = log(proportion) ~ snow_height, data = hunting_tbl)\n\nCoefficients:\n(Intercept)  snow_height  \n     1.0324      -0.0354  \n\n\nMit unseren Startwerten können wir dann einmal schauen, ob unser Modell konvergiert. Dann haben wir auch die Werte für die Koeffizienten \\(a\\) und \\(b\\) und können das Modell als mathematische Formel aufschreiben.\n\nnls(proportion ~ a - I(snow_height^b), data = hunting_tbl, \n    start = c(a = exp(1.0324), b = -0.0354),\n    control = nls.control(maxiter = 1000))\n\nNonlinear regression model\n  model: proportion ~ a - I(snow_height^b)\n   data: hunting_tbl\n    a     b \n3.196 0.248 \n residual sum-of-squares: 0.5484\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 1.949e-07\n\n\nWir können auch nochmal eine \\(e\\)-Funktion mit \\(y = a - e^{b \\cdot x}\\) nutzen und schauen, ob wir damit etwas besser an die Daten näherkommen. Hier müssen wir dann wirklich ausprobieren, was teilweise echt nervig ist. Dafür schauen wir uns gleich nochmal die Funktion mfp() an.\n\nnls(proportion ~ a - exp(b * snow_height), data = hunting_tbl, \n    start = c(a = 1, b = 0),\n    control = nls.control(maxiter = 1000))\n\nNonlinear regression model\n  model: proportion ~ a - exp(b * snow_height)\n   data: hunting_tbl\n      a       b \n2.26755 0.01014 \n residual sum-of-squares: 0.3478\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.307e-07\n\n\nHat jeweils geklappt und dann können wir uns auch schon die Gleichung zusammenbauen. Die Frage ist natürlich, ob die Werte gut zu unseren Daten passen. Das werden wir dann gleich nochmal in der Abbildung 51.6 überprüfen. Hier dann einmal die erste Gleichung für das Polynom und dann die zweite Gleichung mit der \\(e\\)-Funktion.\n\\[\nproportion = 3.196 - snow\\_height^{0.248}\n\\]\n\\[\nproportion = 2.268 - e^{0.0101 \\cdot snow\\_height}\n\\]\nWelche dann die beste mathematische Formel ist sehen wir dann gleich. Bitte schaue dir aber noch den anderen Tab mit der Funktion mfp() an, denn mit der Funktion nls() ist es dann doch manchmal etwas Glücksspiel, ob man die richtige mathematische Formel mit den richtigen Startwerten trifft. Auch hier hilft vielleicht das R Paket {nls.multstart} welches versucht das Problem der Startwerte nochmal algorithmisch zu lösen. Sieht jetzt wilder aus als es ist, aber hier kriege ich dann noch ein \\(c\\) mit in der Formel unter.\n\nnls_multstart(proportion ~ a - c * I(snow_height^b), data = hunting_tbl, \n              lower = c(a = 0, b = 0, c = 0),\n              upper = c(a = Inf, b = Inf, c = Inf),\n              start_lower = c(a = 0, b = 0, c = 0),\n              start_upper = c(a = 500, b = 5, c = 10),\n              iter = 500, supp_errors = \"Y\")\n\nNonlinear regression model\n  model: proportion ~ a - c * I(snow_height^b)\n   data: data\n        a         c         b \n8.888e-01 7.189e-10 4.914e+00 \n residual sum-of-squares: 0.1778\n\nNumber of iterations to convergence: 88 \nAchieved convergence tolerance: 1.49e-08\n\n\nDann erhalten wir als Abschluss die folgende Formel. Du kannst hier wirklich sehr viel rumspielen und schauen, welche der expoentziellen Gleichungen am besten passt. Das ist dann wirklich immer ein rumprobieren.\n\\[\nproportion = 4.914 - 7.189\\cdot10^{-10} \\cdot snow\\_height^{0.888}\n\\]\n\n\nNachdem wir den langen Weg mit der Funktion nls() gegeangen sind, machen wir es jetzt etwas kürzer mit der Funktion mfp(). Wir müssen nur angeben welche Variable als Polynom modelliert werden soll. Den Rest macht dann die Funktion mfp() für uns. Wir erhalten dann auch die Formel für den Zusammenhang der folgenden Form. Wir erhalten dann die Werte für \\(\\beta_0\\), \\(\\beta_1\\) sowie dem Polynom \\(p\\) und der möglichen Transformation von \\(x\\).\n\\[\nproportion = \\beta_0 - \\beta_1 \\cdot \\left(snow\\_height\\right)^p\n\\]\nDann rechnen wir mal die Funktion und schauen welche Werte wir erhalten. Die Kunst ist hier die Werte für die Koeffizienten aus der Ausgabe abzulesen. Aber das mache ich dir hier ja einmal vor.\n\nmfp(proportion ~ fp(snow_height), data = hunting_tbl)\n\nCall:\nmfp(formula = proportion ~ fp(snow_height), data = hunting_tbl)\n\n\nDeviance table:\n         Resid. Dev\nNull model   1.705267\nLinear model     0.412282\nFinal model  0.2184446\n\nFractional polynomials:\n            df.initial select alpha df.final power1 power2\nsnow_height          4      1  0.05        2      3      .\n\n\nTransformations of covariates:\n                           formula\nsnow_height I((snow_height/100)^3)\n\nCoefficients:\n    Intercept  snow_height.1  \n       0.9741        -2.4216  \n\nDegrees of Freedom: 20 Total (i.e. Null);  19 Residual\nNull Deviance:      1.705 \nResidual Deviance: 0.2184   AIC: -30.29 \n\n\nDann können wir die Werte aus der Ausgabe auch schon in unsere Gleichung einsetzen. Der Vorteil ist wirklich, dass ich mir nicht überlegen muss, welche mathematische Formel ich nutzen will. Das schränkt mich zwar ein, macht mir das Leben aber auch einfacher. Wir erhalten dann eben eine Polynom, was auch einfach einzusetzen ist.\n\\[\nproportion = 0.9741 -2.4216 \\cdot \\left(\\cfrac{snow\\_height}{100}\\right)^3\n\\]\nDann schauen wir mal gleich, welche der drei mathematischen Formeln am besten zu unseren Daten in der Abbildung 51.6 passen.\n\n\n\nIm ersten Schritt bauen wir uns einmal in R die drei mathematischen Formeln einmal nach. Wir nutzen dafür die Funktion \\(x){...} und setzen die Zahlen der Koeffizienten ein. Hier musst du nur mit den Klammern aufpassen, sonst bauen sich die Formeln einfach.\n\nnls_poly_func &lt;- \\(x){3.196 - x^(0.248)}\nnls_exp_func &lt;- \\(x){2.268 - exp(0.0101 * x)}\nnls_multstart &lt;- \\(x){4.914 - 7.189e-10 * x^(0.888)}\nmfp_func &lt;- \\(x){0.9741 - 2.4216 * (x/100)^3}\n\nIn der Abbildung 51.6 siehst du einmal den Vergleich der drei nicht linearen Regressionen zu den Datenpunkten. Was will man sagen, dass Modell aus der Funktion mfp() bei weitem das beste Modell liefert um den Verlauf der Punkte durch eine Kurve zu beschreiben. Wenn es dir also nur darum ginge die Werte für die Jagderfolge nach der Schneehöhe vorherzusagen, dann sieht das mfp()-Modell sehr gut aus.\n\nhunting_tbl |&gt; \n  ggplot(aes(snow_height, proportion)) +\n  theme_minimal() +\n  geom_point() +\n  geom_function(fun = nls_poly_func, aes(color = \"nls_poly\")) +\n  geom_function(fun = nls_exp_func, aes(color = \"nls_exp\")) +\n  geom_function(fun = nls_exp_func, aes(color = \"nls_multstart\")) +\n  geom_function(fun = mfp_func, aes(color = \"mfp\")) +\n  scale_color_manual(name = \"Modell\", values = cb_pal[2:5])\n\n\n\n\n\n\n\nAbbildung 51.6— Zusammenhang zwischen dem Jagederfolg von Eisfüchsen und der Schneehöhe in den jeweiligen beobachteten Habitaten zusammen mit den drei nicht linearen Regressionen aus der Funktion nls() und mfp().",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Beta Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-beta.html#sec-mult-comp-beta-reg",
    "href": "stat-modeling-beta.html#sec-mult-comp-beta-reg",
    "title": "51  Beta Regression",
    "section": "51.5 Gruppenvergleich",
    "text": "51.5 Gruppenvergleich\nNeben der klassischen Regression in der wir dann eine Kurve durch die Punkte legen können wir die Beta Regression auch verwenden um einen Gruppenvergleich zu rechnen. Häufig sind wir in den Agrawissenschaften eher an dieser Frage interessiert. Wir machen das jetzt eigentlich wie immer mit dem R Paket {emmeans} und der Funktion Anova() aus dem R Paket {car}. Wenn du noch mehr zu den multiplen Vergleichen lesen willst, dann kannst du nochmal in das Kapitel zu den multiplen Vergleichen und dem Posthoc Test reinschauen. Wir konzentrieren uns hier auf die wichtigsten Aspekte und gehen einmal die Sachlage für die Daten zu den untergewichtigen Brokkoli sowie unsren Pilzen auf dem Weizen durch. Dabei habe ich dann die Daten für die Pilze etwas reduziert, weil es einfach zu viele Faktoren sind.\n\n51.5.1 Untergewichtige Brokkoli\nBeginnen wir also mit den Daten unseres untergewichtigen Brokkoli. Wir haben als Outcome proprotion den Anteil an 500g, also die prozentuale Untergewichtigkeit eines jedes Brokkolikopfes. Dabei habe ich dann alle Brokkoli mit einem Anteil von Eins oder größer entfernt. Wir wollen hier explizit auf die Köpfe schauen, die das Zielgewicht nicht erreicht haben. Damit steht das Modell aus schon fast. Wir haben drei Faktoren, einmal die Zeit der Düngung fert_time sowie dann die Menge der Düngung fert_amount. Dann nehme ich den Block noch mit rein, denn ich will einmal sehen wie stark der Blockeffekt in der ANOVA ist. Damit ich das so auch kann, muss einmal der Block mit ins Modell. Dann haben wir natürlich bei der beta Regression die Frage, ob wir dann die Varianz nochmal direkt mit modellieren sollen. Das würden wir ja mit einem | durchführen. Ich rechne hier jetzt mal drei Modelle, einmal ein Modell ohne explizite Varianz, dann einmal mit der Berücksichtigung der Varianz der Blöcke und zum Schluss nochmal ein Modell mit der Berücksichtigung aller drei Faktoren als Varianzquelle.\n\nbroc_1_fit &lt;- betareg(proportion ~ fert_time + fert_amount + fert_time:fert_amount + block, \n                    data = broc_tbl)\n\nbroc_2_fit &lt;- betareg(proportion ~ fert_time + fert_amount + fert_time:fert_amount + block | block, \n                    data = broc_tbl)\n\nbroc_3_fit &lt;- betareg(proportion ~ fert_time + fert_amount + fert_time:fert_amount + block | \n                      fert_time + fert_amount + block, \n                    data = broc_tbl)\n\nIch nutze jetzt wieder das AIC um zu entscheiden welches Modell das beste Modell ist. Dabei gilt, dass ein niedrigeres AIC immer besser ist. Im zweiten Schritt sind die absoluten Zahlen irrelevant, mich interessiert nur die Differenz der AIC-Werte. Dabei ist ein Modell mit einem Unterschied von \\(2\\) oder größer vorzuziehen.\n\nAIC_vec &lt;- c(\"Modell 1\"= AIC(broc_1_fit), \n             \"Modell 2\" = AIC(broc_2_fit), \n             \"Modell 3\" = AIC(broc_3_fit))\nouter(AIC_vec, AIC_vec, \"-\") |&gt; round(2)\n\n         Modell 1 Modell 2 Modell 3\nModell 1     0.00     3.22    -0.17\nModell 2    -3.22     0.00    -3.40\nModell 3     0.17     3.40     0.00\n\n\nWir sehen, dass das Modell 2 broc_2_fit besser ist als das Modell 1. Daher bevorzugen wir das Modell 2. Dann kriegen wir aber keine Verbesserung mehr hin und deshalb bleiben wir bei dem zweiten Modell. Jetzt können wir dann einmal die ANOVA rechnen und schauen, welche signifikante Effekte wir in den Daten haben.\n\nbroc_2_fit |&gt; \n  Anova() |&gt; \n  model_parameters()\n\nParameter             |  Chi2 | df |      p\n-------------------------------------------\nfert_time             |  0.31 |  1 | 0.576 \nfert_amount           |  5.23 |  2 | 0.073 \nblock                 | 35.81 |  3 | &lt; .001\nfert_time:fert_amount |  3.86 |  2 | 0.145 \n\nAnova Table (Type 2 tests)\n\n\nÄrgerlicherweise sind unsere beiden Behandlungsfaktoren gar nicht signifikant sondern unsere Blöcke. Dann werden wir uns wohl die Blöcke einmal anschauen müssen. Ich entscheide mich mal über den Zeitpunkt der Düngegabe zu mitteln, den der Effekt des Zeitpunktes ist wirklich nicht signifikant.\n\nemm_obj &lt;- broc_2_fit |&gt; \n  emmeans(~ fert_amount * block) \n\nMit dem emm_obj können wir dann uns das compact letter display berechnen lassen. Wir haben hier als Effekt die mittleren Prozente in der Spalte emmean. Also wie viel Prozent der Brokkoliköpfe untergewichtig sind. Ich adjustiere hier dann noch die \\(p\\)-Werte nach Bonferroni.\n\nemm_cld &lt;- emm_obj |&gt; \n  cld(Letters = letters, adjust = \"bonferroni\")\n\nIn der Abbildung 51.7 siehst du die Barplots der mittleren Prozente des errichten Zielgewichtes der Brokkoliköpfe für die Düngemenge und die vier Blöcke. Wie du sehen kannst unterscheiden sich die Blöcke stärker untereinander als die Level der Düngemenge. Wie viel Prozent an untergewichtigen Brokkoli du also anteilig erhälst liegt mehr an dem Standort des Blocks als an der Behandlung mit Dünger. Irgendwie dann auch ein spannendes Ergebnis. Ich nutze hier den Standardfehler als Fehlerbalken, da die Standardabweichung eventuell über die Grenze von Eins steigen würde, was bei Prozenten und Anteilen wenig Sinn ergibt.\n\nemm_cld |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = fert_amount, y = emmean, fill = block)) +\n  theme_minimal() + \n  labs(y = \"[%] erreichtes Zielgewicht\", x = \"Düngemenge [mg/l]\",\n       fill = \"Block\") +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +\n  geom_bar(stat = \"identity\", width = 0.7,\n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_text(aes(label = str_trim(.group), y = emmean + SE + 0.01),  \n            position = position_dodge(width = 0.9), vjust = -0.25) +\n  geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  theme(legend.position = \"top\") +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 51.7— Säulendigramm der mittleren Prozente der untergewichtigen Brokkoli aus einer Beta Regression. Das betareg()-Modell berechnet das mittleren Prozente des Brokkoli in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre.\n\n\n\n\n\n\n\n51.5.2 Pilze auf Weizen\nUnser zweites Beispiel ist wirklich schwierig auszuwerten, da wir so viele Faktoren vorliegen haben. Zum einen haben wir als Faktor der Weizenlinien zehn genetische Linien vorliegen. Dann schauen wir uns auch noch zwanzig Pilzarten an. Das funktioniert dann hier als Demonstration nicht mehr. In der Realität würdest du dir dann auch Linien oder Pilzarten raussuchen, die sich interessieren und nicht wild hier Abbildungen produzieren. Wenn du alle Faktoren mit in die Modelle nimmst, siehst du wirklich gar nichts mehr. Vertraue mir, ich habe das compact letter display bis zum Buchstaben \\(z\\) mit diesem Beispiel ausgereizt.\n\nfungi_tbl &lt;- fungi_tbl |&gt; \n  filter(bunt %in% c(\"B1\", \"B12\", \"B157\", \"B51\", \"B189\", \"B4\")) |&gt; \n  filter(gen %in% c(\"Oro\", \"Albit\", \"Turkey\", \"Hybrid128\")) \n\nMit unserem reduzierten Datensatz können wir dann einmal unsere Beta Regression rechnen. Ich nehme hier mal keinen gesonderten Varianzterm an, einfach um das Beispiel etwas kürzer zu halten.\n\nfungi_fit &lt;- betareg(percent ~ gen + bunt + gen:bunt, data = fungi_tbl)\n\nDann können wir auch schon die ANOVA rechnen. Hier hilft die ANOVA wirklich mal einen schnellen Überblick über die Daten zu erhalten.\n\nfungi_fit |&gt; \n  Anova() |&gt; \n  model_parameters()\n\nParameter |   Chi2 | df |      p\n--------------------------------\ngen       | 416.57 |  3 | &lt; .001\nbunt      | 155.99 |  5 | &lt; .001\ngen:bunt  | 284.37 | 15 | &lt; .001\n\nAnova Table (Type 2 tests)\n\n\nWir sehen, dass wir auf jeden Fall einen Effekt der Weizenlinie wie auch der Pilzart haben. Leider haben wir auch eine signifikante Interaktion, so dass ich die Daten dann für die Faktoren getrennt auswerten werde. Wir nutzen dann das Objekt auch gleich um das compact letter display zu erstellen. Mehr geht dann natürlich immer, aber da schaue doch mal in das Kapitel zu den multiplen Vergleichen und dem Posthoc Test rein.\n\nemm_obj &lt;- fungi_fit |&gt; \n  emmeans(~ gen | bunt) \n\nIm Folgenden einmal der Code für das compact letter display. Ich zeige auch hier die Ausgabe nur einmal als tibble damit die Ausgabe nicht so lang wird. Wir sehen dann gleich mal den mittleren Pilzbefall pro Linie und Pilzart. Es sind wirklich viele Ergebnisse für die Vergleiche, da wir wirklich viele Level der Faktorkombination haben. Irgendwann ist dann auch das compact letter display am Ende.\n\nemm_cld &lt;- emm_obj |&gt; \n  cld(Letters = letters) |&gt; \n  as_tibble() \nemm_cld\n\n# A tibble: 24 × 8\n   gen       bunt   emmean      SE    df asymp.LCL asymp.UCL .group\n   &lt;fct&gt;     &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n 1 Albit     B1    0.00731 0.00499   Inf -0.00247     0.0171 \" a  \"\n 2 Oro       B1    0.0109  0.00708   Inf -0.00293     0.0248 \" a  \"\n 3 Turkey    B1    0.0835  0.0272    Inf  0.0302      0.137  \"  b \"\n 4 Hybrid128 B1    0.832   0.0378    Inf  0.758       0.907  \"   c\"\n 5 Oro       B51   0.0186  0.0107    Inf -0.00246     0.0396 \" a  \"\n 6 Albit     B51   0.0250  0.0133    Inf -0.000979    0.0511 \" a  \"\n 7 Turkey    B51   0.254   0.0445    Inf  0.167       0.341  \"  b \"\n 8 Hybrid128 B51   0.941   0.0226    Inf  0.896       0.985  \"   c\"\n 9 Oro       B4    0.0418  0.0185    Inf  0.00565     0.0780 \" a \" \n10 Albit     B4    0.0906  0.0283    Inf  0.0350      0.146  \" a \" \n# ℹ 14 more rows\n\n\nIn der Abbildung 51.8 siehst du einmal das sehr heterogene Ergebnis. Wir haben teilweise Weizenarten mit einem sehr starken Pilzbefall und teilweise dann mit gar keinem Pilzbefall. Das Ganze ist dann auch immer abhängig von der Art des Pilzes und dann der befallenen Linie. Wir haben hier eben eine klassische Interaktion vorliegen. Hier habe ich mich dann auch für den Standardfehler für die Fehlerbalken entschieden, da die Standardabweichung eventuell über die Eins hinausgeht.\n\nemm_cld |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = bunt, y = emmean, fill = gen)) +\n  theme_minimal() + \n  labs(y = \"[%] infiziert\", x = \"Art des Pilzes\",\n       fill = \"Genetische Linie\") +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1)) +\n  geom_bar(stat = \"identity\", width = 0.7,\n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_text(aes(label = str_trim(.group), y = emmean + SE + 0.01),  \n            position = position_dodge(width = 0.9), vjust = -0.25) +\n  geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  theme(legend.position = \"top\") +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 51.8— Säulendigramm des mittleren Pilzbefalls von Weizen durch verschiedene Pilzarten. Das betareg()-Modell berechnet den mittleren Pilzbefall in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Beta Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-beta.html#referenzen",
    "href": "stat-modeling-beta.html#referenzen",
    "title": "51  Beta Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 51.1— Zusammenhang zwischen dem Jagederfolg von Eisfüchsen und der Schneehöhe in den jeweiligen beobachteten Habitaten.\nAbbildung 51.2— Zusammenhang zwischen erreichten Zielgewicht von \\(500g\\) bei Brokkoli [%] und der Düngermenge sowiw dem Düngezeitpunkt.\nAbbildung 51.3— Sortierte Heatmap der genetischen Linien und der Art des Pilzes. Farbig dargestellt sind die Anteile an infizierten Weizen in [%]. Nach dem Grad der Infektion wurden die Faktoren sortiert um eine bessere Übersicht zu erhalten.\nAbbildung 51.4 (a)— Residualplot I\nAbbildung 51.4 (b)— Cooks distance\nAbbildung 51.4 (c)— Generalized leverage\nAbbildung 51.4 (d)— Residualplot II\nAbbildung 51.5— Zusammenhang zwischen dem Jagederfolg von Eisfüchsen und der Schneehöhe in den jeweiligen beobachteten Habitaten zusammen mit den vier vorgestellten Modellen. Das Modell betareg_phi ist dabei das Modell, was die Daten am besten beschreibt.\nAbbildung 51.6— Zusammenhang zwischen dem Jagederfolg von Eisfüchsen und der Schneehöhe in den jeweiligen beobachteten Habitaten zusammen mit den drei nicht linearen Regressionen aus der Funktion nls() und mfp().\nAbbildung 51.7— Säulendigramm der mittleren Prozente der untergewichtigen Brokkoli aus einer Beta Regression. Das betareg()-Modell berechnet das mittleren Prozente des Brokkoli in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre.\nAbbildung 51.8— Säulendigramm des mittleren Pilzbefalls von Weizen durch verschiedene Pilzarten. Das betareg()-Modell berechnet den mittleren Pilzbefall in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre.\n\n\n\nBurnham KP, Anderson DR. 2004. Multimodel inference: understanding AIC and BIC in model selection. Sociological methods & research 33: 261–304.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Beta Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-mixed.html",
    "href": "stat-modeling-mixed.html",
    "title": "52  Lineare gemischte Modelle",
    "section": "",
    "text": "52.1 Genutzte R Pakete\nNormalerweise nutze ich nur R Pakete, die auch auf CRAN oder eben per p_load() zu installieren sind. In diesem Kapitel brauche ich aber noch ein extra Paket, da die Ausgaben von linearen gemischten Modellen sehr unordentlich sind. Das R Paket {mixedup} hilft mir hier. Deshalb installiere ich einmal wie folgt {mixedup}.\nremotes::install_github('m-clark/mixedup')\nWir wollen folgende R Pakete ganz normal in diesem Kapitel nutzen. Es sind eine Menge geworden, aber das zeigt auch mal wieder, dass gemischte Modelle nicht unbedingt das einfachtse Modell sind.\npacman::p_load(tidyverse, magrittr, broom, see, simstudy,\n               multcomp, emmeans, lme4, broom.mixed, readxl,\n               parameters, ggridges, scales, performance, \n               ggdist, gghalves, glmmTMB, lmerTest, mixedup,\n               multilevelmod, agridat, desplot, modelsummary,\n               ggbeeswarm, ordinal, janitor, RVAideMemoire, \n               conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(lme4::lmer)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\ntheme_set(theme_minimal(base_size = 12))\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Lineare gemischte Modelle</span>"
    ]
  },
  {
    "objectID": "stat-modeling-mixed.html#daten",
    "href": "stat-modeling-mixed.html#daten",
    "title": "52  Lineare gemischte Modelle",
    "section": "52.2 Daten",
    "text": "52.2 Daten\nAls erstes Beispiel nehmen wir einen Datensatz zu den Testergebnissen von Schülern an amerikanischen Schulen. Jetzt ist das kein Beispiel, welches du vielleicht in einem biologischen oder agrarwissenschaftlichen Umfeld erwarten würdest. Ich mache das aber hier bewusst, da wir uns alle sehr gut die Abhängigkeiten von Schülerleistungen von der jeweiligen Klasse und dem Standort der Schule vorstellen können. Jedem wird klar sein, dass ein Testergebnis aus einer Klausur nicht unabhängig davon ist, auf welche Schule der Schüler geht oder in welcher Klasse er unterrichtet wird. Schüler in einer gemeinsamen Klasse oder Schule werden sich ähnlicher sein als Schüler in unterschiedlichen Klassen oder Schulen.\nIn der Abbildung 52.2 siehst du einmal das Abhängigkeitsverhältnis in unserem Schuldatenbeispiel. Wir wenden in den verschiedenen Klassen als Behandlung trt eines von drei Lehrmethoden Frontal, Flipped Classroom oder HyperFlex an. Dabei wird natürlich eine ganze Klasse nach der entsprechenden Lehrmethode unterrichtet. Pro Schule finden sich drei Klassen und eine Klasse ist dann in einer der neun Schulen genestet.\n\n\n\n\n\n\nflowchart LR\n    C(trt):::fixed --- D(((nested))) --&gt; E(class):::random --- F(((nested))) --&gt; G(school):::random\n    classDef fixed fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef random fill:#E69F00,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 52.2— In unseren Schuldaten haben wir verschiedene Schulen school und Klassen class mit zwei innovativen Lehrmethoden unterrichtet. Eine Kontrollgruppe soll die Ergebnisse eines Leistungstests absichern. Daher sind die Lehrmethoden trt in dem Faktor class genestet. Der Faktor class ist dann wiederum in jedem Faktor school genestet.\n\n\n\n\n\nIn dem folgenden Kasten werden einmal die Schuldaten simuliert. Daher können wir dann einmal nachvollziehen, welche Werte wir jeweils für die Effekte der Schule, der Klasse und der Lehrform gesetzt haben. Wir sehen dann auch mal, welche zufälligen Effekte wir eigentlich setzen müssen und wie wir dann die Modelle miteinander vergleichen. Du kannst den Kasten gerne überspringen und dann einfach mit der Visualisierung und Auswertung der Daten weitermachen.\n\n\n\n\n\n\nGenerierung von Schuldaten (3-faktoriell)\n\n\n\n\n\nWarum sollte man Daten simulieren? Reichen da nicht echte Daten? Wir können an den simulierten Daten die Werte zurückverfolgen, wir wir bei der Erstellung voreingestellt haben. Damit können wir dann auch bewerten, wie gut die statistischen Methoden funktioniert haben. Wir machen es uns aber auch etwas einfacher und bauen uns kein kompliziertes Beispiel. Umfangreich ist es nur, da Daten für ein gemischtes Modell eben auch umfangreich sind.\nAus Gründen der Einfachheit haben wir immer ein balanciertes Design vorliegen. Wir haben also immer in allen Faktorkombinationen die gleiche Anzahl an Beobachtungen n_reps vorliegen. In der Anwendung mag es Unterschiede geben, so hat eine Sau sicherlich nicht immer exakt zwölf Ferkel, aber in unseren Beispielen macht es keinen Unterschied. Balanciert oder unbalanciert ist bei gemischten Modellen eher nachrangig wichtig. Das R Paket {simstudy} erlaubt die Simulation von komplexeren Gruppenstrukturen mit auch unbalancierten Daten. Am Ende wäre es dann mit {simstudy} vermutlich einfacher gewesen… hier können wir dann auch unterschiedlich Klassengrößen und Anzahlen simulieren.\nIm Folgenden setze ich einmal Werte für die Schulanzahl, Klassenzahl pro Schule sowie die Anzahl an Behandlungen. Dann müssen wir noch definieren wie viele Schüler dann pro Klasse zu finden sind. Wenn wir das haben, dann können wir auch die Effekte der Klassen, Schulen und der Lehrformate festlegen. Dabei sind die Effekt der zufälligen Effekte der Klassen und Schule dann die zusätzliche Varianz abgebildet durch die Standardabweichungen.\n\npacman::p_load(spatstat.random)\n# set seed\nset.seed(20231208)\n# sample sizes\nn_school &lt;- 9\nn_class_per_school &lt;- 3\nn_class &lt;- n_school * n_class_per_school\nn_trt &lt;- 3\nn_reps &lt;- 20\n# effects and standard deviation\nsd_school &lt;- 10\nsd_class &lt;- 5\nsd_error &lt;- 2\neff_trt &lt;- c(frontal = 10,\n             flipped = -10,\n             hyflex = 30)\n\nDann können wir uns schon das Grid für die Daten erstellen. Dabei müssen wir dann mehrfach expand_grid() nutzen um erst die Schulen zu erschaffen, dann die Lehrformate den Schulen zuordnen und dann die Klassen pro Schule erschaffen. Ende müssen wir noch den Datensatz mit der Anzahl an Schülern pro Klasse erweitern. Dann beschreibt jede Zeile genau einen Schüler. Neben der Zuordnung jedes einzelnen Schülern zu einem Lehrformat, Klasse und Schule, müssen wir noch die Effekte \\(s_0\\), \\(c_0\\) und \\(t_{eff}\\), die jeder Schüler durch eben jene Zuordnung erhält, ergänzen.\n\nschool_grid_tbl &lt;- tibble(s_id = 1:n_school,\n                          s_0 = rnorm(n_school, 0, sd_school)) |&gt; \n  add_column(trt = rep(1:n_trt, n_trt),\n             t_eff = rep(eff_trt, n_trt)) |&gt; \n  expand_grid(c_per_s = 1:n_class_per_school) |&gt;\n  mutate(c_id = 1:n_class,\n         c_0 = rnorm(n_class, 0, sd_class)) |&gt; \n  expand_grid(reps = 1:n_reps)\n\nJetzt können wir unseren Testscore berechnen, der sich aus den einzelnen Effekten der Schule \\(s_0\\), der Klasse \\(c_0\\) sowie dem Lehrformat \\(t_{eff}\\) ergibt, berechnen. Am Ende addieren wir auf jeden Wert noch einen Fehler und runden die Werte des Tests auf zwei Stellen. Dann bauen wir uns noch die Faktorlevel für die Schulen, Klassen und dem Lehrformat.\n\nschool_tbl &lt;- school_grid_tbl |&gt; \n  arrange(trt) |&gt; \n  mutate(test = round(50 + s_0 + c_0 + t_eff + rnorm(n(), 0, sd_error), 2),\n         s_id = factor(s_id, labels = c(\"Springfield School\", \"Jacksonville High\", \"Franklin Country\", \n                                        \"Clinton Christian\", \"Arlington Academy\", \"Georgetown High\", \n                                        \"Greenville School\", \"Bristol Country\", \"Dover Tech Center\")),\n         c_id = as_factor(c_id),\n         c_per_s = factor(c_per_s, labels = c(\"1a\", \"1b\", \"1c\")),\n         trt = factor(trt, labels = c(\"Frontal\", \"Flipped classroom\", \"HyFlex\"))) \n\nDann schreiben wir die Daten noch in eine Exceldatei school_testing.xlsx und können diese dann im weiteren Verlauf der Analyse nutzen. Auch hier passen wir etwas die Namen der Spalten an, damit die Spalten etwas mehr Aussagekraft haben.\n\nschool_tbl |&gt; \n  select(school_id = s_id, class_in_school_id = c_per_s, class_id = c_id, trt, test) |&gt; \n  write_xlsx(\"data/school_testing.xlsx\")\n\n\n\n\nDie Schuldaten liegen dann in dem Datensatz school_testing.xlsx vor. Wir müssen hier dann nur noch die Faktoren bilden, damit wir dann auch die Visualisierungen sauber hinkriegen.\n\nschool_tbl &lt;- read_excel(\"data/school_testing.xlsx\") |&gt; \n  mutate(school_id = as_factor(school_id),\n         class_in_school_id = as_factor(class_in_school_id),\n         class_id = as_factor(class_id),\n         trt = as_factor(trt)) \n\nEs ergibt sich dann der Datensatz der Schuldaten wie in Tabelle 52.1 gekürzt gezeigt.\n\n\n\n\nTabelle 52.1— Datensatz der Testscores für die Schüler an verschiedenen Schulen und Klassen. Die Schüler wurden in den Klassen jewiels mit einem von drei Lehrformaten unterrichtet. Die Klassen und Schulen sind die zufälligen Effekte. Das Lehrformat ist der feste Effekt.\n\n\n\n\n\n\n\n\n\n\n\n\n\nschool_id\nclass_in_school_id\nclass_id\ntrt\ntest\n\n\n\n\nSpringfield School\n1a\n1\nFrontal\n59.75\n\n\nSpringfield School\n1a\n1\nFrontal\n60.29\n\n\nSpringfield School\n1a\n1\nFrontal\n63.43\n\n\nSpringfield School\n1a\n1\nFrontal\n65.36\n\n\n…\n…\n…\n…\n…\n\n\nDover Tech Center\n1c\n27\nHyFlex\n107.29\n\n\nDover Tech Center\n1c\n27\nHyFlex\n108.62\n\n\nDover Tech Center\n1c\n27\nHyFlex\n107\n\n\nDover Tech Center\n1c\n27\nHyFlex\n108.52\n\n\n\n\n\n\n\n\nIn der Tabelle 52.2 im folgenden Kasten findest du den einfachst möglichen Datensatz für nur zwei Schülern pro Klasse sowie insgesamt nur zwei Klassen für zwei Schulen. Damit kannst du dir einmal denn Aufbau visualisieren und siehst auch einmal wie sich die Effekte der Klassen, Schule und Lehrformat für jeden der sechzehn Schüler zusammensetzt. Jede Zeile repräsentiert ja einen Schüler.\n\n\n\n\n\n\nEinfachst möglicher Schuldatensatz (3-faktoriell)\n\n\n\n\n\n\n\n\nTabelle 52.2— Kurzform eines dreifaktoriellen Datensatzes mit zwei zufälligen Effekten für school und class sowie einem Bahandlungsfaktor trt. Die zufälligen Effekte sind normalverteilt mit \\(\\mathcal{N}(0, s^2)\\). Pro Behandlung haben wir dann nur zwei Wiederholungen. Dennoch erreichen wir eine Fallzahl von sechzehn Beobachtungen daher Schülern.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nschool\n\\(\\boldsymbol{eff_{school}}\\)\nclass\n\\(\\boldsymbol{eff_{class}}\\)\ntrt\n\\(\\boldsymbol{eff_{trt}}\\)\nreps\n\n\n\n\n1\n\\(0.23\\)\n1\n\\(-0.14\\)\n1\n\\(10\\)\n1\n\n\n1\n\\(0.23\\)\n1\n\\(-0.14\\)\n1\n\\(10\\)\n2\n\n\n1\n\\(0.23\\)\n1\n\\(-0.14\\)\n2\n\\(5\\)\n1\n\n\n1\n\\(0.23\\)\n1\n\\(-0.14\\)\n2\n\\(5\\)\n2\n\n\n1\n\\(0.23\\)\n2\n\\(0.21\\)\n1\n\\(10\\)\n1\n\n\n1\n\\(0.23\\)\n2\n\\(0.21\\)\n1\n\\(10\\)\n2\n\n\n1\n\\(0.23\\)\n2\n\\(0.21\\)\n2\n\\(5\\)\n1\n\n\n1\n\\(0.23\\)\n2\n\\(0.21\\)\n2\n\\(5\\)\n2\n\n\n2\n\\(0.71\\)\n3\n\\(-0.83\\)\n1\n\\(10\\)\n1\n\n\n2\n\\(0.71\\)\n3\n\\(-0.83\\)\n1\n\\(10\\)\n2\n\n\n2\n\\(0.71\\)\n3\n\\(-0.83\\)\n2\n\\(5\\)\n1\n\n\n2\n\\(0.71\\)\n3\n\\(-0.83\\)\n2\n\\(5\\)\n2\n\n\n2\n\\(0.71\\)\n4\n\\(0.59\\)\n1\n\\(10\\)\n1\n\n\n2\n\\(0.71\\)\n4\n\\(0.59\\)\n1\n\\(10\\)\n2\n\n\n2\n\\(0.71\\)\n4\n\\(0.59\\)\n2\n\\(5\\)\n1\n\n\n2\n\\(0.71\\)\n4\n\\(0.59\\)\n2\n\\(5\\)\n2\n\n\n\n\n\n\n\n\n\nDann einmal den Datenklassiker yates.oats schlechthin als das Split-plot experiment of oats aus dem R Paket {agridat}. Warum ist es der Klassiker? Weil es im Prinzip das erste Split plot Experiment war. Deshalb ist es nicht schlechter als andere. Ich nutze es hier, weil es gut funktioniert und wir uns einmal eine Auswertung eines komplexeren Datensatzes mit einem linearen gemischten Modell anschauen können. Wir haben insgesamt die mittleren Ertragswerte von Hafer für 72 Parzellen vorliegen. Im weiteren haben wir zwei Behandlungsfaktoren mit der Stickstoffgabe nitro und der Sorte gen. Da wir ein Split plot Experiment vorliegen haben, brauchen wir natürlich die Reihen- und Spaltenpositionen sowie die Information über den Block. Alle drei Positionsfaktoren werden wir dann versuchen als zufällige Effekte in das gemischte Modell zu nehmen. In der Abbildung 52.3 siehst du einmal das Abhängigkeitsverhältnis in den Daten.\n\n\n\n\n\n\nflowchart LR\n    A(nitro):::fixed --- B(((nestet))) --&gt; C(gen):::fixed --- D(((nestet))) --&gt; E(cols/rows) --- F(block):::random\n    classDef fixed fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef random fill:#E69F00,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 52.3— Abhängigkeitsstruktur des split plot design. Der Faktor gen ist in den Spalten cols/rows der Blöcke randomisiert und der zweite Faktor nitro innerhalb des anderen Faktors.\n\n\n\n\n\nIch erweitere noch den Datensatz um die einzelnen Pflanzenwerte indem ich für jeden yield-Wert als Mittelwert noch zwölf Pflanzen für die Parzelle simuliere. Damit baue ich die Daten sozusagen wieder zurück und komme auf meine individuellen Werte für jede der 72 Parzellen.\n\ndata(yates.oats)\noats_tbl &lt;- yates.oats |&gt; \n  as_tibble() |&gt; \n  mutate(nitro = as_factor(nitro),\n         row = as_factor(row),\n         col = as_factor(col)) |&gt; \n  expand_grid(plant_id = 1:12) |&gt; \n  mutate(plant_yield = round(rnorm(n(), yield, 2), 2)) |&gt; \n  select(row, col, block, nitro, gen, plant_id, plant_yield)\n\nIn der Tabelle 52.3 siehst du nochmal einen Ausschnitt aus den Daten. Wir fokussieren uns hier auf das Outcome yield was wir als normalverteilt annehmen. Die anderen möglichen Outcomes ignorieren wir dann erstmal. Wir brauchen dann auch die Informationen für die Position auf dem Feld row und col um dann einen gute Abbildung des Designs über das R Paket {desplot} zu erstellen.\n\n\n\n\nTabelle 52.3— Haferdatensatz im Split plot Design für zwei Behandlungsfaktoren nitro und gen sowie drei Positionsfaktoren row, col und block. Wir schauen uns hier nur das Outcome Haferertrag yield an.\n\n\n\n\n\n\nrow\ncol\nblock\nnitro\ngen\nplant_id\nplant_yield\n\n\n\n\n16\n3\nB1\n0\nGoldenRain\n1\n81.69\n\n\n16\n3\nB1\n0\nGoldenRain\n2\n78.75\n\n\n16\n3\nB1\n0\nGoldenRain\n3\n81.68\n\n\n16\n3\nB1\n0\nGoldenRain\n4\n75.71\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n2\n2\nB6\n0.6\nVictory\n9\n100.22\n\n\n2\n2\nB6\n0.6\nVictory\n10\n101.27\n\n\n2\n2\nB6\n0.6\nVictory\n11\n101.9\n\n\n2\n2\nB6\n0.6\nVictory\n12\n101.41\n\n\n\n\n\n\n\n\nNeben einem normalverteilten Outcome wollen wir uns danna auch noch eine andere häufige Art von einem Outcome anschauen. Wir betrachten nämlich noch Zähldaten oder Abundanz von Arten. Wir nutzen hier auch einen Datensatz aus dem R Paket {agridat} und zwar den Datensatz zu Wireworms controlled by fumigants in a latin square. Es geht hier also um die Verwendung von fünf Insektiziden in einem Feld mit \\(5 \\times 5\\) großen Parzellen. In jedem der Parzellen haben wir dann die Würmer an zehn Punkten gezählt. Die zehn Zählpunkte habe ich mir ausgedacht, aber dann aber wir später ein paar mehr Beobachtungen zum darstellen. Wie du siehst, haben wir hier ein latin square design vorliegen, welches ich dir nochmal in der Abbildung 52.4 dargestellt habe.\n\n\n\n\n\n\nflowchart LR\n    A(trt):::fixed --- B(((nested))) --&gt; C(rows):::random\n    B(((nested))) --&gt; D(cols):::random\n    C --- F(block)\n    D --- F\n    classDef fixed fill:#56B4E9,stroke:#333,stroke-width:0.75px\n    classDef random fill:#E69F00,stroke:#333,stroke-width:0.75px\n\n\n\n\nAbbildung 52.4— Schematische Darstellung der Abhängigkeitsstruktur im latin suare design für unsere Wurmdaten. Die Behandlungen werden in rows und cols genestet, die einem quadratischen Block mit den Längen der Anzahl der Level der Behandlungen entsprechen.\n\n\n\n\n\nIm Folgenden habe ich einmal die Daten geladen und die Mittelwerte der Parzellen worms wieder auf die ursprünglichen, ausgedachten zehn Zählpunkte erweitert. Auch hier müssen wir dann unsere Daten wieder entsprechend mit Faktoren versehen, damit wir die Daten dann richtig im R Paket {desplot} abbilden können.\n\ndata(cochran.wireworms)\nwireworms_tbl &lt;- cochran.wireworms |&gt; \n  as_tibble() |&gt; \n  mutate(trt = as_factor(trt),\n         col = as_factor(col),\n         row = as_factor(row)) |&gt; \n  expand_grid(site_id = 1:10) |&gt; \n  mutate(count_worms = rpois(n(), worms))\n\nDu erhälst dann folgenden Auszug in der Tabelle 52.4 von den Wurmdaten. Hier sind dann die Namen der Behandlungen etwas kurz, aber wir belassen es mal bei den Namen. Du kannst dir hier eben fünf Insektizide vorstellen, die wir dann miteinander vergleichen würden. Zu den Gruppenvergleichen findest du dann ganz am Ende des Kapitels nochmal einen eignene Abschnitt sowie dann auch zwei Anwendungsbeispiele.\n\n\n\n\nTabelle 52.4— Auszug aus den Wurmdaten in einem latin square design für fünf verschiedene Insektizide.\n\n\n\n\n\n\nrow\ncol\ntrt\nworms\nsite_id\ncount_worms\n\n\n\n\n1\n1\nP\n3\n1\n4\n\n\n1\n1\nP\n3\n2\n3\n\n\n1\n1\nP\n3\n3\n8\n\n\n1\n1\nP\n3\n4\n4\n\n\n…\n…\n…\n…\n…\n…\n\n\n5\n5\nO\n8\n7\n14\n\n\n5\n5\nO\n8\n8\n10\n\n\n5\n5\nO\n8\n9\n11\n\n\n5\n5\nO\n8\n10\n9\n\n\n\n\n\n\n\n\nIn der folgenden Box findest du noch mehr Daten und experimentelle Designs aus dem R Paket {agridat}. Dort findest du dann noch mehr Inspirationen wie Daten aussehen könnten, die mit einem linearen gemischten Modell ausgewertet werden. Nicht alle der dortigen Daten können nur mit einem gemischten Modell ausgewertet werden, es gibt auch eine Reihe an einfacheren Datensätzen. Ich habe hier jetzt zwei der über hundert Datensätze ausgewählt, die ich relativ repräsentativ finde.\n\n\n\n\n\n\nWeitere Daten zu gemischten Modellen\n\n\n\nAlle Daten hier stammen aus dem R Paket {agridat} und lassen sich somit mit der Funktion data() laden. Die Daten liegen meistens nicht als tibble() vor, so dass manchmal noch etwas Datenaufbereitung notwendig ist.\n\nMating crosses of chickens\nLatin square of four breeds of sheep with four diets\nBirth weight of lambs from different lines/sires\nWeight gain calves in a feedlot\nAverage daily gain of 65 steers for 3 lines, 9 sires.\nMulti-environment trial of oats in United States, 5 locations, 7 years.\n\nEs gibt natürlich noch mehr Datensätze, die du dann mit einem gemischten Modell auswerten kannst, aber das ist hier einmal eine Auswahl an möglichen Datensätzen zum üben.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Lineare gemischte Modelle</span>"
    ]
  },
  {
    "objectID": "stat-modeling-mixed.html#visualisierung",
    "href": "stat-modeling-mixed.html#visualisierung",
    "title": "52  Lineare gemischte Modelle",
    "section": "52.3 Visualisierung",
    "text": "52.3 Visualisierung\nDer wichtigste Teil in einer Analyse ist die Visualisierung der Zusammenhänge. Das ist noch wahrer bei ser komplexen Modellen wie es die linearen gemischten Modelle sind. Wir müssen erstmal verstehen welche Gruppenstrukturen wir in den Daten haben und welchen Einfluss diese auf die jeweiligen Outcomes haben. Häufig müssen wir dazu dann aber mehrere Abbildungen erstellen, den bei so vielen Faktoren reichen dann einfache 2D Abbidlungen dann meistens nicht mehr aus. Ich versuche hier dann einmal zu zeigen, wie du das meiste aus {ggplot} rausholen kannst, um dir komplexe Daten zu visualisieren.\nWie bringen wir also möglichst viele informative Abbildungen sinnvoll zusammen? Wir nutzen dazu das R Paket{gghalves}. Wir können mit {gghalves} halbe Plots erstellen und diese dann miteinander kombinieren für ein Faktorlevel kombinieren. Dabei setzen wir dann in die Mitte Boxplots. Links von den Boxplots zeichnen wir die einzelnen Beobachtungen als Punkte mit stat_dots() und die Verteilung der einzelnen Beobachtungen zeichnen wir mit dem R Paket {ggdist} auf die rechte Seite. Das Tutorium Visualizing Distributions with Raincloud Plots liefert dann noch mehr Anleitungen für noch mehr Varianten. Wie du aber schon am R Code siehst, ist das eine etwas komplexere Abbildung geworden.\nDamit wir den ganzen R Code nicht die ganze zeit kopieren müssen, habe ich im folgenden Chunk einmal ein {ggplot}-Template erstellt, welches ich dann immer wieder mit neuen Daten und einem aes()-Aufruf versehen werde. Das kürzt dann doch ziemlich den Code zusammen. Insbesondere da wir ja sehr viele Abbildungen für unsere drei Datensätz bauen müssen. Du kannst natürlich auch immer dreimal die einzelnen Abbildungen bauen oder aber mit facet_wrap() arbeiten um den dritten Faktor darzustellen.\n\ngg_half_template &lt;- ggplot() +\n  stat_halfeye(adjust = .5, width = .6, \n               .width = 0, justification = -.2, \n               point_colour = NA) + \n  geom_boxplot(width = 0.15, outlier.shape = NA) +\n  stat_dots(side = \"left\", justification = 1.12, binwidth = .25) +\n  coord_cartesian(xlim = c(1.2, 2.9), clip = \"off\") +\n  scale_color_okabeito() +\n  theme(legend.position = \"top\") \n\nBeginnen wir uns nun einmal die drei Datensätze zu visualisieren und nutzen dann die Abbildungen um etwas über die hierarchischen Strukturen in den Daten zu erfahren. Aus den Rückschlüssen können wir dann entscheiden, wie wir unsere lineare gemischten Modelle bauen müssen.\n\n52.3.1 Schuldaten\nDann schauen wir uns einmal in den folgenden beiden Tabs die Schuldaten und damit die Effekte der Schulen und der jeweils drei Klassen auf die Testergebnisse der Schüler an. Es ist immer wichtig sich alle möglichen Kombinationen von Faktoren anzuschauen um dann auch eine Idee für das gemischte Modell im Anschluss zu finden. Sonst stochert man sehr im Nebel rum und mit den Abbildungen hat man dann einen Hinweis, wohin es gehen könnte.\n\nEffekt der SchuleEffekt der Klassen\n\n\nIn der folgenden Abbildung 52.5 sehen wir einmal die Effekte der Schule aufgeteilt nach den Lehrformaten auf die Testergebnisse der jeweiligen Schüler. Es fällt sofort ein Effekt der Schulen auf die Testergebnisse auf. Zum Beispiel hat die Greenville School im Frontalunterricht sehr viel schlechte Testergebnisse als die beiden anderen Schulen mit Frontalunterricht. Ähnliches, aber im positiven Sinne, sehen wir bei der Arlington Academy, die gegen den Trend der beiden anderen Schulen, bessere Ergebnisse bei dem Lehrformat Flipped Classroom erreicht. Somit müssen wir in unserer Analyse die Schule mit berücksichtigen, es macht eben einen Unetrschied, auf welche Schule ein Schüler gegangen ist.\n\ngg_half_template %+%\n  school_tbl + \n  aes(x = trt, y = test, color = school_id) +\n  labs(x = \"Lehrformat\", y = \"Testscore\", color = \"Schule\") +\n  guides(color = guide_legend(nrow = 3, byrow = FALSE))\n\n\n\n\n\n\n\nAbbildung 52.5— Dreifachplot der Testergebnisse der Schüler zusammen über alle drei Klassen in den jeweiligen neun Schulen aufgetrennt nach dem Lehrformat. Teilweise sind starke Effekte der Schulen auf die Testergebnisse der Schüler zu erkennen.\n\n\n\n\n\n\n\nJetzt schauen wir uns noch den Effekt der Klasse an und fragen uns in der Abbildung 52.6, ob wir auch einen starken Effekt der Klassen auf die Testergebnisse haben. Hier sehen wir zwar auch Unterschiede zwischen den Klassen, aber die Effekt sind in den Lehrformaten eher gleichmäßig vertreten. Die kleine Gruppe bei dem Lehrformat Frontal gehört zur einer Schule und nicht zu einer einzelnen Klasse. Damit könnten wir die Klasse eher ignorieren, wenn wir unser Modell bauen. Es macht nicht so einen großen Unterschied in welche Klasse ein Schüler gegangen ist.\n\ngg_half_template %+%\n  school_tbl + \n  aes(x = trt, y = test, color = class_in_school_id) +\n  labs(x = \"Lehrformat\", y = \"Testscore\", color = \"Klasse\") +\n  guides(color = guide_legend(nrow = 1, byrow = FALSE))\n\n\n\n\n\n\n\nAbbildung 52.6— Dreifachplot der Testergebnisse der Schüler zusammen über alle Schulen in den jeweiligen drei Klassen aufgetrennt nach dem Lehrformat. Die Effekte der einzelnen Klassen sind nicht so stark ausgeprägt. Der Effekt der Schulen scheint in diesem Fall stärker zu sein.\n\n\n\n\n\n\n\n\nGerade haben wir gesehen, dass die Schulen mehr der Varianz in den Testergebnissen der Schüler erklären als die Klassen. Brauchen wir eigentlich nur die Schulen oder reichen auch die Informationen die in den einzelnen Klassen stecken? Wir haben ja unsere Daten so gebaut, dass wir immer nur drei Klassen pro Schule haben und jeweils eine der drei Klassen ein Lehrformat erhält. Damit könnte es sein, dass wir mit dem Faktor class_id auch die Varianz der Schulen scholl_id mit abbilden könnten. Das funktioniert hier aber nur, da die immer die gleiche Anzahl an Klassen mit der gleichen Anzahl an Lehrformaten in einer Schule verschachtelt ist. Schauen wir dazu einmal in die Abbildung 52.7. Wie wir sehen, scheinen die einzelnen Klassen die jeweiligen Schulen mit abzubilden. Die Klasse 19, 20 und 21 ist beim Forntalunterreicht schlechter. Dies wird die Schule Greenville School sein. Wir können also alleine durch die Information zu den einzlenen Klassen die Varianz der Schulen erklären! Mal schauen, was das dann später für unser lineares gemischtes Modell bedeutet.\n\nggplot(school_tbl, aes(x = class_id, y = test, fill = trt)) +\n  geom_boxplot(outlier.size = 0.5) +\n  labs(x = \"Individuelle Klassen ID\", y = \"Testscore\", fill = \"Lehrformat\") +\n  scale_fill_okabeito() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nAbbildung 52.7— Boxplot der Testergebnisse der drei Lehrformate aufgeteilt nach den individuellen Klassen. Da immer nur drei Klassen pro Schule erhoben wurden, bilden die individuellen Klassen auch den Effekt der Schulen mit ab.\n\n\n\n\n\n\n\n52.3.2 Weizendaten\nBei den Weizendaten haben wir auch die Positionen der einzelnen Parzellen durch die Faktoren row und col. Damit wissen wir an welcher Stelle die jeweiligen Parzellen auf dem Feld zu finden sind. Damit wissen wir dann auch, welche Behandlung mit Stickstoff und welche Weizenlinie wo aufgebracht wurde. In der Abbildung 52.8 sehen wir die Visualisierung des experimentellen Designs mit dem R Paket {desplot}. Wir sehen klar die Struktur der sechs Blöcke. In jedem Block finden sich die drei Sorten. In jeder Sorte wurde dann unterschiedlich mit Stickstoff gedüngt. Wir haben hier aber keine echte Spaltanlage vorliegen, da die Stickstoffbehandlung als Subplot quadratisch angeordnet ist. Später brauchen wir die Informationen um unser lineares gemischtes Modell sauber zu definieren.\n\ndesplot(oats_tbl, block ~ col*row, \n        num = nitro, col = gen,\n        cex = 1, aspect = 5/3,\n        main = \"\")\n\n\n\n\n\n\n\nAbbildung 52.8— Visualisierung des experimentellen Designs des Split plots für die Weizendaten. In sechs Blöcken wurden die drei Sorten aufgebracht. In jeder Sorte wurden wiederum die Sticktoffmengen randomisiert. Es kiegt keine echte Spaltanlage vor, da die Subplots innberhalb der Blöcke quadratisch angeordnet sind.\n\n\n\n\n\nIn den folgenden Tabs schauen wir uns dann einmal die Effekte der Weizenlinien sowie der Stickstoffdüngung auf den Ertrag an. Dabei trennen wir dann die Abbildung für die Blöcke auf. Auch hier wollen wir uns erstmal einen Überblick verschaffen und schauen, ob wir überhaupt einen Effekt von den Behandlungen haben oder aber ob die Blöcke sich einigermaßen gleich verhalten. Auch könnte es sein, dass die genetische Linien des Weizen an unterschiedlichen Standorten der Blöcke dann auf einmal doch andere Erträge bringen. All das wollen wir uns einmal in den folgenden Abbildungen anschauen.\n\nEffekt der Linie genEffekt des Stickstoff nitroEffekt von gen in block\n\n\nIn der Abbildung 52.9 sehen wir die Ausiwkungend der Sorte des Weizens auf den Ertrag aufgeteilt nach den sechs Blöcken. Klar ist zu erkennen, dass der Block 4 teilweise zu sehr viel höheren Erträgen führt. Auch haben wir bei der Sorte Victory einzelne Gruppen von Pflanzen, die anscheinend mehr Ertrag im Block 4 produzieren. Hier liegt also eine klare Wechselwirkung zwischen den Blöcken und der Sorte vor. Der Block muss auf jeden Fall mit in das lineare gemischte Modell. Die Effekt über die Sorten hinweg deuten auf keinen Trend hin, im Mittel sind alle Sorten des Weizen gleich im Bezug auf den Ertrag.\n\ngg_half_template %+%\n  oats_tbl + \n  aes(x = gen, y = plant_yield, color = block) +\n  labs(x = \"Genetische Linie\", y = \"Ertrag\", color = \"Block\") +\n  guides(color = guide_legend(nrow = 1, byrow = FALSE))\n\n\n\n\n\n\n\nAbbildung 52.9— Betrachtung der Auswirkungen der Sorte gen des Weizens auf den Ertrag, aufgeteilt nach den Blöcken. Einige Blöcke haben klar mehr Ertrag als andere Blöcke, wie auch schon bei den Stickstoffdüngungen.\n\n\n\n\n\n\n\nBetrachten wir in der Abbildung 52.10 den Ertrag in Abhängigkeit von der Stickstoffdüngung. Auch hier teilen wir die Daten wieder nach den Blöcken auf. Zuerst sehen wir einen klaren Trend. mit der Zunahme der Stickstoffkonzentration nimmt auch der Ertrag zu. Dennoch haben wir auch hier ein klares Problem mit dem Block 4. Der Block 4 hat immer am meisten Ertrag über alle Stickstoffstufen. In der Dosis 0.4 gibt es sogar eine Gruppe von Beobachtungen, die eindeutig am meisten Ertrag im Block 4 liefert. Auch hier sehen wir wieder eine Abhängigkeit des Ertrags von dem Block. Gehen wir also mal der Struktur der Daten weiter nach.\n\ngg_half_template %+%\n  oats_tbl + \n  aes(x = nitro, y = plant_yield, color = block) +\n  labs(x = \"Stickstoffkonzentration\", y = \"Ertrag\", color = \"Block\") +\n  guides(color = guide_legend(nrow = 1, byrow = FALSE))\n\n\n\n\n\n\n\nAbbildung 52.10— Betrachtung der Auswirkungen der Stickstoffdüngung nitro des Weizens auf den Ertrag, aufgeteilt nach den Blöcken. Einige Blöcke haben klar mehr Ertrag als andere Blöcke, wie auch schon bei den Sorten.\n\n\n\n\n\n\n\nAbschließend schauen wir nochmal in der Abbildung 52.11 auf die Wechselwirkung zwischen den Blöcken und den Sorten. Hier sehen wir endlich unsere kleinen Gruppen, die wir auch schon in den beiden anderen Abbildungen gesehen haben klar zugeordnet. Die Sorten spalten sich klar über die Blöcke und Stickstoffgaben auf. Es macht also einen Unterschied wo wir die einzelnen Sorten gepflanzt haben. Die Blöcke und Sorten interagieren klar miteinander. Wir können also sagen, dass die Sorten in den Blöcken auf jeden Fall genestet sind. Wir werden also diese Struktur auf jeden Fall berücksichtigen müssen.\n\ngg_half_template %+%\n  oats_tbl + \n  aes(x = nitro, y = plant_yield, color = block) +\n  labs(x = \"Stickstoffkonzentration\", y = \"Ertrag\", color = \"Block\") +\n  guides(color = guide_legend(nrow = 1, byrow = FALSE)) +\n  facet_wrap(~ gen)\n\n\n\n\n\n\n\nAbbildung 52.11— Betrachtung der Auswirkungen der Stickstoffdüngung nitro des Weizens auf den Ertrag, aufgeteilt nach den Blöcken und den Sorten des Weizens. Klar ist zu erkennen, dass einige Sorten in einigen Blöcken klar mehr Ertrag haben.\n\n\n\n\n\n\n\n\nJetzt wollen wir nochmal schauen, ob wir auch eine Interaktion zwischen der Stickstoffdüngung, den Weizensorten und den Blöcken vorliegen haben. Insbesondere müssen wir natürlich schauen, wie sich unsere beiden Behandlungen nitro und gen untereinander verhalten. Wenn wir hier auch eine Interaktion vorliegen haben, dann müssen wir diese Interaktion auch im Modell abbilden. Zuerst erschaffen wir uns aber die Mittelwerte über alle Faktorenkombinationen.\n\nstat_oats_tbl &lt;- oats_tbl |&gt; \n  group_by(nitro, gen, block) |&gt; \n  summarise(mean = mean(plant_yield))\n\nDann sind wir wieder etwas faul und bauen uns erstmal ein {ggplot}-Template für die Interaktionsabbildungen. Sonst produzieren wir wieder sehr viel redunanten Code, was wir uns hier dann sparen können. Wir werden uns einfach die Mittelwerte über die Stickstoffgaben getrennt für die Sorten und die Blöcke einmal anschauen.\n\ngg_inter_template &lt;- ggplot() +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\") +\n  guides(color = guide_legend(nrow = 1, byrow = FALSE)) +\n  scale_color_okabeito() +\n  theme(legend.position = \"top\") \n\nIn der Abbildung 52.12 sehen wir einmal die Interaktionsplots für die verschiedenen möglichen Interaktionen zwischen den Faktoren der Stickstoffdüngung, der Weizensorte und den Blöcken. Abbildung 52.12 (a) zeigt klar, dass es keine Interaktion zwischen der Stickstoffdüngung und den Sorten gibt. Die Graden laufen parallel zueinander. Wir haben einen mittleren Effekt der Stickstoffdüngung, da wir einen Anstieg beobachten. Dennoch ist die Ordnung der Sorten pro Level der Stickstoffdüngung gleich. Würden sich die Graden überschneiden, hätten wir eine Interaktion vorliegen. Da die Graden das nicht tun, können wir also von keiner Interaktion zwischen nitro und gen ausgehen. Wir sehen aber auch in den beiden anderen Abbildungen, dass wir auf jeden Fall den Block mit modellieren müssen. Der Block hat zumindest einen visuellen Einfluss auf den Ertrag.\n\ngg_inter_template %+% \n  oats_tbl + \n  aes(x = nitro, y = plant_yield, color = gen, group = gen) +\n  labs(x = \"Stickstoffkonzentration\", y = \"Ertrag\", color = \"Sorte\") \n\ngg_inter_template %+% \n  oats_tbl + \n  aes(x = nitro, y = plant_yield, color = gen, group = gen) +\n  labs(x = \"Stickstoffkonzentration\", y = \"Ertrag\", color = \"Sorte\") +\n  facet_wrap(~ block)\n\ngg_inter_template %+% \n  oats_tbl + \n  aes(x = nitro, y = plant_yield, color = block, group = block) +\n  labs(x = \"Stickstoffkonzentration\", y = \"Ertrag\", color = \"Block\") +\n  facet_wrap(~ gen) \n\n\n\n\n\n\n\n\n\n\n\n(a) Interaktion nitro:gen\n\n\n\n\n\n\n\n\n\n\n\n(b) Interaktion nitro:gen:block\n\n\n\n\n\n\n\n\n\n\n\n(c) Interaktion nitro:block:gen\n\n\n\n\n\n\n\nAbbildung 52.12— Interaktionsplot der Mittelwerte für die Stickstoffbehandlung nitro, den Sorten des Weizens gen sowie den Blöcken block. Dargestellt sind die Mittelwerte für die jeweilige Faktorkombination. Wenn wir keine Interaktion erwarten, dann laufen die Graden parallel zueinander.\n\n\n\n\n\n\n52.3.3 Wurmdaten\nIn den vorherigen Datensätzen haben wir uns ein eher normalverteiltes Outcome angeschaut. In den Wurmdaten wollen wir uns einmal Zähldaten anschauen. Das hat natürlich auf den Plot des experimentellen Designs erstmal keinen Einfluss. Wir haben die Informationen zu den Reihen und den Spalten und können daran dann unser Latinsquare Design einmal in dem R Paket {desplot} in der Abbildung 52.13 darstellen. In einem Latinsquare Design ist jede unserer fünf Behandlungen genau einmal in jeder Reihe oder Spalte vertreten. Ich habe einmal die Parzellen nach den Behandlungen eingefärbt. Nochmal zur Erinnerung, die Buchstaben haben hier keine tiefere Bedeutung. Die Buchstaben stellen eben nur die fünf verschiedenen Insektiziede gegen den Wurmbefall dar.\n\ndesplot(wireworms_tbl, trt ~ col * row,\n        text = trt, cex = 1, show.key = FALSE,\n        main = \"\") \n\n\n\n\n\n\n\nAbbildung 52.13— Latinsquare Design der Insektizidbehandlung auf einem \\(5 \\times 5\\) großen Versuchsfeld. Jede Behandlung ist genau einmal in jeder Reihe und jeder Spalte vertreten. Die Buchstaben sind willkürlich gewählt.\n\n\n\n\n\nJetzt schauen wir uns in der Abbildung 52.14 nochmal die Effekte der Spalte und der Reihe auf die Anzahl der Würmer an. Hier muss man natürlich bedenken, dass die Reihen und die Spalten verschoben die gleichen Effekte haben. Den jede Spalte ist auch ein Teil einer Reihe und umgekehrt. Wir sehen aber sofort das es Problem mit der Spalte 1 sowie dann mit der Reihe 1 gibt. Hier haben wir bei der Insektizidbehandlung N sehr viel mehr Würmer als in den anderen Parzellen. Teilweise sehen wir auch Abweichungen nach oben bei den anderen Behandlungen, je nachdem welche Parzelle wir betrachten. Hier müssen wir auf jeden Fall unser Modell so anpassen, dass die Spalten und Reihen im Modell berücksichtigt werden.\n\ngg_half_template %+%\n  wireworms_tbl + \n  aes(x = trt, y = count_worms, color = col) +\n  labs(x = \"Insektizidbehandlung\", y = \"Anzahl Würmer\", color = \"Spalte (col)\") +\n  coord_cartesian(xlim = c(1.2, 4.9), clip = \"off\") +\n  guides(color = guide_legend(nrow = 1, byrow = FALSE)) \n\ngg_half_template %+%\n  wireworms_tbl + \n  aes(x = trt, y = count_worms, color = row) +\n  labs(x = \"Insektizidbehandlung\", y = \"Anzahl Würmer\", color = \"Reihe (row)\") +\n  coord_cartesian(xlim = c(1.2, 4.9), clip = \"off\") +\n  guides(color = guide_legend(nrow = 1, byrow = FALSE)) \n\n\n\n\n\n\n\n\n\n\n\n(a) Spalte (col)\n\n\n\n\n\n\n\n\n\n\n\n(b) Reihe (row)\n\n\n\n\n\n\n\nAbbildung 52.14— Betrachtung der Auswirkung der verschiedenen Insketizidbehandlung auf die Anzahl von Würmern aufgeteilt für die Spalten und Reihen in einem Latinsquare Design. Teilweise sind die Effekte der Position col und row auf die Würmeranzahlen klar ersichtlich.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Lineare gemischte Modelle</span>"
    ]
  },
  {
    "objectID": "stat-modeling-mixed.html#modellierung",
    "href": "stat-modeling-mixed.html#modellierung",
    "title": "52  Lineare gemischte Modelle",
    "section": "52.4 Modellierung",
    "text": "52.4 Modellierung\nNachdem wir uns jetzt ausführlich mit der Visualisierung beschäftigt haben, werden wir uns jetzt einmal mit der Modellierung der lineare Modelle befassen. Häufig sind die Modelle sehr komplex und auch ich weiß dann immer nicht, was soll wie in ein Modell rein, deshalb muss ich auch am Ende immer verschiedene Modelle miteinander vergleichen. Das beste Modell sollte so wenige Faktoren und Interaktionen enthalten wie möglich, aber dennoch alle Quellen von möglicher Varianz abdecken. Daher lohnt es sich immer auch ein sehr einfaches Modell mit in die Analyse zu nehmen und zu schauen, ob es nicht auch mit einem einfachen Modell klappen würde. Nicht immer ist ein lineares gemischtes Modell die beste Lösung. Manchmal passt dann auch ein einfaches Modell mit nur festen Effekten.\n\n\n\n\n\n\nMindestanzahl an Leveln für einen zufälligen Effekt\n\n\n\nWir brauchen mindestens 5 bis 6 Level für einen Faktor, den wir als zufälligen Effekt deklarieren. Das würde hier aber leider die Beispiele sehr komplex machen… deshalb hier mit weniger Leveln und dafür dann nicht so guten Ergebnissen.\n\n\nWir immer in R haben wir auch eine ganze Reihe von Paketen zu Verfügung um ein lineares gemischtes Modell zu schätzen. Damit die Sachlage hier nicht ausartet, konzentriere ich mich auf die großen zwei Pakete plus eine etwas andere Implementierung. Zum einen hat Bates u. a. (2014) das R Paket {lme4} entwickelt, welches uns erlaubt lineare gemischte Modelle in R anzuwenden. Es gibt noch das ältere R Paket {nlme} was ich aber nicht mehr für lineare gemischte Modelle nutze. Wir nutzen aber gerne die Funktion gls() aus dem R Paket {nlme}, wenn wir eine lineare Regression mit heterogenen Varianzen rechnen wollen. Eine andere Implementierung mit mehr Möglichkeiten, wenn es um nicht normalverteilte Daten geht, ist das R Paket {glmmTMB}. Wir haben hier insbesondere die Möglichkeit mehr Varianzstrukturen in den Daten abzubilden. Dazu dann gerne mehr in den Vignetten des R Pakets unter Covariance structures with glmmTMB. Du musst dich aber nicht tiefer Einlesen, im prinzip sind die Regeln ähnlich wie bei einem glm(). Mehr dazu dann aber gleich in dem entsprechenden Abschnitt zu dem R Paket {glmmTMB}. Teilweise sind die Ausgaben der verschiedenen R Paket schlecht miteinander zu vergleichen, da man nicht weiß, wo was wiedergegeben wird. Hier hilft das R Paket {mixedup}, welches einem die Arbeit abnimmt gewisse Information aus einem Fit zu einem linearen gemischten Modell zu extrahieren. Abschließend schauen wir uns noch die Implementierung der linearen gemischten Modell in dem R Paket {multilevelmod} an, da wir hier noch einfacher ein gemischtes Modell auswählen können. Wichtig ist hier zu wissen, dass die Funktionen aus {glmmTMB} nicht implementiert sind. Daher musst du dann schauen, was du brauchst und danach entscheiden. Ich stelle alle Varianten hier dann einmal vor.\nIn der folgenden Tabelle findest du nochmal die Schreibweise für die zufälligen Effekte in einem linearen gemischten Modell in R. Glücklicherweise ist die Schreibweise mittlerweile in R bindend und alle neueren Pakete nutzen auch diese Formelschreibweise der zufälligen Effekte. Im Allgmeinen definieren wir einen zufälligen Effekt mit (1 | random). Wir wollen damit einen festen Mittelwert für jedes Level des zufälligen Faktors schätzen. Diese Schreibweise ist damit dann auch der Standard. Wenn du noch eine kontinuierliche Variable c_1 in den Daten hättest, die sich innerhalb der zufälligen Effekte ändert, dann könntest du auch einen variierenden Mittelwert der zufälligen Effekte für die zusätzliche Variable mit (c_1 | random)schätzen. Aber dieser Fall tritt eher selten auf.\n\nBedeutung der Formelschreibweise der zufälligen Effekte in einem linearen gemischten Modell in {lmer} und {glmmTMB}. Mehr zu der Bedeutung dann in der Veröffentlichung von Bates u. a. (2014). Glücklicherweise ist die Schreibweise mittlerweile in R bindend und alle neueren Pakete nutzen auch diese Formelschreibweise der zufälligen Effekte.\n\n\n\n\n\n\nFormula\nBedeutung\n\n\n\n\n\\((1\\; |\\; g)\\)\nZufälliger \\(y\\)-Achsenabschnitt mit festen Mittelwert (eng. Random intercept with fixed mean)\n\n\n\\((1\\; |\\; g_1/g_2)\\)\nDer \\(y\\)-Achsenabschnitt variiert in \\(g_1\\) und \\(g_2\\) innerhalb von \\(g_1\\) (eng. Intercept varying among g1 and g2 within g1)\n\n\n\\((1\\; |\\; g_1) + (1\\; |\\; g_2)\\)\nDer \\(y\\)-Achsenabschnitt variiert zwischen \\(g_1\\) und \\(g_2\\) (eng. Intercept varying among g1 and g2)\n\n\n\\(x + (x\\; |\\; g)\\)\nKorrelierter zufälliger \\(y\\)-Achsenabschnitt und Steigung (eng. Correlated random intercept and slope)\n\n\n\\(x + (x\\; ||\\; g)\\)\nUnkorrelierter zufälliger \\(y\\)-Achsenabschnitt und Steigung (eng. Uncorrelated random intercept and slope)\n\n\n\n\n52.4.1 Mitteln über einen zufälligen Effekt\nManchmal können wir das auch mit den gemischten Modellen einfach lassen und über eine Faktor mitteln und dann ist auch gut. Damit haben wir dann die individuelle Variabilität “weggemittelt”. Das funktioniert in einem balancierten Design teilweise hervorragend und ist auf jeden Fall immer einen Versuch wert. Bei komplexeren Designs lässt sich manchmal dann leider nicht gut festlegen über welchen Faktor am besten gemittelt werden sollte. Dann hilft eben doch nur ein komplexeres gemischtes Modell. Haben wir aber über einen Faktor gemittelt, können wir alles nur mit festen Effekten in einem lm() oder glm() lösen. Das macht uns dann das Modellieren sehr viel einfacher. Deshalb hier einmal als Beispiel das Mitteln über die einzelnen Klassen und damit auch über die Schüler. Wir kriegen dann einen Mittelwert pro Klasse und nehmen damit die individuelle Varianz aus unseren Daten raus.\n\nmean_school_tbl &lt;- school_tbl |&gt; \n  group_by(school_id, trt, class_id) |&gt; \n  summarise(mean_test = mean(test))\n\nNachdem wir über den Faktor class_id gemittelt haben, können wir dann einfach ein lineares Modell mit der Funktion lm() rechnen. Dann schaue ich gleich nochmal im Abschnitt zu dem R Paket lme4() wie gut unser Modell abschneidet. Wir werden vermutlich einen kleineren Fehler haben, da wir natürlich auch Variabilität wegmitteln. Aber das ist ja auch das Ziel der Übung.\n\nmean_lm_fit &lt;- lm(mean_test ~ trt + school_id + trt:school_id, data = mean_school_tbl)\n\nIch kann immer nur empfehlen, einmal den Schritt zu machen und über die individuellen Pflanzen oder Beobachtungen zu mitteln. Häufig lässt sich damit dann ein gemischtes Modell vermeiden, was dann auch die Interpretation der Ergebnisse und deren Darstellung einfacher macht.\n\n\n52.4.2 … mit dem R Paket {lme4}\nDas R Paket {lme4} von Bates u. a. (2014) ist das Standardpaket, welches uns erlaubt lineare gemischte Modelle in R anzuwenden. Hier gibt es dann auch mit der Hilfeseite GLMM FAQ – Ben Bolker and others auch umfangreiche Informationen und Ratschläge für die Nutzung. Auch hier musst du dort nicht alles nachlesen um ein lineares gemischtes Modell in R rechnen zu können. Manchmal kommt es aber zu Problemen im Fit des Modells, so dass hier dann Hilfe zu finden ist. Im Folgenden schauen wir uns einmal die Implementierung von {lme4} für die Schuldaten an. Wir nutzen dazu die Hauptfunktion lmer(), wenn wir normalverteilte Daten als Outcome vorliegen haben. Mit einem Testscore können wir davon ausgehen, dass dieser normalverteilt ist. Wir rechnen jetzt verschiedene Modelle in den folgenden Tabs und schauen dann im Anschluss einmal, welches der Modelle das beste Modell ist. Das beste Modell könnten wir dann zum Beispiel in einem Gruppenvergleich weiter nutzen.\n\nlmer() 2-faktoriell ungenestedlmer() 3-faktoriell ungenestedlmer() 3-faktoriell genested\n\n\nBeginnen wollen wir mit dem einfachsten linearen gemischten Modell. Wir haben hier nur einen festen Effekt trt sowie einen zufälligen Effekt school_id vorliegen. Damit ignorieren wir die Varianzen aus den Klassen. Wir schauen also, ob wir mit einem etwas simpleren Modell schon ein gutes Ergebnis erhalten.\n\nlmer_2fac_fit &lt;- lmer(test ~ trt +\n                        (1 | school_id), \n                      data = school_tbl)\n\nWie gut hat nun das Modell geklappt? Fangen wir einmal mit einem \\(R^2\\) an. Ähnlich wie das Bestimmtheitsmaß \\(R^2\\) gibt der Intraclass Correlation Coefficient (abk. ICC) Aufschluss über die erklärte Varianz und kann als “der Anteil der Varianz, der durch die Gruppierungsstruktur in den Daten erklärt wird” interpretiert werden. Damit haben wir dann auch eine Maßzahl, wie gut unser gemischtes Modell funktioniert hat. Uns interessiert hier nur das adjustierte ICC.\n\nlmer_2fac_fit |&gt; icc()\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.880\n  Unadjusted ICC: 0.213\n\n\nDas sieht gar nicht schlecht aus für einen ersten Versuch. Mit einem ICC von \\(0.88\\) sind wir schon ziemlich weit oben an der Grenze. Jetzt müssen wir noch schauen, wie die anderen Maßzahlen aussehen. Eventuell reicht dieses einfache Modell schon aus um unsere Daten zu modellieren und zu erklären. Wir nutzen hier die Funktion model_performance() aus dem R Paket {performance} um zu schauen, wie gut unser Modell dann zu den Daten gepasst hat.\n\nlmer_2fac_fit |&gt; model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n--------------------------------------------------------------------------------\n3154.459 | 3154.572 | 3175.917 |      0.971 |      0.758 | 0.880 | 4.271 | 4.307\n\n\nHier schauen wir einmal auf das R2 (marg.), was auch eine andere Art des Bestimmtheitsmaßes für die festen Effekte ist. Also auch hier jetzt die Frage, wieviel Prozent der Varianz erklären meine festen Effekte? Hier liefert dann das R2 (marg.) eine Antwort. Das R2 (cond.) berücksichtigt sowohl die festen als auch die zufälligen Effekte bei der Berechnung des \\(R^2\\). Damit haben wir hier die erklärte Varianz von festen und zufälligen Effekten bei \\(0.97\\), also über \\(97\\%\\). Hier haben wir ein echt gutes Modell vorliegen. Die anderen Maßzahlen brauchen wir nur für einen direkten Vergleich von Modellen.\n\n\nNachdem wir schon recht gute Ergebnisse mit dem simplen gemischten Modell mit nur einem zufälligen Effekt erreicht haben, nehmen wir jetzt noch neben dem Effekt der Schule den Effekt der Klassen class_in_school_id als zufälligen Effekt mit ins Modell. Dann wollen wir mal schauen, ob dieses Modell dann besser ist als das einfache Modell. Unser einfaches Modell ist schon so gut, dass wir hier kaum noch Steigerungen hinkriegen und wir müssen uns dann am Ende fragen, ob nicht ein einfacheres Modell nicht auch reichen würde.\n\nlmer_3fac_fit &lt;- lmer(test ~ trt +\n                        (1 | class_in_school_id) + \n                        (1 | school_id), \n                      data = school_tbl)\n\nAuch hier berechnen wir dann einmal den Intraclass Correlation Coefficient (abk. ICC) um mehr über die erklärte Varianz der zufälligen Effekte zu erfahren. Der ICC steigt noch um einen winzigen Betrag gegenüber dem simpleren Modell mit nur einem zufälligen Effekt. Ob sicher hier der Umstieg lohnt, muss man dann nochmal überlegen.\n\nlmer_3fac_fit |&gt; icc()\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.891\n  Unadjusted ICC: 0.216\n\n\nDann schauen wir uns nochmal die Werte für R2 (cond.) und R2 (marg.) an und sehen, dass wir hier auch nur eine kleine Steigerung in den Werten haben. Wir erklären zwar noch mehr Varianz, aber der Anteil ist doch recht gering.\n\nlmer_3fac_fit |&gt; model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n--------------------------------------------------------------------------------\n3115.541 | 3115.699 | 3141.291 |      0.974 |      0.757 | 0.891 | 4.077 | 4.119\n\n\nAm Ende musst du dann überlegen, ob sich hier noch eine weitere Modellierung lohnt. Wir kommen zwar noch höher mit den Werten für das Bestimmtheitsmaß, aber dann wird auch das Modell auch um einiges komplizierter. Daher kannst du dir als letztes noch das genestete Modell einmal anschauen.\n\n\nJetzt bleibt uns eigentlich nur noch als Modell ein genestetes gemischtes Modell übrig indem wir dann die Klassen in den Schulen genestet modellieren. Das entspricht dann natürlich exakt der Abhängigkeitsstruktur, wie wir auch unsere Daten gebaut haben. Also sollten wir mit dem folgenden Modell auch fast die gesamte Varianz erklären. Im echten Leben kennen wir natürlich nicht die Art und Weise wie die Daten entstanden sind. Deshalb hier als Demonstration das Modell mit einem genesteten, zufälligen Term für die Klassen in den Schulen dargestellt durch (1 | class_id/school_id).\n\nlmer_3fac_nested_fit &lt;- lmer(test ~ trt +  \n                               (1 | class_id/school_id), \n                             data = school_tbl)\n\nAuch heir schauen wir dann einmal den Intraclass Correlation Coefficient (abk. ICC) an und sehen, dass der Wert fast Eins ist. Das wundert uns natürlich nicht.\n\nlmer_3fac_nested_fit |&gt; icc()\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.967\n  Unadjusted ICC: 0.195\n\n\nUnd dann sehen wir auch, dass wir mit unseren Modell mit den genesteten zufälligen Effekten ein R2 (cond.) von über \\(99\\%\\) erreichen. Damit bildet unser genestetes Modell exakt die Abhängigkeitsstruktur wieder, mit der wir auch die Daten gebaut haben.\n\nlmer_3fac_nested_fit |&gt; model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n--------------------------------------------------------------------------------\n2458.714 | 2458.871 | 2484.463 |      0.993 |      0.798 | 0.967 | 1.968 | 2.019\n\n\nDamit habe ich gezeigt, dass wir auch ein perfektes Modell erhalten können, wenn wir wissen wie die Daten erschaffen wurden. Unsere anderen Modelle sind noch so gut, da ich ein sehr balanciertes Design für die Erstellung der Schuldaten gewählt habe. Wenn die Klassen unterschiedliche groß wären und auch unterschiedliche Anzahlen von Klassen pro Schule vorliegen würden, dann sehen die anderen Modelle bedeutend schlechter aus.\n\n\n\nManchmal möchten wir dann doch noch mehr Informationen als den Intraclass Correlation Coefficient (abk. ICC) oder das R2 (cond.) oder das R2 (marg.) aus dem linearen gemischten Modell extrahieren. Da mir das aber dann aktuell zu weit geht und hier auch nicht mehr erklärt, verweise ich auf die folgenden Funktionen aus dem R Paket {mixedup}. Du erhälst mit den Funktionen die Effekte für die zufälligen wie auch festen Effekte und das auch übergreifend für andere Pakte. Manchmal ist auch die Funktion summary() sehr klobig für ein lmer()-Objekt, da hilft dann die Funktion summarize_model(). In seltenen Fällen bist du dann auch an der Varianzstruktur und deren Schätzern interessiert, dafür gibt es dann auch noch die Funktion extract_vc(). Ich führe die Funktionen hier jetzt nicht aus, da wir einfach nur Output produzieren.\n\nextract_random_effects(lmer_2fac_fit)\nextract_fixed_effects(lmer_2fac_fit)\nsummarize_model(lmer_2fac_fit)\nextract_vc(lmer_2fac_fit)\n\nWir schauen uns dann einmal in einer Übersichtstabelle die drei Modelle an. Im folgenden Kasten findest du den Modellvergleich mit dem R Paket {modelsummary}. Wir können hier verschiedenste Sachen anschauen. Wichtig ist zum Beispiel ganz am Ende der \\(RMSE\\). Je kleiner der root mean square error ist, desto besser ist das Modell. Hier sehen wir, dass wir das beste Modell mit lmer_3fac_nested_fit vorliegen haben. Der \\(RSME\\) ist mit 1.97 am kleinsten. Danach kommt aber schon das simple lineare Modell mit nur den festen Effekten. Da das lm-Modell auch fast \\(96.5\\%\\) der Varianz erklärt können wir auch das lm-Modell hier nehmen und haben dann ein einfacheres Modell, was wir dann auch einfacher beschreiben können. Da musst du dann abwägen, aber es muss ja nicht immer komplex sein. Ein gutes, solides Modell reicht ja auch.\n\n\n\n\n\n\nModellvergleich mit modelsummary()\n\n\n\n\n\n\nmodelsummary(lst(\"lm (mean model)\" = mean_lm_fit, \n                 \"lmer 2-fakoriell\" = lmer_2fac_fit, \n                 \"lmer 3-fakoriell genested\" = lmer_3fac_nested_fit,\n                 \"lmer 3-fakoriell ungenested\" = lmer_3fac_fit),\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\"))\n\n\n\n\n\n\nlm (mean model)\n lmer 2-fakoriell\n lmer 3-fakoriell genested\n lmer 3-fakoriell ungenested\n\n\n\n\n(Intercept)\n66.341\n60.683\n60.683\n60.683\n\n\n\n[60.709, 71.973]\n[47.419, 73.948]\n[53.547, 67.819]\n[47.306, 74.060]\n\n\n\ns.e. = 2.681\ns.e. = 6.752\ns.e. = 3.633\ns.e. = 6.810\n\n\ntrtFlipped classroom\n−39.715\n−28.335\n−28.335\n−28.335\n\n\n\n[−47.680, −31.750]\n[−47.093, −9.576]\n[−38.426, −18.243]\n[−47.093, −9.576]\n\n\n\ns.e. = 3.791\ns.e. = 9.549\ns.e. = 5.137\ns.e. = 9.549\n\n\ntrtHyFlex\n32.492\n25.569\n25.569\n25.569\n\n\n\n[24.526, 40.457]\n[6.810, 44.327]\n[15.477, 35.660]\n[6.810, 44.327]\n\n\n\ns.e. = 3.791\ns.e. = 9.549\ns.e. = 5.137\ns.e. = 9.549\n\n\nschool_idClinton Christian\n0.542\n\n\n\n\n\n\n[−7.423, 8.507]\n\n\n\n\n\n\ns.e. = 3.791\n\n\n\n\n\nschool_idGreenville School\n−17.515\n\n\n\n\n\n\n[−25.480, −9.550]\n\n\n\n\n\n\ns.e. = 3.791\n\n\n\n\n\nschool_idJacksonville High\n−2.892\n\n\n\n\n\n\n[−10.857, 5.073]\n\n\n\n\n\n\ns.e. = 3.791\n\n\n\n\n\nschool_idArlington Academy\n20.060\n\n\n\n\n\n\n[12.095, 28.025]\n\n\n\n\n\n\ns.e. = 3.791\n\n\n\n\n\nschool_idFranklin Country\n−13.406\n\n\n\n\n\n\n[−21.371, −5.441]\n\n\n\n\n\n\ns.e. = 3.791\n\n\n\n\n\nschool_idGeorgetown High\n−24.335\n\n\n\n\n\n\n[−32.300, −16.370]\n\n\n\n\n\n\ns.e. = 3.791\n\n\n\n\n\nSD (Observations)\n\n4.307\n2.019\n4.119\n\n\nSD (Intercept school_id)\n\n11.682\n\n11.683\n\n\nSD (Intercept school_idclass_id)\n\n\n10.528\n\n\n\nSD (Intercept class_id)\n\n\n2.776\n\n\n\nSD (Intercept class_in_school_id)\n\n\n\n1.529\n\n\nNum.Obs.\n27\n540\n540\n540\n\n\nR2\n0.976\n\n\n\n\n\nR2 Adj.\n0.965\n\n\n\n\n\nR2 Marg.\n\n0.758\n0.798\n0.757\n\n\nR2 Cond.\n\n0.971\n0.993\n0.974\n\n\nAIC\n168.6\n3154.5\n2458.7\n3115.5\n\n\nBIC\n181.5\n3175.9\n2484.5\n3141.3\n\n\nICC\n\n0.9\n1.0\n0.9\n\n\nLog.Lik.\n−74.294\n\n\n\n\n\nRMSE\n3.79\n4.27\n1.97\n4.08\n\n\n\n\n\n\n\n\n\n\n\nUnd dann nochmal die visuelle Überprüfung mit check_model(). Hier schauen wir einmal, ob unser lineares gemischtes Modell dann auch funktioniert hat. Das praktische an der Funktion ist, dass wir in den Überschriften zu den einzelnen Abbildungen immer lesen könne, was wir in den Abbildungen sehen müssen, wenn die Annahme erfüllt sein soll. Wir haben hier also eine wunderbare visuelle Überprüfung des Modells. Ich mache das ganze jetzt nur für das Modell lmer_2fac_fit, was etwas willkürlich ist, aber sonst haben wir hier zig Abbildungen. Du kannst dann ja einfach selber bei den anderen Modellen schauen.\n\n\n\n\n\n\nModellüberprüfung mit check_model()\n\n\n\n\n\n\ncheck_model(lmer_2fac_fit)\n\n\n\n\n\n\n\nAbbildung 52.15— Überprüfung des Modells mit der Funktion check_model() aus dem R Paket {performance}. Eine Reihe von Annahmen an das Modell wird in verschiedenen Abbildungen visuell überprüft. Unter den Überschriften steht die Annahme an die Abbildung und wann die Annahme in der Überschrift als erfüllt gilt.\n\n\n\n\n\n\n\n\nBei den Schuldaten sind wir von einem normalverteilten Outcome testscore ausgegangen. Das R Paket {lme4} hat auch die Möglichkeit mit nicht normalverteilten Daten über die Funktion glmer() umzugehen. Da schauen wir aber gleich mal rein und zwar bei den Würmerdaten und stellen dabei auch das R Paket {glmmTMB} als Alternative vor. Du könnest aber den folgenden Abschnitt auch einfach mit einem glmer() rechnen aus dem Paket {lme4} rechnen, aber das R Paket {glmmTMB} hat ein paar Vorteile bei der Modellierung von nicht-normalverteilten Daten.\n\n\n52.4.3 … mit dem R Paket {glmmTB}\nHier schauen wir uns einmal den Datensatz zu den Würmern an. Wir haben hier kein normalverteiltes Outcome mehr vorliegen sondern zählen ja die Würmer. Wenn wir Zähldaten vorliegen haben, dann nutzen wir die Poissonverteilung um die Daten auszuwerten. Dazu müssen wir dann aber die Funktion glmer() verwenden, welche uns erlaubt auch eine andere Verteilung für das Outcome zu nutzen. Die Funktion glmer() ist in dem R Paket {lme4} implementiert und funktioniert nur, wenn du keine Overdispersion in den Daten vorliegen hast. Overdispersion bedeutet, dass die Varianz mit dem Mittelwert überproportional ansteigt. In einer Poissonverteilung steigt die Varianz der Daten mit dem Mittelwert der Zähldaten in einem 1:1 Verhältnis an. Wenn du ein größeres Verhältnis hast, also mit steigenden Mittelwert proportional größere Varianzen, dann liegt Overdispersion vor. Dafür haben wir dann gleich die Funktion check_overdispersion(). Wichtig ist, dass du keine Poissonregression rechnen kannst, wenn du Overdispersion vorliegen hast. Dann musst du deine Poissonregression für die Overdispersion adjustieren indem du eine andere Verteilungsfamilie wählst. Leider sind in {lme4} keine anderen Poissonfamilien implementiert, so dass wir dann auf das R Paket {glmmTMB} ausweichen. In dem R Paket {glmmTMB} gibt es eine reichhaltige Auswahl an Kovarianzstrukturen und Möglichkeiten Abhängigkeiten zu modellieren. Mehr dazu findest du auf der Hilfeseite zu Covariance structures with glmmTMB und auf der Seite zu glmmTMB: Generalized Linear Mixed Models using Template Model Builder. Auf der letzteren Seite findest du dann auch die Vignetten mit den jeweiligen Hilfsthemen. Leider kann {glmmTB} auch nicht alles modellieren, wenn es um die möglichen Fehlerquellen geht und auch hier verweise ich einmal auf eine Hilfeseite zu Covariance structures for the error term with glmmTMB - a workaround. Wie immer du musst das nicht alles lesen. Es ist auch eine Sammlung an Hilfen hier für den Fall, dass es mal jemand braucht.\nIn den beiden folgenden Tabs wollen wir dann einmal verschiedene Varianten durchprobieren. Zuerst rechnen wir das naive fixe Effekt Modell mit einem glm() und einer Quasipoissonverteilung. Dann probieren wir ein gemischtes Modell mit glmer() und einer Poissonfamilie und schauen, ob wir Overdispersion vorliegen haben. Parallel dazu rechnen wir dann in dem anderen Tab die Poissonregression unter der Annahme von Overdispersion mit glmmTMB() und der Option famliy = nbinom1, was faktisch einer Quasipoissonverteilung entspricht.\n\nglm mit family = quasipoissonglmer() mit family = poissonglmmTMB() mit family = nbinom1\n\n\nDieser Tab ist sehr kurz. Wir rechnen einfach eine Poissonregression unter der Annahme von Overdispersion. Deshalb nutzen wir hier auch gleich eine Quasipoissonverteilung, die es uns erlaubt für das Auftreten von einer Overdispersion zu adjustieren. Mehr zu der einfachen Poissonregression gibt es dann in dem Kapitel Poissonregression. Dort kannst du dann auch noch mehr zum Thema Poissonregression nachlesen. Wir rechnen also eine Poissonregression und nutzen dafür ein glm() und die Option family = \"quasipoisson\". Wir nehmen dabei als Effekte die Behandlung trt sowie die Positionen row und col mit in das Modell. Ich verzichte auf Interaktionen, da das Modell schon so recht groß ist.\n\nglm_quasipoisson_fit &lt;- glm(count_worms ~ trt + row + col, \n                            data = wireworms_tbl, family = \"quasipoisson\")\n\nDas Bestimmtheitsmaß \\(R^2\\) ist in einem glm-Modell nicht so einfach. Deshalb nutzen wir folgende Funktion um uns sowas ähnliches wiedergeben zu lassen. Wir interpretieren aber das \\(R^2\\) ganz gewohnt als den Anteil der erklärten Varianz in dem Outcome durch das Modell.\n\nr2_efron(glm_quasipoisson_fit)\n\n[1] 0.5286153\n\n\nDieses Modell nehmen wir dann als simple Alternative mit in den Vergleich zu den anderen gemischten Modellen. Manchmal reicht auch ein einfaches Modell und es muss nicht immer ein komplexes Modell sein.\n\n\nJetzt aber einmal ein lineares gemischtes Modell mit der Poissonfamilie für die Auswertung der Zähldaten. Daher haben wir dann einen fixen Effekt für die Behandlung trt sowie die beiden zufälligen Effekte für die Positionen der Parzellen mit (1 | row) und (1 | col). Dann wählen wir noch die Poissonfamilie aus und können das Modell einmal rechnen.\n\nglmer_poisson_fit &lt;- glmer(count_worms ~ trt + (1|row) + (1|col), \n                           data = wireworms_tbl, family = \"poisson\")\n\nBevor wir überhaupt etwas machen, schauen wir erstmal ob Overdispersion in unseren Daten vorliegt. Wenn unser Modell Overdispersion anzeigt, dann können wir das Modell gleich lassen. Das ist sehr wichtig zu wissen, ein Modell mit einer Poissonfamilie und Overdispersion wird dir immer falsche Ergebnisse liefern. Insbesondere wenn es dir um die Gruppenvergleiche geht. Die Nichtberücksichtigung der Overdispersion lässt deine Fehler zu klein werden und damit findest du zu viele falsche signifikante Ergebnisse.\n\nglmer_poisson_fit  |&gt; check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =   1.624\n  Pearson's Chi-Squared = 394.713\n                p-value = &lt; 0.001\n\n\nOverdispersion detected.\n\n\nWir haben sehr starke Overdispersion vorliegen und gehen daher in den anderen Tab und rechnen eine Quasipoisson Regression in einem linearen gemischten Modell. Hier nutzen wir dann das R Paket {glmmTMB}. Nur wenn du keine Overdispersion vorliegen hast, dann kannst du eine eine reine Poissonregression rechnen.\n\n\nDa wir in {lme4} keine Quasipoissonverteilung auswählen können, nutzen wir das R Paket {glmmTMB} mit der Verteilungsfamilie nbinom1, was einer Parametrisierung einer Quasipoissonverteilung entspricht. Mehr dazu dann auch auf der Hilfeseite zu Covariance structures with glmmTMB. Eigentlich spricht nichts dagegen gleich das R Paket {glmmTMB} zu nutzen, wenn du mit nicht normalverteilten Outcomes arbeitest. Auch bei einem normalverteilten Outcome liefert dir {glmmTMB} auch \\(p\\)-Werte aus einer ANOVA. Es macht also doch Sinn sich mal andere Pakete anzuschauen.\nUm das Modell zu rechnen nutzen wir die Funktion glmmTMB() und der Rest bleibt glücklicherweise gleich. Wir ändern hier nur die Option family = nbinom1 und können dann einmal das Modell rechnen. Und ja, es gebe noch andere Möglichkeiten, aber wir bleiben hier mal bei einer. Am Ende kannst du dann auch verschiedene Familien durch testen und schauen, wo du den kleinsten Fehler am Ende erhälst. Dafür bietet sich ja das Paket {modelsummary} gerade an.\n\nglmmTMB_nbinom1_fit &lt;- glmmTMB(count_worms ~ trt + (1|row) + (1|col), \n                               data = wireworms_tbl, family = nbinom1) \n\nAuch hier können wir einmal den Intraclass Correlation Coefficient (abk. ICC) schätzen und sehen, dass nicht viel Varianz durch unser Modell erklärt wird. Das ist etwas bedauerlich, aber manchmal kann man nicht mehr aus den Daten herausholen.\n\nglmmTMB_nbinom1_fit |&gt; icc()\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.248\n  Unadjusted ICC: 0.111\n\n\nDann schauen wir nochmal in das Bestimmtheitsmaß \\(R^2\\) und sehen, dass wir auch hier eher bescheidene Werte erhalten. Wir können mit den festen und zufälligen Effekten zusammen nur \\(66.9\\%\\) der Varianz in den Wurmanzahlen erklären. Das ist auch hier kein guter Wert, aber wie immer besser als gar nichts.\n\nglmmTMB_nbinom1_fit |&gt; r2()\n\nWarning: mu of 4.7 is too close to zero, estimate of random effect variances may\n  be unreliable.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.662\n     Marginal R2: 0.551\n\n\nWir nehmen dann auch das Modell hier mit in den Vergleich und schauen einmal welches Modell das beste Modell ist. Wie immer kann ein komplexeres Modell zwar besser sein, aber am Ende wollen wir dann doch eher ein einfaches Modell haben.\n\n\n\nDann wollen wir uns mal die drei Modelle anschauen und entscheiden, welches der drei Modelle das beste Modell ist. Wir wissen aber schon, dass wir Overdispersion in den Daten vorliegen haben und deshalb keine einfache Poissonregression rechnen dürfen. Daher fällt das glmer()-Modell mit der Poissonfamilie aus der Betrachtung. Dann bleibt nur noch das reine fixe Effekt Modell in glm() oder eben das gemischte Modell aus glmmTMB() mit einer Quasipoissonverteilung übrig. Im folgenden Kasten findest du den Modellvergleich mit dem R Paket {modelsummary}. Leider liefert die Funktion glm() keine Bestimmtheitsmaße \\(R^2\\) für unser Modell, aber da haben wir ja oben händisch den Wert von \\(R^2 = 0.53\\) berechnet. Von den reinen Werten her, wäre sogar das glmer() das beste Modell, aber die statistischen Gütezahlen gelten nur, wenn es eben die Annahmen an das Modell auch passen. Und die Grundannahme an das glmer()-Modell ist eben, dass mit einer Poissonverteilung keine Overdispersion vorliegt. Somit ist dann tatsächlich unser glmmTMB()-Modell das Beste. Der RMSE ist klein, wie bei den anderen Modellen, und darüber hinaus haben wir dann noch relativ hohe Werte für das Bestimmtheitsmaße \\(R^2\\).\n\n\n\n\n\n\nModellvergleich mit modelsummary()\n\n\n\n\n\n\nmodelsummary(lst(\"glm quasipoisson\" = glm_quasipoisson_fit,\n                 \"glmer poisson\" = glmer_poisson_fit, \n                 \"glmmTMB nbinom1\" = glmmTMB_nbinom1_fit),\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\"))\n\n\n\n\n\n\nglm quasipoisson\nglmer poisson\nglmmTMB nbinom1\n\n\n\n\n(Intercept)\n−0.041\n0.083\n−0.037\n\n\n\n−0.041\n0.083\n0.678\n\n\n\n[−0.460, 0.349]\n[−0.311, 0.477]\n[−0.491, 0.417]\n\n\n\n[−0.460, 0.349]\n[−0.311, 0.477]\n[0.421, 1.091]\n\n\n\ns.e. = 0.206\ns.e. = 0.201\ns.e. = 0.232\n\n\ntrtM\n1.741\n1.728\n1.852\n\n\n\n[1.391, 2.120]\n[1.447, 2.010]\n[1.467, 2.237]\n\n\n\ns.e. = 0.185\ns.e. = 0.144\ns.e. = 0.196\n\n\ntrtN\n1.709\n1.715\n1.849\n\n\n\n[1.362, 2.086]\n[1.435, 1.995]\n[1.463, 2.235]\n\n\n\ns.e. = 0.184\ns.e. = 0.143\ns.e. = 0.197\n\n\ntrtO\n1.577\n1.575\n1.704\n\n\n\n[1.224, 1.959]\n[1.291, 1.858]\n[1.314, 2.093]\n\n\n\ns.e. = 0.187\ns.e. = 0.145\ns.e. = 0.199\n\n\ntrtP\n1.463\n1.450\n1.589\n\n\n\n[1.102, 1.850]\n[1.161, 1.738]\n[1.197, 1.981]\n\n\n\ns.e. = 0.190\ns.e. = 0.147\ns.e. = 0.200\n\n\nrow2\n0.022\n\n\n\n\n\n[−0.256, 0.302]\n\n\n\n\n\ns.e. = 0.142\n\n\n\n\nrow3\n0.387\n\n\n\n\n\n[0.131, 0.647]\n\n\n\n\n\ns.e. = 0.131\n\n\n\n\nrow4\n0.877\n\n\n\n\n\n[0.641, 1.121]\n\n\n\n\n\ns.e. = 0.122\n\n\n\n\nrow5\n0.423\n\n\n\n\n\n[0.164, 0.685]\n\n\n\n\n\ns.e. = 0.133\n\n\n\n\ncol2\n−0.322\n\n\n\n\n\n[−0.553, −0.094]\n\n\n\n\n\ns.e. = 0.117\n\n\n\n\ncol3\n−0.332\n\n\n\n\n\n[−0.566, −0.102]\n\n\n\n\n\ns.e. = 0.118\n\n\n\n\ncol4\n−0.272\n\n\n\n\n\n[−0.497, −0.047]\n\n\n\n\n\ns.e. = 0.115\n\n\n\n\ncol5\n−0.206\n\n\n\n\n\n[−0.439, 0.024]\n\n\n\n\n\ns.e. = 0.118\n\n\n\n\nSD (Intercept col)\n\n0.118\n0.099\n\n\n\n\n\n[0.032, 0.309]\n\n\nSD (Intercept row)\n\n0.318\n0.302\n\n\n\n\n\n[0.156, 0.588]\n\n\nNum.Obs.\n250\n250\n250\n\n\nR2 Marg.\n\n0.581\n0.551\n\n\nR2 Cond.\n\n0.736\n0.662\n\n\nAIC\n\n1189.9\n1156.5\n\n\nBIC\n\n1214.6\n1184.7\n\n\nICC\n\n0.4\n0.2\n\n\nF\n16.038\n\n\n\n\nRMSE\n2.67\n2.68\n2.69\n\n\n\n\n\n\n\n\n\n\n\nAbschließend überprüfen wir in der Abbildung 52.16 nochmal, ob auch alle Annahmen an das Modell stimmen. Hierzu nutzen wir dann wieder eine visuelle Überprüfung mit der Funktion check_model(). Das Modell sieht einigermaßen okay aus. Die visuelle Überprüfung der Overdispersion ist nicht so super, aber wir sehen ja auch in der obigen Analyse, dass wir eigentlich keine Overdisperion mehr in den Daten vorliegen haben. Bei nicht normalverteilten Outcomes ist die Einschätzung der Modellgüte manchmal etwas schwierig. Aber wir hier bei dem glmmTMB()-Modell, es ist das beste was wir haben. Mehr geben dann die Daten einfach nicht her.\n\n\n\n\n\n\nModellüberprüfung mit check_model()\n\n\n\n\n\n\ncheck_model(glmmTMB_nbinom1_fit)\n\n\n\n\n\n\n\nAbbildung 52.16— Überprüfung des Modells mit der Funktion check_model() aus dem R Paket {performance}. Eine Reihe von Annahmen an das Modell wird in verschiedenen Abbildungen visuell überprüft. Unter den Überschriften steht die Annahme an die Abbildung und wann die Annahme in der Überschrift als erfüllt gilt.\n\n\n\n\n\n\n\n\n\n\n52.4.4 … mit dem R Paket {multilevelmod}\nDas R Paket {parsnip} erlaubt verschiedene Modellierungen sehr schon zu vereinheitlichen. Wir nutzen die Idee auch sehr in den Kapitel zur Klassifikation. Hier stelle ich einmal das R Paket {parsnip} mit dem Fokus auf die lineare Regression vor. Leider ist {glmmTMB} nicht in dem Paket eingebaut, so dass wir hier wieder extra analysieren müssen. Aber gut, man kann nicht alles haben. Um den folgenden Prozess einmal durchlaufen zu lassen, brauchen wir noch das R Paket {multilevelmod}, welches uns erlaubt in {parsnip} dann auch lineare gemischte Modelle anzuwenden.\nIm Folgenden einmal als Beispiel die Weizendaten oats_tbl, die wir dann auch gleich nochmal in dem Gruppenvergleich nutzen. Hier als erstmal zwei lineare Regression. Wir rechnen jetzt einmal ein lineares gemischtes Modell und nutzen dafür die Funktion set_engine(\"lmer\"). Danach rechnen wir dann als Vergleich dazu ein simples lineares Modell mit set_engine(\"lm\"). Wir du siehst, ist die engine unabhängig vom Formelaufruf. Das heißt, wir können die engine jetzt immer wieder verwenden.\n\noats_lmer_spec &lt;- linear_reg() |&gt; \n  set_engine(\"lmer\")\noats_lm_spec &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n\nJetzt können wir mit der Funktion fit() verschiedene Modell anpassen. Ich habe mich jetzt für ein lm()-Modell sowie zwei lmer()-Modelle entschieden. Einmal ein lmer()-Modell mit Interaktionsterm und einmal ohne. Wir haben ja in der obigen Abbildung gesehen, dass wir eigentlich keine Interaktion zwischen den Behandlungen nitro und den Sorten gen vorliegen haben. Dann brauchen wir noch die Funktion extract_fit_engine() damit wir die Ausgabe der Funktion fit() auch für andere Pakete korrekt anwenden können.\n\noats_lm_fit &lt;- oats_lm_spec |&gt; \n  fit(plant_yield ~ nitro + gen + block, data = oats_tbl) |&gt; \n  extract_fit_engine()  \n\noats_lmer_fit &lt;- oats_lmer_spec |&gt; \n  fit(plant_yield ~ nitro + gen + (1|block/gen), data = oats_tbl) |&gt; \n  extract_fit_engine()  \n\noats_lmer_int_fit &lt;- oats_lmer_spec |&gt; \n  fit(plant_yield ~ nitro + gen + nitro:gen + (1|block/gen), data = oats_tbl) |&gt; \n  extract_fit_engine()  \n\nDamit sich hier nicht alles doppelt, einmal als Beispiel die Ausgabe des Intraclass Correlation Coefficient (abk. ICC) für das klassische lineare gemischte Modell. Wir sehen, dass die zufälligen Effekte sehr viel der Varianz in den Daten erklären. Das stimmt dann auch mit der Abbildung von oben überein. Die Blocke sind dort sehr variablen, was den Ertrag vom Weizen angeht.\n\noats_lmer_fit |&gt; icc()\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.749\n  Unadjusted ICC: 0.461\n\n\nUnd dann nochmal als Übersicht die Ergebnisse der Ausgabe der Funktion model_performance(). Auch das funktioniert wunderbar.\n\noats_lmer_fit |&gt; model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n----------------------------------------------------------------------------------\n6666.443 | 6666.654 | 6709.297 |      0.846 |      0.385 | 0.749 | 10.850 | 10.983\n\n\nIm folgenden Kasten findest du den Modellvergleich mit dem R Paket {modelsummary}. Auch hier sehen wir, dass das glmer-Modell mit und ohne Interaktion ungefähr den gleichen RMSE produziert. Der RMSE ist auch kleiner als beim lm()-Modell, deshalb ist das gemischte Modell vorzuziehen. Je kleiner das Modell ist, desto besser, deshalb wäre hier das Modell ohne Interaktion vorzuziehen. Da die Bestimmtheitsmaße \\(R^2\\) zwischen den gemischten Modellen auch ungefähr gleich sind, passt es auch hier. Den etwas geringeren ICC-Wert nehmen wir dann in kauf, wenn dafür unser Modell etwas simpler ist.\n\n\n\n\n\n\nModellvergleich mit modelsummary()\n\n\n\n\n\n\nmodelsummary(lst(\"lm\" = oats_lm_fit, \n                 \"lmer ohne Interaktion\" = oats_lmer_fit,\n                 \"lmer mit Interaktion\" = oats_lmer_int_fit),\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\"))\n\n\n\n\n\n\nlm\n lmer ohne Interaktion\n lmer mit Interaktion\n\n\n\n\n(Intercept)\n66.892\n79.855\n79.914\n\n\n\n[63.728, 70.055]\n[64.528, 95.181]\n[64.487, 95.340]\n\n\n\ns.e. = 1.612\ns.e. = 7.809\ns.e. = 7.860\n\n\nnitro0.2\n19.543\n19.543\n18.693\n\n\n\n[16.845, 22.241]\n[17.469, 21.617]\n[15.157, 22.230]\n\n\n\ns.e. = 1.375\ns.e. = 1.057\ns.e. = 1.802\n\n\nnitro0.4\n34.754\n34.754\n34.696\n\n\n\n[32.056, 37.452]\n[32.679, 36.828]\n[31.160, 38.233]\n\n\n\ns.e. = 1.375\ns.e. = 1.057\ns.e. = 1.802\n\n\nnitro0.6\n44.014\n44.014\n44.685\n\n\n\n[41.316, 46.712]\n[41.940, 46.089]\n[41.149, 48.222]\n\n\n\ns.e. = 1.375\ns.e. = 1.057\ns.e. = 1.802\n\n\ngenMarvellous\n5.294\n5.294\n6.776\n\n\n\n[2.958, 7.631]\n[−8.619, 19.208]\n[−7.471, 21.023]\n\n\n\ns.e. = 1.190\ns.e. = 7.089\ns.e. = 7.259\n\n\ngenVictory\n−6.580\n−6.580\n−8.239\n\n\n\n[−8.916, −4.244]\n[−20.493, 7.333]\n[−22.486, 6.008]\n\n\n\ns.e. = 1.190\ns.e. = 7.089\ns.e. = 7.259\n\n\nblockB2\n5.247\n\n\n\n\n\n[1.943, 8.552]\n\n\n\n\n\ns.e. = 1.683\n\n\n\n\nblockB3\n4.963\n\n\n\n\n\n[1.659, 8.267]\n\n\n\n\n\ns.e. = 1.683\n\n\n\n\nblockB4\n44.214\n\n\n\n\n\n[40.910, 47.518]\n\n\n\n\n\ns.e. = 1.683\n\n\n\n\nblockB5\n7.057\n\n\n\n\n\n[3.753, 10.362]\n\n\n\n\n\ns.e. = 1.683\n\n\n\n\nblockB6\n16.294\n\n\n\n\n\n[12.990, 19.599]\n\n\n\n\n\ns.e. = 1.683\n\n\n\n\nnitro0.2 × genMarvellous\n\n\n2.967\n\n\n\n\n\n[−2.035, 7.968]\n\n\n\n\n\ns.e. = 2.548\n\n\nnitro0.4 × genMarvellous\n\n\n−4.681\n\n\n\n\n\n[−9.682, 0.321]\n\n\n\n\n\ns.e. = 2.548\n\n\nnitro0.6 × genMarvellous\n\n\n−4.213\n\n\n\n\n\n[−9.214, 0.788]\n\n\n\n\n\ns.e. = 2.548\n\n\nnitro0.2 × genVictory\n\n\n−0.418\n\n\n\n\n\n[−5.419, 4.584]\n\n\n\n\n\ns.e. = 2.548\n\n\nnitro0.4 × genVictory\n\n\n4.853\n\n\n\n\n\n[−0.148, 9.855]\n\n\n\n\n\ns.e. = 2.548\n\n\nnitro0.6 × genVictory\n\n\n2.200\n\n\n\n\n\n[−2.801, 7.202]\n\n\n\n\n\ns.e. = 2.548\n\n\nSD (Observations)\n\n10.983\n10.810\n\n\nSD (Intercept genblock)\n\n12.175\n12.179\n\n\nSD (Intercept block)\n\n14.580\n14.580\n\n\nNum.Obs.\n864\n864\n864\n\n\nR2\n0.721\n\n\n\n\nR2 Adj.\n0.718\n\n\n\n\nR2 Marg.\n\n0.385\n0.391\n\n\nR2 Cond.\n\n0.846\n0.851\n\n\nAIC\n7059.9\n6666.4\n6625.8\n\n\nBIC\n7117.0\n6709.3\n6697.2\n\n\nICC\n\n0.7\n0.8\n\n\nLog.Lik.\n−3517.948\n\n\n\n\nRMSE\n14.19\n10.85\n10.64",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Lineare gemischte Modelle</span>"
    ]
  },
  {
    "objectID": "stat-modeling-mixed.html#sec-mult-comp-lmer-reg",
    "href": "stat-modeling-mixed.html#sec-mult-comp-lmer-reg",
    "title": "52  Lineare gemischte Modelle",
    "section": "52.5 Gruppenvergleich",
    "text": "52.5 Gruppenvergleich\nVielleicht bist du direkt aus dem Kapitel zu den multiplen Vergleichen hierher gesprungen. Wenn ja, dann musst du eventuell nochmal weiter oben anfangen zu lesen, denn hier geht es eher um die schnelle Variante, den Gruppenvergleich mit einem linearen gemischten Modell durchzuführen. Details stehen in dem Kapitel. Wir nutzen hier einmal das R Paket {lme4} zusammen mit dem R Paket {lmerTest}, welches uns dann auch erlaubt statistische Tests mit einem \\(p\\)-Wert zurechnen. Leider ist das in der ursprünglichen Version so nicht implementiert und deshalb müssen wir noch ein zusätzliches Paket laden. In dem R Paket {glmmTMB} funktioniert soweit alles wie es soll, nur das wir hier dann noch das R Paket {car} benötigen um die ANOVA zu rechnen. Wir ignorieren hier das Bestimmtheitsmaß oder die Modellgüte. Wir ziehen hier einmal stumpf den Prozess des Testens durch. Der Gruppenvergleich läuft dann wie gewohnt in {emmeans} ab. Mehr dazu dann auch in den beiden Anwendungsbeispielen in den grünen Kästen im Anschluss.\n\n\n\n\n\n\nWelches R Paket mit welchem Endpunkt?\n\n\n\nManchmal ist es etwas verwirrend, welches R Paket für die gemischten linearen Modellen mit welcher Funktion für welchen Endpunkt oder Outcome geeignet ist. Hier mal eine schnelle Übersicht, damit du dich orientieren kannst.\n\nWenn dein Outcome normalverteilt ist, wie zum Beispiel das Trocken- oder Frischgewicht, dann hilft die Funktion lmer() aus dem R Paket {lme4} weiter.\nWenn du Zähldaten hast dann nutzt die Funktion glmer() aus dem R Paket lme4 oder eben die Funktion glmmTMB() aus dem gleichnamigen R Paket mit der entsprechenden Option für die Verteilungsfamilie.\nWenn du Boniturdaten oder aber ein ordinales Outcome vorliegen hast, dann kannst du die Funktion clmm() aus dem R Paket {ordinal} nutzen. Du brauchst hier noch zusätzlich die Funktion Anova.clmm() aus dem R Paket {RVAideMemoire} um die ANOVA rechnen zu können.\n\nAlle drei Varianten zeige ich dir in den grünen Kästen zu den beispielhaften Anwendungen am Ende des Kapitels.\n\n\nIch zeige dir jetzt einmal in den beiden Tabs die Gruppenvergleiche für das R Paket {lme4} und der Funktion lmer() sowie dann den Gruppenvergleich mit dem R Paket {glmmTMB} und der gleichnamigen Funktion. Du kannst die Funktion lmer() ohne weiteres durch die Funktion glmer() ersetzen. Ich würde aber in dem Fall immer zu dem Paket {glmmTMB} wechseln. Für nicht-normalverteilte Outcomes ist {glmmTMB} einfach besser, da wir in dem Paket mehr Möglichkeiten haben die Daten zu modellieren. Wir nutzen hier als Beispiel die Weizendaten mit der Behandlung nitro sowie den Sorten gen und verschiedenen Blöcken im Split Plot Design. Wir wissen dass die Blöcke und die Sorten miteinander interagieren und packen deshalb diese genestete Struktur nochmal in den zufälligen Effekt mit (1|block/gen). Wir brauchen hier den Interaktionsterm, damit wir anschließend in {emmeans} uns alle Faktorkombinationen anzeigen lassen können. Jedenfalls ist es dann so einfacher und wir verlieren nichts.\n\nGruppenvergleich mit lmer()Gruppenvergleich mit glmmTMB()\n\n\nJetzt müssen wir die Funktion lmer() aus dem R Paket {lmerTest} nutzen, damit wir dann später die ANOVA mit \\(p\\)-Werten erhalten. Ich nutze das Paket {lmerTest} sehr punktuell, deshalb lade ich mit dem Aufruf lmerTest::lmer() nur einmal die Funktion lmer() aus dem Paket. Wenn ich sonst lmer() nutze, ist es wieder die Funktion aus dem Paket {lme4}. Sonst kriegt man auch Probleme mit dem R Paket {conflicted}, was zurecht möchte, dass wir uns für eine Funktion aus einem Paket entscheiden. Wenn wir das haben, dann können wir wie gewohnt das Modell anpassen.\n\nlmer_fit &lt;- lmerTest::lmer(plant_yield ~ nitro + gen + nitro:gen + (1|block/gen), data = oats_tbl)\n\nUnd schon können wir die ANOVA rechnen und erhalten dann auch über die Funktion model_parameters() eine schönere Ausgabe wieder. Obwohl es in unserer Abbildung nicht so aussah, haben wir eine signifikante Interaktion in unseren Daten vorliegen. Wir werden daher dann die Daten getrennt einmal auswerten. Auch um ein wenig Platz zu sparen.\n\nlmer_fit |&gt; \n  anova() |&gt; \n  model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |      F |      p\n------------------------------------------------------------\nnitro     |    2.40e+05 |  3 |    79973.41 | 684.31 | &lt; .001\ngen       |      329.19 |  2 |      164.59 |   1.41 | 0.289 \nnitro:gen |     3870.16 |  6 |      645.03 |   5.52 | &lt; .001\n\nAnova Table (Type 3 tests)\n\n\nEinmal die paarweisen Vergleich für unsere Behandlung nitro. Wir sehen, dass alle Vergleiche signifikant sind. Das haben wir aber schon aus der obigen Abbildung erwartet.\n\nlmer_fit |&gt; \n  emmeans(~ nitro) |&gt; \n  pairs()\n\n contrast            estimate   SE  df t.ratio p.value\n nitro0 - nitro0.2     -19.54 1.04 837 -18.787  &lt;.0001\n nitro0 - nitro0.4     -34.75 1.04 837 -33.409  &lt;.0001\n nitro0 - nitro0.6     -44.01 1.04 837 -42.312  &lt;.0001\n nitro0.2 - nitro0.4   -15.21 1.04 837 -14.622  &lt;.0001\n nitro0.2 - nitro0.6   -24.47 1.04 837 -23.525  &lt;.0001\n nitro0.4 - nitro0.6    -9.26 1.04 837  -8.902  &lt;.0001\n\nResults are averaged over the levels of: gen \nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nDann die Vergleiche für die Sorten. Hier haben wir dann keinen signifikanten Unterschied mehr vorliegen. Auch das überrascht uns jetzt nicht sehr. Auch in der obigen Abbildung haben sich die Sorten kaum Unterschiede was den Ertrag anging.\n\nlmer_fit |&gt; \n  emmeans(~ gen) |&gt; \n  pairs()\n\n contrast                estimate   SE df t.ratio p.value\n GoldenRain - Marvellous    -5.29 7.09 10  -0.747  0.7423\n GoldenRain - Victory        6.58 7.09 10   0.928  0.6358\n Marvellous - Victory       11.87 7.09 10   1.675  0.2614\n\nResults are averaged over the levels of: nitro \nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nAbschließend machen wir dann noch das compact letter display fertig. Hier dann mit allen Faktorkombinationen von der Stickstoffdüngung nitro und den Sorten gen, wenn du nitro * gen durchlaufen lässt. Für mich ist die Ausgabe dann zu groß, deshalb hier nur die Anwendung für nitro getrennt für alle Sorten.\n\nlmer_fit |&gt; \n  emmeans(~ nitro | gen) |&gt; \n  cld(Letters = letters)\n\ngen = GoldenRain:\n nitro emmean   SE   df lower.CL upper.CL .group\n 0       79.9 7.86 9.27     62.2     97.6  a    \n 0.2     98.6 7.86 9.27     80.9    116.3   b   \n 0.4    114.6 7.86 9.27     96.9    132.3    c  \n 0.6    124.6 7.86 9.27    106.9    142.3     d \n\ngen = Marvellous:\n nitro emmean   SE   df lower.CL upper.CL .group\n 0       86.7 7.86 9.27     69.0    104.4  a    \n 0.2    108.3 7.86 9.27     90.6    126.1   b   \n 0.4    116.7 7.86 9.27     99.0    134.4    c  \n 0.6    127.2 7.86 9.27    109.5    144.9     d \n\ngen = Victory:\n nitro emmean   SE   df lower.CL upper.CL .group\n 0       71.7 7.86 9.27     54.0     89.4  a    \n 0.2     90.0 7.86 9.27     72.2    107.7   b   \n 0.4    111.2 7.86 9.27     93.5    128.9    c  \n 0.6    118.6 7.86 9.27    100.9    136.3     d \n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nUnd damit sind wir dann auch schon mit dem Gruppenvergleich mit lmer() durch. Die ANOVA und das compact letter display lässt sich schnell und einfach erstellen. Wir müssen eben nur darauf achten, dass wir das richtige Helferpaket mit {lmerTest} für die Modellierung des linearen gemischten Modell nutzen.\n\n\nDann rechnen wir nochmal flux das lineare gemischte Modell mit der Funktion glmmTMB(). Eigentlich ist alles genauso wie in dem andern Tab mit der Funktion lmer(). Wir brauchen hier aber keine Helferfunktionen. Die Funktion glmmTMB() kann das Modell so schätzen, dass wir auch eine ANOVA rechnen können. Wir müssen aber nur gleiche eine spezielle Implementierung wählen, aber das ist auch kein Problem.\n\nglmmTMB_fit &lt;- glmmTMB(plant_yield ~ nitro + gen + nitro:gen + (1|block/gen), data = oats_tbl)\n\nUnd schon können wir die ANOVA rechnen und erhalten dann auch über die Funktion model_parameters() eine schönere Ausgabe wieder. Hier müssen wir die Funktion Anova() aus dem R Paket {car} nutzen. Obwohl es in unserer Abbildung nicht so aussah, haben wir eine signifikante Interaktion in unseren Daten vorliegen. Wir werden daher dann die Daten getrennt einmal auswerten. Auch um ein wenig Platz zu sparen.\n\nglmmTMB_fit |&gt; \n  car::Anova() |&gt; \n  model_parameters()\n\nParameter |    Chi2 | df |      p\n---------------------------------\nnitro     | 2075.01 |  3 | &lt; .001\ngen       |    3.38 |  2 | 0.185 \nnitro:gen |   33.47 |  6 | &lt; .001\n\nAnova Table (Type 2 tests)\n\n\nEinmal die paarweisen Vergleich für unsere Behandlung nitro. Wir sehen, dass alle Vergleiche signifikant sind. Das haben wir aber schon aus der obigen Abbildung erwartet. Also auch hier nichts anderes zu dem anderen Tab.\n\nglmmTMB_fit |&gt; \n  emmeans(~ nitro) |&gt; \n  pairs()\n\n contrast            estimate   SE  df t.ratio p.value\n nitro0 - nitro0.2     -19.54 1.03 849 -18.888  &lt;.0001\n nitro0 - nitro0.4     -34.75 1.03 849 -33.588  &lt;.0001\n nitro0 - nitro0.6     -44.01 1.03 849 -42.538  &lt;.0001\n nitro0.2 - nitro0.4   -15.21 1.03 849 -14.701  &lt;.0001\n nitro0.2 - nitro0.6   -24.47 1.03 849 -23.651  &lt;.0001\n nitro0.4 - nitro0.6    -9.26 1.03 849  -8.950  &lt;.0001\n\nResults are averaged over the levels of: gen \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nDann die Vergleiche für die Sorten. Hier haben wir dann keinen signifikanten Unterschied mehr vorliegen. Auch das überrascht uns jetzt nicht sehr. Auch in der obigen Abbildung haben sich die Sorten kaum Unterschiede was den Ertrag anging.\n\nglmmTMB_fit |&gt; \n  emmeans(~ gen) |&gt; \n  pairs()\n\n contrast                estimate   SE  df t.ratio p.value\n GoldenRain - Marvellous    -5.29 6.47 849  -0.818  0.6919\n GoldenRain - Victory        6.58 6.47 849   1.017  0.5664\n Marvellous - Victory       11.87 6.47 849   1.835  0.1589\n\nResults are averaged over the levels of: nitro \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nAbschließend machen wir dann noch das compact letter display fertig. Für mich ist die Ausgabe dann zu groß, wenn ich mit nitro * gen alle Faktorkombinationen anzeigen lasse, deshalb hier nur die Anwendung für nitro getrennt für alle Sorten.\n\nglmmTMB_fit |&gt; \n  emmeans(~ nitro | gen) |&gt; \n  cld(Letters = letters)\n\ngen = GoldenRain:\n nitro emmean   SE  df lower.CL upper.CL .group\n 0       79.9 7.19 849     65.8     94.0  a    \n 0.2     98.6 7.19 849     84.5    112.7   b   \n 0.4    114.6 7.19 849    100.5    128.7    c  \n 0.6    124.6 7.19 849    110.5    138.7     d \n\ngen = Marvellous:\n nitro emmean   SE  df lower.CL upper.CL .group\n 0       86.7 7.19 849     72.6    100.8  a    \n 0.2    108.3 7.19 849     94.2    122.5   b   \n 0.4    116.7 7.19 849    102.6    130.8    c  \n 0.6    127.2 7.19 849    113.1    141.3     d \n\ngen = Victory:\n nitro emmean   SE  df lower.CL upper.CL .group\n 0       71.7 7.19 849     57.6     85.8  a    \n 0.2     90.0 7.19 849     75.8    104.1   b   \n 0.4    111.2 7.19 849     97.1    125.3    c  \n 0.6    118.6 7.19 849    104.5    132.7     d \n\nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nAuch hier sehen wir dann die Unterschiede in den einzelnen Sorten. Alle sind soweit signifikant. Damit sind wir auch mit der Implementierung von {glmmTMB} durch. Die ANOVA und das compact letter display lässt sich schnell und einfach erstellen.\n\n\n\nIn den folgenden grünen Kästen schauen wir uns jetzt drei Anwenundungsbeispiele für einen Gruppenvergleich mit linearen gemischten Modellen an. Zum einen betrachten wir ein dreifaktorieller Gruppenvergleich für das Gewicht von Brokkoliköpfen und zum anderen ein weiterer dreifaktorieller Gruppenvergleich für Thripsenbefall. Dann betrachten wir noch ein ordinales Modell mit den Boniturnoten bei Schweinen. Alle Auswertungen sind vollständig und haben auch teilweise erklärenden Text. Im Zweifel musst du dann nochmal in diesem Kapitel nachschlagen, wenn dir Teile unklar sind.\n\n\n\n\n\n\nAnwendungsbeispiel: Gruppenvergleich für Brokkolikopfgewicht (3-faktoriell)\n\n\n\nIm folgenden Beispiel schauen wir uns nochmal ein praktische Auswertung von einem agrarwissenschaftlichen Beispiel mit Brokkoli an. Wir haben uns in diesem Experiment verschiedene Dosen fert_amount von einem Dünger aufgebracht sowie verschiedene Zeitpunkte der Düngung fert_time berücksichtigt. Auch hier haben wir einige Besonderheiten in den Daten, da nicht jede Faktorkombination vorliegt. Wir ignorieren aber diese Probleme und rechnen einfach stumpf unseren Gruppenvergleich und ignorieren die fehlenden Faktorkombinationen. Wir haben aber mit den Gewicht ein normalverteiltes Outcome vorliegen und haben die Pflanzen in Blöcken randomisiert. Leider kennen wir nicht das experimentelle Design und müssen daher etwas blind modellieren.\n\nbroc_tbl &lt;- read_excel(\"data/broccoli_weight.xlsx\") |&gt; \n  mutate(fert_time = factor(fert_time, levels = c(\"none\", \"early\", \"late\")),\n         fert_amount = as_factor(fert_amount),\n         block = as_factor(block)) |&gt;\n  select(-stem_hollowness) \n\nJetzt wollen wir einmal das Modell auftellen. Wir nehmen die beiden Behandlungen als festen Effekt mit einem Interaktionsterm in das gemischte Modell. Den Block nehmen wir dann als zufälligen Effekt mit rein. Wir nutze hier dann das R Paket {lmerTest} um das lineare gemischte Modell zu rechnen, da wir dann besser die Gruppenvergleiche und die ANOVA rechnen können. Mit der Funktion asu dem R Paket {lme4} kriegen wir aus der ANOVA keine \\(p\\)-Werte.\n\nlmer_fit &lt;- lmerTest::lmer(weight ~ fert_time + fert_amount + fert_time:fert_amount + (1 | block), \n                           data = broc_tbl) \n\nDann berechnen wir auch einmal die \\(R^2\\)-Werte für unser Modell. Hier gibt es dann nochmal mehr Informationen auf \\(R^2\\) for Mixed Models – Marginal vs. Conditional. Wichtig ist, dass das Conditional R2, die gesamte erklärte Varianz von festen und zufälligen Effekten beschriebt und das Marginal R2 nur die erklärte Varianz aus den festen Effekten. Tja, das Modell ist nun echt nicht gerade das beste Modell nach dem Bestimmtheitsmaß. Die festen Effekte, also unsere Behandlungen, erklären kaum etwas von der Varianz. Aber schauen wir mal weiter.\n\nlmer_fit |&gt; r2()\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.246\n     Marginal R2: 0.081\n\n\nDer Intraclass Correlation Coefficient (ICC) ist dann das Bestimmtheitsmaß \\(R^2\\) für die zufälligen Effekte. Auch hier sehen wir eher maue Werte. Wir haben anscheinend sehr viel Varianz in den Daten, die wir nicht durch unsere Faktoren erklären können.\n\nlmer_fit |&gt; icc()\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.180\n  Unadjusted ICC: 0.165\n\n\nMit der Funktion model_performance() können wir uns dann nochmal alles zusammen anschauen. Viele der Werte sind nur wichtig, wenn wir Modelle miteinander vergleichen. Das Bestimmtheitsmaß \\(R^2\\) als einzelner Wert ist da schon wichtiger.\n\nlmer_fit |&gt; \n  model_performance()\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |    RMSE |   Sigma\n---------------------------------------------------------------------------------------\n20281.884 | 20282.001 | 20330.022 |      0.246 |      0.081 | 0.180 | 165.876 | 166.410\n\n\nJetzt rechnen wir in den beiden folgenden Tabs einmal die ANOVA und dann auch den multiplen Gruppenvergleich mit {emmeans}. Da wir hier normalveteilte Daten haben, können wir dann einfach die Standardverfahren nehmen. Eventuell müssten wir bei dem Gruppenvergleich mit emmeans() nochmal für die multiplen Vergleiche adjustieren, aber da erfährst du dann mehr in dem Kapitel zu den Multiple Vergleichen oder Post-hoc Tests. Bei den gemischten Modellen müssen wir nicht für Varianzheterogenität adjustieren, das machen dann die Modelle intern schon.\n\nANOVA mit anova()Gruppenvergleich mit emmeans()\n\n\nWir rechnen hier einmal die ANOVA und erhalten Dank der Nutzung des R Paketes {lmerTest} auch die \\(p\\)-Werte. Wenn du nicht das R Paket {lmerTest} nutzt, wird per Standardeinstellung kein Test gerechnet. Wir machen dann die Ausgabe nochmal schöner und fertig sind wir.\n\nlmer_fit |&gt; \n  anova() |&gt; \n  model_parameters()\n\nParameter             | Sum_Squares | df | Mean_Square |     F |      p\n-----------------------------------------------------------------------\nfert_time             |    2.89e+06 |  2 |    1.44e+06 | 52.11 | &lt; .001\nfert_amount           |    2.13e+06 |  2 |    1.07e+06 | 38.48 | &lt; .001\nfert_time:fert_amount |    27123.13 |  2 |    13561.57 |  0.49 | 0.613 \n\nAnova Table (Type 3 tests)\n\n\nWir sehen, dass der Effekt der Düngerzeit und die Menge des Düngers signifikant ist, jedoch wir keinen signifikanten Einfluss durch die Interaktion haben. Wir haben aber also keine Interaktion vorliegen. Wir sehen hier den Block nicht mehr, da der Effekt des Blocks in den zufälligen Effekt ist und hier nicht mehr gezeigt wird. Für die ANOVA der zufälligen Effekte können wir dann die Funktion ranova() nutzen.\n\nlmer_fit |&gt; \n  ranova() |&gt; \n  model_parameters(drop = \"&lt;none&gt;\")\n\nParameter   |      AIC | Log_Likelihood | df |      p\n-----------------------------------------------------\n(1 | block) | 20503.99 |      -10243.99 |  8 | &lt; .001\n\nAnova Table (Type 1 tests)\n\n\nNun können wir auch erkennen, dass wir einen signifikanten Effekt des Blocks haben. Du kannst aber die Ausgabe der zufälligen ANOVA nicht mit der obigen ANOVA vergleichen. Wir erhalten aus der ANOVA keine Informationen über die erklärte Varianz. Diese Information erhalten wir dann oben aus der Funktion icc(). Dann können wir auch in dem anderem Tab mit dem Gruppebvergleich in {emmeans} weitermachen.\n\n\nIm Folgenden rechnen wir einmal über alle Faktorkombinationen von fert_time und fert_amount einen Gruppenvergleich. Dafür nutzen wir die Option fert_time * fert_amount. Wenn du die Analyse getrennt für die Menge und den Zeitpunkt durchführen willst, dann nutze die Option fert_time | fert_amount. Dann adjustieren wir noch nach Bonferroni und sind schon fertig.\n\nemm_obj &lt;- lmer_fit |&gt; \n  emmeans(~ fert_time * fert_amount) |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\")\nemm_obj\n\n fert_time fert_amount emmean   SE   df lower.CL upper.CL .group\n none      0              169 51.7 9.08    -9.82      347  a    \n late      150            400 40.5 3.44   167.50      633   b   \n early     150            432 40.5 3.42   198.20      665   bc  \n late      225            467 40.5 3.44   234.22      700    cd \n early     225            497 39.7 3.18   250.86      743     de\n late      300            506 40.6 3.46   273.96      738     de\n early     300            517 40.6 3.45   285.45      750      e\n early     0           nonEst   NA   NA       NA       NA       \n late      0           nonEst   NA   NA       NA       NA       \n none      150         nonEst   NA   NA       NA       NA       \n none      225         nonEst   NA   NA       NA       NA       \n none      300         nonEst   NA   NA       NA       NA       \n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 7 estimates \nP value adjustment: bonferroni method for 21 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nDas emm_obj Objekt werden wir dann gleich einmal in {ggplot} visualisieren. Die emmean stellt den mittleren Gewicht des Brokkoli je Faktorkombination dar unter Berücksichtigung der Varianz der Blöcke als zufälligen Effekt. Du siehst ja auch, dass wir hier keine homogen Varianzen \\(SE\\) haben. Dann können wir zum Abschluss auch das compact letter display anhand der Abbildung interpretieren. Einige der Vergleiche konnten nicht gerechnet werden, da wir die Faktorkombination nicht in den Daten vorliegen haben.\n\n\n\nIn der Abbildung 52.17 siehst du das Ergebnis der Auswertung in einem Säulendiagramm und in der Abbildung 52.18 die Ergebnisse in einem Dot-Boxplot. Wir sehen einen klaren Effekt der Düngezeitpunkte sowie der Düngermenge auf das Gewicht von Brokkoli. Wenn wir ein mittleres Gewicht von \\(500g\\) für den Handel erreichen wollen, dann erhalten wir das Zielgewicht nur bei einer Düngemenge von \\(300mg/l\\). Hier stellt sich dann die Frage, ob wir bei \\(225mg/l\\) und einem frühen Zeitpunkt der Düngung nicht auch genug Brokkoli erhalten. Das Ziel ist es ja eigentlich in einen Zielbereich zu kommen. Die Köpfe sollen ja nicht zu schwer und auch nicht zu leicht sein. Aber diese Frage und andere Fragen der biologischen Anwendung lassen wir dann hier einmal offen, denn das Beispiel soll ja nur ein Beispiel sein. Besonders spannend ist hier der Unterschied in den beiden Abbildungen. Schau dir dazu einmal die beiden verschiedenen Tabs an.\n\nBarplotsDotplots und Boxplots\n\n\nIm Folgenden einmal die Barplots mit den Mittelwerten und dem Standardfehler. Hier sieht es so aus, also ob die Düngezeitpunkte relativ verlässlich ein Zielgewicht von \\(500g\\) im Mittel erreichen würden. Das stimmt auch, wenn wir uns auf den Mittelwert beziehen. Auch der Standardfehler ist relativ klein, da wir ja so viele Beobachtungen vorliegen haben. Schau dir dazu dann einmal den Dot-Boxplot in dem anderen Tab an.\n\nemm_obj |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = fert_time, y = emmean, fill = fert_amount)) +\n  theme_minimal() + \n  labs(y = \"Mittleres Gewicht [g] des Brokkoli\", x = \"Düngezeitpunkt\",\n       fill = \"Düngemenge [mg/l]\") +\n  scale_y_continuous(breaks = seq(0, 500, by = 100)) +\n  geom_hline(yintercept = 500, size = 0.75, linetype = 2) +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_text(aes(label = .group, y = emmean + SE + 0.01),  \n            position = position_dodge(width = 0.9), vjust = -0.25) +\n  geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  theme(legend.position = \"top\") +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 52.17— Säulendigramm der mittleren Brokkoligewichte aus einem linearen gemischten Modell. Das lmer()-Modell berechnet das mittler Gewicht des Brokkoli in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre. Wir haben noch ein Mindestgewicht von 500g ergänzt.\n\n\n\n\n\n\n\nAls erstes müssen wir nochmal das emm_obj in einen tibble umwandeln, damit wir dann das compact letter display direkt in die Grafik setzen können. Wir entfernen dann noch alle Faktorkombinationen, die wir nicht brauchen, damit wir dann auch gleich die Positionen richtig setzen können. Wichtig ist auch, dass die Faktoren richtig sortiert sind, sonst passt es gleich nicht mit dem Zufügen des compact letter display zu der Position.\n\nemm_cld_tbl &lt;- emm_obj |&gt; \n  as_tibble() |&gt; \n  arrange(fert_time, fert_amount) |&gt; \n  na.omit()\n\nDie Positionen für das compact letter display berechnen wir uns aus dem Wert des \\(3^{rd}\\) Quantile. Dann ergänzen wir noch die Werte für das compact letter display und fertig sind wir. Aber auch hier lieber einmal doppelt schauen, dass wir auch wirklich die richtigen Buchstaben zu den richtigen Faktorkombinationen ergänzt haben.\n\ncld_pos_tbl &lt;- broc_tbl |&gt; \n  group_by(fert_time, fert_amount) |&gt; \n  summarise(quant_3rd = quantile(weight, 0.75)) |&gt; \n  add_column(group = emm_cld_tbl$.group)\n\nJetzt stellen wir den Dot-Boxplot einmal in der folgenden Abbildung dar. Dazu nutzen wir das R Paket {gghalves} und die entsprechenden Funktionen. Wir müssen hier relativ viel machen, damit der Plot gut aussieht. Aber das meiste ist einfach nur Code wiederverwendet. Spannend ist hier die Variabilität in den Daten. Wir sehen hier, das unsere Brokkoliköpfe bei weitem nicht so nahe an dem Zielbereich sind, wie die Barplots suggerieren. Wir wollen eigentlich ziemlich eng um die \\(500g\\) sein, sonnst muss der Supermarkt wild packen um immer die gleiche Packungsgröße zu haben. Jetzt wirken die Unterschiede zwischen den Zeitpunkten und den Mengen auch schon wieder etwas anders. Auch mit weniger Dünger können wir relativ große Brokkoli erhalten. Das plazieren der Buchstaben wäre vermutlich mit annotate() einfacher gewesen.\n\nggplot(broc_tbl, aes(x = fert_time, y = weight, fill = fert_amount)) + \n  labs(y = \"Mittleres Gewicht [g] des Brokkoli\", x = \"Düngezeitpunkt\",\n       fill = \"Düngemenge [mg/l]\") +\n  geom_hline(yintercept = 500, size = 0.75, linetype = 2) +\n  geom_half_point(aes(color = fert_amount), \n                  transformation = position_quasirandom(width = 0.05),\n                  side = \"l\", size = 0.5, alpha = 0.5,\n                  position = position_dodge(width = 0.9)) +\n  geom_half_boxplot(aes(fill = fert_amount), \n                    side = \"r\", width = 0.5, outlier.size = 0.8,\n                    position = position_dodge(width = 0.9)) + \n  geom_text(data = cld_pos_tbl, aes(label = str_trim(group), y = quant_3rd + 70),\n            position = position_dodge(width = 0.9), hjust = -1) +\n  scale_fill_okabeito() + scale_color_okabeito() +\n  guides(color = \"none\") + theme(legend.position = \"top\") \n\n\n\n\n\n\n\nAbbildung 52.18— Dotplots und Boxplots der mittleren Brokkoligewichte aus einem linearen gemischten Modell. Das lmer()-Modell berechnet das mittler Gewicht des Brokkoli in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir haben noch ein Mindestgewicht von 500g ergänzt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnwendungsbeispiel: Gruppenvergleich für Thripsenbefall (3-faktoriell)\n\n\n\nIm folgenden Beispiel schauen wir uns nochmal ein praktische Auswertung von einem agrarwissenschaftlichen Beispiel mit jungen Apfelbäumen an. Wir haben uns in diesem Experiment verschiedene Dosen trt von einem Insektizid aufgebracht sowie verschiedene Startanzahlen von Raubmilben als biologische Alternative untersucht. Dann haben wir noch fünf Zeitpunkte bestimmt, an denen wir die Anzahl an Thripsen auf den Blättern gezählt haben. Wir haben nicht die Blätter per se gezählt sondern Fallen waagerecht aufgestellt. Dann haben wir geschaut, wie viele Thripsen wir über above und unter below von den Fallen gefunden haben. In unserem Fall beschränken wir uns auf die obere Anzahl an Thripsen und schauen uns auch nur die Behandlung mit dem Insektizid an.\n\ninsects_tbl &lt;- read_excel(\"data/insects_count.xlsx\") |&gt; \n  mutate(timepoint = factor(timepoint, labels = c(\"1 Tag\", \"4 Tag\", \"7 Tag\", \"11 Tag\", \"14 Tag\")),\n         rep = as_factor(rep),\n         trt = as_factor(trt)) |&gt;\n  select(timepoint, trt, rep, thripse = thripse_above, mite = mite_above) |&gt; \n  filter(trt %in% c(\"10ml\", \"30ml\", \"60ml\"))\n\nWarum bietet sich hier ein lineares gemischtes Modell an? Wir haben hier eine Wiederholung vorliegen, die wir dann als zufälligen Effekt modellieren können. Wir können daher die Poissonregression mit glmer() rechnen. Auch hier wieder darauf achten, dass wir dann als Option family = poisson wählen. Es hängt jetzt davon ab, ob du in deinen Daten Overdispersion vorliegen hast oder nicht, ob wir glmer() nutzen können. In den beiden folgenden Tabs, rechne ich dann mal beide Modelle. Einmal ohne die Berücksichtigung von Overdispersion mit glmer() aus dem R Paket {lme4} und einmal mit Berücksichtigung der Overdispersion mit glmmTMB() aus dem gleichnamigen R Paket.\n\nglmer() mit family = poissonglmmTMB() mit family = nbinom1\n\n\nAls Erstes rechnen wir eine normale Poissonregression mit glmer() und schauen einmal, ob wir Overdispersion vorliegen haben. Wenn wir Overdispersion vorliegen haben, dann können wir keine Poissonregression rechnen, sondern müssen auf eine Quasipoisson Regression ausweichen. Das ist aber sehr einfach, wie du im anderen Tab sehen wirst.\n\ninsects_poisson_fit &lt;- glmer(thripse ~ trt + timepoint + trt:timepoint + (1 | rep), \n                             data = insects_tbl, \n                             family = poisson) \n\nBevor wir uns das Modell mit summary() überhaupt anschauen, wollen wir erstmal überprüfen, ob wir überhaupt Overdispersion vorliegen haben. Wenn ja, dann können wir uns die summary() hier gleich sparen und wir können dann mit dem Modell auch keine Gruppenvergleiche rechnen. Also einmal geguckt, was die Overdispersion macht.\n\ninsects_poisson_fit |&gt; check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =   18.796\n  Pearson's Chi-Squared = 2518.668\n                p-value =  &lt; 0.001\n\n\nOverdispersion detected.\n\n\nWir haben sehr starke Overdispersion vorliegen und gehen daher in den anderen Tab und rechnen eine Quasipoisson Regression mit dem R Paket {glmmTMB}. Nur wenn du keine Overdispersion vorliegen hast, dann kannst du eine eine Poissonregression mit glmer() rechnen.\n\n\nDa wir in {lme4} keine Quasipoissonverteilung auswählen können, nutzen wir das R Paket {glmmTMB} mit der Verteilungsfamilie nbinom1, was einer Parametrisierung einer Quasipoissonverteilung entspricht. Meistens nutze ich gleich die glmmTMB() Funktion, da wir in biologischen Daten sehr häufig Overdispersion vorliegen haben. Dann kann man sich eine Runde gleich sparen. Aber am Ende kannst du dann auch die Modelle miteinander vergleichen und auch berichten wie groß deine Overdispersion ist.\n\ninsects_quasipoisson_fit &lt;- glmmTMB(thripse ~ trt + timepoint + trt:timepoint + (1|rep), \n                                    data = insects_tbl,\n                                    family = nbinom1) \n\nIm Folgedne schauen wir nochmal, ob wir in unserem neuen Modell in glmmTMB() noch Overdispersion vorliegen haben.\n\ninsects_quasipoisson_fit |&gt; check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =   1.014\n  Pearson's Chi-Squared = 134.823\n                p-value =    0.44\n\n\nNo overdispersion detected.\n\n\nDas passt soweit. Wir haben keine Overdispersion mehr. Dann berechnen wir auch einmal die \\(R^2\\)-Werte für unser Modell. Hier gibt es dann nochmal mehr Informationen auf \\(R^2\\) for Mixed Models – Marginal vs. Conditional. Wichtig ist, dass das Conditional R2, die gesamte erklärte Varianz von festen und zufälligen Effekten beschriebt und das Marginal R2 nur die erklärte Varianz aus den festen Effekten.\n\ninsects_quasipoisson_fit |&gt; r2()\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.407\n     Marginal R2: 0.176\n\n\nTja, das Modell ist nun echt nicht gerade das beste Modell nach dem Bestimmtheitsmaß. Die festen Effekte, also unsere Behandlungen, erklären kaum etwas von der Varianz. Aber schauen wir mal weiter. Der Intraclass Correlation Coefficient (ICC) beschreibt nun die erklärte Varianz durch unsere zufälligen Effekte.\n\ninsects_quasipoisson_fit |&gt; icc()\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.280\n  Unadjusted ICC: 0.230\n\n\nAuch hier sind die Ergebnisse eher mau aus. Du kannst dir die gesamte Übersicht auch gleich in der Funktion model_performance() wiedergeben lassen. Das spart dann ein wenig Zeit und Code.\n\ninsects_quasipoisson_fit |&gt; \n  model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma | Score_log | Score_spherical\n----------------------------------------------------------------------------------------------------------------\n1282.606 | 1287.242 | 1333.786 |      0.407 |      0.176 | 0.280 | 23.535 | 21.910 |    -4.414 |           0.063\n\n\nWir haben hier auf jeden Fall Overdispersion vorliegen. Daher nutze ich dann auch das Modell hier in glmmTMB() mit der Annahme an eine Quasipoissonverteilung. Dann stimmt es auch mit unseren Varianzen und wir produzieren nicht zufällig zu viele signifikante Ergebnisse, die es dann gar nicht gibt.\n\n\n\nIch habe mich gerade in den obigen Tabs für eine Quasipoissonregression in einem linearen gemischten Modell mit glmmTMB() entschieden, da wir Overdispersion vorliegen haben. Damit mache ich dann mit dem insects_quasipoisson_fit Modell weiter. In den beiden folgenden Tabs findest du dann einmal das Ergebnis für die ANOVA und einmal für den Gruppenvergleich mit dem R Paket {emmeans}. Bitte beachte, dass die ANOVA für ein glmmTMB()-Objekt nicht ganz gleich wie für ein lm()-Objekt ist. Du kannst aber die ANOVA erstmal ganz normal interpretieren, nur haben wir hier nicht die Möglichkeit ein \\(\\eta^2\\) zu bestimmen. Dann nutzen wir {emmeans} für den Gruppenvergleich. Nochmal, weil wir Overdispersion festgestellt haben, nutzen wir das Objekt insects_quasipoisson_fit mit der Berücksichtigung der Overdispersion.\nIm Folgenden rechnen wir einmal über alle Faktorkombinationen von trt und timepoint einen Gruppenvergleich. Dafür nutzen wir die Opition trt * timepoint. Wenn du die Analyse getrennt für die Zeitpunkte durchführen willst, dann nutze die Option trt | timepoint. Wir wollen die Wahrscheinlichkeiten für das Auftreten einer Beschädigung von wiedergegeben bekommen, deshalb die Option regrid = \"response. Dann adjustieren wir noch nach Bonferroni und sind fertig.\n\nemm_obj &lt;- insects_quasipoisson_fit |&gt; \n  emmeans(~ trt * timepoint, type = \"response\") |&gt;\n  cld(Letters = letters, adjust = \"bonferroni\")\nemm_obj\n\n trt  timepoint response    SE  df asymp.LCL asymp.UCL .group\n 60ml 4 Tag         10.6  3.76 Inf      3.72      30.0  a    \n 60ml 14 Tag        10.6  4.00 Inf      3.50      32.1  ab   \n 60ml 11 Tag        15.7  5.34 Inf      5.80      42.6  abc  \n 30ml 11 Tag        19.3  6.42 Inf      7.28      51.3  abc  \n 60ml 7 Tag         19.7  6.30 Inf      7.69      50.4  abc  \n 30ml 14 Tag        20.0  6.42 Inf      7.82      51.3  abc  \n 60ml 1 Tag         20.0  6.18 Inf      8.11      49.5  abc  \n 30ml 4 Tag         20.8  6.55 Inf      8.25      52.5  abc  \n 30ml 1 Tag         22.7  7.19 Inf      8.93      57.5  abc  \n 10ml 4 Tag         23.3  7.03 Inf      9.64      56.5  abc  \n 10ml 14 Tag        30.8  8.52 Inf     13.70      69.4  abc  \n 30ml 7 Tag         32.0  9.02 Inf     14.00      73.2  abc  \n 10ml 1 Tag         40.3 10.29 Inf     19.03      85.2   bc  \n 10ml 7 Tag         40.7 10.48 Inf     19.09      86.7   bc  \n 10ml 11 Tag        43.6 10.83 Inf     21.06      90.4    c  \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 15 estimates \nIntervals are back-transformed from the log scale \nP value adjustment: bonferroni method for 105 tests \nTests are performed on the log scale \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nDas emm_obj Objekt werden wir dann gleich einmal in {ggplot} visualisieren. Die response stellt die mittlere Anzahl an Thripsen je Faktorkombination dar. Dann können wir auch das compact letter display anhand der Abbildung interpretieren. Ich stelle die Abbildung wieder einmal als Barplot und einmal als Dot-Boxplot in den beiden Tabs dar.\n\nBarplotsDotplots und Boxplots\n\n\nIn der Abbildung 52.19 siehst du das Ergebnis der Auswertung in einem Säulendiagramm. Hier unbedingt SE als den Standardfehler für die Fehlerbalken nutzen, da wir sonst Fehlerbalken größer und kleiner als \\(0\\) erhalten, wenn wir die Standardabweichung nutzen würden. Das ist in unserem Fall nicht so das Problem, aber wenn du eher kleine Anzahlen zählst, kann das schnell zu Werten kleiner Null führen. Wir sehen einen klaren Effekt der Behandlung 60ml. Die Zeit hat keinen Effekt, was ja schon aus der ANOVA klar war, die Säulen sehen für jeden Zeitpunkt vollkommen gleich aus. Gut etwas Unterschied ist ja immer.\n\nemm_obj |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = timepoint, y = response, fill = trt)) +\n  theme_minimal() + \n  labs(y = \"Mittlere Anzahl an Thripsen\", x = \"Messzeitpunkte der Zählungen\",\n       fill = \"Dosis\") +\n  geom_bar(stat = \"identity\", width = 0.8, \n           position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_text(aes(label = str_trim(.group), y = response + SE + 1), size = 3,  \n            position = position_dodge(width = 0.9), vjust = -0.25) +\n  geom_errorbar(aes(ymin = response-SE, ymax = response+SE),\n                width = 0.2,  \n                position = position_dodge(width = 0.9, preserve = \"single\")) +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 52.19— Säulendigramm der mittleren Zahl der Thripsen aus einer Quasipoissonregression mit einem gemischten Modell. Das glmmTMB()-Modell berechnet die mittlere Anzahl in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert.\n\n\n\n\n\n\n\nDamit wir uns gleich einmal die Daten in einem Dotplot und Boxplot anschauen können, brauchen wir noch die Positionen für das compact letter display in unser Abbildung. Daher brauchen wir erstmal das compact letter display und entfernen alle Faktorkombinationen, die keine Werte haben.\n\nemm_cld_tbl &lt;- emm_obj |&gt; \n  as_tibble() |&gt; \n  arrange(timepoint, trt)\n\nJetzt berechnen wir uns noch die Position. Ich setze hier das compact letter display auf die Position des dritten Quartils, was ausreichen sollte und nicht zu weit weg ist. Damit haben wir dann die Buchstaben nahe genug an den Boxen der Boxplots.\n\ncld_pos_tbl &lt;- insects_tbl |&gt; \n  group_by(timepoint, trt) |&gt; \n  summarise(quant_3rd = quantile(thripse, 0.75)) |&gt; \n  add_column(group = emm_cld_tbl$.group)\n\nIn der Abbildung 52.20 siehst du dann einmal das Ergebnis des Dot-Boxplot. Im Gegensatz zu dem anderen Beispiel sind die Daten hier etwas übersichtlicher was die Dotplots angeht. Dennoch sieht man hier teilweise sehr viel mehr. Zum einen wird die große Variabilität in den Daten klar. Teilweise sind die Unterschiede nur durch wenige Beobachtungen zu erklären, die die mittleren Raten nach oben ziehen. So sind in bei allen Dosen immer wieder sehr wenige Thrispen zu beobachten. Die Unterschiede kommen dann von wenigen großen Zählungen. Wenn diese ausbleiben, dann haben wir eher einen Unterschied. Die Aussagekraft von den Dot-Boxplots sind auf jeden Fall sehr viel höher als bei den einfachen Barplots.\n\nggplot(insects_tbl, aes(x = timepoint, y = thripse, fill = trt)) + \n  labs(y = \"Mittlere Anzahl an Thripsen\", x = \"Messzeitpunkte der Zählungen\",\n       fill = \"Dosis\") +\n  geom_half_point(aes(color = trt), \n                  transformation = position_quasirandom(width = 0.05),\n                  side = \"l\", size = 0.5, alpha = 0.5,\n                  position = position_dodge(width = 0.9, preserve = \"single\")) +\n  geom_half_boxplot(aes(fill = trt), \n                    side = \"r\", width = 0.5, outlier.size = 0.8,\n                    position = position_dodge(width = 0.9, preserve = \"single\")) + \n  geom_text(data = cld_pos_tbl, aes(label = group, y = quant_3rd + 5),\n            position = position_dodge(width = 0.9), hjust = 0) +\n  scale_fill_okabeito() + scale_color_okabeito() +\n  guides(color = \"none\") + theme(legend.position = \"top\") \n\n\n\n\n\n\n\nAbbildung 52.20— Dotplots und Boxplots der mittleren Zahl der Thripsen aus einer Quasipoissonregression mit einem gemischten Modell. Das glmmTMB()-Modell berechnet die mittlere Anzahl in jeder Faktorkombination. Durch die Darstellung in Dotplot und Boxplot ist die große Heterogenität in den Daten zu sehen. Das compact letter display wird dann in {emmeans} generiert.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnwendungsbeispiel: Gruppenvergleich für Boniturnoten von Schweinen (3-faktoriell)\n\n\n\nIn dem folgenden agrawissenschaftlichen Beispiel wollen wir uns einmal die Auswertung von verschiedenen Schweinerassen im Bezug auf ihre Fußgesundheit in zwei Platzangeboten in Ställen anschauen. Die Ställe haben dabei zwei Arten von Belegung vorliegen, nämlich einmal eine weite Belegung und einmal eine enge Belegung. Darüber hinaus ist die Studie in verschiedenen Ställen und und verschiedenen Orten durchgeführt worden. Es ergibt sich also ein recht komplexer Datensatz. Dabei sind die Ställe barn auf den Höfe location genestet. Darüber hinaus haben wir eine Behandlung spacing sowie die verschiedenen Schweinerassen breed. Ich habe hier nur eine Auswahl an Schweinerassen getroffen, da mir sonst das clmm()-Modell zu lange dauert bis es gerechnet ist. Wir brauchen dann später für unser clmm()-Modell das Outcome noch als geordneten Faktor. Wenn du den Code nutzt, dann kannst du gerne alle Schweinerassen mit in das Modell nehmen. Ebenfalls hilfreich ist das Tutorium Ordinal regression in R: part 1 auf das ich gerne verweisen möchte. Am Ende haben wir einen sehr großen Datensatz mit 2840 bonitierten Schweinen. Das sind wirklich eine Menge Schweine.\n\npig_tbl &lt;- read_excel(\"data/bonitur_pig.xlsx\") |&gt; \n  mutate(breed = as_factor(breed),\n         spacing = factor(spacing, labels = c(\"Weite Belegung\", \"Enge Belegung\")),\n         location = as_factor(location),\n         barn = as_factor(barn),\n         foot_inflamed_fct = ordered(foot_inflamed)) |&gt; \n  filter(breed %in% c(\"Vietnamese Pot-bellied\", \"Welsh\", \"Belarus Black Pied\", \"Saddleback\", \n                     \"American Yorkshire\", \"Bentheim Black Pied\", \"Mangalica\")) |&gt; \n  na.omit()\n\nDie Daten sehen wir dann auch nochmal in der Abbildung 52.21 als Violinplot visualisiert. Wir sehen, dass wir bei der Anzahl an Beobachtungen dann nichts mehr sehen. Die Violinplots mögen sich unterscheiden, aber so richtig gut erkennen, können wir hier eigentlich nicht. Auch lassen sich hier schwer halbe Abbildungen erstellen. Wir haben teilweise so viele Beobachtungen mit geringen Noten, dass wir kaum etwas erkennen können.\n\nggplot(pig_tbl, aes(breed, foot_inflamed, fill = breed)) +\n  theme_minimal() +\n  geom_violin() +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9)) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ spacing, ncol = 1) +\n  scale_fill_okabeito() +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  labs(x = \"Schweinerasse\", y = \"Boniturnote\")\n\n\n\n\n\n\n\nAbbildung 52.21— Violinplot der Boniturnoten für die Fußgesundheit der Schweinerassen aufgeteilt nach dem Platzangebot. Klar ist die sehr schiefe Verteilung der Boniturnoten zu erkennen.\n\n\n\n\n\nWir rechnen jetzt eine ordinale gemischte Regression mit der Funktion clmm(). Wir schreiben das Modell wie wir es schon kennen mit zwei festen und einem genesteten zufälligen Effekt. Wir betrachten auch die Interaktion zwischen den Schweinerassen und dem Platzangebot. Mich würde wundern wenn alle Schweinerassen auf das Platzangebot gleich reagieren würden. Dann müssen wir noch sichergehen das unser Outcome als Faktor mit der Spalte foot_inflamed_fct kodiert ist, damit wir das Modell rechnen können.\n\nclmm_fit &lt;- clmm(foot_inflamed_fct ~ breed + spacing + breed:spacing + (1 | location/barn), \n                 data = pig_tbl)\n\nWir nutzen das Modell clmm_fit direkt für den Gruppenvergleich und übergehen einmal die Modelldiagnostik hier. Jetzt rechnen wir in den beiden folgenden Tabs einmal die ANOVA und dann auch den multiplen Gruppenvergleich mit {emmeans}. Da wir hier ordinale Daten haben, können wir dann nicht einfach die Standardverfahren nehmen. Wir entscheiden uns dann für den Standardfehler bei der Darstellung.\n\nANOVA mit anova()Gruppenvergleich mit emmeans()\n\n\nWir rechnen hier einmal die ANOVA mit der Funktion Anova.clmm() aus dem R Paket {RVAideMemoire}. In dem Paket {ordinal} ist keine ANOVA für den Gruppenvergleich implementiert. Wir machen dann die Ausgabe nochmal schöner und fertig sind wir.\n\nclmm_fit |&gt; \n  Anova.clmm() |&gt;\n  model_parameters()\n\nParameter     |   Chi2 | df |      p\n------------------------------------\nbreed         | 158.00 |  6 | &lt; .001\nspacing       | 184.31 |  1 | &lt; .001\nbreed:spacing |  30.62 |  6 | &lt; .001\n\nAnova Table (Type 2 tests)\n\n\nWir sehen, dass die Schweinerasse signifikant ist. Ebenso hat der Abstand in den Ställen einen signifikanten Einfluss auf die Fußgesundheit. Wie es auch zu erwarten war, verhalten sich natürlich nicht alle Schweinerassen identisch im Bezug auf das Platzangebot und der Fußgesundheit. Es wäre auch sehr komisch, wenn wir hier keine Interaktion gehabt hätten. Wir werten also gleich mal die Boniturnoten getrennt für das Platzangebot aus. Auch hier müssen wir uns Fragen was ist mehr von Interesse, ich entscheide mich hier für die Rassen.\n\n\nIm Folgenden rechnen wir einmal für den Faktor breed getrennt für die beiden Platzangebote spacing einen Gruppenvergleich. Immerhin haben wir eine Interaktion vorliegen. Wir setzen hier die Option mean.class damit wir dann die mittleren Noten wiedergegeben bekommen. Mit den mittleren Noten können wir dann ein Säulendiagramm erstellen. Dann adjustieren wir nicht und sind auch schon fertig. Warum adjustieren wir hier nicht? Wenn du hier adjustierst ist am Ende nichts mehr signifikant, da du so viele Gruppenvergleiche rechnest, das hier die \\(p\\)-Werte nicht klein genug werden. Irgendwo ist dann auch eine Grenze. Die Effekte sind hier nicht stark genug sein um sie dann noch signifikant nachzuweisen.\n\nemm_obj &lt;- clmm_fit |&gt; \n  emmeans(~ breed | spacing, mode = \"mean.class\") |&gt;\n  cld(Letters = letters, adjust = \"none\")\nemm_obj\n\nspacing = Weite Belegung:\n breed                  mean.class     SE  df asymp.LCL asymp.UCL .group\n American Yorkshire           1.10 0.0843 Inf     0.933      1.26  a    \n Mangalica                    1.13 0.1044 Inf     0.922      1.33  a    \n Saddleback                   1.19 0.1486 Inf     0.895      1.48  a    \n Welsh                        1.19 0.1516 Inf     0.892      1.49  a    \n Vietnamese Pot-bellied       1.20 0.1574 Inf     0.889      1.51  a    \n Belarus Black Pied           1.52 0.3692 Inf     0.795      2.24  a    \n Bentheim Black Pied          1.82 0.5319 Inf     0.777      2.86  a    \n\nspacing = Enge Belegung:\n breed                  mean.class     SE  df asymp.LCL asymp.UCL .group\n American Yorkshire           1.16 0.1326 Inf     0.903      1.42  ab   \n Mangalica                    1.63 0.4327 Inf     0.782      2.48  a    \n Welsh                        2.04 0.6361 Inf     0.793      3.29  ab   \n Saddleback                   2.11 0.6683 Inf     0.798      3.42  abc  \n Bentheim Black Pied          2.41 0.7771 Inf     0.884      3.93   bcd \n Vietnamese Pot-bellied       2.76 0.8871 Inf     1.017      4.49    cd \n Belarus Black Pied           2.84 0.9110 Inf     1.053      4.62     d \n\nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nDas emm_obj Objekt werden wir dann gleich einmal in {ggplot} visualisieren. Die mean.class stellt den mittleren Noten der Fußgesundheit für die Menge die Rassen getrennt nach dem Platzangebot dar. Wir nutzen dann auch hier die Standardfehler für die Abbildungen, da wir sonst schnelle Werte kleiner 0 und größer 9 erhalten. Dann können wir zum Abschluss auch das compact letter display anhand der Abbildung interpretieren.\n\n\n\nIn der Abbildung 52.22 siehst du das Ergebnis der Auswertung in einem Säulendiagramm. In unserem Fall ist die weite Belegung nicht signifikant über die Rassen. Bei der engen Belegung sehen wir schon einige Unterschiede in der Benotung der Fußgesundheit. Die große Frage bleibt hier, ist der Effekt den wir sehen überhaupt relevant oder nicht? Immerhin haben wir sehr geringe mittlere Boniturnoten, die eventuell darüber hinwegtäuschen wie stark der Effekt eigentlich ist. Als Faustformel sage ich mir immer, dass ich eigentlich immer einen Notenschritt als Änderung sehen möchte. Sonst ist es eigentlich zu wenig um von einer Relevanz zu sprechen. Aber das ist eher eine Diskussion im Kontext der wissenschaftlichen Fragestellung als eine statistische Diskussion.\n\nemm_obj |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = breed, y = mean.class, fill = breed)) +\n  theme_minimal() + \n  labs(y = \"Mittlere Boniturnote\", x = \"Schweinerasse\") +\n  geom_bar(stat = \"identity\", position = position_dodge(0.9)) +\n  geom_text(aes(label = str_trim(.group), y = mean.class + SE + 0.01), vjust = -0.25,\n            position = position_dodge(0.9)) +\n  geom_errorbar(aes(ymin = mean.class-SE, ymax = mean.class+SE),\n                width = 0.2, position = position_dodge(0.9)) +\n  facet_wrap(~ spacing, ncol = 1) +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  ylim(0, 4)\n\n\n\n\n\n\n\nAbbildung 52.22— Säulendigramm der mittleren Boniturnote der Fußgesundheit aus einer ordinalen gemischten Regression. Das clmm()-Modell berechnet das mittlere Boniturnote für den Faktor breed getrennt für die beiden Platzangebote spacing. Das compact letter display wird dann in {emmeans} generiert. Fraglich bleibt, ob die Unterschiede wirklich relevant sind, da die mittleren Boniturnoten sehr nahe beieinander liegen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Lineare gemischte Modelle</span>"
    ]
  },
  {
    "objectID": "stat-modeling-mixed.html#referenzen",
    "href": "stat-modeling-mixed.html#referenzen",
    "title": "52  Lineare gemischte Modelle",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 52.5— Dreifachplot der Testergebnisse der Schüler zusammen über alle drei Klassen in den jeweiligen neun Schulen aufgetrennt nach dem Lehrformat. Teilweise sind starke Effekte der Schulen auf die Testergebnisse der Schüler zu erkennen.\nAbbildung 52.6— Dreifachplot der Testergebnisse der Schüler zusammen über alle Schulen in den jeweiligen drei Klassen aufgetrennt nach dem Lehrformat. Die Effekte der einzelnen Klassen sind nicht so stark ausgeprägt. Der Effekt der Schulen scheint in diesem Fall stärker zu sein.\nAbbildung 52.7— Boxplot der Testergebnisse der drei Lehrformate aufgeteilt nach den individuellen Klassen. Da immer nur drei Klassen pro Schule erhoben wurden, bilden die individuellen Klassen auch den Effekt der Schulen mit ab.\nAbbildung 52.8— Visualisierung des experimentellen Designs des Split plots für die Weizendaten. In sechs Blöcken wurden die drei Sorten aufgebracht. In jeder Sorte wurden wiederum die Sticktoffmengen randomisiert. Es kiegt keine echte Spaltanlage vor, da die Subplots innberhalb der Blöcke quadratisch angeordnet sind.\nAbbildung 52.9— Betrachtung der Auswirkungen der Sorte gen des Weizens auf den Ertrag, aufgeteilt nach den Blöcken. Einige Blöcke haben klar mehr Ertrag als andere Blöcke, wie auch schon bei den Stickstoffdüngungen.\nAbbildung 52.10— Betrachtung der Auswirkungen der Stickstoffdüngung nitro des Weizens auf den Ertrag, aufgeteilt nach den Blöcken. Einige Blöcke haben klar mehr Ertrag als andere Blöcke, wie auch schon bei den Sorten.\nAbbildung 52.11— Betrachtung der Auswirkungen der Stickstoffdüngung nitro des Weizens auf den Ertrag, aufgeteilt nach den Blöcken und den Sorten des Weizens. Klar ist zu erkennen, dass einige Sorten in einigen Blöcken klar mehr Ertrag haben.\nAbbildung 52.12 (a)— Interaktion nitro:gen\nAbbildung 52.12 (b)— Interaktion nitro:gen:block\nAbbildung 52.12 (c)— Interaktion nitro:block:gen\nAbbildung 52.13— Latinsquare Design der Insektizidbehandlung auf einem \\(5 \\times 5\\) großen Versuchsfeld. Jede Behandlung ist genau einmal in jeder Reihe und jeder Spalte vertreten. Die Buchstaben sind willkürlich gewählt.\nAbbildung 52.14 (a)— Spalte (col)\nAbbildung 52.14 (b)— Reihe (row)\nAbbildung 52.15— Überprüfung des Modells mit der Funktion check_model() aus dem R Paket {performance}. Eine Reihe von Annahmen an das Modell wird in verschiedenen Abbildungen visuell überprüft. Unter den Überschriften steht die Annahme an die Abbildung und wann die Annahme in der Überschrift als erfüllt gilt.\nAbbildung 52.16— Überprüfung des Modells mit der Funktion check_model() aus dem R Paket {performance}. Eine Reihe von Annahmen an das Modell wird in verschiedenen Abbildungen visuell überprüft. Unter den Überschriften steht die Annahme an die Abbildung und wann die Annahme in der Überschrift als erfüllt gilt.\nAbbildung 52.17— Säulendigramm der mittleren Brokkoligewichte aus einem linearen gemischten Modell. Das lmer()-Modell berechnet das mittler Gewicht des Brokkoli in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre. Wir haben noch ein Mindestgewicht von 500g ergänzt.\nAbbildung 52.18— Dotplots und Boxplots der mittleren Brokkoligewichte aus einem linearen gemischten Modell. Das lmer()-Modell berechnet das mittler Gewicht des Brokkoli in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert. Wir haben noch ein Mindestgewicht von 500g ergänzt.\nAbbildung 52.19— Säulendigramm der mittleren Zahl der Thripsen aus einer Quasipoissonregression mit einem gemischten Modell. Das glmmTMB()-Modell berechnet die mittlere Anzahl in jeder Faktorkombination. Das compact letter display wird dann in {emmeans} generiert.\nAbbildung 52.20— Dotplots und Boxplots der mittleren Zahl der Thripsen aus einer Quasipoissonregression mit einem gemischten Modell. Das glmmTMB()-Modell berechnet die mittlere Anzahl in jeder Faktorkombination. Durch die Darstellung in Dotplot und Boxplot ist die große Heterogenität in den Daten zu sehen. Das compact letter display wird dann in {emmeans} generiert.\nAbbildung 52.21— Violinplot der Boniturnoten für die Fußgesundheit der Schweinerassen aufgeteilt nach dem Platzangebot. Klar ist die sehr schiefe Verteilung der Boniturnoten zu erkennen.\nAbbildung 52.22— Säulendigramm der mittleren Boniturnote der Fußgesundheit aus einer ordinalen gemischten Regression. Das clmm()-Modell berechnet das mittlere Boniturnote für den Faktor breed getrennt für die beiden Platzangebote spacing. Das compact letter display wird dann in {emmeans} generiert. Fraglich bleibt, ob die Unterschiede wirklich relevant sind, da die mittleren Boniturnoten sehr nahe beieinander liegen.\n\n\n\nBates D, Mächler M, Bolker B, Walker S. 2014. Fitting linear mixed-effects models using lme4. arXiv preprint arXiv:1406.5823.\n\n\nRoback P, Legler J. 2021. Beyond multiple linear regression: applied generalized linear models and multilevel models in R. CRC Press.\n\n\nSalinas Ruı́z J, Montesinos López OA, Hernández Ramı́rez G, Crossa Hiriart J. 2023. Generalized Linear Mixed Models with Applications in Agriculture and Biology.\n\n\nWinter B. 2013. Linear models and linear mixed effects models in R with linguistic applications. arXiv preprint arXiv:1308.5499.\n\n\nZuur AF, Ieno EN, Walker NJ, Saveliev AA, Smith GM, others. 2009. Mixed effects models and extensions in ecology with R. Springer.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Lineare gemischte Modelle</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html",
    "href": "stat-modeling-gee.html",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "",
    "text": "53.1 Annahmen an die Daten\nIm folgenden Kapitel zu den Generalized Estimating Equations (GEE) gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\nGrundsätzlich ist das Thema GEE eher ein stiefmütterliches statistisches Thema. Ich selber habe gar nicht so viel zu GEE’s gefunden, so dass wie immer gilt: Augen auf im statistischen Straßenverkehr! Besonders die Variablenselektion, die ja an die Modellklasse gebunden ist, mag nicht so funktionieren wie gewollt. Bitte bei GEE Fragestellungen keine automatisierte Selektion anwenden. Dann lieber über compare_models() aus dem R Paket {parameters} die Modellvergleiche direkt vergleichen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html#annahmen-an-die-daten",
    "href": "stat-modeling-gee.html#annahmen-an-die-daten",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "",
    "text": "Wenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 43 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 41 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 42 bei der Variablenselektion.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html#genutzte-r-pakete",
    "href": "stat-modeling-gee.html#genutzte-r-pakete",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "53.2 Genutzte R Pakete",
    "text": "53.2 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom,\n               parameters, performance, geepack, gee,\n               geesmv, multcomp, emmeans, scales, conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(magrittr::set_names)\nconflicts_prefer(dplyr::filter)\n#cbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n#                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html#daten",
    "href": "stat-modeling-gee.html#daten",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "53.3 Daten",
    "text": "53.3 Daten\n\n\n\n\n\n\nDu musst deine Daten nach der ID sortieren\n\n\n\nGanz wichtig, sonst funktioniert das GEE nicht und du kriegst auch keine Warnmeldung! Du musst die Daten mit arrange für deine ID Spalte sortieren.\n\n\nIch habe hier einmal zwei Datenbeispiel mitgebracht. Wir werden uns aber im folgenden Abschnitt dann nur die Schweine anschauen, das Kuhbeispiel können wir dann nochmal anderweitig nutzen oder aber du rechnest nochmal selber mit den Kühen. Wichtig hierbei ist, dass wir sicher sind, dass wir die Daten nach der ID Spalte der Tiere sortiert haben. Das heist, dass alle Tier ID’s Zeilen wirklich beieinander stehen. Das ist wichtig, sonst schafft GEE nur eine sehr seltsame Ausgaben zu produzieren. Leider ohne eine Warnung auszugeben. Deshalb nutzen wir die Funktion arrange() um nochmal nach der Spalte pig_id zu sortieren.\n\npig_gain_tbl &lt;- read_excel(\"data/pig_feed_data.xlsx\") |&gt; \n  mutate(weight_gain = round(weight_gain, 2)) |&gt; \n  arrange(pig_id)\n\nIn Tabelle 53.1 sehen wir nochmal einen Auszug aus den Daten. Wir haben unsere wiederholte Messung time. Das heißt wir haben unsere Schweine wiederholt gemessen. Jedes Schwein für jede Behandlung fünfmal. Wir brauchen die pig_id um zu wissen, welche Werte der Geichtszunahme dann auch immer zu einem Ferkel gehören. Im Weiteren haben wir noch die Bucht, in der die Ferkel gehalten wurden, notiert. Die Information lassen wir aber hier erstmal im späteren Modell weg.\n\n\n\n\nTabelle 53.1— Auszug aus dem Daten zu den kranken Ferkeln. Jedes Ferkel wurde wiederholt gemessen.\n\n\n\n\n\n\ntime\npig_id\ncove\ntreatment\nweight_gain\n\n\n\n\n1\n1\n1\nfeed_10\n44.29\n\n\n2\n1\n1\nfeed_10\n59\n\n\n3\n1\n1\nfeed_10\n49.96\n\n\n4\n1\n1\nfeed_10\n66.17\n\n\n…\n…\n…\n…\n…\n\n\n2\n120\n10\nfeed_20\n56.17\n\n\n3\n120\n10\nfeed_20\n58.13\n\n\n4\n120\n10\nfeed_20\n68.48\n\n\n5\n120\n10\nfeed_20\n34.99\n\n\n\n\n\n\n\n\nDas zweite Datenbeispiel dient zur Veranschaulichung eines weiteres Messwiederholungsbeispiels. Wir haben drei Kühe wiederholt an drei Zeitpunkten gemessen. JEde Kuh hat immer nur die gleiche Behandlung erhalten. Das Outcome ist einmal die Anzahl an Zellen in der Milch pro ml und einmal der Fettgehalt in %. Die Daten sind in der Form relativ übersichtlich. Wir haben leider sehr wenige Messwiederholungen, so dass hier ein GEE oder aber auch ein lineares gemischtes Modell fraglich ist. Wir wollen eigentlich mindesnten fünf Level für den Clusterfaktor. Wir gehen wieder sicher, dass die Daten auch richtig nach ID sortiert sind.\n\nmilk_tbl &lt;- read_csv2(\"data/milk_feeding.csv\") |&gt; \n  rename(cow_id = id_cow) |&gt; \n  arrange(cow_id)\n\nIn Tabelle 53.2 sehen wir nochmal den Ausschnitt aus den Milchdaten. Wir haben insgesamt auch nur vierzehn Kühe gemessen, was auch nicht so viele Tiere sind. Im Ferkelbeispiel hatten wir uns 120 Ferkel angeschaut. Deshal ist dieser Datensatz sehr klein für ein komplexes Modell wie GEE.\n\n\n\n\nTabelle 53.2— Auszug aus Daten zu Milchkühen. Jede Kuh wurde wiederholt gemessen.\n\n\n\n\n\n\ncow_id\ntrt\ntime_point\ncell_count\nfat_perc\n\n\n\n\n1\n1\n1\n1932\n0.69\n\n\n1\n1\n2\n6771\n0.94\n\n\n1\n1\n3\n2225\n0.01\n\n\n2\n0\n1\n2572\n0.15\n\n\n…\n…\n…\n…\n…\n\n\n13\n1\n3\n8445\n0.06\n\n\n14\n1\n1\n19707\n0.95\n\n\n14\n1\n2\n9428\n0.5\n\n\n14\n1\n3\n8184\n0.96\n\n\n\n\n\n\n\n\nGehen wir einmal auf den theoretischen Hintergrund zu GEE ein und schauen wir mal, wie wir da unser Datenbeispiel zu passt.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html#theoretischer-hintergrund",
    "href": "stat-modeling-gee.html#theoretischer-hintergrund",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "53.4 Theoretischer Hintergrund",
    "text": "53.4 Theoretischer Hintergrund\nGanz wichtig, wir gehen jetzt nicht auf den mathematischen Hintergurnd ein. Das ist auch zu schräg. Das will heißen, dass der mathematische Hintergrund von GEE’s wirklich vieles übersteigt. Ich kann GEE’s anwenden, aber ich weis nicht, wie ein GEE mathematisch funktioniert. Das muss man ja auch nicht. Deshalb hier nur die Theorie, was ein GEE macht und in welchen Hintergründen wir das GEE anwenden. Zuerst schätzt das GEE die durchschnittlichen Auswirkungen auf die Population (eng. population average). Betrachten wir dabei die folgenden zwei Szenarien nach Allison (2009):\n\nSzenario 1: Du bist ein Arzt. Du möchtest wissen, um wie viel ein Cholesterinmedikament die Wahrscheinlichkeit eines Herzinfarkts bei deinem Patienten senkt.\nSzenario 2: Du bist ein staatlicher Gesundheitsbeamter. Du möchtest wissen, wie sich die Zahl der Menschen, die an einem Herzinfarkt sterben, verändern würde, wenn alle Menschen in der Risikogruppe das Cholesterinmedikament einnehmen würden.\n\nIm ersten Szenario wollen wir die subjektspezifischen (eng. subject-specific) Chancen wissen. Im zweiten Fall sind wir an der Vorhersage für die gesamte Bevölkerung interessiert. GEE kann uns Schätzungen für das zweite, aber nicht für das erste Szenario liefern. Dami sind wir schon recht weit. Wir wollen also nichts über die einzelnen Ferkel wissen, sondern nur über die Gesamtzahl an Ferklen mitteln. Das ist natürlich manachmal gewollt und manchmal eher nicht. In der Zucht kommt es drauf an, ob du individuelle Effekte haben möchtest, also für einen Eber oder eben die Leistung der gesamten Rasse bewerten willst. Je nachdem kannst du dan ein GEE einsetzen oder nicht. GEE’s sind somit für einfaches Clustering oder wiederholte Messungen gedacht. Komplexere Designs wie verschachtelte oder gekreuzte Gruppen, z. B. verschachtelte Messwiederholungen innerhalb eines Probanden oder einer Gruppe, können nicht ohne weiteres berücksichtigt werden. Hier nutzen wir dann wieder gemischte lineare Modelle.\nEin großer Vorteil der GEE ist, dass wir eine Korrelation zwischen den wiederholten Messungen, also Ferkeln, annehmen können. Das heist, wir können die Verwandtschaft oder den zeitlichen Zusammenhang zwischend den Messwiederholungen abbilden. Dafür brauchen wir dann natürlich auch Fallzahl, die schnell mal über die hundert Beobachtungen geht. Wir können dann zwischen folgenden Zusammenhängen der Korrelation entscheiden.\n\nindependence, daher sind die Beobachtungen im Zeitverlauf sind unabhängig.\nexchangeable, daher haben alle Beobachtungen im Zeitverlauf die gleiche Korrelation \\(\\rho_{const.}\\).\nar1, die Korrelation \\(\\rho\\) nimmt als Potenz der Anzahl \\(p\\) der Zeitpunkte, die zwischen zwei Beobachtungen liegen, ab. Daher rechnen wir mit \\(\\rho, \\rho^2, \\rho^3,..., \\rho^p\\) über die Zeitpunkte.\nunstructured, daher kann die Korrelation zwischen allen Zeitpunkten unterschiedlich sein.\n\nLeider gibt es keine automatische Auswahl. Wir müssen also überlegen, welche Korrelationmatrix am besten passen würde. Da unstructured sehr viel Fallzahl benötigt um valide zu sein, nehme ich meistens exchangeable, wenn ich ein GEE rechne. Eine unabhänige Korrealtion anzunehmen macht wenig Sinn, dann brauche ich auch kein GEE rechnen. Die Korrelation ist ja die Stärke von einem GEE.\nWir haben nun zwei R Pakete, die beide das gleiche tun, nämlich ein GEE rechnen. Wir haben die Wahl zwischen dem R Paket gee und der Funktion gee() sowie der Funktion geeglm() aus dem R Paket {geepack}. Ich neige zu dem letzteren Paket. Das R Paket {geepack} ist etwas neueren Ursprungs und funktioniert bisher recht reibungslos.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html#modellieren-mit-gee",
    "href": "stat-modeling-gee.html#modellieren-mit-gee",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "53.5 Modellieren mit gee()",
    "text": "53.5 Modellieren mit gee()\nLeider ist es so, dass wir kaum kontrollieren können, was alles aus den Funktionen in die R Console geschrieben wird. Die Funktionen sind schon recht alt und es gab mal einen Trend, dass eine Funktion immer schön was wiedergeben soll. Das ist natürlich immer etwas nervig, wenn man das nicht will. Wir erhalten also bei der Funktion gee() immer die Koeffizienten des Modells ausgegeben, ob wir es nun in einem Objekt speichern oder auch nicht. Ich finde sowas immer ziemlich nervig.\nAlso wir bauen wir uns unser gee Modell? Zuerst kommt wie immer die formula, da ändert sich nichts. Wir nehmen in unser Modell als Outcome die Gewichtszunahme und als Einflusvariablen dann die Behandlung sowie die Zeit und die Interaktion zwischen der Behandlung und der Zeit. Die Daten sind auch gleich. Erst mit der Option id = ändert sich was. Hier geben wir die Spalte ein, in der die ID’s der Ferkel bzw. der Beobachtungen stehen. Das war es auch schon für den CLustereffekt. Dann nach die Verteilungsfamilie, wir können hier auch für nicht normalverteilte Daten ein GEE schätzen. Zum Abschluss noch die Korrelationsstruktur definiert. Wir nehmen hier exchangeable, diese Korrelationsstruktur ist für den Anfang immer ganz gut und macht auch häufig Sinn.\n\ngee_fit &lt;- gee(weight_gain ~ treatment + treatment * time,\n               data = pig_gain_tbl, \n               id = pig_id, \n               family = gaussian,\n               corstr = \"exchangeable\")\n\n             (Intercept)      treatmentfeed_10+10         treatmentfeed_20 \n               56.441300                 1.573500                 3.378825 \n                    time treatmentfeed_10+10:time    treatmentfeed_20:time \n               -2.358900                 0.207000                 0.182875 \n\n\nNachdem wir das Modell gefittet haben, können wir uns einmal die Korrelationsstruktur anschauen. Da ist die Funktion gee wirklich gut. Die Korrelationsstuktur können wir uns einfach so rausziehen.\n\npluck(gee_fit, \"working.correlation\") |&gt; \n  round(3)\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 1.000 0.082 0.082 0.082 0.082\n[2,] 0.082 1.000 0.082 0.082 0.082\n[3,] 0.082 0.082 1.000 0.082 0.082\n[4,] 0.082 0.082 0.082 1.000 0.082\n[5,] 0.082 0.082 0.082 0.082 1.000\n\n\nWas sehen wir? Natürlich muss auf der Diagonalen eine 1 stehen, den untereinander sind die Variablen ja identisch und damit mit 1 korreliert. Auf der Nicht-Diagonalen finden wir dann die Korrelation untereinander. Da wir exchangeable für die Korrelationsstruktur gewählt haben, haben wir überall die gleiche Korrelation. Alle Ferkel sind untereinander über die Zeitpunkte gleich mit \\(\\rho = 0.82\\) korreliert.\nWir lassen uns jetzt noch die Modellparameter ausgeben und schauen uns einmal an, ob wir was signifikantes gefunden haben.\n\ngee_fit |&gt; model_parameters()\n\nParameter                     | Coefficient |   SE |         95% CI |     z |      p\n------------------------------------------------------------------------------------\n(Intercept)                   |       56.44 | 1.23 | [52.53, 60.35] | 28.27 | &lt; .001\ntreatment [feed_10+10]        |        1.57 | 1.65 | [-3.96,  7.11] |  0.56 | 0.341 \ntreatment [feed_20]           |        3.38 | 1.66 | [-2.16,  8.91] |  1.20 | 0.041 \ntime                          |       -2.36 | 0.19 | [-3.49, -1.22] | -4.07 | &lt; .001\ntreatment [feed_10+10] × time |        0.21 | 0.24 | [-1.40,  1.81] |  0.25 | 0.386 \ntreatment [feed_20] × time    |        0.18 | 0.25 | [-1.42,  1.79] |  0.22 | 0.456 \n\n\nWir sehen, dass es einen signifikanten Unterschied in der Zeit gibt, das war ja auch zu erwarten, denn mit der Zeit werden die Ferkel schwerer. Wir haben aber auch einen schwach signifikanten Effekt zwischen feed_10 und feed_20 mit einem \\(p\\)-Wert von \\(0.041\\). Hier machen wir kurz Stop, dann geht es aber in dem Abschnitt zu den Posthoc Tests mit dem Modell weiter. Wir wollen ja noch für alle Behanlungslevel einen paarweisen Vergleich rechnen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html#modellieren-mit-geeglm",
    "href": "stat-modeling-gee.html#modellieren-mit-geeglm",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "53.6 Modellieren mit geeglm()",
    "text": "53.6 Modellieren mit geeglm()\nDer eigentlcihe Unetrschied zwischen der Funktion gee() und geeglm() ist, dass sich im Hintergrund eine Menge anders abspielt, das wir nicht sehen. Für mich war geeglm() immer schneller und stabiler. Der einzige Grund war immer nochmal ein gee() laufen zu lassen, da sich die Korrelationsmatrix so einfach aus dem gee() Objekt ziehen lassen lässt.\n\ngeeglm_fit &lt;- geeglm(weight_gain ~ treatment + treatment * time,\n                     data = pig_gain_tbl, \n                     id = pig_id, \n                     family = gaussian,\n                     corstr = \"exchangeable\")\n\nWir erhalten zwar auch die geschätzte Korrelation, aber nicht in so einer schönen Matrix. Also ist es dann Geschmackssache. Du weist dann ja, das wir mit exchangeable überall die gleiche Korrelation angenommen haben.\n\npluck(geeglm_fit, \"geese\", \"alpha\")\n\n   alpha \n0.082597 \n\n\nAm Ende schauen wir uns dann nochmal den Fit aus dem geeglm() Modell an. Und stellen fest, dass das Modell numerisch fast identisch ist. Wir haben also nur dir Wahl in der Darstellungsform und in der Geschwindigkeit.\n\ngeeglm_fit |&gt; model_parameters()\n\nParameter                     | Coefficient |   SE |         95% CI | Chi2(1) |      p\n--------------------------------------------------------------------------------------\n(Intercept)                   |       56.44 | 1.23 | [54.04, 58.84] | 2120.76 | &lt; .001\ntreatment [feed_10+10]        |        1.57 | 1.65 | [-1.66,  4.81] |    0.91 | 0.341 \ntreatment [feed_20]           |        3.38 | 1.66 | [ 0.13,  6.62] |    4.17 | 0.041 \ntime                          |       -2.36 | 0.19 | [-2.73, -1.99] |  153.65 | &lt; .001\ntreatment [feed_10+10] × time |        0.21 | 0.24 | [-0.26,  0.67] |    0.75 | 0.386 \ntreatment [feed_20] × time    |        0.18 | 0.25 | [-0.30,  0.66] |    0.56 | 0.456 \n\n\nWir haben also dann zwei Funktionen, die wir nutzen können. Am Ende kannst du dann beide ausprobieren. Machmal hat man mit geeglm() etwas mehr Glück, wenn die Daten einfach mal nicht wollen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html#multipler-vergleich-mit-emmeans",
    "href": "stat-modeling-gee.html#multipler-vergleich-mit-emmeans",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "53.7 Multipler Vergleich mit emmeans",
    "text": "53.7 Multipler Vergleich mit emmeans\nGut, soweit sind wir dann gekommen. Wir haben unser Modell gefittet und meistens wollen wir dann noch einen all-pair Vergleich bzw. den paarweisen Vergleich rechnen. Das machen wir erst einmal mit der Funktionalität aus dem R Paket {emmeans}, das uns erlaubt auch das compact letter display wiederzugeben. Wenn dich mehr zum Prozess des Codes für die Nutzung von {emmeans} interessiert, dann schaue doch einfach nochmal ins Kapitel 33. In dem Kapitel zu den multiplen Vergleichen erkläre ich dir nochmal genauer den Funktionsablauf.\nWichtig ist, dass wir unsere Vergleiche mit Bonferroni adjustieren. Wenn du das nicht möchtest, dann musst du adjust = \"none\" auswählen. Sonst machen wir die Ausageb nochmal tidy() und dann runden wir noch. Wir erhalten dann das compact letter display wieder.\n\nres_gee &lt;- geeglm_fit |&gt; \n  emmeans(~ treatment) \n\nres_gee_cld &lt;- res_gee |&gt; \n  cld(adjust = \"bonferroni\", Letters = letters) |&gt; \n  tidy() |&gt; \n  select(treatment, estimate, conf.low, conf.high, .group) |&gt; \n  mutate(across(where(is.numeric), round, 2))\n\nres_gee_cld \n\n# A tibble: 3 × 5\n  treatment  estimate conf.low conf.high .group\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n1 feed_10        49.4     47.0      51.7 \" a \" \n2 feed_10+10     51.6     49.1      54.0 \" ab\" \n3 feed_20        53.3     51.0      55.6 \"  b\" \n\n\nWenn dich die Abbildungen und weiteres interessieren, dann schaue einfach nochmal ins Kapitel zu den multiplen vergleichen. Dort zeige ich dann wie wir das compact letter display in eine Abbildung ergänzen. Der Ablauf ist auch im Kapitel 52 zu den linearen gemischten Modellen gezeigt.\nWir sehen, dass sich die Gruppe feed_10 von der Gruppe feed_20 unterscheidet. Beide Gruppen haben nicht den gleichen Buchstaben. Die Gruppe feed_10+10 unterscheidet sich weder von der Gruppe feed_10 noch von der Gruppe feed_20. Wir können uns im folgenden Codeblock dann auch die \\(p\\)-Werte für die Vergleiche wiedergeben lassen. Die Aussagen sind die selben.\n\nres_gee_tbl &lt;- res_gee |&gt; \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") |&gt; \n  tidy(conf.int = TRUE) |&gt; \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) |&gt; \n  select(contrast, estimate, \n         conf.low, conf.high, p.value) \n\nres_gee_tbl\n\n# A tibble: 3 × 5\n  contrast               estimate conf.low conf.high p.value\n  &lt;chr&gt;                     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 feed_10 - (feed_10+10)    -2.19    -5.56      1.17 0.355  \n2 feed_10 - feed_20         -3.93    -7.2      -0.66 0.012  \n3 (feed_10+10) - feed_20    -1.73    -5.06      1.59 0.636  \n\n\nDie Funktion emmeans() hätten wir auch mit dem Modell aus dem gee() Fit nutzen können. Als letzten Abschnitt wollen wir uns jetzt noch eine Besonderheit der GEE Varianzschätzung anschauen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html#multipler-vergleich-mit-multcomp-und-geesmv",
    "href": "stat-modeling-gee.html#multipler-vergleich-mit-multcomp-und-geesmv",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "53.8 Multipler Vergleich mit multcomp und geesmv",
    "text": "53.8 Multipler Vergleich mit multcomp und geesmv\nNatürlich haben wir uns nicht in den Details wie ein GEE funktioniert verloren. Es ist nun aber so, dass ein GEE auf sehr unterschiedliche Art und Weise die Korrelationsstruktur und die Varianzen dahinter schätzen kann. Je nach geschätzter Varianz kommen natürlich auch eventuell ganz andere Signifikanzen aus dem Modell. Deshalb hat sich wirklich eine Horde an mathematischen Statistikern an der Varianzschätzung im GEE abgearbeitet.\n\n\ngeesmv: Modified Variance Estimators for Generalized Estimating Equations\nDas R Paket {geesmv} bietet ganze neun Implementierungen von Schätzern für die Varianz/Covarianzstruktur der Daten an. Jetzt stellt sich die Frage, welche Implementierung für den Varianzschätzer denn nun nutzen? Zum einen hat natürlich die geschätzte Varianz einen nicht zu unterschätzenden Effekt auf die Signifikanz der Koeffizienten des GEE Models. Zum anderen ist aber der multiple Vergleich nach dem Schätzen des Modells und dem getrennten Schätzen der Varianz sehr mühselig. Leider helfen uns auch unsere Standardpakete nicht so richtig weiter. Die Funktionalität ist nicht für {geesmv} implementiert. Was wiederum dafür spricht, dass der Bedarf von Anwendern sehr eingeschränkt zu seien scheint. Nun müssen wir folgende epischen Schritte durchführen um einen multiplen Vergleich rechnen zu können.\n\nWir fitten unser geeglm() Modell in der mean parametrization, dass heist wir entfernen den Intercept aus dem Modell und lassen unser Modell somit durch den Urspung laufen. Im Prinzip setzen wir den Intercept auf 0 und erhalten so die Mittelwerte jedes Levels des Faktors treatment.\nWir speichern die \\(\\beta\\)-Koeffizienten von dem treatment aus unserem GEE Modell in einem Objekt ab.\nWir rechnen mit der gleichen Angabe wie vorher das geeglm() Modell eine der neun Funktion. Ich habe hier zufällig die Funktion GEE.var.lz() gewählt. Wir speichern die Ausgabe der Varianz der Koeffizienten in einem Objekt.\nWir kombinieren die \\(\\beta\\)-Koeffizienten und die Varianz in einem Objekt mit der Funktion left_join().\nWir bauen uns unsere eigene Kontrastmatrix in der steht welches Level der Behandlung mit welchen anderen Level verglichen werden soll.\nWir übergeben alle Einzelteile an die Funktion glht() aus dem R Paket {multcomp} und rechnen unseren multiplen Vergleich.\n\nNa dann mal auf. Gehen wir die Schritte einmal nacheinander durch und schauen, was wir da so alles gemacht haben. Nochmal Achtung, hier musst du wirklich schauen, ob sich der Aufwand lohnt. Ich zeige es hier einmal, den in bestimmten Fällen kann sich eine andere Implementierung für die Schätzung der Varianz durchaus lohnen. Denn aus Erfahrung weiß ich, dass der Standardvarianzschätzer nicht immer der beste Schätzer sein muss (Kruppa und Hothorn 2021).\nIm Folgenden schätzen wir einmal ein ganz normales GEE Modell mit der Funktion geeglm(). Wir werden aber nur die Koeffizienten brauchen. Die Varianz der Koeffizienten nutzen wir nicht. Ebenso brauchen wir die mean Parametrisierung, dass heißt wir setzen den Intercept auf 0.\n\ngeeglm_fit &lt;- geeglm(weight_gain ~ 0 + treatment + treatment * time,\n                     data = pig_gain_tbl, \n                     id = pig_id, \n                     family = gaussian,\n                     corstr = \"exchangeable\")\n\nWir speichern einmal die Koeffizienten in dem Objekt beta_tbl. Die brauchen wir später um die paarweisen Vergleiche zu rechnen.\n\nbeta_tbl &lt;- coef(geeglm_fit) |&gt; \n  enframe()\n\nUm die Varianz der Koeffizienten zu schätzen nutzen wir jetzt eine der Implementierungen in geesmv. Ich habe mich etwas zufällig für die Implementierung GEE.var.lz() entschieden. Diese Funktion liefer nur die Varianz der Koeffizienten. Leider aber nicht auch gleich noch die Koeffizienten dazu… deshalb der blöde doppelte Schritt. Wir speichern dann die Varianzen in dem Objekt vbeta_tbl.\n\ngee_lz_vcov &lt;- GEE.var.lz(weight_gain ~ 0 + treatment + treatment * time,\n                          data = as.data.frame(pig_gain_tbl), \n                          id = \"pig_id\",\n                          family = gaussian,\n                          corstr = \"independence\") \n\n        treatmentfeed_10      treatmentfeed_10+10         treatmentfeed_20 \n               56.441300                58.014800                59.820125 \n                    time treatmentfeed_10+10:time    treatmentfeed_20:time \n               -2.358900                 0.207000                 0.182875 \n\nvbeta_tbl &lt;- pluck(gee_lz_vcov, \"cov.beta\") |&gt; \n  enframe()\n\nJetzt verbinden wir noch die beiden Objekte beta_tbl und vbeta_tbl über die Funktion left_join(). Wir können mit der Funktion zwei Datensätze nach einer gemeinsamen Spalte zusammenführen. Dann müssen zwar die Einträge in der Spalte gleich sein, aber die Sortierung kann anders sein. Dann müssen wir noch die Zeilen rausfiltern in denen die Behandlungsmittelwerte sind. Am Ende benennen wir die Spalten noch sauber nach dem was die Spalten sind.\n\ncoef_tbl &lt;- left_join(beta_tbl, vbeta_tbl, by = \"name\") |&gt; \n  filter(str_detect(name, \"time\", negate = TRUE)) |&gt; \n  set_names(c(\"parameter\", \"beta\", \"vbeta\"))\n\nDas war jetzt ein Angang. Leider geht es nicht so einfach weiter. Wir müssen uns für die Vergleiche die Kontrastmatrix selberbauen. Wir machen einen paarweisen Vergleich, also wählen wir den Tukey Kontrast aus.\n\ncontrMat_n &lt;- setNames(rep(1, length(coef_tbl$parameter)),\n                       coef_tbl$parameter) |&gt; \n  contrMat(type = \"Tukey\")\n\ncontrMat_n \n\n\n     Multiple Comparisons of Means: Tukey Contrasts\n\n                                       treatmentfeed_10 treatmentfeed_10+10\ntreatmentfeed_10+10 - treatmentfeed_10               -1                   1\ntreatmentfeed_20 - treatmentfeed_10                  -1                   0\ntreatmentfeed_20 - treatmentfeed_10+10                0                  -1\n                                       treatmentfeed_20\ntreatmentfeed_10+10 - treatmentfeed_10                0\ntreatmentfeed_20 - treatmentfeed_10                   1\ntreatmentfeed_20 - treatmentfeed_10+10                1\n\n\nNun können wir alles zusammenbringen. Wir nutzen die Helferfunktion parm() aus dem R Paket {multcomp} um diei Koeffizienten richtig in glht() zuzuordnen. Dann noch der Kontrast mit rein in die Funktion und wir können unseren Vergleich rechnen. Leider fehlen noch die Freiheitsgrade, die wären dann in unserem Fall null, das ist aber Unsinn. Wir ergänzen die Freiheitsgrade aus unserem ursprünglichen Modell für die Koeffizienten.\n\nmult_gee &lt;- glht(parm(coef = coef_tbl$beta, \n                      vcov = diag(coef_tbl$vbeta)), \n                 linfct = contrMat_n)\nmult_gee$df &lt;- geeglm_fit$df.residual\n\nJetzt können wir uns die \\(p\\)-Werte und die 95% Konfidenzintervalle wiedergeben lassen. Du musst echt überlegen, ob sich der Aufwand lohnt. Wir erhalten hier jetzt kein signifikanten Unterschied mehr. Das liegt daran, dass wir in diesem Fall höhere Varianzen geschätzt haben als das geeglm() normalerweise tun würde. Höhere Varianzen der Koeffizienten, weniger signifikante Koeffizienten. Und dann auch weniger signifikante paarweise Unterschiede.\n\nmult_gee |&gt; \n  tidy(conf.int = TRUE) |&gt; \n  select(contrast, estimate, conf.low, conf.high, adj.p.value)\n\n# A tibble: 3 × 5\n  contrast                               estimate conf.low conf.high adj.p.value\n  &lt;chr&gt;                                     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 treatmentfeed_10+10 - treatmentfeed_10     1.57   -2.30       5.45       0.607\n2 treatmentfeed_20 - treatmentfeed_10        3.38   -0.511      7.27       0.103\n3 treatmentfeed_20 - treatmentfeed_10+10     1.81   -1.88       5.49       0.483\n\n\nAls Fazit nehmen wir mit, dass wir noch die Möglichkeit haben auf andere Art und Weise die Varianz in einem GEE zu schätzen. Ob uns das hilft steht auf einen anderem Blatt, aber wir haben die Möglichkeit hier noch nachzuadjustieren, wenn es mit dem Varianzschätzer klemmen sollte. Großartig unterstützt wird das Paket nicht, dass sieht man ja schon daran wie Oldschool die Analyse ist.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-gee.html#referenzen",
    "href": "stat-modeling-gee.html#referenzen",
    "title": "53  Generalized Estimating Equations (GEE)",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nAllison PD. 2009. Fixed effects regression models. SAGE publications.\n\n\nKruppa J, Hothorn L. 2021. A comparison study on modeling of clustered and overdispersed count data for multiple comparisons. Journal of Applied Statistics 48: 3220–3232.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalized Estimating Equations (GEE)</span>"
    ]
  },
  {
    "objectID": "stat-modeling-non-linear.html",
    "href": "stat-modeling-non-linear.html",
    "title": "54  Nicht lineare Regression",
    "section": "",
    "text": "54.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, nlraa, modelsummary,\n               parameters, performance, see, mgcv, mfp, marginaleffects,\n               gratia, readxl, nlstools, janitor, ggeffects, nls.multstart,\n               conflicted)\ncb_pal &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n            \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nicht lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-non-linear.html#daten",
    "href": "stat-modeling-non-linear.html#daten",
    "title": "54  Nicht lineare Regression",
    "section": "54.2 Daten",
    "text": "54.2 Daten\nIn unserem Datenbeispiel schauen wir uns die Wachstumskurve von Hühnchen an. Wir verfolgen das Gewicht über 36 Tage. Dabei messen wir an jedem Tag eine unterschiedliche Anzahl an Kücken bzw. Hünchen. Wir wissen auch nicht, ob wir immer die gleichen Hühnchen jedes Mal messen. Dafür war die Hühnchenmastanlage zu groß. Wir wissen aber wie alt jedes Hühnchen bei der Messung war.\n\nchicken_tbl &lt;- read_csv2(\"data/chicken_growth.csv\")  \n\nIn Tabelle 54.1 sehen wir nochmal die Daten für die ersten drei und die letzten drei Zeilen. Alleine überschlagsmäßig sehen wir schon, dass wir es nicht mit einem linearen Anstieg des Gewichtes zu tun haben. Wenn wir einen linearen Anstieg hätten, dann würde ein Hühnchen, dass am Tag 1 ca. 48g wiegt, nach 36 Tagen ca. 1728g wiegen. Das ist hier eindeutig nicht der Fall. Wir haben vermutlich einen nicht-linearen Zusammenhang.\n\n\n\n\nTabelle 54.1— Auszug aus Hühnchendatensatz.\n\n\n\n\n\n\nage\nweight\n\n\n\n\n1\n48\n\n\n1\n46\n\n\n1\n44\n\n\n…\n…\n\n\n36\n2286\n\n\n36\n2278\n\n\n36\n2309\n\n\n\n\n\n\n\n\nSchauen wir uns die Daten dann gleich einmal in einer Visualisierung mit ggplot() an um besser zu verstehen wie die Zusammenhänge in dem Datensatz sind.\nNeben dem Hünchendatensatz haben wir noch einen Datenstatz zu dem Wachstum von Wasserlinsen. Wir haben einmal händisch die Dichte bestimmt duckweeds_density und einmal mit einem Sensor gemessen. Dabei sind die Einheiten der Sensorwerte erstmal egal, wir wollen aber später eben nur mit einem Sensor messen und dann auf den Wasserlinsengehalt zurückschließen. Wir haben hier eher eine Sätigungskurve vorliegen, denn die Dichte der Wasserlinsen ist ja von der Oberfläche begrenzt. Auch können sich die Wasserlinsen nicht beliebig teilen, es gibt ja nur eine begrenzte Anzahl an Ressourcen.\n\nduckweeds_tbl &lt;- read_excel(\"data/duckweeds_density.xlsx\")\n\nIn der Tabelle 54.2 siehst du dann einmal einen Auszug aus den Daten zu den Wasserlinsen. Es ist ein sehr einfacher Datensatz mit nur zwei Spalten. Wie du siehst, scheint sich das bei der nicht linearen Regression durchzuziehen. Es gehen auch komplexere Modelle, aber dann kann ich die Ergebnisse schlechter visualisieren.\n\n\n\n\nTabelle 54.2— Auszug aus Wasserlinsendatensatz.\n\n\n\n\n\n\nduckweeds_density\nsensor\n\n\n\n\n4.8\n0.4303\n\n\n4.8\n0.4763\n\n\n4.8\n0.4954\n\n\n…\n…\n\n\n53.2\n2.1187\n\n\n53.2\n2.1296\n\n\n53.2\n2.1246\n\n\n\n\n\n\n\n\nAuch die Wasserlinsendaten wollen wir uns erstmal in einer Abbildung anschauen und dann sehen, ob wir eine Kurve durch die Punkte gelegt kriegen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nicht lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-non-linear.html#visualisierung",
    "href": "stat-modeling-non-linear.html#visualisierung",
    "title": "54  Nicht lineare Regression",
    "section": "54.3 Visualisierung",
    "text": "54.3 Visualisierung\nIn Abbildung 54.9 (a) sehen wir die Visualisierung der Hühnchengewichte nach Alter in Tagen. Zum einen sehen wir wie das Körpergewicht exponentiell ansteigt. Zum anderen sehen wir in Abbildung 54.9 (b), dass auch eine \\(log\\)-transformiertes \\(y\\) nicht zu einem linearen Zusammenhang führt. Der Zusammenhang zwischen dem Körpergewicht und der Lebensalter bleibt nicht-linear.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Ohne transformierten \\(y\\).\n\n\n\n\n\n\n\n\n\n\n\n(b) Mit \\(log\\)-transformierten \\(y\\).\n\n\n\n\n\n\n\nAbbildung 54.9— Visualisierung der Hühnchengewichte nach Alter in Tagen. Auch mit \\(log\\)-transformierten Körpergewicht liegt immer noch kein linearer Zusammenhang zwischen dem Lebensalter und dem Körpergewicht vor.\n\n\n\n\nDeshalb wollen wir den Zusammenhang zwischen dem Körpergewicht der Hühnchen und dem Lebensalter einmal mit einer nicht-linearen Regression modellieren. Wir sind also nicht so sehr an \\(p\\)-Werten interessiert, wir sehen ja, dass die gerade ansteigt, sondern wollen wissen wie die Koeffizienten einer möglichen exponentiellen Gleichung aussehen.\nFür die Visualisierung der Wasserlinsendaten in der Abbildung 54.10 verzichte ich einmal auf die logarithmische Darstellung. Wir wollen hier dann eine Kurve durch die Punkte legen so wie die Daten sind. Auffällig ist erstmal, dass wir sehr viel weniger Beobachtungen und auch Dichtemesspunkte auf der \\(x\\)-Achse haben. Wir haben dann zu den jeweiligen Wasserlinsendichten dann drei Sensormessungen. Das könnte noch etwas herausfordernd bei der Modellierung werden.\n\nggplot(duckweeds_tbl, aes(duckweeds_density, sensor)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Gemessene Dichte der Wasserlinsen\", y = \"Sensorwert\")\n\n\n\n\n\n\n\nAbbildung 54.10— Visualisierung der Sensorwerte nach Wasserlinsendichte. Pro Dichtewert liegen drei Sensormessungen vor.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nicht lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-non-linear.html#nonlinear-least-squares-mit-nls",
    "href": "stat-modeling-non-linear.html#nonlinear-least-squares-mit-nls",
    "title": "54  Nicht lineare Regression",
    "section": "54.4 Nonlinear least-squares mit {nls}",
    "text": "54.4 Nonlinear least-squares mit {nls}\nZum nicht-linearen Modellieren nutzen wir die Funktion nls() (eng. nonlinear least-squares). Die Funktion nls() ist das nicht-lineare Äquivalent zu der linearen Funktion lm(). Nur müssen wir mit der nls() Funktion etwas anders umgehen. Zum einen müssen wir die formula() anders definieren. Der große Vorteil von nls() ist, dass wir hier auch die Koeffizienten unser Geradengleichung wiederkriegen. In den anderen Möglichkeiten kriegen wir dann teilweise nicht die Informationen zu einer Kurve wieder. Wir haben dann zwar ein wunderbares Modell, können das Modell aber nicht einfach als eine mathematische Gleichung aufschreiben. Daher hier nls() etwas ausführlicher, da wir dann mit nls() schon die Sachen auch erhalten, die wir meist wollen.\n\n54.4.1 … von Wachstum\nIn unserem Hühnchenbeispiel nehmen ein exponentielles Wachstum an. Daher brauchen wir einen geschätzten Koeffizienten für den Exponenten des Alters sowie einen Intercept. Wir gehen nicht davon aus, dass die Hühnchen mit einem Gewicht von 0g auf die Welt bzw. in die Mastanlage kommen. Unsere Formel sehe dann wie folgt aus.\n\\[\nweight \\sim \\beta_0 + age^{\\beta_1}\n\\]\nDa wir in R keine \\(\\beta\\)’s schreiben können nutzen wir die Buchstaben b0 für \\(\\beta_0\\) und b1 für \\(\\beta_1\\). Im Prinzip könnten wir auch andere Buchstaben nehmen, aber so bleiben wir etwas konsistenter zu der linearen Regression. Somit sieht die Gleichung dann in R wie folgt aus.\n\\[\nweight \\sim b_0 + age^{b_1}\n\\]\nWichtig hier, wir müssen R noch mitteilen, dass wir age hoch b1 rechnen wollen. Um das auch wirklich so zu erhalten, zwingen wir R mit der Funktion I() auch wirklich einen Exponenten zu berechnen. Wenn wir nicht das I() nutzen, dann kann es sein, dass wir aus versehen eine Schreibweise für eine Abkürzung in der formula Umgebung nutzen.\nIm Weiteren sucht die Funktion iterativ die besten Werte für b0 und b1. Deshalb müssen wir der Funktion nls() Startwerte mitgeben, die in etwa passen könnten. Hier tippe ich mal auf ein b0 = 1 und ein b1 = 1. Wenn wir einen Fehler wiedergegeben bekommen, dann können wir auch noch an den Werten drehen.\n\nfit &lt;- nls(weight ~ b0 + I(age^b1), data = chicken_tbl, \n           start = c(b0 = 1, b1 = 1))\n\n\n\n\n\n\n\nBessere Startwerte für nls()\n\n\n\nLeider müssen wir in nls() die Startwerte selber raten. Das kannst du natürlich aus der Abbildung der Daten abschätzen, aber ich muss sagen, dass mir das immer sehr schwer fällt. Deshalb gibt es da einen Trick. Wir rechnen ein lineares Modell und zwar logarithmieren wir beide Seiten der Gleichung. Dann können wir die Koeffizienten aus dem Modell als Startwerte nehmen.\n\nlm(log(weight) ~ log(age), chicken_tbl)\n\n\nCall:\nlm(formula = log(weight) ~ log(age), data = chicken_tbl)\n\nCoefficients:\n(Intercept)     log(age)  \n      3.475        1.085  \n\n\nManchmal musst du auch nur die linke Seite logarithmieren.\n\nlm(log(weight) ~ age, chicken_tbl)\n\nEs hängt dann immer etwas vom Modell ab und wie die Werte dann anschließend in nls() konvergieren. Ich habe eigentlich immer mit einem der beiden Methoden Startwerte gefunden. Wir nehmen hier mal die Startwerte aus dem ersten Ansatz mit der doppelten Logarithmierung.\nWir erhalten hier für den Intercept den Wert 3.475 und für die Steigung den Wert 1.085. Da in meiner obigen Gleichung die Steigung b1 ist und der Intercept dann b0 setzen wir die Zahlen entsprechend ein. Für den Intercept müssen wir dann noch den Exponenten wählen.\n\nnls(weight ~ b0 + I(age^b1), data = chicken_tbl, \n    start = c(b0 = exp(3.475), b1 = 1.085))\n\nNonlinear regression model\n  model: weight ~ b0 + I(age^b1)\n   data: chicken_tbl\n    b0     b1 \n92.198  2.178 \n residual sum-of-squares: 4725115\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 1.176e-06\n\n\nJetzt sollten wir keine Fehlermeldung erhalten haben, dass unser Modell nicht konvergiert ist oder anderweitig kein Optimum gefunden hat.\nDas R Paket {nls.multstart} versucht das Problem der Startwerte nochmal algorithmisch zu lösen. Wenn du also keine guten Startwerte mit den Trick über lm() findest, dann ist das R Paket hier nochmal ein guter Startpunkt. Es geht auch komplexer wie das Tutorium unter Nonlinear Modelling using nls, nlme and brms nochmal zeigt.\n\nnls_multstart(weight ~ b0 + I(age^b1), data = chicken_tbl, \n              lower = c(b0 = 0, b1 = 0),\n              upper = c(b0 = Inf, b1 = Inf),\n              start_lower = c(b0 = 0, b1 = 0),\n              start_upper = c(b0 = 500, b1 = 5),\n              iter = 500)\n\nNonlinear regression model\n  model: weight ~ b0 + I(age^b1)\n   data: data\n    b0     b1 \n92.198  2.178 \n residual sum-of-squares: 4725115\n\nNumber of iterations to convergence: 9 \nAchieved convergence tolerance: 1.49e-08\n\n\nNatürlich kommt hier das Gleiche raus, aber manchmal findet man dann wirklich nicht die passenden Startwerte. Die Funktion macht ja nichts anderes als der ursprünglichen nls() Funktion etwas unter die Arme zugreifen.\n\n\nWir nutzen wieder die Funktion model_parameters() aus dem R Paket {parameters} um uns eine aufgeräumte Ausgabe wiedergeben zu lassen.\n\nfit |&gt; \n  model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\nb0        |       92.20\nb1        |        2.18\n\n\nDie \\(p\\)-Werte interessieren uns nicht weiter. Wir sehen ja, dass wir einen Effekt von dem Alter auf das Körpergewicht haben. Das überrascht auch nicht weiter. Wir wollen ja die Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\) um die Gleichung zu vervollständigen. Mit dem Ergebnis aus der Funktion nls() können wir jetzt wie folgt schreiben.\n\\[\nweight \\sim 92.20 + age^{2.18}\n\\]\nDamit haben wir dann auch unsere nicht-lineare Regressionsgleichung erhalten. Passt den die Gleichung auch zu unseren Daten? Das können wir einfach überprüfen. Dafür müssen wir nur in die Funktion predict() unser Objekt des Fits unseres nicht-linearen Modells fit stecken und erhalten die vorhergesagten Werte für jedes \\(x\\) in unserem Datensatz. Oder etwas kürzer, wir erhalten die “Gerade” der Funktion mit den Koeffizienten aus dem nls() Modell wieder. In Abbildung 54.11 sehen wir die gefittete Gerade.\n\nggplot(chicken_tbl, aes(age, weight)) +\n  geom_line(aes(y = predict(fit)), size = 1, color = \"#CC79A7\") +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 54.11— Visualisierung der Hühnchengewichte nach Alter in Tagen mit der geschätzen nicht-linearen Regressionsgleichung.\n\n\n\n\n\nWie wir erkennen können sieht die Modellierung einigermaßen gut aus. Wir haben zwar einige leichte Abweichungen von den Beobachtungen zu der geschätzten Geraden, aber im Prinzip könnten wir mit der Modellierung leben. Wir hätten jetzt also eine nicht-lineare Gleichung die den Zusammenhang zwischen Körpergewicht und Lebensalter von Hühnchen beschreibt. Die Verwendung von nest() und map() ist schon erweiterete Programmierung in R. Du findest hier mehr über broom and dplyr und die Anwendung auf mehrere Datensätze.\nNun könnte man argumentieren, dass wir vielleicht unterschiedliche Abschnitte des Wachstums vorliegen haben. Also werden wir einmal das Alter in Tagen in vier gleich große Teile mit der Funktion cut_number() schneiden. Beachte bitte, dass in jeder Gruppe gleich viele Beobachtungen sind. Du kannst sonst händisch über case_when() innerhalb von mutate() dir eigene Gruppen bauen. Wir nutzen auch die Funktion map() um über alle Subgruppen des Datensatzes dann ein nls() laufen zu lassen.\n\nnls_tbl &lt;- chicken_tbl |&gt; \n  mutate(grp = as_factor(cut_number(age, 4))) |&gt; \n  group_by(grp) |&gt; \n  nest() |&gt; \n  mutate(nls_fit = map(data, ~nls(weight ~ b0 + I(age^b1), data = .x, \n                                  start = c(b0 = 1, b1 = 2))),\n         pred = map(nls_fit, ~predict(.x))) \n\nUm den Codeblock oben kurz zu erklären. Wir rechnen vier nicht-lineare Regressionen auf den vier Altersgruppen. Dann müssen wir uns noch die vorhergesagten Werte wiedergeben lassen damit wir die gefittete Gerade zeichnen können. Wir nutzen dazu die Funktion unnest() um die Daten zusammen mit den vorhergesagten Werten zu erhalten.\n\nnls_pred_tbl &lt;- nls_tbl |&gt; \n  unnest(c(data, pred))\n\nIn Abbildung 54.12 sehen wir die vier einzelnen Geraden für die vier Altersgruppen. Wir sind visuell besser als über alle Altersgruppen hinweg. Das ist doch mal ein schönes Ergebnis.\n\n\n\n\n\n\n\n\nAbbildung 54.12— Visualisierung der Hühnchengewichte nach Alter in Tagen mit der geschätzen nicht-linearen Regressionsgleichung aufgeteilt nach vier Altersgruppen.\n\n\n\n\n\nWir können uns jetzt noch die b0 und b1 für jede der vier Altergruppen wiedergeben lassen. Wir räumen etwas auf und geben über select() nur die Spalten wieder, die wir auch brauchen und uns interessieren.\n\nnls_tbl |&gt; \n  mutate(tidied = map(nls_fit, tidy)) |&gt; \n  unnest(tidied) |&gt; \n  select(grp, term, estimate) \n\n# A tibble: 8 × 3\n# Groups:   grp [4]\n  grp     term  estimate\n  &lt;fct&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1 [1,2]   b0       44.4 \n2 [1,2]   b1        4.21\n3 (2,8]   b0       60.1 \n4 (2,8]   b1        2.42\n5 (8,25]  b0      128.  \n6 (8,25]  b1        2.18\n7 (25,36] b0      330.  \n8 (25,36] b1        2.14\n\n\nWas sehen wir? Wir erhalten insgesamt acht Koeffizienten und können darüber dann unsere vier exponentiellen Gleichungen für unsere Altergruppen erstellen. Wir sehen, dass besonders in der ersten Gruppe des Alters von 1 bis 2 Tagen wir den Intercept überschätzen und den Exponenten unterschätzen. In den anderen Altersgruppen passt dann der Exponent wieder zu unserem ursprünglichen Modell über alle Altersgruppen.\n\\[\nweight_{[1-2]} \\sim 44.4 + age^{4.21}\n\\]\n\\[\nweight_{(2-8]} \\sim 60.1 + age^{2.42}\n\\]\n\\[\nweight_{(8-25]} \\sim 128.0 + age^{2.18}\n\\]\n\\[\nweight_{(25-36]} \\sim 330.0 + age^{2.14}\n\\]\nJe nachdem wie zufrieden wir jetzt mit den Ergebnissen der Modellierung sind, könnten wir auch andere Altersgruppen noch mit einfügen. Wir belassen es bei dieser Modellierung und schauen uns nochmal die andere Richtung an.\n\n\n54.4.2 … von Sättigung\nSchauen wir uns jetzt einmal ein Beispiel der Sättigung an. Hier nehmen wir dann eine Power-Funktion in der Form \\(y = a + x^b\\). Wir könnten noch eine Konstante \\(c\\) als Multiplikator einfügen, wir schauen jetzt aber mal, ob unsere einfache Parametrisierung jetzt funktioniert. Prinzipiell sehen ja unsere Punkte wie eine Power-Funktion aus. Daher bauen wir uns einmal die Fomel in nls() und lassen uns die Koeffizienten \\(a\\) und \\(b\\) wiedergeben. Dann schauen wir, ob die Koeffizienten Sinn machen und die Punkte auch gut beschreiben.\n\nduckweeds_nls_fit &lt;- nls(sensor ~ a + I(duckweeds_density^b), data = duckweeds_tbl, \n                         start = c(a = 0, b = 0))\nduckweeds_nls_fit \n\nNonlinear regression model\n  model: sensor ~ a + I(duckweeds_density^b)\n   data: duckweeds_tbl\n      a       b \n-1.1468  0.2975 \n residual sum-of-squares: 0.08168\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 1.429e-06\n\n\nSchauen wir uns einmal das Bestimmtheitsmaß \\(R^2\\) für die Anpassung an. Das ist ja immer unser erstes abstraktes Maß für die Modellgüte und wie gut die Kurve durch die Punkte gelaufen ist. Das sieht doch schon sehr gut aus. Wir wollen den Wert aber dann noch gleich einmal visuell überprüfen.\n\nduckweeds_nls_fit |&gt; r2()\n\n  R2: 0.986\n\n\nDann können wir auch schon die mathematische Gleichung aufschreiben. Wir haben einen negativen \\(y\\)-Achsenabschnitt \\(a\\) sowie eine Power kleiner als 1. Damit sollte unsere Kurve mit steigenden \\(x\\)-Werten abflachen. Ich kann mir immer nicht vorstellen, wie so eine Funktion aussehen würde, dafür fehlt mir die mathematische Phantasie.\n\\[\nsensor = -1.1468 + duckweeds\\_density^{0.2975}\n\\]\nDann wollen wir einmal die Funktion visualisieren. Wir haben zwei Möglichkeiten. Entweder bauen wir uns die mathematische Funktion in R nach und plotten dann die mathematische Funktion mit geom_function() oder wir nutzen nur den Fit duckweeds_nls_fit direkt in ggplot(). Erstes erlaubt nochmal sicherzugehen, dass wir auch die mathematische Funktion richtig aufgeschrieben haben. Wenn du die nicht brauchst, dann ist die zweite Variante natürlich weit effizienter.\n\nMit geom_function()Mit geom_line() und predict()\n\n\nJetzt nehmen wir einmal unsere Koeffizienten aus dem nls()-Modell und bauen uns eine Funktion nach. Das ist im Prinzip die mathematische Formel nur in der Schreibweise in R.\n\nduckweed_func &lt;- \\(x){-1.1468 + x^{0.2975}} \n\nDann können wir auch schon die Kurve durch die Punkte in der folgenden Abbildung legen. Da wir hier eine Funktion vorliegen haben, ist der Verlauf auch sehr schön glatt. Wir sehen aber auch, dass die Funktion sehr schön passt. Die Kurve läuft gut durch die Punkte. Etwas was wir auch schon von dem \\(R^2\\) erwartet hatten.\n\nggplot(duckweeds_tbl, aes(duckweeds_density, sensor)) +\n  geom_function(fun = duckweed_func, color = \"#CC79A7\") +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Gemessene Dichte der Wasserlinsen\", y = \"Sensorwert\")\n\n\n\n\n\n\n\nAbbildung 54.13— Angepasste Kurve aus der Funktion nls() dargestellt mit der Funktion geom_function().\n\n\n\n\n\n\n\nSchneller geht es in der Funktion geom_line() und predict(), wo wir dann die Anpassung unseres Modells direkt als \\(y\\)-Werte übergeben. Da wir hier jetzt nur die \\(x\\)-Werte nutzen, die wir auch in den Daten vorliegen haben, wirkt die Kurve bei so wenigen Messpunkten auf der \\(x\\)-Achse etwas stufig. Aber auch hier sehen wir, dass die Kurve gut durch unsere Punkte läuft. Diese Variante ist die etwas schnellere, wenn du nicht an der mathematischen Formulierung interessiert bist.\n\nggplot(duckweeds_tbl, aes(duckweeds_density, sensor)) +\n  geom_line(aes(y = predict(duckweeds_nls_fit)), size = 1, color = \"#0072B2\") +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Gemessene Dichte der Wasserlinsen\", y = \"Sensorwert\")\n\n\n\n\n\n\n\nAbbildung 54.14— Angepasste Kurve aus der Funktion nls() dargestellt mit der Funktion geom_line() und predict().\n\n\n\n\n\n\n\n\nJetzt bietet es sich nochmal an die Vorhersageintervalle oder Prädiktionsintervalle (eng. prediction interval) der Abbildung hinzuzufügen. Neben den Vorhersageintervallen könntne wir auch die 95% Konfidenzintervalle (eng. confidence interval) hinzufügen. Hier müssen wir gleich entscheiden, was wir eigentlich zeigen wollen. Wir müssen also zwischen den beiden Intervallen unterscheiden. Vorhersageintervalle machen eine Aussage zu der Genauigkeit von zukünftigen Beobachtungen wohingegen die die Konfidenzintervalle eine Aussage über die Koeffizienten des Modells treffen. Mehr dazu gibt es auch hier Stack Exchange unter Prediction interval vs. confidence interval in linear regression analysis. Konfidenzintervalle geben dir also die Bandbreite wieder in der die Gerade verläuft. Dafür nutzen die Konfidenzintervalle die Daten und geben dir ein Intervall an, in dem die Gerade durch die Koeffizienten des Modells mit 95% Sicherheit verläuft. Das Vorhersageintervall gibt dir an mit welchen Bereich zukünftige Beobachtung mit 95% Sicherheit fallen werden. Das Vorhersageintervall ist breiter als das Konfidenzintervall und nicht jede Methode liefert auch beide Intervalle.\nEs ist uns möglich über Bootstrap, also einer Simulation aus unseren Daten, ein Vorhersageintervall sowie ein Konfidenzintervall zu generieren. Zu dem Bootstrapverfahren kannst du in den Klassifikationskapiteln mehr lesen. Ich nutze hier 500 Simulationen um mir die Intervalle ausgeben zu lassen. Bei einer echten Analyse würde ich die Anzahl auf 1000 bis 2000 setzen. Wir brauchen also als erstes unser Bootstrapobjekt mit dem wir dann in den Tabs weitermachen.\n\nnls_boot_obj &lt;- nlsBoot(duckweeds_nls_fit, niter = 500)\n\nIch habe jetzt das Prädiktionsintervall und das Konfidenzintervall jeweils in einem der Tabs berechnet. Wenn du deine Daten auswertest musst du dich dann für ein Intervall entscheiden. Meistens nutzen wir das Konfidenzintervall, da die Interpretation und die Darstellung im Allgemeinen bekannter ist.\n\nVorhersageintervall (eng. prediction interval)Konfidenzintervall (eng. confidence interval)\n\n\nUm das Vorhersageintervall zu erstellen nutzen wir die Funktion nlsBootPredict() und übergeben als neue Daten unseren Datensatz. Dann müssen wir natürlich noch als Option interval = \"prediction\" wählen um das Vorhersageintervall wiedergegeben zu bekommen. Ich muss dann noch etwas aufräumen und auch die \\(x\\)-Werte wieder ergänzen damit wir gleich alles in ggplot() darstellen können. Auch nerven mich die doppelten Werte, die brauche ich nicht für die Darstellung und entferne sie über distinct().\n\npred_plim_tbl &lt;- nlsBootPredict(nls_boot_obj, newdata = duckweeds_tbl, interval = \"prediction\") |&gt; \n  as_tibble() |&gt; \n  clean_names() |&gt; \n  mutate(duckweeds_density = duckweeds_tbl$duckweeds_density) |&gt; \n  distinct(duckweeds_density, .keep_all = TRUE)\npred_plim_tbl\n\n# A tibble: 6 × 4\n  median x2_5_percent x97_5_percent duckweeds_density\n   &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1  0.456        0.242         0.571               4.8\n2  1.07         0.853         1.18               14.5\n3  1.44         1.22          1.55               24.2\n4  1.71         1.50          1.82               33.9\n5  1.93         1.72          2.04               43.5\n6  2.12         1.91          2.23               53.2\n\n\nAus der mittleren Abweichung des Medians zu der unteren 2.5% Grenze x2_5_percent sowie zu der oberen 97.5% Grenze x97_5_percent lässt sich leicht noch eine Konstante \\(\\phi\\) errechnen, die wir dann zu unserer mathematischen Formel ergänzen können. Dann hätten wir auch die mathematische Formel für die obere und untere Kurve des Vorhersageintervalls. Im Prinzip ist das Vorhersageintervall ja nur eine verschobene Kurve der ursprünglichen Geradengleichung. Wir subtrahieren und addieren also unser \\(\\phi\\) jeweils zu dem \\(y\\)-Achsenabschnitt von \\(-1.1468\\) aus dem nls()-Modell.\n\nwith(pred_plim_tbl, x2_5_percent - median) |&gt; mean()\n\n[1] -0.2130752\n\nwith(pred_plim_tbl, x97_5_percent - median) |&gt; mean()\n\n[1] 0.1114297\n\n\nDamit hätten wir dann für die 2.5% und 97.% Grenzen des Vorhersageintervalls folgende mathematische Formel. Ich ersetze hier einmal \\(duckweeds\\_density\\) durch \\(x\\) um die Formel etwas aufzuräumen und zu kürzen.\n\\[\n[-1.358 + x^{0.298}; \\; -1.031 + x^{0.298}]\n\\]\nWir machen es uns etwas einfacher und nutzen hier dann die Funktion geom_ribbon() um die Fläche des Vorhersageintervalls. Wir nutzen hier also nicht die Informationen aus unserem nls()-Modell direkt sondern erschaffen uns die Informationen nochmal über eine Bootstrapsimulation. Es ist einfach noch ein Extraschritt, wenn du eben noch ein Intervall haben willst. Nicht immer ist es notwendig und die Breite des Vorhersageintervalls hängt auch maßgeblich von der Anzahl an Beobachtungen ab.\n\nggplot(duckweeds_tbl, aes(duckweeds_density, sensor)) +\n  geom_line(data = pred_plim_tbl, aes(y = median), color = \"#CC79A7\") +\n  geom_ribbon(data = pred_plim_tbl, fill = \"#CC79A7\", alpha = 0.3,\n              aes(x = duckweeds_density, ymin = x2_5_percent, ymax = x97_5_percent), inherit.aes = FALSE) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Gemessene Dichte der Wasserlinsen\", y = \"Sensorwert\")\n\n\n\n\n\n\n\nAbbildung 54.15— Angepasste Kurve aus der Funktion nls() zusammen mit dem Vorhersageintervall.\n\n\n\n\n\n\n\nUm das Konfidenzintervall zu erstellen nutzen wir ebenfalls die Funktion nlsBootPredict() und übergeben als neue Daten unseren Datensatz. Dann müssen wir natürlich noch als Option interval = \"confidence\" auswählen um das Konfidenzintervall wiedergegeben zu bekommen. Ich muss dann auch hier aufräumen und die \\(x\\)-Werte wieder ergänzen damit wir gleich alles in ggplot() darstellen können. Auch nerven mich die doppelten Werte, die brauche ich nicht für die Darstellung und entferne sie über distinct().\n\npred_clim_tbl &lt;- nlsBootPredict(nls_boot_obj, newdata = duckweeds_tbl, interval = \"confidence\") |&gt; \n  as_tibble() |&gt; \n  clean_names() |&gt; \n  mutate(duckweeds_density = duckweeds_tbl$duckweeds_density) |&gt; \n  distinct(duckweeds_density, .keep_all = TRUE)\npred_clim_tbl \n\n# A tibble: 6 × 4\n  median x2_5_percent x97_5_percent duckweeds_density\n   &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1  0.450        0.386         0.499               4.8\n2  1.07         1.03          1.10               14.5\n3  1.43         1.40          1.46               24.2\n4  1.70         1.67          1.73               33.9\n5  1.92         1.88          1.96               43.5\n6  2.11         2.06          2.16               53.2\n\n\nAus der mittleren Abweichung des Medians zu der unteren 2.5% Grenze x2_5_percent sowie zu der oberen 97.5% Grenze x97_5_percent lässt sich leicht noch eine Konstante \\(\\phi\\) errechnen, die wir dann zu unserer mathematischen Formel ergänzen können. Dann hätten wir auch die mathematische Formel für die obere und untere Kurve des Konfidenzintervalls. Im Prinzip ist das Konfidenzintervall ja nur eine verschobene Kurve der ursprünglichen Geradengleichung. Wir subtrahieren und addieren also unser \\(\\phi\\) jeweils zu dem \\(y\\)-Achsenabschnitt von \\(-1.1468\\) aus dem nls()-Modell.\n\nwith(pred_clim_tbl, x2_5_percent - median) |&gt; mean()\n\n[1] -0.04599816\n\nwith(pred_clim_tbl, x97_5_percent - median) |&gt; mean()\n\n[1] 0.03634727\n\n\nDamit hätten wir dann für die 2.5% und 97.% Grenzen des Konfidenzintervalls folgende mathematische Formel. Ich ersetze hier einmal \\(duckweeds\\_density\\) durch \\(x\\) um die Formel etwas aufzuräumen und zu kürzen.\n\\[\n[-1.1978 + x^{0.298}; \\; -1.1098 + x^{0.298}]\n\\]\nWir machen es uns einfacher und nutzen hier dann die Funktion geom_ribbon() um die Fläche des Konfidenzintervalls. Wir nutzen hier also nicht die Informationen aus unserem nls()-Modell direkt sondern erschaffen uns die Informationen nochmal über eine Bootstrapsimulation. Es ist einfach noch ein Extraschritt, wenn du eben noch ein Intervall haben willst. Nicht immer ist es notwendig und die Breite des Konfidenzintervalls hängt auch maßgeblich von der Anzahl an Beobachtungen ab.\n\nggplot(duckweeds_tbl, aes(duckweeds_density, sensor)) +\n  geom_line(data = pred_clim_tbl, aes(y = median), color = \"#0072B2\") +\n  geom_ribbon(data = pred_clim_tbl, fill = \"#0072B2\", alpha = 0.3,\n              aes(x = duckweeds_density, ymin = x2_5_percent, ymax = x97_5_percent), inherit.aes = FALSE) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Gemessene Dichte der Wasserlinsen\", y = \"Sensorwert\")\n\n\n\n\n\n\n\nAbbildung 54.16— Angepasste Kurve aus der Funktion nls() zusammen mit dem Konfidenzintervall.\n\n\n\n\n\n\n\n\n\n\n54.4.3 … von Zerfall\nNachdem wir uns einem exponentiellen Anstieg und die Sättigung angeschaut haben, wollen weit uns nun einmal mit einem exponentiellen Zerfall beschäftigen. Wir betrachten einen exponentziellen Zerfall einer Blattläuse Population. Wir wollen die folgende Gleichung lösen und die Werte für die Konstante \\(a\\) und den Exponenten \\(\\beta_1\\) schätzen. Nun haben wir diesmal keinen Intercept vorliegen.\n\\[\ncount \\sim a \\cdot week^{\\beta_1}\n\\]\nDie Daten sind angelegt an ein Experiment zu Blattlauskontrolle. Wir haben ein neues Biopestizid welchen wir auf die Blattläuse auf Rosen sprühen. Wir zählen dann automatisiert über eine Kamera und Bilderkennung wie viele Blattläuse sich nach den Wochen des wiederholten Sprühens noch auf den Rosen befinden. Wir erhalten damit folgende Daten im Objekt exp_tbl.\n\nset.seed(20221018)\nexp_tbl &lt;- tibble(count = c(rnorm(10, mean = 17906, sd = 17906/4), \n                            rnorm(10, mean =  5303, sd =  5303/4),\n                            rnorm(10, mean =  2700, sd =  2700/4),\n                            rnorm(10, mean =  1696, sd =  1696/4), \n                            rnorm(10, mean =   947, sd =   947/4), \n                            rnorm(10, mean =   362, sd =   362/4)), \n                  weeks = rep(1:6, each = 10)) \n\nWir müssen ja wieder die Startwerte in der Funktion nls() angeben. Meistens raten wir diese oder schauen auf die Daten um zu sehen wo diese Werte in etwa liegen könnten. Dann kann die Funktion nls() diese Startwerte dann optimieren. Es gibt aber noch einen anderen Trick. Wir rechnen eine lineare Regression über die \\(log\\)-transformierten Daten und nehmen dann die Koeffizienten aus dem linearen Modell als Startwerte für unsere nicht-lineare Regression.\n\nlm(log(count) ~ log(weeks), exp_tbl)\n\n\nCall:\nlm(formula = log(count) ~ log(weeks), data = exp_tbl)\n\nCoefficients:\n(Intercept)   log(weeks)  \n      9.961       -2.024  \n\n\nAus der linearen Regression erhalten wir einen Intercept von \\(9.961\\) und eine Steigung von \\(-2.025\\). Wir exponieren den Intercept und erhalten den Wert für \\(a\\) mit \\(\\exp(9.961)\\). Für den Exponenten \\(b1\\) tragen wir den Wert \\(-2.025\\) als Startwert ein. Mit diesem Trick erhalten wir etwas bessere Startwerte und müssen nicht so viel rumprobieren.\n\nfit &lt;- nls(count ~ a * I(weeks^b1), data = exp_tbl, \n           start = c(a = exp(9.961), b1 = -2.025))\n\nWir können uns noch die Koeffizienten wiedergeben lassen und die Geradengleichung vervollständigen. Wie du siehst sind die Werte natürlich anders als die Startwerte. Wir hätten aber ziemlich lange rumprobieren müssen bis wir nahe genug an die Startwerte gekommen wären damit die Funktion nls() iterativ eine Lösung für die Gleichung findet.\n\nfit |&gt; \n  model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\na         |    17812.11\nb1        |       -1.69\n\n\nAbschließend können wir dann die Koeffizienten in die Geradengleichung eintragen.\n\\[\ncount \\sim 17812.11 \\cdot week^{-1.69}\n\\]\nIn Abbildung 54.17 sehen wir die Daten zusammen mit der gefitteten Gerade aus der nicht-linearen Regression. Wir sehen, dass die Gerade ziemlich gut durch die Mitte der jeweiligen Punkte läuft.\n\nggplot(exp_tbl, aes(weeks, count)) +\n  theme_minimal() +\n  geom_point() +\n  geom_line(aes(y = predict(fit)), color = \"#CC79A7\") +\n  scale_x_continuous(breaks = 1:6)\n\n\n\n\n\n\n\nAbbildung 54.17— Visualisierung der Sterberate von Blattläusen nach Aufbringen eines Bio-Pestizides mit der nicht-linearen Regressionsgleichung.\n\n\n\n\n\n\n\n54.4.4 … der Michaelis-Menten Gleichung\nIn diesem Abschnitt wollen wir uns mit dem Modellieren einer Sättigungskurve beschäftigen. Daher bietet sich natürlich die Michaelis-Menten-Gleichung an. Die Daten in enzyme.csv geben die Geschwindigkeit \\(v\\) des Enzyms saure Phosphatase (\\(\\mu mol/min\\)) bei verschiedenen Konzentrationen des Substrats Nitrophenolphosphat, [S] (mM), an. Die Daten können mit der Michaelis-Menten-Gleichung modelliert werden und somit kann eine nichtlineare Regression kann verwendet werden, um \\(K_M\\) und \\(v_{max}\\) zu schätzen.\n\nenzyme_tbl &lt;- read_csv2(file.path(\"data/enzyme.csv\")) |&gt; \n  rename(S = concentration, v = rate)\n\nIn Tabelle 54.3 sehen wir einen Auszug aus den Enzymedaten. Eigentlich relativ klar. Wir haben eine Konzentration \\(S\\) vorliegen und eine Geschwindigkeit \\(v\\).\n\n\n\n\nTabelle 54.3— Auszug aus Enzymedatensatz.\n\n\n\n\n\n\nS\nv\n\n\n\n\n0\n0.05\n\n\n1\n2.78\n\n\n2\n3.35\n\n\n…\n…\n\n\n48\n11.04\n\n\n49\n9.18\n\n\n50\n11.56\n\n\n\n\n\n\n\n\nSchauen wir uns die Daten einmal in der Abbildung 54.18 an. Wir legen die Konzentration \\(S\\) auf die \\(x\\)-Achse und Geschwindigkeit \\(v\\) auf die \\(y\\)-Achse.\n\nggplot(enzyme_tbl, aes(x = S, y = v)) +\n  theme_minimal() +\n  geom_point() +\n  labs(x = \"[S] / mM\", y = expression(v/\"µmol \" * min^-1))\n\n\n\n\n\n\n\nAbbildung 54.18— Visualisierung der Geschwindigkeit \\(v\\) des Enzyms saure Phosphatase bei verschiedenen Konzentrationen des Substrats Nitrophenolphosphat.\n\n\n\n\n\nDie Reaktionsgleichung abgeleitet aus der Michaelis-Menten-Kinetik lässt sich allgemein wie folgt darstellen. Wir haben die Konzentration \\(S\\) und die Geschwindigkeit \\(v\\) gegeben und wollen nun über eine nicht-lineare Regression die Werte für \\(v_{max}\\) und \\(K_M\\) schätzen.\n\\[\nv = \\cfrac{v_{max} \\cdot S}{K_M + S}\n\\]\nDabei gibt \\(v\\) die initiale Reaktionsgeschwindigkeit bei einer bestimmten Substratkonzentration [S] an. Mit \\(v_{max}\\) beschreiben wir die maximale Reaktionsgeschwindigkeit. Eine Kenngröße für eine enzymatische Reaktion ist die Michaeliskonstante \\(K_M\\). Sie hängt von der jeweiligen enzymatischen Reaktion ab. \\(K_M\\) gibt die Substratkonzentration an, bei der die Umsatzgeschwindigkeit halbmaximal ist und somit \\(v = 1/2 \\cdot v_{max}\\) ist. Wir haben dann die Halbsättigung vorliegen.\nBauen wir also die GLeichung in R nach und geben die Startwerte für \\(v_{max}\\) und \\(K_M\\) für die Funktion nls() vor. Die Funktion nls() versucht jetzt die beste Lösung für die beiden Koeffizienten zu finden.\n\nenzyme_fit &lt;- nls(v ~ vmax * S /( KM + S ), data  = enzyme_tbl,\n                  start = c(vmax = 9, KM = 2))\n\nWir können uns dann die Koeffizienten ausgeben lassen.\n\nenzyme_fit |&gt; \n  model_parameters() |&gt; \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\nvmax      |       11.85\nKM        |        4.28\n\n\nJetzt müssen wir die Michaelis-Menten-Gleichung nur noch um die Koeffizienten ergänzen.\n\\[\nv = \\cfrac{11.85 \\cdot S}{4.28 + S}\n\\]\nIn der Abbildung 54.19 können wir die gefittete Gerade nochmal überprüfen und schauen ob das Modellieren geklappt hat. Ja, hat es die Gerade läuft direkt mittig durch die Punkte.\n\nggplot(enzyme_tbl, aes(x = S, y = v)) +\n  theme_minimal() +\n  geom_point() +\n  geom_line(aes(y = predict(enzyme_fit)), color = \"#CC79A7\") +\n  labs(x = \"[S] / mM\", y = expression(v/\"µmol \" * min^-1))\n\n\n\n\n\n\n\nAbbildung 54.19— Visualisierung der Michaelis-Menten-Kinetik zusammen mit der gefitteten Gerade aus einer nicht-linearen Regression.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nicht lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-non-linear.html#multivariate-fractional-polynomials-mit-mfp",
    "href": "stat-modeling-non-linear.html#multivariate-fractional-polynomials-mit-mfp",
    "title": "54  Nicht lineare Regression",
    "section": "54.5 Multivariate Fractional Polynomials mit {mfp}",
    "text": "54.5 Multivariate Fractional Polynomials mit {mfp}\nManchmal haben wir keine Ahnung, welche mathematische Formel denn überhaupt passen könnte. Ohne eine mathematische Formel können wir dann auch schlecht in nls() Startwerte angeben. Ohne die Angabe von Startwerten für die Formel können wir dann auch nichts rechnen. Die wenigsten Menschen haben eine exponentielle Idee im Kopf, wenn sie eine Kurve sehen. Aus dem Grund wurden die Multivariate Fractional Polynomials (abk. mfp) entwickelt, die dir dann eine Formel wiedergeben. Das Schöne daran ist, dass du einfach nur sagen musst, welche Variable als Polynom in die Formel soll und den Rest macht die Funktion mfp aus dem gleichnamigen R Paket {mfp} dann für sich. Hier sei auch einmal auf das Tutorial Multivariate Fractional Polynomials: Why Isn’t This Used More? verwiesen. Die Entwickler des R Paketes {mfp} haben auch eine eigene Hilfeseite unter Multivariable Fractional Polynomials (MFP) eingerichtet. Wir immer ist das Thema zu groß, daher hier nur die simple Anwendung.\nDer wichtigste Schritt ist einmal die Variable zu bezeichnen, die als Polynom behandelt werden soll. Wir machen das hier mit der Funktion fp(). Wir sagen damit der Funktion mfp(), dass wir bitte die Koeffizienten und eine “Hochzahl” \\(a\\) für das Alter haben wollen. Wir erhalten also die Koeffizienten für die folgende mathematische Formel mit Zahlen wieder.\n\\[\nweight \\sim \\beta_0 + \\beta_1 \\cdot age^{a}\n\\]\nDann rechnen wir also einfach schnelle einmal das Modell.\n\nmfp_fit &lt;- mfp(weight ~ fp(age), data = chicken_tbl)\n\nWir können dann auch gleich einmal in die Zusammenfassung reinschauen und sehen, was dort für eine Formel für das Alter age geschätzt wurde.\n\nsummary(mfp_fit) \n\n\nCall:\nglm(formula = weight ~ I((age/10)^2), data = chicken_tbl)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-487.71   -24.18    -7.03    24.88   562.29  \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     66.300      8.229   8.057 1.62e-14 ***\nI((age/10)^2)  188.226      1.520 123.839  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 13642.51)\n\n    Null deviance: 213533638  on 317  degrees of freedom\nResidual deviance:   4311034  on 316  degrees of freedom\nAIC: 3934.1\n\nNumber of Fisher Scoring iterations: 2\n\n\nEs ist auch möglich sich die Formel direkt wiedergeben zu lassen. Hier ist dann wichtig zu verstehen, was wir dort sehen. Die Funktion mfp() testet immer verschiedene ganzzahlige a-Werte für die Potenz. Dann versucht die Funktion mfp() die restlichen Koeffizienten optimal den Daten anzupassen.\n\nmfp_fit$formula\n\nweight ~ I((age/10)^2)\n&lt;environment: 0x11f9d4588&gt;\n\n\nNun haben wir erstmal die Informationen über das Alter und die Potenz \\(a\\) und wie diese beiden miteinander zusammengehören. Jetzt fehlen aber noch die Koeffizienten für die Steigung und den y-Achsenabschnitt. Die haben wir auch oben in der summary() gesehen, aber wir können uns die Werte auch separat anzeigen lassen.\n\nmfp_fit$coefficients \n\nIntercept     age.1 \n 66.30015 188.22589 \n\n\nDann können wir auch einmal die etwas kompliziertere mathematische Gleichung aufschreiben. Da wir an ganzzahlige Potenzen gebunden sind, muss der Rest etwas anderes aussehen. Aber die Formel geht eigentlich noch. Da wir eben auch eine ganzzahlige Potenz haben, können wir auch selber mit einem Taschenrechner rechnen. Das wird ja bei Kommazahlen schon etwas mühseliger.\n\\[\nweight \\sim 66.3 + 188 \\cdot \\left(\\cfrac{age}{10}\\right)^{2}\n\\]\nDann können wir die Funktion auch schon in R übersetzen. Wir machen eigentlich nichts anderes als das wir das \\(age\\) durch durch \\(x\\) ersetzen, was generischer ist. Und wir brauchen diese Art der Darstellung dann auch in ggplot().\n\nage_func &lt;- \\(x) {66.3 + 188 * (x/10)^2}\n\nIn der Abbildung 54.20 sehen wir einmal das Ergebnis der Anpassung. Ich habe hier einmal beides gemacht, einmal mit der Funktion geom_function() und einmal mit geom_line() und predict(). So kannst du mal sehen, dass wir hier das gleiche rauskriegen. Die erste Variante ist eben die bessere, wenn du auch die mathematische Gleichung angeben wilst. Dann bist du dir sicher, dass auch die Gleichung zu der angepassten Kurve passt.\n\nggplot(chicken_tbl, aes(age, weight)) +\n  geom_function(fun = age_func, color = \"#CC79A7\", linetype = 'dashed') +\n  geom_point() +\n  theme_minimal()\n\nggplot(chicken_tbl, aes(age, weight)) +\n  geom_line(aes(y = predict(mfp_fit)), size = 1, color = \"#0072B2\") +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n(a) … mit geom_function()\n\n\n\n\n\n\n\n\n\n\n\n(b) … mit geom_line() und predict()\n\n\n\n\n\n\n\nAbbildung 54.20— Visualisierung der Hühnchengewichte nach Alter in Tagen mit der geschätzen nicht-linearen Regressionsgleichung nach der Funktion mfp(). Die geschätzten Kurven sind natürlich in beiden Fälle die gleichen Kurven. Es geht hier um die Form der Umsetzung in {ggplot}.\n\n\n\n\nUnd welche der beiden mathematischen Gleichungen ist denn nun besser? Dafür fitten wir nochmal das nls() Modell von oben und vergleichen das Modell einmal zu dem mfp()-Modell von eben.\n\nnls_fit &lt;- nls(weight ~ b0 + I(age^b1), data = chicken_tbl, \n               start = c(b0 = 1, b1 = 1))\n\nIn der Tabelle 54.4 sehen wir die Funktion modelsummary() und den entsprechenden Modellvergleich der beiden Modelle zu Anpassung einer nicht-linearen Regression. Welche der beiden Modelle und damit mathematischen Gleichungen beschreibt unsere Daten besser? Leider ist es so, dass wir aus dem nls()-Modell nicht so viele Informationen erhalten wie es zu wünschen wäre. Wir könnten das AIC als Kriterium nehmen und da gilt, dass ein kleineres AIC besser ist, nehmen wir das mfp()-Modell.\n\nmodelsummary(lst(\"nls Modell\" = nls_fit,\n                 \"mfp Modell\" = mfp_fit))\n\n\n\nTabelle 54.4— Modellvergleich für das nls() Modell mit dem mfp() Modell. Wir vergleichen hier nur die beiden Modelle, da beide Modelle eine mathematische Gleichung wiedergeben, die wir dann berichten können.\n\n\n\n\n\n\n\n\nnls Modell\nmfp Modell\n\n\n\n\nb0\n92.198\n\n\n\n\n(8.319)\n\n\n\nb1\n2.178\n\n\n\n\n(0.002)\n\n\n\n(Intercept)\n\n66.300\n\n\n\n\n(8.229)\n\n\nI((age/10)^2)\n\n188.226\n\n\n\n\n(1.520)\n\n\nNum.Obs.\n318\n318\n\n\nR2\n\n0.980\n\n\nAIC\n3963.3\n3934.1\n\n\nBIC\n3974.6\n3945.4\n\n\nLog.Lik.\n−1978.632\n−1964.050\n\n\nRMSE\n\n116.43\n\n\nisConv\nTRUE\n\n\n\nfinTol\n3.58191038264778e-08\n\n\n\n\n\n\n\n\n\n\n\n\nUnd am Ende nochmal, wie gut war die Anpassung des Modells an die Datenpunkte eigentlich? Hier können wir dann wieder das Bestimmtheitsmaß \\(R^2\\) nutzen. Wie immer siehst du Bestimmtheitsmaß \\(R^2\\) auch in der summary() aber hier dann einmal als direkter Aufruf.\n\nmfp_fit |&gt; r2()\n\n  R2: 0.980\n\n\n\nnls_fit |&gt; r2()\n\n  R2: 0.978\n\n\nDa sich jetzt die beiden Bestimmtheitsmaße \\(R^2\\) wirklich nicht unterscheiden, ist es wohl eher eine Frage des Geschmacks, welche mathematische Formel besser passt. In unserem Beispiel sind ja auch die eher jüngeren Hühner das Problem und nicht so die etwas älteren Hühner. Aber diese Frage lasse ich dann mal offen, du kannst oben die map() Funktion auch mit mfp() laufen lassen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nicht lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-non-linear.html#generalized-additive-models-gam-mit-mgcv",
    "href": "stat-modeling-non-linear.html#generalized-additive-models-gam-mit-mgcv",
    "title": "54  Nicht lineare Regression",
    "section": "54.6 Generalized Additive Models (GAM) mit {mgcv}",
    "text": "54.6 Generalized Additive Models (GAM) mit {mgcv}\nNeben den Modellen, die uns eine mathematische Funktion wiedergeben, gibt es natürlich noch Modelle, die effizienter und besser sind. Diese Effizienz und bessere Modellanpassung bezahlen wir dann aber mit der schwierigeren Darstellbarkeit als Formel. Wenn du also ein Modell anpassen willst, was sehr gut durch Punkte läuft und dann dieses Modell nutzen willst um zukünftige Werte vorherzusagen, dann kannst du Generalized Additive Models (abk. GAM) nutzen. Du erhälst aber keine Formel wieder sondern das Modell liegt dann als Objekt in R vor. Damit kannst du dann arbeiten und Prognosen rechnen aber keine Formel in deine Abbildung schreiben. Es gibt zu GAM einmal das gute Tutorium Advanced Data Analysis from an Elementary Point of View sowie das R Paket {gratia}, welches bei der Darstellung von einem GAM-Modell hilft. Aber nochmal, wenn du eine Formel in deine Abbildung schreiben willst, dann ist das Generalized Additive Model nicht die Antwort auf deine Frage. Selbst wenn ein GAM das beste Modell sein sollte, was du findest. Die Hilfeseite How to solve common problems with GAMs ist auch ein guter Anlaufpunkt, wenn mal eine GAM_modellierung nicht funktionieren will.\nWenn wir jetzt ein GAM rechnen wollen, dann müssen wir einmal über die Funktion s() dem GAM mitteilen, welcher der variablen als Polynom in das Modell rein soll. Das ist sehr ähnlich dem mfp-Modell und der Funktion fp(). Wie immer kannst du der Funktion s() noch zusätzliche Informationen mitgeben aber das übersteigt diese Einführung hier.\n\ngam_fit &lt;- gam(weight ~ s(age), data = chicken_tbl)\n\nUnd dann können wir uns auch schon die Modellgüte des GAM einmal anschauen. Das ist eigentlich das Schöne der Implementierung in dem R Paket {mgvc}, dass wir hier auch alle Helferfunktionen der anderen Pakete nutzen können.\n\ngam_fit |&gt; \n  model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 |    RMSE |   Sigma\n----------------------------------------------------------\n3932.091 | 3932.358 | 3954.508 | 0.980 | 114.984 | 116.835\n\n\nWir sehen auch hier, dass wir ein sehr gutes Modell mit einem Bestimmtheitsmaße \\(R^2\\) von \\(98\\%\\). Wir könnten uns auch die Modellkoeffizienten anschauen, aber leider kriegen wir hier nur die Information, ob der Intercept signifikant unterschiedliche von der Null ist und ob wir einen signifikanten Anstieg haben. Beides ist zwar nett, aber interessiert uns eher nicht in unsere Fragestellungen.\n\ngam_fit |&gt; \n  model_parameters() \n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |           95% CI | t(313.04) |      p\n------------------------------------------------------------------------\n(Intercept) |      683.11 | 6.50 | [670.32, 695.89] |    105.11 | &lt; .001\n\n# Smooth Terms\n\nParameter         |       F |   df |      p\n-------------------------------------------\nSmooth term (age) | 3229.85 | 3.96 | &lt; .001\n\n\nDann können wir auch in der Abbildung 54.21 einmal das Modell sehen. Wir sehen, dass wir eine sehr gute Modellanpassung haben. Wir können also das Modell gut nutzen um das Gewicht von zukünftigen Hünchen zu schätzen. Eine mathematische Formel erhalten wir aber nicht.\n\nggplot(chicken_tbl, aes(age, weight)) +\n  geom_line(aes(y = predict(gam_fit)), size = 1, color = \"#CC79A7\") +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 54.21— Visualisierung der Hühnchengewichte nach Alter in Tagen mit der geschätzen nicht-linearen Regressionsgleichung.\n\n\n\n\n\nDann machen wir das auch gleich mal in dem wir uns drei Hühnchen mit einem Alter von 10, 20 und 30 Tagen vorgeben. Wir wollen jetzt das Gewicht der drei Hühnchen vorhersagen.\n\ntest_tbl &lt;- tibble(age = c(10, 20, 30))\n\nDafür können wir dann die Funktion predict() nutzen in der wir dann zum einen unser Modell gam_fit eingeben sowie die Testdaten test_tbl mit den drei Altersangaben für die drei neuen Hühnchen. Wir erhalten dann das vorhergesagt Gewicht an den drei Zeitpunkten. Ob das jetzt sinnvoll ist oder nicht, hängt wie immer von der Fragestellung ab. Hier sei es einfach einmal präsentiert.\n\npredict(gam_fit, newdata = test_tbl)\n\n        1         2         3 \n 276.0335  826.0534 1783.0346 \n\n\nDamit sind wir schon fast durch mit dem ersten Beispiel und GAM. In {ggplot} musst du aber gar nicht den langen Weg gehen, wenn du nur mit GAM eine Kurve in deine Punkte zeichnen willst. Das geht sehr einfach mit der Funktion stat_smooth(). Du hast dann auch dort die Möglichkeit neben GAM auch andere Arten der Anpassung einer Funktion an deine Daten zu wählen.\nAls zweites Beispiel wollen wir uns nochmal die Wasserlinsen anschauen und sehen, was passiert, wenn wir weniger Messpunkte auf der \\(x\\)-Achse haben als bei unseren Hühnchendaten. Wenn du zu wenige Beobachtungen auf der \\(x\\)-Achse hast, dann kann das GAM-Modell Probleme bekommen eine Anpassung zu rechnen. Hier hat mir dann die Seite How to solve common problems with GAMs sehr geholfen. Ich musste einfach die Anzahl an Dimensionen \\(k\\) für den Glättungsterm auf eine niedrigere Zahl setzen. Wenn du also zu wenige Messwerte hast, dann ist das Setzen von \\(k&lt;5\\) eine guter Startpunkt.\n\nduckweeds_gam_fit &lt;- gam(sensor ~ s(duckweeds_density, k = 3), data = duckweeds_tbl)\n\nDann können wir uns auch schon in der Abbildung 54.22 die Anpassung der Kurve aus GAM an die Beobachtungen anschauen. Da die Methode keine mathematische Formel oder entaprechende Koeffizienten wiedergibt, müssen wir hier mit der predict() Funktion arbeiten. Das Ergebnis sieht aber sehr gut aus, wir laufen mit der Kurve sehr gut durch die Beobachtungen.\n\nggplot(duckweeds_tbl, aes(duckweeds_density, sensor)) +\n  geom_point() +\n  geom_line(aes(y = predict(duckweeds_gam_fit)), size = 1, color = \"#CC79A7\") +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 54.22— Angepasste Kurve aus der Funktion gam().\n\n\n\n\n\nWenn wir GAM nutzen dann können wir uns auf verschiedenen Wegen ein Konfidenzintervall wiedergeben lassen. Ein Vorhersageintervall ist nur mit sehr viel Arbeit und eigenem Programmieren möglich. Mir was das in dem Tutorium Prediction intervals for Generalized Additive Models (GAMs) einfach zu krass. Für mich lohnt es sich nicht auf die Art und Weise ein Vorhersageintervall zu berechnen, da bleibe ich lieber bei den Implementierungen, die es schon gibt. Daher jetzt einmal das Konfidenzintervall aus dem R Paket {ggeffects}, dem R Paket {marginaleffects} sowie dem R Paket {gratia}. Wie immer ist es auch eien Sammlung an Möglichkeiten. Wenn ich entscheiden müsste, dann würde ich das R Paket {marginaleffects} bevorzugen, die Funktionalität ist einfach. Das R Paket {gratia} hat den Vorteil für GAM-Modelle entwickelt zus ein.\n\nMit {marginaleffects}Mit {gratia}Mit {ggeffects}\n\n\nDas R Paket {marginaleffects} erlaubt es ziemlich direkt die vorhergesagten Werte über die Funktion predictions() aus den Daten zu erhalten. Es gibt auch eine Hilfeseite unter GAM – Estimate a Generalized Additive Model. Wir müssen hier noch etwas aufräumen und die doppelten Werte für die Wasserlinsendichte über die Funktion distinct() entfernen. Wir erhalten hier auch etwas mehr Informationen also wir in den anderen Paekten wiedergegeben kriegen.\n\nmarg_pred_tbl &lt;- predictions(duckweeds_gam_fit, newdata = duckweeds_tbl) |&gt; \n  as_tibble() |&gt; \n  select(duckweeds_density, sensor, estimate, std.error, conf.low, conf.high) |&gt; \n  distinct(duckweeds_density, .keep_all = TRUE)\nmarg_pred_tbl \n\n# A tibble: 6 × 6\n  duckweeds_density sensor estimate std.error conf.low conf.high\n              &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1               4.8  0.430    0.489    0.0372    0.416     0.562\n2              14.5  0.965    0.994    0.0228    0.949     1.04 \n3              24.2  1.45     1.43     0.0258    1.38      1.48 \n4              33.9  1.72     1.75     0.0257    1.70      1.80 \n5              43.5  1.98     1.95     0.0227    1.91      2.00 \n6              53.2  2.12     2.09     0.0372    2.02      2.16 \n\n\nIm Folgenden werde ich gleich die Ausgabe marg_pred_tbl nutzen um in dem ggplot die Konfidenzintervalle einmal zu visualisieren. Da wir alle Informationen haben, geht das sehr direkt aus der Funktion heraus.\n\n\nDas R Paket {gratia} erlaubt mit der Funktion confint() die Konfidenzintervalle des GAM-Modells zu schätzen. Hier müssen wir dann einiges angeben. Wenn du die Option shift = TRUE vergisst, dann wird nicht der Intercept auf die Konfidenzintervalle addiert und dein Konfidenzintervall liegt dann sauber auf dem Ursprung. Das war super nervig rauszufinden warum das am Anfang so war. Auch heißt es hier mal data statt newdata, aber das war dann schon nicht das Problem mehr. Auch hier schmeiße ich am Ende alle doppelten Wasserlinsendichten aus den Daten mit der Funktion distinct() raus.\n\ngratia_pred_tbl &lt;- confint(duckweeds_gam_fit, parm = \"s(duckweeds_density)\", \n                           shift = TRUE, type = \"confidence\", data = duckweeds_tbl) |&gt; \n  select(duckweeds_density, est, se, lower, upper) |&gt; \n  distinct(duckweeds_density, .keep_all = TRUE)\ngratia_pred_tbl \n\n# A tibble: 6 × 5\n  duckweeds_density   est     se lower upper\n              &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1               4.8 0.489 0.0372 0.416 0.562\n2              14.5 0.994 0.0228 0.949 1.04 \n3              24.2 1.43  0.0258 1.38  1.48 \n4              33.9 1.75  0.0257 1.70  1.80 \n5              43.5 1.95  0.0227 1.91  2.00 \n6              53.2 2.09  0.0372 2.02  2.16 \n\n\nNeben der Funktion confint() hat das R Paket noch eine weitreichende Fülle an zusätzlichen Funktionen für die Darstellung von komplexeren GAM-Modellen. Es lohnt sich also auf jeden Fall einmal die Hilfeseite von {gratia} zu besuchen und mehr über das R Paket zu erfahren, wenn du tiefergreifend mit der Modellierung von GAM’s beginnen willst.\n\n\nR Paket {ggeffects} liefert mit der Funktion ggpredict() eine super aufgeräumte Funktion um die vorhergesagten Werte von dem Sensor für die Wasserlinsendichte zu erhalten. Dann kriegen wir auch noch ein Konfidenzintervall dazu. Der einzige Manko ist, dass wir die Ausgabe für die weitere Verwertung dann etwas mehr bearbeiten müssen.\n\ngg_pred_obj &lt;- ggpredict(duckweeds_gam_fit, terms = \"duckweeds_density\") \ngg_pred_obj\n\n# Predicted values of sensor\n\nduckweeds_density | Predicted |     95% CI\n------------------------------------------\n             4.80 |      0.49 | 0.41, 0.57\n            14.50 |      0.99 | 0.95, 1.04\n            24.20 |      1.43 | 1.37, 1.48\n            33.90 |      1.75 | 1.69, 1.80\n            43.50 |      1.95 | 1.90, 2.00\n            53.20 |      2.09 | 2.01, 2.17\n\n\nWenn du die Ausgabe dann weiter verwenden willst, dann musst du hier noch etwas mehr Arbeit rein stecken und die Ausgabe dann über die Umwandlung in einen tibble weiterverarbeiten.\n\n\n\nIn der Abbildung 54.23 siehst du dann einmal die angepasste Kurve aus der Funktion gam() zusammen mit dem Konfidenzintervall aus {marginaleffects}. Wenn du oben in den Tabs einmal die Ausgaben aus den Funktionen vergleichst, wirst du feststellen, dass die numerischen Werte alle sehr ähnlich sind. Daher ist es eher eine Geschmacks- und Anwendungsfrage, welches Paket du verwendest. Wenn du viel mit GAM-Modellen rechnest, dann kannst du auch alles sehr gut in der Umgebung des R Paketes {gratia} machen. Die Kurve läuft jedenfalls super durch unser Konfidenzintervall, daher können wir uns ziemlich sicher sein, gute Koeffizienten für das GAM-Modell gefunden zu haben. Wie schon oben geschrieben, an ein mathematisches Modell mit Funktion kommst du in GAM nicht ran. Dafür musst du dann eine andere Implementierung wie nls() oder mfp() nutzen.\n\nggplot(marg_pred_tbl, aes(duckweeds_density, sensor)) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"grey90\") +\n  geom_point() +\n  geom_line(aes(y = estimate), color = cb_pal[2]) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 54.23— Angepasste Kurve aus der Funktion gam() zusammen mit dem Konfidenzintervall aus {marginaleffects}.\n\n\n\n\n\n\n\n\n\n\n\nGam und Loess in {ggplot}\n\n\n\nIn {ggplot} können wir direkt über die Funktion stat_smooth() eine Kurve durch unsere Punkte legen. Wenn du mehr Lesen willst, dann empfehle ich einmal das Tutorium zu GAM and LOESS smoothing. Ich gehe hier nicht weiter auf die Local Regression (LOESS) ein, da wir weder ein Bestimmtheitsmaße \\(R^2\\) noch eine mathematische Geradengleichung. Daher lohnt sich aus meiner Sicht die Anwendung der Funktion loess() einfach in der Praxis nicht. für die Visualisierung mag es aber dann doch sinnvoll sein. Damit können wir auch einfach eine GAM oder Loess-Funktion in ggplot() direkt anwenden ohne über zusätzliche Pakete oder Funktionen zu gehen.\n\nggplot(chicken_tbl, aes(age, weight)) +\n  stat_smooth(method = \"gam\") +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n(a) GAM\n\n\n\n\n\n\n\nAbbildung 54.24— Direkte Anwednung von GAM und Loess in ggplot() ohne eine zusätzliche Funktion.\n\n\n\n\n\nggplot(chicken_tbl, aes(age, weight)) +\n  stat_smooth(method = \"loess\") +\n  geom_point() +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nicht lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-non-linear.html#referenzen",
    "href": "stat-modeling-non-linear.html#referenzen",
    "title": "54  Nicht lineare Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 54.1— Ursprüngliche Abbildung, die nachgebaut werden soll. Eine nicht-lineare Regressionen mit der jeweiligen Regressionsgleichung und dem Bestimmtheitsmaß \\(R^2\\).\nAbbildung 54.2— Visualisierung der nicht-linearen Regression mit einem Polynom aus der Funkion nls().\nAbbildung 54.3— Visualisierung der nicht-linearen Regression auf einem erweiterten Wertebereich von \\(x\\) mit einer exponentiellen Funktion mit nls() in Orange sowie der algorithmisch erstellten Funktion aus dem R Paket {mfp} in Dunkelgrün. Die graue Funktion stellt die ursprüngliche Funktion aus einem Polynom über den gesamten Wertebereich dar.\nAbbildung 54.4— Ursprüngliche Abbildung, die nachgebaut werden soll. Zwei nicht-lineare Regession laufen durch Mittelwert plus/minus Standardabweichung. Im Weiteren sind die Regressionsgleichungen noch ergänzt sowie ein Zielbereich farblich hervorgehoben. Am Ende müssen dann die Achsen noch sauber beschriftet werden.\nAbbildung 54.5— Darstellung der beiden Regressionsgleichungen für \\(KI\\) und \\(KIO_3\\).\nAbbildung 54.6— Einmal die komplexe Abbildung der nicht-linearen Regression in ggplot nachgebaut. Am Ende wurde es dann doch noch eine Legende und keine Beschriftung.\nAbbildung 54.7— Ursprüngliche Abbildung, die nachgebaut werden soll. Zwei nicht-lineare Regession mit zwei \\(y\\)-Achsen.\nAbbildung 54.8— Einmal die komplexe Abbildung der nicht-linearen Regression in ggplot nachgebaut. Für beide Quellen Nadeln und Boden wurde jewiels eine eigene \\(y\\)-Achse erstellt. Die Regressionsgleichungen aus der Orginalabbildung entsprechen nicht den Werten hier in der Abbildung.\nAbbildung 54.9 (a)— Ohne transformierten \\(y\\).\nAbbildung 54.9 (b)— Mit \\(log\\)-transformierten \\(y\\).\nAbbildung 54.10— Visualisierung der Sensorwerte nach Wasserlinsendichte. Pro Dichtewert liegen drei Sensormessungen vor.\nAbbildung 54.11— Visualisierung der Hühnchengewichte nach Alter in Tagen mit der geschätzen nicht-linearen Regressionsgleichung.\nAbbildung 54.12— Visualisierung der Hühnchengewichte nach Alter in Tagen mit der geschätzen nicht-linearen Regressionsgleichung aufgeteilt nach vier Altersgruppen.\nAbbildung 54.13— Angepasste Kurve aus der Funktion nls() dargestellt mit der Funktion geom_function().\nAbbildung 54.14— Angepasste Kurve aus der Funktion nls() dargestellt mit der Funktion geom_line() und predict().\nAbbildung 54.15— Angepasste Kurve aus der Funktion nls() zusammen mit dem Vorhersageintervall.\nAbbildung 54.16— Angepasste Kurve aus der Funktion nls() zusammen mit dem Konfidenzintervall.\nAbbildung 54.17— Visualisierung der Sterberate von Blattläusen nach Aufbringen eines Bio-Pestizides mit der nicht-linearen Regressionsgleichung.\nAbbildung 54.18— Visualisierung der Geschwindigkeit \\(v\\) des Enzyms saure Phosphatase bei verschiedenen Konzentrationen des Substrats Nitrophenolphosphat.\nAbbildung 54.19— Visualisierung der Michaelis-Menten-Kinetik zusammen mit der gefitteten Gerade aus einer nicht-linearen Regression.\nAbbildung 54.20 (a)— … mit geom_function()\nAbbildung 54.20 (b)— … mit geom_line() und predict()\nAbbildung 54.21— Visualisierung der Hühnchengewichte nach Alter in Tagen mit der geschätzen nicht-linearen Regressionsgleichung.\nAbbildung 54.22— Angepasste Kurve aus der Funktion gam().\nAbbildung 54.23— Angepasste Kurve aus der Funktion gam() zusammen mit dem Konfidenzintervall aus {marginaleffects}.\nAbbildung 54.24 (a)— GAM\n\n\n\nArchontoulis SV, Miguez FE. 2015. Nonlinear regression models and applications in agricultural research. Agronomy Journal 107: 786–798.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nicht lineare Regression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-robust-quantile.html",
    "href": "stat-modeling-robust-quantile.html",
    "title": "55  Robuste und Quantilesregression",
    "section": "",
    "text": "55.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, conflicted, broom, quantreg,\n               see, performance, emmeans, multcomp, janitor,\n               parameters, effectsize, MASS, modelsummary,\n               robustbase, multcompView, conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\n#cbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n#                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Robuste und Quantilesregression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-robust-quantile.html#daten",
    "href": "stat-modeling-robust-quantile.html#daten",
    "title": "55  Robuste und Quantilesregression",
    "section": "55.2 Daten",
    "text": "55.2 Daten\nFür unser erstes Beispiel nutzen wir die Daten aus einem Wachstumsversuch mit Basilikum mit vier Bodenbehandlungen und vier Blöcken, die Blöcke sind eigentlich Gewächshaustische. Wir wollen also einen klassischen Gruppenvergleich mit Berücksichtigung der Blockstruktur rechnen.\n\nbasi_tbl &lt;- read_excel(\"data/keimversuch_basilikum_block.xlsx\") |&gt;\n  clean_names() |&gt; \n  mutate(versuchsgruppe = as_factor(versuchsgruppe)) |&gt; \n  select(versuchsgruppe, block_1:block_4)\n\nIn Tabelle 55.1 sehen wir einmal die Daten im Wide-Format. Wir haben also das Frischgewicht der Basilikumpflanzen gemessen und wollen wissen, ob die verschiedenen Bodenarten einen Einfluss auf das Wachstum haben.\n\n\n\n\nTabelle 55.1— Datensatz des Frischegewichts von Basilikumpflanzen auf vier Tischen bzw. Blöcken in vier Versuchsgruppen.\n\n\n\n\n\n\nversuchsgruppe\nblock_1\nblock_2\nblock_3\nblock_4\n\n\n\n\nErde\n16\n21\n23\n23\n\n\nErde\n17\n19\n18\n24\n\n\nErde\n16\n22\n23\n24\n\n\nErde\n9\n17\n18\n21\n\n\nErde\n17\n21\n22\n24\n\n\nErde+Fließ\n18\n22\n21\n21\n\n\n…\n…\n…\n…\n…\n\n\nErde+Perlite\n20\n25\n25\n25\n\n\nPerlite+Fließ\n22\n25\n25\n24\n\n\nPerlite+Fließ\n25\n25\n26\n26\n\n\nPerlite+Fließ\n15\n19\n19\n19\n\n\nPerlite+Fließ\n17\n22\n22\n22\n\n\nPerlite+Fließ\n22\n22\n22\n22\n\n\n\n\n\n\n\n\nDa wir die Daten im Wide-Format vorliegen haben, müssen wir die Daten nochmal in Long-Format umwandeln. Wie immer nutzen wir dafür die Funktion pivot_longer().\n\nbasi_block_tbl &lt;- basi_tbl |&gt; \n  pivot_longer(cols = block_1:block_4,\n               values_to = \"weight\",\n               names_to = \"block\") |&gt; \n  mutate(block = as_factor(block))\n\nIn der Abbildung 55.1 siehst du einmal die Daten als Dotplots mit Mittelwert und Standardabweichung. Wir machen hier mal einen etwas komplizierteren Plot, aber immer nur Barplot ist ja auch langweilig.\n\nggplot(basi_block_tbl, aes(versuchsgruppe, weight, color = block)) +\n  theme_minimal() +\n  scale_color_okabeito() +\n  geom_point(position = position_dodge(0.5), shape = 4, size = 2.5) +\n  stat_summary(fun.data=\"mean_sdl\", fun.args = list(mult = 1), \n               geom=\"pointrange\", position = position_dodge(0.5))  +\n  stat_summary(fun = \"mean\", fun.min = \"min\", fun.max = \"max\", geom = \"line\",\n               position = position_dodge(0.5)) \n\n\n\n\n\n\n\nAbbildung 55.1— Dotplot des Frischegewichts von Basilikumpflanzen auf vier Tischen bzw. Blöcken in vier Versuchsgruppen mit Mittelwert und Standardabweichung.\n\n\n\n\n\nUnser zweiter Datensatz ist ein Anwendungsdatensatz aus dem Gemüsebau. Wir schauen uns das Wachstum von drei Gurkensorten über siebzehn Wochen an. Die Gurkensorten sind hier unsere Versuchsgruppen. Da wir es hier mit echten Daten zu tun haben, müssen wir uns etwas strecken damit die Daten dann auch passen. Wir wollen das Wachstum der drei Gurkensorten über die Zeit betrachten - also faktisch den Verlauf des Wachstums. Wir ignorieren hier einmal die abhängige Datenstruktur über die Zeitpunkte.\n\n\nMit einer abhängigen Datenstruktur müssten wir eigentlich ein lineares gemischtes Modell rechnen. Aber wir nutzen hier die Daten einmal anders.\nIm Weiteren haben wir zwei Typen von Daten für das Gurkenwachstum. Einmal messen wir den Durchmesser für jede Sorte (D im Namen der Versuchsgruppe) oder aber die Länge (L im Namen der Versuchsgruppe). Wir betrachten hier nur das Längenwachstum und deshalb filtern wir erstmal nach allen Versuchsgruppen mit einem L im Namen. Am Ende schmeißen wir noch Spalten raus, die wir nicht weiter brauchen.\n\ngurke_raw_tbl &lt;- read_excel(\"data/wachstum_gurke.xlsx\") |&gt; \n  clean_names() |&gt; \n  filter(str_detect(versuchsgruppe, \"L$\")) |&gt; \n  select(-pfl, -erntegewicht) |&gt; \n  mutate(versuchsgruppe = factor(versuchsgruppe, \n                                 labels = c(\"Katrina\", \"Proloog\", \"Quarto\"))) \n\nIn der Tabelle 55.2 sehen wir einmal die rohen Daten dargestellt.\n\n\n\n\nTabelle 55.2— Datensatz zu dem Längen- und Dickenwachstum von Gurken.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nversuchsgruppe\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nt8\nt9\nt10\nt11\nt12\nt13\nt14\nt15\nt16\nt17\n\n\n\n\nProloog\n5.5\n6.1\n7.4\n8.9\n9.9\n12\n14.4\n17\n19.8\n21.2\n23.2\n24\n29.7\n32.8\nNA\nNA\nNA\n\n\nProloog\n4.6\n5.1\n6.4\n5.7\n5.5\n5.2\n5\n5\n4.5\n0\n0\n0\n0\n0\nNA\nNA\nNA\n\n\nProloog\n5.3\n5.8\n6.8\n8.3\n9\n10\n12.3\n14.6\n17.6\n19.3\n23.1\n23.8\n31.7\n32.3\nNA\nNA\nNA\n\n\nProloog\n5.4\n5.7\n6.9\n8.2\n8.6\n10\n12.1\n14.5\n16.2\n17.1\n19.3\n21.6\n28.5\n30\nNA\nNA\nNA\n\n\nProloog\n5\n5.5\n6.3\n7.5\n8.3\n10\n12.2\n14.4\n16.5\n19.9\n21\n22.9\n30.4\n31\nNA\nNA\nNA\n\n\nProloog\n4.2\n4.6\n5.4\n5.5\n5.2\n5.3\n6.1\n6.5\n8\n9.3\n11\n12.5\n22.3\n24.2\nNA\nNA\nNA\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nKatrina\n3.7\n3.9\n3.9\n4\n4\n4\n4\n4\n4\n4\n4\n4.7\n5.2\n5.4\n5.3\n5.4\n5\n\n\nKatrina\n3\n3.2\n3.3\n3.5\n3.6\n4.4\n5.2\n5.8\n6\n6.2\n6.1\n6.2\n6.8\n7.9\n9.4\n10.4\n13.2\n\n\nKatrina\n3.3\n3.3\n3.4\n3.6\n3.5\n3.5\n3.4\n3.7\n3.5\n3.6\n3.6\n3.6\n2.8\n0\nNA\nNA\nNA\n\n\nKatrina\n3.2\n3.4\n3.7\n4\n4.5\n5.9\n6.9\n8.4\n9.4\n11\n12.2\n13.5\n17.9\n18\nNA\nNA\nNA\n\n\nKatrina\n3.3\n3.4\n3.9\n4.6\n5.2\n6.5\n7.9\n9.5\n10.5\n11.7\n13\n13.4\n17.9\n18\nNA\nNA\nNA\n\n\nKatrina\n3.1\n3.6\n3.6\n3.8\n4.2\n4.9\n5.7\n6.8\n7.7\n9.1\n10.7\n12.3\n17.7\n18.6\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nDann müssen wir die Daten noch in Long-Format bringen. Da wir dann auch noch auf zwei Arten die Daten über die Zeit darstellen wollen, brauchen wir einmal die Zeit als Faktor time_fct und einmal als numerisch time_num. Leider haben wir auch Gurken mit einer Länge von 0 cm, diese Gruken lassen wir jetzt mal drin, da wir ja eine robuste Regression noch rechnen wollen. Auch haben wir ab Woche 14 keine Messungen mehr in der Versuchsgruppe Prolong, also nehmen wir auch nur die Daten bis zur vierzehnten Woche.\n\ngurke_time_len_tbl &lt;- gurke_raw_tbl |&gt; \n  pivot_longer(cols = t1:t17,\n               values_to = \"length\",\n               names_to = \"time\") |&gt; \n  mutate(time_fct = as_factor(time),\n         time_num = as.numeric(time_fct)) |&gt; \n  filter(time_num &lt;= 14)\n\nIn der Abbildung 55.2 sehen wir dann nochmal den Scatterplot für das Gurkenwachstum. Die gestrichtelten Linien stellen den Median und die durchgezogene Line den Mittelwert der Gruppen dar.\n\nggplot(gurke_time_len_tbl, aes(time_num, length, color = versuchsgruppe)) +\n  theme_minimal() +\n  geom_point2(position = position_dodge(0.5)) +\n  stat_summary(fun = \"mean\", geom = \"line\") +\n  stat_summary(fun = \"median\", geom = \"line\", linetype = 2) +\n  scale_x_continuous(breaks = 1:14) +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 55.2— Scatterplot des Längenwachstums der drei Gurkensorten über vierzehn Wochen. Die gestrichtelten Linien stellen den Median und die durchgezogene Line den Mittelwert der Gruppen dar.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Robuste und Quantilesregression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-robust-quantile.html#gewöhnliche-lineare-regression",
    "href": "stat-modeling-robust-quantile.html#gewöhnliche-lineare-regression",
    "title": "55  Robuste und Quantilesregression",
    "section": "55.3 Gewöhnliche lineare Regression",
    "text": "55.3 Gewöhnliche lineare Regression\nDamit wir einen Vergleich zu der robusten Regression und der Quantilsregression haben wollen wir hier zu Anfang nochmal schnell die gewöhnliche Regression (eng. ordinary linear regression), die du schon durch die Funktion lm() kennst. Wir fitten also einmal das Modell mit Interaktionsterm für den Basilikumdatensatz.\n\nbasi_lm_fit &lt;- lm(weight ~ versuchsgruppe + block + versuchsgruppe:block, basi_block_tbl)\n\n\n55.3.1 ANOVA\nDann schauen wir uns nochmal den Interaktionsplot in Abbildung 55.3 mit der Funktion emmip() aus dem R Paket {emmeans} an. Wir sehen, das wir eventuell eine leichte Interaktion vorliegen haben könnten, da sich einige der Geraden überschneiden.\n\nemmip(basi_lm_fit, versuchsgruppe ~ block, CIs = TRUE, \n      cov.reduce = FALSE) +\n  theme_minimal() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 55.3— Interaktionsplot über die Versuchsgruppen und Blöcke.\n\n\n\n\n\nSchauen wir einmal in eine zweifaktorielle ANOVA, ob sich unser leichter Verdacht validieren lässt.\n\nbasi_lm_fit |&gt; \n  anova() |&gt; \n  model_parameters()\n\nParameter            | Sum_Squares | df | Mean_Square |     F |      p\n----------------------------------------------------------------------\nversuchsgruppe       |      158.84 |  3 |       52.95 |  6.53 | &lt; .001\nblock                |      355.34 |  3 |      118.45 | 14.60 | &lt; .001\nversuchsgruppe:block |       83.51 |  9 |        9.28 |  1.14 | 0.346 \nResiduals            |      519.20 | 64 |        8.11 |       |       \n\nAnova Table (Type 1 tests)\n\n\nWir sehen, dass wir mindestens einen paarweisen Unterschied zwischen den Versuchsgruppen und den Blöcken erwarten. Die Interaktion ist nicht signifikant. Betrachten wir noch kurz das \\(\\eta^2\\) um zu sehen, wie viel Varianz jeweils die Versuchsgruppe und Blöcke erklären.\n\nbasi_lm_fit |&gt; \n  eta_squared()\n\n# Effect Size for ANOVA (Type I)\n\nParameter            | Eta2 (partial) |       95% CI\n----------------------------------------------------\nversuchsgruppe       |           0.23 | [0.08, 1.00]\nblock                |           0.41 | [0.24, 1.00]\nversuchsgruppe:block |           0.14 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nLeider erklären hier die Blöcke sehr viel der Varianz, das ist nicht so schön, aber in diesem Kapitel lassen wir es dabei.\n\n\n55.3.2 Gruppenvergleich\nWir haben keine Interaktion vorliegen, alos mitteln wir einmal über alle Blöcke. Wenn du den Code | block zu dem Aufruf der emmeans() Funktion hinzufügst, dann hast du die Analyse für die Blöcke getrennt durchgeführt.\n\nbasi_lm_fit |&gt; \n  emmeans(specs = ~ versuchsgruppe) |&gt;\n  cld(Letters = letters, adjust = \"none\") \n\n versuchsgruppe emmean    SE df lower.CL upper.CL .group\n Erde+Fließ       19.4 0.637 64     18.1     20.7  a    \n Erde             19.8 0.637 64     18.5     21.0  a    \n Perlite+Fließ    22.1 0.637 64     20.8     23.3   b   \n Erde+Perlite     22.6 0.637 64     21.4     23.9   b   \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWenn wir uns die unadjustierten \\(p\\)-Werte anschauen, dann sehen wir einen leichten Effekt zwischen den einzelnen Versuchsgruppen. Warum sage ich leichten Effekt? Die Mittelwerte der Gruppen in der spalte emmean unterscheiden sich kaum. Aber soviel zu der gewöhnlichen Regression, das war ja hier nur der Vergleich und die Erinnerung.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Robuste und Quantilesregression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-robust-quantile.html#sec-robust-reg",
    "href": "stat-modeling-robust-quantile.html#sec-robust-reg",
    "title": "55  Robuste und Quantilesregression",
    "section": "55.4 Robuste Regression",
    "text": "55.4 Robuste Regression\n\n\n\n\n\n\nNur Normalverteilung? Geht auch mehr?\n\n\n\nNein es geht auch mit allen anderen Modellen. So ist auch das glm() für die robuste Regression implementiert. Wir schauen uns aber nur die Grundlagen für die klassische robuste Regression unter der Annahme eines nromalverteilten Outcomes an.\n\n\nBei einer robusten Regression werden jeder Beobachtung auf der Grundlage ihres Residuums unterschiedliche Gewichte (eng. robustness weights) von 0 bis 1 zugewiesen. Wir kennen ja ein Residuum als die Differenz zwischen den beobachteten und den vorhergesagten Werten der Daten. Die vorhergesagten Daten sind ja die Punkte auf der Geraden. Je kleiner also das Residuum ist, desto größer ist die Gewichtung und desto näher liegt eine Beobachtung an der geschätzten Geraden. Wir bestrafen also Punkte, die weit weg von unserer potenziellen Gerade liegen.\n\n\nEin englisches Tutorium gibt es dann nochmal ausführlicher unter R demo | Robust Regression (don’t depend on influential data!)\nEbenso liefert auch das Tutorium Robust regression | R data analysis example einen Überblick.\nSchauen wir uns das Problem an einem kleinen Spieldatensatz einmal an, den es so in vielen Geschmacksrichtungen gibt. Wir haben acht Beobachtungen mit jeweils einem \\(x\\) und einem \\(y\\) Wert in unserem Datensatz reg_tbl. Wie folgt einmal dargestellt.\n\nreg_tbl &lt;- tibble(x = c(0, 1, 2, 3, 4, 5, 6, 7),\n                  y = c(1, 1.4, 2.5, 2.7, 4.3, 5.2, 0, 6.7))\n\nWir rechnen jetzt einmal eine gewöhnliche lineare Regression mit der Funktion lm() und eine robuste Regression mit dem Paket {MASS} und der Funtkion rlm(). Es gibt noch das R Paket {robustbase} und der Funktion rq() aber wir können die Ergebnisse dann nicht in {emmeans} weiter nutzen, was uns dann etwas den Weg mit versperrt. Wir können aber für die multiplen Mittelwertsvergleich mit {robustbase} das R Paket {multcomp} nutzen. Aber fangen wir hier erstmal mit dem Beispiel an und vergleichen die beiden Implementierungen der robusten Regression zu der gewöhnlichen Regression\n\nreg_lm_fit &lt;- lm(y ~ x, reg_tbl)\nreg_rlm_fit &lt;- rlm(y ~ x, reg_tbl)\nreg_lmrob_fit &lt;- lmrob(y ~ x, reg_tbl)\n\nIn der Abbildung 55.4 sehen wir einmal den Unterschied zwischen den beiden robusten Regressionen und der gewöhnlichen Regression.\n\nggplot(reg_tbl, aes(x, y, label = x)) +\n  theme_minimal() +\n  geom_label() +\n  geom_line(aes(y = predict(reg_lm_fit), colour = \"lm\")) +\n  geom_line(aes(y = predict(reg_rlm_fit), colour = \"rlm\")) +\n  geom_line(aes(y = predict(reg_lmrob_fit), colour = \"lmrob\")) +\n  scale_colour_okabeito() +\n  labs(colour = \"Method\")\n\n\n\n\n\n\n\nAbbildung 55.4— Scatterplot für den Unterschied zwischen einer robusten und gewöhnlichen Regression. Die gewöhnliche Regression ist gelb, die robuste Regression MASS::rlm() grün und die robuste Regression robustbase::lmrob() blau.\n\n\n\n\n\nIn unserem Beispiel ist die sechste Beobachtung der Ausreißer. Die lm-Regression wird durch den Wert 0 der sechsten Beobachtung nach unten gezogen. Die sechste Beobachtung erhält ein viel zu starkes Gewicht in der gewöhnlichen Regression im Bezug auf den Verlauf der Geraden. Die robusten Regressionen geben der sechsten Beobachtung hingegen ein viel geringeres Gewicht, so dass die Beobachtung keinen Einfluss auf den Verlauf der Geraden hat. In einer robusten Regression erhält eine Beobachtung mit einem großen Residuum sehr wenig Gewicht, in diesem konstruierten Beispiel fast 0. Daher können wir sagen, das unser Modell robust gegen Ausreißer ist.\nOkay, soweit lernt es jeder und die Idee der robusten Regression ist auch ziemlich einleuchtend. Aber in der Wissenschaft zeichnen wir keine Gerade durch Punkte und freuen uns drüber, dass die eine gerade besser passt als die andere Gerade. Wir nutzen dann ja das Modell weiter in eine ANOVA oder aber eben in einem paarweisen Gruppenvergleich. Und hier fangen dann die Probleme der Implementierung an. Leider ist es so, dass nicht alle Funktionalitäten implementiert sind. Das heißt, die Funktion anova() oder emmeans() kann mit der Ausgabe der robusten Regressionen mit MASS::rlm() und robustbase::lmrob() nichts anfangen. Das ist problematisch und nicht anwenderfreundlich. Und wir sind hier nunmal Anwender…\nWarum so unbeliebt? Das ist eine gute Frage. Zu der robusten Regression gibt es eine lange Geschichte mit vielen Wirrungen und Irrungen. Am Ende kannst du in Wikipedia noch etwas zur History and unpopularity of robust regression nachlesen. Nach Stromberg (2004) hat es auch etwas mit der schlechten und mangelhaften Implementierung in gängiger Statistiksoftware zu tun. Das merke ich hier auch gerade, als ich versuche die Funktionen zusammen zubauen und miteinander zu vernetzen. Sicherlich gibt es auch die biologische Diskussion. Gibt es in biologischen Daten eigentlich überhaupt Ausreißer? Oder müssten wir nicht Ausreißer besonders betrachten, weil die Ausreißer dann doch mehr über die Daten verraten? Macht es also Sinn die Ausreißer einfach platt zu bügeln und nicht mehr weiter zu beachten oder ist nicht die Beobachtung Nr.6 oben von Interesse? Denn vielleicht ist was mit dem Experiment schief gelaufen und diese Beobachtung ist gerade ein Anzeichen dafür. Dann müssten auch die anderen Messwerte kritisch hinterfragt werden.\nWie immer es auch sein. Wir fitten jetzt mal die beiden robusten Regression in der Standardausstattung mit der Funktion MASS::rlm() aus dem Paket {MASS} wie folgt.\n\nbasi_rlm_fit &lt;- rlm(weight ~ versuchsgruppe + block + versuchsgruppe:block, basi_block_tbl)\n\nDann natürlich noch die Funktion robustbase::lmrob() aus dem Paket {robustbase}. Hinter dem Paket {robusbase} steht ein ganzen Buch von Maronna u. a. (2019), aber leider keine gute Internetseite mit Beispielen und Anwendungen in R, wie es eigentlich heutzutage üblich ist. Aber dennoch, hier einmal das Modell.\n\nbasi_lmrob_fit &lt;- lmrob(weight ~ versuchsgruppe + block + versuchsgruppe:block, basi_block_tbl)\n\n\n55.4.1 ANOVA\nDann wollen wir einmal eine zweifaktorielle ANOVA auf den Modell der robusten Regression mit der Funktion MASS::rlm() rechnen. Leider kann ich schon gleich sagen, dass wir hier ein Problem kriegen. Es ist nicht ganz klar unter Theoretikern welche Anzahl an Beobachtungen für die Residuen in der robusten Regression genutzt werden soll. Welche Beobachtungen werden den im Modell berücksichtigt und welche nicht? Daher ist dann der Wert für die Freiheitsgrade \\(df\\) leer und wir erhalten keine \\(F\\)-Statistik und dann auch keine \\(p\\)-Werte. Für mich super unbefriedigend, so kann man dann auch keine Funktionen richtig nutzen. Die Funktion robustbase::lmrob() bietet die ANOVA nicht an.\n\nbasi_rlm_aov &lt;- basi_rlm_fit |&gt; \n  anova() |&gt; \n  tidy()\nbasi_rlm_aov\n\n# A tibble: 4 × 6\n  term                    df sumsq meansq statistic p.value\n  &lt;chr&gt;                &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 versuchsgruppe           3 114.   37.9         NA      NA\n2 block                    3 318.  106.          NA      NA\n3 versuchsgruppe:block     9  63.7   7.07        NA      NA\n4 Residuals               NA 527.   NA           NA      NA\n\n\nWas als Freiheitsgrad für die Residuen nehmen? In unserem Datensatz haben wir 80 Basilikumpflanzen und somit auch Beobachtungen vorliegen. Wir setzen jetzt einfach brutal die Freiheitsgrade df für die Residuen auf 80 und können dann die Mean Squares für die Residuen berechnen. Also einmal flott die meansq für die Residuen durch die Anzahl an Beobachtungen geteilt.\n\nms_resid &lt;- 527 / 80\n\nMit den Mean Square der Residuen können wir dann auch die \\(F\\)-Werte berechnen. Den rechnerischen Weg kennen wir ja schon aus der zweifaktoriellen ANOVA.\n\nf_calc &lt;- basi_rlm_aov$meansq / ms_resid\n\nWie kriegen wir jetzt aus den ganzen berechneten \\(F\\)-Werten die entsprechenden \\(p\\)-Werte? Die \\(p\\)-Werte sind die Fläche rechts von den berechneten Werten. Wir können die Funktion pf() nutzen um diese Flächen zu berechnen. Wir machen das einmal beispielhaft für einen \\(F_{calc} = 3.41\\) für 4 Gruppen mit \\(df_1 = 4 - 1 = 3\\) und 12 Beobachtungen \\(df_2 = 12\\). Die Berechnung von \\(df_2\\) ist statistisch falsch aber für unseren MacGyver Hack hinreichend.\n\npf(3.41, 3, 12, lower.tail = FALSE) |&gt; round(3)\n\n[1] 0.053\n\n\nUnd siehe da, wir erhalten einen \\(p\\)-Wert von \\(0.053\\). Nun können wir das nicht nur für einzelnen \\(F\\)-Werte machen sondern auch für unseren ganzen Vektor f_calc. Als \\(df_2\\) setzen wir die Anzahl an Basilikumpflanzen.\n\np_vec &lt;- pf(f_calc, c(3, 3, 9), c(80, 80, 80), lower.tail = FALSE) |&gt; \n  scales::pvalue()\n\nDie berechneten \\(F\\)-Werte und \\(p\\)-Werte können wir jetzt in die ANOVA Tabelle ergänzen.\n\nbasi_rlm_aov |&gt; \n  mutate(statistic = f_calc, \n         p.value = p_vec)\n\n# A tibble: 4 × 6\n  term                    df sumsq meansq statistic p.value\n  &lt;chr&gt;                &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 versuchsgruppe           3 114.   37.9       5.75 0.001  \n2 block                    3 318.  106.       16.1  &lt;0.001 \n3 versuchsgruppe:block     9  63.7   7.07      1.07 0.391  \n4 Residuals               NA 527.   NA        NA    &lt;NA&gt;   \n\n\nDas war jetzt ein ganz schöner Angang und den \\(p\\)-Werten ist nur approximativ zu trauen, aber wenn wir weit vom Signifikanzniveau \\(\\alpha\\) gleich 5% sind, dann macht eine numerische Ungenauigkeit auch nicht viel aus. Achtung deshalb bei \\(p\\)-Werten nahe des Signifikanzniveau \\(\\alpha\\), hier ist dann die Aussage mit Vorsicht zu treffen oder eher nicht möglich.\nGlücklicherweise können wir aus den Sum of squares dann unsere \\(\\eta^2\\)-Werte für die erklärte Varianz unserer ANOVA berechnen. Das ist dann doch schon mal was. Wir sehen, dass die Blöcke am meisten der Varianz erklären und die Versuchsgruppen sehr wenig. Das ist für das Modell dann nicht so schön, deckt sich aber mit den Ergebnissen gewöhnlichen Regression. Wir erhalten eine Konfidenzintervalle, woran du schon erkennen kannst, dass in der Ausgabe der ANOVA der robusten Regression was fehlt.\n\nbasi_rlm_fit |&gt; \n  eta_squared()\n\n# Effect Size for ANOVA (Type I)\n\nParameter            | Eta2 (partial) |       95% CI\n----------------------------------------------------\nversuchsgruppe       |           0.23 | [0.08, 1.00]\nblock                |           0.41 | [0.24, 1.00]\nversuchsgruppe:block |           0.14 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nNochmal als Erinnerung, mit der Funktion robustbase::lmrob() ist die ANOVA nicht möglich. Die Funktionalität ist nicht implementiert.\n\n\n55.4.2 Gruppenvergleich\nDafür geht der Gruppenvergleich für die robuste Regression mit MASS::rlm() wie von alleine. Wir haben keine Interaktion vorliegen, daher müssen wir auch nicht den Block berücksichtigen. Das wir in Block 1 ein Problem haben, steht dann aber auf einem anderen Blatt. Da aber alle Basilikumpflanzen immer im Block 1 kleiner sind, passt es dann wieder im Sinne keiner Interaktion. Mit Interaktion hätten wir sonst versuchsgruppe | block statt nur versuchsgruppe hinter die Tilde geschrieben. Dann wollen wir noch das compact letter disply und alles ist wie es sein sollte.\n\nbasi_rlm_fit |&gt; \n  emmeans(specs = ~ versuchsgruppe) |&gt;\n  cld(Letters = letters, adjust = \"none\") \n\n versuchsgruppe emmean    SE df asymp.LCL asymp.UCL .group\n Erde+Fließ       19.7 0.668 NA      18.4      21.0  a    \n Erde             19.9 0.668 NA      18.6      21.2  a    \n Perlite+Fließ    22.2 0.668 NA      20.9      23.5   b   \n Erde+Perlite     22.4 0.668 NA      21.1      23.7   b   \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWir sehen also, dass sich Erde+Fließ und Erde nicht unterscheiden. Die beiden aber dann von Perlite+Fließ sowie Erde+Perlite. Am Ende unterscheiden sich Perlite+Fließ und Erde+Perlite nicht.\nFür die Funktion robustbase::lmrob() müssen wir dann auf das R Paket {multcomp} umswitchen. Hier kriegen wir dann ein Problem, wenn wir eine Interaktion vorliegen ahben, da ist {multcomp} nicht so schön anzuwenden, wie dann {emmeans}. Aber hier schauen wir uns dann mal die einfache Implementierung ohne Interaktion an. Bei einer Interaktion müssten wir dann händisch über die Funktion filter() die Analysen für die einzelnen Blöcke aufspalten.\nWir lassen jetzt aber erstmal auf dem Modell aus der Funktion robustbase::lmrob() den paarweisen Vergleich nach Tukey laufen.\n\nmult_lmrob &lt;- glht(basi_lmrob_fit, linfct = mcp(versuchsgruppe = \"Tukey\"))\n\nDas erhaltende Objekt können wir dann mit tidy() aufräumen und uns dann die gewünschten Spalten wiedergeben lassen.\n\nmult_lmrob_tbl &lt;- mult_lmrob |&gt; \n  tidy(conf.int = TRUE) |&gt; \n  select(contrast, estimate, conf.low, conf.high, adj.p.value) \n\nmult_lmrob_tbl\n\n# A tibble: 6 × 5\n  contrast                     estimate conf.low conf.high adj.p.value\n  &lt;chr&gt;                           &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Erde+Fließ - Erde              1.68      -1.78      5.14       0.592\n2 Erde+Perlite - Erde            1.64      -2.37      5.64       0.715\n3 Perlite+Fließ - Erde           4.84      -1.33     11.0        0.180\n4 Erde+Perlite - Erde+Fließ     -0.0436    -2.86      2.77       1.00 \n5 Perlite+Fließ - Erde+Fließ     3.16      -2.50      8.83       0.472\n6 Perlite+Fließ - Erde+Perlite   3.21      -2.77      9.19       0.508\n\n\nWir sehen, dass wir mit der Adjustierung keinen signifikanten Unterschied zwischen den Bodensorten finden. Im Folgenden dann noch die Darstellung im compact letter display, wo wir etwas rumfrickeln müssen damit die Funktion multcompLetters() auch die Kontraste aus der Funktion glht() akzeptiert. Aber das haben wir dann alles zusammen.\n\nmult_lmrob_tbl |&gt; \n  mutate(contrast = str_replace_all(contrast, \"\\\\s\", \"\")) |&gt; \n  pull(adj.p.value, contrast) |&gt; \n  multcompLetters() |&gt; \n  pluck(\"Letters\")\n\n   Erde+Fließ  Erde+Perlite Perlite+Fließ          Erde \n          \"a\"           \"a\"           \"a\"           \"a\" \n\n\nAuch hier sehen wir keinen Unterschied im compact letter display für die adjustierten \\(p\\)-Werte. Für die unadjustierten rohen \\(p\\)-Werte nutzen wir dann den Umweg über die summary() Funktion. Die unadjustierten \\(p\\)-Werte können wir dann auch in das compact letter display umwandeln lassen.\n\nsummary(mult_lmrob, test = adjusted(\"none\")) |&gt; \n  tidy() |&gt; \n  mutate(contrast = str_replace_all(contrast, \"\\\\s\", \"\")) |&gt; \n  pull(p.value, contrast) |&gt; \n  multcompLetters() |&gt; \n  pluck(\"Letters\")\n\n   Erde+Fließ  Erde+Perlite Perlite+Fließ          Erde \n         \"ab\"          \"ab\"           \"a\"           \"b\" \n\n\nUnd wenn man eine andere Funktion nutzt, dann kommen auch leicht andere Ergebnisse raus. Bei den rohen \\(p\\)-Werten haben wir jetzt etwas andere Unterschiede. Aber das liegt auch an den sehr ähnlichen Mittelwerten. So groß sind die Unterschiede nicht, so dass wir hier natürlich durch unterschiedliche Algorithmen leicht andere Ergebnisse kriegen. Wieder ein Grund die \\(p\\)-Werte zu adjustieren um dann auch konsistente Ergebnisse zu haben.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Robuste und Quantilesregression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-robust-quantile.html#sec-quantile-reg",
    "href": "stat-modeling-robust-quantile.html#sec-quantile-reg",
    "title": "55  Robuste und Quantilesregression",
    "section": "55.5 Quantilsregression",
    "text": "55.5 Quantilsregression\nJetzt wollen wir uns als Spezialfall der robusten Regression einmal die Quantilesregression anschauen. Wir der Name schon sagt, optimiert die Quantilregression nicht über den Mittelwert und die Standardabweichung als quadratische Abweichungen sondern über den Median und die absoluten Abweichungen. Wir haben es also hier mit einer Alternativen zu der gewöhnlichen Regression zu tun. Das schöne an der Quantilesregression ist, dass der Median nicht so anfällig gegenüber Ausreißern ist. Hier haben wir einen Vorteil. Wir können theoretisch auch andere Quantile als Referenzwert nehmen und die absoluten Abstände berechnen. Wenn wir aber später dann mit emmeans die Gruppenvergleiche rechnen wollen, geht das nur über den Median.\n\n\nEin englisches Tutorium gibt es dann nochmal ausführlicher unter Quantile Regression as an useful Alternative for Ordinary Linear Regression\nUm die Quantilesregression zu rechnen nutzen wir das R Paket {quantreg} und die Funktion rq(). Wichtig ist hier, dass wir das tau auf \\(0.5\\) setzen und somit dann eine Median-Regression rechnen. Wir können theoretisch auch andere Quantile als Referenz wählen, aber dann passt es nicht mehr mit emmeans(). Wir nutzen hier auch nochmal die Gelegenheit zwei Modelle mit anzupassen. Wir rechnen einmal ein Modell mit einer linearen Komponente der Zeit time_num und einmal ein Modell mit der quadratischen Komponente der Zeit mit poly(time_num, 2) was nicht anderes als time_num\\(^2\\) ist. Neben dieser Variante ist mit nlrq() auch eine Möglichkeit implementiert um noch komplexere nicht lineare Zusammenhänge zu modellieren. Hier dann einfach mal die Hilfeseite von ?nlrq anschauen.\n\ntime_rq_lin_fit &lt;- rq(length ~ versuchsgruppe + time_num + versuchsgruppe:time_num, tau = 0.5,\n                      gurke_time_len_tbl)\ntime_rq_quad_fit &lt;- rq(length ~ versuchsgruppe * poly(time_num, 2), tau = 0.5, \n                       gurke_time_len_tbl)\n\nIn der Abbildung 55.5 plotten wir einmal die Modelle aus der linearen und der quadratischen Anpassung des zeitlichen Verlaufs in die Abbildung. Wir sehen, dass das Modell mit der quadratischen Anpassung besser zu den Daten passt. Daher nutzen wir jetzt im weiteren das Modell time_rq_quad_fit.\n\nggplot(gurke_time_len_tbl, aes(time_num, length, color = versuchsgruppe)) +\n  theme_minimal() +\n  geom_point2(position = position_dodge(0.5)) +\n  scale_color_okabeito() +\n  geom_line(aes(y = predict(time_rq_quad_fit), linetype = \"Quadratic\")) +\n  geom_line(aes(y = predict(time_rq_lin_fit), linetype = \"Linear\")) +\n  scale_x_continuous(breaks = 1:14) +\n  labs(linetype = \"\", color = \"\")\n\n\n\n\n\n\n\nAbbildung 55.5— Scatterplot des Längenwachstums der drei Gurkensorten über vierzehn Wochen. Die gestrichtelten Linien stellen den Median und die durchgezogene Line den Mittelwert der Gruppen dar.\n\n\n\n\n\n\n55.5.1 ANOVA\nMachen wir es kurz. Die ANOVA ist für die Quantilsregression nicht implementiert und damit auch nicht anwendbar. Damit können wir aber leben, wenn wir die ANOVA nur als Vortest ansehen. Wir müssen dann eine mögliche Interaktion visuell überprüfen. Um die Interaktion visuell zu überprüfen nutzen wir die Funktion emimp() aus dem R Paket {emmeans}. Wir sehen in der Abbildung 55.6, dass sich die Gerade für die Versuchsgruppen über die Zeit nicht schneiden, was gegen eine starke Interaktion spricht. Die Steigung ist aber für alle drei Versuchsgruppen über die Zeit nicht gleich, wir haben zumindest eine mittlere Interaktion.\n\nemmip(time_rq_lin_fit, versuchsgruppe ~ time_num, CIs = TRUE, \n      cov.reduce = FALSE) +\n  theme_minimal() +\n  scale_color_okabeito()\n\nemmip(time_rq_quad_fit, versuchsgruppe ~ time_num, CIs = TRUE, \n      cov.reduce = FALSE) +\n  theme_minimal() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\n\n\n\n\n(a) Linear\n\n\n\n\n\n\n\n\n\n\n\n(b) Quadratic\n\n\n\n\n\n\n\nAbbildung 55.6— Interaktionsplot über den zeitlichen Verlauf für alle drei Sorten für die Quantilesregression.\n\n\n\n\nWir würden also in unseren Gruppenvergleich der Mediane auf jeden Fall die Interaktion mit rein nehmen und die emmeans() Funktion entsprechend mit dem | anpassen.\n\n\n55.5.2 Gruppenvergleich\nWenn wir den Gruppenvergleich in emmeans() rechnen wollen, dann geht es nur dem Spezialfall der Median-Regression. Wir müssen also zwangsweise in der Quantilesregression rq() das tau = 0.5 setzen um dann eine Median-Regression zu rechnen. Sonst können wir nicht emmeans nutzen.\n\ntime_rq_quad_fit |&gt; \n  emmeans(~ versuchsgruppe | time_num, \n          adjust = \"none\", at = list(time_num = c(1, 7, 14))) |&gt;\n  cld(Letters = letters, adjust = \"none\") \n\ntime_num =  1:\n versuchsgruppe emmean     SE  df lower.CL upper.CL .group\n Quarto           2.30 0.0513 243     2.20     2.40  a    \n Katrina          3.30 0.2879 243     2.73     3.87   b   \n Proloog          5.00 0.2243 243     4.56     5.44    c  \n\ntime_num =  7:\n versuchsgruppe emmean     SE  df lower.CL upper.CL .group\n Quarto           3.83 0.2467 243     3.35     4.32  a    \n Katrina          5.20 0.3184 243     4.57     5.83   b   \n Proloog         11.97 0.2938 243    11.39    12.55    c  \n\ntime_num = 14:\n versuchsgruppe emmean     SE  df lower.CL upper.CL .group\n Quarto           7.30 1.7815 243     3.79    10.81  a    \n Katrina         11.36 3.1038 243     5.25    17.47  a    \n Proloog         30.06 1.6609 243    26.79    33.34   b   \n\nConfidence level used: 0.95 \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nDas sieht ja schon mal ganz gut aus. Interessant ist, dass wir an dann in der vierzehnten Woche dann keinen signifikanten Unterschied mehr vorliegen zwischen Quarto und Katrina vorliegen haben. Das liegt dann aber alleinig an dem hohen Standardfehler SE der Sorte Katrina mit \\(3.1\\) gegenüber den anderen Sorten. Vermutlich sehe das Ergebnis leicht anders aus, wenn wir die unsinnigen Gurken mit einer Länge von 0 cm wirklich aus den Dten entfernen würden und nicht zu Demonstrationszwecken wie hier drin lassen würden.\nAber Achtung, die Spalte emmean beschriebt hier die Mediane. Wir haben also hier die Mediane des Längenwachstums für die Gurken vorliegen. Wenn wir die paarweisen Vergleich rechnen wollen würden dann können wir noch pairwise ~ versuchsgruppe statt ~ versuchsgruppe schreiben. Auch liefert die Funktion pwpm() die Medianunterschiede wie wir im Folgenden einmal sehen. Ich habe hier mal das at entfernt und da die Zeit numrisch ist, haben wir dann auch nur noch eine Tabelle. Die Diagonal: [Estimates] (emmean) sind die Mediane des Längenwachstums.\n\ntime_rq_quad_fit |&gt; \n  emmeans(~ versuchsgruppe | time_num, adjust = \"none\") |&gt;\n  pwpm()\n\n        Katrina Proloog  Quarto\nKatrina [ 5.50]  &lt;.0001  0.0095\nProloog   -7.41 [12.91]  &lt;.0001\nQuarto     1.48    8.89 [ 4.02]\n\nRow and column labels: versuchsgruppe\nUpper triangle: P values   adjust = \"tukey\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Robuste und Quantilesregression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-robust-quantile.html#modellvergleich",
    "href": "stat-modeling-robust-quantile.html#modellvergleich",
    "title": "55  Robuste und Quantilesregression",
    "section": "55.6 Modellvergleich",
    "text": "55.6 Modellvergleich\nIm Folgenden wollen wir einmal verschiedene Modelle miteinander vergleichen und schauen, welches Modell hier das beste ist. Das machen wir dann einmal für die Basilikumdaten sowie die Wachstumsdaten für die Gurken.\n\n55.6.1 Basilikumdaten\nFür den Modellvergleich der gewöhnlichen, robusten und medianen Regression nutzen wir nochmal den Datensatz für das Basilikumwachstum. In einem ersten Schritt fitten wir wieder alle Modelle und achten darauf, dass wir bei der Quantilesregression angeben welches Quantile wir wollen. Wir wählen mit tau = 0.5 dann den Median und rechnen so eine Medianregression.\n\nbasi_lm_fit &lt;- lm(weight ~ versuchsgruppe + block + versuchsgruppe:block, basi_block_tbl)\nbasi_rlm_fit &lt;- rlm(weight ~ versuchsgruppe + block + versuchsgruppe:block, basi_block_tbl)\nbasi_lmrob_fit &lt;- lmrob(weight ~ versuchsgruppe + block + versuchsgruppe:block, basi_block_tbl)\nbasi_rq_fit &lt;- rq(weight ~ versuchsgruppe + block + versuchsgruppe:block, basi_block_tbl,\n                  tau = 0.5)\n\nIm Folgenden nutzen wir dann das fantastische Paket {modelsummary} mit der gleichnamigen Funktion um uns einmal die Modelle im Vergleich anzuschauen. Hier hilft dann wie immer die tolle Hilfsseite von {modelsummary} zu besuchen. Ich möchte nur nicht den Intercept und die Schätzer für die Blöcke haben, deshalb fliegen die hier einmal raus.\n\nmodelsummary(lst(\"Ordinary\" = basi_lm_fit,\n                 \"MASS::rlm\" = basi_rlm_fit,\n                 \"robustbase::lmrob\" = basi_lmrob_fit,\n                 \"Quantile\" = basi_rq_fit),\n             estimate  = \"{estimate}\",\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"),\n             coef_omit = \"Intercept|block\")\n\n\n\n\n\n\nOrdinary\nMASS::rlm\nrobustbase::lmrob\nQuantile\n\n\n\n\nversuchsgruppeErde+Fließ\n2.200\n1.515\n1.675\n1.000\n\n\n\n[−1.399, 5.799]\n[−2.257, 5.286]\n[−1.053, 4.403]\n[−0.374, 2.374]\n\n\n\ns.e. = 1.801\ns.e. = 1.888\ns.e. = 1.365\ns.e. = 0.701\n\n\n\nt = 1.221\nt = 0.802\nt = 1.227\nt = 1.426\n\n\n\np = 0.226\np = 0.425\np = 0.224\np = 0.159\n\n\nversuchsgruppeErde+Perlite\n2.200\n1.515\n1.631\n0.000\n\n\n\n[−1.399, 5.799]\n[−2.257, 5.286]\n[−1.529, 4.791]\n[−3.073, 3.073]\n\n\n\ns.e. = 1.801\ns.e. = 1.888\ns.e. = 1.582\ns.e. = 1.568\n\n\n\nt = 1.221\nt = 0.802\nt = 1.031\nt = 0.000\n\n\n\np = 0.226\np = 0.425\np = 0.306\np = 1.000\n\n\nversuchsgruppePerlite+Fließ\n5.200\n4.683\n4.841\n6.000\n\n\n\n[1.601, 8.799]\n[0.911, 8.455]\n[−0.037, 9.718]\n[1.045, 10.955]\n\n\n\ns.e. = 1.801\ns.e. = 1.888\ns.e. = 2.442\ns.e. = 2.528\n\n\n\nt = 2.887\nt = 2.480\nt = 1.982\nt = 2.373\n\n\n\np = 0.005\np = 0.016\np = 0.052\np = 0.021\n\n\nNum.Obs.\n80\n80\n80\n80\n\n\nR2\n0.535\n\n0.501\n0.480\n\n\nR2 Adj.\n0.426\n\n0.384\n\n\n\nAIC\n410.7\n411.8\n\n404.5\n\n\nBIC\n451.1\n452.3\n\n442.7\n\n\nLog.Lik.\n−188.326\n−188.914\n\n\n\n\nF\n4.912\n3.903\n\n\n\n\nRMSE\n2.55\n2.57\n2.55\n2.69\n\n\n\n\n\n\n\n\nWie wir sehen sehen haben nicht alle Modelle die gleichen Informationen zurück gegeben. Insbesondere das fehlen des Bestimmtheitsmaßes \\(R^2\\) bei der robusten Regression MASS::rlm ist schmerzlich, da wir hier dann nicht die Möglichkeit haben eine Aussage über die erklärte Varianz der robusten Regression zu treffen. Auch fehlt bei der Medianregression das adjustierte \\(R^2\\), was die Nutzung bei Modellen mit mehr als einer Einflussvariable \\(x\\) im Modell erschwert bis nutzlos macht. Da ist dann die Funktion aus dem Paket {robustbase} besser, hier haben wir dann ein \\(R^2\\) vorliegen. Daher bleibt uns am Ende nur das AIC oder BIC, wobei wir dort den kleinsten Wert als besser erachten. Die AIC und BIC Werte sind somit am besten für die Quantilesregression. Für die Funktion robustbase::lmrob haben wir dann kein AIC. Dafür ist dann aber der Fehler RMSE bei der gewöhnlichen Regressionzusammen mit der Funktion robustbase::lmrob am niedrigsten. Und so stehe ich wieder davor und weiß nicht was das beste Modell ist. Hier müsste ich dann nochmal überlegen, ob ich lieber über Mittelwerte oder Mediane berichten möchte und das ist ohne die Forschungsfrage nicht hier zu lösen.\n\n\n55.6.2 Gurkendaten\nFür den Modellvergleich der gewöhnlichen, robusten und medianen Regression nutzen wir nochmal den Datensatz für das Längenwachstum der Gurken. In einem ersten Schritt fitten wir wieder alle Modelle und achten darauf, dass wir bei der Quantilesregression angeben welches Quantile wir wollen. Wir wählen mit tau = 0.5 dann den Median und rechnen so eine Median-Regression. Darüber hinaus schauen wir uns nochmal die quadratische Anpassung der Zeit in dem letzten Modell an. Mal schauen, ob das Modell dann auch bei den Kennzahlen das beste Modell ist. Visuell sah das quadratische Modell des zeitlichen Verlaufs schon mal sehr gut aus.\n\ntime_lm_fit &lt;- lm(length ~ versuchsgruppe + time_num + versuchsgruppe:time_num, gurke_time_len_tbl)\ntime_rlm_fit &lt;- rlm(length ~ versuchsgruppe + time_num + versuchsgruppe:time_num, gurke_time_len_tbl)\ntime_rq_lin_fit &lt;- rq(length ~ versuchsgruppe + time_num + versuchsgruppe:time_num, tau = 0.5,\n                      gurke_time_len_tbl)\ntime_rq_quad_fit &lt;- rq(length ~ versuchsgruppe * poly(time_num, 2), tau = 0.5, \n                       gurke_time_len_tbl)\n\nAuch hier nutzen wir dann das fantastische Paket {modelsummary} mit der gleichnamigen Funktion um uns einmal die Modelle im Vergleich anzuschauen.\n\nmodelsummary(lst(\"Ordinary\" = time_lm_fit,\n                 \"Robust\" = time_rlm_fit,\n                 \"Quantile linear\" = time_rq_lin_fit,\n                 \"Quantile quadratic\" = time_rq_quad_fit),\n             estimate  = \"{estimate}\",\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"),\n             coef_omit = \"Intercept|time_num\")\n\n\n\n\n\n\nOrdinary\nRobust\nQuantile linear\nQuantile quadratic\n\n\n\n\nversuchsgruppeProloog\n−0.218\n−1.028\n−0.823\n8.483\n\n\n\n[−3.056, 2.619]\n[−2.693, 0.638]\n[−2.852, 1.206]\n[6.612, 10.353]\n\n\n\ns.e. = 1.440\ns.e. = 0.846\ns.e. = 1.035\ns.e. = 0.954\n\n\n\nt = −0.152\nt = −1.215\nt = −0.795\nt = 8.890\n\n\n\np = 0.880\np = 0.225\np = 0.427\np = &lt;0.001\n\n\nversuchsgruppeQuarto\n−0.412\n−0.641\n−0.986\n−1.883\n\n\n\n[−3.249, 2.426]\n[−2.307, 1.025]\n[−3.045, 1.074]\n[−3.817, 0.050]\n\n\n\ns.e. = 1.440\ns.e. = 0.846\ns.e. = 1.051\ns.e. = 0.986\n\n\n\nt = −0.286\nt = −0.758\nt = −0.938\nt = −1.909\n\n\n\np = 0.775\np = 0.449\np = 0.349\np = 0.057\n\n\nNum.Obs.\n252\n252\n252\n252\n\n\nR2\n0.581\n\n0.548\n0.537\n\n\nR2 Adj.\n0.572\n\n\n\n\n\nAIC\n1472.0\n1488.9\n1345.9\n1316.6\n\n\nBIC\n1496.7\n1513.6\n1367.1\n1348.4\n\n\nLog.Lik.\n−729.020\n−737.435\n\n\n\n\nF\n68.209\n269.196\n\n\n\n\nRMSE\n4.37\n4.51\n4.53\n4.59\n\n\n\n\n\n\n\n\nHier sieht es ähnlich aus wie bei den Modellvergleichen von den Basilikumdaten. Nur hier wissen wir, dass wir Ausreißer in der Form von Gurken mit einer Länge von 0 cm in den Daten haben. Daher sehen wir auch, dass die robuste Regression und die Median-Regression ein niedrigeres AIC und BIC haben. Für mich interessant ist, dass der Fehler RMSE wieder am kleinsten bei der gewöhnlichen Regression ist, das mag aber auch an der Berechnung liegen. Da wir Ausreißer haben, sind natürlich dann auch die robuste Regression und die Median-Regression vorzuziehen. Da die Median-Regression den kleineren AIC Wert hat, nehmen wir dann die Median-Regression als die beste Regression an. Der RMSE ist am kleinsten für die quadratische Anpassung des zeitlichen Verlaufs. Deshalb wäre dann das vierte Modell mit dem niedrigsten AIC und dem niedrigsten RMSE das Modell der Wahl.\nIch wiederhole mich hier wirklich, aber vermutlich wäre es schlauer zuerst die Daten zu bereinigen und die Gurken mit einem Wachstum von 0 cm zu entfernen. Auch die anderen Wachstumskurven von anderen Gurken sind etwas wirr. Da müsste man auch nochmal ran und schauen, ob nicht die Gurken lieber aus der Analyse raus müssen. Am Ende ist dann natürlich die Frage, ob man die Daten dann nicht doch lieber über ein gemischtes Modell auswertet, da natürlich die Zeitpunkte voneinander abhängig sind, wenn wir immer die glecihen Gurken messen und nicht ernten. Aber wie immer im Leben, alles geht nicht.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Robuste und Quantilesregression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-robust-quantile.html#referenzen",
    "href": "stat-modeling-robust-quantile.html#referenzen",
    "title": "55  Robuste und Quantilesregression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 55.1— Dotplot des Frischegewichts von Basilikumpflanzen auf vier Tischen bzw. Blöcken in vier Versuchsgruppen mit Mittelwert und Standardabweichung.\nAbbildung 55.2— Scatterplot des Längenwachstums der drei Gurkensorten über vierzehn Wochen. Die gestrichtelten Linien stellen den Median und die durchgezogene Line den Mittelwert der Gruppen dar.\nAbbildung 55.3— Interaktionsplot über die Versuchsgruppen und Blöcke.\nAbbildung 55.4— Scatterplot für den Unterschied zwischen einer robusten und gewöhnlichen Regression. Die gewöhnliche Regression ist gelb, die robuste Regression MASS::rlm() grün und die robuste Regression robustbase::lmrob() blau.\nAbbildung 55.5— Scatterplot des Längenwachstums der drei Gurkensorten über vierzehn Wochen. Die gestrichtelten Linien stellen den Median und die durchgezogene Line den Mittelwert der Gruppen dar.\nAbbildung 55.6 (a)— Linear\nAbbildung 55.6 (b)— Quadratic\n\n\n\nMaronna RA, Martin RD, Yohai VJ, Salibián-Barrera M. 2019. Robust statistics: theory and methods (with R). John Wiley & Sons.\n\n\nStromberg A. 2004. Why write statistical software? The case of robust statistical methods. Journal of Statistical Software 10: 1–8.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Robuste und Quantilesregression</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survival.html",
    "href": "stat-modeling-survival.html",
    "title": "56  Überlebenszeitanalysen",
    "section": "",
    "text": "56.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               survminer, survival, parameters,\n               gtsummary, janitor, ranger)\nconflicts_prefer(dplyr::filter)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Überlebenszeitanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survival.html#daten",
    "href": "stat-modeling-survival.html#daten",
    "title": "56  Überlebenszeitanalysen",
    "section": "56.2 Daten",
    "text": "56.2 Daten\nInsgesamt schauen wir uns in diesem Kapitel dann drei Datensätze an. Einmal einen Datensatz der sehr simple ist und nochmal erklärt wie die Datenstruktur in R aussehen muss. Dafür nutzen wir das Überleben von Dodos. Dann einen Datensatz, der etwas komplizierter ist. Wir betrachten hier das Überleben von Fruchtfliegen. Den Abschluss bildet ein Datensatz zu einer klinischen Studie von Lungenkrebs. Der Datensatz wird viel in Beispielen genutzt, so dass ich den Datensatz auch hier nochmal vorstellen möchte.\n\n56.2.1 Überleben von Dodos\nWir schauen uns hier das Überleben von Dodos mit oder ohne Schnabelstutzen an. Wir schauen dann an jedem Lebenstag, wie viele Dodos verstorben sind. Da wir es hier mit sehr schnell wachsenden Dodos zu tun haben, ist der Datensatz nicht so lang was die Zeit angeht. Wir beobachten nur die ersten 55 Lebenstage (eng. days of life, abk. dol) bevor die Dodos dann geschlachtet werden. In der Tabelle 56.1 sehen wir die rohen Daten, die wir in der Form nicht analysieren können. Wir müssen uns hier etwas strecken, damit das Format der Daten für die Überlebenszeitanalyse passt.\n\n\n\n\nTabelle 56.1— Rohe Datentabelle mit den jeweiligen Lebenstagen (abk. dol) und der Anzahl an lebenden sowie toten Dodos an den entsprechenden Tagen.\n\n\n\n\n\n\ntrt\ndol\ncount\ndeath\n\n\n\n\nnone\n1\n250\n0\n\n\nnone\n2\n250\n0\n\n\nnone\n3\n250\n0\n\n\nnone\n4\n250\n0\n\n\nnone\n5\n250\n0\n\n\nnone\n6\n245\n5\n\n\n…\n…\n…\n…\n\n\nclipped\n50\n179\n3\n\n\nclipped\n51\n177\n2\n\n\nclipped\n52\n176\n1\n\n\nclipped\n53\n173\n3\n\n\nclipped\n54\n170\n3\n\n\nclipped\n55\n163\n7\n\n\n\n\n\n\n\n\nUnsere Daten zu den Dodos beinhalten die folgenden Spalten mit den entsprechenden Bedeutungen.\n\ntrt, die Behandlung der Schnäbel mit none und clipped\ndol, der day of life also der Lebenstag der Dodos\ncount, die Anzahl an lebenden Dodos an dem entprechenden day of life\ndeath, die Anzahl an tot aufgefundenen Dodos an dem entpsrechenden day of life\n\nWir haben somit \\(n = 500\\) beobachtete Dodos mit jeweils 250 für jede der beiden Schnabelbehandlungen. Jetzt brauchen wir aber wie immer einen Datansatz in dem jede Zeile einen beobachteten Dodo entspricht. In der Tabelle 56.2 sehen wir welche Art von Tabelle wir bauen müssen.\n\n\n\nTabelle 56.2— Beispielhafte Datentabelle für die Analyse der Dododaten. Jede Zeile entspricht einem beobachteten Dodo und dem entsprechenden Informationen zur Lebensdauer und Schnabelbehandlung.\n\n\n\n\n\ndodo_id\ntrt\ndol\ndeath\n\n\n\n\n1\nnone\n6\n1\n\n\n2\nnone\n6\n1\n\n\n3\nnone\n7\n1\n\n\n…\n…\n…\n…\n\n\n249\nnone\n55\n0\n\n\n250\nnone\n55\n0\n\n\n251\nclipped\n6\n1\n\n\n…\n…\n…\n…\n\n\n500\nclipped\n55\n0\n\n\n\n\n\n\nFangen wir also mit der Information an, die wir haben. Wir wissen wie viele Dodos jeweils zu einem bestimmten Lebenstag gestorben sind. Daher können wir anhand der Spalte death die Anzahl an Zeilen entsprechend vervielfältigen. Sind an einem Lebenstag drei Dodos gestorben, dann brauchen wir dreimal eine Zeile mit der Information des Lebenstages und dass an diesem Tag ein Dodo gestorben ist. Wir nutzen dazu die Funktion uncount(). Dann erschaffen wir noch eine Spalte death in der einfach immer eine 1 steht, da ja an diesem Lebenstag ein Dodo verstorben ist.\n\ndeath_tbl &lt;- dodo_raw_tbl |&gt; \n  uncount(death) |&gt;  \n  mutate(death = 1) \n\nIm nächsten Schritt müssen wir die lebenden Dodos separat für jede Behandlung ergänzen. Daher spalten wir uns die Daten in eine Liste auf und ergänzen dann die Informationen zu den fehlenden, lebenden Dodos. Alle lebenden Dodos haben die maximale Lebenszeit, sind nicht gestorben und damit bleibt die Anzahl auch konstant.\n\nalive_tbl &lt;- death_tbl |&gt; \n  split(~trt) |&gt; \n  map(~tibble(dol = max(.x$dol),\n              death = rep(0, last(.x$count)),\n              count = last(.x$count))) |&gt; \n  bind_rows(.id = \"trt\")\n\nWenn wir die Informationen zu toten und den noch lebenden Dodos gebaut haben, können wir uns dann einen finalen Datensatz zusammenkleben.\n\ndodo_tbl &lt;- bind_rows(death_tbl, alive_tbl)\n\nIn der Tabelle 56.3 sehen wir den finalen Dododatensatz, den wir uns aus den Informationen zusammengebaut haben. Wir haben hier einmal die Struktur eines Überlebenszeitdatensatzes gelernt und das wir manchmal uns ganz schön strecken müssen um die Daten dann auch gut umzubauen. Wir werden am Ende nur die Informationen in der Spalte dol, death und trt nutzen.\n\n\n\n\nTabelle 56.3— Finaler Dododatensatz für die Überlebenszeitanalysen.\n\n\n\n\n\n\ntrt\ndol\ncount\ndeath\n\n\n\n\nnone\n6\n245\n1\n\n\nnone\n6\n245\n1\n\n\nnone\n6\n245\n1\n\n\nnone\n6\n245\n1\n\n\nnone\n6\n245\n1\n\n\nnone\n7\n241\n1\n\n\n…\n…\n…\n…\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\n\n\n\n\n\n\n\n\n56.2.2 Überleben von Fruchtfliegen\nIm folgenden Beispiel in Tabelle 56.4 beobachten wir Fruchtfliegen bis fast alle Insekten verstorben sind. Das ist natürlich das andere Extrem zu den Dododatensatz. Wir testen hier ein Insektizid und am Ende haben wir dann keine lebenden Fruchtfliegen mehr. Das würdest du mit Dodos oder Schweinen nicht machen, denn so lange möchtest du die Tiere ja auch nicht beobachten, bis alle gestorben sind. Bei Fruchtfliegen dauert es eben nicht so lange bis alle Fliegen verstorben sind.\nWir laden die Daten der Fruchtfliegen aus der Datei fruitfly.xlsx.\n\nfruitfly_tbl &lt;- read_excel(\"data/fruitfly.xlsx\")\n\nDamit arbeiten wir dann im Folgenden weiter.\n\n\n\n\nTabelle 56.4— Fruchtfliegendatensatz mit verschiedenen Covariaten zu der Behandlung, der Zeit und dem Status der Fruchtfliegen.\n\n\n\n\n\n\ntrt\ntime\nstatus\nsex\nweight\nweight_bin\n\n\n\n\nfruitflyEx\n2\n1\nmale\n8.13\nlow\n\n\nfruitflyEx\n2\n1\nmale\n7.75\nlow\n\n\nfruitflyEx\n3\n1\nmale\n9.86\nlow\n\n\nfruitflyEx\n3\n1\nmale\n6.36\nlow\n\n\n…\n…\n…\n…\n…\n…\n\n\ncontrol\n23\n1\nfemale\n21.02\nhigh\n\n\ncontrol\n24\n1\nfemale\n16.45\nhigh\n\n\ncontrol\n25\n1\nfemale\n14.17\nlow\n\n\ncontrol\n26\n1\nfemale\n17.73\nhigh\n\n\n\n\n\n\n\n\nUnsere Daten zu den Fruchtfliegen beinhalten die folgenden Spalten mit den entsprechenden Bedeutungen.\n\ntrt, als die Behandlung mit den beiden Leveln fruitflyEx und control\ntime, den Zeitpunkt des Todes der entsprechenden Fruchfliege\nstatus, den Status der Fruchtfliege zu dem Zeitpunkt time. Hier meist 1 und damit tot, aber ein paar Fruchtfliegen sind bei der Überprüfung entkommen und haben dann eine 0.\nsex, das Geschlecht der entsprechenden Fruchtfliege\nweight, das Gewicht der entsprechenden Fruchtfliege in \\(\\mu g\\).\nweight_bin das Gewicht der entsprechenden Fruchtfliege aufgeteilt in zwei Gruppen nach dem Cutpoint von \\(15 \\mu g\\).\n\n\n\n56.2.3 Überleben von Lungenkrebs\nZum Abschluss möchte ich noch den Datensatz lung in der Tabelle 56.5 aus dem R Paket {survival} vorstellen. Das hat vor allem den Grund, dass es sich hier um einen klassischen Datensatz zur Überlebenszeitanalyse handelt und ich auch dieses Teilgebiet einmal mit abdecken möchte. Wie schon weiter oben gesagt, Überlebenszeitanalysen kommen eher in dem Humanbereich vor. Darüber hinaus bedienen sich fast alle anderen Tutorien im Internet diesem Datensatz, so dass du dann einfacher die englischen Texte nachvollziehen kannst.\n\n\n\n\nTabelle 56.5— Der Datensatz lung über eine Beobachtungsstudiue Studie zu Lungenkrebs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninst\ntime\nstatus\nage\nsex\nph.ecog\nph.karno\npat.karno\nmeal.cal\nwt.loss\n\n\n\n\n3\n306\n2\n74\n1\n1\n90\n100\n1175\nNA\n\n\n3\n455\n2\n68\n1\n0\n90\n90\n1225\n15\n\n\n3\n1010\n1\n56\n1\n0\n90\n90\nNA\n15\n\n\n5\n210\n2\n57\n1\n1\n90\n60\n1150\n11\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n13\n191\n1\n39\n1\n0\n90\n90\n2350\n-5\n\n\n32\n105\n1\n75\n2\n2\n60\n70\n1025\n5\n\n\n6\n174\n1\n66\n1\n1\n90\n100\n1075\n1\n\n\n22\n177\n1\n58\n2\n1\n80\n90\n1060\n0\n\n\n\n\n\n\n\n\n\ninst Institution code\ntime Survival time in days\nstatus censoring status 1=censored, 2=dead\nage Age in years\nsex Male=1 Female=2\nph.ecog ECOG performance score (0=good 5=dead)\nph.karno Karnofsky performance score (bad=0-good=100) rated by physician\npat.karno Karnofsky performance score as rated by patient\nmeal.cal Calories consumed at meals\nwt.loss Weight loss in last six months\n\nTrotz seiner Prominenz hat der Datensatz einen Fehler. Wir wollen den Status nicht auf 1/2 kodiert haben sondern auf 0/1. Ebenso wollen wir die Spalte inst nicht, da wir die Informationen nicht brauchen. Dann sind noch die Namen der Spalten hässlich, so dass wir da die Funktion clean_names() nutzen um einmal aufzuräumen.\n\nlung_tbl &lt;- lung |&gt; \n  as_tibble() |&gt; \n  mutate(status = recode(status, `1` = 0, `2` = 1)) |&gt; \n  clean_names() |&gt; \n  select(-inst)",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Überlebenszeitanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survival.html#die-surv-funktion",
    "href": "stat-modeling-survival.html#die-surv-funktion",
    "title": "56  Überlebenszeitanalysen",
    "section": "56.3 Die Surv() Funktion",
    "text": "56.3 Die Surv() Funktion\nAls die Besonderheit bei der Bearbeitung von Überlebenszeitanalysen ist die Andersartigkeit von unserem \\(y\\). Wir haben ja zwei Spalten, die das Outcome beschreiben. Zum einen die Dauer oder Zeit bis zum Ereignis und dann die Spalte, die beschreibt, ob das Ereignis überhaupt eingetreten ist. In R lösen wir dieses Problem in dem wir zwei Spalten in dem Objekt Surv() zusammenführen. Alle Analysen in R gehen nur mit dem Surv() Objekt.\n\nSurv(time, death)\n\nIn dem Objekt Surv() haben wir erst die Spalte für die Zeit und dann die Spalte für das Ereignis. Für den Dododatensatz haben wir dann folgende Zusammenhänge.\n\ndol in den Daten dodo_tbl ist gleich time in dem Surv() Objekt\ndeath in den Daten dodo_tbl ist gleich death in dem Surv() Objekt\n\nBei dem Fruchfliegendatensatz sieht die Sachlage dann so aus.\n\ntime in den Daten fruitfly_tbl ist gleich time in dem Surv() Objekt\nstatus in den Daten fruitfly_tbl ist gleich death in dem Surv() Objekt\n\nFür den Lungenkrebsdatensatz haben wir dann folgende Zuordnung.\n\ntime in den Daten lung_tbl ist gleich time in dem Surv() Objekt\nstatus in den Daten lung_tbl ist gleich death in dem Surv() Objekt\n\nIm Folgenden haben wir dann immer auf der linken Seite vom ~ ein Surv() Objekt stehen. Daran muss man sich immer etwas gewöhnen, sonst kommt sowas ja nicht in den Analysen vor.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Überlebenszeitanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survival.html#visualisierung-über-kaplan-meier-kurven",
    "href": "stat-modeling-survival.html#visualisierung-über-kaplan-meier-kurven",
    "title": "56  Überlebenszeitanalysen",
    "section": "56.4 Visualisierung über Kaplan Meier Kurven",
    "text": "56.4 Visualisierung über Kaplan Meier Kurven\n\n\n\n\n\n\nNur kategoriale Variablen in einer Kaplan Meier Kurve\n\n\n\nWir können nur kategoriale Variablen in einer Kaplan Meier Kurve darstellen. Das heißt, wir müssen alle unsere \\(x\\), die kontinuierlich sind in eine kategoriale Variable umwandeln.\n\n\nWir können nur kategoriale Variablen in einer Kaplan Meier Kurve darstellen. Das heißt, wir müssen alle unser \\(X\\), die wir haben, in Variablen mit Kategorien umwandeln. Wenn du also in deinen Daten eine Spalte für das Gewicht in kg hast, dann musst du dir überlegen, wie du diese Werte in Kategorien änderst. Eine beliebte Variante ist, dass du zwei Gruppen bildest. Einmal die Patienten, die schwerer sind als der Median des Körpergewichts und einmal eine Gruppe für die Patienten, die leichter sind als der Median des Körpergewichts. Die Kategorisierung von Variablen ist faktisch ein eigenes Problem und lässt sich ohne den biologischen Hintergrund eigentlich nicht sauber durchführen. Daher werden in klinischen Studien oder Experimenten die Daten gleich in Kategorien erhoben. Daher gibt es vorab klare Kriterien in welcher Gewichtsklasse oder Altersklasse ein Patient landen wird. Das Gleiche gilt dann auch für andere kontinuierlichen Variablen.\nEin häufiger Fehler bei der Betrachtung der Kaplan Meier Kurve ist diese als die simple lineare Regression der Überlebenszeitanalyse anzusehen. Wir können zwar mit der Kaplan Meier Kurve immer nur ein \\(X\\) betrachten aber der Algorithmus basiert auf dem \\(\\mathcal{X}^2\\)-Test und hat nichts mit einer Regression zu tun. Daher kann es sein, dass du unterschiedliche Ergebnisse in der Visualisierung mit Kaplan Meier Kurven und dann der Analyse mit dem Cox Proportional-Hazards Modell erhälst.\nIn R nutzen wir das Paket {survminer} und die Funktion ggsurvplot() für die Visualisierung der Kaplan Meier Kurven.\n\n56.4.1 Dodos\nUm eine Kaplan Meier Kurve zeichnen zu können, brauchen wie als erstes die Funktion survfit(). Mit der Funktion survfit() können wir zum einen das mediane Überleben berechnen und alle Informationen erhalten, die wir brauchen um die Kaplan Meier Kurve zu plotten.\n\ntrt_fit &lt;- survfit(Surv(dol, death) ~ trt, data = dodo_tbl)\n\nWir können einmal das Objekt trt_fit uns anschauen.\n\ntrt_fit\n\nCall: survfit(formula = Surv(dol, death) ~ trt, data = dodo_tbl)\n\n              n events median 0.95LCL 0.95UCL\ntrt=clipped 250     87     NA      NA      NA\ntrt=none    250    165   44.5      40      49\n\n\nZum einen fallen uns die NA’s in der Wiedergabe des Fits der Überlebenszeit auf. Wenn wir uns die Abbildung 56.1 einmal anschauen, dann wird das Problem etwas klarer. Wir sehen nämlich, dass wir bei den geklippten Tieren gar nicht so weit runter kommen mit den toten Tieren, dass wir das mediane Überleben berechnen könnten. Nicht immer können wir auch alle statistischen Methoden auf alle Fragestellungen anwenden. Insbesondere wenn nicht genug Ereignisse wie in diesem Beispiel auftreten.\n\nggsurvplot(trt_fit, \n           data = dodo_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2:8])\n\n\n\n\n\n\n\nAbbildung 56.1— Kaplan Meier Kurven für unsere geklippten Dodos. Wir sehen, dass die geklippten Tiere in der Zeit der Versuchsdurchführung garnicht zur Hälfte versterben.\n\n\n\n\n\n\n\n56.4.2 Fruchtfliegen\nGehen wir einen Schritt weiter und schauen uns das Modell für die Fruchtfliegen an. Hier haben wir eine Behandlung mit zwei Leveln also Gruppen vorliegen. Wir nutzen wieder die Funktion survfit() um einaml unser Modell der Überlebenszeiten zu fitten.\n\ntrt_fit &lt;- survfit(Surv(time, status) ~ trt, data = fruitfly_tbl)\n\nWir erhalten dann folgende Ausgabe des Modells.\n\ntrt_fit\n\nCall: survfit(formula = Surv(time, status) ~ trt, data = fruitfly_tbl)\n\n                n events median 0.95LCL 0.95UCL\ntrt=control    50     49     15      13      17\ntrt=fruitflyEx 50     44     12      10      14\n\n\nIn der Abbildung 56.2 sehen wir die Visualisierung des Modells als Kaplan Meier Kurve. In diesem Experiment sterben fast alle Fruchtfliegen im Laufe der Untersuchung. Wir können also einfach das mediane Überleben für beide Gruppen berechnen.\n\nggsurvplot(trt_fit, \n           data = fruitfly_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2:8])\n\n\n\n\n\n\n\nAbbildung 56.2— Kaplan Meier Kurven für die Fruchtfliegen nach der Behandlung mit einem Pestizid und einer Kontrolle.\n\n\n\n\n\nNachdem wir die Kaplan Meier Kurven einmal für die Behandlung durchgeführt haben, können wir uns auch anschauen, ob das Überleben der Fruchtfliegen etwas mit dem Gewicht der Fruchtfliegen zu tun hat. Hier können wir nicht auf das Gewicht in der Spalte weight zurückgreifen sondern müssen die Variable weight_bin mit zwei Klassen nehmen.\n\nweight_fit &lt;- survfit(Surv(time, status) ~ weight_bin, data = fruitfly_tbl)\n\nWir erhalten dann die Kaplan Meier Kurven in der Abbildung 56.3 zurück. Hier ist es wichtig sich nochmal klar zu machen, dass wir eben nur kategoriale Variablen in einer Kaplan Meier Kurve darstellen können.\n\nggsurvplot(weight_fit, \n           data = fruitfly_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2:8])\n\n\n\n\n\n\n\nAbbildung 56.3— Kaplan Meier Kurven für die Fruchtfliegen nach der Behandlung mit einem Pestizid und einer Kontrolle.\n\n\n\n\n\n\n\n56.4.3 Lungenkrebs\nAls letztes Beispiel wollen wir uns nochmal den Datensatz lung_tbl anschauen. Zwar ist Lungenkrebs jetzt nichts was Tiere und Pflanzen als eine wichtige Erkrankung haben können, aber der Datensatz wird viel als Beispiel genutzt, so dass ich den Datensatz hier auch nochmal vorstellen möchte. Auch sind teilweise gewisse Schritte von Interesse, die eventuell auch in deiner Tier- oder Mäusestudie von Interesse sein könnten.\nBeginnen wir einmal mit dem Nullmodell. Das heißt, wir schauen uns den Verlauf des gesamten Überlebens einmal an. Wir wollen wissen, wie das mediane Überleben in unseren Daten ausschaut ohne das wir uns irgendeien Variable anschauen.\n\nnull_fit &lt;- survfit(Surv(time, status) ~ 1, data = lung_tbl)\n\nIm Folgenden einmal die Ausgabe des Fits.\n\nnull_fit\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung_tbl)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nWir sehen, dass wir ein medianes Überleben von \\(310\\) Tagen haben. Es gibt insgesamt \\(165\\) Ereignisse zu beobachten von insgesamt \\(228\\) in die Studie eingeschlossenen Patienten. Wir können uns auch für bestimmte Zeitpunkte das Überleben wiedergeben lassen. Wir schauen uns hier einmal das Überleben nach einem Jahr bzw. \\(365.25\\) Tagen an.\n\nnull_fit |&gt; \n  summary(times = 365.25)\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung_tbl)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  365     65     121    0.409  0.0358        0.345        0.486\n\n\nHier sehen wir, dass \\(40.9\\%\\) das eine Jahr überlebt haben. Achtung, immer auf die Kodierung achten. Nur wenn du death = 1 kodiert hast, kannst du hier die Ausgaben der Funktionen in diesem Sinne interpretieren. Gerne kannst du hier auch das 3 Monatsüberleben bestimmen. Das kommt ganz darauf an, was deine Fragestellung ist. In der Abbildung 56.4 siehst du nochmal die Kaplan Meyer Kurve für das Nullmodell.\n\nggsurvplot(null_fit, \n           data = lung_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2])\n\n\n\n\n\n\n\nAbbildung 56.4— Kaplan Meier Kurven für das Nullmodell des Lungenkrebsdatensatzes.\n\n\n\n\n\nSchauen wir uns auch einmal Kaplan Meier Kurven für die Variable ph_ecog an. Hier haben wir das Problem, dass die Kategorie 3 kaum mit Patienten belegt ist. Daher filtern wir die Kategorie 3 einmal aus unseren Daten raus.\n\nlung_tbl %&lt;&gt;% \n  filter(ph_ecog != 3)\n\nWir können dann das Modell der Überlebenszeit einmal fitten.\n\nlung_fit &lt;- survfit(Surv(time, status) ~ ph_ecog, data = lung_tbl)\n\nIn der Abbildung 56.5 sehen wir dann die Kaplan Meier Kurve für die Variable ph_ecog. Du kannst hier schön sehen, dass wenn wir mehrere Kategorien in der Variable haben auch mehrere Graphen erhalten. Wichtig hierbei ist nochmal, dass sich die Graphen nicht überschneiden oder aber in der Mitte kreuzen. Dann haben wir ein Problem und könnten die Daten nicht auswerten. Konkret geht es hier um die proportional hazards assumption, die besagt, dass Überlebenszeitkurven für verschiedene Gruppen Hazardfunktionen haben müssen, die über die Zeit \\(t\\) proportional sind. Daher dürfen sich die Kurven nicht schneiden.\n\nggsurvplot(lung_fit, \n           data = lung_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2:8])\n\n\n\n\n\n\n\nAbbildung 56.5— Kaplan Meier Kurve für die Variable ph_ecog des Lungenkrebsdatensatzes.\n\n\n\n\n\nGanz zum Schluss dann noch die Frage, ob wir einen signifikanten Unterschied zwischen den beiden Kurven sehen. Dafür können wir dann die Funktion survdiff() nutzen. Die Funktion survdiff() gibt uns dann einen p-Wert wieder, ob sich die Kurven unterscheiden. Da es sich hier um einen globalen p-Wert handelt, erfahren wir nur, dass sich die Kurven unterscheiden, aber nicht welche. Dafür müssten wir dann die Kurven paarweise getrennt betrachten. Eigentlich ist nur der p-Wert von Interesse, die anderen Informationen haben eigentlich keinen biologischen Mehrwert.\n\nsurvdiff(Surv(time, status) ~ ph_ecog, data = lung_tbl)\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ ph_ecog, data = lung_tbl)\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nph_ecog=0  63       37     53.9    5.3014    8.0181\nph_ecog=1 113       82     83.1    0.0144    0.0295\nph_ecog=2  50       44     26.0   12.4571   14.9754\n\n Chisq= 18  on 2 degrees of freedom, p= 1e-04",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Überlebenszeitanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survival.html#cox-proportional-hazards-modell",
    "href": "stat-modeling-survival.html#cox-proportional-hazards-modell",
    "title": "56  Überlebenszeitanalysen",
    "section": "56.5 Cox Proportional-Hazards Modell",
    "text": "56.5 Cox Proportional-Hazards Modell\nWenn die Kaplan Meyer Kurven sowas wie die simple lineare Regression sind, dann ist das Cox Proportional-Hazards Modell die multiple Regression in den Ereigniszeitanalysen. Damit haben wir natürlich wieder einen statistischen Engel überfahren. Das Cox Proportional-Hazards Modell ist natürlich etwas anders und lässt sich so einfach auch nicht mit einer multiplen Regression vergleichen, aber die Anwendung ist ähnlich. Wo wir bei den Kaplan Meier Kurven nur ein \\(X\\) in das Modell nehmen können, so können wir beim Cox Proportional-Hazards Modell beliebig viele \\(X\\) mit ins Modell nehmen. Theoretisch müssen die Variablen in einem Cox Proportional-Hazards Modell auch nicht mehr kategorial sein. Da wir aber meist alles schon in Kategorien visualisiert haben, bleiben wir dann meist im Cox Proportional-Hazards Modell auch bei den Kategorien in den Variablen. Auch im Fall des Cox Proportional-Hazards Modells kann ich hier nur eine Übersicht geben. Es findet sich natürlich auch ein Tutorium zum Cox Proportional-Hazards Model Tools. Für das Überprüfen der Modellannahmen empfiehlt sich auch das Tutorium zu Cox Model Assumptions.\nIn R nutzen wir die Funktion coxph() um ein Cox Proportional-Hazards Modell anzupassen. Die Anwendung ist eigentlich ziemlich einfach und lässt sich schnell durchführen.\n\nfit_1 &lt;- coxph(Surv(time, status) ~ trt + sex + weight, data = fruitfly_tbl) \nfit_1 |&gt; \n  model_parameters(exponentiate = TRUE)\n\nParameter        | Coefficient |   SE |        95% CI |    z |      p\n---------------------------------------------------------------------\ntrt [fruitflyEx] |        4.09 | 1.44 | [2.05,  8.17] | 4.00 | &lt; .001\nsex [male]       |        6.91 | 1.82 | [4.12, 11.59] | 7.32 | &lt; .001\nweight           |        1.02 | 0.04 | [0.95,  1.10] | 0.60 | 0.549 \n\n\nAls Koeffizienten erhalten wir das Hazard ratio (abk. HR) wieder. Wie schon bei der logistischen Regression müssen wir auch hier die Koeffizienten exponieren, damit wir die Link-scale verlassen. Wir können das HR wie ein Risk ratio (abk. RR) interpretieren. Es handelt sich also mehr um eine Sterbewahrscheinlichkeit. Ganz richtig ist die Interpretation nicht, da wir hier noch eine Zeitkomponente mit drin haben, aber für den umgangssprachlichen Gebrauch reicht die Interpretation.\nWenn wir ein \\(HR &gt; 1\\) vorliegen haben, so steigert die Variable das Risiko zu sterben. Daher haben wir eine protektive Variable vorliegen, wenn dass \\(HR &lt; 1\\) ist. Häufig wollen wir ein \\(HR &lt; 0.8\\) oder \\(HR &lt; 0.85\\) haben, wenn wir von einem relevanten Effekt sprechen wollen. Sonst reicht uns die Risikoreduktion nicht, um wirklich diese Variable zukünftig zu berücksichtigen. Aber wie immer hängt die Schwelle sehr von deiner Fragestellung ab.\nIch habe nochmal als Vergleich die Variable weight in das Modell genommen und damit den fit_1 angepasst sowie die Variable weight in zwei Gruppen zu weight_bin aufgeteilt. Hier siehst du sehr schön, dass der Effekt der Dichotomisierung nicht zu unterschätzen ist. Im fit_1 ist die kontinuierliche Variable weight eine Risikovariable, daher wir erwarten mehr tote Fruchtfliegen mit einem steigenden Gewicht. In dem fit_2 haben wir die dichotomisierte Variable weight_bin vorliegen und schon haben wir eine protektive Variable. Wenn das Gewicht steigt, dann sterben weniger Fruchtfliegen. Zwar ist in beiden Fällen die Variable nicht signifikant, aber du solltest eine Dichotomisierung immer überprüfen.\n\nfit_2 &lt;- coxph(Surv(time, status) ~ trt + sex + weight_bin, data = fruitfly_tbl) \nfit_2 |&gt; \n  model_parameters(exponentiate = TRUE)\n\nParameter        | Coefficient |   SE |        95% CI |     z |      p\n----------------------------------------------------------------------\ntrt [fruitflyEx] |        5.07 | 1.77 | [2.56, 10.05] |  4.65 | &lt; .001\nsex [male]       |        7.28 | 1.93 | [4.33, 12.23] |  7.50 | &lt; .001\nweight bin [low] |        0.62 | 0.20 | [0.33,  1.16] | -1.48 | 0.138 \n\n\nNachdem wir ein Cox Proportional-Hazards Modell angepasst haben, wollen wir nochmal überprüfen, ob die Modellannahmen auch passen. Insbesondere müssen wir überprüfen, ob das Risiko über die ganze Laufzeit der Studie gleich bleibt oder sich ändert. Wir testen also die proportional hazards assumption. Dafür können wir in R die Funktion cox.zph() nutzen.\n\ntest_ph &lt;- cox.zph(fit_2)\ntest_ph\n\n              chisq df     p\ntrt        0.000866  1 0.977\nsex        3.400374  1 0.065\nweight_bin 0.185700  1 0.667\nGLOBAL     3.971563  3 0.265\n\n\nWir lesen die Ausgabe von unten nach oben. Zuerst könne wir die proportional hazards assumption nicht ablehnen. Unser globaler p-Wert ist mit \\(0.265\\) größer als das Signifikanzniveau \\(\\alpha\\) gleich \\(5\\%\\). Betrachten wir die einzelnen Variablen, dann können wir auch hier die proportional hazards assumption nicht ablehnen. In der Abbildung 56.6 ist der Test auf die proportional hazards assumption nochmal visualisiert. Wenn die durchgezogene Linie innerhalb der gestrichelten Linien, als Bereich von \\(\\pm 2\\) Standardfehlern, bleibt, dann ist soweit alles in Orndung. Bei einem Verstoß gegen die proportional hazards assumption kannst du folgende Maßnahmen ausprobieren:\n\nHinzufügen von einer Kovariate*Zeit-Interaktion in das Cox Proportional-Hazards Modell\nStratifizierung der Daten nach der Kovariaten, die der proportional hazards assumption nicht folgt\n\nAuch hier musst du dann mal tiefer in die Materie einsteigen und einmal in den verlinkten Tutorien schauen, ob da was passendes für dein spezifisches Problem vorliegt.\n\nggcoxzph(test_ph)\n\n\n\n\n\n\n\nAbbildung 56.6— Visualisierung der Überprüfung der Proportional-Hazards-Annahme.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Überlebenszeitanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survival.html#logistische-regression",
    "href": "stat-modeling-survival.html#logistische-regression",
    "title": "56  Überlebenszeitanalysen",
    "section": "56.6 Logistische Regression",
    "text": "56.6 Logistische Regression\nWas schon wieder die logistische Regression? Ja, schon wieder die logistische Regression. Wenn du dich nicht mit der Ereigniszeitanalyse rumschlagen willst oder denkst, dass die Ereigniszeitanalyse nicht passt, dann hast du immer noch die Möglichkeit eine logistische Regression zu rechnen. Dafür müssen wir dann nur ein wenig an den Daten rumbasteln. Wir müssen dann nämlich eine neue Variable erschaffen. Wir schauen einfach zu einem Zeitpunkt \\(t\\), ob die Beobachtung noch lebt. Dadurch bauen wir uns dann eine Spalte mit \\(0/1\\) Werten. Dann kann es auch schon losgehen mit der logistischen Regression.\nIm ersten Schritt bauen wir uns eine neue Variable died_3_m für den Lingenkrebsdatensatz. Da in einer logistsichen Regression das Schlechte immer 1 ist, fragen wir, wer nach 90 Tagen verstorben ist. Also eine Lebenszeit unter 90 Tagen hatte. Diese Beobachtungen kriegen dann eine 1 und die anderen Beobachtungen eine 0.\n\nlung_logreg_tbl &lt;- lung_tbl |&gt; \n  mutate(died_3_m = ifelse(time &lt; 90, 1, 0))\n\nNachdem wir uns recht schnell eine neue Variable gebaut haben, können wir dann einfach die logistische Regression rechnen. Bitte beachte, dass du die Effekte nur auf der log-Scale wiedergegeben kriegst, du musst dann die Ausgabe noch exponieren. Das machen wir hier in der Funktion model_parameters() gleich mit.\n\nglm(died_3_m ~ age + sex + ph_ecog, data = lung_logreg_tbl, family = binomial) |&gt; \n  model_parameters(exponentiate = TRUE)\n\nParameter   | Odds Ratio |       SE |       95% CI |     z |     p\n------------------------------------------------------------------\n(Intercept) |   4.94e-03 | 9.59e-03 | [0.00, 0.18] | -2.74 | 0.006\nage         |       1.06 |     0.03 | [1.01, 1.13] |  2.29 | 0.022\nsex         |       0.45 |     0.23 | [0.16, 1.14] | -1.59 | 0.111\nph ecog     |       1.34 |     0.41 | [0.74, 2.47] |  0.96 | 0.335\n\n\nWas sind die Unterschiede? Eine logistische Regression liefert Odds Ratios, also ein Chancenverhältnis. Aus einer Ereigniszeitanalyse erhalten wir Hazard Ratios, was eher ein Risk Ratio ist und somit eine Wahrscheinlichkeit. Deshalb lassen sich die Ergebnisse an der Stelle nur bedingt vergleichen. Im Falle der logistischen Regression fallen auch Zensierungen weg. Wir betrachten eben nur einen einzigen Zeitpunkt. In der Ereigniszeitanalyse betrachten wir hingegen den gesamten Verlauf. Wir immer musst du überlegen, was ist deine Fragestellung und was möchtest du berichten. Wenn es dir wirklich nur um den Zeitpunkt \\(t\\) geht und dir die Progression dahin oder danach egal ist, dann mag die logistische Regression auch eine Möglichkeit der Auswertung sein.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Überlebenszeitanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survival.html#discrete-time-survival-analysis",
    "href": "stat-modeling-survival.html#discrete-time-survival-analysis",
    "title": "56  Überlebenszeitanalysen",
    "section": "56.7 Discrete Time Survival Analysis",
    "text": "56.7 Discrete Time Survival Analysis\n\n\n\n\n\n\nStand des Abschnitts im September 2023\n\n\n\n\n\n\n\n\nDas Thema discrete time survival analysis ist erstmal auf Stand-By gesetzt, bis mir klar ist, wohin ich mit dem Abschnitt zum Ende hin will. Aktuell fehlt mir ein agarwissenschaftlicher Datensatz zum Themenkomplex und so kommen hier erstmal die Links zu Tutorien, die ich später nochmal nutzen möchte\n\n\nEinmal kurz die Idee der discrete time survival analysis erläutert. Wir schauen uns nicht an, wie lange es dauert bis ein Ereignis eintritt, sondern ob ein Ereignis in einem bestimmten Intervall auftritt. Die Frage ist also, ist der Krebs im \\(i\\)-ten Jahr aufgetreten? Dafür brauchen wir dann auch eine spezielle Form der Datendarstellung, nämlich person period data, die wir uns dann aber meistens aus normalen Überlebenszeitdaten bauen können. Da gibt es dann eine Reihe von Funktion in R für die Umwandlung zu person period data.\nEinmal bitte auch Suresh u. a. (2022) anschauen mit der Veröffentlichung Survival prediction models: an introduction to discrete-time modeling. Dann gibt es noch zwei Tutorien, die ich dann nochmal aufarbeiten muss.\n\nEvent History Analysis - Example 6 - Discrete Time Hazard Model\nIntro to Discrete-Time Survival Analysis in R",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Überlebenszeitanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survival.html#referenzen",
    "href": "stat-modeling-survival.html#referenzen",
    "title": "56  Überlebenszeitanalysen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 56.1— Kaplan Meier Kurven für unsere geklippten Dodos. Wir sehen, dass die geklippten Tiere in der Zeit der Versuchsdurchführung garnicht zur Hälfte versterben.\nAbbildung 56.2— Kaplan Meier Kurven für die Fruchtfliegen nach der Behandlung mit einem Pestizid und einer Kontrolle.\nAbbildung 56.3— Kaplan Meier Kurven für die Fruchtfliegen nach der Behandlung mit einem Pestizid und einer Kontrolle.\nAbbildung 56.4— Kaplan Meier Kurven für das Nullmodell des Lungenkrebsdatensatzes.\nAbbildung 56.5— Kaplan Meier Kurve für die Variable ph_ecog des Lungenkrebsdatensatzes.\nAbbildung 56.6— Visualisierung der Überprüfung der Proportional-Hazards-Annahme.\n\n\n\nDavid GK, Mitchel K. 2012. Survival analysis: a Self-Learning text. Spinger.\n\n\nSuresh K, Severn C, Ghosh D. 2022. Survival prediction models: an introduction to discrete-time modeling. BMC medical research methodology 22: 207.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Überlebenszeitanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-noninferiority.html",
    "href": "stat-modeling-noninferiority.html",
    "title": "57  Äquivalenz oder Nichtunterlegenheit",
    "section": "",
    "text": "57.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, broom, readxl,\n               effectsize, multcompView, multcomp,\n               janitor, see, parameters, yardstick,\n               rcompanion, emmeans, conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(dplyr::mutate)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Äquivalenz oder Nichtunterlegenheit</span>"
    ]
  },
  {
    "objectID": "stat-modeling-noninferiority.html#daten",
    "href": "stat-modeling-noninferiority.html#daten",
    "title": "57  Äquivalenz oder Nichtunterlegenheit",
    "section": "57.2 Daten",
    "text": "57.2 Daten\nAls erstes wollen wir uns einmal die Daten für die Überprüfung der technischen Gleichheit anschauen. Die Daten stammen aus Dronenüberflügen zur Bestimmung der Grasdichte auf Weideflächen aus der Datei drone_tech.xlsx. Dabei haben wir zum einen die Grasdichte traditionell mit einem Druckstab gemessen pressure_stick und vergleichen diese Werte dann mit den Werten aus dem Dronenüberflug. Der Drohnenüberflug liefert uns Bilder und aus den Bildern extrahieren wir einen RGB-Wert (abk. Red, Green, Blue) in der Spalte drone_rgb oder einen CMYK-Wert (abk. Cyan, Magenta, Yellow (Gelb), Key (Schwarz)) in der Spalte drone_cmyk. Wir wollen nun schauen, ob wir die drei Werte sinnvoll in ein Verhältnis setzen können. Ein Auszug aus den Daten ist nochmal in der Tabelle 57.1 dargestellt.\n\n\n\n\nTabelle 57.1— Datentabelle für den technischen Vergleich eines Druckstabes und dem RGB-Werten eines Dronenüberflugs auf die Grasdichte auf Weideflächen.\n\n\n\n\n\n\npressure_stick\ndrone_rgb\ndrone_cmyk\n\n\n\n\n1048.24\n373.31\n254.65\n\n\n1284.31\n671.45\n234.86\n\n\n1170.07\n544.5\n184.01\n\n\n…\n…\n…\n\n\n1013.34\n355.84\n219.67\n\n\n1134.29\n537.43\n212.86\n\n\n917.29\n266.74\n178.46\n\n\n\n\n\n\n\n\nIn unserem zweiten Datenbeispiel schauen wir uns die Keimungsdaten nach Behandlung mit sechs biologischen Pilzmittel unter zwei Kältebehandlungen aus der Datei cold_seeds.xlsx an. Dabei ist wichtig zu wissen, dass es eine Kontrolle gibt, die das chemische Standardpräparat repräsentiert. Wir wollen jetzt wissen, ob unsere biologischen Alternativen gleich gut sind. Das heißt, wir wollen nicht mehr oder weniger als das Standardpräparat sondern gleichviel. Als Outcome zählen wir die Sporen auf den jungen Keimlingen. Da unsere Pflanze auch eine Kältebehandlung überstehen würde, haben wir auch noch die beiden Kältevarianten mit untersucht. In der Tabelle 57.2 sind die Daten einmal dargestellt.\n\n\n\n\nTabelle 57.2— Nicht transformierter Datensatz zu dem Keimungsexperiment mit biologischen Pilzpräparaten.\n\n\n\n\n\n\ntrt\ncold\nnon_cold\n\n\n\n\n1\n386.25\n22.9\n\n\n1\n100.52\n169.59\n\n\n1\n56.84\n65.46\n\n\n1\n357.65\n142.44\n\n\n2\n37668.6\n20659.77\n\n\n2\n28302.99\n7333.37\n\n\n…\n…\n…\n\n\n8\n2334.1\n352.41\n\n\n8\n9776.15\n5025.68\n\n\n8\n1932.27\n918.05\n\n\n8\n777.63\n149.17\n\n\n8\n2933.99\n1416.51\n\n\n8\n5731.01\n2022.39\n\n\n\n\n\n\n\n\nWir müssen jetzt leider nochmal ran und die Daten etwas aufräumen. Zum einen muss die erste Behandlung raus, hier handelt es sich nur um eine positive Kontrolle, ob überhaupt etwas gewachsen ist. Dann wollen wir uns die Daten auch log-transformieren. Das hat den Grund, dass die statistischen Verfahren in der Äquivalenzanalyse eine Normalverteilung verlangen. Mit der log-Transformation erreichen wir log-normalverteilte Daten, die einer Normalverteilung recht nahe kommen. Am Ende wollen wir dann auch die zweite Behandlung so benennen, dass wir auch immer die Kontrolle erkennen.\n\ncold_seed_tbl &lt;- cold_seed_tbl |&gt;   \n  clean_names() |&gt; \n  filter(trt != 1) |&gt; \n  mutate(trt = as_factor(trt),\n         log_cold = log(cold),\n         log_non_cold = log(non_cold),\n         trt = fct_recode(trt, ctrl = \"2\")) \n\nEs ergibt sich dann die Tabelle 57.3. Wir werden dann in der folgenden Analyse nur noch die log-transformierten Spalten log_cold und log_non_cold nutzen.\n\n\n\n\nTabelle 57.3— Transformierter Datensatz zu dem Keimungsexperiment mit biologischen Pilzpräparaten.\n\n\n\n\n\n\ntrt\ncold\nnon_cold\nlog_cold\nlog_non_cold\n\n\n\n\nctrl\n37668.6\n20659.77\n10.54\n9.94\n\n\nctrl\n28302.99\n7333.37\n10.25\n8.9\n\n\nctrl\n2874.76\n1325.42\n7.96\n7.19\n\n\nctrl\n7564.44\n2103.64\n8.93\n7.65\n\n\n…\n…\n…\n…\n…\n\n\n8\n1932.27\n918.05\n7.57\n6.82\n\n\n8\n777.63\n149.17\n6.66\n5.01\n\n\n8\n2933.99\n1416.51\n7.98\n7.26\n\n\n8\n5731.01\n2022.39\n8.65\n7.61",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Äquivalenz oder Nichtunterlegenheit</span>"
    ]
  },
  {
    "objectID": "stat-modeling-noninferiority.html#technische-gleichheit",
    "href": "stat-modeling-noninferiority.html#technische-gleichheit",
    "title": "57  Äquivalenz oder Nichtunterlegenheit",
    "section": "57.3 Technische Gleichheit",
    "text": "57.3 Technische Gleichheit\nBeginnen wir also mit der Beurteilung von der technischen Gleichheit zweier Verfahren. Ich nutze hier das Wort technische Gleichheit, da wir hier nicht zwei Gruppen miteinander vergleichen, sondern eben kontinuierlich gemessene Werte haben und wissen wollen, ob diese gemessenen Werte aus den beiden Verfahren gleich sind. In unserem Beispiel wollen wir wissen, ob wir den Druckstab zum Messen der Grasdichte durch einen Drohnenüberflug erstetzen können. Der Dronenflug produziert Bilder und wir können auf zwei Arten Zahlen aus den Bildern generieren. Wir extrahieren entweder die RGB-Werte der Bilder oder aber die CMYK-Werte. Hier ist natürlich ein Schritt den ich überspringe, wir erhalten am Ende eben einen Wert für ein Bild. Oder andersherum, wir können genau einer Messung mit dem Druckstab ein Bild der Drone zuordnen.\nIn der Abbildung 57.6 (a) und in der Abbildung 57.6 (b) sehen wir den Zusammenhang zwischen dem Druckstab und der Dronenmessung für beide Farbskalenwerte nochmal visualisiert. In einer idealen Welt würden alle Punkte auf einer Linie liegen. Das heißt, wir haben einen perfekten Zusammenhang zwischen dem Druckstab und den Farbskalenwerten. So ein perfekter Zusammenhang tritt in der Natur nie auf, deshalb müssen wir uns nun mit statistischen Maßzahlen behelfen.\nWir können die Funktion geom_smooth() nutzen um eine lineare Funktion durch die Punkte zu legen. Wir sehen ist der Fehler, dargestellt als grauer Bereich, bei den CMYK-Werten größer. Auch haben wir Punkte die etwas anch oben weg streben. In der RGB-Skala haben wir eher einen linearen Zusammenhang. Im Folgenden wollen wir uns dann einmal die statistischen Maßzahlen zu der Visualisierung anschauen.\n\nggplot(drone_tbl, aes(drone_rgb, pressure_stick)) +\n  theme_minimal() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE)\n\nggplot(drone_tbl, aes(drone_cmyk, pressure_stick)) +\n  theme_minimal() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n(a) Dronenmessung mit RGB-Werten.\n\n\n\n\n\n\n\n\n\n\n\n(b) Dronenmessung mit CMYK-Werten.\n\n\n\n\n\n\n\nAbbildung 57.6— Vergleich der beiden Farbskalen aus der Dronenmessung zu der Grasdichte durch den Druckstab.\n\n\n\n\n\n57.3.1 Bestimmtheitsmaß \\(R^2\\)\nFür die genaueren Werte der linearen Funktion nutzen wir dann die Funktion lm(). Wir brauchen die statistischen Maßzahlen höchstens, wenn uns eine Umrechung von den Werten von der einen Messung zu der anderen Messung interessiert.\n\nfit_drone &lt;- lm(pressure_stick ~ drone_rgb, data = drone_tbl)\nfit_drone |&gt; model_parameters()\n\nParameter   | Coefficient |   SE |           95% CI | t(279) |      p\n---------------------------------------------------------------------\n(Intercept) |      766.33 | 5.12 | [756.26, 776.41] | 149.70 | &lt; .001\ndrone rgb   |        0.78 | 0.01 | [  0.76,   0.81] |  61.76 | &lt; .001\n\n\nZum einen können wir uns jetzt auch die lineare Funktion und damit den Zusammenhang von dem Druckstab zu der RGB-Farbskala erstellen. Mir der folgenden Formel können wir dann die Werte der Dronen RGB-Farbskala in die Werte des Druckstabes umrechnen.\n\\[\npressure\\_stick = 766.33 + 0.78 \\cdot drone\\_rgb\n\\]\nZum anderen erhalten wir mit der Funktion lm() dann auch die Möglichkeit das Bestimmtheitsmaß \\(R^2\\) zu berechnen. Du kennst das Bestimmtheitsmaß \\(R^2\\) schon aus dem Kapitel für die Qualität einer linearen Regression. Hier nochmal kurz zusammengefasst, das Bestimmtheitsmaß \\(R^2\\) beschreibt, wie gut die Punkte auf der Geraden liegen. Ein Bestimmtheitsmaß \\(R^2\\) von 1 bedeutet, dass die Punkte perfekt auf der Geraden liegen. Ein Bestimmtheitsmaß \\(R^2\\) von 0, dass die Punkte eher wild um eine potenzielle Graden liegen.\nIm Folgenden können wir uns noch einmal die Formel des Bestimmtheitsmaß \\(R^2\\) anschauen um etwas besser zu verstehen, wie die Zusammenhänge mathematisch sind. Zum einen brauchen wir den Mittelwert von \\(y\\) als \\(\\bar{y}\\) sowie die Werte der einzelnen Punkte \\(\\bar{y}\\) und die Werte auf der Geraden mit \\(\\hat{y}_i\\).\n\\[\n\\mathit{R}^2 =\n\\cfrac{\\sum_{i=1}^N \\left(\\hat{y}_i- \\bar{y}\\right)^2}{\\sum_{i=1}^N \\left(y_i - \\bar{y}\\right)^2}\n\\]\nIn der Abbildung 57.7 sehen wir den Zusammenhang nochmal visualisiert. Wenn die Abstände von dem Mittelwert zu den einzelnen Punkten mit \\(y_i - \\bar{y}\\) gleich dem Abstand der Mittelwerte zu den Punkten auf der Geraden mit \\(\\hat{y}_i- \\bar{y}\\) ist, dann haben wir einen perfekten Zusammenhang.\n\n\n\n\n\n\nAbbildung 57.7— Auf der linken Seite sehen wir eine Gerade die nicht perfekt durch die Punkte läuft. Wir nehmen ein Bestimmtheitsmaß \\(R^2\\) von ca. 0.7 an. Die Abstände der einzelnen Beobachtungen \\(y_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\) ist nicht gleich den Werten auf der Geraden \\(\\hat{y}_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\). Dieser Zusammenhang wird in der rechten Abbildung mit einem Bestimmtheitsmaß \\(R^2\\) von 1 nochmal deutlich.\n\n\n\nWir können die Funktion glance() nutzen um uns das r.squared und das adj.r.squared wiedergeben zu lassen.\n\nfit_drone |&gt; \n  glance() |&gt; \n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.932\n\n\nWir haben wir ein \\(R^2\\) von \\(0.932\\) vorliegen. Damit erklärt unser Modell bzw. die Gerade 93.2% der Varianz. Der Anteil der erklärten Varianz ist auch wunderbar hoch, so dass wir davon ausgehen können, dass der Druckstab und die RGB-Werte der Drone ungefähr das Gleiche wiedergeben.\n\n\n57.3.2 Korrelation\nNeben der Information wie gut die Punkte auf der Geraden liegen, also wie die Punkte um die Gerade streuen, können wir uns auch die Korrelation und damit die Steigung der Gerade wiedergeben lassen. Das Bestimmtheitsmaß \\(R^2\\) sagt uns nämlich nichts über die Richtung der Geraden aus. Die Korrelation liefert uns die Steigung der Geraden mit dem Vorzeichen.\nWir können hier verschiedene Korrelationsmaße berechnen. Am häufigsten werden wir die Korrelation nach Pearson berechnen, da wir von einem normalverteilten \\(y\\) ausgehen. Wenn dies nicht der Fall sein sollte empfiehlt sich stattdessen den Korrelationkoeffizienten nach Spearman zu nutzen.\n\ndrone_tbl %$% \n  cor(pressure_stick, drone_rgb, method = \"pearson\") |&gt; \n  round(2)\n\n[1] 0.97\n\ndrone_tbl %$% \n  cor(pressure_stick, drone_rgb, method = \"spearman\") |&gt; \n  round(2)\n\n[1] 0.96\n\n\n\n\nWir nutzen hier den %$%-Operator, da wir in die Funktion cor() die Spalten übergeben wollen. Die Funktion cor() ist relativ alt und möchte daher keinen Datensatz sondern zwei Vektoren.\nNachdem wir die Korrelation berechnet haben, sehen wir das wir einen positiven Zusammenhang vorliegen haben. Die Gerade durch die Punkte steigt an und ist fast eine 45\\(^{\\circ}\\) Gerade, da wir eine Korrelation nahe 1 vorliegen haben.\n\n\n57.3.3 MSE, RMSE, nRMSE und MAE\nNeben der Betrachtung der Abweichung vom Mittelwert von \\(y\\) können wir uns auch die Abstände von den geschätzten Punkten auf der Geraden \\(\\hat{y}_i\\) zu den eigentlichen Punkten anschauen \\(y_i\\). Wir haben jetzt zwei Möglichkeiten die Abstände zu definieren.\n\nWir schauen uns die quadratischen Abstände mit \\((y_i - \\hat{y}_i)^2\\) an. Wir berechnen dann die mittlere quadratische Abweichung (eng. mean square error abk. MSE).\nWir schauen uns die absoluten Abstände mit \\(|y_i - \\hat{y}_i|\\) an. Wir berechnen dann den mittleren absoluten Fehler (eng. mean absolute error, abk. MAE).\n\nIm Folgenden betrachten wir erst den MSE und seine Verwandten. Wie wir an der Formel sehen, berechnen wir für den MSE einfach nur die quadratische Abweichung zwischen den Beobachtungen \\(y_i\\) und den Werten auf der berechneten Geraden \\(\\hat{y}_i\\). Dann summieren wir alles auf und teilen noch durch die Anzahl der Beobachtungen also Punkte \\(n\\).\n\\[\nMSE = \\cfrac{1}{n}\\sum^n_{i=1}(y_i - \\hat{y}_i)^2\n\\]\nHäufig wollen wir dann nicht die quadratischen Abweichungen angeben. Wir hätten dann ja auch die Einheit der Abweichung im Quadrat. Daher ziehen wir die Wurzel aus dem MSE und erhalten den root mean square error (abk. RMSE). Hierfür gibt es dann keine gute Übersetzung ins Deutsche.\n\\[\nRMSE = \\sqrt{MSE} = \\sqrt{\\cfrac{1}{n}\\sum^n_{i=1}(y_i - \\hat{y}_i)^2}\n\\]\nDer RMSE ist ein gewichtetes Maß für die Modellgenauigkeit, das auf der gleichen Skala wie das Vorhersageziel angegeben wird. Einfach ausgedrückt kann der RMSE als der durchschnittliche Fehler interpretiert werden, den die Vorhersagen des Modells im Vergleich zum tatsächlichen Wert aufweisen, wobei größere Vorhersagefehler zusätzlich gewichtet werden.\nJe näher der RMSE-Wert bei 0 liegt, desto genauer ist das Modell. Der RMSE-Wert wird auf derselben Skala zurückgegeben wie die Werte, für das du Vorhersagen treffen willst. Es gibt jedoch keine allgemeine Regel für die Interpretation von den Wertebereichen des RMSE. Die Interpretation des RMSE kann nur innerhalb deines Datensatzes bewertet werden.\n\ndrone_tbl |&gt;\n  rmse(pressure_stick, drone_rgb)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        694.\n\n\nAls letzte Möglichkeit sei noch der normalisierte root mean square error (abk. nRMSE) genannt. In diesem Fall wird der RMSE nochmal durch den Mittelwert von \\(y\\) geteilt.\n\\[\nnRMSE = \\cfrac{RMSE}{\\bar{y}} = \\cfrac{\\sqrt{MSE}}{\\bar{y}} = \\cfrac{\\sqrt{\\cfrac{1}{N}\\sum^N_{i=1}(y_i - \\hat{y}_i)^2}}{\\bar{y}}\n\\]\nIn wie weit jetzt jedes MSE Abweichungsmaß sinnvoll ist und auch in der Anwendung passen mag, sei einmal dahingestellt. Wichtig ist hier zu Wissen, dass wir die MSE-Fehler nutzen um verschiedene Verfahren zu vergleichen. Ein kleiner Fehler ist immer besser. Ein einzelner MSE-Wert an sich, ist dann immer schwer zu interpretieren.\nAls Alternative zu den MSE-Fehlern bietet sich dann der MAE an. Hier schauen wir dann auf die absoluten Abstände. Wir nehmen also das Vorzeichen raus, damit sich die Abstände nicht zu 0 aufaddieren. Wir haben dann folgende Formel vorliegen.\n\\[\nMAE = \\cfrac{1}{n}\\sum^n_{i=1}|y_i - \\hat{y}_i|\n\\]\nDer MAE hat gegenüber dem RMSE Vorteile in der Interpretierbarkeit. Der MAE ist der Durchschnitt der absoluten Werte der Fehler. MAE ist grundsätzlich leichter zu verstehen als die Quadratwurzel aus dem Durchschnitt der quadrierten Fehler. Außerdem beeinflusst jede einzelne Abweichung den MAE in direktem Verhältnis zum absoluten Wert der Abweichung, was bei der RMSE nicht der Fall ist. Der MAE ist nicht identisch mit dem mittleren quadratischen Fehler (RMSE), auch wenn einige Forscher ihn so angeben und interpretieren. MAE ist konzeptionell einfacher und auch leichter zu interpretieren als RMSE: Es ist einfach der durchschnittliche absolute vertikale oder horizontale Abstand zwischen jedem Punkt in einem Streudiagramm und der Geraden.\n\ndrone_tbl |&gt;\n  mae(pressure_stick, drone_rgb)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        691.\n\n\nWir können uns mit der Funktion metrics() auch die Fehler zusammenausgeben lassen.\n\ndrone_tbl |&gt;\n  metrics(pressure_stick, drone_rgb)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     694.   \n2 rsq     standard       0.932\n3 mae     standard     691.   \n\n\nWie schon oben geschrieben, der MSE und Co. sind nur in einem Vergleich sinnvoll. Deshalb hier nochmal der Vergleich der beiden Farbskalen der Dronenbilder.\n\ndrone_tbl |&gt;\n  metrics(pressure_stick, drone_rgb)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     694.   \n2 rsq     standard       0.932\n3 mae     standard     691.   \n\ndrone_tbl |&gt;\n  metrics(pressure_stick, drone_cmyk)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     843.   \n2 rsq     standard       0.546\n3 mae     standard     831.   \n\n\nWir schon zu erwarten ist auch hier der Fehler bei den RGB-Werten kleiner als bei den CMYK-Werten. Daher würden wir uns hier für die Umrechnung der RGB-Werte entscheiden.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Äquivalenz oder Nichtunterlegenheit</span>"
    ]
  },
  {
    "objectID": "stat-modeling-noninferiority.html#medizinische--oder-behandlungsgleichheit",
    "href": "stat-modeling-noninferiority.html#medizinische--oder-behandlungsgleichheit",
    "title": "57  Äquivalenz oder Nichtunterlegenheit",
    "section": "57.4 Medizinische- oder Behandlungsgleichheit",
    "text": "57.4 Medizinische- oder Behandlungsgleichheit\n\n\n\n\n\n\nDisclaimer - Wichtig! Lesen!\n\n\n\nDer folgende Text ist ein Lehrtext für Studierende. Es handelt sich keinesfalls um eine textliche Beratung für Ethikanträge oder Tierversuchsanträge geschweige den der Auswertung einer klinischen Studie. Alle Beispiel sind im Zweifel an den Haaren herbeigezogen und dienen nur der Veranschaulichung möglicher Sachverhalte.\nAntragsteller:innen ist die statistische Beratung von einer entsprechenden Institution dringlichst angeraten.\n\n\nWir eingangs schon geschrieben wollen wir bei der Medizinische- oder Behandlungsgleichheit nachweisen, dass sich verschiedene Behandlungsgruppen zu einer Kontrolle oder Standard gleich oder äquivalent sind. Wir haben es hier als mit einem klassischen Gruppenvergleich zu tun, bei dem wir die Hypothesen drehen. Wenn wir auf Unterschied testen, dann haben wir in der Nullhypothese \\(H_0\\) die Gleichheit zwischen zwei Mittelwerten stehen. Wir wollen die Gleichheit der Mittelwerte ablehnen. Wir schreiben also unsere beiden Hypothesenpaare wie folgt.\n\n\nWie immer gibt es auch tolle Tutorien wie das Tutorium von Daniël Lakens Equivalence Testing and Interval Hypotheses\n\nStatistischer Test auf Unterschied\n\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{1} &= \\bar{y}_{2} \\\\  \nH_A: \\bar{y}_{1} &\\neq \\bar{y}_{2} \\\\   \n\\end{aligned}\n\\]\n\n\nWenn wir jetzt einen statistischen Teste für die Äquivalenz oder Nichtunterlegenheit rechnen wollen, dann drehen wir das Hypothesenpaar. Wir wollen jetzt in der Nullhypothese die “Ungleichheit” der Mittelwerte ablehnen.\n\nStatistischer Test auf Gleichheit\n\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{1} &\\neq \\bar{y}_{2} \\\\  \nH_A: \\bar{y}_{1} &= \\bar{y}_{2} \\\\   \n\\end{aligned}\n\\]\n\n\nUnd hier beginnt auch schon die Krux. Konnten wir uns relativ einfach einigen, dass ein Mittelwertesunterschied \\(\\Delta\\) von 0 eine Gleichheit zwischen den beiden Mittelwerten der beiden Gruppen bedeutet, so ist die Festlegung auf einen Unterschied schon schwieriger. Die Bewertung, ob zwei Mittelwerte sich für zwei Gruppen unterscheiden, kann nur im Kontext der biologischen oder medizinischen Fragestellung beantwortet werden. Die Diskussion, ob ein \\(\\Delta\\) von 0.1 noch gleich oder ungleich ist, kann rein numerisch schwer geführt werden. Deshalb gibt es einige Richtlinien und Richtwerte.\nAus dem Grund definieren wir Äquivalenzgrenzen oder Äquivalenzzone. Die Äquivalenzzone wird durch eine untere Äquivalenzgrenze und/oder eine obere Äquivalenzgrenze definiert. Die untere Äquivalenzgrenze (UEG) definiert deine untere Grenze der Akzeptanz für die Mittelwertsdifferenz. Die obere Äquivalenzgrenze (UEL) definiert die obere Grenze der Akzeptanz für die Mittelwertsdifferenz. Jede Abweichung von der Mittelwertsdifferenz, die innerhalb dieses Bereichs der Äquivalenzgrenzen liegt, wird als unbedeutend angesehen. Du kannst hier statt Mittelwertsdifferenz natürlich auch in Anteilen denken, wenn es um das Odds ratio oder Risk ratio geht.\nAls erstes Beispiel einer Behörde hier einmal das Zitat der Europäische Behörde für Lebensmittelsicherheit (EFSA) für die Zulassung eines Pilzmittels aus unseren Beispiel. Hier sei angemerkt, dass viele statistische Methoden von einem normalverteilten Outcome oder aber approximativ log-normalverteilten Outcome ausgehen. Deshalb werden die Äquivalenzgrenzen hier auch auf der \\(log\\)-Skala benannt.\n\n“The limits for equivalence were set to \\(-\\cfrac{1}{2}\\log\\) and \\(\\cfrac{1}{2}\\log\\) equal to -0.5 and 0.5 because of the log transformation of the outcome.” — Europäische Behörde für Lebensmittelsicherheit (EFSA)\n\nHäufig werden die Effekte aus verschiedenen Studien auch skaliert, damit wir dann die Effekte besser vergleichen können. Als Skalierung bietet sich eine Normalisierung oder Standardisierung an. Als Beispiel in den Pflanzenwissenschaften sei Voet u. a. (2019) genannt. Voet u. a. (2019) führen Analysen zum Schutz vor unbeabsichtigten Auswirkungen von gentechnisch verändertem Mais auf die Umwelt oder die menschliche Gesundheit durch [Link]. Hier hilft besonders sich von anderen Studien vor dem Experiment zu inspirieren zu lassen. Um eine ausgiebige Literaturrecherche kommt man dann meist nicht rum.\nAuch sei noch das Institut für Qualität und Wirtschaftlichkeit im Gesundheitswesen (IQWiG) erwähnt, welches für die Regulierung von Anwendungen in der Humanmedizin zu tun hat. Da sind ja die Grenzen immer etwas fließend. Wann ist ein Medikament nur für die Agrarwissenschaften relevant und hate keine Auswirkungen auf den Menschen? Diese Frage lasse ich hier offen. Hier hilft aber auch der Blick in das Papier Allgemeine Methoden und dann das Kapitel 9. Hier einmal ein Zitat aus dem Abschnitt zu dem Nachweis zur Gleichheit. Wir sehen. so einfach ist die Sachlage nicht.\n\n“Umgekehrt erfordert auch die Interpretation nicht statistisch signifikanter Ergebnisse Aufmerksamkeit. Insbesondere wird ein solches Ergebnis nicht als Nachweis für das Nichtvorhandensein eines Effekts (Abwesenheit bzw. Äquivalenz) gewertet.” — Kapitel 9.3.5 Nachweis der Gleichheit in Allgemeine Methoden des Institut für Qualität und Wirtschaftlichkeit im Gesundheitswesen (IQWiG)\n\nSchauen wir uns nun nochmal unsere Keimungsdaten nach Behandlung mit sechs biologischen Pilzmittel unter zwei Kältebehandlungen an. Wenn wir den Richtlinien der EFSA folgen, dann rechnen wir auf den \\(\\log\\)-transformierten Daten. Die \\(\\log\\)-transformierten Daten sind damit auch approximativ normalverteilt, so dass wir hier dann alle statistischen Methoden nutzen können, die eine Normalverteilung voraussetzen. In der Abbildung 57.8 sehen wir, dass die \\(\\log\\)-transformierten Daten eindeutig mehr einer Normalverteilung folgen.\n\ncold_seed_tbl |&gt; \n  pivot_longer(cold:last_col(),\n               names_to = \"type\",\n               values_to = \"growth\") |&gt; \n  mutate(type = as_factor(type)) |&gt; \n  ggplot(aes(trt, growth, fill = trt)) +\n  theme_minimal() +\n  geom_boxplot() +\n  facet_wrap(~ type, scales = \"free_y\") +\n  scale_fill_okabeito() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nAbbildung 57.8— Boxplots der Wachstumsraten des Pilzes in der Standardkontrolle sowie den sechs neunen Präparaten. Die oberen Abbildungen sind auf der normalen Datenskala, die unteren Abbildungen sind \\(\\log\\)-transformiert.\n\n\n\n\n\nIm Folgenden gehen wir jetzt von einfach nach kompliziert. Daher schauen wir uns erstmal die einfachste statistische Methode an, die wir nutzen können und werden dann inhaltlich komplizierter.\n\n57.4.1 ANOVA mit Effektschätzer\nAls erstes missbrauchen wir die ANOVA für den Nachweis der Gleichheit. Das ist die Schlechteste der denkbaren Möglichkeiten aber im Rahmen einer Bachelorarbeit oder aber um sich einen ersten Überblick zu verschaffen sinnvoll. Warum ist die die ANOVA so schlecht? Wir testen hier weiterhin die Nullhypothese auf Gleichheit. Wenn wir also einen signifikante ANOVA vorfinden, dann würden wir die Nullhypothese der Gleichheit ablehnen und auf einen Mittelwertsunterschied schließen. Wie wir schon vorab gelernt haben, ist eine nicht signifikante ANOVA kein schlüssiger Beweis für die Gültigkeit der Nullhypothese der Gleichheit. Wir arbeiten mit dem Falsifikationsprinzip, wir können nur Hypothesen ablehnen. Eine abgelehnte Hypothese bedeutet aber nicht im Umkehrschluss, dass die Gegenhypothese wahr ist. Daher nutzen wir hier die ANOVA als einen Art Seismographen. Eine signifikante ANOVA deutet auf einen Unterschied in den Mittelwerten hin, dann ist es vermutlich unwahrscheinlich, dass wir Gleichheit vorliegen haben.\nSchauen wir uns dazu einmal die zwei einfaktoriellen ANOVA’s für die kälte und nicht-kälte Behandlung einmal an. Als erstes rechnen wir eine einfacktorielle ANOVA und schauen, ob wir ein signifkantes Eregbnis vorliegen haben.\n\nlm_non_cold_fit &lt;- lm(log_non_cold ~ trt, data = cold_seed_tbl)\n\nlm_non_cold_fit |&gt; anova() |&gt; model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |    F |     p\n---------------------------------------------------------\ntrt       |       26.39 |  6 |        4.40 | 2.75 | 0.018\nResiduals |      118.50 | 74 |        1.60 |      |      \n\nAnova Table (Type 1 tests)\n\n\nDa der \\(p\\)- Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) müssen wir die Nullhypothese der Gleichheit ablehnen. Mindestens einen paarweisen Unterschied zwischen den Gruppen gibt es. Prinzipiell könnten wir natürlich hoffen, dass alle Gruppen gleich zur Kontrolle sind und sich nur zwei Behandlungsgruppen unterscheiden, aber es ist schonmal ein schlechtes Zeichen, wenn wir eine signifikante ANOVA vorliegen haben und auf Gleichheit der Behandlungen zur Kontrolle aus sind.\nSchauen wir nochmal auf den Effekt. Wenn wir einen großen Effekt der Behandlungsgruppen vorliegen haben, dann deutet dies auch nicht gerade auf gleiche Gruppenunterschiede.\n\nlm_non_cold_fit |&gt; eta_squared() \n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\ntrt       | 0.18 | [0.02, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass wir nur 18% der Varianz durch unsere Behandlungsgruppen erklären. Daher würden wir hier nicht von einem großen Effekt ausgehen. Würde auch hier viel Varianz erklärt, dann könnten wir hier auch aufhören. Schauen wir uns nochmal die andere Art der Vorbehandlung an.\nJetzt schauen wir nochmal wo die paarweisen Unterschiede sind. Wir nutzen dazu den pairwise.t.test(). Darüber hinaus haben wir bei den nicht-kälte behandelten Samen eine stark unterschiedliche Streuung der einzelnen Beobachtungen in den Gruppen. Durch die unterschiedlichen Varianzen in den Gruppen setzten wir pool.sd = FALSE und nehmen hier Varianzheterogenität an. Dann schauen wir uns einmal das compact letter display an und sehen, welche Behandlungen sich voneinander unterscheiden oder nicht.\n\ncold_seed_tbl %$%\n  pairwise.t.test(log_non_cold, trt, pool.sd = FALSE,\n                  p.adjust.method = \"none\") |&gt; \n  extract2(\"p.value\") |&gt; \n  fullPTable() |&gt; \n  multcompLetters()\n\nctrl    3    4    5    6    7    8 \n \"a\" \"ab\" \"ab\"  \"b\"  \"a\"  \"b\" \"ab\" \n\n\nWir sehen, dass wir Unterschiede haben. Da wir hier nur zur Kontrolle vergleichen wollen, schauen wir nach dem Buchstaben der Kontrolle und sehen, dass wir hier den letter ahaben. Wir interpretieren das compact letter display nun etwas statistisch schief in dem Sinne, das gleiche Buchstaben Gleichheit aussagen. Immerhin unterscheiden sich die Behandlungen 3, 4, 6 und 8 nicht von der Kontrolle.\nDen gleichen Ablauf können wir jetzt auch einmal für die kälte-behandleten Samen machen. Wir rechnen wieder als erstes eine einfaktorielle ANOVA und schauen, ob wir einen signifikanten Unterschied zwischen den Gruppen haben.\n\nlm_cold_fit &lt;- lm(log_cold ~ trt, data = cold_seed_tbl)\n\nlm_cold_fit |&gt; anova() |&gt; model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |    F |     p\n---------------------------------------------------------\ntrt       |        9.80 |  6 |        1.63 | 2.71 | 0.020\nResiduals |       44.59 | 74 |        0.60 |      |      \n\nAnova Table (Type 1 tests)\n\n\nAuch hier zeigt die signifikante ANOVA, dass wir mindestens einen paarweisen Mittelwertsunterschied zwischen den Behandlungsgruppen haben. Welche Gruppen sich nun unterscheiden werden wir dann gleich einmal in dem paarweisen t-Test uns anschauen. Vorher nochmal schauen wir nochmal wie stark der Effekt der Behandlungsgruppen ist.\n\nlm_cold_fit |&gt; eta_squared()\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\ntrt       | 0.18 | [0.02, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nAuch hier können wir 18% der Varianz in dem Wachstum durch die Behandlungsgruppen erklären. Das ist recht wenig. Schauen wir aber jetzt einmal den paarweisen t-Test an. Die Varianzen sind ungefähr gleich in den Gruppen, die Boxplots haben ungefähr die gleiche Ausdehnung nach der Logarithmierung. Deshalb nutzen wir hier einmal die Funktionalität von {emmeans}. Wir können bei der Funktion emeans() eine Deltaschranke einführen. Dann haben wir mit dem compact letter display ein Test auf Nichtunterlegenheit oder eben Gleichheit. Ich wähle hier mal eine Option von delta = 1 in der Funktion cld() um einfach mal zu zeigen, wie es dann aussieht.\n\nlm_cold_fit |&gt; \n  emmeans(~ trt) |&gt;\n  cld(Letters = letters, adjust = \"none\", delta = 1) |&gt; \n  arrange()\n\n trt  emmean    SE df lower.CL upper.CL .equiv.set\n 5      7.71 0.234 74     7.24     8.17  a        \n 4      7.87 0.224 74     7.42     8.32  a        \n 8      7.97 0.224 74     7.53     8.42  a        \n 3      8.09 0.224 74     7.64     8.54  a        \n 7      8.13 0.224 74     7.68     8.57  a        \n 6      8.14 0.245 74     7.65     8.62  a        \n ctrl   8.88 0.224 74     8.43     9.32   b       \n\nConfidence level used: 0.95 \nStatistics are tests of equivalence with a threshold of 1 \nP values are left-tailed \nsignificance level used: alpha = 0.05 \nEstimates sharing the same symbol test as equivalent \n\n\nHier unterscheiden sich nun alle Behandlungsgruppen von der Kontrolle. Die neuen Behandlungen unterscheiden sich aber untereinander nicht. Jetzt können wir uns noch anschauen, wie groß den der Mittelwertsunterschied für die einzelnen Behandlungen jeweils ist. Das kann uns die Funktion pwpm() wiedergeben.\n\nlm_cold_fit |&gt; \n  emmeans(~ trt) |&gt; \n  pwpm(adjust = \"none\")\n\n         ctrl        3        4        5        6        7      8\nctrl   [8.88]   0.0151   0.0021   0.0005   0.0286   0.0206 0.0057\n3     0.78844   [8.09]   0.4900   0.2401   0.8895   0.9033 0.7181\n4     1.00830  0.21986   [7.87]   0.6146   0.4258   0.4173 0.7413\n5     1.17217  0.38373  0.16387   [7.71]   0.2088   0.1965 0.4093\n6     0.74212 -0.04632 -0.26618 -0.43005   [8.14]   0.9816 0.6292\n7     0.74980 -0.03865 -0.25851 -0.42238  0.00767   [8.13] 0.6296\n8     0.90329  0.11485 -0.10501 -0.26888  0.16117  0.15350 [7.97]\n\nRow and column labels: trt\nUpper triangle: P values \nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later\n\n\nWir lesen Lower triangle: Comparisons (estimate)   earlier vs. later und damit wird jeweils der ctrl- Mittelwert minus den Behandlungsmittelwerten in der ersten Spalte angegeben. Wir sehen, dass die Kontrolle immer ein größeres Wachstum hat. Damit haben wir zwar keine Gleichheit gezeigt, aber unsere Behandlungen haben alle weniger als die Kontrolle. Eventuell reicht das ja aus, wir haben zwar nicht gleich viel Wachstum wie die Kontrolle, aber durchgehend weniger.\nDas war jetzt hier natürlich die Version für Arme bzw. wenn wir im Rahmen einer Abschlussarbeit noch zusätzlich was berechnen wollen. Wenn es etwas aufwendiger sein soll, dann gibt es natürlich auch richtige statistischen Methoden für den Äquivalenztest.\n\n\n57.4.2 Äquivalenztest\nWenn wir wirklich in unserem Experiment oder Studie einen Äquivalenztest gleich von Anfang an rechnen wollen, dann können wir auch in R auf ein weitreichendes Angebot an Paketen und Funktionen zurückgreifen. Wie immer ist die Frage, was wollen wir? Wichtig ist, dass wir immer eine Gruppe brauchen zu der wir die Äquivalenz oder Gleichheit bestimmen wollen. Im Weiteren haben wir die Wahl zwischen zwei Ansätzen. Einmal den frequentistischen Ansatz sowie die bayesianische Variante. Ich möchte hier nicht mehr so ins Detail gehen, wir nutzen den frequentistischen Ansatz. Das hat auch den Vorteil, dass wir die Äquivalenz an 95% Konfidenzintervallen überprüfen. Mehr gibt es dann jeweils auf den beiden Hilfeseiten der Funktionen.\n\nEinmal den frequentistischen Ansatz aus dem Paket {parameters} durch die Funktion parameters::equivalence_test()\nOder die bayesianische Variante aus dem R Paket {bayestestR} durch die Funktion bayestestR::equivalence_test()\n\nWichtig ist, dass wir immer zum ersten Level des Faktors der Behandlung vergleichen! Das heißt, deine Kontrollgruppe sollte immer die erste Gruppe sein und das erste Level haben. Du kannst sonst mit der Funktion fct_relevel() und mutate() die Level neu anordnen. Wir haben hier aber das Glück, dass wir die Kontrolle als erstes Level der Behandlung trt vorliegen haben.\nWie gehen wir nun vor? Die Funktion equivalence_test() berechnet 95% Konfidenzintervalle für die paarweisen Vergleiche von jeder Behandlung zur Kontrolle. Wir erhalten also sechs 95% Konfidenzintervalle. Im Weiteren müssen wir entscheiden wie groß der Bereich der Äquivalenzzone sein soll. Wir müssen also über die Option range = Grenzen definieren und lassen die Funktion equivalence_test() die Grenzen selbstständig berechnen. Ich empfehle immer die Grenzen aus der Litertaur zu nehmen. In unserem Fall sind es die \\(\\log\\)-Grenzen aus der EFSA Regulierung mit -0.5 und 0.5, die setzen wir dann in die Option range = ein. Die range repräsentiert hierbei die Region of Practical Equivalence (ROPE) oder eben unsere Äquivalenzgrenzen.\n\nres_non_cold &lt;- parameters::equivalence_test(lm_non_cold_fit, \n                                             ci = 0.95,\n                                             range = c(-0.5, 0.5))\nres_non_cold\n\n# TOST-test for Practical Equivalence\n\n  ROPE: [-0.50 0.50]\n\nParameter   |         90% CI |   SGPV | Equivalence |      p\n------------------------------------------------------------\n(Intercept) | [ 6.99,  8.21] | &lt; .001 |    Rejected | &gt; .999\ntrt [3]     | [-1.81, -0.08] | 0.241  |    Rejected | 0.807 \ntrt [4]     | [-1.94, -0.22] | 0.164  |    Rejected | 0.868 \ntrt [5]     | [-2.87, -1.11] | &lt; .001 |    Rejected | 0.997 \ntrt [6]     | [-1.41,  0.39] | 0.494  |   Undecided | 0.541 \ntrt [7]     | [-2.16, -0.43] | 0.038  |    Rejected | 0.936 \ntrt [8]     | [-1.90, -0.18] | 0.188  |    Rejected | 0.851 \n\n\nWir sehen also einmal das Ergebnis unseres Äquivalenztest. Wichtig hierbei ist, dass wir zu der Kontrolle testen. Unsere Kontrolle steckt in dem (Intercept) und die Effekte aus der linearen Regression in lm_non_cold_fit beziehen sich ja alle auf den Intercept, so wird das Modell mit kategorialen Variablen gebaut. In der Spalte % in ROPE sehen wir in wie weit der Unterschied zwischen den Behandlungen, dargestellt als 95% Konfidenzintervall, in den Äquivalenzgrenzen liegt. In der folgenden Spalte H0 dann die Entscheidung für oder gegen die die Nullhypothese im Sinne der Gleichheit. Gut, das ist etwas wirr, schauen wir uns einmal die Abbildung 57.9 an, dann wird es klarer.\n\nplot(res_non_cold) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 57.9— Darstellung der zweiseitigen 95% Konfidenzintervalle für die Vergleiche der Behandlungen zu der Kontrolle. Liegt ein 95% Konfidenzintervall in den Äquivalenzgrenzen so kann die Nullhypothese und damit Gleichheit zur Kontrolle nicht abgelehnt werden.\n\n\n\n\n\nAls Ergebnis können wir mitnehmen, dass keine der Behandlungen gleich zur Kontrolle ist. Die Behandlung 6 ist noch am nächsten dran, gleich zu sein, aber wir können hier anhand den Daten keine Entscheidung treffen. Am Ende unterscheiden sich alle Behandlungen von der Kontrolle.\nAls anderes Beispiel schauen wir uns dann einmal noch die kälte Behandlung an. Hier sehen wir dann schon ein anderes Bild. Die Interpretation ist die gleiche wie eben schon. Wir wollen, dass die 95% Konfidenzintervalle in den Äquivalenzgrenzn von ROPE: [-0.50 0.50] liegen.\n\nres_cold &lt;- parameters::equivalence_test(lm_cold_fit, \n                                         ci = 0.95,\n                                         range = c(-0.5, 0.5))\nres_cold\n\n# TOST-test for Practical Equivalence\n\n  ROPE: [-0.50 0.50]\n\nParameter   |         90% CI |   SGPV | Equivalence |      p\n------------------------------------------------------------\n(Intercept) | [ 8.50,  9.25] | &lt; .001 |    Rejected | &gt; .999\ntrt [3]     | [-1.32, -0.26] | 0.227  |    Rejected | 0.817 \ntrt [4]     | [-1.54, -0.48] | 0.019  |    Rejected | 0.944 \ntrt [5]     | [-1.71, -0.63] | &lt; .001 |    Rejected | 0.979 \ntrt [6]     | [-1.30, -0.19] | 0.281  |    Rejected | 0.766 \ntrt [7]     | [-1.28, -0.22] | 0.263  |    Rejected | 0.784 \ntrt [8]     | [-1.43, -0.38] | 0.118  |    Rejected | 0.896 \n\n\nIn der Abbildung 57.10 sehen wir den Zusammenhang nochmal visualisiert. Hier fällt nochmal deutlicher auf, dass alle Behandlungen geringere Wachstumszahlen aufweisen. Wir könnten also Gleichheit auch so definieren, dass weniger oder gleich auch als äquivalent gilt. Das hängt natürlich vom biologischen Kontext ab, aber wir machen das jetzt einfach mal.\n\nplot(res_cold) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 57.10— Darstellung der zweiseitigen 95% Konfidenzintervalle für die Vergleiche der Behandlungen zu der Kontrolle. Liegt ein 95% Konfidenzintervall in den Äquivalenzgrenzen so kann die Nullhypothese und damit Gleichheit zur Kontrolle nicht abgelehnt werden.\n\n\n\n\n\nWenn du keine untere oder obere Grenze möchtest, also eigentlich nur eine Grenze, dann kannst du die andere Grenze auf Unendlich Inf setzen. Wir setzen hier mal die untere Grenze der Äquivalenz auf -Inf und testen somit faktisch nur einseitig.\n\nres_low_cold &lt;- parameters::equivalence_test(lm_cold_fit, \n                                             ci = 0.95,\n                                             range = c(-Inf, 0.5))\nres_low_cold \n\n# TOST-test for Practical Equivalence\n\n  ROPE: [-Inf 0.50]\n\nParameter   |         90% CI |   SGPV | Equivalence |      p\n------------------------------------------------------------\n(Intercept) | [ 8.50,  9.25] | &lt; .001 |    Accepted | &lt; .001\ntrt [3]     | [-1.32, -0.26] | &gt; .999 |    Accepted | 0.817 \ntrt [4]     | [-1.54, -0.48] | &gt; .999 |    Accepted | 0.944 \ntrt [5]     | [-1.71, -0.63] | &gt; .999 |    Accepted | 0.979 \ntrt [6]     | [-1.30, -0.19] | &gt; .999 |    Accepted | 0.766 \ntrt [7]     | [-1.28, -0.22] | &gt; .999 |    Accepted | 0.783 \ntrt [8]     | [-1.43, -0.38] | &gt; .999 |    Accepted | 0.896 \n\n\nWir sehen jetzt, dass sich alle 95% Konfidenzintervalle im ROPE-Bereich befinden. Damit sind dann auch alle Behandlungen gleich zur Kontrolle. Das gilt natürlich nur, da wir weniger als gleich definiert haben! Schauen wir nochmal in der Abbildung 57.11 uns die Visualisierung an.\n\nplot(res_low_cold) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 57.11— Darstellung der einseitigen 95% Konfidenzintervalle für die Vergleiche der Behandlungen zu der Kontrolle. Liegt ein 95% Konfidenzintervall in den Äquivalenzgrenzen so kann die Nullhypothese und damit Gleichheit zur Kontrolle nicht abgelehnt werden. Hier wurde nur einseitig getestet, da weniger als die Kontrolle in dem biologischen Kontext auch als Äquivalenz zählt.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Äquivalenz oder Nichtunterlegenheit</span>"
    ]
  },
  {
    "objectID": "stat-modeling-noninferiority.html#referenzen",
    "href": "stat-modeling-noninferiority.html#referenzen",
    "title": "57  Äquivalenz oder Nichtunterlegenheit",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 57.1— Ursprüngliche Abbildung, die nachgebaut werden soll. Zwei lineare Regressionen mit den jeweiligen Regressionsgleichungen.\nAbbildung 57.2— Einmal die einfache Abbildung der linearen Regression in ggplot für Basilikum nachgebaut. Beachte die Funktion filter(), die den jeweiligen Datensatz für die beiden Kräuter erzeugt.\nAbbildung 57.3— Einmal die einfache Abbildung der linearen Regression in ggplot für Oregano nachgebaut. Beachte die Funktion filter(), die den jeweiligen Datensatz für die beiden Kräuter erzeugt.\nAbbildung 57.4— Einmal die einfache Abbildung der linearen Regression in ggplot für Oregano nachgebaut. Beachte die Funktion facet_wrap(), die den jeweiligen Datensatz für die beiden Kräuter erzeugt.\nAbbildung 57.5— Einmal die einfache Abbildung der linearen Regression in ggplot nachgebaut. Die Abbildung A zeigt die Punkte und die Geradengleichung für das Basilikum. Die Abbildung B die entsprechenden Informationen für das Oregano. Die beiden Achsenbeschriftungen wurden gekürzt und die Informationen in den Titel übernommen.\nAbbildung 57.6 (a)— Dronenmessung mit RGB-Werten.\nAbbildung 57.6 (b)— Dronenmessung mit CMYK-Werten.\nAbbildung 57.7— Auf der linken Seite sehen wir eine Gerade die nicht perfekt durch die Punkte läuft. Wir nehmen ein Bestimmtheitsmaß \\(R^2\\) von ca. 0.7 an. Die Abstände der einzelnen Beobachtungen \\(y_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\) ist nicht gleich den Werten auf der Geraden \\(\\hat{y}_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\). Dieser Zusammenhang wird in der rechten Abbildung mit einem Bestimmtheitsmaß \\(R^2\\) von 1 nochmal deutlich.\nAbbildung 57.8— Boxplots der Wachstumsraten des Pilzes in der Standardkontrolle sowie den sechs neunen Präparaten. Die oberen Abbildungen sind auf der normalen Datenskala, die unteren Abbildungen sind \\(\\log\\)-transformiert.\nAbbildung 57.9— Darstellung der zweiseitigen 95% Konfidenzintervalle für die Vergleiche der Behandlungen zu der Kontrolle. Liegt ein 95% Konfidenzintervall in den Äquivalenzgrenzen so kann die Nullhypothese und damit Gleichheit zur Kontrolle nicht abgelehnt werden.\nAbbildung 57.10— Darstellung der zweiseitigen 95% Konfidenzintervalle für die Vergleiche der Behandlungen zu der Kontrolle. Liegt ein 95% Konfidenzintervall in den Äquivalenzgrenzen so kann die Nullhypothese und damit Gleichheit zur Kontrolle nicht abgelehnt werden.\nAbbildung 57.11— Darstellung der einseitigen 95% Konfidenzintervalle für die Vergleiche der Behandlungen zu der Kontrolle. Liegt ein 95% Konfidenzintervall in den Äquivalenzgrenzen so kann die Nullhypothese und damit Gleichheit zur Kontrolle nicht abgelehnt werden. Hier wurde nur einseitig getestet, da weniger als die Kontrolle in dem biologischen Kontext auch als Äquivalenz zählt.\n\n\n\nAltman DG, Bland JM. 1995. Statistics notes: Absence of evidence is not evidence of absence. Bmj 311: 485.\n\n\nVoet H van der, Goedhart PW, Garcı́a-Ruiz E, Escorial C, Tulinská J. 2019. Equivalence limit scaled differences for untargeted safety assessments: Comparative analyses to guard against unintended effects on the environment or human health of genetically modified maize. Food and Chemical Toxicology 125: 540–548.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Äquivalenz oder Nichtunterlegenheit</span>"
    ]
  },
  {
    "objectID": "stat-modeling-meta.html",
    "href": "stat-modeling-meta.html",
    "title": "58  Metaanalysen",
    "section": "",
    "text": "58.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, meta, conflicted)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Metaanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-meta.html#daten",
    "href": "stat-modeling-meta.html#daten",
    "title": "58  Metaanalysen",
    "section": "58.2 Daten",
    "text": "58.2 Daten\nWoher kommen eigentlich die Daten einer Metaanalyse? Wir können Google Scholar nutzen um zu einem Thema systematisch wissenschaftliche Veröffentlichungen zu suchen. Der Fokus liegt hier auf systematisch und beschreibt einen strukturierten Reviewprozess. In diesem Kapitel nutzen wir die Daten von Harrer u. a. (2021) modifiziert auf ein agrarwissenschaftliches Beispiel. Wie immer liefert die originale Quelle noch mehr Informationen, wir kürzen hier einmal ab, damit wir die Kerngedanken verstehen.\n\n\nA step by step guide for conducting a systematic review and meta-analysis with simulation data\nBeginnen wir mit einer Datensatz in dem wir uns den Effekt von mittleren Erträgen in Weizen unter der Gabe von Eisen anschauen. Wir haben uns hier für eine Eisendosis mit \\(10\\mu mol\\) entschieden, die in allen Studien vorgekommen ist. Dann haben wir noch geschaut wie viele Pflanzen in der Gruppe untersucht wurden und wie die mitteleren Effekte plus die Standardabweichung waren.\n\ndrymatter_tbl &lt;- read_excel(\"data/meta/drymatter_iron_studies.xlsx\") \n\nSchauen wir uns nochmal die Daten genauer an. Hier ist es besonders wichtig zu beachten, dass wir uns einen Wert aus einem ganzen Experiment anschauen und verschiedene Werte aus verschiedenen Experimenten dann miteinander in Verbindung setzen wollen.\n\n\n\n\nTabelle 58.1— Daten zu den Weizenerträgen nach der Gabe von einer Eisendosis mit \\(10\\mu mol\\). In allen Studien wurde die gleiche Dosis auf die \\(n\\) Pflanzen gegeben.\n\n\n\n\n\n\nauthor\nn\nmean\nsd\n\n\n\n\nDeRubeis, 2005\n180\n32.6\n9.4\n\n\nDimidjian, 2006\n145\n31.9\n7.4\n\n\nDozois, 2009\n48\n28.6\n9.9\n\n\nLesperance, 2007\n142\n30.3\n9.1\n\n\nMcBride, 2007\n301\n31.9\n9.2\n\n\nQuilty, 2014\n104\n29.8\n8.6\n\n\n\n\n\n\n\n\nEin weiterer Effekt den wir uns anschauen können ist der Vergleich von Anteilen. In diesem Fall haben wir Ereignisse (eng. event) gezählt und wissen aber auch die Gesamtzahl an möglichen Ereignissen. Konkret haben wir die Anzahl an infizierten Sonnenblumensamen mit Mehltau nach der Behandlung mit MoldEx betrachtet. Dabei haben wir richtig viele Pflanzen (\\(n\\)) angeschaut und gezählt wie viele Samen dann mit Mehltau infiziert waren (event).\n\nsunflower_tbl &lt;- read_excel(\"data/meta/infected_sunflower_studies.xlsx\") \n\nAuch hier haben wir einmal in die ganzen Studien zu dem Wirkstoff MoldEx geschaut und jeweils rausgeschrieben, wie viele Sonnenblumensamen (\\(n\\)) betrachtet wurden und wie viele von den Sonnenblumen dann infiziert waren (event).\n\n\n\n\nTabelle 58.2— Daten zu den mit Mehltau infizierten Sonnenblumensamen nach der Behandlung mit MoldEx.\n\n\n\n\n\n\nauthor\nevent\nn\n\n\n\n\nBecker, 2008\n2186\n21826\n\n\nBoyd, 2009\n91\n912\n\n\nBoyd, 2007\n126\n1084\n\n\nCerda, 2014\n543\n7646\n\n\nFiellin, 2013\n6496\n55215\n\n\nJones, 2013\n10850\n114783\n\n\nLord, 2011\n86\n527\n\n\nMcCabe, 2005\n668\n9403\n\n\nMcCabe, 2012\n843\n11274\n\n\nMcCabe, 2013\n647\n8888\n\n\nNakawai, 2012\n11521\n126764\n\n\nSung, 2005\n1111\n11554\n\n\nTetrault, 2007\n2090\n16599\n\n\nWu, 2008\n2193\n25127\n\n\nZullig, 2012\n1913\n22783\n\n\n\n\n\n\n\n\nHäufig kann es vorkommen, dass wir weder die Mittelwerte und die Standardabweichung vorliegen haben oder aber die Anteile. Meist haben wir dann Glück, dass wir Effektschätzer wie das Odds ratio (\\(OR\\)), Risk ratio (\\(RR\\)) für die Anteile vorliegen haben. Oder aber wir finden Cohen’s \\(d\\) oder Hedge’s \\(g\\) für den Effekt der standardisierten Mittelwertsunterschiede.\nIn unserem Fall haben wir jetzt Euterkrebsdaten von Kühen und die entsprechenden Hedge’s \\(g\\) Werte für die Differenz der Kontrolle zur Chemotherapie. Auch hier haben alle Kühe die gleiche Chemotherapie erhalten und wir sind nur an dem Effekt zu der Kontrolle interessiert. Es gibt also nur einen paarweisen Gruppenvergleich.\n\ncow_tbl &lt;- read_excel(\"data/meta/cow_cancer_studies.xlsx\") \n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der zu dem Euterkrebs von Kühen an.\n\n\n\n\nTabelle 58.3— Daten zum Euterkrebs von Kühen nach der Behandlung mit einer Chemotherapie zu einer Kontrolle.\n\n\n\n\n\n\nAuthor\nTE\nseTE\n\n\n\n\nCall et al.\n0.71\n0.26\n\n\nCavanagh et al.\n0.35\n0.20\n\n\nDanitzOrsillo\n1.79\n0.35\n\n\nde Vibe et al.\n0.18\n0.12\n\n\nFrazier et al.\n0.42\n0.14\n\n\nFrogeli et al.\n0.63\n0.20\n\n\nGallego et al.\n0.72\n0.22\n\n\nHazlett-Stevens & Oren\n0.53\n0.21\n\n\nHintz et al.\n0.28\n0.17\n\n\nKang et al.\n1.28\n0.34\n\n\nKuhlmann et al.\n0.10\n0.19\n\n\nLever Taylor et al.\n0.39\n0.23\n\n\nPhang et al.\n0.54\n0.24\n\n\nRasanen et al.\n0.43\n0.26\n\n\nRatanasiripong\n0.52\n0.35\n\n\nShapiro et al.\n1.48\n0.32\n\n\nSong & Lindquist\n0.61\n0.23\n\n\nWarnecke et al.\n0.60\n0.25\n\n\n\n\n\n\n\n\nWir haben jetzt also insgesamt drei Datensätze. Einmal einen Datensatz zu Weizenerträgen mit dem Effekt der Mittelwerte, einen Datensatz der Infektionen von Sonnenblumen mit Anteilen sowie einem Datensatz mit Euterkrebs mit vorausberechneten Effektmaß Hedge’s \\(g\\).",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Metaanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-meta.html#das-modell-mit-fixen-effekten",
    "href": "stat-modeling-meta.html#das-modell-mit-fixen-effekten",
    "title": "58  Metaanalysen",
    "section": "58.3 Das Modell mit fixen Effekten",
    "text": "58.3 Das Modell mit fixen Effekten\nDie Idee hinter dem Modell mit fixen Effekten (eng. fixed effect) ist, dass die beobachteten Effektgrößen von Studie zu Studie variieren können, was aber nur auf den Stichprobenfehler zurückzuführen ist. In Wirklichkeit sind die wahren Effektgrößen alle gleich: die Effekte sind fix. Aus diesem Grund wird das Modell mit festen Effekten manchmal auch als Modell mit “gleichen Effekten” oder “gemeinsamen Effekten” bezeichnet.\nDas Modell der festen Effekte geht davon aus, dass alle unsere Studien Teil einer homogenen Population sind und dass die einzige Ursache für Unterschiede in den beobachteten Effekten der Stichprobenfehler der Studien ist. Wenn wir die Effektgröße jeder Studie ohne Stichprobenfehler berechnen würden, wären alle wahren Effektgrößen absolut gleich.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Metaanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-meta.html#das-modell-mit-zufälligen-effekten",
    "href": "stat-modeling-meta.html#das-modell-mit-zufälligen-effekten",
    "title": "58  Metaanalysen",
    "section": "58.4 Das Modell mit zufälligen Effekten",
    "text": "58.4 Das Modell mit zufälligen Effekten\nDas Modell der zufälligen Effekte (eng. random effect) geht davon aus, dass es nicht nur eine wahre Effektgröße gibt, sondern eine Verteilung der wahren Effektgrößen. Das Ziel des Modells mit zufälligen Effekten ist es daher nicht, die eine wahre Effektgröße aller Studien zu schätzen, sondern den Mittelwert der Verteilung der wahren Effekte.\nIn der Praxis ist es sehr ungewöhnlich, eine Auswahl von Studien zu finden, die vollkommen homogen ist. Dies gilt selbst dann, wenn wir uns an bewährte Verfahren halten und versuchen, den Umfang unserer Analyse so präzise wie möglich zu gestalten.\nIn vielen Bereichen, einschließlich der Medizin und der Sozialwissenschaften, ist es daher üblich, immer ein Modell mit zufälligen Effekten zu verwenden, da ein gewisses Maß an Heterogenität zwischen den Studien praktisch immer zu erwarten ist. Ein Modell mit festen Effekten kann nur dann verwendet werden, wenn keine Heterogenität zwischen den Studien festgestellt werden konnte und wenn wir sehr gute Gründe für die Annahme haben, dass der wahre Effekt fest ist. Dies kann zum Beispiel der Fall sein, wenn nur exakte Replikationen einer Studie betrachtet werden oder wenn wir Teilmengen einer großen Studie meta-analysieren. Natürlich ist dies nur selten der Fall, und Anwendungen des Modells mit festem Effekt “in freier Wildbahn” sind eher selten.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Metaanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-meta.html#indirekte-vergleiche-in-r",
    "href": "stat-modeling-meta.html#indirekte-vergleiche-in-r",
    "title": "58  Metaanalysen",
    "section": "58.5 Indirekte Vergleiche in R",
    "text": "58.5 Indirekte Vergleiche in R\n\nm.mean &lt;- metamean(n = n,\n                   mean = mean,\n                   sd = sd,\n                   studlab = author,\n                   data = drymatter_tbl,\n                   sm = \"MRAW\",\n                   fixed = FALSE,\n                   random = TRUE,\n                   method.tau = \"REML\",\n                   hakn = TRUE,\n                   title = \"Ertrag von Weizen nach Eisenbehandlung\")\n\n\nsummary(m.mean)\n\nReview:     Ertrag von Weizen nach Eisenbehandlung\n\n                    mean             95%-CI %W(random)\nDeRubeis, 2005   32.6000 [31.2268; 33.9732]       18.0\nDimidjian, 2006  31.9000 [30.6955; 33.1045]       19.4\nDozois, 2009     28.6000 [25.7993; 31.4007]        9.1\nLesperance, 2007 30.3000 [28.8033; 31.7967]       17.0\nMcBride, 2007    31.9000 [30.8607; 32.9393]       20.7\nQuilty, 2014     29.8000 [28.1472; 31.4528]       15.8\n\nNumber of studies: k = 6\nNumber of observations: o = 920\n\n                        mean             95%-CI\nRandom effects model 31.1221 [29.6656; 32.5786]\n\nQuantifying heterogeneity:\n tau^2 = 1.0937 [0.0603; 12.9913]; tau = 1.0458 [0.2456; 3.6043]\n I^2 = 64.3% [13.8%; 85.2%]; H = 1.67 [1.08; 2.60]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 14.00    5  0.0156\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 5)\n- Untransformed (raw) means\n\n\n\nm.prop &lt;- metaprop(event = event,\n                   n = n,\n                   studlab = author,\n                   data = sunflower_tbl,\n                   method = \"GLMM\",\n                   sm = \"PLOGIT\",\n                   fixed = FALSE,\n                   random = TRUE,\n                   hakn = TRUE,\n                   title = \"Befall von Sonnenblumen mit Mehltau\")\nsummary(m.prop)\n\nReview:     Befall von Sonnenblumen mit Mehltau\n\n               proportion           95%-CI\nBecker, 2008       0.1002 [0.0962; 0.1042]\nBoyd, 2009         0.0998 [0.0811; 0.1211]\nBoyd, 2007         0.1162 [0.0978; 0.1368]\nCerda, 2014        0.0710 [0.0654; 0.0770]\nFiellin, 2013      0.1176 [0.1150; 0.1204]\nJones, 2013        0.0945 [0.0928; 0.0962]\nLord, 2011         0.1632 [0.1327; 0.1976]\nMcCabe, 2005       0.0710 [0.0659; 0.0764]\nMcCabe, 2012       0.0748 [0.0700; 0.0798]\nMcCabe, 2013       0.0728 [0.0675; 0.0784]\nNakawai, 2012      0.0909 [0.0893; 0.0925]\nSung, 2005         0.0962 [0.0908; 0.1017]\nTetrault, 2007     0.1259 [0.1209; 0.1311]\nWu, 2008           0.0873 [0.0838; 0.0908]\nZullig, 2012       0.0840 [0.0804; 0.0876]\n\nNumber of studies: k = 15\nNumber of observations: o = 434385\nNumber of events: e = 41364\n\n                     proportion           95%-CI\nRandom effects model     0.0944 [0.0836; 0.1066]\n\nQuantifying heterogeneity:\n tau^2 = 0.0558; tau = 0.2362; I^2 = 98.3% [97.9%; 98.7%]; H = 7.74 [6.92; 8.66]\n\nTest of heterogeneity:\n           Q d.f.  p-value\n Wald 838.21   14 &lt; 0.0001\n LRT  826.87   14 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Random intercept logistic regression model\n- Maximum-likelihood estimator for tau^2\n- Random effects confidence interval based on t-distribution (df = 14)\n- Logit transformation\n- Clopper-Pearson confidence interval for individual studies\n\n\n\nm.gen &lt;- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = cow_tbl,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\nsummary(m.gen)\n\nReview:     Third Wave Psychotherapies\n\n                          SMD            95%-CI %W(random)\nCall et al.            0.7091 [ 0.1979; 1.2203]        5.0\nCavanagh et al.        0.3549 [-0.0300; 0.7397]        6.3\nDanitzOrsillo          1.7912 [ 1.1139; 2.4685]        3.8\nde Vibe et al.         0.1825 [-0.0484; 0.4133]        7.9\nFrazier et al.         0.4219 [ 0.1380; 0.7057]        7.3\nFrogeli et al.         0.6300 [ 0.2458; 1.0142]        6.3\nGallego et al.         0.7249 [ 0.2846; 1.1652]        5.7\nHazlett-Stevens & Oren 0.5287 [ 0.1162; 0.9412]        6.0\nHintz et al.           0.2840 [-0.0453; 0.6133]        6.9\nKang et al.            1.2751 [ 0.6142; 1.9360]        3.9\nKuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.3\nLever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6\nPhang et al.           0.5407 [ 0.0619; 1.0196]        5.3\nRasanen et al.         0.4262 [-0.0794; 0.9317]        5.1\nRatanasiripong         0.5154 [-0.1731; 1.2039]        3.7\nShapiro et al.         1.4797 [ 0.8618; 2.0977]        4.2\nSong & Lindquist       0.6126 [ 0.1683; 1.0569]        5.7\nWarnecke et al.        0.6000 [ 0.1120; 1.0880]        5.2\n\nNumber of studies: k = 18\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [0.3782; 0.7760] 6.12 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp adjustment for random effects model (df = 17)",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Metaanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-meta.html#forest-plots",
    "href": "stat-modeling-meta.html#forest-plots",
    "title": "58  Metaanalysen",
    "section": "58.6 Forest Plots",
    "text": "58.6 Forest Plots\n\n\nForest Plots\n\nforest(m.mean, \n            sortvar = TE,\n            prediction = TRUE, \n            print.tau2 = FALSE)\n\n\n\n\n\n\n\nAbbildung 58.1— foo.\n\n\n\n\n\n\nforest(m.prop, \n            sortvar = TE,\n            prediction = TRUE, \n            print.tau2 = FALSE,\n            leftlabs = c(\"Author\", \"event\", \"n\"))\n\n\n\n\n\n\n\nAbbildung 58.2— foo.\n\n\n\n\n\n\nforest(m.gen, \n            sortvar = TE,\n            prediction = TRUE, \n            print.tau2 = FALSE,\n            leftlabs = c(\"Author\", \"g\", \"SE\"))\n\n\n\n\n\n\n\nAbbildung 58.3— foo.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Metaanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-meta.html#publication-bias",
    "href": "stat-modeling-meta.html#publication-bias",
    "title": "58  Metaanalysen",
    "section": "58.7 Publication Bias",
    "text": "58.7 Publication Bias\n\n\nPublication Bias\n\nfunnel(m.prop,\n            xlim = c(-3, -1.5),\n            studlab = TRUE)\n\nfunnel(m.mean,\n            xlim = c(27, 35),\n            studlab = TRUE)\n\nfunnel(m.gen,\n            xlim = c(-0.5, 2),\n            studlab = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n(a) Verteilung der beobachteten Werte.\n\n\n\n\n\n\n\n\n\n\n\n(b) Verteilung der theoretischen Werte.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) TEst\n\n\n\n\n\n\n\nAbbildung 58.4— dst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Metaanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-meta.html#referenzen",
    "href": "stat-modeling-meta.html#referenzen",
    "title": "58  Metaanalysen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 58.1— foo.\nAbbildung 58.2— foo.\nAbbildung 58.3— foo.\nAbbildung 58.4 (a)— Verteilung der beobachteten Werte.\nAbbildung 58.4 (b)— Verteilung der theoretischen Werte.\nAbbildung 58.4 (c)— TEst\n\n\n\nBalduzzi S, Rücker G, Schwarzer G. 2019. How to perform a meta-analysis with R: a practical tutorial. BMJ Ment Health 22: 153–160.\n\n\nHarrer M, Cuijpers P, A FT, Ebert DD. 2021. Doing Meta-Analysis With R: A Hands-On Guide. 1st Aufl. Chapman & Hall/CRC Press.\n\n\nTawfik GM, Dila KAS, Mohamed MYF, Tam DNH, Kien ND, Ahmed AM, Huy NT. 2019. A step by step guide for conducting a systematic review and meta-analysis with simulation data. Tropical medicine and health 47: 1–9.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Metaanalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-cluster.html",
    "href": "stat-modeling-cluster.html",
    "title": "59  Clusteranalysen",
    "section": "",
    "text": "59.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\nset.seed(20230727)\npacman::p_load(tidyverse, magrittr, palmerpenguins, readxl,\n               ggdendro, broom, cluster, factoextra, FactoMineR, \n               pheatmap, tidyclust, dlookr, janitor, corrplot,\n               dendextend, see, conflicted)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(dlookr::transform)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Clusteranalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-cluster.html#daten",
    "href": "stat-modeling-cluster.html#daten",
    "title": "59  Clusteranalysen",
    "section": "59.2 Daten",
    "text": "59.2 Daten\nBeginnen wir mit einem normierten Datensatz aus dem R Paket {cluster}. Der Datensatz animals wurde von mir noch mit ein paar Tieren ergänzt und schaut sich sechs Eigenschaften von 23 Tieren an. Wir wollen im Folgenden nun herausfinden, ob wir anhand der Eigenschaften in den Spalten die Tiere in den Zeilen in Gruppen einordnen können. Einige der Tiere sind ja näher miteinander verwandt als andere Tiere. Die ursprünglichen Daten liefen noch auf einem \\(1/2\\)-System, das ändern wir dann zu \\(0/1\\) damit wir dann auch besser mit den Daten arbeiten können. Für die Algorithmen ist es egal, aber ich habe lieber \\(1\\) gleich ja und \\(0\\) gleich nein.\n\nanimals_tbl &lt;- read_excel(\"data/cluster_animal.xlsx\", sheet = 1) |&gt; \n  clean_names() |&gt; \n  mutate(across(where(is.numeric), \\(x) x - 1))\n\nSchauen wir uns einmal den Datensatz in der Tabelle 60.3 an. Wir sehen, dass wir noch einige fehlende Werte in den Daten vorliegen haben. Das ist manchmal ein Problem, deshalb werden wir im Laufe der Analyse die NA Werte mit na.omit() entfernen.\n\n\n\n\nTabelle 59.1— Übersicht über die 23 Tiere mit den sechs Eigenschaften in den Spalten. Eine 1 bedeutet, dass die Eigenschaft vorliegt; eine 0 das die Eigenschaft nicht vorliegt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nanimal\nwarm_blooded\nfly\nvertebrate\nthreatened\nlive_in_groups\nhair\n\n\n\n\nant\n0\n0\n0\n0\n1\n0\n\n\nbee\n0\n1\n0\n0\n1\n1\n\n\ncat\n1\n0\n1\n0\n0\n1\n\n\ncentipede\n0\n0\n0\n0\n0\n1\n\n\nchimpanzee\n1\n0\n1\n1\n1\n1\n\n\ncow\n1\n0\n1\n0\n1\n1\n\n\ndolphin\n1\n0\n1\n1\n1\n0\n\n\nduck\n1\n1\n1\n0\n1\n0\n\n\neagle\n1\n1\n1\n1\n0\n0\n\n\nearthworm\n0\n0\n0\n0\n0\n0\n\n\nelephant\n1\n0\n1\n1\n1\n0\n\n\nfly\n0\n1\n0\n0\n0\n0\n\n\nfrog\n0\n0\n1\n1\nNA\n0\n\n\nherring\n0\n0\n1\n0\n1\n0\n\n\nhorse\n1\n0\n1\n0\n1\n1\n\n\nhuman\n1\n0\n1\n1\n1\n1\n\n\nlion\n1\n0\n1\nNA\n1\n1\n\n\nlizard\n0\n0\n1\n0\n0\n0\n\n\nlobster\n0\n0\n0\n0\nNA\n0\n\n\nrabbit\n1\n0\n1\n0\n1\n1\n\n\nsalmon\n0\n0\n1\n0\nNA\n0\n\n\nspider\n0\n0\n0\nNA\n0\n1\n\n\nwhale\n1\n0\n1\n1\n1\n0\n\n\n\n\n\n\n\n\nDer Tierdatensatz ist schön, da wir es hier nur mit 0/1 Werten zu tun haben. Wir werden später in dem preprocessing der Daten sehen, dass wir alle Spalten in der gleichen Spannweite der Werte wollen. Das klingt immer etwas kryptisch, aber der nächste Datensatz über verschiedene Kreaturen macht es deutlicher.\n\n\nEine andere Art die Daten zu Gruppieren kannst du im Tutorium Clustering Creatures nochmal nachvollziehen.\nIm Folgenen einmal der Datensatz, den wir dann in der gleichen Exceldatei finden nur eben auf dem zweiten Tabellenblatt. Wir reinigen noch die Namen und setzen die creature-Spalte auf Klein geschrieben. Wie du siehst, haben wir dann nur 15 Kreaturen und drei Spalten mit dem Gewicht, der Herzrate und dem maximalen möglichen Alter.\n\ncreature_tbl &lt;- read_excel(\"data/cluster_animal.xlsx\", sheet = 2) |&gt; \n  clean_names() |&gt; \n  mutate(creature = tolower(creature))\n\nIn der Tabelle 60.4 sehen wir nochmal die Daten dargestellt und hier erkennst du auch gut, wo das Problem liegt. Die Masse der Tiere reicht von \\(6g\\) beim Hamster bis \\(120000000g\\) beim Wal. Diese Spannweiten in einer Spalte und zwischen den Spalten führt dann zu Problemen bei den Algorithmen. Deshalb müssen wir hier Daten nochmal normalisieren oder aber standardisieren. Je nachdem was da besser passt.\n\n\n\n\nTabelle 59.2— Übersicht über die 15 Kreaturen mit den drei Eigenschaften in den Spalten. Wir haben hier sehr große Unterschiede in den Datenwerten. Daher müssen wir vor dem Clustern nochmal normalisieren.\n\n\n\n\n\n\ncreature\nmass_grams\nheart_rate_bpm\nlongevity_years\n\n\n\n\nhuman\n9.0e+04\n60\n70\n\n\ncat\n2.0e+03\n150\n15\n\n\nsmall dog\n2.0e+03\n100\n10\n\n\nmedium dog\n5.0e+03\n90\n15\n\n\nlarge dog\n8.0e+03\n75\n17\n\n\nhamster\n6.0e+01\n450\n3\n\n\nchicken\n1.5e+03\n275\n15\n\n\nmonkey\n5.0e+03\n190\n15\n\n\nhorse\n1.2e+06\n44\n40\n\n\ncow\n8.0e+05\n65\n22\n\n\npig\n1.5e+05\n70\n25\n\n\nrabbit\n1.0e+03\n205\n9\n\n\nelephant\n5.0e+06\n30\n70\n\n\ngiraffe\n9.0e+05\n65\n20\n\n\nlarge whale\n1.2e+08\n20\n80\n\n\n\n\n\n\n\n\nIm Weiteren betrachten wir noch das Beispiel der Gummibärchendaten. Auch hier haben wir echte Daten vorliegen, so dass wir eventuell Ausreißer entdecken könnten. Da wir hier fehlende Werte in den Daten haben, entfernen wir alle fehlenden Werte mit der Funktion na.omit(). Damit löschen wir jede Zeile in den Daten, wo mindestens ein fehlender Wert auftritt. Da wir hier mittlerweile sehr viele Daten vorliegen haben, wollen wir das Problem auf die beiden Quellen FU Berlin und dem Girls and Boys Day eingrenzen.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  |&gt;\n  filter(module %in% c(\"FU Berlin\", \"Girls and Boys Day\")) |&gt; \n  select(gender, age, height, semester, most_liked) |&gt; \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) |&gt; \n  na.omit()\n\nAuch hier schauen wir uns in der Tabelle die ersten sieben Beobachtungen von den 192 Beobachtungen an. Wir sehen, dass wir hier mal ganz unterschiedliche Typen an Daten haben. Zum einen sehen wir dichotome Daten, wie das Geschlecht, sowie numerisch wie Alter und Größe, dann noch das Semester mit einer eher geordneten Struktur und nochmal ein Faktor mit sechs Stufen. Das schauen wir uns dann nochmal am Ende an, was wir dann machen können.\n\n\n\n\nTabelle 59.3— Auszuga us den Gummibärchendaten für die ersten sieben Beobachtungen.\n\n\n\n\n\n\ngender\nage\nheight\nsemester\nmost_liked\n\n\n\n\nm\n35\n193\n10\nlightred\n\n\nw\n21\n159\n6\nyellow\n\n\nw\n21\n159\n6\nwhite\n\n\nw\n36\n180\n10\nwhite\n\n\nm\n22\n180\n3\nwhite\n\n\nm\n22\n180\n3\ngreen\n\n\nw\n21\n163\n3\ngreen\n\n\n\n\n\n\n\n\nBevor wir jetzt aber die Daten clustern können, müssen wir die Daten vorher nochmal aufbereiten, damit die Algorithmen mit den Daten arbeiten können. Dann müssen wir am Ende auch noch vom tibble() in den data.frame() wechseln, da wir die Zeilennamen häufig brauchen. Wenn du beim tibble() bleiben willst, dann gibt es am Ende noch eine mögliche Lösung für dich.\n\n\n\n\n\n\nWeitere Datensätze fürs Clustern\n\n\n\n\n\nAndere mögliche Datensätze für die Zukunft: chorSub, flower, plantTraits, pluton, ruspini und agriculture. Die Datensätze sind teilweise im R Paket {cluster} enthalten.\nIm Weiteren noch die Palmer Penguins mit dem Datensatz penguins aus dem R Paket {palmerpenguins}.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Clusteranalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-cluster.html#daten-preprocessing",
    "href": "stat-modeling-cluster.html#daten-preprocessing",
    "title": "59  Clusteranalysen",
    "section": "59.3 Daten preprocessing",
    "text": "59.3 Daten preprocessing\nWir können die Daten so wir wie sie vorliegen haben in einem Clusteralgorithmus verwenden. Wir führen also keine Transformation der Daten durch, wir nutzen die Daten untransformiert. Dieses untransfomierte Verwenden der Daten führt aber meist dazu, dass Variablen nicht im gleichen Maße berücksichtigt werden. Es macht eben einen Unterschied, ob wir wie bei dem Alter sehr viele verschiedene Werte haben als beim Geschlecht. Es macht auch einen Unterschied, ob das Alter numerische Werte von 20 bis 60 haben kann und das Semester nur numerische Werte von 1 bis 10.\nIn dem Kapitel 18 findest du die hier verwendeten Standardfunktionen in R aus dem Paket {dlookr} für die Normalisierung sowie Standardisierung mit der Funktion transform(). Wenn es komplexer wird, dann empfehle ich den Workflow, wie er im Kapitel 67 für die Klassifikation von Daten vorgestellt wird. Am Ende des Kapitels zeige ich dir auf den Gummibärechendaten, wie dann alles einmal zusammenkommt.\n\n59.3.1 Normalisieren\nWenn wir Normalisieren dann Zwängen wir Variablen in ein Intervall von \\([0;1]\\). Es gehen natürlich auch andere Intervalle, aber das Intervall von 0 bis 1 ist wohl das häufigste Intervall was genutzt wird. Dazu nutzen wir die Funktion transform() aus dem R Paket {dlookr} und mit der Option minmax kriegen wir dann alle Werte einer Spalte zwischen 0 und 1. Dann machen wir das mal schnell mit den Daten aus dem Kreaturendatensatz.\n\nnorm_creature_tbl &lt;- creature_tbl |&gt; \n  mutate(mass_grams = transform(mass_grams, \"minmax\"),\n         heart_rate_bpm = transform(heart_rate_bpm, \"minmax\"),\n         longevity_years = transform(longevity_years, \"minmax\")) \nnorm_creature_tbl\n\n# A tibble: 15 × 4\n   creature    mass_grams   heart_rate_bpm longevity_years\n   &lt;chr&gt;       &lt;transfrm&gt;   &lt;transfrm&gt;     &lt;transfrm&gt;     \n 1 human       7.495004e-04 0.09302326     0.87012987     \n 2 cat         1.616667e-05 0.30232558     0.15584416     \n 3 small dog   1.616667e-05 0.18604651     0.09090909     \n 4 medium dog  4.116669e-05 0.16279070     0.15584416     \n 5 large dog   6.616670e-05 0.12790698     0.18181818     \n 6 hamster     0.000000e+00 1.00000000     0.00000000     \n 7 chicken     1.200001e-05 0.59302326     0.15584416     \n 8 monkey      4.116669e-05 0.39534884     0.15584416     \n 9 horse       9.999505e-03 0.05581395     0.48051948     \n10 cow         6.666170e-03 0.10465116     0.24675325     \n11 pig         1.249501e-03 0.11627907     0.28571429     \n12 rabbit      7.833337e-06 0.43023256     0.07792208     \n13 elephant    4.166619e-02 0.02325581     0.87012987     \n14 giraffe     7.499504e-03 0.10465116     0.22077922     \n15 large whale 1.000000e+00 0.00000000     1.00000000     \n\n\nWir sehen hier, dass in der Spalte mass_grams der Wal den maximalen Wert 1 kriegt und der Hamster den Wert 0. Alle anderen Kreaturen spannen sich numerisch zwischen diesen beiden Extremen auf. Das Gleiche erkenne wir dann auch bei der Herzrate und den Lebenspanne. Hier sind dann auch immer der Hamster und der Wal die extremsten numerischen Vertreter in den Spalten.\nHier helfen natürlich auch die Funktionen von dem R Paket {dplyr} und der Hilfsseite von across() um mehrere Spalten schneller in mutate zu transformieren. Aber wir üben hier nur begrenzt Programmierung und es ist dann an dir dieses Problem zu lösen.\n\n\n59.3.2 Standardisieren\nDie Standardisierung zwingt Variablen in eine \\(\\mathcal{N(0,1)}\\) Standardnormalverteilung. Das heißt, wir transformieren alle Variablen auf einen Mittelwert von \\(0\\) und einer Standardabweichung von \\(1\\). Das macht dann auch die Daten sehr schon gleichförmig. Wir wollen also für unseren Gummibärchendatensatz, dass das Alter einen Mittelwert von 0 und eien Standardabweichung von 1 kriegt. Hier nutzen wir auch die Funktion transform() aus dem R Paket {dlookr} mit der Option zscore. Es macht keinen Sinn Faktoren zu standardisieren, wir standardisieren nur numerische Spalte und das Semester zu transformieren ist schon so eine grenzwertige Sache.\n\nscale_gummi_tbl &lt;- gummi_tbl |&gt; \n  mutate(gender = as_factor(gender),\n         age = transform(age, \"zscore\"),\n         height = transform(height, \"zscore\"),\n         semester = transform(semester, \"zscore\"),\n         most_liked = as_factor(most_liked))\nscale_gummi_tbl\n\n# A tibble: 210 × 5\n   gender age        height     semester    most_liked\n   &lt;fct&gt;  &lt;transfrm&gt; &lt;transfrm&gt; &lt;transfrm&gt;  &lt;fct&gt;     \n 1 m       1.3171177  2.2399120  2.65128580 lightred  \n 2 w      -0.4374478 -1.3833258  1.09714758 yellow    \n 3 w      -0.4374478 -1.3833258  1.09714758 white     \n 4 w       1.4424438  0.8545564  2.65128580 white     \n 5 m      -0.3121217  0.8545564 -0.06845609 white     \n 6 m      -0.3121217  0.8545564 -0.06845609 green     \n 7 w      -0.4374478 -0.9570626 -0.06845609 green     \n 8 m      -0.3121217 -0.2111018 -0.06845609 green     \n 9 m      -0.1867956  0.4282931 -0.06845609 white     \n10 m      -0.5627739  0.8545564 -0.06845609 lightred  \n# ℹ 200 more rows\n\n\nWie beim Normalisieren helfen hier natürlich auch die Funktionen von dem R Paket {dplyr} und der Hilfsseite von across() um mehrere Spalten schneller in mutate zu transformieren. Wie oben schon angemerkt, dass ist dann deine Fingerübung.\n\n\n59.3.3 Das data.frame() Problem\nLeider ist es so, dass fast alle Pakete im Kontext der Clusteranalyse mit den Zeilennamen bzw. row.names() eines data.frame() arbeiten. Das hat den Grund, dass wir gut das Label in den Zeilennamen parken können, ohne das uns eine Spalte in den Auswertungen stört. Meistens ist das Label ja ein character und soll gar nicht in den Clusteralgorithmus mit rein. Deshalb müssen wir hier einmal unsere tibble() in einen data.frame() umwandeln. Die tibble() haben aus gutem Grund keine Zeilennamen, die Zeilennamen sind ein Ärgernis und Quelle von Fehlern und aus gutem Grund nicht in einem tibble() drin. Hier brauchen wir die Zeilennamen aber.\nWir bauen uns also einmal einen data.frame() für unseren Tierdatensatz und setzen die Tiernamen als Zeilennamen bzw. row.names(). Wir entfernen dann auch noch schnell alle fehlenden Werte, denn wir wollen usn hier nicht noch mit der Imputation von fehlenden Werten beschäftigen.\n\nanimals_df &lt;- animals_tbl |&gt; \n  na.omit() |&gt; \n  as.data.frame() |&gt; \n  column_to_rownames(\"animal\") \n\nDas Ganze machen wir dann auch noch einmal für die normalisierten Kreaturendaten. Wir wollen dann ja nur auf den normalisierten Daten weitermachen.\n\nnorm_creature_df &lt;- norm_creature_tbl |&gt; \n  as.data.frame() |&gt; \n  column_to_rownames(\"creature\") \n\nWie eben gesagt, ist es teilweise echt nervig immer die row.names() mit zu nehmen und alles ein data.frame() zu nutzen. Insbesondere wenn die Daten sehr groß werden, kann kann es sehr ungünstig sein, alles in einem data.frame() zu lagern. Deshalb gibt es das Paket {tidyclust}, welches ich am Ende nochmal vorstelle.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Clusteranalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-cluster.html#distanzmaße",
    "href": "stat-modeling-cluster.html#distanzmaße",
    "title": "59  Clusteranalysen",
    "section": "59.4 Distanzmaße",
    "text": "59.4 Distanzmaße\nWie nah oder weit entfernt sind jetzt zwei Beobachtungen? Wenn wir die Distanz von zwei Beobachtungen zueinander haben, wie weit ist dann eine dritte Beobachtung entfernt? Aus den paarweisen Abständen aller Beobachtungen zueinander können wir dann Cluster bilden. Wir brauchen aber zuallerst die Distanzen der Beobachtungen zuaeinander. Wir betrachten dabei im Folgenden immer die Distanzen zwischen den Zeilen des Datensatzes. Das heißt, wir wollen immer die Distanzen zwischen den Beobachtungen berechnen. Wie nah oder fern sind sich zwei Beobachtungen gegeben den Spalten? Wir schauen uns einmal zwei sehr intuitive Distanzmaße mit der euklidischen sowie der manhattan Distanz an.\n\nEuklidische Distanz\n\nDie euklidische Distanz \\(d_E\\) ist einfach die Wurzel des quadratischen Abstands zwischen zwei Punkten \\(p\\) und \\(q\\). \\[\nd_E(p,q) = \\sqrt{(p-q)^2}\n\\]\n\nManhattan Distanz\n\nDie manhattan Distanz \\(d_M\\) ist einfach der absolute Abstands zwischen zwei Punkten \\(p\\) und \\(q\\).\\[\nd_M(p,q) = \\lvert p-q \\rvert\n\\]\n\n\nEs gibt noch viel mehr Distanzen, die du berechnen kannst. Je nach R Paket sind unterschiedlich Distanzen dann auszuwählen. Hier eine generelle Empfehlung zu geben ist mir unmöglich. Da müssen wir dann zusammen mal schauen, was für deine Daten dann konkret passt.\n\nDie Funktion dist() als die Standardfunktion: Die Funktion akzeptiert nur numerische Daten als Eingabe und das zu verwendende Abstandsmaß muss eines der Folgenden sein: euclidean, maximum, manhattan, canberra, binary oder minkowski. Die Hilfeseite ?dist() liefert mehr Informationen über die Distanzmaße.\nDie Funktion get_dist() aus dem R Paket {factoextra}: Die Funktion akzeptiert nur numerische Daten als Eingabe. Im Vergleich zur Standardfunktion dist() unterstützt sie korrelationsbasierte Abstandsmaße einschließlich der Methoden pearson, kendall und spearman.\nDie Funktion daisy() aus dem R Paket {cluster}: Die Funktion kann mit anderen Variablentypen umgehen als numerisch, also auch mit Kategorien und Faktoren. In diesem Fall wird automatisch der Gower-Koeffizient als Metrik verwendet. Der Gower-Koeffizient ist eines der beliebtesten Näherungsmaße für gemischte Datentypen. Weitere Einzelheiten findest du auf der Hilfeseite der Funktion ?daisy.\nDie Funktion PCA() aus dem R Paket {FactoMineR}: Die Funktion erlaubt es dir eine Hauptkomponentenanalyse zu rechnen. Das ist aber dann ein eigens Thema für sich. In dem Kapitel 60 erkläre ich nochmal die Hauptkomponentenanalyse und zeige dort auch, wie du mit den Ergebnissen einer Hauptkomponentenanalyse clustern kannst.\n\nWenn du deine Daten transformierst, dann werden auch die Abstandmaße ähnlicher. Zum Beispiel werden durch die Standardisierung die folgenden Abstandsmaße kelinere Werte ergeben: Euklidisch, Manhattan und die Korrelation. Das ist aber nicht so schlimm, wenn du nicht untransformierte Daten mit transformierten Daten vergleichst.\nDie Funktion fviz_dist() aus dem R Paket {factorextra} ermöglicht dir die Distanzmatrix zu visualisieren. In der Abbildung 59.1 kannst du dir die Distanzmatrix der Funktion dist() als Heatmap anzuschauen. Wir nutzen die Distanzmatrix dann aber gleich um auf den Distanzen Cluster zu bilden.\n\nanimals_df  |&gt; \n  dist(method = \"euclidean\") |&gt; \n  fviz_dist()\n\n\n\n\n\n\n\nAbbildung 59.1— Heatmap der euklidischen Distanzen des Tierdatensatzes. Die Matrix ist symmetrisch. Hohe, blaue Werte bedeuten eine große Distanz dagegen kleine,rote Werte eine geringe Distanz zwischen den Beobachtungen.\n\n\n\n\n\nWenn du also die Distanzen zwischen deinen Beobachtungen berechnet hast, kannst du mit der Distanzmatrix entweder hierarchisch Clustern oder über den \\(k\\)-NN Algorithmus Gruppen bilden.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Clusteranalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-cluster.html#algorithmen-fürs-clustern",
    "href": "stat-modeling-cluster.html#algorithmen-fürs-clustern",
    "title": "59  Clusteranalysen",
    "section": "59.5 Algorithmen fürs Clustern",
    "text": "59.5 Algorithmen fürs Clustern\nJetzt schauen wir uns die zwei wichtigsten Algorithmen für das Clustern von Daten einmal an. Zum einen ist es die Hierarchische Clusteranalyse, die einen Baum wachsen lässt und wir so eine Entscheidung über die Gruppenzugehörigkeit bekommen. Der andere Algorithmus ist der \\(k\\) nächste Nachbarn (abk. k-NN) Methode, die sich immer die nächsten Beobachtungen anschaut und versucht ähnliche Beobachtungen zusammen zu führen.\n\n\n\n\n\n\nEin Clusteralgorithmus braucht zwingend eine Distanz(matrix)!\n\n\n\nDu kannst nur auf Distanzen clustern. Das heißt, dass du deine Daten irgendwie in Distanzen zwischen den Beobachtungen umwandeln musst. Im vorherigen Abschnitt hast du ja schon die wichtigsten Distanzalgorithmen kennen gelernt.\n\n\nWie immer gibt es verschiedene Methoden und Optionen für die beiden Algorithmen. Am Ende musst du schauen, ob die Ergebnisse des Clustern einen Sinn innerhalb deiner Fragestellung ergeben. Es kann also sein, dass du recht viele Optionen und Varianten durchprobieren musst, bevor du dann was gefunden hast was dich zufrieden stellt.\n\n59.5.1 Hierarchische Clusteranalyse\nGleich vorweg mal das Ergebnis einer hierarchischen Clusteranalyse der Tierdaten dargestellt als Dendrogramm in der Abbildung 59.2. Wir sehen, dass alle Beobachtungen anhand ihrer Distanz in einen Baum angeordnet werden. Das Tier ganz links ist maximal vom Tier ganz rechts entfernt. Damit ist also der Hering maximal weit vom Elefanten nach den Eigenschaften - also Spalten - in den Tierdaten entfernt.\n\n\n\n\n\n\n\n\nAbbildung 59.2— Beispiel für das Ergebnis einer hierarchischen Clusteranalyse der Tierdaten dargestellt als Dendrogramm\n\n\n\n\n\nWie bauen wir uns so ein Dendrogramm? Wir nehmen als erstes eine Distanzmatrix und nutzen dann die Distanzmatrix um nach verschiedenen Regeln die hierarchische Clusteranalyse durchzuführen. Es gibt vier gängige Ansätze für die Cluster-Cluster-Distanzierung, auch Linkage genannt:\n\nsingle linkage: Der Abstand zwischen zwei Clustern ist der Abstand zwischen den beiden nächstgelegenen Beobachtungen. In R nutzen wir dann single als Option.\naverage linkage: Der Abstand zwischen zwei Clustern ist der Durchschnitt aller Abstände zwischen den Beobachtungen in einem Cluster und den Beobachtungen im anderen Cluster. In R ist es dann die Option average.\ncomplete linkage: Der Abstand zwischen zwei Clustern ist der Abstand zwischen den beiden am weitesten entfernten Beobachtungen. Auch hier passt es dann mit dem Optionnamen von complete.\ncentroid method: Der Abstand zwischen zwei Clustern ist der Abstand zwischen ihren geometrischen Mittel oder Medianen. Hier nutzen wir die Option median.\nward method: Der Abstand zwischen zwei Clustern ist proportional zur Zunahme der Fehlerquadratsumme, die sich aus der Verbindung der beiden Cluster ergeben würde. Die Fehlerquadratsumme wird als Summe der quadrierten Abstände zwischen den Beobachtungen in einem Cluster und dem Schwerpunkt des Clusters berechnet. In R haben wir hier die Wahl zwischen ward.D und ward.D2. Wenn, dann nutze bitte ward.D2 da es sich um die neuere Implementierung handelt, die weniger fehleranfällig ist.\n\nWir nehmen also die Daten und berechnen erst die Distanzmatrix und dann die hierarchische Clusteranalyse. Da wir zum einen aus verschiedenen Distanzalgorithmen plus verschiedenen Algorithmen für das hierarchische Clustern wählen können, musst du hier immer mal wieder rumprobieren, bis du das richtige für dich gefunden hast. Wir schauen uns später noch Kriterien an, aber am Ende musst du entscheiden, ob es inhaltlich mit den gefundenen Gruppen passt.\nDie hierarchische Clusteranalyse führen wir in R mit dem Paket {hclust} durch. Die Distanz berechnen wir hier mit der Funktion dist(). Dann haben wir auch schon unser Clustern abgeschlossen. Der Baum ist gewachsen. Die Funktion hclust() ist nur eine mögliche Funktion für das hierarchische Clustern. Das R Paket {cluster} kennt noch eine mehr auch wenn ohne gute Hilfeseite. Du musst die Paketbeschreibung eine gute Übersicht an möglichen Algorithmen nehmen und unter den plot.* Funktionen schauen. Jeder Algorithmus hat eine eigene Funktion für einen Plot und so findest du schnell die Algorithmen. Hier jetzt aber weiter mit der Standardfunktion hclust().\n\nh_clust_animal &lt;- animals_df  |&gt; \n  dist(method = \"euclidean\") |&gt; \n  hclust(method = \"ward.D\")\n\nJetzt ist aber die Frage, wie kriegen wir die Gruppen? Bis jetzt ist ja jedes einzelne Tier für sich. Wir müssen also von den Enden des Baumes wieder noch oben gehen um unsere Tiere Gruppen zuzuordnen. Das macht die Funktion cutree(), die uns dann \\(k\\) Gruppen bildet. Das können wir dann einfach einmal machen und uns die sortierten Gruppen wiedergeben lassen.\n\ngrp_animal &lt;- cutree(h_clust_animal, k = 3)\ngrp_animal |&gt; sort()\n\n       ant        bee  centipede  earthworm        fly    herring     lizard \n         1          1          1          1          1          1          1 \n       cat        cow      horse     rabbit chimpanzee    dolphin       duck \n         2          2          2          2          3          3          3 \n     eagle   elephant      human      whale \n         3          3          3          3 \n\n\nWie wir sehen haben wir drei Gruppen, die Zahl sagt die Gruppe aus und die Namen dann das Tier. Du kannst dir auch andere Gruppenzuordnungen ansehen und schauen, welche inhaltlich besser passen würde. Hier haben wir das Problem, das der Chimpanse zusammen mit dem Delphin und dem Adler eingeordnet wird. Irgendwie nicht so die richtige Aufteilung. Da würde eine oder zwei Gruppen mehr viellicht besser passen.\nWir nehmen jetzt mal die normalisierten Kreaturen und schauen was wir hier rauskriegen. Die Funktionen sind die gleichen wie eben schon bei den Tierdaten.\n\nh_clust_creature &lt;- norm_creature_df |&gt; \n  dist(method = \"euclidean\") |&gt; \n  hclust(method = \"ward.D\")\n\nAuch hir schneiden wir die Bäume dann so zurecht, dass wir am Ende drei Gruppen erhalten. Auch hier kannst du dann immer wieder spielen, und schauen, ob du nicht mit mehr Gruppen ein besseres Ergebnis erhälst.\n\ngrp_creature &lt;- cutree(h_clust_creature, k = 3)\ngrp_creature |&gt; sort()\n\n      human    elephant large whale         cat     hamster     chicken \n          1           1           1           2           2           2 \n     monkey      rabbit   small dog  medium dog   large dog       horse \n          2           2           3           3           3           3 \n        cow         pig     giraffe \n          3           3           3 \n\n\nJa, jetzt haben wir den gleichen Fall, dass der Mensch zusammen mit dem Wal und dem Elefanten geclustert wurde. Irgendwie nicht so toll. Dann wollen wir uns mal im nächsten Schritt anschauen, wie wir ein Ergebnis einer hierarchischen Clusteranalyse visualisieren können.\n\n\n59.5.2 Dendrogramm\nNun können wir uns die Ergebnisse einer hierarchischen Clusteranalyse visualisieren. Dafür nutzen wir dann Dendrogramme. Dendrogramme sind nichts anders als Bäume. In der Abbildung 59.3 siehst du die Ergebnisse der hierarchischen Clusteranalyse als Dendrogramm. Wir nutzen hier die Funktion ggdendrogram(). Wie immer gibt es noch mehr uns noch schönere Dendrogramme, aber das übersteigt diesen Abschnitt. Schau dir das Paket {dendextend} und das Tutroium Introduction to dendextend einml genauer an. Dort findest du dann noch mehr zu Dendrogrammen.\n\n\nAuch gibt es noch ein schönes Tutorium für die Analyse weiterer spannender Datensätzen unter Hierarchical cluster analysis on famous data sets - enhanced with the dendextend package\n\nggdendrogram(h_clust_animal)\nggdendrogram(h_clust_creature)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten norm_creature_df\n\n\n\n\n\n\n\nAbbildung 59.3— Dendrogramme der hierarchischen Clusteranalyse.\n\n\n\n\nHier sehen wir, was wir schon bei der Gruppenerstellung durch die Funktion cutree() gesehen haben. Irgendwie passt die Einteilung nicht so richtig. Wir sehen, dass bei den Tieren der Elefanten zusammen mit dem Wal und dem Delphin eingeordnet wird. Bei den Kreaturen haben wir den Mensch zusammen mit dem Elefanten. Also müssten wir hier nochmal zurück und entweder an den Distanzmaßen schrauben oder aber eine andere linkage in der hierarchischen Clusteranalyse nehmen.\nAuch geht es noch schöner durch die Funktion fviz_dend() in der wir dann auch direkt die Gruppen farblich kennzeichnen können. Das macht die Überprüfung welches Tier wo eingeordnet wird leichter. Hier habe ich dann bei den Kreaturendaten mal vier Gruppen gewählt und dann ist wenigstens der Wal nicht mehr in der Gruppe der Menschen und Elefanten.\n\nfviz_dend(h_clust_animal, cex = 0.5, k = 4, palette = \"jco\") \nfviz_dend(h_clust_creature, cex = 0.5, k = 4, palette = \"jco\") \n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten norm_creature_df\n\n\n\n\n\n\n\nAbbildung 59.4— Dendrogramme der hierarchischen Clusteranalyse eingefärbt nach den unterschiedlichen, zugeordneten Gruppen.\n\n\n\n\nWie du schön siehst, ist eine hierarchische Clusteranalyse nur so gut wie das entsprechende Dendrogramm. Ohne eine visuelle Überpüfung ist es meistens schwer zu sehen ob es dann wirklich passt. Hast du hunderte von Beobachtungen, dann wird es natürlich schwerer, aber dennoch solltest du dir einmal die Dendrogramme ansehen. Aber da gibt es dann auch bei den Tutorien mehr Hilfe und noch andere Einblicke.\n\n\n59.5.3 k-means Clusteranalyse\nKommen wir dann zum zweiten Algorithmus, der darauf basiert sich immer die \\(k\\) nächsten Nachbarn anzuschauen und zu entscheiden zu welcher Gruppe die Beachtungen gehören sollen. Das ist sehr simple das Prinzip des Algorithmus. Wir gehen hier jetzt aber nicht weiter in die Tiefe, der \\(k\\)-NN Algorithmus clustert nicht in der Form eines Baumes sondern schaut sich die direkten Verwandtschaft zwischen von der Distanz nahe liegenden Beobachtungen an. Daher können wir den Algorithmus auch auf alle Distanzmatrizen anwenden. Auch hier verweise ich nochmal auf das R Paket {cluster} welches als eine mögliche Alternative für die Funktion kmeans() die robustere Funktion pam() kennt. Wir nutzen hier einmal die Standardfunktion für das Clustern mit \\(k\\) nächsten Nachbarn, aber du solltest auch die anderen\n\n\nAuch gibt es noch ein schönes Tutorium für die Analyse weiterer spannender Datensätzen unter K-means Cluster Analysis\nDie kmeans() Funktion ist relativ simple. Wir definieren einfach wie viele Cluster oder Gruppen centers gebildet werden sollen. Der Algorithmus versucht jetzt die Beobachtungen um diese Zentren zu gruppieren. Dabei müssen wir vorab sagen wie viele Gruppen wir wollen. Wir sehen gleich nochmal ein Möglichkeit uns da iterativ zu nähern, das klappt aber nicht immer.\n\nknn_animal &lt;- animals_df |&gt; \n  kmeans(centers = 4)\nknn_creature &lt;- norm_creature_df |&gt; \n  kmeans(centers = 4)\n\nWir schauen uns hetzt nicht die Ausgabe an, sondern wollen gleich mal in der Abbildung 59.5 die Ergebnisse des Clusterings. Da haben wir dann schon fast ein ähnlich gutes Bild drüber. Du kannst dir aber mit animals_df$cluster die Cluster angeben lassen. Wir müssen dann noch die Fläche links und rechts auf dem Plot etwas erweitern damit wir dann dort mehr drauf kriegen. Wir nutzen hier die Funktion fviz_cluster() welche dafür geeignet ist gut Cluster darzustellen.\n\nfviz_cluster(knn_animal, data = animals_df, palette = \"jco\") +\n  theme_minimal() +\n  scale_x_continuous(expand = expansion(add = c(0.5, 0.5))) +\n  scale_y_continuous(expand = expansion(add = c(0.5, 0.5))) \n\nfviz_cluster(knn_creature, data = norm_creature_df, palette = \"jco\") +\n  theme_minimal() +\n  scale_x_continuous(expand = expansion(add = c(0.5, 1))) +\n  scale_y_continuous(expand = expansion(add = c(0.5, 0.5))) \n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten norm_creature_df\n\n\n\n\n\n\n\nAbbildung 59.5— Darstellung des kmeans() Clustering. Die Einfärbung stellt die vier vordefinierten Cluster dar.\n\n\n\n\nWie du siehst funktioniert es einigermaßen für die Tierdaten. Ich möchte nochmal den Fokus auf die Symbole in der Mitte der Clusterflächen lenken, dort siehst du dann den Mittelpunkt. Alle Beobachtungen, die nah an dem Wert sind, werden zu diesem Cluster gezählt. Teilweise funktioniert das etwas komplizierter, den der Regenwurm ist auf jeden Fall näher an dem Mittelpunkt des grauen Clusters als dem gelben Clusters. Es gibt also noch sekundäre Regeln, die versuchen einen Cluster möglichst symmetrisch zu erstellen. Bei den Kreaturdaten haben wir auch das Problem, dass wieder unser Wal mit dem Elefanten und dem Menschen zusammengepackt wird. Hier scheinen die Daten wirklich nicht gut zu sein.\nWas ist das optimale \\(k\\) für die Anzahl an Gruppen, die wir in den Algorithmus stecken wollen? Dafür gibt es die Optimierungsfunktion fviz_nbclust(), die uns versucht die optimale Anzahl an Clustergruppen visuell abzuschätzen. Wir sehen die Kurven einmal in der Abbildung 59.6. Für die Tierdaten hat die Methode gap_stat nicht funktioniert, so dass wir da keine Entscheidung raus kriegen sondern selber schauen müssen.\n\nanimals_df |&gt; \n  fviz_nbclust(kmeans, method = \"wss\")\n\nnorm_creature_df |&gt; \n  fviz_nbclust(kmeans, method = \"gap_stat\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten norm_creature_df\n\n\n\n\n\n\n\nAbbildung 59.6— Bestimmung der optimalen Anzahl an Clustern für den k-means Algorithmus und andere. Die Option gap_stat liefert eine Entscheidung, funktioniert aber nicht auf allen Datensätzen.\n\n\n\n\nWie wir sehen gibt es bei den Tierdaten zwar einen größeren Sprung von einem auf zwei Cluster, aber das hilft uns nichts. Wir wollen ja nicht nur einen Cluster wählen, dann können wir uns das Clustern auch sparen. Die anderen Sprünge sind auch eher gleichmäßig, so dass wir hier gar keine optimale Anzahl an Clustern haben. Das heißt, dass wir vermutlich die Tierdaten gar nicht so gut clustern können. Auch liefert der Algorithmus gap_stat die Empfehlung für nur einen Cluster. Das hilft uns nun auch nichts besonders weiter. Hier scheinen die Daten wirklich etwas schwierig zum clustern zu sein.\nWenn du kontienuierliche Spalten in deinen Daten hast, dann kannst du natürlichb auch zwei Spalten auswählen und dir die Aufteilung der Beobachtungen über diese beiden Spalten anschauen. In der Abbildung 59.7 habe ich das mal für zwei Spalten aus dem Kreaturendaten gemacht und dann die Individuen einmal nach den Clustern eingefärbt. Hier siehst du ganz gut, dass der Elefant und Mensch sowie Wal einfach alle drei lange Leben und eine geringe Herzrate haben. Das macht dann diese drei Wesen sehr ähnlich laut unseren Daten. Das heißt natürlich nicht, das der Mensch ein Wal ist. Es braucht einfach mehr Daten und damit Variablen um die Wesen voneinander zu trennen.\n\nggplot(creature_tbl, aes(longevity_years, heart_rate_bpm, label = creature)) + \n  theme_minimal() +\n  geom_label(aes(fill = as_factor(knn_creature$cluster)), colour = \"white\", \n             fontface = \"bold\", size=2) +\n  scale_fill_okabeito(name = \"Cluster\")\n\n\n\n\n\n\n\nAbbildung 59.7— .\n\n\n\n\n\nDas war es dann schon hier mit dem kurzen Ausflug zum Clustern mit dem kmeans Algorithmus. Du findest dann in dem Kapitel 60 zu der Hauptkomponentenanalyse nochmal die Anwendung auf einem anderem Gebiet. Hier ist aber vorerst einmal Schluß mit den Grundlagen.\n\n\n59.5.4 Silhouettenplot\nJetzt haben wir Gruppen gefunden und wollen wissen, ob das jetzt auch gute Gurppen sind oder nicht. Wenn wir nicht so viele Beobachtungen haben, dann können wir uns ja die Daten direkt anschauen. Aber häufig wollen wir auch einen Wert haben, der uns eine Bewertung erleichtert. Um das Ergebnis der Clusteranalyse zu beurteilen, eignet sich ein Silhouettenplot. Ein Silhouettenplot zeigt für jede Beobachtung \\(i\\) die Silhouettenbreite \\(s_i\\), welche definiert ist als normierte Differenz der kleinsten Distanz zu den Beobachtungen außerhalb der eigenen Gruppe und dem Mittelwert der Distanzen innerhalb einer Gruppe. Die Silhouettenbreite \\(s_i\\) kann jeden Wert im Intervall [-1, 1] annehmen und wird von dir folgendermaßen interpretiert.\n\n\\(s_i = 1\\) Die Beobachtung ist dem richtigen Cluster zugeordnet. Oder eher dem Cluster, der gut passt gegeben den Daten und der Anzahl an Clustern.\n\\(s_i = 0\\) Die Beobachtung hätte ebenso gut einer anderen Gruppe zugeordnet werden können. Und wurde also eher zufällig dem Cluster zugeordnet.\n\\(s_i = -1\\) Die Beobachtung ist schlecht zugeordnet. Und sollte damit eher einem anderen Cluster zugeordnet werden, den es vielleicht nicht gibt, da du zu wenige Clustergruppen vorab definiert hast.\n\nDann kannst du dir darüber hinaus die durchschnittliche Silhouettenbreite über alle Beobachtungen berechnen lassen, womit sich die Gruppenbildung als Ganzes beurteilen lässt. Die durchschnittliche Silhouettenbreite wird dann von dir analog interpretiert. Aber Achtung, nur weil du gute Werte der Qualität hier kriegst, heißt es nicht, dass du gute Cluster im Sinne deiner Fragestellung gefunden hast.\n\nDas R Paket {tidyclust} hat auch die Funktion silhouette(), die dir auch ermöglicht die Silhouettenplots erstellen zu lassen.\n\nWir nutzen jetzt einmal die Funktion silhouette() aus dem R Paket {cluster} und berechnen uns aus dem Vektor der Gruppenzugehörigkeit, also den Clustern, und der Distanzmatrix aus der die Cluster bestimmt wurden dann die Silhouettenplots. Dafür müssen wir dann die Ausgabe der Funktion silhouette() noch auf ein tibble() umbauen damit wir die Ausgabe gut in {ggplot} abbilden können.\n\nsilhouette_animals_tbl &lt;- cluster::silhouette(knn_animal$cluster, \n                                              dist(animals_df, \"canberra\")) |&gt; \n  as_tibble() |&gt; \n  mutate(animal = row.names(animals_df),\n         cluster = as_factor(cluster))\n\nUnd das ganze dann nochmal für die Kreaturendaten.\n\nsilhouette_creature_tbl &lt;- cluster::silhouette(knn_creature$cluster, \n                                               dist(norm_creature_df, \"canberra\")) |&gt; \n  as_tibble() |&gt; \n  mutate(creature = row.names(norm_creature_df),\n         cluster = as_factor(cluster))\n\nIn der Abbildung 59.8 siehst du die Silhouettenplots für die beiden beispielhaften Datensätze. Die Regel ist, dass alles was nach links geht schlecht ist, was bei der Null liegt irrelevant und je weiter nach rechts, desto besser ist.\n\nggplot(silhouette_creature_tbl, aes(x = creature, y = sil_width, fill = cluster)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\", width = 0.5) + \n  coord_flip() + \n  labs(x = \"\")\n\nggplot(silhouette_animals_tbl, aes(x = animal, y = sil_width, fill = cluster)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\", width = 0.5) + \n  coord_flip() + \n  labs(x = \"\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten norm_creature_df\n\n\n\n\n\n\n\nAbbildung 59.8— Silhouettenplot für die Bedeutung der einzelnen Beobachtung für die Erstellung der Cluster aus der verwendeten Distanzmatrix.\n\n\n\n\nBei den Tierdaten sehen wir, dass die Beobachtung des kleinen Hundes sowie des mittleren Hundes eigentlich schädlich für die Zuordnung der Cluster ist. Der Hamster ist dabei faktisch egal. Bei den Kreaturendaten ist es auch spannend, da wir hier den Hering und die Eidechse als problematisch sehen. Beachte, dass wir alles immer im Kontext der Distanzmatrix und dem Algorithmus sehen müssen. Daher kannst du dir für beliebige Kombinationen aus Distanzmatrix, Anzahl Clustern und dem Algorithmus dann jeweils die Silhouettenplots erstellen lassen und schauen, was ist das beste Clustering?\nWenn du dir die Silhouettenplots automatisiert ausgeben willst, dann kriegst du natürlich sehr viele Plots. Da ist es dann besser nach einem möglichst hohen Silhouettenwert zu schauen. Deshalb hier einmal der mittlere Silhouettenwert für die Tierdaten sowie deren Standardabweichung.\n\nsilhouette_animals_tbl |&gt; \n  summarise(mean(sil_width),\n            sd(sil_width))\n\n# A tibble: 1 × 2\n  `mean(sil_width)` `sd(sil_width)`\n              &lt;dbl&gt;           &lt;dbl&gt;\n1             0.271           0.300\n\n\nDas Ganze dann auch einmal für die Kreaturendaten berechnet.\n\nsilhouette_creature_tbl |&gt; \n  summarise(mean(sil_width),\n            sd(sil_width))\n\n# A tibble: 1 × 2\n  `mean(sil_width)` `sd(sil_width)`\n              &lt;dbl&gt;           &lt;dbl&gt;\n1             0.204           0.318\n\n\nWir sehen, dass wir am Ende mit unserem Clustern bei den Kreaturendaten eine bessere Zuordnung zu den Clustern erhalten. Wenn wir den Algorithmus tunen wollen würden, dann würden wir für verschiedene Kombinationen aus Algorithmus und Distanzmatrixen dann jeweils die mittleren Silhouettenwerte berechnen. Dazu kommt dann noch, dass wir auch die Anzahl der Cluster tunen können. Du siehst, es wird sehr schnell sehr viel, was man hier machen kann. Ich schaue dann mal, ob ich dann was finde, was ich in den Beispielhaften Auswertungen zeige.\n\n\n59.5.5 Heatmap\nHeatmaps visualisieren die Distanzen zwischen Beobachtungen indem Heatmaps die numerischen Distanzwerte in einer Farbskala darstellen. Wenn es um Heatmaps in R geht, dann gibt es so viele Möglichkeiten eine Heatmap in R zu erstellen, so dass ich hier nur einen Ausschnitt aus den Sammlungen vorstellen kann. Ich selber mag gerne das R Paket {pheatmap} mit dem guten Tutorium Making a heatmap in R with the pheatmap package. Am Anfang des Kapitels wird dieses Paket auch als empfehlenswert genannt. Daneben schauen wir uns nochmal die Heatmaps in dem R Paket {gplots} mit der Funktion heatmap.2() an.\n\n\n\n\n\n\nWeitere Tutorien für die Erstellung einer Heatmap\n\n\n\n\n\nWie immer gibt es eine Vielzahl an tollen Tutorien, die eine Heatmap gut erklären. Ich habe hier einmal eine Auswahl zusammengestellt und du kannst dich da ja mal vertiefend mit beschäftigen, wenn du willst. Teile der Tutorien findest du vermutlich hier im Kapitel wieder.\n\nEine Übersicht zu Heatmap in R: Static and Interactive Visualization\nDas Tutorium Hierarchical cluster analysis on famous data sets.\nDas R Paket {heatmaps} Tutorial heatmaps liefert nochmal Beispiele mit genetischen Datensätzen.\nDas R Paket ComplexHeatmap, was sich vom R Paket {pheatmap} inspirieren lies. Das geht dann aber hier zu weit, schauen da, wenn du wirklich Heatmaps brauchst.\n\n\n\n\nIn der Abbildung 59.9 sehen wir einmal die Funktion pheatmap() und die sich daraus ergebenden Heatmaps. Die Ähnlichkeit wird einmal in der Legende angegeben. Je nach gewünschten Distanzmaß, kommt da eben was anderes bei raus. Du musst dir da einmal die Hilfeseite ?pheatmap zu anschauen ober aber das Tutorium. In unserem Fall wollte ich vier Gruppen für die Spalten des Dendrogramms bei den Tierdaten haben. Bei den Kreaturen habe ich nur zwei Gruppen gewählt. Du kannst auch über die Zeilen gruppieren, aber das sprengt hier alles. Probiere es einfach selber aus.\n\npheatmap(animals_df, cutree_cols = 4)\npheatmap(norm_creature_df, cutree_cols = 2)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten norm_creature_df\n\n\n\n\n\n\n\nAbbildung 59.9— Darstellung der Ähnlichkeiten zwischen den Datensätzen mit der Funktion pheatmap(). Auch hier gibt es dann eine Menge möglicher Optionen, hier nur sehr sparsam gezeigt.\n\n\n\n\nDie Funktion heatmap.2() liefert sehr viel mehr Optionen und erlaubt noch etwas feinere Abstimmungen. Du kannst ziemlich einfach noch Dendrogramme für die Zeilen und Spalten ergänzen. Damit die Erstellung der Dendrogramme nicht so fehleranfällig ist, packe ich mir den Code immer in eine Funktion und entscheide dann nach der Option dim, ob ich die Daten transponieren oder nicht-transponieren muss. Die Spalten rechne ich auf den untransponierten Daten und die Zeilen mache ich zu Spalten durch das Transponieren mit der Funktion t().\n\nget_dendro_margin &lt;- function(tbl, k, dim = \"row\"){\n  if(dim == \"row\") {\n    tmp_tbl &lt;- tbl |&gt; \n      dist(method = \"man\") |&gt; \n      hclust(method = \"ward.D\") \n  } else {\n    tmp_tbl &lt;- t(tbl) |&gt; \n      dist(method = \"man\") |&gt; \n      hclust(method = \"com\") \n  }\n  dendro_obj &lt;- tmp_tbl |&gt; \n    as.dendrogram() |&gt; \n    ladderize() |&gt;\n    color_branches(k = k)\n  return(dendro_obj)\n}\n\nAlso einmal das Dendrogramm für vier Gruppen über die Zeilen gebaut. Wir nutzen \\(k = 4\\) für die Anzahl an einzufärbenden Gruppen sowie die Option dim = \"row\" um über die untransponierten Zeilen zu Clustern. Das machen wir jetzt einmal für den Tierdatensatz und dann einmal für den Kreaturendatensatz.\n\ndend_animal_r &lt;- get_dendro_margin(animals_df, k = 4, dim = \"row\")\ndend_creature_r &lt;- get_dendro_margin(norm_creature_df, k = 4, dim = \"row\")\n\nDas Ganze dann nochmal über die Spalten auf gleicher Weise nur dann eben mit der Option dim = \"col\". Hier haben wir aber nur drei Spalten in den Daten, da können wir dann keine drei Gruppen einfärben. Das wären ja dann die drei Spalten per se.\n\ndend_animal_c &lt;- get_dendro_margin(animals_df, k = 3, dim = \"col\")\ndend_creature_c &lt;- get_dendro_margin(norm_creature_df, k = 2, dim = \"col\")\n\nIn der Abbildung 59.10 kommt dann alles einmal zusammen gebaut. Wie du siehst, hat die Funktion heatmap.2() noch eine Menge zusätzliche Optionen. Ich habe hier einfach mal ein paar behalten mit denen du dann mal rumspielen kannst. Am Ende weiß ich dann auch immer nicht, was soll da denn so rein? Am besten die Dinge und Färbungen, die dann auch deine Fragestellung mit beantworten. Das ist dann aber an dir zu entscheiden.\n\ngplots::heatmap.2(as.matrix(animals_df), \n          main = \"Eigenschaften von Tieren\",\n          srtCol = 35,\n          Rowv = dend_animal_r, Colv = dend_animal_c,\n          trace=\"row\", hline = NA, tracecol = \"darkgrey\",         \n          margins =c(6, 5), key.xlab = \"no / yes\",\n          denscol = \"grey\", density.info = \"density\",\n          col = gplots::bluered(100))\n\n\ngplots::heatmap.2(as.matrix(norm_creature_df), \n          main = \"Eigenschaften von Kreaturen\",\n          srtCol = 35,\n          Rowv = dend_creature_r, Colv = dend_creature_c,\n          trace = \"row\", hline = NA, tracecol = \"darkgrey\",         \n          margins = c(6, 7), key.xlab = \"no / yes\",\n          denscol = \"grey\", density.info = \"density\",\n          col = gplots::bluered(100))\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten norm_creature_df\n\n\n\n\n\n\n\nAbbildung 59.10— Darstellung der Ähnlichkeiten zwischen den Datensätzen mit der Funktion heatmap.2(). Auch hier gibt es dann eine Menge möglicher Optionen.\n\n\n\n\nDie Tierdaten sind nur \\(0/1\\)-codiert, deshalb sind natürlich die Farben der Heatmap nur rot und blau. Wie du an dem Kreaturendatensatz sehen kannst, werden Heatmaps eigentlich nicht auf so kleinen Datensätzen gerechnet. Wenn du mehr Spalten hast, dann werden auch die Spaltenunterschriften kleiner. Ich habe hier jetzt nicht die Muse nochmal in der Hilfe der Funktion ?heatmap.2 die Optionen zu suchen.\nWir sehen aber auch, dass wir über die Zeilen eine obere Gruppe haben, die Eidechse bis Ameise beinhaltet. Dann kommt eine größere Gruppe der Säugetiere, die sich dann nochmal kleiner aufspaltet. Ich finde die Spalten etwas schwerer zu deuten, deshalb lass ich das mal hier. Bei den Kreaturen finde ich es noch schwerer. Zwar sind der Wal und der Elefant zeilweise zusammen, aber der Rest ist schwer zu deuten. Da ist mir dann doch zu viel Heterogenität drin, dass ich da dem Clustern glauben würde. Vermutlich sind jetzt aber auch die Daten etwas zu klein für eine gute Darstellung in einer Heatmap.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Clusteranalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-cluster.html#sec-clust-tidyclust",
    "href": "stat-modeling-cluster.html#sec-clust-tidyclust",
    "title": "59  Clusteranalysen",
    "section": "59.6 Datenanalyse mit tidyclust",
    "text": "59.6 Datenanalyse mit tidyclust\nWie du schon oben gesehen hast, ist es teilweise echt nervig immer die row.names() mit zu nehmen oder alles ein data.frame() zu nutzen. Insbesondere wenn die Daten sehr groß werden, kann kann es sehr ungünstig sein, alles in einem data.frame() zu lagern. Deshalb gibt es das Paket {tidyclust}, was ich hier nochmal vorstellen möchte. Die Visualisierungen von oben können alle genutzt werden. Der Vorteil ist eben, dass wir hier in der tidy-Welt sind und uns auch die recipes() aus dem Klassifikationskapiteln zu nutze machen können. Da ist dann das Normalisieren und andere Vorbereitungsschritte der Daten viel einfacher. Damit wir auch mal die Datenanalyse mit einem großen Datensatz sehen, nutze ich hier einmal den Datensatz der Gummibärchendaten. Im Folgenden analysieren wir also die Gummibärchendaten einmal mit dem R Paket {tidyclust}.\n\n59.6.1 Hierarchical Clustering\nIm Folgenden also einmal der Ablauf in tidyclust für die hierarchische Clusteranalyse. Auch hier müssen wir im ersten Schritt einmal festlegen welche Anzahl an Clustern num_clusters wir wollen. Dann müssen wir auch noch festlegen, welche linkage Methode wir wollen. Ich nehme hier einmal die average Methode. Damit haben wir aber noch nichts gerechnet, sondern nur ein Objekt erschaffen in dem steht, was wir machen wollen.\n\n\ntidyclust Hilfeseite für das Hierarchical Clustering\n\nhc_spec &lt;- hier_clust(num_clusters = 3,\n                      linkage_method = \"average\")\n\nJetzt können wir das Objekt hc_spec in die Funktion fit() pipen, die uns dann die hierarchische Clusteranalyse rechnet. Da wir in der Funktion fit() bestimmen können, welche Spalten mit in den Algorithmus sollen, müssen wir nicht umständlich mit einem vollem Datensatz arbeiten.\n\nhc_fit &lt;- hc_spec |&gt;\n  fit(~ gender + age + height + semester + most_liked,\n      data = gummi_tbl)\n\nWenn es zu lang wird dann geht auch fit(~ .), dann ist es aber ehrlich gesagt besser, die Variante mit dem data.frame() zu nutzen. In diesem Fall nimmst du dann ja wieder alle Spalten mit rein. Mit dem hc_fit kannst du dann ganz normal weiterarbeiten. Die Funktion extract_fit_summary() erlaubt es dir dann die wichtigsten Informationen aus dem Fit rauszuziehen und über cluster_assignments kommst du dann an die Clusterzuordnungen ran.\n\nhc_summary &lt;- hc_fit |&gt; \n  extract_fit_summary()\n\nIm Folgenden einmal die ersten sieben Clusterzuordnungen.\n\nhc_summary |&gt; \n  pluck(\"cluster_assignments\") |&gt; \n  head(7)\n\n[1] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\nLevels: Cluster_1 Cluster_2 Cluster_3\n\n\nDie weitere Analyse ist dann wie auch oben. Du kannst dir dann einen Silhouettenplot erstellen lassen oder aber die Ergebnisse in einem Dendrogramm visualisieren. Das ist dann alles das Gleiche.\n\n\n59.6.2 k-means Clustering\nAuch gibt es für den kmeans Algorithmus eine bessere Variante für einen sauberen Workflow. Auch hier wird als erstes einmal der Algorithmus definiert. In unserem Fall heißt er dann k_means(). Wir wählen mit num_clusters die Anzahl an Gruppen, die am Ende raus kommen sollen.\n\n\ntidyclust Hilfeseite für das k-means Clustering\n\nkmeans_spec &lt;- k_means(num_clusters = 3)\n\nDanach können wir dann unsere Informationen über den Algorithmus in die Funktion fit() weiterleiten. In der fit() Funktion definieren wir dann die Spalten, die für das Clustern von Bedeutung sein sollen.\n\nkmeans_fit &lt;- kmeans_spec |&gt;\n  fit(~ age + height + semester,\n      data = gummi_tbl)\n\nWir können auch hier über die Funktion extract_cluster_assignment() die Clusterzugehörigkeiten erhalten. Damit können wir dann prinzipiell auch schon aufhören und die Visualisierungen erstellen, die wir aus den vorherigen Abschnitten kennen.\n\nkmeans_fit |&gt;\n  extract_cluster_assignment()\n\n# A tibble: 210 × 1\n   .cluster \n   &lt;fct&gt;    \n 1 Cluster_1\n 2 Cluster_2\n 3 Cluster_2\n 4 Cluster_3\n 5 Cluster_1\n 6 Cluster_1\n 7 Cluster_2\n 8 Cluster_2\n 9 Cluster_1\n10 Cluster_1\n# ℹ 200 more rows\n\n\nMit der Funktion extract_fit_summary() erhalten wir dann noch zusätzliche Informationen, da musst du dann aber mal auf der Hilfeseite schauen, was du konkret an Informationen brauchst. Da gibt es wieder eine Menge. Am Ende kannst du dir dann noch die Informationen zu dem durchschnittlichen Silhouettenwert wiedergeben lassen um Modelle untereinander vergleichen zu können. Der Vergleich geht hier natürlich schneller, da du einfacher Variablen aus dem Modell nehmen kannst als dir immer weider enue Datensätze zu bauen.\n\nkmeans_fit |&gt;\n  tidyclust::silhouette_avg(select(gummi_tbl, age, height, semester))\n\n# A tibble: 1 × 3\n  .metric        .estimator .estimate\n  &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt;\n1 silhouette_avg standard       0.425\n\n\n\n\n\nAbbildung 59.1— Heatmap der euklidischen Distanzen des Tierdatensatzes. Die Matrix ist symmetrisch. Hohe, blaue Werte bedeuten eine große Distanz dagegen kleine,rote Werte eine geringe Distanz zwischen den Beobachtungen.\nAbbildung 59.2— Beispiel für das Ergebnis einer hierarchischen Clusteranalyse der Tierdaten dargestellt als Dendrogramm\nAbbildung 59.3 (a)— Tierdaten animals_df\nAbbildung 59.3 (b)— Kreaturendaten norm_creature_df\nAbbildung 59.4 (a)— Tierdaten animals_df\nAbbildung 59.4 (b)— Kreaturendaten norm_creature_df\nAbbildung 59.5 (a)— Tierdaten animals_df\nAbbildung 59.5 (b)— Kreaturendaten norm_creature_df\nAbbildung 59.6 (a)— Tierdaten animals_df\nAbbildung 59.6 (b)— Kreaturendaten norm_creature_df\nAbbildung 59.7— .\nAbbildung 59.8 (a)— Tierdaten animals_df\nAbbildung 59.8 (b)— Kreaturendaten norm_creature_df\nAbbildung 59.9 (a)— Tierdaten animals_df\nAbbildung 59.9 (b)— Kreaturendaten norm_creature_df\nAbbildung 59.10 (a)— Tierdaten animals_df\nAbbildung 59.10 (b)— Kreaturendaten norm_creature_df",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Clusteranalysen</span>"
    ]
  },
  {
    "objectID": "stat-modeling-pca.html",
    "href": "stat-modeling-pca.html",
    "title": "60  Multivariate Verfahren",
    "section": "",
    "text": "60.1 Theoretischer Hintergrund\nSoweit klingt es ja alles recht einleuchten mit der Hauptkomponentenanalyse. Wie aber kommt jetzt die Idee einer Geraden durch die Punktewolke mit Hauptkomponenten zusammen? Wie geht das mathematisch eigentlich? Da sieht es schon etwas komplizierter aus. Deshalb fangen wir wie immer einmal mit einem Datensatz an. Wir nehmen das Datenbeispiel aus der Abbildung 60.1 (a). In der folgenden Tablle siehst du einmal die ersten sieben Wertepaare für \\(x\\) und \\(y\\) der insgesamt dreißig Zeilen.\nTabelle 60.1— Auszug aus den dreißig Datenpaaren \\(x\\) und \\(y\\).\n\n\n\n\n\n\nx\ny\n\n\n\n\n-1.4660039\n0.5425225\n\n\n-0.4523909\n-0.8303681\n\n\n-2.6538584\n-1.5160298\n\n\n-7.1191853\n-4.0628188\n\n\n2.4368904\n-0.1544630\n\n\n-6.9975329\n-3.7431590\n\n\n4.4292391\n3.4317681\nWie haben wir die Daten gebaut? Wir haben uns folgendes Modell genommen. Die \\(y\\) Werte hängen von den \\(x\\) Werten wfolgt ab.\n\\[\ny \\sim 0 + 0.5 \\cdot x + \\epsilon; \\; \\epsilon \\sim \\mathcal{N}(0,1)\n\\]\nDas heißt, wir haben eine Steigung der Geraden \\(\\beta_x\\) von \\(0.5\\). Unsere Punkte streuen mit einer Varianz von \\(1\\) um die Gerade. Wir wollen es nicht komplizierter machen als sowieso schon ist. Hier dann einmal die Implementierung in R. Das set.seed() nutzen wir um immer die gleichen Zahlen wiedergegeben zu bekommen. Wir ziehen die \\(x\\)-Werte aus einer Normalverteilung mit einem Mittelwert von 0 und einer Standardabweichung von 5.\nset.seed(2045312)\ntbl &lt;- tibble(x = rnorm(30, 0, 5),\n              y = 0 + 0.5 * x + rnorm(30, 0, 1))\nJetzt können wir einmal über die Funktion lm() die Koeffizienten der Geraden schätzen. Also wie ist den nun die Steigung und der Achsenabschnitt in den generierten Daten?\nlm(y ~ x, tbl) |&gt; \n  coefficients() |&gt; \n  round(2)\n\n(Intercept)           x \n       -0.1         0.5\nWir sehen, dass wir den y-Achsenabschnitt mit \\(-0.1\\) nicht ganz treffen, wohl aber die Steigung mit \\(0.5\\). Damit hätten wir die Information für die erste Gerade, was ja auch die 1. Hauptkomponente wäre. Nun müssten wir die Gerade finden, die senkrecht auf der geschätzten Geraden steht und durch den Nullpunkt läuft. Das könnten wir ausprobieren oder aber auch abschätzen. Problematisch wird das ganze, wenn wir mehr als zwei Variablen haben. Hier kommt uns aber die Varianz/Kovarianzmatrix \\(\\boldsymbol{\\Sigma_x}\\) zur Hilfe. In der Varianz/Kovarianzmatrix finden wir alle Informationen die wir brauchen und noch mehr.\nErstmal wollen wir die Varianz/Kovarianzmatrix mit der Funktion cov() aus dem Datensatz berechnen. Wir runden hier ziemlich streng damit wir die Zahlen besser vergleichen können.\ncov(tbl) |&gt; \n  round(1)\n\n     x    y\nx 21.6 10.9\ny 10.9  6.1\nHier nochmal die Varianz/Kovarianzmatrix mathematisch aufgeschrieben. Auf der Diagonalen der Matrix findest du die Varianzen von \\(x\\) sowie von \\(y\\). Auf den Nebendiagonalen dann die Kovarianz von \\(x\\) und \\(y\\). Die Kovarianz beschriebt ja das Variieren von \\(y\\), wenn sich auch \\(x\\) ändert. Eine Kovarianz von 0 bedeutet damit dann auch, dass wir keine Steigung oder einen Trend in der Punktewolke sehen. Ändert sich \\(x\\) ändert sich \\(y\\) nicht, wir haben kein gemeinsames Variieren.\n\\[\n\\Sigma =\n\\begin{pmatrix}\n\\operatorname{Var}(x) & \\operatorname{Cov}(x, y) \\\\\n\\operatorname{Cov}(x, y) & \\operatorname{Var}(y)\n\\end{pmatrix} =\n\\begin{pmatrix}\n21.6 & 10.9 \\\\\n10.9 & 6.1\n\\end{pmatrix}\n\\]\nUnd jetzt kommt der eigentlich Kniff. Wir können aus der Kovarianzmatrix unser \\(\\beta_x\\) berechnen. So einfach zwar nur, wenn wir zwei Variablen haben, aber das Prinzip ist auch bei mehr Variablen gegeben. Wenn wir die Kovarianz \\(\\operatorname{Cov}(x, y)\\) durch die Varianz (x) von \\(x\\) teilen, erhalten wir die Steigung der Geraden.\n\\[\n\\beta_x = \\cfrac{\\operatorname{Cov}(x, y)}{\\operatorname{Var}(x)} = \\cfrac{10.9}{21.6} \\approx 0.5\n\\]\nDas heißt, dass wir in der Varianz/Kovarianzmatrix die Informationen über die Steigung der Geraden haben. Mehr noch, wenn wir mehrere Variablen haben, dann ist das Zusammenspiel der Variablen in der Varianz/Kovarianzmatrix verborgen. Schauen wir dazu nochmal die Abbildung 60.3 (a) an. Hier sehen wir, wie die 1. Hauptkomponente durch die Punktewolke läuft. Wie kriegen wir jetzt in Abbildung 60.3 (b) für die 2. Hauptkomponente die Koeffizienten der Gerade bestimmt? Hier kommt uns eine Eigenschaft einer Matrix und die Zerlegung der Matrix in ihre Eigenwerte und Eigenvektoren zu Hilfe.\n(a) 1. Hauptkomponente (PC1) durch die Punktewolke\n\n\n\n\n\n\n\n\n\n\n\n(b) 2. Hauptkomponente (PC2) durch die Punktewolke\n\n\n\n\n\n\n\n\n\n\n\n(c) Regressionsgleichungen der 1. Hauptkomponente (PC1) sowie deren Rotation zur 2. Hauptkomponente.\n\n\n\n\n\n\n\nAbbildung 60.3— Schematische Darstellung der 1. Hauptkomponente (PC1) durch die Punktewolke mit maximal erklärter Varianz, da minimaler Abstand zu den Punkten. Die 2. Hauptkomponente muss lotrecht auf der 1. Hauptkomponente stehen und dabei die Varianz weiter maximal minimieren. Im Weiteren wurden dann die Regressiongleichungen ergänzt.\nFolgende Funktion eigen() berechnet die Eigenwerte aus der Varianz/Kovarianzmatrix. Erstmal sieht das wieder realtiv wild aus und wir erkennen da noch nicht so richtig ein Muster. Eigenwerte und ihre Eigenvektoren sind nicht miteinander korreliert und stehen sozusagen orthogonal und damit senkrecht aufeinander. Das ist es was wir suchen!\neigen_res &lt;- cov(tbl) |&gt; \n  eigen()\neigen_res\n\neigen() decomposition\n$values\n[1] 27.2356150  0.4907358\n\n$vectors\n           [,1]       [,2]\n[1,] -0.8887583  0.4583762\n[2,] -0.4583762 -0.8887583\nWenn du jetzt die untere Zeile durch die obere Zeile der Eigenvektoren teilst, dann erhälst du zwei Zahlen.\neigen_res$vectors[2,] / eigen_res$vectors[1,]\n\n[1]  0.515749 -1.938928\nUnd wir sehen auch, dass wir in den Eigenwerten die Varianz wiederfinden. Wir hatten ja unsere \\(x\\)-Variable mit einer Standardabweichung von \\(5\\) erschaffen und unsere \\(y\\)-Variable um die Standardabweichung von \\(1\\) erhöht. Das finden wir dann auch hier wieder.\neigen_res$values |&gt; sqrt()\n\n[1] 5.2187752 0.7005254\nDie erste Zahl ist \\(\\beta_x\\) extrem nahe und entspricht auch der Steigung der Geraden durch die Punktewolke. Die zweite Zahl ist die Steigung einer Geraden die orthogonal auf der ersten gerade steht und durch den Ursprung läuft. Wir haben damit die zweite Hauptkomponente gefunden.\nDas Ganze auch mal mit der richtigen Funktion prcomp(), die das Ganze dann auch für mehr als zwei Dimensionen und Variablen kann. Hier siehst du auch warum es in der Funktion dann rotation heißt, es ist eben die Drehung der Hauptkomponenten untereinander. Auch finden wir hier in den Eigenwerten die Standardabweichungen wieder, die wir ursprünglich in der Generierung der Daten verwendet haben.\npca_res &lt;- prcomp(tbl)\npca_res\n\nStandard deviations (1, .., p=2):\n[1] 5.2187752 0.7005254\n\nRotation (n x k) = (2 x 2):\n        PC1        PC2\nx 0.8887583 -0.4583762\ny 0.4583762  0.8887583\nAuch hier können wir wieder die zweite Zeile durch die erste Zeile teilen und kriegen die Steigungen der Geraden aus der Abbildung 60.3 (b) wieder.\npca_res$rotation[2,] / pca_res$rotation[1,]\n\n      PC1       PC2 \n 0.515749 -1.938928\nIch habe dann mal die Geradengleichungen, die sich für die beiden Hauptkomponenten ergeben einmal zu der Abbildung 60.3 (c) ergänzt. Das funktioniert natürlich alles nur auf standardisierten Daten sonst ist ja auch der y-Achsenabschnitt nicht 0. Ja, es geht formal auch auf den nicht transformierten Daten, aber der Standard ist, die Daten zu standardisieren.\nDiese Demonstration funktioniert nur mit zwei Variablen, da wir ja nur zweidimensionale Abbildungen erstellen können und zum anderen der Zusammenhang der Steigungen aus der Varianz/Kovarianzmatrix und den Eigenwerten dann gegeben ist. Das Prinzip bleibt aber das Gleiche. Wir nutzen die mathematischen Eigenschaften der Eigenwertzerlegung von Matrizen um uns orthogonale Geraden wiedergeben zu lassen.\nWas machen wir jetzt also nochmal? Wir wollen unseren Daten, also die ganze Datenmatrix einmal so transformieren, dass wir neue Komponenten aus den Daten extrahieren, die die Daten auf einer anderen Dimension beschreiben. Im Prinzip handelt es sich also bei der Hauptkomponentenanalyse um eine komplexere Transformation der Daten. Wir nutzen dabei die Varianz/Kovarianzmatrix als unsere Distanzmatrix. Wird die gesamte Varianz der Daten vielleicht nur von wenigen Spalten verursacht? Dann brauchen wir ja nur diese wenigen Hauptkomponenten weiter betrachten. Wir nutzen also die Varianz/Kovarianzmatrix als unsere Ähnlichkeitsmatrix, wie schon bei den Clusteranalysen, um hier unbekannte Zusammenhänge zwischen den Spalten und damit dann Hauptkomponenten aufzuklären.\nNeben der PCA existiert noch das Multidimensional Scaling (abk. MDS). Das MDS ist im Prinzip eine Spezialform der PCA. Im Unterschied zur PCA wird die MDS auf einer Distanzmatrix gerechnet. In einer MDS können wir nicht einfach so unsere Daten reinstecken sondern müssen zuerst die Daten in eine Distanzmatrix umrechnen. Dafür gibt es die Funktion dist() oder as.dist(), wenn wir schon Distanzen vorliegen haben. Daher ist die Anwendung einer MDS nicht besonders komplizierter.\nWenn wir eine PCA in R rechnen wollen, dann haben wir zuerst die Wahl zwischen den Funktionen prcomp() und princomp(). Laut der R-Hilfe hat die Funktion prcomp() eine etwas bessere numerische Genauigkeit. Daher ist die Funktion prcomp() gegenüber princomp() vorzuziehen. Es gibt aber noch eine neuere Implementierung der Funktionalität in dem R Paket {FactoMineR} und der Funktion PCA(). Da wir dann alles aus einer Hand haben nutzen wir in diesem Kapitel also das R Paket {factoextra}. Wir haben in dem Paket auch die die Funktionen um sich Faktoranalysen super anzuschauen und durchzuführen. Du kannst mehr auf der Webseite Factoextra R Package: Easy Multivariate Data Analyses and Elegant Visualization mehr über das Paket erfahren.\nEs gibt eine natürlich große Anzahl an Quellen wie du in R eine PCA oder ein MDS durchführst. In der folgenden Box findest du eine Sammlung an Tutorien und R Code, der dir als Inspiration dienen mag. Ich werde Teile von den Tutorien in dem Kapitel hier adaptieren und verwenden, kann aber natürlich nichts alles nochmal nachkochen. Das ist ja auch nicht der Sinn hier.\nWir wollen uns jetzt die Hauptkomponentenanalyse an zwei Spieldaten anschauen. Eigentlich werden ja auch gerne Fragebögen mit der Hauptkomponentenanalyse ausgewertet, aber hier muss ich nochmal warten bis ich ein gutes Beispiel in den Beratungen hatte. Dann ergänze ich natürlich ein Beispiel für die Auswertung eines Fragebogen mittels Hauptkomponentenanalyse beim Skript zu den Beispielhaften Auswertungen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Multivariate Verfahren</span>"
    ]
  },
  {
    "objectID": "stat-modeling-pca.html#theoretischer-hintergrund",
    "href": "stat-modeling-pca.html#theoretischer-hintergrund",
    "title": "60  Multivariate Verfahren",
    "section": "",
    "text": "Exkurs: Die Varianz/Kovarianzmatrix \\(\\boldsymbol{\\Sigma_x}\\)\n\n\n\n\n\nWenn du dich ein wenig mit Eigenwerten auskennst, dann weißt du, dass Eigenwerte immer auf einer Matrix berechnet werden. In unserem Fall werden die Eigenwerte der Hauptkomponeten auf der Varianz/Kovarianzmatrix der standardisierten Daten berechnet. Im Folgenden siehst du einmal die Varianz/Kovarianzmatrix \\(\\boldsymbol{\\Sigma_x}\\) für die Datenmatrix \\(x\\). Du musst dir vorstellen, dass jedes \\(x_1\\) bis \\(x_n\\) jeweils eine Spalte in deiner Datenmatrix \\(\\boldsymbol{x}\\) entspricht, die wir in unser Modell für die Hauptkomponentenanalyse aufnehmen wollen.\n\\[\n\\boldsymbol{\\Sigma_x} = \\begin{pmatrix}\\operatorname{Var}(x_1) & \\operatorname{Cov}(x_1,x_2) & \\cdots & \\operatorname{Cov}(x_1,x_n) \\\\ \\\\\n\\operatorname{Cov}(x_2,x_1)  & \\operatorname{Var}(x_2) & \\cdots & \\operatorname{Cov}(x_2,x_n) \\\\ \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \\\\\n\\operatorname{Cov}(x_n,x_1) & \\operatorname{Cov}(x_n,x_2) & \\cdots & \\operatorname{Var}(x_n)\n\\end{pmatrix}\n\\]\nFür den Fall von zwei Variablen \\(x_1\\) und \\(x_2\\) können wir die Idee der Kovarianz nochmal nachvollziehen. Später rechnen wir dann die Hauptkomponentenanalyse auf der großen Matrix, aber ich kann dir den Zusammenhang leider nur für zwei Variablen erklären, den zweidimensional kriege ich noch hin. Wir können mit folgender Formel die Kovarianzen zwischen den beiden Variablen \\(x_1\\) und \\(x_2\\) berechnen.\n\\[\n\\operatorname{Cov}(x_2,x_1) = \\sum_{i=1}^n(x_{1i}-\\bar{x_1})(x_{2i}-\\bar{x_2})\n\\]\nNochmal als Erinnerung, die Formel berechnet die quadrierten Abweichung der Beobachtungen von \\(x_1\\) zum Mittelwert \\(\\bar{x}_1\\) und somit die Varianz \\(\\operatorname{Var}(x_1)\\) von \\(x_1\\).\n\\[\n\\operatorname{Var}(x_1) = \\sum_{i=1}^n(x_{1i}-\\bar{x}_1)^2\n\\]\nEbenso berechnet diese Formel die quadrierten Abweichung der Beobachtungen von \\(x_2\\) zum Mittelwert \\(\\bar{x}_2\\) und damit die Varianz \\(\\operatorname{Var}(x_2)\\) von \\(x_2\\).\n\\[\n\\operatorname{Var}(x_2) = \\sum_{i=1}^n(x_{2i}-\\bar{x}_2)^2\n\\]\nDas Ganze ist natürlich sehr trocken. Deshalb füttern wir einmal die Variablen \\(x_1\\) und \\(x_2\\) mit echten Daten und erschaffen uns den Datensatz cov_tbl. In Tabelle 60.2 ist der Zusammenhang nochmal Schritt für Schritt aufgeschlüsselt wie sich die Zahlen grob berechnen. Ich habe ein, zwei Schritte ausgelassen, aber die ergänzt du fix selber.\n\ncov_tbl &lt;- tibble(x_1 = c(0.8, 1.0, 1.2, 1.9, 2.0, 2.7, 2.8),\n                  x_2 = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7))\n\nDann können wir uns einmal den Mittelwert und die Varianz für die beiden Variablen \\(x_1\\) und \\(x_2\\) berechnen.\n\ncov_tbl |&gt; \n  gather() |&gt; \n  group_by(key) |&gt; \n  summarise(mean = mean(value), var = var(value))\n\n# A tibble: 2 × 3\n  key    mean   var\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 x_1    1.77 0.642\n2 x_2    1.87 0.339\n\n\nIn der folgenden Abbildung 60.2 siehst du einmal die Konzepte der Varianz für \\(x_1\\) in Subplot A und \\(x_2\\) in Subplot B dargestellt. Die durchgezogene Linie stellt dabei den Mittelwert für die beiden Variablen dar. Die Varianz berechnet sich jetzt als der quadrierte Abstand von den Beobachtungen zu den Mittelwerten. Der Abstand ist als gestrichelte Linie dargestellt. Faktisch addierst du die sich ergebenden Quadrate auf. Bei der Kovarianz sind es keine Quadrate, sondern Rechtecke. Die berechnest nämlich einmal den Abstand einer Beobachtung zum Mittelwert von \\(x_1\\) und einmal den Abstand zum Mittelwert von \\(x_2\\). Die beiden Abstände \\((x_{1i}-\\bar{x_1})\\) und \\((x_{2i}-\\bar{x_2})\\) multiplizierst du und addierst dann diese Rechtecke auf.\n\n\n\n\n\n\n\n\nAbbildung 60.2— Berechnung der Abstände für die Varianz von \\(x_1\\) sowie \\(x_2\\) in der oberen Zeile. In der unteren Zeile die Rechtecke der Berechnung der Kovarianz von \\(x_1\\) und \\(x_2\\).\n\n\n\n\n\nIn der folgenden Tabelle 60.2 siehst du dann das Vorgehen nochmal numerisch. Wichtig ist hierbei, dass wir am Ende die Varianz und die Kovarianz berechnen können indem wir die Summen \\(\\sum\\) durch \\(n-1\\) gleich 6 teilen.\n\n\n\nTabelle 60.2— Tabelle zur Berechnung der Varianz sowie der Kovarianz von \\(x_1\\) und \\(x_2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{x_1}\\)\n\\(\\boldsymbol{x_2}\\)\n\\(\\boldsymbol{(x_{2i}-\\bar{x}_2)^2}\\)\n\\(\\boldsymbol{(x_{1i}-\\bar{x}_1)^2}\\)\n\\(\\boldsymbol{(x_{1i}-\\bar{x}_1)(x_{2i}-\\bar{x}_2)}\\)\n\n\n\n\n1.2\n0.8\n0.94\n0.45\n0.65\n\n\n1.8\n1.0\n0.60\n0.01\n0.06\n\n\n1.3\n1.2\n0.33\n0.33\n0.33\n\n\n1.7\n1.9\n0.02\n0.03\n-0.02\n\n\n2.6\n2.0\n0.05\n0.53\n0.17\n\n\n1.8\n2.7\n0.86\n0.03\n-0.07\n\n\n2.7\n2.8\n1.06\n0.69\n0.85\n\n\n\n\\(\\sum\\)\n3.86\n2.05\n1.97\n\n\n\n\\(\\cfrac{\\sum}{n-1}\\)\n0.64\n0.34\n0.33\n\n\n\n\n\n\nSchauen wir mal, ob wir richtig gerechnet haben und die Varianz für \\(x_1\\) mit 0.64, die Varianz von \\(x_2\\) mit 0.34 sowie die Kovarianz von \\(x_1\\) und \\(x_2\\) auch R wiederfinden. Wir nutzen die Funktion cov() um uns die Varianz/Kovarianzmatrix wiedergeben zu lassen.\n\ncov_tbl |&gt; \n  cov() |&gt; \n  round(2)\n\n     x_1  x_2\nx_1 0.64 0.33\nx_2 0.33 0.34\n\n\nWie wir sehen können wir die Werte in der Varianz/Kovarianzmatrix wiederfinden. Das ist ja mal ein Erfolg. Wir nutzen also die Varianz/Kovarianzmatrix als unsere Ähnlichkeitsmatrix, wie schon bei den Clusteranalysen, um hier unbekannte Zusammenhänge zwischen den Spalten und damit dann den Hauptkomponenten aufzuklären.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeht das etwas korrekter?\n\n\n\nJa, das war jetzt mathematisch teilweise unsauber. Aber darum geht es ja hier auch nicht - wir machen ja hier die Anwendung mit einem erweiterten Grundverständnis. Mathematisch besser geht natürlich in dem A Beginner’s Guide to Eigenvectors, Eigenvalues, PCA, Covariance and Entropy. Auch dem Rahmen ist das Tutorium Eigenvalues and Eigenvectors: Properties interessant. Am Ende noch ein Tutorium von Stackexchange Making sense of principal component analysis, eigenvectors & eigenvalues. Wie immer, du musst wissen wie tief du dann noch abtauchen willst.\n\n\n\n\n\n\n\n\n\nHauptkomponente, Eigenwert und Varianz kurz zusammengefasst\n\n\n\nIn einer Hauptkomponentenanalyse ersetzen wir die ursprünglichen Spalten eines Datensatzes durch Hauptkomponenten. Die Hauptkomponenten haben so viele Dimensionen wie es Spalten im ursprünglichen Datensatz gibt. Jede Hauptkomponente hat einen Eigenwert (eng. eigenvalue), der den Anteil der erklärten Varianz der Hauptkomponente in den Daten beschreibt. Wir können die Beobachtungen oder Individuen (abk. ind) in den Zeilen betrachten oder aber die Variablen (abk. var) in den Spalten.\n\n\n\n\n\n\n\n\n\n\n\nWeitere Tutorien für die Principal Component Analysis\n\n\n\n\n\nWie immer gibt es eine Vielzahl an tollen Tutorien, die die PCA gut erklären. Ich habe hier einmal eine Auswahl zusammengestellt und du kannst dich da ja mal vertiefend mit beschäftigen, wenn du willst. Teile der Tutorien findest du vermutlich hier im Kapitel wieder.\n\nPrincipal Component Analysis (PCA) For Dummies\nPrincipal Component Analysis 4 Dummies: Eigenvectors, Eigenvalues and Dimension Reduction\nThe most gentle introduction to Principal Component Analysis\nWelcome to a Little Book of R for Multivariate Analysis!",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Multivariate Verfahren</span>"
    ]
  },
  {
    "objectID": "stat-modeling-pca.html#genutzte-r-pakete",
    "href": "stat-modeling-pca.html#genutzte-r-pakete",
    "title": "60  Multivariate Verfahren",
    "section": "60.2 Genutzte R Pakete",
    "text": "60.2 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, patchwork,\n               factoextra, FactoMineR, ggpubr, MASS, mda,\n               janitor, corrplot, HDclassif, klaR, see,\n               conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(magrittr::set_names)\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Multivariate Verfahren</span>"
    ]
  },
  {
    "objectID": "stat-modeling-pca.html#daten",
    "href": "stat-modeling-pca.html#daten",
    "title": "60  Multivariate Verfahren",
    "section": "60.3 Daten",
    "text": "60.3 Daten\nWenn wir jetzt die Hauptkomponentenanalyse rechnen wollen, dann brauchen wir auch Datenbeispiele. Ich habe jetzt zwei sehr eingängige Beispiele gewählt, die du nicht so stark inhaltlich durchdringen musst. Es geht um die Unterschiede von Tieren und deren Eigenschaften. Die Datenbeispiele sind so gewählt, dass du das Konzept hinter der Hauptkomponentenanalyse verstehst und nicht so sehr nach der direkten Anwendbarkeit. Danach schauen wir uns noch an, warum wir die Daten standardisieren müssen und wie wir mit dem data.frame()-Problem umgehen. Wir können leider keine tibble() in der Hauptkomponentenanalyse nutzen.\n\n60.3.1 Zwei Beispieldatensätze\nBeginnen wir mit einem normierten Datensatz aus dem R Paket {cluster}. Der Datensatz animals wurde von mir noch mit ein paar Tieren ergänzt und schaut sich sechs Eigenschaften von 23 Tieren an. Wir wollen im Folgenden nun herausfinden, ob wir anhand der Eigenschaften in den Spalten die Tiere in den Zeilen in Gruppen einordnen können. Einige der Tiere sind ja näher miteinander verwandt als andere Tiere. Die ursprünglichen Daten liefen noch auf einem \\(1/2\\)-System, das ändern wir dann zu \\(0/1\\) damit wir dann auch besser mit den Daten arbeiten können. Für die Algorithmen ist es egal, aber ich habe lieber \\(1\\) gleich ja und \\(0\\) gleich nein.\n\nanimals_tbl &lt;- read_excel(\"data/cluster_animal.xlsx\", sheet = 1) |&gt; \n  clean_names() |&gt; \n  mutate(across(where(is.numeric), \\(x) x - 1))\n\nSchauen wir uns einmal den Datensatz in der Tabelle 60.3 an. Wir sehen, dass wir noch einige fehlende Werte in den Daten vorliegen haben. Das ist manchmal ein Problem, deshalb werden wir im Laufe der Analyse die NA Werte mit na.omit() entfernen.\n\n\n\n\nTabelle 60.3— Übersicht über die 23 Tiere mit den sechs Eigenschaften in den Spalten. Eine 1 bedeutet, dass die Eigenschaft vorliegt; eine 0 das die Eigenschaft nicht vorliegt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nanimal\nwarm_blooded\nfly\nvertebrate\nthreatened\nlive_in_groups\nhair\n\n\n\n\nant\n0\n0\n0\n0\n1\n0\n\n\nbee\n0\n1\n0\n0\n1\n1\n\n\ncat\n1\n0\n1\n0\n0\n1\n\n\ncentipede\n0\n0\n0\n0\n0\n1\n\n\nchimpanzee\n1\n0\n1\n1\n1\n1\n\n\ncow\n1\n0\n1\n0\n1\n1\n\n\ndolphin\n1\n0\n1\n1\n1\n0\n\n\nduck\n1\n1\n1\n0\n1\n0\n\n\neagle\n1\n1\n1\n1\n0\n0\n\n\nearthworm\n0\n0\n0\n0\n0\n0\n\n\nelephant\n1\n0\n1\n1\n1\n0\n\n\nfly\n0\n1\n0\n0\n0\n0\n\n\nfrog\n0\n0\n1\n1\nNA\n0\n\n\nherring\n0\n0\n1\n0\n1\n0\n\n\nhorse\n1\n0\n1\n0\n1\n1\n\n\nhuman\n1\n0\n1\n1\n1\n1\n\n\nlion\n1\n0\n1\nNA\n1\n1\n\n\nlizard\n0\n0\n1\n0\n0\n0\n\n\nlobster\n0\n0\n0\n0\nNA\n0\n\n\nrabbit\n1\n0\n1\n0\n1\n1\n\n\nsalmon\n0\n0\n1\n0\nNA\n0\n\n\nspider\n0\n0\n0\nNA\n0\n1\n\n\nwhale\n1\n0\n1\n1\n1\n0\n\n\n\n\n\n\n\n\nDer Tierdatensatz ist schön, da wir es hier nur mit 0/1 Werten zu tun haben. Wir werden später in dem preprocessing der Daten sehen, dass wir alle Spalten in der gleichen Spannweite der Werte wollen. Das klingt immer etwas kryptisch, aber der nächste Datensatz über verschiedene Kreaturen macht es deutlicher.\n\n\nEine andere Art die Daten zu Gruppieren kannst du im Tutorium Clustering Creatures nochmal nachvollziehen.\nIm Folgenen einmal der Datensatz, den wir dann in der gleichen Exceldatei finden nur eben auf dem zweiten Tabellenblatt. Wir reinigen noch die Namen und setzen die creature-Spalte auf Klein geschrieben. Wie du siehst, haben wir dann nur 15 Kreaturen und drei Spalten mit dem Gewicht, der Herzrate und dem maximalen möglichen Alter.\n\ncreature_tbl &lt;- read_excel(\"data/cluster_animal.xlsx\", sheet = 2) |&gt; \n  clean_names() |&gt; \n  mutate(creature = tolower(creature))\n\nIn der Tabelle 60.4 sehen wir nochmal die Daten dargestellt und hier erkennst du auch gut, wo das Problem liegt. Die Masse der Tiere reicht von \\(6g\\) beim Hamster bis \\(120000000g\\) beim Wal. Diese Spannweiten in einer Spalte und zwischen den Spalten führt dann zu Problemen bei den Algorithmen. Deshalb müssen wir hier Daten nochmal normalisieren oder aber standardisieren. Je nachdem was da besser passt.\n\n\n\n\nTabelle 60.4— Übersicht über die 15 Kreaturen mit den drei Eigenschaften in den Spalten. Wir haben hier sehr große Unterschiede in den Datenwerten. Daher müssen wir vor dem Clustern nochmal normalisieren.\n\n\n\n\n\n\ncreature\nmass_grams\nheart_rate_bpm\nlongevity_years\n\n\n\n\nhuman\n9.0e+04\n60\n70\n\n\ncat\n2.0e+03\n150\n15\n\n\nsmall dog\n2.0e+03\n100\n10\n\n\nmedium dog\n5.0e+03\n90\n15\n\n\nlarge dog\n8.0e+03\n75\n17\n\n\nhamster\n6.0e+01\n450\n3\n\n\nchicken\n1.5e+03\n275\n15\n\n\nmonkey\n5.0e+03\n190\n15\n\n\nhorse\n1.2e+06\n44\n40\n\n\ncow\n8.0e+05\n65\n22\n\n\npig\n1.5e+05\n70\n25\n\n\nrabbit\n1.0e+03\n205\n9\n\n\nelephant\n5.0e+06\n30\n70\n\n\ngiraffe\n9.0e+05\n65\n20\n\n\nlarge whale\n1.2e+08\n20\n80\n\n\n\n\n\n\n\n\n\n\n60.3.2 Standardisieren\nDie Standardisierung zwingt Variablen in eine \\(\\mathcal{N(0,1)}\\) Standardnormalverteilung. Das heißt, wir transformieren alle Variablen auf einen Mittelwert von \\(0\\) und einer Standardabweichung von \\(1\\). Hier nochmal die Formel für die Standardisierung oder auch \\(z\\)-Transformation. Achtung, wir müssen unsere Daten standatdisieren, damit wir auf jeden Fall die Graden durch den Ursprung zeichnen können. Wir kriegen ja bei der Eigenwertzerlegung und der Hauptkomponentenanalyse nur die Stiegungen wieder und damit muss alles auf den Ursprung normiert werden.\n\\[\nz = \\cfrac{x_i - \\bar{x}}{s_x}\n\\]\nWie wir hier sehen ziehen wir von jeder \\(i\\)-ten Beobachtung den Mittlwert von allen Beobachtungen ab. Dann teilen wir noch durch die Standardabweichung alle Beobachtungen. Am Ende ist dann damit unser Mittelwert auf \\(0\\) und unsere Standardabweichung auf \\(1\\). Unsere Daten haben damit den Schwerpunkt bei der \\(0\\) und unsere Geraden der Hauptkomponentenanalyse laufen immer durch den Ursprung.\nDie Standardisierung macht dann auch die Daten sehr schon gleichförmig. Hier nutzen wir auch die Funktion transform() aus dem R Paket {dlookr} mit der Option zscore. Damit wir auch auf jeden Fall sicher gehen, dass wir die richtige Funktion nutzen, schreiben wir dlookr::transform() und damit ist sichergestellt, dass wir auch die Funktion transform() aus dem R Paket {dlookr} nutzen.\n\nstd_creature_tbl &lt;- creature_tbl |&gt; \n  mutate(mass_grams = dlookr::transform(mass_grams, \"zscore\"),\n         heart_rate_bpm = dlookr::transform(heart_rate_bpm, \"zscore\"),\n         longevity_years = dlookr::transform(longevity_years, \"zscore\")) \n\nRegistered S3 methods overwritten by 'dlookr':\n  method          from  \n  plot.transform  scales\n  print.transform scales\n\nstd_creature_tbl\n\n# A tibble: 15 × 4\n   creature    mass_grams heart_rate_bpm longevity_years\n   &lt;chr&gt;       &lt;transfrm&gt; &lt;transfrm&gt;     &lt;transfrm&gt;     \n 1 human       -0.2739558 -0.5739180      1.6794647     \n 2 cat         -0.2768074  0.2094888     -0.5409814     \n 3 small dog   -0.2768074 -0.2257372     -0.7428402     \n 4 medium dog  -0.2767102 -0.3127824     -0.5409814     \n 5 large dog   -0.2766129 -0.4433502     -0.4602379     \n 6 hamster     -0.2768702  2.8208449     -1.0254424     \n 7 chicken     -0.2768236  1.2975538     -0.5409814     \n 8 monkey      -0.2767102  0.5576696     -0.5409814     \n 9 horse       -0.2379870 -0.7131904      0.4683123     \n10 cow         -0.2509487 -0.5303954     -0.2583792     \n11 pig         -0.2720115 -0.4868728     -0.1372639     \n12 rabbit      -0.2768398  0.6882374     -0.7832119     \n13 elephant    -0.1148507 -0.8350536      1.6794647     \n14 giraffe     -0.2477083 -0.5303954     -0.3391227     \n15 large whale  3.6116436 -0.9220988      2.0831822     \n\n\nDie Funktion PCA(), die wir im Folgenden verwenden wollen, wird zwar auch die Daten intern von sich aus standardisieren, wenn die nicht standardisiert wurden. Ich mag es aber nicht, wenn wichtige Schritte in Funktionen begraben werden, deshalb hier nochmal das Beispiel, wie man es macht. Doppelt Standardisieren tut aber nicht weh, schon standardiserte Daten ändern sich nicht durch eine weitere Standardisierung.\n\n\n60.3.3 Das data.frame() Problem\nLeider ist es so, dass fast alle Pakete im Kontext der Hauptkomponentenanalyse mit den Zeilennamen bzw. row.names() eines data.frame() arbeiten. Das hat den Grund, dass wir gut das Label in den Zeilennamen parken können, ohne das uns eine Spalte in den Auswertungen stört. Meistens ist das Label ja ein character und soll gar nicht in die Hauptkomponentenanalyse mit rein. Deshalb müssen wir hier einmal unsere tibble() in einen data.frame() umwandeln. Die tibble() haben aus gutem Grund keine Zeilennamen, die Zeilennamen sind ein Ärgernis und Quelle von Fehlern und aus gutem Grund nicht in einem tibble() drin. Hier brauchen wir die Zeilennamen aber.\nWir bauen uns also einmal einen data.frame() für unseren Tierdatensatz und setzen die Tiernamen als Zeilennamen bzw. row.names(). Wir entfernen dann auch noch schnell alle fehlenden Werte, denn wir wollen uns hier nicht noch mit der Imputation von fehlenden Werten beschäftigen.\n\nanimals_df &lt;- animals_tbl |&gt; \n  na.omit() |&gt; \n  as.data.frame() |&gt; \n  column_to_rownames(\"animal\") \n\nDas Ganze machen wir dann auch noch einmal für die normalisierten Kreaturendaten. Wir wollen dann ja nur auf den normalisierten Daten weitermachen.\n\nstd_creature_df &lt;- std_creature_tbl |&gt; \n  as.data.frame() |&gt; \n  column_to_rownames(\"creature\")",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Multivariate Verfahren</span>"
    ]
  },
  {
    "objectID": "stat-modeling-pca.html#hauptkomponentenanalyse",
    "href": "stat-modeling-pca.html#hauptkomponentenanalyse",
    "title": "60  Multivariate Verfahren",
    "section": "60.4 Hauptkomponentenanalyse",
    "text": "60.4 Hauptkomponentenanalyse\nJetzt haben wir uns mit sehr viel Theorie beschäftigt und deshalb nochmal in der Abbildung 60.4 ein schematischer Überblick über den Ablauf der Hauptkomponentenanalyse. Wir nutzen die Daten und berechnen für die Daten die Varianz/Kovarianzmatrix. Auf der Varianz/Kovarianzmatrix rechnen wir dann die Hauptkomponentenanalyse. Beide Schritte sind intern in den Funktionen in R implementiert. Du musst nur auf die Daten achten. Dann können wir uns die Hauptkomponenten und die Eigenwerte als Korrelationsplot anschauen und die Aussagekraft der Variablen sehen.\n\n\n\n\n\n\n\nAbbildung 60.4— Verlauf einer generalisierten Hauptkomponentenanalyse von dem Datensatz über die Varianz/Kovarianzmatrix zu den unkorrelierten Hauptkomponeten und deren Koordinaten unabhängig von den Eigenschaften der Variablen. Abschließend sind die Koordinaten der Hauptkomponenten im Koorelationsplot der 1. und 2. Hauptkomponente dargestellt. Variablen die nahe beieinander sind, sind positiv miteinander korreliert; Variablen in den gegenüberliegenden Quadranten naegativ. Je weiter eine Variable außen liegt, desto höher ist die Aussagekraft. Variablen nahe dem Ursprung, haben eine geringe Bedeutung. Am Beispiel der Variable fly werden nochmal die Koordinaten deutlich.\n\n\n\n\nEs gibt viele Implementierungen der Hauptkomponentenanalyse in R. Wir nutzen die Funktion PCA() aus dem R Paket {FactoMineR} für die Analyse von kontinuierlichen Variablen. Denk immer daran, es kann maximal nur so viele Hauptkomponenten geben, wie wir auch Spalten in den Daten vorliegen haben. Folgende wichtige Funktionen werden wir jetzt einmal nutzen. Es gibt noch mehr, aber das übersteigt die einfache Hauptkomponentenanalyse, die wir uns hier anschauen wollen.\nDie zentrale Funktion ist aber die Funktion PCA() womit wir die eigentliche Hauptkomponentenanalyse erstmal durchführen, wenn kontinuierliche Variablen vorliegen. Die Funktion nimmt die Daten und berechnet dann intern die Varianz/Kovarianzmatrix. Die Varianz/Kovarianzmatrix wird dann als Distanzmatrix genutzt um die Hauptkomponenten und deren Eigenwerte zu berechnen.\nNeben der Funktion PCA() aus dem R Paket {FactoMineR} für die Analyse von kontinuierlichen Variablen gibt es noch eine Reihe anderer Funktionen für eine Hauptkomponentenanalyse wenn wir kategoriale Variablen oder eine Mischung vorliegen haben. Hier mal die vollständige Liste.\n\nPCA - Principal Component Analysis, zur Analyse eines Datensatzes mit kontinuierlichen Variablen\nCA - Correspondence Analysis (Korrespondenzanalyse), für die Analyse der Assoziation zwischen zwei kategorialen Variablen.\nMCA - Multiple Correspondence Analysis, für die Analyse eines Datensatzes mit kategorialen Variablen.\nFAMD - Factor Analysis of Mixed Data (Faktorenanalyse gemischter Daten), für die Analyse eines Datensatzes, der sowohl kontinuierliche als auch kategoriale Variablen enthält.\nMFA - Multiple Factor Analysis, für die Analyse eines Datensatzes, der in Gruppen strukturierte Variablen enthält.\n\n\n60.4.1 PCA - Principal Component Analysis\nBei der Nutzung der Funktion PCA() ist für mich wichtig, dass wir nicht gleich irgendwelche Abbildungen erhalten, deshalb ist hier im Skript graph = FALSE gewählt. Mit der Option scale.unit = TRUE musst du die Daten selber nicht standardisieren sondern die Funktion PCA() macht das für dich. Manchmal sind Fragebögen sehr groß mit mehr als Dutzenden von Fragen, da macht es Sinn sich nicht die Anzahl an Fragen als Hauptkomponenten wiedergeben zu lassen. Die Option ncp = 5 zum Beispiel schränkt hier die Anzahl auf 5 Hauptkomponenten ein. Bei den Kreaturen macht es keinen Sinn, da erhalten wir natürlich nur drei Hauptkomponenten, da wir nur drei Spalten in dem Datensatz haben.\n\npca_animals &lt;- PCA(animals_df, scale.unit = TRUE, \n                   ncp = 5, graph = FALSE)\n\n\npca_creature &lt;- PCA(std_creature_df, scale.unit = TRUE, \n                    ncp = 5, graph = FALSE)\n\nFolgende Funktionen sind wichtig um die klassische Hauptkomponentenanalyse einmal zu visualisieren und zu bewerten. Die Namen folgen alle einem Schema im Paket {FactoMineR}, so dass wir hier nur Kleinigkeiten im Namen ändern müssen um alle Funktionen zu nutzen.\n\nget_pca_ind(), get_pca_var(): Extrahiert die Ergebnisse für Individuen bzw. Variablen.\nfviz_pca_ind(), fviz_pca_var(): Visualisierung der Ergebnisse für Individuen bzw. Variablen.\nfviz_pca_biplot(): Erstellt einen Biplot der Individuen und Variablen.\n\n\n\n60.4.2 CA - Correspondence Analysis\n\n\n\n\n\n\nSpezialfall des \\(\\mathcal{X}^2\\)-Test\n\n\n\nDie CA-Analyse funktioniert nur auf einer \\(n \\times k\\) Tabelle. Daher können wir nur zwei Variablen mit Kategorien miteinander in Verbindung setzen. Eigentlich sehen wir hier eher einen großen \\(\\mathcal{X}^2\\)-Test. Deshalb gehe ich aktuell nicht auf die Lösung hier tiefer ein.\n\n\nWir nutzen die Correspondence Analysis zur Analyse des Zusammenhangs zwischen zwei kategorialen Variablen. Also eher ein großer \\(\\mathcal{X}^2\\)-Test mit anderen Mitteln auf einer \\(n \\times k\\) Tabelle.\n\nCA(tbl, ncp = 5, graph = FALSE)\n\nWir erhalten alle wichtigen Informationen aus einer Correspondence Analysis mit den folgenden Funktionen.\n\nget_ca_row(), get_ca_col(): Extrahiert die Ergebnisse für Zeilen bzw. Spalten.\nfviz_ca_row(), fviz_ca_col(): Visualisierung der Ergebnisse für Zeilen bzw. Spalten.\nfviz_ca_biplot(): Erstellt einen Biplot der Individuen und Variablen.\n\nWir betrachteten den Fall hier nicht weiter, da er wirklich sehr speziell ist und nur selten vorkommt.\n\n\n60.4.3 MCA - Multiple Correspondence Analysis\nWir nutzen die Multiple Correspondence Analysis für die Analyse eines Datensatzes mit kategorialen Variablen.\n\nMCA(tbl, ncp = 5, graph = FALSE)\n\nWir erhalten dann wiederum alle wichtigen Informationen und Abbildungen über folgende Funktionen.\n\nget_mca_ind(), get_mca_var(): Extrahiert die Ergebnisse für Individuen bzw. Variablen.\nfviz_mca_ind(), fviz_mca_var(): Visualisierung der Ergebnisse für Individuen bzw. Variablen.\nfviz_mca_biplot(): Erstellt einen Biplot der Individuen und Variablen.\n\n\n\n60.4.4 FAMD - Factor Analysis of Mixed Data\nFür die Analyse eines Datensatzes, der sowohl kontinuierlichen Variablen als auch kategorialen Variablen enthält nutzen wir die Factor Analysis of Mixed Data.\n\nFAMD(tbl, ncp = 5, graph = FALSE)\n\nAuch hier haben wir die wichtigen Funktionen für die Darstellung einmal dargestellt.\n\nget_famd_ind(), get_famd_var(): Extrahiert die Ergebnisse für Individuen bzw. Variablen.\nfviz_famd_ind(), fviz_famd_var(): Visualisierung der Ergebnisse für Individuen bzw. Variablen.\n\n\n\n60.4.5 MFA - Multiple Factor Analysis\nKomplexer wird es, wenn wir eine Multiple Factor Analysis durchführen wollen. In diesem Fall ist es die Analyse eines Datensatzes, der in Gruppen strukturierte Variablen enthält.\n\nMCA(X, ncp = 5, graph = FALSE)\n\nAuch gibt es bei der Multiple Factor Analysis ein Set an Funktionen, die wir hauptsählcihc benötigen.\n\nget_mfa_ind(), get_mfa_var(): Extrahiert die Ergebnisse für Individuen bzw. Variablen.\nfviz_mfa_ind(), fviz_mfa_var(): Visualisierung der Ergebnisse für Individuen bzw. Variablen.\n\nWeitere Funktionen finden sich dann noch auf der Tutoriumsseite zu der MFA - Multiple Factor Analysis.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Multivariate Verfahren</span>"
    ]
  },
  {
    "objectID": "stat-modeling-pca.html#darstellung-der-ergebnisse",
    "href": "stat-modeling-pca.html#darstellung-der-ergebnisse",
    "title": "60  Multivariate Verfahren",
    "section": "60.5 Darstellung der Ergebnisse",
    "text": "60.5 Darstellung der Ergebnisse\nIm Folgenden betrachten wir dann die Hauptkomponentenanalyse auf drei Ebenen. Wir konzentrieren uns hier erstmal nur auf die Beispiele aus der Funktion PCA(). Prinzipiell sind die Analysen für die anderen Funktionen ähnlich und die Interpretation unterscheidet sich nur leicht.\n\nAuf der Ebene der Eigenwerte: Wir entscheiden, wie viele Hauptkomponenten wir eigentlich in die weitere Analyse nehmen wollen.\nAuf der Ebene der Variablen: Wir betrachten die Spalten und schauen wie sich die Spalten zueinander und den Beobachtungen verhalten. In der R Welt enden dann die Funktionen *_var.\nAuf der Eben der Individuen: Wir schauen uns einmal die Zeilen an und versuchen zu verstehen, wie sich die einzelenen Beobachtungen oder Individuen verhalten. Gibt es hier Auffälligkeiten? In der R Welt enden dann die Funktionen *_ind.\n\n\n60.5.1 Ebene der Eigenwerte\nDie Eigenwerte messen die Menge der von jeder Hauptkomponente beibehaltenen Variation. Damit repräsentieren die Eigenwerte die Varianz/Kovarianzmatrix der Daten. Die Eigenwerte sind für die ersten Hauptkomponenten grundsätzlich groß und für die nachfolgenden Hauptkomponente immer kleiner. Das heißt, die ersten Hauptkomponente beschreiben Variablen mit der größten Variation im Datensatz. Wie schon gesagt, wir erschaffen neue Variablen aus den Daten, die wir Hauptkomponenten nennen. Jede Hauptkomponente hat einen Eigenwert, der beschreibt, wie viel Varianz die Hauptkomponente in den Daten erklären kann.\nMit Hilfe der Eigenwerte lässt sich die Anzahl der Hauptkomponenten bzw. Dimensionen bestimmen, die nach der PCA beibehalten werden sollen. Wir wollen selten alle Hauptkomponenten berücksichtigen. Es geht hier ja auch darum die Dimensionen der Daten zu reduzieren. Wenn wir alle Hauptkomponenten weiterverwenden würden, dann könnten wir auch den ursprünglichen Datensatz nutzen. Die Eigenwerte und der Anteil der Varianzen, die von den Hauptkomponenten beibehalten werden, können mit folgenden Funktionen extrahiert werden.\n\nget_eigenvalue(): Extrahiert die Eigenwerte/Varianzen der Hauptkomponenten\nfviz_eig(): Visualisierung der Eigenwerte\n\nDabei bedeutet ein Eigenwert &gt; 1, dass die Hauptkomponenten mehr Varianz erklären als eine der ursprünglichen Variablen in den standardisierten Daten. Dies wird üblicherweise als Grenzwert für die Beibehaltung der Hauptkomponenten verwendet. Dies trifft nur zu, wenn die Daten standardisiert sind.\n\neig_animals &lt;- get_eigenvalue(pca_animals) |&gt; \n  as_tibble()\neig_animals \n\n# A tibble: 6 × 3\n  eigenvalue variance.percent cumulative.variance.percent\n       &lt;dbl&gt;            &lt;dbl&gt;                       &lt;dbl&gt;\n1      2.54             42.3                         42.3\n2      1.24             20.6                         62.9\n3      0.848            14.1                         77.1\n4      0.723            12.1                         89.1\n5      0.515             8.58                        97.7\n6      0.137             2.29                       100  \n\n\nDie Summe aller Eigenwerte für die Tiere ergibt eine Gesamtvarianz von 6. Jetzt können wir ganz einfach den Anteil der erklärten Varianz von jedem Eigenwert berechnen. In der zweiten Spalte finden wir dann die Werte der Eigenwerte geteilt durch die Gesamtvarianz. Daher ist \\(42.35\\) gleich \\(2.54\\) geteilt durch \\(6\\). Der kumulative Prozentsatz der erklärten Variation wird durch Addition der aufeinander folgenden Anteile der erklärten Variation ermittelt.\n\neig_creature &lt;- get_eigenvalue(pca_creature) |&gt; \n  as_tibble()\neig_creature\n\n# A tibble: 3 × 3\n  eigenvalue variance.percent cumulative.variance.percent\n       &lt;dbl&gt;            &lt;dbl&gt;                       &lt;dbl&gt;\n1      1.99             66.2                         66.2\n2      0.730            24.3                         90.5\n3      0.285             9.49                       100  \n\n\nDie Summe aller Eigenwerte für die Kreaturen ergibt eine Gesamtvarianz von 3. Damit können wir dann auch einfach die anderen Werte in den Spalten nachvollziehen.\nJetzt stellt sich natürlich die Frage, wie viele der Hauptkomponenten sollen den jetzt zukünftig berücksichtigt werden? Leider gibt es keine allgemein anerkannte objektive Methode, um zu entscheiden, wie viele Hauptkomponenten ausreichend sind. Dies hängt von dem jeweiligen Anwendungsbereich und dem jeweiligen Datensatz ab. In der Praxis neigen wir dazu, die ersten paar Hauptkomponenten zu betrachten, um interessante Muster in den Daten zu finden. Dafür nutzen wir den Scree Plot und entscheiden anhand der Beuge in dem Plot. Wenn wir nur wenige Variablen in den Daten haben, dann kann es sein, dass wir nur wenige Hauptkomponenten raus schmeißen. In der Abbildung 60.5 siehst du einmal die beiden Scree Plots für die beiden Datensätze.\n\nfviz_eig(pca_animals, addlabels = TRUE)\nfviz_eig(pca_creature, addlabels = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.5— Eigenwerte für die beiden Datensätze.\n\n\n\n\nWir sehen für die Tierdaten, dass die erste Hauptkomponente gut 42% der Varianz in den Daten erklärt, die folgende Hauptkomponente dann nur noch 20% und so weiter. Hier sehen wir dann auch die Beuge und könnten schließen, dass die ersten beiden Hauptkomponenten ausreichen um den Datensatz zu beschreiben. Bei den Kreaturendaten sieht es so aus, als ob wir die Datan alleinig mit der ersten Hauptkomponente erklären könnten. Wir sehen ja, dass gut 62% der Varianz durch die erste Hauptkomponente erklärt wird.\n\n\n60.5.2 Ebene der Variablen\n\n\n\n\n\n\nget_*_var() für andere Funktionen aus FactoMineR\n\n\n\nDu kannst einfach das * in get_*_var() durch den klein geschriebenen Funktionaufruf von MCA, FAMD und MFA ersetzen um dir dann auch die Ergebnisse analog zu der PCA herausgeben zu lassen. Die Interpretation ist ähnlich bis gleich.\n\n\nSchauen wir jetzt einmal die Informationen der Variablen also Spalten der Hauptkomponentenanalyse an. Wir erhalten jetzt also die Informationen zu den einzelnen Hauptkomponenten, die ja die Variablen bzw. Spalten der Daten repräsentieren. Wir erhalten die Informationen über die Funktion get_pca_var().\n\nvar_animals &lt;- get_pca_var(pca_animals)\nvar_creature &lt;- get_pca_var(pca_creature)\nvar_creature\n\nPrincipal Component Analysis Results for variables\n ===================================================\n  Name       Description                                    \n1 \"$coord\"   \"Coordinates for the variables\"                \n2 \"$cor\"     \"Correlations between variables and dimensions\"\n3 \"$cos2\"    \"Cos2 for the variables\"                       \n4 \"$contrib\" \"contributions of the variables\"               \n\n\nSchauen wir uns einmal an was wir mit den Informationen über die Variablen durch die Funktion get_pca_var() machen können.\n\ncoord: Koordinaten der Variablen zur Erstellung eines Streudiagramms. Wir nutzen meistens nur die ersten beiden Hauptkomponenten, da wir sonst kein zweidimensionalen Scatterplot machen können.\ncor: Die Korrelation zwischen den ursprünglichen Variablenwerten in den Zeilen und den neuen Hauptkomponenten in den Spalten.\ncos2: stellt die Qualität der Darstellung der Variablen auf der Faktorkarte dar. Es wird berechnet als die quadrierten Koordinaten: cos2 = coord * coord\ncontrib: enthält die Beiträge (in Prozent) der Variablen zu den Hauptkomponenten. Der Beitrag einer Variablen zu einer bestimmten Hauptkomponente ist (in Prozent): (cos2 * 100) / (gesamter cos2 der Komponente).\n\nIm Folgenden bewerten wir die Qualität einer Variable nach den cos2-Werten oder ihren Beitragswerten zu den Hauptkomponenten dargestellt durch die contrib-Werte. Wir nutzen hier jetzt aber erstmal die cos2-Werte, weil sonst hier alles an Abbildungen explodiert.\n\nEin hoher cos2-Wert deutet auf eine gute Darstellung der Variablen auf der Hauptkomponente hin. In diesem Fall ist die Variable nahe am Umfang des Korrelationskreises positioniert.\nEin niedriger cos2-Wert zeigt an, dass die Variable nicht perfekt durch die PC’s repräsentiert ist. In diesem Fall befindet sich die Variable in der Nähe der Kreismitte.\nFür eine bestimmte Variable ist die Summe des cos2 aller Hauptkomponenten gleich eins.\n\nIn der Abbildung 60.6 sehen wir einmal die Zusammenhänge von den cos2-Werten und den jeweiligen Hauptkomponenten. Wir setzen is.corr = FALSE, weil wir keine Korrelation abbiden wollen sondern cos2-Werte.\n\ncorrplot(var_animals$cos2, is.corr = FALSE)\ncorrplot(var_creature$cos2, is.corr = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.6— Zusammenhang von den cos2-Werten und den jeweiligen Hauptkomponenten\n\n\n\n\nWir sehen bei den Tierdaten, dass die die erste Hauptkomponente Dim.1 die Variablen warm_blooded, vertebrate und etwas weniger threatened und live_in_groups beschreibt. Die zweite Hauptkomponente Dim.2 repräsentiert die Variable hair. Die dritte Hauptkomponente deckt fly ab. Die vierte Hauptkomponente beschreibt dann live_groups etwas stärker als die erste Hauptkomponente. Die fünfte Hauptkomponente können wir fast ignorieren. Bei den Kreaturendaten sehen wir das die erste Hauptkomponente fast alle Variablen repräsentiert. Die zweite Hauptkomponente beschreibt dann noch etwas mass_grams und heart_rate_bpm.\nIn der Abbildung 60.7 sehen wir die gleichen Informationen nochmal als Säulendiagramm dargestellt. Hier ist es eventuell numerisch schöner und besser zu sehen, wie sich die cos2-Werte für die erste und zweite Hauptkomponente verhalten. Du musst hier nämlich entscheiden, wie viele Hauptkomponenten du zusammen betrachten willst. Hier habe ich über axes = 1:2 die ersten beiden Hauptkomponenten gewählt.\n\nfviz_cos2(pca_animals, choice = \"var\", axes = 1:2)\nfviz_cos2(pca_creature, choice = \"var\", axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.7— Zusammenhang von den cos2-Werten und die ersten beiden Hauptkomponenten Dim.1 und Dim.2.\n\n\n\n\nAls nächstes wollen wir uns dann mal die ersten beiden Hauptkomponenten Dim.1 und Dim.2 in einer Abbildung anschauen. In der Abbildung 60.8 sehen wir das Korrelationsdiagramm der Variablen für die ersten beiden Hauptkomponenten. Über die Option axes = kannst du dir auch andere Kombinationen von zwei Hauptkomponenten anschauen. Das Korrelationsdiagramm zeigt die Beziehungen zwischen allen Variablen und kann wie folgt interpretiert werden:\n\nPositiv korrelierte Variablen sind zusammen gruppiert.\nNegativ korrelierte Variablen befinden sich auf gegenüberliegenden Seiten des Ursprungs der Grafik oder auch den gegenüberliegende Quadranten.\nDer Abstand zwischen den Variablen und dem Ursprung misst die Qualität der Variablen im Bezug auf den erklärenden Anteil. Variablen, die vom Ursprung entfernt sind, sind bedeutender als Variablen nahe des Ursprungs.\n\nWenn eine Variable nur durch zwei Hauptkomponenten, also zum Beispiel die Dim.1 und die Dim., perfekt repräsentiert wird, ist die Summe des cos2 von diesen beiden Hauptkomponenten gleich eins. In diesem Fall werden die Variablen auf dem Korrelationskreis positioniert und der Pfeil geht damit vom Ursprung direkt zum Kreis. In Abbildung 60.8 (b) sind die Variablen heart_rate_bpm und mass_grams direkt auf dem Korrelationskreis. Daher sind diese beiden Variablen erklärend für einen Großteil der Varianz in den Daten. Für einige der Variablen sind möglicherweise mehr als 2 Komponenten erforderlich, um die Daten perfekt zu repräsentieren. In diesem Fall werden die Variablen innerhalb des Korrelationskreises positioniert, was wir eher in der Abbildung 60.8 (a) sehen. Variablen nahe des Zentrum haben keine Bedeutung für die beiden Hauptkomponenten. Je nach betrachteten Hauptkomponentenpaar kann es natürlich wieder anders aussehen.\n\nfviz_pca_var(pca_animals, repel = TRUE, axes = 1:2)\nfviz_pca_var(pca_creature, repel = TRUE, axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.8— Korrelationsdiagramm der Variablen.\n\n\n\n\nIm Weiteren können wir diee cos2-Werte verwenden um die Qualität der Darstellung abzuschätzen. Es ist möglich, Variablen nach ihren cos2-Werten zu färben, indem du das Argument col.var = \"cos2\" verwendest und damit einen Farbverlauf erzeugst. Die Option gradient.cols = definiert dabei den Farbverlauf.\nIn der Abbildung 60.9 siehst du nochmal die Einfärbung. Wie du sehen kannst, hat bei den Tierdaten warm_blooded den größten Einfluss auf die Aufteilung und erklärt auch am meisten Varianz der Daten. Die Variablen fly und live_in_groups helfen nicht um die Daten aufzutrennen und die Varianz zu erklären. Bei den Kreaturendaten sehen wir dann ebenfalls das es zwei Variablen gibt, die viel erklären und eine Variable mit einer schlechteren Qualität.\n\nfviz_pca_var(pca_animals, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE)\n\nfviz_pca_var(pca_creature, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.9— Korrelationsdiagramm der Variablen eingefärbt nach den cos2-Werten.\n\n\n\n\nWir können auch mit der kmeans() Funktion auch versuchen über die Koordinaten Gruppen zu bilden. Wir rechnen also auf den Koordinaten der Variablen eine Clusteranalyse. Faktisch könnten wir hier auch andere Clusteralgorithmen nehmen, aber ich nutze hier mal kmeans(). Im Prinzip kannst du den Algorithmus ändern, denn du brauchst später nur einen Faktor mit der Grupenzuordnung.\n\ngrp_animal_var &lt;- kmeans(var_animals$coord, centers = 3, nstart = 25) |&gt; \n  pluck(\"cluster\") |&gt; \n  as_factor()\n\nWir machen dann die Gruppenbildung auch für die Kreaturendaten. Hier dann nur zwei Gruppen, denn wir haben ja nur drei Variablen. Da machen dann drei Gruppen keinen Sinn.\n\ngrp_creature_var &lt;- kmeans(var_creature$coord, centers = 2, nstart = 25) |&gt; \n  pluck(\"cluster\") |&gt; \n  as_factor()\n\nDann nutzen wir die Gruppen einmal um nach diesen dann unseren Korrelaionsplot einzufärben. Wir sehen jetzt in diesen beiden Beipsielen in der Abbildung 60.10 kein so großen Änderungen oder Überraschungen. Aber je mehr Variablen du hast, desto spannender wird es dann auch.\n\nfviz_pca_var(pca_animals, col.var = grp_animal_var, \n             palette = c(\"#0073C2FF\", \"#EFC000FF\", \"#868686FF\"),\n             legend.title = \"Cluster\")\n\nfviz_pca_var(pca_creature, col.var = grp_creature_var, \n             palette = c(\"#0073C2FF\", \"#EFC000FF\"),\n             legend.title = \"Cluster\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.10— Korrelationsdiagramm der Variablen eingefärbt den Gruppen aus dem k-NN Algorithmus zur Clusterbildung.\n\n\n\n\nAbschließend wollen wir uns noch die Frage stellen, wie stark haben den jetzt die einzelnen Variablen zu den ersten beiden Hauptkomponenten beigetragen? Wir können dazu die Funktion fviz_contrib() nutzen. Die rote gestrichelte Linie in der Abbildung 60.11 zeigt den erwarteten durchschnittlichen Beitrag an. Wäre der Beitrag der Variablen bei den Tierdaten gleichmäßig, wäre der erwartete Wert \\(1/n_{var} = 1/6 = 16\\%\\). Eine Variable mit einem Beitrag, der über diesem Grenzwert liegt, sehen wir als wichtig für die betrachteten Hauptkomponenten an.\n\nfviz_contrib(pca_animals, choice = \"var\", axes = 1:2)\nfviz_contrib(pca_creature, choice = \"var\", axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.11— Beitrag der einzelnen Variablen zu den ersten beiden Hauptkomponenten.\n\n\n\n\n\n\n60.5.3 Ebene der Individuen\n\n\n\n\n\n\nget_*_ind() für andere Funktionen aus FactoMineR\n\n\n\nDu kannst einfach das * in get_*_ind() durch den klein geschriebenen Funktionaufruf von MCA, FAMD und MFA ersetzen um dir dann auch die Ergebnisse analog zu der PCA herausgeben zu lassen. Die Interpretation ist ähnlich bis gleich.\n\n\nNachdem wir uns die Informationen von den Variablen und damit die Informationen von den Spalten angeschaut haben, wollen wir uns nochmal die Informationen aus den Zeilen, also den Individuen anschauen. Auch hier können wir ja einfach mal schauen, wie die Individuen untereinander zusammenhängen. Wir arbeiten jetzt also mit den Informationen zu den einzelnen Individuen. Wir erhalten die Informationen über die Funktion get_pca_var().\n\nind_animals &lt;- get_pca_ind(pca_animals)\nind_creature &lt;- get_pca_ind(pca_creature)\nind_creature\n\nPrincipal Component Analysis Results for individuals\n ===================================================\n  Name       Description                       \n1 \"$coord\"   \"Coordinates for the individuals\" \n2 \"$cos2\"    \"Cos2 for the individuals\"        \n3 \"$contrib\" \"contributions of the individuals\"\n\n\nNachdem wir die Informationen zu den Individuen extrahiert haben, können wir uns die Koordinaten der einzelnen Individuen, die sich aus der Varianz/Kovarianzmatrix der Individuen ergeben, einmal in der Abbildung 60.12 anschauen. Wir interpretieren die cos2-Werte analog zu den Variablen. Das heißt, cos2-Werte geben an, wie bedeutend ein Individuum für die beiden Hauptkomponenten Dim.1 und Dim.2 ist. Wie sehen bei den Tierdaten, dass der Hering nichts zur Aufspaltung der Daten beiträgt. Die Tiere die weiter Außen liegen, haben mehr Informationen. Bei den Kreaturendaten sehen wir klar, dass der Hamster und der Wal nicht zu den anderen Tieren passen will. Beide Tiere pressen förmlich die anderen Tiere in den unteren linken Quadranten zusammen.\n\nfviz_pca_ind(pca_animals,\n             col.ind = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE) +\n  scale_x_continuous(expand = expansion(add = c(0.5, 1))) +\n  scale_y_continuous(expand = expansion(add = c(0.5, 0.5))) \n\nfviz_pca_ind(pca_creature,\n             col.ind = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE) +\n  scale_x_continuous(expand = expansion(add = c(0.5, 1))) +\n  scale_y_continuous(expand = expansion(add = c(0.5, 0.5))) \n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.12— Streudiagramm der individuellen Beobachtungen für die ersten beiden Hauptkomponenten.\n\n\n\n\nWir können uns dann auch recht schnelle einmal die meisten bedeutenden Individuen für die ersten beiden Hauptkomponenten numerisch in der Abbildung 60.13 ansehen. Achtung, wir haben hier einige Individuen entfernt, die fehlende Werte in den Zeielne hatten. Da hätten wir vorher mal reagieren sollen, aber wir machen hier ja keine Imputation von fehlenden Werten. Die rote gestrichelte Linie zeigt den erwarteten durchschnittlichen Beitrag an. Wäre der Beitrag der Variablen gleichmäßig, wäre der erwartete Wert \\(1/n_{ind}\\). Für die ersten beiden Hauptkomponenten kann ein Individuum mit einem Beitrag, der über diesem Grenzwert liegt, als wichtig angesehen werden.\nAuch hier sehen wir, dass die Fliege und der Tausendfüßler den größten Einfluss haben. Bei den Kreaturendaten ist es noch auffälliger, da liegt die gesamte Information der Daten in dem Wal und dem Hamster. Der Rest der Daten ist dagegen recht homogen. Hier siehst du auch, wie man die Hauptkomponentenanalyse zum Detektieren von Ausreißern nutzen kannst.\n\nfviz_contrib(pca_animals, choice = \"ind\", axes = 1:2)\nfviz_contrib(pca_creature, choice = \"ind\", axes = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.13— Beitrag der einzelnen Variablen zu den ersten beiden Hauptkomponenten.\n\n\n\n\nWie schon bei den Variablen können wir auch über das Koordinatensystem der Individuen Gruppen bilden. Wir rechnen auch hier eine Clusteranalyse mit der Funktion kmeans(). Wie auch schon vorab, kannst du auch hier dir einen anderen Clusteralgorithmus auswählen um die Gruppen zu bilden. Wir bleiben hier aber bei einem k-NN Algorithmus der uns einen Faktor mit der Gruppenzugehörigkeit wiedergibt.\n\ngrp_animal_ind &lt;- kmeans(ind_animals$coord, centers = 3, nstart = 25) |&gt; \n  pluck(\"cluster\") |&gt; \n  as_factor()\n\nDas Ganze dann auch nochmal für den Kreaturendatensatz. Hier können wir dann wiederum mehr als zwei Cluster wählen, da wir ja sehr viele Individuen haben, die wir in Gruppen einteilen können. Die Anzahl an Gruppen ist hier mit k = 4 eher zufällig gewählt.\n\ngrp_creature_ind &lt;- kmeans(ind_creature$coord, centers = 4, nstart = 25) |&gt; \n  pluck(\"cluster\") |&gt; \n  as_factor()\n\nIn der Abbildung 60.14 siehst du die Gruppen aus dm k-NN Algorithmus eingefärbt und Ellipsen um die Gruppen gezeichnet. Mit der Option ellipse.type = \"confidence\" kannst du dir auch Ellipsen mit dem Konfidenzbereich anzeigen lassen. Wir bleiben hier aber mal bei dem Standard der Funktion, der auch sonst genutzt wird. Ich habe mich dann noch dazu entschieden den Mittelpunkt der Ellipse mit mean.point = FALSE nicht anzeigen zu lassen. Wir können auch das geom.ind zu text ändern, dann werden nicht die Punkte sondern die Label der Individuen angezeigt.\n\nfviz_pca_ind(pca_animals,\n             geom.ind = \"point\",\n             col.ind = grp_animal_ind, \n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             addEllipses = TRUE, \n             legend.title = \"Groups\", mean.point = FALSE)\n\n\nfviz_pca_ind(pca_creature,\n             geom.ind = \"point\", \n             col.ind = grp_creature_ind, \n             palette = c(\"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"),\n             addEllipses = TRUE, \n             legend.title = \"Groups\", mean.point = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.14— Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix.\n\n\n\n\nTja, sieht dann etwas wild aus. Aber da musst du dann eben mit der Anzahl der Gruppen aus dem k-NN Algorithmus spielen. Auch siehst du, dass einzelne extreme Beobachtungen dir alles zunichte machen können. Hier müsste man überlegen, ob nicht der Wal und der Hamster bei den Kreaturendaten nicht eins zu viel sind. Jetzt stehen wir vor der Frage, passt das so? Was sind den die einzelnen Punkt für Beobachtungen? Da hilft uns dann die Funktion fviz_pca_biplot() weiter. Nicht alle Methoden der Hauptkomponentenanalyse können auch einen Biplot generieren, aber wenn, dann ist der Biplot sehr nützlich. In der Abbildung 60.15 siehst du den Biplot für die Tierdaten sowie für die Kreaturendaten sowie die Clusterinformationen ergänzt.\n\nfviz_pca_biplot(pca_animals, \n                col.ind = grp_animal_ind, \n                col.var = grp_animal_var,\n                addEllipses = TRUE, \n                palette = \"jco\",\n                label = c(\"var\", \"ind\"),\n                repel = TRUE,\n                legend.title = \"Species\", mean.point = FALSE) +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) \n\nfviz_pca_biplot(pca_creature, \n                col.ind = grp_creature_ind, \n                col.var = grp_creature_var,\n                addEllipses = TRUE, \n                label = c(\"var\", \"ind\"),\n                repel = TRUE,\n                legend.title = \"Species\", mean.point = FALSE) +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = c(\"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"))\n\n\n\n\n\n\n\n\n\n\n\n(a) Tierdaten animals_df\n\n\n\n\n\n\n\n\n\n\n\n(b) Kreaturendaten std_creature_df\n\n\n\n\n\n\n\nAbbildung 60.15— Biplot für die Individuen und Variablen eingefäerbt nach Clustern.\n\n\n\n\nBei den Tierdaten passe es eigentlich ganz gut mit den Clustern, aber die Inhalte machen teilweise nicht so viel Sinn. Wir haben Tiere zusammen gruppiert, die eigentlich nicht so richtig zusammen passsen. Zwar sind in der einen Gruppe die Insekten aber der Elefant ist zusammen mit dem Wal, dem Delphin und dem Adler sowie der Ente gruppiert. Da reichen anscheinend die Daten und Variablen noch nicht aus um hier eine gute Trennung hinzukriegen. Immerhin passt es auch mit den Säugetieren. Du siehst hier auch gut, wie die Vektoren in die Richtung der passenden Individuen zeigen. So sind die haarigen Individuen dann unten rechts. Das passt ganz gut. Bei den Kreaturendaten siehst du auch schön, wie der Wal die Variable mass_grams nach oben rechst zu sich zieht. Auch die Variable der Lebendauer zieht den Elefanten und den Menschen nach rechts weg.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Multivariate Verfahren</span>"
    ]
  },
  {
    "objectID": "stat-modeling-pca.html#multi-dimensional-scaling-mds",
    "href": "stat-modeling-pca.html#multi-dimensional-scaling-mds",
    "title": "60  Multivariate Verfahren",
    "section": "60.6 Multi Dimensional Scaling (MDS)",
    "text": "60.6 Multi Dimensional Scaling (MDS)\nEine besondere Form der Hauptkomponentenanalyse ist das Multidimensional Scaling (abk. MDS). Im Prinzip sind die Mechanismen sehr ähnlich. Der Hauptunterschied ist aber, das wir für die MDS eine Distanzmatrix benötigen. Wir können dafür die Funktion dist() oder as.dist() nehmen, wenn wir schon Distanzen vorliegen haben. Nehmen wir als plakatives Beispiel einmal die Distanzen von europäischen Städten zueinander. Wir haben die Daten in der Exceldatei distance.xlsx vorliegen. Wir lesen die Daten einmal ein und schauen uns die ersten fünf Spalten und die ersten fünf Zeilen des Datensatzes einmal an.\n\ndistance_tbl &lt;- read_excel(\"data/distance.xlsx\")\n\ndistance_tbl[1:5, 1:5]\n\n# A tibble: 5 × 5\n  city      Amsterdam Antwerp Athens Barcelona\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 Amsterdam         0     160   3082      1639\n2 Antwerp         160       0   2766      1465\n3 Athens         3082    2766      0      3312\n4 Barcelona      1639    1465   3312         0\n5 Berlin          649     723   2552      1899\n\n\nWenn wir jetzt auf diesem Datensatz jetzt ein MDS rechnen wollen, dann müssen wir zum einen alle Spalten mit einem character entfernen. Wir haben dann nur noch einen Datensatz bzw. Datenmatrix mit den Distanzen vorliegen. Dann kann wir das tibble in einen dist-Objekt mit der Funktion as.dist() umwandeln. Die eigentliche Berechnung für das Multidimensional Scaling findet in der Funktion cmdscale() statt. Mit der Option k = 2 legen wir fest, dass wir nur zwei Hauptkomponenten bzw. Dimensionen bestimmen wollen. Wir machen also aus unserem 37x37 großen Datenmatrix durch Multidimensional Scaling eine Reduktion auf zwei Dimensionen bzw. Spalten.\n\nmds &lt;- distance_tbl |&gt;\n  select(-city) |&gt; \n  as.dist() |&gt;          \n  cmdscale(k = 2) |&gt;\n  as_tibble() |&gt; \n  mutate(V1 = -V1,\n         V2 = -V2)\ncolnames(mds) &lt;- c(\"Dim.1\", \"Dim.2\")\n\nIn Abbildung 60.16 sehen wir das Ergebnis der Dimensionsreduktion auf zwei Dimensionen. Wir erhalten die Zusammenhänge bzw. Distanzen aus der Datenmatrix in einem Scatterplot. Ein Scatterplot ist ja nichts anders als die Darstellung von zwei Dimensionen. Wie wir sehen können nimmt die Anordnung der Orte in etwa die Positionen von den Orten auf der Landkarte in Europa ein. Natürlich stimmen die Relationen nicht perfekt, aber das Abbild ist schon recht nahe dran. Wir können also auf diese Art und Weise auch Ausreißer bestimmen.\n\nggscatter(mds, x = \"Dim.1\", y = \"Dim.2\", \n          label = distance_tbl$city,\n          size = 1,\n          repel = TRUE)\n\n\n\n\n\n\n\nAbbildung 60.16— Scatterplot der zwei Dimensionen nach dem Multidimensional Scaling für den Abstand europäischer Städte.\n\n\n\n\n\nWenn wir keine Distanzmatrix wie im obigen Beispiel zu den Entfernungen der europäischen Städte vorliegen haben, dann können wir uns die Distanzen auch mit der Funktion dist() berechnen lassen. Wir nutzen jetzt mal als Echtdaten die Daten der Gummibärchen. Mal sehen, ob wir hier irgendwelche Gruppen erkennen. Die Hilfeseite der Funktion ?dist zeigt welche mathematischen Distanzmaße wir auf die Daten anwenden können. In unseren Fall berechnen wir die euklidische Distanz zwischen den Beobachtungen. Dann rufen wir über die Funkion cmdsscale das Multidimensional Scaling auf.\n\nmds &lt;- animals_df |&gt;\n  dist(method = \"euclidean\") |&gt;          \n  cmdscale(k = 2) |&gt;\n  as_tibble() |&gt; \n  set_names(c(\"Dim.1\", \"Dim.2\"))\n\nDas Ergebnis des Multidimensional Scaling hat keine Bedeutung für uns. Wir können die Zahlen nicht interpretieren. Was wir können ist das Ergebnis in einem Scatterplot wie in Abbildung 60.17 zu visualisieren.\n\nggscatter(mds, x = \"Dim.1\", y = \"Dim.2\", \n          label = rownames(animals_df),\n          size = 1,\n          repel = TRUE)\n\n\n\n\n\n\n\nAbbildung 60.17— Scatterplot der zwei Dimensionen nach dem Multidimensional Scaling für den Tierdatensatz.\n\n\n\n\n\nZum einen Spalten sich die Tiere sehr gut auf. Wir sehen hier auch, welche Tiere eher nahe beieinander liegen und welche Tiere eher artfremd sind. Manche Tiere sind dann etwas seltsam, wie zum Beispiel der Adler und der Elefant, die nah beieinander liegen. Aber es gibt auf der anderen Seite auch nicht so viele Vögel in den Daten. Beachte immer, alle Beobachtungen werden auch abgebildet. Es gibt kein, passt dann doch nicht.\nWie schon bei der Hauptkomponentenanalyse können wir uns auch das \\(k\\)-NN Verfahren aus dem Kapitel 69 nutzen um Cluster in den Daten zu finden. Das heißt wir nutzen das maschinelle Lernverfahren \\(k-NN\\) um uns \\(k\\) Cluster bestimmen zu lassen. Dafür nutzen wir die Funktion kmeans() und ziehen uns über die Funktion pluck() die Cluster raus. Daher erhalten wir einen Vektor mit Zahlen, die beschreiben in welchem Cluster die jeweilige \\(i\\)-te Beobachtung ist.\n\n# K-means clustering\nclust &lt;- kmeans(mds, centers = 3) |&gt;\n  pluck(\"cluster\") |&gt; \n  as.factor()\n\nWir wollen jetzt unser MDS Ergebnis von den Gummibärchen um eine Spalte für die Clusterergebnisse von \\(k\\)-NN ergänzen.\n\nmds &lt;- mds |&gt;\n  mutate(groups = clust)\n\nNun sehen in Abbildung 60.18 die gleiche Abbildung wie oben nur ergänzt um die farbliche Hinterlegung der \\(k=3\\) Clustern aus dem \\(k\\)-NN Algorithmus. Hier hängt es wieder stark von der Anzahl an Clustern ab. Wenn du mal spielst, wirst du sehen, dass bei vier Clustern dann auch der gelbe Großcluster aufgebrochen wird. Es sind eben dann doch recht wenig Tiere von jeder Art um hier eine saubere Zuordnung zu kriegen.\n\nggscatter(mds, x = \"Dim.1\", y = \"Dim.2\", \n          label = rownames(animals_df),\n          color = \"groups\",\n          palette = \"jco\",\n          size = 1, \n          ellipse = TRUE,\n          ellipse.type = \"convex\",\n          repel = TRUE)\n\n\n\n\n\n\n\nAbbildung 60.18— Scatterplot der zwei Dimensionen nach dem Multidimensional Scaling für den Tierdatensatz mit den \\(k=3\\) Clustern aus dem \\(k\\)-NN Algorithmus.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Multivariate Verfahren</span>"
    ]
  },
  {
    "objectID": "stat-modeling-pca.html#sec-lda",
    "href": "stat-modeling-pca.html#sec-lda",
    "title": "60  Multivariate Verfahren",
    "section": "60.7 Diskriminanzanalyse",
    "text": "60.7 Diskriminanzanalyse\nDie Diskriminanzanalyse (DA) ist eine multivariate Klassifizierungstechnik, die Beobachtungen auf der Grundlage messbarer Merkmale dieser Beobachtungen in zwei oder mehr sich gegenseitig ausschließende Gruppen einteilt. Diese Merkmale nennen wir dann gerne auch Variablen oder aber die Spalten unseres Datensatzes. Die Diskriminanzanalyse unterscheidet sich von der Regressionsanalyse dadurch, dass die abhängige Variable \\(y\\) diskret sein muss. Die Diskriminanzanalyse unterscheidet sich von der Clusteranalyse dadurch, dass die Klassen im Voraus bekannt sein müssen, um das Modell zu erstellen. Faktisch ist es schon alles was du wissen musst. Wir werden jetzt mal schauen, ob wir einen Datensatz, der schon Klassen bzw. Gruppen hat anhand der Diskriminanzanalyse besser verstehen können.\n\n\n\n\n\n\nWeitere Tutorien für die Diskriminanzanalyse\n\n\n\n\n\nWie immer gibt es eine Vielzahl an tollen Tutorien, die die Diskriminanzanalyse gut erklären. Ich habe hier einmal eine Auswahl zusammengestellt und du kannst dich da ja mal vertiefend mit beschäftigen, wenn du willst. Teile der Tutorien findest du vermutlich hier im Kapitel wieder.\n\nEine gute Übersicht als Tutorim Discriminant Analysis Essentials in R\nEinmal das erste Tutorium mit dem Namen Discriminant Analysis in R\nUnd einmal das zweite Tutorium mit dem selben Namen Discriminant Analysis in R\n\n\n\n\nAls Beispiel nutze ich einen Datensatz aus dem R Paket {HDclassif} und zwar den Datensatz wine auch bekannt vom UC Irvine Machine Learning Repository - Wine Dataset. Wie immer von mir leicht im Folgenden modifiziert. Wir haben hier drei Sorten von Wein class vorliegen, die wir anhand von verschiedenen Eigenschaften des Weins beschrieben haben. Welche dieser Eigenschaften hat einen Einfluss auf die Sorte des Weins? Das wollen wir uns jetzt einmal anschauen. Zuerst laden wir einmal die Daten.\n\n\nEine detailierte Auswertung des Datensatzes kann auch bei dem Tutorium Analysis of White Wine Quality Dataset nachvollzogen werden.\n\ndata(wine)\n\nWir müssen noch etwas an den Daten rumspielen und die Namen richtig kriegen. Die Namen fehlen leider in dem Datensatz und ich habe mir die nochmal rausgesucht und zusammen kopiert. Dann müssen wir die Spalten noch in saubere Namen umwandeln und die Klassenvariable class in einen Faktor umwandeln.\n\nwine_tbl &lt;- wine |&gt; \n  set_names(c(\"Class\", \"Alcohol\", \"Malic acid\", \"Ash\", \"Alcalinity of ash\", \"Magnesium\", \"Total phenols\",\n              \"Flavanoids\", \"Nonflavanoid phenols\", \"Proanthocyanins\", \"Color intensity\", \"Hue\", \n              \"OD280/OD315 of diluted wines\", \"Proline\")) |&gt; \n  clean_names() |&gt; \n  mutate(class = as_factor(class))\n\nDann einmal ein schneller Blick auf die Daten. Wir werden hier die Daten nicht standardisieren, obwohl es sicherlich Sinn macht das zu tun. Aber dann wird das hier alles noch länger als sowieso schon. Du musst dir ja bei jeder Variable überlegen, ob es eine kategorielle Variable ist oder nicht. Wir können die Diskriminanzanalyse aber auch ohne eine Standardisierung oder Normalisierung gut durchführen.\n\n\n\n\nTabelle 60.5— Übersicht.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280_od315_of_diluted_wines\nproline\n\n\n\n\n1\n14.23\n1.71\n2.43\n15.6\n127\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050\n\n\n1\n13.16\n2.36\n2.67\n18.6\n101\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185\n\n\n1\n14.37\n1.95\n2.50\n16.8\n113\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480\n\n\n1\n13.24\n2.59\n2.87\n21.0\n118\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735\n\n\n1\n14.20\n1.76\n2.45\n15.2\n112\n3.27\n3.39\n0.34\n1.97\n6.75\n1.05\n2.85\n1450\n\n\n1\n14.39\n1.87\n2.45\n14.6\n96\n2.50\n2.52\n0.30\n1.98\n5.25\n1.02\n3.58\n1290\n\n\n\n\n\n\n\n\nIn diesem Abschnitt gehe ich auf die volle Breite der Diskriminanzanalyse einmal ein. Zwar nicht in die Tiefe aber dafür in der Breite werde ich die folgenden Methoden der Diskriminanzanalyse vorstellen:\n\nLineare Diskriminanzanalyse (LDA): Verwendet die Linearkombinationen von den Variablen zur Vorhersage der Klasse einer gegebenen Beobachtung. Es wird davon ausgegangen, dass die Variablen (\\(p\\)) normal verteilt sind und die Klassen identische Varianzen oder identische Kovarianzmatrizen haben. Also alle approximativ Varianzhomogen sind.\nQuadratische Diskriminanzanalyse (QDA): ist flexibler als die LDA. Hier gibt es keine Annahme, dass die Kovarianzmatrix der Klassen gleich ist. Wir können also auch mit Varianzheterogenität umgehen.\nMischungsdiskriminanzanalyse (MDA): Wir nehmen an, dass jede Klasse eine Gaußsche Mischung von Unterklassen ist. Das heißt, unsere Daten setzen sich aus verschiedenen Normalverteilungen je Klasse zusammen.\nFlexible Diskriminanzanalyse (FDA): Es werden nichtlineare Kombinationen von Variablen verwendet, wie z. B. Splines oder nicht-lineare Geraden.\nRegulierte Diskriminanzanalyse (RDA): Die Regularisierung oder auch Schrumpfung verbessert die Schätzung der Kovarianzmatrizen in Situationen, in denen die Anzahl der Variablen größer ist als die Anzahl der Stichproben in den Daten. Dies führt zu einer Verbesserung der Diskriminanzanalyse.\n\nWir egehn jetzt alle Methoden einmal durch. Bitte melde dich mal, wenn du in diesem Bereich was machst, dann würde ich mich über eine Diskussion der praktischen Anwendung wirklich freuen.\n\n60.7.1 Lineare Diskriminanzanalyse (LDA)\nBeginnen wir also mit der linearen Diskriminanzanalyse (eng. Linear discriminant analysis). Wir versuchen dabei die vordefinierten Klassen anhand der Variablen in den Spalten durch Linien voneinander zu trennen. In der Abbildung 60.19 sehen wir einmal das Prinzip. Als schwarze Zahlen sind die Sorten der Weine angeben. Die Flächen bilden die Zuordnung der der linearen Diskriminanzanalyse ab. Daher werden die roten Zahlen falsch klassifiziert. Wir nennen den Vorgang linear, da wir lineare Abtrennungen durch Linien durchführen und keine Bögen erlauben.\n\n\n\n\n\n\n\n\nAbbildung 60.19— Darstellung des Partition Plot für zwei Variablen aus dem wine Datensatz. Die roten Zahlen sind die falsch von Algorithmus klassifizierten Weinsorten.\n\n\n\n\n\nWir können die lineare Diskriminanzanalyse mit der Funktion lda() aus dem R Paket {MASS} durchführen. Der lda() Auffruf ist nicht anders als alle anderen Formelaufrufe anderer Funktionen auch.\n\nwine_lda &lt;- lda(class ~ ., data = wine_tbl)\nwine_lda\n\nCall:\nlda(class ~ ., data = wine_tbl)\n\nPrior probabilities of groups:\n        1         2         3 \n0.3314607 0.3988764 0.2696629 \n\nGroup means:\n   alcohol malic_acid      ash alcalinity_of_ash magnesium total_phenols\n1 13.74475   2.010678 2.455593          17.03729  106.3390      2.840169\n2 12.27873   1.932676 2.244789          20.23803   94.5493      2.258873\n3 13.15375   3.333750 2.437083          21.41667   99.3125      1.678750\n  flavanoids nonflavanoid_phenols proanthocyanins color_intensity       hue\n1  2.9823729             0.290000        1.899322        5.528305 1.0620339\n2  2.0808451             0.363662        1.630282        3.086620 1.0562817\n3  0.7814583             0.447500        1.153542        7.396250 0.6827083\n  od280_od315_of_diluted_wines   proline\n1                     3.157797 1115.7119\n2                     2.785352  519.5070\n3                     1.683542  629.8958\n\nCoefficients of linear discriminants:\n                                      LD1           LD2\nalcohol                      -0.403399781  0.8717930699\nmalic_acid                    0.165254596  0.3053797325\nash                          -0.369075256  2.3458497486\nalcalinity_of_ash             0.154797889 -0.1463807654\nmagnesium                    -0.002163496 -0.0004627565\ntotal_phenols                 0.618052068 -0.0322128171\nflavanoids                   -1.661191235 -0.4919980543\nnonflavanoid_phenols         -1.495818440 -1.6309537953\nproanthocyanins               0.134092628 -0.3070875776\ncolor_intensity               0.355055710  0.2532306865\nhue                          -0.818036073 -1.5156344987\nod280_od315_of_diluted_wines -1.157559376  0.0511839665\nproline                      -0.002691206  0.0028529846\n\nProportion of trace:\n   LD1    LD2 \n0.6875 0.3125 \n\n\nDie lineare Diskriminanzanalyse ermittelt nun die Gruppenmittelwerte und berechnet für jedes Individuum die Wahrscheinlichkeit, zu den verschiedenen Weinsorten oder besser allgemeiner den Klassen zu gehören. Das Individuum wird dann der Gruppe mit dem höchsten Wahrscheinlichkeitswert zugeordnet. Die Ausgaben von lda() enthalten die folgenden Elemente:\n\nPrior probabilities of groups: der Anteil der Beobachtungen in jeder Gruppe. Zum Beispiel befinden sich 33% der Beobachtungen in der Gruppe mit der Sorte 1.\nGroup means: Die Gruppenschwerpunkt und zeigen damit den Mittelwert jeder Variable in jeder Gruppe.\nCoefficients of linear discriminants: Zeigt die lineare Kombination von Prädiktorvariablen, die zur Bildung der LDA-Entscheidungsregel verwendet werden. Hier ist wichtig, dass die Variablen mit einem hohen absoluten Wert den meisten Anteil haben die Klassen voneinander zu trennen.\nProportions of trace: Beschreibt den Anteil der Varianz zwischen den Klassen, der durch aufeinanderfolgende Diskriminanzfunktionen erklärt wird.\n\nMit der Funktion partimat() können wir uns die Zusammenhänge für einzelne Variablenkombinationen ansehen, vielleicht wird dann einiges klarer. Es empfiehlt sich immer die Variablen zu nehmen, die den größten Betrag in der ersten Diskriminanten (LD1) haben. In unserem Fall sind das dann flavanoids sowie nonflavanoid_phenols und od280_od315_of_diluted_wines die wir uns nochmal paarweise anschauen. Ich kann das leider hier nicht im Skript machen, da leider die entstehende Abbildungen dann leider hier die Ausgabe zerreißen. Das Paket hat eben doch schon ein paar Jahre auf dem Buckel und ist nicht mehr so kompatibel und ich ahbe leider nicht die Zeit es nochmal in ggplot schön zu machen. Die Informationen sind ja alle da.\n\npartimat(class ~ flavanoids + od280_od315_of_diluted_wines + nonflavanoid_phenols, data = wine_tbl, method = \"lda\")\n\nIm Weiteren können wir uns auch die Klassen vorhersagen lassen die sich aus dem Modell der lineare Diskriminanzanalyse ergeben würden. Dafür nutzen wir dann die Funktion predict() und stecken einfach nochmal die Daten in die Funktion. Dann müssen wir noch etwas aufräumen, damit wir einen schönen Datensatz wiederbekommen.\n\nwine_lda_pred_tbl &lt;- predict(wine_lda) |&gt; \n  reduce(bind_cols) |&gt; \n  set_names(c(\"class\", \"pred_1\", \"pred_2\", \"pred_3\", \"ld1\", \"ld2\"))\n\nWir können jetzt den Datensatz nutzen um einmal zu schauen, ob wir in der Lage waren über die lineare Diskriminanzanalyse unsere Klassen sauber aufzutrennen. Wir stellen auf der \\(x\\)-Achse und der \\(y\\)-Achse dann die Diskriminaten LD1 und LD2 dar. Unsere Klassen sind dann nach der vorhergesagten Klassenzugehörigkeit gelabelt. Wir sehen das Ergebnis dann in der Abbildung 60.20.\n\nwine_lda_pred_tbl |&gt; \n  ggplot(aes(ld1, ld2, label = class, color = class)) +\n  theme_minimal() +\n  geom_text() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 60.20— Ergebnis der linearen Diskriminanzanalyse für die Aufteilung der Weinsorten für beide Diskriminanten.\n\n\n\n\n\nJetzt schauen wir noch nach wie akkurat unsere Einteilung wirklich war. Wie viel Prozent der Klassen aus unserem originalen Datensatz können wir mit dem Modell der linearen Diskriminanzanalyse richtig vorhersagen?\n\nmean(wine_lda_pred_tbl$class == wine_tbl$class)\n\n[1] 1\n\n\nPerfekt, wir können alle Sorten richtig zuordnen. Wir haben ein wunderbares Modell gefunden mit dem wir zukünftige Weinsorten in die drei Klassen einordnen können.\n\n\n60.7.2 Quadratische Diskriminanzanalyse (QDA)\nAls nächstes schauen wir kurz auf die quadratische Diskriminanzanalyse (eng. Quadratic discriminant analysis ). Wir gehen hier jetzt nicht nochmal alles im Detail durch. Hier wiederholt sich dann vieles aus der linearen Diskriminanzanalyse.\n\nwine_qda &lt;- qda(class ~ ., data = wine_tbl)\n\nWir erhalten auch wieder die Vorhersagen und damit ein Maß für unsere Güte der KLassenzuordnung über die Funktion predict(). Leider ist es so, dass wir nicht die Informationen über die Diskriminanten erhalten. Daher können wir auch keine Abbildung der Prädiktion zeigen.\n\nwine_qda_pred_tbl &lt;- predict(wine_qda) |&gt; \n  reduce(bind_cols) |&gt; \n  set_names(c(\"class\", \"pred_1\", \"pred_2\", \"pred_3\"))\n\nDas einzige was hier jetzt erstmal bleibt, ist darauf zu schauen, wie akkurat unsere Vorhersage war. Wie viele von den Sorten sagen wir richtig vorher?\n\nmean(wine_qda_pred_tbl$class == wine_tbl$class)\n\n[1] 0.994382\n\n\nHier sind wir dann etwas schlecht als bei der linearen Diskriminanzanalyse.\n\n\n60.7.3 Mischungsdiskriminanzanalyse (MDA)\nFür die Mischungsdiskriminanzanalyse (eng. Mixture discriminant analysis) wechseln wir dann das Paket zu {mda}. Hier können wir uns dann auch wieder die Ergebnisse ausgeben lassen. Wir immer sieht das wieder anders aus, da wir das Paket gewechselt haben.\n\nwine_mda &lt;- mda(class ~ ., data = wine_tbl)\nwine_mda\n\nCall:\nmda(formula = class ~ ., data = wine_tbl)\n\nDimension: 8 \n\nPercent Between-Group Variance Explained:\n    v1     v2     v3     v4     v5     v6     v7     v8 \n 52.82  88.80  93.73  96.63  98.35  99.58  99.97 100.00 \n\nDegrees of Freedom (per dimension): 14 \n\nTraining Misclassification Error: 0 ( N = 178 )\n\nDeviance: 0.349 \n\n\nWas wir sehen ist, dass unser Misclassification Error gleich 0 ist. Daher haben wir keine Klasse falsch zugeordnet. Das ist schon mal super. Das werden wir dann auch gleich nochmal sehen, wenn wir die Funktion predict() auf unsere Daten anwenden. Insgesamt werden dann aber acht Dimensionen geschätzt, wir werden uns aber nur die ersten beiden dann anschauen. Daher auch das [, 1:2], was die ersten beiden Spalten aus den variates auswählt.\n\nwine_mda_pred_tbl &lt;- lst(class = predict(wine_mda, type = \"class\"), \n                         posterior = predict(wine_mda, type = \"posterior\"), \n                         variates = predict(wine_mda, type = \"variates\")[, 1:2]) |&gt; \n  reduce(bind_cols) |&gt; \n  set_names(c(\"class\", \"pred_1\", \"pred_2\", \"pred_3\", \"ld1\", \"ld2\"))\n\nIn der Abbildung 60.21 sehen wir das Ergebnis der Mischungsdiskriminanzanalyse. Sieht nicht so gut aus, wie die lineare Diskriminanzanalyse, da wir hier mehr Gurppen haben, die sich überlappen. Dennoch scheinen die Sorten sauber getrennt zu werden. Dafür schauen wir uns dann nochmal die Güte der Vorhersage an.\n\nwine_mda_pred_tbl |&gt; \n  ggplot(aes(ld1, ld2, label = class, color = class)) +\n  theme_minimal() +\n  geom_text() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 60.21— Ergebnis der Mischungsdiskriminanzanalyse für die Aufteilung der Weinsorten für beide Diskriminanten.\n\n\n\n\n\nSpannend, wie sehen, dass wir alle Klassen richtig vorhersagen. Zwar ist die Abbildung nicht so klar wie bei der linearen Diskriminanzanalyse, aber denoch kriegen wir alle Klassen sauber vorhersagen.\n\nmean(wine_mda_pred_tbl$class == wine_tbl$class)\n\n[1] 1\n\n\nHier gehen wir nicht mehr auf die Fehlermatrix (eng. confusion matrix) ein. Was du schnell erkennst ist, dass du alle Sorten sauber und. richtig vorhersagt. In den Spalten stehen die wahren Sorten und in den Zeilen die vom Modell vorhergesagten Sorten. Wir haben da eine perfekte Übereinstimmung.\n\nwine_mda$confusion\n\n         true\npredicted  1  2  3\n        1 59  0  0\n        2  0 71  0\n        3  0  0 48\n\n\nAbbhängig vom Paket ist dann die Fehlermatrix mit enthalten, manachmal musst du dir die auch selber bauen, wenn du die Matrix brauchts.\n\n\n60.7.4 Flexible Diskriminanzanalyse (FDA)\nDie flexible Diskriminanzanalyse (eng. Flexible Discriminant Analysis) basiert auch auf dem R Paket {mda} und somit haben wir ähnliche Aufrufe wie schon in der Mischungsdiskriminanzanalyse. Da wir hier nur zwei Dimensionen vorhersagen, haben wir es gleich einfach bei der Abbildung.\n\nwine_fda &lt;- fda(class ~ ., data = wine_tbl)\n\nWir bauen uns dann wieder unseren Vorhersagedatensatz zusammen und wollen dann einmal die Vorhersage visualisieren.\n\nwine_fda_pred_tbl &lt;- lst(class = predict(wine_fda, type = \"class\"), \n                         posterior = predict(wine_fda, type = \"posterior\"), \n                         variates = predict(wine_fda, type = \"variates\")) |&gt; \n  reduce(bind_cols) |&gt; \n  set_names(c(\"class\", \"pred_1\", \"pred_2\", \"pred_3\", \"ld1\", \"ld2\"))\n\nIn der Abbildung 60.22 siehst du die Ergebnisse der flexiblen Diskriminanzanalyse. Sieht super aus, wir haben eine perfekte Seperation.\n\nwine_fda_pred_tbl |&gt; \n  ggplot(aes(ld1, ld2, label = class, color = class)) +\n  theme_minimal() +\n  geom_text() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 60.22— Ergebnis der flexiblen Diskriminanzanalyse für die Aufteilung der Weinsorten für beide Diskriminanten.\n\n\n\n\n\nAuch hier einmal geschaut, was die Güte der Vorhersage aussagt. Wir sehen, dass ist dann auch perfekt. War auch zu erwarten nach der Abbildung.\n\nmean(wine_lda_pred_tbl$class == wine_tbl$class)\n\n[1] 1\n\n\nAbschließend noch die Fehlermatrix, die wir frei Haus mitgeliefert kriegen.\n\nwine_fda$confusion\n\n         true\npredicted  1  2  3\n        1 59  0  0\n        2  0 71  0\n        3  0  0 48\n\n\n\n\n60.7.5 Regulierte Diskriminanzanalyse (RDA)\nZum Abschluss noch die regulierte Diskriminanzanalyse (eng. Regularized discriminant anlysis) aus dem dritten R Paket {klaR}. Ja, wir gehen hier echt durch die Pakete durch und erhalten dann wieder eine andere Ausgabe.\n\nwine_rda &lt;- rda(class ~ ., data = wine_tbl)\nwine_rda\n\nCall: \nrda(formula = class ~ ., data = wine_tbl)\n\nRegularization parameters: \n    gamma    lambda \n0.9767300 0.9477272 \n\nPrior probabilities of groups: \n        1         2         3 \n0.3314607 0.3988764 0.2696629 \n\nMisclassification rate: \n       apparent: 27.528 %\ncross-validated: 27.558 %\n\n\nWir sehen vor allem, dass jetzt mal die Vorhersage nicht so super geklappt hat. Wir haben eine Fehlerrate oder Misclassification rate von 28% und damit wird mehr als ein Viertel der Sorten nicht korrekt zugeordnet. Leider haben wir auch hier keine Möglichkeit uns mal etwas anzuschauen. Wir können nur selber nochmal die Vorhersage rechnen und kommen dann vermutlich auf ähnliche Zahlen.\n\nwine_rda_pred_tbl &lt;- predict(wine_rda) |&gt; \n  reduce(bind_cols) |&gt; \n  set_names(c(\"class\", \"pred_1\", \"pred_2\", \"pred_3\"))\n\nAm Ende rechnen wir aus, wie viel Prozent wir richtig klassifiziert haben und erreichen grob 71% was ja auch zu erwatzen war. Das ist jetzt so schlecht, dass wir in diesem Fall die regulierte Diskriminanzanalyse nicht nutzen würden.\n\nmean(wine_rda_pred_tbl$class == wine_tbl$class)\n\n[1] 0.7247191\n\n\nAm Ende musst du aber schauen, wie deine Daten gut zu welcher Methode passen. Da alle Methoden sehr einfach durchzuführen sind, kannst du auch alle rechnen und schauen welche das beste Ergebnis in deinem Fall liefert.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Multivariate Verfahren</span>"
    ]
  },
  {
    "objectID": "stat-modeling-pca.html#referenzen",
    "href": "stat-modeling-pca.html#referenzen",
    "title": "60  Multivariate Verfahren",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 60.1 (a)— Simuliertes Datenbeispiel mit zwei Variablen \\(x\\) und \\(y\\). Die Punkte wurden mit der Regressionsgleichung \\(y = 0 + 0.5 \\cdot x\\) erzeugt und die Varianz \\(\\epsilon \\sim \\mathcal{N}(0,1)\\) gesetzt. Später versuchen wir aus der Hauptkomponenteanalyse die Werte für die Steigung \\(\\beta_x = 0.5\\) wieder zufinden.\nAbbildung 60.1 (b)— Darstellung der 1. Hauptkomponente (PC1) durch die Punktewolke mit maximal erklärter Varianz, da minimaler Abstand zu den Punkten. Die 2. Hauptkomponente muss lotrecht auf der 1. Hauptkomponente stehen und dabei die Varianz maximal von der Hauptkomponente minimieren wird.\nAbbildung 60.2— Berechnung der Abstände für die Varianz von \\(x_1\\) sowie \\(x_2\\) in der oberen Zeile. In der unteren Zeile die Rechtecke der Berechnung der Kovarianz von \\(x_1\\) und \\(x_2\\).\nAbbildung 60.3 (a)— 1. Hauptkomponente (PC1) durch die Punktewolke\nAbbildung 60.3 (b)— 2. Hauptkomponente (PC2) durch die Punktewolke\nAbbildung 60.3 (c)— Regressionsgleichungen der 1. Hauptkomponente (PC1) sowie deren Rotation zur 2. Hauptkomponente.\nAbbildung 60.4— Verlauf einer generalisierten Hauptkomponentenanalyse von dem Datensatz über die Varianz/Kovarianzmatrix zu den unkorrelierten Hauptkomponeten und deren Koordinaten unabhängig von den Eigenschaften der Variablen. Abschließend sind die Koordinaten der Hauptkomponenten im Koorelationsplot der 1. und 2. Hauptkomponente dargestellt. Variablen die nahe beieinander sind, sind positiv miteinander korreliert; Variablen in den gegenüberliegenden Quadranten naegativ. Je weiter eine Variable außen liegt, desto höher ist die Aussagekraft. Variablen nahe dem Ursprung, haben eine geringe Bedeutung. Am Beispiel der Variable fly werden nochmal die Koordinaten deutlich.\nAbbildung 60.5 (a)— Tierdaten animals_df\nAbbildung 60.5 (b)— Kreaturendaten std_creature_df\nAbbildung 60.6 (a)— Tierdaten animals_df\nAbbildung 60.6 (b)— Kreaturendaten std_creature_df\nAbbildung 60.7 (a)— Tierdaten animals_df\nAbbildung 60.7 (b)— Kreaturendaten std_creature_df\nAbbildung 60.8 (a)— Tierdaten animals_df\nAbbildung 60.8 (b)— Kreaturendaten std_creature_df\nAbbildung 60.9 (a)— Tierdaten animals_df\nAbbildung 60.9 (b)— Kreaturendaten std_creature_df\nAbbildung 60.10 (a)— Tierdaten animals_df\nAbbildung 60.10 (b)— Kreaturendaten std_creature_df\nAbbildung 60.11 (a)— Tierdaten animals_df\nAbbildung 60.11 (b)— Kreaturendaten std_creature_df\nAbbildung 60.12 (a)— Tierdaten animals_df\nAbbildung 60.12 (b)— Kreaturendaten std_creature_df\nAbbildung 60.13 (a)— Tierdaten animals_df\nAbbildung 60.13 (b)— Kreaturendaten std_creature_df\nAbbildung 60.14 (a)— Tierdaten animals_df\nAbbildung 60.14 (b)— Kreaturendaten std_creature_df\nAbbildung 60.15 (a)— Tierdaten animals_df\nAbbildung 60.15 (b)— Kreaturendaten std_creature_df\nAbbildung 60.16— Scatterplot der zwei Dimensionen nach dem Multidimensional Scaling für den Abstand europäischer Städte.\nAbbildung 60.17— Scatterplot der zwei Dimensionen nach dem Multidimensional Scaling für den Tierdatensatz.\nAbbildung 60.18— Scatterplot der zwei Dimensionen nach dem Multidimensional Scaling für den Tierdatensatz mit den \\(k=3\\) Clustern aus dem \\(k\\)-NN Algorithmus.\nAbbildung 60.19— Darstellung des Partition Plot für zwei Variablen aus dem wine Datensatz. Die roten Zahlen sind die falsch von Algorithmus klassifizierten Weinsorten.\nAbbildung 60.20— Ergebnis der linearen Diskriminanzanalyse für die Aufteilung der Weinsorten für beide Diskriminanten.\nAbbildung 60.21— Ergebnis der Mischungsdiskriminanzanalyse für die Aufteilung der Weinsorten für beide Diskriminanten.\nAbbildung 60.22— Ergebnis der flexiblen Diskriminanzanalyse für die Aufteilung der Weinsorten für beide Diskriminanten.\n\n\n\nKassambara A. 2017. Practical guide to principal component methods in R: PCA, M (CA), FAMD, MFA, HCPC, factoextra. Sthda.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Multivariate Verfahren</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survey.html",
    "href": "stat-modeling-survey.html",
    "title": "61  Fragebogenanalyse",
    "section": "",
    "text": "61.1 Durchführung\nIm Folgenden möchte ich dir einmal eine Übersicht über die Literatur zur Erstellung eines Fragebogens geben. Wir immer ist dies hier nur eine grobe Übersicht. Je nachdem wie komplex deine Fragestellung ist, musst du natürlich auch einen komplexeren Fragebogen nutzen und dann wird die Erstellung des Fragebogens um einiges komplizierter und anspruchsvoller. Wenn es gar noch weiter gehen soll in die Richtung Marktforschung mit Teilnehmer:innen in einem Labor in Präsenz, kommen noch andere Anforderungen hinzu. So weit wollen wir hier aber erstmal nicht gehen. Ich stelle dir hier Ideen und Anregungen für die Erstellung eines Fragebogens für eine Abschlussarbeit vor. Ich kann natürlich hier nicht die Arbeiten vollständig zitieren, da musst du dann nochmal selber lesen. Die Literatur ermöglicht deshalb sicherlich noch viel mehr als hier kurz vorgestellt wird. Daher kann ich dir nur empfehlen einmal die Literatur quer zu lesen.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Fragebogenanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survey.html#durchführung",
    "href": "stat-modeling-survey.html#durchführung",
    "title": "61  Fragebogenanalyse",
    "section": "",
    "text": "Wie fang ich’s an?\nBeginnen wir mit einer schon etwas älteren Arbeit von Watson (1998) mit Titel A primer in survey research. Die Arbeit ist zwar schon etwas älter und damit nicht auf dem neusten Stand was die Onlineumfragen angeht, das gab es ja zur der Zeit noch nicht richtig. Dafür liefert die Arbeit aber einen super Einstieg in die Grundlagen von Umfragen und was du beachten solltest. Teilweise ist es dann auch in einer Art Checkliste geschrieben.\nDie Arbeit von Story und Tait (2019) mit dem Titel Survey Research sticht durch die Toolboxen hervor. In der Toolbox for Survey Researchers findest du Tipps & Tricks für dich, wenn du einen Fragebogen erstellst. In der Toolbox for Survey Readers dann die umgedrehte Seite, was muss ich beachten, wenn ich Umfragen lese? Auch toll, da du ja was schreibst, was dann auch gelesen werden soll. Die Arbeit hat noch mehr Tollboxen in denen noch mehr Tipps und Anleitungen gegeben werden. Wenn ich einen Fragebogen in einer Beratung habe, dann gebe ich immer gerne diese Arbeit weiter.\nWenn du dann deine Umfrage gemacht hast kannst du dann bei Gaur u. a. (2020) in ihrer Arbeit Reporting Survey Based Studies – a Primer for Authors lesen wie du deine Umfrage publizieren kannst. Die Arbeit beginnt aber schon etwas früher und präsentiert in einer Liste recht aktuelle Tools for survey-based studies und deren Vor- und Nachteile. In dem Algorithm for a survey construct geben die Autoren nochmal eine Handreichung, wie eine Fragebogenstudie bis zur Publikation aufgebaut sein kann.\nSynodinos (2003) gibt in seinem Artikel The “art” of questionnaire construction: some important considerations for manufacturing studies einen sehr detailierten Überblick über die Erstellung eines Fragebogens. Die Arbeit ist interessant, aber auch nur wirklich was für jemanden, der wirklich in die Fragebogenerstellung abtauchen will. Für eine Abschlussarbeit meiner Meinung nach schon zu viel. Ich habe aber die Arbeit gerne einmal quergelesen und die ein oder andere Idee dort mitgenommen. Ich kann mich dann nur folgendem übersetzten Zitat aus seiner Arbeit anschließen.\n\nIn der Tat scheinen einige Forscher die Phase der Datenerhebung zu vernachlässigen und sich nur auf ausgeklügelte statistische Verfahren zu konzentrieren. Es kann eindeutig festgestellt werden, dass keine noch so ausgefeilte statistische Analyse die grundlegenden Unzulänglichkeiten eines schlecht konstruierten Fragebogens korrigieren kann. — Synodinos (2003)\n\nAbschließend möchte ich diese kurze Einleitung hier mit der Arbeit von Brühlmann u. a. (2020) mit dem Titel The quality of data collected online: An investigation of careless responding in a crowdsourced sample. Es geht im Prinzip darum, dass die Teilnehmer:innen in einem Onlinefragebogen dann schlampig antworten und irgendwie dann keine Lust mehr haben. Das solltest du beachten, wenn du selber einen Fragebogen baust. Ob du dann deine Analyse so auswerten musst wie die Autoren vorschlagen, würde ich jetzt nicht meinen. Aber habe den Gedanken im Hinterkopf, dass deine Teilnehmer:innen eine Egalhaltung einnehmen könnten während sie den Fragebogen ausfüllen. Das führt uns dann auch gleich zu dem nächsten Thema der Länge eines Fragebogens.\n\n\nWie lang soll es sein?\nHäufig stellt sich auch die Frage, wie lang soll den so ein Fragebogen sein? Du hast so viele Fragen und wenn dann schon mal einer antwortet, dann kann der doch gefälligst dreißig Seiten Fragebogen ausfüllen? Du kannst dann ja später immer noch Fragen rausschmeißen. Ja, aber das machen wir dann meistens doch nicht. Deshalb bleibt das Zitat von Steven King immer aktuell. Du musst deinen Fragebogen so kürzen, dass er deine Fragestellung beantworten kann und nicht mehr.\n\n“Kill your darlings, kill your darlings, even when it breaks your egocentric little scribbler’s heart, kill your darlings.” — Steven King\n\nRolstad u. a. (2011) haben in ihrer Arbeit mit dem Titel Response Burden and Questionnaire Length: Is Shorter Better? A Review and Meta-analysis sich sehr viele Zusammenhänge angeschaut. Sie versuchten die Frage zu beantworten, ob ein kurzer Fragebogen wirklich besser ist. Hierbei kam heraus, dass angesichts der schwachen Belege für einen Zusammenhang zwischen der Länge des Fragebogens und dem Beantwortungsaufwand Entscheidungen über die Wahl des Fragebogens am besten auf der Qualität des Inhalts aus der Sicht der Beantwortenden beruhen sollten und nicht auf der Länge an sich.\n\nResponse rates were lower for longer questionnaires, but because the P value for test of homogeneity was P = 0.03, this association should be interpreted with caution because it is impossible to separate the impact of content from length of the questionnaires. — Rolstad u. a. (2011)\n\nRoszkowski und Bean (1990) schreiben in ihrer etwas älteren Arbeit mit dem Titel Believe it or not! Longer questionnaires have lower response rates folgendes recht eindeutige Ergebnis.\n\nResponse rate for the short form averaged about 28% higher than for the long form […]. A measure of course satisfaction appearing on both questionnaires showed no significant differences between the long and short form […]. — Roszkowski und Bean (1990)\n\nJa, die Arbeit ist schon etwas älter, aber auch bei Fragebögen steckt dann die Würze in der Kürze. Wenn du deine Fragebögen zu lang machst, dann kann es sein, dass eben deine Teilnehmer:innen keine Lust mehr haben und am Ende nur noch raten oder eben dann den Bogen gar nicht ausfüllen. Und damit kommen wir auch schon zum nächsten Punkt, wie ist denn eigentlich die Rücklaufquote?\n\n\nWieviele antworten mir?\nDu willst ja auch, dass dir Personen auf deinen Fragebogen antworten. Der Fragebogen ist nicht zu lang und du glaubst auch die Interessen der Antwortenden berücksichtigt zu haben. Dein Fragebogen ist damit also keine Qual zu beantworten. Edwards u. a. (2009) hat sich in einer Metanalyse mit dem Titel Methods to increase response to postal and electronic questionnaires einmal verschiedene Möglichkeiten angeschaut, die Antwortraten zu erhöhen. Nicht überraschend konnten durch Geld die Antwortraten verdoppelt werden. Aber es gab noch andere Möglichkeiten. Weitere Punkte, die die Antwortraten verdoppelt haben, waren ein Einschreiben, ein Hinweis auf dem Umschlag - zum Beispiel mit einem Kommentar, der den Teilnehmern suggeriert, dass sie davon profitieren können, wenn sie den Umschlag öffnen oder aber ein interessanteres Thema im Fragebogen. Schau nochmal selber in die Arbeit um dir die weiteren Möglichkeiten anzusehen. Edwards u. a. (2002) hat schon früher eine Arbeit mit dem Titel Increasing response rates to postal questionnaires: systematic review geschrieben. In der sehr kurzen Arbeit geht er nochmal auf verschiedene Punkte ein. Insgesamt schaut der Artikel auf 40 Strategien die Antwortraten zu erhöhen. Ich glaube, dass da auch für dich was bei sein könnte, was einfach durchzuführen ist.\n\n\nWie soll ich’s bauen?\nDie Likertskala ist die häufigste Art die Antwortmöglichkeiten zu bauen. In dem Artikel The 4,5, and 7 Point Likert Scale erhälst du einen guten Überblick, was die Vorteile und Nachteile von den verschiedenen Anzahlen an Antwortmöglichkeiten sind. Im Prinzip hast du die Wahl zwischen vier Antwortmöglichkeiten, so dass du dann die Teilnehmer zwingst sich für eine Seite zu entscheiden. Oder aber du hast fünf Antwortmöglichkeiten, so dass du eine neutrale Antwortmöglichkeit einfügen kannst. Friedman und Amoo (1999) gibt mit seinem Artikel Ratung the rating scales nochmal einen wunderbaren Überblick über die Porblematik der Auswahl der richtigen Antwortmöglichkeiten im Bezug auf die Likertskala. In dem etwas längeren Artikel Survey Response Scales: How to Choose the Right One for your Questionnaire geht der Autor nochmal auf die verschiedenen möglichen Arten von Antworten neben der Likertskala ein. Neben der Likertskala geht natürlich auch eine “Ja/Nein”-Antwort oder eine numerische Einschätzung als mögliche Antwort. So eine ausführliche Betrachtung sprengt dann eben dieses Kapitel und deshalb schaue da nochmal in die Literatur, wenn du die Antworten für deine Fragen baust. Ich denke, dass die drei Quellen schonmal ein guter Start und Inspiration sind.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Fragebogenanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survey.html#genutzte-r-pakete",
    "href": "stat-modeling-survey.html#genutzte-r-pakete",
    "title": "61  Fragebogenanalyse",
    "section": "61.2 Genutzte R Pakete",
    "text": "61.2 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, gtsummary, janitor, FactoMineR,\n               factoextra, corrplot, wesanderson, naniar,\n               scales, likert, sjPlot, parameters, mfp,\n               correlation, conflicted)\n\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Fragebogenanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survey.html#daten",
    "href": "stat-modeling-survey.html#daten",
    "title": "61  Fragebogenanalyse",
    "section": "61.3 Daten",
    "text": "61.3 Daten\nDer Druck im Markt auf Bauernhöfe ist groß. Neben den bekannten Erlebnishöfen mit Spezialisierungen auf Obst und Gemüse muss sich in den Weiten Brandenburgs, Niedersachsens und Mecklenburg-Vorpommern jeder Hof was Neues einfallen lassen um Gäste zu sich zu locken. Heutzutage reicht es einfach nicht mehr Rentnern auf Elektrofahrrädern überdimensionierte Sahnetorten anzubieten. Die Enkel wollen auch was sehen. Inspiriert von der Serie Tiger King: Großkatzen und ihre Raubtiere gibt es nun immer mehr Höfe, dich sich den einen oder anderen Tiger oder Großkatze in den Hinterhof sperren. Aufgeschreckt durch den Zwischenfall mit der Löwin von Kleinmachnow - oder wie eine Wildsau durchs Dorf getrieben wurde, gab der Verband “Erlebnishöfe mit Niveau e.V.” eine Umfrage mit dem Titel Nutzung von Großkatzen zur Steigerung der Attraktivität von Erlebnishöfen unter den Mitgliedern in Auftrag. Gleichzeitig sollte auch die Zufriedenheit der Verbandsarbeit abgefragt werden und wie die betriebswirtschaftliche Struktur der 843 Betriebe um die Erlebnishöfe im Verband eigentlich so aussieht.\n\ntiger_tbl &lt;- read_excel(\"data/survey-serengeti-tiger-king.xlsx\", na = \"NA\",\n                        sheet = \"results-survey-lime\")\n\nIn der Tabelle 61.1 sehen wir einen Auszug aus unseren Ergebnis der Umfrage des Verbandes. Wir haben insgesamt 23 Fragen gestellt und eine Rücklauf von 173 Fragebögen von den Erlebnishöfen erhalten. Das ist eigentlich gar nicht so schlecht, damit haben wir hier eine Rücklaufquote von gut \\(20.5\\%\\) der Fragebögen. Wir werden uns nun an den 23 beispielhaften Fragen verschiedene Herausforderungen anschauen.\n\n\n\n\nTabelle 61.1— Auszug von fünf Fragen aus dem Fragebogen zur Nutzung von Großkatzen zur Steigerung der Attraktivität von Erlebnishöfen. Insgesamt haben 173 Erlebnishöfe den Fragebogen zurückgesendet.\n\n\n\n\n\n\n\n\n\n\n\n\n\ngeschlecht\nalter\nhaben_Sie_tiger\nf1Soziodemografisch\nf2Soziodemografisch\n\n\n\n\nMännlich\nÜber 60 Jahre\nJa\nNebenerwerb\nHolzwirtschaft\n\n\nMännlich\n30-39 Jahre\nJa\nHaupterwerb\nNA\n\n\nMännlich\nÜber 60 Jahre\nJa\nNebenerwerb\nNA\n\n\nMännlich\n40-49 Jahre\nJa\nHaupterwerb\nNA\n\n\n…\n…\n…\n…\n…\n\n\nMännlich\n40-49 Jahre\nNein\nNebenerwerb\nNA\n\n\nMännlich\nÜber 60 Jahre\nNein\nHaupterwerb\nHofladen\n\n\nMännlich\n40-49 Jahre\nNein\nHaupterwerb\nNA\n\n\nMännlich\nÜber 60 Jahre\nNein\nHaupterwerb\nNA\n\n\n\n\n\n\n\n\nWir werden jetzt den Datensatz einmal versuchen auszuwerten. Dabei gehen wir schrittweise verschiedene Analysen durch und schauen, ob wir mit den Fragebogendaten so arbeiten können. Sehr häufig müssen wir nämlich erstmal die Daten so lange bearbeiten, bis die Daten sinnvoll ausgewertet werden können.\nLeider sind die Fragen in den Fragebögen sehr lang. Das macht es schwierig die Fragen sauber als Spaltennamen abzubilden. Wir erhalten zum Beispiel aus dem Online-Query Lime verkürzte Spaltennamen, die sehr wenig aussagen, aber dennoch sehr lang sind. Deshalb empfehle ich immer noch eine zusätzliche Tabelle mit Fragen-ID ques_id, der Lime-ID lime_id sowie der eigentlichen Frage question zu erstellen. Ich mache das meistens dann in einem zusätzlichen Tab in der Exceldatei wie in der folgenden Abbildung gezeigt. In der Exceldatei kannst du dann auch in neuen Tabs die gereinigten oder bearbeiteten Versionen des ursprünglichen Fragebogens ablegen.\n\n\n\n\n\n\nAbbildung 61.1— Nicht alle Änderungen müssen in R durchgeführt werden. Es empfiehlt sich aber die Tabs in Excel zu nutzen um sich neue Versionen des ursprüngliche Lime-Fragebogens anzulegen. So hast du immer das Orginal vorliegen und kannst dann schrittweise nachvollziehen, was du geändert hast.\n\n\n\nIch lade dann einmal den Tab mit den Beschreibungen der Fragen.\n\nshort_question_tbl &lt;- read_excel(\"data/survey-serengeti-tiger-king.xlsx\", sheet = \"question-short\") \n\nIn der Tabelle 61.2 siehst du einmal die Fragen in der Langform und die entsprechenden ID’s für die Kurzform hier in R und dann eben auch in Lime. Ich nutze die Kurzform ques_id für die Beschriftungen von Abbildungen, da es sonst sehr schnell sehr unübersichtlich wird. Finale Abbildungen können dann am Ende immer noch entsprechend beschriftet werden. Die lime_id brauche ich um später noch Fragen entfernen zu können und als Verbindung zu den ursprünglichen Daten.\n\n\n\n\nTabelle 61.2— Tabelle der abgekützen Fragen-ID, der ursprüngichen Lime-ID sowie dem vollständigen Fragetext. Auch hier habe ich den Fragetext gekürzt, später kann dann noch der vollständige Text ergänzt werden.\n\n\n\n\n\n\n\n\n\n\n\nques_id\nlime_id\nquestion\n\n\n\n\ng1\ngeschlecht\nWelches Geschlecht haben Sie?\n\n\ng2\nalter\nWie alt sind Sie?\n\n\ng3\nhaben_Sie_tiger\nHalten Sie aktuell Tiger oder andere Großkatzen auf Ihrem Erlebnishof?\n\n\ns1\nf1Soziodemografisch\nIst Ihr Erlebnishof Haupterwerb oder Nebenerwerb?\n\n\ns2\nf2Soziodemografisch\nHaben Sie weitere Erwerbsquellen neben dem Erlebnishof außer Landwirtschaft?\n\n\ns3\nf3Soziodemografisch\nWelche Stellung haben Sie auf dem Erlebnishof inne?\n\n\nv1\nf1Verbandsarbeit\nSoll die Verbandsarbeit in den nächsten Jahren digital in den sozialen Netzwerken ausgeweitet werden?\n\n\nv2\nf2Verbandsarbeit\nLesen Sie quartalsweise den Newsletter auf der Verbandshomepage?\n\n\nv3\nf3Verbandsarbeit\nHalten Sie die Verbandsarbeit für die Verbreitung von Großkatzen im ländlichen Raum für sinnvoll?\n\n\nv4\nf4Verbandsarbeit\nSehen Sie die Haltung von Großkatzen als eine kulturelle Bereicherung?\n\n\nv5\nf5Verbandsarbeit\nSehen Sie die Notwendigkeit von genetisch veränderten Großkatzen zur Steigerung der Attraktivität des Erlebnishofes?\n\n\nf1\nf1Fuetterung\nFüttern Sie täglich Kellog’s Frosties?\n\n\nf2\nf2Fuetterung\nFüttern Sie wöchentlich Batzen?\n\n\nf3\nf3Fuetterung\nFüttern Sie Joghurt?\n\n\nf4\nf4Fuetterung\nFüttern Sie pflanzlich oder gar vegan?\n\n\ni1\nf1Imagearbeit\nWie aktiv sind Sie bei der Lobbyarbeit in den jeweiligen Landeshauptstädten?\n\n\ni2\nf2Imagearbeit\nWie aktiv sind Sie in der positiven Vermarktung des Verbandes in sozialen Netzwerken?\n\n\ni3\nf3Imagearbeit\nWie aktiv sind Sie in der Vermarktung von Tigernachwuchs an Dritte in den sozialen Netzwerken?\n\n\ni4\nf4Imagearbeit\nWie aktiv sind Sie bei der notwendigen Entnahme von Schadwölfen in Ihrem Einflussgebiet?\n\n\nc1\nf5dauer_verletzung\nWie lange schätzen Sie dauert es im Durchschnitt in Tagen bis sich ein Besucher bei Ihnen gefährlich verletzt?\n\n\nc2\nf6einkommen_jahr\nWieviel Einkommen hatten Sie brutto in Tausend EUR im letzten Jahr?\n\n\nc3\nf7gruppengroesse\nWie groß sind ihre Besuchergruppen im Durchschnitt pro Monat?\n\n\nc4\nf8eis_verkauf\nWie viel 100 Liter Eis verkaufen Sie im Durchschnitt pro Monat?",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Fragebogenanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survey.html#preprocessing",
    "href": "stat-modeling-survey.html#preprocessing",
    "title": "61  Fragebogenanalyse",
    "section": "61.4 Preprocessing",
    "text": "61.4 Preprocessing\nWenn wir uns mit Fragebögen beschäftigen, dann werten wir meisten nicht alle Fragen aus. Das macht auch meistens keinen Sinn. Denn wir stellen zwar recht viele Fragen, aber am Ende müssen wir auch schauen, ob alle Fragen sinnvoll beantwortet wurden oder aber ob wir sehr viele Nichtantworten haben. Davon hängt dann auch die weitere Analyse ab. Wir können uns auch überlegen einzelne Fragen zusammenzufassen, da wir feststellen, dass wir dann am Ende doch nicht so eine feingliedrige Aufteilung wollen.\nIm Weiteren müssen wir dann auch unsere Antworten in Zahlen umwandeln. Wir können ja nur mit Zahlen rechnen. Nur so können wir mittlere Noten ausgeben. Die Umwandlung können wir entweder global wie folgt machen, oder aber wir definieren für jede Antwort die Reihenfolge der Antwortmöglichkeiten. Die automatisierte Zuordnung bewirkt dann, dass wir nicht die Antwortmöglichkeiten in der richtigen, logischen Reihenfolge haben, sondern in der alphanumerischen Ordnung. Für einen ersten Überblick über die Ergebnisse des Fragebogens ist das Vorgehen okay. Später werden wir dann natürlich noch die Fragen richtig formatieren. Aber erstmal müssen wir wissen welche Fragen wir überhaupt auswerten wollen.\n\ntiger_fct_tbl &lt;- tiger_tbl |&gt;\n  mutate(across(everything(), as_factor))\n\nWenn du gleich die richtige Ordnung ahben willst, dann ist es ein wenig mehr Arbeit. Hier können wir dann über die Funktion ordered() die richtige Ordnung der Antwortmöglichkeiten über die Option levels erreichen. Wir haben hier verschiedene Antwortmöglichkeiten in den verschiedenen Fragen. Daher jetzt einmal die Ordnung der Antworten in logischer Reihenfolge. Mir reicht immer der schnelle Überblick, aber das ist dann ja auch Geschmackssache.\n\nalter_ord &lt;- c(\"18-29 Jahre\", \"30-39 Jahre\", \"40-49 Jahre\", \"50-59 Jahre\", \"Über 60 Jahre\")\nzustimmung_ord &lt;- c(\"trifft voll zu\", \"trifft zu\", \"weder noch\", \"trifft nicht zu\", \"trifft gar nicht zu\")\naktiv_ord &lt;- c(\"sehr aktiv\", \"eher aktiv\", \"weder noch\", \"eher nicht aktiv\", \"gar nicht aktiv\")\n\nDu siehst, dass ist eine Menge an Arbeit und zu tippen. Deshalb kann ich dir empfehlen, dass nur bei den Fragen zu machen, die du dann am Ende des Daten Preprocessing auch wirklich nutzen willst. Besonders bei Fragebögen mir sehr vielen Fragen wird es sehr schnell sehr lang. Auf der anderen Seite macht man es dann auch nur einmal. Am Ende wandeln wir dann noch alle Fragen, die wir nicht händisch in einen geordneten Faktor umgewandelt haben, in einen Faktor mit der Funktion mutate_if() um.\n\ntiger_ord_tbl &lt;- tiger_tbl |&gt; \n  mutate(alter = ordered(alter, levels = alter_ord),\n         f1Verbandsarbeit = ordered(f1Verbandsarbeit, levels = zustimmung_ord),\n         f2Verbandsarbeit = ordered(f2Verbandsarbeit, levels = zustimmung_ord),\n         f3Verbandsarbeit   = ordered(f3Verbandsarbeit, levels = zustimmung_ord), \n         f4Verbandsarbeit   = ordered(f4Verbandsarbeit, levels = zustimmung_ord), \n         f5Verbandsarbeit   = ordered(f5Verbandsarbeit, levels = zustimmung_ord),\n         f1Imagearbeit = ordered(f1Imagearbeit, levels = aktiv_ord), \n         f2Imagearbeit = ordered(f2Imagearbeit, levels = aktiv_ord),\n         f3Imagearbeit = ordered(f3Imagearbeit, levels = aktiv_ord),\n         f4Imagearbeit = ordered(f4Imagearbeit, levels = aktiv_ord)) |&gt; \n  mutate_if(is.character, as.factor)\n\nMit unserem geordneten Objekt tiger_ord_tbl können wir uns jetzt einen ersten Überblick über die Antworten der Fragen geben lassen. Dann entscheiden wir, ob wir Fragen zusammenfassen oder gar entfernen wollen. Eventuell müssen wir auch Antwortmöglichkeiten zusammenfassen, wenn einige Antworten einfach nicht ausgewählt wurden. Aber das schauen wir uns jetzt einmal in einer Übersicht an.\n\n\n\n\n\n\nAchtung, as.factor() vs. as_factor() macht einen Unterschied!\n\n\n\nWenn du deine Wörter &lt;chr&gt; in Zahlen umwandeln willst, dann musst du ja den Umweg über eine Faktorumwandlung gehen. Hier ist es jetzt mal sehr wichtig, dass du die Funktion as.factor() nutzt. Eigentlich nutze ich immer die Funktion as_factor() aber diese Funktion hat ein Feature, was uns hier bei den Fragebögen auf die Füße fällt. Schauen wir uns die Sachlage einmal in einem Beispiel an. Wir haben folgende drei Antworten und wollen diese einmal in einen Faktor umwandeln.\n\nanswer_vec &lt;- c(\"weder_noch\",  \"eher_zufrieden\", \"sehr_zufrieden\")\n\nWenn wir jetzt die Funktion as.factor() nutzen, dann werden unsere Level alphanumerisch sortiert. Das hat den Vorteil, dass jede Frage immer die gleichen Antwortsortierungen erhält. Dadurch ist dann auch gewährleistet, dass in jeder Frage die Antwortmöglichkeiten dann nach der Umwandlung in eine Zahl mit as.numeric() auch wirklich das gleiche Wort bedeutet.\n\nanswer_vec |&gt; \n  as.factor() \n\n[1] weder_noch     eher_zufrieden sehr_zufrieden\nLevels: eher_zufrieden sehr_zufrieden weder_noch\n\n\nDie Sortierung in as_factor() wird nach dem Auftreten des Wortes in dem Vektor gemacht. Das heißt, je nach Frage, hat dann eine Frage eine andere Sortierung der Antwortmöglichkeiten. Das ist unglaublich ungünstig, da wir dann ja nach einer Umwandlung der Faktorenlevel in eine Zahl mit as.numeric() nicht mehr die gleichen Antwortmöglichkeiten hinter jeder Zahl haben!\n\nanswer_vec |&gt; \n  as_factor() \n\n[1] weder_noch     eher_zufrieden sehr_zufrieden\nLevels: weder_noch eher_zufrieden sehr_zufrieden\n\n\n\n\n\n61.4.1 Univariate Analyse\nEigentlich ist die univariate Analyse ja auch gleich ein Teil der Darstellung des Fragebogens und gehört nicht so richtig zum Preprocessing. Auf der anderen Seite sehen wir in der univariaten Analyse auch das erste Mal alle Fragen und Antworten auf einem Blick in einer Tabelle und können dann entschieden, ob wir eine Frage rausnehmen wollen oder eben nicht. Oder ob unsere Antworten zu den Fragen passen oder es auffällige Antwortmuster gibt. Nach was wollen wir nun als erstes einmal Ausschau halten?\n\nFehlende Werte\n\nGibt es eine Frage, bei der fast keiner geantwortet hat? Haben wir also eigentlich gar keine Information in der Frage enthalten, da die Antworten fast alle NA also fehlend sind?\n\nGleiche Einträge\n\nGibt es Fragen, bei denen alle das Gleiche geantwortet haben? Haben wir also gar keine Varianz in den Fragen? Wenn alle nur eine und dieselbe Antwort geben, dann ist die Aussage der Frage sehr begrenzt. Wir können dann auch keine Gruppenvergleich rechnen, da ja alle immer das gleiche angekreuzt haben.\n\nSehr diverse Antworten\n\nGibt es Fragen, wo die Antworten sehr heterogen sind und sich somit fast nicht sinnvoll zusammenfassen lassen? Das haben wir dann häufig bei Freitextfeldern. Es kann dann sein, dass wir sehr viele verschiedene Antworten erhalten, die wir irgendwie nicht sinnvoll zusammen kriegen.\n\n\nFür die univariate Analyse nutze ich das R Paket {gtsummary} mit der Funktion tbl_summary(). Die Funktion baut uns über alle Spalten in dem Datensatz eine deskriptive Information mit der Anzahl und der Häufigkeit der jeweiligen Antwortkategorie. Das ist super schnell und super effizient. Innerhalb von Sekunden haben wir unsere Fragen einaml in einer Tabelle wiedergeben. Das einzige was ich hier noch angepasst habe ist, dass ich die Fragen als Text dann als Spaltennamen gesetzt ahbe. So haben wir dann die richtigen Fragen in der Tabelle stehen. Sonst brauchen wir die Langform der Fragen ja nicht in den Abbilungen. Hier in der Tabelle finde ich die richtigen Fragen sehr sinnvoll.\n\ntiger_tbl |&gt; \n  set_names(short_question_tbl$question) |&gt; \n  tbl_summary()\n\nIch habe dir jetzt die Ausgabe der Funktion tbl_summary() einmal in dem folgenden Kasten eingeklappt. Sonst sehen wir vor lauter Tabelle nichts mehr und so kannst du hier übersichtlich lesen. Wir sehen, dass die Frage “Haben Sie weitere Erwerbsquellen neben dem Erlebnishof außer Landwirtschaft?” sinnlos war. Zum einen war es Freitext und zum anderen haben wir mit 139 fehlenden Werten kaum Rückmeldungen zu der Frage. In unserem Freitexten sind dann so heterogene Antworten, dass sich hier eine Auswertung nicht sinnvoll durchführen lässt. Auch das Alter hat mit nur drei unter 29 Jährigen eine ungünstige Verteilung. Hier müsste man mal die Antworten zusammenfassen. Wir sehen auch, dass wir eigentlich gar keine Teilnehmerinnen in unserer Umfrage haben. So können wir dann einmal die Fragen durchgehen und dann immer entscheiden, ob wir eine Frage ganz rausnehmen, wie die Frage zu den Erwerbsquellen, oder aber Zusammenfassen müssen.\n\n\n\n\n\n\nUnivariate Ausgabe der Funktion tbl_summary()\n\n\n\n\n\n\n\n\n\nTabelle 61.3— Univariate Analyse der einzelenen Fragen und deren Antwortverteilung. Anhand der Tabelle können fehlende Werte und ungleichmäßig beantwortete Fragen er kannt und dann ausgeschlossen werden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1731\n\n\n\n\nWelches Geschlecht haben Sie?\n\n\n\n\n    Männlich\n155 (90%)\n\n\n    Weiblich\n18 (10%)\n\n\nWie alt sind Sie?\n\n\n\n\n    18-29 Jahre\n3 (1.7%)\n\n\n    30-39 Jahre\n22 (13%)\n\n\n    40-49 Jahre\n42 (24%)\n\n\n    50-59 Jahre\n49 (28%)\n\n\n    Über 60 Jahre\n57 (33%)\n\n\nHalten Sie aktuell Tiger oder andere Großkatzen auf Ihrem Erlebnishof?\n\n\n\n\n    Ja\n146 (84%)\n\n\n    Nein\n27 (16%)\n\n\nIst Ihr Erlebnishof Haupterwerb oder Nebenerwerb?\n\n\n\n\n    Haupterwerb\n114 (66%)\n\n\n    Nebenerwerb\n59 (34%)\n\n\nHaben Sie weitere Erwerbsquellen neben dem Erlebnishof außer Landwirtschaft?\n\n\n\n\n    Blumenanbau\n2 (5.9%)\n\n\n    Direktvermarktung\n1 (2.9%)\n\n\n    Ferienhof\n2 (5.9%)\n\n\n    forst\n1 (2.9%)\n\n\n    Forst\n3 (8.8%)\n\n\n    Forst, LN verpachtet\n1 (2.9%)\n\n\n    Forstwirtschaft\n5 (15%)\n\n\n    Forstwirtschaft + Verpachtung\n1 (2.9%)\n\n\n    Grünland 1,5 ha und 3 ha Wald\n1 (2.9%)\n\n\n    Hofladen\n2 (5.9%)\n\n\n    Holzwirtschaft\n1 (2.9%)\n\n\n    Kartoffeln\n2 (5.9%)\n\n\n    Pferde\n2 (5.9%)\n\n\n    Pferdehaltung\n1 (2.9%)\n\n\n    Schafhaltung\n2 (5.9%)\n\n\n    Strohhandel\n1 (2.9%)\n\n\n    Vermietung und Verpachtung\n1 (2.9%)\n\n\n    Wald\n3 (8.8%)\n\n\n    Weihnachtsbäume\n2 (5.9%)\n\n\n    Unknown\n139\n\n\nWelche Stellung haben Sie auf dem Erlebnishof inne?\n\n\n\n\n    Angestellter\n1 (0.6%)\n\n\n    Betriebsleiter\n141 (82%)\n\n\n    Hofnachfolger\n14 (8.1%)\n\n\n    König\n5 (2.9%)\n\n\n    Leitung Feldwirtschaft\n4 (2.3%)\n\n\n    Leitung Tierproduktion\n3 (1.7%)\n\n\n    Sonstiges\n5 (2.9%)\n\n\nSoll die Verbandsarbeit in den nächsten Jahren digital in den sozialen Netzwerken ausgeweitet werden?\n\n\n\n\n    trifft voll zu\n8 (4.7%)\n\n\n    trifft zu\n36 (21%)\n\n\n    weder noch\n45 (26%)\n\n\n    trifft nicht zu\n49 (29%)\n\n\n    trifft gar nicht zu\n32 (19%)\n\n\n    Unknown\n3\n\n\nLesen Sie quartalsweise den Newsletter auf der Verbandshomepage?\n\n\n\n\n    trifft voll zu\n19 (11%)\n\n\n    trifft zu\n50 (29%)\n\n\n    weder noch\n46 (27%)\n\n\n    trifft nicht zu\n32 (19%)\n\n\n    trifft gar nicht zu\n23 (14%)\n\n\n    Unknown\n3\n\n\nHalten Sie die Verbandsarbeit für die Verbreitung von Großkatzen im ländlichen Raum für sinnvoll?\n\n\n\n\n    trifft voll zu\n9 (5.3%)\n\n\n    trifft zu\n97 (57%)\n\n\n    weder noch\n39 (23%)\n\n\n    trifft nicht zu\n19 (11%)\n\n\n    trifft gar nicht zu\n6 (3.5%)\n\n\n    Unknown\n3\n\n\nSehen Sie die Haltung von Großkatzen als eine kulturelle Bereicherung?\n\n\n\n\n    trifft voll zu\n23 (14%)\n\n\n    trifft zu\n79 (46%)\n\n\n    weder noch\n34 (20%)\n\n\n    trifft nicht zu\n26 (15%)\n\n\n    trifft gar nicht zu\n8 (4.7%)\n\n\n    Unknown\n3\n\n\nSehen Sie die Notwendigkeit von genetisch veränderten Großkatzen zur Steigerung der Attraktivität des Erlebnishofes?\n\n\n\n\n    trifft voll zu\n8 (4.7%)\n\n\n    trifft zu\n57 (34%)\n\n\n    weder noch\n50 (29%)\n\n\n    trifft nicht zu\n37 (22%)\n\n\n    trifft gar nicht zu\n18 (11%)\n\n\n    Unknown\n3\n\n\nFüttern Sie täglich Kellog's Frosties?\n\n\n\n\n    Ja\n101 (70%)\n\n\n    Nein\n44 (30%)\n\n\n    Unknown\n28\n\n\nFüttern Sie wöchentlich Batzen?\n\n\n\n\n    Ja\n111 (77%)\n\n\n    Nein\n34 (23%)\n\n\n    Unknown\n28\n\n\nFüttern Sie Joghurt?\n\n\n\n\n    Ja\n63 (43%)\n\n\n    Nein\n82 (57%)\n\n\n    Unknown\n28\n\n\nFüttern Sie pflanzlich oder gar vegan?\n\n\n\n\n    Ja\n40 (28%)\n\n\n    Nein\n105 (72%)\n\n\n    Unknown\n28\n\n\nWie aktiv sind Sie bei der Lobbyarbeit in den jeweiligen Landeshauptstädten?\n\n\n\n\n    sehr aktiv\n4 (2.9%)\n\n\n    eher aktiv\n51 (37%)\n\n\n    weder noch\n60 (44%)\n\n\n    eher nicht aktiv\n20 (15%)\n\n\n    gar nicht aktiv\n2 (1.5%)\n\n\n    Unknown\n36\n\n\nWie aktiv sind Sie in der positiven Vermarktung des Verbandes in sozialen Netzwerken?\n\n\n\n\n    sehr aktiv\n20 (15%)\n\n\n    eher aktiv\n69 (50%)\n\n\n    weder noch\n41 (30%)\n\n\n    eher nicht aktiv\n6 (4.4%)\n\n\n    gar nicht aktiv\n1 (0.7%)\n\n\n    Unknown\n36\n\n\nWie aktiv sind Sie in der Vermarktung von Tigernachwuchs an Dritte in den sozialen Netzwerken?\n\n\n\n\n    sehr aktiv\n2 (1.4%)\n\n\n    eher aktiv\n44 (31%)\n\n\n    weder noch\n66 (47%)\n\n\n    eher nicht aktiv\n24 (17%)\n\n\n    gar nicht aktiv\n5 (3.5%)\n\n\n    Unknown\n32\n\n\nWie aktiv sind Sie bei der notwendigen Entnahme von Schadwölfen in Ihrem Einflussgebiet?\n\n\n\n\n    sehr aktiv\n18 (13%)\n\n\n    eher aktiv\n81 (58%)\n\n\n    weder noch\n33 (24%)\n\n\n    eher nicht aktiv\n7 (5.0%)\n\n\n    gar nicht aktiv\n1 (0.7%)\n\n\n    Unknown\n33\n\n\nWie lange schätzen Sie dauert es im Durchschnitt in Tagen bis sich ein Besucher bei Ihnen gefährlich verletzt?\n75 (26, 131)\n\n\nWieviel Einkommen hatten Sie brutto in Tausend EUR im letzten Jahr?\n769 (395, 1,326)\n\n\nWie groß sind ihre Besuchergruppen im Durchschnitt pro Monat?\n6.0 (4.0, 9.0)\n\n\nWie viel 100 Liter Eis verkaufen Sie im Durchschnitt pro Monat?\n19 (15, 25)\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n61.4.2 Fehlende Werte\nWarum haben wir fehlende Werte in einem Fragebogen? Wenn wir nicht die Teilnehmer:innen zwingen jede Frage zu beantworten, dann kann es sein, dass einige Fragen nicht beantworten. Warum das so ist, ist eine gute Frage. Wenn die unbeantworteten Fragen gegen Ende auftreten, dann mag es Ermüdung sein. Ab und an übersieht man eventuell auch eine Frage oder aber versteht die Frage nicht. Es gibt also eine Menge Möglichkeiten. In unserem Beispiel treten fehlende Antworten bei der Fütterung auf. Wenn ein Hof keine Tiger hat, dann muss er auch die Tiger nicht füttern. Das macht dann eher einen Block an fehlenden Daten aus. Wir wollen jetzt einmal das R Paket {naniar} nutzen um fehlende Werte zu visualisieren. Etwas dagegen tun, werden wir hier nicht, dazu gibt es dann ein extra Kapitel Imputation von fehlende Werten in diesem Buch.\nIn der Abbildung 61.2 siehst du einmal die fehlenden Werte als schwarzen Balken dargestellt. Die Spalten stellen dabei die Fragen dar und die Prozente hinter den Fragen die Anteile an fehlenden Werten. Teilweise wird hier die letzte Spalte abgeschnitten und wir müssten hier dann besser mit den verkürzten Namen arbeiten. Da ich aber die Abbildungen nur für eine Übersicht für mich selber nutze, kann ich damit leben. Wir sehen einmal als Block die fehlenden Werte für die Fütterung. Das sind all die Teilnehmer:innen, die keine Tiger halten. Dann siehst du noch einige horizontale Linien. Diese Linien sind Teilnehmer, die systematisch keine Fragen beantwortet haben. Sonst haben wir eigentlich keine richtigen auffälligen Muster in den Daten. Die Imagearbeit wurde nur relativ spärlich im Vergleich zur Verbandsarbeit beantwortet.\n\ntiger_tbl |&gt; \n  vis_miss() \n\n\n\n\n\n\n\nAbbildung 61.2— Darstellung der fehlenden Werte in dem Tigerdatensatz. Die fehlenden Werte werden als schwarzer Balken dargstellt. Der Block fehlender Werte bei der Fütterung stellt die Teilnehmer:innen dar, die keine Tiger halten. Horizontale Linien stellen Teilnehmer dar, die systematisch keine Fragen beantwortet haben.\n\n\n\n\n\nHäufig stellt sich dann auch die Frage, wie denn die fehlenden Werte untereinander zusammenhängen. Haben viele nur eine Frage nicht beantwortet oder haben die Teilnehmer dann auch andere Fragen ebenfalls ähnlich nicht beantwortet? Diese Zusammenhänge versucht die Abbildung 61.3 darzustellen. Wir haben einmal für fünfzehn Kombinationen der fehlenden Werte die Anzahlen dargestellt. So sehen wir das wir bei 72 Personen keine Antwort bei der Frage f2Soziodemografisch haben. Sonst finden Sie bei diesen Personen aber keine weiteren fehlenden Werte. Anders sieht es dann bei den nächsten zwanzig Personen aus. Hier haben wir fehlende Werte bei der Frage f2Soziodemografisch und f2Imagearbeit. So können wir dann schauen, ob wir auch dort noch Muster erkennen. Wir sehen auch wieder die Blöcke der Fütterungsfrage. Da haben wir natürlich dann auch immer die vier Fragen zur Fütterung als fehlend.\n\ntiger_tbl |&gt; \n  gg_miss_upset(nsets = 15)\n\n\n\n\n\n\n\nAbbildung 61.3— Darstellung des Zusammenhangs der fehlenden Werte unter den Teilnehmer:innen des Fragebogens. Unter den Balkendiagramm sind die jeweiligen Kombinationen der fehlenden Werte der Fragen für diese Gruppe der Teilnehmer:innen dargestellt.\n\n\n\n\n\nAm Ende werten wir ja viele Fragen erstmal alleine aus. Daher schauen wir uns an, wie die Verteilung der Antworten pro Frage ist. Daher macht es dort noch nicht so viel aus, wenn du fehlende Werte hast. Wenn du dann aber Fragen miteinander in Bezug setzen willst und eventuell die Anteile der Antworten in Beziehung setzen willst, dann macht schon was aus, wie viele fehlende Werte du pro Frage hast. Aber auch hier kommt es dann sehr stark auf den Kontext und die einzelnen Fragen an. Daher kann ich dir hier keine generelle Antwort liefern. Wichtig ist erstmal, dass du das Problem der fehlenden Werte kennst und darstellen kannst.\nJetzt können wir noch ein wenig Zahlen wiedergeben lassen. Wir wollen ja auch wissen, wie viele fehlenden Werte wir dann in den Daten überhaupt haben. Dafür können wir dann die fehlenden Werte über die Funktion n_miss() zählen und durch die gesamte Anzahl an Beobachtungen mit n_complete() teilen. Wir kriegen dann raus, dass wir gut 11.3% fehlende Werte in unserem Datensatz haben.\n\nn_miss(tiger_tbl)/n_complete(tiger_tbl)\n\n[1] 0.1126957\n\n\nÜber die Funktion miss_case_summary() kannst du dir dann noch mehr Informationen zu den einzelnen Beobachtungen wiedergeben lassen. Über die Funktion print() kannst du dir dann noch mehr Zeilen als die normalen zehn Zeilen ausgeben lassen.\n\ntiger_tbl |&gt; \n  miss_case_summary() |&gt; \n  print(n = 12)\n\n# A tibble: 173 × 3\n    case n_miss pct_miss\n   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1    66     11     47.8\n 2   158      9     39.1\n 3   173      9     39.1\n 4    79      8     34.8\n 5   170      8     34.8\n 6    31      7     30.4\n 7   154      7     30.4\n 8   155      7     30.4\n 9   160      7     30.4\n10   147      6     26.1\n11   151      6     26.1\n12   159      6     26.1\n# ℹ 161 more rows\n\n\nSchauen wir uns mal die Personen mit sieben oder mehr fehlenden Werten einmal an. Wir nutzen dazu die Funktion slice(), die es uns erlaubt die Zeilen zu extrahieren, welche für die Beobachtungen stehen.\n\ntiger_tbl |&gt; \n  slice(c(66, 158, 173, 79, 170, 31, 154, 155, 160))\n\n# A tibble: 9 × 23\n  geschlecht alter       haben_Sie_tiger f1Soziodemografisch f2Soziodemografisch\n  &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;               &lt;chr&gt;              \n1 Weiblich   30-39 Jahre Ja              Haupterwerb         &lt;NA&gt;               \n2 Männlich   40-49 Jahre Nein            Nebenerwerb         &lt;NA&gt;               \n3 Männlich   Über 60 Ja… Nein            Haupterwerb         &lt;NA&gt;               \n4 Männlich   Über 60 Ja… Ja              Nebenerwerb         Forst              \n5 Männlich   40-49 Jahre Nein            Nebenerwerb         &lt;NA&gt;               \n6 Männlich   50-59 Jahre Ja              Nebenerwerb         &lt;NA&gt;               \n7 Männlich   Über 60 Ja… Nein            Nebenerwerb         &lt;NA&gt;               \n8 Männlich   40-49 Jahre Nein            Nebenerwerb         Pferde             \n9 Weiblich   40-49 Jahre Nein            Haupterwerb         Wald               \n# ℹ 18 more variables: f3Soziodemografisch &lt;chr&gt;, f1Verbandsarbeit &lt;chr&gt;,\n#   f2Verbandsarbeit &lt;chr&gt;, f3Verbandsarbeit &lt;chr&gt;, f4Verbandsarbeit &lt;chr&gt;,\n#   f5Verbandsarbeit &lt;chr&gt;, f1Fuetterung &lt;chr&gt;, f2Fuetterung &lt;chr&gt;,\n#   f3Fuetterung &lt;chr&gt;, f4Fuetterung &lt;chr&gt;, f1Imagearbeit &lt;chr&gt;,\n#   f2Imagearbeit &lt;chr&gt;, f3Imagearbeit &lt;chr&gt;, f4Imagearbeit &lt;chr&gt;,\n#   f5dauer_verletzung &lt;dbl&gt;, f6einkommen_jahr &lt;dbl&gt;, f7gruppengroesse &lt;dbl&gt;,\n#   f8eis_verkauf &lt;dbl&gt;\n\n\nWir sehen dann einmal die ganzen Personen, die sehr viele fehlenden Werte bei den Antworten haben. Ich möchte jetzt ungern noch mehr Beobachtungen rauschmeißen, die keine Tiger halten. Deshalb bleiben fliegen nur die drei Tigerhalter aus den Daten und der Rest bleibt drin. Das ist jetzt eine bewusste Entscheidung von mir, du musst dann schauen, wie du das in deinen Daten machst. Dann entferne ich einmal die drei Beobachtungen mit slice() aus meinen Daten.\n\ntiger_tbl &lt;- tiger_tbl |&gt; \n  slice(-c(66, 79, 31))\n\nDann schauen wir zum Anschluss nochmal, ob wir auch das entfernt haben, was wir entfernen wollten. In der Abbildung 61.4 siehst du nochmal die Visualisierung der fehlenden Werte nachdem wir unsere drei Beobachtungen entfernt haben. Das sieht soweit super aus, denn wir haben jetzt schon ein etwas einheitlicheres Bild vorliegen.\n\ntiger_tbl |&gt; \n  vis_miss() \n\n\n\n\n\n\n\nAbbildung 61.4— Darstellung der fehlenden Werte in dem Tigerdatensatz nachdem wir unsere drei Beobachtungen entfernt haben. Die fehlenden Werte werden als schwarzer Balken dargstellt. Der Block fehlender Werte bei der Fütterung stellt die Teilnehmer:innen dar, die keine Tiger halten. Horizontale Linien stellen Teilnehmer dar, die systematisch keine Fragen beantwortet haben.\n\n\n\n\n\n\n\n\n\n\n\nImputation von fehlenden Werten\n\n\n\nEine Möglichkeit mit fehlenden Werten umzugehen ist die Imputation von fehlende Werten, die ich in einem anderen Kapitel erläutere. Ich wäre aber mit dem Einsatz der Methoden sehr vorsichtig, da diese Methoden teilweise nicht in Veröffentlichungen anerkannt werden. Die Kritik lautet dabei, dass künstliche Daten erstellt werden, die nicht den realen Hintergrund abbilden. Daher ist die Verwendung von einem Imputationsalgorithmus immer gesondert zu diskutieren und zu bewerten.\n\n\n\n\n61.4.3 Korrelation\nWarum Korrelation der Fragen? Nun es kann sein, dass wir Fragen gebaut haben, die eventuell das Gleiche abfragen. Oder aber, dass die Teilnehmer:innen unseres Fragebogen eine Frage genau so beantworten wie eine andere Frage. Das lässt sich bei der Vielzahl an Fragen kaum überblicken. Deshalb schauen wir uns einmal die Korrelation zwischen den Fragen an um zu sehen, ob wir nicht Fragen haben, die das Gleiche aussagen.\nZuerst bauen wir uns einen Datensatz tiger_clean_num_tbl indem alle Spalten nur noch numerisch sind. Wir können die Korrelation nur mit numerischen Spalten rechnen. Wenn du noch Wörter als Antworten in den Spalten stehen hast, dann kannst du über as.factor() erstmal die Antworten in Faktoren umwandeln. Das habe ich hier schon gemacht und nutze daher den Datensatz tiger_fct_tbl. Am Ende setze ich dann noch die Bezeichnung der Fragen auf die Abkürzungen der Fragen, sonst klappt es nicht richtig mit der Darstellung. Dafür sind dann die ursprünglichen Spaltenbezeichungen zu lang.\n\ntiger_clean_num_tbl &lt;- tiger_fct_tbl |&gt; \n  mutate_all(as.numeric) |&gt;  \n  set_names(short_question_tbl$ques_id)\n\nWenn du dir nur die Korrelation zwischen den Spalten hier anschaust, dann nutze zuerst die Option use = \"pairwise.complete.obs\". Dann werden nur die Beobachtungen genutzt die in den jeweiligen Spalten, die verglichen werden, vorhanden sind. Ich nutze hier dann auch den Korrelationskoeffizienten nach Pearson. Da wir eigentlich keine normalverteilten Daten in den Antwortmöglichkeiten vorliegen haben, müsste ich eigentlich besser Spearman nutzen, aber Spearman neigt zu Fehlern, wenn die Daten nicht richtig passen. Da ich mir hier aber nur einen groben Überblick verschaffen möchte, kommt es auf Abweichungen in dem Korrelationskoeffizienten nicht an. Auch wenn mein Korrelationskoeffizienten verzerrt ist, wird ein hoher Korrelationskoeffizienten nach Pearson auf eine Korrelation hindeuten. Das ist ja auch nur mein Ziel hier, nämlich ähnliche Fragen zu erkennen. Den berechneten Korrelationskoeffizienten nehme ich dann aber nicht zu ernst. Am Ende nehme ich noch die Frage f2Soziodemografisch aus der Betrachtung, da hier zu viele fehlende Werte vorliegen.\n\ncor_mat &lt;- tiger_clean_num_tbl |&gt; \n  select(-matches(\"Soziodemografisch\")) |&gt; \n  cor(use = \"pairwise.complete.obs\")\n\nIch könnte mir jetzt die berechnete Korrelationsmatrix cor_mat einmal in R als Matrix anschauen. Schöner ist es in der Abbildung 61.5 als Korrelationsplot. Trotz der Verwendung des Korrelationskoeffizienten nach Person konnten wir nicht für alle paarweisen Vergleiche der Fragen eine Korrelation berechnen. Wenn es nicht möglich war, dann sehen wir ein Fragezeichen in der Abbildung. Wenn wir eine zeitlang auf die Abbildung schauen, sehen wir, dass wir eigentlich keinen große Korrelation zwischen den Fragen haben. Der größte Wert ist \\(-0.38\\) sowie \\(0.31\\) und damit noch sehr nah an der Null und keiner Korrelation. Ich bin somit mit der Korrelation zufrieden. Wir müssen keine Fragen zusammenfassen oder entfernen, weil die Korrelation zwischen den Fragen zu groß ist und damit die Fragen das Gleiche aussagen.\ncorrplot(cor_mat, method = 'number', \n         col = rev(wes_palette(\"Zissou1\", 8, type = \"continuous\")))\n\ncorrplot.mixed(cor_mat, lower = 'pie', upper = 'ellipse')\n\n\n\n\n\n\n\n\n\n\n\n(a) Numerische Werte.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Tortendiagramm und Ellipse\n\n\n\n\n\n\n\nAbbildung 61.5— Visualisierung der Korrelationsmatrix nach Pearson für alle Fragen. Teilweise konnte der Korrelationskoeffizient nicht berechnet werden. Wir haben keine Korrelation zwischen den Fragen vorliegen. Teilweise ist es sher schwer was in den Zahlen zu erkennen, da helfen dann mal wirklich die Tortendiagramme und die Elipsen weiter um die entsprechenden Korrelationen zu finden.\n\n\n\n\n\n61.4.4 Zusammenfassen\nManchmal wollen wir dann doch Fragen zusammenfassen. Irgendwie stellen wir fest, nachdem wir die Antworten gesehen haben, dass wir dann doch irgendwie nicht so eine detaillierte Fragenaufteilung wollen. Dann können wir natürlich auch Fragen zusammenfassen. Eine goldene Regel gibt es dafür nicht, ich zeige aber mal die eine oder andere Möglichkeit. Es kann auch sein, dass wir festgestellt haben, dass einzelne Fragen sehr stark miteinander korrelieren und wir deshalb die Fragen zusammenfasen wollen anstatt eine der Fragen aus der Analyse zu nehmen. Neben dem Fragen zusammenfassen, können wir auch Antwortkategorien zusammenfassen. Wir wollen dann nicht die fünf Antwortmöglichkeiten sondern eben nur drei oder zwei. Dann können wir das auch machen, müssen es nur in unserem Report oder der Abschlussarbeit berichten.\n\n…von einzelnen Fragen\nIm Folgenden habe ich mich dazu entschieden, dass mich die Fütterung dann doch nicht so sehr im Detail interesiert. Ich möchte dann alle Fragen einmal zusammenfassen, die mit der Fütterung zu tun haben. Dafür nutze ich dann den folgenden Code. Die Idee ist sehr simple. Ich addiere die Spalten für Fütterung auf und erhalte die neue Spalte Feutterung. Dann kann ich entscheiden, wie ich mit der Spalte weiter umgehen will. Theoretisch können natürlich noch andere mathematischen Operatoren, wie Subtraktion einer Spalte von der anderen oder das Produkt genutzt werden. Da kommt es dann aber auf das konkrete Beispiel an. Der BMI ist sicherlich die bekanneste Zusammenfassung. Häufig kennen die Teilnehmer:innen ihre Größe und das Gewicht, aber selten den BMI oder wissen die entsprechende Formel.\n\nfeed_tbl &lt;- tiger_fct_tbl |&gt;\n  select(matches(\"Fuetterung\")) |&gt; \n  mutate(across(matches(\"Fuetterung\"), \\(x) as.numeric(x)-1)) |&gt; \n  mutate(Fuetterung = rowSums(across(matches(\"Fuetterung\"))))\n\nIn der Tabelle 61.4 siehst du dann einmal das Ergebnis unserer Summenbildung über die Spalten der Fütterung. Wir haben jetzt in der Spalte Fuetterung die Summen der Spalten enthalten. Je größer die Zahl, desto mehr wurde gefüttert. Wenn ich das mal so ganz allgemein formulieren will.\n\n\n\n\nTabelle 61.4— Tabelle der Zusammenfassung der Fragen zur Fütterung unserer Großkatzen als Summe in der Spalte Fuetterung. Ein hoher Wert bedeutet, dass eben viel gefüttert wurde.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuetterung\nf1Fuetterung\nf2Fuetterung\nf3Fuetterung\nf4Fuetterung\n\n\n\n\n0\n0\n0\n0\n0\n\n\n3\n1\n0\n1\n1\n\n\n1\n1\n0\n0\n0\n\n\n2\n1\n0\n0\n1\n\n\n2\n1\n0\n0\n1\n\n\n1\n0\n0\n0\n1\n\n\n4\n1\n1\n1\n1\n\n\n\n\n\n\n\n\nWir nutzen jetzt einmal die Funktion tabyl() um uns die prozentuale Verteilung der neuen Einträge in der Spalte Futterung anzeigen zu lassen. Ich empfehle immer die Spalte valid_percent vorzuziehen, da wir in dieser Spalte nicht die fehlenden Werte mit reingerechnet haben.\n\nfeed_perc_obj &lt;- feed_tbl |&gt; \n  tabyl(Fuetterung) |&gt; \n  round(2)\n\nIn der Tabelle 61.5 sehen wir dann einmal die Übersicht unser Zusammenfassung. Spannenderweise füttern sechs Betriebe ihre Tiger gar nicht. Was ein spannendes Ergebnis ist. Oder aber sie füttern die Tiger nicht nach den entsprechenden Fragen. Das kann natürlich auch Auftreten. Wie wir jetzt die Zusammenfassung weiter interpretieren ist so ein Sache. Das Beispiel ist jetzt etwas konstruiert, aber wir wissen immerhin, dass die meisten Teilnehmer:innen zwei der Fütterungsfragen mit Ja beantwortet haben.\n\n\n\n\nTabelle 61.5— Zusammenfassung der Spalte Fuetterung nach den neuen Einträgen und den entsprechenden prozentualen Auftreten. Die vorzuziehende Spalte valid_percent beschreibt das prozentuale Auftreten ohne die fehlenden Werte in der Spalte.\n\n\n\n\n\n\nFuetterung\nn\npercent\nvalid_percent\n\n\n\n\n0\n6\n0.03\n0.04\n\n\n1\n38\n0.22\n0.26\n\n\n2\n50\n0.29\n0.34\n\n\n3\n39\n0.23\n0.27\n\n\n4\n12\n0.07\n0.08\n\n\nNA\n28\n0.16\nNA\n\n\n\n\n\n\n\n\n\n\n… von Antwortenmöglichkeiten\nManchmal ist es sinnvoll Antwortmöglichkeiten zusammenzulegen. In unserem Fragebogen haben wir zum Beispiel nur drei 18 bis 29 Jährige. Das ist echt wenig. Dann können wir diese drei Personen auch zu den 30 bis 39 Jährigen packen und eine neue Kategorie erschaffen. Dann fassen wir die beiden mittleren Kategorien auch noch zusammen, weil wir es können. Wir nutzen dazu die R Funktion case_when() mit der wir uns dann nach Regeln neue Werte erschaffen können. Hier einmal das Beispiel für das Alter. Ich behalte mir immer die alte Spalte mit in den Daten um im nachhinein nochmal zu schauen, ob auch alles richtig transformiert wurde.\n\nalter_combined_tbl &lt;- tiger_fct_tbl |&gt; \n  select(alter) |&gt; \n  mutate(alter_recode = case_when(alter %in% c(\"Über 60 Jahre\") ~ \"&gt;60 Jahre\",\n                                  alter %in% c(\"40-49 Jahre\", \"50-59 Jahre\") ~ \"40-59 Jahre\",\n                                  alter %in% c(\"18-29 Jahre\", \"30-39 Jahre\") ~ \"&lt;39 Jahre\")) |&gt; \n  mutate(alter_recode = ordered(alter_recode, levels = c(\"&lt;39 Jahre\", \"40-59 Jahre\", \"&gt;60 Jahre\")))\n\nDann wollen wir uns nochmal das Ergebnis als eine Tabelle anschauen um zu sehen, ob auch alles in der Transformation geklappt hat. Ziel soll es ja sein Kategorien zu finden, die dann auch wirklich Beobachtungen enthalten. Wenn wir uns also wieder halb leere Kategorien bauen, dann müssen wir nochmal ran.\n\nalter_combined_perc_obj &lt;- alter_combined_tbl |&gt; \n  tabyl(alter_recode) |&gt; \n  mutate_if(is.numeric, round, 2)\n\nIn der Tabelle 61.6 siehst du dann das Ergebnis der Zusammenfassung der Antwortkategorien zum Alter. Wir haben zwar in der kleinsten Kategorie immer nur noch 14% der Beobachtungen aber das sind jetzt immerhin 25 Personen und nicht mehr 3 Teilnehmer. Das macht dann schon einen Unterschied mit dem wir dann besser rechnen könnten.\n\n\n\n\nTabelle 61.6— Zusammenfassung der Kategorien der Spalte Alter nach neuen, selbstdefinierten Kriterien. Ziel war es wenig belegte Kategorien zusammenzufassen.\n\n\n\n\n\n\nalter_recode\nn\npercent\n\n\n\n\n&lt;39 Jahre\n25\n0.14\n\n\n40-59 Jahre\n91\n0.53\n\n\n&gt;60 Jahre\n57\n0.33\n\n\n\n\n\n\n\n\nAm Ende ist es natürlich wichtig, das du dann im Methodenteil deiner Arbeit oder Veröffentlichung genau beschreibst, wie du und warum du die Antwortmöglichkeiten zusammengefasst hast. Allgemein spricht nichts dagegen Antworten zusammenzufassen, es sollte nur nicht der Anschein erweckt werden, dass du dir dann am Ende die Ergebnisse zurechtbiegst.\n\n\n\n61.4.5 Flowchart\nDas Flussdiagramm (eng. flowchart) liefert nochmal eien schönen Überblick über das Preprocessing der Daten. Du kannst damit gut zeigen, wie sich die Anzahl der Teilnehmer:innen von der Rekrutierung hin zur eigentlichen Analyse entwickelt hat. Es gibt eine große Anzahl an möglichen Darstellungsformen, wie eine Google Suche nach consort flowchart zeigt. Deshalb gibt es auch nicht die Flowchart für eine Fragebogenanalyse. Lasse dich einfach inspirieren und baue deine Flowchart so, dass jeder einfach sehen kann, wie sich deine Anzahl der Teilnehmer:innen geändert hat. Es kann auch sein, das diene Flowchart nur aus drei Kacheln besteht. Das ist dann auch in Ordnung und trotzdem besser als ein Fließtext.\nIn der Abbildung 61.6 siehst du dann einmal eine beispielhafte Flowchart für unseren Fragebogen zu der Haltung von Großkatzen. Ich habe die Flowchart in PowerPoint gebaut und die PowerPoint Vorlage kannst du gerne selber verändern. Wie du dann so eine Flowchart aufbaust, hängt dann sehr von deinen Daten ab. In dem Kasten der entfernten Fragen, habe ich die gängigen Kriterien einmal aufgeführt. Du musst die nicht alle in der Form durchgehen. Es kann auch sein, dass du aus anderen Gründen Fragen entfernen möchtest, weil die Fragen zum Beispiel dann doch nicht mehr zur Forschunsgfrage passen. Ich habe dann noch unten die bivariaten Vergleiche für die beiden Fragen zur Haltung von Großkatzen und dem Erwerb zugefügt. Das kann man machen, muss man aber nicht. Je nachdem was du noch gemacht hast, kannst gerne noch weitere Kacheln hinzufügen oder aber Arme ergänzen.\n\n\n\n\n\n\nAbbildung 61.6— Beispielhafte Flowchart für unsere Tigerumfrage. Zu Beginn sind 843 Fragebögen gesendet worden von denen dann 173 als Rücklauf vorliegen. Wir haben dann dann noch die Fragen aufgereinigt und die Infromationen ebenfalls in die Flowchart geschriben. Abschließend haben wir noch für zwei Fragen eine bivariate Analyse für alle anderen Fragen durchgeführt.\n\n\n\n\n\n\n\n\n\nGeht das Ganze auch in R?\n\n\n\nNatürlich können wir auch eine Flowchart mit etwas Aufwand in R bauen. Hier gibt es einmal das Tutorium Building a flowchart sowie das R Paket {DiagrammeR} mit dem entsprechenden Tutorium Data-driven flowcharts in R using {DiagrammeR}. Eine Weitere Alternative ist das Tutorium Introducing {ggflowchart} wobei hier die Flowcharts eher sehr simple gehalten sind.\n\n\n\n\n61.4.6 Repräsentative:r Teilnehmer:innen\nWenn ich einen Fragebogen auswerte, dann lasse ich mir gerne eine Tabelle der repräsentativen Teilnehmer rausgeben. Nicht immer funktioniert es, denn wir brauchen natürlich auch einiges an Teilnehmern. Manchmal sind die Anzahlen auch zu gering, dass ich von einem repräsentativen Teilnehmer sprechen würde. Nichtsdestotrotz, ich schaue mir das gerne einmal an. Ich schaue mir dafür die demographischen Fragen einmal an und erstelle mir dann eine Tabelle mit den Eigenschaften der häufigsten Teilnehmern. In unserem Fall möchte ich wissen, ob es Überschneidungen im Geschlecht, dem Alter sowie den Erwerb und der betrieblichen Stellung gibt. Technisch klebe ich die vier Spalten einfach mit str_c() zu einem String zusammen und zähle dann wie oft so ein String dann vorkommt. Dafür nutze ich die Funktion tabyl(), die mir sehr angenehm die Anzahlen und Prozente wiedergibt. Die Funktion reframe() erlaubt es uns dann die Rückgabe von tabyl() in einen Datensatz.\n\nmost_common_participant_tbl &lt;- tiger_tbl |&gt; \n  select(geschlecht, alter, f1Soziodemografisch, f3Soziodemografisch) |&gt; \n  mutate(string = str_c(geschlecht, alter, f1Soziodemografisch, f3Soziodemografisch, sep = \" \")) |&gt; \n  na.omit() |&gt; \n  reframe(janitor::tabyl(string)) |&gt; \n  arrange(desc(n)) |&gt; \n  mutate(percent = percent(percent))\n\nIn der Tabelle 61.7 findest du die tabellarische Übersicht über die vier häufigsten Teilnehmer in unserem Fragebogen nach den oben ausgewählten vier demographischen Fragen. Du siehst hier sehr schön welches Problem unsere Teilnehmer haben. Gut 50% der häufigsten Teilnehmer sind männlich und Betriebsleiter. Wir haben nur eine kleine Variation im Erwerb und im Alter. Wobei der Großteil unser Teilnehmer eben dann auch schon alt ist.\n\n\n\n\nTabelle 61.7— Tabellarische Übersicht über die vier häufigsten Teilnehmer in unserem Fragebogen nach vier demographischen Fragen.\n\n\n\n\n\n\n\nAnzahl\n[%]\n\n\n\n\nMännlich 50-59 Jahre Haupterwerb Betriebsleiter\n27\n15.88%\n\n\nMännlich Über 60 Jahre Haupterwerb Betriebsleiter\n24\n14.12%\n\n\nMännlich 40-49 Jahre Haupterwerb Betriebsleiter\n21\n12.35%\n\n\nMännlich Über 60 Jahre Nebenerwerb Betriebsleiter\n19\n11.18%\n\n\n\n\n\n\n\n\nMusst du die Tabelle jetzt angeben? Vermutlich nicht. Aber wir wissen jetzt immerhin, wie sich unsere Population zusammensetzt. Das müssen wir natürlich beachten, wenn wir unsere Rückschlüsse aus dem Fragebogen ziehen. Das erlaubt uns dann auch eine sorgfältigere Diskussion unserer Ergebnisse.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Fragebogenanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survey.html#visualisierung",
    "href": "stat-modeling-survey.html#visualisierung",
    "title": "61  Fragebogenanalyse",
    "section": "61.5 Visualisierung",
    "text": "61.5 Visualisierung\nWie solltest du die Ergebnisse eines Fragebogens darstellen? Damit wir hier nicht immer wieder die Barplots oder Säulendiagramme aus dem Hut ziehen, zeige ich dir hier nochmal andere Möglichkeiten, die sehr ähnlich sind, aber doch anders. Prinzipiell ist es natürlich auch wieder nichts anderes als Säulendiagramme, aber dann doch in schöner. Wichtig ist nur, das du dir bewusst machst, welche Fragen du zusammen zeigen möchtest. Es macht meistens keinen Sinn alle Fragen in einer Abbildung darzustellen oder eben nur eine Frage zu visualisieren. Du willst ja meistens den Zusammenhang zwischen verschiedenen Fragen zeigen. Daher überlege dir, welche Fragen du zusammen in einem Block zeigen möchtest. Die Blöcke kannst du dann thematisch benennen und so dem Leser es einfacher machen deinen Gedankengang in der Auswertung zu folgen.\n\n\n\n\n\n\nAbbildung 61.7— “The reason to avoid pie charts” Quelle: wumo.com\n\n\n\nEine Sache möchte ich dann aber doch noch loswerden und zwar meine tiefe Abneigung gegenüber Tortendiagrammen (eng. pie charts). Gerne kannst du Tortendiagramme in deiner Arbeit verwenden, ich kann dir nur sagen warum ich sie nichts nutzen würde. Neben dem wunderbarer Cartoon von Wulff & Morgenthaler gibt es aber noch andere wissenschaftlichere Gründe, die gegen die Verwendung sprechen. Barnett und Oguoma schreiben in Here’s why you should (almost) never use a pie chart for your data folgendes Fazit dem ich mich anschließen möchte.\n\nWhenever we see pie charts, we think one of two things: their creators don’t know what they’re doing, or they know what they are doing and are deliberately trying to mislead. — Barnett, A. und Oguoma V. (2024)\n\nBesuche gerne die Quelle und schaue dir die Visualisierungen einmal an. Beide zeigen sehr schön in dem Beitrag warum Tortendiagramme sehr verzerrend wirken und häufig nicht das mitteilen was man möchte. Die Beispiel finde ich sehr überzeugend. Es gibt mit Siirtola (2019) auch eine wissenschaftliche Veröffentlichung The Cost of Pie Charts die als Fazit unter anderem folgendes Zitat hat.\n\nThe results show that the pie chart is slower and less accurate than the stacked bar chart, especially when the difference between the elements is small, but the participant find it slightly more pleasant to use. The participants also perceive the stacked bar chart as the most effective visualization. — Siirtola (2019)\n\nAlso klare Empfehlung für gestapelte Balkendiagramme für die Darstellung. Dann wollen wir uns verscheidende Pakete und deren Realisierung von der Darstellung von Likert Daten anschauen. Ich habe mir jetzt drei Pakete rausgesucht, die uns dabei behilflich sein können. Dann schauen wir uns im Folgenden einmal die Darstellung der Likert-Skala in drei R Paketen an.\n\nDem R Paket {likert}, dem auch irgendwie eine gute Tutoriumsseite des Paketes fehlt. Hier hilft dann das Tutorium Survey data I/O with {likert} weiter.\nDem R Paket {ggplot2}, als Selbstbausatz in unserem universalen Visualisierungspaket.\nDem R Paket {sjPlot} als eine weitere Möglichkeit Daten mit dem Fokus aus den Sozialwissenschaften darzustellen.\n\nWir immer kannst du schauen, ob du diese Pakete nutzt willst oder dann doch zu Excel umschwenkst. Prinzipiell lässt sich ja auch alles mit etwas mehr Arbeit in Excel nachbauen. Die Pakete erleichtern einem meiner Meinung nach nur die Darstellung.\n\n61.5.1 … mit {likert}\nIch zeige hier nur einen Ausschnitt mit einer einzigen Abbildung. Wenn du mehr zu dem R Paket {likert} erfahren möchtest, dann nutze das tolle Tutorium Survey data I/O with {likert}. Wenn ich hier das gesamte Tutorium nachkoche ist ja dann auch niemanden geholfen. Was ist also die Idee des R Paketes {likert}? Wir können mir der Funktion likert() Fragen in der selben Likertskala zusammen in einer Abbildung darstellen. Wichtig ist hier, dass das Paket schon etwas älter ist und nur ein Objekt als data.frame() und keine tibble() akzeptiert.\nIn der Abbildung 61.8 siehst du einmal die Darstellung der fünf Fragen zur Verbandsarbeit. Die Funktion likert() sortiert dabei auch nochmal sinnig deine Fragen von dem Anteil trifft zu und trifft voll zu zu den Anteilen von trifft gar nicht zu und trifft nicht zu. Diese Anteile siehst du dann auch nochmal an der rechten und linken Seite in absteigender und aufsteigender Reihenfolge abgebildet. Aus dem Grund kannst du auch nur Fragen mit den gleichen Antwortmöglichkeiten zusammenfassen. In der Mitte findest du dann die weder noch Angaben in Prozent. Eigentlich eine sehr schöne Form der Darstellung der Fragen. Ich musste noch die Legende etwas anpassen und die Beschreibung auf der \\(y\\)-Achse ausblenden.\nDie Darstellungsform, wie das R Paket {likert} die Daten darstellt ist nicht unumstritten, wie das Tutorium The case against diverging stacked bars einmal diskutiert. Dabei wird davon abgeraten die divergierenden, gestapelten Balkendiagramme für die Darstellung von Prozentsätzen zu verwenden. Gestapelte 100%-Balkendiagramme wie in der Abbildung 61.10 sind oft die bessere Option, vor allem, wenn es wichtig ist, den Anteil der äußersten Kategorien zu vergleichen. Daher bauen wir usn in {ggplot} nochmal die Darstellung anders nach.\n\ntiger_ord_tbl |&gt; \n  select(f1Verbandsarbeit:f5Verbandsarbeit) |&gt;\n  mutate_all(fct_rev) |&gt; \n  as.data.frame() |&gt; \n  likert() |&gt; \n  plot() +\n  ylab(\"\") +\n  guides(fill = guide_legend(\"\"))\n\n\n\n\n\n\n\nAbbildung 61.8— Darstellung der Antworten zu der Verbandsarbeit auf der Likertskala. Die Fragen sind nach dem Anteil trifft zu und trifft voll zu zu den Anteilen von trifft gar nicht zu und trifft nicht zu auf der rechten bzw. linken Seite der Balkendiagramme sortiert. Die mittlere graue Fläche stellt die weder noch Antworten dar.\n\n\n\n\n\nNeben den Balkendiagrammen können wir uns mit der Option type = \"heat\" in dem plot()-Aufruf auch eine Heatmap der Fragen darstellen lassen. Wir haben dann eine andere Sortierung wie bei den Balkendiagrammen aber die Prozente der Antworthäufigkeiten sind dann nochmal farblich hervorgehoben. Die Sortierung entspricht dann der Reihenfolge der Fragen. Ich finde diese Art der Abbildung nochmal sehr spannend, da wir hier nochmal sehen, wo wir am meisten Antworten haben. So geht in der obigen Abbildung manchmal unter, wie viel dann eine Antwortmöglichkeit pro Frage genannt wurde. Darüber hinaus haben wir dann in den grauen Kästen noch die Durchschnittsnote dargestellt zusammen mit der Streuung.\n\ntiger_ord_tbl |&gt; \n  select(f1Verbandsarbeit:f5Verbandsarbeit) |&gt;\n  mutate_all(fct_rev) |&gt; \n  as.data.frame() |&gt; \n  likert() |&gt;\n  plot(type = \"heat\") \n\n\n\n\n\n\n\nAbbildung 61.9— Heatmap der Häufigkeiten der Antworten pro Frage kombiniert mit der durchschnittlichen Note auf der Likertskala in den grauen Kästen.\n\n\n\n\n\n\n\n61.5.2 … mit {ggplot}\nManchmal müssen wir die Daten dann auch anders zusammenfassen oder aber keine standardisierten Antworten vorliegen. Dann hilf natürlich die ganze Abbildung einmal von Grund auf neu zu bauen. Dafür können wir dann {ggplot} nutzen. Im Folgenden wähle ich wieder die Fragen zur Verbandsarbeit und verwandle alle Fragen in eine numerische Variable. Dann baue ich mir für jede Frage eine zusammenfassende Tabelle über tabyl() und zwar über jede Frage mit der Funktion map(). Die Funktion bind_rows() liefert mir dann auch gleich einen Datensatz wieder. Am ende muss ich noch ein wenig an dem Datensatz rumarbeiten und fertig ist unser Objekt für den gestapelten Barplot.\n\ntiger_percent_tbl &lt;- tiger_ord_tbl |&gt; \n  select(f1Verbandsarbeit:f5Verbandsarbeit) |&gt;\n  mutate_all(as.numeric) |&gt; \n  map(tabyl) |&gt; \n  bind_rows(.id = \"id\") |&gt; \n  dplyr::rename(answer = \".x[[i]]\") |&gt; \n  mutate(answer = factor(answer, labels = zustimmung_ord))\n\nIn der Abbildung 61.10 siehst du einmal das Ergebnis der Generierung in ggplot(). Wir immer kann man an ganz vielen Schrauben drehen, so dass ich hier noch die Prozente als Prozente habe wiedergeben lassen und auch die Farben etwas angepasst habe. Du kannst im Prinzip ja die Optionen rein und rausnehmen. Dann kannst du schauen, was passiert. Mit hat es natürlich Spaß gemacht die Abbildung nachzubauen, aber das R Paket {likert} liefert hier schneller was ich brauche. Ich habe hier natürlich nichts sortiert und damit bist du dann näher an den originalen Fragen dran. Beides hat dann natürlich Vor- und Nachteile. Dann hatte ich noch Probleme, dass die Prozente natürlich irgendwann in den Flächen verschwinden oder nicht mehr passen, wenn die Belegung zu klein ist. Dabei hilft dann das nächste Paket {sjPlot} aus.\n\ntiger_percent_tbl |&gt; \n  ggplot(aes(x = fct_rev(id), y = valid_percent, fill = answer)) +\n  theme_minimal() +\n  geom_bar(position = \"fill\", stat = \"identity\", color = \"black\", width=0.7) +\n  scale_y_continuous(labels = scales::percent) +\n  geom_text(aes(label = scales::percent(valid_percent)), \n            position = position_stack(vjust = 0.5), size = 2) +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = wes_palette(\"Zissou1\", 8, type = \"continuous\")) +\n  labs(x = \"\" , y = \"\") +\n  coord_flip()\n\n\n\n\n\n\n\nAbbildung 61.10— Gestapelte Balkendiagramme für die Fragen zur Vereinsarbeit. Die Fragen sind unsortiert nach der Reihenfolge des Auftretens. Teilweise sind die Flächen zu klein für die Darstellung der Prozente.\n\n\n\n\n\n\n\n61.5.3 … mit {sjPlot}\nWir hatten ja eben das Problem, dass die Prozente nicht so richtig in die Kacheln passen plus wir hatten in {ggplot} ja auch nicht die Aufteilung wie in dem R Paket {likert}. Hier gibt es dann das R Paket {sjPlot} mit der Funktion plot_likert(). Das Paket wurde direkt für die Sozialwissenschaften gebaut und hat auch noch andere Funktionen, die eventuell von Interesse sind. Schau da doch einfach mal in die Referenz des Pakets. Wir konzentrieren uns also hier nur auf die eine Funktion. Wichtig ist hier, dass wir eigentlich nur gerade Anzahlen von Likertskalen darstellen können. Wenn wir eine ungerade Anzahl haben, dann müssen wir numerisch über die Option cat.neutral mitteilen, welche Position unsere neutrale Kategorie hat. In unserem Fall haben wir fünf Skalenstufen und deshalb ist unsere neutrale Stufe die dritte Stufe. Wir haben hier wiederum keine interne Sortierung der Fragen. Im Prinzip können wir hier die Zustimmung mit der Ablehnung direkt visuell vergleichen.\n\ntiger_ord_tbl |&gt; \n  select(f1Verbandsarbeit:f5Verbandsarbeit) |&gt; \n  plot_likert(cat.neutral = 3) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAbbildung 61.11— Drastellung der Likertskala der Antworten zur Verbandsarbeit. Wir haben hier wiederum keine Sortierung in den Fragen. Die Antworten werden nach den positiven und negativen Antwortkategorien sortiert. Die neutrale Kategorie wird links an der Seite dargestellt.\n\n\n\n\n\nUnd weil es so schön ist, dass ganze dann in der Abbildung 61.12 nochmal für die Verbandskommunikation. Hier sieht man dann nochmal den Nachteil der neutralen Kategorie. Teilweise haben wir dann bis zur Hälfte der Teilnehmer:innen keine Angabe gemacht. Dann ist es immer schwierig daraus eine Schlussfolgerung zu ziehen. Ich würde dahwer eher auf neutrale Kategorien verzichten.\n\ntiger_ord_tbl |&gt; \n  select(f1Imagearbeit:f4Imagearbeit) |&gt; \n  plot_likert(cat.neutral = 3) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAbbildung 61.12— Darstellung der Likertskala der Antworten zur Verbandkommunikation. Wir haben hier wiederum keine Sortierung in den Fragen. Die Antworten werden nach den positiven und negativen Antwortkategorien sortiert. Die neutrale Kategorie wird links an der Seite dargestellt.\n\n\n\n\n\nNeben dem R Paket {sjPlot} gibt es auch die Möglichkeit das R Paket {HH} aus dem Buch Heiberger u. a. (2015) zu nutzen. Das Tutorium Plot likert scale data stellt beide R Pakete {sjPlot} sowie {HH} einmal vor. Ich konzentriere mich hier auf das R Paket {sjPlot} mit der Funktion plot_likert(), da mir die Funktion irgendwie leichter von der Hand ging. Auf der anderen Seite reichen dann auch irgendwie drei Implementierungen der gleichen Darstellungsform irgendwie.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Fragebogenanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survey.html#bivariate-analysen",
    "href": "stat-modeling-survey.html#bivariate-analysen",
    "title": "61  Fragebogenanalyse",
    "section": "61.6 Bivariate Analysen",
    "text": "61.6 Bivariate Analysen\nWas sind bivariate Analysen? Eigentlich nichts anderes als Gruppenvergleich zwischen den kategoriellen Fragen oder aber eine Regression zwischen den kontinuierlichen Fragen. Also wir vergleichen zwei Fragen miteinander und wollen wissen, ob es in dem Antwortmuster der einen Fragen einen Unterschied gegeben der anderen Frage gibt. Eigentlich wiederholen wir hier sehr schnell, den \\(\\mathcal{X}^2\\)-Test für zwei kategorielle Variable, den t-Test oder den Wilcoxon-Test für einen Vergleich einer kontinuierlichen Variable nach zwei Gruppen oder aber der linearen Regression, wenn wir zwei kontinuierliche Variablen in Verbindung setzen wollen. Der Wahnsinn einer Fragebogenanalyse ist eigentlich, dass wir natürlich das für jede Fragenkombination machen können und uns dann vor lauert Testen gar nicht mehr wiederfinden. Deshalb kann ich nur Raten sich zu überlegen, was war nochmal meine Fragestellung und welche Fragen will ich jetzt im Bezug zu einer anderen Frage analysieren. Ich werde deshalb auch gar nicht mit der Problematik der Adjustierung für multiple Vergleiche beginnen, aber nur soviel, wenn du nur lange genug testest, wirst du auf jeden Fall was signifikantes finden, auch wenn in Wahrheit gar kein Unterschied zwischen den Fragen vorliegt. Siehe hierzu dann auch die tolle Arbeit von Austin u. a. (2006) mit dem Titel Testing multiple statistical hypotheses resulted in spurious associations: a study of astrological signs and health. Leider ist das PDF nicht mehr frei verfügbar, aber der Abstrakt sagt schon alles aus.\n\n61.6.1 Kategorial vs. kategoriale Frage\nWas machen wir, wenn wir zwei Fragen mit Antwortmöglichkeiten mit Likertskalen miteinander verglichen wollen? Wir rechnen einen \\(\\mathcal{X}^2\\)-Test auf verschieden großen Kreuztabellen (eng. cross table). In R ist die einfachste Funktion sich eine Kreuztabelle zu erstellen die Funktion xtabs(). Du erhälst dann für die beiden Fragen die kreuzweisen Häufigkeiten wiedergeben. Wir machen das jetzt einmal für die beiden Spalten haben_Sie_tiger und dem Erwerb in der Spalte f1Soziodemografisch. Damit erhalten wir dann eine klassische 2x2 Kreuztabelle wieder.\n\ncross_tab &lt;- xtabs(~ haben_Sie_tiger + f1Soziodemografisch, data = tiger_ord_tbl)\ncross_tab\n\n               f1Soziodemografisch\nhaben_Sie_tiger Haupterwerb Nebenerwerb\n           Ja            96          50\n           Nein          18           9\n\n\nMit einem schnellen Blick sehen wir eine recht gleichmäßige Verteilung der Antworten in der 2x2 Kreuztabelle. Wenn wir wirklich einen Unterschied hätten, dann wären die größeren Anzahlen dann auf einer der Diagonalen und nicht gleichmäßig verteilt. Wir können uns dass dann nochmal als relative Häufigkeiten in der Funktion prop.table() anschauen.\n\nprop.table(cross_tab) |&gt; \n  round(2)\n\n               f1Soziodemografisch\nhaben_Sie_tiger Haupterwerb Nebenerwerb\n           Ja          0.55        0.29\n           Nein        0.10        0.05\n\n\nUnd dann kommen wir auch schon zu dem Test mit der Funktion assocstats() aus dem R Paket vcd(). Die drei Koeffizienten am Ende des Ausdrucks sagen alle im Prinzip aus, wie stark die Assoziation zwischen den beiden Fragen ist. Wenn die Zahlen nahe Eins sind, dann haben die Fragen im Antwortmuster viel miteinander zu tun, wenn wir Zahlen nahe Null beobachten, dann haben wir eigentlich keinen Zusammenhang. Das deckt sich dann auch in unserem Beispiel mit dem \\(p\\)-Wert aus dem \\(\\mathcal{X}^2\\)-Test nach Pearson.\n\nvcd::assocstats(cross_tab)\n\n                       X^2 df P(&gt; X^2)\nLikelihood Ratio 0.0084818  1  0.92662\nPearson          0.0084562  1  0.92673\n\nPhi-Coefficient   : 0.007 \nContingency Coeff.: 0.007 \nCramer's V        : 0.007 \n\n\nJetzt müsstest du jede Frage für die Frage f1Soziodemografisch testen indem du dann immer wieder neu eine Kreuztablle baust. Das ist natürlich sehr aufwendig. Am Ende zeige ich dir dann nochmal wie es mit dem R Paket {gtsummary} sehr viel schneller geht.\n\n\n61.6.2 Kontinuierlich vs. kategoriale Frage\nWenn du eine kontinuierliche Antwort wie zum Beispiel auf die Frage nach der durchschnittlichen Dauer bis eine schwere Verletzung im Erlebnishof stattfindet mit einer kategoriellen Frage nach der Haltung von Tigern vergleichen willst, dann kannst du entweder den t-Test oder aber den Wilcoxon-Test nutzen. Den t-Test nutzt du, wenn du annimmst das deine kontinuierliche Variable einigermaßen normalverteilt ist. Wenn das nicht der Fall ist, dann nutzt du den Wilcoxon-Test. Das ist hier natürlich stark verkürzt, aber für hier reicht es allemal. In der Abbildung 61.13 siehst du dann einmal die beiden Boxplots für die beiden Gruppen der Tigerhaltung (ja/nein) und dann der Dauer bis zu einer Verletzung. Ich würde mal sagen, dass unsere Verteilungen eher schief sind und nicht sehr normalverteilt Aussehen. Wir haben einige Ausreißer in den Daten und deshalb würde ich hier eher zu dem Wilcoxon-Test raten.\n\nggplot(tiger_ord_tbl, aes(haben_Sie_tiger, f5dauer_verletzung, \n                          fill = haben_Sie_tiger)) +\n  theme_minimal() +\n  geom_boxplot(show.legend = FALSE) +\n  scale_fill_okabeito() \n\n\n\n\n\n\n\nAbbildung 61.13— Boxplot der durschnittlichen Dauer bis zu einer schweren Verletzung eines Besuches auf einem Erlebnishof aufgeteilt nach der Haltung von Großkatzen oder eben Tigern.\n\n\n\n\n\nIch rechne dir dann aber in den beiden Tabs dann doch nochmal beide Tests durch. Dann kannst du bei dir in deinen Daten entscheiden welchen Test du nutzen willst. Wenn du später das R Paket {gtsummary} nutzt, dann wird meistens der Wilcoxon-Test für den Vergleich genutzt. Du kannst es aber auch umstellen auf den t-Test, ich belasse es aber meistens bei dem Wilcoxon-Test. Über alle Fragen anzunehmen, dass wir eine Normalverteilung vorliegen haben, ist dann doch eher unwahrscheinlich.\n\nDer t-TestDer Wilcoxon-Test\n\n\nIm Folgenden dann einmal der Code für den einfachen t-Test. Wir sehen dann auch, dass wir keinen signifikanten Unterschied nachweisen können, da unser p-Wert über dem Signifikanzniveau mit \\(\\alpha\\) gleich 5% liegt. Da ich hier nicht mit der Annahme der Normalverteilung an die Daten leben kann, ist das hier nur eine Demonstration.\n\nt.test(f5dauer_verletzung ~ haben_Sie_tiger, tiger_ord_tbl) \n\n\n    Welch Two Sample t-test\n\ndata:  f5dauer_verletzung by haben_Sie_tiger\nt = -1.2491, df = 33.501, p-value = 0.2203\nalternative hypothesis: true difference in means between group Ja and group Nein is not equal to 0\n95 percent confidence interval:\n -66.50115  15.88928\nsample estimates:\n  mean in group Ja mean in group Nein \n           92.0274           117.3333 \n\n\n\n\nDann einmal der Code für den Wilcoxon-Test. Wir sehen dann auch hier, dass wir keinen signifikanten Unterschied nachweisen können, da unser p-Wert über dem Signifikanzniveau mit \\(\\alpha\\) gleich 5% liegt. Der t-Test ist etwas verzerrt, da wir vermutlich keine Normalverteilung in den Daten haben. Der p-Wert beim Wilcoxon-Test ist daher erwartungsgemäß niedriger.\n\nwilcox.test(f5dauer_verletzung ~ haben_Sie_tiger, tiger_ord_tbl, \n            conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  f5dauer_verletzung by haben_Sie_tiger\nW = 1528.5, p-value = 0.06447\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -47.000039   1.999984\nsample estimates:\ndifference in location \n             -23.00003 \n\n\n\n\n\n\n\n61.6.3 Kontinuierlich vs. Kontinuierlich Frage\nAm Ende dann noch den Fall, dass du dir zwei kontinuierliche Fragen anschauen möchtest und wissen willst, ob diese Fragen etwas miteinander zu tun haben. Prinzipiell ist es dann eine simple lineare Regression oder aber nicht lineare Regression je nachdem, ob du an einen linearen Zusammenhang glaubst oder nicht. Wir haben uns ja in dem Abschnitt weiter oben schon die Korrelation angeschaut, wenn du von einem linearen Zusammenhang ausgehst, dann dann kannst du natürlich auch die Korrelation zwischen den kontinuierliche Fragen berechnen.\nDas R Paket {correlation} erlaubt es uns dabei auch relativ flott und einfach die Korrelation zu berechnen. Es gibt eine Menge an Korrelationskoeffizienten, die das Paket berechnen kann. Wir bleiben hier dann bei den Klassiker nach Spearman, wenn keine Normalverteilung vorliegt. In dem folgenden Tabs habe ich dir dann einmal berechnet, wie die Korrelation über alle Fragebögen aussieht und dann einmal getrennt nach der Haltung von Tigern. Ich verzichte hier auch auf eine Adjustierung für multiple Vergleiche. Ich schmeiße dann noch die Teststatistik S raus, dann ist es kompakter. Auch können wir mit dem Paket sehr einfach gute Abbildungen für die Korrelation erstellen, aber das wird dann hier einfach zu viel. Schaue dazu dann einfach in das Kapitel zu Korrelation.\n\nÜber alle FragebögenGruppiert nach Tigerhaltung\n\n\nWenn wir über alle kontinuierlichen Fragen hinweg die Korrelation rechnen wollen, dann können wir die Funktion correlation() aus dem gleichnamigen R Paket nutzen. Ich nutze hier die Methode nach Spearman, aber es gibt eine gewaltige Menge an möglichen Typen von Methoden um die Korrelation zu berechnen. Wir kriegen auch gleich einen Test für die Signifikanz mitgeliefert.\n\ntiger_tbl |&gt; \n  select(f5dauer_verletzung, f6einkommen_jahr, f7gruppengroesse, f8eis_verkauf) |&gt; \n  correlation(method = \"spearman\", p_adjust = \"none\") |&gt; \n  select(-S)\n\nParameter1         |       Parameter2 |   rho |        95% CI |      p\n----------------------------------------------------------------------\nf5dauer_verletzung | f6einkommen_jahr |  0.07 | [-0.09, 0.22] | 0.392 \nf5dauer_verletzung | f7gruppengroesse | -0.11 | [-0.26, 0.05] | 0.173 \nf5dauer_verletzung |    f8eis_verkauf | -0.11 | [-0.26, 0.04] | 0.137 \nf6einkommen_jahr   | f7gruppengroesse | -0.07 | [-0.22, 0.09] | 0.397 \nf6einkommen_jahr   |    f8eis_verkauf |  0.11 | [-0.04, 0.26] | 0.139 \nf7gruppengroesse   |    f8eis_verkauf |  0.16 | [ 0.00, 0.31] | 0.038*\n\nObservations: 170\n\n\n\n\nHäufig wollen wir uns dann die Korrelation von kontinuierlichen Fragen auch noch aufgeteilt nach einer anderen kategoriellen Frage anschauen. In dem Fall nutzen wir einfach die Funktion group_by() um unseren Datensatz einmal nach einer anderen Frage zu gruppieren. Dann berechnen wir wieder die paarweise Korrelation. Auch hier erhalten wir gleich die Signifikanz mitgeliefert.\n\ntiger_tbl |&gt; \n  select(haben_Sie_tiger, f5dauer_verletzung, f6einkommen_jahr, f7gruppengroesse, f8eis_verkauf) |&gt; \n  group_by(haben_Sie_tiger) |&gt; \n  correlation(method = \"spearman\", p_adjust = \"none\") |&gt; \n  select(-S)\n\nGroup |         Parameter1 |       Parameter2 |   rho |         95% CI |      p\n-------------------------------------------------------------------------------\nJa    | f5dauer_verletzung | f6einkommen_jahr |  0.09 | [-0.08,  0.26] | 0.284 \nJa    | f5dauer_verletzung | f7gruppengroesse | -0.07 | [-0.23,  0.10] | 0.434 \nJa    | f5dauer_verletzung |    f8eis_verkauf | -0.08 | [-0.25,  0.09] | 0.337 \nJa    |   f6einkommen_jahr | f7gruppengroesse | -0.08 | [-0.25,  0.09] | 0.328 \nJa    |   f6einkommen_jahr |    f8eis_verkauf |  0.03 | [-0.14,  0.20] | 0.712 \nJa    |   f7gruppengroesse |    f8eis_verkauf |  0.15 | [-0.02,  0.31] | 0.081 \nNein  | f5dauer_verletzung | f6einkommen_jahr |  0.13 | [-0.28,  0.49] | 0.533 \nNein  | f5dauer_verletzung | f7gruppengroesse | -0.42 | [-0.70, -0.04] | 0.028*\nNein  | f5dauer_verletzung |    f8eis_verkauf | -0.04 | [-0.42,  0.36] | 0.841 \nNein  |   f6einkommen_jahr | f7gruppengroesse | -0.11 | [-0.48,  0.29] | 0.588 \nNein  |   f6einkommen_jahr |    f8eis_verkauf |  0.24 | [-0.17,  0.57] | 0.234 \nNein  |   f7gruppengroesse |    f8eis_verkauf |  0.06 | [-0.34,  0.44] | 0.754 \n\nObservations: 27-143\n\n\n\n\n\nIch zeige dir hier einmal den Fall für die lineare Regression und die nicht linearen Regression mit einem Polynom. Den Rest müsstest du dann dir nochmal in den entsprechenden Kapiteln anschauen, wenn du hier tiefer einstiegen willst. Meistens ist das nicht nötig und frage dich immer, brauche ich diese Analyse von zwei kontinuierlichen Fragen wirklich oder bin ich hier gerade in einem Kaninchenbau, weil es geht?\n\nLineare RegressionNicht lineare Regression\n\n\nDie lineare Regression ist sehr schnell für zwei Variablen gerechnet. Wir wollen hier einmal sehen, in wie weit ein Zusammenhang zwischen dem Eisverkauf und der Gruppengröße vorliegt. Ich lege hier einmal den Eisverkauf auf die \\(y\\)-Achse und den die Gruppengröße auf die \\(x\\)-Achse. Wenn also die Gruppengröße ansteigt, wie verändert sich dann der Eisverkauf auf dem Hof. Aus der Korrelationsanalyse wissen wir ja schon, dass es hier einen Effekt gibt. Und wir sehen tatsächlich einen kanpp signifikanten Zusammenhang.\n\nlm(f8eis_verkauf ~ f7gruppengroesse, tiger_tbl) |&gt; \n  model_parameters()\n\nParameter        | Coefficient |   SE |         95% CI | t(168) |      p\n------------------------------------------------------------------------\n(Intercept)      |       19.55 | 1.24 | [17.09, 22.00] |  15.72 | &lt; .001\nf7gruppengroesse |        0.28 | 0.14 | [ 0.00,  0.55] |   1.98 | 0.049 \n\n\nSchauen wir uns den Zusammenhang nochmal in der Abbildung 61.14 einmal an. Auch hier wäre ich dann eher vorsichtig. Der Anstieg ist eher schwach und durch ein paar Punkte an der rechten Seite getrieben. Wir könnten zwar sagen, dass wir mit jedem Besucher pro Gruppe mehr Eis verkaufen, aber das ist eher von schwacher Validität. Ich wäre hier also vorsichtig von einem echten Zusammenhang zu sprechen.\n\nggplot(tiger_tbl, aes(f7gruppengroesse, f8eis_verkauf)) +\n  theme_minimal() +\n  geom_smooth(method = \"lm\", color = \"#CC79A7\", se = FALSE) +\n  geom_point() \n\n\n\n\n\n\n\nAbbildung 61.14— Scatterplot des Eisverkaufs abhängig von der Gruppengröße der Besucher auf einem Erlebnishof. Eine Gerade aus einer linearen Regression wurde ergänzt.\n\n\n\n\n\n\n\nWir können auch eine nicht lineare Regression einmal ausprobieren. Hier hilft dann auch wieder das richtige Kapitel zur nicht linearen Regression weiter. Hier rechne ich dann eine Multivariate Fractional Polynomials (abk. mfp) Regression. Details sind hier nicht so wichtig, die Idee ist jedoch simple. Anstatt selber zu überlegen, wie die nicht lineare Formel für unsere Regression wäre, lassen wir das den Algorithmus mfp() machen.\n\nmfp_fit &lt;- mfp(f8eis_verkauf ~ fp(f7gruppengroesse), tiger_tbl)\n\nWir brauchen zum einen das Polynom, hier haben wir gar keins, denn die Funktion mfp() sagt, dass der lineare Zusammenhang am besten wäre.\n\nmfp_fit$formula\n\nf8eis_verkauf ~ I((f7gruppengroesse/10)^1)\n&lt;environment: 0x13c2c69c0&gt;\n\n\nGut, dann eben noch die Koeffizienten für unsere Geradengleichung.\n\nmfp_fit$coefficients\n\n         Intercept f7gruppengroesse.1 \n         19.545233           2.761718 \n\n\nUnd dann können wir auch schon alles zusammenbauen. Wenn du dir jetzt die Funktion eine Minute länger ansiehst, dann kommt im Prinzip das gleiche wie bei der linearen Regression raus. Wir können die Gleichung auch auflösen, da unser Polynom ja nur 1. Grades ist.\n\\[\nweight \\sim 19.55 + 2.76 \\cdot \\left(\\cfrac{f7gruppengroesse}{10}\\right)^{1}\n\\]\nWir bauen uns dann noch die Geradengleichung diesmal händisch über eine selber erstellte Funktion mit \\(x) nach. Es gibt wie immer verschiedene Möglichkeiten, diese hier ist dann die etwas einfachere.\n\nf7gruppengroesse_func &lt;- \\(x) {19.55 + 2.76 * (x/10)^1}\n\nDann können wir uns auch schon den Zusammenhang nochmal in der Abbildung 61.15 anschauen. Es ist der gleiche Zusammenhang wie dann schon in der linearen Regression. Die Funktion mfp() hat kein besseres Polynom gefunden.\n\nggplot(tiger_tbl, aes(f7gruppengroesse, f8eis_verkauf)) +\n  theme_minimal() +\n  geom_function(fun = f7gruppengroesse_func, color = \"#CC79A7\") +\n  geom_point() \n\n\n\n\n\n\n\nAbbildung 61.15— Scatterplot des Eisverkaufs abhängig von der Gruppengröße der Besucher auf einem Erlebnishof. Eine Gerade aus einer nicht linearen Regression wurde ergänzt.\n\n\n\n\n\n\n\n\n\n\n61.6.4 Mit tbl_summary()\nWenn du einmal schnell über alle Fragen aus deinem Fragebogen wissen willst, ob es einen Zusammenhang zu einer anderen kategoriellen Frage gibt, dann nutze das R Paket {gtsummary}. Das Paket hat über die Funktion tbl_summary() die Möglichkeit die Ausgabe für eine andere kategorielle Frage aufzuspalten und zu testen. Damit haben wir dann gleich sehr viel weniger Arbeit. Wir machen das jetzt einmal exemplarisch für die Frage Halten Sie aktuell Tiger oder andere Großkatzen auf Ihrem Erlebnishof?.\n\ntiger_ord_tbl |&gt; \n  set_names(short_question_tbl$question) |&gt;\n  tbl_summary(by = \"Halten Sie aktuell Tiger oder andere Großkatzen auf Ihrem Erlebnishof?\") |&gt; \n  add_p()\n\nIch habe dir wie immer die Ausgabe einmal zusammengeklappt, damit hier nicht die gesamte Tabelle alles überlappt. Wir sehen, dass wir einige Unterschiede zwischen den Höfen mit und ohne Großkatzen über alle Fragen in den Antwortmustern finden. Beachte immer, dass die Prozente bei den kategoriellen Fragen getestet werden und nicht die absoluten Anzahlen direkt. Auch gibt es keine Richtung des Effekts. Du musst dann selber schauen wo in den Antworten der Unterschied zu finden ist. Kontinuierliche Antworten werden dann entsprechend auch über einen t-Test oder einer ANOVA getestet. Teilweise dann auch über einen Wilcoxon-Test, wenn die Normalverteilung nicht gegeben ist. Daher macht es einem diese Funktion schon sehr einfach die Fragen flott auszuwerten.\n\n\n\n\n\n\nBivariate Analyse mit der Funktion tbl_summary() für die Tigerhaltung\n\n\n\n\n\n\n\n\n\nTabelle 61.8— Bivariate Analyse der einzelenen Fragen und deren Antwortverteilung aufgeteilt nach der Frage zur der Haltung von Tigern. Anhand der Tabelle können fehlende Werte und ungleichmäßig beantwortete Fragen erkannt und dann ausgeschlossen werden.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nJa, N = 1461\nNein, N = 271\np-value2\n\n\n\n\nWelches Geschlecht haben Sie?\n\n\n\n\n0.040\n\n\n    Männlich\n134 (92%)\n21 (78%)\n\n\n\n\n    Weiblich\n12 (8.2%)\n6 (22%)\n\n\n\n\nWie alt sind Sie?\n\n\n\n\n0.2\n\n\n    18-29 Jahre\n3 (2.1%)\n0 (0%)\n\n\n\n\n    30-39 Jahre\n22 (15%)\n0 (0%)\n\n\n\n\n    40-49 Jahre\n34 (23%)\n8 (30%)\n\n\n\n\n    50-59 Jahre\n41 (28%)\n8 (30%)\n\n\n\n\n    Über 60 Jahre\n46 (32%)\n11 (41%)\n\n\n\n\nIst Ihr Erlebnishof Haupterwerb oder Nebenerwerb?\n\n\n\n\n&gt;0.9\n\n\n    Haupterwerb\n96 (66%)\n18 (67%)\n\n\n\n\n    Nebenerwerb\n50 (34%)\n9 (33%)\n\n\n\n\nHaben Sie weitere Erwerbsquellen neben dem Erlebnishof außer Landwirtschaft?\n\n\n\n\n0.8\n\n\n    Blumenanbau\n1 (3.6%)\n1 (17%)\n\n\n\n\n    Direktvermarktung\n1 (3.6%)\n0 (0%)\n\n\n\n\n    Ferienhof\n1 (3.6%)\n1 (17%)\n\n\n\n\n    forst\n1 (3.6%)\n0 (0%)\n\n\n\n\n    Forst\n3 (11%)\n0 (0%)\n\n\n\n\n    Forst, LN verpachtet\n1 (3.6%)\n0 (0%)\n\n\n\n\n    Forstwirtschaft\n5 (18%)\n0 (0%)\n\n\n\n\n    Forstwirtschaft + Verpachtung\n1 (3.6%)\n0 (0%)\n\n\n\n\n    Grünland 1,5 ha und 3 ha Wald\n1 (3.6%)\n0 (0%)\n\n\n\n\n    Hofladen\n1 (3.6%)\n1 (17%)\n\n\n\n\n    Holzwirtschaft\n1 (3.6%)\n0 (0%)\n\n\n\n\n    Kartoffeln\n2 (7.1%)\n0 (0%)\n\n\n\n\n    Pferde\n1 (3.6%)\n1 (17%)\n\n\n\n\n    Pferdehaltung\n1 (3.6%)\n0 (0%)\n\n\n\n\n    Schafhaltung\n2 (7.1%)\n0 (0%)\n\n\n\n\n    Strohhandel\n1 (3.6%)\n0 (0%)\n\n\n\n\n    Vermietung und Verpachtung\n1 (3.6%)\n0 (0%)\n\n\n\n\n    Wald\n2 (7.1%)\n1 (17%)\n\n\n\n\n    Weihnachtsbäume\n1 (3.6%)\n1 (17%)\n\n\n\n\n    Unknown\n118\n21\n\n\n\n\nWelche Stellung haben Sie auf dem Erlebnishof inne?\n\n\n\n\n0.025\n\n\n    Angestellter\n1 (0.7%)\n0 (0%)\n\n\n\n\n    Betriebsleiter\n123 (84%)\n18 (67%)\n\n\n\n\n    Hofnachfolger\n11 (7.5%)\n3 (11%)\n\n\n\n\n    König\n5 (3.4%)\n0 (0%)\n\n\n\n\n    Leitung Feldwirtschaft\n2 (1.4%)\n2 (7.4%)\n\n\n\n\n    Leitung Tierproduktion\n1 (0.7%)\n2 (7.4%)\n\n\n\n\n    Sonstiges\n3 (2.1%)\n2 (7.4%)\n\n\n\n\nSoll die Verbandsarbeit in den nächsten Jahren digital in den sozialen Netzwerken ausgeweitet werden?\n\n\n\n\n0.4\n\n\n    trifft voll zu\n7 (4.9%)\n1 (3.7%)\n\n\n\n\n    trifft zu\n31 (22%)\n5 (19%)\n\n\n\n\n    weder noch\n39 (27%)\n6 (22%)\n\n\n\n\n    trifft nicht zu\n37 (26%)\n12 (44%)\n\n\n\n\n    trifft gar nicht zu\n29 (20%)\n3 (11%)\n\n\n\n\n    Unknown\n3\n0\n\n\n\n\nLesen Sie quartalsweise den Newsletter auf der Verbandshomepage?\n\n\n\n\n&gt;0.9\n\n\n    trifft voll zu\n16 (11%)\n3 (11%)\n\n\n\n\n    trifft zu\n42 (29%)\n8 (30%)\n\n\n\n\n    weder noch\n38 (27%)\n8 (30%)\n\n\n\n\n    trifft nicht zu\n28 (20%)\n4 (15%)\n\n\n\n\n    trifft gar nicht zu\n19 (13%)\n4 (15%)\n\n\n\n\n    Unknown\n3\n0\n\n\n\n\nHalten Sie die Verbandsarbeit für die Verbreitung von Großkatzen im ländlichen Raum für sinnvoll?\n\n\n\n\n0.7\n\n\n    trifft voll zu\n8 (5.6%)\n1 (3.7%)\n\n\n\n\n    trifft zu\n81 (57%)\n16 (59%)\n\n\n\n\n    weder noch\n31 (22%)\n8 (30%)\n\n\n\n\n    trifft nicht zu\n18 (13%)\n1 (3.7%)\n\n\n\n\n    trifft gar nicht zu\n5 (3.5%)\n1 (3.7%)\n\n\n\n\n    Unknown\n3\n0\n\n\n\n\nSehen Sie die Haltung von Großkatzen als eine kulturelle Bereicherung?\n\n\n\n\n0.7\n\n\n    trifft voll zu\n18 (13%)\n5 (19%)\n\n\n\n\n    trifft zu\n66 (46%)\n13 (50%)\n\n\n\n\n    weder noch\n29 (20%)\n5 (19%)\n\n\n\n\n    trifft nicht zu\n23 (16%)\n3 (12%)\n\n\n\n\n    trifft gar nicht zu\n8 (5.6%)\n0 (0%)\n\n\n\n\n    Unknown\n2\n1\n\n\n\n\nSehen Sie die Notwendigkeit von genetisch veränderten Großkatzen zur Steigerung der Attraktivität des Erlebnishofes?\n\n\n\n\n0.8\n\n\n    trifft voll zu\n7 (4.9%)\n1 (3.7%)\n\n\n\n\n    trifft zu\n48 (34%)\n9 (33%)\n\n\n\n\n    weder noch\n41 (29%)\n9 (33%)\n\n\n\n\n    trifft nicht zu\n33 (23%)\n4 (15%)\n\n\n\n\n    trifft gar nicht zu\n14 (9.8%)\n4 (15%)\n\n\n\n\n    Unknown\n3\n0\n\n\n\n\nFüttern Sie täglich Kellog's Frosties?\n\n\n\n\n&gt;0.9\n\n\n    Ja\n101 (70%)\n0 (NA%)\n\n\n\n\n    Nein\n44 (30%)\n0 (NA%)\n\n\n\n\n    Unknown\n1\n27\n\n\n\n\nFüttern Sie wöchentlich Batzen?\n\n\n\n\n&gt;0.9\n\n\n    Ja\n111 (77%)\n0 (NA%)\n\n\n\n\n    Nein\n34 (23%)\n0 (NA%)\n\n\n\n\n    Unknown\n1\n27\n\n\n\n\nFüttern Sie Joghurt?\n\n\n\n\n&gt;0.9\n\n\n    Ja\n63 (43%)\n0 (NA%)\n\n\n\n\n    Nein\n82 (57%)\n0 (NA%)\n\n\n\n\n    Unknown\n1\n27\n\n\n\n\nFüttern Sie pflanzlich oder gar vegan?\n\n\n\n\n&gt;0.9\n\n\n    Ja\n40 (28%)\n0 (NA%)\n\n\n\n\n    Nein\n105 (72%)\n0 (NA%)\n\n\n\n\n    Unknown\n1\n27\n\n\n\n\nWie aktiv sind Sie bei der Lobbyarbeit in den jeweiligen Landeshauptstädten?\n\n\n\n\n0.2\n\n\n    sehr aktiv\n2 (1.7%)\n2 (9.5%)\n\n\n\n\n    eher aktiv\n44 (38%)\n7 (33%)\n\n\n\n\n    weder noch\n49 (42%)\n11 (52%)\n\n\n\n\n    eher nicht aktiv\n19 (16%)\n1 (4.8%)\n\n\n\n\n    gar nicht aktiv\n2 (1.7%)\n0 (0%)\n\n\n\n\n    Unknown\n30\n6\n\n\n\n\nWie aktiv sind Sie in der positiven Vermarktung des Verbandes in sozialen Netzwerken?\n\n\n\n\n0.7\n\n\n    sehr aktiv\n16 (13%)\n4 (22%)\n\n\n\n\n    eher aktiv\n60 (50%)\n9 (50%)\n\n\n\n\n    weder noch\n37 (31%)\n4 (22%)\n\n\n\n\n    eher nicht aktiv\n5 (4.2%)\n1 (5.6%)\n\n\n\n\n    gar nicht aktiv\n1 (0.8%)\n0 (0%)\n\n\n\n\n    Unknown\n27\n9\n\n\n\n\nWie aktiv sind Sie in der Vermarktung von Tigernachwuchs an Dritte in den sozialen Netzwerken?\n\n\n\n\n0.033\n\n\n    sehr aktiv\n1 (0.8%)\n1 (4.5%)\n\n\n\n\n    eher aktiv\n34 (29%)\n10 (45%)\n\n\n\n\n    weder noch\n56 (47%)\n10 (45%)\n\n\n\n\n    eher nicht aktiv\n24 (20%)\n0 (0%)\n\n\n\n\n    gar nicht aktiv\n4 (3.4%)\n1 (4.5%)\n\n\n\n\n    Unknown\n27\n5\n\n\n\n\nWie aktiv sind Sie bei der notwendigen Entnahme von Schadwölfen in Ihrem Einflussgebiet?\n\n\n\n\n0.3\n\n\n    sehr aktiv\n15 (13%)\n3 (14%)\n\n\n\n\n    eher aktiv\n69 (58%)\n12 (55%)\n\n\n\n\n    weder noch\n27 (23%)\n6 (27%)\n\n\n\n\n    eher nicht aktiv\n7 (5.9%)\n0 (0%)\n\n\n\n\n    gar nicht aktiv\n0 (0%)\n1 (4.5%)\n\n\n\n\n    Unknown\n28\n5\n\n\n\n\nWie lange schätzen Sie dauert es im Durchschnitt in Tagen bis sich ein Besucher bei Ihnen gefährlich verletzt?\n70 (24, 131)\n91 (54, 129)\n0.064\n\n\nWieviel Einkommen hatten Sie brutto in Tausend EUR im letzten Jahr?\n787 (469, 1,336)\n511 (142, 1,104)\n0.034\n\n\nWie groß sind ihre Besuchergruppen im Durchschnitt pro Monat?\n6.0 (4.0, 9.8)\n5.0 (4.0, 8.0)\n0.3\n\n\nWie viel 100 Liter Eis verkaufen Sie im Durchschnitt pro Monat?\n20 (16, 26)\n15 (13, 16)\n&lt;0.001\n\n\n\n1 n (%); Median (IQR)\n\n\n2 Fisher’s exact test; Pearson’s Chi-squared test; Wilcoxon rank sum test",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Fragebogenanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survey.html#multivariate-analysen",
    "href": "stat-modeling-survey.html#multivariate-analysen",
    "title": "61  Fragebogenanalyse",
    "section": "61.7 Multivariate Analysen",
    "text": "61.7 Multivariate Analysen\nKommen wir am Ende noch zu den multivariate Verfahren. Also eigentlich etwas, was in den meisten Abschlussarbeiten dann nicht mehr gemacht wird. Auch hier gebe ich dann keine Übersicht über die Methoden und was alles geht, sondern konzentriere mich auf die Hauptkomponentenanalyse oder auch PCA genannt. Wenn du mehr lesen willst, dann besuche einfach das entsprechende Kapitel hier in dem Buch zu den multivariate Verfahren. Die Idee ist eigentlich ganz simple, wir wollen uns alle Fragen und alle Antworten gemeinsame anschauen und herausfinden, ob es eine Korrelation zwischen den Fragen oder den Teilnehmern gibt. Wir betrachten dabei nicht einzelne Fragen sondern die gesamte Fragenmatrix. Diese Korrelation zwischen allen Fragen simultan wollen wir dann einmal visualisieren. Auch hier stellt sich natürlich die Frage, ob es so sinnvoll ist wirklich alles in die PCA zu schmeißen. Vermutlich ist eine thematische Auswahl über select() besser. Hier machen wir aber einmal alles zusammen.\nWir brauchen auf jeden Fall nur numerische Daten. Daher wandele ich meinen Faktordatensatz tiger_fct_tbl einmal in einen numerischen Datensatz um indem ich jede Spalte auf numerisch setze. Dann möchte ich noch die Kurznamen der Fragen haben, da ich sonst nichts mehr in den Abbildungen erkenne.\n\ntiger_num_tbl &lt;- tiger_fct_tbl |&gt; \n  mutate_all(as.numeric) |&gt; \n  set_names(short_question_tbl$ques_id)\n\nIch lasse dann einmal die Funktion PCA() auf den Daten laufen. Das wichtige ist hier noch, dass ich gerne die Antworten alle über die Option scale.unit skalieren möchte. Damit haben dann alle Antworten den gleichen numerischen Bereich. Das ist natürlich eine künstliche Maßnahme, die aber die PCA verbessert. Dann schaue ich mi nur die ersten beiden Hauptkomponenten mit der Option ncp = 2 an.\n\npca_tiger &lt;- PCA(tiger_num_tbl, scale.unit = TRUE, \n                 ncp = 2, graph = FALSE)\n\nIn der Abbildung 61.16 schaue ich mir einmal die Effekt der Fragen, also der Spalten, an. Die ersten beiden Dimensionen der Hauptkomponentenanalyse werden dargestellt. Leider erklären beide Hautkomponenten nur sehr wenig der Varianz, so dass die Aussagekraft der Hauptkomponentenanalyse schwach ist. Daher möchte ich wissen, ob irgendwelche Fragen ähnliche Korrelationen zusammen haben. Je größer der cos2-Wert ist, desto mehr Einfluss auf die erklärte Varianz haben die Variablen. Fragen die in unterschedliche Richtungen zeigen, haben ein gegensätzliches Antwortmuster.\n\nfviz_pca_var(pca_tiger, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE)\n\n\n\n\n\n\n\nAbbildung 61.16— Betrachtung der einzelnen Fragen zueinander. Die ersten beiden Dimensionen der Hauptkomponentenanalyse werden dargestellt. Leider erklären beide Hauptkomponenten nur sehr wenig der Varianz, so dass die Aussagekraft der Hauptkomponentenanalyse schwach ist.\n\n\n\n\n\nIn der Abbildung 61.17 schauen wir einmal auf die Individuen, also den Zeilen, gegeben der Fragen. Auch hier sehen wir, dass wir eigentlich keine großen Auffäligkeiten haben. Eventulle zwei der Teilnehmer:innen in der Zeile 131 und 172 könnten wir nochmal anschauen.\n\nfviz_pca_ind(pca_tiger,\n             col.ind = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE) +\n  scale_x_continuous(expand = expansion(add = c(0.5, 1))) +\n  scale_y_continuous(expand = expansion(add = c(0.5, 0.5))) \n\n\n\n\n\n\n\nAbbildung 61.17— Betrachtung der einzelnen Individuen zueinander. Die ersten beiden Dimensionen der Hauptkomponentenanalyse werden dargestellt. Leider erklären beide Hauptkomponenten nur sehr wenig der Varianz, so dass die Aussagekraft der Hauptkomponentenanalyse schwach ist.\n\n\n\n\n\nMit slice() könnten wir uns jetzt die Zeilen für die Individuen 131 und 172 einmal anschauen. Oder wir gehen dann in den Exceldatensatz und schauen in die entsprechenden Zeilen. Aber Achtung, in Excel zählt die Überschrift immer als eine Zeile. Daher musst du dann in Excel in die Zeile 132 und 173 für die entsprechenden Einträge schauen. Ich habe das mal gemacht und ich finde keine Besonderheiten, die jetzt einen Ausschluss rechtfertigen würden. Aber nochmal, die Hauptkomponenten erklären auch nur sehr wenig der Varianz, daher ist die Aussagekraft eben vage.\nAbschließend können wir nochmal schauen, ob wir Cluster in unseren Daten wiederfinden. Ich nutze dazu den kmeans()-Algorithmus und möchte mal sehen, ob ich auf den Individuen etwas finden kann.\n\nind_tiger &lt;- get_pca_ind(pca_tiger)\ngrp_tiger_ind &lt;- kmeans(ind_tiger$coord, centers = 2, nstart = 25) %&gt;% \n  pluck(\"cluster\") %&gt;% \n  as_factor()\n\nAbschließend siehst du dann einmal die beiden Cluster mit centers = 2 auf der PCA der Individuen. Ich habe dann die Individuen nach der Haltung der Tiger gelabelt. Wie du siehst, sehen wir zwar zwei Cluster, die sich auch recht stark überlappen, und auch nicht die Tigerhaltung widerspiegeln. Wir schon oben gesagt, die Hauptkomponenten erklären zu wenig der Varianz um hier etwas sinniges zu finden.\n\nfviz_pca_ind(pca_tiger,\n             geom.ind = \"point\", \n             col.ind = grp_tiger_ind, \n             palette = c(\"#0072B2\", \"#CC79A7\"),\n             addEllipses = TRUE, \n             legend.title = \"Groups\", mean.point = FALSE) +\n  geom_label(aes(label = tiger_num_tbl$s1))\n\n\n\n\n\n\n\nAbbildung 61.18— Clusteranalyse mit dem \\(k\\)-means Algorithmus auf dem gesamten Fragebogen. Die Teilnehmer sind nach der Haltung der Tiger gelabelt. Es findet sich jedoch kein Zusammenhang zwischen den gefunden Clustern und der Tigerhaltung.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Fragebogenanalyse</span>"
    ]
  },
  {
    "objectID": "stat-modeling-survey.html#referenzen",
    "href": "stat-modeling-survey.html#referenzen",
    "title": "61  Fragebogenanalyse",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 61.1— Nicht alle Änderungen müssen in R durchgeführt werden. Es empfiehlt sich aber die Tabs in Excel zu nutzen um sich neue Versionen des ursprüngliche Lime-Fragebogens anzulegen. So hast du immer das Orginal vorliegen und kannst dann schrittweise nachvollziehen, was du geändert hast.\nAbbildung 61.2— Darstellung der fehlenden Werte in dem Tigerdatensatz. Die fehlenden Werte werden als schwarzer Balken dargstellt. Der Block fehlender Werte bei der Fütterung stellt die Teilnehmer:innen dar, die keine Tiger halten. Horizontale Linien stellen Teilnehmer dar, die systematisch keine Fragen beantwortet haben.\nAbbildung 61.3— Darstellung des Zusammenhangs der fehlenden Werte unter den Teilnehmer:innen des Fragebogens. Unter den Balkendiagramm sind die jeweiligen Kombinationen der fehlenden Werte der Fragen für diese Gruppe der Teilnehmer:innen dargestellt.\nAbbildung 61.4— Darstellung der fehlenden Werte in dem Tigerdatensatz nachdem wir unsere drei Beobachtungen entfernt haben. Die fehlenden Werte werden als schwarzer Balken dargstellt. Der Block fehlender Werte bei der Fütterung stellt die Teilnehmer:innen dar, die keine Tiger halten. Horizontale Linien stellen Teilnehmer dar, die systematisch keine Fragen beantwortet haben.\nAbbildung 61.5 (a)— Numerische Werte.\nAbbildung 61.5 (b)— Tortendiagramm und Ellipse\nAbbildung 61.6— Beispielhafte Flowchart für unsere Tigerumfrage. Zu Beginn sind 843 Fragebögen gesendet worden von denen dann 173 als Rücklauf vorliegen. Wir haben dann dann noch die Fragen aufgereinigt und die Infromationen ebenfalls in die Flowchart geschriben. Abschließend haben wir noch für zwei Fragen eine bivariate Analyse für alle anderen Fragen durchgeführt.\nAbbildung 61.7— “The reason to avoid pie charts” Quelle: wumo.com\nAbbildung 61.8— Darstellung der Antworten zu der Verbandsarbeit auf der Likertskala. Die Fragen sind nach dem Anteil trifft zu und trifft voll zu zu den Anteilen von trifft gar nicht zu und trifft nicht zu auf der rechten bzw. linken Seite der Balkendiagramme sortiert. Die mittlere graue Fläche stellt die weder noch Antworten dar.\nAbbildung 61.9— Heatmap der Häufigkeiten der Antworten pro Frage kombiniert mit der durchschnittlichen Note auf der Likertskala in den grauen Kästen.\nAbbildung 61.10— Gestapelte Balkendiagramme für die Fragen zur Vereinsarbeit. Die Fragen sind unsortiert nach der Reihenfolge des Auftretens. Teilweise sind die Flächen zu klein für die Darstellung der Prozente.\nAbbildung 61.11— Drastellung der Likertskala der Antworten zur Verbandsarbeit. Wir haben hier wiederum keine Sortierung in den Fragen. Die Antworten werden nach den positiven und negativen Antwortkategorien sortiert. Die neutrale Kategorie wird links an der Seite dargestellt.\nAbbildung 61.12— Darstellung der Likertskala der Antworten zur Verbandkommunikation. Wir haben hier wiederum keine Sortierung in den Fragen. Die Antworten werden nach den positiven und negativen Antwortkategorien sortiert. Die neutrale Kategorie wird links an der Seite dargestellt.\nAbbildung 61.13— Boxplot der durschnittlichen Dauer bis zu einer schweren Verletzung eines Besuches auf einem Erlebnishof aufgeteilt nach der Haltung von Großkatzen oder eben Tigern.\nAbbildung 61.14— Scatterplot des Eisverkaufs abhängig von der Gruppengröße der Besucher auf einem Erlebnishof. Eine Gerade aus einer linearen Regression wurde ergänzt.\nAbbildung 61.15— Scatterplot des Eisverkaufs abhängig von der Gruppengröße der Besucher auf einem Erlebnishof. Eine Gerade aus einer nicht linearen Regression wurde ergänzt.\nAbbildung 61.16— Betrachtung der einzelnen Fragen zueinander. Die ersten beiden Dimensionen der Hauptkomponentenanalyse werden dargestellt. Leider erklären beide Hauptkomponenten nur sehr wenig der Varianz, so dass die Aussagekraft der Hauptkomponentenanalyse schwach ist.\nAbbildung 61.17— Betrachtung der einzelnen Individuen zueinander. Die ersten beiden Dimensionen der Hauptkomponentenanalyse werden dargestellt. Leider erklären beide Hauptkomponenten nur sehr wenig der Varianz, so dass die Aussagekraft der Hauptkomponentenanalyse schwach ist.\nAbbildung 61.18— Clusteranalyse mit dem \\(k\\)-means Algorithmus auf dem gesamten Fragebogen. Die Teilnehmer sind nach der Haltung der Tiger gelabelt. Es findet sich jedoch kein Zusammenhang zwischen den gefunden Clustern und der Tigerhaltung.\n\n\n\nAustin PC, Mamdani MM, Juurlink DN, Hux JE. 2006. Testing multiple statistical hypotheses resulted in spurious associations: a study of astrological signs and health. Journal of clinical epidemiology 59: 964–969.\n\n\nBrühlmann F, Petralito S, Aeschbach LF, Opwis K. 2020. The quality of data collected online: An investigation of careless responding in a crowdsourced sample. Methods in Psychology 2: 100022.\n\n\nEdwards PJ, Roberts I, Clarke MJ, DiGuiseppi C, Wentz R, Kwan I, Cooper R, Felix LM, Pratap S. 2009. Methods to increase response to postal and electronic questionnaires. Cochrane database of systematic reviews.\n\n\nEdwards P, Roberts I, Clarke M, DiGuiseppi C, Pratap S, Wentz R, Kwan I. 2002. Increasing response rates to postal questionnaires: systematic review. Bmj 324: 1183.\n\n\nFriedman HH, Amoo T. 1999. Rating the rating scales. Friedman, Hershey H. and Amoo, Taiwo (1999).\" Rating the Rating Scales.\" Journal of Marketing Management, Winter 114–123.\n\n\nGaur PS, Zimba O, Agarwal V, Gupta L. 2020. Reporting survey based studies—a primer for authors. J Korean Med Sci 35: e398.\n\n\nHeiberger RM, Heiberger RM, Burt Holland BH. 2015. Statistical Analysis and Data Display An Intermediate Course with Examples in R. Springer.\n\n\nRolstad S, Adler J, Rydén A. 2011. Response burden and questionnaire length: is shorter better? A review and meta-analysis. Value in Health 14: 1101–1108.\n\n\nRoszkowski MJ, Bean AG. 1990. Believe it or not! Longer questionnaires have lower response rates. Journal of Business and Psychology 4: 495–509.\n\n\nSiirtola H. 2019. The cost of pie charts. 2019.\n\n\nStory DA, Tait AR. 2019. Survey research. Anesthesiology 130: 192–202.\n\n\nSynodinos NE. 2003. The „art“ of questionnaire construction: some important considerations for manufacturing studies. Integrated manufacturing systems 14: 221–237.\n\n\nWatson SC. 1998. A primer in survey research. The Journal of Continuing Higher Education 46: 31–40.",
    "crumbs": [
      "Statistisches Modellieren",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Fragebogenanalyse</span>"
    ]
  },
  {
    "objectID": "time-space-preface.html",
    "href": "time-space-preface.html",
    "title": "Zeitliche und räumliche Analysen",
    "section": "",
    "text": "Analyse von (pseudo) Zeitreihen\nIn dem folgenden Kapitel zu Zeitreihen (eng. time series) wollen wir uns mit dem verstreichen der Zeit beschäftigen. Was ja auch irgendwie zu erwarten war. Wir haben ganz einfach auf der \\(x\\)-Achse einer potenziellen Visualisierung die Zeit dargestellt. Wir wollen dann analysieren, ob es über den zeitlichen Verlauf einen Trend gibt oder wir ein gutes Modell für den Verlauf der Beobachtungen anpassen können. Hierbei unterscheide ich einmal zwischen einer “pseudo” Zeitreihenanalyse und der “standard” Zeitreihenanalyse, wie sie klassisch und statistisch verstanden wird. Nur weil wir einen Zeitraum auf der \\(x\\)-Achse darstellen, haben wir nicht unbedingt eine klassische Zeitreihe vorliegen. Deshalb schaue ich mir erstmal Fälle an, die wirken wir eine Zeitreihe aber im klassischen Sinne keine sind. Im nächsten Kapitel schauen wir uns dann “echte” Zeitreihen an und analysieren die Daten dann mit den entsprechenden R Paketen.\nPrinzipiell ist ist die Analyse von Zeitreihen nicht so kompliziert, aber es gibt immer wieder Verwirrungen mit anderen Analysen, die auch eine Zeit messen. Wenn du die Zeit bis zu einem Ereignis misst, dann bist du bei den Ereigniszeitanalysen richtig aufgehoben. Dann willst du auf jeden Fall nicht eine Zeitreihe analysieren. Es kann auch sein, dass du nur bestimmte Zeitpunkte vorliegen hast, an denen du immer wieder verschiedenen Beobachtungen in Gruppen misst. Das wäre dann eher ein Experiment mit Messwiederholung (eng. repeated measurement) und dort würden wir dann ein gemischtes Modell rechnen. Du merkst schon, so einfach ist es manchmal nicht zu erkennen, welche Analyse den nun passen würde. Wenn du noch mehr über die Analyse von Zeitreihen lesen willst, dann kann ich dir folgende Literatur empfehlen. Robert u. a. (2006) liefert eine gute Übersicht über die Anwendung in R, ist aber schon etwas älter. Das Gleiche gilt dann auch für das Buch von Chan und Cryer (2008) und Cowpertwait und Metcalfe (2009). Dennoch bilden alle drei Bücher die Grundlagen der Analysen von Zeitreihen super ab. Für eine Abschlussarbeit sollten die Quellen also allemal reichen.",
    "crumbs": [
      "Zeitliche und räumliche Analysen"
    ]
  },
  {
    "objectID": "time-space-preface.html#analyse-von-pseudo-zeitreihen",
    "href": "time-space-preface.html#analyse-von-pseudo-zeitreihen",
    "title": "Zeitliche und räumliche Analysen",
    "section": "",
    "text": "Welche Zeitanalyse soll es denn nun sein?\n\n\n\nWenn du dir unsicher bist, was denn nun sein soll, dann komm doch einfach bei mir in der statistischen Beratung vorbei. Dafür musst du mir nur eine Mail schreiben und du erhälst dann einen Termin für ein Onlineberatung.",
    "crumbs": [
      "Zeitliche und räumliche Analysen"
    ]
  },
  {
    "objectID": "time-space-preface.html#analyse-von-räumliche-daten",
    "href": "time-space-preface.html#analyse-von-räumliche-daten",
    "title": "Zeitliche und räumliche Analysen",
    "section": "Analyse von räumliche Daten",
    "text": "Analyse von räumliche Daten\nMein Feld, das hat drei Ecken. Drei Ecken hat mein Feld. In dem folgenden Kapitel zu räumlichen Daten (eng. spatial data) wollen wir uns mit dem großen Feld von geologischen Daten beschäftigen. Wir haben also nicht nur Messwerte von Pflanzen oder Tieren, sondern wissen auch wo wir auf der Fläche diese Pflanzen oder Tiere beobachtet wurden. Da vermutlich räumlich nahe stehende Beobachtungen ähnlicher auf Umgebungsbedingungen reagieren als weit entfernte, müssen wir hier unsere Modelle anpassen. Wir berücksichtigen also die Positionen von Beobachtungen in unseren Modellen. Hier gibt es dann natürlich einiges an statistischen Modellen, so dass wir hier auch wieder nur eine Auswahl treffen. Manchmal reicht dann aber auch eine tolle Abbildung, auch hier kann dir dann im folgenden Kapitel geholfen werden. Auch hier ein kurzer Literaturabriss, später dann mehr. Wir haben einmal die Anwendung von der Analyse von räumlichen Daten von Bivand u. a. (2008). Ein sehr ausführliches Buch, was auch einigermaßen aktuell ist, ist die Arbeit von Plant (2018). Wenn du also tiefer in das Feld einsteigen willst, dann ist das Buch von Plant (2018) das richtige Buch für die Anwendung in den Agarwissenschaften.",
    "crumbs": [
      "Zeitliche und räumliche Analysen"
    ]
  },
  {
    "objectID": "time-space-preface.html#das-r-paket-plotly",
    "href": "time-space-preface.html#das-r-paket-plotly",
    "title": "Zeitliche und räumliche Analysen",
    "section": "Das R Paket plotly",
    "text": "Das R Paket plotly\nFür die Analyse von Zeitreihen und räumlichen Daten bietet sich im besonderen das R Paket plotly an. Nicht, dass es sich bei plotly um eine Analyse im Sinne eines statistischen Test handelt. Wir haben mit plotly aber die Möglichkeit unsere Daten sehr gut zu visualisieren, wie in ggplot, aber wir können uns auch einzelne Punkte in den Abbildungen anzeigen lassen. Mit der Funktion ggplotly() können wir sogar statische Abbildungen in ggplot in eine plotly Abbildung umwandeln. Mehr dazu dann in den folgenden Kapiteln sowie natürlich auf der Hilfeseite Plotly R Open Source Graphing Library.",
    "crumbs": [
      "Zeitliche und räumliche Analysen"
    ]
  },
  {
    "objectID": "time-space-preface.html#referenzen",
    "href": "time-space-preface.html#referenzen",
    "title": "Zeitliche und räumliche Analysen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nBivand RS, Pebesma EJ, Gómez-Rubio V, Pebesma EJ. 2008. Applied spatial data analysis with R. Springer.\n\n\nChan K-S, Cryer JD. 2008. Time series analysis with applications in R. Springer.\n\n\nCowpertwait PS, Metcalfe AV. 2009. Introductory time series with R. Springer Science & Business Media.\n\n\nPlant RE. 2018. Spatial data analysis in ecology and agriculture using R. cRc Press.\n\n\nRobert H, others. 2006. Time Series Analysis and Its Applications With R Examples Second Edition.",
    "crumbs": [
      "Zeitliche und räumliche Analysen"
    ]
  },
  {
    "objectID": "time-space-pseudo-time-series.html",
    "href": "time-space-pseudo-time-series.html",
    "title": "62  Pseudo Zeitreihen",
    "section": "",
    "text": "62.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, janitor, see, readxl,\n               lubridate, plotly, zoo, timetk, conflicted)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(magrittr::set_names)\nconflicts_prefer(dplyr::slice)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Pseudo Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-pseudo-time-series.html#daten",
    "href": "time-space-pseudo-time-series.html#daten",
    "title": "62  Pseudo Zeitreihen",
    "section": "62.2 Daten",
    "text": "62.2 Daten\nDen ersten Datensatz, den wir uns anschauen wollen, ist in einer CSH-Datei abgespeichert, die ich schon in Excel exportiert habe. Eine CSH-Datei ist ein Datenformat aus Adobe Photoshop und eigentlich nichts anders als eine Information über eine Bilddatei. Wir haben aber hier nicht Pixel oder aber ein Foto vorliegen, sondern das Bild wurde schon in einen numerischen Wert pro Bild weiter verarbeitet. Das hier so ausgedachte Experiment war ein Dronenüberflug über eine Wiese und einem Feld in Uelzen. Dabei wurden Fotos gemacht und es sollten verschiedene Grünlandwerte aus den Fotos berechnet werden. Wir haben aber den Überflug nicht an einem einzigen Tag gemacht, sondern gleich an mehreren über das Jahr verteilt. Das ist jetzt auch dann gleich unsere Zeitreihe. Jetzt können wir uns Fragen, ob es einen Unterschied zwischen den Messwerten der beiden Dronenüberflüge gibt. Wir lesen wie immer erstmal die Daten ein.\n\ncsh_tbl &lt;- read_excel(\"data/csh_data.xlsx\") |&gt; \n  clean_names() |&gt; \n  mutate_if(is.numeric, round, 2) |&gt; \n  mutate(day_num = as.numeric(as.factor(day)))\n\nIn der Tabelle 62.1 siehst du einen Ausschnitt aus den 926 Überflügen. Hier wurden die Daten natürlich schon zusammengefasst. Aus jedem Bild wurde dann ein Wert für zum Beispiel kg_tm_ha berechnet. Hier interessiert uns aber nicht die Berechnungsart. Wir wollen jetzt gleich mit den Daten weiterarbeiten. Wie immer ist das Beispiel so semi logisch, hier geht es aber auch eher um die Anwendung der Methoden.\n\n\n\n\nTabelle 62.1— Auszug aus den Daten der CSH-Datei von Dronenüberflügen über eine Wiese und einem Feld in Uelzen. Die Daten sind abgeändert von den Orginaldaten.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparzelle\nday\ng_tm_plot\nkg_tm_ha\nnetto_csh_cm\ntm_gehlt\nday_num\n\n\n\n\nWiese\n428\n15.2\n1148.47\n7.8\n0.22\n1\n\n\nWiese\n428\n11.4\n813.57\n8\n0.21\n1\n\n\nWiese\n428\n24.7\n1771.79\n9.3\n0.21\n1\n\n\nWiese\n428\n20.9\n1520.52\n8\n0.23\n1\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\nUelzen\n920\n42.7\n3104.49\n3.6\n0.2\n30\n\n\nUelzen\n920\n31.9\n2455.84\n1.8\n0.26\n30\n\n\nUelzen\n920\n47\n3396.17\n3.4\n0.24\n30\n\n\nUelzen\n920\n30.2\n2201.32\n2.5\n0.27\n30\n\n\n\n\n\n\n\n\nIm nächsten Datensatz schauen wir uns einmal die Daten von vier Loggern an. Hier haben wir mehr oder minder einfach jeweils einen Temperaturlogger an den jeweiligen Seiten unseres Folientunnels geworfen und dann nochmal einen Logger einfach so auf das Feld gelegt. In den Folientunneln haben wir dann Salat hochgezogen. Wir betrachten jetzt hier nur das Freiland, sonst wird es einfach zu viel an Daten.\n\nsalad_tbl &lt;- read_excel(\"data/temperatur_salad.xlsx\") |&gt; \n  clean_names() |&gt; \n  mutate_if(is.numeric, round, 2) |&gt; \n  select(datum, uhrzeit, matches(\"freiland\"))\n\nIn der Tabelle 62.2 siehst du einmal die 2447 automatisch erfassten Messungen der Temperatur pro Tag und dann Stunde. Hier müssen wir dann einmal schauen, wie wir die Daten dann sinnvoll zusammenfassen. Es sind wirklich viele Datenpunkte. Aber gut wir schauen uns die Daten erstmal an und entscheiden dann später weiter. Wir sehen aber schon, dass wir die Daten nochmal bearbeiten müssen, denn irgendwas stimmt mit der Uhrzeitspalte und dem Datum nicht. Dazu dann aber gleich mehr im Abschnitt zum Datumsformat.\n\n\n\n\nTabelle 62.2— Auszug aus den Daten zur auromatischen Erfassung von Klimadaten im Feld für Kopfsalat.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatum\nuhrzeit\nfreiland_messw\nfreiland_min\nfreiland_max\n\n\n\n\n2023-04-11\n1899-12-31 13:30:00\n21.9\n21.9\n21.9\n\n\n2023-04-11\n1899-12-31 14:00:00\n18.8\n18.8\n22.5\n\n\n2023-04-11\n1899-12-31 14:30:00\n14.6\n13.8\n18.8\n\n\n2023-04-11\n1899-12-31 15:00:00\n13.2\n13.2\n15.6\n\n\n…\n…\n…\n…\n…\n\n\n2023-06-01\n1899-12-31 11:00:00\nNA\nNA\nNA\n\n\n2023-06-01\n1899-12-31 11:30:00\nNA\nNA\nNA\n\n\n2023-06-01\n1899-12-31 12:00:00\nNA\nNA\nNA\n\n\n2023-06-01\n1899-12-31 12:30:00\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nAm Ende wollen wir uns dann nochmal Daten einer Wetterstation in Hagebüchen an. Auch hier haben wir wieder sehr viele Daten vorliegen und wir müssen uns überlegen, welche der Daten wir nutzen wollen. Aus Gründen der Machbarkeit wähle ich die Spalte temp_boden_durch und solar_mv aus, die wir uns dann später anschauen wollen. Sonst wird mir das zu groß und unübersichtlich.\n\nstation_tbl &lt;- read_excel(\"data/Wetterstation_Hagebuechen.xlsx\") |&gt; \n  clean_names() |&gt; \n  select(datum_uhrzeit, temp_boden_durch, solar_mv) |&gt; \n  mutate_if(is.numeric, round, 2)\n\nAuch hier haben wir in der Tabelle 62.3 gut 4163 einzelne Messungen vorliegen. Das ist dann auch unserer größter Datensatz von Klimadaten. Wir werden die Daten dann aber sehr anschaulich einmal in einer Übersicht darstellen.\n\n\n\n\nTabelle 62.3— Auszug aus den Daten zur Wetterstation in Hagebüchen.\n\n\n\n\n\n\ndatum_uhrzeit\ntemp_boden_durch\nsolar_mv\n\n\n\n\n2022-09-21 18:00:00\n12.9\n6978\n\n\n2022-09-21 17:00:00\n15.2\n10223\n\n\n2022-09-21 16:00:00\n15.6\n10343\n\n\n2022-09-21 15:00:00\n15.7\n10348\n\n\n…\n…\n…\n\n\n2022-04-01 11:00:00\n-1.3\n9854\n\n\n2022-04-01 10:00:00\n-1.1\n9892\n\n\n2022-04-01 09:00:00\n-1.3\n6983\n\n\n2022-04-01 08:00:00\n-1.3\n4083\n\n\n\n\n\n\n\n\nDamit hätten wir uns eine Reihe von landwirtschaftlichen Datensätzen angeschaut. Sicherlich gibt es noch mehr, aber diese Auswahl erlaubt es uns gleich einmal die häufigsten Fragen rund um Zeitreihen in den Agrarwissenschaften einmal anzuschauen. Bitte beachte, dass es natürlich noch andere Formen von Zeitreihen und damit Datensätzen gibt. Deshalb gleich noch ein Datensatz, der künstlich ist und damit eine eher perfekte Zeitreihe repräsentiert. In dem folgenden Kasten findest du darüber hinaus nochmal eine Anregung zu Klimadaten aus deiner Region.\n\n\n\n\n\n\nMehr Wetter- und Klimadaten aus deiner Region\n\n\n\nDu kannst gerne die entgeltfreien Informationen auf der DWD-Website nutzen um mehr Informationen zu dem Klima und deiner Region zu erhalten. Wir finden dort auf der Seite die Klimadaten für Deutschland und natürlich auch die Daten für Münster/Osnabrück. Sie dazu auch Isoplethendiagramm für Münster & Osnabrück im Skript zu beispielhaften Anwendung. Ich habe mir dort flux die Tageswerte runtergeladen und noch ein wenig den Header der txt-Datei angepasst. Du findest die Datei day_values_osnabrueck.txt wie immer auf meiner GitHub Seite. Du musst dir für andere Orte die Daten nur entsprechend zusammenbauen. Am Ende brauchen wir noch die Informationen zu den Tages- und Monatswerten damit wir auch verstehen, was wir uns da von der DWD runtergeladen haben.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Pseudo Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-pseudo-time-series.html#sec-time-data-format",
    "href": "time-space-pseudo-time-series.html#sec-time-data-format",
    "title": "62  Pseudo Zeitreihen",
    "section": "62.3 Das Datumsformat",
    "text": "62.3 Das Datumsformat\nWenn wir von Zeitreihen sprechen dann sprechen wir auch von dem Datumsformat. Eine Zeitreihe ohne eine richtig formatierte Datumsspalte macht ja auch überhaupt keinen Sinn. Es ist eigentlich immer einer ewige Qual Daten in das richtige Zeitformat zu kriegen. Deshalb hier vorab einmal die folgende Abbildung, die nochmal die Wirrnisse des Datumsformat gut aufzeigt.\n\n\n\n\n\n\n\nQuelle: https://xkcd.com/\n\n\nWichtig ist, dass wir das richtige Datumsformat haben. Siehe bitte dazu auch das Kapitel Zeit und Datum. Das einzig richtige Datumsformat ist und bleibt eben Jahr-Monat-Tag. Häufig ist eben dann doch anders, so dass wir uns etwas strecken müssen um unser Format in das richtige Format zu überführen. Bitte beachte aber, dass du auf jeden Fall einheitlich dein Datum einträgst. Am besten auch immer zusammen mit dem Jahr, dass macht vieles einfacher. Wie immer gibt es auch noch das Tutorium zu Date Formats in R und natürlich das R Paket {lubridate} mit dem Einstieg Do more with dates and times in R.\nWir werden uns jetzt einmal am Beispiel die Transformation der Datumsformate in den jeweiligen Daten anschauen. Je nach Datensatz müssen wir da mehr oder weniger machen. Auch hier, wenn du weniger Arbeit möchtest, dann achte auf eine einheitliche Form der Datumsangabe\n\n\n\n\n\n\nKonvertierung von verschiedenen Datumsformaten in R\n\n\n\nDas R Paket {timetk} liefert dankenswerterweise Funktionen für die Konvertierung von verschiedenen Zeitformaten in R. Deshalb schaue einmal in die Hilfeseite Time Series Class Conversion – Between ts, xts, zoo, and tbl und dann dort speziell der Abschnitt Conversion Methods. Leider ist Zeit in R wirklich relativ.\n\n\n\n62.3.1 Die CSH-Daten\nDas Datum in den CSH-Daten leidet unter zwei Besonderheiten. Zum einen fehlt das Jahr und zum anderen die Null vor der Zahl. Wir haben nämlich für den 28. April die Datumsangabe 428 in der Spalte day. Das hat zur Folge, dass Excel die Spalte als Zahl erkennt und keine vorangestellten Nullen erlaubt. Wir brauchen aber einen String und den Monat als zweistellig mit 04 für den Monat April. Deshalb nutzen wir die Funktion str_pad() um eine 0 an die linke Seite zu kleben, wenn der Wert in der Spalte kleiner als vier Zeichen lang ist. Somit würde der 1. Oktober mit 1001 so bleiben, aber der 1. September mit 901 zu 0901. Dann nutzen wir die Funktion as.Date() um aus unserem Sting dann ein Datum zu machen. Das Format ist hier dann %m%d und somit Monat und Tag ohne ein Trennzeichen.\n\ncsh_tbl &lt;- csh_tbl |&gt; \n  mutate(day = as.Date(str_pad(day, 4, pad = \"0\", side = \"left\"), format = \"%m%d\"))\n\nUnd dann erhalten wir auch schon folgenden Datensatz mit dem korrekten Datumsformat mit dem wir dann weiterarbeiten werden.\n\ncsh_tbl |&gt; \n  head(4)\n\n# A tibble: 4 × 7\n  parzelle day        g_tm_plot kg_tm_ha netto_csh_cm tm_gehlt day_num\n  &lt;chr&gt;    &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 Wiese    2024-04-28      15.2    1148.          7.8     0.22       1\n2 Wiese    2024-04-28      11.4     814.          8       0.21       1\n3 Wiese    2024-04-28      24.7    1772.          9.3     0.21       1\n4 Wiese    2024-04-28      20.9    1521.          8       0.23       1\n\n\nWie du siehst, wird dann automatisch das aktuelle Jahr gesetzt. Das heißt, da ich dieses Text hier im Jahr 2024 schreibe, erscheint natürlich auch eine 2024 vor dem Monat und Tag. Hier musst du dann schauen, ob das Jahr wirklich von Interesse ist oder du es dann später nochmal anpasst. Wir lassen jetzt erstmal alles so stehen. Es ist immer einfacher das Datum dann sauber in Excel zu setzen, als sich dann hier nochmal einen Abzubrechen, denn du bist ja keine Informatiker der eine generelle Lösung sucht sondern hast ja nur einen Datensatz vorliegen. Das wäre jedenfalls mein Tipp um es schneller hinzukriegen.\n\n\n62.3.2 Die Salatdaten\nBei den Salatdaten haben wir ein anderes Problem vorliegen. Wir haben einmal eine Datumsspalte und dann noch eine Spalte mit der Uhrzeit. Da wir aber keine reine Uhrzeitspalte haben können, wurde noch das Datum 1899-12-31 als default ergänzt. Das macht natürlich so überhaupt keinen Sinn. Deshalb müssen wir dann einmal die Uhrzeit als korrektes Uhrzeit-Format umwandeln und dann die Spalte datum in ein Datum-Format. Dann können wir die beiden Spalten addieren und schon haben wir eine Datumsspalte mit der entsprechenden Uhrzeit.\n\nsalad_datetime_tbl &lt;- salad_tbl |&gt; \n  mutate(uhrzeit = format(uhrzeit, format = \"%H:%M:%S\"),\n         datum = format(datum, format = \"%Y-%m-%d\"),\n         datum = ymd(datum) + hms(uhrzeit)) |&gt; \n  select(-uhrzeit) \n\nIm Folgenden siehst du einmal das Ergebnis unserer Umfaormung. Wir haben jetzt eine Spalte mit dem Datum und der Uhrzeit vorliegen, so wir das auch wollen und dann auch abbilden können.\n\nsalad_datetime_tbl |&gt; \n  head(4)\n\n# A tibble: 4 × 4\n  datum               freiland_messw freiland_min freiland_max\n  &lt;dttm&gt;                       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 2023-04-11 13:30:00           21.9         21.9         21.9\n2 2023-04-11 14:00:00           18.8         18.8         22.5\n3 2023-04-11 14:30:00           14.6         13.8         18.8\n4 2023-04-11 15:00:00           13.2         13.2         15.6\n\n\nDa wir später noch für die Visualisierung die einzelnen Spalten freiland_messw bis freiland_max einmal darstellen wollen, bauen wir uns nochmal mit pivot_longer() einen entsprechenden Datensatz, der uns diese Art der Visualiserung dann auch möglich macht.\n\nsalad_long_tbl &lt;- salad_datetime_tbl |&gt; \n  pivot_longer(freiland_messw:last_col(),\n               names_sep = \"_\",\n               names_to = c(\"location\", \"type\"),\n               values_to = \"temp\") \n\nWie du nun siehst, haben wir nur noch eine Spalte temp mit unseren Messwerten der Temperatur. Der Rest an Informationen ist dann alles in anderen Spalten untergebracht. Mit diesem Datensatz können wir dann auch in {ggplot} gut arbeiten.\n\nsalad_long_tbl |&gt; \n  head(4)\n\n# A tibble: 4 × 4\n  datum               location type   temp\n  &lt;dttm&gt;              &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;\n1 2023-04-11 13:30:00 freiland messw  21.9\n2 2023-04-11 13:30:00 freiland min    21.9\n3 2023-04-11 13:30:00 freiland max    21.9\n4 2023-04-11 14:00:00 freiland messw  18.8\n\n\nWie du aber auch schon hier siehst sind die Werte für den Messwert, den Minimalwert und den Maximalwert faktisch identisch. Diese geringe Abweichung werden wir dann auch nur schwerlich schön in einer Abbildung zeigen können. Ich würde dann die Min/Max-Werte rausschmeißen und mich nur auf die Messwerte in diesem Fall konzentrieren.\n\n\n62.3.3 Die Wetterstationsdaten\nZu guter Letzt noch die Daten zu der Wetterstation. Hier haben wir das Problem, dass wir dann aus der Spalte mit den Informationen zu dem Datum und der Zeit noch einzelne Informationen extrahieren wollen. Wir wollen die Stunde oder den Tag haben. Dafür nutzen wir nun Funktionen wie day() oder month() aus dem R Paket {lubridate} um uns diese Informationen zu extrahieren. Wir können uns so auch die Uhrzeit wieder zusammenbauen, indem wir die Stunde, Minute und dann die Sekunden herausziehen.\n\nstation_tbl &lt;- station_tbl |&gt; \n  mutate(datum_uhrzeit = as_datetime(datum_uhrzeit),\n         month = month(datum_uhrzeit),\n         day = day(datum_uhrzeit),\n         hour = hour(datum_uhrzeit),\n         minute = minute(datum_uhrzeit),\n         second = second(datum_uhrzeit),\n         format_hour = paste(hour, minute, second, sep = \":\"))\n\nDamit haben wir auch den letzten Datensatz so umgebaut, dass wir eine Spalte haben in der das Datum sauber kodiert ist. Sonst macht ja eine Zeitreihe keinen Sinn, wenn die Zeit nicht stimmt. Damit können wir uns dann auch der Visualisierung der Wetterstationsdaten zuwenden.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Pseudo Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-pseudo-time-series.html#visualisierung",
    "href": "time-space-pseudo-time-series.html#visualisierung",
    "title": "62  Pseudo Zeitreihen",
    "section": "62.4 Visualisierung",
    "text": "62.4 Visualisierung\nWie auch bei anderen Analysen ist die Visualisierung von Zeitreihen das Wichtigste. Da wir im Besonderen bei Zeitreihen eben meistens keine Verläufe nur in den reinen Daten sehen können. Wir haben einfach zu viele Datenpunkte vorliegen. Meistens hilft uns dann auch eine Darstellung aller Datenpunkte auch nicht weiter, so dass wir uns entscheiden eine Glättung (eng. smoother) durchzuführen, damit wir überhaupt etwas sehen. Daher gehen wir hier einmal verschiedene Probleme an den Datensätzen durch. Vorab stelle ich dann aber nochmal das R Paket {plotly} vor, was es ermöglicht semi-interaktive Abbildungen zu erstellen. Wir haben mit {plotly} die Möglichkeit direkt Werte für die Punkte abzulesen. Das können wir mit einer fixen Abbildung in {ggplot} nicht. Mehr findest du auch auf der Seite von R Coder - Evolution charts.\n\n62.4.1 Das R Paket {plotly}\nDas R Paket {plotly} erlaubt interaktive Abbildungen zu erstellen. Zwar ist die Interaktivität nicht so ausgeprägt wie bei einer R Shiny App, aber wir haben hier auf jeden Fall die Möglichkeit in die Abbildung hineinzuzoomen oder aber Werte direkt aus der Abbildung abzulesen. Die beiden Möglichkeiten sind insbesondere bei sehr langen Zeitreihen oder aber bei vielen verschiedenen Zeitreihen in einer Abbildung super hilfreich. Wie bauen wir uns nun eine Abbildung in {plotly}? Ich nutze dafür die Funktion ggplotly(), die aus einer {ggplot} Abbildung dann ganz einfach eine {plotly} Abbildung baut. Du musst natürlich schauen, dass die {ggplot} Abbildung nicht zu komplex wird. Nicht alles was in {ggplot} möglich ist, lässt sich dann Eins zu Eins dann in {plotly} reproduzieren. Auf jeden Fall hilft wie immer die Hilfeseite {plotly} R Open Source Graphing Library sowie im besonderen dann die Seite Getting Started with {plotly} in {ggplot2}. Im Folgenden werde ich einzelne Abbildungen in {plotly} umwandeln und dann einmal erklären, was du so machen kannst. Aber eigentlich ist eine {plotly} Abbildung selbsterklärend, klicke einfach mal auf der Abbildung 62.9 zu dem Salat herum.\n\n\n62.4.2 Die CSH Daten\nDie CSH-Daten stellen ja die Messung eines Dronenüberflugs von zwei Parzellen einmal in Uelzen und einer Kontrollparzelle dar. In der Abbildung 62.7 (a) siehst du einmal die beobachteten g_tm_plot-Werte für jeden der Messtage getrennt nach Parzelle aufgetragen. Hier sieht man auf den ersten Blick keine Unterschied. Deshalb hilft es immer einmal eine geglättete Funktion durch die Punkte zu legen. Wir nutzen dazu die Funktion geom_smooth() und erhalten die Abbildung 62.7 (b). Hier sehen wir schon, dass es einen Unterschied zwischen den beiden Parzellen gibt. Wir sind also nicht an den einzelnen Punkten interessiert sondern eigentlich an der Differenz zwischen den beiden Geraden. Wir wollen also die Fläche zwischen den beiden Linien berechnen und so feststellen wie groß der Unterschied zwischen den Messungen an den beiden Parzellen ist.\n\ncsh_tbl |&gt; \n  ggplot(aes(day, g_tm_plot, color = parzelle)) +\n  theme_minimal() +\n  geom_point() +\n  scale_color_okabeito() \n\ncsh_tbl |&gt; \n  ggplot(aes(day, g_tm_plot, color = parzelle)) +\n  theme_minimal() +\n  geom_point() +\n  stat_smooth(se = FALSE) +\n  scale_color_okabeito() \n\n\n\n\n\n\n\n\n\n\n\n(a) Darstellung der Beobachtungen\n\n\n\n\n\n\n\n\n\n\n\n(b) Mit stat_smooth-Funktion\n\n\n\n\n\n\n\nAbbildung 62.7— Datensatz der g_tm_plot-Werte in Abhängigkeit von dem Tag der Messung für die beiden Parzellen einmal in Uelzen und der Kontrollwiese. Ein Unterschied lsäät sich nur durch die Anpassung der beiden Linien durch die Punkte erkennen.\n\n\n\n\nIn dem nächsten Abschnitt wollen wir dann einmal die Fläche zwischen den Linien bestimmen und schauen, ob wir hier wirklich einen Unterschied vorliegen haben. Wenn du mehr Linien oder Gruppen hast, dann musst du dann immer die Fläche zwischen zwei Linien berechnen bist du alle Kombinationen durch hast.\n\n\n62.4.3 Die Salat Daten\nDie Salatdaten schauen wir uns jetzt einmal in der Abbildung 62.8 als statische Abbildungen in {ggplot} an. Besonders in der Abbildung 62.8 (a) siehtst du dann wegen den geringen Unterschieden der Temperaturen fast nichts auf der Abbildung. Dafür hilft es dann auch in der Abbildung 62.8 (b) einmal die Temperaturen aufzuteilen. Jetzt haben wir alle Temperaturen einmal als Vergleich vorliegen. Aber auch hier können wir schlecht die Werte an einem Datum ablesen und direkt vergleichen. Hier hilft dann gleich {plotly} weiter.\n\np_loc &lt;- salad_long_tbl |&gt; \n  ggplot(aes(datum, temp, color = type)) +\n  theme_minimal() +\n  scale_color_okabeito() +\n  geom_line() +\n  facet_wrap(~ location) +\n  theme(legend.position = \"none\")\np_loc\n\np_type &lt;- salad_long_tbl |&gt; \n  ggplot(aes(datum, temp, color = type)) +\n  theme_minimal() +\n  scale_color_okabeito() +\n  geom_line() +\n  facet_wrap(~ type, ncol = 1) +\n  theme(legend.position = \"none\")\np_type\n\n\n\n\n\n\n\n\n\n\n\n(a) Alle drei Temperaturen in einer Abbildung.\n\n\n\n\n\n\n\n\n\n\n\n(b) Die Temperaturen in drei separaten Abbildungen.\n\n\n\n\n\n\n\nAbbildung 62.8— Verlauf der Temperaturen auf dem Freilandfeld für Kopfsalat. Es wurden die minimalen, maximalen und ein durchschnittlicher Temperaturwert gemessen. Die Werte leiegn alle sehr nahe beieinander, so dass eine gute Darstellung mit einem statischen {ggplot} sehr schwer ist.\n\n\n\n\nIn der Abbildung 62.9 siehst du einmal die Version der ersten Abbildung in {plotly} dargestellt. Auf den ersten Blick ist alles gleich und auch wenn du die Abbildung ausdruckst oder in Word einfügst, wirst du nichts großartig anders machen können. Als Webseite oder im RStudio geht dann mehr. Du kannst jetzt mit der Maus über die Abbildung gleiten und dann werden dir die Werte an dem jeweiligen Punkt angezeigt. Das tolle ist, dass wir mit der Funktion ggplotly() viele Abbildungen aus {ggplot} direkt als {plotly} Abbildung wiedergeben lassen können. Wie immer gilt auch hier, dass die Hilfeseite Getting Started with {plotly} in ggplot2 einem enorm weiterhilft.\n\nggplotly(p_loc)\n\n\n\n\n\n\n\nAbbildung 62.9— Die Darstellung der Temperaturverläufe in {plotly}. Einzelne Werte können jetzt angezeigt werden und somit auch verglichen werden. Besonders der Button Compare data on hover ist hier sehr nützlich.\n\n\n\n\nIn der Abbildung 62.10 siehst du nochmal den Button Compare data on hover in Aktion. Du kannst dann direkt die drei Punkte miteinander vergleichen auch wenn die Punkte in der Abbildung schlecht auseinander zu halten sind. Wir können uns dann damit die Werte auf der \\(y\\)-Achse für jeden Zeitpunkt anzeigen lassen. Das “Hovern” über die Werte macht die visuelle Auswertung sehr viel einfacher als eine statische Abbildung.\n\n\n\n\n\n\nAbbildung 62.10— Wenn du oben rechts auf die beiden doppelten Pfeile klickst, dann aktivierst du Compare on hover, was dir ermöglicht direkt die Werte von \\(y\\) and einem Zeitpunkt zu vergleichen.\n\n\n\n\n\n62.4.4 Die Wetterstationsdaten\nDie Wetterstationsdaten können wir uns natürlich aucb so anschauen wie die Daten aus den Loggern bei den Salatdaten. Das habe ich dann auch einmal in der Abbildung 62.11 gemacht. Wir sehen in der Abbildung den Temperaturverlauf von April bis Ende Oktober. Das Problem ist auch wieder hier, dass wir einzelne Werte für ein Datum sehr schlecht ablesen können. Auch hier hilft dann {plotly} weiter, da können wir dann schön die Werte ablesen. Das Ziel ist es hier aber nicht eine einfache Scatterabbildung zu bauen sondern gleich nochmal ein 2D Konturplot. Aber fangen wir erstmal mit der Übersicht an.\n\np &lt;- station_tbl |&gt; \n  ggplot(aes(datum_uhrzeit, temp_boden_durch)) +\n  theme_minimal() +\n  geom_line() \np\n\n\n\n\n\n\n\nAbbildung 62.11— Verlauf der durchschnittlichen Temperatur an der Wetterstation von April bis Ende September.\n\n\n\n\n\nDann können wir natürlich auch wieder unsere statische Abbildung einmal in {plotly} umwandeln und uns die einzelnen Werte anschauen. Wir haben hier aber eher weniger Informationen, da der lineare Ablauf doch recht schwer über die Monate zu vergleichen ist. Viel besser wären da die Tage für jeden Monat nebeneinander. Oder aber wir schauen uns einmal die Temperatur für jeden Tag an. Wir haben ja auch die Uhrzeiten vorliegen.\n\nggplotly(p)\n\n\n\n\n\n\n\nAbbildung 62.12— Die Darstellung der durchschnittlichen Temperatur der Wetterstation in {plotly}. Einzelne Werte können jetzt angezeigt werden und somit auch verglichen werden.\n\n\n\n\nWir können uns jetzt in Abbildung 62.13 die drei Konturplots ansehen. Wichtig ist natürlich hier, dass wir vorher die Tage und den Monat aus dem Datum extrahiert haben. Jetzt geht es aber los mit dem Bauen der Konturplots. Wir mussten noch das Spektrum der Farben einmal drehen, damit es auch mit den Temperaturfarben passt und wir haben noch ein paar Hilfslinien mit eingezeichnet. Sie dazu auch meine Auswertung zum Isoplethendiagramm für Münster & Osnabrück im Skript zu beispielhaften Anwendung als ein anderes Beispiel mit DWD Daten. Wenn du die Daten aus deiner Region runterlädst, kannst du dir auch ähnliche Abbildungen bauen.\nIm Folgenden spiele ich mit den Funktionen geom_contour_filled() und geom_contour() rum um zum einen die Flächen und dann die Ränder des Isoplethendiagramms zu erhalten. Die Färbung ergibt sich dann aus der Funktion scale_fill_brewer(). Da wir hier exakt dreizehn Farben zu Verfügung haben, habe ich dann auch entschieden dreizehn Konturen zu zeichnen. Sonst musst du mehr Farben definieren, damit du auch mehr Flächen einfärben kannst. Teilweise musst du hier etwas mit den Optionen spielen, bis du bei deinen Daten dann eine gute Einteilung der Farben gefunden hast. Hier helfen dir dann die Optionen binwidth und bins weiter. Darüber hinaus habe ich mich auch entschieden hier mit einem Template in {ggplot} zu arbeiten, damit ich nicht so viel Code produziere. Ich baue mir im Prinzip einmal einen leeren Plot ohne die Funktion aes(). Die Definition was auf die \\(x\\)-Achse kommt und was auf die \\(y\\)-Achse mache ich dann später.\n\np &lt;- ggplot(station_tbl) +\n  theme_minimal() +\n  geom_contour_filled(bins = 13) +\n  scale_fill_brewer(palette = \"Spectral\", direction = -1) +\n  scale_x_continuous(breaks = 1:12) +\n  geom_vline(xintercept = 4:9, alpha = 0.9, linetype = 2) +\n  geom_hline(yintercept = c(4, 8, 12, 16, 20, 24), \n             alpha = 0.9, linetype = 2) +\n  labs(x = \"Monat\", y = \"Stunde\", fill = \"Temperatur [°C]\")\n\nWir können mit dem Operator %+% zu einem bestehenden {ggplot} neue Daten hinzufügen. Dann können wir auch wie gewohnt neue Optionen anpassen. Deshalb dann einmal das Template zusammen mit der Temperatur als Kontur aufgeteilt nach Monat und Stunde sowie Monat und Tag. Dann habe ich mir noch die Leistung der Sonne über den Monat und der Stunde anzeigen lassen. Je nach Fragestellung kommt es dann eben auf die Abbildung drauf an. Bei Zeitreihen haben wir mit dem Konturplot noch eine weitere Möglichkeit Daten spannend und aufschlussreich darzustellen.\n\np %+%\n  aes(month, hour, z = temp_boden_durch) +\n  geom_contour(binwidth = 2, color = \"black\") +\n  scale_y_continuous(limits = c(1, 24), breaks = c(4, 8, 12, 16, 20, 24)) +\n  labs(x = \"Monat\", y = \"Stunde\", fill = \"Temperatur [°C]\")\n\np %+%\n  aes(month, day, z = temp_boden_durch) +\n  geom_contour(binwidth = 2, color = \"black\") +\n  scale_y_continuous(limits = c(1, 30), breaks = c(5, 10, 15, 10, 25, 30)) +\n  labs(x = \"Monat\", y = \"Tag\", fill = \"Temperatur [°C]\")\n\np %+%\n  aes(month, hour, z = solar_mv) +\n  scale_y_continuous(limits = c(1, 24), breaks = c(4, 8, 12, 16, 20, 24)) +\n  labs(x = \"Monat\", y = \"Stunde\", fill = \"Solar [MV]\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Durchschnittstemperaturen der Wetterstation über den Tag.\n\n\n\n\n\n\n\n\n\n\n\n(b) Durchschnittstemperaturen der Wetterstation über den Monat.\n\n\n\n\n\n\n\n\n\n\n\n(c) Solare Leistung über den Tag.\n\n\n\n\n\n\n\nAbbildung 62.13— Konturplot der verschiedenen Temperaturen der Wetterstation in Hagebüchen in den Monaten April bis Mitte September. Die Temperaturen wurden jede Stunde einmal erfasst. Dargestellt sind die Durchschnittstemperaturen.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Pseudo Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-pseudo-time-series.html#sec-time-roll-mean",
    "href": "time-space-pseudo-time-series.html#sec-time-roll-mean",
    "title": "62  Pseudo Zeitreihen",
    "section": "62.5 Einfache Glättungen",
    "text": "62.5 Einfache Glättungen\nBis jetzt haben wir uns die Visualisierung von Zeitreihen angeschaut. Häufig reicht die Visualisierung auch aus, wenn es um die Darstellung von Temperaturverläufen in einer Abschlussarbeit geht. Darüber hinaus wollen wir dann aber doch irgendwie eine statistische Analyse auf der Zeitreihe rechnen. Deshalb habe ich hier mal angefangen Beispiele zu Auswertungen von “pseudo” Zeitreihen zu sammeln und vorzustellen. Hauptsächlich nehme ich natürlich die drei Datensätze von weiter oben im Kapitel. Da die drei Datensätze zwar echte Daten aus dem agrarwissenschaftlichen Kontext repräsentieren, genügen die Datensätze dann doch nicht immer einer klassischen Zeitreihenanalyse.\nFangen wir also einmal mit eine einfachen Glättung an. Wir schauen uns hier als erstes die Standardvariante in R an. Ich zeige dir verschiedene Beispiele für die Glättung von Zeitreihen durch Mittelwert, Median oder aber auch der Summe. Du könntest auch stat_smooth() aus {ggplot} verwenden, aber hier zeige ich dir noch ein paar differenzierte Möglichkeiten. Das heißt, wir nehmen hier die einfachen Funktionen, die in R implementiert sind und rechnen damit eine Zeitreihenanalyse. Damit haben wir dann einige Nachteile, da wir uns die Funktionen dann eventuell nochmal aus Paketen zusammensuchen müssen. Dafür ist es aber schön kleinteilig und du kannst die Analysen Schritt für Schritt durchführen. Wenn dir das zu kleinteilig oder aber veraltet ist, dann schaue gleich weiter unten in den Abschnitten zu den R Paketen {tktime} und {modeltime} nach. Beide Pakete sind die Antwort auf eine Analyse von Zeitreihen im {tidyverse}.\n\n\n\n\n\n\nKonvertierung von verschiedenen Datumsformaten in R\n\n\n\nDas R Paket {timetk} liefert dankenswerterweise Funktionen für die Konvertierung von verschiedenen Zeitformaten in R. Deshalb schaue einmal in die Hilfeseite Time Series Class Conversion – Between ts, xts, zoo, and tbl und dann dort speziell der Abschnitt Conversion Methods. Leider ist Zeit in R wirklich relativ.\n\n\nLeider ist das Zeitformat ts etwas quälend. Dennoch basieren viele Tutorien auf diesem Format, deshalb hier auch einmal die Erklärung dafür. Es ist aber auch verständlich, denn das Format ist sozusagen der eingebaute Standard in R. Standard heißt hier aber nicht toll, sondern eher veraltet aus den 90zigern. Dann gibt es mit dem R Paket {zoo} noch ein Palette an nützlichen Funktionen, wenn du nicht so viel machen willst. Mit so viel meine ich, dass du eher an einem rollenden Mittelwert oder aber der rollenden Summe interessiert bist. Dann macht das R Paket {zoo} sehr viel Sinn. Einen Überblick liefert hier auch das Tutorium Reading Time Series Data.\nWenn wir viele Datenpunkte über die Zeit messen, dann hilft es manchmal die Spitzen und Täler aus den Daten durch eine rollende statistische Maßzahl zusammenzufassen. Das R Paket {zoo} hat die Funktion rollmean() sowie rollmax() und rollsum(). Es gibt aber noch eine Reihe weiterer Funktionen. Du musst hier einfach mal die Hilfeseite ?rollmean() für mehr Informationen aufrufen. Mit den Funktionen können wir für ein Zeitintervall \\(k\\) den Mittelwert bzw. der anderen Maßzahlen berechnen. In unserem Fall habe ich einmal das rollende Monatsintervall genommen. Du kannst aber auch andere Zeiten für \\(k\\) einsetzen und überlegen welcher Wert besser zu deinen Daten passt. Hier einmal die Berechnung für das rollende Mittel, das rollende Maximum und die rollende Summe. In allen drei Fällen nutzen wir die Funktion split() und map() um effizient unseren Code auszuführen.\n\nrollmean()rollmax()rollsum()\n\n\n\nroll_mean_tbl &lt;- salad_long_tbl |&gt; \n  split(~type) |&gt; \n  map(~zoo(.x$temp, .x$datum)) |&gt; \n  map(~rollmean(.x, k = 29)) |&gt; \n  map(tk_tbl) |&gt; \n  bind_rows(.id = \"type\")\n\n\n\n\nroll_max_tbl &lt;- salad_long_tbl |&gt; \n  split(~type) |&gt; \n  map(~zoo(.x$temp, .x$datum)) |&gt; \n  map(~rollmax(.x, k = 29)) |&gt; \n  map(tk_tbl) |&gt; \n  bind_rows(.id = \"type\")\n\n\n\n\nroll_sum_tbl &lt;- salad_long_tbl |&gt; \n  split(~type) |&gt; \n  map(~zoo(.x$temp, .x$datum)) |&gt; \n  map(~rollsum(.x, k = 29)) |&gt; \n  map(tk_tbl) |&gt; \n  bind_rows(.id = \"type\")\n\n\n\n\nIn der Abbildung 62.14 siehst du einmal das Ergebnis der drei rollenden Maßzahlen. Im Folgenden habe ich zuerst das Template p_temp erstellt und dann über den Operator %+% die Datensätze zum rollenden Mittelwert, zu dem rollenden Maximum und der rollenden Summe ersetzt. Die rollende Summe habe ich noch auf der \\(\\log\\)-transformierten \\(y\\)-Achse dargestellt.\n\np_temp &lt;- ggplot() +\n  aes(index, value, color = type) +\n  theme_minimal() +\n  geom_point2() +\n  stat_smooth(se = FALSE) +\n  labs(x = \"Datum\", y = \"Rollende statistische Maßzahl\", \n       color = \"Type\") +\n  scale_color_okabeito() \n\nWir sehen in der folgenden Abbildung, dass sich die Messtypen dann doch nicht so stark in durch die rollenden Maßzahlen unterscheiden. Wir haben ja schon in der Orginalabbildung das Problem gehabt, dass sich die Werte sehr stark ähneln. Das scheint auch über 29 Tage der Fall zu ein. Was man besser sieht, ist das wellenförmige Ansteigen der Temperatur über die gemessene Zeit. Wir hatten also immer mal wieder etwas kältere Phasen, die von wärmeren Phasen abgelöst wurden.\n\np_temp %+%\n  roll_mean_tbl +\n  ylim(0, 40)\n\np_temp  %+%\n  roll_max_tbl +\n  ylim(0, 40)\n\np_temp  %+%\n  roll_sum_tbl +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\n\n(a) Rollender Mittelwert.\n\n\n\n\n\n\n\n\n\n\n\n(b) Rollendes Maximum.\n\n\n\n\n\n\n\n\n\n\n\n(c) Rollende Summe\n\n\n\n\n\n\n\nAbbildung 62.14— Darstellung der rollenden Mittelwerte, maximalen Werte sowie aufsummierten Werte über 29 Tage. Die aufsummierten Werte sind auf logarithmischen Skala dargestellt. Gegenüber der orginalen Abbildung sehen wir schon etwas mehr Ordnung. Die drei Arten der Messung unterscheiden sich aber weiterhin kaum.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Pseudo Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-pseudo-time-series.html#sec-time-two-compare",
    "href": "time-space-pseudo-time-series.html#sec-time-two-compare",
    "title": "62  Pseudo Zeitreihen",
    "section": "62.6 Vergleich von Zeitreihen",
    "text": "62.6 Vergleich von Zeitreihen\nUnsere Dronenüberflugdaten sind etwas besondere Daten, wenn wir uns Zeitreihen anschauen. Wir haben zwar auch einen zeitlichen Verlauf auf der \\(x\\)-Achse, aber der Zeitrahmen ist mit unter einem Jahr zu klein um einen zyklischen Verlauf zu beobachten. Wir wollen hier auch etwas anderes erreichen. Uns interessieren die einzelnen Beobachtungen nicht, wir wollen die angepasste Graden durch die Punktewolken vergleichen. In der Abbildung 62.15 siehst du nochmal die angepassten Kurven ohne die einzelnen Messpunkte. Eigentlich rechnen wir hier einen Gruppenvergleich über die Zeit. Spannende Sache, die wollen wir uns dann mal genauer ansehen. Wir werden hier aber keinen statistischen Test rechnen, sondern nur ausrechnen in wie weit sich die beiden Parzellen numerisch im Ertrag unterscheiden.\n\ncsh_tbl |&gt; \n  ggplot(aes(day, g_tm_plot, color = parzelle)) +\n  theme_minimal() +\n  geom_point(alpha = 0.4) +\n  stat_smooth(se = FALSE) +\n  scale_color_okabeito() \n\n\n\n\n\n\n\nAbbildung 62.15— Die Graserträge für die beiden Parzellen Uelzen und Wiese von Mai bis Ende September. Wir sind an der Fläche zwischen den beiden Graden interessiert.\n\n\n\n\n\nGibt es also einen Unterschied zwischen den beiden Parzellen \\(Uelzen\\) und \\(Wiese\\) im Bezug auf den Ertrag? Dafür müssen wir die Differenz der Graden an jedem Punkt berechnen. Oder anders formuliert, wir wollen die Fläche zwischen den beiden Kurven berechnen. Um die Fläche zu berechnen, brauchen wir die Koordinaten, die die Kurven beschreiben. Wir machen uns es hier aber etwas einfacher und berechnen die Kurven nochmal separat mit der Funktion gam() aus dem R Paket {mgcv}. In beiden Tabs sehen wir dann jeweils die Modellanpassungen für die beiden Parzellen in Uelzen und auf der Wiese.\n\ngam() auf Uelzengam() auf Wiese\n\n\nWenn du ein GAM rechnest, dann musst du auf jeden Fall, die Variable, die auf der \\(x\\)-Achse ist nochmal in ein s() packen, damit hier auch ein Spline bzw. eine Glättung mit der Variable gerechnet wird.\n\ngam_uelzen_fit &lt;- csh_tbl |&gt; \n  filter(parzelle == \"Uelzen\") |&gt; \n  gam(g_tm_plot ~ s(day_num), data = _)\ngam_uelzen_fit\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ng_tm_plot ~ s(day_num)\n\nEstimated degrees of freedom:\n8.56  total = 9.56 \n\nGCV score: 311.7048     \n\n\nDas Ergebnis ist relativ nichts sagend für uns, wir nutzen jetzt gleich den Fit um die Fläche unter der Kurve zu berechnen. Der GCV score ist in etwa ein AIC-Wert. Damit sind Modelle mit kleineren GCV-Werten zu bevorzugen.\n\n\nWenn du ein GAM rechnest, dann musst du auf jeden Fall, die Variable, die auf der \\(x\\)-Achse ist nochmal in ein s() packen, damit hier auch ein Spline bzw. eine Glättung mit der Variable gerechnet wird.\n\ngam_wiese_fit &lt;- csh_tbl |&gt; \n  filter(parzelle == \"Wiese\") |&gt; \n  gam(g_tm_plot ~ s(day_num), data = _)\ngam_wiese_fit\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ng_tm_plot ~ s(day_num)\n\nEstimated degrees of freedom:\n8.61  total = 9.61 \n\nGCV score: 190.3596     \n\n\nDas Ergebnis ist relativ nichts sagend für uns, wir nutzen jetzt gleich den Fit um die Fläche unter der Kurve zu berechnen. Der GCV score ist in etwa ein AIC-Wert. Damit sind Modelle mit kleineren GCV-Werten zu bevorzugen.\n\n\n\nDann können wir schon über die Funktion integrate() die Fläche unter der Kurve (eng. area under the curve, abk. AUC) berechnen. Dafür brauchen wir dann einmal die Funktion der Kurve, die wir uns mit predict() generieren. Dann wollen wir noch den ersten bis zum letzten Zeitpunkt integrieren. Da wir dann einunddreißig Messzeitpunkte haben, integrieren wir von Eins bis Einunddreißig. Ich mache dann beides einmal in den folgenden Tabs. Hier musst du manchmal etwas frickeln, bist du die Anzahl deiner Messtage weißt. Ich muss ja immer schauen, denn ich habe das Experiment ja nicht gemacht.\n\nAUC für UelzenAUC für Wiese\n\n\n\nf_uelzen_gam &lt;- function(x) predict(gam_uelzen_fit, tibble(day_num = x))\nintegrate(f_uelzen_gam, 1, 31)  \n\n1297.45 with absolute error &lt; 0.076\n\n\n\n\n\nf_wiese_gam &lt;- function(x) predict(gam_wiese_fit, tibble(day_num = x))\nintegrate(f_wiese_gam, 1, 31)  \n\n1182.063 with absolute error &lt; 0.11\n\n\n\n\n\nAm Ende haben wir dann die Fläche unter der Kurve für die Parzelle in Uelzen mit \\(1297\\) und die Fläche der Parzelle der Wiese mit \\(1182\\). Damit unterscheiden sich die beiden Parzellen um einen Ertrag von \\(115\\). Wir können eine einzelne Zahl nicht statistisch Testen, daher steht die Zahl so erstmal im Raum. Ob das jetzt viel oder wenig Ertrag ist, muss du selber entscheiden. Im folgenden Kasten zeige ich nochmal, wie du mit Messwiederholungen dann doch noch eine statistische Aussage über zwei Gruppen erhalten kannst.\n\n\n\n\n\n\nAnwendungsbeispiel: Vergleich von Zeitreihen mit Wiederholungen\n\n\n\nWir hatten uns eben gerade zwar ein Beispiel für den Vergleich zweier Zeitreihen angeschaut, aber hatten wir dort keine Wiederholungen, die wir zuordnen konnten. Wenn wir mit Pflanzen arbeiten, dann können wir das Wachstum über die Zeit bei jeder einzelnen Pflanze messen. Dann können wir aber auch den Verlauf des Wachstums für jede Pflanze darstellen. Damit haben wir dann auch noch eine andere Möglichkeit die Daten zu analysieren. Dazu nutzen wir jetzt ein Beispiel mit Baumwolle. Wir haben zwei Typen von Baumwole vorliegen. Einmal die Standardsorte ctrl sowie eine neue genetisch veränderte Sorte genetic. Wir haben das Wachstum an zwölf Pflanzen P1 bis P12 an insgesamt achtzehn Terminen gemessen. Wir haben die Daten in einem Wide-Format vorliegen und bauen uns deshalb mit der Funktion pivot_longer() einen Long-Format. Vorher müssen wir noch das Datum in Messtage umwandeln, damit wir besser mit den Werten rechnen können. Für die Darstellung nutzen wir dann die ursprünglcihe Datumsspalte.\n\ncotton_time_tbl &lt;- read_excel(\"data/timeseries_cotton.xlsx\", sheet = \"diameter\") |&gt; \n  group_by(trt) |&gt; \n  mutate(day_measured = 1:n()) |&gt; \n  pivot_longer(cols = P1:P12,\n               names_to = \"plant_id\",\n               names_pattern = \".(\\\\d+)\",\n               names_transform = as.numeric,\n               values_to = \"height\") |&gt; \n  ungroup()\n\nIn der Abbildung 62.16 siehst du einmal die Daten der Wuchshöhe der zwölf Baumwollpflanzen pro Sorte über die achtzehn Messtermine visualisiert. Die beiden Baumwolllinien sind jeweils farbig markiert, die einzelnen Linien entsprechen den jeweils zwölf Pflanzen pro Linie. Wir müssen hier etwas mit der Funktion new_scale_color() arbeiten, damit wir dann wieder einen neuen Farbgradienten einführen können. Die Linien sind mit der loess()-Funktion durch die Punkte geschätzt. Wir speichern die Abbildung auch einmal in dem Objekt p1, da wir gleich noch die Abbildung mit der Auswertung der Flächen unter den Kurven zusammenbringen wollen.\n\np1 &lt;- ggplot() +\n  aes(date, height, group = plant_id) +\n  theme_minimal() +\n  geom_smooth(data = filter(cotton_time_tbl, trt == \"ctrl\"),\n              aes(color = plant_id),\n              method = \"loess\", se = FALSE) +\n  scale_color_gradientn(colors = c('#99000d', '#fee5d9'), guide=\"none\") +\n  new_scale_color() +\n  geom_smooth(data = filter(cotton_time_tbl, trt == \"genetic\"), \n              aes(color = plant_id),\n              method = \"loess\", se = FALSE) +\n  scale_color_gradientn(colors = c('#084594', '#4292c6'), guide=\"none\") +\n  new_scale_color() +\n  geom_point(data = cotton_time_tbl, aes(color = trt)) +\n  scale_color_manual(values = c('#99000d', '#084594')) +\n  theme(legend.position = \"top\") +\n  labs(color = \"Baumwolllinie\", x = \"\", y = \"Wuchshöhe in [cm]\")\np1  \n\n\n\n\n\n\n\nAbbildung 62.16— Wuchshöhe der zwölf Baumwollpflanzen pro Sorte über die achtzehn Messtermine. Die beiden Baumwolllinien sind jeweils farbig markiert, die einzelnen Linien entsprechen den jeweils zwölf Pflanzen pro Linie.\n\n\n\n\n\nJetzt stellt sich die Frage, ob sich das Wachstum über die Zeit in den beiden Linien unterscheidet. Dafür müssen wir die Fläche unter der Kurve für jede Pflanze berechnen. Dann können wir die Flächen vergleichen. Wenn das Wachstum gleich wäre, dann wären auch die Flächen gleich. Ich habe mir dafür die Funktion get_area() gebaut, die intern die Funktion loess() angepasst für diesen Datensatz aufruft. Dann brauche die noch die Funktion die integriert werden soll, die brauche ich mir mit der Funktion predict(). Abschließend muss ich noch der Funktion integrate() mitteilen von welchem \\(x\\)-Wert ich integrieren will. Ich entscheide mich hier von der ersten Messung bis zur letzten Messung am achtzehnten Tag zu integrieren.\n\nget_area &lt;- function(tbl) {\n  fit &lt;- loess(height ~ day_measured, tbl)\n  f &lt;- function(x) predict(fit, newdata = x)\n  integrate(f, 1, 18)$value  \n}\n\nStatt mit der Funktion loess() lassen sich die Kurven auch mit der Funktion gam() aus dem R Paket {mgcv} bauen. Ich nutze hier loess(), da die Funktion einfach zu nutzen ist und für die Genauigkeit hier ausreicht. Wenn du die Funktion gam() nutzt, dann musst die Funktion predict() anpassen, da bei gam() nur Datensätze als newdata akzeptiert werden.\nNachdem ich mir die nun Funktion gebaut habe, kann ich dann für jede Kombination aus Linie und Pflanze einmal die Fläche unter der Kurve auc (eng. area under the curve) berechnen. Ich nutze dazu die Funktionen nest() und map() um mir die Sache einfacher zu machen. Ich könnte auch mit filter() mir alle Subgruppen rausfiltern und am Ende zusammenbauen. Aber so geht es eben schneller.\n\nauc_cotton_tbl &lt;- cotton_time_tbl |&gt; \n  group_by(trt, plant_id) |&gt; \n  nest() |&gt; \n  mutate(auc = map(data, ~get_area(.x))) |&gt; \n  unnest(auc) \n\nDann kann ich auch schon einen t-Test für den Vergleich der zwölf Flächen unter der Kurve für die Kontrolllinie und der neuen genetischen Linie rechnen. Prinzipiell ginge hier auch ein nicht-parametrischer Test, aber die Daten sehen einigermaßen normalverteilt aus, wie du gleich in den Boxplots in der Abbildung 62.17 sehen wirst.\n\nt.test(auc ~ trt, data = auc_cotton_tbl)\n\n\n    Welch Two Sample t-test\n\ndata:  auc by trt\nt = 2.5392, df = 21.985, p-value = 0.0187\nalternative hypothesis: true difference in means between group ctrl and group genetic is not equal to 0\n95 percent confidence interval:\n 0.3128222 3.1016126\nsample estimates:\n   mean in group ctrl mean in group genetic \n             15.91998              14.21277 \n\n\nWir haben also einen signifikanten \\(p\\)-Wert für den Verglich der Flächen unter der Kurve für die beiden Linien. Ich baue mir jetzt einmal den Boxplot in Abbildung 62.17 und speichere mir den Plot auch gleich in ein Objekt p2. Ich werde dann die beiden Abbildungen p1 und p2 gleich einmal zusammenbringen. Vieles von dem Code dient nur die Abbildung schöner zu machen.\n\np2 &lt;- auc_cotton_tbl |&gt; \n  ggplot(aes(trt, auc, fill = trt)) +\n  theme_minimal() +\n  geom_boxplot(alpha = 0.8)  +\n  scale_fill_manual(values = c('#99000d', '#084594')) +\n  theme(legend.position = \"none\") +\n  labs(y = \"Fläche unter der Kurve\", x = \"\") +\n  annotate(\"text\", x = 1.5, y = 19, label = \"p = 0.019\") +\n  scale_y_continuous(position = \"right\")\np2\n\n\n\n\n\n\n\nAbbildung 62.17— Boxplot der Fläche unter der Kurve für die beiden Baumwolllinien. Die Achsenbeschriftung ist auf der rechten Seite, da die Abbildung gleich mit den zeitlichen Verläufen zusammengebracht wird.\n\n\n\n\n\nDann können wir auch die beiden Abbildungen p1 und p2 mit dem R Paket {patchwork} zusammenbringen und eine Annotation mit den Buchstaben A und B ergänzen. Dann mache ich den Boxplot noch über die Funktion plot_layout() sehr viel schmaler, als den zeitlichen Verlauf. Eigentlich brauchen wir dann nur die Abbildung 62.18 veröffentlichen, die Abbildung bringt ja alles zusammen. Anscheinend gibt es einen Unterschied zwischen den beiden Linien. Überraschenderweise ist unsere neue genetische Linie signifikant kleiner. Oder andersherum, die Linie ist kleiner, ob das gewollt ist war oder nicht, kann ich dir nicht beantworten.\n\n\n\n\n\n\n\n\nAbbildung 62.18— (A) Wuchshöhe der zwölf Baumwollpflanzen pro Sorte über die achtzehn Messtermine. Die beiden Baumwolllinien sind jeweils farbig markiert, die einzelnen Linien entsprechen den jeweils zwölf Pflanzen pro Linie. Die Kurven wurden über eine loess-Funktion geschätzt. (B) Boxplot der Fläche unter der Kurve für die beiden Baumwolllinien. Der \\(p\\)-Wert stammt aus einem Welch t-Test.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Pseudo Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-pseudo-time-series.html#wie-weiter",
    "href": "time-space-pseudo-time-series.html#wie-weiter",
    "title": "62  Pseudo Zeitreihen",
    "section": "62.7 Wie weiter?",
    "text": "62.7 Wie weiter?\nUnd wie jetzt weiter? Was ist wenn du eine Vorhersage machen willst? Ja in dem Fall können wir dann im nächsten Kapitel einmal schauen, wie wir eine klassische Zeitreihenanalyse rechnen. Wir brauchen aber auf jeden Fall eine lange Beobachtungsdauer mit vielen Zeitpunkten. Aber das schaue dir am besten einmal im nächsten Kapitel an.\n\n\n\nAbbildung 62.1— Ursprüngliche Abbildung, die nachgebaut werden soll. Wir haben einen zeitlichen Verlauf des Wachstums von Petunien vorliegen. Am Ende interessiert uns dann nur der Vergleich am letzten Tag der Messung. Wirrerweiseist die \\(x\\)-Achse nicht mit den Messtagen beschriftet sondern anderen Tagen. Das macht wenig Sinn. Auch fehlt die Standardabweichung. Hier sind nur Mittelwerte dargestellt.\nAbbildung 62.2— Visualisierung des Wachstums von Petunien mit Gerade durch die Mittelwerte und die entsprechenden Standardabweichungen in den Gruppen. Die Punkte wurden etwas gestreut, damit wir nicht alles übereinander liegen haben. Darüber hinaus werden auch die richtigen Zeitpunkte auf der \\(x\\)-Achse angezeigt. Die Abstände sind jetzt alle gleich lang.\nAbbildung 62.3— Visualisierung des Mittelwertsunterschied für den 03. Juni mit Standardabweichung und compact letter display.\nAbbildung 62.4— Darstellung des Wachstums von Petunien mit Gerade durch die Mittelwerte und die entsprechenden Standardabweichungen in den Gruppen. Ergänzt wurde auf der rechten Seite der Mittelwertsunterschied für den 03. Juni mit Standardabweichung und compact letter display.\nAbbildung 62.5— Zeitlicher Verlauf der Messerte vom Ozone von 1957 bis 2022. Die verschieden farbigen Punkte stellen die unterschiedlichen Messverfahren dar.\nAbbildung 62.6— Zeilicher Verlauf der Messerte vom Ozone von 1957 bis 2022. Die verschieden farbigen Punkte stellen die unterschiedlichen Messverfahren dar. Die Regressionsgerade dient der Veranschaulichung des Trends des Ozongehalts in den Siebzigern. Die Linie bei 180 stellt den Grenzwert der NASA für einen Fehler in den Messwerten dar und damit einen zu niedrigen Wert.\nQuelle: https://xkcd.com/\nAbbildung 62.7 (a)— Darstellung der Beobachtungen\nAbbildung 62.7 (b)— Mit stat_smooth-Funktion\nAbbildung 62.8 (a)— Alle drei Temperaturen in einer Abbildung.\nAbbildung 62.8 (b)— Die Temperaturen in drei separaten Abbildungen.\nAbbildung 62.10— Wenn du oben rechts auf die beiden doppelten Pfeile klickst, dann aktivierst du Compare on hover, was dir ermöglicht direkt die Werte von \\(y\\) and einem Zeitpunkt zu vergleichen.\nAbbildung 62.11— Verlauf der durchschnittlichen Temperatur an der Wetterstation von April bis Ende September.\nAbbildung 62.13 (a)— Durchschnittstemperaturen der Wetterstation über den Tag.\nAbbildung 62.13 (b)— Durchschnittstemperaturen der Wetterstation über den Monat.\nAbbildung 62.13 (c)— Solare Leistung über den Tag.\nAbbildung 62.14 (a)— Rollender Mittelwert.\nAbbildung 62.14 (b)— Rollendes Maximum.\nAbbildung 62.14 (c)— Rollende Summe\nAbbildung 62.15— Die Graserträge für die beiden Parzellen Uelzen und Wiese von Mai bis Ende September. Wir sind an der Fläche zwischen den beiden Graden interessiert.\nAbbildung 62.16— Wuchshöhe der zwölf Baumwollpflanzen pro Sorte über die achtzehn Messtermine. Die beiden Baumwolllinien sind jeweils farbig markiert, die einzelnen Linien entsprechen den jeweils zwölf Pflanzen pro Linie.\nAbbildung 62.17— Boxplot der Fläche unter der Kurve für die beiden Baumwolllinien. Die Achsenbeschriftung ist auf der rechten Seite, da die Abbildung gleich mit den zeitlichen Verläufen zusammengebracht wird.\nAbbildung 62.18— (A) Wuchshöhe der zwölf Baumwollpflanzen pro Sorte über die achtzehn Messtermine. Die beiden Baumwolllinien sind jeweils farbig markiert, die einzelnen Linien entsprechen den jeweils zwölf Pflanzen pro Linie. Die Kurven wurden über eine loess-Funktion geschätzt. (B) Boxplot der Fläche unter der Kurve für die beiden Baumwolllinien. Der \\(p\\)-Wert stammt aus einem Welch t-Test.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Pseudo Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-time-series.html",
    "href": "time-space-time-series.html",
    "title": "63  Zeitreihen",
    "section": "",
    "text": "63.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, janitor, see, readxl,\n               xgboost, tidymodels, modeltime, forecast,\n               lubridate, plotly, zoo, timetk, xts,\n               corrplot, GGally, conflicted)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(magrittr::set_names)\nconflicts_prefer(plyr::mutate)\nconflicts_prefer(dplyr::slice)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-time-series.html#daten-visualisierung",
    "href": "time-space-time-series.html#daten-visualisierung",
    "title": "63  Zeitreihen",
    "section": "63.2 Daten & Visualisierung",
    "text": "63.2 Daten & Visualisierung\nIn diesem Abschnitt wollen wir uns einmal ein paar “echte” Zeitreihen anschauen, die den Anforderungen der klassischen Zeitreihenanalyse an eine Zeitreihe genügen. Du kennst ja schon aus dem vorherigen Kapitel die Möglichkeiten der Visualisierung, deshalb packe ich hier auch gleich zu den Daten die entsprechenden Abbildungen. Dann wird dir der Zusammenhang auch schneller bewusst. Wenn die Daten nochmal wegen dem Datum formatiert werden mussten, dann habe ich das auch gleich hier in einem Abwasch erledigt. Wenn da noch Fragen sind, dann schaue gerne nochmal in das vorherige Kapitel, da findest du dann noch mehr Beispiele. Hier soll es mehr um die Verläufe und die Vorhersage gehen.\n\n\n\n\n\n\nInspirationen von The R Graph Gallery\n\n\n\nWenn du noch Inspirationen suchst, wie du deine Zeitreihe noch schöner darstellen könntest, dann besuche doch The R Graph Gallery - Time Series. Dort findest du verschiedene Darstellungen einer Zeitreihe mit {ggplot}. Ich finde den Lollipop-Chart im Kontext einer Zeitreihe sehr spannend. Aber lass dich einfach mal inspirieren.\n\n\nFangen wir also mit dem ersten Datensatz an. Im Sommer 1987 maßen die Ranger des Yellowstone-Nationalparks die Zeit zwischen den Ausbrüchen des Old Faithful Geysirs. Dieser Geysir ist für seine relativ regelmäßigen Ausbrüche bekannt, aber wie du dir vorstellen kannst, ist der Geysir keine Uhr. Ein Ziel bei der Erfassung der Zeiten war es, eine Möglichkeit zu finden, den Zeitpunkt des nächsten Ausbruchs vorherzusagen, um den Touristen, die auf einen Ausbruch warten, die Wartezeit zu erleichtern. Wir haben also eine Zeitreihe in Minuten aus dem Yellowstone-Nationalpark vorliegen. Die Daten in Minuten für \\(n=107\\) fast aufeinanderfolgende Wartezeiten lauten dann wie folgt.\n\nerupt &lt;- c(78, 74, 68, 76, 80, 84, 50, 93, 55, 76, 58, 74, 75, 80, 56, 80, 69, 57,\n           90, 42, 91, 51, 79, 53, 82, 51, 76, 82, 84, 53, 86, 51, 85, 45, 88, 51,\n           80, 49, 82, 75, 73, 67, 68, 86, 72, 75, 75, 66, 84, 70, 79, 60, 86, 71,\n           67, 81, 76, 83, 76, 55, 73, 56, 83, 57, 71, 72, 77, 55, 75, 73, 70, 83,\n           50, 95, 51, 82, 54, 83, 51, 80, 78, 81, 53, 89, 44, 78, 61, 73, 75, 73,\n           76, 55, 86, 48, 77, 73, 70, 88, 75, 83, 61, 78, 61, 81, 51, 80, 79)\n\nWir haben hier also eine echte Zeitreihe vorliegen. Wir haben die zeitlichen Abstände zwischen zwei Ausbrüchen. Das ist dann auch die einfachste mögliche Zeitreihe. Dennoch können wir dann an diesem Beispiel sehr viel erklären und uns die gängigen Konzepte der klassischen Zeitreihenanalyse veranschaulichen. In der Abbildung 63.2 siehst du dann nochmal den Verlauf der Eruptionen über die Zeit. Dir kommt die Abbildung sicherlich aus der Einleitung des Kapitels bekannt vor. Dort habe ich die Abbildung einmal für ein generelles Beispiel genutzt, hier ist dann der richtige Kontext.\n\n\n\n\n\n\n\n\nAbbildung 63.2— Dauer zwischen zwei Eruptionen des Old Faithful Geysirs. Eine klare zyklische Zeitreihe lässt sich aus der Abbildung ableiten.\n\n\n\n\n\nAls nächstes Datenbeispiel möchte ich den schon fast ikonischen Anstieg an \\(CO_2\\) in der Atmosphäre einmal betrachten. Ich habe die Daten von dem Global Monitoring Laboratory heruntergeladen. Du kannst dir auch gern einmal die Daten für \\(CH_4\\) \\(N_2O\\) und \\(SF_6\\) anschauen. Auch hier liegen Verläufe vor, die du dann einmal visualisieren und vorhersagen kannst. Wenn du noch mehr Beispiele von echten Daten haben möchtest, dann besuch doch die Webseite Our World in Data.\nIch muss mich hier etwas strecken um die Daten richtig sauber einzulesen. Zum einen nutze ich die \\(CO_2\\) Daten für die Monate und dann einmal für die Tage. Als erste habe ich das Problem, dass ich erstmal einiges an zeilen überspringen muss, bis die echten Daten kommen. Leider hat das Datumsformat auch eine echt schlimme Kodierung in den Daten. Daher nutze ich die Funktion parse_date_time() um mir ein Datum wiedergeben zu lassen. Vorher muss ich die Monate noch so umbauen, dass die Monate immer zwei Zeichen lang sind. Leider braucht as.Date() immer einen Tag, was bei den monatlichen Daten nicht gegeben ist. In den folgenden Analysen nutze ich dann beide Datensätz um mal zu schauen, wie deatiliert die Informationen vorliegen müssen. Reicht ein Wert pro Monat oder aber macht es ein Wert pro Tag besser?\n\nco2_monthly_tbl &lt;- read_csv(\"data/co2_mm_mlo.csv\", skip = 40) |&gt; \n  mutate(month = str_pad(month, 2, pad = \"0\", side = \"left\"),\n         date = parse_date_time(str_c(year, month), \"ym\"))\n\nco2_daily_tbl &lt;- read_csv(\"data/co2_daily_mlo.csv\", skip = 32, col_names = FALSE) |&gt; \n  set_names(c(\"year\", \"month\", \"day\", \"decimal date\", \"average\")) |&gt; \n  mutate(month = str_pad(month, 2, pad = \"0\", side = \"left\"),\n         date = parse_date_time(str_c(year, month, day), \"ymd\"))\n\nIn der Abbildung 63.3 siehst du einmal die Verläufe des \\(CO_2\\) Anstiegs in der Atmosphäre. Beachte hier, dass die beiden Zeitreihen unterschiedlich lang sind. Die monatlichen Daten beginnen in den 1960ziger Jahren wobei wir die Daten für die täglichen Messungen erst ab den späten 70zigern vorliegen haben. Ich habe hier jetzt einmal die Funktion plot_time_series() aus dem R Paket {timetk} genutzt. Es gibt natürlich auch noch andere Möglichkeiten, aber ich mag die interaktive Funktion, die du sonst über den Umweg über {plotly} nutzen müsstest. Aber da gibt es dann in dem vorherigen Kapitel genug Beispiele für. Mehr findest du auch auf der Seite von R Coder - Evolution charts.\nco2_monthly_tbl |&gt; \n    plot_time_series(date, average, .interactive = TRUE) \nco2_daily_tbl |&gt; \n    plot_time_series(date, average, .interactive = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Gemittelt über den Monat.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Gemittelt über den Tag.\n\n\n\n\n\n\n\nAbbildung 63.3— Verlauf des \\(CO_2\\) Anstiegs in der Atmosphäre. Hier einmal die Darstellung mit der Funktion plot_time_series() aus dem R Paket {timetk}. Die beiden Datensätze haben aber nicht die gleiche Zeitspanne beobachtet.\n\n\n\nEinen weiteren Datensatz, den wir uns anschauen wollen, ist ein Datensatz zu der Milchleistung von Kühen stammt aus dem Tutorium Analysing Time Series Data – Modelling, Forecasting and Data Formatting in R. Wir haben hier ein idealisierten Datensatz vorliegen, so dass wir uns nicht mit dem Datumsformat quälen müssen. Der Datensatz wurde auch für die Analysen künstlich erstellt. Daher ist die Milchleistung auch nicht als echt anzusehen. Wir haben es hier im Prinzip mit simulierten Daten zu tun.\n\nmilk_tbl &lt;- read_csv(\"data/monthly_milk.csv\") \n\nIn der Tabelle 63.1 siehst du nochmal einen Auszug aus den Milchdaten. An jedem Tag haben wir die Milchleistung für eine Kuh aufgetragen. Ich würde hier davon ausgehen, dass es sich um die mittlere Leistung handelt. In Wirklichkeit sind die Daten vermutlich etwas komplizierter und wir haben nicht nur eine Leistungsbewertung pro Tag für eine Kuh. Aber für diese Übersicht soll es reichen.\n\n\n\n\nTabelle 63.1— Auszug aus den Daten zu der Milchleistung von Kühen.\n\n\n\n\n\n\nmonth\nmilk_prod_per_cow_kg\n\n\n\n\n1962-01-01\n265.05\n\n\n1962-02-01\n252.45\n\n\n1962-03-01\n288\n\n\n1962-04-01\n295.2\n\n\n…\n…\n\n\n1975-09-01\n367.65\n\n\n1975-10-01\n372.15\n\n\n1975-11-01\n358.65\n\n\n1975-12-01\n379.35\n\n\n\n\n\n\n\n\nUm die Milchdaten in der Abbildung 63.4 darzustellen nutzen wir die Funktion plot_time_series() aus dem R Paket {timetk}. Eigentlich ist es ein Zusammenschluss von {ggplot} und {plotly}. Wenn du die Option .interactive = TRUE wie ich setzt, dann bekommst du einen semi-interaktiven Plot durch {plotly} wiedergegeben. Mehr Informationen erhälst du dann auf der Hilfeseite von timetk zu Visualizing Time Series. Wie immer wenn du so generische Funktionen nutzt, musst du schauen, ob dir die Abbildung so gefällt. Du verlierst hier etwas Flexibilität und erhälst dafür aber schneller deine Abbildungen.\nWir erkennen ganz gut, dass wir hier einen Effekt der Saison oder aber der Jahreszeit haben. Wir haben zyklische Peaks der Milchleistung über das Jahr verteilt. Gegen Ende unserer Zeitreihe sehen wir aber eine Art Plateau der Milchleistung. In der folgenden Analyse wollen wir einmal schauen, ob wir die zukünftige Milchleistung anhand der bisherigen Daten vorhersagen können.\n\nmilk_tbl |&gt;\n  plot_time_series(month, milk_prod_per_cow_kg, .interactive = TRUE)\n\n\n\n\n\n\n\nAbbildung 63.4— Die Darstellung der Milchdaten durch das R Paket {timetk} und der Funktion plot_time_series(), die durch die Option .interactive = TRUE intern dann {plotly} aufruft.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-time-series.html#analysen-von-zeitreihen",
    "href": "time-space-time-series.html#analysen-von-zeitreihen",
    "title": "63  Zeitreihen",
    "section": "63.3 Analysen von Zeitreihen",
    "text": "63.3 Analysen von Zeitreihen\nBis jetzt haben wir uns in dem vorherigen Kapitel die Visualisierung von Zeitreihen angeschaut. Häufig reicht die Visualisierung auch aus, wenn es um die Darstellung von Temperaturverläufen in einer Abschlussarbeit geht. Wir gehen jetzt aber einen Schritt weiter und wollen unsere Zeitreihe einmal statistisch Analysieren. Die Zeitreihenanalyse oder auch Zeitreihenprognose ist die Verwendung eines Modells zur Vorhersage künftiger Werte auf der Grundlage zuvor beobachteter Werte. Für dich bedeutet dies, dass Prognosen darüber erstellt werden, wie sich solche Zeitreihen in der Zukunft entwickeln werden. Wie in vielen anderen Tutorien auch, werde ich mir hier mal das häufigste Modell mit dem ARIMA Modell anschauen. Das ARIMA Modell ist eine Kombination aus der Auto-Regression (AR) und gleitendem Durchschnitt (MA). Neben dem ARIMA Modell gibt es noch weitere Möglichkeiten eine Zeitreihe zu analysieren. Ich gehe zwar auf einige Aspekte ein, aber eine bessere Übersicht als Michael Clark in seinem Artikel Exploring Time - Multiple avenues to time-series analysis schaffe ich hier nicht. Es gibt dann auch zu viele R Pakete aus denen du wählen könntest.\n\n63.3.1 Definitionen und Überblick\nIn dem folgenden Abschnitt möchte ich gerne einmal einen Überblick über die wichtigsten Begriffe in der Analyse von Zeitreihen geben. Jetzt geht es also los mit der “richtigen” Zeitreihenanalyse. Teilweise sind es etwas speziellere Begriffe, so dass ich hier erstmal etwas zu den Begriffen schreibe und dann die einzelnen Begriffe nochmal tiefer erkläre. Als vertiefenden Einstieg kann ich hier auch das Buch Forecasting: principles and practice von Hyndman und Athanasopoulos (2018) empfehlen. Das Buch ist dann gleichzeitig als eine Webseite hinterlegt. Darüber hinaus gibt es dann noch passende erklärende Videos dazu.\n\nStationarität (eng. stationarity)\n\nEine gängige Annahme bei vielen Zeitreihenverfahren ist, dass die Daten stationär sind. Das klingt etwas seltsam, denn eigentlich soll sich doch was über die Zeit verändern. Wie kann dann eine Vorbedingung an Zeitreihen sein, dass Zeitreihen stationär sind? Ein stationärer Prozess hat die Eigenschaft, dass sich der Mittelwert, die Varianz und die Autokorrelationsstruktur im Laufe der Zeit nicht ändern. Wir sprechen hier also von statistischen Eigenschaften über den Zeitverlauf. Solange die Zeitreihe nicht stationär ist, können wir kein Zeitreihenmodell erstellen. Insbesondere sind AR oder MA nicht auf nicht-stationäre Reihen anwendbar. Es gibt drei grundlegende Kriterien, damit eine Reihe als stationäre Reihe eingestuft wird:\n\n\n\nDer Mittelwert der Reihe darf keine Funktion der Zeit sein, sondern muss konstant sein. Zeitpunkte in der Zukunft haben keine größeren mittleren Werte.\nDie Varianz der Reihe sollte keine Funktion der Zeit sein. Diese Eigenschaft wird als Homoskedastizität bezeichnet. Die Varianz steigt nicht mit der Zeit an und damit ist die Varianz homogen über die Zeitpunkte.\nDie Kovarianz des \\(i\\)-ten Messwerts und des (\\(i + m\\))-ten Messwert sollte keine Funktion der Zeit sein. Die Zeitpunkte untereinander zeigen nur eine eingeschränkte Korrelation. Das heißt, dass unsere Zeitpunkte zwar korreliert sein dürfen, aber der Effekt darf nicht zu stark sein.\n\nEs gibt verschiedene Methoden um aus Daten, die keine stationären Daten sind, einen stationären Datensatz zu erschaffen. Wir transformieren also unsere Daten so, dass es dann stationäre Daten werden. Für die spätere Darstellung können wir dann die Daten wieder zurücktransformieren.\n\nLag (deu. Zeitverzögerung)\n\nMit Lag (deu. Zeitverzögerung, abk. p) ist im Wesentlichen eine Verzögerung gemeint. Das Konzept des Lag ist zentral für das Verständnis der Zeitreihenanalyse. Betrachte dafür eine Folge von Zeitpunkten. Bei einem Lag von 1 vergleichst du die Zeitreihe \\(t\\) mit einer verzögerten Zeitreihe \\(t-1\\). Du verschiebst die Zeitreihe um einen Zeitpunkt, bevor du die Zeitreihe mit sich selbst vergleichst. So gehst du dann für gesamte Länge der Zeitreihe vor. Machen wir einmal ein Beispiel. Wir haben eine Autokorrelationsfunktion für das Lag 1 vorliegen. Wenn wir also eine Zeitreihe \\(t\\) haben und das Lag 1 berechnen wollen, dann entfernen wir den letzten Zeitpunkt und haben eine \\(t-1\\) gelaggte Zeitreihe. Wenn du das Lag 2 berechnest, dann entfernst du die letzten beiden Zeitpunkte aus den Daten. Wir schreiben anstatt des Lag 1 auch gerne \\(p(1)\\). Das Lag 2 wäre dann durch \\(p(2)\\) dargestellt. Mehr zu Lags mit Beispielen kannst du unter Time Series as Features finden.\n\nDifferenz\n\nDie Differenz (abk. d) zwischen zwei Zeitpunkten in einer Zeitreihe wird auch häufig benötigt um einen stationäre Zeitreihe zu erreichen. Dabei wird häufig auch von der Ordnung (eng. order) der Differenz gesprochen. Die Ordnung gibt aber nur an, die wievielte Differenz wir berechnet haben. Klingt wild, ist aber nichts anders als immer wieder die Differenz zwischen zwei Zahlen zu berechnen. Die Differenz 1. Ordnung \\(d(1)\\) zwischen den Zahlen \\(y = {2, 6, 7, 5}\\) ist \\(d(1) = {4, 1, 2}\\). Die Differenz 2. Ordnung \\(d(2)\\) ist dann nur noch die Differenz in der 1. Ordnung und damit \\(d(2) = {3, 1}\\).\n\nAutokorrelation\n\nWenn wir die Zeitreihen \\(t_1\\) und \\(t_2\\) vorliegen haben, so zeigt die normale Korrelation \\(\\rho(t_1, t_2)\\), wie sehr sich die zwei Zeitreihen ähneln. Die Autokorrelation hingegen beschreibt, wie ähnlich die Zeitreihe \\(t_1\\) oder \\(t_2\\) sich selbst ist. Damit beschreibt die Autokorrelation die inhärente Ähnlichkeit einer Zeitreihe \\(t\\). Anhand der Werte der Autokorrelationsfunktion können wir erkennen, wie stark sie mit sich selbst korreliert. Dafür nutzen wir ein sogenanntes Lag (deu. Verzögerung) um aus einer Zeitreihe \\(t\\) eine zweite Zeitreihe zu bauen. Bei jeder Zeitreihe ist die Korrelation bei einem Lag von 0 perfekt, da man dieselben Werte miteinander vergleicht. Wenn du nun eine Zeitreihe verschiebst, wirst du feststellen, dass die Korrelationswerte zwischen den Lags abnehmen. Wenn die Zeitreihe aus völlig zufälligen Werten besteht, gibt es nur eine Korrelation bei Lag gleich 0, aber keine Korrelation überall sonst. Bei den meisten Zeitreihen ist dies nicht der Fall, da die Werte im Laufe der Zeit abnehmen und somit eine gewisse Korrelation bei niedrigen Lag-Werten besteht. Damit kann die Autokorrelationsfunktion die Frequenzkomponenten einer Zeitreihe aufzeigen.\n\nAR Modell\n\nDas AR-Modell (autoregressive model) setzt den aktuellen Wert der Reihe in Beziehung zu ihren vergangenen Werten. Es geht davon aus, dass die Vergangenheitswerte in einem linearen Verhältnis zum aktuellen Wert stehen. Deshalb brauchen wir auch eine Art stationären Zeitverlauf.\n\nMA Modell\n\nDas MA-Modell (moving average model) setzt den aktuellen Wert der Reihe mit dem weißen Rauschen oder den Fehlertermen der Vergangenheit in Beziehung. Es erfasst die Schocks oder unerwarteten Ereignisse der Vergangenheit, die sich noch immer auf die Reihe auswirken. Wir betrachten hier also hier den Fehler aus einer Regression und nicht die tatsächlichen Werte.\n\nARIMA Modell\n\nWenn wir die beiden Modelle AR und MA kombinieren erhalten wir das ARIMA Modell (abk. autoregressive integrated moving average, deu. autoregressiver gleitender Durchschnitt). Das ARIMA Modell ist dabei eine Erweiterung schon existierender Modelle und wird sehr häufig für die Analyse von Zeitreihen genutzt. Als wichtigste Anwendung gilt die kurzfristige Vorhersage. Das ARIMA Modell besitzt einen autoregressiven Teil (AR-Modell) und einen gleitenden Mittelwertbeitrag (MA-Modell). Das ARIMA Modell erfordern eigentlich stationäre Zeitreihen. Eine stationäre Zeitreihe bedeutet, dass sich die Randbedingungen einer Zeitreihe nicht verändern. Die zugrunde liegende Verteilungsfunktion der gemessenen Werte über die Zeitreihe muss zeitlich konstant sein. Das heißt konkret, dass die Mittelwerte und die Varianz zu jeder Zeit gleich sind. Gewisse Trends lassen sich durch ein ARIMA Modell herausfiltern.\n\nExponential smoothing\n\nAlternative zu ARIMA Modell gibt es noch das Exponential smoothing (abk. ets) was wir uns hier aber nicht mehr tiefer anschauen. Später nehmen wir das Exponential smoothing nochmal mit in den Modellvergleich gehen aber nicht auf die Theorie dahin ein.\n\nCross Correlation Functions and Lagged Regressions\n\nCross Correlation Functions and Lagged Regressions beschreibt die Korrelation zwischen verschiedenen Zeitreihen. Dabei vergleichen wir aber nicht die Zeitreihen an einem Zeitpunkt, sondern wollen wissen ob die eine Zeitreihe eine verschobene Zeitreihe der anderen ist. Auch hier gehe ich vorerst nicht auf die Analyse ein.\n\n\n\n\n63.3.2 Grundlagen der Modellierung von Zeitreihen\nIm Folgenden wollen wir einmal die Grundlagen der Zeitreihenanalyse verstehen. Wir brauchen nämlich als erstes stationäre Daten, wenn wir mit einem ARIMA-Modell unsere Daten analysieren wollen. Dann arbeiten wir uns zu der Autokorrelation bzw. dem autoregressiven Prozess (abk. AR-Prozess) vor. Wir brauchen die Idee des Lag und der Differenz um eine Autokorrelation und daraus dann auch einen AR-Prozess ableiten zu können. Zum Verständnis des Lag nutzen wir einmal unsere Daten für die Ausbrüchen des Old Faithful Geysirs im Yellowstone-Nationalpark.\nFangen wir also mit dem Beispiel für ein Lag an. Was ist nun also ein Lag? Ein Lag ist einfach nur eine Verschiebung der Daten um einen Wert. Wir schauen uns also für das Lag 1 die Werte ohne den letzten Wert an. Bei dem Lag 2 löschen wir die ersten letzten beiden Werte. Und dann ist schon fast klar was wir bei dem Lag 3 machen, wir löschen die letzten drei Werte. Wenn wir dann dennoch die paarweise Korrelation berechnen, dann berechnen wir nicht die Korrelation mit sich selber, das wäre bei Lag 0, sondern eben die Korrelation mit sich selbst verschoben um den Lag. Deshalb nennt sich das dann auch Autokorrelation der Zeitreihe.\nDann berechnen wir einmal mit der Funktion lag aus dem R Paket {dplyr} die Lags 1, 2, und 3. Alle Lags packen wir dann mit den originalen Daten zusammen in einen Datensatz und haben somit auch gleich einen Vergleich.\n\nerupt_tbl &lt;- tibble(erupt, \n                    erupt_p1 = dplyr::lag(erupt, 1),\n                    erupt_p2 = dplyr::lag(erupt, 2),\n                    erupt_p3 = dplyr::lag(erupt, 3))\nerupt_tbl\n\n# A tibble: 107 × 4\n   erupt erupt_p1 erupt_p2 erupt_p3\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1    78       NA       NA       NA\n 2    74       78       NA       NA\n 3    68       74       78       NA\n 4    76       68       74       78\n 5    80       76       68       74\n 6    84       80       76       68\n 7    50       84       80       76\n 8    93       50       84       80\n 9    55       93       50       84\n10    76       55       93       50\n# ℹ 97 more rows\n\n\nWie du siehst, haben wir immer eine Verschiebung um einen Wert dadurch, dass wir die letzten Werte entfernen. Daher können wir jetzt trotzdem eine lineare Regression auf die verschiedenen Lags und den originalen Daten rechnen. Dabei fliegen natürlich alle Zeilen aus den Datensatz wo wir einen fehlenden Wert haben, aber das ist ja dann auch gewünscht. In den folgenden Tabs kannst du einmal sehen, wie ich für verschiedene Lags die lineare Regression gerechnet habe. Nichts anderes ist dann auch ein AR-Modell. Wir berechnen über die lineare Regression die Korrelation für jedes Lag zu den originalen Daten.\n\nLag 1 oder \\(p(1)\\)Lag 2 oder \\(p(2)\\)Lag 3 oder \\(p(3)\\)\n\n\n\nlm(erupt_p1 ~ erupt, erupt_tbl) |&gt; \n  coef()\n\n(Intercept)       erupt \n119.4877108  -0.6846253 \n\n\n\n\n\nlm(erupt_p2 ~ erupt, erupt_tbl) |&gt; \n  coef()\n\n(Intercept)       erupt \n 31.8097211   0.5504338 \n\n\n\n\n\nlm(erupt_p3 ~ erupt, erupt_tbl) |&gt; \n  coef()\n\n(Intercept)       erupt \n101.2796080  -0.4264714 \n\n\n\n\n\nIn der Abbildung 63.5 siehst du nochmal die verschobenen Daten auf der \\(y\\)-Achse und die originalen Daten auf der \\(x\\)-Achse. Durch die Verschiebung ändert sich immer die Punktewolke und damit dann auch die Regression sowie die berechnete Korrelation als Vorzeichen der Steigung der Geraden. Die Idee ist eben, einen Zyklus in den Eruptionen zu finden. Wenn wir also immer kurz/lang Wartezeiten hätten, dann würde sich auch immer die Korrelation ändern, wenn wir um einen Wert verschieben. Das trifft natürlich nur zu, wenn es eben der Rhythmus um einen Zeitpunkt verschoben ist. Häufig ist aber nicht ein Zeitpunkt sondern eben zwei oder mehr. Das müssen wir dann durch eine Modellierung herausfinden.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) p(1) vs. Orginal Eruptionszeiten.\n\n\n\n\n\n\n\n\n\n\n\n(b) p(2) vs. Orginal Eruptionszeiten.\n\n\n\n\n\n\n\n\n\n\n\n(c) p(3) vs. Orginal Eruptionszeiten.\n\n\n\n\n\n\n\nAbbildung 63.5— Dastellung der orginalen Eruptionszeiten des Old Faithful Geysirs sowie die entsprechenden Daten für die Lag 1 bis 3. Die Regressiongleichung wurde für alle Zusammenhänge ergänzt.\n\n\n\n\nBevor wir aber mit der Modellierung von Zeitreihen beginnen, hier nochmal die vollständige Korrelationsmatrix für alle Lags \\(p\\) und den originalen Daten. Ich habe den Zusammenhang einmal in der Abbildung 63.6 dargestellt. Wenn du mehr über die Visualisierung der Korrelation erfahren möchtest, dann besuche das Kapitel zur Korrelation. Hier sehen wir dann gut, wie mit jedem Schritt im Lag die Korrelation im Vorzeichen flippt. Ein Zeichen, dass wir es hier mit einer zyklischen Zeitreihe zu tun haben und wir etwas in den Daten entdecken könnten.\n\n\n\n\n\n\n\n\nAbbildung 63.6— Korrelation zwischen den originalen Daten und den Lags, auch als \\(p\\) bezeichnet. Im unteren Bereich des Korrelationsplot sind die Scatterplost mit der Regressionsgraden eingezeichnet. Im oberen Bereich finden sich die berechneten Korrelationskoeffizienten \\(\\rho\\) für die paarweisen Vergleiche.\n\n\n\n\n\nDas war jetzt die händische Darstellung. Wir können uns die Autokorrelation auch über eine Zeitreihe anschauen. Wir nutzen hier die etwas primitive Funktion acf(), da nur die “alten” Funktionen nur mit einem Vektor von Zeiten klar kommen. Eigentlich brauchen wir ja zu jedem Wert ein Datum. Das haben wir hier aber nicht, deshalb müssen wir hier zu der alten Implementierung greifen.\n\nerupt_ts &lt;- tk_ts(erupt)\n\nIn der Abbildung Abbildung 63.7 (a) sehen wir die Autokorrelation zwischen den Orginaldaten des Geysirs und den entsprechenden Korrelationen zu den Lags 1 bis 5. Wenn du nochmal weiter oben schaust, dann haben wir für die Korrelationen von den Orginaldaten zu den entsprechenden Lags folgende Korrelationen berechnet. Wir hatten eine Korrelation von \\(\\rho = -0.68\\) zu Lag 1, eine Korrelation von \\(\\rho = 0.55\\) zu Lag 2 und ein \\(\\rho = -0.43\\) zum Lag 3 beobachtet. Die Korrelationen findest du dann als Striche auch in der Abbildung wieder.\nNun ist es aber so, dass natürlich die Lags untereinander auch korreliert sind. Diese Korrelation untereinander wollen wir dann einmal raus rechnen, so dass wir nur die partielle Korrelation haben, die zu den jeweiligen Lags gehört. Dabei entsteht natürlich eine Ordnung. Das Lag 1 wird vermutlich die meiste Korrelation erklären und dann folgen die anderen Lags. Deshalb nennen wir diese Art der Korrelation auch partial Autokorrelation (deu. partielle). Du siehst die Anteile der partiellen Korrelation zu den jeweiligen Lags dann in der Abbildung 63.7 (b). Wir werden dann später bei {tktime} eine bessere Art und Weise sehen die Abbildungen zu erstellen. Zum Verstehen sind die Abbildungen gut, aber schön sind die Abbildungen nicht.\n\nerupt_ts |&gt; \n  acf(main = \"Correlogram\", lag.max = 5, \n      ylim = c(-1, 1), xlim = c(1, 5))\n\nerupt_ts |&gt; \n  pacf(main = \"Partial Correlogram\", lag.max = 5, \n       ylim = c(-1, 1))\n\n\n\n\n\n\n\n\n\n\n\n(a) Correlogram.\n\n\n\n\n\n\n\n\n\n\n\n(b) Partial Correlogram.\n\n\n\n\n\n\n\nAbbildung 63.7— Korrelationsabbildungen mit der Option lag.max = 5. Daher werden nur die ersten fünf Lags betrachtet. Die Abbildungen dienen der Veranschaulichung vom Lag. Für eine Veröffentlichung bitte die Funktionen aus {tktime} verwenden. Beim Correlogram ist das Lag 0 entfernt, da die Korrelation mit sich selbst immer 1 ist.\n\n\n\n\nAls zweites schauen wir uns die Differenz einer Zeitreihe an um einen stationäre Zeitreihe zu erhalten. Wir berechnen hierbei die Differenz der Zeitpunkte untereinander. Das klingt jetzt wieder komplizierter als es eigentlich ist. Wir können die Berechnung der Differenzen einmal in den folgenden Tabs durchführen. Die Differenz höher Ordnung ist dann einfach die Differenz der vorherigen Differenz.\n\nOrginalDifferenz 1Differenz 2Differenz 3\n\n\n\nerupt[1:10]\n\n [1] 78 74 68 76 80 84 50 93 55 76\n\n\n\n\n\ndiff(erupt[1:10], differences = 1)\n\n[1]  -4  -6   8   4   4 -34  43 -38  21\n\n\n\n\n\ndiff(erupt[1:10], differences = 2)\n\n[1]  -2  14  -4   0 -38  77 -81  59\n\n\n\n\n\ndiff(erupt[1:10], differences = 3)\n\n[1]   16  -18    4  -38  115 -158  140\n\n\n\n\n\nDann können wir einmal die Differenzen für die Ausbrüche des Old Faithful Geysirs im Yellowstone-Nationalpark berechnen. Ich beschränke mich hier einmal auf die ersten drei Differenzen. Auch dafür können wir dann die Funktion diff() nutzen und müssen immer nur noch ein NA am Anfang hinzufügen damit die Vektoren die gleiche Länge behalten. Bei jeder Differenzenbildung verlieren wir ja einen Wert aus dem Vektor.\n\ndiff_tbl &lt;- tibble(erupt,\n                   erupt_d1 = c(NA, diff(erupt, 1)),\n                   erupt_d2 = c(NA, NA, diff(erupt, 2)),\n                   erupt_d3 = c(NA, NA, NA, diff(erupt, 3)))\n\nIn der Abbildung 63.8 siehst du einmal die verschiedenen Differenzen und deren Korrelationen abgebildet. Wir erreichen dann mit einer Differenz \\(d(3)\\) schon eine fast stationäre Zeitreihe.\n\n\n\n\n\n\n\n\nAbbildung 63.8— Korrelation zwischen den originalen Daten und den Lags. Im unteren Bereich des Korrelationsplot sind die Scatterplost mit der Regressionsgraden eingezeichnet. Im oberen Bereich finden sich die berechneten Korrelationskoeffizienten \\(\\rho\\) für die paarweisen Vergleiche. Wir erreichen immer stationärere Daten.\n\n\n\n\n\nDann wenden wir die Differenzen auch einmal auf unsere Klimadaten sowie unserer Milchleistung einmal an. Wir nutzen dann hier die Funktion plot_time_series() aus dem R Paket {timetk}. Die Funktion ist super einfach zu nutzen uns liefert auch die Ergebnisse, die wir wollen. Darüber hinaus können wir dann auch direkt {plotly} aktivieren. Wie wir dann in der Abbildung 63.9 sehen, kommen wir mir der Differenz \\(d(1)\\) schon sehr nah an einen stationären verlauf der Zeitreihe ran. Später müssen wir das dann nicht händisch machen, es gibt Funktionen, die für uns das beste Lag und die optimale Differenz bestimmen. Dafür nutzen wir zum Beispiel dann die Funktion auto.arima(), die uns dann die optimale Ordung für das Lag und die Differenz wiedergibt.\n\nco2_monthly_tbl |&gt; \n  mutate(diff = c(NA, diff(average, 1))) |&gt; \n  plot_time_series(date, diff, .interactive = TRUE) \nmilk_tbl |&gt; \n  mutate(diff = c(NA, diff(milk_prod_per_cow_kg, 1))) |&gt; \n  plot_time_series(month, diff, .interactive = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) CO\\(_2\\) Daten.\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Milch Daten.\n\n\n\n\n\n\n\nAbbildung 63.9— Stationäre Abbildungen der CO\\(_2\\)-Daten sowie der Milchdaten nach der Differenzbildung mit \\(d(1)\\). Wir sehen, dass wir einen stationären Verlauf erreichen.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-time-series.html#vorhersagen-von-zeitreihen",
    "href": "time-space-time-series.html#vorhersagen-von-zeitreihen",
    "title": "63  Zeitreihen",
    "section": "63.4 Vorhersagen von Zeitreihen",
    "text": "63.4 Vorhersagen von Zeitreihen\nImmer wenn ich mit Zeitreihen anfange, muss ich mich erinnern, dass die Algorithmen immer einer Vorhersage treffen wollen. Das heißt, wir wollen wissen wie sich eine Zeitreihe gegeben der vorherigen Ereignisse in der Zukunft verändern wird. Wir haben eine weitreichende Auswahl an R Paketen und jedes löst das Problem der Vorhersage von Zeitreihen etwas anders. Damit ich hier nicht endlos immer das Gleiche darstelle, konzentriere ich mich einmal auf die Standardimplementierung mit auto.arima() sowie dem R Paket {modeltime}. Am Ende sei hier nochmal auf die Übersicht von Michael Clark in seinem Artikel Exploring Time - Multiple avenues to time-series analysis verwiesen. Wir halten uns aber an seine folgende Empfehlung.\n\n“I would also recommend {modeltime} as starting point for implementing a variety of model approaches for time series data with R. It was still pretty new when this was first written, but has many new features and capabilities, and could do some version of the models shown here.”\n\nIch verweise aber gerne nochmal in dem folgenden Kasten auf die anderen Möglichkeiten in R eine Zeitreihe auszuwerten. Eventuell ist etwas dabei, was für sich besser passt. Man kann sich hier sehr schnell in den Möglichkeiten verlieren.\n\n\n\n\n\n\nWeitere R Paket zur Vorhersage von Zeitreihen\n\n\n\nFolgende R Pakete lösen ebenfalls das Problem einer Vorhersage über die Zeit. Wir immer, ist es meistens eine andere Implementierung. Ob die Implementierung besser ist, hängt dann von vielen Faktoren ab.\n\nDas R Paket {timetk} ist von den gleichen Machern wie das hier vorgestellte R Paket {modeltime}. Ich kann das Pakt empfehlen und es gibt auch keinen Grund es nicht zu nutzen. Ich halte aber {modeltime} für das aktuellere Paket.\nDas R Paket {fable} und das R Paket {feasts} ist die Implementierung des Onlinebuches Forecasting: Principles and Practice. Dementsprechend empfiehlt sich auch diese Pakete zu nutzen, wenn du dich tiefer mit den dortigen Quellen beschäftigst.\nR Paket {tstibble} ist nochmal eine andere Art Zeit in R darzustellen. Ich habe mich ehrlich gesagt nicht weiter mit dem R Paket beschäftigt. Das R Paket {feasts} baut aber auf {tstibble}, so dass ich das Paket hier noch erwähnen wollte.\n\n\n\n\n63.4.1 mit {ts}\nEigentlich ist {ts} kein eigens Paket sondern die built-in Lösung von R, aber ich möchte hier dann doch {ts} als Paket schreiben, damit hier mehr Ordnung drin ist. Wenn wir eine Vorhersage auf einem zeitlichen Verlauf rechnen wollen, dann brauchen wir als aller erstes einen Datensatz, der auch eine echte Zeitreihe über mehrere zeitliche Zyklen enthält. Das ist dann meistens die Herausforderung so eine Zeitreihe in einer Abschlussarbeit zu erzeugen. In ein paar Monaten einen zyklischen Verlauf zu finden ist schon eine echte Leistung. Deshalb nehmen wir hier als Beispiel einmal unsere künstlichen Daten zur Milchleistung von Kühen. Wie du in der obigen Abbildung 63.4 klar erkennen kannst, haben wir hier Zyklen über die einzelnen Jahre hinweg. Es liegt ein stetiger, zyklischer Anstieg der Milchleistung über die beobachteten Jahre vor. Wir wollen jetzt den Verlauf modellieren und einen zukünftigen Verlauf vorhersagen. Oder andersherum, wie die zukünftigen Zyklen aussehen könnten.\nIn diesem Beispiel nutzen wir das R Paket {zoo} und die Funktion ts() für die Standardimplemetierung von Zeitreihen in R. Das hat immer ein paar Nachteile, da wir hier die veralteten Speicherformen für eine Zeitreihe nutzen. Auf der anderen Seite sind viele Tutorien im Internet noch genau auf diese Funktionen ausgerichtet. Hier seien dann die beiden Tutorien Time Series - ARMA Models in R und Time Series Analysis with auto.arima() in R als Anlaufpunkt empfohlen. Deshalb auch hier einmal die Erklärung der etwas älteren Funktionen. Später schauen wir dann auch die neuere Implementierung in dem R Paket {modeltime} einmal an.\nWir wandeln also erstmal unsere Milchdaten mit der Funktion tk_ts() in ein ts-Zeitobjekt um. Dafür müssen wir angeben von wann bis wann die Jahre laufen und wie viele Beobachtungen jedes Jahr hat. Glücklicherweise müssen nicht alle Jahre gefüllt werden und die Funktion erlaubt auch einen anderen Startmonat als Jan. Wir haben nämlich bei den CO\\(_2\\)-Daten den März als Startdatum, deshalb müssen wir den Start etwas anpassen. Wie du gleich siehst, dann haben wir eine Art Matrix als Ausgabe.\n\nMilchdaten als ts()CO\\(_2\\)-Daten als ts()\n\n\nIm Folgenden also einmal die Milchdaten als ein ts-Objekt. Wichtig ist hier zu wissen, wie oft den nun ein Wert gemessen wurde. Problematisch wird es, wenn wir Nesswiederholungen vorliegen haben, dann können wir hier nicht weiterarbeiten.\n\nmilk_ts &lt;- milk_tbl %$%\n  tk_ts(milk_prod_per_cow_kg, start = 1962, end = 1975, frequency = 12)\nmilk_ts\n\n        Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct\n1962 265.05 252.45 288.00 295.20 327.15 313.65 288.00 269.55 255.60 259.65\n1963 270.00 254.70 293.85 302.85 333.90 322.20 297.00 277.65 262.35 264.15\n1964 282.60 278.10 309.60 317.25 346.50 331.20 305.10 287.55 271.80 274.95\n1965 296.10 279.90 319.05 324.90 351.90 340.20 315.90 293.85 276.75 279.45\n1966 304.65 285.75 331.20 339.75 364.95 359.10 330.75 313.65 297.45 300.15\n1967 320.85 300.15 342.90 352.80 376.65 367.65 345.15 324.90 306.45 309.15\n1968 322.65 313.20 348.75 358.20 386.10 371.70 352.35 333.00 315.45 317.70\n1969 330.30 310.50 353.25 362.25 391.95 380.25 360.45 343.80 326.25 325.35\n1970 337.50 318.15 363.15 370.80 398.70 386.55 368.55 352.35 333.00 336.15\n1971 361.80 340.20 387.00 395.10 423.90 410.85 391.05 375.30 355.50 360.00\n1972 371.70 359.55 400.50 405.00 432.45 420.75 402.30 384.75 364.05 364.50\n1973 369.45 347.85 397.35 404.10 430.65 415.80 396.45 376.65 352.80 355.95\n1974 372.60 350.10 400.05 405.90 436.05 426.15 408.60 390.15 366.75 365.40\n1975 375.30                                                               \n        Nov    Dec\n1962 248.85 261.90\n1963 254.25 269.10\n1964 267.30 285.30\n1965 270.90 285.75\n1966 290.25 309.60\n1967 297.00 314.10\n1968 304.65 319.95\n1969 310.50 330.30\n1970 319.95 337.95\n1971 343.35 360.00\n1972 344.70 362.25\n1973 342.00 360.90\n1974 347.85 365.85\n1975              \n\n\nWir erhalten eine Matrix mit den 12 Monaten als Spalten und in den Zeilen die jeweiligen Jahr. Wenn du nicht im Januar startest, wie bei den CO\\(_2\\)-Daten, dann musst du das explizit angeben.\n\n\nDie CO\\(_2\\)-Daten starten nicht im Januar sondern im März, so dass wir hier nochmal den Startpunkt ändern müssen. Ich zeige dir hier nicht die Ausgabe, weil die Matrix entschieden zu groß ist. Immerhin sind es ja fünfundsechzig Zeilen für die gesamte Zeitspanne.\n\nco2_monthly_ts &lt;- co2_monthly_tbl  %$%\n  tk_ts(average, start = c(1958, 3), end = 2023, frequency = 12)\n\n\n\n\nEin ARIMA Modell setzt sich aus, wie oben schon vorgestellt, aus drei Komponenten wie folgt zusammen. Du kennst die Modelle und Konzepte schon aus der Einleitung dieses Abschnitts.\n\nDem verwendeten Lag \\(p\\) der Zeitreihe (eng. autoregressiv model oder AR-Modell), die wir für die Berechnung der Autokorrelation der Zeitreihe nutzen.\nDer verwendeten Differenz \\(d\\), mit der wir unsere Zeitreihe anpassen.\nDer laufenden Durchschnitt \\(q\\) (eng. moving average model oder MA-Modell). Dies ist im Wesentlichen die Größe der “Fenster”-Funktion über die Zeitreihendaten. Ein MA-Prozess ist eine lineare Kombination von Fehlern aus der Vergangenheit oder eben das Modellieren des Rauschens.\n\nNeben diesen Komponenten gibt es auch noch die saisonalen Komponenten, aber das überstiegt hier das Kapitel bei weitem. Insbesondere sei dir hier nochmal die Hilfe unter Tips to using auto_arima() empfohlen. Auf alle Details gehe ich dann hier nicht ein.\nIn den folgenden beiden Tabs findest du einmal die Korrelationsabbildungen für die Milchdaten wir auch die CO\\(_2\\)-Daten. Beachte bitte, dass die \\(x\\)-Achse etwas ungünstig formatiert ist. Die Lags werden nicht als ganzzahlig angezeigt sondern als seltsame Kommazahlen. Bitte schau dir auch die Funktion plot_acf_diagnostics() in der Abbildung 63.16 an, die Funktion ist um Welten besser. Auch hilft hier die Hilfeseite von {timetk} mit Plotting Seasonality and Correlation. Ja, die neueren Pakete können dann wirklich mehr. Insbesondere da wir hort dann auch das Paket {ploty} nutzen.\n\nKorrelationsabbildungen der MilchdatenKorrelationsabbildungen der CO\\(_2\\)-Daten\n\n\n\nmilk_ts |&gt; \n  acf(main = 'Correlogram')\n\nmilk_ts |&gt; \n  pacf(main = 'Partial Correlogram' )\n\n\n\n\n\n\n\n\n\n\n\n(a) Correlogram.\n\n\n\n\n\n\n\n\n\n\n\n(b) Partial Correlogram.\n\n\n\n\n\n\n\nAbbildung 63.10— Korrelationsabbildungen der Lags über die Zeit für die Milchdaten. Wir sehen eine klaren zyklischen Verlauf, die sich über die Zeit ausdehnt. Das macht ja auch Sinn, den die Milchleistung wird auch von den Jahreszeiten abhängen. Bitte bachte, dass Lag 1.0 nicht das erste Lag beschreibt. Bei der partiziellen Korrelation beobachten wir, dass wir ebenfalls einen kurvigen Verlauf vorliegen haben.\n\n\n\n\n\n\n\nco2_monthly_ts |&gt; \n  acf(main = 'Correlogram')\n\nco2_monthly_ts |&gt; \n  pacf(main = 'Partial Correlogram' )\n\n\n\n\n\n\n\n\n\n\n\n(a) Correlogram.\n\n\n\n\n\n\n\n\n\n\n\n(b) Partial Correlogram.\n\n\n\n\n\n\n\nAbbildung 63.11— Korrelationsabbildungen der Lags über die Zeit für die CO\\(_2\\)-Daten. Wir sehen, dass die Korrelation der Werte über die Zeit langsam abnimmt. Bitte bachte, dass Lag 1.0 nicht das erste Lag beschreibt. Dabei ist der erste Wert des Lag immer am stärksten mit seinem Nachbarn korreliert.\n\n\n\n\n\n\n\nMit der Funktion decompose() können wir uns anschauen, wie die Zeitreihe aufgebaut ist. Gibt es einen Trend? Haben wir einen saisonalen Effekt über die Zeit? Und wie sieht dann unser Rauschen aus, wenn wir den Trend und den saisonalen Effekt rausgerechnet haben?\n\ndecomp_milk_obj &lt;- decompose(milk_ts)\ndecomp_co2_obj &lt;- decompose(co2_monthly_ts)\n\nWir wollen uns dann in den beiden Tabs dann einmal die Dekomposition der Zeitreihen anschauen. Wir werden die Ergebnisse jetzt nicht tiefer nutzen. Wir könnten an der Dekomposition unser armia()-Modell optimieren, aber wir sind hier gleich faul und nutzen einen automatischen Algorithmus.\n\nDekomposition der MilchdatenDekomposition der CO\\(_2\\)-Daten\n\n\n\nplot(decomp_milk_obj)\n\n\n\n\n\n\n\nAbbildung 63.12— Dekomposition der Zeitreihen der Milchdaten in dn Trend, den saisonalen Effekt sowie das Rauschen was übrig bleibt. Wir sehen einen klaren Trend in der Milchleitsung mit einem zyklischen Effekt über die Jahre.\n\n\n\n\n\n\n\n\nplot(decomp_co2_obj)\n\n\n\n\n\n\n\nAbbildung 63.13— Dekomposition der Zeitreihen der CO\\(_2\\)-Daten in dn Trend, den saisonalen Effekt sowie das Rauschen was übrig bleibt. Wir sehen einen klaren Trend in des CO\\(_2\\) Gehaltes mit einem zyklischen Effekt über die Jahre.\n\n\n\n\n\n\n\n\nDann können wir auch schon mit der Funktion auto.arima() unser Modell für die Vorhersage der Zeitreihen rechnen. Ich verweise hier nochmal auf das Tutorium Tips to using auto_arima(), wenn du tiefer in die Modellinterpretation einsteigen willst. Wir nutzen gleich einmal das Modell um dann unsere Prognose zu rechnen.\n\nPrognose der Milchdaten mit auto.arima()Prognose der CO\\(_2\\)-Daten mit auto.arima()\n\n\n\nmilk_arima_obj &lt;- auto.arima(milk_ts)\nmilk_arima_obj\n\nSeries: milk_ts \nARIMA(2,0,1)(2,1,1)[12] with drift \n\nCoefficients:\n         ar1     ar2      ma1     sar1     sar2     sma1   drift\n      0.7957  0.1309  -0.0896  -0.1313  -0.1141  -0.4901  0.7465\ns.e.  0.3089  0.2774   0.3015   0.1681   0.1197   0.1563  0.1173\n\nsigma^2 = 11:  log likelihood = -379.43\nAIC=774.86   AICc=775.92   BIC=798.67\n\n\n\n\n\nco2_arima_obj &lt;- auto.arima(co2_monthly_ts)\nco2_arima_obj\n\nSeries: co2_monthly_ts \nARIMA(1,1,1)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1     sar1     sar2     sma1     sma2\n      0.2177  -0.5819  -0.3161  -0.0161  -0.5462  -0.2731\ns.e.  0.0883   0.0740   1.6095   0.0421   1.6094   1.3910\n\nsigma^2 = 0.1001:  log likelihood = -204.22\nAIC=422.43   AICc=422.58   BIC=454.92\n\n\n\n\n\nDie Funktion forecast() erlaubt uns über einen Zeitraum die folgenden Verläufe einer Zeitreihe aus einem ARIMA-Modell vorherzusagen. Ich habe mich hier einmal für zwei Jahre also 24 Monate entschieden. Tendenziell ist natürlich anzuraten nicht so weit in die Zukunft vorherzusagen, aber prinzipiell ist es technisch möglich. Aber Achtung, manchmal macht das Ergebnis biologisch keinen Sinn mehr. Irgendwann ist die Milchleistung von Kühen auch mal gedeckelt und mehr geht dann einfach nicht mehr.\n\nmilk_mdl &lt;- forecast(milk_arima_obj, 24)\nco2_mdl &lt;- forecast(co2_arima_obj, 24)\n\nIn den Folgenden beiden Tabs siehst du dann einmal die Vorhersagen der weiteren zeitlichen Verläufen für die Milchleistung und den CO\\(_2\\) Gehalt in der Luft. Wir nutzen hier die Funktion auto_plot() die es uns direkt erlaubt eine Abbildung zu erstellen. Wir immer kannst du das Objekt natürlich auch mit str() zerlegen und dann deine eigene ggplot()-Abbildung bauen.\n\nVorhersage der MilchdatenVorhersage der CO\\(_2\\)-Daten\n\n\n\nautoplot(milk_mdl) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 63.14— Vorhersage der Milchleistung der Kühe für die nächsten zwei Jahre zusammen mit einem Fehlerbereich.\n\n\n\n\n\n\n\n\nautoplot(co2_mdl) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 63.15— Vorhersage der CO\\(_2\\)-Daten für die nächsten zwei Jahre zusammen mit einem Fehlerbereich.\n\n\n\n\n\n\n\n\n\n\n63.4.2 mit {modeltime}\nAm Ende hier nochmal eine Möglichkeit sehr effizient Zeitreihen mit dem R Paket {modeltime} zu modellieren und eine Vorhersage für zukünftige Verläufe zu machen. Ich empfehle hier die Hilfeseite Getting Started with {modeltime}. Auf der Seite erfährst du dann die Grundlagen für die Anwendung von dem R Paket {modeltime}. Hier kannst du dann tief in den Kaninchenbau reingehen. Wir machen hier nur die einfache Vorhersage ohne viel Schnickschnack. Neben dem R Paket {modeltime} findest du im Ökosystem {modeltime} noch andere spannende R Pakete, die dir weitere und vertiefende Modellierungen zur Vorhersage erlauben. Wenn dir die Begriffe zu der Vorhersage und der Klassifikation etwas komisch vorkommen, dann kannst du in dem Kapitel Grundlagen der Klassifikation reinschauen und nachlesen wo du dann noch Lücken hast. Ich gehe hier dann nicht mehr so tief auf die einzelnen Punkte ein, sondern führe hier eher grob durch den Code.\nWir konzentrieren uns hier jetzt nur auf die Milchdaten. Wenn du willst, kannst du den folgenden Code auch einfach für die CO\\(_2\\)-Daten anpassen und durchführen. Da wir auch eine lineare Regression auf den Daten durchführen wollen, muss ich nochmal das Datum in eine numerische Variable sowie einen Faktor für den Monat aufteilen. Aber ja, du kannst den ganzen Kram auch hier nur mit einer linearen Regression lösen, was auch irgendwie spannend ist, denn so ein lineares Modell ist auch wirklich ein einfaches Modell.\n\nmilk_tbl &lt;- milk_tbl |&gt; \n  mutate(month_fac = factor(month(month, label = TRUE), ordered = FALSE),\n         month_num = as.numeric(month)) \nmilk_tbl |&gt; print(n = 7)\n\n# A tibble: 168 × 4\n  month      milk_prod_per_cow_kg month_fac month_num\n  &lt;date&gt;                    &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n1 1962-01-01                 265. Jan           -2922\n2 1962-02-01                 252. Feb           -2891\n3 1962-03-01                 288  Mar           -2863\n4 1962-04-01                 295. Apr           -2832\n5 1962-05-01                 327. May           -2802\n6 1962-06-01                 314. Jun           -2771\n7 1962-07-01                 288  Jul           -2741\n# ℹ 161 more rows\n\n\nIn der Abbildung 63.16 siehst du die so viel besseren Korrelationsabbildungen der Lags für die Milchdaten. Wir sehen hier erstmal die richtige Bezeichnung auf der \\(x\\)-Achse und habe auch einen sehr weiten Verlauf. Dank {plotly} können wir dann auch die Werte in der Abbildung ablesen und einen Schluss daraus ziehen. Dementsprechend ist diese Art der Abbildung auf jeden Fall den anderen vorzuziehen. Hier ist das Paket {timetk} wirklich super gemacht.\n\nmilk_tbl |&gt; \n  plot_acf_diagnostics(month, milk_prod_per_cow_kg)\n\n\n\n\n\n\n\nAbbildung 63.16— Korrelationsabbildungen der Lags über die Zeit für die Milchdaten in {plotly}. Wir sehen eine klaren zyklischen Verlauf, die sich über die Zeit ausdehnt. Das macht ja auch Sinn, den die Milchleistung wird auch von den Jahreszeiten abhängen. Bei der partiziellen Korrelation beobachten wir, dass wir ebenfalls einen kurvigen Verlauf vorliegen haben, aber der Effekt sich auf die ersten Lags beschränkt.\n\n\n\n\nWenn wir jetzt eine Vorhersage rechnen wollen, dann müssen wir eigentlich unsere Daten in einen Trainings- und Testdatensatz aufteilen. Das machen wir hier mit der Funktion initial_time_split() die uns einen Trainings- und Testdatensatz baut. Wir wollen dann hier 90% der Daten in den Trainingsdaten haben und 10% in den Testdaten.\n\nsplits &lt;- initial_time_split(milk_tbl, prop = 0.9)\n\nIm Folgenden bauen wir uns dann einmal drei Modelle. Zum einen das auto.arima() Modell und lassen darauf unsere Trainsgdaten modellieren.\n\nmodel_fit_arima_no_boost &lt;- arima_reg() |&gt;\n    set_engine(engine = \"auto_arima\") |&gt;\n    fit(milk_prod_per_cow_kg ~ month, data = training(splits))\n\nDamm nutzen wir noch ein anderes Modell mit exponential smoothing auf das wir hier nicht tiefer eingehen wollen.\n\nmodel_fit_ets &lt;- exp_smoothing() |&gt;\n    set_engine(engine = \"ets\") |&gt;\n    fit(milk_prod_per_cow_kg ~ month, data = training(splits))\n\nAbschließen nutze ich noch ein lineares Modell und modelliere den Trend mit month_num und den Effekt der Saison mit month_fac. Schauen wir mal wie gut das Modell so ist.\n\nmodel_fit_lm &lt;- linear_reg() |&gt;\n    set_engine(\"lm\") |&gt;\n    fit(milk_prod_per_cow_kg ~ month_num + month_fac,\n        data = training(splits))\n\nDann packe ich alle Modelle einmal in einen Datensatz zusammen.\n\nmodels_tbl &lt;- modeltime_table(\n    model_fit_arima_no_boost,\n    model_fit_ets,\n    model_fit_lm\n)\n\nIch nutze nun die Funktion modeltime_calibrate() um auf die Modelle aus den Trainingsdaten einmal die Testdaten anzuwenden.\n\ncalibration_tbl &lt;- models_tbl |&gt;\n    modeltime_calibrate(new_data = testing(splits))\n\nAbschließend können wir uns in der Abbildung 63.17 einmal die Vorhersage aus den drei Modellen anschauen. Wir stellen mit erstaune fest, dass die lineare Regression gar nicht so schlecht abschneidet. Hier hilft dann vor allem einmal das Reinzoomen mit {plotly} sehr um sich die Unterschiede nochmal genauer anzuschauen. Am Ende packe ich noch meine Legende unter die Abbildung, damit die Abbildung nicht so gestaucht wird.\n\ncalibration_tbl |&gt;\n    modeltime_forecast(\n        new_data = testing(splits),\n        actual_data = milk_tbl\n    ) |&gt;\n    plot_modeltime_forecast(\n      .legend_max_width = 25, \n      .interactive = TRUE) |&gt;\n  plotly::layout(legend = list(orientation = \"h\",   \n                               xanchor = \"center\",  \n                               x = 0.5)) \n\n\n\n\n\n\n\nAbbildung 63.17— Vorhersageplot der drei Modelle für die Milchdaten dargestellt mit {plotly}.\n\n\n\n\nDann können wir uns in der Tabelle 63.2 nochmal die Gütezahlen ausgeben lassen, die wir aus der Klassifikation schon kennen. Ich habe dir dann gleich nochmal die Übersetzungen der Maßzahlen unter die Tabelle gepackt. Am Ende können wir uns hier verschiedene Maßzahlen anschauen und entscheiden welches Modell das beste Modell ist. Ich finde hier am spannendsten, dass unsere einfache lineare Regression gar nicht mal so schlecht abgeschnitten hat und ähnlich gute Fehlerwerte und ein Bestimmtheitsmaß hat, wie die anderen beiden Modelle, die um einiges komplizierter sind.\n\ncalibration_tbl |&gt;\n    modeltime_accuracy() |&gt;\n    table_modeltime_accuracy(\n      .interactive = FALSE\n    )\n\n\n\nTabelle 63.2— Gütezahlen für die Vorhersage der Testdaten auf den Modellen der Trainiungsdaten. Verschiedene Maßzahlen wie Fehler und das Bestimmtheitsmaß erlauben eine Bewertung der Modelle.\n\n\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nARIMA(2,0,1)(2,1,1)[12] WITH DRIFT\nTest\n12.03\n3.13\n0.68\n3.08\n13.02\n0.97\n\n\n2\nETS(A,A,A)\nTest\n11.72\n3.06\n0.67\n3.01\n12.59\n0.96\n\n\n3\nLM\nTest\n15.51\n4.06\n0.88\n3.97\n16.30\n0.96\n\n\n\n\n\n\n\n\n\n\n\nHier nochmal die Übersetzung der Maßzahlen und die Links zu den Hilfeseiten der entsprechenden Funktionen. Fehler sollten immer klein sein und somit ist ein kleiner Fehler bei einem Modell ein gutes Zeichen. Deshalb ist zum Beispiel das ETS Modell in unserem Fall zu bevorzugen, denn das Modell hat den geringsten Fehler über alle Fehlerarten.\n\nMAE - Mean absolute error, mae()\nMAPE - Mean absolute percentage error, mape()\nMASE - Mean absolute scaled error, mase()\nSMAPE - Symmetric mean absolute percentage error, smape()\nRMSE - Root mean squared error, rmse()\nRSQ - R-squared, rsq()\n\nWar es das schon? Nein, natürlich nicht. Das R Paket {modeltime} liefert noch mehr Modelle um deine Zeitreihe zu modellieren. Ebenso haben wir kein Refit durchgeführt, also die neuen Daten nochmal genutzt um das Modell noch besser zu machen. Darüber hinaus gibt es noch mehr Diagnosemöglichkeiten und Darstellungsformen. Aber das sprengt hier den Rahmen und ich entscheide mal, dass dieses Kapitel jetzt gut genug ist.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-time-series.html#referenzen",
    "href": "time-space-time-series.html#referenzen",
    "title": "63  Zeitreihen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 63.1— Eine Zeitreihe, wie sie in der klassischen Analyse von Zeitreihen erwartet wird. Wir haben über einen längeren Zeitraum ein zyklisches Verhalten von Messwerten vorliegen. Nicht immer liegt so eine Form der Zeitreihe vor. Häufig ist der Zeitraum zu kurz gewählt.\nAbbildung 63.2— Dauer zwischen zwei Eruptionen des Old Faithful Geysirs. Eine klare zyklische Zeitreihe lässt sich aus der Abbildung ableiten.\nAbbildung 63.5 (a)— p(1) vs. Orginal Eruptionszeiten.\nAbbildung 63.5 (b)— p(2) vs. Orginal Eruptionszeiten.\nAbbildung 63.5 (c)— p(3) vs. Orginal Eruptionszeiten.\nAbbildung 63.6— Korrelation zwischen den originalen Daten und den Lags, auch als \\(p\\) bezeichnet. Im unteren Bereich des Korrelationsplot sind die Scatterplost mit der Regressionsgraden eingezeichnet. Im oberen Bereich finden sich die berechneten Korrelationskoeffizienten \\(\\rho\\) für die paarweisen Vergleiche.\nAbbildung 63.7 (a)— Correlogram.\nAbbildung 63.7 (b)— Partial Correlogram.\nAbbildung 63.8— Korrelation zwischen den originalen Daten und den Lags. Im unteren Bereich des Korrelationsplot sind die Scatterplost mit der Regressionsgraden eingezeichnet. Im oberen Bereich finden sich die berechneten Korrelationskoeffizienten \\(\\rho\\) für die paarweisen Vergleiche. Wir erreichen immer stationärere Daten.\nAbbildung 63.10 (a)— Correlogram.\nAbbildung 63.10 (b)— Partial Correlogram.\nAbbildung 63.11 (a)— Correlogram.\nAbbildung 63.11 (b)— Partial Correlogram.\nAbbildung 63.12— Dekomposition der Zeitreihen der Milchdaten in dn Trend, den saisonalen Effekt sowie das Rauschen was übrig bleibt. Wir sehen einen klaren Trend in der Milchleitsung mit einem zyklischen Effekt über die Jahre.\nAbbildung 63.13— Dekomposition der Zeitreihen der CO\\(_2\\)-Daten in dn Trend, den saisonalen Effekt sowie das Rauschen was übrig bleibt. Wir sehen einen klaren Trend in des CO\\(_2\\) Gehaltes mit einem zyklischen Effekt über die Jahre.\nAbbildung 63.14— Vorhersage der Milchleistung der Kühe für die nächsten zwei Jahre zusammen mit einem Fehlerbereich.\nAbbildung 63.15— Vorhersage der CO\\(_2\\)-Daten für die nächsten zwei Jahre zusammen mit einem Fehlerbereich.\n\n\n\nChan K-S, Cryer JD. 2008. Time series analysis with applications in R. Springer.\n\n\nCowpertwait PS, Metcalfe AV. 2009. Introductory time series with R. Springer Science & Business Media.\n\n\nHyndman RJ, Athanasopoulos G. 2018. Forecasting: principles and practice. OTexts.\n\n\nRobert H, others. 2006. Time Series Analysis and Its Applications With R Examples Second Edition.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Zeitreihen</span>"
    ]
  },
  {
    "objectID": "time-space-spatial-data.html",
    "href": "time-space-spatial-data.html",
    "title": "64  Räumliche Daten",
    "section": "",
    "text": "64.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, magrittr, \n               conflicted)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Räumliche Daten</span>"
    ]
  },
  {
    "objectID": "time-space-spatial-data.html#daten",
    "href": "time-space-spatial-data.html#daten",
    "title": "64  Räumliche Daten",
    "section": "64.2 Daten",
    "text": "64.2 Daten\nWenn wir bei räumlichen Daten von Daten sprechen, dann kommen wir an GIS-Daten nicht vorbei. Wenn du nicht weißt, was das geographic information system (abk. GIS) ist, dann empfehle ich als Einstieg What is GIS. Traditionelle GIS-Software wie ArCGIS und QGIS sind fantastische Programme, die eine grafische Benutzeroberfläche verwenden, um auf ihre Funktionen zugreifen zu können. Siehe auch A Crash Course in Geographic Information Systems (GIS) using R.\nWenn es um Daten geht, dann gibt es natürlich eine Reihe von möglichen Quellen. Wenn es sehr viele Daten seinen sollen, die meistens einen örtlichen Bezug haben, dann empfehle ich die Webseite Our World in Data. Dort gibt es so viele räumliche Daten, da eigentlich alles dort als räumlich dargestellt wird. es ist ja schließlich auch unsere Welt als Daten. Dann gibt es noch die Quelle Free GIS Data Sources: Best Global Raster and Vector Datasets. Hier findest du dann auch nochmal Inspiration für mögliche Datensätze für diene Fragestellung.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Räumliche Daten</span>"
    ]
  },
  {
    "objectID": "time-space-spatial-data.html#visualisierung",
    "href": "time-space-spatial-data.html#visualisierung",
    "title": "64  Räumliche Daten",
    "section": "64.3 Visualisierung",
    "text": "64.3 Visualisierung\n\n\n\n\n\n\nInspirationen von The R Graph Gallery\n\n\n\nWenn du noch Inspirationen suchst, wie du deine räumlichen Daten noch schöner darstellen könntest, dann besuche doch The R Graph Gallery - Spatial. Dort findest du verschiedene Darstellungen von räumlichen Daten mit {ggplot}. Lasse dich einfach mal inspirieren.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Räumliche Daten</span>"
    ]
  },
  {
    "objectID": "time-space-spatial-data.html#modellierung",
    "href": "time-space-spatial-data.html#modellierung",
    "title": "64  Räumliche Daten",
    "section": "64.4 Modellierung",
    "text": "64.4 Modellierung\nR Paket {ggspatial}\nR Paket {rspatial}\nR Paket {spatstat.random}\nR Paket {spatstat}",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Räumliche Daten</span>"
    ]
  },
  {
    "objectID": "time-space-spatial-data.html#referenzen",
    "href": "time-space-spatial-data.html#referenzen",
    "title": "64  Räumliche Daten",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nBivand RS, Pebesma EJ, Gómez-Rubio V, Pebesma EJ. 2008. Applied spatial data analysis with R. Springer.\n\n\nPlant RE. 2018. Spatial data analysis in ecology and agriculture using R. cRc Press.",
    "crumbs": [
      "Zeitliche und räumliche Analysen",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Räumliche Daten</span>"
    ]
  },
  {
    "objectID": "classification-preface.html",
    "href": "classification-preface.html",
    "title": "Klassifikation oder maschinelles Lernen",
    "section": "",
    "text": "Ausgewählte Algorithmen\nNachdem wir also wissen, dass wir Werte in einer Spalte vorhersagen wollen, können wir uns verschiedene Algorithmen einmal anschauen. Ich kann Mueller und Massaron (2021) als einen Einstieg ins maschinelle Lernen empfehlen.\nWas fehlt noch? Sicherlich fehlen noch andere Algorithmen. Aber das ist auch nicht der Sinn dieses Abschnitts eine umfassende Übersicht über alle Algorithmen des maschinellen Lernens zu geben. Wir wollen uns aber hier auf die großen und meist angewandten Algorithmen beschränken. Vielleicht ergänze ich dann nochmal ein Kapitel, wenn ich einen spannenden Algorithmus entdecke.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen"
    ]
  },
  {
    "objectID": "classification-preface.html#ausgewählte-algorithmen",
    "href": "classification-preface.html#ausgewählte-algorithmen",
    "title": "Klassifikation oder maschinelles Lernen",
    "section": "",
    "text": "In dem 69  k nearest neighbor schauen wir uns den \\(k\\)-NN Algorithmus einmal an. In diesem Algorithmus werden neue Beoabchtungen anhand der nächstliegenden Nachbarn klassifiziert.\nIn dem 70  Decision trees betrachten wir Entscheidungsbäume. Wir lassen also immer einen Entscheidungsbaum mit zwei Zweigen wachsen und nutzen diese Entscheidungsbäume für die Vorhersage.\nIn dem 71  Support vector machines werden wir uns mit der Support Vector Machine beschäftigen. Wir werden hier aber nur auf die Anwendung eingehen und ein sehr anschauliches Beispiel für die Funktionsweise nutzen.\nIn dem 72  Neural networks betrachten wir dann neuronale Netzwerke. Damit ist dann auch Deep learning gemeint und somit der letzte Stand des maschinellen Lernens.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen"
    ]
  },
  {
    "objectID": "classification-preface.html#referenzen",
    "href": "classification-preface.html#referenzen",
    "title": "Klassifikation oder maschinelles Lernen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nMueller JP, Massaron L. 2021. Machine learning for dummies. John Wiley & Sons.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen"
    ]
  },
  {
    "objectID": "classification-basic.html",
    "href": "classification-basic.html",
    "title": "65  Grundlagen der Klassifikation",
    "section": "",
    "text": "65.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, tidymodels, magrittr, conflicted)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Grundlagen der Klassifikation</span>"
    ]
  },
  {
    "objectID": "classification-basic.html#daten",
    "href": "classification-basic.html#daten",
    "title": "65  Grundlagen der Klassifikation",
    "section": "65.2 Daten",
    "text": "65.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt. Wir wählen auch nur ein kleines Subset aus den Daten aus, da wir in diesem Kapitel nur Funktion demonstrieren und nicht die Ergebnisse interpretieren.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") |&gt; \n  mutate(pig_id = 1:n(),\n         infected = as_factor(infected)) |&gt; \n  select(pig_id, infected, age:crp) |&gt; \n  select(pig_id, infected, everything())  \n\nIn Tabelle 69.1 siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 65.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\npig_id\ninfected\nage\nsex\nlocation\nactivity\ncrp\n\n\n\n\n1\n1\n61\nmale\nnortheast\n15.31\n22.38\n\n\n2\n1\n53\nmale\nnorthwest\n13.01\n18.64\n\n\n3\n0\n66\nfemale\nnortheast\n11.31\n18.76\n\n\n4\n1\n59\nfemale\nnorth\n13.33\n19.37\n\n\n5\n1\n63\nmale\nnorthwest\n14.71\n21.57\n\n\n6\n1\n55\nmale\nnorthwest\n15.81\n21.45\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\nfemale\nnorth\n11.82\n21.5\n\n\n408\n0\n56\nmale\nwest\n13.91\n20.8\n\n\n409\n1\n57\nmale\nnorthwest\n12.49\n21.95\n\n\n410\n1\n61\nmale\nnorthwest\n15.26\n23.1\n\n\n411\n0\n59\nfemale\nnorth\n13.13\n20.23\n\n\n412\n1\n63\nfemale\nnorth\n10.01\n19.89\n\n\n\n\n\n\n\n\nGehen wir jetzt mal die Wörter und Begrifflichkeiten, die wir für das maschinelle Lernen später brauchen einmal durch.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Grundlagen der Klassifikation</span>"
    ]
  },
  {
    "objectID": "classification-basic.html#what-he-says",
    "href": "classification-basic.html#what-he-says",
    "title": "65  Grundlagen der Klassifikation",
    "section": "65.3 What he say’s?",
    "text": "65.3 What he say’s?\nIn diesem Teil des Skriptes werden wir wieder mit einer Menge neuer Begriffe konfrontiert. Deshalb steht hier auch eine Menge an neuen Worten drin. Leider ist es aber auch so, dass wir bekanntes neu bezeichnen. Wir tauchen jetzt ab in die Community der Klassifizierer und die haben dann eben die ein oder andere Sache neu benannt.\n\n\nKurze Referenz zu What he says?\nDie gute nachticht zuerst, wir haben ein relativ festes Vokabular. Das heißt, wir springen nicht so sehr zwischen den Begrifflichkeiten wie wir es in den anderen Teilen des Skriptes gemacht haben. Du kennst die Modellbezeichnungen wie folgt.\n\\[\ny \\sim x\n\\]\nmit\n\n\\(y\\), als Outcome oder Endpunkt.\n\\(x\\), als Covariate oder Einflussvariable.\n\nDas bauen wir jetzt um. Wir nennen in dem Bereich des maschinellen Lernen jetzt das \\(y\\) und das \\(x\\) wie folgt.\n\n\\(y\\) ist unser label, dafür gibt es kein deutsches Wort.\n\\(x\\) sind unsere features und mehrere Features bilden den feature space, dafür gibt es jeweils auch kein deutsches Wort.\n\nLabel meint also das \\(y\\) oder Outcome. Feature beschreibt das \\(x\\) oder die Einflussvariablen.\nIm folgenden Text werde ich also immer vom Label schreiben und dann damit das \\(y\\) links von dem ~ in der Modellgleichung meinen. Wenn ich dann von den Features schreibe, meine ich alle \\(x\\)-Variablen rechts von dem ~ in der Modellgleichung. Ja, daran muss du dich dann gewöhnen. Es ist wieder ein anderer sprachlicher Akzent in einem anderen Gebiet der Statistik.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Grundlagen der Klassifikation</span>"
    ]
  },
  {
    "objectID": "classification-basic.html#klassifikation-vs.-regression",
    "href": "classification-basic.html#klassifikation-vs.-regression",
    "title": "65  Grundlagen der Klassifikation",
    "section": "65.4 Klassifikation vs. Regression",
    "text": "65.4 Klassifikation vs. Regression\nWenn mich etwas aus der Bahn geworfen hat, dann waren es die Terme classification und regression im Kontext des maschinellen Lernens. Wenn ich von classification schreibe, dann wollen wir ein kategoriales Label vorhersagen. Das bedeutet wir haben ein \\(y\\) vorliegen, was nur aus Klassen bzw. Kategorien besteht. Im Zweifel haben wir dann ein Label mit \\(0/1\\) einträgen. Wenn mehr Klassen vorliegen, wird auch gerne von multiclass Klassifikation gesprochen.\nDazu steht im Kontrast der Term regression. In dem Kontext vom maschinellen Lernen meint regression die Vorhersage eines numerischen Labels. Das heißt, wir wollen die Körpergröße der Studierenden vorhersagen und nutzen dazu einen regression Klassifikator. Das ist am Anfang immer etwas verwirrend. Wir unterschieden hier nur die Typen der Label, sonst nichts. Wir fassen also wie folgt zusammen.\n\nclassification, wir haben ein Label bzw. \\(y\\) mit Kategorien. Nehmen wir einmal unser Ferkelbeispiel. In unserer Spalte infected sind die Ferkel infiziert \\((1)\\) oder nicht-infiziert daher gesund \\((0)\\). Du wählst dann den Modus set_mode(\"classification\").\nregression, wir haben ein Label bzw. \\(y\\) mit kontinuierlichen Werten. Unsere Ferkel haben ein Gewicht in \\(kg\\) und daher nehmen wir die Spalte weight. Du wählst dann den Modus set_mode(\"regression\").\n\nWir brauchen die Begriffe, da wir später in den Algorithmen spezifizieren müssen, welcher Typ die Klassifikation sein soll.\n\n\n\n\n\n\nWo ist die Regression?\n\n\n\nWir werden uns in diesen und den folgenden Kapiteln hauptsächlich mit der Klassifikation beschäftigen. Wenn du eine Regression rechnen willst, also ein kontinuierliches Label vorliegen hast, dann musst du bei dem Modellvergleich andere Maßzahlen nehmen und auch eine ROC Kurve passt dann nicht mehr. Du findest dann hier bei den Metric types unter dem Abschnitt numeric Maßzahlen für die Güte der Regression in der Prädiktion.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Grundlagen der Klassifikation</span>"
    ]
  },
  {
    "objectID": "classification-basic.html#supervised-vs.-unsupervised",
    "href": "classification-basic.html#supervised-vs.-unsupervised",
    "title": "65  Grundlagen der Klassifikation",
    "section": "65.5 Supervised vs. unsupervised",
    "text": "65.5 Supervised vs. unsupervised\nDer Unterschied zwischen einer suprvised Lernmethode oder Algorithmus ist, dass das Label bekannt ist. Das heißt, dass wir in unseren Daten eine \\(y\\) Spalte haben an der wir unser Modell dann trainieren können. Das Modell weiß also an was es sich optimieren soll. In Tabelle 65.2 sehen wir einen kleinen Datensatz in einem supervised Setting. Wir haben ein \\(y\\) in den Daten und können an diesem Label unser Modell optimieren. Oft sagen wir auch, dass wir gelabelte Daten vorliegen haben. Daher haben wir eine Spalte, die unser LAbel mit \\(0/1\\) enthält.\nSupervised heißt, dass die Daten ein Label haben und damit eine \\(y\\) Spalte haben. Wir sagen, dass die Daten gelabelt sind. Unsupervised heißt, dass wir ungelabelte Daten vorliegen haben. In dem Fall von semi-supervised Daten, haben wir Beobachtungen mit Label und ohne Label\n\n\n\nTabelle 65.2— Beispieldatensatz für supervised learning. Unsere Daten haben eine Spalte \\(y\\), die wir als Label in unserem Modell nutzen können. Wir haben gelabelte Daten vorliegen.\n\n\n\n\n\n\\(y\\)\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n\n1\n0.2\n1.3\n1.2\n\n\n0\n0.1\n0.8\n0.6\n\n\n1\n0.3\n2.3\n0.9\n\n\n1\n0.2\n9.1\n1.1\n\n\n\n\n\n\nIn der Tabelle 65.3 sehen wir als Beispiel einen Datensatz ohne eine Spalte, die wir als Label nutzen können. Nazürlich haben wir in echt dann keine freie Spalte. Ich habe das einmal so gebaut, damit du den Unterschied besser erkennen kannst. Beim unsuoervised Lernen muss der Algorithmus sich das Label selber bauen. Wir müssen meist vorgeben, wie viele Gruppen wir im Label erwarten würden. Dann können wir den Algorithmus starten.\n\n\n\nTabelle 65.3— Beispieldatensatz für unsupervised learning. Unsere Daten haben keine Spalte \\(y\\), die wir als Label in unserem Modell nutzen können. Wir haben ungelabelte Daten vorliegen.\n\n\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n\n\\(\\phantom{0}\\)\n0.2\n1.3\n1.2\n\n\n\n0.1\n0.8\n0.6\n\n\n\n0.3\n2.3\n0.9\n\n\n\n0.2\n9.1\n1.1\n\n\n\n\n\n\nDann gibt es natürlich auch den Fall, dass wir ein paar Beobachtungen mit einem Eintrag haben und wiederum andere Beobachtungen ohne eine Eintragung. Dann sprechen wir von einem semi-supervised learning. Im Prinzip ist es ein Mischmasch aus supervised learning und dem unsupervised learning. Es gibt hier aber keine genaue Grenze wie viele gelabelete Beobachtungen zu ungelabelten Beobachtungen da sein müssen.\nWir haben sehr oft eine superised Setting in unseren Daten vorliegen. Aber wie immer, du wirst vielleicht auch Cluster bilden wollen und dann ist das unsupervised Lernen eine Methode, die du gut nutzen kannst. Am Ende müssen jeder Beobachtung ein Label zugeordnet werden. Wer das dann macht, ist wiederum die Frage.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Grundlagen der Klassifikation</span>"
    ]
  },
  {
    "objectID": "classification-basic.html#bias-vs.-varianz",
    "href": "classification-basic.html#bias-vs.-varianz",
    "title": "65  Grundlagen der Klassifikation",
    "section": "65.6 Bias vs. Varianz",
    "text": "65.6 Bias vs. Varianz\nIm Bereich des maschinellen Lernens sprechen wir oft von einem Bias/Varianz Trade-off. Das heißt, wir haben zum einen eine Verzerrung (eng. Bias) in unserem Auswahlprozess des anzuwendenden Algorithmus. Zum anderen ist unser Algorithmus nur bedingt genau, das heißt wir haben auch eine Varianz die durch den Algorithmus hervorgerufen wird. Hierbei musst du dich etwas von dem Begriff Varianz im Sinne der deskriptiven Statistik lösen. Die Varianz beschreibt hier die Variabilität in der Vorhersage. Wir meinen hier schon eine Art Abweichung, aber das Wort Varianz mag hier etwas verwirrend sein. In Abbildung 65.1 sehen wir nochmal den Zusammenhang zwischen dem Bias und der Varianz.\n\n\n\n\n\n\nAbbildung 65.1— Der Bias ist eine menschliche Komponente des Modells. Wir wählen das Modell aus und bringen damit eine mögliche Verzerrung in die Auswertung. Die Varianz wird vom Modell selber verursacht und beschreibt den Zusammenhang zwischen dem Traings- und Testdaten.\n\n\n\nWir können daher wie folgt den Bias und die Varianz beschreiben. Wichtig ist hier nochmal, dass wir uns hier die Worte etwas anders benutzen, als wir es in der klassischen Statistik tun würden.\n\nBias: Der Bias (deu. Verzerrung) unseres Modells hat mit den Annahmen zu tun, die wir über die Daten machen. Und damit auch wie gut das Modell zu den Daten passt, auf denen das Modell trainiert wird. Ein Modell mit einem hohen Bias passt nicht gut zu den Trainingsdaten, hat eine begrenzte Flexibilität oder ist extrem einfach für die vorliegenden Daten. Wenn ein Modell zu simpel ist, führt es häufig zu einem hohen Trainingsfehler. Das Label der Traingsdaten wird daher nicht gut wiedergegeben.\nVarianz: Die Varianz unseres Modells sagt aus, wie das Modell seine Vorhersageergebnisse in Abhängigkeit von den Traingsdaten variiert. Ein Modell mit hoher Varianz kann sich gut an die Trainingsdaten anpassen und hat daher Probleme bei der Verallgemeinerung auf die ungesehene Testdaten, was zu einem hohen Testfehler führt.\n\nDer Bias zeigt uns, wie gut unser Modell der Realität entspricht. Die Varianz sagt uns, wie gut unser Modell auf die Trainingsdaten abgestimmt ist.\nIn Abbildung 65.2 shen wir den Zusammenhang zwischen Bias und Varianz an einer Dartscheibe dargestellt. Wenn wir eine hohe Varianz und einen hohen Bias haben, dann treffen wir großflächig daneben. Wenn sich der Bias verringert, dann treffen wir mit einer großen Streuung in die Mitte. Eine geringe Varianz und ein hoher Bias lässt uns präsize in daneben treffen. Erst mit einem niedrigen Bias und einer niedrigen Varianz treffen wir in die Mitte der Dartscheibe.\n\n\n\n\n\n\nAbbildung 65.2— Abstrakte Darstellung des Bias vs. Varianz Trade-off anhand einer Dartscheibe.\n\n\n\nDer gesamte Fehler unseres Modells setzt sich dann wie folgt aus dem Bias und der Varianz zusammen. Wir können den Bias kontrollieren in dem wir verschiedene Algorithmen auf unsere Daten testen und überlegen, welcher Algorithmus am besten passt. Die Varianz können wir dadurch verringern, dass wir unsere Modelle tunen und daher mit verschiedenen Parametern laufen lassen. Am Ende haben wir aber immer einen Restfehler \\(\\epsilon\\), den wir nicht reduzieren können. Unser Modell wird niemals perfekt zu generalisieren sein. Wenn \\(\\epsilon\\) gegen Null laufen sollte, spricht es eher für ein Auswendiglernen des Modells als für eine gute Generalisierung.\n\\[\nerror = variance + bias + \\epsilon\n\\]\nIn der Abbildung 65.3 sehen wir den Zusammenhang zwischen Bias und Varianz nochmal in einer Abbildung im Zusammenhang mit der Modellkomplxität gezeigt. Je größer die Modellkomplexität wird, desto geringer wird der Bias. Dafür wird das Modell aber überangepasst und die Varianz des Modells steigt. Daher gibt es ein Optimum des total errors bei dem der Bias und Varianz jeweils Minimal sind.\n\n\n\n\n\n\nAbbildung 65.3— Zusammenhang zwischen der Modellkomplexität, dem Bias und der Varainz. Es gibt ein Optimum des total errors.\n\n\n\nJetzt wollen wir uns den Zusammenhang zwischen Bias und Varianz nochmal an der Bilderkennung veranschaulichen. Wir nutzen dafür die Bilderkennung um Meerschweinchen und Schafe auf Bildern zu erkennen. In der Abbildung 65.4 sehen wir mich in einem Krokodilkostüm. Unser erstes Modell 1 klassifiziert mich als Meerschweinchen. Wir haben also ein sehr hohes Bias vorliegen. Ich bin kein Meerschweinchen.\n\n\n\n\n\n\nAbbildung 65.4— Unser erstes Modell hat ein hohes Bias. Daher klassifiziert mich das Modell 1 als ein Meerschweinchen, obwohl ich ein Krokodil bin.\n\n\n\nIn der Abbildung 65.5 (a) sehen wir ein durch Model 2 korrekt klassifiziertes Meerschweinchen. Nun hat dieses Modell 2 aber eine zu hohe Varianz. Die hohe Varianz in dem Modell 2 sehen wir in der Abbildung 65.5 (b). Das Meerschweinchen wird nicht als Meerschweinchen von Modell 2 erkannt, da es keine krausen Haare und eine andere Fellfarbe hat. Wir sind also auch mit diesem Modell 2 nicht zufrieden. Nur exakt die gleichen Meerschweinchen zu klassifizieren ist uns nicht genug.\n\n\n\n\n\n\n\n\n\n\n\n(a) In unserem Trainingsdatensatz hat unser Modell 2 eine hohe Varianz. Das Modell 2 findet zwar das Meerschweinchen im Bild, aber hat Probleme auf dem folgenden Testdaten.\n\n\n\n\n\n\n\n\n\n\n\n(b) In unseren Testdaten zu dem trainierten Modell 2 kann das Meerschweinchen im Bild nicht erkannt werden. Das Modell 2 hat eine zu hohe Varianz.\n\n\n\n\n\n\n\nAbbildung 65.5— Unser zweites Modell hat eine hohe Varianz. Es erkennt zwar perfekt eine Meerschweinchenart, muss aber bei einer anderen Art passen.\n\n\n\nIn der Abbildung 65.5 (b) sehen wir nun unser Modell 3 mit einem niedrigen Bias und einer niedrigen Varianz. Das Modell 3 kann Schafe in einer Herde als Schafe klassifizieren. Aber auch hier sehen wir gewisse Grenzen. Das Schaf welches den Kopf senkt, wird nicht von dem Modell 3 als ein Schaf erkannt. Das kann vorkommen, wenn in dem Traingsdaten so ein Schaf nicht als Bild vorlag. Häufig brauchen wir sehr viele gute Daten. Mit guten Daten, meine ich nicht immer die gleichen Beobachtungen oder Bilder sondern eine gute Bandbreite aller möglichen Gegebenheiten.\n\n\n\n\n\n\nAbbildung 65.6— Unser letztes Modell 3 hat eine niedrige Varianz und ist in der Lage die Schafe auch als Schafe zu entdecken. Ein Schaf senkt den Kopf und schon kann unser Modell 3 das Schaf nicht mehr finden.\n\n\n\nWir sehen also, das Thema Bias und Varianz beschäftigt uns bei der Auswahl des richtigen Modells und bei der Festlegung der Modellkomplexität. Du kannst dir merken, dass ein komplexeres Modell auf den Trainingsdaten meistens bessere Ergebnisse liefert und dann auf den Testdaten schlechtere. Ein komplexes Modell ist meist überangepasst (eng. overfitted).",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Grundlagen der Klassifikation</span>"
    ]
  },
  {
    "objectID": "classification-basic.html#problem-der-fehlenden-werte",
    "href": "classification-basic.html#problem-der-fehlenden-werte",
    "title": "65  Grundlagen der Klassifikation",
    "section": "65.7 Problem der fehlenden Werte",
    "text": "65.7 Problem der fehlenden Werte\n\n\n\n\n\n\nMehr zu fehlenden Werten\n\n\n\nIn dem Kapitel 67 erfährst du, wie du mit den fehlenden Werten im maschinellen Lernen umgehst. Wir werden dort aber nicht alle Details wiederholen. In dem Kapitel 43 erfährst du dann mehr über die Hintergründe und die Verfahren zum Imputieren von fehlenden Werten.\n\n\nEin wichtiger Punkt ist bei der Nutzung von maschinellen Lernen, dass wir keine fehlenden Beobachtungen in den Daten haben dürfen. Es darf kein einzelner Wert fehlen. Dann funktionieren die Algorithmen nicht und wir erhalten eine Fehlermeldung. Deshalb ist es die erste Statistikerpflicht darauf zu achten, dass wir nicht so viele fehlenden Werte in den Daten haben. Das ist natürlich nur begrenzt möglich. Wenn wir auf die Gummibärchendaten schauen, dann wurden die Daten ja von mir mit Erhoben. Dennoch haben wir viele fehlende Daten mit drin, da natürlich Studierende immer was eingetragen haben. Wenn du wissen willst, wie du mit fehlenden Werten umgehst, dann schaue einmal dazu das Kapitel 43 an. Wir gehen hier nicht nochmal auf alle Verfahren ein, werden aber die Verfahren zur Imputation von fehlenden Werten dann am Beispiel der Gummibärchendaten anwenden. Müssen wir ja auch, sonst könnten wir auch die Daten nicht für maschinelle Lernverfahren nutzen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Grundlagen der Klassifikation</span>"
    ]
  },
  {
    "objectID": "classification-basic.html#normalisierung",
    "href": "classification-basic.html#normalisierung",
    "title": "65  Grundlagen der Klassifikation",
    "section": "65.8 Normalisierung",
    "text": "65.8 Normalisierung\n\n\n\n\n\n\nMehr zur Normalisierung\n\n\n\nIn dem Kapitel 67 erfährst du, wie du die Normalisierung von Daten im maschinellen Lernen anwendest. In dem Kapitel 18 kannst du dann mehr über die Hintergründe und die Verfahren zur Normalisierung nachlesen. Wir wenden in hier nur die Verfahren an, gehen aber nicht auf die Details weiter ein.\n\n\nUnter Normalisierung der Daten fassen wir eigentlich ein preprocessing der Daten zusammen. Wir haben ja unsere Daten in einer ursprünglichen Form vorliegen. Häufig ist diese Form nicht geeignet um einen maschinellen Lernalgorithmus auf diese ursprüngliche Form der Daten anzuwenden. Deshalb müssen wir die Daten vorher einmal anpassen und in eine geleiche Form über alle Variablen bringen. Was meine ich so kryptisch damit? Schauen wir uns einmal in der Tabelle 65.4 ein Beispiel für zu normalisierende Daten an.\n\n\n\nTabelle 65.4— Beispieldatensatz für einen Datensatz der normiert werden muss. Die einzelnen Spalten haben sehr unterschiedliche Wertebereiche eingetragen.\n\n\n\n\n\n\\(y\\)\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n\n1\n0.2\n1430\n23.54\n\n\n0\n0.1\n1096\n18.78\n\n\n1\n0.4\n2903\n16.89\n\n\n1\n0.2\n7861\n12.98\n\n\n\n\n\n\nWarum müssen diese Daten normalisiert werden? Wir haben mit \\(x_1\\) eine Variable vorliegen, die im Iterval \\([0;1]\\) liegt. Die Variable \\(x_2\\) liegt in einem zehntausendfach größeren Wertebereich. Die Werte der Variable \\(x_3\\) ist auch im Vergleich immer noch hundertfach im Wertebereich unterschiedlich. Dieser großen Unterschiede im Wertebereich führen zu Fehlern bei Modellieren. Wir können hierzu das Kapitel 18 betrachten. Dort werden gängige Transformationen einmal erklärt. Wir gehen hier nicht nochmal auf alle Verfahren ein, sondern konzentrieren uns auf die häufigsten Anwendungen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Grundlagen der Klassifikation</span>"
    ]
  },
  {
    "objectID": "classification-basic.html#das-rezept-mit-recipe",
    "href": "classification-basic.html#das-rezept-mit-recipe",
    "title": "65  Grundlagen der Klassifikation",
    "section": "65.9 Das Rezept mit recipe()",
    "text": "65.9 Das Rezept mit recipe()\nWenn wir jetzt in den folgenden Kapiteln mit den maschinellen Lernverfahren arbeiten werden, nutzen wir das R Paket {recipes} um uns mit der Funktion recipe() ein Rezept der Klassifikation zu erstellen. Warum brauchen wir das? Wir werden sehen, dass wir auf verschiedene Datensätze immer wieder die gleichen Algorithmen anwenden. Auch wollen wir eine Reihe von Vorverarbeitungsschritten (eng. preprocessing steps) auf unsere Daten anwenden. Dann ist es einfacher, wenn wir alles an einem Ort abgelegt haben. Am Ende haben wir auch verschiedene Spalten in unseren Daten. Meistens eine Spalte mit dem Label und dann sehr viele Spalten für unsere Features oder Prediktoren. Vielleicht noch eine Spalte für die ID der Beobachtungen. Das macht alles sehr unübersichtlich. Deshalb nutzen wir recipes um mehr Ordnung in unsere Klassifikation zu bekommen.\n\n\nDu findest hier die Introduction to recipes und dann eine Idee wie recipes funktionieren mit Preprocess your data with recipes.\nWir gehen nun folgende vier Schritte für die Erstellung eines Modellfits mit dem R Paket {recipe} einmal durch. Am Ende haben wir dann unsere Klassifikation durchgeführt. Vorher haben wir aber unseren Algorithmus und damit unser Modell definiert und auch festgelegt, was in den Daten noch angepasst und transformiert werden soll. Alles zusammen bringen wir dann in ein workflow Objekt in dem alles, was wir mit den Daten machen wollen, gespeichert ist.\n\nErstellen des Modells (logreg_mod),\nein Vorverarbeitungsrezept (eng. preprocessing) für unseren Datensatz pig_tbl erstellen (pig_rec),\ndas Modell und das Rezept in einem Wokflow bündeln (pig_wflow), und\nunseren Workflow mit einem einzigen Aufruf von fit() trainieren.\n\nEs geht los in dem wir als erstes unser Modell definieren. Wir wollen hier aus einfachen Gründen eine logistische Regression rechnen. Dafür nutzen wir die Funktion logistic_reg() um eben eine logistische Regression zu rechnen. Es gibt aber eine große Anzahl an möglichen Implementierungen bzw. engine in R. Wir wählen hier die Implementierung des glm mit der Funktion set_engine(\"glm\"). Faktisch haben wir hier also die Funktion glm(..., family = binomial) definiert. Nur ohne die Daten und die Formel.\n\n\nDu findest auf Fitting and predicting with parsnip eine große Auswahl an implementierten Algorithmen.\n\nlogreg_mod &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\n\nNachdem wir den Algorithmus für unser Modell definiert haben, wollen wir natürlich noch festlegen, was jetzt gerechnet werden soll. Unser Modell definieren wir in der Funktion recipe(). Hier haben wir definiert, was in das Modell soll. Links steht das Outcome und rechts nur ein .. Damit haben wir alle anderen Spalten als Einflussvariablen ausgewählt. Das stimmt aber nur halb. Den in dem Rezept können wir auch Rollen für unsere Variablen definieren. Mit der Funktion update_role() definieren wir die Variable pig_id als \"ID\". In der Klassifikation wird jetzt diese Variable nicht mehr berücksichtigt. Dann können wir noch Variablen transfomationen definieren. Wir wollen hier eine Dummykodierung für alle nominalen Prädiktoren, daher Faktoren, durchführen. Und wir wollen alle Variablen entfernen, in denen wir nur einen Eintrag haben oder eben eine Varianz von Null.\n\npig_rec &lt;- recipe(infected ~ ., data = pig_tbl) |&gt; \n  update_role(pig_id, new_role = \"ID\")  |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors())\n\npig_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()\n\n\nWir du siehst wird hier noch nichts gerechnet. Es gilt jetzt zu definieren was wir tun wollen. Damit wir das Rezept einfach immer wieder auf neue Daten anwenden können. Die Rollen der Variablen kannst du dir auch über die Funktion summary() wiedergeben lassen.\n\nsummary(pig_rec)\n\n# A tibble: 7 × 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 pig_id   &lt;chr [2]&gt; ID        original\n2 age      &lt;chr [2]&gt; predictor original\n3 sex      &lt;chr [3]&gt; predictor original\n4 location &lt;chr [3]&gt; predictor original\n5 activity &lt;chr [2]&gt; predictor original\n6 crp      &lt;chr [2]&gt; predictor original\n7 infected &lt;chr [3]&gt; outcome   original\n\n\nDu siehst, dass die Variable pig_id eine ID ist und die Variable infected das Outcome darstellt. Der Rest sind die Prädiktoren mit ihren jeweiligen Typen. Wir können über die Hilfsfunktionen all_predictor() oder all_nominal_predictor() eben nur bestimmte Spalten für eine Transformation auswählen.\nIm nächsten Schritt bringen wir das Modell logreg_mod und das Rezept pig_rec mit den Informationen über die Variablen und die notwendigen Transformationsschritte in einem workflow() zusammen. In diesem workflow() sind alle wichtigen Information drin und wir können den Workflow mit immer wieder neuen Subsets von unseren ursprünglichen Daten füttern.\n\npig_wflow &lt;- workflow() |&gt; \n  add_model(logreg_mod) |&gt; \n  add_recipe(pig_rec)\n\npig_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nNun heißt es noch den Wirkflow mit echten Daten zu füttern. Wir rechnen also erst jetzt mit echten Daten. Vorher aber wir nur gesagt, was wir machen wollen. Erst die Funktion fit() rechnet das Modell auf den Daten mit den Regeln in dem Rezept. Wir nehmen hier wieder unsere ursprünglichen Daten, aber du könntest hier auch den Traingsdatensatz nehmen.\n\npig_fit &lt;- pig_wflow |&gt; \n  fit(data = pig_tbl)\n\npig_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n       (Intercept)                 age            activity                 crp  \n         -19.46706             0.01100             0.06647             0.96804  \n          sex_male  location_northeast  location_northwest       location_west  \n          -0.51320             0.01848            -0.51613            -0.26807  \n\nDegrees of Freedom: 411 Total (i.e. Null);  404 Residual\nNull Deviance:      522.6 \nResidual Deviance: 402.3    AIC: 418.3\n\n\nWir erhalten den klassischen Fit einer logististischen Regression wieder, wenn wir die Funktion extract_fit_parsnip() verwenden. Die Funktion gibt uns dann alle Informationen wieder. Dann können wir uns über die Funktion tidy() auch eine aufgeräumte Wiedergabe erstellen lassen.\n\npig_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  tidy() |&gt; \n  mutate(across(where(is.numeric), round, 2),\n         p.value = pvalue(p.value))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 8 × 5\n  term               estimate std.error statistic p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 (Intercept)          -19.5       3.02     -6.45 &lt;0.001 \n2 age                    0.01      0.03      0.4  0.690  \n3 activity               0.07      0.09      0.72 0.470  \n4 crp                    0.97      0.11      8.8  &lt;0.001 \n5 sex_male              -0.51      0.32     -1.61 0.110  \n6 location_northeast     0.02      0.36      0.05 0.960  \n7 location_northwest    -0.52      0.32     -1.6  0.110  \n8 location_west         -0.27      0.36     -0.75 0.460  \n\n\nUnd was ist jetzt mit der Prädiktion? Dafür können wir entweder die Funktion predict() nutzen oder aber die Funktion augment(). Mir persönlich gefällt die Funktion augment() besser, da ich hier mehr Informationen zu den vorhergesagten Werten erhalte. Ich wähle mir dann die Spalte infected aus und alle Spalten, die ein .pred beinhalten. Dann runde ich noch auf die zweite Kommastelle.\n\naugment(pig_fit, new_data = pig_tbl) |&gt; \n  select(infected, matches(\".pred\")) |&gt; \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 412 × 4\n   infected .pred_class .pred_0 .pred_1\n   &lt;fct&gt;    &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 1        1              0.03    0.97\n 2 1        0              0.73    0.27\n 3 0        1              0.45    0.55\n 4 1        1              0.31    0.69\n 5 1        1              0.11    0.89\n 6 1        1              0.13    0.87\n 7 1        0              0.6     0.4 \n 8 0        0              0.66    0.34\n 9 1        1              0.05    0.95\n10 1        1              0.21    0.79\n# ℹ 402 more rows\n\n\nDamit hätten wir einmal das Prinzip des Rezeptes für die Klassifikation in R durchgeführt. Dir wird das Rezept in den nächsten Kapiteln wieder über den Weg laufen. Für die Anwendung gibt es eigentlich keine schönere Art die Klassifikation sauber durchzuführen. Wir erhalten gute Ergebnisse und wissen auch was wir getan haben.\n\n\n\nAbbildung 65.1— Der Bias ist eine menschliche Komponente des Modells. Wir wählen das Modell aus und bringen damit eine mögliche Verzerrung in die Auswertung. Die Varianz wird vom Modell selber verursacht und beschreibt den Zusammenhang zwischen dem Traings- und Testdaten.\nAbbildung 65.2— Abstrakte Darstellung des Bias vs. Varianz Trade-off anhand einer Dartscheibe.\nAbbildung 65.3— Zusammenhang zwischen der Modellkomplexität, dem Bias und der Varainz. Es gibt ein Optimum des total errors.\nAbbildung 65.4— Unser erstes Modell hat ein hohes Bias. Daher klassifiziert mich das Modell 1 als ein Meerschweinchen, obwohl ich ein Krokodil bin.\nAbbildung 65.5 (a)— In unserem Trainingsdatensatz hat unser Modell 2 eine hohe Varianz. Das Modell 2 findet zwar das Meerschweinchen im Bild, aber hat Probleme auf dem folgenden Testdaten.\nAbbildung 65.5 (b)— In unseren Testdaten zu dem trainierten Modell 2 kann das Meerschweinchen im Bild nicht erkannt werden. Das Modell 2 hat eine zu hohe Varianz.\nAbbildung 65.6— Unser letztes Modell 3 hat eine niedrige Varianz und ist in der Lage die Schafe auch als Schafe zu entdecken. Ein Schaf senkt den Kopf und schon kann unser Modell 3 das Schaf nicht mehr finden.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Grundlagen der Klassifikation</span>"
    ]
  },
  {
    "objectID": "classification-data.html",
    "href": "classification-data.html",
    "title": "66  Data splitting",
    "section": "",
    "text": "66.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, tidymodels, magrittr, conflicted)\nconflicts_prefer(magrittr::extract)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Data splitting</span>"
    ]
  },
  {
    "objectID": "classification-data.html#daten",
    "href": "classification-data.html#daten",
    "title": "66  Data splitting",
    "section": "66.2 Daten",
    "text": "66.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt. Auch brauchen wir nicht alle Spalten, da wir hier um die Zeilen und damit die Beobachtungen geht.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") |&gt; \n  mutate(pig_id = 1:n()) |&gt; \n  select(pig_id, infected, age:crp) |&gt; \n  select(pig_id, infected, everything())  \n\nIn Tabelle 69.1 siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 66.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\npig_id\ninfected\nage\nsex\nlocation\nactivity\ncrp\n\n\n\n\n1\n1\n61\nmale\nnortheast\n15.31\n22.38\n\n\n2\n1\n53\nmale\nnorthwest\n13.01\n18.64\n\n\n3\n0\n66\nfemale\nnortheast\n11.31\n18.76\n\n\n4\n1\n59\nfemale\nnorth\n13.33\n19.37\n\n\n5\n1\n63\nmale\nnorthwest\n14.71\n21.57\n\n\n6\n1\n55\nmale\nnorthwest\n15.81\n21.45\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\nfemale\nnorth\n11.82\n21.5\n\n\n408\n0\n56\nmale\nwest\n13.91\n20.8\n\n\n409\n1\n57\nmale\nnorthwest\n12.49\n21.95\n\n\n410\n1\n61\nmale\nnorthwest\n15.26\n23.1\n\n\n411\n0\n59\nfemale\nnorth\n13.13\n20.23\n\n\n412\n1\n63\nfemale\nnorth\n10.01\n19.89\n\n\n\n\n\n\n\n\nGehen wir jetzt mal die verschiedenen Datensätze und Begrifflichkeiten, die wir für das maschinelle Lernen später brauchen, einmal durch.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Data splitting</span>"
    ]
  },
  {
    "objectID": "classification-data.html#trainingsdatensatz-und-testdatensatz",
    "href": "classification-data.html#trainingsdatensatz-und-testdatensatz",
    "title": "66  Data splitting",
    "section": "66.3 Trainingsdatensatz und Testdatensatz",
    "text": "66.3 Trainingsdatensatz und Testdatensatz\nUm zu beginnen, teilen wir unseren einen Datensatz in zwei: einen Trainingssatz und einen Testsatz. Die meisten Zeilen und damit Beobachtungen des Originaldatensatzes werden im Trainingssatz sein. Wir nutzen die Trainingsdaten zum Anpassen des Modells. Wir trainieren das Modell auf den Daten des Trainingsdatensatzes. Wir messen dann das Modell auf den Testdatensatz. Warum machen wir das? Wenn wir auf dem Trainingsdatensatz auch die Modelgüte testen würden, dann könnten wir eine Überanpassung (eg. overfitting) auf die Trainingsdaten beobachten. Das Modell ist so gut an die spezifischen Trainingsdaten angepasst, dass es mit neuen Daten schwer umgehen kann.\n\n\nDas R Paket {resample} stellt die Common Resampling Patterns nochmal da. Auch findest unter Resampling for Evaluating Performance noch eine Menge mehr Ideen für das Resampling.\nZu diesem Zweck können wir das R Paket {rsample} verwenden. Wir nutzen dann die Funktion initial_split() um die Daten in einen Trainingsdatensatz und einen Testdatensatz aufzuteilen. Dann müssen wir noch den Trainingsdatensatz und den Testdatensatz einmal getrennt in einem Objekt abspeichern.\n\npig_split &lt;- initial_split(pig_tbl, prop = 3/4)\n\npig_split\n\n&lt;Training/Testing/Total&gt;\n&lt;309/103/412&gt;\n\n\nWie wir sehen, sehen wir gar nichts. Das ist auch so gewollt. Da wir im maschinellen Lernen gerne mal mit Datensätzen mit mehreren tausend Zeilen arbeiten würde es wenig helfen, wenn wir gleich alles auf der R Console ausgegeben kriegen. Die Information wie viel wir in den jeweiligen Gruppen haben, hilft schön genug.\n\ntrain_pig_tbl &lt;- training(pig_split)\ntest_pig_tbl &lt;- testing(pig_split)\n\nNun haben wir die beiden Datensätze jeweils separat und können auf dem Trainingsdatensatz die jeweiligen Algorithmen bzw. Modelle trainieren.\nEs ist schön, wenn wir Funktionen wie initial_split(), die für uns die Arbeit machen. Wir haben dann aber auch sehr schnell das Gefühl mit einer Black Box zu arbeiten. Man weiß gar nicht, was da eigentlich passiert ist. Deshalb hier nochmal der Code, den ich dann auch immer zur Demonstration nutze. Wenn wir eine ID Spalte haben, dann können wir auch über die Funktion sample_frac() und dem Anteil der ausgewählten Beobachtungen und der Funktion anti_join(), die Trainings- und Testdaten erstellen.\n\npig_train_tbl &lt;- pig_tbl |&gt; sample_frac(0.75)\npig_test_tbl &lt;- anti_join(pig_tbl,\n                          pig_train_tbl, by = 'pig_id')\n\nWir können dann auch überprüfen, ob wir die gleichen Anteile von den infizierten Ferkeln in den jeweiligen Datensätzen haben. Wir berechnen dafür einfach die relativen Anteile. Ein wenig komplizierter als nötig, aber hier geht es jetzt um die Veranschaulichung.\n\ntable(pig_train_tbl$infected)/sum(table(pig_train_tbl$infected))\n\n\n        0         1 \n0.3300971 0.6699029 \n\ntable(pig_test_tbl$infected)/sum(table(pig_test_tbl$infected))\n\n\n        0         1 \n0.3300971 0.6699029 \n\n\nDu kannst die Generierung häufiger wiederholen und du wirst sehen, dass wir es mit einem Zufallsprozess zu tun haben. Mal sind die Anteile ähnlicher mal eher nicht. Das ist dann auch der Grund warum wir unsere Modelle tunen müssen und Modelle häufig wiederholt rechnen und die Ergebnisse dann zusammenfassen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Data splitting</span>"
    ]
  },
  {
    "objectID": "classification-data.html#validierungsdatensatz",
    "href": "classification-data.html#validierungsdatensatz",
    "title": "66  Data splitting",
    "section": "66.4 Validierungsdatensatz",
    "text": "66.4 Validierungsdatensatz\nDie finalen Modelle sollten nur einmal anhand ihres Testdatensatzes evaluieren werden. Das Überpfrüfen auf dem Testdatensatz geschieht nachdem die Optimierung und das Training der Modelle vollständig abgeschlossen ist. Was natürlich für uns nicht so schön ist, wir wollen ja auch zwischendurch mal schauen, ob wir auf dem richtigen Weg mit dem Training sind. Wir solle es auch sonst mit dem Tuning funktionieren? Deshalb ist möglich, zusätzliche Datensätze aus dem Trainingsprozess herauszuhalten, die zur mehrmaligen Evaluierung von Modellen verwendet werden können. Das machen wir dann solange bis wir bereit sind anhand des endgültigen Testsatzes zu evaluieren.\nDiese zusätzlichen, aufgeteilten Datensätze werden oft als Validierungssätze bezeichnet und können in über die Funktion validation_split() erstellt werden.\n\nval_pig_lst &lt;- validation_split(pig_tbl, prop = 0.8)\n\nWarning: `validation_split()` was deprecated in rsample 1.2.0.\nℹ Please use `initial_validation_split()` instead.\n\nval_pig_lst\n\n# Validation Set Split (0.8/0.2)  \n# A tibble: 1 × 2\n  splits           id        \n  &lt;list&gt;           &lt;chr&gt;     \n1 &lt;split [329/83]&gt; validation\n\n\nIn diesem Fall lassen wir den Validierungsdatensatz einmal so in der Liste stehen. Es ist faktisch wider ein Split der Daten, nur das wir jetzt auf diesem Datensatz unser Modell während des Tunings testen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Data splitting</span>"
    ]
  },
  {
    "objectID": "classification-data.html#kreuzvalidierung",
    "href": "classification-data.html#kreuzvalidierung",
    "title": "66  Data splitting",
    "section": "66.5 Kreuzvalidierung",
    "text": "66.5 Kreuzvalidierung\nBei der Abstimmung von Hyperparametern und der Modellanpassung ist es oft nützlich, das Modell anhand von mehr als nur einem einzigen Validierungssatz zu bewerten, um eine stabilere Schätzung der Modellleistung zu erhalten. Wir meinen hier mit Hyperparametern die Optionen, die ein Algorithmus hat um diesen Algorithmus zu optimieren. Aus diesem Grund verwenden Modellierer häufig ein Verfahren, das als Kreuzvalidierung bekannt ist und bei dem die Daten mehrfach in Analyse- und Valisierungsdaten aufgeteilt werden.\nDie vielleicht häufigste Methode der Kreuzvalidierung ist die \\(V\\)-fache Kreuzvalidierung. Bei dieser auch als \\(k\\)-fold cross-validation bezeichneten Methode werden \\(V\\) neue Stichproben bzw. Datensätze erstellt, indem die Daten in \\(V\\) Gruppen (auch folds genannt) von ungefähr gleicher Größe aufgeteilt werden. Der Analysesatz jeder erneuten Stichprobe besteht aus \\(V-1\\) Gruppen, wobei die verbleibende Gruppe als Validierungsdatensatz verwendet wird. Insgesamt führen wir dadurch dann den Algorithmus \\(V\\)-mal durch. Auf diese Weise wird jede Beobachtung in Daten in genau einem Beurteilungssatz verwendet.\nIn R können wir dafür die Funktion vfold_cv() nutzen. Im Folgenden einmal Split für \\(V = 5\\). Wir führen also eine \\(5\\)-fache Kreuzvalidierung durch.\n\nvfold_cv(pig_tbl, v = 3)\n\n#  3-fold cross-validation \n# A tibble: 3 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [274/138]&gt; Fold1\n2 &lt;split [275/137]&gt; Fold2\n3 &lt;split [275/137]&gt; Fold3\n\n\nAls ein Nachteil wird oft angesehen, dass die Kreuzvalidierung eine hohe Varianz in den Daten verursacht. Dagegen hilft dann die wiederholte Kreuzvalidierung (eng. repeated cross-validation). Wir bauen in jede Kreuzvalidierung nochmal eine oder mehr Wiederholungen ein. In unserem Fall dann drei Wiederholungen je Kreuzvalidierung \\(V\\).\n\nvfold_cv(pig_tbl, v = 3, repeats = 2)\n\n#  3-fold cross-validation repeated 2 times \n# A tibble: 6 × 3\n  splits            id      id2  \n  &lt;list&gt;            &lt;chr&gt;   &lt;chr&gt;\n1 &lt;split [274/138]&gt; Repeat1 Fold1\n2 &lt;split [275/137]&gt; Repeat1 Fold2\n3 &lt;split [275/137]&gt; Repeat1 Fold3\n4 &lt;split [274/138]&gt; Repeat2 Fold1\n5 &lt;split [275/137]&gt; Repeat2 Fold2\n6 &lt;split [275/137]&gt; Repeat2 Fold3\n\n\nWir sehen das der Split ungefähr immer gleich groß ist. Manchmal haben wir durch die Trennung eine Beobachtung mehr in dem Analysedatensatz mit \\(n = 329\\) oder \\(n = 330\\) Beobachtungen. Dementsprechend hat der Validierungsdatensatz einmal \\(n = 82\\) und einmal \\(n = 83\\) Beobachtungen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Data splitting</span>"
    ]
  },
  {
    "objectID": "classification-data.html#monte-carlo-kreuzvalidierung",
    "href": "classification-data.html#monte-carlo-kreuzvalidierung",
    "title": "66  Data splitting",
    "section": "66.6 Monte-Carlo Kreuzvalidierung",
    "text": "66.6 Monte-Carlo Kreuzvalidierung\nWir haben als eine Alternative zur V-fachen Kreuzvalidierung die Monte-Carlo-Kreuzvalidierung vorliegen. Während bei der V-fachen Kreuzvalidierung jede Beobachtung in den Daten einem - und zwar genau einem - Validierungsdatensatz zugewiesen wird, wird bei der Monte-Carlo-Kreuzvalidierung für jeden Validierungsdatensatz eine zufällige Teilmenge der Daten ausgewählt, d. h. jede Beobachtung kann in 0, 1 oder vielen Validierungsdatensätzen verwendet werden. Der Analysesatz besteht dann aus allen Beobachtungen, die nicht ausgewählt wurden. Da jeder Validierungsdatensatz unabhängig ausgewählt wird, können wir diesen Vorgang so oft wie gewünscht wiederholen. Das stimt natürlich nur bedingt, denn irgendwann haben wir auch bei perfekter Permutation dann Wiederholungen der Datensätze.\nDie Funktion mc_cv() liefert uns dann die Datensätze für die Monte-Carlo Kreuzvalidierung. Wir geben dabei an, wieviel der Daten in den jeweiligen Datensatz hinein permutiert werden soll.\n\nmc_cv(pig_tbl, prop = 0.6, times = 3)\n\n# Monte Carlo cross-validation (0.6/0.4) with 3 resamples  \n# A tibble: 3 × 2\n  splits            id       \n  &lt;list&gt;            &lt;chr&gt;    \n1 &lt;split [247/165]&gt; Resample1\n2 &lt;split [247/165]&gt; Resample2\n3 &lt;split [247/165]&gt; Resample3",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Data splitting</span>"
    ]
  },
  {
    "objectID": "classification-data.html#bootstraping",
    "href": "classification-data.html#bootstraping",
    "title": "66  Data splitting",
    "section": "66.7 Bootstraping",
    "text": "66.7 Bootstraping\nDie letzte Stichprobengenierungsmethode ist der Bootstrap. Eine Bootstrap Stichprobe ist eine Stichprobe des Datensatzes mit der gleichen Größe wie der Datensatz. Nur werden die Bootstrap Stichproben mit Ersetzung gezogen, so dass eine einzelne Beobachtung mehrfach in die Stichprobe aufgenommen werden können. Der Validierungsdatensatz besteht dann aus allen Beobachtungen, die nicht für den Analysesatz ausgewählt wurden. Im Allgemeinen führt das Bootstrap-Resampling zu pessimistischen Schätzungen der Modellgenauigkeit.\nWir können die Funktion bootstraps() für die Generierung der Bootstrap Stichprobe nutzen.\n\npig_boot_tbl &lt;- pig_tbl |&gt; \n  extract(1:10, 1:5)\n\npig_boot &lt;- bootstraps(pig_boot_tbl, times = 3)\n\nNun haben wir auch die Möglichkeit uns die einzelnen Bootstraps Stichproben mit pluck() rauszuziehen. Hier sehen wir auch, dass einzelne Beobachtungen doppelt in der Bootstrap Stich probe vorkommen.\n\npluck(pig_boot, \"splits\", 1) |&gt; \n  as_tibble() \n\n# A tibble: 10 × 5\n   pig_id infected   age sex    location \n    &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1      3        0    66 female northeast\n 2      4        1    59 female north    \n 3      8        0    53 male   northwest\n 4     10        1    57 male   northwest\n 5      2        1    53 male   northwest\n 6      9        1    58 female west     \n 7      6        1    55 male   northwest\n 8      9        1    58 female west     \n 9     10        1    57 male   northwest\n10      5        1    63 male   northwest",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Data splitting</span>"
    ]
  },
  {
    "objectID": "classification-data.html#weitere-valdierungen",
    "href": "classification-data.html#weitere-valdierungen",
    "title": "66  Data splitting",
    "section": "66.8 Weitere Valdierungen",
    "text": "66.8 Weitere Valdierungen\nNeben den hier vorgestellten Varianten gibt es noch weitere Möglichkeiten in dem R Paket {rsample} sich Stichprobendatensätze zu generieren. Wir gehen jetzt hier nicht mehr im Detail auf die verschiedenen Möglichkeiten ein. Dafür dann einfach die Links auf die {rsample} Hilfeseite nutzen.\n\nStratifiziertes Resampling nutzen wir, wenn wir eine Gruppe in den Daten haben, die nicht gleichmäßig über die Daten verteilt ist. Das heißt, wir haben ein nicht balanciertes Design. Kann plakativ wäre das der Fall, wenn wir fast nur Frauen oder Männer in unseren Daten vorliegen hätten. Hier kann es dann passieren, dass wir zufällig Datensätze ziehen, die nur Frauen oder nur Männer beinhalten. Das wollen wir natürlich verhindern.\nGruppiertes Resampling nutzen wir, wenn wir korrelierte Beobachtungen haben. Oft sind einige Beobachtungen in deinen Daten ähnlicher als es der Zufall vermuten ließe, z. B. weil sie wiederholte Messungen desselben Probanden darstellen oder alle an einem einzigen Ort gesammelt wurden. Dann müssen wir eventuell auch hierfür das Resampling anpassen.\nZeitpunkt basiertes Resampling sind in dem Sinne eine Besonderheit, da wir natürlich berücksichtigen müssen, wann eine Beobachtung im zeitlichen Verlauf gemacht wurde. Hier hat die Zeit einen Einfluss auf das Resampling.\n\nAm Ende musst du entscheiden, welche der Resamplingmethoden für dich am besten geeignet ist. Wir müssen eben einen Trainingsdatensatz und einen Testdatensatz haben. Die Validierungsdaten dienen dann zum Tuning deiner Modelle. Nicht immer nutzen wir auch Validierungsdatensätze. In dem einfachsten Anwendungsfall nutzt du immer wieder deine Traingsdaten mit unterschiedlichen Einstellungen in deinem Algorithmus.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Data splitting</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html",
    "href": "classification-pre-processing.html",
    "title": "67  Data preprocessing",
    "section": "",
    "text": "67.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor,\n               conflicted)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#daten",
    "href": "classification-pre-processing.html#daten",
    "title": "67  Data preprocessing",
    "section": "67.2 Daten",
    "text": "67.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt. Wir wählen auch nur ein kleines Subset aus den Daten aus, da wir in diesem Kapitel nur Funktion demonstrieren und nicht die Ergebnisse interpretieren.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") |&gt; \n  mutate(pig_id = 1:n()) |&gt; \n  select(pig_id, infected, age, crp, sex, frailty) |&gt; \n  select(pig_id, infected, everything())  \n\nIn Tabelle 69.1 siehst du nochmal einen Ausschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 67.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\npig_id\ninfected\nage\ncrp\nsex\nfrailty\n\n\n\n\n1\n1\n61\n22.38\nmale\nrobust\n\n\n2\n1\n53\n18.64\nmale\nrobust\n\n\n3\n0\n66\n18.76\nfemale\nrobust\n\n\n4\n1\n59\n19.37\nfemale\nrobust\n\n\n5\n1\n63\n21.57\nmale\nrobust\n\n\n6\n1\n55\n21.45\nmale\nrobust\n\n\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\n21.5\nfemale\npre-frail\n\n\n408\n0\n56\n20.8\nmale\nfrail\n\n\n409\n1\n57\n21.95\nmale\npre-frail\n\n\n410\n1\n61\n23.1\nmale\nrobust\n\n\n411\n0\n59\n20.23\nfemale\nrobust\n\n\n412\n1\n63\n19.89\nfemale\nrobust\n\n\n\n\n\n\n\n\nGehen wir jetzt mal die Preprocessing Schritte, die wir für das maschinelle Lernen später brauchen einmal durch. Am Ende des Kapitels schauen wir uns dann die Anwendung nochmal im Ganzen auf den Gummibärchendaten einmal an.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#das-rezept-mit-recipe",
    "href": "classification-pre-processing.html#das-rezept-mit-recipe",
    "title": "67  Data preprocessing",
    "section": "67.3 Das Rezept mit recipe()",
    "text": "67.3 Das Rezept mit recipe()\nIn dem Einführungskapitel zur Klassifikation haben wir uns ja mit dem Rezept und dem Workflow schon mal beschäftigt. Hier möchte ich dann nochmal etwas mehr auf das Rezept eingehen und zeigen, wie das Rezept für Daten dann mit den Daten zusammenkommt. Wir bauen uns wie immer mit der Funktion recipe() das Datenrezept in R zusammen. Ich empfehle grundsätzlich vorab einen select() Schritt durchzuführen und nur die Variablen in den Daten zu behalten, die wir wirklich brauchen. Dann können wir auch mit dem . einfach alle Spalten ohne das Outcome als Prädiktoren definieren.\n\npig_rec &lt;- recipe(infected ~ ., data = pig_tbl) |&gt; \n  update_role(pig_id, new_role = \"ID\")\n\npig_rec |&gt; summary()\n\n# A tibble: 6 × 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 pig_id   &lt;chr [2]&gt; ID        original\n2 age      &lt;chr [2]&gt; predictor original\n3 crp      &lt;chr [2]&gt; predictor original\n4 sex      &lt;chr [3]&gt; predictor original\n5 frailty  &lt;chr [3]&gt; predictor original\n6 infected &lt;chr [2]&gt; outcome   original\n\n\nNachdem wir dann unser Rezept definiert haben, können wir auch noch Rollen vergeben. Die Rollen sind nützlich, wenn wir später auf bestimmten Variablen etwas rechnen wollen oder eben nicht. Wir können die Rollen selber definieren und diese Rollen dann auch über die Funktion has_role() ein- oder ausschließen. Neben dieser Möglichkeit gezielt Variablen nach der Rolle anzusprechen, können wir auch alle Prädiktoren oder alle Outcomes auswählen.\nWir haben Funktionen, die die Rolle der Variablen festlegen:\n\nall_predictors() wendet den Schritt nur auf die Prädiktorvariablen an, daher auf die Features.\nall_outcomes() wendet den Schritt nur auf die Outcome-Variable(n) an, daher auf die Label.\n\nUn wir haben Funktionen, die den Typ der Variablen angeben:\n\nall_nominal() wendet den Schritt auf alle Variablen an, die nominal (kategorisch) sind.\nall_numeric() wendet den Schritt auf alle Variablen an, die numerisch sind.\n\nUnd natürlich deren Kombination wie all_nominal_predictors() oder all_numeric_predictors(), die dann eben auf die Prädiktoren, die nominal also Faktoren oder Gruppen repräsentieren oder eben numerischen Variablen, angewendet werden. Du wirst die Anwendung gleich später in den Rezeptschritten sehen, da macht die Sache dann sehr viel mehr Sinn.\nNun ist es aber auch so, dass es bei dem Rezept auf die Reihenfolge der einzelnen Schritte ankommt. Die Reihenfolge der Zutaten und damit der Rezeptschritte sind ja auch beim Kuchenbacken sehr wichtig! Da das Rezept wirklich in der Reihenfolge durchgeführt wird, wie du die einzelnen Schritte angibst, empfiehlt sich folgende Reihenfolge. Du musst natürlich nicht jeden dieser Schritte auch immer durchführen.\n\n\nBitte die Hinweise zur Ordnung der Schritte eines Rezeptes beachten: Ordering of steps\n\nEntfernen von Beobachtungen mit einem fehlenden Eintrag für das Label.\nImputation von fehlenden Werten in den Daten.\nIndividuelle Transformationen auf einzelnen Spalten.\nUmwandeln von einzelnen numerischen Variablen in eine diskrete Variable.\nErstellung der Dummyvariablen für jede diskrete Variable.\nEventuell Berücksichtigung der Interaktion zwischen Variablen.\nTransformation der numerischen Variablen mit zum Beispiel der Standarisierung oder Normalisierung.\nMultivariate Transformationen über alle Spalten hinweg wie zum Beispiel PCA.\n\nAm Ende wollen wir dann natürlich auch die Daten wiederhaben. Das heißt, wir bauen ja das Rezept auf einem Datensatz. Wenn wir dann das fertige Rezept in die Funktion prep() pipen können wir über die Funktion juice() den ursprünglichen jetzt aber transformierten Datensatz wieder erhalten. Wenn wir das Rezept auf einen neuen Datensatz anwenden wollen, dann nutzen wir die Funktion bake(). Mit einem neuen Datensatz meine ich natürlich einen Split in Training- und Testdaten von dem ursprünglichen Datensatz. In dem neuen Datensatz müssen natürlich alle Spaltennamen auch enthalten sein, sonst macht die Sache recht wenig Sinn.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#fehlende-werte-im-y",
    "href": "classification-pre-processing.html#fehlende-werte-im-y",
    "title": "67  Data preprocessing",
    "section": "67.4 Fehlende Werte im \\(Y\\)",
    "text": "67.4 Fehlende Werte im \\(Y\\)\nWenn wir mit maschinellen Lernverfahren rechnen, dann dürfen wir im Outcome \\(Y\\) oder dem Label keine fehlenden Werte vorliegen haben. Das Outcome ist in dem Sinne hielig, dass wir hier keine Werte imputieren. Wir müssen daher alle Zeilen und damit Beobachtungen aus den Daten entfernen in denen ein NA im Outcome vorliegt. Wir können dazu die Funktion drop_na() nutzen. Wir können in der Funktion spezifizieren, dass nur für eine Spalte die NA entfernt werden sollen. In unserem Beispiel für die Ferkeldaten wäre es dann die Spalte infected.\n\ndrop_na(infected)\n\nAktuell haben wir ja keine fehlenden Werte in der Spalte vorliegen, so dass wir die Funktion hier nicht benötigen. In dem Beispiel zu den Gummibärchendaten wollen wir das Geschlecht vorhersagen und hier haben wir dann fehlende Werte im Outcome. Mit der Funktion drop_na(gender) entfernen wir dann alle Beobachtungen aus den Daten mit einem fehlenden Eintrag für das Geschlecht.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-dummy",
    "href": "classification-pre-processing.html#sec-preprocess-dummy",
    "title": "67  Data preprocessing",
    "section": "67.5 Dummycodierung von \\(X\\)",
    "text": "67.5 Dummycodierung von \\(X\\)\nWir werden immer häufiger davon sprechen, dass wir alle kategorialen Daten in Dummies überführen müssen. Das heißt, wir dürfen keine Faktoren mehr in unseren Daten haben. Wir wandeln daher alle Variablen, die ein Faktor sind, in Dummyspalten um. Die Idee von der Dummyspalte ist die gleiche wie bei der multiplen Regression. Da ich aber nicht davon ausgehe, dass du dir alles hier durchgelesen hast, kommt hier die kurze Einführung zur Dummycodierung.\n\n\nMehr Information zu Create Traditional Dummy Variables\nDie Dummycodierung wird nur auf den Features durchgeführt. Dabei werden nur Spalten erschaffen, die \\(0/1\\), für Level vorhanden oder Level nicht vorhanden, beinhalten. Wir werden also nur alle \\(x\\) in Dummies umwandeln, die einem Faktor entsprechen. Dafür nutzen wir dann später eine Funktion, hier machen wir das einmal zu Veranschaulichung per Hand. In Tabelle 67.2 haben wir einen kleinen Ausschnitt unser Schweinedaten gegeben. Wir wollen zuerst die Spalte sex in eine Dummycodierung umwandeln.\n\n\n\nTabelle 67.2— Beispieldatensatz für die Dummycodierung. Wir wollen die Spalten sex und frailty als Dummyspalten haben.\n\n\n\n\n\ninfected\nage\nsex\nfrailty\n\n\n\n\n1\n24\nmale\nrobust\n\n\n0\n36\nmale\npre-frail\n\n\n0\n21\nfemale\nfrail\n\n\n1\n34\nfemale\nrobust\n\n\n1\n27\nmale\nfrail\n\n\n\n\n\n\nIn der Tabelle 67.3 sehen wir das Ergebnis für die Dummycodierung der Spalte sex in die Dummyspalte sex_male. Wir haben in der Dummyspalte nur noch die Information, ob das Ferkel mänlich ist oder nicht. Wenn wir eine Eins in der Spalte finden, dann ist das Ferkel männlich. Wenn wir eine Null vorfinden, dann ist das Ferkel nicht männlich also weiblich. Das Nicht müssen wir uns dann immer merken.\n\n\n\nTabelle 67.3— Ergebnis der Dummycodierung der Spalte sex zu der Spalte sex_male.\n\n\n\n\n\ninfected\nage\nsex_male\n\n\n\n\n1\n24\n1\n\n\n0\n36\n1\n\n\n0\n21\n0\n\n\n1\n34\n0\n\n\n1\n27\n1\n\n\n\n\n\n\nIn der Tabelle 67.4 betrachten wir einen komplexeren Fall. Wenn wir eine Spalte vorliegen haben mit mehr als zwei Leveln, wie zum Beispiel die Spalte frailty, dann erhalten wir zwei Spalten wieder. Die Spalte frailty_robust beschreibt das Vorhandensein des Levels robust und die Spalte frailty_pre-frail das Vorhandensein des Levels pre-frail. Und was ist mit dem Level frail? Das Level wir durch das Nichtvorhandensein von robust und dem Nichtvorhandensein von pre-frail abgebildet. Beinhalten beide Spalten die Null, so ist das Ferkel frail.\n\n\n\nTabelle 67.4— Ergebnis der Dummycodierung für eine Spalte mit mehr als zwei Leveln.\n\n\n\n\n\ninfected\nage\nfrailty_robust\nfrailty_pre-frail\n\n\n\n\n1\n24\n1\n0\n\n\n0\n36\n0\n1\n\n\n0\n21\n0\n0\n\n\n1\n34\n1\n0\n\n\n1\n27\n0\n0\n\n\n\n\n\n\nWenn wir einen Faktor mit \\(l\\) Leveln haben, erhalten wir immer \\(l-1\\) Spalten nach der Dummycodierung wieder.\nWir nutzen dann die Funktion step_dummy() um eine Dummaycodierung für alle nominalen Prädiktoren spezifiziert durch all_nominal_predictors() durchzuführen. Das tolle ist hier, dass wir durch die Helferfunktionen immer genau sagen können welche Typen von Spalten bearbeitet werden sollen.\n\npig_dummy_rec &lt;- pig_rec |&gt; \n  step_dummy(all_nominal_predictors()) \n\npig_dummy_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n\nWenn wir das Rezept fertig haben, dann können wir uns die Daten einmal anschauen. Durch die Funktion prep() initialisieren wir das Rezept und mit der Funktion juice() teilen wir mit, dass wir das Rezept gleich auf die Trainingsdaten mit denen wir das Rezept gebaut haben, anweden wollen.\n\npig_dummy_rec |&gt;\n  prep() |&gt;\n  juice() \n\n# A tibble: 412 × 7\n   pig_id   age   crp infected sex_male frailty_pre.frail frailty_robust\n    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1      1    61  22.4        1        1                 0              1\n 2      2    53  18.6        1        1                 0              1\n 3      3    66  18.8        0        0                 0              1\n 4      4    59  19.4        1        0                 0              1\n 5      5    63  21.6        1        1                 0              1\n 6      6    55  21.4        1        1                 0              1\n 7      7    49  19.0        1        1                 1              0\n 8      8    53  19.0        0        1                 0              1\n 9      9    58  21.9        1        0                 0              1\n10     10    57  21.0        1        1                 0              1\n# ℹ 402 more rows\n\n\nDie Dummycodierung verwandelt alle nominalen Spalten in mehrere \\(0/1\\) Spalten um. Das ermöglicht den Algorithmen auch mit nominalen Spalten eine Vorhersage zu machen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-zv",
    "href": "classification-pre-processing.html#sec-preprocess-zv",
    "title": "67  Data preprocessing",
    "section": "67.6 Zero Variance Spalten",
    "text": "67.6 Zero Variance Spalten\nEin häufiges Problem ist, dass wir manchmal Spalten in unseren Daten haben in denen nur ein Eintrag steht. Das heißt wir haben überall die gleiche Zahl oder eben das gelche Wort stehen. Das tritt häufiger auf, wenn wir uns riesige Datenmengen von extern herunterladen. Manchmal haben wir so viele Spalten, dass wir die Daten gr nicht richtig überblicken. Oder aber, wir haben nach einer Transformation nur noch die gleiche Zahl. Dagegen können wir filtern.\n\n\nMehr Information zu Zero Variance Filter und Near-Zero Variance Filter\nWir haben die Auswahl zwischen step_zv(), die Funktion entfernt Spalten mit einer Vaianz von Null. Das mag seltener vorkommen, als eine sehr kleine Varianz. Hier hilft die Funktion step_nzv(). Wir können beide Funktionen auf alle Arten von Prädiktoren anwenden, nur eben nicht gleichzeitig.\n\npig_zero_rec &lt;- pig_rec |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_nzv(all_predictors())\n\npig_zero_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_predictors()\n\n\nDa wir in unseren Daten mit den infizierten Ferkeln jetzt keine Spalten mit einer sehr kleinen Varianz haben, passiert auch nichts, wenn wir die Funktion auf unsere Daten anwenden würden. Demensprechend sparen wir uns an dieser Stelle auch die Datengenerierung.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-standard",
    "href": "classification-pre-processing.html#sec-preprocess-standard",
    "title": "67  Data preprocessing",
    "section": "67.7 Standardisieren \\(\\mathcal{N}(0,1)\\)",
    "text": "67.7 Standardisieren \\(\\mathcal{N}(0,1)\\)\nIn dem Kapitel zu der Transformation von Daten haben wir ja schon von der Standardisierung gelesen und uns mit den gängigen Funktion beschäftigt. Deshalb hier nur kurz die Schritte und Funktionen, die wir mit den Rezepten machen können. Zum einen können wir nur die Daten mit der Funktion step_scale() skalieren, dass heißt auf eine Standardabweichung von 1 bringen. Oder aber zum anderen nutzen wir die Funktion scale_center() um die Daten alle auf einen Mittelwert von 0 zu schieben. Manchmal wollen wir nur den einen Schritt getrennt von dem anderen Schritt durchführen. Beide Schritte können wir dann einfach auf allen numerischen Prädiktoren durchführen.\n\n\nMehr Information zu Scaling Numeric Data sowie Centering Numeric Data und Center and Scale Numeric Data\n\npig_scale_center_rec &lt;- pig_rec |&gt; \n  step_center(all_numeric_predictors()) |&gt; \n  step_scale(all_numeric_predictors()) \n\npig_scale_center_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Centering for: all_numeric_predictors()\n\n\n• Scaling for: all_numeric_predictors()\n\n\nWenn wir aber auf eine getrennte Durchführung keine Lust haben, gibt es auch die etwas schief benannte Funktion step_normalize(), die beide Schritte kombiniert und uns damit die Daten auf eine Standardnormalverteilung transformiert. Ich persönlich nutze dann meist die zweite Variante, dann hat man alles in einem Schritt zusammen. Das hängt aber sehr vom Anwendungsfall ab und du musst dann schauen, was besser für dich und deine Daten dann passt.\n\npig_scale_center_rec &lt;- pig_rec |&gt; \n  step_normalize(all_numeric_predictors()) \n\npig_scale_center_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\nJetzt können wir noch die Daten generieren und sehen, dass wir alle numerischen Spalten in eine Standardnormalverteilung transformiert haben. Wir runden hier nochmal alle numerischen Variablen, damit wir nicht so einen breiten Datensatz erhalten. Das hat jetzt aber eher was mit der Ausgabe hier auf der Webseite zu tun. Wir müssen nicht runden um die Daten dann zu verwenden.\n\npig_scale_center_rec |&gt;\n  prep() |&gt;\n  juice() |&gt; \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 412 × 6\n   pig_id   age   crp sex    frailty   infected\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt;\n 1      1  0.22  1.62 male   robust           1\n 2      2 -1.55 -0.99 male   robust           1\n 3      3  1.32 -0.91 female robust           0\n 4      4 -0.23 -0.48 female robust           1\n 5      5  0.66  1.05 male   robust           1\n 6      6 -1.11  0.97 male   robust           1\n 7      7 -2.44 -0.76 male   pre-frail        1\n 8      8 -1.55 -0.76 male   robust           0\n 9      9 -0.45  1.27 female robust           1\n10     10 -0.67  0.62 male   robust           1\n# ℹ 402 more rows",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-normal",
    "href": "classification-pre-processing.html#sec-preprocess-normal",
    "title": "67  Data preprocessing",
    "section": "67.8 Normalisieren \\([0; 1]\\)",
    "text": "67.8 Normalisieren \\([0; 1]\\)\nAuch bei der Normalisierung möchte ich wieder auf das Kapitel zum Transformation von Daten verweisen. In dem tidymodels Universum heißt dann das Normalisieren, also die Daten auf eine Spannweite zwischen 0 und 1 bringen, dann eben step_range(). Das ist natürlich dann schön generalisiert. Wir könnten uns auch andere Spannweiten überlegen, aber hier nehmen wir natürlich immer den Klassiker auf eine Spannweite \\([0; 1]\\). Unsere Daten liegen dann nach der Normalisierung mit der Funktion step_range() zwischen 0 und 1. Wir können die Normalisierung natürlich nur auf numerischen Variablen durchführen.\n\n\nMehr Information zu Scaling Numeric Data to a Specific Range\n\npig_range_rec &lt;- pig_rec |&gt; \n  step_range(all_numeric_predictors(), min = 0, max = 1) \n\npig_range_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Range scaling to [0,1] for: all_numeric_predictors()\n\n\nAuch hier können wir dann die Daten generieren und uns einmal anschauen. Im Gegensatz zu der Standardisierung treten jetzt in unseren Spalten keine negativen Werte mehr auf. Wir runden hier ebenfalls nochmal alle numerischen Variablen, damit wir nicht so einen breiten Datensatz erhalten. Das hat jetzt aber eher was mit der Ausgabe hier auf der Webseite zu tun. Wir müssen nicht runden um die Daten dann zu verwenden.\n\npig_range_rec |&gt;\n  prep() |&gt;\n  juice() |&gt; \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 412 × 6\n   pig_id   age   crp sex    frailty   infected\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt;\n 1      1  0.52  0.82 male   robust           1\n 2      2  0.17  0.34 male   robust           1\n 3      3  0.74  0.36 female robust           0\n 4      4  0.43  0.44 female robust           1\n 5      5  0.61  0.72 male   robust           1\n 6      6  0.26  0.7  male   robust           1\n 7      7  0     0.39 male   pre-frail        1\n 8      8  0.17  0.39 male   robust           0\n 9      9  0.39  0.76 female robust           1\n10     10  0.35  0.64 male   robust           1\n# ℹ 402 more rows",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-impute",
    "href": "classification-pre-processing.html#sec-preprocess-impute",
    "title": "67  Data preprocessing",
    "section": "67.9 Imputieren von fehlenden Werten",
    "text": "67.9 Imputieren von fehlenden Werten\nIn dem Kapitel zur Imputation von fehlenden Werten haben wir uns mit verschiedenen Methoden zur Imputation von fehlenden Werten beschäftigt. Auch gibt es verschiedene Rezepte um die Imputation durchzuführen. Wir haben also wieder die Qual der Wahl welchen Algorithmus wir nutzen wollen. Da wir wieder zwischen numerischen und nominalen Variablen unterscheiden müssen, haben wir immer zwei Imputationsschritte. Ich mache es mir hier sehr leicht und wähle die mean Imputation für die numerischen Variablen aus und die mode Imputation für die nominalen Variablen. Das sind natürlich die beiden simpelsten Imputation die gehen. Ich würde dir empfehlen nochmal die Alternativen anzuschauen und vorab auf jeden Fall nochmal dir die fehlenden Daten zu visualisieren. Es macht auch hier keinen Sinn nicht vorhandene Spalten mit künstlichen Daten zu füllen.\n\n\nMehr Information zu Step Functions - Imputation\nDa wir es uns in diesem Schritt sehr einfach machen, nutzen wir die Funktionen step_impute_mean() auf allen numerischen Variablen und die Funktion step_impute_mode() auf alle nominalen Variablen. Es geht wie immer natürlich besser, das heißt auch komplexerer. Hier ist es auch wieder schwierig zu sagen, welche Methode die beste Methode zur Imputation von fehlenden Werten ist. Hier hilft es dann nichts, du musst dir die imputierten Daten anschauen.\n\npig_imp_rec &lt;- pig_rec |&gt; \n  step_impute_mean(all_numeric_predictors()) |&gt; \n  step_impute_mode(all_nominal_predictors())\n\npig_imp_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Mode imputation for: all_nominal_predictors()\n\n\nDann können wir uns auch schon die Daten generieren. Wir sehen, dass wir keine fehlenden Werte mehr in unseren Daten vorliegen haben. Wie immer können wir uns die gerundeten Daten dann einmal anschauen.\n\npig_imp_rec  |&gt;\n  prep() |&gt;\n  juice() |&gt; \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 412 × 6\n   pig_id   age   crp sex    frailty   infected\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt;\n 1      1    61  22.4 male   robust           1\n 2      2    53  18.6 male   robust           1\n 3      3    66  18.8 female robust           0\n 4      4    59  19.4 female robust           1\n 5      5    63  21.6 male   robust           1\n 6      6    55  21.4 male   robust           1\n 7      7    49  19.0 male   pre-frail        1\n 8      8    53  19.0 male   robust           0\n 9      9    58  21.9 female robust           1\n10     10    57  21.0 male   robust           1\n# ℹ 402 more rows\n\n\nDie Imputationrezepte bieten sich natürlich auch für die ganz normale Statistik an. Du kannst ja dann mit den imputierten Daten rechnen was du möchtest. Wir nutzen die Daten hier ja nur im Kontext der Klassifikation. Es gingt natürlich auch die Daten für die lineare Regression zu nutzen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-discrete",
    "href": "classification-pre-processing.html#sec-preprocess-discrete",
    "title": "67  Data preprocessing",
    "section": "67.10 Kategorisierung",
    "text": "67.10 Kategorisierung\nManchmal wollen wir nicht mit numerischen Variablen arbeiten sondern uns nominale Variablen erschaffen. Das sollten wir eigentlich nicht so häufig tun, denn die numerischen Variablen haben meist mehr Informationen als nominale Variablen. Wir müssen dann ja unsere nominalen Daten dann wieder in Dummies umkodieren. Das sind dann zwei zusätzliche Schritte. Aber wie immer in der Datenanalyse, es gibt Fälle in denen es Sinn macht und wir eben keine numerischen Variablen haben wollen. Dann können wir eben die Funktion step_discretize() nutzen um verschiedene Gruppen oder bins (eng. Dosen) zu bilden. Das R Paket {embed} bietet noch eine Vielzahl an weiteren Funktionen für die Erstellung von kategorialen Variablen.\nEs kann natürlich sinnvoll sein aus einer numerischen Outcomevariablen eine binäre Outcomevariable zu erzeugen. Dann können wir wieder eine Klassifikation rechnen. Aber auch hier musst du überlegen, ob das binäre Outcome dann dem numerischen Outcome inhaltlich entspricht. Wir können natürlich aus dem numerischen Lichteinfall die binäre Variable wenig/viel Licht transformieren. Dann muss die neue binäre Variable aber auch zur Fragestellung passen. Oder aus Noten auf der Likert-Skala nur zwei Noten mit schlecht/gut erschaffen.\n\n\nMehr Information zu Step Functions - Discretization\nWir wollen jetzt die Spalten age und crp in mindestens drei gleich große Gruppen aufspalten. Wenn wir mehr Gruppen brauchen, dann werden es mehr Gruppen werden. Das wichtige ist hier, dass wir gleich große Gruppen haben wollen.\n\npig_discrete_rec &lt;- pig_rec |&gt;\n  step_discretize(crp, age, min_unique = 3)\n\npig_discrete_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Discretize numeric variables from: crp and age\n\n\nUnd dann können wir uns auch schon die Daten generieren. Wir immer gibt es noch andere Möglichkeiten um aus einer numerischen Spalte eine nominale Spalte zu generieren. Du musst dann abgleichen, welche Variante dir am besten passt.\n\npig_discrete_tbl &lt;- pig_discrete_rec  |&gt;\n  prep() |&gt;\n  juice() \n\nWarning: Note that the options `prefix` and `labels` will be applied to all\nvariables.\n\n\nWir sehen, dass wir dann jeweils vier bins erhalten mit gut 25% Beobachtungen in jedem bin. Wir können dann mit der neuen Variable weiterrechnen und zum Beispiel diese neue nominale Variable dann in eine Dummykodierung umwandeln. Hier siehst du, dass du gewisse Schritte in einem Rezept in der richtigen Reihenfolge durchführen musst.\n\npig_discrete_tbl |&gt; pull(crp) |&gt; tabyl()\n\n pull(pig_discrete_tbl, crp)   n   percent\n                        bin1 104 0.2524272\n                        bin2 102 0.2475728\n                        bin3 103 0.2500000\n                        bin4 103 0.2500000\n\npig_discrete_tbl |&gt; pull(age) |&gt; tabyl()\n\n pull(pig_discrete_tbl, age)   n   percent\n                        bin1 122 0.2961165\n                        bin2 103 0.2500000\n                        bin3  99 0.2402913\n                        bin4  88 0.2135922",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-corr",
    "href": "classification-pre-processing.html#sec-preprocess-corr",
    "title": "67  Data preprocessing",
    "section": "67.11 Korrelation zwischen Variablen",
    "text": "67.11 Korrelation zwischen Variablen\nAls einer der letzten Schritte für die Aufreinigung der Daten schauen wir uns die Korrelation an. Du kannst dir die Korrelation im Kapitel Kapitel 40 nochmal näher anlesen. Wie schon bei der Imputation kann ich nur davon abraten einfach so den Filter auf die Daten anzuwenden. Es ist besser sich die numerischen Variablen einmal zu visualisieren und die Korrelation einmal zu berechnen. Das blinde Filtern von Variablen macht auf jeden Fall keinen Sinn!\n\n\nMehr Information zu High Correlation Filter\nIn der Klassifikation müssen wir schauen, dass wir keine numerischen Variablen haben, die im Prinzip das gleiche Aussagen also hoch miteinander korreliert sind. Die Variablen müssen wir dann entfernen. Oder besser eine von den beiden Variablen. Wir können den Schritt mit der Funktion step_corr() durchführen und einen Threshold für die Entfernung von numerischen Variablen festlegen. Wir nehmen hier ein \\(\\rho = 0.5\\). Nochmal, das ist nicht sehr gut blind Vairablen zu entfernen. Schaue dir vorher einen paarweisen Korrelationsplot an und entscheide dann, ob du und welche Variablen du entfernen möchtest.\n\npig_corr_rec &lt;- pig_rec |&gt; \n  step_corr(all_numeric_predictors(), threshold = 0.5)\n\npig_corr_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Correlation filter on: all_numeric_predictors()\n\n\nDann können wir auch schon die Daten generieren. In unserem Fall wurde keine Variable entfernt. Die Korrelation untereinander ist nicht so groß. Wir runden hier wieder, damit sich die Tabelle nicht so in die Breite auf der Webseite entwickelt.\n\npig_corr_tbl &lt;- pig_corr_rec  |&gt;\n  prep() |&gt;\n  juice() |&gt; \n  mutate(across(where(is.numeric), round, 2))",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-pre-processing.html#beispiel-gummibärchendaten",
    "href": "classification-pre-processing.html#beispiel-gummibärchendaten",
    "title": "67  Data preprocessing",
    "section": "67.12 Beispiel Gummibärchendaten",
    "text": "67.12 Beispiel Gummibärchendaten\nSchauen wir uns ein Rezept einmal in einem Rutsch auf den Gummibärchendaten einmal an. Wir müssen natürlich erstmal alle nominalen Variablen auch als solche umwandeln. Wir erschaffen also die passenden Faktoren für das Geschlecht und den Lieblingsgeschmack. Dann erschaffen wir noch eine ID für die Studierenden. Am Ende wählen wir noch ein paar Spalten aus, damit wir nicht alle Variablen vorliegen haben. Sonst wird der endgültige Datensatz sehr breit. Wir entfernen dann noch alle Beobachtungen aus den Daten, die einen fehlenden Wert bei dem Geschlecht haben. Das machen wir immer für die Variable, die dann unser Outcome sein soll.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") |&gt; \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked),\n         student_id = 1:n()) |&gt; \n  select(student_id, gender, most_liked, age, semester, height) |&gt;  \n  drop_na(gender)\n\nIn Tabelle 67.5 sehen wir dann die Daten nochmal vor dem Preprocessing dargestellt. Wir sind nicht an den ursprünglichen Daten interessiert, da wir nur die Spalte gender vorhersagen wollen. Wir wollen hier keine Effekt schätzen oder aber Signifikanzen berechnen. Unsere Features dienen nur der Vorhersage des Labels. Wie die Features zahlenmäßig beschaffen sind, ist uns egal.\n\n\n\n\nTabelle 67.5— Auszug aus dem Daten zu den Gummibärchendaten.\n\n\n\n\n\n\nstudent_id\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\n\n1\nm\nlightred\n35\n10\n193\n\n\n2\nw\nyellow\n21\n6\n159\n\n\n3\nw\nwhite\n21\n6\n159\n\n\n4\nw\nwhite\n36\n10\n180\n\n\n5\nm\nwhite\n22\n3\n180\n\n\n7\nm\ngreen\n22\n3\n180\n\n\n…\n…\n…\n…\n…\n…\n\n\n778\nm\ndarkred\n24\n2\n193\n\n\n779\nm\nwhite\n27\n2\n189\n\n\n780\nm\ndarkred\n24\n2\n187\n\n\n781\nm\ngreen\n24\n2\n182\n\n\n782\nw\nwhite\n23\n2\n170\n\n\n783\nw\ngreen\n24\n2\n180\n\n\n\n\n\n\n\n\nWir erschaffen uns nun das Rezept in dem wie definieren, dass das gender unser Label ist und der Rest der Vairablen unsere Features. Da wir noch die Spalte student_id haben, geben wir dieser Spalte noch die Rolle ID. Wir können dann in den Rezeptschritten dann immer diese Rolle ID aus dem Prozess der Transformation ausschließen.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_tbl) |&gt; \n  update_role(student_id, new_role = \"ID\")\n\ngummi_rec |&gt; summary()\n\n# A tibble: 6 × 4\n  variable   type      role      source  \n  &lt;chr&gt;      &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 student_id &lt;chr [2]&gt; ID        original\n2 most_liked &lt;chr [3]&gt; predictor original\n3 age        &lt;chr [2]&gt; predictor original\n4 semester   &lt;chr [2]&gt; predictor original\n5 height     &lt;chr [2]&gt; predictor original\n6 gender     &lt;chr [3]&gt; outcome   original\n\n\nUnd dann haben wir hier alle Schritte einmal zusammen in einem Block. Wir imputieren die fehlenden Werte für die numerischen und nominalen Variablen getrennt. Dann verwandeln wir das Semester in mindestens vier Gruppen. Im nächsten Schritt werden dann alle numerischen Variablen auf eine Spannweite von \\([0;1]\\) gebracht. Wir erschaffen dann noch die Dummies für die nominalen Daten. Am Ende wollen wir dann alle Variablen mit fast keiner Varianz entfernen. Wir wollen dann immer die Spalte ID aus den Schritten ausschließen. Wir machen das mit der Funktion has_role() und dem - vor der Funktion. Damit schließen wir die Rolle ID aus dem Transformationsschritt aus.\n\ngummi_full_rec &lt;- gummi_rec |&gt; \n  step_impute_mean(all_numeric_predictors(), -has_role(\"ID\")) |&gt; \n  step_impute_bag(all_nominal_predictors(), -has_role(\"ID\")) |&gt; \n  step_discretize(semester, num_breaks = 3, min_unique = 4) |&gt; \n  step_range(all_numeric_predictors(), min = 0, max = 1, -has_role(\"ID\")) |&gt; \n  step_dummy(all_nominal_predictors(), -has_role(\"ID\")) |&gt; \n  step_nzv(all_predictors(), -has_role(\"ID\"))\n\ngummi_full_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors() and -has_role(\"ID\")\n\n\n• Bagged tree imputation for: all_nominal_predictors() and -has_role(\"ID\")\n\n\n• Discretize numeric variables from: semester\n\n\n• Range scaling to [0,1] for: all_numeric_predictors() and -has_role(\"ID\")\n\n\n• Dummy variables from: all_nominal_predictors() and -has_role(\"ID\")\n\n\n• Sparse, unbalanced variable filter on: all_predictors() and -has_role(\"ID\")\n\n\nDann können wir wieder unsere Daten generieren. Ich runde hier wieder, da wir schnell sehr viele Kommastellen produzieren. In der Anwendung machen wir das natürlich dann nicht.\n\ngummi_class_tbl &lt;- gummi_full_rec |&gt;\n  prep() |&gt;\n  juice() |&gt; \n  mutate(across(where(is.numeric), round, 2)) \n\nIn der Tabelle 67.6 können wir uns die transformierten Daten einmal anschauen. Wir sehen das zum einen die Variable student_id nicht transformiert wurde. Alle numerischen Spalten sind auf einer Spannweite zwischen 0 und 1. Das Geschlecht wurde nicht transformiert, da wir das Geschlecht ja als Outcome festgelegt haben. Dann kommen die Dummykodierungen für die nominalen Spalten des Lieblingsgeschmack und des Semesters.\n\n\n\n\nTabelle 67.6— Der transformierte Gummibärchendatensatz nach der Anwendung des Rezepts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudent_id\nage\nheight\ngender\nmost_liked_white\nmost_liked_green\nmost_liked_darkred\nmost_liked_none\nsemester_bin2\nsemester_bin3\n\n\n\n\n1\n0.48\n0.79\nm\n0\n0\n0\n0\n0\n1\n\n\n2\n0.2\n0.21\nw\n0\n0\n0\n0\n0\n1\n\n\n3\n0.2\n0.21\nw\n1\n0\n0\n0\n0\n1\n\n\n4\n0.5\n0.57\nw\n1\n0\n0\n0\n0\n1\n\n\n5\n0.22\n0.57\nm\n1\n0\n0\n0\n1\n0\n\n\n6\n0.22\n0.57\nm\n0\n1\n0\n0\n1\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n694\n0.26\n0.79\nm\n0\n0\n1\n0\n1\n0\n\n\n695\n0.32\n0.72\nm\n1\n0\n0\n0\n1\n0\n\n\n696\n0.26\n0.69\nm\n0\n0\n1\n0\n1\n0\n\n\n697\n0.26\n0.6\nm\n0\n1\n0\n0\n1\n0\n\n\n698\n0.24\n0.4\nw\n1\n0\n0\n0\n1\n0\n\n\n699\n0.26\n0.57\nw\n0\n1\n0\n0\n1\n0\n\n\n\n\n\n\n\n\nBis hierher haben wir jetzt die Rezepte nur genutzt um uns die Daten aufzuarbeiten. Das ist eigentlich nur ein Schritt in der Klassifikation. Mit der Funktion workflow() können wir dann Rezepte mit Algorithmen verbinden. Dann nutzen wir die Funktion fit() um verschiedene Daten auf den Workflow anzuwenden. Das musst du aber nicht tun. Du kannst die Rezepte hier auch verwenden um deine Daten einfach aufzuarbeiten und dann eben doch ganz normale Statistik drauf zu rechnen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "classification-model-compare.html",
    "href": "classification-model-compare.html",
    "title": "68  Vergleich von Algorithmen",
    "section": "",
    "text": "68.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor, xgboost, ranger, kknn,\n               see, conflicted)\nconflicts_prefer(magrittr::set_names)\n##\nset.seed(20234534)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Vergleich von Algorithmen</span>"
    ]
  },
  {
    "objectID": "classification-model-compare.html#daten",
    "href": "classification-model-compare.html#daten",
    "title": "68  Vergleich von Algorithmen",
    "section": "68.2 Daten",
    "text": "68.2 Daten\nIn diesem Kapitel wolle wir uns aber mal auf einen echten Datensatz anschauen und sehen wie sich drei Algorithmen auf diesem Daten so schlagen. Welcher Algorithmus ist am besten für die Klassifikation geeignet? Wir nutzen daher hier einmal als echten Datensatz den Gummibärchendatensatz. Als unser Label nehmen wir das Geschlecht gender. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir als Prädiktoren die Spalten most_liked, age, semester, und height mit in unsere Analysedaten.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") |&gt; \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) |&gt; \n  select(gender, most_liked, age, semester, height) |&gt; \n  drop_na(gender)\n\nWir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein NA in der Variable gender haben. Wir haben dann insgesamt \\(n = 699\\) Beobachtungen vorliegen. In Tabelle 72.3 sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.\n\n\n\n\nTabelle 68.1— Auszug aus dem Daten zu den Gummibärchendaten.\n\n\n\n\n\n\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\n\nm\nlightred\n35\n10\n193\n\n\nw\nyellow\n21\n6\n159\n\n\nw\nwhite\n21\n6\n159\n\n\nw\nwhite\n36\n10\n180\n\n\nm\nwhite\n22\n3\n180\n\n\nm\ngreen\n22\n3\n180\n\n\n…\n…\n…\n…\n…\n\n\nm\ndarkred\n24\n2\n193\n\n\nm\nwhite\n27\n2\n189\n\n\nm\ndarkred\n24\n2\n187\n\n\nm\ngreen\n24\n2\n182\n\n\nw\nwhite\n23\n2\n170\n\n\nw\ngreen\n24\n2\n180\n\n\n\n\n\n\n\n\nUnsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.\n\ngummi_data_split &lt;- initial_split(gummi_tbl, prop = 3/4)\n\nWir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluß einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.\n\ngummi_train_data &lt;- training(gummi_data_split)\ngummi_test_data  &lt;- testing(gummi_data_split)\n\nNachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht gender als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem . Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_train_data) |&gt; \n  step_impute_mean(all_numeric_predictors()) |&gt; \n  step_impute_bag(all_nominal_predictors()) |&gt; \n  step_range(all_numeric_predictors(), min = 0, max = 1) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_nzv(all_predictors())\n\ngummi_rec |&gt; summary()\n\n# A tibble: 5 × 4\n  variable   type      role      source  \n  &lt;chr&gt;      &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 most_liked &lt;chr [3]&gt; predictor original\n2 age        &lt;chr [2]&gt; predictor original\n3 semester   &lt;chr [2]&gt; predictor original\n4 height     &lt;chr [2]&gt; predictor original\n5 gender     &lt;chr [3]&gt; outcome   original\n\n\nIm Folgenden vergleichen wir einmal drei Algorithmen miteinander. Daher halten wir den Code für die Durchführung sehr kurz.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Vergleich von Algorithmen</span>"
    ]
  },
  {
    "objectID": "classification-model-compare.html#k-nn-algorithm",
    "href": "classification-model-compare.html#k-nn-algorithm",
    "title": "68  Vergleich von Algorithmen",
    "section": "68.3 \\(k\\)-NN Algorithm",
    "text": "68.3 \\(k\\)-NN Algorithm\n\n\n\n\n\n\nHuch, der Code ist aber sehr kurz…\n\n\n\nIn diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den \\(k\\)-NN Algorithmus wissen willst, schaue bitte in Kapitel 69.\n\n\nFür den \\(k\\)-NN Algorithmus nutzen wir \\(k=11\\) Nachbarn. Mehr brauchen wir hier nicht angeben.\n\nknn_mod &lt;- nearest_neighbor(neighbors = 11) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") \n\nDann nehmen wir das Modell für den \\(k\\)-NN Algorithmus und verbinden das Modell mit dem Rezept für die Gummibärchendaten in einem Workflow.\n\nknn_wflow &lt;- workflow() |&gt; \n  add_model(knn_mod) |&gt; \n  add_recipe(gummi_rec)\n\nNun können wir auch schon den Fit des Modells rechnen und in einem Rutsch den Fit auch gleich auf die Testdaten anwenden.\n\nknn_aug &lt;- knn_wflow |&gt; \n  parsnip::fit(gummi_train_data) |&gt; \n   augment(gummi_test_data)\n\nMehr wollen wir hier auch nicht. Wir brauchen nur die Prädiktion, da wir hier ja nur das Konzept der Modellvergleiche einmal durchgehen wollen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Vergleich von Algorithmen</span>"
    ]
  },
  {
    "objectID": "classification-model-compare.html#random-forest",
    "href": "classification-model-compare.html#random-forest",
    "title": "68  Vergleich von Algorithmen",
    "section": "68.4 Random Forest",
    "text": "68.4 Random Forest\n\n\n\n\n\n\nHuch, der Code ist aber sehr kurz…\n\n\n\nIn diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den Random Forest Algorithmus wissen willst, schaue bitte in Kapitel 70.4.\n\n\nFür den Random Forest Algorithmus nutzen wir drei Variablen je Baum (mtry = 3), mindestens zehn Beobachtungen je Knoten (min_n = 10) sowie eintausend gewachsene Bäume in unserem Wald (trees = 1000). Mehr brauchen wir hier nicht angeben.\n\nranger_mod &lt;- rand_forest(mtry = 3, min_n = 10, trees = 1000) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"classification\")\n\nDann nehmen wir das Modell für den Random Forest Algorithmus und verbinden das Modell mit dem Rezept für die Gummibärchendaten in einem Workflow.\n\nranger_wflow &lt;- workflow() |&gt; \n  add_model(ranger_mod) |&gt; \n  add_recipe(gummi_rec)\n\nNun können wir auch schon den Fit des Modells rechnen und in einem Rutsch den Fit auch gleich auf die Testdaten anwenden.\n\nranger_aug &lt;- ranger_wflow |&gt; \n  parsnip::fit(gummi_train_data) |&gt; \n  augment(gummi_test_data ) \n\nMehr wollen wir hier auch nicht von dem Random Forest Algorithmus. Wir brauchen nur die Prädiktion, da wir hier ja nur das Konzept der Modellvergleiche einmal durchgehen wollen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Vergleich von Algorithmen</span>"
    ]
  },
  {
    "objectID": "classification-model-compare.html#xgboost",
    "href": "classification-model-compare.html#xgboost",
    "title": "68  Vergleich von Algorithmen",
    "section": "68.5 xgboost",
    "text": "68.5 xgboost\n\n\n\n\n\n\nHuch, der Code ist aber sehr kurz…\n\n\n\nIn diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den xgboost Algorithmus wissen willst, schaue bitte in Kapitel 70.5.\n\n\nFür den xgboost Algorithmus nutzen wir drei Variablen je Baum (mtry = 3), mindestens zehn Beobachtungen je Knoten (min_n = 10) sowie eintausend gewachsene Bäume in unserem Wald (trees = 1000). Mehr brauchen wir hier nicht angeben.\n\nxgboost_mod &lt;- boost_tree(mtry = 3, min_n = 10, trees = 1000) |&gt; \n  set_engine(\"xgboost\") |&gt; \n  set_mode(\"classification\")\n\nDann nehmen wir das Modell für den xgboost Algorithmus und verbinden das Modell mit dem Rezept für die Gummibärchendaten in einem Workflow.\n\nxgboost_wflow &lt;- workflow() |&gt; \n  add_model(xgboost_mod) |&gt; \n  add_recipe(gummi_rec)\n\nNun können wir auch schon den Fit des Modells rechnen und in einem Rutsch den Fit auch gleich auf die Testdaten anwenden.\n\nxgboost_aug &lt;- xgboost_wflow |&gt; \n  parsnip::fit(gummi_train_data) |&gt; \n  augment(gummi_test_data ) \n\nDas war jetzt der dritte und letzte Algorithmus. Wir brauchen auch hier nur die Prädiktion, da wir hier ja nur das Konzept der Modellvergleiche einmal durchgehen wollen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Vergleich von Algorithmen</span>"
    ]
  },
  {
    "objectID": "classification-model-compare.html#sec-class-model-compare",
    "href": "classification-model-compare.html#sec-class-model-compare",
    "title": "68  Vergleich von Algorithmen",
    "section": "68.6 Vergleich der Modelle",
    "text": "68.6 Vergleich der Modelle\nIn der folgenden Liste haben wir einmal alle vorhergesagten Werte der drei Algorithmen zusammengefügt. Wir können jetzt auf der Liste aug_lst mit der Funktion map() aus dem R Paket {purrr} schnell rechnen. Anstatt für jedes der Objekte in der Liste einzeln den Code anzugeben, können wir den Code über die Funktion map() bündeln.\n\naug_lst &lt;- lst(knn = knn_aug,\n               rf = ranger_aug,\n               xgboost = xgboost_aug)\n\nIm folgenden Schritt berechnen wir für alle Algorithmen die Konfusionsmatrix als eine 2x2 Tabelle. Wir schauen uns gleich einmal die Konfusionsmatrix nur für den xgboost Algorithmus an. Auf der Konfusionsmatrix können wir viele Gütekriterien für die Klassifikation berechnen.\n\nconf_mat_lst &lt;- aug_lst |&gt; \n  map(~conf_mat(.x, gender, .pred_class))\n\nUnd diese große Anzahl an Gütekriterien berechnen wir dann auch gleich. Die Funktion summary() gibt uns die Gütekriterien für alle Algorithmen wieder. Wir müssen dann noch etwas aufräumen und die Wiedergaben dann passend zusammenfassen, so dass wir eine schöne Tabelle wiedergegeben kriegen. So das sind jetzt aber ganz schön viele Maßzahlen.\n\nconf_mat_lst |&gt; \n  map(summary) |&gt; \n  map(~select(.x, .metric, .estimate)) |&gt; \n  reduce(left_join, by = \".metric\") |&gt; \n  set_names(c(\"metric\", \"knn\", \"rf\", \"xboost\")) |&gt; \n  mutate(across(where(is.numeric), round, 3))\n\n# A tibble: 13 × 4\n   metric                 knn    rf xboost\n   &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 accuracy             0.817 0.817  0.811\n 2 kap                  0.635 0.635  0.624\n 3 sens                 0.855 0.855  0.867\n 4 spec                 0.783 0.783  0.761\n 5 ppv                  0.78  0.78   0.766\n 6 npv                  0.857 0.857  0.864\n 7 mcc                  0.638 0.638  0.629\n 8 j_index              0.638 0.638  0.628\n 9 bal_accuracy         0.819 0.819  0.814\n10 detection_prevalence 0.52  0.52   0.537\n11 precision            0.78  0.78   0.766\n12 recall               0.855 0.855  0.867\n13 f_meas               0.816 0.816  0.814\n\n\nUm jetzt zu verstehen, wie scih diese Maßzahl jetzt alle berechnen ziehen wir uns einmal die Konfusionsmatrix für den xgboost Algorithmus aus dem Objekt conf_mat_lst raus. Wir sehen, dass wir die meisten Männer und Frauen richtig klassifiziert haben. Neben dieser Information, brauchen wir noch die Informationen der Randsummen.\n\npluck(conf_mat_lst, \"xgboost\")\n\n          Truth\nPrediction  m  w\n         m 72 22\n         w 11 70\n\n\nWir können berechnen, dass wir in den Testdaten (Truth) dann 58 Männer vorliegen haben sowie 61 Frauen. In den vorhergesagten Daten (Prediction) haben wir dann 63 Männer und 56 Frauen. Die beiden Zahlen brauchen wir noch und daher ergänzen wir diese Zahlen dann auch in der Tabelle 68.2 (b).\n\n\n\nTabelle 68.2— Die theoretische Konfusionsmatrix sowie die ausgefüllte Konfusionmatrix nach dem xgboost Algorithmus.\n\n\n\n\n\n\n\n(a) Die Konfusionsmatrix als eine 2x2 Tabelle oder Vierfeldertafel\n\n\n\n\n\n\n\nTruth\n\n\n\n\n\n\\(Positiv\\)\n\\(Negativ\\) (0)\n\n\n\n\n\\((PP)\\) (1)\n\\((PN)\\) (0)\n\n\nPrädiktion\n\\(Positiv\\) (1)\n\\(TP\\)\n\\(FP\\)\n\n\n\n\\((P)\\) (1)\n\n\n\n\n\n\\(Negativ\\) (0)\n\\(FN\\)\n\\(TN\\)\n\n\n\n\\((N)\\) (0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Die Konfusionsmatrix für den xgboost Algorithmus.\n\n\n\n\n\n\n\nTruth\n\n\n\n\n\n\\(Positiv\\) (m)\n\\(Negativ\\) (w)\n\n\n\n\n\\((PP = 58)\\) (m)\n\\((PN = 61)\\) (w)\n\n\nPrädiktion\n\\(Positiv\\) (m)\n\\(51\\)\n\\(12\\)\n\n\n\n\\((P = 63)\\) (m)\n\n\n\n\n\n\\(Negativ\\) (w)\n\\(7\\)\n\\(49\\)\n\n\n\n\\((N = 56)\\) (w)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWir können auch noch ganz viel mehr Beurteilungskriterien für die Klassifikation in einer Konfusionmatrix berechnen lassen. Wir wollen jetzt aber nur die dreizehn Beurteilungskriterien, die wir von der Funktion summary() berechnet kriegen, einmal durchgehen. Die Frage was du jetzt von den Maßzahlen alles berichten sollst, hängt wiederum davon ab, wenn du die Maßzahlen berichten willst. Die Accuarcy und die ROC Kurven sind sicherlich die wichtigsten Maßzahlen. Der Rest geht eher in die optionale Richtung.\n\nAccuarcy\n\nDie accuracy (deu. Genauigkeit, nicht verwendet) ist der Anteil der Label, die richtig vorhergesagt werden. Das Problem bei der Verwendung der Genauigkeit als Hauptgütekriterium besteht darin, dass sie bei einem starken Klassenungleichgewicht nicht gut funktioniert.\n\nKappa\n\nDas kap beschreibt Kappa und damit ein ähnliches Maß wie die accuracy. Dabei wird aber Kappa durch die accuarcy normalisiert, die allein durch Zufall zu erwarten wäre. Damit ist Kappa sehr nützlich, wenn eine oder mehrere Klassen große Häufigkeitsverteilungen haben.\n\nSensitivität\n\nDie sens beschreibt die Sensitivität oder die true positive rate (TPR). Eine Methode die erkrankte Personen sehr zuverlässig als krank (1) erkennt hat eine hohe Sensitivität. Das heißt, sie übersieht kaum erkrankte (1) Personen.\n\n\n\\[\n\\mbox{Sensitivität} = \\mbox{sens} = \\cfrac{TP}{TP + FN} = \\cfrac{51}{51 + 7} = 0.879\n\\]\n\nSpezifität\n\nDie spec beschreibt die Spezifität oder die true negative rate (TNR). Eine Methode die gesunde Personen zuverlässig als gesund (0) einstuft, hat eine hohe Spezifität. Das heißt, die Methode liefert in der Regel nur bei Erkrankten ein positives Ergebnis.\n\n\n\\[\n\\mbox{Spezifität} = \\mbox{spec} = \\cfrac{TN}{TN + FP} = \\cfrac{49}{49 + 12} = 0.803\n\\]\n\nPositiver prädiktiver Wert\n\nDer ppv beschreibt den positiven prädiktiven Wert (eng. positive predictive value).\n\n\n\\[\n\\mbox{Positiver prädiktiver Wert} = \\mbox{ppv} = \\cfrac{TP}{PP} = \\cfrac{51}{63} = 0.81\n\\]\n\nNegativer prädiktiver Wert\n\nDer npv beschreibt den negativen prädiktiven Wert (eng. negative predictive value).\n\n\n\\[\n\\mbox{Negativer prädiktiver Wert} = \\mbox{npv} = \\cfrac{TN}{PN} = \\cfrac{49}{56} = 0.875\n\\]\n\nMatthews Korrelationskoeffizienten\n\nDas mcc beschreibt den Matthews Korrelationskoeffizienten (eng. Matthews correlation coefficient). Der Matthews-Korrelationskoeffizient (MCC) ist ein zuverlässiger statistischer Wert, der nur dann einen hohen Wert hat, wenn die Vorhersage in allen vier Kategorien der Konfusionsmatrix (richtig positiv, falsch negativ, richtig negativ und falsch positiv) gute Ergebnisse erzielt. Wir berechnen den Wert hier jetzt nicht, da die Formel insgesamt acht zusammengesetzte Terme aus der Konfusionsmatrix beinhaltet. Für die Berechnung einmal beim Matthews correlation coefficient nachlesen oder aber auch Chicco und Jurman (2020) berücksichtigen.\n\nYouden-J-Statistik\n\nDer j_index beschreibt die Youden-J-Statistik und ist definiert als \\(J = sens + spec - 1\\). Wenn wir also eine hohe Sensitivität und eine hohe Spezifität haben dann nähert sich \\(J\\) der Eins an.\n\n\n\\[\n\\mbox{Youden-J} = \\mbox{j index} = sens + spec - 1 = 0.879 + 0.803 - 1  = 0.682\n\\]\n\nBalancierte Accuarcy\n\nDie bal_accuracy beschreibt die balancierte accuarcy und wird hier in der Funktion als der Durchschnitt von Sensitivität und Spezifität berechnet. Leider hat die balancierte Accuarcy mit der Accuarcy wie oben beschrieben weniger zu tun.\n\n\n\\[\n\\mbox{Balanced accuracy} = \\cfrac{TPR + TNR}{2} = \\cfrac{0.879 + 0.803}{2} = 0.841\n\\]\n\nEntdeckungsprävalenz\n\nDie detection_prevalence Die Entdeckungsprävalenz (eng. detection prevalence) ist definiert als die Anzahl der vorhergesagten positiven Ereignisse (sowohl richtig als auch falsch positiv) geteilt durch die Gesamtzahl der Vorhersagen.\n\n\n\\[\n\\mbox{Entdeckungsprävalenz} = \\cfrac{TP + FP}{TP + FP + FN + TN} = \\cfrac{51 + 12}{51 + 12 + 7 + 49} = 0.529\n\\]\n\nPrecision und Recall\n\nDie precision Bei der binären Klassifizierung ist die precision der positiv prädiktive Wert. Damit ist die precision die Anzahl der richtig positiven Ergebnisse geteilt durch die Anzahl aller positiven Ergebnisse, einschließlich derer, die nicht richtig erkannt wurden.\n\n\n\n\n\n\n\n\nAbbildung 68.1— Visualisierung der Berechung der Precision und des Recalls anhand von einem Venndiagramm.\n\n\n\nPräzision hilft, wenn die Kosten für falsch positive Ergebnisse hoch sind. Nehmen wir einmal an wir wollen Hautkrebs erkennen. Wenn wir ein Modell mit sehr geringer Präzision haben, teilen wir vielen Patienten mit, dass sie ein Melanom haben, und dies schließt einige Fehldiagnosen ein. Es stehen viele zusätzliche Tests und Stress für die Patienten auf dem Spiel. Wenn die Fehlalarme zu hoch sind, lernen diejenigen, die die Ergebnisse überwachen, sie zu ignorieren, nachdem sie mit Fehlalarmen bombardiert wurden.\n\\[\n\\mbox{Precision} = \\mbox{Positiver prädiktiver Wert}  = \\cfrac{TP}{PP} = \\cfrac{51}{63} = 0.81\n\\]\nDer recall Bei der binären Klassifizierung ist der recall die Sensitivität. Damit ist der recall die Anzahl der tatsächlich positiven Ergebnisse geteilt durch die Anzahl aller Ergebnisse, die als positiv hätten identifiziert werden müssen.\nDer Recall hilft, wenn die Kosten für falsch negative Ergebnisse hoch sind. Was ist, wenn wir einfallende Atomraketen erkennen müssen? Ein falsches Negativ hat verheerende Folgen. Versteh es falsch und wir alle sterben. Wenn falsche Negative häufig sind, wirst du von dem getroffen, was du vermeiden möchten. Ein falsch Negatives ist, wenn du sich entscheidest, das Geräusch eines Zweigs zu ignorieren, der in einem dunklen Wald bricht, und du dann von einem Bären gefressen wirst. Ein falsch Positives Ereignis wäre dann, dass du die ganze Nacht schlaflos in deinem Zelt in kaltem Schweiß aufbleibst und jedem Durcheinander im Wald zuhörst, nur um am nächsten Morgen zu erkennen, dass diese Geräusche von einem Waschbären gemacht wurden. Auch kein Spaß.\n\\[\n\\mbox{Recall} = \\mbox{Sensitivität} = \\cfrac{TP}{TP + FN} = \\cfrac{51}{51 + 7} = 0.879\n\\]\n\nF\\(_1\\) Score\n\nDie f_meas beschreibt den F\\(_1\\) Score und damit das harmonische Mittel aus Precision und Recall. Der höchstmögliche Wert eines F\\(_1\\) Scores ist 1, was perfekte Präzision und Recall bedeutet, und der niedrigstmögliche Wert ist 0, wenn sowohl Präzision als auch Recall null sind. Das heißt, ein guter F1-Score bedeutet, dass du niedrige Fehlalarme und niedrige Falschnegative hast, sodass du echte Ereignisse oder Bedrohungen korrekt identifizieren und nicht durch Fehlalarme gestört wirst.\n\n\n\\[\nF_1 = \\cfrac{2 \\cdot TP}{2 \\cdot TP + FP + FN} = \\cfrac{2 \\cdot 51}{2 \\cdot 51 + 12 + 7} = 0.843\n\\]\n\nROC & Precision recall Kurven\n\nWenn wir von der Visualisierung von Klassifikationsergebnissen sprechen, dann kommen wir an der ROC Kurve und der PR Kurve nicht vorbei. Beide Kurven lassen sich ziemlich zügig erstellen. Wir kennen ja schon die Funktion roc_curve() für die ROC Kurve.\n\n\nIn dem Kapitel 35 erfährst du mehr darüber was eine ROC Kurve ist und wie du die ROC Kurve interpretieren kannst.\n\nroc_tbl &lt;- aug_lst |&gt; \n  map(~roc_curve(.x, gender, .pred_w, event_level = \"second\")) |&gt; \n  bind_rows(.id = \"model\")\n\nDie PR Kurve, für die Darstellung der Precision und des Recalls können wir dann die Funktion pr_curve() nutzen. Im Gegesatz zu der ROC Kurve wollen wir das die PR Kurve erstmal waagerecht verlauft und am Ende senkrecht nach unten fällt. Die Spitzen und Zacken in der Kurve sind normal und hat mit der Berechnung der beiden Werte zu tun. Wir wollen aber auch hier eine möglichst große Fläche unter der Kurve haben.\n\npr_tbl &lt;- aug_lst |&gt; \n  map(~pr_curve(.x, gender, .pred_w, event_level = \"second\")) |&gt; \n  bind_rows(.id = \"model\")\n\nIn der Abbildung 68.2 sind die ROC Kurven und die PR Kurven für die drei Algorithmen nochmal dargestellt. Zum einen sehen wir, dass wir nicht das beste Modell haben. Alle Modelle laufen übereinander und sind sich recht ähnlich. Das Bild wiederholt sich dann auch bei der PR Kurve wie bei der ROC Kurve. Dennoch sind die Algorithmen einigermaßen gut, denn wir haben ja weder eine Kreuzvalidierung noch ein Tuning durchgeführt. Wir bewerten die Modelle als gut, da die Flächen unter der Kurve relativ groß sind. Wenn es ein Modell gibt, was im Verhältnis zu den anderen Modellen abfällt, dann ist es das \\(k\\)-NN Modell. Das \\(k\\)-NN Modell hat einen starken Abfall zu Beginn der PR-Kurve.\n\nroc_tbl |&gt; \n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  theme_minimal() +\n  geom_path() +\n  geom_abline(lty = 3) + \n  scale_color_okabeito()\n\npr_tbl |&gt; \n  ggplot(aes(x = recall, y = precision, col = model)) + \n  theme_minimal() +\n  geom_path() +\n  scale_color_okabeito()\n\n\n\n\n\n\n\n\n\n\n\n(a) Receiver operator curve.\n\n\n\n\n\n\n\n\n\n\n\n(b) Precision recall curve.\n\n\n\n\n\n\n\nAbbildung 68.2— Darstellung der Vorhersagegüte der drei Modelle k-NN, ranger und xgboost.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Vergleich von Algorithmen</span>"
    ]
  },
  {
    "objectID": "classification-model-compare.html#referenzen",
    "href": "classification-model-compare.html#referenzen",
    "title": "68  Vergleich von Algorithmen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 68.1— Visualisierung der Berechung der Precision und des Recalls anhand von einem Venndiagramm.\nAbbildung 68.2 (a)— Receiver operator curve.\nAbbildung 68.2 (b)— Precision recall curve.\n\n\n\nChicco D, Jurman G. 2020. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC genomics 21: 1–13.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Vergleich von Algorithmen</span>"
    ]
  },
  {
    "objectID": "classification-knn.html",
    "href": "classification-knn.html",
    "title": "69  \\(k\\) nearest neighbor",
    "section": "",
    "text": "69.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, tidymodels, magrittr, see,\n               caret, kknn, MachineShop, conflicted)\n##\nset.seed(2025429)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>$k$ nearest neighbor</span>"
    ]
  },
  {
    "objectID": "classification-knn.html#daten",
    "href": "classification-knn.html#daten",
    "title": "69  \\(k\\) nearest neighbor",
    "section": "69.2 Daten",
    "text": "69.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") |&gt; \n  mutate(pig_id = 1:n(),\n         infected = as_factor(infected)) |&gt; \n  select(pig_id, infected, everything())  \n\nIn Tabelle 69.1 siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 69.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npig_id\ninfected\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\n\n\n\n\n1\n1\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\n\n\n2\n1\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\n\n\n3\n0\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\n\n\n4\n1\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.15\n19.05\n4.35\n\n\n5\n1\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n55.38\n18.44\n5.27\n\n\n6\n1\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n60.29\n18.42\n4.78\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\nfemale\nnorth\n11.82\n21.5\npre-frail\n55.32\n19.75\n3.92\n\n\n408\n0\n56\nmale\nwest\n13.91\n20.8\nfrail\n58.37\n17.28\n7.44\n\n\n409\n1\n57\nmale\nnorthwest\n12.49\n21.95\npre-frail\n56.66\n16.86\n2.44\n\n\n410\n1\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\n\n\n411\n0\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\n\n\n412\n1\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\n\n\n\n\n\n\n\n\nGehen wir jetzt mal die Wörter und Begrifflichkeiten, die wir für das maschinelle Lernen mit dem \\(k\\)-NN Algorithmus später brauchen einmal durch.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>$k$ nearest neighbor</span>"
    ]
  },
  {
    "objectID": "classification-knn.html#k-nn-theoretisch",
    "href": "classification-knn.html#k-nn-theoretisch",
    "title": "69  \\(k\\) nearest neighbor",
    "section": "69.3 \\(k\\)-NN theoretisch",
    "text": "69.3 \\(k\\)-NN theoretisch\nIm Folgenden betrachten wir uns den \\(k\\)-NN Algorithmus einmal ganz simpel. Dafür nutzen wir die Abbildung 69.1 als Startpunkt. Wir haben dort 11 Beobachtungen im Trainingsdatensatz dargestellt. Wir finden in dem Trainingsdatensatz acht infizierte Personen soiwe drei gesunde Personen. Darüber hinaus eine neue rote Beobachtung. Gegeben den Traingsdaten, welchen Status wollen wir der neuen roten Beobachtung geben?\n\n\n\n\n\n\nAbbildung 69.1— Darstellung von 11 Beobachtungen aus dem Traingsdatensatz und einer neuen roten Beobachtung aus den Testdaten. Die schwarzen Kugeln stellen kranke Personen und die grünen die gesunde Personen dar.\n\n\n\nIn der Abbildung 69.2 sehen wir die Klassifizierung nach \\(k = 1\\). Wir nehmen daher die \\(k = 1\\) nächsten Beobachtungen und bestimmen daran den neuen Status der roten Beobachtung. Wenn wir nur die eine nächste Beobachtung als Nachbarn betrachten, so setzen wir den Status unser neuen Beobachtung auf grün und daher gesund.\n\n\n\n\n\n\nAbbildung 69.2— Wir nehmen mit \\(k=1\\) nur die nächste Beobachtung zu unserer neuen Beobachtung hinzu und bestimmen die neue Beobachtung als grün.\n\n\n\nNun können wir das Spiel weiterspielen und wählen in der Abbildung 69.3 die \\(k = 2\\) nächsten Nachbarn zu unser neuen Beobachtung aus. Wir erhalten jetzt ein Unentschieden. Wir haben eine schwarze Beobachtung und eine grüne Beobachtung als \\(k=2\\) nächste Nachbarn. Wir können hier keine Entscheidung treffen. Eine gerade Anzahl an nächsten Nachbarn ist prinzipiell nicht anzuraten. Ich empfehle immer eine ungerade Anzhl. Auch wenn es natürlich auch für eine gerade Anzahl eine algorithmische Lösung gibt. Das ist aber weit über die Anwendung hinaus und geht in die Tiefe des Algorithmus, die wir hier nicht behandeln wollen.\n\n\n\n\n\n\nAbbildung 69.3— Mit \\(k = 2\\) nächste Nachbarn haben wir ein Patt vorliegen. Wir können uns nicht entscheiden, ob wir die neue Beobachtung als grün oder schwarz klassifizieren.\n\n\n\nIn der Abbildung 69.4 sehen wir, dass wir jetzt \\(k = 3\\) Nachbarn betrachten. Damit haben wir auf jeden Fall wieder eine Entscheidung. Wenn auch hier nur sehr knapp, da wir ja zwei schwarze und einen grünen Nachbarn haben. Wir klassifizieren dennoch die neue Beobachtung als schwarz.\n\n\n\n\n\n\nAbbildung 69.4— Die Klassifizierung mit \\(k = 3\\) nächsten Nachbarn. Wir erhalten hier eine , wenn auch knappe, Entscheidung für den schwarzen Status und damit krank.\n\n\n\nSoweit so gut. Und wie entscheide ich jetzt was weit weg ist? Wenn wir uns mit dem \\(k\\)-NN Algorithmus näher beschäftigen würden, dann werden wir feststellen, dass es eine Vielzahl an Abstandsmaßen gibt. Wir du dir vorstellen kannst, kann man die Entfernung zwischen zwei Punkten als den absoluten Abstand messen. Oder aber als den quadratischen Abstand. Es wäre auch möglich einen gewichteten Abstand einzuführen, so dass nähere Beobachtungen einen größeren Einfluss auf die Vorhersage haben als weiter entfernte Beobachtungen. Dann würden wir auch das Problem von geraden \\(k\\) Anzahlen lösen. Du musst dann leider in den jeweiligen R Paketen schauen, welche Optionen es dort geben mag. Wir werden uns hier auf eins der R Pakete mit {kknn} konzentrieren.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>$k$ nearest neighbor</span>"
    ]
  },
  {
    "objectID": "classification-knn.html#klassifikation",
    "href": "classification-knn.html#klassifikation",
    "title": "69  \\(k\\) nearest neighbor",
    "section": "69.4 Klassifikation",
    "text": "69.4 Klassifikation\nSchauen wir uns als erstes eine simple Klassifikation mit dem \\(k\\)-NN Algorithmus an. Wir brauchen dafür erstmal einen Trainings- und Testdatensatz. Wir trainieren dann den \\(k\\)-NN Algorithmus auf den Trainingsdaten. Wenn wir dann mit dem Modell zufrieden sind, schauen wir, ob unserer Modell auch auf den Trainingsdaten funktioniert. Wir trennen daher die Daten mit \\(3/4\\) Trainingsdaten und \\(1/4\\) Testdaten auf. Wir nutzen dazu die Funktion initial_split(). Es gibt auch andere Möglichkeiten sich den Split in Trainings- und Testdatensatz zu erstellen, aber so geht es relativ einfach und schnell. Im Kapitel 66 kannst du dir auch noch eine Alternative anschauen.\n\npig_data_split &lt;- initial_split(pig_tbl, prop = 3/4)\n\nJetzt haben wir in dem Objekt pig_data_split die beiden Datensätze vorliegen. Wir ziehen uns nun die Trainingsdaten und die Testdaten in zwei neue Objekten heraus. Wir werden jetzt im weiteren Verlauf nur die Trainingsdaten nutzen. Die Testdaten nur einmal ganz am Ende, wenn wir die ROC-Kurven darstellen.\n\npig_train_data &lt;- training(pig_data_split)\npig_test_data  &lt;- testing(pig_data_split)\n\nWir brauchen wieder unser Rezept, in dem wir definieren, was an Schritten im Preproessing durchgeführt werden soll. Zuerst definieren wir unser Modell in der Funktion recipe(). Wir haben als unser Label die Variable infected, also ob ein Ferkel infiziert ist oder eben nicht. Wir nehmen dann die restlichen Variablen als Features mit ins Modell.\nNachdem wir dann das Rezept haben, wollen wir noch alle numerischen Prädiktoren, also die Features, auf die Spannweite von \\([0;1]\\) bringen. Dann werden noch alle nominalen Variablen in Dummies kodiert. Abschließend entfernen wir dann noch eventuelle Variablen, die kaum noch eine Varianz vorliegen haben. Das soll es für diese Anwendung des \\(k\\)-NN Algorithmus hier erstmal reichen.\n\npig_rec &lt;- recipe(infected ~ age + sex + location + activity + crp + \n                   frailty + bloodpressure + weight + creatinin,\n                  data = pig_train_data) |&gt; \n step_range(all_numeric_predictors(), min = 0, max = 1) |&gt; \n step_dummy(all_nominal_predictors()) |&gt; \n step_nzv(all_predictors())\n\nJetzt kommen wir zu dem Modell. Wir wollen den \\(k\\)-NN Algorithmus rechnen und nutzen deshalb die Funktion nearest_neighbor(). Wir wollen dann neighbors = 11 in dem Algorithmus nutzen. In der Funktion heißt dann das \\(k\\) eben neighbors. Ist zwar nicht schön, aber das kennen wir ja schon alles von anderen Funktionen. Dann nutzen wir die kknn Engine und wollen eine Klassifikation rechnen. Wir rechnen eine Klassifikation, da wir als Outcome die Variable infected vorliegen haben und diese Variable binär ist.\n\nknn_mod &lt;- nearest_neighbor(neighbors = 11) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") \n\nDann haben wir also unser Modell definiert. Auch haben wir dann auch das Rezept, was wir ausführen wollen. Wir kombinieren jetzt das Modell zusammen mit dem Rezept in einen Workflow durch die Funktion workflow(). Bis jetzt haben wir noch nichts gerechnet. Das Rechnen kommt jetzt im nächsten Schritt.\n\npig_wflow &lt;- workflow() |&gt; \n  add_model(knn_mod) |&gt; \n  add_recipe(pig_rec)\n\nWir wollen jetzt den Workflow auf den Trainingsdaten ausführen. Dazu nutzen wir die Funktion fit(). Da es leider sehr viele R Pakete gibt, die die Funktion fit() implementiert haben, lege ich mit parsnip::fit() definitiv fest, dass wir die fit() Funktion aus dem R Paket {parsnip} nutzen wollen.\n\npig_fit &lt;- pig_wflow |&gt; \n  parsnip::fit(pig_train_data)\n\nJetzt haben wir den Fit des Modells vorliegen. Mit dem Modell werden wir jetzt schauen, wie gut wir das Outcome infected in den Testdaten vorhersagen können. Wir nutzen dazu die Funktion augment(). Die Funktion verbindet den Testdatensatz mit den Information aus der Vorhersage. Wie immer brauchen wir nicht alles, was wir wiedergegeben kriegen. Daher wählen wir nur die Spalte infected, da stehen ja unsere wahren Werte für den Infektionsstatus drin und die Vorhersagen aus dem Modell. Die Vorhersagen des Modells haben alle ein pred im Namen, also können wir die Funktion matches() nutzen um diese Spalten auszuwählen.\n\npig_aug &lt;- augment(pig_fit, pig_test_data ) |&gt; \n  select(infected, matches(\"pred\"))\n\npig_aug\n\n# A tibble: 103 × 4\n   infected .pred_class .pred_0 .pred_1\n   &lt;fct&gt;    &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 1        1             0.310   0.690\n 2 1        1             0.324   0.676\n 3 0        0             0.639   0.361\n 4 1        1             0.235   0.765\n 5 1        1             0.414   0.586\n 6 1        1             0.111   0.889\n 7 1        1             0       1    \n 8 0        1             0.208   0.792\n 9 0        0             0.567   0.433\n10 1        0             0.521   0.479\n# ℹ 93 more rows\n\n\nWir erhalten also den Infektionsstatus der Testdaten, den vorhergesagte Infektionsstatus aus dem \\(k\\)-NN Algorithmus, die Wahrscheinlichkeit für einen Infektionsstatus von 0 und die die Wahrscheinlichkeit für einen Infektionsstatus von 1. Damit haben wir alles zusammen um die ROC Kurven zu zeichnen. Dafür müssen wir die truth Spalte angeben und nennen in welcher Spalte die Wahrscheinlichkeit für die truth stehen. Wir definieren auch das event_level als second. Wenn die ROC Kurve auf der falschen Seite der Diagonalen ist, dann liegt es an dem falschen event_level. Die falsche Seite ist unterhalb der Diagonalen. Wenn die ROC also gespiegelt ist, dann versuche einmal event_level = \"first\" und erstelle die ROC Kurve neu.\n\npig_aug |&gt; \n  roc_curve(truth = infected, .pred_1, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\nAbbildung 69.5— ROC Kurve für den kknn Algorithmus.\n\n\n\n\n\nLeider sieht die ROC Kurve nicht sehr gut aus. Eine sehr gute Vorhersage hat eine ROC Kurve die senkrecht ansteigt und dann waagerecht nach rechts verläuft. Die Fläche zwischen der Kurve und der Diagonalen sollte so große wie möglich sein.\nWenn wir jezt noch wissen wollen, wie groß die Fläche unter der Kurve ist (eng. area under the curve, abk. AUC) können wir die Funktion roc_auc() nutzen. Auch hier müssen wir das event_level richtig definieren. Wir kopieren hier den Code einfach rüber.\n\npig_aug |&gt; \n  yardstick::roc_auc(truth = infected, .pred_1, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.673\n\n\nWie wir oben schon in der ROC Kurve gesehen haben ist ein Wert von \\(0.673\\) für die AUC auch nicht sehr gut. Wir liegen unter \\(0.7\\) und damit wären wir mit dem Modell nicht zufrieden. Wir müssten hier nochmal den \\(k\\)-NN Algorithmus tunen.\nAuch können wir uns die Genauigkeit (eng. accuary) berechnen lassen. Die Accuary beschreibt wie viel Prozent des Infektionsstatus wir richtig vorhergesagt haben. Wenn wir eine Accuary von 1 haben, dann haben wir alle Label korrekt vorhergesagt. Die Spalte infected enthält die gleichen Werte wie die Spalte .pred_class aus der Funktion augment(). Wenn wir eine Accuary von 0 vorliegen haben, dann konnten wir kein Label richtig vorhersagen.\n\npig_aug |&gt; \n  yardstick::accuracy(truth = infected, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.680\n\n\nWir auch die AUC ist auch die Accuary nicht besonders gut. Wir können nur ca. \\(68\\%\\) der Label richtig vorhersagen. Damit haben wir nur jeden dritten Infektionsstatus richtig vorhergesagt. Die Accuary ist dann eben auch nicht gut, wie wir es schon dann oben bei der ROC Kurve gesehen haben. Wenigstens passen dann die wichtigsten Beurteilungskriterien inhaltlich zusammen.\nWir können auch ganz viele Beurteilungskriterien für die Klassifikation in einer Confusion matrix berechnen lassen. Dabei ist wichtig, das wir hier eine binäre Klassifikation vorliegen haben. Unser Infektionsstatus hat eben nur zwei Ausprägungen. Die Ferkel sind entweder krank oder gesund. Wir können die Funktion conf_mat() nutzen um uns die 2x2 Tabelle erstellen zu lassen.\n\npig_cm &lt;- pig_aug |&gt; \n  conf_mat(infected, .pred_class)\n\npig_cm\n\n          Truth\nPrediction  0  1\n         0 10 11\n         1 22 60\n\n\nWenn wir dann die Funktion summary() nutzen, dann erhalten wir insgesamt dreizehn Beurteilungskriterien für die Klassifikation. Wir gehen jetzt nicht auf alle Kriterien ein, das sprengt hier den Rahmen. Wir schauen uns die Kriterien dann in dem Kapitel 68.6 nochmal teilweise an. Wie immer musst du nicht alle Kriterien angeben sondern nur die Kriterien, die der Fragestellung dienen.\n\npig_cm |&gt; summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.680\n 2 kap                  binary         0.174\n 3 sens                 binary         0.312\n 4 spec                 binary         0.845\n 5 ppv                  binary         0.476\n 6 npv                  binary         0.732\n 7 mcc                  binary         0.181\n 8 j_index              binary         0.158\n 9 bal_accuracy         binary         0.579\n10 detection_prevalence binary         0.204\n11 precision            binary         0.476\n12 recall               binary         0.312\n13 f_meas               binary         0.377\n\n\nWie immer können wir uns eine 2x2 Tabelle auch mit einem Mosaicplot visualisieren. Das machen wir dann auch mit der Funktion autoplot(). Wir können natürlich auch die ggplot Funktionen nutzen, aber wir nutzen hier ja die Visualisierung nur um unsere Klassifikation zu überprüfen. Dann reicht auch die schnellere Variante.\n\nautoplot(pig_cm, type = \"mosaic\") +\n  theme_minimal() \n\n\n\n\n\n\n\nAbbildung 69.6— Mosaicplot der Konfusionsmatrix für den kknn Algorithmus.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>$k$ nearest neighbor</span>"
    ]
  },
  {
    "objectID": "classification-knn.html#resampling",
    "href": "classification-knn.html#resampling",
    "title": "69  \\(k\\) nearest neighbor",
    "section": "69.5 Resampling",
    "text": "69.5 Resampling\nWir können den \\(k\\)-NN Algorithmus nicht nur auf dem Trainingsdaten anwenden sondern auch auf Validierungsdaten optimieren. Dabei sind die Validierungsdaten wiederum aufgeteilte Trainingsdaten. Wir nutzen die Funktion vfold_cv() um uns zehn Kreuzvalidierungsdatensätze zu erschaffen. Meistens rechnen wir eine 10-fache Kreuzvalidierung. Die 10-fache Kreuzvalidierung ist eigentlich der Standard im Bereich der Kreuzvaldidierung.\n\nfolds &lt;- vfold_cv(pig_train_data, v = 10)\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [278/31]&gt; Fold01\n 2 &lt;split [278/31]&gt; Fold02\n 3 &lt;split [278/31]&gt; Fold03\n 4 &lt;split [278/31]&gt; Fold04\n 5 &lt;split [278/31]&gt; Fold05\n 6 &lt;split [278/31]&gt; Fold06\n 7 &lt;split [278/31]&gt; Fold07\n 8 &lt;split [278/31]&gt; Fold08\n 9 &lt;split [278/31]&gt; Fold09\n10 &lt;split [279/30]&gt; Fold10\n\n\nDank der Funktion fit_resample() können wir einen Workflow nicht nur auf einen Datensatz wie mit der Funktion fit() anwenden, sondern auf ein ganzes Set an Validierungsdaten. Die Funktion fit_resample() rechnet jetzt auf jenden der zehn Validierungsdatensätze einen \\(k\\)-NN Algorithmus wie im Workflow beschreiben.\n\npig_cv_fit &lt;- pig_wflow |&gt; \n  fit_resamples(folds)\n\nNachdem wir die zehn Validierungsdatensätze durchgerechnet haben, müssen wir noch die Informationen aus jedem der zehn Validierungsdatensätze einsammeln. Das macht die Funktion collect_metrics() für uns.\n\ncollect_metrics(pig_cv_fit)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.718    10  0.0220 Preprocessor1_Model1\n2 roc_auc  binary     0.700    10  0.0357 Preprocessor1_Model1\n\n\nWir sehen, dass wir eine Accuarcy von \\(0.718\\) erreichen und eine AUC von \\(0.7\\). Damit sind wir ein bisschen besser als in unserem einfachen Lauf auf nur den Trainingsdaten. Die eigentliche Stärke der Kreuzvalidierung kommt aber erst mit dem Tuning zu tage. Hier nutzen wir dann die Kreuzvalidierung um die Parameter des \\(k\\)-NN Algorithmus zu optimieren.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>$k$ nearest neighbor</span>"
    ]
  },
  {
    "objectID": "classification-knn.html#tuning",
    "href": "classification-knn.html#tuning",
    "title": "69  \\(k\\) nearest neighbor",
    "section": "69.6 Tuning",
    "text": "69.6 Tuning\nWas heißt Tuning? Wie bei einem Auto können wir an verschiedenen Stellschrauben bei einem mathematischen Algorithmus schrauben. Welche Schrauben und Teile das sind, hängt dann wieder vom Algorithmus ab. Im Falle des \\(k\\)-NN Algorithmus können wir an folgenden Parametern drehen und jeweils schauen, was dann mit unserer Vorhersage passiert.\n\nneighbors, eine einzelne Zahl für die Anzahl der zu berücksichtigenden Nachbarn (oft \\(k\\) genannt). Für kknn wird ein Wert von 5 verwendet, wenn keine Anzahl angegeben ist.\nweight_func ein Wort für den Typ der Kernel-Funktion, die zur Gewichtung der Abstände zwischen den Beobachtungen verwendet wird.\ndist_power, eine einzelne Zahl für den Parameter, der bei der Berechnung der Minkowski-Distanz verwendet wird. Wir nutzen also die dist_power nicht bei jedem Tuningschritt, da nicht jede Gewichtsfunktion eine dist_power braucht.\n\nNun ist es so, dass wir natürlich nicht händisch alle möglichen Kombinationen von der Anzahl der Nachbarn, der Distanzfunktion und der Gewichtung der Distanz berechnen wollen. Das sind ziemlich viele Kombinationen und wir kommen dann vermutlich schnell durcheinander. Deshalb gibt es die Funktion tune() aus dem R Paket {tune}, die uns einen Prozess anbietet, das Tuning automatisiert durchzuführen.\nAls erstes müssen wir uns ein Objekt bauen, das aussieht wie ein ganz normales Modell in der Klassifikation. Aber wir ergänzen jetzt noch hinter jeder zu tunenden Option noch die Funktion tune(). Das sind die Parameter des Algorithmus, die wir später tunen wollen.\n\ntune_spec &lt;- nearest_neighbor(neighbors = tune(),\n                              weight_func = tune(), \n                              dist_power = tune()) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") \n\ntune_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n  weight_func = tune()\n  dist_power = tune()\n\nComputational engine: kknn \n\n\nJetzt bauen wir uns den Workflow indem wir statt unserem Modell, die Tuninganweisung in den Workflow reinnehmen. Echt simpel und straightforward. Das Rezept bleibt ja das Gleiche.\n\npig_tune_wflow &lt;- workflow() |&gt; \n  add_model(tune_spec) |&gt; \n  add_recipe(pig_rec)\n\nJetzt müssen wir noch alle Kombinationen aus den drei Parametern neighbors, weight_func und dist_power ermitteln. Das macht die Funktion grid_regular(). Es gibt da noch andere Funktionen in dem R Paket {tune}, aber ich konzentriere mich hier auf die einfachste. Jetzt müssen wir noch die Anzahl an Kombinationen festlegen. Ich möchte für jeden Parameter fünf Werte tunen. Daher nutze ich hier die Option levels = 5 auch damit hier die Ausführung nicht so lange läuft. Fange am besten mit levels = 5 an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert. Dann kannst du die Levels noch hochschrauben. Beachte aber, dass mehr Level nur mehr Zwischenschritte bedeutet. Jede Option hat eine Spannweite range, die du dann anpassen musst, wenn du höhere Werte haben willst. In unserem Fall ist die default Anzahl an neighbors auf 1 bis 10 gesetzt. Mehr Level würden nur mehr Zwischenschritte bedeuten. Deshalb habe ich die Spannweite auf 1 bis 20 Nachbarn gesetzt. Jetzt wählt die Funktion fünf Zwischenschritte (levels = 5) zwischen ein und zwanzig aus (range = c(1, 20)).\n\npig_grid &lt;- grid_regular(neighbors(range = c(1, 20)),\n                         weight_func(),\n                         dist_power(),\n                         levels = 5)\n\nDas Tuning nur auf dem Trainingsdatensatz durchzuführen ist nicht so eine gute Idee. Deshalb nutzen wir hier auch die Kreuzvalidierung. Eigentlich ist eine 10-fache Kreuzvalidierung mit \\(v=10\\) besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur \\(v=5\\) gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.\n\npig_folds &lt;- vfold_cv(pig_train_data, v = 5)\n\nNun bringen wir den Workflow zusammen mit dem Tuninggrid und unseren Sets der Kreuzvaidierung. Daher pipen wir den Workflow in die Funktion tune_grid(). Als Optionen brauchen wir die Kreuzvaldierungsdatensätze und das Tuninggrid. Wenn du control_grid(verbose = TRUE) wählst, dann erhälst du eine Ausgabe wie weit das Tuning gerade ist. Achtung!, das Tuning dauert seine Zeit. Im Falle des \\(k\\)-NN Algorithmus dauert das Tuning zwar nicht so lange, aber immer noch ein paar Minuten. Du kannst das Ergebnis des Tunings auch in der Datei pig_knn_tune_res.rds finden.\n\npig_tune_res &lt;- pig_tune_wflow |&gt; \n   tune_grid(resamples = pig_folds,\n             grid = pig_grid,\n             control = control_grid(verbose = FALSE))\n\nDamit du nicht das Tuning durchlaufen lassen musst, habe ich das Tuning in die Datei pig_knn_tune_res.rds abgespeichert und du kannst dann über die Funktion read_rds() wieder einlesen. Dann kannst du den R Code hier wieder weiter ausführen.\n\npig_tune_res &lt;- read_rds(\"data/pig_knn_tune_res.rds\")\n\nNachdem das Tuning durchgelaufen ist, können wir uns über die Funktion collect_metrics(), die Ergebnisse des Tunings für jede Kombination der drei Parameter neighbors, weight_func und dist_power wiedergeben lassen. Diese Ausgabe ist super unübersichtlich. Deshalb einmal die Abbildung der mittleren Accuarcy und der mittleren AUC-Werte über alle Kreuzvalidierungen.\n\npig_tune_res |&gt;\n  collect_metrics() |&gt;\n  mutate(weight_func = as_factor(weight_func),\n         dist_power = as_factor(dist_power)) |&gt;\n  ggplot(aes(neighbors, mean, color = weight_func, linetype = dist_power)) +\n  theme_minimal() +\n  geom_line(alpha = 0.6) +\n  geom_point() +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 69.7— Tuning Kurven für den kknn Algorithmus.\n\n\n\n\n\nDamit wir nicht händisch uns die beste Kombination raussuchen müssen, können wir die Funktion show_best() nutzen. Wir wählen hier die beste Accuarcy und erhalten dann die sortierten Ergebnisse nach der Accuarcy des Tunings.\n\npig_tune_res |&gt;\n  show_best(\"accuracy\")\n\n# A tibble: 5 × 9\n  neighbors weight_func  dist_power .metric  .estimator  mean     n std_err\n      &lt;int&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1        15 rectangular        1.25 accuracy binary     0.692     5  0.0189\n2        15 rectangular        1.75 accuracy binary     0.689     5  0.0311\n3        15 rectangular        1.5  accuracy binary     0.683     5  0.0225\n4        20 epanechnikov       1.5  accuracy binary     0.683     5  0.0154\n5        20 rectangular        1.5  accuracy binary     0.679     5  0.0127\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\nDas war die Funktion show_best() aber wir können uns auch die gleich die besten Parameter nach der Accuracy raus ziehen. Das Rausziehen der besten Parameter macht für uns die Funktion select_best().\n\nbest_knn &lt;- pig_tune_res |&gt;\n  select_best(\"accuracy\")\n\nbest_knn\n\n# A tibble: 1 × 4\n  neighbors weight_func dist_power .config               \n      &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                 \n1        15 rectangular       1.25 Preprocessor1_Model059\n\n\nWir sehen, dass wir neighbors = 15 wählen sollten. Dann müssen wir als Gewichtungsfunktion rectangular nutzen. Die Gewichtung der Distanz wäre dann 1.25. Müssen wir jetzt die Zahlen wieder in ein Modell eingeben? Nein, müssen wir nicht. Mit der Funktion finalize_workflow() können wir dann die besten Parameter aus unserem Tuning gleich mit dem Workflow kombinieren. Dann haben wir unseren finalen, getunten Workflow. Du siehst dann auch in der Ausgabe, dass die neuen Parameter in dem \\(k\\)-NN Algorithmus übernommen wurden\n\nfinal_pig_wf &lt;- pig_tune_wflow |&gt; \n  finalize_workflow(best_knn)\n\nfinal_pig_wf \n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_range()\n• step_dummy()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 15\n  weight_func = rectangular\n  dist_power = 1.25\n\nComputational engine: kknn \n\n\nJetzt bleibt uns nur noch der letzte Fit übrig. Wir wollen unseren finalen, getunten Workflow auf die Testdaten anwenden. Dafür gibt es dann auch die passende Funktion. Das macht für uns die Funktion last_fit(), die sich dann die Informationen für die Trainings- und Testdaten aus unserem Datensplit von ganz am Anfang extrahiert.\n\nfinal_fit &lt;- final_pig_wf |&gt;\n  last_fit(pig_data_split) \n\nDa wir immer noch eine Kreuzvaldierung rechnen, müssen wir dann natürlich wieder alle Informationen über alle Kreuzvaldierungsdatensätze einsammeln. Dann erhalten wir unsere beiden Gütekriterien für die Klassifikation der Infektion von Ferkeln nach dem \\(k\\)-NN Algorithmus. So super sind die Zahlen nicht. Eine Accuracy von 73% bedeutet das wir nur knapp drei von vier Ferkeln richtig klassifizieren. Die AUC ist auch nicht berauschend, wir bringen also eine Menge Label durcheinander. Wir klassifizieren also gesunde Ferkeln als krank und umgekehrt.\n\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.728 Preprocessor1_Model1\n2 roc_auc  binary         0.662 Preprocessor1_Model1\n\n\nDann bleibt uns nur noch die ROC Kurve zu visualisieren. Da wir wieder etwas faul sind, nutzen wir die Funktion autoplot(). Als Alternative geht natürlich auch das R Paket {pROC}, was eine Menge mehr Funktionen und Möglichkeiten bietet.\n\nfinal_fit |&gt;\n  collect_predictions() |&gt; \n  roc_curve(infected, .pred_1, event_level = \"second\") |&gt; \n  autoplot() +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 69.8— ROC Kurve für den kknn Algorithmus nach der Kreuvalidierung und dem Tuning.\n\n\n\n\n\nEine gute ROC Kurve würde senkrecht nach oben gehen und dann waagrecht nach rechts. Dann hätten wir eine AUC von 1 und eine perfekte Separation der beiden Label durch unseren Algorithmus. Unser Algorithmus würde jedes Ferkel in dem Testdatensatz korrekt dem Infektionsstatus krank und gesund zuweisen. Da wir eine ROC Kurve hier vorliegen haben, die sehr nahe an der Diagonalen ist, haben wir sehr viele falsch vorhergesagte Ferkel in unseren Testdaten. Ferkel die gesund sind, werden als krank klassifiziert uns umgekehrt.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>$k$ nearest neighbor</span>"
    ]
  },
  {
    "objectID": "classification-knn.html#kmeans-clustering",
    "href": "classification-knn.html#kmeans-clustering",
    "title": "69  \\(k\\) nearest neighbor",
    "section": "69.7 kmeans Clustering",
    "text": "69.7 kmeans Clustering\nNeben der Klassifikation können wir den \\(k\\)-NN Algorithmus auch nutzen um Gruppen in den Daten zu finden. Die Idee ist recht einfach. Wir geben \\(k\\) Cluster vor und der Algorithmus versucht nun die Daten nach einer gegebenen Distanzfunktion so zu ordnen, dass sich \\(k\\) Cluster bilden. Je nach der Nähe der Beobachtungen zueinander lassen sich dann mehr oder minder klar abgegrenzte Cluster bilden. Das Problem an der Sache ist die Definition von \\(k\\) für die Anzahl der zu bildenden Cluster. Wir müssen nämlich selber festlegen, wie viele Cluster wir erwarten würden und der Algorithmus dann finden wird. Wenn wir \\(k = 3\\) der Funktion kmeans mitgeben, dann findet die Funktion drei Cluster. Auch wenn zwei mehr Sinn gemacht hätten. Daher müssen wir immer selber ausprobieren und uns die Daten visualisieren, ob das mit den Clustern so passt.\nFür Clusterbildung können wir nur numerische Variablen verwenden. Daher müssen wir hier über die Funktion step_dummy alle nominalen Variablen wie Faktoren in eine \\(0/1\\)-Kodierung umwandeln. Das ist eine Einschränkung des kmeans Algorithmus. Wir bauen uns also als erstes ein simples Rezept für unsere Ferkeldaten.\n\npig_kmeans_rec &lt;- recipe(infected ~ age + sex + location + activity + crp + \n                           frailty + bloodpressure + weight + creatinin,\n                         data = pig_train_data) |&gt; \n  step_dummy(all_nominal_predictors()) \n\nDann müssen wir noch unser Rezept auf Daten anwenden. Da wir hier die gleichen Daten nutzen wollen, auf denen wir auch das Rezept definiert haben, nutzen wir die Funktion juice(). Sonst müssten wir in der Funktion bake() einen neuen Datensatz definieren.\n\npig_dummy_tbl &lt;- pig_kmeans_rec |&gt; \n  prep() |&gt; \n  juice()\n\nNachdem wir jetzt einen Datensatz mit nur numerischen Variablen vorliegen haben, können wir die Funktion kmeans() ausführen. Wir wollen dabei aber drei Cluster bilden, das machen wir mit der Option centers = 3.\n\nkmeans_obj &lt;- kmeans(pig_dummy_tbl, centers = 3)\n\nJetzt ziehen wir uns aus dem Objekt kmeans_obj noch die Cluster raus und kombinieren die Information welche Beobachtung in welchen Cluster fällt mit den ursprünglichen Daten. Damit sind wir dann hier schon fertig. Häufig wird die Funktion kmeans in der Detektion von Ausreißern zusammen mit dem Multidimensional Scaling verwendet.\n\npig_dummy_tbl |&gt; \n  bind_cols(cluster = pluck(kmeans_obj, \"cluster\")) |&gt; \n  select(cluster, everything())\n\n# A tibble: 309 × 14\n   cluster   age activity   crp bloodpressure weight creatinin infected sex_male\n     &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n 1       1    57     12.0  19.2          53.8   17.4      2.9  0               1\n 2       1    56     12.0  19.5          56.3   19.4      3.89 0               0\n 3       1    54     14.6  20.2          55.2   19.3      5.3  1               1\n 4       1    55     13.8  20.3          51.9   18.4      6.44 1               1\n 5       1    51     12.3  19.5          55.7   20.0      3.12 1               0\n 6       2    68     14.6  20.6          57.4   17.4      6.22 1               0\n 7       2    61     12.0  21.0          53.3   19.5      4.57 1               1\n 8       3    56     14.7  22.0          61.4   21.9      3.35 1               0\n 9       3    60     10.8  21.8          59.0   18.0      4.85 1               0\n10       1    56     14.4  19.6          55.8   17.2      5.13 0               1\n# ℹ 299 more rows\n# ℹ 5 more variables: location_northeast &lt;dbl&gt;, location_northwest &lt;dbl&gt;,\n#   location_west &lt;dbl&gt;, frailty_pre.frail &lt;dbl&gt;, frailty_robust &lt;dbl&gt;\n\n\nDas R Paket {embed} bietet noch eine Vielzahl an weiteren Funktionen für die Erstellung von kategorialen Variablen. Bier musst du schauen, ob die Funktionen dann univariat sind und daher immer nur eine Variable nutzen oder aber multivariat und daher mehrere Spalten simultan. Der Vorteil von kmeans ist ja, das der Algorithmus mehrere numerische Spalten für die Clusterbildung nutzen kann.\n\n\n\nAbbildung 69.1— Darstellung von 11 Beobachtungen aus dem Traingsdatensatz und einer neuen roten Beobachtung aus den Testdaten. Die schwarzen Kugeln stellen kranke Personen und die grünen die gesunde Personen dar.\nAbbildung 69.2— Wir nehmen mit \\(k=1\\) nur die nächste Beobachtung zu unserer neuen Beobachtung hinzu und bestimmen die neue Beobachtung als grün.\nAbbildung 69.3— Mit \\(k = 2\\) nächste Nachbarn haben wir ein Patt vorliegen. Wir können uns nicht entscheiden, ob wir die neue Beobachtung als grün oder schwarz klassifizieren.\nAbbildung 69.4— Die Klassifizierung mit \\(k = 3\\) nächsten Nachbarn. Wir erhalten hier eine , wenn auch knappe, Entscheidung für den schwarzen Status und damit krank.\nAbbildung 69.5— ROC Kurve für den kknn Algorithmus.\nAbbildung 69.6— Mosaicplot der Konfusionsmatrix für den kknn Algorithmus.\nAbbildung 69.7— Tuning Kurven für den kknn Algorithmus.\nAbbildung 69.8— ROC Kurve für den kknn Algorithmus nach der Kreuvalidierung und dem Tuning.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>$k$ nearest neighbor</span>"
    ]
  },
  {
    "objectID": "classification-randomforest.html",
    "href": "classification-randomforest.html",
    "title": "70  Decision trees",
    "section": "",
    "text": "70.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor, vip, rpart.plot, see,\n               xgboost, Ckmeans.1d.dp, conflicted)\n##\nset.seed(2025429)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Decision trees</span>"
    ]
  },
  {
    "objectID": "classification-randomforest.html#daten",
    "href": "classification-randomforest.html#daten",
    "title": "70  Decision trees",
    "section": "70.2 Daten",
    "text": "70.2 Daten\nBei dem vorherigen Beispielen haben wir immer unseren Datensatz zu den infizierten Ferkeln genutzt. In diesem Kapitel wolle wir uns aber mal auf einen echten Datensatz anschauen. Wir nutzen daher einmal den Gummibärchendatensatz. Als unser Label und daher als unser Outcome nehmen wir das Geschlecht gender. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir nur die Spalte Geschlecht sowie als Prädiktoren die Spalten most_liked, age, semester, und height.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") |&gt; \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) |&gt; \n  select(gender, most_liked, age, semester, height) |&gt; \n  drop_na(gender)\n\nWir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein NA in der Variable gender haben. Wir haben dann insgesamt \\(n = 699\\) Beobachtungen vorliegen. In Tabelle 70.1 sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.\n\n\n\n\nTabelle 70.1— Auszug aus dem Daten zu den Gummibärchendaten.\n\n\n\n\n\n\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\n\nm\nlightred\n35\n10\n193\n\n\nw\nyellow\n21\n6\n159\n\n\nw\nwhite\n21\n6\n159\n\n\nw\nwhite\n36\n10\n180\n\n\nm\nwhite\n22\n3\n180\n\n\nm\ngreen\n22\n3\n180\n\n\n…\n…\n…\n…\n…\n\n\nm\ndarkred\n24\n2\n193\n\n\nm\nwhite\n27\n2\n189\n\n\nm\ndarkred\n24\n2\n187\n\n\nm\ngreen\n24\n2\n182\n\n\nw\nwhite\n23\n2\n170\n\n\nw\ngreen\n24\n2\n180\n\n\n\n\n\n\n\n\nUnsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Um die Klassifikation mit Entscheidungsbäumen rechnen zu können brauchen wir wie bei allen anderen Algorithmen auch einen Trainings- und Testdatensatz. Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Der Traingsdatensatz ist dabei immer der größere Datensatz. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.\nIm maschinellen Lernen sind alle Datensätze, die weniger als tausend Beobachtungen vorliegen haben, klein.\n\ngummi_data_split &lt;- initial_split(gummi_tbl, prop = 3/4)\n\nWir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluss einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.\n\ngummi_train_data &lt;- training(gummi_data_split)\ngummi_test_data  &lt;- testing(gummi_data_split)\n\nNachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht gender als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem . Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Es gibt wie immer noch andere Imputationsmöglichkeiten, ich habe mich jetzt aus praktischen Gründen für dies beiden Verfahren entschieden. Ich überspringe hier auch die Diagnose der Imputation, also ob das jetzt eine gute und sinnvolle Imputation der fehlenden Werte war oder nicht. Die Diagnoseschritte müsstest du im Anwendungsfall nochmal im Kapitel zur Imputation nachlesen und anwenden. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_train_data) |&gt; \n  step_impute_mean(all_numeric_predictors()) |&gt; \n  step_impute_bag(all_nominal_predictors()) |&gt; \n  step_range(all_numeric_predictors(), min = 0, max = 1) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_nzv(all_predictors())\n\ngummi_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Bagged tree imputation for: all_nominal_predictors()\n\n\n• Range scaling to [0,1] for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_predictors()\n\n\nAlles in allem haben wir ein sehr kleines Modell. Wir haben ja nur ein Outcome und vier Prädiktoren. Trotzdem sollte dieser Datensatz reichen um zu erklären wie Entscheidungsbäume funktionieren.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Decision trees</span>"
    ]
  },
  {
    "objectID": "classification-randomforest.html#sec-rpart",
    "href": "classification-randomforest.html#sec-rpart",
    "title": "70  Decision trees",
    "section": "70.3 Entscheidungsbaum mit Rpart",
    "text": "70.3 Entscheidungsbaum mit Rpart\nWie funktioniert nun ein Entscheidungsbaum? Ein Entscheidungsbaum besteht aus Knoten (eng. nodes) und Ästen (eng. edge). Dabei hat immer ein Knoten zwei Äste. Die Beobachtungen in einem Knoten fallen nach einer Entscheidungsregel anhand eines Prädiktors in entlang zweier Äste in zwei separate Knoten. So können wir unsere \\(n = 699\\) zum Beispiel anhand des Alters in zwei Gruppen aufteilen. Wir legen willkürlich die Altersgrenze bei 22 fest.\n\ngummi_tbl |&gt; \n  mutate(grp = if_else(age &gt;= 22, 1, 0)) |&gt; \n  pull(grp) |&gt; \n  tabyl()\n\n pull(mutate(gummi_tbl, grp = if_else(age &gt;= 22, 1, 0)), grp)   n   percent\n                                                            0 306 0.4377682\n                                                            1 393 0.5622318\n\n\nWir erhalten mit diesem Split zwei Gruppen mit je \\(n_0 = 207\\) und \\(n_1 = 259\\) Beobachtungen. Wir haben jetzt diesen Split willkürlich gewählt. In dem Algorithmus für die Entscheidungsbäume wird dieser Schritt intern optimiert, so dass wir den besten Wert für den Alterssplit finden, der uns möglichst reine Knoten im Bezug auf das Label liefert. Wir wollen ja am Ende einen Algorithmus trainieren, der uns die Geschlechter bestmöglich auftrennt, so dass wir eine neue Beobachtung bestmöglich vorhersagen können. Wenn keine Aufteilungen in einem Knoten mehr möglich sind, dann nennen wir diesen Knoten einen Terminalknoten.\nIn Abbildung 70.5 sehen wir ein Beispiel für zwei numerische Prädiktoren \\(X_1\\) und \\(X_2\\). Auf der linken Seite ist das Koordinatensystem mit dreizehn Beobachtungen dargestellt. Von den dreizehn Beobachtungen sind zehn Fälle (eng. cases) und drei Kontrollen (eng. control). Wir wollen uns jetzt an dem Koordinatensystem die Idee der Splits für ein Baumwachstum veranschaulichen. Auf der rechten Seite sehen wir nämlich den ersten Knoten des Entscheidungsbaums (eng. root node) in dem sich alle Beobachtungen befinden. Wir wollen jetzt die Beobachtungen anhand der Prädiktoren \\(X_1\\) und \\(X_2\\) so aufspalten, dass für möglichst reine Knoten erhalten. Wir stoppen auch im Splitting wenn wir weniger oder gleich vier Beobachtungen nach einem Split in einem Knoten erhalten.\n\n\n\n\n\n\nAbbildung 70.1— Darstellung des Anwachsen des Entscheidungsbaumes. Links sind die beiden Prädiktoren \\(X_1\\) und \\(X_2\\) als Koordinatensysten dargestellt. Die Punkte stllen die Beobachtungen mit den jeweiligen Label weiß und schwarz dar. Rechts ist der Knoten \\(t_1\\) dargestellt, der alle Beobachtungen beinhaltet..\n\n\n\nIn Abbildung 70.6 sehen wir den ersten Split des Prädiktors \\(X_1\\) anhand des Wertes \\(c_1\\). Wir erhalten nach dem Split die zwei neuen Knoten \\(t_2\\) und \\(t_3\\). Wir haben den Split so gewählt, dass wir einen reinen Knoten \\(t_3\\) erhalten. Da der Knoten \\(t_3\\) jetzt nur noch Fälle enthaält, wird dieser Knoten zu einem Terminalknoten und es finden keine weiteren Aufspaltungen mehr statt. Wir machen jetzt also mit dem Knoten \\(t_2\\) weiter.\n\n\n\n\n\n\nAbbildung 70.2— Darstellung des ersten Splits anhand des Prädiktors \\(X_1\\). Wir wählen den Wert \\(c_1\\) für den Split so, dass wir möglichst reine Knoten produzieren. Wir erhalten zwei neue Knoten \\(t_2\\) und \\(t_3\\). Der Knoten \\(t_3\\) ist maximal rein und wird daher zu einem Terminalknoten.\n\n\n\nIn Abbildung 70.8 sehen wir den Split durch den Prädiktor \\(X_2\\) nach dem Wert \\(c_2\\). Wir erhalten wieder zwei neue Knotenn \\(t_4\\) und \\(t_5\\). Der Knoten \\(t_4\\) wird nach unseren Regeln wieder zu einem Terminalknoten. Wir haben nur Fälle in dem Knoten \\(t_4\\) vorliegen. Wir stoppen auch bei dem Knoten \\(t_5\\) unsere weitere Aufteilung, da wir hier vier oder weniger Beobachtungen vorliegen haben. Damit sind wir mit dem Split zu einem Ende gekommen.\n\n\n\n\n\n\nAbbildung 70.3— Darstellung des zweiten Splits anhand des Prädiktors \\(X_2\\). Wir wählen wiederum den Wert \\(c_2\\) für den Split so, dass wir möglichst reine Knoten erhalten. So erhalten wir zwei neue Knoten \\(t_4\\) und \\(t_5\\). Da nun \\(t_4\\) ebenfalls ein reiner Knoten ist, wird der Knoten \\(t_4\\) ebenfalls zu einem Terminalknoten. Wir stoppen hier das Wachstum, da mir eine mindest Anzahl von vier Beobachtungen in den Knoten erreicht haben.\n\n\n\nIn Abbildung 70.12 sehen wir jetzt eine neue Beobachtung ? die mit gegebenen Werten für \\(X_1\\) und \\(X_2\\) in den terminalen Knoten \\(t_5\\) fällt. Wir zählen dort die Fälle und erhalten eine Klassenzugehörigkeitswahrscheinlichkeit von 25%. Daher würden wir sagen, dass die neue Beobchtung eine Kontrolle ist. Es handelt sich damit um eine weiße Beoabchtung.\n\n\n\n\n\n\nAbbildung 70.4— Darstellung der Vorhersage einer neuen Beobachtung mit Werten für die Prädiktoren \\(X_1\\) und \\(X_2\\). Unsere neue Beobachtung ? fällt in den Terminalknoten \\(t_5\\). Dort zählen wir die schwarzen Kreise. Wir stellen fest, dass die neue Beobachtung mit 25% Wahrscheinlichkeit ein Fall und damit schwarz ist. Daher ist die neue Beobachtung weiß.\n\n\n\nDamit haben wir einmal den simplen Fall mit zwei numerischen Prädiktoren durchgespielt. Auch haben wir wenige Beobachtungen und sind schnell zu reinen Knoten gekommen. Wenn wir jetzt natürlich sehr viel mehr Beobachtungen haben oder sehr viele Prädiktoren dann wird die Sache sehr schnell sehr rechenintensiv. Dafür haben wir dann eben R.\nWenn wir in R einen Entscheidungsbaum rechnen wollen, dann nutzen wir die Funktion decision_tree() wir wollen nur eine maximale Tiefe von 5 Knoten haben und/oder mindestens 10 Beobachtungen in einem Knoten. Je nachdem welche Bedingung wir eher erreichen. Ebenfalls können wir das Wachstum mit dem Parameter cost_complexity kontrollieren. Sollte sich das Modell nicht um mindestens 0.001 verbessern, dann werden wir den nächsten Knoten nicht anlegen. Wir wählen als Engine den Algorithmus rpart, da wir uns diese Art von Algorithmus gut mit dem R Paket {rpart.plot} visualisieren können.\n\nrpart_mod &lt;- decision_tree(tree_depth = 5, min_n = 10, cost_complexity = 0.001) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"classification\")\n\nJetzt kommt wieder das Modell zusammen mit dem Rezept. Wir speichern wieder beides in einen Workflow.\n\nrpart_wflow &lt;- workflow() |&gt; \n  add_model(rpart_mod) |&gt; \n  add_recipe(gummi_rec)\n\nDen Workflow können wir dann mit dem Traingsdatensatz einmal durchlaufen lassen und uns das gefittete Modell wiedergeben lassen.\n\nrpart_fit &lt;- rpart_wflow |&gt; \n  parsnip::fit(gummi_train_data)\n\nNachdem wir das trainierte Modell vorliegen haben, nutzen wir die Funktion augment() um das Modell auf die Testdaten anzuwenden.\n\nrpart_aug &lt;- augment(rpart_fit, gummi_test_data ) \n\nJetzt geht es los und wir schauen uns einmal an, wie gut die Klassifizierung mit dem Modell funktioniert hat. Als erstes bauen wir uns einmal die Konfusionsmatrix um zu sehen wie gut die beiden Geschlechter in dem Testdatensatz vorhergesagt wurden.\n\nrpart_cm &lt;- rpart_aug |&gt; \n  conf_mat(gender, .pred_class)\n\nrpart_cm\n\n          Truth\nPrediction  m  w\n         m 75 12\n         w 16 72\n\n\nDas freut einen doch. Das sieht ziemlich gut aus. Wir haben auf der Diagonalen fast alle Beoabchtungen und nur sehr wenige falsche Vorhersagen auf der Nichtdiagonalen. Jetzt können wir uns noch eine ganze Reihe an anderen Gütekriterien für den Vergleich von Modellen ausgeben lassen.\n\nrpart_cm |&gt; summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.84 \n 2 kap                  binary         0.680\n 3 sens                 binary         0.824\n 4 spec                 binary         0.857\n 5 ppv                  binary         0.862\n 6 npv                  binary         0.818\n 7 mcc                  binary         0.681\n 8 j_index              binary         0.681\n 9 bal_accuracy         binary         0.841\n10 detection_prevalence binary         0.497\n11 precision            binary         0.862\n12 recall               binary         0.824\n13 f_meas               binary         0.843\n\n\nWir besprechen hier nicht alle, du kannst dann gerne nochmal in dem Kapitel über die Modellvergleiche nachlesen, was die ganze Gütekriterien alles bedeuten. Wenn wir uns auf die Accuarcy konzentrieren, erhalten wir einen guten Wert von 83% richtig klassifizierter Geschlechter. Das ist für echte Daten ohne Tuning und Kreuzvaldierung schon ein echt guter Wert.\nNun schauen wir uns noch schnell die ROC Kurve an und sehen, dass die Kurve schon weit von der Diagonalen entfernt ist. Wir sehen eine gute ROC Kurve. Die AUC sollte auch recht groß sein.\n\nrpart_aug |&gt; \n  roc_curve(gender, .pred_w, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\nAbbildung 70.5— ROC Kurve für den Entscheidungsbaum mit dem rpart Algorithmus.\n\n\n\n\n\nEs gibt viele Möglichkeiten sich einen Entscheidungsbaum anzuschauen. Wir nutzen hier das R Paket {rpart.plot} und die gleichnamige Funktion rpart.plot(). Die vielen Möglichkeiten der Darstellung und der Optionen findest in der Vignette Plotting rpart trees with the rpart.plot package.. Wir gehen hier einmal auf die Variante extra = 101 ein. Es gibt insgesamt elf verschiedene Arten plus eben noch die Möglichkeit 100 zu einer der elf genannten Varianten hinzufügen, um auch den Prozentsatz der Beobachtungen im Knoten anzuzeigen. Zum Beispiel zeigt extra = 101 die Anzahl und den Prozentsatz der Beobachtungen in dem Knoten an.\n\nrpart_fit |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot(roundint = FALSE, extra = 101)\n\n\n\n\n\n\n\nAbbildung 70.6— Visualisierung des finalen rpart Entscheidungsbaums.\n\n\n\n\n\nIn Abbildung 70.6 sehen wir den finalen Entscheidungsbaum. Wir sehen, dass wir nicht weiter als fünf Splits nach unten gewandert sind. Das hatten wir ja auch mit dem Parameter tree_depth so eingestellt. Jetzt sehen wir aber auch, dass wir mit dem Preprocessing auch eine Grube graben können. Wir haben in unserem ersten Knoten 189 Männer und 165 Frauen. Daher hat der Knoten nach Mehrheitsentscheidung den Status m. Jetzt spalten wir den Knoten nach der Körpergröße von \\(0.48\\) in zwei Gruppen. Was soll jetzt \\(0.48\\) heißen? Keine Ahnung. Wir haben die Daten normalisiert. Wenn du hier die Werte für die Splits interpretieren willst, dann musst du auf den Orginaldaten rechnen. Nach dem Split sehen wir zwei Knoten, in denen zum einen die Männer domiern und zum anderen die Frauen. Wir splitten wieder nach der Körpergröße und erhalten immer reinere Knoten in den fast nur noch Männer oder Frauen sind.\nSchaue dir auch die anderen Arten der Visualisierung in rpart.plot an und entscheide, ob dir die anderen Varianten bessere Informationen liefern, die zu deiner wissenschaftlichen Fragestellung passen.\nAn der Stelle trifft dann immer die Klassifikation auf die Interpretation. Du kannst nicht das Modell im Nachgang wieder entnormalisieren. Das geht nicht. Wenn du auf den Orginaldaten rechnest, dann wirst du ein anderes Modell erhalten. Das Modell mag besser oder schlechter sein, auf jeden Fall anders. Wie so oft hängt es von der wissenschaftlichen Fragestellung ab.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Decision trees</span>"
    ]
  },
  {
    "objectID": "classification-randomforest.html#sec-rf",
    "href": "classification-randomforest.html#sec-rf",
    "title": "70  Decision trees",
    "section": "70.4 Random Forest mit ranger",
    "text": "70.4 Random Forest mit ranger\nBis jetzt haben wir einen Entscheidungsbaum wachsen lassen. Was wäre, wenn wir statt einen Baum mehrere Bäume wachsen lassen. Wir lassen einen ganzen Wald (eng. forest) entstehen. Nun macht es wenig Sinn, immer den gleichen Baum auf immer den selben Daten wachsen zu lassen. Daher wählen wir zufällig eine Anzahl an Zeilen und Spalten aus bevor wir einen Baum in unserem Wald wachsen lassen. Dabei bringen wir zwei den Zufall in die Generierung eines Baums mit ein.\n\nDurch die zufällige Auswahl der Beobachtungen mit Zurücklegen. Wir haben also einzelne Zeilen und damit Beobachtungen mehrfach in den Daten.\nDurch die zufällige Auswahl eines Sets an Variablen. Wir nutzen nicht immer alle Variablen in unserem Modell sondern nur ein Set an Spalten.\n\nIm maschinellen Lernen nennen wir diese Methode Bagging. Das Wort Bagging steht für bootstrap aggregating und ist eine Methode, um Vorhersagen aus verschiedenen Modellen zu kombinieren. In unserem Fall sind es die verschiedenen Entscheidungsböume. Dabei müssen alle Modelle mit dem gleichen Algorithmus laufen, können aber auf verschiedenen Datensätzen oder aber Variablensätzen zugreifen. Häufig haben die Modelle eine hohe Varianz in der Vorhersage und wir nutzen dann Bagging um die Modelle miteinander zu kombinieren und dadurch die Varianz zu verringern. Die Ergebnisse der Modelle werden dann im einfachsten Fall gemittelt. Das Ergebnis jeder Modellvorhersage geht mit gleichem Gewicht in die Vorhersage ein. Wir haben auch noch andere Möglichkeiten, aber du kannst dir Vorstellen wir rechnen verschiedene Modelle \\(j\\)-mal und bilden dann ein finales Modell in dem wir alle \\(j\\)-Modelle zusammenfassen. Wie wir die Zusammenfassung rechnen, ist dann immer wieder von Fall zu Fall unterschiedlich. Wir erhalten am Ende einen Ensemble Klassifizierer, da ja ein Ensemble von Modellen zusammengefasst wird. In dem Fall von den Entscheidungsbäumen ist das Ensemble ein Wald an Bäumen.\n\n\n\n\n\n\nParallele CPU Nutzung\n\n\n\n\n\nWenn wir wirklich viele Bäume wachsen lassen wollen, dann bietet sich die parallele Berechnung an. Das können wir über das R Paket {parallel} realisieren. Wir detektieren erstmal wie viele Kerne wir auf dem Rechner zu Verfügung haben.\n\ncores &lt;- parallel::detectCores()\ncores\n\n[1] 8\n\n\nWenn wir das gemacht haben, dann können wir in set_engine(\"ranger\", num.threads = cores) auswählen, dass die Berechnung parallel verlaufen soll. Besonders auf Großrechnern macht die parallele Berechnung am meisten Sinn.\n\n\n\nAuch hier ist es so, dass es verschiedene Algorithmen für den Random Forest gibt. Wir nehmen hier dann den ranger Algorithmus. Du kannst wie immer schauen, welche Algorithmen es noch gibt und auch wiederum verschiedene Algorithmen ausprobieren. In jedem Baum sollen drei Prädiktoren (mtry = 3) und einer Anzahl von mindestens zehn Beobachtungen je Knoten (min_n = 10) und wir wollen insgesamt eintausend Bäume wachsen lassen (trees = 1000). Darüber hinaus wollen wir uns auch die Variable Importance wiedergeben lassen. Die Variable Importance beschreibt, wie gut ein Prädiktor über alle Bäume des Waldes, in der Lage war Splits in möglichst reine Knoten durchzuführen. Ein Prädiktor mit einer hohen Variable Importance, ist also besonders geeignet für gute Splits mit hoher Reinheit.\n\nranger_mod &lt;- rand_forest(mtry = 3, min_n = 10, trees = 1000) |&gt; \n  set_engine(\"ranger\", importance = \"impurity\") |&gt; \n  set_mode(\"classification\")\n\nNun bauen wir uns wieder unseren Workflow indem wir das Modell mit dem Rezept für die Gummidatensatz verbinden. Das tolle ist jetzt, dass wir hier wieder des Rezept vom Anfang verwenden können. Wir müssen also nicht das Rezept neu definieren. Wir bauen uns also einfach nur einen neuen Workflow.\n\nranger_wflow &lt;- workflow() |&gt; \n  add_model(ranger_mod) |&gt; \n  add_recipe(gummi_rec)\n\nWenn wir den Workflow haben, dann können wir wieder mit der Funktion fit() unser Modell anpassen.\n\nranger_fit &lt;- ranger_wflow |&gt; \n  parsnip::fit(gummi_train_data)\n\nIn der Abbildung 70.7 sehen wir dann die Variable Importance sortiert für alle Prädiktoren. Ganz wichtig, die Variable Importance ist nicht numerisch zu interpretieren und auch nicht über verschiedene Datensäze hinweg. Wir können nur die Variable Importance von einem Datensatz anschauen und dort sehen welche Variablen den meisten Einfluss haben. Wir sehen also, dass die Körpergröße eine sehr große Wichtigkeit hat um die Männer von den Frauen in den Gummibärchendaten zu trennen. Das macht auch Sinn. Frauen und Männer sind nun mal unterschiedlich groß. Nicht mehr so wichtig ist das Alter und das Semester. Beide Prädiktoren haben einen ehr geringeren Einfluss auf die Aufteilung der beiden Geschlechter. Der Lieblingsgeschmack tut bei der Einteilung in Männer und Frauen nichts zur Sache.\n\nranger_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  vip(num_features = 20) +\n  theme_minimal()\n\n\n\n\n\n\n\nAbbildung 70.7— Visualisierung der Variable Importance aus unseren ranger Algorithmus.\n\n\n\n\n\nNach unserem kleinen Ausflug zu der Variable Importance können wir jetzt wieder unser Modell auf den Testdatensatz anwenden und schauen, wie gut der Random Forest unsere Geschlechter vorhersagen kann.\n\nranger_aug &lt;- augment(ranger_fit, gummi_test_data ) \n\nNun schauen wir uns an wie gut die Klassifizierung mit dem ranger Modell funktioniert hat. Als erstes bauen wir uns einmal die Konfusionsmatrix um zu sehen wie gut die beiden Geschlechter in dem Testdatensatz vorhergesagt wurden.\n\nranger_cm &lt;- ranger_aug |&gt; \n  conf_mat(gender, .pred_class)\n\nranger_cm\n\n          Truth\nPrediction  m  w\n         m 75 12\n         w 16 72\n\n\nJa, das sieht ähnlich gut aus wie der rpart Algorithmus. Wir haben eine gute Aufspaltung nach dem Geschlechtern. Viele der Beobachtungen liegen auf der Diagonalen und nur wenige Beobachtungen wurden falsch klassifiziert. Jetzt können wir uns noch eine ganze Reihe an anderen Gütekriterien für den Vergleich von Modellen ausgeben lassen.\n\nranger_cm |&gt; summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.84 \n 2 kap                  binary         0.680\n 3 sens                 binary         0.824\n 4 spec                 binary         0.857\n 5 ppv                  binary         0.862\n 6 npv                  binary         0.818\n 7 mcc                  binary         0.681\n 8 j_index              binary         0.681\n 9 bal_accuracy         binary         0.841\n10 detection_prevalence binary         0.497\n11 precision            binary         0.862\n12 recall               binary         0.824\n13 f_meas               binary         0.843\n\n\nWir besprechen wie beim rpart Algorithmus nicht alle Kriterien, du kannst dann gerne nochmal in dem Kapitel über die Modellvergleiche nachlesen, was die ganze Gütekriterien alles bedeuten. Wenn wir uns auf die Accuarcy konzentrieren, erhalten wir einen guten Wert von 84% richtig klassifizierter Geschlechter. Das ist für echte Daten ohne Tuning und Kreuzvaldierung schon ein echt guter Wert.\nNun schauen wir uns noch schnell die ROC Kurve an und sehen, dass die Kurve schon weit von der Diagonalen entfernt ist. Wir sehen eine gute ROC Kurve. Die AUC sollte auch recht groß sein. Damit sind wir mit dem Random Forest Algorithmus soweit durch und wir schauen uns jetzt einen etwas komplexeren xgboost Algorithmus an.\n\nranger_aug |&gt; \n  roc_curve(gender, .pred_w, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\nAbbildung 70.8— ROC Kurve für den Random Forest mit dem ranger Algorithmus.\n\n\n\n\n\n\n\n\n\n\n\nKann ich auch eine Kreuzvalidierung und Tuning für Random Forest durchführen?\n\n\n\nJa, kannst du. Wenn du nur eine Kreuzvalidierung durchführen willst, findest du alles im Kapitel 69 für den \\(k\\)-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf den Random Forest Algorithmus anwenden. Wir nutzen gleich die Kreuzvalidierung in Kombination mit dem Tuning vom xgboost Algorithmus.\nWenn du also den Random Forest Algorithmus auch tunen willst, dann schaue einfach weiter unten nochmal bei dem Tuning des xgboost Algorithmus rein. Es ändert sich kaum was für die Auwahl der Tuning Parameter vom Random Forest Algorithmus.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Decision trees</span>"
    ]
  },
  {
    "objectID": "classification-randomforest.html#sec-xgboost",
    "href": "classification-randomforest.html#sec-xgboost",
    "title": "70  Decision trees",
    "section": "70.5 Gradient boosting mit xgboost",
    "text": "70.5 Gradient boosting mit xgboost\nAls letztes Beispiel für Entscheidungsbäume schauen wir uns das Boosting an. Auch hier haben wir es wieder mit einem Wald an Entscheidungsbäumen zu tun, die wir auch wieder zusammenfassen wollen. Wir verlassen uns also nicht auf die Klassifikation von einem Baum, sondern nehmen die Informationen von vielen Bäumen zusammen. Was ist jetzt der Unterschied zu einem Random Forest? Bei einem Random Forest bauen wir uns im Prinzip hunderte einzelne Bäume und trainieren darauf den Algorithmus. Am Ende fassen wir dann alle Bäume für die Vorhersage zusammen. Beim Boosting nutzen wir die Information des ersten Baumes für das Wachstum des zweiten Baumes und so weiter. Das Boosting verkettet also die Informationen der einzelnen Bäume zu einem kontinuierlichen Lernen. Daher sind Bossting Algorithmen meist sehr gute Klassifizierer.\nWir unterscheiden beim Boosting grob in zwei Kategorien. Zum einen gibt es das adaptive Boosting und das gradient Boosting. Beim adaptiven Boosting erhalten die Beobachtungen über die verschiedenen Klassifizierungsschritte unterschiedliche Gewichte für ihre Bedeutung. In Abbildung 70.9 sehen wir ein Beispiel für den adaboost Algorithmus. Wir haben einen ursprünglichen Datensatz mit blauen und roten Beobachtungen. Wir wollen nun diese Beobachtungen voneinander trennen und damit einen Klassifizierer bauen. Wir fangen mit einem simplen Entscheidungsbaum an, der nur einen Split durchführt. Jetzt haben wir zwei falsch klassifizierte blaue Beobachtungen und eine falsche rote Beobachtung. Nun erhöhen wir das Gewicht dieser drei Beobachtungen. Der nächste Klassifizierer soll nun insbesondere auf diese drei Beobachtungen achten. Wir erhalten daher einen anderen Split und damit zwei blaue Beobachtungen die nicht richtig klassifiziert wurden. Wir erhöhen wieder das Gewicht der beiden falsch klassifizierten blauen Beobachtungen. Der dritte Klassifizierer schafft es jetzt die beiden blauen Beobachtungen gut von den roten Beobachtungen zu trennen. Wir stoppen jetzt hier und bringen alle Klassifiziererregeln, also wo der Split liegen soll, in einen Klassifizierer zusammen.\n\n\n\n\n\n\nAbbildung 70.9— Darstellung von adaptive Boosting an drei Klassifizieren, die nacheinander auf die neu gewichteten Daten angewendet werden. Am Ende werden alle drei Klassifizierer dann in einen Klassifizierer kombiniert.\n\n\n\nIn der Abbildung 70.10 sehen wir die Idee des gradient Boosting einmal dargestellt. Die Idee ist recht simple. Wir wollen wieder nacheinander einen Klassifizierer auf schon klassifizierte Daten anwenden. Wir wollen also das unser zweiter Klassifizierer von dem ersten Klassifizier lernt. Wie machen wir das? Indem wir im ersten Schritt unsere Daten klassifizieren. Wir machen das mit einem Entscheidungsbaum, der mehrere Splits durchführt, die wir dann zu einer eckigen Graden zusammenfassen. Dann haben wir aber einen Fehler als Abstand zu den Splits oder eben zu der Graden. Diese Abstände übertragen wir dann in einen neuen Datensatz auf dem wir dann den nächsten Entscheidungsbaum wachsen lassen. Wir reduzieren also den Fehler des ersten Klassifizierers durch den zweiten Klassifizierer. Dann übertragen wir den Fehler des zweiten Klassifizierers in einen neuen Datensatz und lassen den dritten Klassifizierer den Fehler weiter reduzieren. Am Ende kombinieren wir alle drei Klassifizierer in ein Modell. Durch das gradient Boosting erhalten wir ziemlich gute Entscheidungsbäume, die in der Lage sind sehr schnell und effizient eine Vorhersage zu treffen.\n\n\n\n\n\n\nAbbildung 70.10— Darstellung von gradient Boosting an drei Klassifizieren, die nacheinander auf die Fehler des vorherigen Klassifizierers angewendet werden. Beachte die Nulllinie bei dem Klassifizierer zwei und drei.\n\n\n\nNach dieser theoretischen Einführung wollen wir uns einmal mit der Implementierung beschäftigen. Wir nutzen hier einmal die bekannten Parameter aus dem Random Forest Algorithmus um unseren xgboost Algorithmus zu trainieren. Wie wir gleich noch im Tuning sehen werden, hatr der xgboost Algorithmus noch mehr Parameter an denen du schrauben kannst. In jedem Baum sollen drei Prädiktoren (mtry = 3) und einer Anzahl von mindestens zehn Beobachtungen je Knoten (min_n = 10) und wir wollen insgesamt eintausend Bäume wachsen lassen (trees = 1000).\n\nxgboost_mod &lt;- boost_tree(mtry = 3, min_n = 10, trees = 1000) |&gt; \n  set_engine(\"xgboost\") |&gt; \n  set_mode(\"classification\")\n\nNun bauen wir uns wieder unseren Workflow indem wir das Modell mit dem Rezept für die Gummidatensatz verbinden. Das tolle ist jetzt, dass wir hier wieder des Rezept vom Anfang verwenden können. Wir müssen also nicht das Rezept neu definieren. Wir bauen uns also einfach nur einen neuen Workflow.\n\nxgboost_wflow &lt;- workflow() |&gt; \n  add_model(xgboost_mod) |&gt; \n  add_recipe(gummi_rec)\n\nWenn wir den Workflow haben, dann können wir wieder mit der Funktion fit() unser Modell anpassen. Es ist eine wahre Freude. Ich mache das ja jetzt auch schon hier eine Weile im Skript und es ist echt super, wie gut das funktioniert.\n\nxgboost_fit &lt;- xgboost_wflow |&gt; \n  parsnip::fit(gummi_train_data)\n\nWie auch beim Random Forest Algorithmus können wir uns beim xgboost Algorithmus die Variable Importance wiedergeben lassen. Die Wichtigkeit der Variablen wird in xgboost anhand von drei verschiedenen Wichtigkeiten für eine Variable berechnet. Hier unterscheidet sich dann der Algorithmus xgboost von dem Random Forest Algorithmen. Achtung, wir können nicht einfach die Variable Importance von einem Random Forest Algorithmus mit der eines xgboost Algorithmus vergleichen. Wir kriegen hier andere Werte zurück, die wir dann auch anders interpretieren können.\n\nGain ist der relative Beitrag der entsprechenden Variable zum entgültigen Modell. Wir addieren dafür den Beitrag der Variable für die Splits für jeden Baum auf. Eine höhere Punktzahl deutet darauf hin, dass die Variable für die Vorhersage des Baums wichtiger ist. Die Variable war in der Lage die Klassen gut voneinander zu trennen.\nCover ist die relative Beobachtung, die mit einem Prädiktor verbunden ist. Also der Anteil der Beobachtungen, die mit dieser Variable zusammenhängen. Nehmen wir an Merkmal \\(X_1\\) wird dazu verwendet, um einen Terminalknoten für 10 Beobachtungen in einem Baum zu erschaffen. Im in einem weiteren Baum ist es ein Terminalkonten mit 20 Beobachtungen. Damit haben wir 30 absolute Beobachtungen, die mit Merkmal \\(X_1\\) verbunden sind. Die relative Beobachtung ist dann 30 geteilt durch die Summe aller absoluten Beobachtungen für alle Merkmale.\nHäufigkeit bezieht sich auf die relative Häufigkeit, mit der eine Variable in den zusammengestellten Bäumen vorkommt. Nehmen wir an Merkmal \\(X_1\\) kommt in Baum A in einem Split und in Baum B in zwei Splits vor. Die absolute Häufigkeit von Merkmal \\(X_1\\) ist 3 und die relative Häufigkeit ist dann 3 durch die Summe aller absoluten Vorkommen für alle Merkmale.\n\nSchauen wir uns also einmal die Kriterien der Variable Importance für unsere Gummibärchendaten einmal an. Gehen wir mal die Parameter gain, cover und frequency einmal für unsere Körpergröße durch. Zuerst hat die Körpergröße den höchsten Wert in gain mit \\(0.84\\). Da wir das Gain auf 1 skaliert haben, macht die Körpergröße 84% des gesamten Gain in dem Modell aus. Daher wissen wir, dass die Körpergröße einen überaus bedeutenden Anteil an der Vorhersage des Geschlechts hat. Im Weiteren sehen wir an dem Parameter cover, dass in 34% der Beobachtungen ein Split mit der Körpergröße vorausgeht. Das heißt, 34% der Beobachtungen wurden anhand der Körpergröße aufgeteilt. Da wir nicht wissen wie viele Splits es ingesamt gab, muss man dieses Wert immer etwas vorsichtig bewerten. Die frequency teilt uns mit, dass in 33% der der Splits auch die Körpergröße vor kam. Wir sehen, die Körpergröße ist wichtig für die Vorhersage des Geschlechts. Wenn Variablen fehlen, dann haben diese keinen Einfluss auf die Klassifikation gehabt.\n\nxg_imp &lt;- xgboost_fit |&gt; \nextract_fit_parsnip() %$% \n  xgboost::xgb.importance(model = fit) |&gt; \n  mutate(across(where(is.numeric), round, 2))\n\nxg_imp\n\n              Feature  Gain Cover Frequency\n               &lt;char&gt; &lt;num&gt; &lt;num&gt;     &lt;num&gt;\n1:             height  0.78  0.33      0.34\n2:                age  0.11  0.28      0.28\n3:           semester  0.07  0.22      0.22\n4: most_liked_darkred  0.02  0.11      0.12\n5:   most_liked_green  0.01  0.05      0.04\n6:   most_liked_white  0.00  0.00      0.00\n\n\nIn der Abbildung 70.11 sehen wir dann die Variable Importance sortiert für alle Prädiktoren und eingeteilt in Cluster. Die Funktion xgb.ggplot.importance() versucht ähnlich bedeutende Prädiktoren in gleiche Cluster zuzuordnen.\n\nxg_imp |&gt; \n  xgb.ggplot.importance() +\n  theme_minimal() +\n  scale_fill_okabeito()\n\n\n\n\n\n\n\nAbbildung 70.11— Visualisierung der Variable Importance aus unseren xgboost Algorithmus. Wir sehen, dass sich grob drei Gruppen für Bedeutung der Variablen für die Klassifikation gebildet haben.\n\n\n\n\n\nNach unserem kleinen Ausflug zu der Variable Importance können wir jetzt wieder unser xgboost Modell auf den Testdatensatz anwenden und schauen, wie gut das gradient Boosting unsere Geschlechter vorhersagen kann.\n\nxgboost_aug &lt;- augment(xgboost_fit, gummi_test_data ) \n\nNun schauen wir uns an wie gut die Klassifizierung mit dem xgboost Modell funktioniert hat. Als erstes bauen wir uns einmal die Konfusionsmatrix um zu sehen wie gut die beiden Geschlechter in dem Testdatensatz vorhergesagt wurden.\n\nxgboost_cm &lt;- xgboost_aug |&gt; \n  conf_mat(gender, .pred_class)\n\nxgboost_cm\n\n          Truth\nPrediction  m  w\n         m 75 11\n         w 16 73\n\n\nJa, das sieht ähnlich gut aus wie der Random Forest Algorithmus. Wir haben eine gute Aufspaltung nach dem Geschlechtern. Viele der Beobachtungen liegen auf der Diagonalen und nur wenige Beobachtungen wurden falsch klassifiziert. Jetzt können wir uns noch eine ganze Reihe an anderen Gütekriterien für den Vergleich von Modellen ausgeben lassen.\n\nxgboost_cm |&gt; summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.846\n 2 kap                  binary         0.692\n 3 sens                 binary         0.824\n 4 spec                 binary         0.869\n 5 ppv                  binary         0.872\n 6 npv                  binary         0.820\n 7 mcc                  binary         0.693\n 8 j_index              binary         0.693\n 9 bal_accuracy         binary         0.847\n10 detection_prevalence binary         0.491\n11 precision            binary         0.872\n12 recall               binary         0.824\n13 f_meas               binary         0.847\n\n\nWier vorher schon besprechen wir nicht alle Kriterien, du kannst dann gerne nochmal in dem Kapitel über die Modellvergleiche nachlesen, was die ganze Gütekriterien alles bedeuten. Wenn wir uns auf die Accuarcy konzentrieren, erhalten wir einen guten Wert von 86% richtig klassifizierter Geschlechter. Besonders die Sensitivität ist mit 92% sehr gut. Die Sensitivität gibt ja an, wie zuverlässig unser xgboost Algorithmus erkennt, ob man eine Frau ist. Die Spezifität ist etwas niedriger, also die Fähigkeit die Männer auch als Männer zu erkennen. Das ist für echte Daten ohne Tuning und Kreuzvaldierung schon ein echt sehr guter Wert. Da sind wir noch besser als beim Random Forest.\nNun schauen wir uns noch schnell die ROC Kurve an und sehen, dass die Kurve schon weit von der Diagonalen entfernt ist. Wir sehen eine gute ROC Kurve. Die AUC sollte auch recht groß sein. In den folgenden Schritten wollen wir einmal den xgboost Algorithmus tunen und schauen, ob wir noch bessere Ergebnisse für die Klassifikation mit anderen Parametern für den Algorithmus hin bekommen.\n\nxgboost_aug |&gt; \n  roc_curve(gender, .pred_w, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\nAbbildung 70.12— ROC Kurve für den Entscheidungsbaum mit dem xgboost Algorithmus.\n\n\n\n\n\n\n\n\n\n\n\nKann ich auch eine Kreuzvalidierung für xgboost durchführen?\n\n\n\nJa, kannst du. Wenn du nur eine Kreuzvalidierung durchführen willst, findest du alles im Kapitel 69 für den \\(k\\)-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf den xgboost Algorithmus anwenden. Wir nutzen gleich die Kreuzvalidierung in Kombination mit dem Tuning vom xgboost Algorithmus.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Decision trees</span>"
    ]
  },
  {
    "objectID": "classification-randomforest.html#tuning",
    "href": "classification-randomforest.html#tuning",
    "title": "70  Decision trees",
    "section": "70.6 Tuning",
    "text": "70.6 Tuning\nWas heißt Tuning? Wie bei einem Auto können wir an verschiedenen Stellschrauben bei einem mathematischen Algorithmus schrauben. Welche Schrauben und Teile das sind, hängt dann wieder vom Algorithmus ab. Im Falle des xgboost Algorithmus können wir an folgenden Parametern drehen und jeweils schauen, was dann mit unserer Vorhersage passiert. Insgesamt hat der xgboost Algorithmus acht Tuningparameter, wir wählen jetzt für uns hier drei aus. Ich nehme hier auch nur drei Parameter, da sich dann drei Parameter noch sehr gut visuell darstellen lassen. In der Anwendung wäre dann natürlich besser alle Parameter zu tunen, aber das dauert dann auch lange.\n\nmtry, zufällig ausgewählte Anzahl an Variablen für jeden Baum. Das heißt, für jeden Baum werden von unseren Variablen die Anzahl mtry zufällig ausgewählt und auf diesem kleineren Datensatz der Baum erstellt.\nmin_n, kleinste Knotengröße, die noch akzeptiert wird. Wenn ein Knoten unter min_n fällt, dann endet hier das Wachstum des Baumes.\ntrees, Anzahl der Bäume die in einem xgboost Algorithmus erstellt werden.\n\nNun ist es so, dass wir natürlich nicht händisch alle möglichen Kombinationen von der Anzahl der ausgewählten Variablen pro Baum, der kleinsten Knotengröße und der Anzahl der Bäume berechnen wollen. Das sind ziemlich viele Kombinationen und wir kommen dann vermutlich schnell durcheinander. Deshalb gibt es die Funktion tune() aus dem R Paket {tune}, die uns einen Prozess anbietet, das Tuning automatisiert durchzuführen.\nAls erstes müssen wir uns ein Objekt bauen, das aussieht wie ein ganz normales Modell in der Klassifikation. Aber wir ergänzen jetzt noch hinter jeder zu tunenden Option noch die Funktion tune(). Das sind die Parameter des Algorithmus, die wir später tunen wollen.\n\ntune_spec &lt;-  boost_tree(mtry = tune(), \n                         min_n = tune(), \n                         trees = tune()) |&gt; \n  set_engine(\"xgboost\") |&gt; \n  set_mode(\"classification\")\n\ntune_spec\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = tune()\n  min_n = tune()\n\nComputational engine: xgboost \n\n\nJetzt bauen wir uns den Workflow indem wir statt unserem Modell, die Tuninganweisung in den Workflow reinnehmen. Echt simpel und straightforward. Das Rezept bleibt ja das Gleiche.\n\ngummi_tune_wflow &lt;- workflow() |&gt; \n  add_model(tune_spec) |&gt; \n  add_recipe(gummi_rec)\n\nJetzt müssen wir noch alle Kombinationen aus den drei Parametern mtry, min_n und trees ermitteln. Das macht die Funktion grid_regular(). Es gibt da noch andere Funktionen in dem R Paket {tune}, aber ich konzentriere mich hier auf die einfachste. Jetzt müssen wir noch die Anzahl an Kombinationen festlegen. Ich möchte für jeden Parameter fünf Werte tunen. Daher nutze ich hier die Option levels = 5 auch damit hier die Ausführung nicht so lange läuft. Fange am besten mit levels = 5 an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert. Dann kannst du die Levels noch hochschrauben. Beachte aber, dass mehr Level nur mehr Zwischenschritte bedeutet. Jede Option hat eine Spannweite range, die du dann anpassen musst, wenn du höhere Werte haben willst. Mehr Level würden nur mehr Zwischenschritte bedeuten. In unserem Fall weiß zum Beispiel die Funktion mtry() nicht, wie viele Variablen in dem Datensatz sind. Wir müssen also die range für die Anzahl an ausgewählten Variablen selber setzen. Ich wähle daher eine Variable bis vier Variablen.\n\ngummi_grid &lt;- grid_regular(mtry(range = c(1, 4)),\n                           trees(),\n                           min_n(),\n                           levels = 5)\n\nDas Tuning nur auf dem Trainingsdatensatz durchzuführen ist nicht so eine gute Idee. Deshalb nutzen wir hier auch die Kreuzvalidierung. Eigentlich ist eine 10-fache Kreuzvalidierung mit \\(v=10\\) besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur \\(v=5\\) gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.\n\ngummi_folds &lt;- vfold_cv(gummi_train_data, v = 5)\n\nNun bringen wir den Workflow zusammen mit dem Tuninggrid und unseren Sets der Kreuzvaidierung. Daher pipen wir den Workflow in die Funktion tune_grid(). Als Optionen brauchen wir die Kreuzvaldierungsdatensätze und das Tuninggrid. Wenn du control_grid(verbose = TRUE) wählst, dann erhälst du eine Ausgabe wie weit das Tuning gerade ist. Achtung!, das Tuning dauert seine Zeit. Im Falle des xgboost Algorithmus dauert das Tuning zwar nicht so lange, aber immer noch ein paar Minuten. Wenn du dann alle acht Parameter des xgboost Algorithmustunen wollen würdest, dann würde die Berechnung sehr viel länger dauern. Du kannst das Ergebnis des simpleren Tunings auch in der Datei gummi_xgboost_tune_res.rds finden.\n\ngummi_tune_res &lt;- gummi_tune_wflow |&gt; \n   tune_grid(resamples = gummi_folds,\n             grid = gummi_grid,\n             control = control_grid(verbose = FALSE))\n\nDamit du nicht das Tuning durchlaufen lassen musst, habe ich das Tuning in die Datei gummi_xgboost_tune_res.rds abgespeichert und du kannst dann über die Funktion read_rds() wieder einlesen. Dann kannst du den R Code hier wieder weiter ausführen.\nNachdem das Tuning durchgelaufen ist, können wir uns über die Funktion collect_metrics(), die Ergebnisse des Tunings für jede Kombination der drei Parameter mtry, min_n und trees wiedergeben lassen. Diese Ausgabe ist super unübersichtlich. Ich habe mich ja am Anfange des Abschnitts auch für drei Tuningparameter entschieden, da sich dann diese drei Parameter noch gut visualisieren lassen. Deshalb einmal die Abbildung der mittleren Accuarcy und der mittleren AUC-Werte über alle Kreuzvalidierungen.\n\ngummi_tune_res |&gt;\n  collect_metrics() |&gt;\n  mutate(trees = as_factor(trees),\n         min_n = as_factor(min_n)) |&gt;\n  ggplot(aes(mtry, mean, color = min_n, linetype = trees)) +\n  theme_minimal() +\n  geom_line(alpha = 0.6) +\n  geom_point() +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 70.13— Tuning Kurven für den xgboost Algorithmus.\n\n\n\n\n\nDamit wir nicht händisch uns die beste Kombination raussuchen müssen, können wir die Funktion show_best() nutzen. Wir wählen hier die beste Accuarcy und erhalten dann die sortierten Ergebnisse nach der Accuarcy des Tunings.\n\ngummi_tune_res |&gt;\n  show_best(\"accuracy\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric  .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     2  1000    11 accuracy binary     0.839     5  0.0105 Preprocessor1_Model…\n2     2  1500    11 accuracy binary     0.839     5  0.0105 Preprocessor1_Model…\n3     2  2000    11 accuracy binary     0.839     5  0.0105 Preprocessor1_Model…\n4     4   500    11 accuracy binary     0.839     5  0.0138 Preprocessor1_Model…\n5     4  1000    11 accuracy binary     0.839     5  0.0138 Preprocessor1_Model…\n\n\nDas war die Funktion show_best() aber wir können uns auch die gleich die besten Parameter nach der Accuracy raus ziehen. Das Rausziehen der besten Parameter macht für uns die Funktion select_best().\n\nbest_xgboost &lt;- gummi_tune_res |&gt;\n  select_best(\"accuracy\")\n\nbest_xgboost\n\n# A tibble: 1 × 4\n   mtry trees min_n .config               \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 \n1     2  1000    11 Preprocessor1_Model033\n\n\nWir sehen, dass wir mtry = 3 wählen sollten. Dann müssen wir als Anzahl der Bäume trees = 1000 nutzen. Die minimale Anzahl an Beobachtungen pro Knoten ist dann 11. Müssen wir jetzt die Zahlen wieder in ein Modell eingeben? Nein, müssen wir nicht. Mit der Funktion finalize_workflow() können wir dann die besten Parameter aus unserem Tuning gleich mit dem Workflow kombinieren. Dann haben wir unseren finalen, getunten Workflow. Du siehst dann auch in der Ausgabe, dass die neuen Parameter in dem xgboost Algorithmus übernommen wurden.\n\nfinal_gummi_wf &lt;- gummi_tune_wflow |&gt; \n  finalize_workflow(best_xgboost)\n\nfinal_gummi_wf \n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_mean()\n• step_impute_bag()\n• step_range()\n• step_dummy()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 2\n  trees = 1000\n  min_n = 11\n\nComputational engine: xgboost \n\n\nJetzt bleibt uns nur noch der letzte Fit übrig. Wir wollen unseren finalen, getunten Workflow auf die Testdaten anwenden. Dafür gibt es dann auch die passende Funktion. Das macht für uns die Funktion last_fit(), die sich dann die Informationen für die Trainings- und Testdaten aus unserem Datensplit von ganz am Anfang extrahiert.\n\nfinal_fit &lt;- final_gummi_wf |&gt;\n  last_fit(gummi_data_split) \n\nDa wir immer noch eine Kreuzvaldierung rechnen, müssen wir dann natürlich wieder alle Informationen über alle Kreuzvaldierungsdatensätze einsammeln. Dann erhalten wir unsere beiden Gütekriterien für die Klassifikation des Geschlechts unser Studierenden nach dem xgboost Algorithmus. Die Zahlen sind schon gut für echte Daten. Eine Accuracy von 84% bedeutet das wir über acht von zehn Studierenden richtig klassifizieren. Die AUC ist auch schon fast hervorragend, wir bringen kaum Label durcheinander.\n\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.84  Preprocessor1_Model1\n2 roc_auc  binary         0.932 Preprocessor1_Model1\n\n\nDann bleibt uns nur noch die ROC Kurve zu visualisieren. Da wir wieder etwas faul sind, nutzen wir die Funktion autoplot(). Als Alternative geht natürlich auch das R Paket {pROC}, was eine Menge mehr Funktionen und Möglichkeiten bietet.\n\nfinal_fit |&gt;\n  collect_predictions() |&gt; \n  roc_curve(gender, .pred_w, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\nAbbildung 70.14— ROC Kurve für den Entscheidungsbaum mit dem xgboost Algorithmus nach der Kreuvalidierung und dem Tuning.\n\n\n\n\n\nEine gute ROC Kurve würde senkrecht nach oben gehen und dann waagrecht nach rechts. Dann hätten wir eine AUC von 1 und eine perfekte Separation der beiden Label durch unseren Algorithmus. Unser Algorithmus würde jedem weiblichen Studierenden in dem Testdatensatz korrekt dem Geschlecht w zuweisen. Da wir eine ROC Kurve hier vorliegen haben, die sehr weit weg von der Diagonalen ist, haben wir sehr viele richtig vorhergesagte Studierende in unseren Testdaten. Unser Modell funktioniert um das Geschlecht von Studierenden anhand unserer Gummibärchendaten vorherzusagen.\n\n\n\nAbbildung 70.1— Darstellung des Anwachsen des Entscheidungsbaumes. Links sind die beiden Prädiktoren \\(X_1\\) und \\(X_2\\) als Koordinatensysten dargestellt. Die Punkte stllen die Beobachtungen mit den jeweiligen Label weiß und schwarz dar. Rechts ist der Knoten \\(t_1\\) dargestellt, der alle Beobachtungen beinhaltet..\nAbbildung 70.2— Darstellung des ersten Splits anhand des Prädiktors \\(X_1\\). Wir wählen den Wert \\(c_1\\) für den Split so, dass wir möglichst reine Knoten produzieren. Wir erhalten zwei neue Knoten \\(t_2\\) und \\(t_3\\). Der Knoten \\(t_3\\) ist maximal rein und wird daher zu einem Terminalknoten.\nAbbildung 70.3— Darstellung des zweiten Splits anhand des Prädiktors \\(X_2\\). Wir wählen wiederum den Wert \\(c_2\\) für den Split so, dass wir möglichst reine Knoten erhalten. So erhalten wir zwei neue Knoten \\(t_4\\) und \\(t_5\\). Da nun \\(t_4\\) ebenfalls ein reiner Knoten ist, wird der Knoten \\(t_4\\) ebenfalls zu einem Terminalknoten. Wir stoppen hier das Wachstum, da mir eine mindest Anzahl von vier Beobachtungen in den Knoten erreicht haben.\nAbbildung 70.4— Darstellung der Vorhersage einer neuen Beobachtung mit Werten für die Prädiktoren \\(X_1\\) und \\(X_2\\). Unsere neue Beobachtung ? fällt in den Terminalknoten \\(t_5\\). Dort zählen wir die schwarzen Kreise. Wir stellen fest, dass die neue Beobachtung mit 25% Wahrscheinlichkeit ein Fall und damit schwarz ist. Daher ist die neue Beobachtung weiß.\nAbbildung 70.5— ROC Kurve für den Entscheidungsbaum mit dem rpart Algorithmus.\nAbbildung 70.6— Visualisierung des finalen rpart Entscheidungsbaums.\nAbbildung 70.7— Visualisierung der Variable Importance aus unseren ranger Algorithmus.\nAbbildung 70.8— ROC Kurve für den Random Forest mit dem ranger Algorithmus.\nAbbildung 70.9— Darstellung von adaptive Boosting an drei Klassifizieren, die nacheinander auf die neu gewichteten Daten angewendet werden. Am Ende werden alle drei Klassifizierer dann in einen Klassifizierer kombiniert.\nAbbildung 70.10— Darstellung von gradient Boosting an drei Klassifizieren, die nacheinander auf die Fehler des vorherigen Klassifizierers angewendet werden. Beachte die Nulllinie bei dem Klassifizierer zwei und drei.\nAbbildung 70.11— Visualisierung der Variable Importance aus unseren xgboost Algorithmus. Wir sehen, dass sich grob drei Gruppen für Bedeutung der Variablen für die Klassifikation gebildet haben.\nAbbildung 70.12— ROC Kurve für den Entscheidungsbaum mit dem xgboost Algorithmus.\nAbbildung 70.13— Tuning Kurven für den xgboost Algorithmus.\nAbbildung 70.14— ROC Kurve für den Entscheidungsbaum mit dem xgboost Algorithmus nach der Kreuvalidierung und dem Tuning.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>Decision trees</span>"
    ]
  },
  {
    "objectID": "classification-svm.html",
    "href": "classification-svm.html",
    "title": "71  Support vector machines",
    "section": "",
    "text": "71.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor, see, conflicted)\nconflicts_prefer(magrittr::set_names)\n##\nset.seed(2025429)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Support vector machines</span>"
    ]
  },
  {
    "objectID": "classification-svm.html#daten",
    "href": "classification-svm.html#daten",
    "title": "71  Support vector machines",
    "section": "71.2 Daten",
    "text": "71.2 Daten\nIn diesem Kapitel wollen wir uns auch auf einen echten Datensatz konzentrieren. Wir nutzen daher einmal den Gummibärchendatensatz. Als unser Label und daher als unser Outcome nehmen wir das Geschlecht gender. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir nur die Spalte Geschlecht sowie als Prädiktoren die Spalten most_liked, age, semester, und height.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") |&gt; \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) |&gt; \n  select(gender, most_liked, age, semester, height) |&gt; \n  drop_na(gender)\n\nWir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein NA in der Variable gender haben. Wir haben dann insgesamt \\(n = 699\\) Beobachtungen vorliegen. In Tabelle 67.5 sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.\n\n\n\n\nTabelle 71.1— Auszug aus dem Daten zu den Gummibärchendaten.\n\n\n\n\n\n\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\n\nm\nlightred\n35\n10\n193\n\n\nw\nyellow\n21\n6\n159\n\n\nw\nwhite\n21\n6\n159\n\n\nw\nwhite\n36\n10\n180\n\n\nm\nwhite\n22\n3\n180\n\n\nm\ngreen\n22\n3\n180\n\n\n…\n…\n…\n…\n…\n\n\nm\ndarkred\n24\n2\n193\n\n\nm\nwhite\n27\n2\n189\n\n\nm\ndarkred\n24\n2\n187\n\n\nm\ngreen\n24\n2\n182\n\n\nw\nwhite\n23\n2\n170\n\n\nw\ngreen\n24\n2\n180\n\n\n\n\n\n\n\n\nUnsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Um die Klassifikation mit Entscheidungsbäumen rechnen zu können brauchen wir wie bei allen anderen Algorithmen auch einen Trainings- und Testdatensatz. Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf.\nIm maschinellen Lernen sind alle Datensätze, die weniger als tausend Beobachtungen vorliegen haben, klein.\n\ngummi_data_split &lt;- initial_split(gummi_tbl, prop = 3/4)\n\nWir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluß einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.\n\ngummi_train_data &lt;- training(gummi_data_split)\ngummi_test_data  &lt;- testing(gummi_data_split)\n\nNachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht gender als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem . Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_train_data) |&gt; \n  step_impute_mean(all_numeric_predictors()) |&gt; \n  step_impute_bag(all_nominal_predictors()) |&gt; \n  step_range(all_numeric_predictors(), min = 0, max = 1) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_nzv(all_predictors())\n\ngummi_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Bagged tree imputation for: all_nominal_predictors()\n\n\n• Range scaling to [0,1] for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_predictors()\n\n\nAlles in allem haben wir ein sehr kleines Modell. Wir haben ja nur ein Outcome und vier Prädiktoren.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Support vector machines</span>"
    ]
  },
  {
    "objectID": "classification-svm.html#theoretischer-hintergrund",
    "href": "classification-svm.html#theoretischer-hintergrund",
    "title": "71  Support vector machines",
    "section": "71.3 Theoretischer Hintergrund",
    "text": "71.3 Theoretischer Hintergrund\nDer theoretische Hintergrund zu dem SVM Algorithmus ist sehr mathematisch. So mathematisch, dass wir hier daraus keinen tieferen Nutzen mehr ziehen. Hier geht es ja um die Anwendung des SVM Algorithmus und nicht um das tiefere mathematische Verständnis. Wie immer gibt es sehr viele Möglichkeiten sich tiefer mit der Mathematik hinter dem SVM Algorithmus zu beschäftigen. Hier wollen wir das nicht.\n\n\nEs gibt wir immer ein schönes (mathematisches) Tutorial zu den Support vector machines. Von dort ist auch das Beispiel mit den farbigen Kugeln entnommen.\nDaher wollen wir mal den SVM Algorithmus etwas anders verstehen. Wir nutzen wieder die Idee, dass wir farbige Punkte oder Bälle voneinander trennen wollen. Im Prinzip kannst du dir die Bälle in der Abbildung 71.1 genau so vorstellen. Wir haben dort sieben gesunde Personen als blaue Kugeln und vier kranke Personen als rote Kugeln, die wir trennen wollen.\n\n\n\n\n\n\nAbbildung 71.1— Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen die Personen und die rote die kranken Personen dar.\n\n\n\nIn Abbildung 71.2 zeichnen wir eine Gerade, die die Patienten gut voneinander trennt. Auf der einen Seite der Geraden sind die sieben gesunden Patienten und auf der anderen Seite der Geraden die vier kranken Personen.\n\n\n\n\n\n\nAbbildung 71.2— Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Wir trennen die gesunden Patienten von den kranken Patienten mit einer Geraden.\n\n\n\nNun kommt zu unserem Trainingsdatensatz ein Schwall neuer Patienten hinzu und wir ergänzen die Beobachtungen in der Abbildung 71.3. Wir haben immer noch unsere ursprüngliche Gerade, aber diese Gerade trennt die neuen Beobachtungen nicht mehr gut auf. Ein kranker Patient ist auf der falschen Seite der Geraden. Es gibt wahrscheinlich einen besseren Platz, um die Gerade jetzt zu platzieren.\n\n\n\n\n\n\nAbbildung 71.3— Darstellung von elf gesunden Beobachtungen und acht kranken Beobachtungen aus dem neuen, angewachsenen Traingsdatensatz. Die Gerade trennt die Beobachtugen nur noch ungünstig.\n\n\n\nIn der Abbildung 71.4 sehen wir die Vorgegehensweise des SVM Algorithmus. Der SVM Algorithmus versucht die Gerade an der bestmöglichen Stelle zu platzieren, indem der Algorithmus auf beiden Seiten der Geraden einen möglichst großen Abstand einhalten.\n\n\n\n\n\n\nAbbildung 71.4— Visualisierung des SVM Algorithmus an den ursprünglichen elf Beobachtungen.\n\n\n\nWenn wir jetzt in der Abbildung 71.5 wieder zu unserem angewachsenen Trainingsdaten zurückkehren, sehen wir, dass unsere Klassifikation der gesunden und kranken Beobachtungen gut funktioniert. Der SVM Algorithmus hat durch den optimierten Abstand der Geraden einen optimalen Klassifikator gefunden.\n\n\n\n\n\n\nAbbildung 71.5— Darstellung von elf gesunden Beobachtungen und acht kranken Beobachtungen aus dem neuen, angewachsenen Traingsdatensatz mit der SVM optimierten Geraden.\n\n\n\nNun gibt es aber neben der Geraden noch einen anderen Trick, den wir mit dem SVM Algorithmus durchführen können. Schauen wir uns dazu einmal die Abbildung 71.6 an. Wir sehen in dem neuen Trainingsdatensatz fünf gesunde und fünf kranke Beobachtungen. nur sind diese Beobachtungen nicht mehr so verteilt, dass wir die Beobachtungen mit einer Geraden trennen könnten. Hier kommt jetzt der Kerneltrick des SVM Algorithmus zu tragen.\n\n\n\n\n\n\nAbbildung 71.6— Darstellung von zehn Beobachtungen aus einem weiteren Traingsdatensatz. Die blauen Kugeln stellen die fünf gesunden Personen und die rote die fünf kranken Personen dar.\n\n\n\nWir können mit keiner Geraden der Welt die Punkte voneinander trennen. Jetzt nutzen wir den Kerneltrick in Abbildung 71.7 um unsere 2-D Abbildung in eine 3-D Abbildung umzuwandeln. Jetzt können wir mit einer Ebene die Patienten voneinander trennen. Wir bringen also unsere Beobachtungen durch eine Transformation in eine andere Dimension und können in dieser Dimension die Beobachtungen mit einer Ebene trennen.\n\n\n\n\n\n\nAbbildung 71.7— Umwandlung des Input Space in einen beliebigen Feature Space durch den Kernel \\(\\Phi\\).\n\n\n\nWenn wir dann die Ebene wieder zurücktransfomieren erhalten wir eine kurvige Linie, die unsere Beobachtungen in Abbildung 71.8 voneinander trennt.\n\n\n\n\n\n\nAbbildung 71.8— Rücktransformation der Ebene aus dem Feature Space in den Input Space. Wir haben dann eine Schlangenlinie, die die Beobachtungen voneinander trennt.\n\n\n\nDas war jetzt eine sehr bildliche Darstellung des SVM Algorithmus. Aber im Prinzip ist das die Idee. Wir machen den Kernel Trick nur matematisch komplizierter und auch die Rücktransformation ist nicht simpel. Das müssen wir aber auch nicht selber für uns machen, denn dafür haben wir ja einen Computer. Das eigentliche Problem ist die Wahl des korrekten Kernels. Und das ist eigentlich auch die Qual der Wahl. Wir müssen vorab festlegen, welcher Kernel es sein soll. Und da geht dann das Tuning los.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Support vector machines</span>"
    ]
  },
  {
    "objectID": "classification-svm.html#svm-algorithm",
    "href": "classification-svm.html#svm-algorithm",
    "title": "71  Support vector machines",
    "section": "71.4 SVM Algorithm",
    "text": "71.4 SVM Algorithm\nLeider ist es nicht so, dass wir eine SVM Funktion haben. Wir haben insgesamt drei Funktionen. Jede dieser Funktionen entspricht einem Kernel und muss getrennt voneinander einem Tuning unterzogen werden. Wir haben folgende Funktionen mit den entsprechenden Kernels zu Verfügung.\n\nsvm_linear heißt, wir nehmen einen linearen Zusammenhang an. Wir können die Beobachtungen mit einer einfachen Gerade voneinander trennen.\nsvm_poly heißt, wir nehmen ein Polynom eines bestimmten Gerades und glauben, dass wir mit diesem Kernel die Beobachtungen voneinander trennen können.\nsvm_rbf_mod heißt, wir haben einen radialen Kernel und hoffen, dass wir mit einer radialen Funktion die Beobachtungen trennen können.\n\nUnd damit geht das Leid eigentlich schon los. Wir können gar nicht wissen, welcher der drei SVM Algorithmen am besten auf unsere Daten passt. Also müssen wir alle drei einemal anwenden. Dann müssten wir eigentlich auch alle drei Algorithmen einem Tuning unterziehen. Du siehst, es wird viel Arbeit. Wir lassen hier das Tuning weg und ich zeige dir, wie du mit der Funktion map() dir etwas Arbeit ersparen kannst.\nAls erstes wollen wir den linearen Kernel einmal definieren. Wir haben hier zwei Parameter die wir einem Tuning unterziehen könnten.\n\nsvm_lin_mod &lt;- svm_linear(cost = 1, margin = 0.1) |&gt; \n  set_engine(\"kernlab\") |&gt; \n  set_mode(\"classification\") \n\nAls zweites schauen wir uns den polynominale Kernel an und setzen einmal den Grade des Polynomes auf drei. Einfach mal so aus dem Bauch raus um zu zeigen, was dann so passieren kann.\n\nsvm_poly_mod &lt;- svm_poly(cost = 1, margin = 0.1, degree = 3) |&gt; \n  set_engine(\"kernlab\") |&gt; \n  set_mode(\"classification\") \n\nAls letztes schauen wir uns noch den radialen Kernel einmal an. Auch hier haben wir nur zwei Tuningparameter zu Verfügung.\n\nsvm_rbf_mod &lt;- svm_rbf(cost = 1, margin = 0.1) |&gt; \n  set_engine(\"kernlab\") |&gt; \n  set_mode(\"classification\") \n\nJetzt machen wir alles in einem Schritt. Was wir vorher in mehreren Schritten gemaht haben, machen wir jetzt auf einer Liste lst() in der die Modelle der drei Kernel definiert sind. Wir nutzen die Funktion map() um auf dieser Liste die Workflows mit dem Rezept der Gummibärchen zu initialisieren. Dann Pipen wir die Workflows weiter in die fit() Funktion und wollen dann danach auch gleich die Vorhersage auf dem Testdatensatz rechnen. Danach wählen wir dann auf allen Listen noch gender und die Vorhersagen als die pred-Spalten aus.\n\nsvm_aug_lst &lt;- lst(svm_lin_mod,\n                   svm_poly_mod,\n                   svm_rbf_mod) |&gt; \n  map(~workflow(gummi_rec, .x)) |&gt; \n  map(~fit(.x, gummi_train_data)) |&gt; \n  map(~augment(.x, gummi_test_data)) |&gt; \n  map(~select(.x, gender, matches(\"pred\")))\n\n Setting default kernel parameters  \n\nsvm_aug_lst\n\n$svm_lin_mod\n# A tibble: 175 × 4\n   gender .pred_class .pred_m .pred_w\n   &lt;fct&gt;  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 m      m            0.691    0.309\n 2 w      w            0.0463   0.954\n 3 m      w            0.200    0.800\n 4 m      m            0.789    0.211\n 5 w      w            0.0659   0.934\n 6 m      m            0.838    0.162\n 7 m      m            0.804    0.196\n 8 w      w            0.0462   0.954\n 9 m      w            0.416    0.584\n10 w      w            0.0503   0.950\n# ℹ 165 more rows\n\n$svm_poly_mod\n# A tibble: 175 × 4\n   gender .pred_class .pred_m .pred_w\n   &lt;fct&gt;  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 m      m             0.608   0.392\n 2 w      w             0.408   0.592\n 3 m      w             0.439   0.561\n 4 m      m             0.543   0.457\n 5 w      w             0.378   0.622\n 6 m      m             0.638   0.362\n 7 m      m             0.563   0.437\n 8 w      w             0.390   0.610\n 9 m      w             0.447   0.553\n10 w      w             0.172   0.828\n# ℹ 165 more rows\n\n$svm_rbf_mod\n# A tibble: 175 × 4\n   gender .pred_class .pred_m .pred_w\n   &lt;fct&gt;  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 m      m            0.863   0.137 \n 2 w      w            0.0610  0.939 \n 3 m      w            0.193   0.807 \n 4 m      m            0.887   0.113 \n 5 w      w            0.0415  0.959 \n 6 m      m            0.908   0.0916\n 7 m      m            0.856   0.144 \n 8 w      w            0.0598  0.940 \n 9 m      w            0.383   0.617 \n10 w      w            0.0726  0.927 \n# ℹ 165 more rows\n\n\nJetzt haben wir also alles als eine Liste vorliegen. Das macht uns dann die weitere Darstellung einfach. Wenn du einen Listeneintrag haben willst, dann kannst du auch mit der Funktion pluck() dir einen Eintrag nach dem Namen herausziehen. Wenn du den Listeneintrag $svm_rbf_mod willst, dann nutze pluck(svn_aug_lst, \"svm_rbf_mod\").\n\n\n\n\n\n\nKann ich auch eine Kreuzvalidierung und Tuning für die Support Vector Machines durchführen?\n\n\n\nJa, kannst du. Wenn du nur eine Kreuzvalidierung durchführen willst, findest du alles im Kapitel 69 für den \\(k\\)-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf den Support Vector Machine Algorithmus anwenden. Wenn du den Support Vector Machine Algorithmus auch tunen willst, dann schaue einfach nochmal im Kapitel 70.5 zum Tuning von xgboost rein.\n\n\nJetzt lassen wir uns auf der Liste der Vorhersagen nochmal für alle Kernel der SVM Algorithmen die Konfusionsmatrizen ausgeben.\n\nsvm_cm &lt;- svm_aug_lst |&gt;\n  map(~conf_mat(.x, gender, .pred_class))\nsvm_cm\n\n$svm_lin_mod\n          Truth\nPrediction  m  w\n         m 70 10\n         w 21 74\n\n$svm_poly_mod\n          Truth\nPrediction  m  w\n         m 72 16\n         w 19 68\n\n$svm_rbf_mod\n          Truth\nPrediction  m  w\n         m 71 11\n         w 20 73\n\n\nDas sieht doch recht gut aus. Nur unser Polynomerkernel hat anscheinend Probleme die Geschlechter gut voneinander aufzutrennen. Du siehst, hier muss eben auch ein Tuning her. Selber den Grad des Polynoms zu treffen das passt ist sehr schwer oder eigentlich nur mit Glück hinzukriegen.\nIm folgenden Schritt müssen wir uns etwas strecken. Ich will nämlich die summary() Funktion auf die Konfusionsmatrizen anwenden und dann die drei Ausgaben in einem Datensatz zusammenführen. Wir haben dann die Metriknamen als eine Spalte und dann die drei Spalten für die Zahlenwerte der drei Methoden.\n\nsvm_cm |&gt; \n  map(summary)  |&gt; \n  map(~select(.x, .metric, .estimate)) |&gt; \n  reduce(left_join, by = \".metric\") |&gt; \n  set_names(c(\"metric\", \"linear\", \"poly\", \"radial\")) |&gt; \n  mutate(across(where(is.numeric), round, 3))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 13 × 4\n   metric               linear  poly radial\n   &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 accuracy              0.823 0.8    0.823\n 2 kap                   0.647 0.6    0.647\n 3 sens                  0.769 0.791  0.78 \n 4 spec                  0.881 0.81   0.869\n 5 ppv                   0.875 0.818  0.866\n 6 npv                   0.779 0.782  0.785\n 7 mcc                   0.652 0.6    0.65 \n 8 j_index               0.65  0.601  0.649\n 9 bal_accuracy          0.825 0.8    0.825\n10 detection_prevalence  0.457 0.503  0.469\n11 precision             0.875 0.818  0.866\n12 recall                0.769 0.791  0.78 \n13 f_meas                0.819 0.804  0.821\n\n\nWenn wir wieder auf unsere Accuracy als unser primäres Gütemaß schauen, dann sehen wir, dass wir hier ohne Tuning mit dem linearen Kernel am besten fahren würden. Auch sind die anderen Werte meistens für den linearen Kernel am besten. Daher würde ich mich hier für den linearen Kernel entscheiden. Die Frage wäre natürlich, ob die anderen Kernel mit einem Tuning nicht besser wären. Aber diese Frage lassen wir mal offen im Raum stehen.\nSchauen wir uns in einem letzten Schritt noch die ROC Kurven für die drei Kernels an. Dafür müssen wir einen Datensatz aus der Liste bilden nachdem wir die Sensitivität und Spezifität für die drei Kernels in der Listenform berechnet haben. Wir können dafür die Funktion bind_rows() nutzen.\n\nroc_tbl &lt;- svm_aug_lst |&gt; \n  map(~roc_curve(.x, gender, .pred_w, event_level = \"second\")) |&gt; \n  bind_rows(.id = \"model\")\n\nIn Abbildung 71.9 sehen wir die drei ROC Kurven für die drei Kernels. Wie zu erwarten war, ist der lineare Kernel der beste Kernel. Das hatten wir ja schon oben in der Zusammenfassung der Konfusionsmatrix gesehen. Auch hier zeigt sich sehr schön, wie schlecht dann unser polynominaler Kernel ist. Das war jetzt hier zur Demonstration, aber dennoch zeigt es wie wichtig ein gutes Tuning ist.\n\nroc_tbl |&gt; \n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  theme_minimal() +\n  geom_path() +\n  geom_abline(lty = 3) + \n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 71.9— Darstellung der Vorhersagegüte der drei Modelle linear, polynomial und radial.\n\n\n\n\n\nDamit wären wir auch schon am Ende des Kapitels über den SVM Algorithmus. Wie du schon merkst, müssen wir viel rechnen, wenn wir mit den SVM Kerneln was Vorhersagen wollen. Wenn wir den richtigen Kernel gefunden haben, dann können wir auch eine gute Vorhersage erreichen. Nun müssen auch diesen Kernel erstmal algorithmisch finden, dass heißt also viele Kernels ausprobieren. Und am Ende ist natürlich die Implementierung hier im genutzten R Paket {parsnip} nicht die Weisheit letzter Schluss. Es gibt noch sehr viel mehr R Pakete, die sich mit SVM Algorithmen beschäftigen. Aber das wäre dann eine Literatursuche für dich. Vorerst endet das Kapitel jetzt hier.\n\n\n\nAbbildung 71.1— Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen die Personen und die rote die kranken Personen dar.\nAbbildung 71.2— Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Wir trennen die gesunden Patienten von den kranken Patienten mit einer Geraden.\nAbbildung 71.3— Darstellung von elf gesunden Beobachtungen und acht kranken Beobachtungen aus dem neuen, angewachsenen Traingsdatensatz. Die Gerade trennt die Beobachtugen nur noch ungünstig.\nAbbildung 71.4— Visualisierung des SVM Algorithmus an den ursprünglichen elf Beobachtungen.\nAbbildung 71.5— Darstellung von elf gesunden Beobachtungen und acht kranken Beobachtungen aus dem neuen, angewachsenen Traingsdatensatz mit der SVM optimierten Geraden.\nAbbildung 71.6— Darstellung von zehn Beobachtungen aus einem weiteren Traingsdatensatz. Die blauen Kugeln stellen die fünf gesunden Personen und die rote die fünf kranken Personen dar.\nAbbildung 71.7— Umwandlung des Input Space in einen beliebigen Feature Space durch den Kernel \\(\\Phi\\).\nAbbildung 71.8— Rücktransformation der Ebene aus dem Feature Space in den Input Space. Wir haben dann eine Schlangenlinie, die die Beobachtungen voneinander trennt.\nAbbildung 71.9— Darstellung der Vorhersagegüte der drei Modelle linear, polynomial und radial.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>Support vector machines</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html",
    "href": "classification-neural-networks.html",
    "title": "72  Neural networks",
    "section": "",
    "text": "72.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor, keras, tensorflow, see,\n               neuralnet, NeuralNetTools,\n               OneR, readxl, \n               conflicted)\n##\nset.seed(2025429)\nAn der Seite des Kapitels findest du den Link Quellcode anzeigen, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html#neuronale-netzwerke-theoretisch",
    "href": "classification-neural-networks.html#neuronale-netzwerke-theoretisch",
    "title": "72  Neural networks",
    "section": "72.2 Neuronale Netzwerke theoretisch",
    "text": "72.2 Neuronale Netzwerke theoretisch\nNeuronale Netze ordnen Inputs den Outputs zu. Wir haben also Eingaben und erhalten eine Ausgabe zurück. Neuronale Netze finden Korrelationen. Neuronale Netzwerke sin auch als “universeller Approximator” bekannt, weil dad Netzwerk lernen kann, eine unbekannte Funktion \\(f(x) = y\\) zwischen einer beliebigen Eingabe \\(x\\) und einer beliebigen Ausgabe \\(y\\) zu approximieren. Dabei gilt die Vorraussetzung, dass \\(x\\) und \\(y\\) in einem Zusammenhang durch Korrelation oder Kausalität stehen. Während des Lernprozesses findet ein neuronales Netz das richtige \\(f()\\) oder die richtige Art der Umwandlung von \\(x\\) in \\(y\\), sei es \\(f(x) = 3x + 12\\) oder \\(g(f(x)) = 9x - 0.1\\). Wie du sehen kannst, gibt es auch bei dem neuralen Netzwerk eigentlich um ein Modell. Und unser Modell ist nicht anders, als eine multiple lineare Regresion in der klassischen Statistik.\nDeep Learning ist der Name, den wir für gestapelte neuronale Netze verwenden und damit meinen wir Netze, die aus mehreren Schichten bestehen. Die einzelnen Schichten bestehen aus Knotenpunkten. Ein Knoten ist einfach ein Ort, an dem Berechnungen stattfinden, frei nach dem Vorbild eines Neurons im menschlichen Gehirn, das feuert, wenn es auf ausreichende Reize trifft. Ein Knoten kombiniert Eingaben aus den Daten mit einer Reihe von Koeffizienten oder Gewichten, die diese Eingaben entweder verstärken oder abschwächen. Somit geben dann die Knoten den Eingaben eine Bedeutung im Hinblick auf die Aufgabe die der Algorithmus zu lernen versucht. Häufig ist dies die Aufgabe die Eingabe zu finden die am hilfreichsten die Daten fehlerfrei klassifiziert? Diese Eingangsgewichtungen werden summiert, und die Summe wird dann durch die so genannte Aktivierungsfunktion eines Knotens geleitet, um zu bestimmen, ob und in welchem Ausmaß dieses Signal weiter durch das Netzwerk geleitet werden soll. Am Ende kann nur ein weitergeleitetes Signal das Endergebnis als einen Klassifizierungsvorgang beeinflussen. Wenn das Signal durch das Neuron durchläuft, ist dieses Neuron “aktiviert” worden.\nIn Abbildung 72.5 ist ein Diagramm dargestellt, dass einen Knoten darstellt. Wir haben immer ein Inputlayer in dem wir hier drei Inputneuronen \\(x_1\\), \\(x_2\\) und \\(x_3\\) finden. Das sind auch unsere Variablen in den Daten, die wir in das Modell stecken. Ganz oben finden wir noch als blaues Neuron ein Biasneuron dargestellt. Du kannst dir das Biasneuron wie den Intercept in der linearen Regresion vorstellen. Jedes der Neuronen hat ein Gewicht \\(w_0\\) bis \\(w_3\\). Diese Gewichte werden durch eine Netzinputfunktion in der Form \\(w_0 + w_1 x_1 + w_2x_3\\) aufsummiert und dann an eine Aktivierungsfunktion weitergeleitet. Die Aktivierungsfunktion entscheidet hierbei, ob das Neuron aktiv wird und damit dann auch die Gewichte weiterleitet oder eben inaktiv wird. Es gibt viele Aktivierungsfunktionen, die alle unterschiedliche Eigenschaften haben. Im Folgenden sind einmal die wichtigisten Aktivierungsfunktionen beschrieben.\n\nDie lineare Aktivierungsfunktion skaliert eine Eingabe einfach um einen Faktor, was bedeutet, dass es eine lineare Beziehung zwischen den Eingaben und der Ausgabe gibt.\nSigmoid-Aktivierungsfunktion ist “S”-förmig. Sie kann der Ausgabe Nichtlinearität hinzufügen und gibt einen binären Wert von 0 oder 1 zurück.\nDie Tanh-Aktivierungsfunktion ist eine Erweiterung der sigmoidalen Aktivierungsfunktion. Daher kann Tanh verwendet werden, um der Ausgabe Nichtlinearität hinzuzufügen. Die Ausgabe liegt im Bereich von -1 bis 1. Die Tanh-Funktion verschiebt das Ergebnis der sigmoiden Aktivierungsfunktion.\nDie Rektifizierte lineare Einheits-Aktivierungsfunktion (RELU) ist eine der am häufigsten verwendeten Aktivierungsfunktionen. RELU wird bevorzugt in den Hidden Layer verwendet. Das Konzept ist linear vom Nullpunkt ausgehend. Die RELU fügt der Ausgabe auch Nichtlinearität hinzu. Allerdings kann das Ergebnis von 0 bis unendlich reichen.\nDie Softmax-Aktivierungsfunktion ist eine Erweiterung der Sigmoid-Aktivierungsfunktion. Die Softmax-Funktion fügt der Ausgabe eine Nichtlinearität hinzu. Sie wird jedoch hauptsächlich für Klassifizierungen verwendet, bei denen mehrere Klassen von Ergebnissen berechnet werden können. Wir haben dann einen Multiclass-Fall vorliegen.\n\nIm Prinzip ist eine Aktivierungsfunktion nichts anderes als die Link Funktion in der multiplen linearen Regression. Aber das geht dann hier zu weit. Häufig wird dann die Netzinputfunktion und die Aktivierungsfunktion in einem Knotenpunkt dargestellt.\n\n\nMehr über Aktivierungsfunktionen kannst du im Tutorium Neural Networks In a Nutshell erfahren.\n\n\n\n\n\n\nAbbildung 72.1— Darstellung von drei Inputneuronen \\(x_1, x_2, x_3\\), einem Biasneuron \\(1\\) mit den jeweiligen weitergeleiteten Gewichten \\(w_1, w_2, w_3\\) und \\(w_0\\). Die Summierungsfunktion sowie die Aktivierungsfunktion werden meist in einen gemeinsamen Knoten dargestellt. Hier sind beide Formen einmal abgebildet. Wenn das Neuron aktiviert ist, gibt es die Summe als Output weiter.\n\n\n\nIn der Abbildung 72.2 sehen wir dann ein ganze Netz an Neuronen. Wir haben ein Inputlayer und mehrere Hiddenlayer die am Ende dann in ein Outputlayer enden. Meistens wollen wir eine binäre Klassifikation rechnen, so dass am Ende dann zwi Outputknoten stehen. Die Hiddenlayer können unterschiedlich viele Knoten enthalten und meistens gibt es auch mehrere Abstufungen. Das heißt wir fnagen mit mehreren Knoten pro Hiddenlayer an und reduzieren dann die Anzahl der Knoten pro Hiddenlayer über die Breite des neuronalen Netzwerkes.\n\n\n\n\n\n\nAbbildung 72.2— Darstellung von drei Inputneuronen \\(x_1, x_2, x_3\\) ohne ein Biasneuron. Die drei Inputbeurnen leiten ihre Gewichte an die Hidden Layer Neoronen weiter. In jedem diesem Neuron findet eine Summiierung in eine eventuelle Aktivierung statt. Aktivierte Neuronen leiten die Summation als Gewichte dann an weitere Hidden Layer Neuronen weiter. Am Ende findet eine Entscheidung in den Outputneuronen statt.\n\n\n\nSpannenderweise sind viele Dinge in einem neuronalen Netzwerk nichts anderes als eine intelligente Hintereinanderschaltung von multiple linearen Regressionen Deshalb gibt es in der Tabelle 72.1 auch einmal eine Übersicht der Begriffe in dem Sprachraum der neuronalen Netze und der klassischen logistischen Regression. Wir sehen hier einiges an gleichen Konzepten.\n\n\n\nTabelle 72.1— Welche Begriff in dem Sprachraum der neuronalen Netze lässt sich zu welchem Begriff in der logistischen Regression zuordnen?\n\n\n\n\n\n\n\n\n\n\nNeural network\nLogistic regression (eng.)\nLogistische Regression (deu.)\n\n\n\n\nActivation function\nLink function\nLink Funktion\n\n\nWeights\nCoefficients / Slope\nKoeffizienten / Steigung\n\n\nBias\nIntercept\nIntercept\n\n\nVariance\nResiduals\nFehler / Residuen\n\n\nLearning\nFitting\nModellieren\n\n\n\n\n\n\n\n\nWhat is the role of the bias in neural networks?\nWenn ein neuronales Netz auf dem Trainingssatz trainiert wird, wird es mit einer Reihe von Gewichten initialisiert. Diese Gewichte werden dann während der Trainingsperiode optimiert und die optimalen Gewichte werden erzeugt. Das ist ein wichtiger Punkt. Wir erzeugen zufällig die Gewichte am Anfang und lassen uns dann die Gewichte mehr oder minder zufällig weiteroptimieren. Sonst würden ja bei jedem Knoten die gleichen Zahlen rauskommen. Wir optimieren aber nicht nur einmal sondern meistens mehrfach. Das heißt wir lassen das neuronale Netzwerk mehrfach wachsen und optimieren bei jedem Wachstum die Gewicte so, dass der Fehler geringer wird.\nDie Epoche (eng. epoch) ist einer der Eingabeparameter des Algorithmus. Stelle dir die Epoche als eine Schleife vor. Die Schleife bestimmt, wie oft ein Lernalgorithmus die Gewichte aktualisiert. Wenn der Wert der Epoche 1 ist, bedeutet dies, dass das neuronale Netz einmal läuft um die Gewichte zu aktualisieren. Wenn die Epoche einen Wert von 5 hat, wird das neuronale Netzwerk fünfmal aktualisiert. Hier ist der Unterschied zu den Entscheidungsbäumen auffällig. Entscheidungsbäume werden in einem Random Forest gemittelt. Die Epochen eines neuronalen Netzwerkes hängen aber miteinander zusammen.\nEin neuronales Netz ist eine korrigierende Rückkopplungsschleife, die Gewichte belohnt, die seine korrekten Vermutungen unterstützen, und Gewichte bestraft, die es zu Fehlern verleiten.\nDamit wir wissen, ob unser Netzwerk über die Epochen besser wird, brauchen wir eine Verlustfunktion (eng. loss function). Die Verlustfunktion wird auch als Kostenfunktion (eng. cost function) bezeichnet. Sie errechnet den Fehler. Um genau zu sein, ist die Kostenfunktion der Durchschnitt der Verlustfunktionen. Dies ist die Funktion, die der Optimierungsalgorithmus zu minimieren versucht. Es gibt eine große Anzahl von Verlustfunktionen, wie den mittleren quadratischen Fehler oder die binäre Kreuzentropie.\nDie Verlustfunktion sagt dem neuronalen Netz im Wesentlichen, welche Maßnahmen es ergreifen muss, um die Accuracy zu verbessern. Diese Information wird dann verwendet, um genaueren Gewichte zu erzeugen. Danach kann dann das neuronale Netz kann die Daten erneut weiterverarbeiten.\nAm Rande möchte ich noch die Begriffe Forward Propagation und Back Propagation erwähnen. Beide Begriffe beschreiben, wie das Lernen innerhalb eines neuronalen Netzwerk abläuft. Klassisch ist die Forward Propagation. Dabei reicht ein Knoten die Informationen an den nächsten Knoten weiter. Das Lernen erfolgt vorwärts. Die andere Möglichkeit ist, das Netzwerk wachsen zu lassen und dann rückwärts die Gewichte der Knoten zu verbessern. Wir haben dann eine Back Propagation vorliegen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html#neuronales-netz-anschaulicher",
    "href": "classification-neural-networks.html#neuronales-netz-anschaulicher",
    "title": "72  Neural networks",
    "section": "72.3 Neuronales Netz anschaulicher",
    "text": "72.3 Neuronales Netz anschaulicher\nIn unserem folgenden Beispiel ist Rotkäppchen das neuronale Netz. Rotkäppchen hat folgende Informationen zu drei möglichen Outcomes vorliegen. Rotkäppchen weiß also, dass es im Wald oder im Haus drei Personen treffen kann. Entweder trifft sie die Großmutter, den großen, bösen Wolf oder den Holzfäller. Gott sei Dank kennt Rotkäppchen die Eigenschaften der drei Charaktere und kann daran sich folgende Matrix aufbauen. Wir lesen die Tabelle wie folgt, wir haben die Spalte grosse_ohren und wir haben drei Werte mit der Spalte assoziiert. Wir wissen aber nicht welche Zeile welcher Charakter ist. Wir wollen die Zuordnung einmal mit dem neuronalen Netzwerk durchführen.\n\nlittle_red_tbl &lt;- tibble(grosse_ohren = c(1, 0, 1), \n                         grosse_augen = c(1, 1, 0),\n                         grosse_zaehne = c(1, 0, 0) , \n                         freundlich = c(0, 1, 1), \n                         faltig = c(0, 1, 0), \n                         gutaussehend = c(0, 0, 1),\n                         renn_weg = c(1, 0, 0), \n                         schrei = c(1, 0, 0), \n                         ruf_holzfaeller = c(1, 0, 0), \n                         plaudere = c(0, 1, 1), \n                         geh_hin = c(0, 1, 0), \n                         biete_essen = c(0, 1, 1), \n                         rettung = c(0, 0, 1))\n\nIn der Tabelle 72.2 sehen wir die Daten nbochmal in das Input Layer und das Output Layer aufgespaltet. Die Frage ist, was soll Rotkäppchen tun, wenn die die Eigenschaften des Input Layers beobachtet? Wir wollen jetzt anhand eines neuronalen Netzes die Input Layer dem Output Layer zuordnen.\n\n\n\nTabelle 72.2— Die beiden Datensätze für das neuronale Netzwerk. Wie lässt sich der Input sinnvoll mit dem Output verbinden? Wir geben dafür drei Hidden Layers vor, die dann die Charaktere Wolf, Goßmutter und den Holzfäller repräsentieren.\n\n\n\n\n\n\n\n(a) Daten des Input Layers.\n\n\n\n\n\ngrosse_ohren\n1\n0\n1\n\n\ngrosse_augen\n1\n1\n0\n\n\ngrosse_zaehne\n1\n0\n0\n\n\nfreundlich\n0\n1\n1\n\n\nfaltig\n0\n1\n0\n\n\ngutaussehend\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n(b) Daten des Output Layers.\n\n\n\n\n\nrenn_weg\n1\n0\n0\n\n\nschrei\n1\n0\n0\n\n\nruf_holzfaeller\n1\n0\n0\n\n\nplaudere\n0\n1\n1\n\n\ngeh_hin\n0\n1\n0\n\n\nbiete_essen\n0\n1\n1\n\n\nrettung\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n\nIm Folgenden siehst du einmal den Code für das simple neuronale Netzwerk. Wir haben die Spalten des Input Layer durch das ~ von den Spalten des Output Layers getrennt. Darüber hinaus wollen wir noch drei Hidden Layer Knoten haben. Jeweils einen Knoten für jeden unserer drei Charaktere.\n\nneuralnetwork &lt;- neuralnet(renn_weg + schrei + ruf_holzfaeller + plaudere + \n                             geh_hin + biete_essen + rettung ~ \n                             grosse_ohren + grosse_augen + grosse_zaehne + \n                             freundlich + faltig + gutaussehend,\n                           data = little_red_tbl, hidden = 3, \n                           exclude = c(1, 8, 15, 22, 26, 30, 34, 38, 42, 46), \n                           lifesign = \"none\", linear.output = FALSE)\n\nIn Abbildung 72.3 sehen wir das neuronale Netzwerk einmal abgebildet. Da wir uns so ein simples Beispiel ausgedacht haben, können wir das Beispiel hier auch einmal visualisieren. Wir sehen hier nochmal auf der linken Seite das Input Layer und auf der rechten Seite das Output Layer. Die schwarzen, dicken Linien stellen die bedeutenden Gewichte dar. Wir sehen also, dass grosse_ohren, grosse_augen und grosse_zaehne mit dem Hidden Layer H3 verbunden sind. Von dem Hidden Layer H3 gehen dann die Linien zu renn_weg, schrei und ruf_holzfaeller. Wir sehen daran, dass das neuronale Netzwerk in H3 den großen, bösen Wolf erkannt hat. Da wir jetzt sehen, dass H1 hauptsächlich faltig ist, können wir hier auf die Repräsentation der Großmutter schließen. Ebenso ist H2 gutaussehend, so dass wir hierauf die Repräsenrtation des Holzfällers schließen können. Die Zuordnungen des Output Layers passen dementsprechend dann auch.\n\nplotnet(neuralnetwork, bias = FALSE, pad_x = 0.73)\n\n\n\n\n\n\n\nAbbildung 72.3— Visualisierung des neuronalen Netzwerkes mit drei vorgebenen Hidden Layers. Die Hidden Layers repräsentieren in diesem Beispiel die Characktere Wolf, Großmutter und den Holzfäller.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html#neuronales-netz-mathematischer",
    "href": "classification-neural-networks.html#neuronales-netz-mathematischer",
    "title": "72  Neural networks",
    "section": "72.4 Neuronales Netz mathematischer",
    "text": "72.4 Neuronales Netz mathematischer\nDas folgende etwas mathematische Beispiel ist von Kubat (2017), pp. 65-73, entnommen. Ich habe das Beispiel dann für R adaptiert, so dass wir hier auch R Code zum ausprobieren haben. Bevor wir damit anfangen, hier nochmal auf einfache Weise erklärt, was beim Lernen mit einem neuronalen Netzwerk geschieht.\nEingaben werden als Inputs in das Netz eingegeben. Die Koeffizienten bzw. Gewichte ordnen diese Eingabe einer Reihe von Vermutungen zu, die das Netz am Ende anstellt. Hierbei erfolgt die Zuornung mehr oder minder zufällig. Wir beginnen ja auch mit einem Satz an zufällig ausgewählten Gewichten, die wir dann innerhalb des neuronalen Netzwerks optimieren wollen.\n\\[\nEingabe * Gewichtung = Vermutung\n\\]\n\n\n\\[\ninput * weight = guess\n\\]\nDie gewichtete Eingabe führt zu einer Vermutung darüber, was die Eingabe ist. Das neuronale Netz vergleicht dann seine Vermutung mit einer Wahrheit über die Daten und berechnet daraus einen Fehler. Wir wissen, dass wir zehn kranke und acht gesunde Ferkel in dem Datensatz haben, wie viele kann das neuronale Netzwerk anhand der Gewichte und dem Input richtig zuordnen oder eben falsch zuordnen?\n\\[\nWahrheit - Vermutung = Fehler\n\\]\n\n\n\\[\ntruth - guess = error\n\\]\nDie Differenz zwischen der Schätzung des neuronalen Netzes und der Wahrheit ist der Fehler. Das Netzwerk misst diesen Fehler und minimiert den Fehler über das Modell, indem es die Gewichte in dem Maße anpasst, wie sie zum Fehler beigetragen haben.\n\\[\nFehler * Beitrag\\; des\\; Gewichts\\; zum\\; Fehler = Anpassung\n\\]\n\n\n\\[\nerror * weight's\\; contribution\\; to\\; error = adjustment\n\\]\nDie drei obigen Formeln beschreiben die drei Hauptfunktionen neuronaler Netze: Bewertung der Eingaben, Berechnung des Verlusts und Aktualisierung des Modells, um den dreistufigen Prozess von vorne zu beginnen. Ein neuronales Netz ist eine korrigierende Rückkopplungsschleife, die Gewichte belohnt, die seine korrekten Vermutungen unterstützen, und Gewichte bestraft, die es zu Fehlern verleiten.\n\n\nDas Buch An Introduction to Machine Learning kannst du dir an der HS Osnabrück als PDF über die Hochschule runterladen.\nBetrachten wir also einmal ein simples Datenbeispiel von vier Beobachtungen mit jeweils einem \\(x_1\\) und einem \\(x_2\\) Wert als Prädiktor. Der Wert den \\(x_1\\) oder \\(x_2\\) annehmen können sind binär. Wir haben also für unsere beiden Prädiktoren nur \\(0/1\\) Werte vorliegen. Unser Label \\(y\\) ist ebenfalls binär. Entweder ist die betreffende Beobachtung erkrankt oder eben nicht. In unserem Beispiel sind die ersten beiden Beobachtungen nicht erkrankt und die letzten beiden Beobachtungen sind erkrankt. Schauen wir uns den Datensatz einmal an.\n\ndata_tbl &lt;- tibble(y = c(0, 0, 1, 1),\n                   x_1 = c(0, 1, 0, 1),\n                   x_2 = c(0, 0, 1, 1))\ndata_tbl\n\n# A tibble: 4 × 3\n      y   x_1   x_2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0     0\n2     0     1     0\n3     1     0     1\n4     1     1     1\n\n\nFaktisch wollen wir jetzt eine Grade durch die Punkte legen, so dass wir die gesunden von den kranken Beobachtungen trennen können. Praktisch machen wir das mit einer linearen Funktion \\(h(x)\\), die uns anhand von \\(x_1\\) und \\(x_2\\) eine Aussagen über den Status von \\(y\\) ermöglicht. Wir erhalten zuerst einen numerischen Wert, den wir dann noch mit einer Regel in eine \\(0/1\\) Entscheidung umwandeln müssen.\n\\[\nh(x) \\sim w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2\n\\]\nNun können wir die Formel nochmal kompakter schreiben.\n\\[\nh(x) \\sim \\sum_{i = 0}^{n=2} w_i x_i\n\\]\nWir drücken im Folgenden damit aus, das wir auch die Gewichte \\(w_i\\) mit den einzelnen \\(x_i\\) multiplizieren und anschließend aufsummieren. Anhand der aufsummierten Zahl aus \\(h(x)\\) können wir dann eine Entscheidung für \\(0/1\\) treffen. In unserem Beispiel entscheiden wir uns dazu, das wir \\(y=0\\) annehmen wenn \\(h(x) &lt; 0\\) ist oder aber \\(y=1\\) annehmen, wenn \\(h(x) \\geq 0\\) ist. Wir können das einmal formal aufschreiben.\n\\[\nh(x)=\n\\begin{cases}\n    1,& \\text{wenn } h(x)\\geq 0\\\\\n    0,              & \\text{ansonsten}\n\\end{cases}\n\\]\nNichts anders ist dann auch unser Neuron, was die Entscheidungen trifft. Wir haben vier verschiedene \\(x_1\\) und \\(x_2\\) Kombinationen und gewichten diese beiden \\(x\\) dann noch einem Gewichtsvektor. Wenn wir dann als aufsummiertes Ergebnis eine Zahl größer als \\(0\\) erhalten, dann gibt unser Neuron als Klassifikationsergebnis ein \\(1\\) wieder.\n\nneuron &lt;- function(input, weights) {ifelse(input %*% weights &gt; 0, 1, 0)}\n\nWir brauchen also zum einen die Inputmatrix. Die bauen wir uns einmal mit der Funktion model.matrix(). Dann haben wir drei Spalten für jedes Gewicht \\(w\\). Dann brauchen wir noch die drei Gewichte \\(w_0\\), \\(w_1\\) und \\(w_2\\). Nichts anders als der Intercept und die Steigung in einem linearen Modell.\n\ninput &lt;- data_tbl %$%\n  model.matrix(~ x_1 + x_2)\ninput\n\n  (Intercept) x_1 x_2\n1           1   0   0\n2           1   1   0\n3           1   0   1\n4           1   1   1\nattr(,\"assign\")\n[1] 0 1 2\n\n\nWir wählen zufällig drei Gewichte aus, die wir dann in unser Modell geben. Die Gwichte werden dann innerhalb des neuronalen Netzwerks dann optimiert. Die Wahl der passenden Gewichte ist dann noch eine Frage für sich, aber hier haben wir diese drei Werte ausgewählt.\n\nweights &lt;- c(0.1, 0.3, 0.4)\n\nDann brauchen wir noch ein \\(\\eta\\), dass beschreibt, um wie viel wir die Gewichte pro Runde der Optimierung verändern wollen. Wir wählen hier einen Wert von \\(0.2\\). Je kleiner der Wert, desto länger braucht das neuronale Netzwerk um ein Optimum zu finden. Pro Schritt können ja die Gewichte nur wenig geändertw werden. Ist das \\(\\eta\\) zu groß dann sind die Änderungen der Gewichte auch groß und es kann sein, dass das neuronale Netzwerk gar keine optimalen Gewichte findet. Die Auflösung ist einfach nicht gering genug.\n\neta &lt;- 0.2\n\nJetzt laufen wir einmal durch vier Epochen. In jeder Epoche werden wir unser Gewicht dann wieder optimieren und dann mit den optimierten Gewichten weiter rechnen. Wir lassen uns aber in jeder Schleife einmal die Gewichte ausgeben.\n\nfor(i in 1:4){\n  adjust &lt;- (data_tbl$y[i] - neuron(weights, input[i,])) * input[i,]\n  weights &lt;- weights + eta * adjust\n  cat(\"Adjust: \", adjust, \"\\n\")  \n  cat(\"Weights: \", weights, \"\\n\")\n}\n\nAdjust:  -1 0 0 \nWeights:  -0.1 0.3 0.4 \nAdjust:  -1 -1 0 \nWeights:  -0.3 0.1 0.4 \nAdjust:  0 0 0 \nWeights:  -0.3 0.1 0.4 \nAdjust:  0 0 0 \nWeights:  -0.3 0.1 0.4 \n\n\nDie Gewichte ändern sich in jedem Schritt um den Wert von \\(0.2\\). Mehr geht auch nicht, denn wir geben mit \\(\\eta\\) vor, um wieviel sich die Gewichte erhöhen oder erniedrigen können. Im ersten Schritt reduzieren wir das erste Gewicht um den Wert von \\(\\eta\\). Im zweiten Schritt reduzieren wir erneut das erste Gewicht und darüber hinaus auch noch das zweite Gewicht. Wir sind dann schon am Optimum, denn wir erhalten keine weiteren Anpassungen mehr. Vermutlich können wir schon am zweiten Schritt das Outcome perfekt auftrennen.\nSchauen wir einmal was passiert, wenn wir unser input mit den Gewichten aus unserem simplen Algorithmus multiplizieren.\n\ninput %*% c(-0.3, 0.1, 0.4)\n\n  [,1]\n1 -0.3\n2 -0.2\n3  0.1\n4  0.2\n\n\nUnsere ersten zwei Beobachtungen erhalten einen negativen Wert und unsere letzten beiden Beobachtungen einen positiven Wert. Nach unserer Regeln werden Zahlen kleiner als Null zu \\(0\\) und Zahlen größer als Null zu \\(1\\). Da wir die Regel auch in dem Neuron abgespeichert haben, können wir uns einmal das Outcome mit den Input und den berechnete Gewichten wiedergeben lassen.\n\nneuron(input, weights = c(-0.3, 0.1, 0.4))\n\n  [,1]\n1    0\n2    0\n3    1\n4    1\n\n\nWir erhalten eine perfekte Übereinstimmung von der Vorhersage mit unseren Trainingsdaten. Der Algorithmus ist in der Lage mit der Regel in dem Neuron und den berechneten Gewichten unser Outcome korrekt mit den Trainingsdaten vorherzusagen.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html#daten",
    "href": "classification-neural-networks.html#daten",
    "title": "72  Neural networks",
    "section": "72.5 Daten",
    "text": "72.5 Daten\nIn Folgenden wollen wir uns aber mal auf einen echten Datensatz konzentrieren. Wir nutzen daher einmal den Gummibärchendatensatz. Als unser Label und daher als unser Outcome nehmen wir das Geschlecht gender. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir nur die Spalte Geschlecht sowie als Prädiktoren die Spalten most_liked, age, semester, und height.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") |&gt; \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) |&gt; \n  select(gender, most_liked, age, semester, height) |&gt; \n  drop_na(gender)\n\nWir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein NA in der Variable gender haben. Wir haben dann insgesamt \\(n = 699\\) Beobachtungen vorliegen. In Tabelle 67.5 sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.\n\n\n\n\nTabelle 72.3— Auszug aus dem Daten zu den Gummibärchendaten.\n\n\n\n\n\n\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\n\nm\nlightred\n35\n10\n193\n\n\nw\nyellow\n21\n6\n159\n\n\nw\nwhite\n21\n6\n159\n\n\nw\nwhite\n36\n10\n180\n\n\nm\nwhite\n22\n3\n180\n\n\nm\ngreen\n22\n3\n180\n\n\n…\n…\n…\n…\n…\n\n\nm\ndarkred\n24\n2\n193\n\n\nm\nwhite\n27\n2\n189\n\n\nm\ndarkred\n24\n2\n187\n\n\nm\ngreen\n24\n2\n182\n\n\nw\nwhite\n23\n2\n170\n\n\nw\ngreen\n24\n2\n180\n\n\n\n\n\n\n\n\nUnsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Um die Klassifikation mit Entscheidungsbäumen rechnen zu können brauchen wir wie bei allen anderen Algorithmen auch einen Trainings- und Testdatensatz. Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Der Traingsdatensatz ist dabei immer der größere Datensatz. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.\n\ngummi_data_split &lt;- initial_split(gummi_tbl, prop = 3/4)\n\nWir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluss einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.\n\ngummi_train_data &lt;- training(gummi_data_split)\ngummi_test_data  &lt;- testing(gummi_data_split)\n\nNachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht gender als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem . Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_train_data) |&gt; \n  step_impute_mean(all_numeric_predictors()) |&gt; \n  step_impute_bag(all_nominal_predictors()) |&gt; \n  step_range(all_numeric_predictors(), min = 0, max = 1) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_nzv(all_predictors())\n\ngummi_rec |&gt; summary()\n\n# A tibble: 5 × 4\n  variable   type      role      source  \n  &lt;chr&gt;      &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 most_liked &lt;chr [3]&gt; predictor original\n2 age        &lt;chr [2]&gt; predictor original\n3 semester   &lt;chr [2]&gt; predictor original\n4 height     &lt;chr [2]&gt; predictor original\n5 gender     &lt;chr [3]&gt; outcome   original\n\n\nAlles in allem haben wir ein sehr kleines Modell. Wir haben ja nur ein Outcome und vier Prädiktoren. Trotzdem sollte dieser Datensatz reichen um zu erklären wie Keras oder Tensorflow funktionieren. Am Ende muss man sich aber auch ehrlich machen und sagen, dass ein Datensatz mit unter tausend Beobachtungen eigentlich keinen großen Sinn für ein neuronales Netz macht. Deshalb ist das hier eher eine Demonstration des Algorithmus.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html#neuronale-netze-mit-neuralnet",
    "href": "classification-neural-networks.html#neuronale-netze-mit-neuralnet",
    "title": "72  Neural networks",
    "section": "72.6 Neuronale Netze mit neuralnet",
    "text": "72.6 Neuronale Netze mit neuralnet\nNeuronale Netze mit den R Paketen {neuralnet} und dem R Paket {nnet} sind mehr oder minder veraltet (eng. outdated). Wir können das Paket {neuralnet} nicht über die {parsnip} Umgebung nutzen. Deshalb hier einmal zu Fuß mit all den Komplikationen, die das so mit sich bringt. Auf der anderen Seite liefert das Paket {neuralnet} auch gute Ergebnisse mit wenig rechenlaufzeit. Da musst du dann einmal abwägen, was du in deiner Arbei so brauchst.\nDas die Funktion neuralnet() nicht mit den Workflow kann, müssen wir uns erstmal wieder den Traingsdatendatz und den Testdatensatz aus unserem Rezept extrahieren. Den Traingsdatensatz können wir uns über die Funktion juice() einmal aus dem Rezept ziehen.\n\ngummi_train_tbl &lt;- gummi_rec |&gt; \n  prep() |&gt; \n  juice()\n\nDen Testdatensatz müssen wir mit dem Rezept einmal backen. Dann müssen wir noch die Spalte gender in eine numerische Spalte umwandeln. Sonst klappt das später nicht mit der Prädiktion und der Konfusionsmatrix.\n\ngummi_test_tbl &lt;- gummi_rec |&gt; \n  prep() |&gt; \n  bake(gummi_test_data) |&gt; \n  mutate(gender = as_factor(ifelse(gender == \"m\", 0, 1)))\n\nDann können wir auch schon die Funktion neuralnet auf unsere Daten anwenden. Wir wollen fünfmal über die Traingsdaten iterieren (rep = 5). Später heißt dieses Iterieren dann auch epoch. Dann müssen wir noch den Threshold für den Fehler festlegen, der gerade noch so akzeptabel ist und wo das Wachstum endet. Je kleiner, desto länger dauer der Prozess. Mit einem threshold = 0.2 sind wir aber schon sehr weit oben, sonst ist der Wert bei \\(0.01\\). Hier musst ein wenig selber mit den Parametern spielen. Eine Tuningmöglichkeit oder eine Kreuzvalidierung musst du dir dann selber programmieren. Wir nehmen dann fünf Hiddenlayers mit jeweils fünf Knoten pro Hiddenlayer.\n\nneuralnet_fit &lt;- neuralnet(gender ~., data = gummi_train_tbl, rep = 5, threshold = 0.2,\n                           hidden = c(5, 5), lifesign = \"minimal\")\n\nhidden: 5, 5    thresh: 0.2    rep: 1/5    steps:    6051   error: 40.44275 time: 2.03 secs\nhidden: 5, 5    thresh: 0.2    rep: 2/5    steps:    1620   error: 54.85255 time: 0.51 secs\nhidden: 5, 5    thresh: 0.2    rep: 3/5    steps:    2584   error: 49.12246 time: 0.82 secs\nhidden: 5, 5    thresh: 0.2    rep: 4/5    steps:     455   error: 57.46034 time: 0.14 secs\nhidden: 5, 5    thresh: 0.2    rep: 5/5    steps:    2062   error: 50.92853 time: 0.62 secs\n\n\nWenn wir das Modell haben, dann können wir uns hier ganz einfach mal das beste neuronale Netzwerk anschauen. Also die Wiederholung mit dem kleinsten Fehler. In Abbildung Abbildung 72.4 sehen wir das Netzwerk einmal dargestellt. Die blauen Knoten stellen die Biasknoten dar. Die Zahlen an den Kanten stellen dann die Gewichte dar, die von dem jeweiligen Knoten weitergegeben werden. Die Interpretation des Netzwerks ist so schwer, es ist eben nur eine visuelle Darstellung. Da so eine Abbildung etwas schwer zu interpretieren ist, erlaubt ein neurales Interpretationsdiagramm mehr Einblicke. Die schwarzen Kanten haben einen höheren Einfluss als die grauen Kanten. Die exakte Interpretation der Knoten und der Kanten ist aber dennoch schwierig.\n\nplot(neuralnet_fit, rep = \"best\")\n\nplotnet(neuralnet_fit, rep = \"best\", bias = FALSE, pad_x = 0.59)\n\n\n\n\n\n\n\n\n\n\n\n(a) Neuronales Netzwerk mit den Gewichten und dem Bias als numerische Representation.\n\n\n\n\n\n\n\n\n\n\n\n(b) Neurales Interpretationsdiagramm für ein neurales Netzwerk.\n\n\n\n\n\n\n\nAbbildung 72.4— Abbildung des neuronalen Netzwerks mit dem kleinsten Fehler.\n\n\n\n\nAm Ende machen wir das Ganze ja nicht um etwas interpretieren zu können, sondern um eine Vorhersage zu treffen. Das machen wir mit der Funktion predict(). Jetzt wird es wieder nervig. Wir müssen usn merken, dass unser Faktor zwei Level hat mit 0 und 1 wobei die m = 0 und w = 1 ist. Als wäre das nicht schon nervig genug, haben wir dann in der Ausgabe von predict() nur eine Matrix mit zwei Spalten. Wir brauchen die zweite Spalte, da wir das Geschlecht w vorhersagen wollen.\n\nneuralnet_pred &lt;- predict(neuralnet_fit, gummi_test_tbl) |&gt; \n  round(2)\n\nKurzer Check, ob wir auch alles richtig gemacht haben.\n\nrange(neuralnet_pred[,1])\n\n[1] -0.94  1.18\n\nrange(neuralnet_pred[,2])\n\n[1] -0.19  1.95\n\n\nUnd wir stellen fest, dass hier irgendwas mit unserer Wahrscheinlichkeit für die Klassenzugehörigkeit nicht stimmt. Wir haben negative Werte und Werte über Eins. Das macht für eine Wahrscheinlichkeit keinen Sinn.\n\n\n\n\n\n\nStopp!\n\n\n\nJetzt müssen wir hier erstmal anhalten, denn wir erhalten sinnlose Wahrscheinlichkeiten zurück. Ich würde hier erstmal das Modell überprüfen und die Daten anpassen. Jedenfalls geht es so nicht weiter…\n\n\nIch zeige aber noch wie du dir die Konfusionsmatrix berechnest. Da musst du dich wieder strecken um alles in die Funktion conf_mat() richtig rein zu kriegen. Aber Vorsicht, erst wenn du die Wahrscheinlichkeiten hingekriegt hast, dann kannst du mit der Konfusionsmatrix weitermachen.\n\nneuralnet_cm &lt;- conf_mat(data = data.frame(.pred_class = as.factor(round(neuralnet_pred[,2])),\n                                           gender = as.factor(pull(gummi_test_tbl, gender))), \n                         gender, .pred_class)\n\nDann können wir uns die Konfusionsmatrix auch einmal wiedergeben lassen. Ich wäre hier sehr vorsichtig, was die Werte angeht. Wir haben gerade komische Wahrscheinlichkeiten wiedergegeben bekommen. Daher würde ich der Sache hier nicht trauen und nochmal an der Funktion neuralnet() mit anderen Parametern herumprobieren. Man sieht, es hat auch einen Grund warum manche Funktionen nicht in der parsnip Umgebung implementiert sind.\n\nneuralnet_cm |&gt; \n  summary |&gt; \n  mutate_if(is.numeric, round, 2)\n\nHier ist also wirklich Vorsicht geboten, wenn wir uns die Ergebnisse anschauen. Die Ergebnisse sind zwar nicht so schlecht, aber wir vertrauen da nicht dem Algorithmus, wenn wir ungültige Wahrscheinlichkeiten erhalten.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html#neuronale-netze-mit-nnet",
    "href": "classification-neural-networks.html#neuronale-netze-mit-nnet",
    "title": "72  Neural networks",
    "section": "72.7 Neuronale Netze mit nnet",
    "text": "72.7 Neuronale Netze mit nnet\nWir können aber das R Paket {nnet} mit unserer bekannten Rezeptumgebung nutzen und uns damit das Leben einfacher machen. Das macht auch in diesem Fall sehr viel mehr Sinn, da wir ja nur komische Wahrscheinlichkeiten der Klassenzugehörigkeit aus der Funktion neuralnet() wiederbekommen. Also das ganze einmal ohne wildes Installieren von Tensorflow / Keras. Ein simples neurales Netzwerk in R mit der Engine aus nnet.\nIn unserem Beispiel lassen wir einhundert Replikationen laufen (epoch = 100) und wählen auch hier mal fünf Hidden Layers (hidden_units = 5). Dann wollen wir natürlich eine Klassifikation rechnen.\n\nnnet_mod &lt;- mlp(epochs = 100, hidden_units = 5) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\")\n\nWir bringen wieder unser Modell mit dem Rezept des Gummibärchendatensatzes zusammen und können dann den Workflow abspeicherb.\n\nnnet_wflow &lt;- workflow() |&gt; \n  add_model(nnet_mod) |&gt; \n  add_recipe(gummi_rec)\n\nWie immer starten wir dann den Workflow mit der Funktion fit() und erhalten das nnet Modell zurück.\n\nnnet_fit &lt;- nnet_wflow |&gt; \n  parsnip::fit(gummi_train_data)\n\nJetzt müssen wir nur noch mit der Funktion augment uns die Vorhersagen mit dem Testdatensatz wiedergeben lassen.\n\nnnet_aug &lt;- augment(nnet_fit, gummi_test_data ) \n\nDa wir hier etwas vorsichtig geworden sind, nochmal schnell schauen, ob unsere Wahrscheinlichkeiten der Klassenzugehörigkeit auch wirklich eine Wahrscheinlichkeit ist.\n\npluck(nnet_aug, \".pred_w\") |&gt; range()\n\n[1] 0.2689414 0.7310586\n\n\nJa, das passt soweit und wir können uns dann die Konfusionsmatrix berechnen lassen. Die Ergebnisse sind jetzt nicht so berauschend, aber auf der anderen Seite richtiger als in der Funktion neuralnet().\n\nnnet_cm &lt;- nnet_aug |&gt; \n  conf_mat(gender, .pred_class)\n\nnnet_cm\n\n          Truth\nPrediction  m  w\n         m 71 26\n         w 11 67\n\n\nDann schauen wir uns nochmal die ganzen anderen Gütekriterien aus der Konfusionsmatrix einmal an.\n\nnnet_cm |&gt; summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.789\n 2 kap                  binary         0.580\n 3 sens                 binary         0.866\n 4 spec                 binary         0.720\n 5 ppv                  binary         0.732\n 6 npv                  binary         0.859\n 7 mcc                  binary         0.589\n 8 j_index              binary         0.586\n 9 bal_accuracy         binary         0.793\n10 detection_prevalence binary         0.554\n11 precision            binary         0.732\n12 recall               binary         0.866\n13 f_meas               binary         0.793\n\n\nDie Ergebnisse sind höchstens okay. Die Accuracy ist nicht sehr hoch und auch der Rest der Werte ist eher mittelmäßig. Das Ganze sehen wir dann in Abbildung 72.5 auch nochmal entsprechend in der ROC Kurve visualisiert. Die ROC Kurve sieht nur mittelmäßig aus. Wir müssten hier auf jeden Fall nochmal über Kreuzvalidierung und Tuning nachdenken. Ohne Kreuzvalidierung und Tuning würde ich das Modell nicht anwenden.\n\nnnet_aug |&gt; \n  roc_curve(gender, .pred_w, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\nAbbildung 72.5— ROC Kurve für den nnet Algorithmus.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html#neuronale-netze-mit-keras-tensorflow",
    "href": "classification-neural-networks.html#neuronale-netze-mit-keras-tensorflow",
    "title": "72  Neural networks",
    "section": "72.8 Neuronale Netze mit Keras / Tensorflow",
    "text": "72.8 Neuronale Netze mit Keras / Tensorflow\nJetzt kommen wir zum dicksten Brett. Was wir hier machen ist eigentlich nur ein schwacher Abglanz was Tensorflow eigentlich kann. Über den Algorithmus werden ganze Bücher geschrieben und die Anwendung auf einem Laptop oder Standrechner ist eigentlich dem Algorithmus nicht würdig. Wir werden hier auch nicht alles aus dem Algorithmus raus holen. Das geht auch gar nicht. Wenn du dich tiefer mit der Materie beschäftigen willst, dann ist dies hier ein guter Startpunkt. Wenn du Probleme hast Tensorflow zum Laufen zu kriegen, dann kannst du auch für die einfache Anwendung nnet nutzen. Mit ein wenig Tuning sollten da auch gute Ergebnisse bei herauskommen.\n\n\nWenn du richtig Tensorflow mit R nutzen willst, dann gibt es hier noch das umfangreiche Tutorium für Tensorflow with R. Insbesondere die Nutzung von lime um die Black Box des neuronalen Netzwerks zu erklären wird hier nochmal gezeigt.\nDie Funktion mlp() erlaubt uns als Engine keras zu verweden und damit ein neurales Netzwerk mit dem Tensorflow Algorithmus zu rechnen. Mehr brauchen wir an dieser Stelle erstaml nicht tun. Wir werden hier erstmal keine Tuning Parameter angeben. Später im Kapitel werden wir dann noch ein Tuning für den Algorithmus rechnen.\n\nkeras_mod &lt;- mlp() |&gt; \n  set_engine(\"keras\") |&gt; \n  set_mode(\"classification\")\n\nJetzt bringen wir noch das Rezept des Gummibärchendatensatzes mit dem Modell in einem Workflow zusammen.\n\nkeras_wflow &lt;- workflow() |&gt; \n  add_model(keras_mod) |&gt; \n  add_recipe(gummi_rec)\n\nJetzt können mit mit der Funktion fit() das Modell rechnen. Wenn du Keras und Tensorflow nicht installiert hast, dann wird jetzt meist eine automatische Installation starten. Oder aber du hast dir vorher schon Tensorflow und Keras installiert. Schaue dazu gerne einmal den Quick start um Tensorflow zu installieren an.\n\n\nEpoch 1/20\n17/17 - 4s - loss: 0.6937 - 4s/epoch - 250ms/step\nEpoch 2/20\n17/17 - 0s - loss: 0.6930 - 84ms/epoch - 5ms/step\nEpoch 3/20\n17/17 - 0s - loss: 0.6926 - 67ms/epoch - 4ms/step\nEpoch 4/20\n17/17 - 0s - loss: 0.6921 - 66ms/epoch - 4ms/step\nEpoch 5/20\n17/17 - 0s - loss: 0.6917 - 69ms/epoch - 4ms/step\nEpoch 6/20\n17/17 - 0s - loss: 0.6913 - 67ms/epoch - 4ms/step\nEpoch 7/20\n17/17 - 0s - loss: 0.6908 - 66ms/epoch - 4ms/step\nEpoch 8/20\n17/17 - 0s - loss: 0.6905 - 65ms/epoch - 4ms/step\nEpoch 9/20\n17/17 - 0s - loss: 0.6900 - 66ms/epoch - 4ms/step\nEpoch 10/20\n17/17 - 0s - loss: 0.6896 - 65ms/epoch - 4ms/step\nEpoch 11/20\n17/17 - 0s - loss: 0.6892 - 65ms/epoch - 4ms/step\nEpoch 12/20\n17/17 - 0s - loss: 0.6887 - 66ms/epoch - 4ms/step\nEpoch 13/20\n17/17 - 0s - loss: 0.6883 - 65ms/epoch - 4ms/step\nEpoch 14/20\n17/17 - 0s - loss: 0.6878 - 67ms/epoch - 4ms/step\nEpoch 15/20\n17/17 - 0s - loss: 0.6873 - 66ms/epoch - 4ms/step\nEpoch 16/20\n17/17 - 0s - loss: 0.6868 - 65ms/epoch - 4ms/step\nEpoch 17/20\n17/17 - 0s - loss: 0.6864 - 65ms/epoch - 4ms/step\nEpoch 18/20\n17/17 - 0s - loss: 0.6859 - 65ms/epoch - 4ms/step\nEpoch 19/20\n17/17 - 0s - loss: 0.6853 - 65ms/epoch - 4ms/step\nEpoch 20/20\n17/17 - 0s - loss: 0.6847 - 65ms/epoch - 4ms/step\n\n\n6/6 - 0s - 75ms/epoch - 13ms/step\n6/6 - 0s - 17ms/epoch - 3ms/step\n\n\n\nkeras_fit &lt;- keras_wflow |&gt; \n  parsnip::fit(gummi_train_data)\n\nWenn der Algorithmus durchgelaufen ist, was schon ein paar Sekunden dauern kann, dann können wir danach das Modell nutzen um unser Geschlecht vorherzusagen.\n\nkeras_aug &lt;- augment(keras_fit, gummi_test_data) \n\nWir lassen uns dann wieder die Konfusionsmatrix wiedergeben. Wir sehen, dass wir sehr mies dran sind. Wir haben eine nahezu zufällige Einteilung der Geschlechter durch die Vorhersage.\n\nkeras_cm &lt;- keras_aug |&gt; \n  conf_mat(gender, .pred_class)\n\nkeras_cm\n\n          Truth\nPrediction  m  w\n         m 82 92\n         w  0  1\n\n\nWas schon in der Konfusionsmatrix ziemlich mies aussah, wird natürlich auch so in der Zusammenfassung wiedergegeben.\n\nkeras_cm |&gt; summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.474 \n 2 kap                  binary        0.0101\n 3 sens                 binary        1     \n 4 spec                 binary        0.0108\n 5 ppv                  binary        0.471 \n 6 npv                  binary        1     \n 7 mcc                  binary        0.0712\n 8 j_index              binary        0.0108\n 9 bal_accuracy         binary        0.505 \n10 detection_prevalence binary        0.994 \n11 precision            binary        0.471 \n12 recall               binary        1     \n13 f_meas               binary        0.641 \n\n\nWas sehen wir? Wir sehen, dass unsere Accuracy mit unter 50% schon mehr schlecht ist. Die Zuordnung der Geschlechter wird vom Algorithmus rein zufällig durchgeführt. Wir können daher nicht von einem guten Algorithmus sprechen. In Abbildung 72.6 sehen wir die gewollt schlechte ROC Kurve aus einem keras Algorithmus ohne Tuning. Warum war die nochmal gewollt schlecht? Ich will hier einmal zeigen, dass ein neuronales Netz aus dem Tensorflow Algorithmus meistens ohne ein Tuning sehr schlecht ist. Das kann sich aber durch ein Tuning sehr schnell drehen.\n\nkeras_aug |&gt; \n  roc_curve(gender, .pred_w, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\nAbbildung 72.6— ROC Kurve für den keras Algorithmus.\n\n\n\n\n\n\n\n\n\n\n\nKann ich auch eine Kreuzvalidierung für Keras / Tensorflow durchführen?\n\n\n\nJa, kannst du. Wenn du nur eine Kreuzvalidierung durchführen willst, findest du alles im Kapitel 69 für den \\(k\\)-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf Keras / Tensorflow Algorithmus anwenden.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html#tuning",
    "href": "classification-neural-networks.html#tuning",
    "title": "72  Neural networks",
    "section": "72.9 Tuning",
    "text": "72.9 Tuning\nWas heißt Tuning? Wie bei einem Auto können wir an verschiedenen Stellschrauben bei einem mathematischen Algorithmus schrauben. Welche Schrauben und Teile das sind, hängt dann wieder vom Algorithmus ab. Im Falle des xgboost Algorithmus können wir an folgenden Parametern drehen und jeweils schauen, was dann mit unserer Vorhersage passiert. Insgesamt hat der keras Algorithmus fünf Tuningparameter, wir wählen jetzt für uns hier drei aus. Ich nehme hier auch nur drei Parameter, da sich dann drei Parameter noch sehr gut visuell darstellen lassen. In der Anwendung wäre dann natürlich besser alle Parameter zu tunen, aber das dauert dann auch lange.\n\nhidden_units, Anzahl der Ebenen (eng. layer) in dem neuronalen Netzwerk. Wie viele Ebenen soll unser Netzwerk haben? Oder auch wie deep soll das Netzwerk gebaut werden?\npenalty, ein Wert für die Regulierung des neuronalen Netzwerk.\nepochs, bezieht sich auf einen Zyklus durch die Layer für den gesamten Trainingsdatensatz. Wie oft rechnen wir den Trainingsdatensatz und trainieren unser Netzwerk?\n\nNun ist es so, dass wir natürlich nicht händisch alle möglichen Kombinationen von der Anzahl der ausgewählten Variablen pro Baum, der kleinsten Knotengröße und der Anzahl der Bäume berechnen wollen. Das sind ziemlich viele Kombinationen und wir kommen dann vermutlich schnell durcheinander. Deshalb gibt es die Funktion tune() aus dem R Paket {tune}, die uns einen Prozess anbietet, das Tuning automatisiert durchzuführen.\nDa ich nicht ewig warten wollte, habe ich noch das parallele Rechnern aktiviert, in dem ich mir die Anzahl an Rechenkernen minus eins wiedergeben habe lassen.\n\ncores &lt;- parallel::detectCores() - 1\n\nAls erstes müssen wir uns ein Objekt bauen, das aussieht wie ein ganz normales Modell in der Klassifikation. Aber wir ergänzen jetzt noch hinter jeder zu tunenden Option noch die Funktion tune(). Das sind die Parameter des Algorithmus, die wir später tunen wollen.\n\ntune_spec &lt;- mlp(hidden_units = tune(),\n                 penalty = tune(), \n                 epochs = tune()) |&gt; \n  set_engine(\"keras\", num.threads = cores) |&gt; \n  set_mode(\"classification\") \n\ntune_spec\n\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = tune()\n  penalty = tune()\n  epochs = tune()\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: keras \n\n\nJetzt bauen wir uns den Workflow indem wir statt unserem Modell, die Tuninganweisung in den Workflow reinnehmen. Echt simpel und straightforward. Das Rezept bleibt ja das Gleiche.\n\ngummi_tune_wflow &lt;- workflow() |&gt; \n  add_model(tune_spec) |&gt; \n  add_recipe(gummi_rec)\n\nJetzt müssen wir noch alle Kombinationen aus den drei Parametern hidden_units, penalty und epochs ermitteln. Das macht die Funktion grid_regular(). Es gibt da noch andere Funktionen in dem R Paket {tune}, aber ich konzentriere mich hier auf die einfachste. Jetzt müssen wir noch die Anzahl an Kombinationen festlegen. Ich möchte für jeden Parameter fünf Werte tunen. Daher nutze ich hier die Option levels = 5 auch damit hier die Ausführung nicht so lange läuft. Fange am besten mit levels = 5 an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert. Dann kannst du die Levels noch hochschrauben. Beachte aber, dass mehr Level nur mehr Zwischenschritte bedeutet. Jede Option hat eine Spannweite range, die du dann anpassen musst, wenn du höhere Werte haben willst. Mehr Level würden nur mehr Zwischenschritte bedeuten.\n\ngummi_grid &lt;- grid_regular(hidden_units(range = c(1, 100)),\n                           penalty(),\n                           epochs(range = c(10, 200)),\n                           levels = 5)\n\nDas Tuning nur auf dem Trainingsdatensatz durchzuführen ist nicht so eine gute Idee. Deshalb nutzen wir hier auch die Kreuzvalidierung. Eigentlich ist eine 10-fache Kreuzvalidierung mit \\(v=10\\) besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur \\(v=5\\) gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.\n\ngummi_folds &lt;- vfold_cv(gummi_train_data, v = 5)\n\nNun bringen wir den Workflow zusammen mit dem Tuninggrid und unseren Sets der Kreuzvaidierung. Daher pipen wir den Workflow in die Funktion tune_grid(). Als Optionen brauchen wir die Kreuzvaldierungsdatensätze und das Tuninggrid. Wenn du control_grid(verbose = TRUE) wählst, dann erhälst du eine Ausgabe wie weit das Tuning gerade ist. Achtung!, das Tuning dauert seine Zeit. Im Falle des keras Algorithmus dauert das Tuning extrem lange, aber immer noch nur ein paar Stunden. Wenn du dann alle fünf Parameter des keras Algorithmustunen wollen würdest, dann würde die Berechnung Tage dauern. Deshalb ist ein Großerechner mit mehreren Kernen unabdingbar für die Nutzung von deep learning Du kannst das Ergebnis des simpleren Tunings auch in der Datei gummi_xgboost_tune_res.rds finden.\n\ngummi_tune_res &lt;- gummi_tune_wflow |&gt; \n   tune_grid(resamples = gummi_folds,\n             grid = gummi_grid,\n             control = control_grid(verbose = FALSE))\n\nDamit du nicht das Tuning durchlaufen lassen musst, habe ich das Tuning in die Datei gummi_xgboost_tune_res.rds abgespeichert und du kannst dann über die Funktion read_rds() wieder einlesen. Dann kannst du den R Code hier wieder weiter ausführen.\nNachdem das Tuning durchgelaufen ist, können wir uns über die Funktion collect_metrics(), die Ergebnisse des Tunings für jede Kombination der drei Parameter hidden_units, penalty und epochs wiedergeben lassen. Diese Ausgabe ist super unübersichtlich. Ich habe mich ja am Anfange des Abschnitts auch für drei Tuningparameter entschieden, da sich dann diese drei Parameter noch gut visualisieren lassen. Deshalb einmal die Abbildung der mittleren Accuarcy und der mittleren AUC-Werte über alle Kreuzvalidierungen.\n\ngummi_tune_res |&gt;\n  collect_metrics() |&gt;\n  mutate(hidden_units = as_factor(hidden_units),\n         penalty = as_factor(penalty)) |&gt;\n  ggplot(aes(epochs, mean, color = hidden_units, linetype = penalty)) +\n  theme_minimal() +\n  geom_line(alpha = 0.6) +\n  geom_point() +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_okabeito()\n\n\n\n\n\n\n\nAbbildung 72.7— Tuning Kurven für den keras Algorithmus.\n\n\n\n\n\nDamit wir nicht händisch uns die beste Kombination raussuchen müssen, können wir die Funktion show_best() nutzen. Wir wählen hier die beste Accuarcy und erhalten dann die sortierten Ergebnisse nach der Accuarcy des Tunings.\n\ngummi_tune_res |&gt;\n  show_best(\"accuracy\")\n\n# A tibble: 5 × 9\n  hidden_units     penalty epochs .metric .estimator  mean     n std_err .config\n         &lt;int&gt;       &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n1           75    1   e-10    200 accura… binary     0.819     5  0.0142 Prepro…\n2           75    3.16e- 8    200 accura… binary     0.813     5  0.0150 Prepro…\n3          100    1   e- 5    200 accura… binary     0.813     5  0.0192 Prepro…\n4          100    1   e-10    200 accura… binary     0.811     5  0.0183 Prepro…\n5           25    3.16e- 8    200 accura… binary     0.811     5  0.0198 Prepro…\n\n\nDas war die Funktion show_best() aber wir können uns auch die gleich die besten Parameter nach der Accuracy raus ziehen. Das Rausziehen der besten Parameter macht für uns die Funktion select_best().\n\nbest_keras &lt;- gummi_tune_res |&gt;\n  select_best(\"accuracy\")\n\nbest_keras\n\n# A tibble: 1 × 4\n  hidden_units      penalty epochs .config               \n         &lt;int&gt;        &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;                 \n1           75 0.0000000001    200 Preprocessor1_Model104\n\n\nWir sehen, dass wir hidden_units = 75 wählen sollten. Dann müssen wir als Penalty penalty = 0.0000000001 nutzen. Die Anzahl an Durchläufen pro Training ist dann epochs = 200. Müssen wir jetzt die Zahlen wieder in ein Modell eingeben? Nein, müssen wir nicht. Mit der Funktion finalize_workflow() können wir dann die besten Parameter aus unserem Tuning gleich mit dem Workflow kombinieren. Dann haben wir unseren finalen, getunten Workflow. Du siehst dann auch in der Ausgabe, dass die neuen Parameter in dem keras Algorithmus übernommen wurden\n\nfinal_gummi_wf &lt;- gummi_tune_wflow |&gt; \n  finalize_workflow(best_keras)\n\nfinal_gummi_wf \n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mlp()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_mean()\n• step_impute_bag()\n• step_range()\n• step_dummy()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = 75\n  penalty = 1e-10\n  epochs = 200\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: keras \n\n\nJetzt bleibt uns nur noch der letzte Fit übrig. Wir wollen unseren finalen, getunten Workflow auf die Testdaten anwenden. Dafür gibt es dann auch die passende Funktion. Das macht für uns die Funktion last_fit(), die sich dann die Informationen für die Trainings- und Testdaten aus unserem Datensplit von ganz am Anfang extrahiert.\n\nfinal_fit &lt;- final_gummi_wf |&gt;\n  last_fit(gummi_data_split) \n\n6/6 - 1s - 509ms/epoch - 85ms/step\n6/6 - 0s - 18ms/epoch - 3ms/step\n\n\nDa wir immer noch eine Kreuzvaldierung rechnen, müssen wir dann natürlich wieder alle Informationen über alle Kreuzvaldierungsdatensätze einsammeln. Dann erhalten wir unsere beiden Gütekriterien für die Klassifikation des Geschlechts unser Studierenden nach dem keras Algorithmus. Die Zahlen sind schon gut für echte Daten. Eine Accuracy von 81% bedeutet das wir über acht von zehn Studierenden richtig klassifizieren. Die AUC ist auch schon fast hervorragend, wir bringen kaum Label durcheinander.\n\nfinal_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.806 Preprocessor1_Model1\n2 roc_auc  binary         0.910 Preprocessor1_Model1\n\n\nDann bleibt uns nur noch die ROC Kurve zu visualisieren. Da wir wieder etwas faul sind, nutzen wir die Funktion autoplot(). Als Alternative geht natürlich auch das R Paket {pROC}, was eine Menge mehr Funktionen und Möglichkeiten bietet.\n\nfinal_fit |&gt;\n  collect_predictions() |&gt; \n  roc_curve(gender, .pred_w, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\nAbbildung 72.8— ROC Kurve für den keras Algorithmus nach der Kreuvalidierung und dem Tuning.\n\n\n\n\n\nDa wir eine ROC Kurve hier vorliegen haben, die sehr weit weg von der Diagonalen ist, haben wir sehr viele richtig vorhergesagte Studierende in unseren Testdaten. Unser Modell funktioniert um das Geschlecht von Studierenden anhand unserer Gummibärchendaten vorherzusagen. Besonders bei den neuronalen Netzwerken sieht man, wenn du die ROC Kurven vor und nach dem Tuning vergleichst, wie wichtig das Tuning ist. Dabei haben wir hier nur die abgespeckte Variante genutzt, da mein Rechner nicht länger laufen sollte.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "classification-neural-networks.html#referenzen",
    "href": "classification-neural-networks.html#referenzen",
    "title": "72  Neural networks",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\nAbbildung 72.1— Darstellung von drei Inputneuronen \\(x_1, x_2, x_3\\), einem Biasneuron \\(1\\) mit den jeweiligen weitergeleiteten Gewichten \\(w_1, w_2, w_3\\) und \\(w_0\\). Die Summierungsfunktion sowie die Aktivierungsfunktion werden meist in einen gemeinsamen Knoten dargestellt. Hier sind beide Formen einmal abgebildet. Wenn das Neuron aktiviert ist, gibt es die Summe als Output weiter.\nAbbildung 72.2— Darstellung von drei Inputneuronen \\(x_1, x_2, x_3\\) ohne ein Biasneuron. Die drei Inputbeurnen leiten ihre Gewichte an die Hidden Layer Neoronen weiter. In jedem diesem Neuron findet eine Summiierung in eine eventuelle Aktivierung statt. Aktivierte Neuronen leiten die Summation als Gewichte dann an weitere Hidden Layer Neuronen weiter. Am Ende findet eine Entscheidung in den Outputneuronen statt.\nAbbildung 72.3— Visualisierung des neuronalen Netzwerkes mit drei vorgebenen Hidden Layers. Die Hidden Layers repräsentieren in diesem Beispiel die Characktere Wolf, Großmutter und den Holzfäller.\nAbbildung 72.4 (a)— Neuronales Netzwerk mit den Gewichten und dem Bias als numerische Representation.\nAbbildung 72.4 (b)— Neurales Interpretationsdiagramm für ein neurales Netzwerk.\nAbbildung 72.5— ROC Kurve für den nnet Algorithmus.\nAbbildung 72.6— ROC Kurve für den keras Algorithmus.\nAbbildung 72.7— Tuning Kurven für den keras Algorithmus.\nAbbildung 72.8— ROC Kurve für den keras Algorithmus nach der Kreuvalidierung und dem Tuning.\n\n\n\nKubat M. 2017. An introduction to machine learning. Springer.\n\n\nMueller JP, Massaron L. 2019. Deep Learning for dummies. John Wiley & Sons.",
    "crumbs": [
      "Klassifikation oder maschinelles Lernen",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>Neural networks</span>"
    ]
  },
  {
    "objectID": "abspann.html",
    "href": "abspann.html",
    "title": "Abspann",
    "section": "",
    "text": "Letzte Änderung am 25. March 2024 um 21:05:34\n\n“I may not have gone where I intended to go, but I think I have ended up where I needed to be.” — Douglas Adams\n\nAls ich vor ungefähr 2 Jahren anfing auf einem großen, weißen Blatt alles aufzuschreiben, was mir so einfiel und in ein Buch Bio Data Science gehört, war ich am Ende etwas schockiert. Das war echt eine Menge an Begriffen und Methoden. Teilweise nur die Oberbegriffe, die sich dann später als ein Kaleidoskop an Unterthemen herausstellen sollte. Also machte ich mich ans Werk wie Beppo, der Straßenkehrer, immer schön eine Sache nach der nächsten. Und so wuchs fast unmerklich dieses Werk hier an. Am Ende bin ich recht froh so weit gekommen zu sein. Sicherlich fehlt noch das ein oder andere Thema – wie du auch immer auf meiner Willkommensseite im Baustellenkasten siehst – aber soweit bin ich erstmal zufrieden. Jedenfalls so zufrieden, dass ich diesen Abspann schreiben kann. Ein wunderbarer Erfolg. Oder um es mit den Worten von James Clear zu sagen…\n\n“The imperfect project you actually complete is worth more than the perfect project you never finish.” — James Clear\n\nWas noch offen ist und vermutlich noch meiner Aufmerksamkeit bedarf, ist die bayesianische Statistik. Aktuell habe ich nur Bruckstücke von Ideen, wie ich das komplexe Thema aufarbeiten kann, aber da habe ich ja jetzt noch etwas Zeit mich mit zu beschäftigen.\nEine andere große Baustelle und auch ein Teil, den ich noch schreiben möchte oder vielleicht auch nicht, ist der Bereich genetische Analysen in der Bioinformatik. Wann ich dazu kommen werde, steht dann aber noch in den Sternen… Aber dieses Thema wird dann das Skript Bioinformatik werden. Genetische Analyse passen hier definitiv nicht mehr rein. Ich werde aber wirklich eine Zeit brauchen, um dort “fertig” zu werden.\nWas es definitiv nicht geben wird sind quantitative Methoden der Sozialwissenschaftlichen. Das ist weder mein Bereich noch mein Interesse. Da gibt es sicherlich andere Lehrende, die sich mit diesem Thema hervorragend auseinander gestetzt haben.\nWenn dir also noch was fehlt, dann schreibe mir doch eine Mail, dann schaue ich, dass ich das Thema in den nächsten Wochen und Monaten ergänze. Einfach fühlen sich hier die Seiten dann nämlich doch nicht. Und so Ende ich vorerst mal mit dem Zitat von Richard Feynman über das Lernen und Lehren.\n\n“If you want to master something, teach it. The more you teach, the better you learn. Teaching is a powerful tool to learning.” — Richard Feynman\n\nIch hoffe du hast ähnlich viel gelernt wie ich, als ich das hier alles geschrieben habe. Möge dir das Buch Bio Data Science von Nutzen sein. Für mich ist es wahrliche eine etwas andere Reise…\n\n“To write a book you must become the book” — Naval Ravikant",
    "crumbs": [
      "Abspann"
    ]
  },
  {
    "objectID": "app-spielecke.html",
    "href": "app-spielecke.html",
    "title": "73  Spielecke",
    "section": "",
    "text": "ggplot defaults\ntheme_gray()\nggplot defaults\ntheme(\nline = element_line(colour = \"black\", size = 0.5, linetype = 1,  lineend = \"butt\"), \nrect = element_rect(fill = \"white\",  colour = \"black\", size = 0.5, linetype = 1), \ntext = element_text(family = base_family, \nface = \"plain\", colour = \"black\", size = base_size, hjust = 0.5, vjust = 0.5, angle = 0, lineheight = 0.9), \naxis.text = element_text(size = rel(0.8), colour = \"grey50\"), \nstrip.text = element_text(size = rel(0.8)), \naxis.line = element_blank(), \naxis.text.x = element_text(vjust = 1), \naxis.text.y = element_text(hjust = 1), \naxis.ticks = element_line(colour = \"grey50\"), \naxis.title.x = element_text(), \naxis.title.y = element_text(angle = 90), \naxis.ticks.length = unit(0.15, \"cm\"), \naxis.ticks.margin = unit(0.1,  \"cm\"), \nlegend.background = element_rect(colour = NA), \nlegend.margin = unit(0.2, \"cm\"), \nlegend.key = element_rect(fill = \"grey95\", colour = \"white\"), \nlegend.key.size = unit(1.2, \"lines\"), \nlegend.key.height = NULL, \nlegend.key.width = NULL, \nlegend.text = element_text(size = rel(0.8)), \nlegend.text.align = NULL, \nlegend.title = element_text(size = rel(0.8), face = \"bold\", hjust = 0),    \nlegend.title.align = NULL, \nlegend.position = \"right\", \nlegend.direction = NULL, \nlegend.justification = \"center\", \nlegend.box = NULL, \npanel.background = element_rect(fill = \"grey90\", colour = NA), \npanel.border = element_blank(), \npanel.grid.major = element_line(colour = \"white\"), \npanel.grid.minor = element_line(colour = \"grey95\", size = 0.25), \npanel.margin = unit(0.25, \"lines\"), \npanel.margin.x = NULL, \npanel.margin.y = NULL, \nstrip.background = element_rect(fill = \"grey80\", colour = NA), \nstrip.text.x = element_text(), \nstrip.text.y = element_text(angle = -90), \nplot.background = element_rect(colour = \"white\"), \nplot.title = element_text(size = rel(1.2)), \nplot.margin = unit(c(1, 1, 0.5, 0.5), \"lines\"), complete = TRUE)\n)",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#quecksilber-in-walen",
    "href": "app-spielecke.html#quecksilber-in-walen",
    "title": "73  Spielecke",
    "section": "Quecksilber in Walen",
    "text": "Quecksilber in Walen\nTuna-Steak und Co: Methylquecksilber im Fisch\nFäröer - Walfleisch-Verseuchung\nMinamata-Übereinkommen",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#paper-ideen",
    "href": "app-spielecke.html#paper-ideen",
    "title": "73  Spielecke",
    "section": "Paper Ideen",
    "text": "Paper Ideen\nTierpaper\n\nArginine Nutrition in Neonatal Pigs\nFiber effects in nutrition and gut health in pigs\nPhosphorus nutrition of growing pigs\n\nPflanzenpaper\n\nHoverfly pollination enhances yield and fruit quality in mango under protected cultivation\nPlant Growth, Yield, and Fruit Size Improvements in ‘Alicia’ Papaya Multiplied by Grafting",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#zerforschen",
    "href": "app-spielecke.html#zerforschen",
    "title": "73  Spielecke",
    "section": "Zerforschen",
    "text": "Zerforschen\nAktuell leider nichts da… juchee!",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#zitate",
    "href": "app-spielecke.html#zitate",
    "title": "73  Spielecke",
    "section": "Zitate",
    "text": "Zitate\n\n“I never once failed at making a light bulb. I just found out 99 ways not to make one.” — Thomas A. Edison\n\n\n“Leben heißt leiden, überleben heißt, im Leiden einen Sinn finden.” — Friedrich Nietzsche\n\n\n“Wachstum ist nicht alles, das ist wahr. Aber ohne Wachstum ist alles nichts.” — Angela Merkel\n\n\n“Competition is for losers!” — Peter Thiel\n\n\n“Das Pferd frisst keinen Gurkensalat” — Philipp Reis erster 1981 telefonisch übertragende Satz\n\n\n“One glance at a book and you hear the voice of another person perhaps someone dead for thousands of years. Across the millennia the author is speaking clearly and silently inside your head, directly to YOU.” — Carl Sagan\n\n\n“If you feel safe in the area that you’re working in, you’re not working in the right area. Always go a little further into the water than you feel you’re capable of being in. Go a little bit out of your depth, and when you don’t feel that your feet are quite touching the bottom, you’re just about in the right place to do something exciting.” – David Bowie\n\n\n“(1) Alles was es schon gab, als Du geboren wurdest, ist normal und gewöhnlich. Diese Dinge werden als natürlich wahrgenommen und halten die Welt am Laufen. (2) Alles was zwischen Deinem 16ten und 36ten Lebensjahr erfunden wird ist neu, aufregend und revoltionär. Und vermutlich kannst Du in dem Bereich sogar Karriere machen. (3) Alles was nach dem 36ten Lebensjahr erfunden wird ist gegen die natürliche Ordnung der Dinge.” — Douglas Adams, Per Anhalter durch die Galaxis\n\n\n“Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.” — Brian Kernighan, professor at Princeton University.\n\n\n“The three stages of career development are: 1. I want to be in the meeting; 2. I want to run the meeting; 3. I want to avoid meetings.” — Jay Ferro\n\n\n“Freude ist ein Akt des Trotzes. Mit Freude gewinnen wir, auch wenn wir verlieren. Gut gelebt zu haben ist alles was uns bleibt, denn sterben müssen wir alle.” — Jaghatai Khan, The Lost and the Damned\n\n\n“Freude ist ein Akt des Trotzes. Durch sie gewinnen wir, auch wenn wir verlieren. Denn sterben müssen wir alle und ein schönes Leben ist alles was uns bleibt.” — Jaghatai Khan, The Lost and the Damned",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#korrelation",
    "href": "app-spielecke.html#korrelation",
    "title": "73  Spielecke",
    "section": "Korrelation",
    "text": "Korrelation\nHow does Polychoric Correlation Work? (aka Ordinal-to-Ordinal correlation)\nAn Alternative to the Correlation Coefficient That Works For Numeric and Categorical Variables",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#pakete-die-ich-mal-anschauen-will",
    "href": "app-spielecke.html#pakete-die-ich-mal-anschauen-will",
    "title": "73  Spielecke",
    "section": "Pakete, die ich mal anschauen will…",
    "text": "Pakete, die ich mal anschauen will…\nR Paket {ggdist}\nDas R Paket {visibly} auf An Introduction to Visibly\nR Paket {innsight}",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#learning-text",
    "href": "app-spielecke.html#learning-text",
    "title": "73  Spielecke",
    "section": "Learning text",
    "text": "Learning text\nWachsamkeit und Konzentration kann ein Mensch nur für 90 Minuten halten. Selbst dann ist Aufmerksamkeit ein Flackern von höherer und niedrigerer Intensität. Danach muss der Mensch 1-2 Stunden lang wirklich ruhen, bevor er wieder sehr hart arbeiten & lernen kann.\nFolgende Dinge, die innerhalb von 4 Stunden nach diesen 90-minütigen Lerneinheiten durchgeführt werden, beschleunigen das Lernen.\n\nKurzes Nickerchen\nNichtschlafende tiefe Ruhe (NSDR)\nYoga Nidra\nFormen der Meditation, die nicht viel fokussierte Konzentration erfordern,\n\nFolgende Dinge helfen während der Lernphase das Lernen zu verstärken und zu festigen. Der Hippocampus wiederholt während dieser Zeit die Informationen mit 20-facher Geschwindigkeit und beschleunigt das Lernen und das Behalten der neu gelernten Informationen.\n\nMache ab und zu 10 Sekunden Pause vom Lernen, in denen du absolut nichts tust\nMache den Kopf frei (Lückeneffekt/Mikropausen),\nInkrementelles Lernen. Du kannst das Lernen in kleine, konzentrierte Einheiten aufteilen.\nStelle dir einen Timer für 3 Minuten ein, schalte das Telefon aus und verbringe die 3 Minuten damit, eine Sache intensiv zu lernen, auch wenn es sich anfühlt, als würde es aktuell nichts bringen.\nWenn du das wiederholt tust, können diese kleinen Schritte des Lernens zu einer übergroßen Menge des Lernens insgesamt führen.\n\nHow to Learn Anything You Want | Andrew Huberman",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#weitere-datenquellen",
    "href": "app-spielecke.html#weitere-datenquellen",
    "title": "73  Spielecke",
    "section": "Weitere Datenquellen",
    "text": "Weitere Datenquellen\nFood and agriculture data\nMit dem R Paket {FAOSTAT} und der Vignette FAOSTAT: Download Data from the FAOSTAT Database\nMit dem R Paket {owidR} haben wir auch eine Möglichkeit direkt auf die Datenbank von Our World in Data zuzugreifen.\n\nlibrary(owidR)\nfoo &lt;- owid_search(\"annual\") \nowid(\"annual-co2-emissions-by-region\")\nowid(foo[3])\n\nEine wunderbare Sammlung von Datensätzen aus dem Bereich der Agarwissenschaften liefert das R Paket {agridat}. Über die Hilfeseite agridat: Agricultural Datasets findest du dann einmal einen gesamten Überblick und auch die Informationen über einige ausgewählte Datensätze aus Dutzenden von Datensätzen. Alle Datensätze der wichtigen Bücher zu dem experimentellen Designs sind dort eigentlich enthalten und einmal kuratiert.\nHier noch der Link zu agridat - Datensätze mit Abbildungen in {desplot}. Du musst dann auf die jeweiligen Datensätze in der Liste klicken und dann kommst du zu dem Datensatz mit mehr Details sowie meistens auch einer Abbildung in desplot.",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#marginal-effects",
    "href": "app-spielecke.html#marginal-effects",
    "title": "73  Spielecke",
    "section": "Marginal effects",
    "text": "Marginal effects\nMarginal Effects Zoo\nR Paket {marginaleffects}\nMarginal and conditional effects for GLMMs with {marginaleffects}\nMarginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are\n\nLatent Class Analysis\nWie immer gibt es eine Reihe von Tutorien auf denen dieser Abschnitt aufbaut. Zum einen wirf einfach mal einen Blick in das Tutorium Latent Class Analysis Using R. Eine leider etwas veraltete Übersicht über mögliche R Pakete liefert Ways to do Latent Class Analysis in R. Ich habe da immer mal quer geschaut und mich dann für die Pakete hier entschieden. Es gibt sicherlich noch andere Möglichkeiten eine latent class analysis zu rechnen.\nWenn du mehr über latent class analysis erfahren möchtest, dann kann ich dir nur das LCA Frequently Asked Questions (FAQ) empfehlen. Das FAQ ist sehr umfangreich und beschäftigt sich mit allen wichtigen Punkten. Wir wollen uns ja mit dem R Paket poLCA beschäftigen. Hier gibt es zwei Tutorien. Einmal gibt es das Tutorium Example for a latent class analysis with the poLCA-package in R und das Tutroium Latent Class Analysis. Und natürlich die Litertur von Linzer und Lewis (2011) mit der entsprechenden Veröffentlichung poLCA: An R Package for Polytomous Variable Latent Class Analysis\nGrundsätzlich basiert die latent class analysis nicht auf Distanzen sondern versucht über eine Modellierung der Klassenzugehörigkeitswahrscheinlichkeit getrennte Gruppen zu bilden. Wir wollen also \\(k\\) Klassen haben und im idealen Fall können wir durch unsere Variablen in dem Datensatz jeweils mit einer 100% Wahrscheinlichkeit einer der drei Klassen zuordnen. Was dann diese \\(k\\) Klassen aussagen, müssen wir dann selber anhand der zugewiesenen Variablen aus unseren Daten interpretieren.\n\npacman::p_load(tidyverse, magrittr, janitor, conflicted)\n\nanimals_tbl &lt;- read_excel(\"data/cluster_animal.xlsx\", sheet = 1) |&gt; \n  clean_names() \n\n\npacman::p_load(poLCA)\n\npoLCA(cbind(warm_blooded, fly, vertebrate, threatened, live_in_groups) ~ 1,\n      nclass = 3,\n      data = animals_tbl,\n      nrep = 1,\n      na.rm = FALSE,\n      graphs = TRUE,\n      maxiter = 100000\n)\n\nHier hängen wir dann an der Interpretation. Da müssen wir nochmal tiefer schauen.\n\n\nStructural Equation Modeling\nVan Lissa u. a. (2023) tidySEM\nStructural Equation Modeling\nIntroduction to structural equation modeling (sem) in r with lavaan\nIntro to structural equation modeling\nSchöne Diagramme Structural Equation Models",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#links",
    "href": "app-spielecke.html#links",
    "title": "73  Spielecke",
    "section": "Links",
    "text": "Links\nLarge language models, explained with a minimum of math and jargon\nData Science\n\nReal World Data Liu und Panagiotakos (2022)\nWarum Data Science Hariri u. a. (2019)\nParadigmenwechsel?\n\nDeutsches Sprichwort: Deutsch mit jemanden Reden; Verstehe nur Spanisch\n\n“Gott würfelt nicht!” — Albert Einstein\n\n\nFrequentistischer Wahrscheinlichkeitsbegriff\nBayessche Wahrscheinlichkeitsbegriff",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "app-spielecke.html#referenzen",
    "href": "app-spielecke.html#referenzen",
    "title": "73  Spielecke",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nHariri RH, Fredericks EM, Bowers KM. 2019. Uncertainty in big data analytics: survey, opportunities, and challenges. Journal of Big Data 6: 1–16.\n\n\nLinzer DA, Lewis JB. 2011. poLCA: An R package for polytomous variable latent class analysis. Journal of statistical software 42: 1–29.\n\n\nLiu F, Panagiotakos D. 2022. Real-world data: a brief review of the methods, applications, challenges and opportunities. BMC Medical Research Methodology 22: 287.\n\n\nVan Lissa CJ, Villarreal MG, Anadria D. 2023. Best Practices in Latent Class Analysis using the Open-Source R-Package tidySEM.",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#probability-model",
    "href": "stat-modeling-preface.html#probability-model",
    "title": "Statistisches Modellieren",
    "section": "Probability Model",
    "text": "Probability Model\n\n\n\n\n\n\n\n\nAbbildung 4— Verschiedene Ziele und Möglichkeiten des statistischen Modellierens. Grob können die Möglichkeiten in drei große thematische Zusammenhänge eingeteilt werden.",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#wir-mitteln-uns-die-welt-wie-sie-mir-gefällt",
    "href": "stat-modeling-preface.html#wir-mitteln-uns-die-welt-wie-sie-mir-gefällt",
    "title": "Statistisches Modellieren",
    "section": "Wir mitteln uns die Welt, wie sie mir gefällt…",
    "text": "Wir mitteln uns die Welt, wie sie mir gefällt…\n\n“2 x 3 macht 4; Widdewiddewitt; und Drei macht Neune!; Wir machen uns die Welt; Widdewidde wie sie uns gefällt…” — Hey Pippi Langstrumpf\n\nWo sind die Barplots hin?\n\n\n\n\n\n\n\n\nAbbildung 1— Verschiedene Ziele und Möglichkeiten des statistischen Modellierens. Grob können die Möglichkeiten in drei große thematische Zusammenhänge eingeteilt werden.",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#wir-mitteln-uns-die-welt-wie-sie-uns-gefällt",
    "href": "stat-modeling-preface.html#wir-mitteln-uns-die-welt-wie-sie-uns-gefällt",
    "title": "Statistisches Modellieren",
    "section": "Wir mitteln uns die Welt, wie sie uns gefällt…",
    "text": "Wir mitteln uns die Welt, wie sie uns gefällt…\n\n“2 x 3 macht 4; Widdewiddewitt; und Drei macht Neune!; Wir machen uns die Welt; Widdewidde wie sie uns gefällt…” — Hey Pippi Langstrumpf\n\nWo sind die Barplots hin?\n\n\n\n\n\n\n\n\nAbbildung 2— Visueller Zusammenhang eines gemittelten Outcomes (\\(y\\)) aus verschiedenen Verteilung im Verhältnis zu der Einflussvariable (\\(x\\)) mit zwei oder mehr Kategorien anhand von Barplots. Hauptsächlich unterscheiden sich die Barplots durch die unterschiedlichen Einheiten auf der \\(y\\)-Achse. Die Fehlerbalken stellen den Standardfehler (SE) dar. (A) Mittler Ertrag [t/ha]. (B) Mittlerer Befahll [Anzahl/Parzelle]. (C) Mittlere Note [Likert-Skala] (D) Mittlerer Anteil [%] (E) Mittlerer Anteil infiziert (%). [Zum Vergrößern anklicken]",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  },
  {
    "objectID": "app-spielecke.html#blaubeeren-aus-peru",
    "href": "app-spielecke.html#blaubeeren-aus-peru",
    "title": "73  Spielecke",
    "section": "Blaubeeren aus Peru",
    "text": "Blaubeeren aus Peru\nWas ist ökologisch vertretbar bei Früchten aus Peru/Meiko und anderen ländern? Waserverbvrauch von Avocado mit beachten.\nLink\nLink\nLink\nLink\nLink\nRiesenfaultier\nBlauzahn & Bluetooth\nLink\nLink",
    "crumbs": [
      "Abspann",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Spielecke</span>"
    ]
  },
  {
    "objectID": "stat-modeling-preface.html#verteilungsfamilien",
    "href": "stat-modeling-preface.html#verteilungsfamilien",
    "title": "Statistisches Modellieren",
    "section": "",
    "text": "Gaussian\nDie Gaussianverteilung ist die Normalverteilung. Da wir nur im Deutschen wirklich von einer Normalverteilung sprechen, müssen wir hier auch wissen, dass wir in R die Verteilung Gaussian wählen müssen. Häufige Messwerte oder Outcomes, die einer Normalverteilung folgen, sind das Gewicht, die Größe, die Höhe oder der Umfang einer Beobachtung. Dementsprechend geht es dann in dem Kapitel zu der Gaussian Regression weiter.\n\n\n\n\n\n\n\n\nAbbildung 1— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Normalverteilung (Gaussian) im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\n\n\n\n\n\n\n\n\n\n\n\nWir mitteln uns die Welt, wie sie uns gefällt…\n\n\n\n\n“2 x 3 macht 4; Widdewiddewitt; und Drei macht Neune!; Wir machen uns die Welt; Widdewidde wie sie uns gefällt…” — Hey Pippi Langstrumpf\n\nWo sind die Barplots hin?\n\n\n\n\n\n\n\n\nAbbildung 2— Visueller Zusammenhang eines gemittelten Outcomes (\\(y\\)) aus verschiedenen Verteilung im Verhältnis zu der Einflussvariable (\\(x\\)) mit zwei oder mehr Kategorien anhand von Barplots. Hauptsächlich unterscheiden sich die Barplots durch die unterschiedlichen Einheiten auf der \\(y\\)-Achse. Die Fehlerbalken stellen den Standardfehler (SE) dar. (A) Mittler Ertrag [t/ha]. (B) Mittlerer Befahll [Anzahl/Parzelle]. (C) Mittlere Note [Likert-Skala] (D) Mittlerer Anteil [%] (E) Mittlerer Anteil infiziert (%). [Zum Vergrößern anklicken]\n\n\n\n\n\n\n\n\n\nPoisson\n\n\n\n\n\n\n\n\nAbbildung 3— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Poissonverteilung zu Zähldaten im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\n\n\n\n\n\n\n\nBeta\n\n\n\n\n\n\n\n\nAbbildung 4— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Betaverteilung zu Häufigkeiten im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\n\n\n\n\n\n\n\nOrdinal\n\n\n\n\n\n\n\n\nAbbildung 5— Visueller Zusammenhang eines geordneten, kategoriellen Outcomes (\\(y\\)) aus einer Ordinalverteilung wie Noten auf der Likert-Skala im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\n\n\n\n\n\n\n\nBinomial\n\n\n\n\n\n\n\n\nAbbildung 6— Visueller Zusammenhang eines kategoriellen, binären Outcomes (\\(y\\)) aus einer Binomialverteilung im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 7— Visueller Zusammenhang eines kategoriellen, binären Outcomes (\\(y\\)) aus einer Binomialverteilung im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)) modelliert mit einem Probability model wie eine normalverteiltes Outcome. Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\n\n\n\n\n\n\n\n\nAbbildung 1— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Normalverteilung (Gaussian) im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\nAbbildung 2— Visueller Zusammenhang eines gemittelten Outcomes (\\(y\\)) aus verschiedenen Verteilung im Verhältnis zu der Einflussvariable (\\(x\\)) mit zwei oder mehr Kategorien anhand von Barplots. Hauptsächlich unterscheiden sich die Barplots durch die unterschiedlichen Einheiten auf der \\(y\\)-Achse. Die Fehlerbalken stellen den Standardfehler (SE) dar. (A) Mittler Ertrag [t/ha]. (B) Mittlerer Befahll [Anzahl/Parzelle]. (C) Mittlere Note [Likert-Skala] (D) Mittlerer Anteil [%] (E) Mittlerer Anteil infiziert (%). [Zum Vergrößern anklicken]\nAbbildung 3— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Poissonverteilung zu Zähldaten im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\nAbbildung 4— Visueller Zusammenhang eines kontinuierlichen Outcomes (\\(y\\)) aus einer Betaverteilung zu Häufigkeiten im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\nAbbildung 5— Visueller Zusammenhang eines geordneten, kategoriellen Outcomes (\\(y\\)) aus einer Ordinalverteilung wie Noten auf der Likert-Skala im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\nAbbildung 6— Visueller Zusammenhang eines kategoriellen, binären Outcomes (\\(y\\)) aus einer Binomialverteilung im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)). Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]\nAbbildung 7— Visueller Zusammenhang eines kategoriellen, binären Outcomes (\\(y\\)) aus einer Binomialverteilung im Verhätnis zu verschiedenen Skalen der Einflussvariable (\\(x\\)) modelliert mit einem Probability model wie eine normalverteiltes Outcome. Ein Punkt stellt eine Beobachtung dar. (A) \\(x\\) ist kontinuierlich. (B) \\(x\\) ist kategoriell mit zwei oder mehr Gruppen. (C) \\(x\\) ist kategoriell mit zwei Gruppen. [Zum Vergrößern anklicken]",
    "crumbs": [
      "Statistisches Modellieren"
    ]
  }
]