```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra)
```

# Logistische Regression {#sec-logistic}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

Die logistische Regression ist *die* Regression, wenn wir in die Medizin schauen. Wohl in keinem Bereich der Wissenschaften wird so viel eine logistische Regression gerechnet wie in der Humanmedizin, Epidemiologie oder Pharmazie. Wir haben in der logistischen Regression ein $0/1$ Outcome als $y$ vorliegen. Also entweder ist eine Beobachtung erkrankt oder nicht. Meistens beschränkt sich die Betrachtung auf erkrankt ($1$, ja) oder eben nicht erkrankt ($0$, nein) bzw. gesund. Wichtig hierbei ist, dass wir eigentlich immer sagen, dass das *Schlechte* mit $1$ kodiert wird. Wenn du das machst, dann wird dir die Interpretation der Effektschätzer der logistischen Regression leichter fallen.

Gleich zu Beginn dann nochmal wir werden die logistische Regression in den Agrarwisenschaften eher selten sehen. Im Bereich der Pflanzenwissenschaften kommt die logistsische Regression kaum bis gar nicht vor. Im Bereich der Tierwissenschaften schon eher, aber dort dann im Bereich der Tiermedizin und eben wieder Erkrankungen.

Wo wir hingegen dann wieder die logistsiche Regression brauchen, ist bei der Klassifikaton oder eben der Vorhersage von einem binären Ereignis. Dafür bietet sich dann die logistische Regression wieder an. Deshalb werden wir am Ende des Kapitels nochmal was zur Klassifikation machen, obwohl das hier eigentlich nur so halb reinpasst. Wenn du nicht Klassifizieren willst, dann lasse den letzten Abschnitt einfach weg.

## Annahmen an die Daten

[Unser gemessenes Outcome $y$ folgt einer Binomialverteilung. Damit finden wir im Outcome nur $0$ oder $1$ Werte.]{.aside}

Im folgenden Kapitel zu der multiplen logistischen linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form *ideal* sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.

-   Wenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das @sec-missing zu Imputation von fehlenden Werten.
-   Wenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das @sec-outlier zu Ausreißer in den Daten.
-   Wenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das @sec-variable-selection bei der Variablenselektion.

Daher sieht unser Modell wie folgt aus. Wir haben ein $y$ und $p$-mal $x$. Wobei $p$ für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser $y$ einer Binomailverteilung. Damit finden wir im Outcome nur $0$ oder $1$ Werte. Das ist hier sehr wichtig, denn wir wollen ja eine multiple logistische lineare Regression rechnen.

$$
y \sim x_1 + x_2 + ... + x_p 
$$

Wir können in dem Modell auch Faktoren $f$ haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in @sec-posthoc nochmal nachlesen, wir du dann das Modell weiterverwendest.

## Genutzte R Pakete für das Kapitel

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, conflicted, broom,
               parameters, performance, gtsummary,
               tidymodels)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

In diesem Kapitel nutzen wir die infizierten Ferkel als Beispieldatensatz. Wir haben in dem Datensatz über vierhundert Ferkel untersucht und festgehalten, ob die Ferkel infiziert sind ($1$, ja) oder nicht infiziert ($0$, nein). Wir haben daneben noch eine ganze Reihe von *Risiko*faktoren erhoben. Hier sieht man mal wieder wie wirr die Sprache der Statistik ist. Weil wir rausfinden wollen welche Variable das Risiko für die Infektion erhöht, nennen wir diese Variablen Risikofaktoren. Obwohl die Variablen gar keine kategorialen Spalten sin bzw. nicht alle. So ist das dann in der Statistik, ein verwirrender Begriff jagt den Nächsten.

```{r}
pig_tbl <- read_excel("data/infected_pigs.xlsx") 
```

Schauen wir uns nochmal einen Ausschnitt der Daten in der @tbl-log-pigs an.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-log-pigs
#| tbl-cap: Auszug aus dem Daten zu den kranken Ferkeln.
#| column: page

rbind(head(pig_tbl),
      rep("...", times = ncol(pig_tbl)),
      tail(pig_tbl)) %>% 
  kable(align = "c", "pipe")
```

In dem nächsten Abschnitt werden wir die Daten nutzen um rauszufinden welche Variablen einen Einfluss auf den Infektionsstatus der Ferkel hat.

## Theoretischer Hintergrund

Wir schaffen wir es, durch einen $0/1$ Outcome auf der y-Achse eine gerade Linie durch die Punkte zu zeichnen und die Koeffiziente dieser Gerade zu bestimmen? Immerhin gibt es ja gar keine Werte zwischen $0$ und $1$. In @fig-log-activity sehen wir beispielhaft den Zusammenhang zwischen dem Infektionsstatus und der Aktivität der Ferkel. Wir haben zwei horizontale Linien. Wie zeichen wir jetzt da eine Gerade durch?

```{r}
#| echo: true
#| message: false
#| label: fig-log-activity
#| fig-align: center
#| fig-height: 3
#| fig-width: 5
#| fig-cap: "Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und der Aktivität der Ferkel."


ggplot(pig_tbl, aes(x = activity, y = infected)) +
  theme_bw() +
  geom_point() 

```

Der Trick hierbei ist wieder die Transformation des *Zusammenhangs* von $y \sim x$ auf einen $\log$-scale. Das heißt wir Rechnen nicht mit den $0/1$ Werten sondern transformieren den gesamten Zusammenhang. Das ist wichtig, den es gibt einen Unterschied zwischen der Transformation von $y$ und der Transformation die hier gemeint ist. Wir halten fest, wir rechnen also nciht auf der ursprünglichen Skala der Daten sondern auf der $\log$-scale. Allgemeiner wird auch von der *link*-Funktion gesprochen, da wir ja verschiedene Möglichkeiten der Transformation des Zusammenhangs haben.

[Hier gibt es nur die Kurzfassung der *link*-Funktion. @dormann2013parametrische liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.]{.aside}

Wir gehen wir also vor. Zuerst Modellieren wir die Wahrscheinlichkeit für den Eintritt des Ereignisses. Wir machen also aus unseren binären $0/1$ Daten eine Wahrscheinlichkeit für den Eintritt von 1.

$$
Y \rightarrow Pr(Y = 1)
$$

Damit haben wir schon was erreicht den $Pr(Y = 1)$ liegt zwischen $0$ und $1$. Damit haben wir also schon Werte *dazwischen*. Wenn wir aber normalverteilte Residuen haben wollen, dann müssen unsere Werte von $-\infty$ bis $+\infty$ laufen können. Daher rechnen wir im Weiteren die Chance.

$$
\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}
$$ Die Chance (eng. *Odds*) für das Eintreten von $Y=1$ ist eben die Wahrscheinlichkeit *für* das Eintreten geteilt durch die Gegenwahrscheinlichkeit. Das ist schon besser, denn damit liegen unsere transformierten Werte für den Zusammenhang schon zwischen $0$ und $+\infty$. Wenn wir jetzt noch den $\log$ von den Chancen rechnen, dann haben wir schon fast alles was wir brauchen.

$$
\log\left(\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\right)
$$

Der Logarithmus der Chance liegt dann zwischen $-\infty$ und $+\infty$. Deshalb spricht man auch von den $\log$-Odds einer logistischen Regression. Auch sieht man hier woher das *logistisch* kommt. Wir beschreiben im Namen auch gleich die Transformation mit. Am ende kommen wir somit dann auf folgendes Modell.

$$
\log\left(\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\right) = \beta_0 + \beta_1 x_1 + ...  + \beta_p x_p + \epsilon
$$ Vielleicht ist dir der Begriff Wahrscheinlichkeit und der Unterschied zur Chance nicht mehr so präsent. Deshalb hier nochmal als Wiederholung oder Auffrischung.

-   Eine *Wahrscheinlichkeit* beschreibt dem Anteil an Allen. Zum Beispiel den Anteil Gewinner an allen Teilnehmern. Den Anteil Personen mit Therapieerfolg an allen Studienteilnehmern.
-   Eine *Chance* oder (eng. *Odds*) beschreibt ein Verhältnis. Somit das Verhältnis Gewinner zu Nichtgewinner. Oder das Verhältnis Personen mit Therapieerfolg zu Personen ohne Therapieerfolg

Nochmal an einem Zahlenbeispiel. Wenn wir ein Glücksspiel haben, in dem es 2 Kombinationen gibt die gewinnen und drei 3 Kombinationen die verlieren, dann haben wir eine Wahrscheinlichkeit zu gewinnen von $2 / 5 = 0.40 = 40\%$. Wenn wir die Chance zu gewinnen ausrechnen erhalten wir $2:3 = 0.67 = 67\%$. Wir sehen es gibt einen deutlichen Unterschied zwischen Chance und Wahrscheinlichkeit. Wenn wir große Fallzahl haben bzw. kleine Wahrscheinlichkeiten, dann ist der Unterschied nicht mehr so drastisch. Aber von einer *Gleichkeit* von Wahrscheinlichkeit und Chance zu sprechen kann nicht ausgegangen werden.

Was ist nun das Problem? Wir erhalten aus einer logistischen Regression $\log$-Odds wieder. Der Effektchätzer ist also eine Chance. Wir werden aber das Ergebnis wie eine Wahrscheinlichkeit interpretieren. Diese Diskrepanz ist wenigen bekannt und ein Grund, warum wir in der Medizin immer uns daran erinnern müssen, was wir eigentlich mit der logistischen Regression *aussagen können*.

## Modellierung

Die Modellerierung der logistischen Regression ist sehr einfach. Wir nutzen wieder die Formelschreibweise im `glm()` um unsere Variablen zu definieren. Wenn unser Outcome nicht binär ist, dann jammert R und gibt uns einen Fehler aus. Ich kann hier nur dringlichst raten, das Outcome in $0/1$ zu kodieren mit dem Schlechten als $1$.

Das `glm()` muss dann noch wissen, dass es eine logistische Regression rechnen soll. Das machen wir in dem wir als Verteilungsfamilie die Binomialverteilung auswählen. Wir geben also an `family = binomial` und schon können wir das volle Modell fitten.

```{r}
#| message: false
#| warning: false
fit_1 <- glm(infected ~ age + sex + location + activity + crp + frailty + bloodpressure + weight + creatinin, 
             data = pig_tbl, family = binomial)

```

Wir immer schauen wir uns nicht die rohe Ausgabe an, sondern lassen uns die Ausgabe mit der Funktion `model_parameters()` aus dem R Paket `parameters` wiedergeben. Wir müssen noch die Option `exponentiate = TRUE` wählen, damit unsere Koeffizienten nicht als $\log$-Odds sondern als Odds wiedergeben werden. Korrekterweise erhalten wir die Odds ratio wieder was wir auch als $OR$ angegeben.

```{r}
#| message: false
#| warning: false
model_parameters(fit_1, exponentiate = TRUE)
```

Wie interpretieren wir nun das $OR$ einer logistischen Regression? Wenn wir darauf gechtet haben, dass wir mit $1$ das Schlechte meinen, dann können wir wir folgt mit dem $OR$ sprechen. Wenn wir ein $OR > 1$ haben, dann haben wir ein Risiko vorliegen. Die Variable mit einem $OR$ größer als $1$ wird die Chance auf den Eintritt des schlechten Ereignisses erhöhen. Wenn wir ein $OR < 1$ haben, dann sprechen wir von einem protektiven Faktor. Die Variable mit einem $OR$ kleiner $1$ wird vor dem Eintreten des schlechten Ereignisses schützen. Schauen wir uns den Zusammenhang mal im Detail für die Ferkeldaten an.

-   `(intercept)` beschreibt den Intercept der logistischen Regression. Wenn wir mehr als eine simple Regression vorliegen haben, wie in diesem Fall, dann ist der Intercept schwer zu interpretieren. Wir konzentrieren uns auf die Effekte der anderen Variablen.
-   `sex` beschreibt den Effekt der männlichen Ferkel zu den weiblichen Ferkeln. Daher haben männliche Ferkel eine $2.75$ höhere Chance infiziert zu werden als weibliche Ferkel.
-   `location [northeast]`, `location [northwest]` und `location [west]` beschreibt den Unterschied zur `location [north]`. Alle Orte haben eine geringere Chance für eine Infektion zum Vergleich der Bauernhöfe im Norden. Zwar ist keiner der Effekte signifikant, aber ein interessantes Ergebnis ist es allemal.
-   `activity` beschreibt den Effekt der Aktivität der Ferkel. Wenn sich die Ferkel mehr bewegen, dann ist die Chance für eine Infektion gemindert.
-   `crp` beschreibt den Effekt des CRP-Wertes auf den Infektionsgrad. Pro Einheit CRP steigt die Chance einer Infektion um $2.97$ an. Das ist schon ein beachtlicher Wert.

```{r}
#| message: false
#| warning: false
r2(fit_1)
```

@tbl-tbl-summary-logreg

```{r}
#| message: false
#| warning: false
#| label: tbl-tbl-summary-logreg
#| tbl-cap: "."

pig_tbl %>% tbl_summary(by = infected) %>% add_p() %>% as_flex_table()
```

@tbl-tbl-regression-logreg

```{r}
#| message: false
#| warning: false
#| label: tbl-tbl-regression-logreg
#| tbl-cap: "."

fit_1 %>% tbl_regression(exponentiate = TRUE) %>% as_flex_table()
```

Wir können uns einmal die Ergebnisse des Modellfits die logistischen Gerade für eine simple lineare Regression mit dem Modell $infected \sim crp$ anschauen. Wie immer können wir uns den Zusammenhang nur in einem simplen Modell anschauen. Im Fall einer multiplen linearen Regresion können wir nicht so viele Dimensionen in einer Grpahik darstellen. Wir fitten also das Modell `fit_2` wie im folgenden dargestellt.

```{r}
fit_2 <- glm(infected ~ crp, data = pig_tbl, family = binomial)
```

Nun können wir uns mit der Funktion `predict()` die Wert auf der Geraden wiedergeben lassen. Wenn wir `predict()` nur so aufrufen, dann erhalten wir die Werte für $y$ auf der transformierten $link$-Scale wieder. Das hilft uns aber nicht weiter, wir haben ja nur 0 und 1 Werte für $y$ vorliegen.

```{r}
predict(fit_2, type = "link") %>% 
  extract(1:10) %>% 
  round(2)
```

Da wir die Werte für die Wahrscheinlichkeit das ein Ferkel infiziert ist, also die Wahrscheinlichkeit $Pr(infected = 1)$, müssen wir noch die Option `type = reponse` wählen. So erhalten wir die Wahrscheinlichkeiten wiedergegeben.

```{r}
predict(fit_2, type = "response") %>% 
  extract(1:10) %>% 
  round(2)

```

Abschließend können wir uns die Gerade auch in der @fig-log-pred visualisieren lassen. Auf der x-Achse sehen wir die `crp`-Werte und auf der y-Achse den Infektionsstatus. Auf der $reponse$-scale sehen wir eine S-Kurve. Auf der $link$-scale würden wir eine Gerade sehen.

```{r}
#| echo: true
#| message: false
#| label: fig-log-pred
#| fig-align: center
#| fig-height: 3
#| fig-width: 5
#| fig-cap: "Visualisierung der logistischen Gerade in einer simplen logistischen Regression mit der Variable `crp`."

ggplot(pig_tbl, aes(x = crp, y = infected)) +
  theme_bw() +
  geom_point() +
  geom_line(aes(y = predict(fit_2, type = "response")), color = "red") 

```

## Prädiktion

Da wir später die logistische Regression auch als Vergleich in der Klassifikation nutzen werden, hier auch einmal wie wir eine Klassifikation, also eine Vorhersage, für das Outcome `infected` mit einer logistischen Regression rechnen.

::: column-margin
Mehr zu Rezepten (eng. *recipes*) kannst du im @sec-class-basic zu den Grundlagen des maschinellen Lernens erfahren.
:::

```{r}
pig_tbl <- pig_tbl %>% 
  mutate(infected = factor(infected, levels = c(0, 1)))
```

```{r}
pig_rec <- recipe(infected ~ ., data = pig_tbl) %>% 
  step_dummy(all_nominal_predictors())
```

```{r}
logreg_mod <- logistic_reg() %>% 
  set_engine("glm")
```

```{r}
pig_wflow <- workflow() %>% 
  add_model(logreg_mod) %>% 
  add_recipe(pig_rec)

pig_wflow
```

```{r}
pig_fit <- pig_wflow %>% 
  fit(data = pig_tbl)
```

```{r}
predict(pig_fit, new_data = pig_tbl)
```

```{r}
pig_aug <- augment(pig_fit, new_data = pig_tbl) %>% 
  select(infected, matches("^\\."))

pig_aug
```

```{r}
pig_aug %>% 
  roc_curve(truth = infected, .pred_1, event_level = "second") %>% 
  autoplot()
```

```{r}
pig_aug %>% 
  roc_auc(truth = infected, .pred_1, event_level = "second")
```
