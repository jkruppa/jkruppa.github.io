```{r echo = FALSE}
#| message: false
#| warning: false
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc, quantreg,
               multcomp, emmeans, ggpubr, multcompView, nlme, tinytable,
               see, patchwork, ggrepel, conflicted)
conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
```

```{r}
#| echo: false
#| message: false
#| warning: false
source("images/R/eda-transform.R")
```

# Transformieren von Daten {#sec-eda-transform}

*Letzte Änderung am `r format(fs::file_info("eda-transform.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"It's morphin' time! I never get tired of that! Now, go, go Power Rangers!" --- Power Rangers morphing phrase*

![](images/caution.png){fig-align="center" width="100%"}

::: {.callout-caution appearance="simple"}
## Stand des Kapitels: Baustelle (seit 06.2025)

Dieses Kapitel wird überarbeitet, da ich nochmal an den Kern der Transformation ran möchte. Daher kann es sein, dass es in den nächsten Wochen hier zu Problemen und nicht funktionierenden Code kommt. Das repariere ich dann aber meistens in den folgenden Tagen. Ziel ist es bis zum Start des WiSe 2025/26 das Kapitel neu aufgestellt zu haben.
:::

In diesem Kapitel wollen wir uns nochmal mit der Transformation von Daten beschäftigen. Hier geht es aber jetzt nicht nur um vollständige Transformationen ganzer Exceldateien, sondern eher um die Transformation einzelner Messwerte. Wir haben also etwas gemessen, was uns auf der Skala der Einheiten wie wir es gemessen haben nicht gefällt. Deshalb nutzen wir eine mathematische Funktione, die uns aus unseren originalen Messwerten neue einheitslose Werte erschafft. Hier ist mal wieder wichtig, dass wir am Ende einheitslose Messwerte haben, die sich dann der biologischen Interpretation entziehen. Aber wie immer gilt, je nach Fragestellung ist die Transformation von Daten wichtig bis notwendig. Deshalb einmal hier alle gesammelt zu der Transformation.

## Allgemeiner Hintergrund

Wenn wir von der Transformation sprechen, dann sprechen wir von einer überschaubaren Anzahl von sinnhaften mathematischen Operationen, die wir anwenden können. Es gibt natürlich unendliche viele mathematische Funktion und daher auch eine unendliche Anzahl an möglichen Transformationen. Dennoch haben sich in den Jahrzehnten der Anwendung mehrere Algorithmen durchgestetzt. Diese Hauptalgorithmen wollen wir hier einmal anaschauen. Teilweise sind es eben dann sehr simple Funktionen wie der Kehrwert oder aber die quadratische Wurzel. Alles im allen also kein Hexenwerk. Beginnen wir also einmal mit einigen Beispielen aus wissenschaftlichen Kontexten um uns die Transformation etwas klarer werden zu lassen.

In der folgenden Abbildung aus @yang2014duration siehst du einmal den Zusammenhang des Körpergewichts und der Dauer des Urinierens. Wir haben hier eine Darstellung auf der logarithmischen Skala der Zeit und des Gewichts. Daher können wir auch deas Gewicht der Maus und das Gewicht des Elefanten einigermaßen sinnvoll auf der Abbildung darstellen ohne das wir die Abbuildung sehr stark stauchen würden. Wir sehen das es keinen Unterschied gibt, die Gerade läuft parallel zu der Masse. Egal wie schwer ein Säugetier ist, es pinkelt immer die gleiche Zeit. Daher ist eine Anwendung der Transformation zusammenhänge zu zeigen, die auf sehr großen Skalen von Einheiten laufen. Die Maus ist leicht in Gramm und der Elefant ist schwer in Tonnen. Schnell haben wir hier Unetrschiede in der Einheit von $10^6$ Gramm.

![*Zusammenhang zwischen dem Körpergewicht und der Dauer des Urinierens von verschiedneen Säugetieren auf der logarithmischen Skala. Einige der Punkte sind als Pictogramme der entprechenden Tiere dargestellt.* Quelle: @yang2014duration](images/preface_transformation.jpeg){#fig-utest-intro fig-align="center" width="100%"}

Nun stellt sich schnell die Frage, was ist eigentlich die beste Transformation für unsere Messwerte? Häufig hängt die Entscheidung auch von dem Modell ab was wir rechnen wollen. Viele Modelle verlangen einen linearen Zusammenhang. In der folgenden Abbildung siehst du einmal das exponentielle Wachstum der Bevölkerung in den Vereinigten Staaten. Wie du sehen kannst, haben wir einen sehr schnellen Anstieg. Um diesen Zusammenhang gut mit einem linearen Modell modellieren zu können, müssen wir einen einigermaßen linaren Zusammenhang erzeugen. Hier können wir die Anzahl der Menschen als Messwert $y$ einmal mit dem Logarithmus transformieren. Wir sehen dann, dass wir hier einen fast linearen Zusammenhang vorliegen haben. Die Log-Transformation hat hier also gut funktioniert.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-ggplot-utest-intro-sim-02
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Wachstum der Bevölkerung in den Vereinigten Staaten. Viele Modelle verlangen einen linearen Zusammenhang. **(A)** Exponentzielles Wachstum auf der originalen Skala. **(B)** Log-Transformation der Population um einen linearen Zusammenhang zu erreichen. *[Zum Vergrößern anklicken]*"

p1_intro_pop + p2_intro_pop +
  plot_layout(ncol = 2) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

Nun könnte man annehmen, dass es recht mühselig ist sich verschiedene Transformationen auszudenken und dann jedes Mal zu gucken, ob wir einen linearen Zusammenhang vorliegen haben. Daher gab es den Vorschlag von Tukey eine [Ladder of Powers](https://onlinestatbook.com/2/transformations/tukey.html) zu nutzen. Wir modellieren folgende simple lineare Regression. Dabei wählen wir verschiedene Potenzen $\lambda$ und schauen jedes Mal, ob wir einen linearen Zusammenhang vorliegen haben.

$$
y^{\lambda} = \beta_0 + \beta_1 \cdot x_1
$$

mit

-   $y$, dem Messwert wie Frischmasse oder Population.
-   $\lambda$, einer beliebigen Potenz.
-   $\beta_0, \beta_1$, den Koeffizienten der linearen Regression.
-   $x_1$, der Einflussvariabel, wie Wassergabe oder das Jahr.

Jetzt fragst du dich sicherlich, welche $\lambda$-Werte soll ich nehmen? Folgende Tabelle zeigt dir einmal die Zusammenhänge zwischen ausgewählten Lambda-Werten $\lambda$ und der entsprechenden Transformation in dem Messwert $y$. Du kannst auch jeden belieben Kommawert für $\lambda$ einsetzen, aber die ganzahligen Werte können wir dann einfach darstellen. So ist ein $\lambda$-Wert von 1/2 nichts anderes als die Transformation mit der Quadratwurzel.

| $\boldsymbol{\lambda}$ | $\boldsymbol{y^\lambda}$ |
|:----------------------:|:------------------------:|
|          $-2$          |     $\cfrac{1}{y^2}$     |
|          $-1$          |      $\cfrac{1}{y}$      |
|         $-1/2$         |  $\cfrac{1}{\sqrt{y}}$   |
|          $0$           |        $\log{y}$         |
|         $1/2$          |        $\sqrt{y}$        |
|          $1$           |           $y$            |
|          $2$           |          $y^2$           |

: Tukey's Ladder of Powers oder wie kann können Messwerte $y$ transformiert werden? Das $\lambda$ gibt die Potenz von dem Messwert $y$ an. {#tbl-tukey-ladder}

In der folgenden Abbildung habe ich dir dann mal verschiedene $\lambda$-Werte visualisiert. Das Ziel ist immer einen linearen Zusammenhang zu finden. Wie können wir die Potenz von $y$ wählen, so dass wir dann einen linearen Zusammenhang finden? In der Abbildung hier haben wir dann einen linearen Zusammenhang gegeben und ich zeige einmal wie die verschiedene $\lambda$-Werte den Zusammenhang ändern. Wir müssen später wirklich nicht alle händisch durchprobieren, wir haben dann dafür automatisierte Transformationen in R vorliegen. Diese Algorithmen finden dann das beste $\lambda$ für uns.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-ggplot-utest-intro-lambda
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Verschiedene $\\lambda$ Transformationen für einen linearen Zusammenhang zwischen $x$ und $y$. Der $\\lambda$-Wert ist als $y^{\\lambda}$ zu verstehen. Somit beschreibt der $\\lambda$-Wert die Potenz vom Messwert. Gesucht ist der $\\lambda$-Wert bei dem ein linearer Zusammenhang zwischen $x$ und $y$ vorliegt. *[Zum Vergrößern anklicken]*"


map(c(-2, -1, -0.5, 0, 0.5, 1, 2,3), \(x){
  if(x < 0)
    return(-(seq(1, 10, by = 0.5))^x)
  if(x == 0)
    return(log(seq(1, 10, by = 0.5)))
  if(x > 0)
    return((seq(1, 10, by = 0.5))^x)
}) |> 
  set_names(c(-2, -1, -0.5, 0, 0.5, 1, 2,3)) |> 
  bind_cols() |> 
  mutate(x = seq(1, 10, by = 0.5)) |> 
  pivot_longer("-2":"3",
               names_to = "lambda",
               values_to = "values") |> 
  mutate(lambda_fct = as_factor(lambda)) |> 
  ggplot(aes(x, values, group = lambda_fct)) +
  theme_minimal() +
  geom_point() +
  geom_smooth(method = "loess", color = "#56B4E9", se = FALSE) +
  facet_wrap(~lambda_fct + lambda, scales = "free", nrow = 2,
             labeller = label_bquote(lambda~"="~.(lambda))) +
  labs(x = "", y = "") +
  theme(axis.text.y = element_blank(),
        axis.text.x = element_blank(),       
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        strip.text = element_text(size=14)) 

```

Warum müssen wir Daten transformieren? Meistens hat dies drei Hauptgründe. Ich habe hier nochmal die Gründe zusammengefasst und sortiert. Sicherlich gibt es neben diesen drei Gründen auch noch weitere Gründe, aber ich denke, dass es die häufigsten sind. In einem faktoriellen Design wollen wir häufig für die Gruppenvergleiche eine ANOVA rechnen, so dass es sicherlich eine der häufigsten Ziele einer Transformation ist. Gehen wir mal die Gründe durch.

#### Wir brauchen eine Normalverteilung {.unnumbered .unlisted}

Warum brauchen wir eine Normalverteilung in unseren Messwerten? Wir wollen eine ANOVA oder eine Gaussian lineare Regression rechen und benötigen ein normalverteiltes Outcome $y$. Wir wollen also meist unsere Daten $log$-Transformieren um aus einem nicht-normalverteilten Messwert $y$ ein $log$-normalverteilten Messwert $y$ zu erschaffen. Dann können wir relativ entspannt einen Mittelwertsvergleich über den ANOVA Pfad rechnen.

#### Wir wollen eine Vorhersage machen {.unnumbered .unlisted}

Immer häufiger wollen wir auch Modelle rechnen, wo wir verschiedene Einflussvariablen und Messwerte vorliegen haben. Wir wollen jetzt einen einen Algorithmus zur Prädiktion (deu. *Vorhersage*) nutzen und haben sehr viele Einflussvariablen $x$ in sehr unterschiedlichen Einheiten. Wir wollen dann unsere Einflussvariablen Standardisieren oder Normalisieren. Wir brauchen normalisierte Daten später beim Klassifizieren im Rahmen von maschinellen Lernverfahren. Bitte beachte auch, dass die Transformationen hier in diesem Kapitel eher für kleine Datensätze geeignet sind. Im [Kapitel zur Klassifikation](#sec-pre-processing) gehe ich nochmal auf die Automatisierung über das R Paket `{recipes}` ein. Wenn du also einen großen Datensatz hast, den du vielleicht oft bearbeiten musst, dann mag dir dort mehr geholfen sein.

#### Wir wollen Gruppen / Cluster erkennen {.unnumbered .unlisted}

Eine andere Anwendung ist auch das Gruppieren von Dtaen. Wir wollen eine komplexere Analyse wie die Hauptkomponentenanalyse oder Clusteranalyse rechnen und brauchen Variablen, die alle eine ähnliche Spannweite haben. Hier haben wir dann auch das Problem der Einheiten. Wenn wir zu unterschiedliche Einheiten vorliegen haben, funktionieren die Modelle nicht mehr. Wir wollen dann unsere Einflussvariablen Standardisieren oder Normalisieren. Auch hier können viele Funktionen in R das schon automatisch, deshalb einmal schauen, ob du überhaupt Transformieren musst. Es spricht aber nichts dagegen vorher zu Transformieren, da du dich dann schonmal mit den Ergebnissen der Transformation auseinander gesetzt hast. Dann hast du auch nicht eine algorithmische Blackbox vorliegen.

Wir wollen uns nun die Verfahren zur Transformation von Daten in den folgenden Abschnitten einmal näher anschauen. Zuerst gehe auf die Standardverfahren mit `mutate()` und den entsprechenden Funktionen ein. Danach zeige ich dir dann noch die automatisierten Varianten. Bei den automatisierten Varianten habe ich schon angefangen einmal aufzuräumen und die veralteten Funktionen in den Friedhof zur Referenzierung geschoben.

::: callout-tip
## Weitere Tutorien für das Transformieren von Daten.

Wir oben schon erwähnt, kann dieses Kapitel nicht die umfangreiche Literatur der Transformation von Daten abarbeiten. Daher präsentiere ich hier eine Liste von Literatur und Links, die mich für dieses Kapitel hier inspiriert haben. Nicht alles habe ich genutzt, aber vielleicht ist für dich was dabei.

-   Ich verweise hier auch nochmal auf das tolle Tutorium von Matus Seci auf dem [Coding Club - Transforming and scaling data: Understand the fundamental concepts of manipulating data distributions for modelling and visualization](https://ourcodingclub.github.io/tutorials/data-scaling/). Du findest auch dort mehr Informationen zu der Rücktransformation und der Anwendung von Transformationen auf einen komplexeren Datensatz.
-   Das Kapitel [Transformations](https://onlinestatbook.com/2/transformations/contents.html) liefert auch nochmal einen schönen Überblick über verschiedene Transformtionen.
-   Das Kapitel [Transforming Data](https://rcompanion.org/handbook/I_12.html) geht nochmal auf speziellere Funktionen ein und hilft hier nochmal bei der automatisierten Verwendung.
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r}
#| message: false
#| warning: false
#| echo: true

pacman::p_load(tidyverse, magrittr, scales, see, MASS, bestNormalize,
               rcompanion, LambertW, trafo, conflicted)
conflicts_prefer(MASS::boxcox)
conflicts_prefer(dplyr::select)
conflicts_prefer(dplyr::filter)
conflicts_prefer(parameters::skewness)
conflicts_prefer(parameters::kurtosis)
conflicts_prefer(bestNormalize::boxcox)
conflicts_prefer(magrittr::set_names)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", 
                "#009E73", "#F0E442", "#F5C710", 
                "#0072B2", "#D55E00", "#CC79A7")
```

An der Seite des Kapitels findest du den Link *Quellcode anzeigen*, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.

## Daten

Wie immer brauchen wir Daten, wenn wir uns einmal mit den Transformationen von Daten beschäfltigen wollen. Ich habe hier wieder zwei Datensätze mitgebracht. Der erste Datensatz beschreibt die Sprungweite von Hunde-, Katzen und Fuchsflöhen sowie deren jeweiligen Schlupfzeiten. Der zweite Datensatz kommt von der Datenbank [AnAge Database of Animal Ageing and Longevity](https://genomics.senescence.info/species/index.html) und beschreibt das Alter von verschiedenen Tierarten und weiteren Eigenschaften, die mit dem Alter verbunden sein könnten.

#### Schlupfzeiten von Flöhen {.unnumbered .unlisted}

In unserem ersten Datensatz betrachten wir die Schlupfzeiten sowie wie die Sprungweiten über alle Tiere hinweg. Wir werden dann später nochmal die Messwerte für die Hunde-, Katzen- und Fuchsflöhe aufteilen. Dann schauen wir, wie sich die jeweiligen Messwerte verteilen oder besser welcher Verteilung die Messwerte folgen.

```{r}
#| message: false

fac1_tbl <- read_excel("data/flea_dog_cat_length_weight.xlsx") |>
  select(animal, jump_length, weight, hatch_time) |> 
  filter(hatch_time <= 1500)
```

In der folgenden Tabelle siehst du dann nochmal die Werte für die Sprungweite und die Schlupfweiten. Wir nehmen mal an, dass die Sprungweiten eher normalverteilt sind und die Schlupfzeiten eher nicht.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-trans-1
#| tbl-cap: "Selektierter Datensatz mit einer normalverteilten Variable `jump_length` und der nicht-normalverteilten Variable `hatch_time`. Wir betrachten die ersten sieben Zeilen des Datensatzes."

fac1_raw_tbl <- read_excel("data/flea_dog_cat_length_weight.xlsx") |>
  select(animal, jump_length, hatch_time) 

rbind(head(fac1_raw_tbl, n = 3),
      rep("...", times = ncol(fac1_raw_tbl)),
      tail(fac1_raw_tbl, n = 3)) |> 
  tt(width = 3/3, align = "c", theme = "striped")
```

Dann wollen wir den Zusammenhang auch nochmal visualiseren. Wir sehen in der Abbildung sehr gut, dass die Sprungweiten eher einer Normalverteilung über alle Floharten folgt. Auch die Schlupfzeiten sind eher nicht normalverteilt. Wir sehen eine sehr rechtsschiefe Verteilung. Wir haben einige Flöhe mit sehr langen Schlupfzeiten. Daher könnten wir überlegen, die Schlupfzeiten zu transformieren und in eine Normalverteilung zu überführen. Dabei wirden wir natürlich die Einheit in Stunden verlieren.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-data-intro
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Densityplot zur Abschätzung der Verteilung unserer Messwerte in dem Flohdatensatz. **(A)** Sprungweiten in [cm]. **(B)** Schlupfzeiten in [h]. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(fac1_tbl, aes(jump_length)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Sprungweiten in [cm]", y = "") +
  xlim(8, 33)

p2 <- ggplot(fac1_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, 1500)

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

#### Das Alter von Tieren {.unnumbered .unlisted}

In dem zweiten Datensatz schauen wir uns das Alter von verschiedenen Tierarten an. Wir wollen wissen, ob es einen Zusammenhang zwischen dem Alter der Tiere und dem Gewicht der Tiere gibt. Leben also Elefanten länger als Mäuse, weil die Elefanten einfach schwerer sind? Es ist ein schöner Datensatz um nochmal die Skalierung von Daten zu zeigen oder aber die Log-Transformation aus der Einleitung zu zeigen. Wir wollen dann doch das Gewicht einmal in Kilogramm haben und nicht in Gramm. Dafür passe ich die Daten einmal an.

```{r}
#| message: false
#| warning: false
anage_tbl <- read_delim("data/anage_data.txt", delim = "\t") |> 
  select(name = "Common name", weight = "Adult weight (g)", 
         age = "Maximum longevity (yrs)") |> 
  mutate(weight = weight/1000) |> 
  na.omit() 
```

Dann schauen wir uns nochmal die Daten im Original an. Wie du sehen kannst, haben wir verschiedenste Tierarten und die entsprechende maximale Lebensspanne. Das Problem bei solchen Daten wird dann meistens nicht sofort klar, daher schauen wir uns gleich einmal die Visualisierung an.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-trans-weight-age
#| tbl-cap: "foo"

anage_raw_tbl <- read_delim("data/anage_data.txt", delim = "\t") |> 
  select("Common name", "Adult weight (g)", 
         "Maximum longevity (yrs)") |> 
  set_names(c("Common name", "Adult weight (kg)", 
              "Maximum longevity (yrs)")) |> 
  na.omit()

rbind(head(anage_raw_tbl, n = 3),
      rep("...", times = ncol(anage_raw_tbl)),
      tail(anage_raw_tbl, n = 3)) |> 
  tt(width = 3/3, align = "c", theme = "striped")
```

In der folgenden Abbildung siehst du dann einmal den Zusammenhang zwischen dem Körpergewicht und dem Alter in Jahren der ganzen Tierarten in dem Datensatz. Ich habe dir einmal die Arten mit einem hohen Gewicht und einem langem Lebensalter hervorgehoben. Einen Großteil der Arten siehst du aber gar nicht, da die Wale die Skala so weit nach rechts schieben, dass alles links zusammengepresst wird. Hier bieten sich Transformationen und Skalierungen an um mehr aus den Daten visuelle herauszuholen. Wir sehen hier gar nicht den Zusammenhang oder einen irgendwie gearteten Trend in den Daten.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-ggplot-weight-age
#| fig-align: center
#| fig-height: 5
#| fig-width: 8
#| fig-cap: "Zusammenhang zwischen dem Gewicht in [kg] und dem Alter in [yrs] von Tierarten. Durch die große Bandbreite der Körpergewichte sind der überwiegende Teil der Tierarten nicht zu erkennen. Ein Trend lässt sich ebenfalls nicht ablesen. *[Zum Vergrößern anklicken]*"

anage_tbl |> 
  ggplot(aes(weight, age)) +
  theme_minimal() +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = 1e4, color = "gray") +
  geom_hline(yintercept = 150, color = "gray") +
  geom_text_repel(data = filter(anage_tbl, weight > 1e4),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  geom_text_repel(data = filter(anage_tbl, age > 150 & weight < 1e4),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  labs(x = "Gewicht in [kg]", y = "Alter in [yrs]") +
  scale_x_continuous(labels = scales::comma,
                     breaks = c(1e4, 5e4, 1e5)) +
  theme(axis.title.y = element_text(size = 16, face = 2),
        axis.text.y = element_text(size = 12),
        axis.title.x = element_text(size = 16, face = 2),
        axis.text.x = element_text(size = 12),  
        plot.title = element_text(size = 17),
        plot.subtitle = element_text(size = 12, face = "italic"),
        plot.caption = element_text(size = 12),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "top")
```

## Transformationen

Beginnen wir also einmal mit der Transformation. Hier wollen wir durch eine mathematische Funktion die Messwerte so verändern, dass wir am Ende eine Normalverteilung oder eine approximative Normalverteilung erhalten. Hierbei ist nochmal wichtig zu wissen, dass wir eigentlich nur das Ziel einer Normalverteilung durch die Transformation kennen. Wir nutzen eigentlich keine andere Form der Transformation um nicht Linearität durch einen normalverteilten Messpunkt annährend zu erreichen. Daher schauen wir uns einmal in der folgenden Abbildung verschiedene Arten einer Verteilung an. Neben wilden Wabbeln gibt es eher linksschiefe Verteilungen und rechtsschiefe Verteilungen. Die Schiefe definiert sich durch einen längeren Verteilungsschwanz. Wir können auch durch die Ordnung des Modus, Median und dem Mittelwert abschätzen, ob wir eine schiefe Verteilung vorliegen haben oder nicht.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pretest-rank
#| fig-align: center
#| fig-height: 4
#| fig-width: 8
#| fig-cap: "Zusammenhang vom Mittelwert, Median und dem Modus zur Feststellung einer Normalverteilung. Der Modus ist hierbei der häufigste Wert. **(A)** Linksschiefe Verteilung. Der Modus ist größer als der Median ist größer als der Mittelwert. **(B)** Symmetrische Normalverteilung. Der Mittelwert und Median sowie Modus sind gleich. **(C)** Rechtsschiefe Verteilung der Mittelwert ist größer als der Median ist größer als der Modus. *[Zum Vergrößern anklicken]*"

p2kurt + p1kurt + p3kurt +
  plot_layout(ncol = 3) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

Wir haben jetzt also erkannt, dass wir eine schiefe Verteilung in unseren Daten vorliegen haben. Wir nutzen dafür immer eine visuelle Überprüfung. Zwar gibt es statistsiche Tests auf die Abweichung von der Normalverteilung, aber diese Tests geben nicht wieder wie die Schiefe der Verteilung ist. Dann müssen wir auch bei einem signifikanten Vortest und der Ablehnung der Normalverteilung nochmal schauen in welche Richtung die Schiefe geht. Da sind wir dann wieder bei der visuellen Betrachtung. Jetzt gibt es verschiedene gängige mathematische Funktionen, die dir erlauben aus einer schiefen Verteilung eine Normalverteilung zu bauen.

**Die Quadratwurzel für moderat schiefe Messwerte:**

-   `sqrt(y)` für positiv, schiefe Messwerte
-   `sqrt(max(y+1) - y)` für negative, schiefe Messwerte

**Der Logarithmus für starke schiefe Messwerte:**

-   `log10(y)` oder `log()` für positiv, schiefe Messwerte
-   `log10(max(y+1) - y)` oder `log(max(y+1) - y)` für negative, schiefe Messwerte

**Die Inverse für extrem, schiefe Messwerte:**

-   `1/y` für positiv, schiefe Messwerte
-   `1/(max(y+1) - y)` für negative, schiefe Messwerte

Neben diesen sehr häufig vorkommenden Transformationen können wir auch eine inverse Normaltransformation nutzen oder aber die Transformation mit Rängen. Die inverse Normaltransformation ist teilweise sehr effizient aber auch umstritten, weil sie im Prinzip alle Daten in eine Normalverteilung presst, egal wie gut die Messwerte passen würden. Die Rangtransformation führt dann zu den Tests der Parametrik, wie dem U-Test und dem Kruskal-Wallis-Test. Hierzu erfährst du dann mehr in dem Abschnit weiter unten.

::: callout-warning
## Achtung, bitte beachten!

Wir müssen immer nach einer Transformation schauen, ob unser Messwert jetzt mehr einer Normalverteilung folgt oder nicht. Es kann auch sein, dass du durch eine Transformation einen Messwert erschaffst, der weniger normalverteilt ist als ohne Transformation. Bitte also immer die Transformation anschauen.
:::

### Quadratwurzel

Die Quadratwurzel-Transformationen ist eine etwas seltenere Transformation. Damit meine ich, dass wir die Transformation gerne einmal ausprobieren, uns aber dann meistens doch für eine andere Transformation entscheiden. Da die Transformation so einfach ist, probiere ich die Quadratwurzel-Transformationen immer aus. Auch hier müssen wir dann schauen, ob wir eine linksschiefe oder rechtsschiefe Verteilung vorliegen haben und dann entsprechend die Transformation wie weiter oben beschreiben anpassen. Meist wird die Quadratwurzel-Transformationen als die schwächere $log$-Transformation bezeichnet. Wir sehen in der folgenden Abbildung den Grund dafür. Aber zuerst müssen wir aber über die Funktion `sqrt()` unsere Daten transformieren.

```{r}
sqrt_tbl <- fac1_tbl |> 
  mutate(sqrt_hatch_time = sqrt(hatch_time))
```

In der folgenden Abbildung sehen wir die nicht transformierte, rohe Daten sowie die transformierten Daten mit der Quadratwurzel. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam nach rechts aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Unser Ziel besser normalverteilte Daten vorliegen zu haben, haben wir aber mit der Quadratwurzel-Transformationen nicht erreicht. Die Daten sind immer noch rechtsschief. Alle drei Floharten verhalten sich dabei gleich.

```{r}
#| message: false
#| echo: false
#| label: fig-trans-sqrt
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** Quadratwurzel-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(sqrt_tbl, aes(hatch_time, fill = animal)) +
  theme_minimal() +
  geom_density(color = "black", alpha = 0.5) +
  scale_fill_okabeito() +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "", fill = "") +
  xlim(-100, NA) +
  theme(legend.position = "top")

p2 <- ggplot(sqrt_tbl, aes(sqrt_hatch_time, fill = animal)) +
  theme_minimal() +
  geom_density(color = "black", alpha = 0.5) +
  scale_fill_okabeito() +
  labs(x = "Zeit bis zum Schlüpfen in sqrt(h)", y = "", fill = "") +
  theme(legend.position = "top")

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

### Logarithmus

Wir nutzen die $log$-Transformation, wenn wir aus einem nicht-normalverteiltem Outcome $y$ ein approxomativ normalverteiltes Outcome $y$ machen wollen. Diese Transformation wird so häufig verwendet, dass wir dann sogar sagen, dass unere Messwerte jetzt lognormalverteilt sind. Auch hier müssen wir dann schauen, ob wir eine linksschiefe oder rechtsschiefe Verteilung vorliegen haben und dann entsprechend die Transformation wie weiter oben beschreiben anpassen. Dabei ist wichtig, dass wir natürlich auch die Einheit mit $log$-transformieren. Im Folgenden sehen wir die $log$-Transformation der Variable `hatch_time` mit der Funktion `log()`. Wir erschaffen eine neue Spalte im `tibble` damit wir die beiden Variable vor und nach der $log$-Transformation miteinander vergleichen können.

```{r}
log_tbl <- fac1_tbl |> 
  mutate(log_hatch_time = log(hatch_time))
```

Wir können dann über ein Histogramm die beiden Verteilungen anschauen. In der folgenden Abbildung sehen wir die nicht transformierte, rohe Daten sowie die Log-transformierten Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Die Log-Transformation hat einigermaßen funktioniert. Wir sehen in diesem Fall approximativ normalverteilte Daten. Wir haben also ein lognormalverteiltes Outcome $y$ mit dem wir jetzt weiterechnen können. Die drei Floharten verhalten sich dabei gleich.

```{r}
#| message: false
#| echo: false
#| label: fig-log
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** log-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(log_tbl, aes(hatch_time, fill = animal)) +
  theme_minimal() +
  geom_density(color = "black", alpha = 0.5) +
  scale_fill_okabeito() +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "", fill = "") +
  xlim(-100, NA) +
  theme(legend.position = "top")

p2 <- ggplot(log_tbl, aes(log_hatch_time, fill = animal)) +
  theme_minimal() +
  geom_density(color = "black", alpha = 0.5) +
  scale_fill_okabeito() +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "", fill = "") +
  theme(legend.position = "top") 

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

In der folgenden Abbildung siehst du nochmal die Anwendung der $\log$-Skala in `{ggplot}` auf die Daten und Messwerte. Aber Achtung, die ursprünglichen Daten sind *nicht* transformiert. Du schaust dir hier nur an, wie die Daten transformiert aussehen würden. Es gibt auch hierzu ein kleines Tutorium zu [ggplot log scale transformation](https://www.datanovia.com/en/blog/ggplot-log-scale-transformation/). Da kannst du dann auch einmal nachschauen, wie die $\log_2$-Skala in `{ggplot}` funktioniert. Es ist immer so die Frage, ob du die Messwerte dann nicht doch lieber transformierst und darstellst. Hier kommt es dann auf den Geschmack drauf an.

```{r}
#| message: false
#| echo: true
#| label: fig-log-scale-ggplot
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten Daten einmal auf der $log_{10}$-Skala in `ggplot` dargestellt. Achtung, hier ist nur die Darstellung transformiert. Die ursprünglichen Daten bleiben von der Transformation *in* `{ggplot}` unberührt."

ggplot(log_tbl, aes(hatch_time, fill = animal)) +
  geom_histogram(color = "black", position="dodge") +
  theme_minimal() +
  labs(x = expression("Zeit bis zum Schlüpfen in" ~ log[10](h)), y = "Anzahl",
       fill = "Flohart") +
  scale_fill_okabeito() +
  theme(legend.position = "top") +
  scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x), 
                labels = trans_format("log10", math_format(10^.x))) + 
  annotation_logticks(sides = "b")  
```

In der Einleitung haben wir ja die Abbildung zu der Dauer des Urinierens und verschiedener Säugetiere gesehen. In der Abbildung wurde die x-Achse wie auch die y-Achse logtransformiert um einmal dazustellen, ob es einen Zusammenhang zwischen dem Gewicht und der Pinkeldauer gibt. Wir sehen dort eine Paralelle zu der x-Achse, so dass wir daraus schließen können, dass die Körpergröße nichts mit der Dauer des Urinierens zu tun hat. Hier habe ich dann nochmal die Beispieldaten zu dem Alter und dem Gewicht von Säugetieren mitgebracht. Wenn wir beide Messwerte logtransformieren, dann sehen wir auf einmal einen klaren Zusammenhang. Mit stiegendem Gewicht steigt auch die Lebensdauer an. Hier sehen wir dann einmal den Vorteil der Logtransformation, da wir jetzt einen besseren Zusammenhang erahnen können. Da beides auf der Logskala läuft ist es natürlich etwas schwerer die möglichen Koeffizienten einer linearen Regression zu interpretieren. Aber das ist dann noch ein anderes Thema.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-ggplot-weight-age-log
#| fig-align: center
#| fig-height: 5
#| fig-width: 8
#| fig-cap: "Logaritmierter Zusammenhang zwischen dem Gewicht in [log(kg)] und dem Alter in [log(yrs)] von Tierarten. Durch die Logtransformation wird die große Bandbreite der Körpergewichte aufgebrochen. Ein linearer Trend lässt sich so ablesen. *[Zum Vergrößern anklicken]*"

anage_log_tbl <- anage_tbl |> 
  mutate(weight = log(weight),
         age = log(age))

anage_log_tbl |> 
  ggplot(aes(weight, age)) +
  theme_minimal() +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = 10, color = "gray") +
  geom_hline(yintercept = 4, color = "gray") +
  geom_hline(yintercept = 0, color = "gray") +
  geom_smooth(method = "lm", se = FALSE, color = "#0072B2") +
  geom_text_repel(data = filter(anage_log_tbl, age > 4 & weight > 10),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  geom_text_repel(data = filter(anage_log_tbl, age <= 0),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  labs(x = "Gewicht in [log(kg)]", y = "Alter in [log(yrs)]") +
  theme(axis.title.y = element_text(size = 16, face = 2),
        axis.text.y = element_text(size = 12),
        axis.title.x = element_text(size = 16, face = 2),
        axis.text.x = element_text(size = 12),  
        plot.title = element_text(size = 17),
        plot.subtitle = element_text(size = 12, face = "italic"),
        plot.caption = element_text(size = 12),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "top")
```

### Inverse

Die inverse Transformation ist recht simple. Wir rechnen einfach $1/y$ und haben dann schon unsere Transformation. Auch hier müssen wir dann schauen, ob wir eine linksschiefe oder rechtsschiefe Verteilung vorliegen haben und dann entsprechend die Transformation wie weiter oben beschreiben anpassen. Die Transformation ist dabei natürlich sehr einfach in R selber zu implementieren.

```{r}
inverse_tbl <- fac1_tbl |> 
  mutate(inverse_hatch_time = 1/hatch_time)
```

In der folgenden Abbildung sehen wir, dass die Inverseransformation nicht funktioniert hat und unseren Messwert noch schiefer gemaht hat als vorher. Ein schönes Beispiel dafür, dass wir immer die Transformation nochmal anschauen müssen. Eine Transformation kann auch zu einem noch schlechter normalverteiltem Messwert führen. Daher immer schauen, ob die Transformation einen annährend normalverteilten Messwert hervorgebracht hat.

```{r}
#| message: false
#| echo: false
#| label: fig-inverse
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** inverse-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(inverse_tbl, aes(hatch_time, fill = animal)) +
  theme_minimal() +
  geom_density(color = "black", alpha = 0.5) +
  scale_fill_okabeito() +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "", fill = "") +
  xlim(-100, NA) +
  theme(legend.position = "top")

p2 <- ggplot(inverse_tbl, aes(inverse_hatch_time, fill = animal)) +
  theme_minimal() +
  geom_density(color = "black", alpha = 0.5) +
  scale_fill_okabeito() +
  labs(x = "Zeit bis zum Schlüpfen in (1/h)", y = "") +
  theme(legend.position = "top")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

### Inverse Normaltransformation

Kommen wir fast zum Schluß noch zur inversen Normaltransformation. Die inverse Normaltransformation sieht ein wenig wild in der Formel aus. Ich zeige hier die Implementierung in R einmal und gehe nicht weiter mathematisch auf die Transformation ein. In den letzten Jahren wird die Transformation immer beliebter, so dass ich hier auch gerne auf die Arbeit von @beasley2009rank und [Rank-Based Inverse Normal Transformations are Increasingly Used, But are They Merited?](https://pmc.ncbi.nlm.nih.gov/articles/PMC2921808/) verweise.

```{r}
invers_norm <- function(y) qnorm((rank(y, na.last="keep") - 0.5)/sum(!is.na(y)))
```

[What is the inverse normal transformation (INT) and what are the reasons behind using it?](https://stats.stackexchange.com/questions/588992/what-is-the-inverse-normal-transformation-int-and-what-are-the-reasons-behind)

von @beasley2009rank

[Inverse normal transformation -- What is it and how to do it?](https://yingji15.github.io/2019-08-17-inverse-normal-transformation/)

```{r}
invers_norm_tbl <- fac1_tbl |> 
  mutate(inverse_hatch_time = invers_norm(hatch_time))
```

```{r}
#| message: false
#| echo: false
#| label: fig-inverse-norm
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $inverse$-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(invers_norm_tbl, aes(hatch_time, fill = animal)) +
  theme_minimal() +
  geom_density(color = "black", alpha = 0.5) +
  scale_fill_okabeito() +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "", fill = "") +
  xlim(-100, NA) +
  theme(legend.position = "top")

p2 <- ggplot(invers_norm_tbl, aes(inverse_hatch_time, fill = animal)) +
  theme_minimal() +
  geom_density(color = "black", alpha = 0.5) +
  scale_fill_okabeito() +
  labs(x = "Zeit bis zum Schlüpfen in (1/h)", y = "") +
  theme(legend.position = "top")   

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

### Rangtransformation

Der Ausweg schlechthin bis in die 90ziger Jahre war vermutlich die nichtparametrische Statistik, wenn es um nicht normalverteilte Messwerte ging. Es wird dann eben ein nichtparametrischer Test, wie der Wilcoxon oder eben Mann-Whitney gerechnet. Und hier kommt dann die Rangtransformations ins Spiel. *Eigentlich* ist die gesamte Nichtparametrik nur eine Rangtransformation auf der wir dann auch genauso gut dann einen t-Test oder eine ANOVA rechnen könnten. Dazu dann aber mehr in den jeweiligen Kapiteln zu den einzelnen nichtparametrischen Tests.

-   [Der Wilcoxon-Mann-Whitney-Test](#sec-utest) oder auch U-Test ist der t-Test auf den Rängen eines Messwertes. Wir vergleichen hier zwei Gruppen miteinander. Wenn wir mehr Gruppen haben, die wir vergleichen wollen, dann brauchen wir mehrere paarweise Wilcoxon Tests um die signifikanten Unterschiede zu bestimmen.
-   [Der Kruskal-Wallis-Test](#sec-kruskal) ist die einfaktorielle ANOVA auf den Rängen eines Messwertes. Wir vergleichen hier drei oder mehr Gruppen simultan miteinander. Wenn wir dann wissen wollen, welcher paarweise Vergleich signifikant ist, brauchen wir dann einen Posthoc-Test.

Was ist also die Rangtransformation? Wir geben einfach den sortierten Rang des Messwertes über alle Gruppen. Dann können wir auf dem rangierten Messwert weiterrechnen. In den folgenden Tabellen siehst du dann einmal die orginalen Sprungweiten gemessen in \[cm\] sowie deren rangierten Gegenstücke.

```{r}
signed_rank <- function(x) sign(x) * rank(abs(x))
```

`rank()`

## Skalierung

### Standardisierung

Die Standardisierung wird auch $z$-Transformation genannt. In dem Fall der Standardisierung schieben wir die Daten auf den Ursprung, in dem wir von jedem Datenpunkt $y_i$ den Mittelwert $\bar{y}$ abziehen. Dann setzen wir noch die Standardabweichung auf Eins in dem wir durch die Standardabweichung $y_s$ teilen. Unser standardisiertes $y$ ist nun standardnormalverteilt mit $\mathcal{N(0,1)}$. Wir nutzen für die Standardisierung folgende Formel.

$$
y_z = \cfrac{y_i - \bar{y}}{s_y} 
$$

In R können wir für die Standardisierung die Funktion `scale()` verwenden. Wir müssen auch nichts weiter in den Optionen von `scale()` angeben. Die Standardwerte der Funktion sind so eingestellt, dass eine Standardnormalverteilung berechnet wird.

```{r}
scale_tbl <- fac1_tbl |> 
  mutate(scale_jump_length = scale(jump_length))
```

In der folgenden Abbildung sehen wir nochmal die nicht transformierten, rohen Daten. Wir haben in diesem Beispiel die normalvertielte Variable `jump_length` gewählt. Der Mittelwert von `jump_length` ist `r round(mean(fac1_tbl$jump_length), 2)` und die Standardabweichung ist `r round(sd(fac1_tbl$jump_length), 2)`. Ziehen wir nun von jedem Wert von `jump_length` den Mittelwert mit 19.3 ab, so haben wir einen neuen Schwerpunkt bei Null. Teilen wir dann jede Zahl durch 3.36 so haben wir eine reduzierte Spannweite der Verteilung. Es ergibt sich die folgende Abbildung als Standardnormalverteilung. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die Sprungweite auf der $z$-Skala nicht mehr biologisch interpretieren.

```{r}
#| message: false
#| echo: false
#| label: fig-log-scale-3
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $inverse$-transformierte Daten. *[Zum Vergrößern anklicken]*"

p1 <- ggplot(scale_tbl, aes(jump_length)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = "Sprungweite in [cm]", y = "Anzahl")

p2 <- ggplot(scale_tbl, aes(scale_jump_length)) +
  geom_histogram(fill = cbbPalette[3], color = "black") +
  theme_minimal() +
  labs(x = "Sprungweite auf der z-Skala", y = "Anzahl") +
  theme(axis.text.x = element_text(color = "#CC79A7"))

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-ggplot-weight-age-standard
#| fig-align: center
#| fig-height: 5
#| fig-width: 8
#| fig-cap: "foo. **(A)** br. **(B)** fff. *[Zum Vergrößern anklicken]*"

anage_scale_tbl <- anage_tbl |> 
  mutate(weight = scale(weight),
         age = scale(age))

anage_scale_tbl |> 
  ggplot(aes(weight, age)) +
  theme_minimal() +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = 5, color = "gray") +
  geom_hline(yintercept = 7.5, color = "gray") +
  geom_text_repel(data = filter(anage_scale_tbl, weight > 5),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  geom_text_repel(data = filter(anage_scale_tbl, age > 7.5 & weight < 5),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  labs(x = "Standardisiertes Gewicht", y = "Standardisiertes Alter") +
  theme(axis.title.y = element_text(size = 16, face = 2),
        axis.text.y = element_text(size = 12, color = "#CC79A7"),
        axis.title.x = element_text(size = 16, face = 2),
        axis.text.x = element_text(size = 12, color = "#CC79A7"),  
        plot.title = element_text(size = 17),
        plot.subtitle = element_text(size = 12, face = "italic"),
        plot.caption = element_text(size = 12),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "top")
```

### Normalisierung

Abschließend wollen wir uns nochmal die Normalisierung anschauen. In diesem Fall wollen wir die Daten so transformieren, dass die Daten nur noch in der Spannweite 0 bis 1 vorkommen. Egal wie die Einheiten vorher waren, alle Variablen haben jetzt nur noch eine Ausprägung von 0 bis 1. Das ist besonders wichtig wenn wir viele Variablen haben und anhand der Variablen eine Vorhersage machen wollen. Uns interessieren die Werte in den Variablen an sich nicht, sondern wir wollen ein Outcome vorhersagen. Wir brauchen die Normalisierung später für das maschinelle Lernen und die Klassifikation. Die Formel für die Normalisierung lautet wie folgt.

$$
y_n = \cfrac{y_i - \min(y)}{\max(y) - \min(y)} 
$$

```{r}
normalize <- function(y) {(y - min(y))/(max(y) - min(y))}
```

In R gibt es die Normalisierungsfunktion nicht direkt. Wir könnten hier ein extra Paket laden, aber bei so einer simplen Formel können wir auch gleich die Berechnung in der Funktion `mutate()` machen. Wir müssen nur etwas mit den Klammern aufpassen.

```{r}
norm_tbl <- fac1_tbl |> 
  mutate(norm_jump_length = (jump_length - min(jump_length))/(max(jump_length) - min(jump_length)))
```

In der folgenden Abbildung auf der linken Seite sehen wir nochmal die nicht transformierten, rohen Daten. Auf der rechten Seiten sehen wir die normalisierten Daten. Hier fällt dann auf, dass die normalisierten Sprungweiten nur noch Werte zwischen Null und Eins annehmen. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die normalisierten Sprungweiten nicht mehr biologisch interpretieren.

```{r}
#| message: false
#| echo: false
#| label: fig-log-scale-4
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $inverse$-transformierte Daten. *[Zum Vergrößern anklicken]*"

p1 <- ggplot(norm_tbl, aes(jump_length)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = "Sprungweite in [cm]", y = "Anzahl")

p2 <- ggplot(norm_tbl, aes(norm_jump_length)) +
  geom_histogram(fill = cbbPalette[3], color = "black") +
  theme_minimal() +
  labs(x = "Normalisierte Sprungweite", y = "Anzahl") +
  theme(axis.text.x = element_text(color = "#CC79A7"))

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-ggplot-weight-age-norm
#| fig-align: center
#| fig-height: 5
#| fig-width: 8
#| fig-cap: "foo. **(A)** br. **(B)** fff. *[Zum Vergrößern anklicken]*"

anage_normalize_tbl <- anage_tbl |> 
  mutate(weight = normalize(weight),
         age = normalize(age))

anage_normalize_tbl |> 
  ggplot(aes(weight, age)) +
  theme_minimal() +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = 0.1, color = "gray") +
  geom_hline(yintercept = 0.65, color = "gray") +
  geom_text_repel(data = filter(anage_normalize_tbl, weight > 0.1),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  geom_text_repel(data = filter(anage_normalize_tbl, age > 0.65 & weight < 0.1),
                  aes(label = name), size = 3,
                  min.segment.length = unit(0, 'lines')) +
  labs(x = "Normalisiertes Gewicht", y = "Normalisiertes Alter") +
  theme(axis.title.y = element_text(size = 16, face = 2),
        axis.text.y = element_text(size = 12, color = "#CC79A7"),
        axis.title.x = element_text(size = 16, face = 2),
        axis.text.x = element_text(size = 12, color = "#CC79A7"),  
        plot.title = element_text(size = 17),
        plot.subtitle = element_text(size = 12, face = "italic"),
        plot.caption = element_text(size = 12),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        legend.position = "top")
```

## Automatische Transformationen...

[Normalisierung vs. Standardisierung: Wie man den Unterschied erkennt](https://www.datacamp.com/de/tutorial/normalization-vs-standardization)

Die Standardisierung versucht auch, die Symmetrie mit dem Mittelwert 0 und der Standardabweichung 1 zu erzwingen. Bei der Normalisierung ist die Wahrscheinlichkeit einer Schiefe größer, da Ihre Skalierungsfaktoren die beiden extremsten Datenpunkte sind.

### ... mit `{bestNormalize}`

Dann sind wir bis hierher gekommen und fragen uns nun welche ist den jetzt die beste Transformation für unsere Daten um einigermaßen eine Normalverteilung hinzubekommen? Da hilft uns jetzt das [R Paket `{bestNormalize}`](https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html). Die [Kurzanleitung zum Paket](https://petersonr.github.io/bestNormalize/index.html) erlaubt es automatisiert auf einem Datenvektor die beste Transformation zu finden um die Daten in eine Normalverteilung zu verwandeln.

Jetzt können wir die Funktion `bestNormalize()` nutzen um uns die beste Transformationsmethode wiedergeben zu lassen. Das ist super praktisch in der Anwendung. Wir können uns theoretisch noch verschiedene Gütekriterien aussuchen aber für hier reicht die Standardimplementierung. Wichtig ist noch, dass wir hier eine Kreuzvalidierung durchführen, so dass die Ergebnisse und die Auswahl des Algorithmus robust sein sollte.

```{r}
bn_obj <- bestNormalize(fac1_tbl$hatch_time)
bn_obj
```

Laut dem Algorithmus sollen wir eine `Box Cox Transformation` durchführen. Okay, wie machen wir das jetzt? Wir hätten zwar die Parameter in der Ausgabe angegeben und die könnten wir dann auch in einer Veröffentlichung angeben, aber wenn es schneller gehen soll, dann können wir die Funktion `predict()` nutzen, die uns die transformierten `x` Werte wiedergibt.

```{r}
x_trans <- predict(bn_obj)
```

Dann nochmal schnell gucken, ob das auch mit der Rücktransformation klappen würde.

```{r}
x_trans_back <- predict(bn_obj, newdata = x_trans, inverse = TRUE)
```

Einmal dann der Vergleich ob alle `x_trans_back` Werte den ursprünglichen `x` Werten entsprechen. Ja, das sieht gut aus.

```{r}
all.equal(x_trans_back, fac1_tbl$hatch_time)
```

Und zum Abschluss nochmal eine Abbildung der transformierten `x` Werte.In der folgenden Abbildung siehst du dann einmal das entsprechende Histogramm. Das sieht doch sehr gut aus und wir mussten nicht zig verschiedene Algorithmen selber testen.

```{r}
#| message: false
#| echo: false
#| label: fig-trans-bestNorm-2
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der transformierten `x` Werte mit der Box-Cos Transformation aus dem R Paket `{bestNormalize}`."

p1 <- ggplot(log_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(as_tibble(x_trans), aes(x_trans)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

### ... mit `{rcompanion}`

`transformTukey()` aus dem R Paket `{rcompanion}`

```{r}
tt_obj <- transformTukey(fac1_tbl$hatch_time,
                         plotit = FALSE)
```

```{r}
#| message: false
#| echo: false
#| label: fig-trans-transformTukey
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm ."

p1 <- ggplot(log_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(as_tibble(tt_obj), aes(tt_obj)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

### ... mit `{LambertW}`

`Gaussianize()` aus dem [R Paket `{LambertW}`](https://cran.r-project.org/web/packages/LambertW/index.html)

-   type: what type of non-normality: symmetric heavy-tails "h" (default), skewed heavy-tails "hh", or just skewed "s".

```{r}
gaus_obj <- Gaussianize(fac1_tbl$hatch_time, type = "s")
```

```{r}
#| message: false
#| echo: false
#| label: fig-trans-Gaussianize
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm ."

p1 <- ggplot(log_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(as_tibble(gaus_obj), aes(gaus_obj)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

### ... mit `{trafo}`

[The R Package trafo for Transforming Linear Regression Models](https://cran.r-project.org/web/packages/trafo/vignettes/vignette_trafo.pdf)

```{r}
lm_fit <- lm(hatch_time ~ weight, data = fac1_tbl)
```

```{r}
trafo_lm(lm_fit)
```

## Friedhof der R Pakete

Warum gibt es hier den Friedhof? Nun, es gibt nur eine begrenzte Anzahl an möglichen Transformationen. Diese Transformationen sind dann aber in vielen R Paketen eingebaut. Mit der Zeit habe ich dann neuere und bessere R Pakete gefunden, die ich hier weiter oben vorstelle. Die alten Pakete, die ich auch mal genutzt habe, schiebe ich dann hier in den Friedhof. Meistens können die neuren Pakete das was die alten können plus die neuen Pakete sind einfacher zu bedienen oder aber aber haben andere Vorteile. Als Referenz bleibt der Firedhof aber offen.

### ... mit `{MASS}`

Die Box-Cox-Transformation ist ein statistisches Verfahren zur Umwandlung von nicht normalverteilten Daten in eine Normalverteilung. Die Transformation ist nicht so einfach wie die logarithmische Transformation oder die Quadratwurzeltransformation und erfordert etwas mehr Erklärung. Beginnen wir zunächst die Gleichung zu verstehen, die die Transformation beschreibt. Die grundsätzliche Idee ist, dass wir unser $y$ als Outcome mit einem $\lambda$-Exponenten transformieren. Die Frage ist jetzt, welches $\lambda$ produziert die besten normalverteilten Daten? Wenn wir ein $\lambda$ von Null finden sollten, dann rechnen wir einfach eine $\log$-Transformation. Die Idee ist also recht simpel.

$$
y(\lambda)=\left\{\begin{matrix} \dfrac{y^{\lambda}-1} {\lambda} & \quad  \mathrm{f\ddot ur\;\;}\lambda \ne 0 \\[10pt] \log(y) &\quad  \mathrm{f\ddot ur\;\;}\lambda = 0\end{matrix}\right.
$$

Wir werden natürlich jetzt nicht händisch alle möglichen $\lambda$ durchprobieren bis wir das beste $\lambda$ gefunden haben. Dafür gibt es die Funktion `boxcox` aus dem R Paket `{MASS}`, die ein lineares Modell benötigt. Daher bauen wir usn erst unser lineares Modell und dann stecken wir das Modell in die Funktion `boxcox()`.

```{r}
jump_mod <- lm(jump_length ~ 1, data = fac1_tbl)
```

Jetzt einmal die Funktion `boxcox()` ausführen und danach das $\lambda$ extrahieren. Hier ist es etwas umständlicher, da das $\lambda$ in der Ausgabe der Funktion etwas vergraben ist. Dafür ist das R Paket `{MASS}` einfach nicht mehr das jüngste Paket und hat keine so guten Funktionen zum Erhalten von wichtigen Parametern. Ich möchte die Abbildung nicht haben, daher die Option `plotit = FALSE`.

```{r}
bc_obj <- MASS::boxcox(jump_mod, plotit = FALSE)
lambda <- bc_obj$x[which.max(bc_obj$y)]
lambda
```

Wir erhalten also ein $\lambda$ von `r lambda` wieder. Dieses $\lambda$ können wir dann nutzen um unsere Daten nach Box-Cox zu transformieren. Dafür übersetzen wir dann die obige Matheformel einmal in R Code.

```{r}
fac1_tbl <- fac1_tbl |>
  mutate(jump_boxcox = ((jump_length ^ lambda - 1)/lambda))
```

In der @fig-boxcox-ggplot sehen wir dann einmal das Ergebnis der Transformation. Sieht gar nicht mal so schlecht aus und noch besser als die reine $\log$-Transformation. Wie immer musst du aber auch hier rumprobieren, was dann am besten einer Normalverteilung folgt.

```{r}
#| message: false
#| echo: true
#| label: fig-boxcox-ggplot
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm der Box-Cox transfomierten Daten in `ggplot` dargestellt."
ggplot(data = fac1_tbl) +
  theme_minimal() +
  geom_histogram(aes(x = jump_boxcox), alpha = 0.9,
                 fill = cbbPalette[3], color = "black") +
  labs(x = 'Box-Cox transformierte Sprungweiten', y = 'Anzahl')

```

### ... mit `{dlookr}`

::: callout-warning
## Achtung, bitte beachten!

Das R Paket `{dlookr}` lies sich eine Zeit nicht über CRAN installieren. Das Paket benötigt auch sehr lange um geladen zu werden. Daher habe ich mich dagegen entschieden, das Paket hier nochmal tiefer vorzustellen.
:::

Zwischenzeitlich musste ich das R Paket `{dlookr}` aus dem Code entfernt, da ich das Paket über GitHub installieren musste. Das R Paket war zwischenzeitlich nicht mehr bei CRAN gelistet. Das führt bei mir zu Schluckauf im Code, so dass ich die Funktionen jetzthier benenne. Sollte das Paket sich mal nicht installieren lassen, dann gibt es hier noch einen alternativen Weg.

::: callout-caution
## Alternative Installation von `{dlookr}`

Ist das R Paket `{dlookr}` mal nicht per Standard aus RStudio zu installieren kannst du das R Paket auch über GitHub installieren. Dafür einmal das R Paket `{devtools}` installieren und dann folgenden Code ausführen.

```{r}
#| eval: false
devtools::install_github("choonghyunryu/dlookr")
```
:::

Du findest auf der Hilfeseite [Data Transformation mit `{dlookr}`](https://choonghyunryu.github.io/dlookr/articles/transformation.html#standardization-and-resolving-skewness) die Unterstützung, die du brauchst um mit der Funktion `transform()` deine einzelnen Messwerte einmal zu transformieren. Du kannst dann die Funktion einfach im Kontext von `mutate()` nutzen. Wie immer, dass Paket ist gut und hat auch seinen Nutzen. Viele Funktionen habe ich aus dem Paket auch selber schon genutzt. Es ist aber super ärgerlich, wenn das Paket auf einmal nicht mehr gewartet wird. Das ist zwar immer in R so, dass mal Pakete rausgehen können, aber hier war mir das dann doch zu wild.

## Referenzen {.unnumbered}
