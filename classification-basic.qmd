```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Grundlagen der Klassifikation {#sec-class-basic}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

Dieses Kapitel dient als Einführung in die *Klassifikation* mit maschinellen Lernmethoden. Leider müssen wir wieder einiges an Worten lernen, damit wir überhaupt mit den Methoden anfangen können. Vieles dreht sich um die Aufbereitung der Daten, damit wir dann auch mit den Modellen anfangen können zu *arbeiten*. Ja ich meine wirklich Arbeiten, denn wir werden eher einen Prozess durchführen. Selten rechnet man einmal ein Modell und ist zufrieden. Meistens müssen wir noch die Modelle *tunen* um mehr aus den Modellen rauszuholen. Wir wollen bessere Vorhersagen mit einem kleineren Fehler erreichen. Das ganze können wir dann aber nicht in einem Schritt machen, sondern brauchen viele Schritte nacheinander. Damit müssen wir auch mir R umgehen können sonst ist der Prozess nicht mehr abzubilden.

[Mit *Tuning* bezeichnen wir den Prozess, ein Modell wiederholt zu verändern und dabei zu verbessern. Was wir verändern können, hängt vom gewählten Algorithums ab.]{.aside}

## Genutzte R Pakete für das Kapitel

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

```{r}
pig_tbl <- read_excel("data/infected_pigs.xlsx") %>% 
  mutate(pig_id = 1:n()) %>% 
  select(pig_id, infected, everything())  
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-ml-basic-pig
#| tbl-cap: Auszug aus dem Daten zu den kranken Ferkeln.
#| column: page

rbind(head(pig_tbl),
      rep("...", times = ncol(pig_tbl)),
      tail(pig_tbl)) %>% 
  kable(align = "c", "pipe")
```

## What he say's?

In diesem Teil des Skriptes werden wir wieder mit einer Menge neuer Begriffe konfrontiert. Deshalb steht hier auch eine Menge an neuen Worten drin. Leider ist es aber auch so, dass wir *bekanntes* neu bezeichnen. Wir tauchen jetzt ab in die Community der Klassifizierer und die haben dann eben die ein oder andere Sache neu benannt.

::: column-margin
Kurze Referenz zu [What he says?](https://youtu.be/RAidDYQLIK4)
:::

Die gute nachticht zuerst, wir haben ein relativ festes Vokabular. Das heißt, wir springen nicht so sehr zwischen den Begrifflichkeiten wie wir es in den anderen Teilen des Skriptes gemacht haben. Du kennst die Modellbezeichnungen wie folgt.

$$
y \sim x
$$

mit

-   $y$, als Outcome oder Endpunkt.
-   $x$, als Covariate oder Einflussvariable.

Das bauen wir jetzt um. Wir nennen in dem Bereich des maschinellen Lernen jetzt das $y$ und das $x$ wie folgt.

-   $y$ ist unser *label*, dafür gibt es kein deutsches Wort.
-   $x$ sind unsere *features* und mehrere Features bilden den *feature space*, dafür gibt es jeweils auch kein deutsches Wort.

Im folgenden Text werde ich also immer vom Label schreiben und dann damit das $y$ links von dem `~` in der Modellgleichung meinen. Wenn ich dann von den Features schreibe, meine ich alle $x$-Variablen rechts von dem `~` in der Modellgleichung. Ja, daran muss du dich dann gewöhnen. Es ist wieder ein anderer sprachlicher Akzent in einem anderen Gebiet der Statistik.

[*Label* meint also das $y$ oder Outcome. *Feature* beschreibt das $x$ oder die Einflussvariablen.]{.aside}

## Klassifikation vs. Regression

Wenn mich etwas aus der Bahn geworfen hat, dann waren es die Terme *classification* und *regression* im Kontext des maschinellen Lernens. Wenn ich von *classification* schreibe, dann wollen wir ein kategoriales Label vorhersagen. Das bedeutet wir haben ein $y$ vorliegen, was nur aus Klassen bzw. Kategorien besteht. Im Zweifel haben wir dann ein Label mit $0/1$ einträgen. Wenn mehr Klassen vorliegen, wird auch gerne von *multiclass* Klassifikation gesprochen.

Dazu steht im Kontrast der Term *regression*. In dem Kontext vom maschinellen Lernen meint *regression* die Vorhersage eines numerischen Labels. Das heißt, wir wollen die Körpergröße der Studierenden vorhersagen und nutzen dazu einen *regression* Klassifikator. Das ist am Anfang immer etwas verwirrend. Wir unterschieden hier nur die Typen der Label, sonst nichts. Wir fassen also wie folgt zusammen.

-   *classification*, wir haben ein Label bzw. $y$ mit Kategorien. Nehmen wir einmal unser Ferkelbeispiel. In unserer Spalte `infected` sind die Ferkel infiziert $(1)$ oder nicht-infiziert daher gesund $(0)$.
-   *regression*, wir haben ein Label bzw. $y$ mit kontinuierlichen Werten. Unsere Ferkel haben ein Gewicht in $kg$ und daher nehmen wir die Spalte `weight`.

Wir brauchen die Begriffe, da wir später in den Algorithmen spezifizieren müssen, welcher Typ die Klassifikation sein soll.

## Trainingsdatensatz und Testdatensatz

Um zu beginnen, teilen wir diesen einen Datensatz in zwei: einen *Trainings*satz und einen *Test*satz. Die meisten Zeilen und damit Beobachtungen des Originaldatensatzes werden im Trainingssatz sein. Wir nutzen die Trainingsdaten zum Anpassen des Modells. Wir *trainieren* das Modell auf den Daten des Trainingsdatensatzes. Wir messen dann das Modell auf den Testdatensatz. Warum machen wir das? Wenn wir auf dem Trainingsdatensatz auch die Modelgüte testen würden, dann könnten wir eine Überanpassung (eg. *overfitting*) auf die Trainingsdaten beobachten. Das Modell ist so gut an die spezifischen Trainingsdaten angepasst, dass es mit neuen Daten schwer umgehen kann.

::: column-margin
Das R Paket `resample` stellt die [Common Resampling Patterns](https://rsample.tidymodels.org/articles/Common_Patterns.html) nochmal da. Hier finden sich dann auch noch mehr Möglichkeiten.
:::

Zu diesem Zweck können wir das R Paket `rsample` verwenden. Wir nutzen dann die Funktion `initial_split()` um die Daten in einen Trainingsdatensatz und einen Testdatensatz aufzuteilen. Dann müssen wir noch den Trainingsdatensatz und den Testdatensatz einmal getrennt in einem Objekt abspeichern.

```{r}
pig_split <- initial_split(pig_tbl, prop = 3/4)

pig_split
```

Wie wir sehen, sehen wir gar nichts. Das ist auch so gewollt. Da wir im maschinellen Lernen gerne mal mit Datensätzen mit mehreren tausend Zeilen arbeiten würde es wenig helfen, wenn wir gleich alles auf der R Console ausgegeben kriegen. Die Information wie viel wir in den jeweiligen Gruppen haben, hilft schön genug.

```{r}
train_pig_tbl <- training(pig_split)
test_pig_tbl <- testing(pig_split)
```

Nun haben wir die beiden Datensätze jeweils separat und können auf dem Trainingsdatensatz die jeweiligen Algorithmen bzw. Modelle trainieren.

Es ist schön, wenn wir Funktionen wie `initial_split()`, die für uns die Arbeit machen. Wir haben dann aber auch sehr schnell das Gefühl mit einer Black Box zu arbeiten. Man weiß gar nicht, was da eigentlich passiert ist. Deshalb hier nochmal der Code, den ich dann auch immer zur Demonstration nutze.

```{r}
pig_train_tbl <- pig_tbl %>% sample_frac(0.75)
pig_test_tbl <- anti_join(pig_tbl,
                          pig_train_tbl, by = 'pig_id')

```

Wir können dann auch überprüfen, ob wir die gleichen Anteile von den infizierten Ferkeln in den jeweiligen Datensätzen haben. Wir berechnen dafür einfach die relativen Anteile. Ein wenig komplizierter als nötig, aber hier geht es jetzt um die Veranschaulichung.

```{r}
table(pig_train_tbl$infected)/sum(table(pig_train_tbl$infected))
table(pig_test_tbl$infected)/sum(table(pig_test_tbl$infected))
```

Du kannst die Generierung häufiger wiederholen und du wirst sehen, dass wir es mit einem Zufallsprozess zu tun haben. Mal sind die Anteile ähnlicher mal eher nicht. Das ist dann auch der Grund warum wir unsere Modelle *tunen* müssen und Modelle häufig *wiederholt* rechnen und die Ergebnisse dann zusammenfassen.

## Validierungsdatensatz

Die finalen Modelle sollten nur *einmal* anhand ihres Testdatensatzes evaluieren werden. Das Überpfrüfen auf dem Testdatensatz geschieht nachdem die Optimierung und das Training der Modelle vollständig abgeschlossen ist. Was natürlich für uns nicht so schön ist, wir wollen ja auch zwischendurch mal schauen, ob wir auf dem richtigen Weg mit dem Training sind. Wir solle es auch sonst mit dem Tuning funktionieren? Deshalb ist möglich, zusätzliche Datensätze aus dem Trainingsprozess *herauszuhalten*, die zur mehrmaligen Evaluierung von Modellen verwendet werden können. Das machen wir dann solange bis wir bereit sind anhand des endgültigen Testsatzes zu evaluieren.

Diese zusätzlichen, aufgeteilten Datensätze werden oft als Validierungssätze bezeichnet und können in über die Funktion `validation_split()` erstellt werden.

```{r}
val_pig_lst <- validation_split(pig_tbl, prop = 0.8)
val_pig_lst
```

In diesem Fall lassen wir den Validierungsdatensatz einmal so in der Liste stehen. Es ist faktisch wider ein Split der Daten, nur das wir jetzt auf diesem Datensatz unser Modell während des Tunings testen.

## Kreuzvalidierung

Bei der Abstimmung von Hyperparametern und der Modellanpassung ist es oft nützlich, das Modell anhand von mehr als nur einem einzigen Validierungssatz zu bewerten, um eine stabilere Schätzung der Modellleistung zu erhalten. Wir meinen hier mit Hyperparametern die Optionen, die ein Algorithmus hat um diesen Algorithmus zu optimieren. Aus diesem Grund verwenden Modellierer häufig ein Verfahren, das als Kreuzvalidierung bekannt ist und bei dem die Daten mehrfach in Analyse- und Valisierungsdaten aufgeteilt werden.

Die vielleicht häufigste Methode der Kreuzvalidierung ist die $V$-fache Kreuzvalidierung. Bei dieser auch als $k$-fold cross-validation bezeichneten Methode werden $V$ neue Stichproben bzw. Datensätze erstellt, indem die Daten in $V$ Gruppen (auch *folds* genannt) von ungefähr gleicher Größe aufgeteilt werden. Der Analysesatz jeder erneuten Stichprobe besteht aus $V-1$ Gruppen, wobei die verbleibende Gruppe als Validierungsdatensatz verwendet wird. Insgesamt führen wir dadurch dann den Algorithmus $V$-mal durch. Auf diese Weise wird jede Beobachtung in Daten in genau einem Beurteilungssatz verwendet.

In R können wir dafür die Funktion `vfold_cv()` nutzen. Im Folgenden einmal Split für $V = 5$. Wir führen also eine $5$-fache Kreuzvalidierung durch.

```{r}
vfold_cv(pig_tbl, v = 3)
```

Als ein Nachteil wird oft angesehen, dass die Kreuzvalidierung eine hohe Varianz in den Daten verursacht. Dagegen hilft dann die wiederholte Kreuzvalidierung (eng. *repeated cross-validation*). Wir bauen in jede Kreuzvalidierung nochmal eine oder mehr Wiederholungen ein. In unserem Fall dann drei Wiederholungen je Kreuzvalidierung $V$.

```{r}
vfold_cv(pig_tbl, v = 3, repeats = 2)
```

Wir sehen das der Split ungefähr immer gleich groß ist. Manchmal haben wir durch die Trennung eine Beobachtung mehr in dem Analysedatensatz mit $n = 329$ oder $n = 330$ Beobachtungen. Dementsprechend hat der Validierungsdatensatz einmal $n = 82$ und einmal $n = 83$ Beobachtungen.

## Monte-Carlo Kreuzvalidierung

Wir haben als eine Alternative zur V-fachen Kreuzvalidierung die Monte-Carlo-Kreuzvalidierung vorliegen. Während bei der V-fachen Kreuzvalidierung jede Beobachtung in den Daten einem - und zwar genau einem - Validierungsdatensatz zugewiesen wird, wird bei der Monte-Carlo-Kreuzvalidierung für jeden Validierungsdatensatz eine zufällige Teilmenge der Daten ausgewählt, d. h. jede Beobachtung kann in 0, 1 oder vielen Validierungsdatensätzen verwendet werden. Der Analysesatz besteht dann aus allen Beobachtungen, die nicht ausgewählt wurden. Da jeder Validierungsdatensatz unabhängig ausgewählt wird, können wir diesen Vorgang so oft wie gewünscht wiederholen. Das stimt natürlich nur bedingt, denn irgendwann haben wir auch bei perfekter Permutation dann Wiederholungen der Datensätze.

Die Funktion `mc_cv()` liefert uns dann die Datensätze für die Monte-Carlo Kreuzvalidierung. Wir geben dabei an, wieviel der Daten in den jeweiligen Datensatz hinein permutiert werden soll.

```{r}
mc_cv(pig_tbl, prop = 0.6, times = 3)
```

## Bootstraping

Die letzte Stichprobengenierungsmethode ist der Bootstrap. Eine Bootstrap Stichprobe ist eine Stichprobe des Datensatzes mit der gleichen Größe wie der Datensatz. Nur werden die Bootstrap Stichproben mit Ersetzung gezogen, so dass eine einzelne Beobachtung mehrfach in die Stichprobe aufgenommen werden können. Der Validierungsdatensatz besteht dann aus allen Beobachtungen, die nicht für den Analysesatz ausgewählt wurden. Im Allgemeinen führt das Bootstrap-Resampling zu pessimistischen Schätzungen der Modellgenauigkeit.

Wir können die Funktion `bootstraps()` für die Generierung der Bootstrap Stichprobe nutzen.

```{r}
pig_boot_tbl <- pig_tbl %>% 
  extract(1:10, 1:5)

pig_boot <- bootstraps(pig_boot_tbl, times = 3)
```

Nun haben wir auch die Möglichkeit uns die einzelnen Bootstraps Stichproben mit `pluck()` rauszuziehen. Hier sehen wir auch, dass einzelene Beobachtungen doppelt in der Bootstrap Stich probe vorkommen.

```{r}
pluck(pig_boot, "splits", 1) %>% 
  as_tibble 
```

## Weitere Valdierungen

Neben den hier vorgestellten Varianten gibt es noch weitere Möglichkeiten in dem R Paket `rsample` sich Stichprobendatensätze zu generieren. Wir gehen jetzt hier nicht mehr im Detail auf die verschiedenen Möglichkeiten ein. Dafür dann einfach die Links auf die `rsample` Hilfeseite nutzen.

-   [Stratifiziertes Resampling](https://rsample.tidymodels.org/articles/Common_Patterns.html#stratified-resampling) nutzen wir, wenn wir eine Gruppe in den Daten haben, die nicht gleichmäßig über die Daten verteilt ist. Das heißt, wir haben ein nicht balanciertes Design. Kann plaktiv wäre das der Fall, wenn wir fast nur Frauen oder Männer in unseren Daten vorliegen hätten. Hier kann es dann passieren, dass wir zufällig Datensätze ziehen, die nur Frauen oder nur Männer beinhalten. Das wollen wir natürlich verhindern.
-   [Gruppiertes Resampling](https://rsample.tidymodels.org/articles/Common_Patterns.html#grouped-resampling) nutzen wir, wenn wir korrelierte Beobachtungen haben. Oft sind einige Beobachtungen in deinen Daten ähnlicher als es der Zufall vermuten ließe, z. B. weil sie wiederholte Messungen desselben Probanden darstellen oder alle an einem einzigen Ort gesammelt wurden. Dann müssen wir eventuell auch hierfür das Resampling anpassen.
-   [Zeitpunkt basiertes Resampling](https://rsample.tidymodels.org/articles/Common_Patterns.html#time-based-resampling) sind in dem Sinne eine besonderheit, da wir natürlich berücksichtigen müssen, wann eine Beobachtung im zeitlichen Verlauf gemacht wurde. Hier hat die Zeit einen Einfluss auf das Resampling.

Am Ende musst du entscheiden, welche der Resamplingmethoden für dich am besten geeignet ist. Wir müssen eben einen Trainingsdatensatz und einen Testdatensatz haben. Der Rest dient dann zum Tuning deiner Modelle.

## Supervised vs. unsupervised

Der Unterschied zwischen einer *suprvised* Lernmethode oder Algorithmus ist, dass das Label bekannt ist. Das heißt, dass wir in unseren Daten eine $y$ Spalte haben an der wir unser Modell dann trainieren können. Das Modell weiß also an was es sich optimieren soll.

| $y$ | $x_1$ | $x_2$ | $x_3$ |
|-----|-------|-------|-------|
| 1   | 0.2   | 1.3   | 1.2   |
| 0   | 0.1   | 0.8   | 0.6   |
| 1   | 0.3   | 2.3   | 0.9   |
| 1   | 0.2   | 9.1   | 1.1   |

: test {#tbl-class-supervised}

|     | $x_1$ | $x_2$ | $x_3$ |
|-----|-------|-------|-------|
|     | 0.2   | 1.3   | 1.2   |
|     | 0.1   | 0.8   | 0.6   |
|     | 0.3   | 2.3   | 0.9   |
|     | 0.2   | 9.1   | 1.1   |

: test {#tbl-class-unsupervised}

## Bagging

## Boosting

## Recipes

## Problem der fehlenden Werte

Siehe hierzu das @sec-missing. Wir gehen hier nicht nochmal auf alle Verfahren ein.

## Normalisierung der Beobachtungen

Siehe hierzu das @sec-eda-transform. Wir gehen hier nicht nochmal auf alle Verfahren ein, sondern konzentrieren uns auf die häufigsten Anwendungen.

R Paket `resample`

R Paket `tune`

R Paket `yardstick`

https://www.tmwr.org/resampling.html https://www.tidymodels.org/start/resampling/

## Needed R packages

https://parsnip.tidymodels.org/ https://rsample.tidymodels.org/ https://recipes.tidymodels.org/

## Visualisation

https://datascienceplus.com/machine-learning-results-one-plot-to-rule-them-all/ https://datascienceplus.com/machine-learning-results-in-r-one-plot-to-rule-them-all-part-2-regression-models/ https://www.r-bloggers.com/2021/04/the-good-the-bad-and-the-ugly-how-to-visualize-machine-learning-data/ https://neptune.ai/blog/visualizing-machine-learning-models https://uc-r.github.io/lime
