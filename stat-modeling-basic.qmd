```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Multiple lineare Regression {#sec-mult-reg-basic}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

Ein multiples lineare Modell hat mehrere $x$. Daher haben wir auf der linken Seite ein $y$ und auf der rechten Seite $p$-mal ein $x$. Wir können daher ein multiples lineares Modell daher wie folgt in R schreiben.

$$
y \sim x_1 + x_2 + ... + x_p 
$$

Oder konkreter können wir sagen, dass `jump_length` von `animal`, `sex` und `weight` abhängt und wir diesen Zusammenhang modellieren wollen.

$$
jump\_length \sim animal + sex + weight 
$$

Das hilft uns jetzt nur bedingt, denn wir wollen ja aus einer Modellierung in R die Koeffizienten der Regression wiederbekommen. Dafür müssen wir uns nochmal klar werden, was die Koeffizienten einer Regression sind. Das sind zum einen der y-Achsenabschnitt $\beta_0$ und die Steigung der einzelnen Variablen mit $\beta_1$ bis $\beta_p$. Wir erhalten auch bei einem multiplen Regressionsmodell nur einen Vektor mit den Residuen wieder.

$$
y \sim \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon
$$

Dennoch sind die Residuen $\epsilon$ normalverteilt. Wir haben im Mittel einen Abstand der $\epsilon$'s zur geraden von 0 und eine Streuung von $s^2_{\epsilon}$.

$$
\epsilon \sim \mathcal{N}(0, s^2_{\epsilon})
$$

Wir zeichen im Prinzip eine Gerade durch den $p$-dimensioneln Raum.

## Genutzte R Pakete für das Kapitel

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, conflicted, broom,
               see, performance, car)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Die Modelmatrix in R

Als erstes wollen wir verstehen, wie ein Modell *in* R aussieht. Dann können wir auch besser verstehen, wie die eigentlichen Koeffizienten aus dem Modell entstehen. Wir bauen uns dafür ein sehr simplen Datensatz.

```{r}
#| message: false
snake_tbl <- tibble(
  svl = c(40, 45, 39, 51, 52, 57, 58, 49),
  mass = c(6, 8, 5, 7, 9, 11, 12, 10),
  region = as_factor(c("west","west", "west", "nord","nord","nord","nord","nord")),
  color = as_factor(c("schwarz", "schwarz", "rot", "rot", "rot", "blau", "blau", "blau"))
) 
```

In der @tbl-snakes ist der Datensatz `snake_tbl` nochmal dargestellt. Wir haben die Schlangenlänge `svl` als Outcome $y$ sowie das Gewicht der Schlangen `mass`, die Sammelregion `region` und die Farbe der Schlangen `color`. Dabei ist `mass` eine kontinuierliche Variable, `region` eine kategorielle Variable als Faktor mit zwei Leveln und `color` eine kategorielle Variable als Faktor mit drei Leveln.

```{r}
#| message: false
#| echo: false
#| tbl-cap: Datensatz zu Schlangen ist entlehnt und modifiiert nach Kéry [-@kery2010introduction, p. 77]
#| label: tbl-snakes

snake_tbl %>% 
  kable(align = "c", "pipe")

```

Wir wollen uns nun einmal anschauen, wie ein Modell in R sich zusammensetzt.

### Kontinuierliches $x$

Im ersten Schritt wollen wir uns einmal das Modell mit einem kontenuierlichen $x$ anschauen. Wir bauen uns ein Modell mit der Variable `mass`. Die Funktion `model.matrix()` gibt uns die Modelmatrix wieder.

```{r}
model.matrix(svl ~ mass, data = snake_tbl) %>% as_tibble
```

In der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte `mass` als kontinuierliche Variable.

Wir können die Modellmatrix auch mathematisch schreiben und die $y$ Spalte für das Outcome `svl` ergänzen. Eben so ergänzen wir die $\beta$-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  1 & 6 \\
  1 & 8 \\
  1 & 5 \\
  1 & 7 \\
  1 & 9 \\
  1 & 11\\
  1 & 12\\
  1 & 10\\
 \end{pmatrix}
 \times
  \begin{pmatrix}
  \beta_0 \\
  \beta_{mass} 
 \end{pmatrix} +
  \begin{pmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5 \\
  \epsilon_6 \\
  \epsilon_7 \\
  \epsilon_8 \\
 \end{pmatrix}
$$

Jetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion `coef()` um uns die Koeffizienten wiedergeben zu lassen. Die Funktion `residuals()` gibt uns die Residuen der Geraden wieder.

```{r}
fit_1 <- lm(svl ~ mass, data = snake_tbl) 
fit_1 %>% coef %>% round(2)
fit_1 %>% residuals() %>% round(2)
```

Wir können jetzt die Koeffizienten ergänzen mit $\beta_0 = 26.71$ für den Intercept. Weiter ergänzen wir die Koeffizienten für `mass` mit $\beta_{mass}=2.61$. Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  26.71 & \phantom{0}6 \cdot 2.61\\
  26.71 & \phantom{0}8 \cdot 2.61\\
  26.71 & \phantom{0}5 \cdot 2.61\\
  26.71 & \phantom{0}7 \cdot 2.61\\
  26.71 & \phantom{0}9 \cdot 2.61\\
  26.71 & 11\cdot 2.61\\
  26.71 & 12\cdot 2.61\\
  26.71 & 10\cdot 2.61\\
 \end{pmatrix}
 \times
  \begin{pmatrix}
  \beta_0 \\
  \beta_{mass} 
 \end{pmatrix} +
  \begin{pmatrix}
  -2.36\\
  -2.57\\
  -0.75 \\
  +6.04\\
  +1.82\\
  +1.61\\
  \phantom{+}0.00\\
  -3.79\\
 \end{pmatrix}
$$

Wir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.

```{r}
c(26.71 +  6*2.61 - 2.36,
  26.71 +  8*2.61 - 2.57,
  26.71 +  5*2.61 - 0.75,
  26.71 +  7*2.61 + 6.04,
  26.71 +  9*2.61 + 1.82,
  26.71 + 11*2.61 + 1.61,
  26.71 + 12*2.61 + 0.00,
  26.71 + 10*2.61 - 3.79) %>% round() 
```

Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome $y$ hat.

### Kategorielles $x$ mit 2 Leveln

Im diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen $x$ mit 2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable ``` region``. Die Funktion ```model.matrix()\` gibt uns die Modelmatrix wieder.

```{r}
model.matrix(svl ~ region, data = snake_tbl) %>% as_tibble
```

In der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte `regionnord`. In dieser Spalte steht die Dummykodierung für die Variable `region`. Die ersten drei Schlangen kommen nicht aus der Region `nord` und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region `nord` und erhalten daher eine 1 in der Spalte.

Wir können die Modellmatrix auch mathematisch schreiben und die $y$ Spalte für das Outcome `svl` ergänzen. Eben so ergänzen wir die $\beta$-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  1 &  0  \\
  1 &  0 \\
  1 &  0\\
  1 &  1\\
  1 &  1 \\
  1 &  1 \\
  1 &  1 \\
  1 &  1 \\
 \end{pmatrix}
 \times
  \begin{pmatrix}
  \beta_0 \\
  \beta^{region}_{nord} \\
 \end{pmatrix} +
  \begin{pmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5 \\
  \epsilon_6 \\
  \epsilon_7 \\
  \epsilon_8 \\
 \end{pmatrix}
$$

Jetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion `coef()` um uns die Koeffizienten wiedergeben zu lassen. Die Funktion `residuals()` gibt uns die Residuen der Geraden wieder.

```{r}
fit_2 <- lm(svl ~ region, data = snake_tbl) 
fit_2 %>% coef %>% round(2)
fit_2 %>% residuals() %>% round(2)
```

Wir können jetzt die Koeffizienten ergänzen mit $\beta_0 = 41.33$ für den Intercept. Weiter ergänzen wir die Koeffizienten für die Region und das Level `nord` mit $\beta^{region}_{nord} = 12.07$. Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  41.33 & 0 \cdot 12.07  \\
  41.33 & 0 \cdot 12.07  \\
  41.33 & 0 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
 \end{pmatrix} +
  \begin{pmatrix}
  -1.33\\
  +3.67 \\
  -2.33 \\
  -2.40 \\
  -1.40 \\
  +3.60 \\
  +4.60 \\
  -4.40 \\
 \end{pmatrix}
$$

Wir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.

```{r}
c(41.33 + 0*12.07 - 1.33,
  41.33 + 0*12.07 + 3.67,
  41.33 + 0*12.07 - 2.33,
  41.33 + 1*12.07 - 2.40,
  41.33 + 1*12.07 - 1.40,
  41.33 + 1*12.07 + 3.60,
  41.33 + 1*12.07 + 4.60,
  41.33 + 1*12.07 - 4.40) %>% round() 
```

Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome $y$ hat.

### Kategorielles $x$ mit \>2 Leveln

Im diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen $x$ mit \>2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable `color`. Die Funktion `model.matrix()` gibt uns die Modelmatrix wieder.

```{r}
model.matrix(svl ~ color, data = snake_tbl) %>% as_tibble
```

In der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalten für `color`. Die Spalten `colorrot` und `colorblau` geben jeweils an, ob die Schlange das Level `rot` hat oder `blau` oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.

Wir können die Modellmatrix auch mathematisch schreiben und die $y$ Spalte für das Outcome `svl` ergänzen. Eben so ergänzen wir die $\beta$-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  1 & 0 & 0 \\
  1 & 0 & 0\\
  1 & 1 & 0\\
  1 & 1 & 0\\
  1 & 1 & 0\\
  1 & 0 & 1\\
  1 & 0 & 1\\
  1 & 0 & 1\\
 \end{pmatrix}
 \times
  \begin{pmatrix}
  \beta_0 \\
  \beta^{color}_{rot} \\
  \beta^{color}_{blau} \\
 \end{pmatrix} +
  \begin{pmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5 \\
  \epsilon_6 \\
  \epsilon_7 \\
  \epsilon_8 \\
 \end{pmatrix}
$$

Jetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion `coef()` um uns die Koeffizienten wiedergeben zu lassen. Die Funktion `residuals()` gibt uns die Residuen der Geraden wieder.

```{r}
fit_3 <- lm(svl ~ color, data = snake_tbl) 
fit_3 %>% coef %>% round(2)
fit_3 %>% residuals() %>% round(2)
```

Wir können jetzt die Koeffizienten ergänzen mit $\beta_0 = 25$ für den Intercept. Weiter ergänzen wir die Koeffizienten für die Farbe und das Level `rot` mit $\beta^{color}_{rot} = 4.83$ und für die Farbe und das Level `blau` mit $\beta^{color}_{blau} = 12.17$. Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  42.50 & 0 \cdot 4.83& 0 \cdot 12.17 \\
  42.50 & 0 \cdot 4.83& 0 \cdot 12.17\\
  42.50 & 1 \cdot 4.83& 0 \cdot 12.17\\
  42.50 & 1 \cdot 4.83& 0 \cdot 12.17\\
  42.50 & 1 \cdot 4.83& 0 \cdot 12.17\\
  42.50 & 0 \cdot 4.83& 1 \cdot 12.17\\
  42.50 & 0 \cdot 4.83& 1 \cdot 12.17\\
  42.50 & 0 \cdot 4.83& 1 \cdot 12.17\\
 \end{pmatrix} +
  \begin{pmatrix}
  -2.50 \\
  +2.50 \\
  -8.33 \\
  +3.67 \\
  +4.67 \\
  +2.33 \\
  +3.33 \\
  -5.67 \\
 \end{pmatrix}
$$

Wir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.

```{r}
c(42.50 + 0*4.83 + 0*-12.17 - 2.50,
  42.50 + 0*4.83 + 0*-12.17 + 2.50,
  42.50 + 1*4.83 + 0*-12.17 - 8.33,
  42.50 + 1*4.83 + 0*-12.17 + 3.67,
  42.50 + 1*4.83 + 0*-12.17 + 4.67,
  42.50 + 0*4.83 + 1*-12.17 + 2.33,
  42.50 + 0*4.83 + 1*-12.17 + 3.33,
  42.50 + 0*4.83 + 1*-12.17 - 5.67) %>% round()
```

Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome $y$ hat.

### Das volle Modell

Im letzten Schritt wollen wir uns einmal das volle Modell anschauen. Wir bauen uns ein Modell mit allen Variablen in dem Datensatz `snake_tbl`. Die Funktion `model.matrix()` gibt uns die Modelmatrix wieder.

```{r}
model.matrix(svl ~ mass + region + color, data = snake_tbl) %>% as_tibble() 
```

In der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte `mass` als kontenuierliche Variable. In der Spalte `regionnord` steht die Dummykodierung für die Variable `region`. Die ersten drei Schlangen kommen nicht aus der Region `nord` und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region `nord` und erhalten daher eine 1 in der Spalte. Die nächsten beiden Spalten sind etwas komplizierter. Die Spalten `colorrot` und `colorblau` geben jeweils an, ob die Schlange das Level `rot` hat oder `blau` oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.

Wir können die Modellmatrix auch mathematisch schreiben und die $y$ Spalte für das Outcome `svl` ergänzen. Eben so ergänzen wir die $\beta$-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  1 & 6 & 0 & 0 & 0 \\
  1 & 8 & 0 & 0 & 0\\
  1 & 5 & 0 & 1 & 0\\
  1 & 7 & 1 & 1 & 0\\
  1 & 9 & 1 & 1 & 0\\
  1 & 11& 1 & 0 & 1\\
  1 & 12& 1 & 0 & 1\\
  1 & 10& 1 & 0 & 1\\
 \end{pmatrix}
 \times
  \begin{pmatrix}
  \beta_0 \\
  \beta_{mass} \\
  \beta^{region}_{nord} \\
  \beta^{color}_{rot} \\
  \beta^{color}_{blau} \\
 \end{pmatrix} +
  \begin{pmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5 \\
  \epsilon_6 \\
  \epsilon_7 \\
  \epsilon_8 \\
 \end{pmatrix}
$$

Jetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion `coef()` um uns die Koeffizienten wiedergeben zu lassen. Die Funktion `residuals()` gibt uns die Residuen der Geraden wieder.

```{r}
fit_4 <- lm(svl ~ mass + region + color, data = snake_tbl) 
fit_4 %>% coef %>% round(2)
fit_4 %>% residuals() %>% round(2)
```

Wir können jetzt die Koeffizienten ergänzen mit $\beta_0 = 25$ für den Intercept. Weiter ergänzen wir die Koeffizienten für `mass` mit $\beta_{mass}=2.5$, für Region und das Level `nord` mit $\beta^{region}_{nord} = 5$, für die Farbe und das Level `rot` mit $\beta^{color}_{rot} = 1.5$ und für die Farbe und das Level `blau` mit $\beta^{color}_{blau} = -2.83$. Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  25 & \phantom{0}6 \cdot 2.5 & 0 \cdot 5 & 0 \cdot 1.5& 0 \cdot -2.83 \\
  25 & \phantom{0}8 \cdot 2.5 & 0 \cdot 5 & 0 \cdot 1.5& 0 \cdot -2.83\\
  25 & \phantom{0}5 \cdot 2.5 & 0 \cdot 5 & 1 \cdot 1.5& 0 \cdot -2.83\\
  25 & \phantom{0}7 \cdot 2.5 & 1 \cdot 5 & 1 \cdot 1.5& 0 \cdot -2.83\\
  25 & \phantom{0}9 \cdot 2.5 & 1 \cdot 5 & 1 \cdot 1.5& 0 \cdot -2.83\\
  25 & 11\cdot 2.5 & 1 \cdot 5 & 0 \cdot 1.5& 1 \cdot -2.83\\
  25 & 12\cdot 2.5 & 1 \cdot 5 & 0 \cdot 1.5& 1 \cdot -2.83\\
  25 & 10\cdot 2.5 & 1 \cdot 5 & 0 \cdot 1.5& 1 \cdot -2.83\\
 \end{pmatrix} +
  \begin{pmatrix}
  \phantom{+}0.00 \\
  \phantom{+}0.00 \\
  \phantom{+}0.00 \\
  +2.00 \\
  -2.00 \\
  +2.33 \\
  +0.83 \\
  -3.17 \\
 \end{pmatrix}
$$

Wir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.

```{r}
c(25 +  6*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,
  25 +  8*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,
  25 +  5*2.5 + 0*5 + 1*1.5 + 0*-2.83 + 0.00,
  25 +  7*2.5 + 1*5 + 1*1.5 + 0*-2.83 + 2.00,
  25 +  9*2.5 + 1*5 + 1*1.5 + 0*-2.83 - 2.00,
  25 + 11*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 2.33,
  25 + 12*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 0.83,
  25 + 10*2.5 + 1*5 + 0*1.5 + 1*-2.83 - 3.17) 
```

Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome $y$ hat. Was haben wir gelernt?

-   In einem Modell gibt es immer ein Faktorlevel weniger als ein Faktor Level hat. Die Information des *alphanumerisch* ersten Levels steckt dann mit in dem Intercept.
-   In einem Modell geht eine kontinuierliche Variable als eine Spalte mit ein.
-   In einem Modell gibt es immer nur eine Spalte für die Residuen und damit nur eine Residue für jede Beobachtung, egal wie viele Variablen ein Modell hat.

## Interpretation von $x$ {#sec-interpret-x}

### Kontinuierliches $x$ {#sec-interpret-x-cont}

```{r}
set.seed(20137937)
cont_tbl <- tibble(x = seq(from = 1, to = 7, by = 1),
                   y = 5 + 1.5 * x + rnorm(length(x), 0, 1))
cont_tbl
```

```{r}
model.matrix(y ~ x, data = cont_tbl)
```

```{r}
lm(y ~ x, data = cont_tbl) %>% 
  tidy() %>% 
  select(term, estimate)
```

```{r}
#| message: false
#| warning: false
#| echo: true
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: foo
#| label: fig-stat-modeling-basic-00

ggplot(cont_tbl, aes(x = x, y = y)) +
  theme_bw() +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 8), breaks = 0:8) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 17), breaks = 0:17) +
  annotate("text", x = 4, y = 9, label = "y ==  4.32 + 1.71 %.% x", parse = TRUE,
           hjust = 0)

```

### Kategorielles $x$ mit 2 Leveln {#sec-interpret-x-cat2}

```{r}
set.seed(20339537)
cat_two_tbl <- tibble(A = rnorm(n = 7, mean = 10, sd = 1),
                      B = rnorm(n = 7, mean = 15, sd = 1)) %>% 
  gather(key = x, value = y) %>% 
  mutate(x = as_factor(x))
cat_two_tbl
```

```{r}
model.matrix(y ~ x, data = cat_two_tbl)
```

```{r}
lm(y ~ x, data = cat_two_tbl) %>% 
  tidy() %>% 
  select(term, estimate)
```

```{r}
#| message: false
#| echo: false
#| tbl-cap: Datensatz mit mehreren Outcomes zu Flöhen auf verschiedenen Tierarten.
#| label: tbl-cat-2

cat_two_tbl %>% 
  group_by(x) %>% 
  summarise(mean = mean(y)) %>% 
  mutate(diff = c(0, diff(mean)),
         sum = cumsum(diff)) %>% 
  mutate(across(where(is.numeric), round, 2)) %>% 
  select(x, mean, sum) %>% 
  set_names(c("Factor x", "Mean of level", "Difference to level A")) %>% 
  kable(align = "c", "pipe")
```

```{r}
#| message: false
#| echo: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: foo
#| label: fig-stat-modeling-basic-01

ggplot(cat_two_tbl, aes(x, y, fill = x)) +
  theme_bw() +
  geom_hline(yintercept = c(10.01, 10.01 + 5.27), 
             color = cbbPalette[c(2:3)], size = 1) +
  geom_dotplot(binaxis = "y", stackdir = "center") +
  scale_fill_okabeito() +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = c(9, 10.01, 11, 12, 13, 14, 15.27, 16, 17)) +
  geom_segment(aes(x = 1, y = 10.01, xend = 2, yend = 10.01+5.27)) +
  annotate("text", x = 1.5, y = 15.7, label = "bar(y)[B] == 10.01 + 5.27", parse = TRUE,
           hjust = 0)

```

### Kategorielles $x$ mit \>2 Leveln {#sec-interpret-x-cat3}

```{r}
set.seed(20339537)
cat_three_tbl <- tibble(A = rnorm(n = 7, mean = 10, sd = 1),
                        B = rnorm(n = 7, mean = 15, sd = 1),
                        C = rnorm(n = 7, mean = 3, sd = 1)) %>% 
  gather(key = x, value = y) %>% 
  mutate(x = as_factor(x))
cat_three_tbl
```

```{r}
model.matrix(y ~ x, data = cat_three_tbl)
```

```{r}
lm(y ~ x, data = cat_three_tbl) %>% 
  tidy() %>% 
  select(term, estimate)
```

```{r}
#| message: false
#| echo: false
#| tbl-cap: Datensatz mit mehreren Outcomes zu Flöhen auf verschiedenen Tierarten.
#| label: tbl-cat-3

cat_three_tbl %>% 
  group_by(x) %>% 
  summarise(mean = mean(y)) %>% 
  mutate(diff = c(0, diff(mean)),
         sum = cumsum(diff)) %>% 
  mutate(across(where(is.numeric), round, 2)) %>% 
  select(x, mean, sum) %>% 
  set_names(c("Factor x", "Mean of level", "Difference to level A")) %>% 
  kable(align = "c", "pipe")
```

```{r}
#| message: false
#| echo: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: foo
#| label: fig-stat-modeling-basic-02

ggplot(cat_three_tbl, aes(x, y, color = x)) +
  theme_bw() +
  geom_jitter(width = 0.1) +
  geom_hline(yintercept = c(10.1, 10.1 + 5.27, 10.1 - 7.41), 
             color = cbbPalette[c(2:4)], size = 1) +
  scale_color_okabeito() +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = c(0, 10.1 - 7.41, 5, 10.1, 10.1 + 5.27), 
                     limits = c(0, NA)) +
  geom_segment(aes(x = 1, y = 10.1, xend = 2, yend = 10.1+5.27), color = "black") +
  geom_segment(aes(x = 1, y = 10.1, xend = 3, yend = 10.1-7.41), color = "black") +
  annotate("text", x = 2.1, y = 14, label = "bar(y)[B] == 10.1 + 5.27", parse = TRUE,
           hjust = 0) +
  annotate("text", x = 2.5, y = 2, label = "bar(y)[C] == 10.1 - 7.41", parse = TRUE,
           hjust = 0)


```

## Adjustierung für Confounder

```{r}
#| message: false

model_tbl <- read_csv2("data/flea_dog_cat_length_weight.csv") %>%
  mutate(animal = as_factor(animal),
         sex = as_factor(sex),
         log_hatch_time = round(log(hatch_time), 2))
```

In der @tbl-model-1 ist der Datensatz `model_tbl` nochmal dargestellt.

```{r}
#| message: false
#| echo: false
#| tbl-cap: Datensatz mit mehreren Outcomes zu Flöhen auf verschiedenen Tierarten.
#| label: tbl-model-1

model_raw_tbl <- model_tbl %>% 
  mutate(animal = as.character(animal),
         sex = as.character(sex))
rbind(head(model_raw_tbl),
      rep("...", times = ncol(model_raw_tbl)),
      tail(model_raw_tbl)) %>% 
  kable(align = "c", "pipe")

```

@fig-stat-modeling-mult-01-1

@fig-stat-modeling-mult-01-2

@fig-stat-modeling-mult-01-3

```{r}
#| message: false
#| echo: false
#| fig-align: center
#| fig-height: 4
#| fig-width: 5
#| label: fig-stat-modeling-mult-01
#| fig-cap: "Darstellung des *counfounder* Effekts anhand des Zusammenhangs der Sprungweite in [cm] und dem Gewicht von Flöhen [mg]."
#| fig-subcap: 
#|   - "jump_length ~ weight"
#|   - "jump_length ~ weight + animal"
#|   - "jump_length ~ weight + animal + sex"
#| layout-nrow: 1
#| column: page

ggplot(model_tbl, aes(x = weight, y = jump_length)) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  geom_point() 

ggplot(model_tbl, aes(x = weight, y = jump_length, color = animal)) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_okabeito() +
  theme_bw() +
  geom_point() +
  labs(color  = "Tierart")

ggplot(model_tbl, aes(x = weight, y = jump_length, color = animal, shape = sex)) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_okabeito() +
  theme_bw() +
  geom_point() +
  labs(color  = "Tierart", shape = "Geschlecht")

```

## Variance inflation factor (VIF)

VIF kann nicht für kategoriale Daten verwendet werden. Statistisch gesehen würde es keinen Sinn machen.

```{r}

model <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)

model <- lm(jump_length ~ sex + weight, data = model_tbl)

vif(model)

check_model(model, check = "vif")

```

## Vergleich von Modellen {#sec-model-basic-compare}

```{r}
fit_1 <- lm(jump_length ~ animal, data = model_tbl)
fit_2 <- lm(jump_length ~ animal + sex, data = model_tbl)
fit_3 <- lm(jump_length ~ animal + sex + weight, data = model_tbl)
fit_4 <- lm(jump_length ~ animal + sex + sex:weight + animal:weight, data = model_tbl)
fit_5 <- lm(log(jump_length) ~ animal + sex, data = model_tbl)
```

Akaike information criterion (AIC)

Bayesian information criterion (BIC)

$$
\Delta_i = AIC_i - AIC_{min}
$$

-   wenn $\Delta_i < 2$, dann gibt es eine deutliche Unterstützung für das $i$-te Modell;;
-   wenn $2 < \Delta_i < 4$, dann gibt es eine starke Unterstützung für das $i$-te Modell;
-   wenn $4 < \Delta_i < 7$, dann gibt es deutlich weniger Unterstützung für das $i$-te Modell;
-   Modelle mit $\Delta_i > 10$ haben im Wesentlichen keine Unterstützung.

$AIC_1 = AIC_{min} = 100$ und $AIC_2$ ist $100,7$. Dann ist $\Delta_2=0,7<2$, so dass es keinen wesentlichen Unterschied zwischen den Modellen gibt. $AIC_1 = AIC_{min} = 100000$ und $AIC_2$ ist $100700$. Dann ist $\Delta_2 = 700 \gg 10$, also gibt es keine Unterstützung für das $2$-te Modell.

Je kleiner das AIC ist, desto besser ist das AIC.

Je kleiner das BIC ist, desto besser ist das BIC.

$$
p_i = \exp\left(\cfrac{-\Delta_i}{2}\right)
$$

Das $p_i$ ist die relative (im Vergleich zu $AIC_{min}$) Wahrscheinlichkeit, dass das $i$-te Modell den AIC minimiert. Zum Beispiel entspricht $\Delta_i = 1.5$ einem $p_i$ von $0.47$ (ziemlich hoch) und ein $\Delta_ = 15$ entspricht einem $p_i =0.0005$ (ziemlich niedrig). Im ersten Fall besteht eine Wahrscheinlichkeit von 47%, dass das $i$-te Modell tatsächlich eine bessere Beschreibung ist als das Modell, das $AIC_{min}$ ergibt, und im zweiten Fall beträgt diese Wahrscheinlichkeit nur 0,05%.

```{r}
model_performance(fit_1) %>% 
  as_tibble() %>% 
  select(AIC, BIC) %>% 
  mutate(across(where(is.numeric), round, 2))
```

```{r}
comp_res <- compare_performance(fit_1, fit_2, fit_3, fit_4, fit_5, rank = TRUE)

comp_res
```

```{r}
plot(comp_res)
```

```{r}
test_vuong(fit_1, fit_2, fit_3, fit_4, fit_5)
```

```{r}
#pacman::p_load(report)

#report(fit_1)

```

War die Transformation sinnvoll?

```{r}
fit_1 <- lm(hatch_time ~ animal + sex, data = model_tbl)
fit_2 <- lm(log_hatch_time ~ animal + sex, data = model_tbl)
```

```{r}
comp_res <- compare_performance(fit_1, fit_2, rank = TRUE)

comp_res
```

## Generalisierung von `lm()` zu `glm()` und `[g]lmer()`

-   Die Funktion `lm()` nutzen wir, wenn das Outcome $y$ einer Normalverteilung folgt.
-   Die Funktion `glm()` nutzen wir, wenn das Outcome $y$ einer *andere* Verteilung folgt.
-   Die Funktion `lmer()` nutzen wir, wenn das Outcome $y$ einer Normalverteilung folgt *und* wir noch einen Block- oder Clusterfaktor vorliegen haben.
-   Die Funktion `glmer()` nutzen wir, wenn das Outcome $y$ einer *andere* Verteilung folgt *und* wir noch einen Block- oder Clusterfaktor vorliegen haben.

## Referenzen {.unnumbered}
