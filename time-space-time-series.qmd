```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, openxlsx)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

# Zeitreihen (eng. *time series*) {#sec-time-series}

*Letzte Änderung am `r format(fs::file_info("time-space-time-series.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"Die Vergangenheit ist geschrieben, aber die Zukunft ist noch nicht in Stein gemeißelt." --- Jean-Luc Picard, Star Trek: The Next Generation*

![](images/caution.png){fig-align="center" width="50%"}

::: callout-important
## Bitte bei der Analyse von Zeitreihen gleich von Anfang an beachten!

Die Analyse von Zeitreihen kann sehr quälend sein, wenn dein [Datumsformat nicht richtig](#sec-time-data-forma) ist. Bitte achte darauf, dass du in der Spalte mit dem Datum immer das gleiche Format vorliegen hast. Dann können wir das Format später richtig transformieren. Danach geht dann alles einfacher...
:::

In diesem Kapitel wollen wir uns mit Zeitreihen (eng. *time series*) beschäftigen. Was ja auch irgendwie zu erwarten war, denn so heißt ja auch das Kapitel hier. Wir haben ganz einfach auf der $x$-Achse einer potenziellen Visualisierung die Zeit dargestellt. Wir wollen dann auswerten, ob es über den zeitlichen Verlauf einen Trend gibt oder wir ein gutes Modell für den Verlauf der Beobachtungen anpassen können. Es kann auch sein, dass wir zwei zeitliche Verläufe miteinander vergleichen wollen. Dabei haben wir dann aber meistens nicht einen super simplen Verlauf, sondern Spitzen oder Täler in den Daten, so dass wir hier die Daten entsprechend glätten (eng. *to smooth*) müssen.

Was sollst du lesen, wenn du mehr wissen willst als hier in dem Kapitel steht? Ich würde dir [Forecasting: Principles and Practice](https://otexts.com/fpp3/) als erstes empfehlen. Du hast hier sogar YouTube Videos für die wichtigsten Inhalte. Dann würde ich [STAT 510: Applied Time Series Analysis](https://online.stat.psu.edu/stat510/) als umfangreichen, theoretischen Hintergrund empfehlen. R Pakete gibt es dann mit `{tktime}`, `{fable}` und `{modeltime}` einige zur Auswahl. Ich werde hier aber auch alle drei pakete einmal vorstellen und diskutieren.

Bei Zeitreihen geht es also zum einen um die Vorhersage zukünftiger Ereignisse anhand schon gemessener Werte der Zeitreihe oder aber um die Darstellung und den Vergleich von zeitlichen Verläufen. Wir werden uns beide Fälle einmal anschauen. Deshalb sind es hier auch recht viele Beispiele geworden.

Wenn du noch mehr lesen willst, dann kann ich dir folgende Literatur empfehlen. @robert2006time liefert eine gute Übersicht über die Anwendung in R, ist aber schon etwas älter. Das Gleiche gilt dann auch für das Buch von @chan2008time und @cowpertwait2009introductory. Dennoch bilden alle drei Bücher die Grundlagen der Analysen von Zeitreihen super ab. Für eine Abschlussarbeit sollten die Quellen also allemal reichen.

Wenn es um Zeitreihen geht, dann ist die Formatierung der Spalte mit dem Datum eigentlich so ziemlich das aufwendigste. In dem Kapitel [Zeit und Datum](#sec-time-date) findest du dann nochmal mehr Informationen dazu. Achte bitte darauf, dass du eine einheitlich formatierte Datumsspalte hast, die sich nicht im Laufe der Zeilen ändert. Wenn das der Fall ist, dann musst du meist händisch nochmal die Daten anpassen und das ist meistens sehr aufwendig.

Wenn es um Daten geht, dann gibt es natürlich eine Reihe von möglichen Quellen. Wenn es sehr viele Daten seinen sollen, die meistens einen zeitlichen Bezug haben, dann empfehle ich die Webseite [Our World in Data](https://ourworldindata.org/). Dort gibt es so viele zeitliche Verläufe, da eigentlich alles dort als zeitlicher Verlauf dargestellt wird. Schau doch einfach dort mal rein.

::: {.callout-tip collapse="true"}
## Weitere Tutorien für die Analyse von Zeitreihen

Wir immer kann ein Kapitel hier nicht das umfangreiche Analysieren von Zeitreihen abarbeiten. Daher auch wie in anderen Kapiteln schon eine Liste von Literatur und Links, die mich für dieses Kapitel hier inspiriert haben. Nicht alles habe ich genutzt, aber vielleicht ist für dich was dabei, was dir dann mehr konkret hilft als dieses Kapitel.

-   [Forecasting: Principles and Practice](https://otexts.com/fpp2/) -- Wenn ich keine Zeit hätte mich durch sehr viele Tutorien zu arbeiten, dann würde ich vermutlich mit diesem Online-Buch starten. Es ist alles drin was man braucht um die Zeitreihenanalyse tiefer zu verstehen, als dass ich es hier aufarbeiten kann. Wenn du dann noch das R Paket `{tktime}` nutzt, passt dann wieder alles.
-   [STAT 510: Applied Time Series Analysis](https://online.stat.psu.edu/stat510/) -- Ein umfangreicher Kurs der Penn State - Eberly College of Science. Hier ist dann der R Anteil sehr eingeschränkt bzw. schon etwas älter aber dafür sind die Erklärungen umfangreicher als in diesem Kapitel. Es lohnt sich also nochmal dort für weitere Beispiele einmal reinzuschauen.
-   [Welcome to a Little Book of R for Time Series!](https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/) -- Du findest hier eine sehr gute Übersicht über die Möglichkeiten einer Zeitreihenanalyse mit der Möglichkeit Vorhersagen durchzuführen. Teilweise sind die Funktionen etwas veraltet, ich würde da eher das R Paket `{timetk}` empfehlen. Der Text um die Funktionen herum ist immer noch lesenswert.
-   [timetk for R](https://business-science.github.io/timetk/) -- Hier hast du das aktuelle R Paket, welches ich auch in diesem Kapitel teilweise vorstelle. Leider gibt es hier aber nicht sehr viel Erklärtext, daher musst du dich dann auf andere Quellen verlassen. Die Funktionen und die Anwendbarkeit ist aber sehr schön.
-   [14 Time Series Analysis](https://rc2e.com/timeseriesanalysis) -- Du findest hier neben der klassischen Analyse von Zeitreihen noch Informationen zu anderen Möglichkeiten der Analyse von Zeitreihen. Eingebettet ist das Kapitel zu Zeitreihen in ein umfangreiches Buch über die Analyse von Daten und der Programmierung.
-   [Analysing Time Series Data -- Modelling, Forecasting and Data Formatting in R](https://ourcodingclub.github.io/tutorials/time/) -- Du findest hier nochmal ein gutes und in sich kompaktes Tutorium zu der Zeitreihenanalyse. Hier findest du dann auch mehr erklärenden Text. Auch werden die Datumsformate nochmal genauer auseinander genommen und erklärt. Für den Einstieg sicherlich eine gute Quelle.
:::

Ich habe mich auch wieder bemüht ein Beispiel für eine Zeitreihe zu finden, die ich als Abbildung einmal zerforschen kann. Damit sich hier nicht so viel verdoppelt, schaue auch einmal gerne in das Kapitel zur Visualisierung von Daten rein, dort sammle ich alle Beispiele zum Zerforschen. Hier also eine Abbildung, bei der es um eine Zeitreihe und der Trächtigkeit von Kühen geht.

::: {.callout-caution collapse="true"}
## Zerforschen: Zeitlicher Verlauf von einem Laborwert zur Trächtigkeit

![](images/caution.png){fig-align="center" width="50%"}

{{< include zerforschen/zerforschen-time-series-cow.qmd >}}
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, janitor, see, readxl,
               xgboost, tidymodels, modeltime, forecast,
               lubridate, plotly, zoo, timetk, xts,
               corrplot, GGally, conflicted)
conflicts_prefer(dplyr::filter)
conflicts_prefer(magrittr::set_names)
conflicts_prefer(plyr::mutate)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Den ersten Datensatz, den wir uns anschauen wollen, ist in einer CSH-Datei abgespeichert, die ich schon in Excel exportiert habe. Eine CSH-Datei ist ein Datenformat aus Adobe Photoshop und eigentlich nichts anders als eine Information über eine Bilddatei. Wir haben aber hier nicht Pixel oder aber ein Foto vorliegen, sondern das Bild wurde schon in einen numerischen Wert pro Bild weiter verarbeitet. Das hier so ausgedachte Experiment war ein Dronenüberflug über eine Wiese und einem Feld in Uelzen. Dabei wurden Fotos gemacht und es sollten verschiedene Grünlandwerte aus den Fotos berechnet werden. Wir haben aber den Überflug nicht an einem einzigen Tag gemacht, sondern gleich an mehreren über das Jahr verteilt. Das ist jetzt auch dann gleich unsere Zeitreihe. Jetzt können wir uns Fragen, ob es einen Unterschied zwischen den Messwerten der beiden Dronenüberflüge gibt. Wir lesen wie immer erstmal die Daten ein.

```{r}
#| message: false
#| warning: false

csh_tbl <- read_excel("data/csh_data.xlsx") %>% 
  clean_names() %>% 
  mutate_if(is.numeric, round, 2)
```

In der @tbl-time-csh siehst du einen Ausschnitt aus den `r nrow(csh_tbl)` Überflügen. Hier wurden die Daten natürlich schon zusammengefasst. Aus jedem Bild wurde dann ein Wert für zum Beispiel `kg_tm_ha` berechnet. Hier interessiert uns aber nicht die Berechnungsart. Wir wollen jetzt gleich mit den Daten weiterarbeiten. Wie immer ist das Beispiel so semi logisch, hier geht es aber auch eher um die Anwendung der Methoden.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-time-csh
#| tbl-cap: "Auszug aus den Daten der CSH-Datei von Dronenüberflügen über eine Wiese und einem Feld in Uelzen. Die Daten sind abgeändert von den Orginaldaten."

rbind(head(csh_tbl, n = 4),
      rep("...", times = ncol(csh_tbl)),
      tail(csh_tbl, n = 4)) %>% 
  kable(align = "c", "pipe")
```

Im nächsten Datensatz schauen wir uns einmal die Daten von vier Loggern an. Hier haben wir mehr oder minder einfach jeweils einen Temperaturlogger an den jeweiligen Seiten unseres Folientunnels geworfen und dann nochmal einen Logger einfach so auf das Feld gelegt. In den Folientunneln haben wir dann Salat hochgezogen. Wir betrachten jetzt hier nur das Freiland, sonst wird es einfach zu viel an Daten.

```{r}
#| message: false
#| warning: false

salad_tbl <- read_excel("data/temperatur_salad.xlsx") %>% 
  clean_names() %>% 
  mutate_if(is.numeric, round, 2) %>% 
  select(datum, uhrzeit, matches("freiland"))
```

In der @tbl-time-temp siehst du einmal die `r nrow(salad_tbl)` automatisch erfassten Messungen der Temperatur pro Tag und dann Stunde. Hier müssen wir dann einmal schauen, wie wir die Daten dann sinnvoll zusammenfassen. Es sind wirklich viele Datenpunkte. Aber gut wir schauen uns die Daten erstmal an und entscheiden dann später weiter. Wir sehen aber schon, dass wir die Daten nochmal bearbeiten müssen, denn irgendwas stimmt mit der Uhrzeitspalte und dem Datum nicht. Dazu dann aber gleich mehr im Abschnitt zum Datumsformat.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-time-temp
#| tbl-cap: "Auszug aus den Daten zur auromatischen Erfassung von Klimadaten im Feld für Kopfsalat."

raw_salad_tbl <- salad_tbl %>% 
  mutate_all(as.character)

rbind(head(raw_salad_tbl , n = 4),
      rep("...", times = ncol(raw_salad_tbl)),
      tail(raw_salad_tbl, n = 4)) %>% 
  kable(align = "c", "pipe")
```

Am Ende wollen wir uns dann nochmal Daten einer Wetterstation in Hagebüchen an. Auch hier haben wir wieder sehr viele Daten vorliegen und wir müssen uns überlegen, welche der Daten wir nutzen wollen. Aus Gründen der Machbarkeit wähle ich die Spalte `temp_boden_durch` und `solar_mv` aus, die wir uns dann später anschauen wollen. Sonst wird mir das zu groß und unübersichtlich.

```{r}
#| message: false
#| warning: false

station_tbl <- read_excel("data/Wetterstation_Hagebüchen.xlsx") %>% 
  clean_names() %>% 
  select(datum_uhrzeit, temp_boden_durch, solar_mv) %>% 
  mutate_if(is.numeric, round, 2)
```

Auch hier haben wir in der @tbl-time-station gut `r nrow(station_tbl)` einzelne Messungen vorliegen. Das ist dann auch unserer größter Datensatz von Klimadaten. Wir werden die Daten dann aber sehr anschaulich einmal in einer Übersicht darstellen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-time-station
#| tbl-cap: "Auszug aus den Daten zur Wetterstation in Hagebüchen."

raw_station_tbl <- station_tbl %>% 
  mutate_all(as.character)

rbind(head(raw_station_tbl , n = 4),
      rep("...", times = ncol(raw_station_tbl)),
      tail(raw_station_tbl, n = 4)) %>% 
  kable(align = "c", "pipe")
```

Damit hätten wir uns eine Reihe von landwirtschaftlichen Datensätzen angeschaut. Sicherlich gibt es noch mehr, aber diese Auswahl erlaubt es uns gleich einmal die häufigsten Fragen rund um Zeitreihen in den Agrarwissenschaften einmal anzuschauen. Bitte beachte, dass es natürlich noch andere Formen von Zeitreihen und damit Datensätzen gibt. Deshalb gleich noch ein Datensatz, der künstlich ist und damit eine eher perfekte Zeitreihe repräsentiert. In dem folgenden Kasten findest du darüber hinaus nochmal eine Anregung zu Klimadaten aus deiner Region.

::: callout-tip
## Mehr Wetter- und Klimadaten aus deiner Region!

Du kannst gerne die [entgeltfreien Informationen auf der DWD-Website](https://www.dwd-shop.de/index.php/default/kostenfreie-informationen.html) nutzen um mehr Informationen zu dem Klima und deiner Region zu erhalten. Wir finden dort auf der Seite die [Klimadaten für Deutschland](https://www.dwd.de/DE/leistungen/klimadatendeutschland/klimadatendeutschland.html#buehneTop) und natürlich auch die Daten für Münster/Osnabrück. Sie dazu auch [Isoplethendiagramm für Münster & Osnabrück](https://jkruppa.github.io/application/example-analysis-01.html) im *Skript zu beispielhaften Anwendung*. Ich habe mir dort flux die Tageswerte runtergeladen und noch ein wenig den Header der txt-Datei angepasst. Du findest die Datei [`day_values_osnabrueck.txt`](https://github.com/jkruppa/jkruppa.github.io/tree/master/data) wie immer auf meiner GitHub Seite. Du musst dir für andere Orte die Daten nur entsprechend zusammenbauen. Am Ende brauchen wir noch die [Informationen zu den Tages- und Monatswerten](https://www.dwd.de/DE/leistungen/klimadatendeutschland/beschreibung_tagesmonatswerte.html) damit wir auch verstehen, was wir uns da von der DWD runtergeladen haben.
:::

Der folgende Datensatz zu der Milchleistung von Kühen stammt aus dem Tutorium [Analysing Time Series Data -- Modelling, Forecasting and Data Formatting in R](https://ourcodingclub.github.io/tutorials/time/). Wir haben hier ein idealisierten Datensatz vorliegen, so dass wir uns nicht mit dem Datumsformat quälen müssen. Der Datensatz wurde auch für die Analysen künstlich erstellt. Daher ist die milchleistung auch nicht als echt anzusehen. Wir haben es hier im Prinzip mit simulierten Daten zu tun.

```{r}
#| message: false
#| warning: false
milk_tbl <- read_csv("data/monthly_milk.csv") 
```

In der @tbl-time-milk siehst du nochmal einen Auszug aus den Milchdaten. An jedem Tag haben wir die Milchleistung für eine Kuh aufgetragen. Ich würde hier davon ausgehen, dass es sich um die mittlere Leistung handelt. In Wirklichkeit sind die Daten vermutlich etwas komplizierter und wir haben nicht nur eine Leistungsbewertung pro Tag für eine Kuh. Aber für diese Übersicht soll es reichen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-time-milk
#| tbl-cap: "Auszug aus den Daten zu der Milchleistung von Kühen."

raw_milk_tbl <- milk_tbl %>% 
  mutate_all(as.character)

rbind(head(raw_milk_tbl , n = 4),
      rep("...", times = ncol(raw_milk_tbl)),
      tail(raw_milk_tbl, n = 4)) %>% 
  kable(align = "c", "pipe")
```

## Das Datumsformat {#sec-time-data-format}

Wenn wir von Zeitreihen sprechen dann sprechen wir auch von dem Datumsformat. Eine Zeitreihe ohne eine richtig formatierte Datumsspalte macht ja auch überhaupt keinen Sinn. Es ist eigentlich immer einer ewige Qual Daten in das richtige Zeitformat zu kriegen. Deshalb hier vorab einmal die folgende Abbildung, die nochmal die Wirrnisse des Datumsformat gut aufzeigt.

![Quelle: https://xkcd.com/](https://imgs.xkcd.com/comics/iso_8601.png){fig-align="center" width="70%"}

Wichtig ist, dass wir das richtige Datumsformat haben. Siehe bitte dazu auch das Kapitel [Zeit und Datum](#sec-time-date). Das einzig richtige Datumsformat ist und bleibt eben `Jahr-Monat-Tag`. Häufig ist eben dann doch anders, so dass wir uns etwas strecken müssen um unser Format in das richtige Format zu überführen. Bitte beachte aber, dass du auf jeden Fall einheitlich dein Datum einträgst. Am besten auch immer zusammen mit dem Jahr, dass macht vieles einfacher. Wie immer gibt es auch noch das Tutorium zu [Date Formats in R](https://www.r-bloggers.com/2013/08/date-formats-in-r/) und natürlich das R Paket `{lubridate}` mit dem Einstieg [Do more with dates and times in R](https://lubridate.tidyverse.org/articles/lubridate.html).

Wir werden uns jetzt einmal am Beispiel die Transformation der Datumsformate in den jeweiligen Daten anschauen. Je nach Datensatz müssen wir da mehr oder weniger machen. Auch hier, wenn du weniger Arbeit möchtest, dann achte auf eine einheitliche Form der Datumsangabe

::: callout-important
## Konvertierung von verschiedenen Datumsformaten in R

Das R Paket `{timetk}` liefert dankenswerterweise Funktionen für die Konvertierung von verschiedenen Zeitformaten in R. Deshalb schaue einmal in die Hilfeseite [Time Series Class Conversion -- Between ts, xts, zoo, and tbl](https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html#introduction) und dann dort speziell der Abschnitt [Conversion Methods](https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html#conversion-methods). Leider ist Zeit in R wirklich relativ.
:::

### Die CSH-Daten

Das Datum in den CSH-Daten leidet unter zwei Besonderheiten. Zum einen fehlt das Jahr und zum anderen die Null vor der Zahl. Wir haben nämlich für den 28. April die Datumsangabe `428` in der Spalte `day`. Das hat zur Folge, dass Excel die Spalte als Zahl erkennt und keine vorangestellten Nullen erlaubt. Wir brauchen aber einen String und den Monat als zweistellig mit `04` für den Monat April. Deshalb nutzen wir die Funktion `str_pad()` um eine `0` an die linke Seite zu kleben, wenn der Wert in der Spalte kleiner als vier Zeichen lang ist. Somit würde der 1. Oktober mit `1001` so bleiben, aber der 1. September mit `901` zu `0901`. Dann nutzen wir die Funktion `as.Date()` um aus unserem Sting dann ein Datum zu machen. Das Format ist hier dann `%m%d` und somit Monat und Tag ohne ein Trennzeichen.

```{r}
csh_tbl <- csh_tbl %>% 
  mutate(day = as.Date(str_pad(day, 4, pad = "0", side = "left"), format = "%m%d"))
```

Und dann erhalten wir auch schon folgenden Datensatz mit dem korrekten Datumsformat mit dem wir dann weiterarbeiten werden.

```{r}
csh_tbl %>% 
  head(4)
```

Wie du siehst, wird dann automatisch das aktuelle Jahr gesetzt. Das heißt, da ich dieses Text hier im Jahr `r year(Sys.Date())` schreibe, erscheint natürlich auch eine `r year(Sys.Date())` vor dem Monat und Tag. Hier musst du dann schauen, ob das Jahr wirklich von Interesse ist oder du es dann später nochmal anpasst. Wir lassen jetzt erstmal alles so stehen. Es ist immer einfacher das Datum dann sauber in Excel zu setzen, als sich dann hier nochmal einen Abzubrechen, denn du bist ja keine Informatiker der eine generelle Lösung sucht sondern hast ja nur einen Datensatz vorliegen. Das wäre jedenfalls mein Tipp um es schneller hinzukriegen.

### Die Salatdaten

Bei den Salatdaten haben wir ein anderes Problem vorliegen. Wir haben einmal eine Datumsspalte und dann noch eine Spalte mit der Uhrzeit. Da wir aber keine reine Uhrzeitspalte haben können, wurde noch das Datum `1899-12-31` als default ergänzt. Das macht natürlich so überhaupt keinen Sinn. Deshalb müssen wir dann einmal die Uhrzeit als korrektes Uhrzeit-Format umwandeln und dann die Spalte `datum` in ein Datum-Format. Dann können wir die beiden Spalten *addieren* und schon haben wir eine Datumsspalte mit der entsprechenden Uhrzeit.

```{r}
salad_datetime_tbl <- salad_tbl %>% 
  mutate(uhrzeit = format(uhrzeit, format = "%H:%M:%S"),
         datum = format(datum, format = "%Y-%m-%d"),
         datum = ymd(datum) + hms(uhrzeit)) %>% 
  select(-uhrzeit) 
```

Im Folgenden siehst du einmal das Ergebnis unserer Umfaormung. Wir haben jetzt eine Spalte mit dem Datum und der Uhrzeit vorliegen, so wir das auch wollen und dann auch abbilden können.

```{r}
salad_datetime_tbl %>% 
  head(4)
```

Da wir später noch für die Visualisierung die einzelnen Spalten `freiland_messw` bis `freiland_max` einmal darstellen wollen, bauen wir uns nochmal mit `pivot_longer()` einen entsprechenden Datensatz, der uns diese Art der Visualiserung dann auch möglich macht.

```{r}
salad_long_tbl <- salad_datetime_tbl %>% 
  pivot_longer(freiland_messw:last_col(),
               names_sep = "_",
               names_to = c("location", "type"),
               values_to = "temp") 
```

Wie du nun siehst, haben wir nur noch eine Spalte `temp` mit unseren Messwerten der Temperatur. Der Rest an Informationen ist dann alles in anderen Spalten untergebracht. Mit diesem Datensatz können wir dann auch in `{ggplot}` gut arbeiten.

```{r}
salad_long_tbl %>% 
  head(4)
```

Wie du aber auch schon hier siehst sind die Werte für den Messwert, den Minimalwert und den Maximalwert faktisch identisch. Diese geringe Abweichung werden wir dann auch nur schwerlich schön in einer Abbildung zeigen können. Ich würde dann die Min/Max-Werte rausschmeißen und mich nur auf die Messwerte in diesem Fall konzentrieren.

### Die Wetterstationsdaten

Zu guter Letzt noch die Daten zu der Wetterstation. Hier haben wir das Problem, dass wir dann aus der Spalte mit den Informationen zu dem Datum und der Zeit noch einzelne Informationen extrahieren wollen. Wir wollen die Stunde oder den Tag haben. Dafür nutzen wir nun Funktionen wie `day()` oder `month()` aus dem R Paket `{lubridate}` um uns diese Informationen zu extrahieren. Wir können uns so auch die Uhrzeit wieder zusammenbauen, indem wir die Stunde, Minute und dann die Sekunden herausziehen.

```{r}
station_tbl <- station_tbl %>% 
  mutate(datum_uhrzeit = as_datetime(datum_uhrzeit),
         month = month(datum_uhrzeit),
         day = day(datum_uhrzeit),
         hour = hour(datum_uhrzeit),
         minute = minute(datum_uhrzeit),
         second = second(datum_uhrzeit),
         format_hour = paste(hour, minute, second, sep = ":"))
```

Damit haben wir auch den letzten Datensatz so umgebaut, dass wir eine Spalte haben in der das Datum sauber kodiert ist. Sonst macht ja eine Zeitreihe keinen Sinn, wenn die Zeit nicht stimmt. Damit können wir uns dann auch der Visualisierung der Wetterstationsdaten zuwenden.

## Visualisierung

Wie auch bei anderen Analysen ist die Visualisierung von Zeitreihen das Wichtigste. Da wir im Besonderen bei Zeitreihen eben meistens keine Verläufe nur in den reinen Daten sehen können. Wir haben einfach zu viele Datenpunkte vorliegen. Meistens hilft uns dann auch eine Darstellung *aller* Datenpunkte auch nicht weiter, so dass wir uns entscheiden eine Glättung (eng. *smoother*) durchzuführen, damit wir überhaupt etwas sehen. Daher gehen wir hier einmal verschiedene Probleme an den Datensätzen durch. Vorab stelle ich dann aber nochmal das R Paket `{plotly}` vor, was es ermöglicht semi-interaktive Abbildungen zu erstellen. Wir haben mit `{plotly}` die Möglichkeit direkt Werte für die Punkte abzulesen. Das können wir mit einer fixen Abbildung in `{ggplot}` nicht.

### Das R Paket `{plotly}`

![](images/caution.png){fig-align="center" width="50%"}

Hilfeseite [{plotly} R Open Source Graphing Library](https://plotly.com/r/).

[Getting Started with {plotly} in {ggplot2}](https://plotly.com/ggplot2/getting-started/)

### Die CSH Daten

Die CSH-Daten stellen ja die Messung eines Dronenüberflugs von zwei Parzellen einmal in Uelzen und einer Kontrollparzelle dar. In der @fig-time-csh-1-1 siehst du einmal die beobachteten `g_tm_plot`-Werte für jeden der Messtage getrennt nach Parzelle aufgetragen. Hier sieht man auf den ersten Blick keine Unterschied. Deshalb hilft es immer einmal eine geglättete Funktion durch die Punkte zu legen. Wir nutzen dazu die Funktion `geom_smooth()` und erhalten die @fig-time-csh-1-2. Hier sehen wir schon, dass es einen Unterschied zwischen den beiden Parzellen gibt. Wir sind also nicht an den einzelnen Punkten interessiert sondern eigentlich an der Differenz zwischen den beiden Geraden. Wir wollen also die Fläche zwischen den beiden Linien berechnen und so feststellen wie groß der Unterschied zwischen den Messungen an den beiden Parzellen ist.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-csh-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Datensatz der `g_tm_plot`-Werte in Abhängigkeit von dem Tag der Messung für die beiden Parzellen einmal in Uelzen und der Kontrollwiese. Ein Unterschied lsäät sich nur durch die Anpassung der beiden Linien durch die Punkte erkennen."
#| fig-subcap: 
#|   - "Darstellung der Beobachtungen"
#|   - "Mit `stat_smooth`-Funktion"
#| layout-nrow: 1
#| column: page

csh_tbl %>% 
  ggplot(aes(day, g_tm_plot, color = parzelle)) +
  theme_bw() +
  geom_point() +
  scale_color_okabeito() 

csh_tbl %>% 
  ggplot(aes(day, g_tm_plot, color = parzelle)) +
  theme_bw() +
  geom_point() +
  stat_smooth(se = FALSE) +
  scale_color_okabeito() 

```

In dem nächsten Abschnitt wollen wir dann einmal die Fläche zwischen den Linien bestimmen und schauen, ob wir hier wirklich einen Unterschied vorliegen haben. Wenn du mehr Linien oder Gruppen hast, dann musst du dann immer die Fläche zwischen zwei Linien berechnen bist du alle Kombinationen durch hast.

### Die Salat Daten

Die Salatdaten schauen wir uns jetzt einmal in der @fig-time-salad-1 als statische Abbildungen in `{ggplot}` an. Besonders in der @fig-time-salad-1-1 siehtst du dann wegen den geringen Unterschieden der Temperaturen fast nichts auf der Abbildung. Dafür hilft es dann auch in der @fig-time-salad-1-2 einmal die Temperaturen aufzuteilen. Jetzt haben wir alle Temperaturen einmal als Vergleich vorliegen. Aber auch hier können wir schlecht die Werte an einem Datum ablesen und direkt vergleichen. Hier hilft dann gleich `{plotly}` weiter.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-salad-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Verlauf der Temperaturen auf dem Freilandfeld für Kopfsalat. Es wurden die minimalen, maximalen und ein durchschnittlicher Temperaturwert gemessen. Die Werte leiegn alle sehr nahe beieinander, so dass eine gute Darstellung mit einem statischen `{ggplot}` sehr schwer ist."
#| fig-subcap: 
#|   - "Alle drei Temperaturen in einer Abbildung."
#|   - "Die Temperaturen in drei separaten Abbildungen."
#| layout-nrow: 1
#| column: page
p_loc <- salad_long_tbl %>% 
  ggplot(aes(datum, temp, color = type)) +
  theme_bw() +
  scale_color_okabeito() +
  geom_line() +
  facet_wrap(~ location) +
  theme(legend.position = "none")
p_loc

p_type <- salad_long_tbl %>% 
  ggplot(aes(datum, temp, color = type)) +
  theme_bw() +
  scale_color_okabeito() +
  geom_line() +
  facet_wrap(~ type, ncol = 1) +
  theme(legend.position = "none")
p_type
```

In der @fig-time-salad-2 siehst du einmal die Version der ersten Abbildung in `{plotly}` dargestellt. Auf den ersten Blick ist alles gleich und auch wenn du die Abbildung ausdruckst oder in Word einfügst, wirst du nichts großartig anders machen können. Als Webseite oder im RStudio geht dann mehr. Du kannst jetzt mit der Maus über die Abbildung gleiten und dann werden dir die Werte an dem jeweiligen Punkt angezeigt. Das tolle ist, dass wir mit der Funktion `ggplotly()` viele Abbildungen aus `{ggplot}` direkt als `{plotly}` Abbildung wiedergeben lassen können. Wie immer gilt auch hier, dass die Hilfeseite [Getting Started with {plotly} in ggplot2](https://plotly.com/ggplot2/getting-started/) einem enorm weiterhilft.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-salad-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Die Darstellung der Temperaturverläufe in `{plotly}`. Einzelne Werte können jetzt angezeigt werden und somit auch verglichen werden. Besonders der Button *Compare data on hover* ist hier sehr nützlich."
ggplotly(p_loc)
```

In der @fig-time_plotly_1 siehst du nochmal den Button *Compare data on hover* in Aktion. Du kannst dann direkt die drei Punkte miteinander vergleichen auch wenn die Punkte in der Abbildung schlecht auseinander zu halten sind. Wir können uns dann damit die Werte auf der $y$-Achse für jeden Zeitpunkt anzeigen lassen. Das "Hovern" über die Werte macht die visuelle Auswertung sehr viel einfacher als eine statische Abbildung.

![Wenn du oben rechts auf die beiden doppelten Pfeile klickst, dann aktivierst du *Compare on hover*, was dir ermöglicht direkt die Werte von $y$ and einem Zeitpunkt zu vergleichen.](images/timeseries/plotly_1.png){#fig-time_plotly_1 fig-align="center" width="100%"}

### Die Wetterstationsdaten

Die Wetterstationsdaten können wir uns natürlich aucb so anschauen wie die Daten aus den Loggern bei den Salatdaten. Das habe ich dann auch einmal in der @fig-time-station-1 gemacht. Wir sehen in der Abbildung den Temperaturverlauf von April bis Ende Oktober. Das Problem ist auch wieder hier, dass wir einzelne Werte für ein Datum sehr schlecht ablesen können. Auch hier hilft dann `{plotly}` weiter, da können wir dann schön die Werte ablesen. Das Ziel ist es hier aber nicht eine einfache Scatterabbildung zu bauen sondern gleich nochmal ein 2D Konturplot. Aber fangen wir erstmal mit der Übersicht an.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-station-1
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "Verlauf der durchschnittlichen Temperatur an der Wetterstation von April bis Ende September."
p <- station_tbl %>% 
  ggplot(aes(datum_uhrzeit, temp_boden_durch)) +
  theme_bw() +
  geom_line() 
p
```

Dann können wir natürlich auch wieder unsere statische Abbildung einmal in `{plotly}` umwandeln und uns die einzelnen Werte anschauen. Wir haben hier aber eher weniger Informationen, da der lineare Ablauf doch recht schwer über die Monate zu vergleichen ist. Viel besser wären da die Tage für jeden Monat nebeneinander. Oder aber wir schauen uns einmal die Temperatur für jeden Tag an. Wir haben ja auch die Uhrzeiten vorliegen.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-station-2
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "Die Darstellung der durchschnittlichen Temperatur der Wetterstation in `{plotly}`. Einzelne Werte können jetzt angezeigt werden und somit auch verglichen werden."
ggplotly(p)
```

Wir können uns jetzt in @fig-weather-station-hage die drei Konturplots ansehen. Wichtig ist natürlich hier, dass wir vorher die Tage und den Monat aus dem Datum extrahiert haben. Jetzt geht es aber los mit dem Bauen der Konturplots. Wir mussten noch das Spektrum der Farben einmal drehen, damit es auch mit den Temperaturfarben passt und wir haben noch ein paar Hilfslinien mit eingezeichnet. Sie dazu auch meine Auswertung zum [Isoplethendiagramm für Münster & Osnabrück](https://jkruppa.github.io/application/example-analysis-01.html) im *Skript zu beispielhaften Anwendung* als ein anderes Beispiel mit DWD Daten. Wenn du die Daten aus deiner Region runterlädst, kannst du dir auch ähnliche Abbildungen bauen.

Im Folgenden spiele ich mit den Funktionen `geom_contour_filled()` und `geom_contour()` rum um zum einen die Flächen und dann die Ränder des Isoplethendiagramms zu erhalten. Die Färbung ergibt sich dann aus der Funktion `scale_fill_brewer()`. Da wir hier exakt dreizehn Farben zu Verfügung haben, habe ich dann auch entschieden dreizehn Konturen zu zeichnen. Sonst musst du mehr Farben definieren, damit du auch mehr Flächen einfärben kannst. Teilweise musst du hier etwas mit den Optionen spielen, bis du bei deinen Daten dann eine gute Einteilung der Farben gefunden hast. Hier helfen dir dann die Optionen `binwidth` und `bins` weiter. Darüber hinaus habe ich mich auch entschieden hier mit einem Template in `{ggplot}` zu arbeiten, damit ich nicht so viel Code produziere. Ich baue mir im Prinzip einmal einen leeren Plot ohne die Funktion `aes()`. Die Definition was auf die $x$-Achse kommt und was auf die $y$-Achse mache ich dann später.

```{r}
#| message: false
#| echo: true
#| warning: false

p <- ggplot(station_tbl) +
  theme_minimal() +
  geom_contour_filled(bins = 13) +
  scale_fill_brewer(palette = "Spectral", direction = -1) +
  scale_x_continuous(breaks = 1:12) +
  geom_vline(xintercept = 4:9, alpha = 0.9, linetype = 2) +
  geom_hline(yintercept = c(4, 8, 12, 16, 20, 24), 
             alpha = 0.9, linetype = 2) +
  labs(x = "Monat", y = "Stunde", fill = "Temperatur [°C]")

```

Wir können mit dem Operator `%+%` zu einem bestehenden `{ggplot}` neue Daten hinzufügen. Dann können wir auch wie gewohnt neue Optionen anpassen. Deshalb dann einmal das Template zusammen mit der Temperatur als Kontur aufgeteilt nach Monat und Stunde sowie Monat und Tag. Dann habe ich mir noch die Leistung der Sonne über den Monat und der Stunde anzeigen lassen. Je nach Fragestellung kommt es dann eben auf die Abbildung drauf an. Bei Zeitreihen haben wir mit dem Konturplot noch eine weitere Möglichkeit Daten spannend und aufschlussreich darzustellen.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-weather-station-hage
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Konturplot der verschiedenen Temperaturen der Wetterstation in Hagebüchen in den Monaten April bis Mitte September. Die Temperaturen wurden jede Stunde einmal erfasst. Dargestellt sind die Durchschnittstemperaturen."
#| fig-subcap: 
#|   - "Durchschnittstemperaturen der Wetterstation über den Tag."
#|   - "Durchschnittstemperaturen der Wetterstation über den Monat."
#|   - "Solare Leistung über den Tag."
#| layout-nrow: 1
#| column: page

p %+%
  aes(month, hour, z = temp_boden_durch) +
  geom_contour(binwidth = 2, color = "black") +
  scale_y_continuous(limits = c(1, 24), breaks = c(4, 8, 12, 16, 20, 24)) +
  labs(x = "Monat", y = "Stunde", fill = "Temperatur [°C]")

p %+%
  aes(month, day, z = temp_boden_durch) +
  geom_contour(binwidth = 2, color = "black") +
  scale_y_continuous(limits = c(1, 30), breaks = c(5, 10, 15, 10, 25, 30)) +
  labs(x = "Monat", y = "Tag", fill = "Temperatur [°C]")

p %+%
  aes(month, hour, z = solar_mv) +
  scale_y_continuous(limits = c(1, 24), breaks = c(4, 8, 12, 16, 20, 24)) +
  labs(x = "Monat", y = "Stunde", fill = "Solar [MV]")

```

### Die Milchdaten

Um die Milchdaten in der @fig-time-milk-1 darzustellen nutzen wir die Funktion `plot_time_series()` aus dem R Paket `{timetk}`. Eigentlich ist es ein Zusammenschluss von `{ggplot}` und `{plotly}`. Wenn du die Option `.interactive = TRUE` wie ich setzt, dann bekommst du einen semi-interaktiven Plot durch `{plotly}` wiedergegeben. Mehr Informationen erhälst du dann auf der Hilfeseite von `timetk` zu [Visualizing Time Series](https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html). Wie immer wenn du so generische Funktionen nutzt, musst du schauen, ob dir die Abbildung so gefällt. Du verlierst hier etwas Flexibilität und erhälst dafür aber schneller deine Abbildungen.

Wir erkennen ganz gut, dass wir hier einen Effekt der Saison oder aber der Jahreszeit haben. Wir haben zyklische Peaks der Milchleistung über das Jahr verteilt. Gegen Ende unserer Zeitreihe sehen wir aber eine Art Plateau der Milchleistung. In der folgenden Analyse wollen wir einmal schauen, ob wir die zukünftige Milchleistung anhand der bisherigen Daten vorhersagen können.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-milk-1
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "Die Darstellung der Milchdaten durch das R Paket `{timetk}` und der Funktion `plot_time_series()`, die durch die Option `.interactive = TRUE` intern dann `{plotly}` aufruft."
milk_tbl %>%
  plot_time_series(month, milk_prod_per_cow_kg, .interactive = TRUE)
```

## Analysen von Zeitreihen

Bis jetzt haben wir uns die Visualisierung von Zeitreihen angeschaut. Häufig reicht die Visualisierung auch aus, wenn es um die Darstellung von Temperaturverläufen in einer Abschlussarbeit geht. Häufig wollen wir dann aber doch irgendwie eine statistische Analyse auf der Zeitreihe rechnen. Deshalb habe ich hier mal angefangen Beispiele zu Auswertungen von Zeitreihen zu sammeln und vorzustellen. Hauptsächlich nehme ich natürlich die drei Datensätze von weiter oben im Kapitel.

-   **Vergleich zweier Zeitreihen**
-   **Vorhersage eines zukünftigen zeitlichen Verlaufs**
-   **Einfache Glättungen**
-   **Zerlegung einer Zeitreihe in verschiedene Komponenten**

### Definitionen und Überblick

In dem folgenden Abschnitt möchte ich gerne einmal einen Überblick über die wichtigsten Begriffe in der Analyse von Zeitreihen geben. Teilweise sind es etwas speziellere Begriffe, so dass ich hier erstmal etwas zu den Begriffen schreibe und dann die einzelnen Begriffe nochmal tiefer erkläre.

Stationarität (eng. *stationarity*)

:   Eine gängige Annahme bei vielen Zeitreihenverfahren ist, dass die Daten stationär sind. Ein stationärer Prozess hat die Eigenschaft, dass sich der Mittelwert, die Varianz und die Autokorrelationsstruktur im Laufe der Zeit nicht ändern.

Autokorrelation

:   Wenn wir Zeitreihen $t_1$ und $t_2$ vorliegen haben, so zeigt die Korrelation $\rho(t_1, t_2)$, wie sehr sich die zwei Zeitreihen ähneln. Die Autokorrelation beschreibt, wie ähnlich die Zeitreihe $t_1$ sich selbst ist. Damit beschreibt die Autokorrelation die inhärente Ähnlichkeit einer Zeitreihe. Anhand der Werte der Autokorrelationsfunktion können wir erkennen, wie stark sie mit sich selbst korreliert. Bei jeder Zeitreihe ist die Korrelation bei Lag/Delay = 0 perfekt, da man dieselben Werte miteinander vergleicht. Wenn du deine Zeitreihe verschiebst, wirst du feststellen, dass die Korrelationswerte abnehmen. Wenn die Zeitreihe aus völlig zufälligen Werten besteht, gibt es nur eine Korrelation bei lag = 0, aber keine Korrelation überall sonst. Bei den meisten Datensätzen/Zeitreihen ist dies nicht der Fall, da die Werte im Laufe der Zeit abnehmen und somit eine gewisse Korrelation bei niedrigen Lag-Werten besteht.Damit kann die Autokorrelationsfunktion die Frequenzkomponenten einer Zeitreihe aufzeigen.

ARIMA Modell

:   Wir nutzen zur Analyse von Zeitreihen das ARIMA Modell (abk. *autoregressive integrated moving average*, deu. *autoregressiver gleitender Durchschnitt*). Das ARIMA Modell ist dabei eine Erweiterung schon existierender Modelle und wird sehr häufig für die Analyse von Zeitreihen genutzt. Als wichtigste Anwendung gilt die kurzfristige Vorhersage. Das ARIMA Modell besitzt einen autoregressiven Teil (AR-Modell) und einen gleitenden Mittelwertbeitrag (MA-Modell). Das ARIMA Modell erfordern eigentlich stationäre Zeitreihen. Eine stationäre Zeitreihe bedeutet, dass sich die Randbedingungen einer Zeitreihe nicht verändern. Die zugrundeliegende Verteilungsfunktion der gemessenen Werte über die Zeitreihe muss zeitlich konstant sein. Das heißt konkret, dass die Mittelwerte und die Varianz zu jeder Zeit gleich sind. Gewisse Trends lassen sich durch ein ARIMA Modell herausfiltern.

Lag (deu. *Zeitverzögerung*)

:   Mit eng. *lag* ist im Wesentlichen eine Verzögerung gemeint. Betrachte eine Folge von Zeitpunkten. Bei einem Lag von 1 vergleichst du die Zeitreihe mit einer verzögerten Zeitreihe. Du verschiebst die Zeitreihe um 1 Wert, bevor du sie mit sich selbst vergleichst. So gehst du dann für gesamte Länge der Zeitreihe vor. Wir haben nun eine Autokorrelationsfunktion für das Lag 1 vorliegen. Wenn wir also eine Zeitreihe $t$ vorliegen haben und das Lag 1 berechnen sollen, dann entfernen wir die erste Beobachtung und haben eine $t-1$ *gelaggte* Zeitreihe.

Differenz

:   Die Differenz zwischen zwei Zeitpunkten in einer Zeitreihe wird auch häufig benötigt. Dabei wird häufig auch von der Ordnung (eng. *order*) geschrieben. Die Ordnugn gibt nur an, die wievielte Differenz wir berechnet haben. Klingt wild, ist aber bnnichts anders als immer wieder die Differenz zwischen Zahlen zu berechnen. Die Differenz 1. Ordnung $d(1)$ zwischen den Zahlen $y = {2, 6, 7}$ ist $d(1) = {4, 1}$. Die Differenz 2. Ordnung $d(2)$ ist dann nur noch die Differenz in der 1. Ordnung und damit $d(2) = 3$.

Im Folgenden wollen wir einmal den englischen Begriff *lag* (deu. *Zeitverzögerung*) verstehen. Dazu nutzen wir eine [Zeitreihe in Minuten aus dem Yellowstone-Nationalpark](https://math.stackexchange.com/questions/2548314/what-is-lag-in-a-time-serie). Im Sommer 1987 maßen die Ranger des Yellowstone-Nationalparks die Zeit zwischen den Ausbrüchen des Old Faithful Geysirs. Dieser Geysir ist für seine relativ regelmäßigen Ausbrüche bekannt, aber wie du dir vorstellen kannst, ist der Geysir keine Uhr. Ein Ziel bei der Erfassung der Zeiten war es, eine Möglichkeit zu finden, den Zeitpunkt des nächsten Ausbruchs vorherzusagen, um den Touristen, die auf einen Ausbruch warten, die Wartezeit zu erleichtern. Die Daten in Minuten für $n=107$ fast aufeinanderfolgende Wartezeiten lauten dann wie folgt.

```{r}
erupt <- c(78, 74, 68, 76, 80, 84, 50, 93, 55, 76, 58, 74, 75, 80, 56, 80, 69, 57,
           90, 42, 91, 51, 79, 53, 82, 51, 76, 82, 84, 53, 86, 51, 85, 45, 88, 51,
           80, 49, 82, 75, 73, 67, 68, 86, 72, 75, 75, 66, 84, 70, 79, 60, 86, 71,
           67, 81, 76, 83, 76, 55, 73, 56, 83, 57, 71, 72, 77, 55, 75, 73, 70, 83,
           50, 95, 51, 82, 54, 83, 51, 80, 78, 81, 53, 89, 44, 78, 61, 73, 75, 73,
           76, 55, 86, 48, 77, 73, 70, 88, 75, 83, 61, 78, 61, 81, 51, 80, 79)
```

Was ist nun ein Lag? Ein Lag ist einfach nur eine Verschiebung der Daten um einen Wert. Wir schauen uns also für Lag 1 die Werte ohne den ersten Wert an. Bei dem Lag 2 löschen wir die ersten beiden Werte. Und dann ist schon fast klar was wir bei dem Lag 3 machen, wir löschen die ersten drei Werte. Wenn wir dann dennoch die paarweise Korrelation berechnen, dann berechnen wir nicht die Korrelation mit sich selber, das wäre bei Lag 0, sondern eben die Korrelation mit sich selbst verschoben um den Lag. Deshalb nennt sich das dann auch Autokorrelation.

Dann berechnen wir einmal mit der Funktion `lag` aus dem R Paket `{dplyr}` die Lags 1, 2, und 3. Alle Lags packen wir dann mit den originalen Daten zusammen in einen Datensatz und haben somit auch gleich einen Vergleich.

```{r}
erupt_tbl <- tibble(erupt, 
                    erupt_l1 = dplyr::lag(erupt, 1),
                    erupt_l2 = dplyr::lag(erupt, 2),
                    erupt_l3 = dplyr::lag(erupt, 3))
erupt_tbl
```

Wie du siehst, haben wir immer eine Verschiebung um einen Wert. Daher können wir jetzt trotzdem eine lineare Regression auf die verschiedenen Lags und den originalen Daten rechnen. Dabei fleigen natürlich alle Zeilen aus den Daten wo wir einen fehlenden Wert haben, aber das ist ja dann auch gewünscht. In den folgenden Tabs kannst du einmal sehen, wie ich für verschiedene Lags die lineare Regression gerechnet habe. Nichts anderes ist dann auch ein AR-Modell. Wir berechnen über die lineare Regression die Korrelation für jedes Lag zu den originalen Daten.

::: panel-tabset
## Lag 1

```{r}
lm(erupt_l1 ~ erupt, erupt_tbl) %>% 
  coef()
```

## Lag 2

```{r}
lm(erupt_l2 ~ erupt, erupt_tbl) %>% 
  coef()
```

## Lag 3

```{r}
lm(erupt_l3 ~ erupt, erupt_tbl) %>% 
  coef()
```
:::

In der @fig-time-lag siehst du nochmal die verschobenen Daten auf der $y$-Achse und die originalen Daten auf der $x$-Achse. Durch die Verschiebung ändert sich immer die Punktewolke und damit dann auch die Regression sowie die berechnete Korrelation als "Steigung der Geraden". Die Idee ist eben, einen Zyklus in den Eruptionen zu finden. Wenn wir also immer kurz/lang Wartezeiten hätten, dann würde sich auch immer die Korrelation ändern, wenn wir um einen Wert verschieben. Das trifft natürlich nur zu, wenn es eben der Rhythmus um *einen* Zeitpunkt verschoben ist. Häufig ist aber nicht *ein* Zeitpunkt sondern een *zwei* oder mehr. Das müssen wir dann durch eine Modellierung herausfinden.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-lag
#| fig-align: center
#| fig-height: 4
#| fig-width: 4
#| fig-cap: "Dastellung der orginalen Eruptionszeiten des Old Faithful Geysirs sowie die entsprechenden Daten für die Lag 1 bis 3. Die Regressiongleichung wurde für alle Zusammenhänge ergänzt."
#| fig-subcap: 
#|   - "Lag 1 vs. Orginal Eruptionszeiten."
#|   - "Lag 2 vs. Orginal Eruptionszeiten."
#|   - "Lag 3 vs. Orginal Eruptionszeiten."
#| layout-nrow: 1

erupt_tbl %>% 
  ggplot(aes(erupt, erupt_l1)) +
  theme_bw() +
  geom_point2(color = cbbPalette[7]) +
  geom_function(fun = \(x){119.49 - 0.68 * x}, 
                color = cbbPalette[7]) +
  annotate("text", x = 50, y = 60, label = "r = -0.69", size = 6) +
  labs(x = "Orginal Eruptionszeiten", y =  "Lag 1 Eruptionszeiten")

erupt_tbl %>% 
  ggplot(aes(erupt, erupt_l2)) +
  theme_bw() +
  geom_point2(color = cbbPalette[6]) +
  geom_function(fun = \(x){31.81 + 0.55 * x}, 
                color = cbbPalette[6]) +
  annotate("text", x = 50, y = 90, label = "r = 0.55", size = 6) +
  labs(x = "Orginal Eruptionszeiten", y =  "Lag 2 Eruptionszeiten")

erupt_tbl %>% 
  ggplot(aes(erupt, erupt_l3)) +
  theme_bw() +
  geom_point2(color = cbbPalette[2]) +
  geom_function(fun = \(x){101.28 - 0.42 * x}, 
                color = cbbPalette[2]) +
  annotate("text", x = 50, y = 60, label = "r = 0.55", size = 6) +
  labs(x = "Orginal Eruptionszeiten", y =  "Lag 3 Eruptionszeiten")

```

Bevor wir aber mit der Modellierung von Zeitreihen beginnen, hier nochmal die vollständige Korrelationsmatrix für alle Lags und den originalen Daten. Ich habe den Zusammenhang einmal in der @fig-time-corrplot-1 dargestellt. Wenn du mehr über die Visualisierung der Korrelation erfahren möchtest, dann besuche das [Kapitel zur Korrelation](#sec-linear-corr-visu). Hier sehen wir dann gut, wie mit jedem Schritt im Lag die Korrelation im Vorzeichen flippt. Ein Zeichen, dass wir es hier mit einer zyklischen Zeitreihe zu tun haben und wir etwas in den Daten entdecken könnten.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-corrplot-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Korrealtion zwischen den originalen Daten und den Lags. Im unteren Bereich des Korrelationsplot sind die Scatterplost mit der Regressionsgraden eingezeichnet. Im oberen Bereich finden sich die berechneten Korrelationskoeffizienten $\\rho$ für die paarweisen Vergleiche."

cor_func <- function(data, mapping, method, symbol, ...){
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  corr <- cor(x, y, method=method, use='complete.obs')
  ggally_text(
    label = paste(symbol, as.character(round(corr, 2))), 
    mapping = aes(),
    xP = 0.5, yP = 0.5,
    color = 'black', size = 6
  ) 
}

lower_fun <- function(data, mapping) {
  ggplot(data = data, mapping = mapping) +
    geom_point2(alpha = 0.7) + 
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE)
}

ggpairs(erupt_tbl, 
        upper = list(continuous = wrap(cor_func, method = 'pearson', 
                                       symbol = expression('\u03C1 ='))),
        lower = list(continuous = wrap(lower_fun)),
        columnLabels = c("Original", "Lag 1", "Lag 2", "Lag 3"),
        axisLabels = "internal") +
  theme_bw()
```

Das war jetzt die händische Darstellung. Wir können uns die Autokorrelation auch über eine Zeitreihe anschauen. Wir nutzen hier die etwas primitive Funktion `acf()`, da nur die "alten" Funktionen nur mit einem Vektor von Zeiten klar kommen. Eigentlich brauchen wir ja zu jedem Wert ein Datum. Das haben wir hier aber nicht, deshalb müssen wir hier zu der alten Implementierung greifen.

```{r}
erupt_ts <- tk_ts(erupt)
```

In der Abbildung @fig-time-correlogram-1 sehen wir die Autokorrelation zwischen den Orginaldaten des Geysirs und den entsprechenden Korrelationen zu den Lags 1 bis 5. Wenn du nochmal weiter oben schaust, dann haben wir für die Korrelationen von den Orginaldaten zu den entsprechenden Lags folgende Korrelationen berechnet. Wir hatten eine Korrelation von $\rho = -0.68$ zu Lag 1, eine Korrelation von $\rho = 0.55$ zu Lag 2 und ein $\rho = -0.43$ zum Lag 3 beobachtet. Die Korrelationen findest du dann als Striche auch in der Abbildung wieder.

Nun ist es aber so, dass natürlich die Lags untereinander auch korreliert sind. Diese Korrelation untereinander wollen wir dann einmal raus rechnen, so dass wir nur die partielle Korrelation haben, die zu den jeweiligen Lags gehört. Dabei entsteht natürlich eine Ordnung. Das Lag 1 wird vermutlich die meiste Korrelation erklären und dann folgen die anderen Lags. Deshalb nennen wir diese Art der Korrelation auch *partial* Autokorrelation. Du siehst die Anteile der partiellen Korrelation zu den jeweiligen Lags dann in der @fig-time-correlogram-2. Wir werden dann später bei `{tktime}` eine bessere Art und Weise sehen die Abbildungen zu erstellen. Zum Verstehen sind die Abbildungen gut, aber schön sind die Abbildungen nicht.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-correlogram
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Korrelationsabbildungen mit der Option `lag.max = 5`. Daher werden nur die ersten fün Lags betrachtet. Die Abbildungen dienen der Veranschaulichung vom Lag. Für eine Veröffentlichung bitte die Funktionen aus `{tktime}` verwenden."
#| fig-subcap: 
#|   - "Correlogram."
#|   - "Partial Correlogram."
#| layout-nrow: 1
#| column: page

erupt_ts %>% 
  acf(main = "", lag.max = 5, ylim = c(-1, 1))

erupt_ts %>% 
  pacf(main = "", lag.max = 5)

```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-ar-1
#| fig-align: center
#| fig-height: 6
#| fig-width: 8
#| fig-cap: "AR(0); AR(1) mit AR-Parameter 0.3; AR(1) mit AR-Parameter 0.9; AR(2) mit AR-Parameter 0.3 und 0.3; und AR(2) mit AR-Parameter 0.9 und -0.8."

set.seed(202318080)

plot_tbl <- map(lst(0.01, 0.3, 0.9, c(0.3, 0.3), c(0.2, 0.7)), \(x) {
  arima.sim(model = list(ar = x), n = 100) %>% 
    as_tibble() 
}) %>% 
  bind_cols() %>% 
  set_names(c("AR(0) 0", "AR(1) 0.3", "AR(1) 0.9", "AR(2) 0.3; 0.3", "AR(2) 0.9; -0.8")) %>% 
  mutate(index = 1:100) %>% 
  pivot_longer(cols = matches("AR"),
               values_to = "value",
               names_to = "key") %>% 
  mutate(key = as_factor(key))

ggplot(plot_tbl, aes(index, value, fill = key)) +
  theme_bw() +
  facet_wrap(~ key, nrow = 5, strip.position="right") + 
  geom_line() +
  geom_hline(yintercept = 0) +
  geom_ribbon(aes(ymin = ifelse(value <= 0, value, 0), 
                  ymax = ifelse(value >= 0, value, 0)), 
              alpha=0.5) +
  ylim(-5, 5) +
  labs(x = "", y = "") +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        legend.position = "none", 
        strip.text = element_text(size = 10))

```

[Time Series as Features](https://www.kaggle.com/code/ryanholbrook/time-series-as-features)

[Time series forecasting](https://medium.com/analytics-vidhya/time-series-forecasting-d611fa8ae6ca)

### Einfache Analysen

Wir schauen uns hier als erstes die Standardvariante in R an. Das heißt, wir nehmen die einfachen Funktionen, die in R implementiert sind und rechnen damit eine Zeitreihenanalyse. Damit haben wir dann einige Nachteile, da wir uns die Funktionen dann eventuell nochmal aus Paketen zusammensuchen müssen. Dafür ist es aber schön kleinteilig und du kannst die Analysen Schritt für Schritt durchführen. Wenn dir das zu kleinteilig oder aber veraltet ist, dann schaue gleich weiter unten in den Abschnitten zu den R Paketen `{tktime}` und `{modeltime}` nach. Beide Pakete sind die Antwort auf eine Analyse von Zeitreihen im `{tidyverse}`.

::: callout-important
## Konvertierung von verschiedenen Datumsformaten in R

Das R Paket `{timetk}` liefert dankenswerterweise Funktionen für die Konvertierung von verschiedenen Zeitformaten in R. Deshalb schaue einmal in die Hilfeseite [Time Series Class Conversion -- Between ts, xts, zoo, and tbl](https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html#introduction) und dann dort speziell der Abschnitt [Conversion Methods](https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html#conversion-methods). Leider ist Zeit in R wirklich relativ.
:::

Leider ist das Zeitformat `ts` etwas quälend. Dennoch basieren viele Tutorien auf diesem Format, deshalb hier auch einmal die Erklärung dafür. Es ist aber auch verständlich, denn das Format ist sozusagen der eingebaute Standard in R. Standard heißt hier aber nicht toll, sondern eher veraltet aus den 90zigern. Dann gibt es mit dem R Paket `{zoo}` noch ein Palette an nützlichen Funktionen, wenn du nicht so viel machen willst. Mit so viel meine ich, dass du eher an einem rollenden Mittelwert oder aber der rollenden Summe interessiert bist. Dann macht das R Paket `{zoo}` sehr viel Sinn. Einen Überblick liefert hier auch das Tutorium [Reading Time Series Data](https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/src/timeseries.html#reading-time-series-data).

#### Rollender Mittelwert und Co. {.unnumbered}

Wenn wir viele Datenpunkte über die Zeit messen, dann hilft es manchmal die Spitzen und Täler aus den Daten durch eine rollende statistische Maßzahl zusammenzufassen. Das R Paket `{zoo}` hat die Funktion `rollmean()` sowie `rollmax()` und `rollsum()`. Es gibt aber noch eine Reihe weiterer Funktionen. Du musst hier einfach mal die Hilfeseite `?rollmean()` für mehr Informationen aufrufen. Mit den Funktionen können wir für ein Zeitintervall $k$ den Mittelwert bzw. der anderen Maßzahlen berechnen. In unserem Fall habe ich einmal das rollende Monatsintervall genommen. Du kannst aber auch andere Zeiten für $k$ einsetzen und überlegen welcher Wert besser zu deinen Daten passt. Hier einmal die Berechnung für das rollende Mittel, das rollende Maximum und die rollende Summe. In allen drei Fällen nutzen wir die Funktion `split()` und `map()` um effizient unseren Code auszuführen.

::: panel-tabset
## `rollmean()`

```{r}
roll_mean_tbl <- salad_long_tbl %>% 
  split(.$type) %>% 
  map(~zoo(.x$temp, .x$datum)) %>% 
  map(~rollmean(.x, k = 29)) %>% 
  map(tk_tbl) %>% 
  bind_rows(.id = "type")
```

## `rollmax()`

```{r}
roll_max_tbl <- salad_long_tbl %>% 
  split(.$type) %>% 
  map(~zoo(.x$temp, .x$datum)) %>% 
  map(~rollmax(.x, k = 29)) %>% 
  map(tk_tbl) %>% 
  bind_rows(.id = "type")
```

## `rollsum()`

```{r}
roll_sum_tbl <- salad_long_tbl %>% 
  split(.$type) %>% 
  map(~zoo(.x$temp, .x$datum)) %>% 
  map(~rollsum(.x, k = 29)) %>% 
  map(tk_tbl) %>% 
  bind_rows(.id = "type")
```
:::

In der @fig-time-salad-roll-1 siehst du einmal das Ergebnis der drei rollenden Maßzahlen. Im Folgenden habe ich zuerst das Template `p_temp` erstellt und dann über den Operator `%+%` die Datensätze zum rollenden Mittelwert, zu dem rollenden Maximum und der rollenden Summe ersetzt. Die rollende Summe habe ich noch auf der $\log$-transformierten $y$-Achse dargestellt.

```{r}
p_temp <- ggplot() +
  aes(index, value, color = type) +
  theme_bw() +
  geom_point2() +
  stat_smooth(se = FALSE) +
  labs(x = "Datum", y = "Rollende statistische Maßzahl", 
       color = "Type") +
  scale_color_okabeito() 
```

Wir sehen in der folgenden Abbildung, dass sich die Messtypen dann doch nicht so stark in durch die rollenden Maßzahlen unterscheiden. Wir haben ja schon in der Orginalabbildung das Problem gehabt, dass sich die Werte sehr stark ähneln. Das scheint auch über 29 Tage der Fall zu ein. Was man besser sieht, ist das wellenförmige Ansteigen der Temperatur über die gemessene Zeit. Wir hatten also immer mal wieder etwas kältere Phasen, die von wärmeren Phasen abgelöst wurden.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-salad-roll-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Darstellung der rollenden Mittelwerte, maximalen Werte sowie aufsummierten Werte über 29 Tage. Die aufsummierten Werte sind auf logarithmischen Skala dargestellt. Gegenüber der orginalen Abbildung sehen wir schon etwas mehr Ordnung. Die drei Arten der Messung unterscheiden sich aber weiterhin kaum."
#| fig-subcap: 
#|   - "Rollender Mittelwert."
#|   - "Rollendes Maximum."
#|   - "Rollende Summe"
#| layout-nrow: 1
#| column: page

p_temp %+%
  roll_mean_tbl +
  ylim(0, 40)

p_temp  %+%
  roll_max_tbl +
  ylim(0, 40)

p_temp  %+%
  roll_sum_tbl +
  scale_y_log10()
 
```

#### Vorhersage von Milchleistung {.unnumbered}

Wenn wir eine Vorhersage auf einem zeitlichen Verlauf rechnen wollen, dann brauchen wir als aller erstes einen Datensatz, der auch eine *echte* Zeitreihe über mehrere zeitliche Zyklen enthält. Das ist dann meistens die Herausforderung so eine Zeitreihe in einer Abschlussarbeit zu erzeugen. In ein paar Monaten einen zyklischen Verlauf zu finden ist schon eine echte Leistung. Deshalb nehmen wir hier als Beispiel einmal unsere künstlichen Daten zur Milchleistung von Kühen. Wie du in der obigen @fig-time-milk-1 klar erkennen kannst, haben wir hier Zyklen über die einzelnen Jahre hinweg. Es liegt ein stetiger, zyklischer Anstieg der Milchleistung über die beobachteten Jahre vor. Wir wollen jetzt den Verlauf modellieren und einen zukünftigen Verlauf vorhersagen. Oder andersherum, wie die zukünftigen Zyklen aussehen könnten.

In diesem Beispiel nutzen wir das R Paket `{zoo}` und die Funktion `ts()` für die Standardimplemetierung von Zeitreihen in R. Das hat immer ein paar Nachteile, da wir hier die veralteten Speicherformen für eine Zeitreihe nutzen. Auf der anderen Seite sind viele Tutorien im Internet noch genau auf diese Funktionen ausgerichtet. Deshalb auch hier einmal die Erklärung der etwas älteren Funktionen. Später schauen wir dann auch die neuere Implementierung in dem R Paket `{tktime}` einmal an. Wir wandeln also erstmal unsere Milchdaten mit der Funktion `tk_ts()` in ein `ts`-Zeitobjekt um. Dafür müssen wir angeben von wann bis wann die Jahre laufen und wie viele Beobachtungen jedes Jahr hat. Glücklicherweise müssen nicht alle Jahre gefüllt werden und die Funktion erlaubt auch einen anderen Startmonat als `Jan`. Wie du gleich siehst, dann haben wir eine Art Matrix als Ausgabe.

```{r}
milk_ts <- milk_tbl %$%
  tk_ts(milk_prod_per_cow_kg, start = 1962, end = 1975, frequency = 12)
milk_ts
```

Ein ARIMA Modell setzt sich aus drei Komponenten wie folgt zusammen.

1)  Dem Lag $p$ (eng. *autoregressiv* oder AR-Modell)
2)  Der verwendeten Differenz $d$
3)  Der laufenden Durchschnitt $q$ (eng. *moving average* oder MA-Modell)

::: panel-tabset
## Orginal

```{r}
erupt[1:10]
```

## Differenz 1

```{r}
diff(erupt[1:10], differences = 1)
```

## Differenz 2

```{r}
diff(erupt[1:10], differences = 2)
```
:::

In der folgenden @fig-time-ts-3 siehst du die Zeitverzögerung (eng. *lag*).

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-ts-3
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "foo."

milk_ts %>% 
  acf(main = 'Correlogram')
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-ts-4
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "foo."

milk_ts %>% 
  pacf(main = 'Partial Correlogram' )

```

```{r}
decomp <- decompose(milk_ts)
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-ts-1
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "foo."
plot(decomp)
```

```{r}
#| cache: true
#| warning: false
#| message: false
#| label: arima_milk
milk_arima_obj <- auto.arima(milk_ts)
```

```{r}
arima_mdl <- forecast(milk_arima_obj, 24)
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-ts-2
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "foo."

autoplot(arima_mdl) +
  theme_bw()



```

### Vergleich von Zeitreihen

Unsere Dronenüberflugdaten sind etwas besondere Daten, wenn wir uns Zeitreihen anschauen. Wir haben zwar auch einen zeitlichen Verlauf auf der $x$-Achse, aber der Zeitrahmen ist mit unter einem Jahr zu klein um einen zyklischen Verlauf zu beobachten. Wir wollen hier auch etwas anderes erreichen. Uns interessieren die einzelnen Beobachtungen nicht, wir wollen die angepasste Graden durch die Punktewolken vergleichen. In der @fig-time-csh-2 siehst du nochmal die angepassten Kurven ohne die einzelnen Messpunkte. Eigentlich rechnen wir hier einen Gruppenvergleich über die Zeit. Spannende Sache, die wollen wir uns dann mal genauer ansehen. Wir werden hier aber keinen statistischen Test rechnen, sondern nur ausrechnen in wie weit sich die beiden Parzellen numerisch im Ertrag unterscheiden.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-csh-2
#| fig-align: center
#| fig-height: 4
#| fig-width: 5
#| fig-cap: "Die Graserträge für die beiden Parzellen Uelzen und Wiese von Mai bis Ende September. Wir sind an der Fläche zwischen den beiden Graden interessiert."

p_csh <- csh_tbl %>% 
  ggplot() +
  theme_bw() +
  scale_color_okabeito() +
  stat_smooth(aes(day, g_tm_plot, color = parzelle), 
              se = FALSE, n = 100) # <1>
p_csh
```

1.  Setze hier `n = 1000` um wirklich eine gute Abdeckung später für die Berechung der Fläche zu haben.

Gibt es also einen Unterschied zwischen den beiden Parzellen $Uelzen$ und $Wiese$ im Bezug auf den Ertrag? Dafür müssen wir die Differenz der Graden an jedem Punkt berechnen. Oder anders formuliert, wir wollen die Fläche zwischen den beiden Kurven berechnen. Um die Fläche zu berechnen, brauchen wir die Koordinaten, die die Kurven beschreiben. Dafür können wir die Funktion `ggpülot_build()` nutzen, die uns dann die @fig-time-csh-2 in seine Einzelteile zerlegt. Insbesondere brauchen wir die Koordinaten für die beiden Kurven.

```{r}
#| message: false
#| echo: true
#| warning: false
p_csh_str <- ggplot_build(p_csh)
```

Tja, und dann geht es schon los. Der folgende Code beschreibt die Extraktion der Informationen zu den einzelnen Kurven. Am Ende wollen wir für jedes $x$ die beiden $y$-Werte für Uelzen und die Wiese haben. Dann können wir immer die Differenz zwischen dem beiden $y$-Werten `y_2` für Uelzen und `y_5` für die Wiese berechnen. Ja, die Indizes sind etwas irre, aber das kommt dann von dem Zusammenbauen der Daten.

```{r}
#| message: false
#| echo: true
#| warning: false
ribbon_tbl <- p_csh_str %>% 
  pluck("data", 1) %>% 
  as_tibble() %>% 
  select(x, y, group) %>% 
  mutate(group = factor(group, labels = c("Uelzen", "Wiese"))) %>% 
  split(.$group) %>% 
  bind_cols() %>% 
  clean_names() %>% 
  select(x = x_1, Uelzen = y_2, Wiese = y_5) %>%  # <1>
  mutate(x = as_date(x, origin = lubridate::origin)) # <2>
```

1.  Die `y_2` Werte sind die Werte aus Uelzen und die `y_5` Werte von der Wiese.
2.  Die `x`-Werte sind noch das Datum in numerischer Form aus `ggplot()`. Hier wieder zurück ins `yyyy-mm-dd` Format.

Im Folgenden sehen wir dann die `g_tm_plot`-Werte auf den beiden Kurven für jedes Datum getrennt nach Uelzen und Wiese. Es ist nur ein Ausschnitt aus den Daten und du kontrollierst über `n = 100` bei der Funktion `geom_smooth()` wie viele $x$-Werte gebildet werden. Wenn du genauere Werte haben willst, also mit weniger Lücken zwischen den einzelnen $x$-Werten, dann musst du $n$ weiter erhöhen.

```{r}
head(ribbon_tbl)
```

Jetzt können wir einmal die Fläche, die wir berechnet haben abbilden. In der @fig-time-csh-3 siehst du einmal auf der linken Seite nur die beiden Graden mit der berechneten Fläche. Auf der rechten Seite nochmal die Anbildung mit allen Beobachtungen zusammen.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-csh-3
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Die Graserträge für die beiden Parzellen Uelzen und Wiese von Mai bis Ende September. Wir sind an der Fläche zwischen den beiden Graden interessiert. Je nachdem wie viele $x$-Punkte für die geglättete Kurven verwendet wird, wird die Berechnung der Fläche genauer."
#| fig-subcap: 
#|   - "Ohne einzelne Messwerte."
#|   - "Mit einzelnen Messwerten."
#| layout-nrow: 1
#| column: page

p_csh + 
  geom_ribbon(data = ribbon_tbl, aes(x = x, ymin = Wiese, ymax = Uelzen),
                    fill = "grey", alpha = 0.4) +
  ylim(10, 120)

p_csh + 
  geom_ribbon(data = ribbon_tbl, aes(x = x, ymin = Wiese, ymax = Uelzen),
                    fill = "grey", alpha = 0.4) +
  geom_point(aes(day, g_tm_plot, color = parzelle)) +
  ylim(10, 120)

```

Und dann können wir uns auch die Fläche als Differenz der $y$-Werte für Uelzen und die Wiese berechnen lassen. Wir schauen uns dann gleich nochmal den Effekt von `n` in `stat_smooth()` an, dann siehst du die Auswirkungen. Hier sei noch gesagt, dass du im Zweifel mit `filter()` die Abbildung in kleinere Teile zerhaken kannst. Aktuell wird ja die Fläche durch das umgedrehte Vorzeichen der Fläche am Anfang etwas reduziert. Aber das sind dann schon Kleinigkeiten. Wir haben also eine Differenz an Ertrag von $441.03 g_{tm}$ zwischen der Parzelle in Uelzen und der Wiese. Du kannst hier keinen statistischen Test rechnen, da wir ja dann nur einen Wert vorliegen haben. Es kann ja nur eine Fläche zwischen den beiden Kurven entstehen.

```{r}
ribbon_tbl %>% 
  mutate(diff = Uelzen - Wiese) %>% 
  pull(diff) %>% 
  sum()
```

Hier in der folgenden @fig-time-csh-4 nochmal die *Dichte* der $x$-Werte, die dann natürlich auch die Differenz mit bedingen. Wie du siehst sind da einiges an Lücken in den Kurven. Wenn wir oben `n = 1000` in der Funktion `stat_smooth()` gesetzt hätten, dann hätten wir hier fast eine Linie aus Punkten ohne Lücken. Da musst du dann etwas mit den Werten spielen. Ich möchte ja hier eine effiziente Darstellung, die auch schnell durchläuft. Da opfere ich dann die Genauigkeit - man kann ja leider nicht alles haben.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-csh-4
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Darstellung der Dichte der $x$-Werte aus der Funktion `stat_smooth()` mit `n = 100`. Es sind klare Lücken zu erkennen, die wir dann durch ein größeres `n = 1000` schließen könnten."
ribbon_tbl %>% 
  pivot_longer(cols = Uelzen:Wiese,
               values_to = "g_tm",
               names_to = "parzelle") %>% 
  ggplot(aes(x, g_tm, color = parzelle)) +
  theme_bw() +
  scale_color_okabeito() +
  geom_point()
```

Damit habe ich hier einmal gezeigt, wie du zwei Kurven deskriptiv miteinander vergleichst und die Differenz als Fläche zwischen den Kurven berechnest. Die Prozedur lässt sich einfach auch auf mehr Kurven erweitern, wenn du als mehr als zwei Parzellen vorliegen hättest. Wie immer, wenn es hier mehr Bedarf gibt, dann würde ich mir die Sachlage nochmal tiefer anschauen.

### Komplexere Analysen mit `{tktime}`

[{timetk}](https://business-science.github.io/timetk/)

### Vorhersagen mit `{fable}` und `{feasts}`

[{fable}](https://fable.tidyverts.org/articles/fable.html) [{feasts}](https://feasts.tidyverts.org/index.html)

### Vorhersagen mit `{modeltime}`

Am Ende hier nochmal eine Möglichkeit sehr effizient Zeitreihen zu modellieren und eine Vorhersage für zukünftige Verläufe zu machen. Ich empfehle hier die Hilfeseite [Getting Started with Modeltime](https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html). Auf der Seite erfährst du dann die Grundlagen für die Anwendung von dem R Paket `{modeltime}`. Hier kannst du dann tief in den Kaninchenbau reingehen. Neben dem R Paket `{modeltime}` findest du im Ökosystem `{modeltime}` noch andere spannende R Pakete, die dir weitere und vertiefende Modellierungen zur Vorhersage erlauben. Wenn dir die Begriffe zu der Vorhersage und der Klassifikation etwas komisch vorkommen, dann kannst du in dem Kapitel [Grundlagen der Klassifikation](#sec-class-basic) reinschauen und nachlesen wo du dann noch Lücken hast. Ich gehe hier dann nicht mehr so tief auf die einzelnen Punkte ein, sondern führe hier eher grob durch den Code.

```{r}
#| warning: false
#| message: false
splits <- initial_time_split(milk_tbl, prop = 0.9)
```

```{r}
#| cache: true
#| warning: false
#| message: false
#| label: arima_modeltime
model_fit_arima_no_boost <- arima_reg() %>%
    set_engine(engine = "auto_arima") %>%
    fit(milk_prod_per_cow_kg ~ month, data = training(splits))
```

```{r}
#| warning: false
#| message: false
model_fit_ets <- exp_smoothing() %>%
    set_engine(engine = "ets") %>%
    fit(milk_prod_per_cow_kg ~ month, data = training(splits))
```

```{r}
#| warning: false
#| message: false

milk_tbl %>% 
  mutate(month_fac = factor(month(month, label = TRUE), ordered = FALSE),
         month_num = as.numeric(month)) %>% 
  head(7)
```

```{r}
#| warning: false
#| message: false
model_fit_lm <- linear_reg() %>%
    set_engine("lm") %>%
    fit(milk_prod_per_cow_kg ~ as.numeric(month) + factor(month(month, label = TRUE), ordered = FALSE),
        data = training(splits))
```

```{r}
#| warning: false
#| message: false
models_tbl <- modeltime_table(
    model_fit_arima_no_boost,
    model_fit_ets,
    model_fit_lm
)
```

```{r}
#| warning: false
#| message: false
calibration_tbl <- models_tbl %>%
    modeltime_calibrate(new_data = testing(splits))
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-modeltime-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Eigenwerte für die beiden Datensätze."

calibration_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = milk_tbl
    ) %>%
    plot_modeltime_forecast(
      .legend_max_width = 25, # For mobile screens
      .interactive      = FALSE
    )
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: tbl-time-modeltime-1
#| tbl-cap: "Eigenwerte für die beiden Datensätze."

calibration_tbl %>%
    modeltime_accuracy() %>%
    table_modeltime_accuracy(
        .interactive = FALSE
    )

```

-   MAE - Mean absolute error, [`mae()`](https://yardstick.tidymodels.org/reference/mae.html)
-   MAPE - Mean absolute percentage error, [`mape()`](https://yardstick.tidymodels.org/reference/mape.html)
-   MASE - Mean absolute scaled error, [`mase()`](https://yardstick.tidymodels.org/reference/mase.html)
-   SMAPE - Symmetric mean absolute percentage error, [`smape()`](https://yardstick.tidymodels.org/reference/smape.html)
-   RMSE - Root mean squared error, [`rmse()`](https://yardstick.tidymodels.org/reference/rmse.html)
-   RSQ - R-squared, [`rsq()`](https://yardstick.tidymodels.org/reference/rsq.html)

```{r}
#| warning: false
#| message: false
refit_tbl <- calibration_tbl %>%
    modeltime_refit(data = milk_tbl)
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-modeltime-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Eigenwerte für die beiden Datensätze."
refit_tbl %>%
    modeltime_forecast(h = "3 years", actual_data = milk_tbl) %>%
    plot_modeltime_forecast(
      .legend_max_width = 25, # For mobile screens
      .interactive      = FALSE
    )
```

## Referenzen {.unnumbered}
