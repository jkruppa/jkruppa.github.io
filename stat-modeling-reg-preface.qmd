```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Statistisches Modellieren {#sec-modeling-simple-stat}

## Genutzte R Pakete für das Kapitel

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, conflicted, broom,
               multcomp, emmeans, ggpubr)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

```{r}
#| message: false

model_tbl <- read_csv2("data/flea_dog_cat_length_weight.csv") %>%
  mutate(animal = as_factor(animal))
```

In der @tbl-model-1 ist der Datensatz `model_tbl` nochmal dargestellt.

```{r}
#| message: false
#| echo: false
#| tbl-cap: Selektierter Datensatz mit einer normalverteilten Variable `jump_length` sowie der multinominalverteilten Variable `grade` und einem Faktor `animal` mit drei Leveln.
#| label: tbl-model-1

model_tbl %>% head(15) %>% kable(align = "c", "pipe")
```

## Simple lineare Regression

$$
y \sim x_1
$$

$$
y \sim \beta_0 + \beta_1 x_1 + \epsilon
$$

| $\boldsymbol{y \sim \beta_0 + \beta_1 x_1 + \epsilon}$ | $\boldsymbol{y = mx +b}$ |      Deutsch      |  Englisch   |
|:------------------------------------------------------:|:------------------------:|:-----------------:|:-----------:|
|                       $\beta_1$                        |           $m$            |     Steigung      |    Slope    |
|                         $x_1$                          |           $x$            | Einflussvariable  | Risk factor |
|                       $\beta_0$                        |           $b$            | y-Achsenabschnitt |  Intercept  |
|                       $\epsilon$                       |                          |     Residuen      |  Residual   |

: test

![Visualisierung der Korrelation.](images/statistical_modeling_lm){#fig-corr2 fig-align="center" width="100%"}

$$
\epsilon \sim \mathcal{N}(0, s^2_y)
$$

## Multiple lineare Regression

$$
y \sim x_1 + x_2 + ... + x_p
$$

$$
y \sim \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon
$$

```{r}

sim_tbl <- tibble(dog = rnorm(100, 10, 2),
                  cat = rnorm(100, 15, 2),
                  fox = rnorm(100, 20, 2)) %>% 
  gather(animal, jump_length) %>% 
  mutate(animal = as_factor(animal))

lm(jump_length ~ animal, data = sim_tbl)

model.matrix(jump_length ~ animal, data = sim_tbl)
```

## Korrelation

$$
\rho = r_{x,y} = \cfrac{s_{x,y}}{s_x \cdot s_y}
$$

$$
s_{x,y} = \sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})
$$

$$
s_x = \sum\_{i=1}^n(x_i-^\bar{x})2
$$

$$
s_y = \sum_{i=1}^n(y_i-\bar{y})^2
$$

::: column-page
![Visualisierung der Korrelation.](images/statistical_modeling_corr){#fig-corr1 fig-align="center" width="80%"}
:::

![Visualisierung der Korrelation.](images/statistical_modeling_corr_2){#fig-corr2 fig-align="center" width="80%"}

## Bestimmtheitsmaß $R^2$

::: column-page
![Visualisierung der Korrelation.](images/statistical_modeling_rsquare){#fig-rsquare fig-align="center" width="60%"}
:::

## Schlangen Beispiel

Modell matrix?

::: callout-note
## Was macht das statistische Modellieren?

Der Kruskal-Wallis-Test vergleicht die Mediane mehrerer beliebiger Verteilungen miteinander.
:::

::: callout-tip
## Modellierung in der Statistik

Du findest auf YouTube [Statistik und Data Science - Teil 15.0 - Modellierung in der Statistik](https://youtu.be/2fXExWzipDI) als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.
:::

::: {.callout-caution collapse="true"}
## Ein Wort zur Klausur

Wir nutzen folgende
:::

## Verzerrung oder Bias

Wenn ein statistischer Test ein falsches Ergebnis liefert, weil die Daten nicht die Voraussetzungen des statistischen Tests erfüllt haben.

## Robust

Wir sagen, dass ein statistischer Test *robust* ist, wenn wir meinen, dass Annahmen an die Daten falsch sein können und der statistsiche Test dennoch unverzerrte Ergebnisse liefert.

## Homogenität und Heterogenität der Varianzen

Eine Varianzhomogenität liegt vor, wenn die Varianzen der Gruppen gleich sind.

$$
s^2_{dog} = s^2_{cat} = s^2_{fox}
$$

Eine Varianzheterogenität liegt vor, wenn die Varianzen der Gruppen ungleich sind.

$$
s^2_{dog} \neq s^2_{cat} \neq s^2_{fox}
$$

## Balancierte Daten

Wir haben balancierte Daten vorliegen, wenn in jeder Gruppe gleich viele Beobachtungen sind.

$$
n_{dog} = n_{cat} = n_{fox}
$$

Ein unbalanziertes Design heißt, dass wir nicht in jeder Gruppe die gleiche Anzahl an Beobachtungen vorliegen haben.

$$
n_{dog} \neq n_{cat} \neq n_{fox}
$$

Wir reden von balanziertem Design und meinen dmit die Behandlungsgruppe oder den Faktor mit den zu vergleichenden Leveln. Natürlich kann es über den gesamten Datensatz Faktoren mit unterschiedlichen Belegungen an Beobachtungen geben.

## Abhänig vs. unabhänige Daten

## Formula

![Das Regressionskreuz](images/statistical_modeling_0.png){#fig-reg-model-0 fig-align="center" width="30%"}

$$
y \sim x
$$

## `lm()`

## `glm()`

## Residualplot

```{r}
##glance()

##augment()
```

## QQ-Plot

## Das Regressionskreuz

::: column-page
![Das Regressionskreuz](images/Regressionskreut_advanced.png){#fig-reg-cross fig-align="center" width="100%"}
:::

## Kausales Modell

## Prädiktives Modell

## tidymodels

https://www.tmwr.org/models.html

## Package see

https://easystats.github.io/see/articles/performance.html
