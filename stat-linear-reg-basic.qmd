```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, see,
               broom)
cov1_tbl <- read_xlsx("data/regression_data.xlsx", sheet = "covariate")
source("images/R/stat-modeling-R.R")
source("images/R/stat-linear-reg-basic.R")
set.seed(20250708)
theme_modeling <- function() {
  theme_minimal() +
    theme(panel.grid.minor = element_blank(),
          plot.background = element_rect(fill = "white", color = NA),
          plot.title = element_text(size = 16, face = "bold"),
          plot.subtitle = element_text(size = 12, face = "italic"),
          plot.caption = element_text(face = "italic"),
          axis.title = element_text(size = 12, face = "bold"),
          axis.text = element_text(size = 12),
          strip.text = element_text(face = "bold"),
          strip.background = element_rect(fill = "grey80", color = NA))
}
```

# Simple lineare Regression {#sec-modeling-simple-stat}

*Letzte Änderung am `r format(fs::file_info("stat-linear-reg-basic.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"Farewell to the Fairground; These rides aren't working anymore." --- White Lies, Farewell to the Fairground*

![](images/caution.png){fig-align="center" width="100%"}

::: {.callout-caution appearance="simple"}
## Stand des Kapitels: Konstruktion (seit 07.2025)

Dieses Kapitel wird in den nächsten Wochen geschrieben und ist damit meine aktuelle Großbaustelle. Ich plane zum Beginn des WiSe 2025/26 eine fertige Version des Kapitels erstellt zu haben. Während das Kapitel entsteht, funktioniert so manches dann nicht so wie es soll. Bitte daher hier dann abwarten.
:::

In diesem Kapitel wollen wir mit den Grundlagen des statistischen Modellierens beginnen. Wir nutzen dazu die simple lineare Regression als eine Methode um zu Verstehen, was eigentlich das statistische Modellieren macht. Dabei konzentrieren wir uns auf die lineare Regression, da wir hier ein lineares Modell vorliegen haben. Wir wollen also eine Grade Linie durch Punkte ziehen. Die Punkte sind in einem Scatterplot mit einer x-Achse und einer y-Achse dargestellt. Das ist für den Anfang schwierig genug. Später können wir auch nicht lineare Zusammenhänge modellieren. Dann nutzen wir auch nur eine Einflussvariable und nennen deshalb auch die Regression eine simple lineare Regression. Später in den folgenden Kapiteln erweitern wir dann die Regression und schauen uns auch an, wie gut die Regression geklappt hat. Mit gut meine ich, in wie weit die Grade durch die Punkte auch den Zusammenhang zwischen den x-Werten und den y-Werte erklärt. Dazu dann mehr mehr in dem Kapitel zu der Modelgüte einer Regression.

## Allgemeiner Hintergrund

Was macht also eine simple lineare Regression? Die Regression ist eine Methode um eine lineare Grade durch eine Punktewolke in einem Scatterplot zu zeichnen. Dabei haben wir nur eine Einflussvariable $x$ vorliegen, da unser Modell zuallerst simple sein soll. Der Messwert $y$ ist in diesem Kapitel noch normalverteilt und damit auch kontinuierlich. Wir wollen also den Zusammenhang zwischen zwei kontinuierlichen Variablen durch eine Regression bestimmen. Wir sprechen in diesem Zusammenhang mit einer kontinuierlichen Einflussvariable von einer Kovariate. In einem nächsten Schritt ändern wir dann die Einflussvariable auf eine kategoriale Einflussvariable. Wir haben dann einen Faktor mit unterschiedlichen Gruppen vorliegen. Beide Fälle wollen wir uns dann einmal anschauen. Beginnen wollen wir aber mit einer allgemeinern Betrachtung. Welche Modelltypen oder Fragestellungen gibt es eigentlich? Bevor wir damit anfangen, einmal etwas sprachlicher Hintergrund.

#### Sprachlicher Hintergrund {.unnumbered .unlisted}

> *"In statistics courses taught by statisticians we don't use "independent variable" because we use independent on to mean stochastic independence. Instead we say predictor or covariate (either). And, similarly, we don't say "dependent variable" either. We say response." --- [User berf auf r/AskStatistics](https://www.reddit.com/r/AskStatistics/comments/qt1hvu/comment/hkigiks/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)*

Wenn wir uns mit dem statistischen Modellieren beschäftigen wollen, dann brauchen wir auch Worte um über das Thema reden zu können. Statistik wird in vielen Bereichen der Wissenschaft verwendet und in jedem Bereich nennen wir dann auch Dinge anders, die eigentlich gleich sind. Daher werde ich mir es hier herausnehmen und auch die Dinge so benennen, wie ich sie für didaktisch sinnvoll finde. Wir wollen hier was verstehen und lernen, somit brauchen wir auch eine klare Sprache.

::: {layout="[15,85]" layout-valign="top"}
![](images/personal_opinion.png){fig-align="center" width="100%"}

> *"Jeder nennt in der Statistik sein Y und X wie er möchte. Da ich hier nicht nur von Y und X schreiben will, führe ich eben die Worte ein, die ich nutzen will. Damit sind die Worte dann auch richtig, da der Kontext definiert ist. Andere mögen es dann anders machen. Ich mache es eben dann so. Danke." --- Jochen Kruppa-Scheetz, meiner bescheidener Meinung nach.*
:::

In dem folgenden Kasten erkläre ich nochmal den Gebrauch meiner Begriffe im statistischen Testen. Es ist wichtig, dass wir hier uns klar verstehen. Zum einen ist es angenehmer auch mal ein Wort für ein Symbol zu schreiben. Auf der anderen Seite möchte ich aber auch, dass du dann das Wort richtig einem Konzept im statistischen Modellieren zuordnen kannst. Deshalb einmal hier meine persönliche und didaktische Zusammenstellung meiner Wort im statistischen Modellieren. Du kannst dann immer zu dem Kasten zurückgehen, wenn wir mal ein Wort nicht mehr ganz klar ist. Die fetten Begriffe sind die üblichen in den folgenden Kapiteln. Die anderen Worte werden immer mal wieder in der Literatur genutzt.

{{< include stat-modeling/stat-modeling-callout-words.qmd >}}

#### Das Modell {.unnumbered .unlisted}

> *"Models are about what changes, and what doesn't. Some are useful." --- [Markus Gesmann](https://magesblog.com/post/modelling-change/)*

Fangen wir also erstmal allgemeiner an ein Modell und deren schreibweise zu verstehen. Was ist ein Modell im statistischen Sinne? Wenn du an ein Modell denkst und ein Modellauto oder aber ein Modell eines Hauses oder Landschaft vor dir siehst, dann ist das gar nicht so weit weg vonm einem statistischen Modell. Ein Modell eines Autos hat die Essenz eines Autos in sich. Wir erkennen das Auto wieder, haben aber natürlich nicht die gleichen Funktionalitäten wie bei einem echten Auto. So ist es auch bei dem statistischen Modellieren von Daten. Unser Auto sind also die erhobenen Daten und wir wollen ein Modell finden, was die Zusammenhänge und die Struktur in den Daten vereinfacht wiedergibt. Damit ist per se jedes Modell falsch, da es nur einen Teil der Wirklichkeit abbilden kann. Modelle sind aber manchmal nützlich. Wir können also erstmal wie folgt ein Modell zusammenfassen.

Was ist ein Modell?

:   Ein Modell vereinfacht. Dabei behält es die Essenz der modellierten Sache bei. Ein statistisches Modell versucht eine Struktur in Daten zu finden. Dabei modelliert es den Zusammenhang zwischen dem Messwert und Einflussvariablen.

Allgemeiner gesprochen versucht ein statistisches Modell deine Daten in ein erklärbaren Teil und einen unerklärbaren Teil zu zerlegen. Dafür nutzen wir dann den Zusammenhang von den Messwerten und den Einflussvariablen. Je mehr dei Einflussvariablen unseren Messwert erklären können, desto weniger bleibt unerklärt und somit kleiner ist auch der Fehler. Hierbei müssen wir etwas mit dem Wort Fehler aufpassen. Wir haben fast immer einen unerklärten Anteil von unserem Messwert nach dem Modellieren. Wir nennen diesen Anteil Fehler, meinen damit aber nicht, dass das Modell per se fehlerhaft ist.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 1.5
#| fig-width: 7
#| fig-cap: "Der Zusammenhang von Daten, dem statistischen Modell und dem Fehler. Das Modell versucht durch ein statistisches Modell $f(x)$ den Zusammenhang zwischen den Einflussvariablen dem Messwert zu erklären. Den Anteil des unerklärten Messwert geht in den Fehlerterm. *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-model-abstract

p_model_abstract 
```

Da wir uns natürlich für die parktische Anwendung in R bewegen, nutzen wir auch die Modellschreibweise, die in R üblich ist. In R wird diese Schreibweise auch mit der Funktion `formula()` genutzt. Im Folgenden siehst du einmal ein Modell in einer abstrakten Form. Wir haben den Messwert $Y$ auf der linken Seite (eng. *left hand side*, abk. *LHS*) der Tilde und die Einflussvariablen $X$ auf der rechten Seite (eng. *right hand side*, abk. *RHS*). Dabei steht dann das $X$ hier einmal als Platzhalter und Sammelbegriff für verschiedene Arten von möglichen Variablen.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 2
#| fig-width: 7
#| fig-cap: "Statistische Modellschreibweise mit dem Messwert auf der linken Seite und den Einflussvariablen auf der rechten Seite der Tilde. *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-03

p_lhs_rhs
```

Wenn wir jetzt zu R wechseln, sieht es dann etwas anders aus, da wir die Platzhalter $Y$ für den Messwert und $X$ für die Einflussvariable durch die Namen der Spalten in unserem Datensatz ersetzen. Der Datendatz liegt dann als `tibble()` in R vor. Mehr dann dazu in den folgenden Beispielen in den jeweiligen Kapiteln zum Modellieren. Dann sieht das Modell in R wie in der folgenden Abbildung aus.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 2
#| fig-width: 7
#| fig-cap: "Statistische Modellschreibweise in R mit dem Messwert auf der linken Seite und den Einflussvariablen auf der rechten Seite der Tilde. Die Platzhalter $Y$ und $X$ werden durch die Spaltennamen im Datensatz ersetzt. *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-20

p_lhs_rhs_r
```

Jetzt haben wir verstanden über was wir sprechen wollen, was ein Modell ist und wie wir das Modell in R aufbauen. Dann können wir schon einen Schritt weitergehen und uns fragen, was wir wigentlich mit dem Modell machen wollen. Welche Forschungsfragen können wir denn mit einem statistischen Modell beantworten? Prinzipiell haben wir da zwei große Felder, die wir bearbeiten können.

#### Kausales vs. prädiktives Modell {.unnumbered .unlisted}

> *"Vor 2001 bestand Statistik gefühlt zu 10% aus der Prädiktion -- heutzutage macht die Prädiktion die Mehrheit der Modellierungen aus." --- Gefühle einer anonymen Biometrikerin*

Wir können ein kausuales oder ein prädiktives Modell rechnen. Wenn wir ein kausales Modell rechnen, dann haben wir die Frage im Kopf, wie ändert sich der Messwert $y$, wenn sich die Einflussvariable $x$ um eine Einheit ändert. Wir können die Einheit auch weglassen und allgmeiner nach der Änderung in $y$ durch die Änderung in $x$ sprechen. Es geht hier also um Steigung (eng. *slopes*) oder eben die Ableitung (eng. *derivative*) von $dy/dx$. In einem hier besprochenen linearen Modell ist die Frage nach der Steigung einfach zu beantworten. Schwieriger wird es im nicht linearen Modell. Hier nutzen wir dann [*Marginal effect models*](#sec-marginal) um die Steigungen entlang der Kurve zu bestimmen.

Eine andere Frage können wir durch ein prädiktives Modell oder eine Vorhersagen (eng. *predictions*) herausfinden. Hier wollen wir wissen, welche Messwerte ergeben sich durch unser Modell für neue Werte der Einflussvariablen. Das können wir durch eine Vorhersage erreichen. Auch hier ist das lineare Modell einfacher zu verstehen ale eine nicht lineare Kurve. Wir können einfach neue $x$-Werte in die Formel der Gradengleichung einsetzen und erhalten die entsprechenden vorhergesagten $y$-Werte wiedergegeben. Wenn wir kein normalverteilten Messwert haben, müssen wir hier nochmal etwas mehr aufpassen. Dazu dann aber auch mehr in den entsprechenden Kapiteln zu den verschiedenen Modellierungen.

Was sind Steigungen?

:   Die Traktion der Graden über die Werte auf der x-Achse hinweg. Wenn $x$ sich ändert, wir ändert sich dann der Messwert $y$? In einem linearen Modell haben wir nur einen Wert als Koeffizienten für die Steigung.

Was ist die Vorhersage?

:   Die y-Werte auf der Graden für jeden beliebigen Wert auf der x-Achse. Welche Werte vom Messwert $y$ sagt das Modell für $x$-Werte vorraus? Wenn wir eine neue Beobachtung machen, welchen Messwert würde sich gegeben der Einflussvariable einstellen?

Damit ergibt sich grob die folgende Abbildung als Möglichkeiten in dem statistischen Modellieren. Wir haben hier nur ein simples Modell vorliegen und betrachten hier nur eine Einflussvariable. Die Einflussvariable kann kontinuierliche oder eben kategorial sein. Im kausalen Modell wollen wir wissen, wie sich eine Änderung in der Einflussvariablen auf den Messwert auswirkt. In dem prädiktiven Modell wollen wir eine Vorhersage des Messwertes für neue Beobachtungen machen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-scatter-modeling-R-01
#| fig-align: center
#| fig-height: 5
#| fig-width: 15
#| fig-cap: "Das kausale und prädiktive Modell im statistischen Modellieren für den simplen Fall. In allen Fällen ist der Messwert $y$ normalverteilt. **(A)** *Kausales Modell* -- Wie verändert sich der Messwert $y$, wenn sich die kontinuierliche Einflussvariable $x$ ändert? Wie ist der numerische Zusammenhang zwischen $y$ und $x$? Eine Frage nach der Steigung der Graden. **(B)** *Kausales Modell* -- Wie verändert sich der Messwert $y$, wenn sich die kategoriale Einflussvariable als Faktor $f_A$ ändert? Wie ist der numerische Zusammenhang zwischen $y$ und $f_A$? Eine Frage nach der Steigung der Graden. **(C)** *Prädikitives Modell* -- Wenn $y$ und $x$ gemessen wurden, wie sehen dann die Werte von $y$ für neue $x$-Werte aus? Können wir mit der Einflussvariable $x$ die neuen Messwerte in $y$ vorhersagen? *[Zum Vergrößern anklicken]*"

source("images/R/stat-modeling-R-01.R")

p11 + p14 + p12 + 
  plot_layout(ncol = 3) +
  plot_annotation(tag_levels = 'A') 

```

Soviel zu dem allgemeinen Hintergrund. Nachdem wir jetzt auch die Fragestellungen erörtert haben, wollen wir jetzt einmal verstehen, wie die simple lineare Regression als ein statistisches Modell funktioniert und wie wir dort die Gradengleichung schätzen können. Die Berechungen kommen dann nochmal extra nach der Besprechung der Daten für die Beispiele.

## Theoretischer Hintergrund

Beginnen wir damit was wir eigentlich erreichen wollen. In der folgenden Abbildung siehst du einmal die kontinuierliche Einflussvariable $x$ und den kontinuierlichen Messwert $y$ für sieben Beobachtungen. Jede der Beobachtungen hat einen Wert für $x$ und einen Wert für $y$. Wir können dann einen Scatterplot zeichnen. Jetzt wollen wir die optimale lineare Grade durch die Punkte finden. Wir erhalten dann die Gradengleichung. Darüber hinaus wollen wir auch noch wissen, wie gut die Grade durch die Punkte läuft.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-lin-reg-theo-00
#| fig-align: center
#| fig-height: 3.75
#| fig-width: 7
#| fig-cap: "Visualisierung einer simplen linearen Regression mit einem kontinuierlichen Messwert (y) und einer kontinuierlichen Einflussvariable (x) dargestellt. Eine geschätzte grade wird durch die Punktewolke gelegt. Wie lautet die Gradengleichung und liegen die Punkte auf der Graden? *[Zum Vergrößern anklicken]*"

p1_theo_00 
```

Wie finden wir die optimlare Grade? Hier kommt die [Methode der kleinsten Quadrate](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate) (eng. *least square method*) zum Einsatz. Die Methode ist alt und wird dem Mathematiker Carl Gauß zugeschrieben. Im Jahr 1801 nutze Gaus die Methode um die Bahn des Zwergplaneten Ceres für eien kurze Zeitspanne korrekt vorherzusagen. Wir wollen hier die Methode der kleinsten Quadrate anwenden, um eben die beste Grade durch die Punkte zu finden. Wie wir in den folgenden Abbildung sehen, hat die lineare Grade eine kleinere Summe an Abweichungsquadraten als die quadratische Funktion. Wir würden hier also die blaue Grade der orangenen Graden vorziehen. Prinzipiell müssten wir also eine belibige Anzahl an Graden testen, in der Praxis gibt es aber eine geschlossende Formel für die optimale lineare Grade. Bei nicht linearen Zusammenhängen sieht es dann etwas anders aus. Dazu aber später mehr in dem entsprechenden [Kapitel zu nicht lineare Regression](#sec-non-linear).

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-lin-reg-theo-02
#| fig-align: center
#| fig-height: 5.5
#| fig-width: 11
#| fig-cap:  "Methode der kleinsten Quadrate zur Bestimmung der optimalen Grade durch eine Punktewolke. **(A)** Zwei Gradengleichungen werden beispielhaft durch die Punkte gelegt und die Abweichung der Punkte auf der Graden zu den beobachteten Werten quadriert. Anschließend werden die Abweichungsquadrate aufsummiert. **(B)** Je kleiner die Fläche der Abweichungsquadrate ist, desto besser passt die Grade zu den Punkten. Hier ist die Fläche der linearen Grade kleiner als der quadratischen Anpassung. *[Zum Vergrößern anklicken]*"

p_square_01 + p_square_02 +
  plot_layout(ncol = 2) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

::: {layout="[15,85]" layout-valign="center"}
![](images/angel_01.png){fig-align="center" width="100%"}

> *Wenn du jetzt denkst 'Hä?', dann besuche doch einmal die fantastische Seite [Explained Visually \| Ordinary Least Squares Regression](https://setosa.io/ev/ordinary-least-squares-regression/) um selber zu erfahren, was eine lineare Regression macht. Auf der Seite findest du interaktive Abbildungen, die dir das Konzept der linearen Regression sehr anschaulich nachvollziehen lassen.*
:::

Am Ende modellieren oder minimieren wir die Varianz. Den nichts anders sind ja die Abweichungsquadrate. Je kleiner die Beobachtungen um die Grade streuen, desto besser passt auch die Grade zu den Punkten. Und eine quadratische Streuung in der Statistik ist nichts anderes als die Varianz, die wir schon aus der deskriptiven Statistik kennen.

Nachdem wir die Methode der kleinsten Quadrate durchgeführt haben, kriegen wir folgende Gradengleichung wieder. Die Grade wird durch die Koeffizienten des y-Achsenabschnitts (eng. *intercept*) und der Steigung (eng. *slope*) beschrieben. Der nicht durch die Gradengleichung erklärte Rest wird auch Fehler (eng. *error*) oder im Kontext der linearen Regression dann Residuen genannt.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 2.25
#| fig-width: 7
#| fig-cap: "Formelschreibweise der simplen linearen Regression beinhaltend die Koeffizienten $\\beta_0$ für den y-Achsenabschnitt sowie $\\beta_1$ für die Steigung der Graden für eine Einflussvariable $x_1$. Die Residuen werden durch $\\epsilon$ abgebildet. *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-07

p_simple_model
```

Die Grade wird somit durch eine Gradengleichung wie oben beschreiben. Du kennst vermutlich noch die Form $y = mx + b$. In der folgenden Tabelle siehst du nochmal den Vergleich von der Schreibweise der linearen Regression in der Schule und in der Statistik. Darüber hinaus sind die deutschen Begriffe den englischen Begriffen gegenüber gestellt. Warum schreiben wir die Gleichung in der Form? Damit wir später noch weitere $\beta_px_p$-Paare ergänzen könen und so [multiple lineare Modelle](#sec-mult-reg-basic) bauen können.

| Schule | Statistik  |      Deutsch      |  Englisch   |
|:------:|:----------:|:-----------------:|:-----------:|
|  $m$   | $\beta_1$  |     Steigung      |    Slope    |
|  $x$   |   $x_1$    | Einflussvariable  | Risk factor |
|  $b$   | $\beta_0$  | y-Achsenabschnitt |  Intercept  |
|        | $\epsilon$ |     Residuen      |  Residual   |

: Vergleich und Übersicht der schulischen vs. statistischen Begriffe in den linearen Regression sowie die deutschen und englischen Begriffe. {#tbl-reg-deu-eng}

Wir können jetzt die Gradengleichung auch gleich nochmal in der Visualisierung beschriften. In der folgenden Abbildung sehen wir die Visualisierung des kovariaten Beispieldatensatzes. Die Grade läuft durch die Punktewolke und wird durch die statistischen Maßzahlen bzw. Parameter $\beta_0$, $\beta_1$ sowie den $\epsilon$ beschrieben. Wir sehen, dass das $\beta_0$ den *Intercept* darstellt und das $\beta_1$ die Steigung der Graden. Wenn wir $x$ um 1 Einheit erhöhen $x+1$, dann steigt der $y$ Wert um den Wert von $\beta_1$. Die einzelnen Abweichungen der beobachteten $y$-Wert zu den $y$-Werten auf der Grade - auch $\hat{y}$ bezeichnet - werden als Residuen oder auch $\epsilon$ abgebildet.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-lin-reg-theo-01
#| fig-align: center
#| fig-height: 3.75
#| fig-width: 7
#| fig-cap: "Visualisierung einer simplen linearen Regression mit einem kontinuierlichen Messwert (y) und einer kontinuierlichen Einflussvariable (x) dargestellt. Eine geschätzte Grade wird durch die Punktewolke gelegt. Die Grade wird durch die Koeffizienten des Intercept $\\beta_0$ sowie der Steigung $\\beta_1$ beschrieben. *[Zum Vergrößern anklicken]*"

p1_theo_01
```

Hier wird dann auch nochmal schneller klar, was der y-Achsenabschnitt ist und wie wir die Steigung zu interpretieren haben. Die Steigung sagt aus um welchen Wert sich der Messwert $Y$ ändert, wenn sich die Einflussvariable um eine Einheit erhöht. Die Residuen sind hier auch nochmal dargestellt und beschreiben eben den Rest oder Abstand, den die lineare Gradengleichung nicht an den Messwert beschreiben kann. Wir hätten nur keine Residuen, wenn die Graden sich durch alle Punkte schlängeln würde. Das geht jedoch mit einer linearen Funktion nicht. Außer die Punkte liegen alle auf einer Linie, was in einem biologischen System sehr selten vorkommt.

Schauen wir uns einmal den Zusammenhang von $y$, den beobachteten Werten, und $\hat{y}$, den geschätzen Werten auf der Grade in unserem Beispiel an. In der folgenden Tabelle sehen wir die Berechnung der einzelnen Residuen für die Grade aus der obigen Abbdilung. Wir nehmen jedes beobachtete $y$ und ziehen den Wert von $y$ auf der Grade, bezeichnet als $\hat{y}$, ab. Diesen Schritt machen wir für jedes Wertepaar $(y_i; \hat{y}_i)$. In R werden die $\hat{y}$ auch angepasste Werte (eng *fitted values*) genannt. Die $\epsilon$ Werte werden dann Residuen (eng. *residuals*) bezeichnet.

| x | y | $\boldsymbol{\hat{y}}$ | Residuen ($\boldsymbol{\epsilon}$) | Wert |
|:--:|:--:|:--:|:--:|:--:|
| 0.8 | 1.2 | 1.38 | $\epsilon_1 = y_1 - \hat{y}_1$ | $\epsilon_1 = 1.2 - 1.38 = -0.18$ |
| 1.0 | 1.8 | 1.48 | $\epsilon_2 = y_2 - \hat{y}_2$ | $\epsilon_2 = 1.8 - 1.48 = +0.32$ |
| 1.2 | 1.3 | 1.58 | $\epsilon_3 = y_3 - \hat{y}_3$ | $\epsilon_3 = 1.3 - 1.58 = -0.28$ |
| 1.9 | 1.7 | 1.94 | $\epsilon_4 = y_4 - \hat{y}_4$ | $\epsilon_4 = 1.7 - 1.94 = -0.24$ |
| 2.0 | 2.6 | 1.99 | $\epsilon_5 = y_5 - \hat{y}_5$ | $\epsilon_5 = 2.6 - 1.99 = +0.61$ |
| 2.7 | 1.8 | 2.34 | $\epsilon_6 = y_6 - \hat{y}_6$ | $\epsilon_6 = 1.8 - 2.34 = -0.54$ |
| 2.8 | 2.7 | 2.40 | $\epsilon_7 = y_7 - \hat{y}_7$ | $\epsilon_7 = 2.7 - 2.40 = +0.30$ |

: Zusammenhang zwischen den $y$, den beobachteten Werten, und $\hat{y}$, den geschätzen Werten auf der Grade. Wir nennen den Abstand $y_i - \hat{y}_i$ auch Residuum oder *Epsilon* $\epsilon$. {#tbl-lin-reg-epsilon}

```{r}
#| echo: false

epsilon <- c(-0.18, 0.32, -0.28, -0.24, 0.61, -0.54, 0.30)
mean_e <- round(mean(epsilon), 2)
var_e <- round(var(epsilon), 2)

```

Die Residuen haben einen Mittelwert von $\bar{\epsilon} = `r mean_e`$ und eine Varianz von $s^2_{\epsilon} = `r var_e`$. Wir schreiben, dass die Residuen normalverteilt sind mit $\epsilon \sim \mathcal{N}(0, s^2_{\epsilon})$. Wir zeichnen die Grade also so durch die Punktewolke, dass die Abstände zu den Punkten, die Residuen, im Mittel null sind. Die Optimierung erreichen wir in dem wir die Varianz der Residuuen minimieren. Folglich modellieren wir die Varianz.

::: callout-tip
## Weitere Tutorien für die simple lineare Regression

Wir immer geht natürlich mehr als ich hier Vorstellen kann. Du findest im Folgenden Tutorien, die mich hier in dem Kapitel inspiriert haben. Ich habe mich ja in diesem Kapitel auf die Durchführbarkeit in R und die allgemeine Verständlichkeit konzentriert. Es geht aber natürlich wie immer auch mathematischer...

-   Wir funktioniert nun so eine lineare Regression und was sind den jetzt eigentlich die Koeffizienten $\beta_0$ und $\beta_1$ eigentlich? Hier gibt es die fantastische Seite [Explained Visually \| Ordinary Least Squares Regression](https://setosa.io/ev/ordinary-least-squares-regression/), die dir nochmal erlaubt selbe mit Punkten in einem Scatterplot zu spielen und zu sehen wie sich dann die Regressionsgleichung ändert.
-   Du kannst auf der Seite [Manual linear regression analysis using R](https://davetang.org/muse/2012/02/10/manual-linear-regression-analysis-using-r/) nochmal weiter über die lineare Regression lesen. Der Blogpost ist sehr umfangreich und erklärt nochmal schrittweise, wie die lineare Regression in R per hand funktioniert.
-   [Simple Regression auf Wikipedia](https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line) mit weit mehr Informationen zu den Formeln und den Zusammenhängen. Ein toller zusammenfassender Artikel.
-   Mehr zum Modellieren findest du im Openbook "Statistical Thinking for the 21st Century" in den beiden Kapiteln [Statistical Thinking for the 21st Century --- What is a model?](https://statsthinking21.github.io/statsthinking21-core-site/fitting-models.html#what-is-a-model) und [Statistical Thinking for the 21st Century --- Practical statistical modeling?](https://statsthinking21.github.io/statsthinking21-core-site/practical-example.html#the-process-of-statistical-modeling)
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, broom,
               readxl, ggpmisc, conflicted)
conflicts_prefer(magrittr::set_names)
conflicts_prefer(ggplot2::annotate)
```

An der Seite des Kapitels findest du den Link *Quellcode anzeigen*, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.

## Daten

#### Theoretischer Datensatz {.unnumbered .unlisted}

```{r}
#| message: false
snake_tbl <- read_xlsx("data/regression_data.xlsx", sheet = "theory")
```

In der folgenden Tabelle ist der Datensatz `snake_tbl` nochmal dargestellt. Wir haben die Schlangenlänge `svl` als Messwert $y$ sowie das Gewicht der Schlangen `mass`, die Sammelregion `region` und die Farbe der Schlangen `color`. Dabei ist `mass` eine kontinuierliche Variable, `region` eine kategorielle Variable als Faktor mit zwei Leveln und `color` eine kategorielle Variable als Faktor mit drei Leveln.

```{r}
#| message: false
#| echo: false
#| tbl-cap: "Datensatz zu Schlangen ist entlehnt und modifiiert nach @kery2010introduction."
#| label: tbl-snakes

snake_tbl |> 
  kable(align = "c", "pipe")

```

#### Einkovariater Datensatz {.unnumbered .unlisted}

Wir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die lineare Regression auf sehr großen Datensätzen anwenden, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit $n=7$ Beobachtungen. In der folgenden Tabelle ist der Datensatz `cov1_tbl` dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in \[cm\] und dem Gewicht in \[mg\] für sieben Beobachtungen modellieren.

```{r}
#| message: false

cov1_tbl <- read_xlsx("data/regression_data.xlsx", sheet = "covariate")
```

Dann schauen wir uns die Daten einmal in der folgenden Tabelle als Auszug einmal an. Wichtig ist hier nochmal, dass du eben einen Faktor `animal` mit drei Leveln also Gruppen vorliegen hast. Wir wollen jetzt die drei Tierarten hinsichtlich ihrer Sprungweite in \[cm\] miteinander vergleichen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-1cov-table
#| tbl-cap: "Tabelle der Sprungweiten in [cm] als Messwert $y$ von Hunde-, Katzen- und Fuchsflöhen. Der Datensatz ist einfaktoriell, da wir nur einen Faktor vorliegen haben."

read_xlsx("data/regression_data.xlsx", sheet = "covariate") |> 
  kable(align = "c", "pipe")
```

In der folgenden Abbildung sehen wir die Visualisierung der Daten `cov1_tbl` in einem Scatterplot mit einer geschätzen Grade.

```{r}
#| echo: false
#| message: false
#| label: fig-scatter-lin-01
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 7
#| fig-cap: "Scatterplot der Beobachtungen der Sprungweite in \\[cm\\] und dem Gewicht in \\[mg\\]. Die Grade verläuft mittig durch die Punkte."

ggplot(cov1_tbl, aes(weight, jump_length)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  theme_minimal() +
  xlim(0, 3.5) + ylim(0, 3.5)
```

#### Einfaktorieller Datensatz {.unnumbered .unlisted}

```{r}
#| message: false

fac1_tbl <- read_xlsx("data/regression_data.xlsx", sheet = "factorial") |>
  select(animal, jump_length) |> 
  mutate(animal = as_factor(animal))
```

Dann schauen wir uns die Daten einmal in der folgenden Tabelle als Auszug einmal an. Wichtig ist hier nochmal, dass du eben einen Faktor `animal` mit drei Leveln also Gruppen vorliegen hast. Wir wollen jetzt die drei Tierarten hinsichtlich ihrer Sprungweite in \[cm\] miteinander vergleichen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-1fac-table
#| tbl-cap: "Tabelle der Sprungweiten in [cm] als Messwert $y$ von Hunde-, Katzen- und Fuchsflöhen. Der Datensatz ist einfaktoriell, da wir nur einen Faktor vorliegen haben."

fac1_raw_tbl <- read_xlsx("data/flea_dog_cat_fox.xlsx") |>
  select(animal, jump_length) 

rbind(head(fac1_raw_tbl, n = 3),
      rep("...", times = ncol(fac1_raw_tbl)),
      tail(fac1_raw_tbl, n = 3)) |> 
  kable(align = "c", "pipe")
```

Und dann wollen wir uns noch einmal die Daten als einen einfachen Boxplot anschauen. Wir sehen, dass die Daten so gebaut sind, dass wir einen signifikanten Unterschied zwischend den Sprungweiten der Floharten erwarten. Die Boxen der Boxplots überlappen sich nicht und die Boxplots liegen auch nicht auf einer Ebene.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-ggplot-simple-reg-boxplot-1fac
#| fig-align: center
#| fig-height: 4
#| fig-width: 4
#| fig-cap: "Beispielhafter einfaktorieller Boxplot für die Sprungweiten in [cm] gruppiert nach den Floharten."

ggplot(data = fac1_tbl, 
       aes(x = animal, y = jump_length, fill = animal)) +
  theme_minimal() +
  geom_boxplot() +
  stat_summary(fun.y = mean, geom = "point", 
               shape = 23, size = 3, fill = "gray50") +
  labs(x = "Flohart", y = "Sprungweite in [cm]") +
  theme(legend.position = "none") + 
  scale_fill_okabeito() 
```

## Kausales Modell

### Einkovariates Modell

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 2
#| fig-width: 7
#| fig-cap: "Schemantisches simples Modell mit einem Messwert $Y$ und einer kontinuierlichen Einflussvariable als Kovariate $c_1$ dargestellt. *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-10

p_1cov_model 
```

::: panel-tabset
## Praktisch in R

```{r}
lm(jump_length ~ weight, data = cov1_tbl) |> coef() |> round(2)
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "foo *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-c1

ggplot(cov1_tbl, aes(weight, jump_length)) +
  theme_modeling() +
  geom_vline(xintercept = 0, linewidth = 0.5, color = "grey50") +
  geom_hline(yintercept = 0, linewidth = 0.5, color = "grey50") +
  geom_point() +
  stat_poly_line(color = "#E69F00", linewidth = 0.5, 
                 fullrange = TRUE, se = FALSE) +
  stat_poly_eq(use_label("eq"), size = 7) +
  labs(title = "Simple lineare Regression",
       subtitle = "Intercept ist der y-Achsenabschnitt",
       x = "Gewicht [mg]", y = "Sprungweite [cm]")
```

Im Folgenden rechnen wir ein kausales Modell, da wir an dem Effekt des $x$ interessiert sind. Wenn also das $x_1$ um eine Einheit ansteigt, um wie viel verändert sich dann das $y$? Der Schätzer $\beta_1$ gibt uns also den Einfluss oder den kausalen Zusammenhang zwischen $y$ und $x_1$ wieder. Im ersten Schritt schauen wir uns die Ausgabe der Funktion `lm()` in der Funktion `summary()` an. Daher pipen wir das Objekt `fit_1` in die Funktion `summary()`.

```{r}
cov1_fit <- lm(jump_length ~ weight, data = cov1_tbl) 
cov1_fit |> summary()
```

```{r}
aov(jump_length ~ weight, data = cov1_tbl) |> summary()
```

Wir erhalten folgende Ausgabe dargestellt in @fig-lin-reg-3.

Was sehen wir in der Ausgabe der `summary()` Funktion? Als erstes werden uns die Residuen wiedergegeben. Wenn wir nur wenige Beobachtungen haben, dann werden uns die Residuen direkt wiedergegeben, sonst die Verteilung der Residuen. Mit der Funktion `augment()` aus dem R Paket `{broom}` können wir uns die Residuen wiedergeben lassen. Die Residuen schauen wir uns aber nochmal im @sec-lin-reg-quality genauer an.

```{r}
cov1_fit |> augment()
```

Im zweiten Block erhalten wir die Koeffizienten (eng. *coefficients*) der linearen Regression. Das heißt, wir kriegen dort $\beta_0$ als y-Achsenabschnitt sowie die Steigung $\beta_1$ für das Gewicht. Dabei ist wichtig zu wissen, dass immer als erstes der y-Achsenabschnitt `(Intercept)` auftaucht. Dann die Steigungen der einzelnen $x$ in dem Modell. Wir haben nur *ein* kontinuierliches $x$, daher ist die Interpretation der Ausgabe einfach. Wir können die Gradengleichung wie folgt formulieren.

$$
jump\_length \sim 0.97 + 0.51 \cdot weight
$$

Was heißt die Gleichung nun? Wenn wir das $x$ um eine Einheit erhöhen dann verändert sich das $y$ um den Wert von $\beta_1$. Wir haben hier eine Steigung von $0.51$ vorliegen. Ohne Einheit keine Interpretation! Wir wissen, dass das Gewicht in \[mg\] gemessen wurde und die Sprungweite in \[cm\]. Damit können wir aussagen, dass wenn ein Floh 1 mg mehr wiegt der Floh 0.51 cm weiter springen würde.

Schauen wir nochmal in die *saubere* Ausgabe der `tidy()` Funktion. Wir sehen nämlich noch einen $p$-Wert für den Intercept und die Steigung von `weight`.

```{r}
cov1_fit |> tidy()
```

Wenn wir einen $p$-Wert sehen, dann brauchen wir eine Nullhypothese, die wir dann eventuell mit der Entscheidung am Signifikanzniveau $\alpha$ von 5% ablehnen können. Die Nullhypothese ist die Gleichheitshypothese. Wenn es also keinen Effekt von dem Gewicht auf die Sprungweite gebe, wie groß wäre dann $\beta_1$? Wir hätten dann keine Steigung und die Grade würde parallel zur x-Achse laufen. Das $\beta_1$ wäre dann gleich null.

$$
\begin{align*} 
H_0: \beta_i &= 0\\  
H_A: \beta_i &\neq 0 \\   
\end{align*}
$$

Wir haben für jedes $\beta_i$ ein eigenes Hypothesenpaar. Meistens interessiert uns der Intercept nicht. Ob der Intercept nun durch die Null geht oder nicht ist eher von geringem Interessen.

Spannder ist aber wie sich der $p$-Wert berechnet. Der $p$-Wert basiert auf einer t-Statistik, also auf dem t-Test. Wir rechnen für jeden Koeffizienten $\beta_i$ einen t-Test. Das machen wir in dem wir den Koeffizienten `estimate` durch den Fehler des Koeffizienten `std.error` teilen.

$$
\begin{align*} 
T_{(Intercept)} &= \cfrac{\mbox{estimate}}{\mbox{std.error}}  = \cfrac{0.969}{0.445} = 2.18\\  
T_{weight} &= \cfrac{\mbox{estimate}}{\mbox{std.error}}  = \cfrac{0.510}{0.232} = 2.20\\   
\end{align*}
$$

Wir sehen in diesem Fall, dass weder der Intercept noch die Steigung von `weight` signifikant ist, da die $p$-Werte mit $0.081$ und $0.079$ leicht über dem Signifikanzniveau von $\alpha$ gleich 5% liegen. Wir haben aber einen starkes Indiz gegen die Nullhypothese, da die Wahrscheinlichkeit die Daten zu beobachten sehr gering ist unter der Annahme das die Nullhypothese gilt.

Zun Abschluß noch die Funktion `glance()` ebenfalls aus dem R Paket `{broom}`, die uns erlaubt noch die Qualitätsmaße der linearen Regression zu erhalten. Wir müssen nämlich noch schauen, ob die Regression auch funktioniert hat. Die Überprüfung geht mit einem $x$ sehr einfach. Wir können uns die Grade ja anschauen. Das geht dann mit einem Model mit mehreren $x$ nicht mehr und wir brauchen andere statistische Maßzahlen.

```{r}
cov1_fit |> glance() 
```

## Theoretisch in R

Im ersten Schritt wollen wir uns einmal das Modell mit einem kontinuierlichen $x$ anschauen. Daher bauen wir uns ein lineares Modell mit der Variable `mass`. Wir erinnern uns, dass `mass` eine kontinuierliche Variable ist, da wir hier nur Zahlen in der Spalte finden. Die Funktion `model.matrix()` gibt uns die Modellmatrix wieder.

```{r}
model.matrix(svl ~ mass, data = snake_tbl) |> as_tibble()
```

In der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte `mass` als kontinuierliche Variable.

Wir können die Modellmatrix auch mathematisch schreiben und die $y$ Spalte für das Outcome `svl` ergänzen. Eben so ergänzen wir die $\beta$-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Grade.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  1 & 6 \\
  1 & 8 \\
  1 & 5 \\
  1 & 7 \\
  1 & 9 \\
  1 & 11\\
  1 & 12\\
  1 & 10\\
 \end{pmatrix}
 \times
  \begin{pmatrix}
  \beta_0 \\
  \beta_{mass} 
 \end{pmatrix} +
  \begin{pmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5 \\
  \epsilon_6 \\
  \epsilon_7 \\
  \epsilon_8 \\
 \end{pmatrix}
$$

Jetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt mit der Funktion `lm()` fitten. Wir nutzen dann die Funktion `coef()` um uns die Koeffizienten aus dem Objekt `fit_1` wiedergeben zu lassen.

```{r}
fit_1 <- lm(svl ~ mass, data = snake_tbl) 
fit_1 |> coef() |> round(2)

```

Die Funktion `residuals()` gibt uns die Residuen der Graden aus dem Objekt `fit_1` wieder.

```{r}
fit_1 |> residuals() |> round(2)
```

Wir können jetzt die Koeffizienten in die Modellmatrix ergänzen. Wir haben den Intercept mit $\beta_0 = 26.71$ geschätzt. Weiter ergänzen wir die Koeffizienten aus dem linearen Modell für `mass` mit $\beta_{mass}=2.61$. Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein. Wir erhalten dann folgende ausgefüllte Gleichung mit den Matrixen.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  26.71 & \phantom{0}6 \cdot 2.61\\
  26.71 & \phantom{0}8 \cdot 2.61\\
  26.71 & \phantom{0}5 \cdot 2.61\\
  26.71 & \phantom{0}7 \cdot 2.61\\
  26.71 & \phantom{0}9 \cdot 2.61\\
  26.71 & 11\cdot 2.61\\
  26.71 & 12\cdot 2.61\\
  26.71 & 10\cdot 2.61\\
 \end{pmatrix}
  +
  \begin{pmatrix}
  -2.36\\
  -2.57\\
  -0.75 \\
  +6.04\\
  +1.82\\
  +1.61\\
  \phantom{+}0.00\\
  -3.79\\
 \end{pmatrix}
$$

Wir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis. Wie du siehst ergänzen wir hier noch eine Reihe von $+$ um den Intercept mit der Steigung zu verbinden. Steht ja auch so in der Gleichung des linearen Modells drin, alles wird mit einem $+$ miteinander verbunden.

```{r}
c(26.71 +  6*2.61 - 2.36,
  26.71 +  8*2.61 - 2.57,
  26.71 +  5*2.61 - 0.75,
  26.71 +  7*2.61 + 6.04,
  26.71 +  9*2.61 + 1.82,
  26.71 + 11*2.61 + 1.61,
  26.71 + 12*2.61 + 0.00,
  26.71 + 10*2.61 - 3.79) |> round() 
```

Oh ha! Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome $y$ hat. Das heißt, die ganze Sache hat funktioniert.
:::

:::: callout-note
## Simple lineare Regression händisch

Gut, das war jetzt die theoretische Abhandlung der linearen Regression ohne eine mathematische Formel. Es geht natürlich auch mit den nackten Zahlen. In der folgenden Tabelle siehst du einmal sieben Beobachtungen mit dem Körpergewicht als $y$ sowie der Körpergröße als $x$. Wir wollen jetzt einmal die Regressionsgleichung bestimmen. Wir sehen also unsere Werte für $\beta_0$ und $\beta_1$ aus?

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-example-by-hand-01
#| tbl-cap: "Sieben Messungen der Körpergröße $x$ und dem zugehörigen Körpergewicht $y$."

by_hand_tbl <- tibble(height = c(167, 188, 176, 186, 192, 205, 198),
                      weight = c(70, 83, 81, 90, 94, 100, 106)) 
by_hand_tbl |> 
  kable(align = "c", "pipe")
```

Damit wir einmal wissen, was wir als Lösung erhalten würden, hier einmal die lineare Regression mit der Funktion $lm()$ und die entsprechenden Werte für den `(Intercept)` und der Steigung.

```{r}
lm(weight ~ height, data = by_hand_tbl) |> 
  coef()
```

Wir suchen dann damit die folgende Regressionsgleichung mit der Körpergröße als $x$ und dem zugehörigen Körpergewicht als $y$.

$$
weight = \beta_0 + \beta_1 \cdot height
$$

Da es dann immer etwas schwer ist, sich den Zusammenhang zwischen Körpergewicht und Körpergröße vorzustellen, habe ich nochmal in der folgenden Abbildung den Scatterplot der Daten erstellt. Die rote Grade stellt die Regressiongleichung dar. Wir erhalten ein y-Achsenabschnitt mit $\beta_0$ von $-75.39$ sowie eine Steigung mit $\beta_1$ von $0.88$ aus unseren Daten.

```{r}
#| echo: true
#| message: false
#| label: fig-example-by-hand-03
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 7
#| fig-cap: "Scatterplot der sieben Messungen der Körpergröße $x$ und dem zugehörigen Körpergewicht $y$ sowie der RegressionsGrade mit $y = -75.39 + 0.88 \\cdot x$. Die Grade verlauf wie erwartet mittig durch die Punktewolke."

ggplot(by_hand_tbl, aes(height, weight)) +
  theme_minimal() +
  geom_point() +
  geom_function(fun = \(x) -75.39 + 0.88 * x, color = "#CC79A7")
  
```

Jetzt stellt sich die Frage, wie wir *händisch* die Werte für den y-Achsenabschnitt mit $\beta_0$ sowie der Steigung mit $\beta_1$ berechnen. Dafür gibt es jeweils eine Formel. Hier müssen wir dann sehr viele Summen berechnen, was ich dann gleich einmal in einer Tabelle zusammenfasse.

Formel für y-Achsenabschnitt mit $\beta_0$

:   $$
    \beta_0 = \cfrac{(\Sigma Y)(\Sigma X^2) - (\Sigma X)(\Sigma XY)}{n(\Sigma X^2) - (\Sigma X)^2}
    $$

Formel für Steigung mit $\beta_1$

:   $$
    \beta_1 = \cfrac{n(\Sigma XY) - (\Sigma X)(\Sigma Y)}{n(\Sigma X^2) - (\Sigma X)^2} 
    $$

In der folgenden Tabelle siehst du nochmal die originalen Datenpunkte und dann die entsprechenden Werte für das Produkt von `weight` und `height` mit $XY$ und dann die jeweiligen Quadrate der beiden mit $X^2$ und $Y^2$. Wir brauchen dann aber nicht diese Werte sondern die Summen der Werte. Das Summieren lagere ich dann nochmal in eine weitere Tabelle aus.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-example-by-hand-02
#| tbl-cap: "Berechnungen des Produkts von $X$ und $Y$ sowie deren Quadrate mit $X^2$ und $Y^2$."
by_hand_tbl |> 
  mutate(a = height * weight,
         b = height^2,
         c = weight^2) |> 
  set_names(c("height", "weight", "$XY$", "$X^2$", "$Y^2$")) |> 
  kable(align = "c", "pipe")
```

In der abschließenden Tabelle findest du dann einmal die Summen der beobachteten Werte $X$ und $Y$ sowie des Produkts von $X$ und $Y$ sowie deren Quadrate mit $X^2$ und $Y^2$. Damit haben wir dann alles zusammen um die Formel oben zu füllen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-example-by-hand-03
#| tbl-cap: "Summe der Datenpunkte für $X$ und $Y$ sowie des Produkts von $X$ und $Y$ sowie deren Quadrate mit $X^2$ und $Y^2$"
tibble(height = sum(by_hand_tbl$height),
       weight = sum(by_hand_tbl$weight),
       a = sum(by_hand_tbl$height * by_hand_tbl$weight), 
       b = sum(by_hand_tbl$height^2), 
       c = sum(by_hand_tbl$weight^2),) |> 
  set_names(c("height $(\\Sigma X)$", "weight $(\\Sigma Y)$", "$\\Sigma XY$", "$\\Sigma X^2$", "$\\Sigma Y^2$")) |> 
  kable(align = "c", "pipe")
```

Ich habe dann die ganzen Summen einmal händisch berechnet und dann in den Formeln von oben eingesetzt. Wir erhalten dann für den y-Achsenabschnitt $\beta_0$ folgenden Wert.

$$
\beta_0 = \cfrac{624 \cdot 246898 - 1312 \cdot 117826}{7\cdot 246898 - 1312^2} = -75.39038
$$

Die ganze Berechnung habe ich dann auch einmal für die Steigung $\beta_1$ ebenfalls einmal durchgeführt.

$$
b_1 = \cfrac{7\cdot 117826 - 1312\cdot624}{7\cdot246898 - 1312^2} = 0.877845
$$

Wir sehen, es kommen die gleichen Werte für den y-Achsenabschnitt $\beta_0$ und die Steigung $\beta_1$ raus. Das hat ja schonmal sehr gut geklappt. Eine andere Art die gleiche Werte effizienter zu berechnen ist die Matrixberechnung der Koeffizienten der linearen Regression. Wir könnten dann auch komplexere Modelle mit mehr als nur einem $x$ und einem $\beta_1$ berechnen. Die grundlegende Formel siehst du einmal im Folgenden dargestellt.

$$
\begin{pmatrix}
\beta_0 \\ 
\beta_1 
\end{pmatrix}
= \mathbf{(X^T X)^{−1}(X^T Y)}
$$

Wir brauchen jetzt einiges an Matrixrechnung um die jeweiligen Formelteile zu berechnen. Ich habe dir in den folgenden Tabs einmal Schritt für Schritt die einzelnen Teile berechnet. Wir immer machen wir das eigentlich nicht so richtig per Hand, sondern nutzen einen Computer. Prinzipiell wäre eine händische Lösung natürlich möglich.

::: panel-tabset
## $X$

```{r}
X <- as.matrix(c(167, 188, 176, 186, 192, 205, 198))
X <- cbind(rep(1, 7), X) 
X 
```

## $Y$

```{r}
Y <- as.matrix(c(70, 83, 81, 90, 94, 100, 106))
Y
```

## $X^T$

```{r}
Xt <- t(X) 
Xt
```

## $X^T X$

```{r}
XtX <- Xt %*% X
XtX
```

## $X^T Y$

```{r}
XtY <- Xt %*% Y
XtY
```

## $(X^T X)^{−1}$

```{r}
XtXinv <- solve(XtX)
XtXinv
```
:::

Am Ende müssen wir dann alle Teile in der Form $\mathbf{(X^T X)^{−1}(X^T Y)}$ einmal zusammenbringen. Das siehst dann in R wie folgt aus. Wir erhalten dann eine Matrix wieder wobei die erste Zeile der y-Achsenabschnitt $\beta_0$ und die zweite Zeile die Steigung $\beta_1$ ist. Wir erhalten fast die gleichen Werte wie auch schon oben.

```{r}
XtXinv %*% Xt %*% Y
```

Wenn du dich tiefer in die Thematik einlesen willst, dann sind hier weitere Quellen zu der Thematik unter den folgenden Links und Tutorien.

-   [Hands-On Machine Learning with R \| Linear Regression](https://bradleyboehmke.github.io/HOML/linear-regression.html)
-   [Manual linear regression analysis using R](https://davetang.org/muse/2012/02/10/manual-linear-regression-analysis-using-r/)
-   [Linear Regression by Hand](https://towardsdatascience.com/linear-regression-by-hand-python-and-r-79994d47f68)
-   [How to Perform Linear Regression by Hand](https://www.statology.org/linear-regression-by-hand/)
-   [Matrix Approach to Simple Linear Regression in R](https://lasanthiwatagoda.github.io/STT4830ClassRepo/RCodes/Ch5Script.html)
::::

### Einfaktorielles Modell

#### $f_A$ mit 2 Leveln {.unnumbered .unlisted}

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 2
#| fig-width: 7
#| fig-cap: "Schemantisches simples Modell mit einem Messwert $Y$ und einer kategorialen Einflussvariablen als Faktor $f_A$ mit zwei Leveln dargestellt. *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-15

p_1fac_2lvl_model
```

#### $f_A$ mit \>2 Leveln {.unnumbered .unlisted}

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 2
#| fig-width: 7
#| fig-cap: "Schemantisches simples Modell mit einem Messwert $Y$ und einer kategorialen Einflussvariablen als Faktor $f_A$ mit drei Leveln dargestellt. *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-11

p_1fac_3lvl_model
```

::: panel-tabset
## Theoretisch

## Theoretisch in R

#### $f_A$ mit 2 Leveln {.unnumbered .unlisted}

Im diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen $x$ mit 2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable ``` region``. Die Funktion ```model.matrix()\` gibt uns die Modelmatrix wieder.

```{r}
model.matrix(svl ~ region, data = snake_tbl) |> as_tibble()
```

In der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte `regionnord`. In dieser Spalte steht die Dummykodierung für die Variable `region`. Die ersten drei Schlangen kommen nicht aus der Region `nord` und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region `nord` und erhalten daher eine 1 in der Spalte.

Wir können die Modellmatrix auch mathematisch schreiben und die $y$ Spalte für das Outcome `svl` ergänzen. Eben so ergänzen wir die $\beta$-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Grade.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  1 &  0  \\
  1 &  0 \\
  1 &  0\\
  1 &  1\\
  1 &  1 \\
  1 &  1 \\
  1 &  1 \\
  1 &  1 \\
 \end{pmatrix}
 \times
  \begin{pmatrix}
  \beta_0 \\
  \beta^{region}_{nord} \\
 \end{pmatrix} +
  \begin{pmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5 \\
  \epsilon_6 \\
  \epsilon_7 \\
  \epsilon_8 \\
 \end{pmatrix}
$$

Jetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion `coef()` um uns die Koeffizienten wiedergeben zu lassen.

```{r}
fit_2 <- lm(svl ~ region, data = snake_tbl) 
fit_2 |> coef() |> round(2)
```

Die Funktion `residuals()` gibt uns die Residuen der Graden aus dem Objekt `fit_2` wieder.

```{r}
fit_2 |> residuals() |> round(2)
```

Wir können jetzt die Koeffizienten ergänzen mit $\beta_0 = 41.33$ für den Intercept. Weiter ergänzen wir die Koeffizienten für die Region und das Level `nord` mit $\beta^{region}_{nord} = 12.07$. Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  41.33 & 0 \cdot 12.07  \\
  41.33 & 0 \cdot 12.07  \\
  41.33 & 0 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
  41.33 & 1 \cdot 12.07  \\
 \end{pmatrix} +
  \begin{pmatrix}
  -1.33\\
  +3.67 \\
  -2.33 \\
  -2.40 \\
  -1.40 \\
  +3.60 \\
  +4.60 \\
  -4.40 \\
 \end{pmatrix}
$$

Wir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.

```{r}
c(41.33 + 0*12.07 - 1.33,
  41.33 + 0*12.07 + 3.67,
  41.33 + 0*12.07 - 2.33,
  41.33 + 1*12.07 - 2.40,
  41.33 + 1*12.07 - 1.40,
  41.33 + 1*12.07 + 3.60,
  41.33 + 1*12.07 + 4.60,
  41.33 + 1*12.07 - 4.40) |> round() 
```

Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome $y$ hat.

#### $f_A$ mit \>2 Leveln {.unnumbered .unlisted}

Im diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen $x$ mit \>2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable `color`. Die Funktion `model.matrix()` gibt uns die Modelmatrix wieder.

```{r}
model.matrix(svl ~ color, data = snake_tbl) |> as_tibble()
```

In der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalten für `color`. Die Spalten `colorrot` und `colorblau` geben jeweils an, ob die Schlange das Level `rot` hat oder `blau` oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.

Wir können die Modellmatrix auch mathematisch schreiben und die $y$ Spalte für das Outcome `svl` ergänzen. Eben so ergänzen wir die $\beta$-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Grade.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  1 & 0 & 0 \\
  1 & 0 & 0\\
  1 & 1 & 0\\
  1 & 1 & 0\\
  1 & 1 & 0\\
  1 & 0 & 1\\
  1 & 0 & 1\\
  1 & 0 & 1\\
 \end{pmatrix}
 \times
  \begin{pmatrix}
  \beta_0 \\
  \beta^{color}_{rot} \\
  \beta^{color}_{blau} \\
 \end{pmatrix} +
  \begin{pmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5 \\
  \epsilon_6 \\
  \epsilon_7 \\
  \epsilon_8 \\
 \end{pmatrix}
$$

Jetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion `coef()` um uns die Koeffizienten wiedergeben zu lassen.

```{r}
fit_3 <- lm(svl ~ color, data = snake_tbl) 
fit_3 |> coef() |> round(2)
```

Die Funktion `residuals()` gibt uns die Residuen der Graden aus dem Objekt `fit_3` wieder.

```{r}
fit_3 |> residuals() |> round(2)
```

Wir können jetzt die Koeffizienten ergänzen mit $\beta_0 = 25$ für den Intercept. Weiter ergänzen wir die Koeffizienten für die Farbe und das Level `rot` mit $\beta^{color}_{rot} = 4.83$ und für die Farbe und das Level `blau` mit $\beta^{color}_{blau} = 12.17$. Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.

$$
 \begin{pmatrix}
  40 \\
  45 \\
  39 \\
  50 \\
  52 \\
  57 \\
  58 \\
  59 \\
 \end{pmatrix}
 =
  \begin{pmatrix}
  42.50 & 0 \cdot 4.83& 0 \cdot 12.17 \\
  42.50 & 0 \cdot 4.83& 0 \cdot 12.17\\
  42.50 & 1 \cdot 4.83& 0 \cdot 12.17\\
  42.50 & 1 \cdot 4.83& 0 \cdot 12.17\\
  42.50 & 1 \cdot 4.83& 0 \cdot 12.17\\
  42.50 & 0 \cdot 4.83& 1 \cdot 12.17\\
  42.50 & 0 \cdot 4.83& 1 \cdot 12.17\\
  42.50 & 0 \cdot 4.83& 1 \cdot 12.17\\
 \end{pmatrix} +
  \begin{pmatrix}
  -2.50 \\
  +2.50 \\
  -8.33 \\
  +3.67 \\
  +4.67 \\
  +2.33 \\
  +3.33 \\
  -5.67 \\
 \end{pmatrix}
$$

Wir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.

```{r}
c(42.50 + 0*4.83 + 0*-12.17 - 2.50,
  42.50 + 0*4.83 + 0*-12.17 + 2.50,
  42.50 + 1*4.83 + 0*-12.17 - 8.33,
  42.50 + 1*4.83 + 0*-12.17 + 3.67,
  42.50 + 1*4.83 + 0*-12.17 + 4.67,
  42.50 + 0*4.83 + 1*-12.17 + 2.33,
  42.50 + 0*4.83 + 1*-12.17 + 3.33,
  42.50 + 0*4.83 + 1*-12.17 - 5.67) |> round()
```

Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome $y$ hat.

## Praktisch in R

```{r}
lm(svl ~ region, data = snake_tbl) |> coef() |> round(2)
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "foo *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-fa-1

ggplot(snake_tbl, aes(region, svl)) +
  theme_modeling() +
  geom_vline(xintercept = 1, linewidth = 0.5, color = "grey50") +
  geom_hline(yintercept = 0, linewidth = 0.5, color = "grey50") +
  geom_hline(yintercept = 41.33, linewidth = 0.5, color = "#CC79A7") +
  geom_point() +
  stat_summary(fun = "mean", geom = "label", 
               aes(label = round(..y.., 2)), position = position_nudge(0.12)) +
  scale_x_discrete(labels = c("west", "nord")) +
  labs(title = "Treatment coding",
       subtitle = "Intercept ist der Mittelwert von Gruppe A.1",
       x = "Messregion", y = "Schlangenlänge [cm]")
```

```{r}
lm(svl ~ color, data = snake_tbl) |> 
  coef() |> round(2)
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "foo *[Zum Vergrößern anklicken]*"
#| label: fig-model-in-R-fa-21

ggplot(snake_tbl, aes(color, svl)) +
  theme_modeling() +
  geom_vline(xintercept = 1, linewidth = 0.5, color = "grey50") +
  geom_hline(yintercept = 0, linewidth = 0.5, color = "grey50") +
  geom_hline(yintercept = 42.50, linewidth = 0.5, color = "#CC79A7") +
  geom_point() +
  stat_summary(fun = "mean", geom = "label", 
               aes(label = round(..y.., 2)), position = position_nudge(0.17)) +
  scale_x_discrete(labels = c("schwarz", "rot", "blau")) +
  labs(title = "Treatment coding",
       subtitle = "Intercept ist der Mittelwert von Gruppe A.1",
       x = "Hautfarbe", y = "Schlangenlänge [cm]")
```
:::

## Prädiktives Modell

Neben dem kausalen Modell gibt es auch die Möglichkeit ein prädiktives Modell zu rechnen. Im Prinzip ist die Sprache hier etwas ungenau. Wir verwenden das gefittete Modell nur anders. Anstatt das Modell `fit_1` in die Funktion `summary()` zu pipen, pipen wir die das Modell in die Funktion `predict()`. Die Funktion `predict()` kann dann für neue Daten über die Option `newdata =` das $y$ vorhersagen.

In unserem Fall müssen wir uns deshalb ein `tibble` mit einer Spalte bauen. Wir haben ja oben im Modell auch nur ein $x_1$ mit aufgenommen. Später können wir natürlich auch für multiple Modelle die Vorhersage machen. Wichtig ist, dass die Namen gleich sind. Das heißt in dem neuen Datensatz müssen die Spalten *exakt* so heißen wir in dem alten Datensatz in dem das Modell gefittet wurde.

```{r}
fit_1 <- lm(jump_length ~ weight, data = cov1_tbl)

simple_new_tbl <- tibble(weight = c(1.7, 1.4, 2.1, 3.0)) 

predict(fit_1, newdata = simple_new_tbl) |> round(2)
```

Wie wir sehen ist die Anwendung recht einfach. Wir haben die vier `jump_length` Werte vorhergesagt bekommen, die sich mit dem Fit des Modells mit den neuen `weight` Werten ergeben. In @fig-scatter-lin-pred sehen wir die Visualisierung der vier vorhergesagten Werte. Die Werte müssen auf der Graden liegen.

```{r}
#| echo: false
#| message: false
#| label: fig-scatter-lin-pred
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 7
#| fig-cap: "Scatterplot der *alten* Beobachtungen der Sprungweite in \\[cm\\] und dem Gewicht in \\[mg\\]. Sowie der *neuen* vorhergesagten Beobachtungen auf der Graden."

pred_tbl <- bind_rows(mutate(cov1_tbl, status = "beobachtet"),
                      tibble(weight = c(1.7, 1.4, 2.1, 3.0),
                             jump_length = predict(fit_1, newdata = tibble(weight)),
                             status = "vorhergesagt"))

ggplot(pred_tbl, aes(weight, jump_length, color = status, shape = status)) +
  stat_smooth(method = "lm", se = FALSE, fullrange = TRUE, 
              color = "black") +
  labs(color = "", shape = "") +
  geom_point(size = 4) +
  theme_minimal() +
  scale_color_okabeito() +
  xlim(0, 3.5) + ylim(0, 3.5)
```

Wir werden später in der [Klassifikation](#sec-class-basic), der Vorhersage von $0/1$-Werten, sowie in der multiplen Regression noch andere Prädktionen und deren Maßzahlen kennen lernen. Im Rahmen der simplen Regression soll dies aber erstmal hier genügen.

## Glossar

::: callout-important
## Glossar zum statistischen Modellieren

| Symbol | Deutsch |
|:--:|----|
| $RHS \sim LHS$ | Modellschreibweise mit der rechten Seite (eng. *right hand site*) und der linken Seite (eng. *left hand side*) |
| $RHS$ | Rechte Seite der Tilde in der Modellschreibweise (eng. *right hand site*) |
| $LHS$ | Linke Seite der Tilde in der Modellschreibweise (eng. *left hand site*) |
| $Y \sim X$ | Modelschreibweise mit Platzhaltervariable X und Y |
| $Y / y$ | Messwert |
| $X / x$ | Einflussvariable |
| $\epsilon$ | Residuen oder Fehler aus einer Regressionsanalyse |
| Fehler | Abweichung der vorhergesagten y-Werte durch das Modell auf der Graden zu den beobachteten y-Werten in den Daten. |
| $\beta_0$ | Intercept oder y-Achsenabschnitt |
| $\beta_1$ | Steigung der ersten Einflussvariable |
| Intercept | y-Achsenabschnitt |
| $\hat{y}$ | Vorhergesagte y-Werte durch das Modell auf der Graden |
| Kovariate | Kontinuierliche Einflussvariable |
| $c_1$ | Kovariate |
| Faktor | Kategoriale Einflussvariable |
| Level | Gruppen eines Faktors |
| $f_A$ | Faktor |
| $A.1, ..., A.3$ | Level eines Faktors als Gruppen |
| $\mathcal{N}(0, 1)$ | Standardnormalverteilung mit einem Mittelwert von 0 und einer Varianz von 1 |
| Zufälliger Effekt | Faktor in einem gemischten Modell, der aus einer zufälligen Population stammt |
| $Z / z$ | Zufälliger Effekt |
| Fester Effekt | Faktor in einem gemischten Modell, der aus einer experimentellen Population stammt |
| schätzen | Berechnung der Koeffizienten in einem statistischen Modell |
| Koeffizienten | Steigung und Intercept in einem statistsichen Modell. Allgemeiner die Parameter die ein statistsiches Modell wiedergibt und die Grade beschreibt. |
| Residuen | Unerklärter Rest durch die Modellierung. Konkret der Abstand zwischen den y-Messwerten und den y-Werten auf der Geraden. |
|  |  |
|  |  |

: Gossar der häufig verwendeten Symbole und Fachbegriffe im statistischen Modellieren. {#tbl-glossar-regression tbl-colwidths="25, 75"}
:::

## Referenzen {.unnumbered}
