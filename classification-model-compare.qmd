```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Vergleich von Algorithmen {#sec-class-model-compare}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

[In dem @sec-test-diag erfährst du mehr darüber was eine ROC Kurve ist und wie du die ROC Kurve interpretieren kannst.]{.aside}

::: callout-caution
## Tuning und Resampling?

In diesem Kapitel werde ich auf das Tuning und das Resampling verzichten. Du findest aber in jedem Anwendungskapitel nochmal den R Code für das Tuning und das Resampling. Wenn du willst, kannst und solltest du auch die beiden Schritte noch in den Code mit dazwischen schalten. Auf der Webseite [Tidymodels - A predictive modeling case study](https://www.tidymodels.org/start/case-study/) findest du nochmal Hilfe dazu.
:::

## Genutzte R Pakete für das Kapitel

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, 
               janitor, xgboost, ranger, kknn,
               see, conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
##
set.seed(20234534)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

https://parsnip.tidymodels.org/ https://rsample.tidymodels.org/

R Paket `tune`

R Paket `yardstick`

https://rsample.tidymodels.org/articles/Applications/Recipes_and_rsample.html

https://www.tmwr.org/resampling.html

https://en.wikipedia.org/wiki/Confusion_matrix

```{r}
gummi_tbl <- read_excel("data/gummibears.xlsx") %>% 
  mutate(gender = as_factor(gender),
         most_liked = as_factor(most_liked),
         student_id = 1:n()) %>% 
  select(student_id, gender, most_liked, age, semester, height) %>% 
  drop_na(gender)

```

In @tbl-gummi-prepro

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-gummi-model-compare
#| tbl-cap: Auszug aus dem Daten zu den Gummibärchendaten.

gummi_raw_tbl <- gummi_tbl %>% 
  mutate(gender = as.character(gender),
         most_liked = as.character(most_liked))

rbind(head(gummi_raw_tbl),
      rep("...", times = ncol(gummi_raw_tbl)),
      tail(gummi_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

```{r}
gummi_data_split <- initial_split(gummi_tbl, prop = 3/4)

gummi_train_data <- training(gummi_data_split)
gummi_test_data  <- testing(gummi_data_split)
```

```{r}
gummi_rec <- recipe(gender ~ ., data = gummi_train_data) %>% 
  update_role(student_id, new_role = "ID") %>% 
  step_impute_mean(all_numeric_predictors(), -has_role("ID")) %>% 
  step_impute_bag(all_nominal_predictors(), -has_role("ID")) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1, -has_role("ID")) %>% 
  step_dummy(all_nominal_predictors(), -has_role("ID")) %>% 
  step_nzv(all_predictors(), -has_role("ID"))

gummi_rec %>% summary()

```

## $k$-NN Algorithm

::: callout-note
## Huch, der Code ist aber sehr kurz...

In diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den $k$-NN Algorithmus wissen willst, schaue bitte in @sec-knn.
:::

```{r}
knn_mod <- nearest_neighbor(neighbors = 11) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") 
```

```{r}
knn_wflow <- workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
knn_fit <- knn_wflow %>% 
  parsnip::fit(gummi_train_data)
```

```{r}
knn_aug <- augment(knn_fit, gummi_test_data ) 
```

## Random Forest

::: callout-note
## Huch, der Code ist aber sehr kurz...

In diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den Random Forest Algorithmus wissen willst, schaue bitte in @sec-class-rf.
:::

::: {.callout-caution collapse="true"}
## Parallele CPU Nutzung

```{r}
cores <- parallel::detectCores()
cores
```

Und dann in `set_engine("ranger", num.threads = cores)`
:::

```{r}
ranger_mod <- rand_forest(mtry = 5, min_n = 10, trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")
```

```{r}
ranger_wflow <- workflow() %>% 
  add_model(ranger_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
ranger_fit <- ranger_wflow %>% 
  parsnip::fit(gummi_train_data)
```

```{r}
ranger_aug <- augment(ranger_fit, gummi_test_data ) 
```

## xgboost

::: callout-note
## Huch, der Code ist aber sehr kurz...

In diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den xgboost Algorithmus wissen willst, schaue bitte in @sec-xgboost.
:::

```{r}
xgboost_mod <- boost_tree(mtry = 5, min_n = 10, trees = 1000) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```

```{r}
xgboost_wflow <- workflow() %>% 
  add_model(xgboost_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
xgboost_fit <- xgboost_wflow %>% 
  parsnip::fit(gummi_train_data)
```

```{r}
xgboost_aug <- augment(xgboost_fit, gummi_test_data ) 
```

## Vergleich der Modelle

```{r}
fit_lst <- lst(knn = knn_aug,
               rf = ranger_aug,
               xgboost = xgboost_aug)
```

```{r}

fit_lst %>% 
  map(~conf_mat(.x, gender, .pred_class)) %>% 
  map(summary) %>% 
  map(~select(.x, .metric, .estimate)) %>% 
  reduce(left_join, by = ".metric") %>% 
  set_names(c("metric", "knn", "rf", "xboost")) %>% 
  mutate(across(where(is.numeric), round, 3))

```

Wir können auch *ganz viele* Beurteilungskriterien für die Klassifikation in einer [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) berechnen lassen.

|              |                     |                      |                      |
|:------------:|:-------------------:|:--------------------:|:--------------------:|
|              |                     |    **Prädiktion**    |                      |
|              |                     | $Positiv\; (PP)$ (1) | $Negativ\; (PN)$ (0) |
| **Wahrheit** | $Positiv\; (P)$ (1) |         $TP$         |         $FN$         |
|              | $Negativ\; (N)$ (0) |         $FP$         |         $TN$         |

: Die [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) als eine 2x2 Tabelle oder Vierfeldertafel {#tbl-confusion-matrix}

-   **accuracy** ist der Anteil der Label, die richtig vorhergesagt werden.
-   **kap** beschreibt Kappa und damit ein ähnliches Maß wie die *accuracy*. Dabei wird aber Kappa durch die *accuarcy* normalisiert, die allein durch Zufall zu erwarten wäre. Damit ist Kappa sehr nützlich, wenn eine oder mehrere Klassen große Häufigkeitsverteilungen haben.
-   **sens** beschreibt die Sensitivität.
-   **spec** beschreibt die Spezifität.
-   **ppv** beschreibt den positiven prädiktiven Wert (eng. *positive predictive value*).
-   **npv** beschreibt den negativen prädiktiven Wert (eng. *negative predictive value*).
-   **mcc** beschreibt den Matthews Korrelationskoeffizienten (eng. *Matthews correlation coefficient*). Der Matthews-Korrelationskoeffizient (MCC) ist ein zuverlässiger statistischer Wert, der nur dann einen hohen Wert hat, wenn die Vorhersage in allen vier Kategorien der Konfusionsmatrix (richtig positiv, falsch negativ, richtig negativ und falsch positiv) gute Ergebnisse erzielt. Siehe dazu auch @chicco2020advantages.
-   **j_index** beschreibt die Youden-J-Statistik und ist definiert als $J = sens + spec - 1$. Wenn wir also eine hohe Sensitivität und eine hohe Spezifität haben dann nähert sich $J$ der Eins an.
-   **bal_accuracy** beschreibt die balancierte *accuarcy* und wird hier in der Funktion als der Durchschnitt von Sensitivität und Spezifität berechnet.
-   **detection_prevalence** Die Entdeckungsprävalenz (eng. *detection prevalence*) ist definiert als die Anzahl der vorhergesagten positiven Ereignisse (sowohl richtig als auch falsch positiv) geteilt durch die Gesamtzahl der Vorhersagen.
-   **precision** Bei der binären Klassifizierung ist die *precision* der positiv prädiktive Wert. Damit ist die *precision* die Anzahl der richtig positiven Ergebnisse geteilt durch die Anzahl aller positiven Ergebnisse, einschließlich derer, die nicht richtig erkannt wurden. Siehe auch [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).
-   **recall** Bei der binären Klassifizierung ist der *recall* die Sensitivität. Damit ist der *recall* die Anzahl der tatsächlich positiven Ergebnisse geteilt durch die Anzahl aller Ergebnisse, die als positiv hätten identifiziert werden müssen. Siehe auch [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).
-   **f_meas** beschreibt den F$_1$ Score und damit das harmonische Mittel aus Precision und Recall. Der höchstmögliche Wert eines F$_1$ Scores ist 1, was perfekte Präzision und Recall bedeutet, und der niedrigstmögliche Wert ist 0, wenn sowohl Präzision als auch Recall null sind. Siehe auch [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).

```{r}
roc_tbl <- fit_lst %>% 
  map(~roc_curve(.x, gender, .pred_w, event_level = "second")) %>% 
  bind_rows(.id = "model")
```

```{r}
pr_tbl <- fit_lst %>% 
  map(~pr_curve(.x, gender, .pred_w, event_level = "second")) %>% 
  bind_rows(.id = "model")
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| label: fig-roc-pr-compare
#| fig-cap: "Darstellung der Vorhersagegüte der drei Modelle k-NN, ranger und xgboost."
#| fig-subcap: 
#|   - "Receiver operator curve."
#|   - "Precision recall curve."
#| layout-nrow: 1
#| column: page

roc_tbl %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  theme_minimal() +
  geom_path() +
  geom_abline(lty = 3) + 
  scale_color_okabeito()

pr_tbl %>% 
  ggplot(aes(x = recall, y = precision, col = model)) + 
  theme_minimal() +
  geom_path() +
  scale_color_okabeito()
```

## Visualisation

https://datascienceplus.com/machine-learning-results-one-plot-to-rule-them-all/ https://datascienceplus.com/machine-learning-results-in-r-one-plot-to-rule-them-all-part-2-regression-models/ https://www.r-bloggers.com/2021/04/the-good-the-bad-and-the-ugly-how-to-visualize-machine-learning-data/ https://neptune.ai/blog/visualizing-machine-learning-models https://uc-r.github.io/lime https://cran.r-project.org/web/packages/mboost/vignettes/mboost_tutorial.pdf
