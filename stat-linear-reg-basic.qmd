```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra)
```

# Simple lineare Regression {#sec-modeling-simple-stat}

::: callout-note
## Was macht die simple lineare Regression?

Die simple lineare Regression legt eine Grade durch eine Punktwolke.
:::

::: {.callout-caution collapse="true"}
## Ein Wort zur Klausur

Wir nutzen folgende
:::

## Genutzte R Pakete für das Kapitel

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, conflicted, broom,
               readxl)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Wir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die lineare Regression auf sehr großen Datensätzen anwenden, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit $n=7$ Beobachtungen. In der @tbl-model-0 ist der Datensatz `simplel_tbl` dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in \[cm\] und dem Gewicht in \[mg\] für sieben Beobachtungen modellieren.

```{r}
#| message: false
#| echo: false
#| tbl-cap: Selektierter Datensatz mit einer normalverteilten Variable `jump_length` und der normalverteilten Variable `weight`. 
#| label: tbl-model-0

simple_tbl <- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),
                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))

simple_tbl %>% kable(align = "c", "pipe")
```

In @fig-scatter-lin-01 sehen wir die Visualisierung der Daten `simple_tbl` in einem Scatterplot mit einer geschätzen Gerade.

```{r}
#| echo: false
#| message: false
#| label: fig-scatter-lin-01
#| fig-align: center
#| fig-height: 4
#| fig-width: 4
#| fig-cap: "Scatterplot der Beobachtungen der Sprungweite in \\[cm\\] und dem Gewicht in \\[mg\\]. Die Gerade verläuft mittig durch die Punkte."

ggplot(simple_tbl, aes(weight, jump_length)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  theme_bw() +
  xlim(0, 3.5) + ylim(0, 3.5)
```

[Wir schauen uns in diesem Kapitel nur eine *simple* lineare Regression mit einem $x_1$ an. In unserem Fall ist das $x_1$ gleich dem `weight`. Später schauen wir dann *multiple* lineare Regressionen mit mehreren $x_1,..., x_p$ an]{.aisde}

Bevor wir mit dem Modellieren anfangen können, müssen wir verstehen, wie ein *simples* Modell theoretisch aufgebaut ist. Danach können wir uns das lineare Modell in R anschauen.

## Simple lineare Regression theoretisch

Wir haben nun die ersten sieben Beobachtungen in dem Objekt `simple_tbl` vorliegen. Wie sieht nun theoretisch eine lineare Regression aus? Wir wollen eine Grade durch Punkte legen, wie wie wir es in @fig-scatter-lin-01 sehen. Die blaue Gerade wir durch eine Geradengleichung beschreiben. Du kenst vermutlich noch die Form $y = mx + b$. In der Statistik beschreiben wir eine solche Gerade aber wie folgt.

$$
y \sim \beta_0 + \beta_1 x_1 + \epsilon
$$

mit

-   $\beta_0$ als den y-Achsenabschnitt.
-   $\beta_1$ als der Steigung der Geraden.
-   $\epsilon$ als Residuen oder die Abweichungen von den $y$-Werten auf Geraden zu den einzelnen $y$-Werten der Beobachtungen.

In @tbl-reg-deu-eng siehst du nochmal in einer Tabelle den Vergleich von der Schreibweise der linearen Regression in der Schule und in der Statistik. Darüber hinaus sind die deutschen Begriffe den englischen Begriffen gegenüber gestellt. Warum schreiben wir die Gleichung in der Form? Damit wir später noch weitere $\beta_px_p$-Paare ergänzen könen und so *multiple* Modelle bauen können.

| $\boldsymbol{y = mx +b}$ | $\boldsymbol{y \sim \beta_0 + \beta_1 x_1 + \epsilon}$ |      Deutsch      |  Englisch   |
|:------------------------:|:------------------------------------------------------:|:-----------------:|:-----------:|
|           $m$            |                       $\beta_1$                        |     Steigung      |    Slope    |
|           $x$            |                         $x_1$                          | Einflussvariable  | Risk factor |
|           $b$            |                       $\beta_0$                        | y-Achsenabschnitt |  Intercept  |
|                          |                       $\epsilon$                       |     Residuen      |  Residual   |

: Vergleich und Übersicht der schulischen vs. statistischen Begriffe in den linearen Regression sowie die deutschen und englischen Begriffe. {#tbl-reg-deu-eng}

In @fig-lin-reg-01 sehen wir die Visualisierung der Gleichung in einer Abbildung. Die Gerade läuft durch die Punktewolke und wird durch die statistischen Maßzahlen bzw. Parameter $\beta_0$, $\beta_1$ sowie den $\epsilon$ beschrieben. Wir sehen, dass das $\beta_0$ den *Intercept* darstellt und das $\beta_1$ die Steigung der Geraden. Wenn wir $x$ um 1 Einheit erhöhen $x+1$, dann steigt der $y$ Wert um den Wert von $\beta_1$. Die einzelnen Abweichungen der beobachteten $y$-Wert zu den $y$-Werten auf der Gerade ($\hat{y}$) werden als Residuen oder auch $\epsilon$ bezeichnet.

![Visualisierung der linearen Regression. Wir legen eine Gerade durch eine Punktewolke. Die Gerade wird durch die statistischen Maßzahlen bzw. Parameter $\beta_0$, $\beta_1$ sowie den $\epsilon$ beschrieben.](images/statistical_modeling_lm){#fig-lin-reg-01 fig-align="center" width="100%"}

[In R werden die $\hat{y}$ auch *fitted values* genannt. Die $\epsilon$ Werte werden dann *residuals* bezeichnet.]{.aside}

Schauen wir uns einmal den Zusammenhang von $y$, den beobachteten Werten, und $\hat{y}$, den geschätzen Werten auf der Gerade in unserem Beispiel an. In @tbl-lin-reg-epsilon sehen wir die Berechnung der einzelnen Residuen für die Gerade aus der @fig-scatter-lin-01. Wir nehmen jedes beobachtete $y$ und ziehen den Wert von $y$ auf der Gerade, bezeichnet als $\hat{y}$, ab. Diesen Schritt machen wir für jedes Wertepaar $(y_i; \hat{y}_i)$.

::: column-page
|  x  |  y  | $\boldsymbol{\hat{y}}$ | Residuen ($\boldsymbol{\epsilon}$) |               Wert                |
|:---:|:---:|:----------------------:|:----------------------------------:|:---------------------------------:|
| 0.8 | 1.2 |          1.38          |   $\epsilon_1 = y_1 - \hat{y}_1$   | $\epsilon_1 = 1.2 - 1.38 = -0.18$ |
| 1.0 | 1.8 |          1.48          |   $\epsilon_2 = y_2 - \hat{y}_2$   | $\epsilon_2 = 1.8 - 1.48 = +0.32$ |
| 1.2 | 1.3 |          1.58          |   $\epsilon_3 = y_3 - \hat{y}_3$   | $\epsilon_3 = 1.3 - 1.58 = -0.28$ |
| 1.9 | 1.7 |          1.94          |   $\epsilon_4 = y_4 - \hat{y}_4$   | $\epsilon_4 = 1.7 - 1.94 = -0.24$ |
| 2.0 | 2.6 |          1.99          |   $\epsilon_5 = y_5 - \hat{y}_5$   | $\epsilon_5 = 2.6 - 1.99 = +0.61$ |
| 2.7 | 1.8 |          2.34          |   $\epsilon_6 = y_6 - \hat{y}_6$   | $\epsilon_6 = 1.8 - 2.34 = -0.54$ |
| 2.8 | 2.7 |          2.40          |   $\epsilon_7 = y_7 - \hat{y}_7$   | $\epsilon_7 = 2.7 - 2.40 = +0.30$ |

: Zusammenhang zwischen den $y$, den beobachteten Werten, und $\hat{y}$, den geschätzen Werten auf der Gerade. Wir nennen den Abstand $y_i - \hat{y}_i$ auch Residuum oder *Epsilon* $\epsilon$. {#tbl-lin-reg-epsilon}
:::

```{r}
#| echo: false

epsilon <- c(-0.18, 0.32, -0.28, -0.24, 0.61, -0.54, 0.30)
mean_e <- round(mean(epsilon), 2)
var_e <- round(var(epsilon), 2)

```

Die Abweichungen $\epsilon$ oder auch Residuen genannt haben einen Mittelwert von $\bar{\epsilon} = `r mean_e`$ und eine Varianz von $s^2_{\epsilon} = `r var_e`$. Wir schreiben, dass die Residuen normalverteilt sind mit $\epsilon \sim \mathcal{N}(0, s^2_{\epsilon})$.

## Simples lineare Regression in R

### Daten

Wir wollen uns erszmal mit einem einfachen Datenbeispiel beschäftigen. Wir brauchen dafür den Datensatz `flea_dog_cat_length_weight.xlsx`. In einer simplen linearen Regression schauen wir uns den Zusammenhang zwischen einem $y$ und einem $x_1$ an. Daher wählen wir aus dem Datensatz `flea_dog_cat_length_weight.xlsx` die beiden Spalten `jump_length` und `weight`. Wir wollen nun feststellen, ob es einen Zusammenhang zwischen der Sprungweite in \[cm\] und dem Flohgewicht in \[mg\] gibt. In dem Datensatz finden wir 400 Flöhe, wir wollen uns aber nur die ersten sieben Zeilen des Datensatzes zuerst anschauen.

```{r}
#| message: false

simple_tbl <- read_excel("data/flea_dog_cat_length_weight.xlsx") %>%
  select(jump_length, weight) 
```

In der @tbl-model-1 ist der Datensatz `simplel_tbl` nochmal dargestellt. Wir wollen jetzt den Zusammenhang zwischen der Sprungweite in \[cm\] und dem Gewicht in \[mg\] für die ersten sieben Beobachtungen modellieren.

```{r}
#| message: false
#| echo: false
#| tbl-cap: Selektierter Datensatz mit einer normalverteilten Variable `jump_length` und der normalverteilten Variable `weight`. Es werden die ersten sieben Zeilen des Datensatzes benutzt.
#| label: tbl-model-1

simple_tbl %>% head(7) %>% kable(align = "c", "pipe")
```

$$
y \sim x_1
$$

$$
jump\_length \sim weight
$$

```{r}
fit_1 <- lm(jump_length ~ weight, data = simple_tbl)
```

```{r}
fit_1 %>% summary
```

```{r}
fit_1 %>% tidy
```

```{r}
fit_1 %>% glance
```
