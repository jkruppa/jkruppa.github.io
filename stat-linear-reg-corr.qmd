```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
set.seed(20230523)
```

# Korrelation {#sec-lin-reg-corr}

*Letzte Änderung am `r format(fs::file_info("stat-linear-reg-corr.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"Most of you will have heard the maxim 'correlation does not imply causation.' Just because two variables have a statistical relationship with each other does not mean that one is responsible for the other. For instance, ice cream sales and forest fires are correlated because both occur more often in the summer heat. But there is no causation; you don't light woodlands on fire when you buy a scoop of ice cream." --- Nate Silver, The Signal and the Noise: Why So Many Predictions Fail -- But Some Don't*

------------------------------------------------------------------------

![](images/caution.png){fig-align="center" width="50%"}

**Ab hier ist es eine Baustelle und wird es vermutlich auch über das Sommersemester 2024 bleiben. Aktuell weiß ich nämlich noch nicht, wo ich ab diesem Abschnitt hin will und wo es stehen sollte. Daher können Teile des Codes und des Textes kurzfristig keinen Sinn ergeben oder nicht funktional sein. Es ist geplant eine fertige Version im Juni 2024 vorliegen zu haben.**

------------------------------------------------------------------------

Die Korrelation gibt uns die Information welche Steigung die Gerade in einer simplen linearen Regression hat. Das ist die einfache Antwort auf die Frage, was die Korrelation aus statistischer Sicht ist. Dabei erlaubt es die Korrelation uns verschiedene Geraden miteinander zu vergleichen auch wenn unsere Variablen unterschiedliche Einheiten haben. Die Korrelation ist nämlich einheitslos. Wir standardisieren durch die Anwendung der Korrelation die Steigung der Geraden dafür auf -1 bis +1. Damit ist die Korrelation ein bedeutendes Effektmaß für die Abschätzung eines Zusammenhangs zwischen zwei Variablen. Wir nutzen die Korrelation in vielen Bereichen der Agrarwissenschaften. Wichtig ist, dass wir immer zwei Variablen, also damit auch Spalten, miteinander vergleichen. Diese Variablen müssen eine Zahl, also numerisch, sein. Im Weiteren schaue dann auch mal in dem [Kapitel zu den Effektmaßen und Interrater Reliabilität](#sec-effect-interrater). Wir wollen dort die Frage beantworten, in wie weit zwei Bewerter die gleichen Boniturnoten vergeben.

Es gibt aber ein Problem in dem allgemeinen Sprachgebrauch. Das Wort *korrelieren* ist zum Gattungsbegriff in der Statistik geworden, wenn es um den Vergleich oder den Zusammenhang von zwei oder mehreren Variablen geht. Das heißt, in der Anwendung wird gesagt, dass wir A mit B *korrelieren* lassen wollen. Das Wort *korrelieren* steht jetzt aber nicht für das Konzept statistische Korrelation sondern ist Platzhalter für eine noch vom Anwender zu definierende oder zu findende statistische Methode. Im Weiteren müssen wir beachten, dass nur weil etwas miteinander korreliert muss es keinen kausalen Zusammenhang geben. So ist die Ursache und Wirkung manchmal nicht klar zu benennen. Nehmen wir als plakatives Beispiel dicke Kinder, die viel Fernsehen. Wir würden annehmen, dass zwischen dem Fernsehkonsum und dem Gewicht von Kindern eine hohe Korrelation vorliegt. Sind jetzt aber die Kinder dick, weil die Kinder so viel Fernsehen oder schauen einfach dicke Kinder mehr Fernsehen, da die Kinder dick sind und sich nicht mehr so viel bewegen wollen? Die Internetseite [Spurious correlations](https://www.tylervigen.com/spurious-correlations) zeigt verschiedene *zufällige* Korrelationen zwischen zwei *zufällig* ausgewählten Variablen aus den USA. Es ist immer wieder spannend zu sehen, was da zufällig miteinander ähnlich agiert.

Wie immer gibt es auch Literatur zu dem Thema Korrelation. Denn eigentlichbist nichts so häufig genutzt wie auch falsch verstanden oder falsch interpretiert als die Korrelation. @akoglu2018user [User's guide to correlation coefficients](https://www.sciencedirect.com/science/article/pii/S2452247318302164) mit Interpretation of the Pearson's and Spearman's correlation coefficients.

@asuero2006correlation [The Correlation Coefficient: An Overview](https://www.unife.it/economia/lm.economics/lectures/applied-econometrics/materiale-didattico-2018-2019/Correlation_overview.pdf) Bedeutung der Visualisierung

@bewick2003statistics [Statistics review 7: Correlation and regression](https://link.springer.com/content/pdf/10.1186/cc2401.pdf)

@taylor2013discussion [A discussion on the significance associated with Pearson’s correlation in precision agriculture studies](https://link.springer.com/article/10.1007/s11119-013-9314-9)

@shao2022accurate [How Accurate Is Your Correlation? Different Methods Derive Different Results and Different Interpretations](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2022.901412/full)

In diesem Kapitel wollen wir uns mit der statistischen Korrelation beschäftigen. Die statistische Korrelation ist weniger aufregender, denn am Ende ist die Korrelation nur eine Zahl zwischen -1 und +1. Eigentlich ist das eine wichtige Botschaft, denn wenn deine berechnete Korrelation nicht zwischen -1 und +1 liegt, dann hast du was anderes berechnet oder aber hier stimmt was nicht.

{{< video https://youtu.be/dLlgWQI4M8w?si=IacLwPIwurRGbLHU >}}

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, conflicted, readxl,
               corrplot, GGally, ggraph, correlation, see)
conflict_prefer("select", "dplyr")
conflicts_prefer(dplyr::filter)
conflict_prefer("mutate", "dplyr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Wir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die Korrelation auf sehr großen Datensätzen berechnen, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit $n=7$ Beobachtungen. In der @tbl-corr-0 ist der Datensatz `simplel_tbl` dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in \[cm\] und dem Gewicht in \[mg\] für sieben Beobachtungen modellieren.

```{r}
#| message: false

simple_tbl <- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),
                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))
```

```{r}
#| message: false
#| echo: false
#| tbl-cap: Datensatz mit einer normalverteilten Variable `jump_length` und der normalverteilten Variable `weight`. 
#| label: tbl-corr-0

simple_tbl <- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),
                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))

simple_tbl %>% kable(align = "c", "pipe")
```

In @fig-scatter-corr-01 sehen wir die Visualisierung der Daten `simple_tbl` in einem Scatterplot mit einer geschätzen Gerade. Wir wollen jetzt mit der Korrelation die Steigung der Geraden *unabhängig* von der Einheit beschreiben. Oder wir wollen die Steigung der Geraden standardisieren auf -1 bis 1.

```{r}
#| echo: false
#| message: false
#| label: fig-scatter-corr-01
#| fig-align: center
#| fig-height: 4
#| fig-width: 4
#| fig-cap: "Scatterplot der Beobachtungen der Sprungweite in \\[cm\\] und dem Gewicht in \\[mg\\]. Die Gerade verläuft mittig durch die Punkte."

ggplot(simple_tbl, aes(weight, jump_length)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  theme_minimal() +
  xlim(0, 3.5) + ylim(0, 3.5)
```

Als zweiten Datensatz nutzen wir die Gummibärchendaten und zwar alle numerischen Spalten von `count_bears` bis zum `semester`. Dann nehme ich nur die Teilnehmerinnen an der Umfrage, da ich sonst Probleme mit der Körpergröße kriege. Männer und Frauen sind unterschiedlich groß und dann würden wir immer eine Art Effekt von dieser Sachlage bekommen, wenn wir Männer und Frauen nicht getrennt betrachten. Wenn du keine numerischen Daten sondern Worte vorliegen hast, dann kannst du über die Funktion `as.factor()` dir erst einen Faktor erschaffen und dann über `as.numeric()` aus dem Faktor eine Zahl machen. Ich zeige dir das hier einmal an der Spalte `most_liked`.

```{r}
#| message: false
corr_gummi_tbl <- read_excel("data/gummibears.xlsx") |> 
  filter(gender == "w") |> 
  select(count_bears:semester) |> 
  mutate(most_liked = as.numeric(as.factor(most_liked))) |> 
  select_if(is.numeric)
```

Dann in der @tbl-corr-1 nochmal ein Auszug aus den Daten. Wir betrachten hier nur die numerischen Spalten, da wir nur über diese eine Korrelation berechnen können. Du siehst wie wir die Worte in der Spalte `most_liked` dann in einer Zahl umgewandelt haben. Jede der Zahlen steht dann für eine der sechs Farben der Gummibärchen plus die "keine Präferenz"-Kategorie. Da wir die eigentliche Übersetzung nicht brauchen um die Korrelation zu interpretieren, passt das hier ganz gut.

```{r}
#| message: false
#| echo: false
#| tbl-cap: "foo." 
#| label: tbl-corr-1

corr_gummi_raw_tbl <- corr_gummi_tbl %>% 
  mutate_all(as.character)
  

rbind(head(corr_gummi_raw_tbl, 4),
      rep("...", times = ncol(corr_gummi_raw_tbl)),
      tail(corr_gummi_raw_tbl, 4)) %>% 
  kable(align = "c", "pipe")
```

## Korrelation theoretisch

Wir schauen uns hier die Korrelation nach Pearson an. Die Korrelation nach Pearson nimmt an, dass beide zu korrelierende Variablen einer Normalverteilung entstammen. Wenn wir keine Normalverteilung vorliegen haben, dann nutzen wir die Korrelation nach Spearman. Die Korrelation nach Spearman basiert auf den Rängen der Daten und ist ein nicht-parametrisches Verfahren. Die Korrelation nach Pearson ist die parametrische Variante. Wir bezeichnen die Korrelation entweder mit $r$ oder dem griechischen Buchstaben $\rho$ als *rho* gesprochen.

Was macht nun die Korrelation? Die Korrelation gibt die Richtung der Geraden an. Oder noch konkreter die Steigung der Geraden normiert auf -1 bis 1. Die @fig-corr1 zeigt die Visualisierung der Korrelation für drei Ausprägungen. Eine Korrelation von $r = -1$ bedeutet eine maximale negative Korrelation. Die Gerade fällt in einem 45° Winkel. Eine Korrelation von $r = +1$ bedeutet eine maximale positive Korrelation. Die gerade steigt in einem 45° Winkel. Eine Korrelation von $r = 0$ bedeutet, dass keine Korrelation vorliegt. Die Grade verläuft parallel zur $x$-Achse.

![Visualisierung der Korrelation für drei Ausprägungen des Korrelationskoeffizient.](images/statistical_modeling_corr){#fig-corr1 fig-align="center" width="100%"}

Im Folgenden sehen wir die Formel für den Korrelationskoeffizient nach Pearson. Es gibt natürlich auch noch andere Korrelationskoeffizienten aber hier geht es dann einmal darum das Prinzip zu verstehen und das können wir am Korrelationskoeffizient nach Pearson am einfachsten. Wichtig hierbei, die beiden Variablen, die wir korrelieren wollen, müssen bei dem Korrelationskoeffizienten nach Pearson normalverteilt sein.

$$
\rho = r_{x,y} = \cfrac{s^2_{x,y}}{s_x \cdot s_y}
$$

Wir berechnen die Korrelation immer zwischen *zwei* Variablen $x$ und $y$. Es gibt keine multiple Korrelation über mehr als zwei Variablen. Im Zähler der Formel zur Korrelation steht die Kovarianz von $x$ und $y$.

Wir können mit folgender Formel die Kovarianzen zwischen den beiden Variablen $x$ und $y$ berechnen.

$$
s^2_{x,y} = \sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})
$$

Die folgende Formel berechnet die quadrierten Abweichung der Beobachtungen von $x$ zum Mittelwert $\bar{x}$.

$$
s_x = \sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}
$$

Die folgende Formel berechnet die quadrierten Abweichung der Beobachtungen von $y$ zum Mittelwert $\bar{y}$.

$$
s_y = \sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}
$$

In @tbl-corr-example ist der Zusammenhang nochmal Schritt für Schritt aufgeschlüsselt. Wir berechnen erst die Abweichungsquadrate von $x$ und die Abweichungsquadrate von $y$. Dann noch die Quadrate der Abstände von $x$ zu $y$. Abschließend summieren wir alles und ziehen noch die Wurzel für die Abweichungsquadrate von $x$ und $y$.

| jump_length $\boldsymbol{y}$ | weight $\boldsymbol{x}$ | $\boldsymbol{(y_i-\bar{y})^2}$ | $\boldsymbol{(x_i-\bar{x})^2}$ | $\boldsymbol{(x_i-\bar{x})(y_i-\bar{y})}$ |
|:----------------------------:|:-----------------------:|:------------------------------:|:------------------------------:|:-----------------------------------------:|
|             1.2              |           0.8           |              0.45              |              0.94              |                   0.65                    |
|             1.8              |           1.0           |              0.01              |              0.60              |                   0.06                    |
|             1.3              |           1.2           |              0.33              |              0.33              |                   0.33                    |
|             1.7              |           1.9           |              0.03              |              0.02              |                   -0.02                   |
|             2.6              |           2.0           |              0.53              |              0.05              |                   0.17                    |
|             1.8              |           2.7           |              0.03              |              0.86              |                   -0.07                   |
|             2.7              |           2.8           |              0.69              |              1.06              |                   0.85                    |
|                              |         $\sum$          |              2.05              |              3.86              |                   1.97                    |
|                              |      $\sqrt{\sum}$      |              1.43              |              1.96              |                                           |

: Tabelle zur Berechnung des Korrelationskoeffizient {#tbl-corr-example}

Wir können die Zahlen dann aus der Tabelle in die Formel der Korrelation nach Pearson einsetzen. Wir erhalten eine Korrelation von 0.70 und haben damit eine recht starke positve Korrelation vorliegen.

$$
\rho = r_{x,y} = \cfrac{1.97}{1.96 \cdot 1.43} = 0.70
$$

Wir können mit der Funktion `cor()` in R die Korrelation zwischen zwei Spalten in einem Datensatz berechnen. Wir überprüfen kurz unsere Berechnung und stellen fest, dass wir richtig gerechnet haben.

```{r}
cor(simple_tbl$jump_length, simple_tbl$weight)
```

## Korrelation in R

In R haben wir dann die Möglichkeit die Korrelation mit der Standardfunktion `cor()` ganz klassisch zu bestimmen oder aber auch das [R Paket `{correlation}`](https://easystats.github.io/correlation/index.html) zu nutzen. Das R Paket kann dabei vollumfänglich auf das `{tidyverse}` zugreifen und ist von mir bevorzugt. Wenn es schnell gehen muss oder ich mir nur aml als Demonstration eine Korrelation berechnen will, dann ist die klassiche Variante in R gut, aber für größere Datensätze nutze ich nur noch das R Paket `{correaltion}` und deren tollen Funktionen.

### Klassisch

Wir nutzen die Korrelation in R selten nur für zwei Variablen. Meistens schauen wir uns alle *numerischen* Variablen gemeinsam in einer Abbildung an. Wir nennen diese Abbildung auch Korrelationsplot. Faktoren sind keine numerischen Variablen. Daher kann es sein, dass für dein Experiment kein Korrelationsplot in Frage kommt.

Wir schauen uns jetzt nochmal einen die Berechnung für den Datensatz `simple_tbl` an. Wir müssen für die Korrelation zwischen zwei Variablen diese Variablen mit dem `$`-Zeichen aus dem Datensatz extrahieren. Die Funktion `cor()` kann nur mit Vektoren oder *ganzen* numerischen Datensätzen arbeiten.

Wir können den Korrelationskoeffizienten nach Pearson mit der Option `method = "pearson"` auswählen, wenn wir normalverteilte Daten vorliegen haben. Das heißt, dass alle unsere Spalten, mit denen wir die paarweisen Korrelationen berechnen wollen, einer Normalverteilung folgen müssen.

```{r}
cor(simple_tbl$jump_length, simple_tbl$weight, method = "pearson")
```

Je mehr Variablen du dann hast, desto unwahrscheinlicher wird es natürlich, dass alle einer Normalverteilung folgen. Dann können wir die nicht-parametrische Variante des Korrelationskoeffizienten nach Spearman berechnen. Wir nutzen dazu die Option `method = "spearman"`.

```{r}
cor(simple_tbl$jump_length, simple_tbl$weight, method = "spearman")
```

Bei stetigen Daten wird dann meist statt des Korrelationskoeffizienten nach Spearman gerne der nach Kendall berechnet. Aber das sind dann schon die Feinheiten. Wir nutzen dazu die Option `method = "kendall"`.

```{r}
cor(simple_tbl$jump_length, simple_tbl$weight, method = "kendall")
```

Wir können auch einen statistischen Test für die Korrelation rechnen. Die Nullhypothese $H_0$ wäre hierbei, dass die Korrelation $r = 0$ ist. Die Funktion `cor.test()` liefert den entsprechenden $p$-Wert für die Entscheidung gegen die Nullhypothese.

```{r}
cor.test(simple_tbl$jump_length, simple_tbl$weight, method = "pearson")
```

Aus dem Test erhalten wir den $p$-Wert von $0.079$. Damit liegt der $p$-Wert über den Signifikanzniveau von $\alpha$ gleich 5%. Wir können somit die Nullhypothese nicht ablehnen. Wir sehen hier, die Problematik der kleinen Fallzahl. Obwohl unsere Korrelation mit $0.7$ groß ist erhalten wir einen $p$-Wert, der nicht die Grenze von 5% unterschreitet. Wir sehen, dass die starre Grenze von $\alpha$ auch Probleme bereitet.

### Mit `{correlation}`

Das [R Paket `{correlation}`](https://easystats.github.io/correlation/index.html) erlaubt es, die Korrelation in R zu berechnen und hilft wirklich bei der Anwendung. Die ursprünglichen Funktion in R passen überhaupt nicht zum `{tidyverse}` und so ist es gut, dass wir hier eine andere Möglichkeit haben, die auch noch viel mehr kann als die klassischen Funktion in R. Die zentrale Funktion ist dabei die Funktion `correlation()`. Wir können eine Vielzahl an Korrelationskoeffizienten auswählen. Wenn du keine Methode über die Option `method =` eingibst, dann wird der Korrelationskoeffizient nach Pearson unter der Annahme von normalverteilten Daten gerechnet. Die kannst es dir auch einfacher machen und die Funktion über die Option `method = "auto"` selber wählen lassen. Ich bevorzuge aber den Korrelationskoeffizient nach Spearman, wenn ich sehr viele Variablen miteinander vergleichen möchte. Im Normalfall werden die $p$-Werte adjustiert, dass stelle ich hier aber einmal ab. Sonst nutze gerne die Option `p_adjust = "bonferroni"`, wenn du für sehr viele Vergleiche adjustieren willst.

Wenn du nun die Funktion aufrufst, dann erhälst du alle paarweisen Korrelationen in deinem Datensatz über alle Variablen. Daher baue ich mir immer erstmal über `select()` einen Datensatz zusammen, den ich dann analysieren will. Sonst hast du da Spalten in deiner Auswertung, die dich gar nicht interessieren. Die Spalte `rho` gibt dir dann den Korrelationskoeffizient wieder. Dann erhälst du noch die 95% Konfidenzintervalle sowie den $p$-Wert für den Korrelationskoeffizient. Hier testen wir, ob der Korrelationskoeffizient unterschiedlich von der Null ist.

```{r}
cor_gummi_res <- corr_gummi_tbl |> 
  correlation(method = "spearman", p_adjust = "none")
cor_gummi_res
```

Wir haben jetzt einige signifikante Korrelationskoeffizienten. Jetzt ist es wichtig, dass wir nicht nur auf die Signifikanz schauen, sondern auch fragen, ist der Korrelationskoeffizient *relevant*? Das heißt, ist der Korrelationskoeffizient weit genug weg von der Null um für uns auch eine Aussagekraft zu haben. Hier können wir uns für die bessere Übersicht auch einmal die Korrelationsmatrix wiedergeben lassen.

```{r}
cor_gummi_res |> 
  summary(redundant = TRUE)
```

Das sieht dann nicht mehr so relevant aus. Viele der signifikanten Korrelationskoeffizienten sind irgendwie bei knapp $0.25$ was zwar signifikant unterschiedlich von Null ist, aber auch eher eine sehr kleine Korrelation beschreibt. Ich würde hier nicht von einem Zusammenhang ausgehen. In der @fig-scatter-corr-rpaket-01 siehst du dann die Matrix nochmal visualisiert. Die Farben entsprechen dann der Richtung der Korrelation. Je kräftiger der Farbton, desto größer ist die Korrelation.

```{r}
#| echo: false
#| message: false
#| label: fig-scatter-corr-rpaket-01
#| fig-align: center
#| fig-height: 6
#| fig-width: 7
#| fig-cap: "Darstellung der Korrealtion zwischen den Variablen in den Gummibärchendatensatz. Eine rote Einfärbung deutet auf eine positive Korrelation und eine blaue Einfärbung auf eine negative Korrelation hin."
cor_gummi_res %>%
  summary(redundant = TRUE) %>%
  plot()
```

Eine angenehme Funktion in dem R Paket `{correlation}` ist die Funktion `cor_test()`. Wir haben hier zum einen die Möglichkeit direkt Spaltennamen in die Funktion zu pipen. Auf der anderen Seite können wir die Funktion auch gleich in der Funktion `plot()` visualisieren. Das ist super praktisch, wenn du dir die Daten einmal anschauen willst und sehen willst, ob der Korrelationskoeffizient gepasst hat. Ich nutze die Funktion gerne in meiner Arbeit als schnelle Visualisierung einer Korrelation in einem Scatterplot. Sonst sind es schon ein paar mehr Zeilen Code mit `{ggplot}`.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-scatter-corr-rpaket-02
#| fig-align: center
#| fig-height: 6
#| fig-width: 7
#| fig-cap: "Scatterplot zwischen der Körpergröße und dem Alter aller Teilnehmerinnen in dem Gummibärchendatensatz."

corr_gummi_tbl |> 
  cor_test("age", "height") |> 
  plot() +
  theme_minimal()
```

Als letztes erlaubt das R Paket noch die Darstellung der Daten in einem gaußsches grafisches Modell (eng. *Gaussian Graphical Model*). Ein gaußsches grafisches Modell besteht dabei aus einer Reihe von Variablen, die durch Kreise dargestellt werden, und einer Reihe von Linien, die die Beziehungen zwischen den Variablen visualisieren. Die Dicke dieser Linien stellt die Stärke der Beziehungen zwischen den Variablen dar. Daher kannst du davon ausgehen, dass ein Fehlen einer Linie keine oder nur sehr schwache Beziehungen zwischen den relevanten Variablen darstellt. Insbesondere erfassen diese Linien im Gaußschen Grafikmodell partielle Korrelationen (eng. *partial correlation*). Partielle Korrelationen beschreiben die Korrelation zwischen zwei Variablen, wenn für alle Variablen im Datensatz kontrolliert wird. Ein wesentlicher Vorteil der partiellen Korrelationen besteht nun darin, dass unerwünschte Korrelationen vermieden werden. Mehr zu dem Thema findest du auch bei @bhushan2019using in deren Arbeit [Using a Gaussian Graphical Model to Explore Relationships Between Items and Variables in Environmental Psychology Research](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.01050/full).

::: callout-note
## Partielle Korrelation in a Nutshell

```{mermaid}
%%| label: fig-mermaid-part-cor
%%| fig-width: 6
%%| fig-cap: "Flowchart der "
flowchart LR
    A("x"):::factor <-. r<sub>xz</sub> ..-> B("z"):::confound;
    A("x"):::factor <-- r<sub>xy</sub> --> C("y"):::factor;
    B("z"):::confound <-. r<sub>yz</sub> ..-> C("y"):::factor;
    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px
    classDef confound fill:#CC79A7,stroke:#333,stroke-width:0.75px

```

$$
r_{xy, z} = \cfrac{r_{xy} - r_{xz} \cdot r_{yz}}{\sqrt{(1-r^2_{xz}) \cdot (1-r^2_{yz})}}
$$

mit

-   $r_{xy}$ der Korrelation zwischen den Variablen $x$ und $y$
-   $r_{xz}$ der Korrelation zwischen den Variablen $x$ und $z$
-   $r_{yz}$ der Korrelation zwischen den Variablen $y$ und $z$

@matthews2000storks [Storks Deliver Babies (p = 0.008)](https://www.researchgate.net/profile/Robert-Matthews-2/publication/227763292_Storks_Deliver_Babies_p_0008/links/56f138d708aec9e096b316c9/Storks-Deliver-Babies-p-0008.pdf) fand eine Korrelation $r_{xy}$ von $0.62$ zwischen $x$ gleich Anzahl der brütenden Störche und $y$ gleich der Anzahl an Babies in den entsprechenden untersuchten Gebieten.

```{mermaid}
%%| label: fig-mermaid-part-cor-storch
%%| fig-width: 6
%%| fig-cap: "Flowchart der "
flowchart LR
    A("Störche"):::factor <-. 0.54 ..-> B("Babies"):::confound;
    A("Störche"):::factor <-- 0.6 --> C("Bevölkerungsdichte"):::factor;
    B("Bevölkerungsdichte"):::confound <-. 0.89 ..-> C("Babies"):::factor;
    classDef factor fill:#56B4E9,stroke:#333,stroke-width:0.75px
    classDef confound fill:#CC79A7,stroke:#333,stroke-width:0.75px

```

$$
r_{xy, z} = \cfrac{0.62 - 0.54 \cdot 0.89}{\sqrt{(1-0.54^2) \cdot (1-0.89^2)}} = 0.36
$$
:::

Im Folgenden nutzen wir einmal die Funktion `correlation()` mit der Option `partial = TRUE`. Zuerst haben wir dann nur noch die Anzahl an Beobachtungen in den Daten für die wir auch überall einen Wert haben. Sonst hätten wir ja noch die Möglichkeit, dass bei einzelnen Variablenpaaren mehr Beobachtungen übrig bleiben. Dann können wir auch schon unsere partielle Korrelation einmal in der @fig-scatter-corr-rpaket-03 abbilden. Wir setzen dann noch andere Farben mit der Funktion `scale_edge_color_continuous()` für wenig partielle Korrelation zu viel Korrelation.

```{r}
#| echo: false
#| message: false
#| label: fig-scatter-corr-rpaket-03
#| fig-align: center
#| fig-height: 8
#| fig-width: 10
#| fig-cap: "Partielle Korrelation in einem *Gaussian Graphical Model*. Je dicker die Linien desto stärker ist der Zusammenhang zwischen den Variablen desto sträker ist dann auch die partielle Korrelation."
corr_gummi_tbl |> 
  correlation(partial = TRUE) |> 
  plot() +
  scale_edge_color_continuous(low = "#000004FF", high = "#FCFDBFFF")
```

::: callout-tip
## Anwendungsbeispiel: Die Korrelation als Vergleich zweier Steigungen

In der @fig-corr-01 können wir uns noch einmal den Vorteil der Korrelation als ein einheitsloses Maß anschauen. Wenn wir uns nur die Steigung der beiden Gerade betrachten würden, dann wäre die Steigung $\beta_{kopfgewicht} = 0.021$ und die Steigung $\beta_{strunkdurchmesser} = 5.15$. Man könnte meinen, das es keinen Zusammenhang zwischen der Boniturnote und dem Kopfgewicht gäbe wohl aber einen starken Zusammenhang zwischen der Boniturnote und dem Durchmesser. Die Steigung der Geraden wird aber stark von den unterschiedlich skalierten Einheiten von Kopfgewicht in \[g\] und dem Strunkdurchmesserdurchmesser in \[cm\] beeinflusst.

![Der Zusammenhang von Hohlstrunk Boniturnote und Kopfgewicht sowie Strunkdurchmesser. In dem Beispiel ist gut der Zusammenhang zwischen der Steigung $\beta_1$ von $x$ und der Einheit von $x$ zu erkennen.](images/corr_example_einheit.jpeg){#fig-corr-01 fig-align="center"}

Wir wollen den Zusammenhang nochmal mit der Korrelation überprüfen, da die Korrelation nicht durch die Einheiten von $y$ und $x$, in diesem Fall den Einheiten von Kopfgewicht in \[g\] und dem Durchmesser in \[cm\], beeinflusst wird. Wir bauen uns zuerst einen künstlichen Datensatz in dem wir die Informationen aus der Geradengleichung nutzen. Dann addieren wir mit der Funktion `rnorm()` noch einen kleinen Fehler auf jede Beobachtung drauf.

```{r}
strunk_tbl <- tibble(durchmesser = seq(3.5, 4.5, by = 0.05),
                     bonitur = 5.15 * durchmesser - 16.65 + rnorm(length(durchmesser), 0, 1))
kopf_tbl <- tibble(gewicht = seq(410, 700, by = 2),
                   bonitur = 0.021 * gewicht - 8.42 + rnorm(length(gewicht), 0, 1))
```

Wir können und jetzt einmal die Korrelation aus den Daten berechnen. Die Koeffizienten der Geraden sind die gleichen Koeffizienten wie in der @fig-corr-01. Was wir aber sehen, ist das sich die Korrelation für beide Gerade sehr ähnelt oder fast gleich ist.

```{r}
strunk_tbl %$% 
  cor(durchmesser, bonitur, method = "spearman")

kopf_tbl %$% 
  cor(gewicht, bonitur, method = "spearman")
```

Wie wir sehen, können wir mit der Korrelation sehr gut verschiedene Zusammenhänge vergleichen. Insbesondere wenn die Gerade zwar das gleiche Outcome haben aber eben verschiedene Einheiten auf der $x$-Achse. Prinzipiell geht es natürlich auch für die Einheiten auf der $y$-Achse, aber meistens ist das Outcome der konstante Modellteil.
:::

## Visualisierung {#sec-linear-corr-visu}

Abschließend wollen wir uns noch die Funktion `corrplot()` aus dem gleichnamigen R Paket `{corrplot}` anschauen. Die [Hilfeseite zum Paket](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) ist sehr ausführlich und bietet noch eine Reihe an anderen Optionen. Wir benötigen dafür einen etwas größeren Datensatz mit mehreren numerischen Variablen. Wir nutzen daher den Gummibärchendatensatz und selektieren die Spalten `count_bears` bis `semester` aus.

Wir brauchen für die Funktion `corrplot()` eine Matrix mit den paarweisen Korrelationen. Wir können diese Matrix wiederum mit der Funktion `cor()` erstellen. Wir müssen dazu aber erstmal alle numerischen Variablen mit `select_if()` selektieren und dann alle fehlenden Werte über `na.omit()` entfernen.

```{r}
cor_mat <- corr_gummi_tbl %>% 
  select_if(is.numeric) %>% 
  na.omit %>% 
  cor()

cor_mat %>% round(3)
```

Wir sehen das in der Korrelationsmatrix jeweils über und unterhalb der Diagonalen die gespiegelten Zahlen stehen. Wir können jetzt die Matrix `cor_mat` in die Funktion `corrplot()` stecken und uns den Korrelationsplot in @fig-corrplot-01 einmal anschauen.

```{r}
#| echo: true
#| message: false
#| label: fig-corrplot-01
#| fig-align: center
#| fig-height: 6
#| fig-width: 6
#| fig-cap: "Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke."

corrplot(cor_mat)
```

Wir sehen in @fig-corrplot-01, dass wir eine schwache positive Korrelation zwischen `count_color` und `count_bears` haben, angezeigt durch den schwach blauen Kreis. Der Rest der Korrelation ist nahe Null, tendiert aber eher ins negative.

Nun ist in dem Plot natürlich eine der beiden Seiten überflüssig. Wir können daher die Funktion `corrplot.mixed()` nutzen um in das untere Feld die Zahlenwerte der Korrelation darzustellen.

```{r}
#| echo: true
#| message: false
#| label: fig-corrplot-mixed-01
#| fig-align: center
#| fig-height: 6
#| fig-width: 6
#| fig-cap: "Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke. In das untere Feld werden die Werte der Korrelation angegeben."

corrplot.mixed(cor_mat)
```

Es gibt noch eine Vielzahl an weiteren Möglichkeiten in den Optionen von der Funktion `corr.mixed()`. Hier hilft dann die Hilfeseite der Funktion oder aber die [Hilfeseite zum Paket](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html).

Eine weitere Möglichkeit kontinuierliche Daten darzustellen ist das R Paket `{GGally}` mit der Funktion `ggpairs()`. Hier können wir die paarweisen Zusammenhänge von Variablen, also den Spalten, darstellen. Prinzipiell geht es auch mit kategorialen Variablen, aber wir konzentrieren uns hier nur auf die numerischen. Im Folgenden wählen wir also nur die numerischen Spalten in unseren Gummibärchendaten einmal aus und nutzen die slektierten Daten dann einmal in der Funktion `ggpairs()`.

```{r}
corr_gummi_tbl <- corr_gummi_tbl %>% 
  select_if(is.numeric)
```

Der `ggpairs`-Plot baut sich als eine MAtrix auf, in der jede Variable mit jeder anderen Variable verglichen wird. Damit ergibt sich auf der Diagonalen ein Selbstvergleich und die obere Hälfte und untere Hälfte der Matrix beinhalten die gleichen Informationen. Hier setzt dann `ggpairs()` an und erlaubt in jede der drei Bereiche, obere Hälfte (`upper`), der Diagonalen (`diag`) sowie der unteren Hälfte (`lower`), eigene Abbildungen oder Maßzahlen für die Vergleiche der Variablen zu verwenden. In der @fig-corrplot-pairs-0 siehst du die Standardausgabe der Funktion `ggpairs()` auf einen Datensatz. Auf der unteren Hälfte ist der Scatterplot mit den einzelnen Beobachtungen, in der Diagonalen die Dichte der Variablen sowie im oberen Bereich die Korrelation zwischen den Variablen angegeben. Die Korrelation wurde auch noch einen statistischen Test unterworfen, so dass wir hier auch Sternchen für die Signifikanz bekommen.

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-corrplot-pairs-0
#| fig-align: center
#| fig-height: 6
#| fig-width: 6
#| fig-cap: "Unmodifizierte Abbildung aus `ggpairs()` mit allen paarweisen Vergleichen der numerischen Variablen. Auf der unteren Hälfte ist der Scatterplot, in der Diagonalen die Dichte der Variablen sowie im oberen Bereich die Korrelation zwischen den Variablen angegeben."
ggpairs(corr_gummi_tbl) +
  theme_minimal()
```

Die Standardabbildung ist okay, wenn du mal in die Daten schauen willst. Aber eigentlich sind wir an einer schöneren Abbildung interessiert. Wie immer, was ist schon schön, aber ich zeige dir einmal, wie du die Abbildungen in den jeweiligen Bereichen ändern kannst. Bei `{GGally}` hilft mir eigentlich immer am besten den konkreten Sachverhalt zu googlen, den ich ändern will. Wenn es zu viel wird, dann hilft es mehr sich die Abbildungen dann doch selber zu bauen und über `{patachwork}` zusammenzukleben. Es geht halt nicht beides, schnell und flexibel. In den folgenden Tabs findest du jeweils eine Funktion, die den oberen, diagonalen und unteren Bereich modifiziert. Die Funktion rufen wir dann in der Funktion `ggpairs()` auf.

::: panel-tabset
## Upper

Wir wollen in den oberen Bereich die Korrelation haben, aber ohen die Sternchen und ohne das Wort `Corr:`. Deshalb müssen wir uns hier nochmal die Korrelationsfunktion selber nachbauen.

```{r}
#| message: false
#| warning: false

cor_func <- function(data, mapping, method, symbol, ...){
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  corr <- cor(x, y, method=method, use='complete.obs')
  ggally_text(
    label = paste(symbol, as.character(round(corr, 2))), 
    mapping = aes(),
    xP = 0.5, yP = 0.5,
    color = 'black'
  ) 
}
```

## Diag

Auf der Diagonalen wollen wir die Desnityplots haben. Die sind auch so da, aber ich färbe die Plots hier nochmal rot ein. Einfach damot du siehst, was man machen kann.

```{r}
#| message: false
#| warning: false
diag_fun <- function(data, mapping) {
  ggplot(data = data, mapping = mapping) +
    geom_density(fill = "red", alpha = 0.5)
}
```

## Lower

In dem unteren Bereich wollen wir die Punkte etwas kleiner haben, deshalb das `feom_point2()` aus dem R Paket `{see}`. Dann möchte ich noch die Regressionsgrade einmal zeichnen. Auch hier geht dann mehr, wenn du `loess` oder aber den Standardfehler sehen willst.

```{r}
#| message: false
#| warning: false

lower_fun <- function(data, mapping) {
  ggplot(data = data, mapping = mapping) +
    geom_point2() + 
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE)
}
```
:::

Und dann sammeln wir alles ein und bauen uns die @fig-corrplot-pairs-1. Wir machen uns es hier etwas einfacher und schreiben gleich das Symbol $\rho$ als ASCII-Zeichen, da sparen wir etwas nerven. Ansonsten siehst du wie durch die Optionen `upper =`, `diag =` und `lower =` die obigen Funktionen zugewiesen werden und damit dann die einzelnen Bereiche individuell gebaut werden. Wichtig finde ich noch die Möglichkeit, die Seitennamen der Abbildung dann hier in der Funktion über `columnLabels =` sauber zu benennen.

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-corrplot-pairs-1
#| fig-align: center
#| fig-height: 6
#| fig-width: 6
#| fig-cap: "Modifizierte Abbildung aus `ggpairs()` mit allen paarweisen Vergleichen der numerischen Variablen. Auf der unteren Hälfte ist der Scatterplot zusammen mit der Regressionsgrade aus `stat_smooth()`, in der Diagonalen die eingefärbte Dichte der Variablen sowie im oberen Bereich die Korrelation zwischen den Variablen ohne die Signifikanz und der Überschrift `Corr:` sondern mit $\\rho$ angegeben."
ggpairs(corr_gummi_tbl, 
        upper = list(continuous = wrap(cor_func, method = 'pearson', symbol = expression('\u03C1 ='))),
        diag = list(continuous = wrap(diag_fun)),
        lower = list(continuous = wrap(lower_fun)),
        columnLabels = c("Anzahl Bärchen", "Anzahl Farben", "Meist gemocht", 
                         "Alter", "Größe", "Semester")) +
  theme_minimal()
```

Hier hilft es dann auch mal mit den Themes `theme_minimal()` oder `theme_void()`. In der @fig-corrplot-pairs-2 habe ich die Labels durch die Funktion `axisLabels = "internal"` auf die Diagonale gesetzt. Dann musst du entweder die Namen kürzer machen oder aber den Plot größer. Ich habe mich hier für kürzere Namen entschieden. Dementsprechenden spiele einfach mal mit den Möglichkeiten, bis du eine gute Abbildung für dich gefunden hast.

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-corrplot-pairs-2
#| fig-align: center
#| fig-height: 9
#| fig-width: 9
#| fig-cap: "Modifizierte Abbildung aus `ggpairs()` mit allen paarweisen Vergleichen der numerischen Variablen. Hier einmal mit internen Achsenbeschrfitungen und zur Abwechselung dem Theme `theme_minimal()`."
ggpairs(corr_gummi_tbl, 
        lower = list(continuous = wrap(lower_fun)),
        columnLabels = c("Bärchen", "Farben", "Gemocht",
                         "Alter", "Größe", "Semester"),
        axisLabels = "internal") +
  theme_minimal()

```

## Referenzen {.unnumbered}
