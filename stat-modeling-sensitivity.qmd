```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Sensitivitätsanalyse {#sec-sensitivity}

*Letzte Änderung am `r format(fs::file_info("stat-modeling-sensitivity.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"Zu tun, worauf man Lust hat, und nicht, was man muss -- das ist eines der ältesten und wichtigsten Ziele der Menschheit. Wir arbeiten hart daran, das zu verdrängen." --- Die Not des Müßiggangs, Magazin Brand Eins*

::: {.callout-caution appearance="simple"}
## Stand des Kapitels: Archiviert (seit 01.2025)

Dieses Kapitel ist archiviert, da ich die Thematik des Kapitels aktuell nicht in meiner Lehre oder der statistischen Beratung benötige. Archivierte Kapitel werden nicht von mir weiter gepflegt oder ergänzt. Auftretende Fehler werden aber natürlich beseitigt, wenn die Fehler mir auffallen.
:::

Wir brauchen die Sensitivitätsanalyse wenn wir Beobachtungen aus unseren Daten entfernt oder aber hinzugefügt haben. Sensitivitätsanalysen finden eigentlich in dem Kontext von klinischen Studien statt. Der Trend geht aber natürlich auch nicht an den Agrarwissenschaften vorbei und solltest du den Begriff mal hören, weist du wo Sensitivitätsanalyse hingehören. Machen wir also eine Sensitivitätsanalyse. Was haben wir aber *davor* gemacht? Das heißt du hast entweder eine Variablenselektion wie im @sec-variable-selection beschrieben durchgeführt. Oder aber du hast fehlende Werte wie in @sec-missing beschrieben imputiert. Es kann auch sein, dass du Ausreißer aus den Daten entfernt oder aber imputiert hast, wie es in @sec-outlier beschrieben ist. Im Prinzip kannst du auch alles drei gemacht haben, aber meistens beschränkt sich die *Veränderung* der Daten nur auf eins der drei Möglichkeiten.

Wie immer brauchen wir natürlich auch Fallzahl. Eine Sensitivitätsanalyse kannst du nicht auf zwanzig bis fünfzig Beobachtungen machen. Du brauchst schon eine gute dreistellige Anzahl, damit du hier sauber Modellieren und Darstellen kannst. Wenn du weniger Beobachtungen hast, dann ist ganz natürlich das einzelne Werte einen riesigen Einfluss haben *müssen*. Im Zweifel frag einfach einmal bei mir nach, dann können wir die Sachlage diskutieren.

::: {layout="[15,85]" layout-valign="center"}
![](images/angel_01.png){fig-align="center" width="100%"}

> Das ist hier natürlich eine Sensitivitätsanalyse für Arme. Wie man es richtig umfangreich macht, findest du in einem sehr gutem und umfangreichen Tutorial zu [What Makes a Sensitivity Analysis?](https://lesslikely.com/statistics/sensitivity/)
:::

Dieses Kapitel ist relativ übersichtlich. Wir werden die Modelle nach der jeweiligen algorithmischen Veränderung uns nochmal anschauen und dann *deskriptive* entscheiden, ob wir eine große Veränderung in den Daten sehen. Es gibt zwar auch die Möglichkeit die Modelle untereinander zu vergleichen, aber ist hier die Aussagekraft nicht so stark. Die Idee hinter dem Modellvergleich ist eher die *Anzahl an Spalten* zu verändern und nicht die Werte in der Datenmatrix. Deshalb machen wir es zwar, genießen die Sache aber mit Vorsicht.

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, dlookr, broom, modelsummary,
               see, performance, ggpubr, factoextra, FactoMineR,
               conflicted)
```

An der Seite des Kapitels findest du den Link *Quellcode anzeigen*, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.

## Daten

In diesem Beispiel betrachten wir wieder die Gummibärchendaten. Auch hier haben wir echte Daten vorliegen, so dass wir Ausreißer entdecken könnten. Da wir hier auch fehlende Werte in den Daten haben, können wir diese fehlenden Werte auch einfach imputieren und uns dann die Effekte anschauen. Das heißt wir haben also einen idealen Datensatz für unsere Sensitivitätsanalysen.

```{r}
#| message: false

gummi_tbl <- read_excel("data/gummibears.xlsx")  |>
  select(gender, age, height, semester) |> 
  mutate(gender = as_factor(gender)) 
```

In der @tbl-gummi-1 ist der Datensatz `gummi_tbl` nochmal für die ersten sieben Zeilen dargestellt. Wir werden später sehen, wie sich die Fallzahl von $n = `r nrow(gummi_tbl)`$ immer wieder ändert, je nachdem wie wir mit den fehlenden Daten und den Variablen umgehen.

```{r}
#| message: false
#| echo: false
#| tbl-cap: "Auszug aus dem Datensatz `gummi_tbl`. Wir betrachten die ersten sieben Zeilen des Datensatzes."
#| label: tbl-gummi-sens-1

gummi_tbl |> head(7) |> kable(align = "c", "pipe")
```

## Das Modell

Wir wollen jetzt als erstes das volle Modell schätzen. Das heißt wir packen alle Variablen in das Modell und rechnen dann die lineare Regression. Wir wollen herausfinden in wie weit das Alter, das Geschlecht und das Semester einen Einfluss auf die Körpergröße von Studierenden hat.

$$
height \sim gender + age + semester 
$$

Wir haben nichts an den Daten geändert und somit dient unser volles Modell als Benchmark für die anderen. Wenn sich einige Werte der Modellgüten im Vergleich zum vollen Modell ändern, dann wissen wir, dass etwas nicht stimmt.

```{r}
#| message: false
#| warning: false
fit_full <- lm(height ~ gender + age + semester, data = gummi_tbl)

```

Neben dem vollen Modell rechnen wir auch noch das Nullmodel. Das Nullmodell beinhaltet nur den Intercept und sonst *keine* Einflussvariable. Wir wollen schauen, ob es überhaupt was bringt eine unserer Variablen in das Modell zu nehmen oder ob wir es auch gleich lassen können. Im Prinzip unsere Kontrolle für das Modellieren.

$$
height \sim 1
$$

In R fitten wir das Nullmodell in dem wir keine Variablen mit in das Modell nehmen sondern nur eine 1 schreiben. Wir haben dann nur den Intercept mit in dem Modell und sonst nichts. Was wir schon aus den anderen Kapiteln wissen ist, dass das Nullmodell ein schlechtes Modell sein wird.

```{r}
#| message: false
#| warning: false

fit_null <- lm(height ~ 1, data = gummi_tbl)
```

Wir schauen uns die Modelle hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert.

## ... nach der Detektion von Ausreißer

Teilweise können wir eine Überprüfung auf Ausreißer nur auf einen Datensatz *ohne* fehlende Werte durchführen. Hier beißt sich dann die Katze in den Schwanz. Deshalb nutzen wir die Funktion `diagnose_outlier()`, die intern die fehlenden Werte entfernt. Das ist natürlich kein richtiges Vorgehen! Aber wir nutzen ja diesen Abschnitt nur als Beispiel. Du findest die Detektion von Ausreißern im @sec-outlier beschrieben.

```{r}
#| message: false
#| warning: false
diagnose_outlier(gummi_tbl) 
```

Wir sehen, dass wir in der Variable `age` und `semester` nach der Funktion zu urteilen Ausreißer gefunden haben. Deshalb werden wir jetzt diese Ausreißer durch die Funktion `imputate_outlier()` entsprechend ersetzen. Mal schauen, ob wir damit eine substanzielle Änderung in der Modellierung erhalten.

```{r}
#| message: false
#| warning: false
gummi_out_imp_tbl <- gummi_tbl |> 
  mutate(age = imputate_outlier(gummi_tbl, age, method = "capping"),
         semester = imputate_outlier(gummi_tbl, semester, method = "capping"))
```

Nun modellieren wir noch mit unseren ersetzten und angepassten Daten die Körpergröße und erhalten den Modellfit zurück. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.

```{r}
#| message: false
#| warning: false
fit_outlier <- lm(height ~ gender + age + semester, data = gummi_out_imp_tbl)
```

Wir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert.

## ... nach der Imputation von fehlenden Werten

Nehmen wir wieder den Gummibärechendatensatz von neuen und imputieren diesmal die fehlenden Werte mit einer univariaten Imputation. Wir machen uns hier nicht die Mühe ein multivariates Verfahren zu nutzen. Das könnte man tun, aber wir wollen hier ja nur den Weg aufzeigen, wie wir den Vergleich der Modelle zur Sensitivitätsanalyse durchführen. Du findest die Imputation von fehlenden Werten im @sec-missing beschrieben.

In unserem Fall imputieren wir alle numerischen Variablen mit dem Mittelwert und die kategoriale Variable mit der Methode `rpart`. Damit haben wir dann keine fehlenden Werte mehr in den Daten und somit sollte das jetzt auch unserer größter Datensatz für die lineare Regression sein. Nicht vergessen, sobald wir einen fehlenden Wert bei einer Variable in einem Modell haben, fällt die ganze Beobachtung aus dem Modell heraus.

```{r}
#| message: false
#| warning: false
gummi_imp_tbl <- gummi_tbl |> 
  mutate(age = imputate_na(gummi_tbl, age, method = "mean"),
         gender = imputate_na(gummi_tbl, gender, method = "rpart"),
         height = imputate_na(gummi_tbl, height, method = "median"),
         semester = imputate_na(gummi_tbl, semester, method = "mode"))

```

Dann rechnen wir noch schnell das Modell für die imputierten Daten. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.

```{r}
#| message: false
#| warning: false
fit_imp <- lm(height ~ gender + age + semester, data = gummi_imp_tbl)

```

Wir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert.

## ... nach der Variablen Selektion

Für die Variablenselektion machen wir es uns sehr einfach. Wir müssen ja nur eine Spalte aus den Daten werfen, mehr ist ja Variablenselektion auch nicht. Wir machen dort nur eine algorithmengetriebene Auswahl. In diesem Fall entscheide ich einfach zufällig welche Variable aus dem Modell muss. Du findest die Variablen Selektion im @sec-variable-selection beschrieben. Somit nehmen wir an, wir hätten eine Variablenselektion durchgeführt und die Variable `semester` aus dem Modell entfernt.

```{r}
#| message: false
#| warning: false
fit_var_select <- lm(height ~ gender + age, data = gummi_tbl)
```

Auch dieses Modell schauen wir nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert.

## Modellvergleich

Kommen wir zu dem eigentlichen Modellvergleich. In @tbl-model-comp-sens sehen wir den Modellvergleich aller fünf Modelle aus diesem Kapitel. Dazu nutzen wir die Funktion `modelsummary()` aus dem R Paket `{modelsummary}`. Wir vergleichen die Modelle untereinander aber vor allem mit dem vollen Modell. Das volle Modell basiert ja auf den ursprünglichen nicht veränderten Daten. Den Intercept können wir erstmal ignorieren. Spannend ist, dass sich der Effekt von `gender` auf die Körpergröße durch die Imputation um eine Einheit ändert. Der Effekt des Alters verfünffacht sich durch die Outlieranpassung und verdoppelt sich durch die Imputation. Durch die Imputation wird der Effekt des Semesters abgeschwächt.

Wenn wir auf das $R^2_{adj}$ schauen, dann haben wir eine Verschlechterung durch die Imputation. Sonst bleibt der Wert mehr oder minder konstant. Das ist ein gutes Zeichen, dass wir unser Modell nicht vollkommen an die Wand gefahren haben durch unsere Änderung der Daten. Das $AIC$ wird folglich für die Imputationsdaten sehr viel schlechter und nähert sich dem Nullmodell an. Das ist wirklcih kein gutes Zeichen für die Imputation. Da haben wir mehr kaputt als heile gemacht. Wir sehen keinen Efdekt bei dem Fehler $RMSE$, der noch nach dem Fit des Modell übrig bleibt. Aber das kann passieren. Nicht jede Maßzahl muss sich auch ändern. Deshalb haben wir ja mehrere Maßzahlen vorliegen.

```{r}
#| message: false
#| echo: true
#| tbl-cap: "Modellvergleich mit den fünf Modellen. Wir schauen in wie weit sich die Koeffizienten und Modelgüten für die einzelnen Modelle im direkten Vergleich zum vollen Modell verändert haben."
#| label: tbl-model-comp-sens

modelsummary(lst("Null Modell" = fit_null,
                 "Volles Modell" = fit_full,
                 "Outlier" = fit_outlier,
                 "Imputation" = fit_imp,
                 "Variablen Selektion" = fit_var_select),
             estimate  = "{estimate}",
             statistic = c("conf.int",
                           "s.e. = {std.error}", 
                           "t = {statistic}",
                           "p = {p.value}"))
```

Das vergleichen von Modellen, die auf *unterschiedlichen* Daten basieren ist nicht anzuraten. Wir erhalten auch die passende Warnung von der Funktion `compare_performance()` aus dem R Paket `{performance}`. Dennoch hier einmal der Vergleich. Wir sehen, dass die Modelle mit der Ersetzung der Ausreißer und das volle Modell sich stark ähneln. Das selektierte Modell und das imputierte Modell fallen dagegen ab. Da wir ja hier nicht zeigen wollen, dass sich die Modelle unterscheiden, ist das Ergebnis ähnlich zu der Übersicht. Die Imputation hat so nicht funktioniert.

```{r}
#| message: false
#| warning: false
#| echo: false

compare_performance(fit_null, fit_full, fit_outlier, fit_imp, fit_var_select,
                    rank = TRUE)
```

Was ist das Fazit aus der Sensitivitätsanalyse für Arme? Nun wir konnten einmal sehen, dass wir auch mit einfachen Werkzeugen Modelle deskriptiv miteinander vergleichen können und dann einen Schluss über die Güte der Detektion von Ausreißern, der Imputation von fehlenden Werten oder aber der Variablenselektion treffen können. Denk immer dran, die Sensitivitätsanalyse findet *nach* einer sauberen Detektion, Imputation oder Selektion statt und soll nochmal sicherstellen, dass wir nicht künstliche Effekte der Algorithmen modellieren sondern die Effekte in den Daten sehen.
