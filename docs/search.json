[
  {
    "objectID": "stat-modeling-poisson.html",
    "href": "stat-modeling-poisson.html",
    "title": "41  Poisson Regression",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:48:48\nIn diesem Kapitel wollen wir eine Poisson Regression rechnen. Wir müssen uns hier wieder überlegen, was ist eigentlich unser Outcome \\(y\\) und was sind unsere Einflussvariablen \\(x\\). Die Poisson Regression ist je nach Hintergurnd des Anwenders eher selten. In der Ökologie, wo gerne mal gezaählt wird, wie oft etwas vorkommt, ist die Poisson Regression häufig vertreten. Sonst fristet die Poisson Regresson eher ein unbekanntes Dasein.\nEin häufig unterschätzter Vorteil der Poisson Regression ist, dass wir auch auch \\(0/1\\) Daten eine Poisson Regression rechnen können. Moment, wirst du jetzt vielleicht denken, das machen wir doch mit der logistsichen Regression. Ja, das stimmt, aber wir können auf Zahlen viel rechnen. Wenn wir auf ein \\(0/1\\) Outcome eine Poisson Regression rechnen, dann kriegen wir nicht Odds Ratios \\(OR\\) als Effektschätzer sondern Risk Ratios \\(RR\\). Wir erhalten also keine Chancen sondern Wahrscheinlichkeiten. Unter der Annahme, dass das Modell auch konvergiert und wir sinnvolle Zahlen erhalten.\nEin weiteres Problem sind die zu vielen Nullen in dem Outcome \\(y\\). Daherher wir zählen über die Maßen viel Nichts. Wir nennen diesen Fall zero inflation und beschreiben damit die zu vielen Nullen in den Daten. Hier muss dann noch speziell modelliert werden. Eine Poisson Regression hat schon so seine speziellen Tücken."
  },
  {
    "objectID": "stat-modeling-poisson.html#annahmen-an-die-daten",
    "href": "stat-modeling-poisson.html#annahmen-an-die-daten",
    "title": "41  Poisson Regression",
    "section": "\n41.1 Annahmen an die Daten",
    "text": "41.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Poissonverteilung.\nIm folgenden Kapitel zu der multiplen Poisson linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 36 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Poissonverteilung. Das ist hier sehr wichtig, denn wir wollen ja eine multiple Poisson lineare Regression rechnen.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen."
  },
  {
    "objectID": "stat-modeling-poisson.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-poisson.html#genutzte-r-pakete-für-das-kapitel",
    "title": "41  Poisson Regression",
    "section": "\n41.2 Genutzte R Pakete für das Kapitel",
    "text": "41.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, MASS, pscl, see,\n               modelsummary, scales)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-poisson.html#daten",
    "href": "stat-modeling-poisson.html#daten",
    "title": "41  Poisson Regression",
    "section": "\n41.3 Daten",
    "text": "41.3 Daten\nIm folgenden schauen wir uns ein Datenbeispiel mit Hechten an. Es handelt sich um langnasige Hechte in nordamerikanischen Flüssen. Wir haben uns insgesamt \\(n = 68\\) Flüsse einmal angesehen und dort die Anzahl an Hechten gezählt. Im Weiteren haben wir dann noch andere Flussparameter erhoben und fragen uns nun, welche dieser Parameter einen Einfluss auf die Anzahl an Hechten in den Flussarmen haben. In Kapitel 10.2 findest du nochmal mehr Informationen zu den Daten. Wir entfernen hier die Informationen zu den Flüssen, die brauchen wir in dieser Analyse nicht.\n\n\nDie Daten zu den langnasigen Hechten stammt von Salvatore S. Mangiafico - An R Companion for the Handbook of Biological Statistics.\n\nlongnose_tbl <- read_csv2(\"data/longnose.csv\") %>% \n  select(-stream)\n\n\n\n\n\nTabelle 41.1— Auszug aus dem Daten zu den langnasigen Hechten.\n\nlongnose\narea\ndo2\nmaxdepth\nno3\nso4\ntemp\n\n\n\n13\n2528\n9.6\n80\n2.28\n16.75\n15.3\n\n\n12\n3333\n8.5\n83\n5.34\n7.74\n19.4\n\n\n54\n19611\n8.3\n96\n0.99\n10.92\n19.5\n\n\n19\n3570\n9.2\n56\n5.44\n16.53\n17\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n26\n1450\n7.9\n60\n2.96\n8.84\n18.6\n\n\n20\n4106\n10\n96\n2.62\n5.45\n15.4\n\n\n38\n10274\n9.3\n90\n5.45\n24.76\n15\n\n\n19\n510\n6.7\n82\n5.25\n14.19\n26.5\n\n\n\n\n\n\nIm Folgenden werden wir die Daten nur für das Fitten eines Modells verwenden. In den anderen oben genannten Kapiteln nutzen wir die Daten dann anders. In Abbildung 41.1 sehen wir nochmal die Verteilung der Anzahl der Hechte in den Flüssen.\n\nggplot(longnose_tbl, aes(longnose)) +\n  theme_bw() +\n  geom_histogram()\n\n\n\nAbbildung 41.1— Histogramm der Verteilung der Hechte in den beobachteten Flüssen."
  },
  {
    "objectID": "stat-modeling-poisson.html#fit-des-modells",
    "href": "stat-modeling-poisson.html#fit-des-modells",
    "title": "41  Poisson Regression",
    "section": "\n41.4 Fit des Modells",
    "text": "41.4 Fit des Modells\nIn diesem Abschnitt wollen wir verschiedene Modelle für Zähldaten schätzen. Die Poissonverteilung hat keinen eignen Parameter für die Streung wie die Normalverteilung. Die Poissonverteilung ist mit \\(\\mathcal{Pois}(\\lambda)\\) definiert und hat somit die Eigenschaft das die Varianz eins zu eins mit dem Mittelwert \\(\\lambda\\) der Poissonverteilung ansteigt. Es kann aber sein, dass wir in den Daten nicht diesen ein zu eins Zusammenhang von Mittelwert und Varianz vrliegen haben. Häufig ist die Varianz viel größer und steigt schneller an. Wenn die Varianz in Wirklichkeit sehr viel größer ist, dann würden wir die Varianz in unseren Modell unterschätzen.\n\nEin klassisches Poissonmodell glm(..., familiy = poisson) mit der Annahme keiner Overdisperison.\nEin Quasi-Poissonmodell glm(..., family = quasipoisson) mit der Möglichkeit der Berücksichtigung einer Overdispersion.\nEin negative Binomialmodell glm.nb(...) ebenfalls mit der Berücksichtigung einer Overdispersion.\n\nBeginnen wollen wir aber mit einer klassischen Poissonregression ohne die Annahme von einer Overdispersion in den Daten. Wir nutzen dafür die Funktion glm() und spezifizieren die Verteilungsfamilie als poisson. Wir nehmen wieder alle Variablen in das Modell auf der rechten Seite des ~. Auf der linken Seite des ~ kommt dann unser Outcome longnose was die Anzahl an Hechten erhält.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\nWir müssen für die Possionregression noch beachten, dass die Zähldaten von \\(0\\) bis \\(+\\infty\\) laufen. Damit wir normalverteilte Residuen erhalten und einen lineren Zusammenhang, werden wir das Modell auf dem \\(\\log\\)-scale fitten. Das heißt, wir werden den Zusammenhang von \\(y\\) und \\(x\\) logarithmieren. Wichtig ist hierbei der Zusammenhang. Wir transformieren nicht einfach \\(y\\) und lassen den Rest unberührt. Das führt dazu, dass wir am Ende die Koeffizienten der Poissonregression exponieren müssen. Das können die gängigen Funktionen, wir müssen das Exponieren aber aktiv durchführen. Deshalb hier schon mal erwähnt.\n\npoisson_fit <- glm(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                    longnose_tbl, family = poisson)\n\nWir schauen uns die Ausgabe des Modells einmal mit der summary() Funktion an, da wir hier einmal händisch schauen wollen, ob eine Overdispersion vorliegt. Sonst könnten wir auch die Funktion model_parameters() nehmen. Die nutzen wir später für die Interpretation des Modells, hier wollen wir erstmal sehen, ob alles geklappt hat.\n\npoisson_fit %>% summary\n\n\nCall:\nglm(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, family = poisson, data = longnose_tbl)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.234  -4.086  -1.662   1.771  14.362  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.564e+00  2.818e-01  -5.551 2.83e-08 ***\narea         3.843e-05  2.079e-06  18.480  < 2e-16 ***\ndo2          2.259e-01  2.126e-02  10.626  < 2e-16 ***\nmaxdepth     1.155e-02  6.688e-04  17.270  < 2e-16 ***\nno3          1.813e-01  1.068e-02  16.974  < 2e-16 ***\nso4         -6.810e-03  3.622e-03  -1.880   0.0601 .  \ntemp         7.854e-02  6.530e-03  12.028  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2766.9  on 67  degrees of freedom\nResidual deviance: 1590.0  on 61  degrees of freedom\nAIC: 1936.9\n\nNumber of Fisher Scoring iterations: 5\n\n\nWir schauen in die Summary-Ausgabe des Poissonmodells und sehen, dass dort steht, dass Dispersion parameter for poisson family taken to be 1. Wir modellieren also einen eins zu eins Zusammenhang von Mittelwert und Varianz. Wenn dieser Zusammenhang nicht in unseren Daten existiert, dann haben wir eine Overdispersion vorliegen.\nWir können die Overdispersion mit abschätzen indem wir die Residual deviance durch die Freiheitsgrade der Residual deviance teilen. Daher erhalten wir eine Overdispersion von \\(\\cfrac{1590.04}{61} \\approx 26.1\\). Damit haben wir eine eindeutige Overdispersion vorliegen. Damit steigt die Varianz in einem Verhältnis von ca. 1 zu 26. Wir können auch die Funktion check_overdispersion() aus dem R Paket performance nutzen um die Overdispersion zu berechnen. Die Funktion kann das schneller und ist auch in der Abfolge einer Analyse besser geeignet.\n\npoisson_fit %>% check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =   29.403\n  Pearson's Chi-Squared = 1793.599\n                p-value =  < 0.001\n\n\nOverdispersion detected.\n\n\nWenn wir Overdispersion vorliegen haben und damit die Varianz zu niedrig schätzen, dann erhalten wir viel mehr signifikante Ergebnisse als es in den Daten zu erwarten wäre. Schauen wir uns nochmal die Parameter der Poissonverteilung und die \\(p\\)-Werte einmal an.\n\npoisson_fit %>% model_parameters()\n\nParameter   |  Log-Mean |       SE |         95% CI |     z |      p\n--------------------------------------------------------------------\n(Intercept) |     -1.56 |     0.28 | [-2.12, -1.01] | -5.55 | < .001\narea        |  3.84e-05 | 2.08e-06 | [ 0.00,  0.00] | 18.48 | < .001\ndo2         |      0.23 |     0.02 | [ 0.18,  0.27] | 10.63 | < .001\nmaxdepth    |      0.01 | 6.69e-04 | [ 0.01,  0.01] | 17.27 | < .001\nno3         |      0.18 |     0.01 | [ 0.16,  0.20] | 16.97 | < .001\nso4         | -6.81e-03 | 3.62e-03 | [-0.01,  0.00] | -1.88 | 0.060 \ntemp        |      0.08 | 6.53e-03 | [ 0.07,  0.09] | 12.03 | < .001\n\n\nIn der Spalte p finden wir die \\(p\\)-Werte für alle Variablen. Wir sehen, dass fast alle Variablen signifikant sind und das wir eine sehr niedrige Varianz in der Spalte SE sehen. Das heißt unser geschätzer Fehler ist sehr gering. Das ahnten wir ja schon, immerhin haben wir eine Overdisperson vorliegen. Das Modell ist somit falsch. Wir müssen uns ein neues Modell suchen, was Overdispersion berückscihtigen und modellieren kann.\nDie Quasi-Poisson Verteilung hat einen zusätzlichen, unabhänigen Parameter um die Varianz der Verteilung zu schätzen. Daher können wir die Overdispersion mit einer Quasi-Poisson Verteilung berückscihtigen. Wir können eine Quasi-Poisson Verteilung auch mit der Funktion glm() schätzen nur müssen wir als Verteilungsfamilie quasipoisson angeben.\n\nquasipoisson_fit <- glm(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                        data = longnose_tbl, family = quasipoisson)\n\nNach dem Modellti können wir nochmal in der summary() Funktion schauen, ob wir die Overdispersion richtig berücksichtigt haben.\n\nquasipoisson_fit %>% summary\n\n\nCall:\nglm(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, family = quasipoisson, data = longnose_tbl)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.234  -4.086  -1.662   1.771  14.362  \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.564e+00  1.528e+00  -1.024  0.30999   \narea         3.843e-05  1.128e-05   3.408  0.00116 **\ndo2          2.259e-01  1.153e-01   1.960  0.05460 . \nmaxdepth     1.155e-02  3.626e-03   3.185  0.00228 **\nno3          1.813e-01  5.792e-02   3.130  0.00268 **\nso4         -6.810e-03  1.964e-02  -0.347  0.73001   \ntemp         7.854e-02  3.541e-02   2.218  0.03027 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 29.40332)\n\n    Null deviance: 2766.9  on 67  degrees of freedom\nResidual deviance: 1590.0  on 61  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nAn der Zeile Dispersion parameter for quasipoisson family taken to be 29.403319 in der Summary-Ausgabe sehen wir, dass das Modell der Quasi-Possion Verteilung die Overdispersion korrekt berücksichtigt hat. Wir können uns nun einmal die Modellparameter anschauen. Die Interpretation machen wir am Ende des Kapitels.\n\nquasipoisson_fit %>% model_parameters()\n\nParameter   |  Log-Mean |       SE |        95% CI | t(61) |      p\n-------------------------------------------------------------------\n(Intercept) |     -1.56 |     1.53 | [-4.57, 1.41] | -1.02 | 0.306 \narea        |  3.84e-05 | 1.13e-05 | [ 0.00, 0.00] |  3.41 | < .001\ndo2         |      0.23 |     0.12 | [ 0.00, 0.45] |  1.96 | 0.050 \nmaxdepth    |      0.01 | 3.63e-03 | [ 0.00, 0.02] |  3.18 | 0.001 \nno3         |      0.18 |     0.06 | [ 0.07, 0.29] |  3.13 | 0.002 \nso4         | -6.81e-03 |     0.02 | [-0.05, 0.03] | -0.35 | 0.729 \ntemp        |      0.08 |     0.04 | [ 0.01, 0.15] |  2.22 | 0.027 \n\n\nJetzt sieht unser Modell und die \\(p\\)-Werte zusammen mit dem Standardfehler SE schon sehr viel besser aus. Wir können also diesem Modell erstmal von der Seite der Overdispersion vertrauen.\nAm Ende wollen wir nochmal das Modell mit der negativen Binomialverteilung rechnen. Die negativen Binomialverteilung erlaubt auch eine Unabhängigkeit von dem Mittelwert zu der Varianz. Wir können hier auch für die Overdispersion adjustieren. Wir rechnen die negativen Binomialregression mit der Funktion glm.nb() aus dem R Paket MASS. Wir müssen keine Verteilungsfamilie angeben, die Funktion glm.nb() kann nur die negative Binomialverteilung modellieren.\n\nnegativebinomial_fit <- glm.nb(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                               data = longnose_tbl)\n\nAuch hier schauen wir mit der Funktion summary() einmal, ob die Overdisprsion richtig geschätzt wurde oder ob hier auch eine Unterschätzung des Zusammenhangs des Mittelwerts und der Varianz vorliegt.\n\nnegativebinomial_fit %>% summary()\n\n\nCall:\nglm.nb(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, data = longnose_tbl, init.theta = 1.666933879, link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4601  -0.9876  -0.4426   0.4825   2.2776  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.946e+00  1.305e+00  -2.256 0.024041 *  \narea         4.651e-05  1.300e-05   3.577 0.000347 ***\ndo2          3.419e-01  1.050e-01   3.256 0.001130 ** \nmaxdepth     9.538e-03  3.465e-03   2.752 0.005919 ** \nno3          2.072e-01  5.627e-02   3.683 0.000230 ***\nso4         -2.157e-03  1.517e-02  -0.142 0.886875    \ntemp         9.460e-02  3.315e-02   2.854 0.004323 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.6669) family taken to be 1)\n\n    Null deviance: 127.670  on 67  degrees of freedom\nResidual deviance:  73.648  on 61  degrees of freedom\nAIC: 610.18\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.667 \n          Std. Err.:  0.289 \n\n 2 x log-likelihood:  -594.175 \n\n\nAuch hier sehen wir, dass die Overdispersion mit dem Parameter \\(\\theta\\) berücksichtigt wird. Wir können die Zahl \\(1.67\\) nicht direkt mit der Overdispersion aus einer Poissonregression verglechen, aber wir sehen dass das Verhältnis von Residual deviance zu den Freiheitsgraden mit \\(\\cfrac{73.65}{61} \\approx 1.20\\) fast bei 1:1 liegt. Wir könnten also auch eine negative Binomialverteilung für das Modellieren nutzen.\n\nnegativebinomial_fit %>% model_parameters()\n\nParameter   |  Log-Mean |       SE |         95% CI |     z |      p\n--------------------------------------------------------------------\n(Intercept) |     -2.95 |     1.31 | [-5.85, -0.10] | -2.26 | 0.024 \narea        |  4.65e-05 | 1.30e-05 | [ 0.00,  0.00] |  3.58 | < .001\ndo2         |      0.34 |     0.11 | [ 0.11,  0.58] |  3.26 | 0.001 \nmaxdepth    |  9.54e-03 | 3.47e-03 | [ 0.00,  0.02] |  2.75 | 0.006 \nno3         |      0.21 |     0.06 | [ 0.10,  0.32] |  3.68 | < .001\nso4         | -2.16e-03 |     0.02 | [-0.03,  0.03] | -0.14 | 0.887 \ntemp        |      0.09 |     0.03 | [ 0.03,  0.16] |  2.85 | 0.004 \n\n\n\n\nWie immer gibt es reichtlich Tipps & Tricks welches Modell du nun nehmen solltest. How to deal with overdispersion in Poisson regression: quasi-likelihood, negative binomial GLM, or subject-level random effect? und das Tutorial Modeling Count Data. Auch ich mus immer wieder schauen, was am besten konkret in der Anwendung passen könnte und würde.\nWelches Modell nun das beste Modell ist, ist schwer zu sagen. Wenn du Overdisperion vorliegen hast, dann ist natürlich nur das Quasi-Poissonmodell oder das negative Binomialmodell möglich. Welche der beiden dann das bessere ist, hängt wieder von der Fragestellung ab. Allgemein gesprochen ist das Quasi-Poissonmodell besser wenn dich die Zusammenhänge von \\(y\\) zu \\(x\\) am meisten interessieren. Und das ist in unserem Fall hier die Sachlage. Daher gehen wir mit den Quasi-Poissonmdell dann weiter."
  },
  {
    "objectID": "stat-modeling-poisson.html#performance-des-modells",
    "href": "stat-modeling-poisson.html#performance-des-modells",
    "title": "41  Poisson Regression",
    "section": "\n41.5 Performance des Modells",
    "text": "41.5 Performance des Modells\nIn diesem kurzen Abschnitt wollen wir uns einmal anschauen, ob das Modell neben der Overdispersion auch sonst aus statistischer Sicht in Ordnung ist. Wir wollen ja mit dem Modell aus dem Fit quasipoisson_fit weitermachen. Also schauen wir uns einmal das pseudo-\\(R^2\\) für die Poissonregression an. Da wir es mit einem GLM zu tun haben, ist das \\(R^2\\) mit vorsicht zu genießen. In einer Gaussianregression können wir das \\(R^2\\) als Anteil der erklärten Varianz durch das Modell interpretieren. Im Falle von GLM’s müssen wir hier vorsichtiger sein. In GLM’s gibt es ja keine Varianz sondern eine Deviance.\n\nr2_efron(quasipoisson_fit)\n\n[1] 0.3257711\n\n\nMit einem pseudo-\\(R^2\\) von \\(0.33\\) erklären wir ca. 33% der Varianz in der Anzahl der Hechte. Das ist zwar keine super gute Zahl, aber dafür, dass wir nur eine handvoll von Parametern erfasst haben, ist es dann auch wieder nicht so schlecht. Die Anzahl an Hechten wird sicherlich an ganz vielen Parametern hängen, wir konnten immerhin einige wichtige Stellschrauben vermutlich finden.\nIn Abbildung 41.2 schauen wir uns nochmal die Daten in den Modelgüteplots an. Wir sehen vorallem, dass wir vielelicht doch einen Ausreißer mit der Beobachtung 17 vorliegen haben. Auch ist der Fit nicht so super, wie wir an dem QQ-Plot sehen. Die Beobachtungen fallen in dem QQ-Plot nicht alle auf eine Linie. Auch sehen wir dieses Muster in dem Residualplot. Hiererwarten wir eine gerade blaue Linie und auch hier haben wir eventuell Ausreißer mit in den Daten.\n\ncheck_model(quasipoisson_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\", \"pp_check\", \"homogeneity\")) \n\n\n\nAbbildung 41.2— Ausgabe ausgewählter Modelgüteplots der Funktion check_model()."
  },
  {
    "objectID": "stat-modeling-poisson.html#interpretation-des-modells",
    "href": "stat-modeling-poisson.html#interpretation-des-modells",
    "title": "41  Poisson Regression",
    "section": "\n41.6 Interpretation des Modells",
    "text": "41.6 Interpretation des Modells\nUm die Effektschätzer einer Poissonregression oder aber einer Quasipoisson-Regression interpretieren zu können müssen wir uns einmal einen Beispieldatensatz mit bekannten Effekten zwischen den Gruppen bauen. Im Folgenden bauen wir uns einen Datensatz mit zwei Gruppen. Einmal einer Kontrollgruppe mit einer mittleren Anzahl an \\(15\\) und einer Behandlungsgruppe mit einer um \\(\\beta_1 = 10\\) höheren Anzahl. Wir haben also in der Kontrolle im Mittel eine Anzahl von \\(15\\) und in der Behandlungsgruppe eine mittlere Anzahl von \\(25\\).\n\nsample_size <- 100\nlongnose_small_tbl <- tibble(grp = rep(c(0, 1), each = sample_size),\n                             count = 15 + 10 * grp + rnorm(2 * sample_size, 0, 1)) %>%\n  mutate(count = round(count),\n         grp = factor(grp, labels = c(\"ctrl\", \"trt\")))\n\nIn Tabelle 41.2 sehen wir nochmal die Daten als Ausschnitt dargestellt.\n\n\n\n\nTabelle 41.2— How much is the fish? Der Datensatz über \\(n = 1000\\) Beobachtungen an dem wir überlegen wollen wie wir die Effektschätzer einer Poissonregression zu interpretieren haben.\n\ngrp\ncount\n\n\n\nctrl\n15\n\n\nctrl\n14\n\n\nctrl\n15\n\n\nctrl\n13\n\n\n…\n…\n\n\ntrt\n25\n\n\ntrt\n26\n\n\ntrt\n26\n\n\ntrt\n25\n\n\n\n\n\n\nDa sich die Tabelle schlecht liest hier nochmal der Boxplot in Abbildung 41.3. Wir sehen den Grupenunterschied von \\(10\\) sowie die unterschiedlichen mittleren Anzahlen für die Kontrolle und die Behandlung.\n\nggplot(longnose_small_tbl, aes(x = grp, y = count, fill = grp)) +\n  theme_bw() +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\nggplot(data = longnose_small_tbl, aes(x = count, fill = grp)) +\n  theme_bw() +\n  geom_density(alpha = 0.75) +\n  labs(x = \"\", y = \"\", fill = \"Gruppe\") +\n  scale_fill_okabeito() +\n  scale_x_continuous(breaks = seq(10, 30, by = 5), limits = c(10, 30)) \n\n\n\n\n\n(a) Verteilung der Werte als Boxplot.\n\n\n\n\n\n\n\n\n(b) Verteilung der Werte als Densityplot.\n\n\n\n\nAbbildung 41.3— How much is the fish? Der Boxplot über \\(n = 1000\\) Beobachtungen an dem wir überlegen wollen wie wir die Effektschätzer einer Poissonregression zu interpretieren haben.\n\n\n\nJetzt fitten wir einmal das simple Poissonmodell mit der Anzahl als Outcome und der Gruppe mit den zwei Leveln als \\(x\\). Wir pipen dann das Ergebnis des Fittes gleich in die Funktion model_parameters() weiter um die Ergebnisse des Modellierens zu erhalten.\n\nglm(count ~ grp, data = longnose_small_tbl, family = poisson) %>%\n  model_parameters(exponentiate = TRUE)\n\nParameter   |   IRR |   SE |         95% CI |      z |      p\n-------------------------------------------------------------\n(Intercept) | 14.95 | 0.39 | [14.20, 15.72] | 104.58 | < .001\ngrp [trt]   |  1.69 | 0.06 | [ 1.58,  1.80] |  16.06 | < .001\n\n\nAls erstes fällt auf, dass wir die Ausgabe des Modells exponieren müssen. Um einen linearen Zusamenhang hinzukriegen bedient sich die Poissonregression den Trick, das der Zusammenhang zwischen dem \\(y\\) und dem \\(x\\) transformiert wird. Wir rechnen unsere Regression nicht auf den echten Daten sondern auf dem \\(\\log\\)-scale. Daher müssen wir die Koeffizienten der Poissonregression wieder zurücktransfomieren, wenn wir die Koeffizienten interpretieren wollen. Das können wir mit der Option exponentiate = TRUE durchführen.\nGut soweit, aber was heißen den jetzt die Zahlen? Wir haben einen Intercept von \\(14.99\\) das entspricht der mittleren Anzahl in der Kontrollgruppe. Und was sagt jetzt die \\(1.67\\) vom Level trt des Faktors grp? Wenn wir \\(14.99 \\cdot 1.67\\) rechnen, dann erhalten wir als Ergebnis \\(25.03\\), also die mittlere Anzahl in der Behandlungsgruppe. Was sagt uns das jetzt aus? Wir erhalten aus der Poissonregression eine Wahrscheinlichkeit oder aber ein Risk Ratio. Wir können sagen, dass die Anzahl in der Behandlungsgruppe \\(1.67\\)-mal so groß ist wie in der Kontrollgruppe.\nSchauen wir uns nochmal das volle Modell an und interpretieren die Effekte der einzelnen Variablen.\n\nquasipoisson_fit %>% \n  model_parameters(exponentiate = TRUE) \n\nParameter   |  IRR |       SE |       95% CI | t(61) |      p\n-------------------------------------------------------------\n(Intercept) | 0.21 |     0.32 | [0.01, 4.11] | -1.02 | 0.306 \narea        | 1.00 | 1.13e-05 | [1.00, 1.00] |  3.41 | < .001\ndo2         | 1.25 |     0.14 | [1.00, 1.57] |  1.96 | 0.050 \nmaxdepth    | 1.01 | 3.67e-03 | [1.00, 1.02] |  3.18 | 0.001 \nno3         | 1.20 |     0.07 | [1.07, 1.34] |  3.13 | 0.002 \nso4         | 0.99 |     0.02 | [0.95, 1.03] | -0.35 | 0.729 \ntemp        | 1.08 |     0.04 | [1.01, 1.16] |  2.22 | 0.027 \n\n\nSo schön auch die Funktion model_parameters() ist, so haben wir aber hier das Problem, dass wir den Effekt von area nicht mehr richtig sehen. Wir kriegen hier eine zu starke Rundung auf zwei Nachkommastellen. Wir nutzen jetzt mal die Funktion tidy() um hier Abhilfe zu leisten. Ich muss hier noch die Spalte estimate mit num(..., digits = 5) anpassen, damit du in der Ausgabe auf der Webseite auch die Nachkommastellen siehst.\n\nquasipoisson_fit %>% \n  tidy(exponentiate = TRUE, digits = 5) %>% \n  select(term, estimate, p.value) %>% \n  mutate(p.value = pvalue(p.value),\n         estimate = num(estimate, digits = 5))\n\n# A tibble: 7 x 3\n  term         estimate p.value\n  <chr>       <num:.5!> <chr>  \n1 (Intercept)   0.20922 0.310  \n2 area          1.00004 0.001  \n3 do2           1.25342 0.055  \n4 maxdepth      1.01162 0.002  \n5 no3           1.19879 0.003  \n6 so4           0.99321 0.730  \n7 temp          1.08171 0.030  \n\n\nSchauen wir uns die Effekte der Poissonregression einmal an und versuchen die Ergebnisse zu interpretieren. Dabei ist wichtig sich zu erinnern, dass kein Effekt eine 1 bedeutet. Wir schauen hier auf einen Faktor. Wenn wir eine Anzahl mal Faktor 1 nehmen, dann ändert sich nichts an der Anzahl.\n\n\n(Intercept) beschreibt den Intercept der Poissonregression. Wenn wir mehr als eine simple Regression vorliegen haben, wie in diesem Fall, dann ist der Intercept schwer zu interpretieren. Wir konzentrieren uns auf die Effekte der anderen Variablen.\n\narea, beschreibt den Effekt der Fläche. Steigt die Fläche um ein Quadratmeter an, so erhöht sich die Anzahl an Fischen um den \\(1.00001\\). Daher würde man hier eher sagen, erhöht sich die Fläche um jeweils 1000qm so erhöht sich die Anzahl an Fischen um den Faktor \\(1.1\\). Dann haben wir auch einen besser zu interpretierenden Effektschätzer. Die Signifikanz bleibt hier davon unbetroffen.\n\ndo2, beschreibt den Partzialdruck des Sauerstoffs. Steigt dieser um eine Einheit an, so sehen wie eine Erhöhung der Anzahl an Fischen um den Faktor \\(1.25\\). Der Effekt ist gerade nicht signifikant.\n\nmaxdepth, beschreibt die maximale Tiefe. Je tiefer ein Fluß, desto mehr Hechte werden wir beobachten. Der Effekt von \\(1.01\\) pro Meter Tiefe ist signifikant.\n\nno3, beschreibt den Anteil an Nitrat in den Flüssen. Je mehr Nitrat desto signifiant mehr Hechte werden wir beobachten. Hier steigt der Faktor auch um \\(1.20\\).\n\nso4, beschreibt den Schwefelgehalt und mit steigenden Schwefelgehalt nimmt die Anzahl an Fischen leicht ab. Der Effekt ist aber überhauot nicht signifikant.\n\ntemp, beschreibt die Temperatur der Flüsse. Mit steigender Tempertaur erwarten wir mehr Hechte zu beobachten. Der Effekt von \\(1.08\\) Fischen pro Grad Erhöhung ist signifikant.\n\nWas nehmen wir aus der Poissonregression zu den langnasigen Hechten mit? Zum einen haben die Fläche, die Tiefe und der Nitratgehalt einen signifikanten Einfluss auf die Anzahl an Hechten. Auch führt eine höhere Temperatur zu mehr gefundenen Hechten. Die erhöhte Temperatur steht etwas im Widerspuch zu dem Sauerstoffpartizaldruck. Denn je höher die Temperatur desto weniger Sauerstoff wird in dem Wasser gelöst sein. Auch scheint die Oberfläche mit der Tiefe korreliert. Allgemein scheinen Hechte große Flüße zu mögen. Hier bietet sich also noch eine Variablenselektion oder eine Untersuchung auf Ausreißer an um solche Effekte nochmal gesondert zu betrachten."
  },
  {
    "objectID": "stat-modeling-poisson.html#zeroinflation",
    "href": "stat-modeling-poisson.html#zeroinflation",
    "title": "41  Poisson Regression",
    "section": "\n41.7 Zeroinflation",
    "text": "41.7 Zeroinflation\nSo eine Poissonregression hat schon einiges an Eigenheiten. Neben dem Problem der Overdispersion gibt es aber noch eine weitere Sache, die wir beachten müssen. Wir können bei einer Poissonregression auch eine Zeroinflation vorliegen haben. Das heißt, wir beobachten viel mehr Nullen in den Daen, als wir aus der Poissonverteilung erwarten würden. Es gibt also einen biologischen oder künstlichn Prozess, der uns Nullen produziert. Häufig wissen wir nicht, ob wir den Prozess, der uns die Nullen in den Daten produziert, auch abbilden. Das heißt, es kann sein, dass wir einfach nichts Zählen, weil dort nichts ist oder aber es gibt dafür einen Grund. Diesen Grund müssten wir dann irgendwie in unseren Daten erfasst haben, aber meistens haben wir das nicht.\nSchauen wir usn dafür einmal ein Datenbeispiel von Eidechsen in der Lüneburgerheide an. Wir haben Eidechsen lizard in zwei verschiedenen Habitaten grp gezählt. Einmal, ob die Eidechsen eher im offenen Gelände oder eher im bedeckten Gelände zu finden waren. Im Weiteren haben wir geschaut, ob der Boden keinen Regen erhalten hatte, trocken war oder gar feucht. Mit trocken ist hier eine gewisse Restfeuchte gemeint. Am Ende haben wir noch bestimmt, ob wir eher nah an einer Siedlung waren oder eher weiter entfernt. Du kannst dir den Daten satz in der Datei lizards.csv nochmal anschauen. In Tabelle 41.3 sind die Daten nochmal dargestellt.\n\n\n\n\nTabelle 41.3— Ausschnitt aus den Eidechsendaten für die zwei Habitate unter verschiedenen Feuchtigkeitsbedingungen und Nähe zur nächsten Siedlung.\n\ngrp\nrain\npop\nlizard\n\n\n\nopen\nno\nnear\n0\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n0\n\n\nopen\nno\nfar\n2\n\n\nopen\nno\nfar\n4\n\n\n\n\n\n\nIn Abbildung 41.4 sehen wir die Zähldaten der Eidechsen nochmal als Histogramm dargestellt. Wenn wir an einem Punkt keine Eidechsen gefunden haben, dann haben wir keine fehlenden Werte eingetragen, sondern eben, dass wir keine Eidechsen gezählt haben. Wir sehen das wir sehr viele Nullen in unseren Daten haben. Ein Indiz für eine Inflation an Nullen oder eben einer Zeroinflation.\n\nggplot(lizard_zero_tbl, aes(lizard)) +\n  theme_bw() +\n  geom_histogram() +\n  labs(x = \"Anzahl der gefundenen Eidechsen\", y = \"Anzahl\") +\n  scale_x_continuous(breaks = 0:7)\n\n\n\nAbbildung 41.4— Histogramm der Verteilung der Hechte in den beobachteten Flüssen.\n\n\n\n\nUm zu überprüfen, ob wir eine Zeroinflation in den Daten vorliegen haben, werden wir erstmal eine ganz normale Poissonregression auf den Daten rechnen. Wir ignorieren auch eine potenzielle Overdispersion. Das schauen wir uns dann in den Daten später nochmal an.\n\nlizard_fit <- glm(lizard ~ grp + rain + pop, data = lizard_zero_tbl,\n                  family = poisson)\n\nWie immer nutzen wir die Funktion model_parameters() um uns die exponierten Koeffizienten aus dem Modell wiedergeben zu lassen. Das Modell dient uns jetzt nur als Ausgangsmodell und wir werden das Poissonmodell jetzt nicht weiter tiefer verwenden.\n\nlizard_fit %>% model_parameters(exponentiate = TRUE)\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.29 | [0.60, 1.77] |  0.20 | 0.840 \ngrp [cover] | 1.88 | 0.46 | [1.18, 3.07] |  2.61 | 0.009 \nrain [dry]  | 0.31 | 0.09 | [0.17, 0.53] | -4.12 | < .001\nrain [wet]  | 0.13 | 0.05 | [0.06, 0.28] | -4.98 | < .001\npop [far]   | 2.41 | 0.61 | [1.49, 4.04] |  3.47 | < .001\n\n\nWir sehen, dass wir in der Variable rain eine starke Reduzierung der Anzahl an Eidechsen sehen. Vielleicht ist dies eine Variable, die zu viele Nullen produziert. Auch hat die Variable pop, die für die Nähe an einer Siedlung kodiert, einen starken positiven Effekt auf unsere Anzahl an Eidechsen. Hier wollen wir also einmal auf eine Zeroinflation überprüfen. Wir nutzen dazu die Funktion check_zeroinflation() aus dem R Paket performance. Die Funktion läuft nur auf einem Modellfit.\n\ncheck_zeroinflation(lizard_fit)\n\n# Check for zero-inflation\n\n   Observed zeros: 31\n  Predicted zeros: 27\n            Ratio: 0.87\n\n\nDie Funktion gibt uns wieder, dass wir vermutlich eine Zeroinflation vorliegen haben. Das können wir aber Modellieren. Um eine Zeroinflation ohne Overdispersion zu modellieren nutzen wir die Funktion zeroinfl() aus dem R Paket pscl. Der erste Teil der Funktion ist leicht erkläret. Wir bauen uns wieder unswer Model zusammen, was wir fitten wollen. Dann kommt aber ein | und mit diesem Symbol | definieren wir, ob wir wissen, woher die Nullen kommen oder aber ob wir die Nullen mit einem zufälligen Prozess modellieren wollen.\nWenn wir das Modell in der Form y ~ f1 + f2 | 1 schreiben, dann nehmen wir an, dass das Übermaß an Nullen in unseren Daten rein zufällig entstanden sind. Wir haben keine Spalte in de Daten, die uns eine Erklärung für die zusätzlichen Nullen liefern würde.\nWir können auch y ~ f1 + f2 | x3 schreiben. Dann haben wir eine Variable x3 in den Daten von der wir glauben ein Großteil der Nullen herrührt. Wir könnten also in unseren Daten annehmen, dass wir den Überschuss an Nullen durch den Regen erhalten haben und damit über die Spalte rain den Exzess an Nullen modellieren.\nMan sollte immer mit dem einfachsten Modell anfangen, deshalb werden wir jetzt einmal ein Modell fitten, dass annimmt, dass die Nullen durch einen uns unbekannten Zufallsprozess entstanden sind.\n\nlizard_zero_infl_intercept_fit <- zeroinfl(lizard ~ grp + pop + rain | 1, \n                                           data = lizard_zero_tbl) \n\nWir schauen uns das Modell dann wieder einmal an und sehen eine Zweiteilung der Ausgabe. In dem oberen Teil der Ausgabe wird unsere Anzahl an Eidechsen modelliert. In dem unteren Teil wird der Anteil der Nullen in den Daten modelliert. Daher können wir über Variablen in dem Zero-Inflation Block keine Aussagen über die Anzahl an Eidechsen treffen. Variablen tauchen nämlich nur in einem der beiden Blöcke auf.\n\nlizard_zero_infl_intercept_fit %>% \n  model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.31 | [0.60, 1.87] |  0.22 | 0.830 \ngrp [cover] | 2.03 | 0.51 | [1.25, 3.31] |  2.84 | 0.005 \npop [far]   | 2.59 | 0.67 | [1.56, 4.31] |  3.67 | < .001\nrain [dry]  | 0.31 | 0.10 | [0.17, 0.56] | -3.82 | < .001\nrain [wet]  | 0.14 | 0.06 | [0.06, 0.31] | -4.73 | < .001\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |   SE |       95% CI |     z |     p\n--------------------------------------------------------------\n(Intercept) |       0.11 | 0.11 | [0.02, 0.74] | -2.26 | 0.024\n\n\nAls erstes beobachten wir einen größeren Effekt der Variable grp. Das ist schon mal ein spannender Effekt. An der Signifikanz hat scih nicht viel geändert. Wir werden am Ende des Kapitels einmal alle Modell für die Modellierung der Zeroinflation vergleichen.\nNun könnte es auch sein, dass der Effekt der vielen Nullen in unserer Variable rain verborgen liegt. Wenn es also regnet, dann werden wir viel weniger Eidechsen beoabchten. Nehmen wir also rain als ursächliche Variable mit in das Modell für die Zeroinflation.\n\nlizard_zero_infl_rain_fit <- zeroinfl(lizard ~ grp + pop | rain, \n                                      data = lizard_zero_tbl)\n\nWieder schauen wir uns einmal die Ausgabe des Modells einmal genauer an.\n\nlizard_zero_infl_rain_fit %>% model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |    z |     p\n-------------------------------------------------------\n(Intercept) | 1.13 | 0.34 | [0.63, 2.03] | 0.42 | 0.677\ngrp [cover] | 1.60 | 0.42 | [0.95, 2.67] | 1.77 | 0.077\npop [far]   | 1.84 | 0.51 | [1.07, 3.18] | 2.20 | 0.028\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |     SE |          95% CI |     z |     p\n-------------------------------------------------------------------\n(Intercept) |       0.04 |   0.08 | [0.00,    2.09] | -1.60 | 0.109\nrain [dry]  |      27.94 |  59.06 | [0.44, 1760.08] |  1.58 | 0.115\nrain [wet]  |      83.71 | 178.43 | [1.28, 5459.08] |  2.08 | 0.038\n\n\nEs ändert sich einiges. Zum einen erfahren wir, dass der Regen anscheined doch viele Nullen in den Daten produziert. Wir haben ein extrem hohes \\(OR\\) für die Variable rain. Die Signifikanz ist jedoch eher gering. Wir haben nämlich auch eine sehr hohe Streuung mit den großen \\(OR\\) vorliegen. Au der anderen Seite verlieren wir jetzt auch die Signifikanz von unseren Habitaten und dem Standort der Population. Nur so mäßig super dieses Modell.\nWir können jetzt natürlich auch noch den Standort der Population mit in den Prozess für die Entstehung der Nullen hineinnehmen. Wir schauen uns dieses Modell aber nicht mehr im Detail an, sondern dann nur im Vergleich zu den anderen Modellen.\n\nlizard_zero_infl_rain_pop_fit <- zeroinfl(lizard ~ grp | rain + pop, \n                                          data = lizard_zero_tbl)\n\nDie Gefahr besteht immer, das man sich an die Wand modelliert und vor lauter Modellen die Übersicht verliert. Neben der Zeroinflation müssen wir ja auch schauen, ob wir eventuell eine Overdispersion in den Daten vorliegen haben. Wenn das der Fall ist, dann müsen wir nochmal überlegen, was wir dann machen. Wir testen nun auf Ovrdisprsion in unserem ursprünglichen Poissonmodell mit der Funktion check_overdispersion().\n\ncheck_overdispersion(lizard_fit)\n\n# Overdispersion test\n\n       dispersion ratio =  1.359\n  Pearson's Chi-Squared = 74.743\n                p-value =  0.039\n\n\nTja, und so erfahren wir, dass wir auch noch Overdispersion in unseren Daten vorliegen haben. Wir müsen also beides Modellieren. Einmal modellieren wir die Zeroinflation und einmal die Overdispersion. Wir können beides in einem negativen binominalen Modell fitten. Auch hier hilft die Funktion zeroinfl() mit der Option dist = negbin. Mit der Option geben wir an, dass wir eine negative binominal Verteilungsfamilie wählen. Damit können wir dann auch die Ovrdispersion in unseren Daten modellieren.\n\nlizard_zero_nb_intercept_fit <- zeroinfl(lizard ~ grp + rain + pop | 1, \n                                         dist = \"negbin\", data = lizard_zero_tbl)\n\nDann schauen wir usn einmal das Modell an. Zum einen sehen wir, dass der Effekt ähnlich groß ist, wie bei dem Intercept Modell der Funktion zeroinfl. Auch bleiben die Signifikanzen ähnlich.\n\nlizard_zero_nb_intercept_fit %>% model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.31 | [0.60, 1.87] |  0.22 | 0.830 \ngrp [cover] | 2.03 | 0.51 | [1.25, 3.31] |  2.84 | 0.005 \nrain [dry]  | 0.31 | 0.10 | [0.17, 0.56] | -3.82 | < .001\nrain [wet]  | 0.14 | 0.06 | [0.06, 0.31] | -4.73 | < .001\npop [far]   | 2.59 | 0.67 | [1.56, 4.31] |  3.67 | < .001\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |   SE |       95% CI |     z |     p\n--------------------------------------------------------------\n(Intercept) |       0.11 | 0.11 | [0.02, 0.74] | -2.26 | 0.024\n\n\nNun haben wir vier Modelle geschätzt und wolen jetzt wissen, was ist das beste Modell. Dafür hilft usn dann eine Gegenüberstellung der Modelle mit der Funktion modelsummary(). Wir könnten die Modelle auch gegeneinander statistsich Testen, aber hier behalten wir uns einmal den beschreibenden Vergleich vor. In Tabelle 41.4 sehen wir einmal die vier Modelle nebeneinander gestellt. Für eine bessere Übrsicht, habe ich aus allen Modellen den Intercept entfernt.\n\nmodelsummary(lst(\"ZeroInfl Intercept\" = lizard_zero_infl_intercept_fit,\n                 \"ZeroInfl rain\" = lizard_zero_infl_rain_fit,\n                 \"ZeroInfl rain+pop\" = lizard_zero_infl_rain_pop_fit,\n                 \"NegBinom intercept\" = lizard_zero_nb_intercept_fit),\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"),\n             coef_omit = \"Intercept\", \n             exponentiate = TRUE)\n\n\n\n\nTabelle 41.4—  Modellvergleich mit den vier Modellen. Wir schauen in wie weit sich die Koeffizienten und Modelgüten für die einzelnen Modelle im direkten Vergleich zum vollen Modell verändert haben. \n \n   \n    ZeroInfl Intercept \n    ZeroInfl rain \n    ZeroInfl rain+pop \n    NegBinom intercept \n  \n\n\n count_grpcover \n    2.031 \n    1.595 \n    1.611 \n    2.031 \n  \n\n  \n    [1.245, 3.313] \n    [0.951, 2.675] \n    [0.912, 2.845] \n    [1.245, 3.313] \n  \n\n  \n    s.e. = 0.507 \n    s.e. = 0.421 \n    s.e. = 0.468 \n    s.e. = 0.507 \n  \n\n  \n    t = 2.839 \n    t = 1.771 \n    t = 1.642 \n    t = 2.839 \n  \n\n  \n    p = 0.005 \n    p = 0.077 \n    p = 0.101 \n    p = 0.005 \n  \n\n count_popfar \n    2.591 \n    1.844 \n     \n    2.591 \n  \n\n  \n    [1.558, 4.310] \n    [1.069, 3.183] \n     \n    [1.558, 4.310] \n  \n\n  \n    s.e. = 0.673 \n    s.e. = 0.513 \n     \n    s.e. = 0.673 \n  \n\n  \n    t = 3.667 \n    t = 2.199 \n     \n    t = 3.667 \n  \n\n  \n    p = <0.001 \n    p = 0.028 \n     \n    p = <0.001 \n  \n\n count_raindry \n    0.308 \n     \n     \n    0.308 \n  \n\n  \n    [0.168, 0.564] \n     \n     \n    [0.168, 0.564] \n  \n\n  \n    s.e. = 0.095 \n     \n     \n    s.e. = 0.095 \n  \n\n  \n    t = -3.816 \n     \n     \n    t = -3.816 \n  \n\n  \n    p = <0.001 \n     \n     \n    p = <0.001 \n  \n\n count_rainwet \n    0.135 \n     \n     \n    0.135 \n  \n\n  \n    [0.059, 0.310] \n     \n     \n    [0.059, 0.310] \n  \n\n  \n    s.e. = 0.057 \n     \n     \n    s.e. = 0.057 \n  \n\n  \n    t = -4.726 \n     \n     \n    t = -4.726 \n  \n\n  \n    p = <0.001 \n     \n     \n    p = <0.001 \n  \n\n zero_raindry \n     \n    27.939 \n    98.196 \n     \n  \n\n  \n     \n    [0.443, 1760.078] \n    [0.0001, 76101776.131] \n     \n  \n\n  \n     \n    s.e. = 59.059 \n    s.e. = 679.400 \n     \n  \n\n  \n     \n    t = 1.575 \n    t = 0.663 \n     \n  \n\n  \n     \n    p = 0.115 \n    p = 0.507 \n     \n  \n\n zero_rainwet \n     \n    83.713 \n    402.409 \n     \n  \n\n  \n     \n    [1.284, 5459.081] \n    [0.0004, 418712524.080] \n     \n  \n\n  \n     \n    s.e. = 178.434 \n    s.e. = 2844.677 \n     \n  \n\n  \n     \n    t = 2.077 \n    t = 0.848 \n     \n  \n\n  \n     \n    p = 0.038 \n    p = 0.396 \n     \n  \n\n zero_popfar \n     \n     \n    0.148 \n     \n  \n\n  \n     \n     \n    [0.022, 1.000] \n     \n  \n\n  \n     \n     \n    s.e. = 0.144 \n     \n  \n\n  \n     \n     \n    t = -1.960 \n     \n  \n\n  \n     \n     \n    p = 0.050 \n     \n  \n\n Num.Obs. \n    60 \n    60 \n    60 \n    60 \n  \n\n R2 \n    0.620 \n    0.477 \n    0.454 \n    0.620 \n  \n\n R2 Adj. \n    0.585 \n    0.449 \n    0.435 \n    0.585 \n  \n\n AIC \n    157.3 \n    167.2 \n    167.4 \n    159.3 \n  \n\n BIC \n    169.8 \n    179.8 \n    180.0 \n    173.9 \n  \n\n RMSE \n    1.27 \n    1.27 \n    1.32 \n    1.27 \n  \n\n\n\n\n\nDie beiden Intercept Modelle haben die kleinsten \\(AIC\\)-Werte der vier Modelle. Darüber hinaus haben dann beide Modelle auch die höchsten \\(R^2_{adj}\\) Werte. Beide Modelle erklären also im Verhältnis viel Varianz mit 58.5%. Auch ist der \\(RMSE\\) Wert als Fehler bei beiden Modellen am kleinsten. Damit haben wir die Qual der Wahl, welches Modell wir nehmen. Ich würde das negative binominal Modell nehmen. Wir haben ins unseren Daten vermutlich eine Zeroinflation sowie eine Overdispersion vorliegen. Daher bietest es sich an, beides in einer negativen binominalen Regression zu berücksichtigen. Zwar sind die beiden Intercept Modelle in diesem Beispielfall von den Koeffizienten fast numerisch gleich, aber das hat eher mit dem reduzierten Beispiel zu tun, als mit dem eigentlichen Modell. In unserem Fall ist die Overdispersion nicht so extrem.\nWie sehe den unser negative binominal Modell aus, wenn wir mit dem Modell einmal die zu erwartenden Eidechsen vorhersagen würden? Auch das kann helfen um abzuschätzen, ob das Modelle einigermaßen funktioniert hat. Wir haben ja hier den Vorteil, dass wir nur mit kategorialen Daten arbeiten. Wir haben keine kontiniuerlichen Variablen vorliegen und darüber hinaus auch nicht so viele Variablen insgesamt.\nDaher bauen wir uns mit expand_grid() erstmal einen Datensatz, der nur aus den Faktorkombinationen besteht. Wir haben also nur eine Beobachtung je Faktorkombination. Danach nutzen wir die Daten einmal in der Funktion predict() um uns die vorhergesagten Eidechsen nach dem gefitten Modell wiedergeben zu lassen.\n\nnewdata_tbl <- expand_grid(grp = factor(1:2, labels = c(\"open\", \"cover\")),\n                           rain = factor(1:3, labels = c(\"no\", \"dry\", \"wet\")),\n                           pop = factor(1:2, labels = c(\"near\", \"far\")))\n\npred_lizards <- predict(lizard_zero_nb_intercept_fit, newdata = newdata_tbl) \n  \nnewdata_tbl <- newdata_tbl %>% \n  mutate(lizard = pred_lizards)\n\nNachdem wir in dem Datensatz newdata_tbl nun die vorhergesagten Eidechsen haben, können wir uns jetzt in der Abbildung 41.5 die Zusammenhänge nochmal anschauen.\n\nggplot(newdata_tbl, aes(x = rain, y = lizard, colour = grp, group = grp)) +\n  theme_bw() +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ pop) +\n  labs(x = \"Feuchtigkeit nach Regen\", y = \"Anzahl der gezählten Eidechsen\"\",\n       color = \"Gruppe\") +\n  scale_color_okabeito()\n\n\n\nAbbildung 41.5— Scatterplot der vorhergesagten Eidechsen in den Habitaten (grp), der Feuchtigkeit des Bodens nach Regen und dem Abstand zur nächsten Ortschaft.\n\n\n\n\nWir erkennen, dass mit der Erhöhung der Feuchtigkeit die Anzahl an aufgefundenen Eidechsen sinkt. Der Effekt ist nicht mehr so stark, wenn es schon einmal geregnet hat. Ebenso macht es einen Unterschied, ob wir nahe einer Siedlung sind oder nicht. Grundsätzlich finden wir immer mehr Eidechsen in geschützten Habitaten als in offenen Habitaten."
  },
  {
    "objectID": "stat-modeling-poisson.html#referenzen",
    "href": "stat-modeling-poisson.html#referenzen",
    "title": "41  Poisson Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "eda-distribution.html",
    "href": "eda-distribution.html",
    "title": "\n18  Verteilung von Daten\n",
    "section": "",
    "text": "Version vom November 16, 2022 um 13:12:05\nIn diesem Kapitel wollen wir uns mit Verteilungen beschäftigen. Dormann (2013) liefert eine weitreichende Übersicht über verschiedene Verteilungen. Natürlich ist diese Übersicht auch nicht abschließend. Es gibt eine sehr große Anzahl an Verteilungen, aber wir werden uns nur mit einer kleinen Auswahl beschäftigen. Die folgenden Verteilungen haben eine praktische Verwendung in der Data Science. Wir wollen uns in diesem Kapitel mit folgenden Verteilungen beginnen.\nWir wollen uns jetzt die verschiedenen Verteilungen einmal in der Anwendung anschauen. Dabei lassen wir viel Mathematik recht und links liegen. Du kannst bei Dormann (2013) mehr zu dem Thema statistische Verteilungen anlesen. Dort gibt es auch nochmal mehr Informationen zu den einzelnen Eigenschaften, die eine Verteilung noch so haben kann. Wir konzentrieren uns hier auf die Lageparameter und die Streuung der Verteilungen.\nIn diesem Kapitel geht es erstmal um das Grundverständnis, das Daten einer Verteilung folgen. Oder noch konkreter, dass unser Outcome \\(y\\) einer Verteilung folgt. Wir müssen später unseren Alogrithmen sagen, welcher Verteilung \\(y\\) entspringt, sonst können wir keine korrekte Analyse unser Daten rechnen."
  },
  {
    "objectID": "eda-distribution.html#genutzte-r-pakete-für-das-kapitel",
    "href": "eda-distribution.html#genutzte-r-pakete-für-das-kapitel",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.1 Genutzte R Pakete für das Kapitel",
    "text": "18.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, see, readxl)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "eda-distribution.html#daten-für-verteilungen",
    "href": "eda-distribution.html#daten-für-verteilungen",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.2 Daten für Verteilungen",
    "text": "18.2 Daten für Verteilungen\nDamit wir uns auch eine Verteilung anschauen können bruachen wir viele Beobachtungen. Wir haben das ja schon bei den Histogrammen gesehen, wenn wir ein aussagekräftiges Histogramm erstellen wollen, dann brauchen wir mehr als zwanzig Beobachtungen. Daher nehmen wir für dieses Kapitel einmal den Gummibärchendatensatz und schauen uns dort die Variablen gender, height, count_bears und count_color einmal genauer an. Wie immer nutzen wir die Funktion select() um die Spalten zu selektieren. Abschließend verwandeln wir das Geschlecht gender und das module noch in einen Faktor.\n\ngummi_tbl <- read_excel(\"data/gummibears.xlsx\")  %>%\n  select(year, module, gender, height, count_bears, count_color,\n         most_liked) %>% \n  mutate(gender = as_factor(gender),\n         module = as_factor(module))\n\nWir erhalten das Objekt gummi_tbl mit dem Datensatz in Tabelle 18.1 nochmal dargestellt. Wir brauchen nicht alle Spalten aus dem ursprünglichen Datensatz und somit ist die Tabelle etwas übersichtlicher.\n\n\n\n\nTabelle 18.1— Auszug aus den selektierten Daten zu den Gummibärchendaten.\n\n\n\n\n\n\n\n\n\n\nyear\nmodule\ngender\nheight\ncount_bears\ncount_color\nmost_liked\n\n\n\n2018\nFU Berlin\nm\n193\n9\n3\nlightred\n\n\n2018\nFU Berlin\nw\n159\n10\n5\nyellow\n\n\n2018\nFU Berlin\nw\n159\n9\n6\nwhite\n\n\n2018\nFU Berlin\nw\n180\n10\n5\nwhite\n\n\n2018\nFU Berlin\nm\n180\n10\n6\nwhite\n\n\n2018\nFU Berlin\nm\nNA\n10\n5\nwhite\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n2022\nStatistik\nm\n197\n9\n4\nwhite\n\n\n2022\nStatistik\nm\n180\n10\n3\ngreen\n\n\n2022\nStatistik\nm\n187\n11\n5\ndarkred\n\n\n2022\nStatistik\nm\n186\n10\n5\ngreen\n\n\n2022\nStatistik\nm\nNA\n9\n4\ndarkred\n\n\n2022\nStatistik\nNA\nNA\n10\n6\nNA"
  },
  {
    "objectID": "eda-distribution.html#sec-normal",
    "href": "eda-distribution.html#sec-normal",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.3 Die Normalverteilung",
    "text": "18.3 Die Normalverteilung\nWir sprechen in der Statistik auch von Verteilungsfamilien. Daher schreiben wir in R auch family = gaussian, wenn wir sagen wollen, dass unsere Daten einer Normalverteilung entstammen.\nWenn wir von de Normalverteilung sprechen, dann schreiben wir ein \\(\\mathcal{N}\\) Symbol - also ein großes N mit Serifen. Die Normalverteilung sieht aus wie eine Glocke, deshalb wird die Normalverteilung auch Glockenkurve genannt. Im englischen Sprachgebrauch und auch in R nutzen wir dagegen die Bezeichnung nach dem “Entdecker” der Normalverteilung, Carl Friedrich Gauß (1777 - 1985). Wir nennen daher die Normalverteilung auch Gaussian-Verteilung.\nParameter sind Zahlen, die eine Verteilungskurve beschreiben.\nEine Normalverteilung wird ruch zwei Verteilungsparameter definiert. Eine Verteilung hat Parameter. Parameter sind die Eigenschaften einer Verteilung, die notwendig sind um eine Verteilung vollständig zu beschreiben. Im Falle der Normalverteilung brauchen wir zum einen den Mittelwert \\(\\bar{y}\\), der den höchsten Punkt unserer Glockenkurve beschreibt. Zum anderen brauchen wir auch die Standardabweichung \\(s^2_y\\), die die Ausbreitung oder Breite der Glockenkurve bestimmt. Wir beschreiben eine Normalverteilung für eine Stichprobe mit \\(\\bar{y}\\) und \\(s^2_y\\) wie folgt.\n\\[\n\\mathcal{N}(\\bar{y}, s^2_y)\n\\]\nOder mit mehr Details in folgender Form. Wir können hier Verallgemeinern und schreiben in der Grundgesamtheit mit \\(\\mu = \\bar{y}\\) und \\(\\sigma^2 = s^2_y\\). Das heißt, wenn wir unendlich viele Beobachtungen vorliegen hätten, dann wüssetn wir auch den wahren Mittelwert \\(\\mu\\) und die wahre Varianz \\(\\sigma^2\\) der Daten.\n\\[\nf(y \\mid\\mu,\\sigma^2)=\\cfrac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\cfrac{(y-\\mu)^2}{2\\sigma^2}}\\quad -\\infty<y<\\infty\n\\]\nIm Falle der Normalverteilung brauchen wir einen Paramter für den höchsten Punkt der Kurve, sowie einen Parameter für die Ausbreitung, also wie weit geht die Kurve nach links und nach rechts. Je nach \\(\\bar{y}\\) und \\(s^2_y\\) können wir verschiedenste Normalverteilungen vorliegen haben. Eine Sammlung von Verteilungen nennen wir auch Familie (eng. family).\nWir haben Varianzhomogenität vorliegen, wenn \\(s^2_{1} = s^2_{2} = s^2_{3}\\) sind. Wir haben Varianzheterogenität vorliegen, wenn \\(s^2_{1} \\neq s^2_{2} \\neq s^2_{3}\\) sind.\nIn Abbildung 18.1 sehen wir verschiedene Normalverteilungen mit unterschiedlichen Mittelwerten. In Abbildung 18.1 (a) sehen wir eine Varianzhomogenität vorliegen, da die Varianzen in allen drei Normalverteilungen gleich sind. Wir können auch schreiben, dass \\(s^2_{1} = s^2_{2} = s^2_{3} = 2\\). In Abbildung 18.1 (b) haben wir Varianzheterogenität vorliegen, da die Varianzen der Normalverteilungen ungleich sind. Wir können hier dann schreiben, dass \\(s^2_{1} = 6 \\neq s^2_{2} = 1 \\neq s^2_{3} = 3\\) sind. Häufig gehen statistische Verfahren davon aus, dass wir Varianzhomogenität über die Gruppen und daher auch die Normalverteilungen vorliegen haben. Konkret, wenn wir die Sprungweiten in[cm] von Hunde- und Katzenflöhen mit einander vergleichen wollen, dann gehen wir erstmal davon aus, dass die Mittelwerte verschieden sind, aber die Varianzen gleich sind.\n\n\n\n\n\n(a) Drei Normalverteilungen mit Varianzhomogenität.\n\n\n\n\n\n\n(b) Drei Normalverteilungen unter Varianzheterogenität.\n\n\n\n\nAbbildung 18.1— Histogramm verschiedener Normalverteilungen mit unterschiedlichen Mittelwerten.\n\n\nIn einer Normalverteilung liegen 68% der Werte innerhalb \\(\\bar{y}\\pm 1 \\cdot s_y\\) und 95% der Werte innerhalb \\(\\bar{y}\\pm 2 \\cdot s_y\\)\nWenn wir eine Normalverteilung vorliegen haben, dann liegen 68% der Werte plus/minus einer Standardabweichung vom Mittelwert. Ebenso liegen 95% der Werte plus/minus zwei Standabweichungen vom Mittelwert. Über 99% der Werte befinden sich innerhalb von drei Standardabweichungen vom Mittelwert. Diese Eigenschaft einer Normalverteilung können wir später noch nutzen um abzuschätzen, ob wir einen relevanten Gruppenunterschied vorliegen haben oder aber ob unsere Daten unnatürlich breit streuen.\nWir nutzen das Wort approximativ wenn wir sagen wollen, dass ein Outcome näherungsweise normalverteilt ist.\nSchauen wir uns die Normalverteilung einmal am Beispiel unserer Gummibärchendaten und der Körpergröße der Studierenden an. Wir färben das Histogramm nach dem Geschlecht ein. In Abbildung 18.2 sehen wir das Ergebnis einmal als Histogramm und einmal als Densityplot dargestellt. Wir können annehmen, dass die Größe approximativ normalverteilt ist.\n\n\n\n\n\n(a) Histogramm.\n\n\n\n\n\n\n(b) Densityplot.\n\n\n\n\nAbbildung 18.2— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt.\n\n\nWir können die Funktion rnorm() nutzen um uns zufällige Zahlen aus der Normalverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert mean = und die gewünschte Standardabweichung mit sd = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rnorm() mit zehn Werten.\n\nrnorm(n = 10, mean = 5, sd = 2) %>% round(2)\n\n [1] 6.97 6.24 5.59 5.27 6.30 4.91 4.13 4.92 3.67 4.42\n\n\nDu kannst ja mal den Mittelwert und die Standardabweichung der zehn Zahlen ausrechnen. Da wir es hier mit einer Stichprobe mit zehn Beobachtungen zu tun haben, wird der Mittelwert \\(\\bar{y}\\) und die Standardabweichung \\(s_y\\) sich von den vorher definierten Mittelwert \\(\\mu_y = 5\\) und Standardabweichung \\(\\sigma_y = 2\\) der Grundgesamtheit unterscheiden.\nWir können auch aus unseren Gummibärchendaten für die Körpergröße in [cm] jeweils den Mittelwert und die Standardabweichung getrennt für die Geschlechter berechnen und dann die theoretische Normalverteilung zeichenen. In Abbildung 18.3 (b) und Abbildung 18.3 (d) sehen wir die Verteilung der theoretischen Werte, wenn wir die Mittelwerte und die Standardabweichung aus den Verteilungen in Abbildung 18.3 (a) schätzen. Spannderweise bildet sich den zufällig gezogenen Daten auch eine leichte Schulter bei der Verteilung der Körpergrößen. Auch \\(n = 399\\) vollständige Beobachtungen bedeuten nicht, dass wir eine perfekte Normalverteilung erhalten.\n\n\n\n\n\n(a) Verteilung der beobachteten Werte.\n\n\n\n\n\n\n(b) Verteilung der theoretischen Werte.\n\n\n\n\n\n\n\n\n(c) Verteilung der beobachteten Werte.\n\n\n\n\n\n\n(d) Verteilung der theoretischen Werte.\n\n\n\n\nAbbildung 18.3— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte. Einmal dargestellt als Histogramm und einmal als Densityplot."
  },
  {
    "objectID": "eda-distribution.html#die-standardnormalverteilung",
    "href": "eda-distribution.html#die-standardnormalverteilung",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.4 Die Standardnormalverteilung",
    "text": "18.4 Die Standardnormalverteilung\nEs gibt viele Normalverteilungen. Eiegntlich gibt es unednlich viele Normalverteilunge, da wir für die Parameter Mittelwert \\(\\bar{y}\\) und die Standardabweichung \\(s_y\\) beliebige Zahlen einsetzen können. Aber es gibt eine besondere Normalverteilung, so dass diese Verteilung einen eigenen Namen hat. Wir sprechen von der Standardnormalverteilung, wenn der Mittelwert gleich Null ist und die Standardabweichung gleich Eins. Du siehst hier nochmal die Standardnormalverteilung ausgeschrieben.\n\\[\n\\mathcal{N}(0, 1)\n\\]\nFolgende Eigenschaften sind der Standardnormalverteilung gegeben. Die Standardnormalverteilung hat eine Fläche von \\(A = 1\\) unter der Kurve. Darüber hinaus liegen 95% der Werte zwischen \\(\\approx -2\\) und \\(\\approx 2\\). Die einzelnen Werte einer Standardnormalverteilung nennen wir \\(z\\)-Werte. Wenn wir eine beliebige Normalverteilung in eine Standardnormalverteilung überführen wollen so machen wir die Umwandlung mit der \\(z\\)-Transformation. Und jetzt fahren wir wieder in die Doppeldeutigkeit in R.\n\n\nAbbildung 18.4— Darstellung von dem Zusammenhang von pnorm(p = 1.96) und qnorm(q = 0.025). Mit der Option lower.tail bestimmen wir auf welche Seite der Verteilung wir sein wollen.\n\n\nIn Abbildung 18.4 haben wir eine Standardnormalverteilung gegeben. Können jetzt verschiedene Werte auf der \\(x\\)-Achse und die Flächen links und rechts von diesen Werten berechnen. Wir nutzen die Funktion pnorm() wenn wir die Fläche rechts oder links von einem Wert \\(q\\) berechnen wollen.\n\npnorm(q = 1.96, mean = 0, sd = 1, lower.tail = FALSE) %>% \n  round(3)\n\n[1] 0.025\n\n\nWir berechen die Fläche links von \\(q\\) und damit auch die Wahrscheinlichkeit \\(Pr(X \\leq q)\\) mit lower.tail = TRUE. Warum ist die Fläche jetzt eine Wahrscheinlichkeit? Wir haben unter der Kurve der Standardnormalverteilung eine Fläche von \\(A = 1\\). Damit ist jede Fläche auch gleich einer Wahrscheinlichkeit. Wenn wir an der Fläche rechts von \\(q\\) interessiert sind und damit auch an der Wahrscheinlichkeit \\(Pr(X > q)\\) nutzen wir die Option lower.tail = FALSE. Das ist erstmal immer etwas verwirrend, aber schau dir den Zusammenhang nochmal in der Abbildung 18.4 an. Wir brauchen diese Idee von der Fläche ist auch gleich Wahrscheinlichkeit im Kapitel 20 zum statistischen Testen.\nWir können die Berechnung von \\(q\\) zu \\(p\\) auch umdrehen. Wir geben eine Fläche vor und wollen wissen wie der Wert auf der x-Achse zu der entsprechenden Fläche ist. In diesem Fall will ich die Werte zu den Flächen von \\(p = 0.025\\) und \\(p = 0.05\\). Da wir lower.tail = FALSE ausgewählt haben, sind wir auf der rechten Seite der Verteilung.\n\nqnorm(p = c(0.025, 0.05), mean = 0, sd = 1, lower.tail = FALSE) %>% \n  round(3)\n\n[1] 1.960 1.645\n\n\nUnd hier einmal als Gegenprobe mit der Option lower.tail = TRUE. Wir springen dann damit auf die linke Seite der Verteilung und wie zu erwarten erhlaten wir dann auch den negativen Wert für die Fläche von \\(p = 0.05\\).\n\nqnorm(p = 0.05, mean = 0, sd = 1, lower.tail = TRUE) %>% \n  round(3)\n\n[1] -1.645\n\n\nDie ganzen Berechnungen funktionieren natürlich auch, wenn wir nicht die Fläche \\(A=1\\) unterhalb der Standardnormalverteilung hätten. Aber wir nutzen hier eben den Zusammenhang von Fläche zu Wahrscheinlichkeit um mit der Verteilung zu rechnen und Wahrscheinlichkeiten abzuschätzen."
  },
  {
    "objectID": "eda-distribution.html#sec-poisson",
    "href": "eda-distribution.html#sec-poisson",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.6 Die Poissonverteilung",
    "text": "18.6 Die Poissonverteilung\nEine weitere wichtige Verteilung ist die Poissonverteilung. Die Poissonverteilung ist eine diskrete Verteilung. Daher kommen nur ganze Zahlen vor. Damit bildet die Poissonverteilung die Zähldaten ab. Wenn wir also etwas Zählen, dann ist diese Variable mit den gezählten Ergebnissen poissonverteilt. Im Folgenden sehen wir die Poissonverteilung einmal dargestellt.\n\\[\n\\mathcal{Pois}(\\lambda)\n\\]\nOder mit mehr Details in folgender Form.\n\\[\nP_\\lambda (k) = \\frac{\\lambda^k}{k!}\\, \\mathrm{e}^{-\\lambda}\n\\]\nDie Poisson-Verteilung gibt dann die Wahrscheinlichkeit einer bestimmten Ereignisanzahl \\(k\\) im Einzelfall an, wenn die mittlere Ereignisrate \\(\\lambda\\) bekannt ist. Im Gegensatz zur Normalverteilung hat die Poissonverteilung nur einen Parameter. Den Lageparameter \\(\\lambda\\) ausgedrückt durch den griechischen Buchstaben Lambda. Eine Poissonverteilung mit \\(\\mathcal{Pois}(4)\\) hat den höchsten Punkt bei vier. Nun hat die Poissonverteilung hat mehrere Besonderheiten. Da die Poissonverteilung keinen Streuungsparameter hat, steigt mit dem \\(\\lambda\\) auch die Streuung. Daher haben Poissonverteilungen mit einem großen \\(\\lambda\\) auch eine große Streuung. ie Ausbreitung der Kurve ist eine Funktion von \\(\\lambda\\) und steigt mit \\(\\lambda\\) an. Du kannst diesen Zusammenhang in Abbildung 18.6 beobachten.\nDarüber hinaus kann eine Poissonverteilung nicht negativ werden. Es kann keine kleinere Zahl als die Null geben. Durch die diskreten Zahlen haben wir auch immer mal Lücken zwischen den Balken der Poissonverteilung. Das passiert besonders, wenn wir eine kleine Anzahl an Beobachtungen haben. Abschließend konvergiert die Poissonverteilung bei großen \\(\\lambda\\) hin zu einer Normalverteilung.\n\n\n\n\nAbbildung 18.6— Histogramm verschiedener Poissonverteilungen.\n\n\n\n\nSchauen wir uns nun einmal die Poissonverteilung im Beispiel an. In Abbildung 18.7 sehen wir die Histogramme der Anzahl an Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte. Da wir es hier mit Zähldaten zu tun haben, könnte es sich um eine Poissonverteilung handeln. Wie müssen uns nun die Frage stellen, ob die Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte wirklich eine zufällige Realistierung sind. Daher eine zufällige Stichprobe der Grundgesamtheit. Wir können diese Annahme überprüfen in dem wir die theoretischen Werte für die beiden Poissonverteilung mit \\(\\mathcal{Pois}(10)\\) und \\(\\mathcal{Pois}(5)\\) genieren.\n\n\n\n\n\n(a) Anzahl an Bärchen\n\n\n\n\n\n\n(b) Anzahl an Farben\n\n\n\n\nAbbildung 18.7— Histogramme der Anzahl an Gummibärchen und die Anzahl an Farben in einer Tüte. Es gibt nicht mehr als sechs Farben.\n\n\nWir können die Funktion rpois() nutzen um uns zufällige Zahlen aus der Poissonverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert lambda = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rpois() mit zehn Werten.\n\nrpois(n = 10, lambda = 5)\n\n [1] 3 5 4 5 3 3 9 3 5 5\n\n\nEs gibt neben der Poissonverteilung auch die negative Binomialverteilung sowie die Quasi-Poissonverteilung, die es erlauben einen Streuungsparameter für die Poissonverteilung zu schätzen.\nWir können nun auch aus unseren Gummibärchendaten für die Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte die theoretische Poissonverteilung berechnen. In Abbildung 18.8 sehen wir die Verteilung der beobachteten Werte für Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte und deren theoretischen Verteilung nach dem geschätzen \\(\\lambda = 10\\) und \\(\\lambda = 5\\). Wir sehen ganz klar, dass die beide Variablen keine Zufallsrealisierung sind. Zum einen haben wir das auch nicht erwartet, es gibt nicht mehr als sechs Farben und zum anderen ist zu vermuten, dass Haribo technisch in den Auswahlprozess eingreift. Wir haben auf jeden Fall eine sehr viel kleinere Streuung als bei einer klassischen Poissonverteilung anzunehmen wäre.\n\n\n\n\n\n(a) Verteilung der beobachteten Anzahl an Bärchen.\n\n\n\n\n\n\n(b) Verteilung der theoretischen Anzahl an Bärchen.\n\n\n\n\n\n\n\n\n(c) Verteilung der beobachteten Anzahl an Farben.\n\n\n\n\n\n\n(d) Verteilung der theoretischen Anzahl an Farben.\n\n\n\n\nAbbildung 18.8— Darstellung Anzahl an Bärchen und Anzahl an Farben. Es gibt nicht mehr als sechs Farben. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte.\n\n\nIn Abbildung 18.9 schauen wir uns nochmal an in wie weit sich die Füllung der Tütchen im Laufe der Jahre entwickelt hat. Die Daten werden ja schon seit 2018 erhoben. Wir schauen uns daher die Densityplot einmal aufgetrennt für die Jahre 2018 bis heute an. Das Jahr 2020 fehlt, da bedingt durch die Coronapandemie keine Präsenslehre stattfand. Wir sehen, dass sich die Verteilung anscheinend in dem Jahr 2022 langsam nach links zu weniger Bärchen in einer Tüte bewegt. Wir bleiben gespannt auf den weiteren Trend.\n\n\n\n\nAbbildung 18.9— Densityplot der Anzahl an Bärchen in einer Tüte aufgetrennt nach den Jahren der Erhebung. Das Jahr 2020 fehlt bedingt durch die Coronapandemie.\n\n\n\n\nIn Abbildung 18.10 betrachten wir die Verteilung der am meisten gemochten Gummibärchen aufgeteilt nach dem angegebenen Geschlecht im Vergeich zu den Gummibärchen in den Tütchen. Wir sehen, dass Haribo die Tütchen sehr gleichmäßig verteilt und auf die Geschmäcker keinerlei Rücksicht nimmt. Entweder weiß Haribo nichts von den Vorlieben seiner Käufer:innen oder aber es ist dann doch zu viel Aufwand die Produktion anzupassen.\n\n\n\n\n\n(a) Anzahl am liebsten gemochten Gummibärchen aufgeteilt nach Geschlecht.\n\n\n\n\n\n\n(b) Anzahl der Gummibärchen pro Tüte nach Farbe.\n\n\n\n\nAbbildung 18.10— Histogramme der am liebsten gemochten Gummibärchchen im Vergleich zum Inhalt der Tütchen."
  },
  {
    "objectID": "eda-distribution.html#weitere-verteilungen",
    "href": "eda-distribution.html#weitere-verteilungen",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.9 Weitere Verteilungen",
    "text": "18.9 Weitere Verteilungen\n\n\nWir besuchen gerne die R Shiny App The distribution zoo um mehr über die verschiedenen Verteilungen und deren Parameter zu erfahren.\nWeitere Beispiele finden sich unter Basic Probability Distributions in R. Im Weiteren liefert Dormann (2013) eine gute Übersicht über verschiedene Verteilungen und deren Repräsentation in R. Das ist nur eine Auswahl an möglichen Verteilungen. Bitte hier nicht ins rabbit hole der Verteilungen gehen. Wir benötigen in unserer täglichen Arbeit nur einen kleinen Teil der Verteilungen. Es reicht, wenn du eine Vorstellungen der Verteilungen in diesem Kapitel hat."
  },
  {
    "objectID": "eda-distribution.html#referenzen",
    "href": "eda-distribution.html#referenzen",
    "title": "\n18  Verteilung von Daten\n",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "eda-distribution.html#sec-t-dist",
    "href": "eda-distribution.html#sec-t-dist",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.5 Die t-Verteilung",
    "text": "18.5 Die t-Verteilung\nDie t-Verteilung ist eine Abwandlung der Standardnormalverteilung. Wir haben wieder eine Fläche \\(A = 1\\) unter der Verteilungskurve. Wir benötigen die t-Verteilung, als eine künstliche Verteilung, im Kapitel 22 zum statistischen Testen mit dem t-Test. Wir bezeichnen die t-Verteilung als eine künstliche Verteilung, da wir in der Biologie nichts beobachten können, was t-verteilt ist. Wir nutzen die t-Verteilung nur im statistischen Kontext und in diesem Kontekt nur um uns klar zu machen wie statistisches Testen konzeptionell funktioniert. Anwenden werden wir die Verteilung nicht.\nDer Unterschied ist die Form der t-Verteilung. Wir geben mit der Option df = die Freiheitsgrade der Verteilung an. Hier soll es reichen, dass mit \\(\\lim_{df \\to \\infty}\\) sich die t-Verteilung der Standardnormalverteilung fast gleicht. Bei niedrigeren Freiheitsgraden ist die t-Verteilung nicht mehr so hoch und daher sind die Verteilungsenden weiter nach außen geschoben. Die t-Verteilung ist gestaucht wie wir in Abbildung 18.5 etwas überspitzt gezeichnet sehen. Die Freiheitsgrade hängen direkt an der beobachteten Fallzahl mit \\(df = n_1 + n_2 - 2\\).\n\n\nAbbildung 18.5— Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und damit die Fallzahl, desto näher kommt die t-Verteilung einer Normalverteilung nahe.\n\n\nWie auch bei der Standardnormalverteilung gilt folgender Zusammenhang, wenn wir die Flächen anhand eines gegebenen t-Wertes berechnen wollen. Wenn wir die Fläche links von dem t-Wert berechnen wollen, also die Wahrscheinlichkeit \\(Pr(X \\leq t)\\), dann nutzen wir die Option lower.tail = TRUE. Wenn wir die Fläche auf der rechten Seite von unserem t-Wert berechnen wollen, dann nutzen wir mit \\(Pr(X > t)\\) die Option lower.tail = FALSE. In der Funktion pt() ist das q= als t= zu lesen. Das macht das Verständnis vielleicht leichter.\n\npt(q = 2.571, df = 5, lower.tail = FALSE) %>% \n  round(3)\n\n[1] 0.025\n\n\nNeben der Berechnung der Wahrscheinlichkeit rechts und links eines gegebenen Wertes \\(t\\) können wir auch \\(t\\) berechnen, wenn wir eine Fläche vorgeben. Das kann uns dann die Funktion qt() liefern. Wir sehen, dass mit steigender Fallzahl und damit steigenden Freiheitsgrad sich der berechnete Wert sich dem Wert der Standardnormalverteilung von \\(1.96\\) für \\(p = 0.05\\) annähert.\n\nqt(p = c(0.025), df = c(5, 10, 20, 100, 1000), lower.tail = FALSE) %>% \n  round(3)\n\n[1] 2.571 2.228 2.086 1.984 1.962\n\n\nWir haben gelernt, dass der Zusammenhang zwischen der Standardnormalverteilung und der t-Verteilung ziemlich stark ist. Nutzen werden wir die t-Verteilung aber nur im Rahmen des statistischen Testens."
  },
  {
    "objectID": "eda-distribution.html#sec-uniform",
    "href": "eda-distribution.html#sec-uniform",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.8 Die Uniformverteilung",
    "text": "18.8 Die Uniformverteilung\nDie Gleichverteilung oder Uniformverteilung brauchen wir in der Statistik eher selten. Da wir aber hin und wieder mal auf die Gleichverteilung in technischen Prozessen stoßen, wollen wir uns die Gleichverteilung nochmal anschauen. Wenn wir eine Gleichverteilung vorliegen haben, dann sind alle Kategorien gleich häufig vertreten. Es ergibt sich dann folgende Verteilung als Plateau. Das Eintreten jedes Ereignisses ist gleich wahrscheinlich.\n\\[\nf(y)=\n\\begin{cases}\n  \\cfrac 1{b-a} & a \\le y \\le b\\\\\n  0            & \\text{sonst.}\n\\end{cases}\n\\]\n\n\nAbbildung 18.13— Darstellung der Uniformverteilung zwischen den beiden Punkten \\(a\\) und \\(b\\).\n\n\nDa gibt es auch sonst wenig mehr zu berichten. Nehmen wir daher nochmal ein technisches Beispiel aus unseren Gummibärchendaten. Wir würden je Farbe 681 Gummibärchen erwarten. Warum ist das so? Wir haben insgesamt 4087 ausgezählt. Wenn jede der sechs Kategorien mit der gleichen Wahrscheinlichkeit auftritt, dann erwarten wir jeweils \\(1/6\\) von der Gesamtzahl. Wir erkennen, dass wir etwas zu wenig grüne Bärchen haben. Ebenso sind die hellroten Bärchen unterrepräsentiert. Dafür haben wir dann zwangsweise etwas mehr an gelben und orangen Gummibärchen. Dennoch würde ich hier von einer Gleichverteilung ausgehen.\n\n\n\n\n\n(a) Verteilung der beobachteten Anzahl der Gummibärchen pro Tüte nach Farbe.\n\n\n\n\n\n\n(b) Verteilung der theoretischen Anzahl der Gummibärchen pro Tüte nach Farbe.\n\n\n\n\nAbbildung 18.14— Beispiel für eine uniforme Verteilung anhand der Anzahl der Gummibärchen pro Tüte nach Farbe"
  },
  {
    "objectID": "eda-distribution.html#sec-binom",
    "href": "eda-distribution.html#sec-binom",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.7 Die Binominalverteilung",
    "text": "18.7 Die Binominalverteilung\nDie Binomialverteilung wird uns vor allem später in den logistischen Regression wieder begegnen. An dieser Stelle ist es wichtig zu wissen, dass wir es bei der Binomialverteilung mit binären Ereignissen zu tun haben. Wir haben nur Erfolg oder nicht. Daher haben wir nur das Ergebnis \\(0/1\\) daher Null oder Eins. Dieses Ergebnis ist im Prinzip auch die Beschreibung eines Patienten, ob dieser krank oder nicht krank ist. Deshalb finden wir die Binomialverteilung auch häufig in einem medizinischen Kontext.\n\n\nEs gibt auch ein schönes Tutorial zur Binomial Distribution von David Arnold.\nSchauen wir uns die Formel für die Binomialverteilung einmal genauer an. Wichtig ist, dass wir etwas \\(n\\)-mal wiederholen und uns dann fragen, wie exakt \\(k\\)-oft haben wir Erfolg.\n\\[\nB(k\\mid p,n)=\n\\begin{cases}\n  \\binom nk p^k (1-p)^{n-k} &\\text{falls} \\quad k\\in\\left\\{0,1,\\dots,n\\right\\}\\\\\n  0            & \\text{sonst.}\n  \\end{cases}\n\\]\nmit\n\n\n\\(n\\) gleich der Anzahl an Versuchen (eng. trails)\n\n\\(k\\) gleich der Anzahl an Erfolgen\n\n\\(p\\) gleich der Wahrscheinlichkeit für einen Erfolg.\n\nBevor wir mit dem Beispiel beginnen können brauchen wir noch etwas mehr für die Berechnung der Formel. Wir brauchen noch für die Berechnung der Binomalverteilung den Binomialkoeffizienten \\(\\tbinom {n}{k}\\), den wir wie folgt bestimmen können. Dabei bedeutet das \\(!\\), dass wir eine Zahl aufmultiplizieren. Daher müssen wir für \\(4!\\) dann wie folgt rechnen \\(4! = 1 \\cdot 2 \\cdot 3 \\cdot 4 = 24\\).\n\\[\n\\binom nk = \\cfrac{n!}{k! \\cdot (n-k)!}\n\\]\nNehmen wir dafür einmal ein Beispiel mit 5 über 3 und schauen uns die Rechnung einmal an. Wir erhalten den Binomialkoeffizienten \\(\\tbinom {5}{3}\\) wie folgt.\n\\[\n\\binom 5 3 = \\frac{5!}{3! \\cdot (5-3)!} = \\frac{5!}{3! \\cdot 2!} = \\frac{1\\cdot 2\\cdot 3\\cdot 4\\cdot 5}{(1\\cdot 2\\cdot 3) \\cdot (1\\cdot 2)} = \\frac{4\\cdot 5}{1\\cdot 2} = 10\n\\]\nViele Taschenrechner können den Binomialkoeffizienten flott ausrechnen. Wenn wir keinen Taschenrechner haben, dann können wir auch das Pascalsche (oder Pascal’sche) Dreieck nutzen. Das Pascalsche Dreieck ist eine Form der grafischen Darstellung der Binomialkoeffizienten \\(\\tbinom {n}{k}\\). Wir sehen einmal in Abbildung 18.11 den Zusammenhang mit dem Binomialkoeffizienten dargestellt. Mit dem Pascalsche Dreieck können wir auch ohne Taschenrechner den Binomialkoeffizienten bestimmen.\n\n\nAbbildung 18.11— Darstellung des Pascalsche (oder Pascal’sche) Dreieckes im Zusammenhang zum Binomialkoeffizienten.\n\n\nSomit können wir auch einmal ein erweitertes Beispiel der Binomialverteilung rechnen. Was ist die Wahrscheinlichkeit bei \\(n = 5\\) Münzwürfen genau dann \\(k = 2\\) Erfolge zu erzielen, wenn die Münze fair ist und damit gilt \\(p = 0.5\\)?\n\\[\n\\begin{aligned}\nPr(Y = 3) &= \\binom {5}{3} 0.5^{3} (1-0.5)^{5-3} \\\\  \n&= 10 \\cdot 0.5^3 \\cdot 0.5^2 \\\\\n&= 0.31\n\\end{aligned}\n\\]\nWir immer können wir die ganze Rechnung dann auch in R durchführen. Dank der Funktion choose() können wir schnell den Binomialkoeffizienten berechnen. Der Rest ist dann nur noch das Einsetzen.\n\nchoose(5,3) * 0.5^3 * 0.5^2\n\n[1] 0.3125\n\n\nAuch hier geht es natürlich auch in R noch einen Schritt schneller. Leider heißt dann wieder alles anders. Wir wollen x = k = 3 Erfolge aus size = n = 5 Versuchen mit einer Erfolgswahrscheinlichkeit von prob = 0.5. Daran muss man sich dann gewöhnen, dass sich die Begrifflichkeiten dann doch immer mal wieder ändern.\n\ndbinom(x = 3, size = 5, prob = 0.5)\n\n[1] 0.3125\n\n\nWas wäre wenn wir jetzt die Wahrscheinlichkeit \\(Pr(Y \\leq 3)\\) berechnen wollen? Also nicht exakt die Wahrscheinlichkeit für \\(k=3\\) Erfolge sondern eben \\(k\\) Erfolge oder weniger \\(k \\leq 3\\). Dann müssen wir die Wahrscheinlichkeiten für \\(Pr(Y = 0)\\), \\(Pr(Y = 1)\\), \\(Pr(Y = 2 )\\) und \\(Pr(Y = 3)\\) berechnen und diese Wahrscheinlichkeiten aufaddieren.\n\ndbinom(0, 5, 0.5) + dbinom(1, 5, 0.5) + dbinom(2, 5, 0.5) + dbinom(3, 5, 0.5)\n\n[1] 0.8125\n\n\nOder wir rechen einfach die Fläche und damit die Wahrscheinlichkeit links von \\(k = 3\\) aus. Dafür haben wir dann die Funktion pbinom(). Es geht dann eben doch etwas flotter. Wie immer können wir dann über die Option lower.tail = entscheiden, auf welche Seite der Verteilung wir schauen wollen.\n\npbinom(3, 5, 0.5, lower.tail = TRUE)\n\n[1] 0.8125\n\n\nAngenommen, eine Münze wird so gewichtet, dass sie in 60 % der Fälle Kopf ergibt. Wie hoch ist die Wahrscheinlichkeit, dass Sie nach 50 Würfen 25 oder mehr Köpfe erhalten? Dafür können wir dann auch die Funktion pbinom() einmal nutzen. Da wir mehr wollen, also “größer als”, müssen wir rechts von dem berechneten Wert schauen, also auswählen, dass lower.tail = FALSE ist.\n\npbinom(25, 50, 0.6, lower.tail = FALSE)\n\n[1] 0.9021926\n\n\nZum Abschluss schauen wir nochmal in unseren Gummibärchendaten, wie dort ein Histogramm einer binären Variable mit nur zwei Ausprägungen aussehen würde. In Abbildung 18.12 sehen wir einmal das Geschlecht als Balkendiagramm dargestellt. Mehr gibt es zu diesem Diagramm erstmal nicht zu berichten. Bei einer Variable bei einem unbekannten \\(p\\) für eine der Kategorien, ist schwer etwas zu bewerten. Wir sehen aber, dass wir eine sehr schöne Gleichverteilung von den Geschlechtern in den Daten haben.\n\n\n\n\nAbbildung 18.12— Beispiel für eine Binomialverteilung anhand des Geschlechts. Die fehlenden Angaben wurden entfernt."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skript Bio Data Science",
    "section": "",
    "text": "Auf den folgenden Seiten wirst du eine Menge über Statistik oder Data Science lernen. Du musst dafür nicht eine meiner Veranstaltungen besuchen. Gerne kannst du hier und dort einmal schauen, ob etwas für dich dabei ist. Das Skript wird fortlaufend von mir ergänzt. Neben dem Skript gibt es auch noch die erklärenden YouTube Videos. Ich freue mich, dass du Lust hast hier etwas zu lernen… oder aber du musst – da bald eine Klausur ansteht. Wie auch immer – schau dich einfach mal um. Im Anhang findest du auch einen kleinen Leitfaden für das Schreiben einer Abschlussarbeit. Vielleicht hilft dir die Anleitung ja beim Schreiben.\n\n\n\n\n\n\nGesammelte Klausurfragen in der Bio Data Science\n\n\n\n\n\nDu findest die gesammelten Klausurfragen auf GitHub oder auf ILIAS in dem entsprechenden Modul. Die Klausurfragen zu den einzelnen Vorlesungen in einem Modul werden in den entsprechenden Übungen des Moduls besprochen. Bitte komme in die Übungen.\nDu brauchst dir die Fragen nicht alle auszudrucken. Wir besprechen die Fragen teilweise in den Übungen.\nDie finale Version für die Klausur veröffentliche ich Ende November für das Wintersemester bzw. Ende Mai für das Sommersemester.\n\n\n\n\n\n\n\n\n\nInformationen zu einer Bachelorarbeit in der Bio Data Science\n\n\n\n\n\nWenn du dich fragst, wie die Rahmenbedingungen einer Bachelorarbeit bei mir sind findest du hier die Informationen zu einer Bachelorarbeit in der Bio Data Science. Oder du fragst mich einfach unverbindlich per Mail oder in einer meiner Vorlesungen. Wie es dir am besten passt.\n\n\n\n\n\nDu liest hier gerade das Skript für meine Vorlesungen an der Hochschule Osnabrück an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Wie immer Leben kannst du auf verschiedene Arten und Weisen den Stoff, den ich vermitteln will, lernen. Daher gibt es noch zwei andere Möglichkeiten. Zum einen Lernen auf YouTube, mit meinen Lernvideos oder du schaust dir das Material auf GitHub an. Auf GitHub habe ich auch Informationen, die du vielleicht brauchen kannst. Ebenso findest du im Kapitel 2 noch andere Literaturempfehlungen.\n\n\n\n\n\n\n\nWenn du möchtest kannst du auf YouTube unter https://www.youtube.com/c/JochenKruppa noch einige Lehrvideos als Ergänzung schauen. In den Videos wiederhole ich Inhalte und du kannst auf Pause drücken um nochmal Programmierschritte nachverfolgen zu können.\n\n\n\n\n\n\n\n\nAlle Materialien von mir findest du immer auf GitHub unter https://github.com/jkruppa/teaching. Selbst wenn du nicht mehr in einem meiner Kurse bist, kannst du so auf die Lehrinhalte immer nochmal zugreifen und die aktuellen Versionen haben. Auf GitHub liegt auch immer eine semesteraktuelle Version der gesammelten Klausurfragen für meine Module.\n\n\n\n\nWie erreichst du mich? Am einfachsten über die gute, alte E-Mail. Bitte bachte, dass gerade kurz vor den Prüfungen ich mehr E-Mails kriege. Leider kann es dann einen Tick dauern.\n\n\n\n\n\nEinfach an j.kruppa@hs-osnabrueck.de schreiben. Du findest hier auch eine kurze Formulierungshilfe. Einfach auf den Ausklapppfeil klicken.\nBitte gib immer in deiner E-Mail dein Modul - was du belegst - mit an. Pro Semester unterrichte ich immer drei sehr ähnlich klingende Module. Daher schau nochmal hier in der Liste, wenn du unsicher bist.\n\n\n\n\n\n\nE-Mailvorlage mit beispielhafter Anrede\n\n\n\n\n\nHallo Herr Kruppa,\n… ich belege gerade Ihr Modul Modulname und hätte eine Bitte/Frage/Anregung…\n… ich benötige Hilfe bei der Planung/Auswertung meiner Bachelorarbeit…\nMit freundlichen Grüßen\nM. Muster"
  },
  {
    "objectID": "organisation.html",
    "href": "organisation.html",
    "title": "\n1  Organisation\n",
    "section": "",
    "text": "Version vom November 11, 2022 um 15:26:50\nDen Teil kannst du hier überspringen, wenn es dich nicht so richtig interessiert, was ich alles an Vorlesungen an der Hochschule Osnabrück anbiete. Wenn es dir um statistische Inhalte geht, dann gehe einfach weiter in die nächsten Kapitel. In diesem Kapitel geht es nochmal Orientierung über meine Vorlesungen zu geben, wenn dich noch mehr als nur der Pflichtkurs interessiert."
  },
  {
    "objectID": "organisation.html#statistische-beratung",
    "href": "organisation.html#statistische-beratung",
    "title": "\n1  Organisation\n",
    "section": "\n1.1 Statistische Beratung",
    "text": "1.1 Statistische Beratung\nNeben der klassischen Vorlesung biete ich auch Termine für die statistische Beratung von Abschlussarbeiten sowie Projekten an. Dieses Angebot gilt es für alle Mitglieder der Hochschule Osnabrück. Primär für Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL), aber ntürlich auch für alle anderen Fakultäten. Dafür musst du mir einfach nur eine E-Mail schreiben und dann erhälst du einen Termin innerhalb der nächsten zwei Wochen.\nDie Beratung ist grundsätzlich anonym und vertraulich. Wenn du willst kannst du gerne noch dein:e Betreuer:in mitbringen. Das ist aber keine Voraussetzung oder Notwendigkeit. Meistens finden mehrere Besprechungen statt, wir versuchen aber natürlich zusammen zügig dein Problem zu lösen. Ziel ist der Beratung ist es dich in die Lage zu versetzen selbstständig deine Analyse zu rechnen."
  },
  {
    "objectID": "organisation.html#sec-r-tutorium",
    "href": "organisation.html#sec-r-tutorium",
    "title": "\n1  Organisation\n",
    "section": "\n1.2 R Tutorium",
    "text": "1.2 R Tutorium\nZusätzlich zu der statistischen Beratung bieten wir auch ein R Tutorium für alle Mitglieder der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) an. Theoretisch können auch hier andere Mitglieder der anderen Faklutäten vorbeischauen, der Ort ist aber aktuell ein Raum auf dem Gelände in Haste. Die aktuellen Termine findest du in Tabelle 1.1.\nIm R Tutorium besprechen wir aktuelle Themen der Teilnehmer:innen. Meist sind dies aktuelle Fragen zu den Bachelorarbeiten. Auch wenn du kein dringendes Problem hast, kannst du gerne kommen und dir die Fragestellungen anhören. Meistens ist es auch interessant mal die Fragestellungen der anderen Studierenden sich anzuhören oder aber schon mal zu Üben ein anderes Experiment zu verstehen.\nBitte beachte folgende Hinweise zu den Terminen.\n\n\nHier findest du den Lage- und Gebäudeplan vom Standort Haste.\n\n\n\n\n\n\nHinweise zu dem R Tutorium\n\n\n\nDas R Tutorium findet nicht im Prüfungszeitraum der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) statt.\nDas R Tutorium findet nicht im Februar und März statt.\nDas R Tutorium findet nicht im August und September statt.\n\n\n\n\n\nTabelle 1.1— Aktuelle Termine des R Tutoriums im Semester. Hier findest du den Lage- und Gebäudeplan vom Standort Haste.\n\nTermin\nUhrzeit\nRaum\nAnmerkung\n\n\n\nDienstag, den 15. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 22. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 29. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 06. Dezember 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 13. Dezember 2022\n9:45 - 11:15\nHM 0115\n\n\n\nOnlinetermine ab jetzt…\n\n\n\n\n\nDienstag, den 20. Dezember 2022\n9:45 - 11:15\n[online]\nohne Kruppa\n\n\nWeihnachtswoche\n\n\n\n\n\nDienstag, den 03. Januar 2023\n9:45 - 11:15\n[online]\n\n\n\nDienstag, den 10. Januar 2023\n9:45 - 11:15\n[online]\n\n\n\nDienstag, den 17. Januar 2023\n9:45 - 11:15\n[online]\nOptionaler Termin\n\n\nDienstag, den 23. Januar 2023\n9:45 - 11:15\n[online]\nOptionaler Termin"
  },
  {
    "objectID": "organisation.html#sec-vorlesungen-hs",
    "href": "organisation.html#sec-vorlesungen-hs",
    "title": "\n1  Organisation\n",
    "section": "\n1.3 Vorlesungen an der Hochschule Osnabrück",
    "text": "1.3 Vorlesungen an der Hochschule Osnabrück\nVon mir angebotene Vorlesungen werden an der Hochschule Osnabrück an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) in ILIAS verwaltet. Alle notwendigen Informationen und Materialien sind auf ILIAS unter https://lms.hs-osnabrueck.de/ zu finden. Wenn du in dem Kurs nicht angemeldet bist, dann kontaktiere mich bitte per Mail. Auch die Kommunikation erfolgt von meiner Seite aus über ILIAS.\nAuf ILIAS findest du alle aktuellen Kursinformationen und erhälst auch die Mails, wenn Änderungen im Kursablauf stattfinden.\nWenn du nicht in der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) studierst oder aber in einem Studiengang, der meine Module nicht anbietet, steht es dir natürlich frei, sich in meine Vorlesungen zu setzten. Du findest in Tabelle 1.2 eine Übersicht der angebotenen Module und auch die inhaltliche Ordnung nach Lernstufe. Bitte informiere dich in deinem Studierendensekretariat über die Modalitäten zur Prüfungsteilnahme.\n\n\n\n\n\n\nAktuelle Entwürfe der Modulbeschreibungen\n\n\n\nDu findest die aktuellen Entwürfe der Modulbeschreibungen im Kapitel E. Im Rahmen der Weiterentwicklung der Studiengänge arbeite ich immer parallel zu den offiziellen Modulbeschreibungen an den Inhalten der Vorlesungen weiter. Dieses Entwickeln findet Niederschlag in den Entwürfe der Modulbeschreibungen.\nEine inhaltliche Übersicht und die Planung des Vorlesungsverlaufs findest du auf dem Google Spreadsheet zur inhaltlichen Planung.\n\n\n\n\n\nTabelle 1.2— Angebotene Statistik Module an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Die Stufe gibt das Lernniveau an.\n\nStufe\nLandwirtschaft; Angewandte Pflanzenbiologie – Gartenbau, Pflanzentechnologie\nWirtschafts- ingenieurwesen Agrar / Lebensmittel\nBioverfahrenstechnik in Agrar- und Lebensmittelwirtschaft\nAngewandte Nutztier- und Pflanzenwissenschaften\n\n\n\n1\nMathematik und Statistik\nStatistik\nAngewandte Statistik für Bioverfahrenstechnik\n\n\n\n2\nAngewandte Statistik und Versuchswesen\nAngewandte Statistik und Versuchswesen\n\n\n\n\n3\nSpezielle Statistik und Versuchswesen\n\n\nBiostatistik"
  },
  {
    "objectID": "organisation.html#sec-bachelorarbeit",
    "href": "organisation.html#sec-bachelorarbeit",
    "title": "\n1  Organisation\n",
    "section": "\n1.4 Bachelorarbeit",
    "text": "1.4 Bachelorarbeit\nHier findest sich eine aktuelle Struktursammlung für die Bachelorarbeit. Hier findest du keine Themen. Dafür musst du mich bitte ansprechen oder eine E-Mail schreiben. Die Themen finden sich dann etwa mit Kooperationspartern oder aber eher methodisch ohne echte Daten. Das müssen wir dann aber Absprechen.\nBitte halte Rücksprache, wenn dir Teile der Regeln für die Bachelorarbeit unklar sind.\n\nIch empfehle die Arbeit in engischer Sprache zu verfassen.\nDie Bachelorarbeit umfasst einen Zeitraum von 12 Wochen. Bei Unsicherheit über das Thema kann einmalig eine 4-wöchige Einarbeitungsphase vereinbart werden. Danach wird das Thema der Bachelorarbeit konkretisiert.\nDer Umfang sollte die 30 Seiten nicht überschreiten.\nDie Arbeit umfasst ca. 30 Referenzen, davon sind die meisten aktuelleren Datums. Internetseiten zählen ausdrücklich nicht als Referenz.\nEin Bewertungsbogen für die Bachelorarbeit steht zu Beginn der Arbeit zu Verfügung und kann jederzeit eingesehen werden.\nIm Rahmen der Betreuung finden mindestens jede Wochen ein kurzes Zoom-Treffen statt in dem der aktuelle Fortschritt der Arbeit besprochen wird.\nIn der 4-ten Woche wird eine kurze Präsentation der bisherigen erarbeiteten Inhalte gegeben. Diese Präsentation kann in der 8-ten Woche erneut erfolgen.\nMit einem methodischen Thema wird die Arbeit in Quarto in R oder in LaTeX in Overleaf geschreiben.\n\nIm Folgenden siehst du nochmal den groben zeitlichen Ablauf in Abbildung 1.1. Der Ablauf dient der groben Orientierung, damit du auch weißt, wo du etwa stehst. Nach vier Wochen solltest du gut 3500 Wörter geschrieben haben und nach 8 Wochen ca. 7000 Worte. Damit solltest du dann am Ende auf die 30 Seiten mit ca. 10500 Worten kommen.\n\n\nAbbildung 1.1— Grober zeitlicher Ablauf einer Bachelorarbeit. Die Präsentation umfasst ca. 10 Slides und folgt ebenfalls dem IMRAD Schema. Ich rechne mit 350 Worten pro deutscher Standardseite.\n\n\nBitte bachte auch die Hilfestellungen und die Erfahrungsberichte in dem Kapitel C, wo ich nochmal über Writing principles etwas aufgeschrieben habe. Vielleicht hilft dir das dann auch.\n\n\n\n\n\n\nKorrekte Schreibweise von einer Formel\n\n\n\n\n\n\\[\ny \\sim x_1 + x_2\n\\]\nmit\n\n\n\\(y\\) gleich dem gemessenen Frischgewicht in [kg/ha],\n\n\\(x_1\\) als der kontinuierliche Einflussvariable 1,\n\n\\(x_2\\) als der Einflussvariable 2 als Faktor mit den Leveln \\(a\\), \\(b\\) und \\(c\\).\n\n\n\n\n\n\n\n\n\n\nKorrekte Beschriftung und Referenzierung einer Abbildung\n\n\n\n\n\n\n\n\n\nAbbildung 1.2— Boxplots der Sprungweiten in [cm] getrennt für Hunde- und Katzenflöhe.\n\n\n\n\nIn Abbildung 1.2 sind die Sprungweiten in [cm] von Hunde- und Katzenflöhen als Boxplots dargestellt.\n\n\n\n\n\n\n\n\n\nKorrekte Beschriftung und Referenzierung einer Tabelle\n\n\n\n\n\n\n\n\n\nTabelle 1.3— Tabelle der Sprunglängen in [cm] von Hunde- und Katzenflöhen. Es wurden sieben Hundeflöhe und sieben Katzenflöhe gemessen (\\(n= 14\\)).\n\nanimal\njump_length\n\n\n\ndog\n5.7\n\n\ndog\n8.9\n\n\ndog\n11.8\n\n\ndog\n8.2\n\n\ndog\n5.6\n\n\ndog\n9.1\n\n\ndog\n7.6\n\n\ncat\n3.2\n\n\ncat\n2.2\n\n\ncat\n5.4\n\n\ncat\n4.1\n\n\ncat\n4.3\n\n\ncat\n7.9\n\n\ncat\n6.1\n\n\n\n\n\n\nIn Tabelle 1.3 sind die Sprunglängen in [cm] in der Spalte jump_length von Hunde- und Katzenflöhen in der Spalte animal dargestellt. Insgesamt wurden \\(n = 14\\) Flöhe gemessen davon sieben Hundeflöhe und sieben Katzenflöhe. Wir haben ein balanciertes Design vorliegen."
  },
  {
    "objectID": "stat-tests-basic.html",
    "href": "stat-tests-basic.html",
    "title": "19  Die Testentscheidung",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:47:58\nDu erfährst im diesem Kapitel mehr zur statistischen Testentscheidung und welche Konzepte wir beim statistischen Testen nutzen:"
  },
  {
    "objectID": "stat-tests-basic.html#sec-hypothesen",
    "href": "stat-tests-basic.html#sec-hypothesen",
    "title": "19  Die Testentscheidung",
    "section": "\n19.1 Die Hypothesen",
    "text": "19.1 Die Hypothesen\nWir können auf allen Daten einen statistischen Test rechnen und erhalten statistische Maßzahlen wie eine Teststatistik oder einen p-Wert. Nur leider können wir mit diesen statistischen Maßzahlen nicht viel anfangen ohne die Hypothesen zu kennen. Jeder statistische Test testet eine Nullhypothese. Ob diese Hypothese dem Anwender nun bekannt ist oder nicht, ein statistischer Test testet eine Nullhypothese. Daher müssen wir uns immer klar sein, was die entsprechende Nullhypothese zu unserer Fragestellung ist. Wenn du hier stockst, ist das ganz normal. Eine Fragestellung mit einer statistischen Hypothese zu verbinden ist nicht immer so einfach gemacht.\n\n\n\n\n\n\nDie Nullhypothese \\(H_0\\) und die Alternativhypothese \\(H_A\\)\n\n\n\nDie Nullhypothese \\(H_0\\) nennen wir auch die Null oder Gleichheitshypothese. Die Nullhypothese sagt aus, dass zwei Gruppen gleich sind oder aber kein Effekt zu beobachten ist.\n\\[\nH_0: \\bar{y}_{1} = \\bar{y}_{2}\n\\]\nDie Alternativhypothese \\(H_A\\) oder \\(H_1\\) auch Alternative genannt nennen wir auch Unterschiedshypothese. Die Alternativhypothese besagt, dass ein Unterschied vorliegt oder aber ein Effekt vorhanden ist.\n\\[\nH_A: \\bar{y}_{1} \\neq \\bar{y}_{2}\n\\]\n\n\nAls Veranschaulichung nehmen wir das Beispiel aus der unterschiedlichen Sprungweiten in [cm] für Hunde- und Katzenflöhe. Wir formulieren als erstes die Fragestellung. Eine Fragestellung endet mit einem Fragezeichen.\nLiegt ein Unterschied zwischen den Sprungweiten von Hunde- und Katzenflöhen vor?\nWir können die Frage auch anders formulieren.\nSpringen Hunde- und Katzenflöhe unterschiedlich weit?\nWichtig ist, dass wir eine Fragestellung formulieren. Wir können auch mehrere Fragen an einen Datensatz haben. Das ist auch vollkommen normal. Nur hat jede Fragestellung ein eigenes Hypothesenpaar. Wir bleiben aber bei dem simplen Beispiel mit den Sprungweiten von Hunde- und Katzenflöhen.\nEine statistische Hypothese ist eine Aussage über einen Parameter einer Population.\nWie sieht nun die statistische Hypothese in diesem Beispiel aus? Wir wollen uns die Sprungweite in [cm] anschauen und entscheiden, ob die Sprungweite für Hunde- und Katzenflöhen sich unterscheidet. Eine statistische Hypothese ist eine Aussage über einen Parameter einer Population. Wir entscheiden jetzt, dass wir die mittlere Sprungweite der Hundeflöhe \\(\\bar{y}_{dog}\\) mit der mittleren Sprungweite der Katzenflöhe \\(\\bar{y}_{cat}\\) vergleichen wollen. Es ergibt sich daher folgendes Hypothesenpaar.\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{dog} &= \\bar{y}_{cat} \\\\  \nH_A: \\bar{y}_{dog} &\\neq \\bar{y}_{cat} \\\\   \n\\end{aligned}\n\\]\nDas Falisifkationsprinzip - wir können nur Ablehnen - kommt hier zusammen mit der frequentistischen Statistik in der wir nur eine Wahrscheinlichkeitsaussage über das Auftreten der Daten \\(D\\) - unter der Annahme \\(H_0\\) gilt - treffen können.\nEs ist wichtig sich in Erinnerung zu rufen, dass wir nur und ausschließlich Aussagen über die Nullhypothese treffen werden. Das frequentistische Hypothesentesten kann nichts anders. Wir kriegen keine Aussage über die Alternativhypothese sondern nur eine Abschätzung der Wahrscheinlichkeit des Auftretens der Daten im durchgeführten Experiment, wenn die Nullhypothese wahr wäre. Wenn die Nullhypothese war ist, dann liegt kein Effekt oder Unterschied vor."
  },
  {
    "objectID": "stat-tests-basic.html#die-testentscheidung",
    "href": "stat-tests-basic.html#die-testentscheidung",
    "title": "19  Die Testentscheidung",
    "section": "\n19.2 Die Testentscheidung…",
    "text": "19.2 Die Testentscheidung…\nIn den folgenden Kapiteln werden wir verschiedene statistische Tests kennenlernen. Alle statistischen Tests haben gemein, dass ein Test eine Teststatistik \\(T_{calc}\\) berechnet. Darüber hinaus liefert jeder Test auch einen p-Wert (eng. p-value). Manche statistischen Test geben auch ein 95% Konfidenzintervall wieder. Eine Testentscheidung gegen die Nullhypothese \\(H_0\\) kann mit jedem der drei statistischen Maßzahlen - Teststatistik, $p$-Wert und Konfidenzintervall - durchgeführt werden. Die Regel für die Entscheidung, ob die Nullhypothese \\(H_0\\) abgelehnt werden kann, ist nur jeweils anders. In Tabelle 19.1 sind die Entscheidungsregeln einmal zusammengefasst.\n\n\n\nTabelle 19.1— Zusammenfassung der statistischen Testentscheidung unter der Nutzung der Teststatistik, dem p-Wert und dem 95% Konfidenzintervall. Die Entscheidung nach der Teststatistik ist veraltet und dient nur dem konzeptionellen Verständnisses. In der Forschung angewandt wird der \\(p\\)-Wert und das 95% Konfidenzintervall. Im Fall des 95% Konfidenzintervalls müssen wir noch unterschieden, ob wir einen Mittelwertsunterschied \\(\\Delta_{A-B}\\) oder aber einen Anteilsunterschied \\(\\Delta_{A/B}\\) betrachten.\n\n\n\n\n\n\n\n\nTeststatistik\np-Wert\n95% Konfidenzintervall\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(\\boldsymbol{KI_{1-\\alpha}}\\)\n\n\nH\\(_0\\) ablehnen\n\\(T_{calc} \\geq T_{\\alpha = 5\\%}\\)\n\\(Pr(\\geq T_{calc}| H_0) \\leq \\alpha\\)\n\n\\(\\Delta_{A-B}\\): enthält nicht 0\n\n\n\nH\\(_0\\) ablehnen\n\n\n\n\\(\\Delta_{A/B}\\): enthält nicht 1\n\n\n\n\n\n\nWir wollen in den folgenden Abschnitten die jeweiligen Entscheidungsregeln eines statistisches Tests einmal durchgehen.\n\nDie Testentscheidung gegen die Nullhypothese anhand der Teststatistik in Kapitel 19.2.1\n\nDie Testentscheidung gegen die Nullhypothese anhand dem p-Wert in Kapitel 19.2.2\n\nDie Testentscheidung gegen die Nullhypothese anhand des 95% Konfidenzintervall in Kapitel 19.2.3\n\n\n\n\n\n\n\n\nStreng genommen gilt die Regel \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) nur für eine Auswahl an statistischen Tests siehe dazu auch Kapitel 19.2.1. Bei manchen statistischen Tests ist die Entscheidung gedreht. Hier lassen wir das aber mal so stehen…\n\n19.2.1 … anhand der Teststatistik\n\n\n\n\n\n\nPrinzip des statistischen Testens I - Die Teststatistik\n\n\n\nDu findest auf YouTube Prinzip des statistischen Testens I - Die Teststatistik als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nWir wollen uns dem frequentistischen Hypothesentesten über die Idee der Teststatistik annähern. Im folgenden sehen wir die Formel für den t-Test. Den t-Test werden wir im Kapitel 22 uns nochmal detaillierter anschauen. Hier nutzen wir die vereinfachte Formel um das Konzept der Teststatistik \\(T\\) zu verstehen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_2}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nmit\n\n\n\\(\\bar{y}_1\\) dem Mittelwert für die erste Gruppe.\n\n\\(\\bar{y}_2\\) dem Mittelwert für die zweite Gruppe.\n\n\\(s_{p}\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{s_A + s_B}{2}\\).\n\n\\(n_g\\) der Gruppengröße der gruppen. Wir nehmen an beide Gruppen sind gleich groß.\n\nWir benötigen also zwei Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) und deren gepoolte Standardabweichung \\(s_p\\) sowie die Anzahl der Beobachtungen je Gruppe \\(n_g\\). Wenden wir die Formel des t-Tests einmal auf den folgenden Beispieldatensatz zu den Sprunglängen in [cm] von jeweils vier Hunde- und Kätzenflohen an. In Tabelle 19.2 ist das Datenbeispiel gegeben.\n\n\n\n\nTabelle 19.2— Beispiel für die Berechnung von einem Mittelwertseffekt an der Sprunglänge [cm] von Hunde und Katzenflöhen.\n\nanimal\njump_length\n\n\n\ncat\n8.5\n\n\ncat\n9.9\n\n\ncat\n8.9\n\n\ncat\n9.4\n\n\ndog\n8.0\n\n\ndog\n7.2\n\n\ndog\n8.4\n\n\ndog\n7.5\n\n\n\n\n\n\nNun berechnen wir die Mittelwerte und die Standardabweichungen aus der obigen Datentabelle für die Sprungweiten getrennt für die Hunde- und Katzenflöhe. Die Werte setzen wir dann in die Formel ein und berechnen die Teststatistik \\(T_{calc}\\).\n\\[\nT_{calc}=\\cfrac{9.18 - 7.78}{\\cfrac{(0.61 + 0.53)}{2} \\cdot \\sqrt{2/4}} = 3.47\n\\]\nmit\n\n\n\\(\\bar{y}_{cat} = 9.18\\) dem Mittelwert für die Gruppe cat.\n\n\\(\\bar{y}_{dog} = 7.78\\) dem Mittelwert für die Gruppe dog.\n\n\\(s_p = 0.57\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.61 + 0.53}{2}\\).\n\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nWir haben nun die Teststatistik \\(T_{calc} = 3.47\\) berechnet. In der ganzen Rechnererei verliert man manchmal den Überblick. Erinnern wir uns, was wir eigentlich wollten. Die Frage war, ob sich die mittleren Sprungweiten der Hunde- und Katzenflöhe unterschieden. Wenn die \\(H_0\\) wahr wäre, dann wäre der Unterschied \\(\\Delta\\) der beiden Mittelwerte der Hunde- und Katzenflöhe gleich null. Oder nochmal in der Analogie der t-Test Formel, dann wäre im Zähler \\(\\Delta = \\bar{y}_{cat} - \\bar{y}_{dog} = 0\\). Wenn die Mittelwerte der Sprungweite [cm] der Hunde- und Katzenflöhe gleich wäre, dann wäre die berechnete Teststatistik \\(T_{calc} = 0\\), da im Zähler Null stehen würde. Die Differenz von zwei gleichen Zahlen ist Null.\nJe größer die berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die beiden Mittelwerte per Zufall gleich sind. Wie groß muss nun die berechnete Teststatistik \\(T_{calc}\\) werden damit wir die Nullhypothese ablehnen können?\n\n\n\nAbbildung 19.1— Die t-Verteilung aller möglichen \\(T_{calc}\\) wenn die Nullhypothese wahr ist. Der Mittelwert der t-Verteilung ist \\(T=0\\). Wenn wir keinen Effekt erwarten würden dann wären die beiden Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) gleich groß. Die Differenz wäre 0. Je größer der \\(T_{calc}\\) wird desto weniger können wir davon ausgehen, dass die beiden Mittelwerte gleich sind. Liegt der \\(T_{calc}\\) über dem kritischen Wert von \\(T_{\\alpha = 5\\%}\\) dann wir die Nullhypothese abgelehnt.\n\n\n\nIn Abbildung 19.1 ist die Verteilung aller möglichen \\(T_{calc}\\) Werte unter der Annahme, dass die Nullhypothese wahr ist, dargestellt. Wir sehen, dass die t-Verteilung den Gipfel bei \\(T_{calc} = 0\\) hat und niedrigere Werte mit steigenden Werten der Teststatistik annimmt. Wenn \\(T = 0\\) ist, dann sind auch die Mittelwerte gleich. Je größer unsere berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die Nullhypothese gilt.\nDie t-Verteilug ist so gebaut, dass die Fläche \\(A\\) unter der Kurve gleich \\(A=1\\) ist. Wir können nun den kritschen Wert \\(T_{\\alpha = 5\\%}\\) berechnen an dem rechts von dem Wert eine Fläche von 0.05 oder 5% liegt. Somit liegt dann links von dem kritischen Wert die Fläche von 0.95 oder 95%. Den kritischen Wert \\(T_{\\alpha = 5\\%}\\) können wir statistischen Tabellen entnehmen. Oder wir berechnen den kritischen Wert direkt in R mit \\(T_{\\alpha = 5\\%} = 2.78\\).\nKommen wir zurück zu unserem Beispiel. Wir haben in unserem Datenbeispiel für den Vergleich von der Sprungweite in [cm] von Hunde- und Katzenflöhen eine Teststatistik von \\(T_{calc} = 3.47\\) berechnet. Der kritische Wert um die Nullhypothese abzulehnen liegt bei \\(T_{\\alpha = 5\\%} = 2.78\\). Wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. In unserem Fall ist \\(3.47 \\geq 2.78\\). Wir können die Nullhypothese ablehnen. Es gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen.\n\n\n\n\n\n\nWoher kommt die Testverteilung von \\(T\\), wenn \\(H_0\\) gilt?\n\n\n\n\n\nIn diesem Exkurs wollen wir einmal überlegen, woher die Testverteilung \\(T\\) herkommt, wenn die \\(H_0\\) gilt. Wir wollen die Verteilung der Teststatistik einmal in R herleiten. Zuerst gehen wir davon aus, dass die Mittelwerte der Sprungweite der Hunde- und Katzenflöhe gleich sind \\(\\bar{y}_{cat} = \\bar{y}_{dog} = (9.18 + 7.78)/2 = 8.48\\). Daher nehmen wir an, dass die Mittelwerte aus der gleichen Normalverteilung kommen. Wir ziehen also vier Sprungweiten jeweils für die Hunde- und Katzenflöhe aus einer Normalverteilung mit \\(\\mathcal{N}(8.48, 0.57)\\). Wir nutzen dafür die Funktion rnorm(). Anschließend berechnen wir die Teststatistik. Diesen Schritt wiederholen wir eintausend Mal.\n\nset.seed(20201021)\nT_vec <- map_dbl(1:1000, function(...){\n  dog_vec <- rnorm(n = 4, mean = 8.48, sd = 0.57)\n  cat_vec <- rnorm(n = 4, mean = 8.48, sd = 0.57)\n  s_p <- (sd(cat_vec) + sd(dog_vec))/2 \n  T_calc <- (mean(cat_vec) - mean(dog_vec))/(s_p * sqrt(2/4)) \n  return(T_calc)  \n}) %>% round(2)\n\nNachdem wir eintausend Mal die Teststatistik unter der \\(H_0\\) berechnet haben, schauen wir uns die sortierten ersten 100 Werte der Teststatistik einmal an. Wir sehen, dass extrem kleine Teststatistiken bis sehr große Teststatistiken zufällig auftreten können, auch wenn die Mittelwerte für das Ziehen der Zahlen gleich waren.\n\nT_vec %>% magrittr::extract(1:100) %>% sort()  \n\n  [1] -5.19 -3.48 -3.29 -2.65 -2.40 -2.10 -1.48 -1.35 -1.30 -1.29 -1.29 -1.27\n [13] -1.24 -1.22 -1.10 -1.03 -1.02 -1.02 -0.91 -0.87 -0.84 -0.79 -0.79 -0.76\n [25] -0.76 -0.76 -0.73 -0.66 -0.63 -0.63 -0.62 -0.61 -0.57 -0.56 -0.55 -0.52\n [37] -0.52 -0.50 -0.48 -0.48 -0.43 -0.35 -0.33 -0.32 -0.26 -0.26 -0.22 -0.21\n [49] -0.20 -0.18 -0.17 -0.17 -0.14 -0.14 -0.12 -0.12 -0.10 -0.06  0.04  0.10\n [61]  0.14  0.16  0.17  0.31  0.34  0.41  0.45  0.50  0.50  0.51  0.55  0.63\n [73]  0.63  0.68  0.73  0.73  0.77  0.89  0.92  0.95  0.99  1.07  1.07  1.09\n [85]  1.12  1.16  1.22  1.33  1.33  1.76  2.11  2.16  2.51  2.79  2.87  3.24\n [97]  3.48  3.56  3.60  6.56\n\n\nUnsere berechnete Teststatistik war \\(T_{calc} = 3.47\\). Wenn wir diese Zahl mit den ersten einhundert, sortierten Teststatistiken vergleichen, dann sehen wir, dass nur 4 von 100 Zahlen größer sind als unsere berechnete Teststatistik. Wir beobachten also sehr seltene Daten wie in Tabelle 19.2, wenn wir davon ausgehen, dass kein Unterschied zwischen der Sprungweite der Hunde- und Katzenflöhe vorliegt.\nIn Abbildung 19.2 sehen wir die Verteilung der berechneten eintausend Verteilungen nochmal als ein Histogramm dargestellt. Wiederum sehen wir, dass unsere berechnete Teststatistik - dargestellt als rote Linie - sehr weit rechts am Rand der Verteilung liegt.\n\nggplot(as_tibble(T_vec), aes(x = value)) +\n  theme_bw() +\n  labs(x = \"Teststatistik\", y = \"Anzahl\") +\n  geom_histogram() +\n  geom_vline(xintercept = 3.47, color = \"red\")\n\n\n\nAbbildung 19.2— Histogramm der 1000 gerechneten Teststaistiken \\(T_{calc}\\), wenn die \\(H_0\\) war wäre und somit kein Unterschied zwischen den Mittelwerten der Sprungweiten der Hunde- und Katzenflöhe vorliegen würde.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen. Die Aussage ist statistisch falsch. Wir können im frequentistischen Hypothesentesten keine Aussage über die \\(H_A\\) treffen. Im Sinne der Anwendbarkeit soll es hier so stehen bleiben.\nNun ist es leider so, dass jeder statistische Test seine eigene Teststatistik \\(T\\) hat. Daher ist es etwas mühselig sich immer neue und andere kritische Werte für jeden Test zu merken. Es hat sich daher eingebürgert, sich nicht die Teststatistik für die Testentscheidung gegen die Nullhypothese zu nutzen sondern den \\(p\\)-Wert. Den \\(p\\)-Wert wollen wir uns in dem folgenden Abschnitt anschauen.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik\n\n\n\nBei der Entscheidung mit der Teststatistik müssen wir zwei Fälle unterschieden.\n\nBei einem t-Test und einem \\(\\mathcal{X}^2\\)-Test gilt, wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nBei einem Wilcoxon-Mann-Whitney-Test gilt, wenn \\(T_{calc} < T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\n\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.\n\n\n\n19.2.2 … anhand dem p-Wert\n\n\n\n\n\n\nPrinzip des statistischen Testens II - Der p-Wert\n\n\n\nDu findest auf YouTube Prinzip des statistischen Testens II - Der p-Wert als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nIn dem vorherigen Abschnitt haben wir gelernt, wie wir zu einer Entscheidung gegen die Nullhypothese anhand der Teststatistik kommen. Wir haben einen kritischen Wert \\(T_{\\alpha = 5\\%}\\) definiert bei dem rechts von dem Wert 5% der Werte liegen. Anstatt nun den berechneten Wert \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) zu vergleichen, vergleichen wir jetzt die Flächen rechts von den jeweiligen Werten. Wir machen es uns an dieser Stelle etwas einfacher, denn wir nutzen immer den absoluten Wert der Teststatistik.\nWir schreiben \\(\\boldsymbol{Pr}\\) und meinen damit eine Wahrscheinlichkeit (eng. probability). Häufig wird auch nur das \\(P\\) verwendet, aber dann kommen wir wieder mit anderen Konzepten in die Quere.\nIn Abbildung 19.1 sind die Flächen auch eingetragen. Da die gesamte Fläche unter der t-Verteilung mit \\(A = 1\\) ist, können wir die Flächen auch als Wahrscheinlichkeiten lesen. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert genannt. Die gesamte Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%. Wir können also die Teststatistiken oder den p-Wert mit dem \\(\\alpha\\)-Niveau von 5% vergleichen.\n\n\nTabelle 19.3— Zusammenhang zwischen der Teststatistik \\(T\\) und der Fläche \\(A\\) rechts von der Teststatistik. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T|H_0)\\) oder \\(p\\)-Wert genannt. Die Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%.\n\nTeststatistik \\(T\\)\n\nFläche \\(A\\)\n\n\n\n\n\\(T_{calc}\\)\n\n\\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert\n\n\n\\(T_{\\alpha = 5\\%}\\)\n\\(\\alpha\\)\n\n\n\n\nIn der folgenden Abbildung 19.3 ist dann nochmal der Zusammenhang aus der Tabelle als eine Abbildung visualisiert. Mit dem \\(p\\)-Wert entscheiden wir anhand von Flächen. Wir schauen uns in diesem Fall die beiden Seiten der Testverteilung mit jeweils \\(T_{\\alpha = 2.5\\%}\\) für \\(-T_K\\) und \\(T_K\\) an und vergleichen die Flächen rechts neben der berechneten Teststatistik \\(T_{calc}\\).\n\n\n\nAbbildung 19.3— Die Flächen links und rechts von \\(T_{\\alpha = 2.5\\%}\\) nochmal separat dargestellt. Wir vergleichen bei der Entscheidung mit dem \\(p\\)-Wert nicht die berechnete Teststatistik \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) sondern die Flächen rechts von den jeweiligen Teststatistiken mit \\(A_K = 5\\%\\) und \\(A_{calc}\\) als den \\(p\\)-Wert. An dem Flächenvergleich machen wir dann die Testentscheidung fest.\n\n\n\nDer p-Wert oder \\(Pr(T|H_0)\\) ist eine Wahrscheinlichkeit. Eine Wahrscheinlichkeit kann die Zahlen von 0 bis 1 annehmen. Dabei sind die Grenzen einfach zu definieren. Eine Wahrscheinlichkeit von \\(Pr(A) = 0\\) bedeutet, dass das Ereignis A nicht auftritt; eine Wahrscheinlichkeit von \\(Pr(A) = 1\\) bedeutet, dass das Ereignis A eintritt. Der Zahlenraum dazwischen stellt jeden von uns schon vor große Herausforderungen. Der Unterschied zwischen 40% und 60% für den Eintritt des Ereignisses A sind nicht so klar zu definieren, wie du auf den ersten Blick meinen magst.\nEin frequentistischer Hypothesentest beantwortet die Frage, mit welcher Wahrscheinlichkeit \\(Pr\\) die Teststatistik \\(T\\) aus dem Experiment mit den Daten \\(D\\) zu beobachten wären, wenn es keinen Effekt gäbe (\\(H_0\\) ist wahr).\nLikelihood heißt Plausibilität und Probability heißt Wahrscheinlichkeit.\nIn anderen Büchern liest man an dieser Stelle auch gerne etwas über die Likelihood, nicht so sehr in deutschen Büchern, schon aber in englischen Veröffentlichungen. Im Englischen gibt es die Begrifflichkeiten einer Likelihood und einer Probability. Meist wird beides ins Deutsche ungenau mit Wahrscheinlichkeit übersetzt oder wir nutzen einfach Likelihood. Was aber auch nicht so recht weiterhilft, wenn wir ein Wort mit dem gleichen Wort übersetzen. Es handelt sich hierbei aber um zwei unterschiedliche Konzepte. Deshalb Übersetzen wir Likelihood mit Plausibilität und Probability mit Wahrscheinlichkeit.\nIm Folgenden berechnen wir den \\(p\\)-Wert in R mit der Funktion t.test(). Mehr dazu im Kapitel 22, wo wir den t-Test und deren Anwendung im Detail besprechen. Hier fällt der \\(p\\)-Wert etwas aus den Himmel. Wir wollen aber nicht per Hand Flächen unter einer Kurve berechnen sondern nutzen für die Berechnung von \\(p\\)-Werten statistische Tests in R.\n\n\n# A tibble: 1 x 2\n  statistic p.value\n      <dbl>   <dbl>\n1      3.47  0.0133\n\n\nWir sagen, dass wir ein signifikantes Ergebnis haben, wenn der \\(p\\)-Wert kleiner ist als die Signifikanzschwelle \\(\\alpha\\) von 5%.\nWir erhalten einen \\(p\\)-Wert von 0.013 und vergleichen diesen Wert zu einem \\(\\alpha\\) von 5%. Ist der \\(p\\)-Wert kleiner als der \\(\\alpha\\)-Wert von 5%, dann können wir die Nullhypothese ablehnen. Da 0.013 kleiner ist als 0.05 können wir die Nullhypothese und damit die Gleichheit der mittleren Sprungweiten in [cm] ablehnen. Wir sagen, dass wir ein signifikantes Ergebnis vorliegen haben.\n\n\n\n\n\n\nEntscheidung mit dem p-Wert\n\n\n\nWenn der p-Wert \\(\\leq \\alpha\\) dann wird die Nullhypothese (H\\(_0\\)) abgelehnt. Das Signifikanzniveau \\(\\alpha\\) wird als Kulturkonstante auf 5% oder 0.05 gesetzt. Die Nullhypothese (H\\(_0\\)) kann auch Gleichheitshypothese gesehen werden. Wenn die H\\(_0\\) gilt, liegt kein Unterschied zwischen z.B. den Behandlungen vor.\n\n\n\n19.2.3 … anhand des 95% Konfidenzintervall\nEin statistischer Test der eine Teststatistik \\(T\\) berechnet liefert auch immer einen \\(p\\)-Wert. Nicht alle statistischen Tests ermöglichen es ein 95% Konfidenzintervall zu berechnen. Abbildung 19.4 zeigt ein 95% Konfidenzintervall.\n\n\nAbbildung 19.4— Ein 95% Konfidenzintervall. Der Punkt in der Mitte entspricht dem Unterschied oder Effekt \\(\\Delta\\).\n\n\nMit p-Werten haben wir Wahrscheinlichkeitsaussagen und damit über die Signifikanz. Damit haben wir noch keine Aussage über die Relevanz des beobachtenten Effekts.\nMit der Teststatistik \\(T\\) und dem damit verbundenen \\(p\\)-Wert haben wir uns Wahrscheinlichkeiten angeschaut und erhalten eine Wahrscheinlichkeitsaussage. Eine Wahrscheinlichkeitsaussage sagt aber nichts über den Effekt \\(\\Delta\\) aus. Also wie groß ist der mittlere Sprungunterschied zwischen Hunde- und Katzenflöhen.\nDie Idee von 95% Kondifenzintervallen ist es jetzt den Effekt mit der Wahrscheinlichkeitsaussage zusammenzubringen und beides in einer Visualisierung zu kombinieren. Im Folgenden sehen wir die vereinfachte Formel für das 95% Konfidenzintervall eines t-Tests.\n\\[\n\\left[\n(\\bar{y}_1-\\bar{y}_2) -\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}}; \\;\n(\\bar{y}_1-\\bar{y}_2) +\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}};\n\\right]\n\\]\nDie Formel ist ein wenig komplex, aber im Prinzip einfach, wenn du ein wenig die Formel auf dich wirken lässt. Der linke und der rechte Teil neben dem Semikolon sind fast gleich, bis auf das Plus- und Minuszeichen. Abbildung 19.5 visualisert die Formel einmal. Wir sehen Folgendes in der Formel und dann in der entsprechenden Abbildung:\n\n\n\\((\\bar{y}_{1}-\\bar{y}_{2})\\) ist der Effekt \\(\\Delta\\). In diesem Fall der Mittelwertsunterschied. Wir finden den Effekt als Punkt in der Mitte des Intervals.\n\n\\(T_{\\alpha = 5\\%} \\cdot \\frac {s}{\\sqrt{n}}\\) ist der Wert, der die Arme des Intervals bildet. Wir vereinfachen die Formel mit \\(s_p\\) für die gepoolte Standardabweichung und \\(n_g\\) für die Fallzahl der beiden Gruppen. Wir nehmen an das beide Gruppen die gleiche Fallzahl \\(n_1 = n_2\\) haben.\n\n\n\nAbbildung 19.5— Zusammenhang zwischen der vereinfachten Formel für das 95% Konfidenzintervall und der Visualisierung des 95% Konfidenzintervalls. Der Effektschätzer wird als Punkt in der Mitte des Intervalls dargestellt. Der Effektschäter \\(\\Delta\\) kann entweder ein Mittelwertsunterschied sein oder ein Anteilsunterschied. Bei einem Mittelwertsunterschied kann die Nullhypothese abgelehnt werden, wenn die 0 nicht im Konfidenzintervall ist; bei einem Anteilsunterschied wenn die 1 nicht im Konfidenzintervall ist. Die Arme werden länger oder kürzer je nachdem wie sich die statistischen Maßzahlen \\(s\\) und \\(n\\) verändern.\n\n\nDie Funktion factor() in R erlaubt es dir die Level eines Faktors zu sortieren und so festzulegen ob Level cat minus Level dog oder umgekehrt von R gerechnet wird.\nWir können eine biologische Relevanz definieren, dadurch das ein 95% Konfidenzintervall die Wahrscheinlichkeitsaussage über die Signifkanz, daher ob die Nullhypothese abgelehnt werden kann, mit dem Effekt zusammenbringt. Wo die Signifikanzschwelle klar definiert ist, hängt die Relevanzschwelle von der wissenschaftlichen Fragestellung und weiteren externen Faktoren ab. Die Signifikanzschwelle liegt bei 0, wenn wir Mittelwerte miteinander vergleichen und bei 1, wenn wir Anteile vergleichen. Abbildung 19.6 zeigt fünf 95% Konfidenzintervalle (a-e), die sich anhand der Signifikanz und Relevanz unterscheiden. Bei der Relevanz ist es wichtig zu wissen in welche Richtung der Effekt gehen soll. Erwarten wir einen positiven Effekt wenn wir die Differenz der beiden Gruppen bilden oder einen negativen Effekt?\n\n\nAbbildung 19.6— Verschiedene signifikante und relevante Konfidenzintervalle: (a) signifikant und nicht relevant; (b) nicht signifikant und nicht relevant; (c) signifikant und relevant; (d) signifikant und nicht relevant, der Effekt ist zu klein; (e) signifikant und potenziell relevant, Effekt zeigt in eine unerwartete Richtung gegeben der Relevanzschwelle.\n\n\nWir wollen uns nun einmal anschauen, wie sich ein 95% Konfidenzintervall berechnet. Wir nehmen dafür die vereinfachte Formel und setzen die berechneten statistischen Maßzahlen ein. In der Anwendung werden wir die Konfidenzintervalle nicht selber berechnen. Wenn ein statistisches Verfahren konfidenzintervalle berechnen kann, dann liefert die entsprechende Funktion in R das Konfidenzintervall.\nEs ergibt sich Folgende ausgefüllte, vereinfachte Formel für das 95% Konfidenzintervalls eines t-Tests für das Beispiel des Sprungweitenunterschieds [cm] zwischen Hunde- und Katzenflöhen.\n\n\n\n\n\n\nWir nutzen hier eine vereinfachte Formel für das Konfidenzintervall um das Konzept zu verstehen. Später berechnen wir das Konfidenzintervall in R.\n\\[\n\\left[\n(9.18-7.78) -\n2.78 \\cdot \\frac {0.57}{\\sqrt{4}}; \\;\n(9.18-7.78) +\n2.78 \\cdot \\frac {0.57}{\\sqrt{4}};\n\\right]\n\\]\nmit\n\n\n\\(\\bar{y}_{cat} = 9.18\\) dem Mittelwert für die Gruppe cat.\n\n\\(\\bar{y}_{dog} = 7.78\\) dem Mittelwert für die Gruppe dog.\n\n\\(T_{\\alpha = 5\\%} = 2.78\\) dem kritischen Wert.\n\n\\(s_p = 0.57\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.61 + 0.53}{2}\\).\n\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nLösen wir die Formel auf, so ergibt sich folgendes 95% Konfidenzintervall des Mittelwertsunterschiedes der Hunde- und Katzenflöhe.\n\\[[0.64; 2.16]\\]\nWir können sagen, dass mit 95% Wahrscheinlichkeit das Konfidenzintervall den wahren Effektunterschied \\(\\Delta\\) überdeckt. Oder etwas mehr in Prosa, dass wir eine Sprungweitenunterschied von 0.64 cm bis 2.16 cm zwischen Hunde- und Katzenflöhen erwarten würden.\nDie Entscheidung gegen die Nullhypothese bei einem Mittelwertsunterschied erfolgt bei einem 95% Konfidenzintervall danach ob die Null mit im Konfidenzintervall liegt oder nicht. In dem Interval \\([0.64; 2.16]\\) ist die Null nicht enthalten, also können wir die Nullhypothese ablehnen. Es ist mit einem Unterschied zwischen den mittleren Sprungweiten von Hunde- und Katzenflöhen auszugehen.\nIn unserem Beispiel, könnten wir die Relevanzschwelle für den mittleren Sprungweitenunterschied zwischen Hund- und Katzenflöhen auf 2 cm setzen. In dem Fall würden wir entscheiden, dass der mittlere Sprungweitenunterschied nicht relevant ist, da die 2 cm im Konfidenzintervall enthalten sind. Was wäre wenn wir die Relevanzschwelle auf 4 cm setzen? Dann wäre zwar die Relevanzschwelle nicht mehr im Konfidenzintervall, aber wir hätten Fall (d) in der Abbildung 19.6 vorliegen. Der Effekt ist einfach zu klein, dass der Effekt relevant sein könnte.\nWir können dann die 95% Konfidenzintervall des Mittelwertsunterschiedes der Hunde- und Katzenflöhe auch nochmal richtig in R berechnen. Wir haben ja oben eine einfachere Formel für die gepoolte Standardabweichung genutzt. Wenn wir also ganz genau rechnen wollen, dann sind die 95% Konfidenzintervall wie folgt. Wir nutzen auch hier die Funktion t.test(). Mehr dazu im Kapitel 22, wo wir den t-Test und deren Anwendung im Detail besprechen.\n\n\n# A tibble: 1 x 2\n  conf.low conf.high\n     <dbl>     <dbl>\n1    0.412      2.39\n\n\n\n\n\n\n\n\nEntscheidung mit dem 95% Konfidenzintervall\n\n\n\nBei der Entscheidung mit dem 95% Konfidenzinterval müssen wir zwei Fälle unterscheiden.\n\nEntweder schauen wir uns einen Mittelwertsunterschied (\\(\\Delta_{y_1-y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 0 im 95% Konfidenzinterval ist.\nOder wir schauen uns einen Anteilsunterschied (\\(\\Delta_{y_1/y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 1 im 95% Konfidenzinterval ist."
  },
  {
    "objectID": "stat-tests-basic.html#sec-delta-n-s",
    "href": "stat-tests-basic.html#sec-delta-n-s",
    "title": "19  Die Testentscheidung",
    "section": "\n19.3 Auswirkung des Effektes, der Streuung und der Fallzahl",
    "text": "19.3 Auswirkung des Effektes, der Streuung und der Fallzahl\nWir wollen einmal den Zusammenhang zwischen dem Effekt \\(\\Delta\\), der Streuung als Standardabweichung \\(s\\) und Fallzahl \\(n\\) uns näher anschauen. Wir können die Formel des t-Tests wie folgt vereinfachen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_1}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nFür die Betrachtung der Zusammenhänge wandeln wir \\(\\sqrt{2/n_g}\\) in \\(1/n\\) um. Dadurch wandert die Fallzahl \\(n\\) in den Zähler. Die Standardabweichung verallgemeinern wir zu \\(s\\) und damit allgemein zur Streuung. Abschließend betrachten wir \\(\\bar{y}_A-\\bar{y}_B\\) als den Effekt \\(\\Delta\\). Es ergibt sich folgende vereinfachte Formel.\n\\[\nT_{calc} = \\cfrac{\\Delta \\cdot n}{s}\n\\]\nWir können uns nun die Frage stellen, wie ändert sich die Teststatistik \\(T_{calc}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\) in den Daten. Die Tabelle 19.4 zeigt die Zusammenhänge auf. Die Aussagen in der Tabelle lassen sich generalisieren. So bedeutet eine steigende Fallzahl meist mehr signifikante Ergebnisse. Eine stiegende Streuung reduziert die Signifikanz eines Vergleichs. Ein Ansteigen des Effektes führt zu mehr signifikanten Ergebnissen. Ebenso verschiebt eine Veränderung des Effekt das 95% Konfidenzintervall, eine Erhöhung der Streuung macht das 95% Konfidenzintervall breiter, eine sinkende Streuung macht das 95% Konfidenzintervall schmaler. bei der Fallzahl verhält es sich umgekehrt. Eine Erhöhung der Fallzahl macht das 95% Konfidenzintervall schmaler und eine sinkende Fallzahl das Konfidenzintervall breiter.\n\n\n\nTabelle 19.4— Zusammenhang von der Teststatistik \\(T_{calc}\\) und dem p-Wert \\(Pr(\\geq T_{calc}|H_0)\\) sowie dem \\(KI_{1-\\alpha}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\n\n\\(\\Delta \\uparrow\\)\nsteigt\nsinkt\nverschoben\n\\(\\Delta \\downarrow\\)\nsinkt\nsteigt\nverschoben\n\n\n\\(s \\uparrow\\)\nsinkt\nsteigt\nbreiter\n\\(s \\downarrow\\)\nsteigt\nsinkt\nschmaler\n\n\n\\(n \\uparrow\\)\nsteigt\nsinkt\nschmaler\n\\(n \\downarrow\\)\nsinkt\nsteigt\nbreiter"
  },
  {
    "objectID": "stat-tests-anova.html",
    "href": "stat-tests-anova.html",
    "title": "23  Die ANOVA",
    "section": "",
    "text": "Version vom November 18, 2022 um 15:53:08\nDie ANOVA (eng. analysis of variance) ist wichtig. Was für ein schöner Satz um anzufangen. Wir brauchen die ANOVA aus mehreren Gründen. Die Hochzeiten der ANOVA sind eigentlich vorbei, wir haben in der Statistik für viele Fälle mittlerweile besser Werkzeuge, aber als Allrounder ist die ANOVA immer noch nutzbar. Wir werden immer wieder auf die ANOVA inhaltlich zurück kommen und in vielen Abschlussarbeiten wird die ANOVA immer noch als integraler Bestandteil genutzt.\nWofür und wann brauchen wir die ANOVA, wenn wir Daten auswerten?\nWir können die einfaktorielle ANOVA in folgender Form schreiben mit einem \\(f_1\\) für den Faktor. Damit haben wir einen bessere Übersicht. Wieder ist das \\(y\\) und der Faktor \\(f_1\\) jeweils eine separate Spalte in unserem Datensatz.\n\\[\ny \\sim f_1\n\\]\nSomit erklärt sich die zweifaktorielle ANOVA schon fast von alleine. Wir erweitern einfach das Modell um einen zweiten Faktor \\(f_2\\) und haben somit eine zweifaktorielle ANOVA vorliegen. Auch hier müssen wir beachten, dass das \\(y\\) und die Faktoren \\(f_1\\) und \\(f_2\\) jeweils eine separate Spalte in unserem Datensatz sind.\n\\[\ny \\sim f_1 + f_2\n\\]\nWir sehen also, dass die ANOVA zum einen alt ist, aber auch heute noch viel verwendet wird. Daher werden wir in diesem langem Kapitel uns einmal mit der ANOVA ausgiebig beschäftigen. Fangen wir also an, dieses großartige Schweizertaschenmesser der Statistik besser zu verstehen."
  },
  {
    "objectID": "stat-tests-anova.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-anova.html#genutzte-r-pakete-für-das-kapitel",
    "title": "23  Die ANOVA",
    "section": "\n23.1 Genutzte R Pakete für das Kapitel",
    "text": "23.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, effectsize, ggpubr, \n               see)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac1",
    "href": "stat-tests-anova.html#sec-fac1",
    "title": "23  Die ANOVA",
    "section": "\n23.2 Einfaktorielle ANOVA",
    "text": "23.2 Einfaktorielle ANOVA\n\n\n\n\n\n\nEinführung in die einfaktorielle ANOVA per Video\n\n\n\nDu findest auf YouTube die einfaktorielle ANOVA als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nDie einfaktorielle ANOVA ist die simpelste Form der ANOVA. Wir nutzen einen Faktor \\(x\\) mit mehr als zwei Leveln. Im Rahmen der einfaktoriellen ANOVA wollen wir uns auch die ANOVA theoretisch einmal anschauen. Danach wie die einfaktorielle ANOVA in R genutzt wird. Ebenso werden wir die einfaktorielle ANOVA visualisieren. Abschließend müssen wir uns noch überlegen, ob es einen Effektschätzer für die einfaktorielle ANOVA gibt.\n\n\n\n\n\n\nDie einfaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität über den Behandlungsfaktor \\(x\\). Daher alle Level von \\(x\\) sollen die gleiche Varianz haben.\nUnsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x\\) homogen in den Varianzen sind. Später mehr dazu, wenn wir beides nicht vorliegen haben…\n\n23.2.1 Daten für die einfaktorielle ANOVA\nWir wollen uns nun erstmal den einfachsten Fall anschauen mit einem simplen Datensatz. Wir nehmen ein normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Hätten wir nur zwei Level, dann hätten wir auch einen t-Test rechnen können.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac1_tbl <- read_csv2(\"data/flea_dog_cat_fox.csv\") %>%\n  select(animal, jump_length) %>% \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt fac1_tbl mit dem Datensatz in Tabelle 23.1 nochmal dargestellt.\n\n\n\n\nTabelle 23.1— Selektierter Datensatz für die einfaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\n\n\n\ndog\n5.7\n\n\ndog\n8.9\n\n\ndog\n11.8\n\n\ndog\n8.2\n\n\ndog\n5.6\n\n\ndog\n9.1\n\n\ndog\n7.6\n\n\ncat\n3.2\n\n\ncat\n2.2\n\n\ncat\n5.4\n\n\ncat\n4.1\n\n\ncat\n4.3\n\n\ncat\n7.9\n\n\ncat\n6.1\n\n\nfox\n7.7\n\n\nfox\n8.1\n\n\nfox\n9.1\n\n\nfox\n9.7\n\n\nfox\n10.6\n\n\nfox\n8.6\n\n\nfox\n10.3\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt fac1_tbl folgendes Modell für die spätere Analyse in R.\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der einfaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n23.2.2 Hypothesen für die einfaktorielle ANOVA\nDie ANOVA betrachtet die Mittelwerte und nutzt die Varianzen um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mittelwerte jedes Levels des Faktors animal gleich sind.\n\\[\nH_0: \\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Mittelwerten unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mittelwerte unterschieden oder eben nur ein Paar. Wenn eine ANOVA die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mittelwerte sich unterscheiden.\n\\[\n\\begin{aligned}\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nWir schauen uns jetzt einmal die ANOVA theoretisch an bevor wir uns mit der Anwendung der ANOVA in R beschäftigen.\n\n23.2.3 Einfaktoriellen ANOVA theoretisch\nKommen wir zurück zu den Daten in Tabelle 23.1. Wenn wir die ANOVA per Hand rechnen wollen, dann ist nicht das Long Format die beste Wahl sondern das Wide Format. Wir haben ein balanciertes Design vorliegen, dass heißt in jeder Level sind die gleiche Anzahl Beobachtungen. Wir schauen uns jeweils sieben Flöhe von jeder Tierart an. Für eine ANOVA ist aber ein balanciertes Design nicht notwendig, wir können auch mit ungleichen Gruppengrößen eine ANOVA rechnen.\nStatt einer einfaktoriellen ANOVA könnten wir auch gleich einen pairwise.t.test()rechnen. Historisch betrachtet ist die einfaktorielle ANOVA die Visualisierung des paarweisen t-Tests.\nEine einfaktorielle ANOVA macht eigentlich keinen großen Sinn, wenn wir anschließend sowieso paarweise Vergleich, wie in Kapitel 31 beschrieben, rechnen. Aus der Historie stellte sich die Frage, ob es sich lohnt die ganze Arbeit für die paarweisen t-Tests per Hand zu rechnen. Daher wurde die ANOVA davor geschaltet. War die ANOVA nicht signifikant, dann konnte man sich dann auch die Rechnerei für die paarweisen t-Tests sparen.\nIn Tabelle 23.2 sehen wir die Daten einmal als Wide Format dargestellt.\n\n\nTabelle 23.2— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal.\n\nj\ndog\ncat\nfox\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\n\nWir können jetzt für jedes der Level den Mittelwert über all \\(j=7\\) Beobachtungen berechnen.\n\\[\n\\begin{aligned}\n\\bar{y}_{dog} &= 8.13 \\\\\n\\bar{y}_{cat} &= 4.74 \\\\\n\\bar{y}_{fox} &= 9.16 \\\\\n\\end{aligned}\n\\]\nWir tun jetzt für einen Moment so, als gebe es den Faktor animal nicht in den Daten und schauen uns die Verteilung der einzelnen Beobachtungen in Abbildung 23.1 (a) einmal an. Wir sehen das sich die Beobachtungen von ca. 2.2cm bis 11 cm streuen. Woher kommt nun diese Streuung bzw. Varianz? Was ist die Quelle der Varianz? In Abbildung 23.1 (b) haben wir die Punkte einmal nach dem Faktor animal eingefärbt. Wir sehen, dass die blauen Beobachtungen eher weitere Sprunglängen haben als die grünen Beobachtungen. Wir gruppieren die Beobachtungen in Abbildung 23.1 (c) nach dem Faktor animal und sehen, dass ein Teil der Varianz der Daten von dem Faktor animal ausgelöst wird.\n\n\n\n\n\n(a) Die Sprungweite in [cm] ohne den Faktor animal betrachtet.\n\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal eingefärbt.\n\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal eingefärbt und gruppiert.\n\n\n\n\nAbbildung 23.1— Die Spungweite in [cm] in Abhängigkeit von dem Faktor animal dargestellt.\n\n\nGehen wir einen Schritt weiter und zeichnen einmal das globale Mittel in die Abbildung 23.2 (a) von \\(\\bar{y}_{..} = 7.34\\) und lassen die Beobachtungen gruppiert nach dem Faktor animal. Wir sehen, dass die Level des Faktors animal um das globale Mittel streuen. Was ja auch bei einem Mittelwert zu erwarten ist. Wir können jetzt in Abbildung 23.2 (b) die lokalen Mittel für die einzelnen Level dog, catund fox ergänzen. Und abschließend in Abbildung 23.2 (c) die Abweichungen \\(\\\\beta_i\\) zwischen dem globalen Mittel \\(\\bar{y}_{..} = 7.34\\) und den einzelnen lokalen Mittel berechnen. Die Summe der Abweichungen \\(\\\\beta_i\\) ist \\(0.79 + (-2.6) + 1.81 \\approx 0\\). Das ist auch zu erwarten, den das globale Mittel muss ja per Definition als Mittelwert gleich großen Abstand “nach oben” wie “nach unten” haben.\n\n\n\n\n\n(a) Die Sprungweite in [cm] mit den Faktor animal gruppiert und das globale Mittel \\(\\bar{y}_{..} = 7.34\\) ergänzt.\n\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die lokalen Mittel \\(\\bar{y}_{i.}\\) für jedes Level ergänzt.\n\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die Abweichungen \\(\\beta_i\\) ergänzt.\n\n\n\n\nAbbildung 23.2— Dotplot der Spungweite in [cm] in Abhängigkeit von dem Faktor animal.\n\n\nWir tragen die Werte der lokalen Mittlwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\) noch in die Tabelle 23.3 ein. Wir sehen in diesem Beispiel warum das Wide Format besser zum Verstehen der ANOVA ist, weil wir ja die lokalen Mittelwerte und die Abweichungen per Hand berechnen. Da wir in der Anwendung aber nie die ANOVA per Hand rechnen, liegen unsere Daten immer in R als Long Format vor. Es handelt sich hier nur um die Veranschaulichung des Konzepts der ANOVA.\n\n\nTabelle 23.3— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal. Wir ergänzen die lokalen Mittlwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\).\n\nj\ndog\ncat\nfox\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\\(\\bar{y}_{i.}\\)\n\\(8.13\\)\n\\(4.74\\)\n\\(9.16\\)\n\n\n\\(\\beta_i\\)\n\\(-2.6\\)\n\\(0.79\\)\n\\(1.81\\)\n\n\n\n\nWie kriegen wir nun die ANOVA rechnerisch auf die Straße? Schauen wir uns dazu einmal die Abbildung 23.3 an. Auf der linken Seiten sehen wir vier Gruppen, die keinen Effekt haben. Die Gruppen liegen alle auf der gleichen Höhe. Es ist mit keinem Unterschied zwischen den Gruppen zu rechnen. Alle Gruppenmittel liegen auf dem globalen Mittel. Die Abweichungen der einzelnen Gruppenmittel zum globalen Mittel ist damit gleich null. Auf der rechten Seite sehen wir vier Gruppen mit einem Effekt. Die Gruppen unterscheiden sich in ihren Gruppenmitteln. Dadurch unterscheide sich aber auch die Gruppenmittel von dem globalen Mittel.\n\n\n\n\n\n(a) Kein Effekt\n\n\n\n\n\n\n(b) Leichter bis mittlerer Effekt\n\n\n\n\nAbbildung 23.3— Darstellung von keinem Effekt und leichtem bis mittleren Effekt in einer einfaktoriellen ANOVA mit einem Faktor mit vier Leveln A - D.\n\n\nWir können daher wie in Tabelle 23.4 geschrieben die Funktionsweise der ANOVA zusammenfassen. Wir vergleichen die Mittelwerte indem wir die Varianzen nutzen.\n\n\nTabelle 23.4— Zusammenfassung der ANOVA Funktionsweise.\n\n\n\n\n\n\nAll level means are equal.\n=\nThe differences between level means and the total mean are small.\n\n\n\nNun kommen wir zum eigentlichen Schwenk und warum eigentlich die ANOVA meist etwas verwirrt. Wir wollen eine Aussage über die Mittelwerte machen. Die Nullhypothese lautet, dass alle Mittelwerte gleich sind. Wie wir in Tabelle 23.4 sagen, heißt alle Mittelwerte gleich auch, dass die Abweichungen von den Gruppenmitteln zum globalen Mittel klein ist.\nWie weit die Gruppenmittel von dem globalen Mittel weg sind, dazu nutzt die ANOVA die Varianz. Die ANOVA vergleicht somit\n\ndie Varianz der einzelnen Mittelwerte der (Gruppen)Level zum globalen Mittel (eng. variability between levels)\nund die Varianz der Beobachtungen zu den einzelnen Mittelwerten der Level (eng. variability within one level)\n\nDie sum of squares sind nichts anderes als die Varianz. Wir nennen das hier nur einmal anders…\nWir berechnen also wie die Beobachtungen jeweils um das globale Mittel streuen (\\(SS_{total}\\)), die einzelnen Beobachtungen um die einzelnen Gruppenmittel \\(SS_{error}\\) und die Streuung der Gruppenmittel um das globale Mittel (\\(SS_{animal}\\)). Wir nennen die Streuung Abstandquadrate (eng. sum of squares) und damit sind die Sum of Square \\((SS)\\) nichts anderes als die Varianz. Die Tabelle 23.5 zeigt die Berechnung des Anteils jeder einzelnen Beobachtung an den jeweiligen Sum of Squares.\n\n\n\nTabelle 23.5— Berechnung der \\(SS_{animal}\\), \\(SS_{error}\\) und \\(SS_{total}\\) anhand der einzelnen gemessenen Werte \\(y\\) für durch die jeweiligen Gruppenmittel \\(\\bar{y}_{i.}\\) und dem globalen Mittel \\(\\bar{y}_{..}\\) über alle Beobachtungen\n\n\n\n\n\n\n\n\n\nanimal (x)\njump_length (y)\n\\(\\boldsymbol{\\bar{y}_{i.}}\\)\nSS\\(_{\\boldsymbol{animal}}\\)\n\nSS\\(_{\\boldsymbol{error}}\\)\n\nSS\\(_{\\boldsymbol{total}}\\)\n\n\n\n\ndog\n\\(5.7\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.7 - 8.13)^2 = 5.90\\)\n\\((5.7 - 7.34)^2 = 2.69\\)\n\n\ndog\n\\(8.9\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.9 - 8.13)^2 = 0.59\\)\n\\((8.9 - 7.34)^2 = 2.43\\)\n\n\ndog\n\\(11.8\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((11.8 - 8.13)^2 = 13.47\\)\n\\((11.8 - 7.34)^2 = 19.89\\)\n\n\ndog\n\\(8.2\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.2 - 8.13)^2 = 0.00\\)\n\\((8.2 - 7.34)^2 = 0.74\\)\n\n\ndog\n\\(5.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.6 - 8.13)^2 = 6.40\\)\n\\((5.6 - 7.34)^2 = 3.03\\)\n\n\ndog\n\\(9.1\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((9.1 - 8.13)^2 = 0.94\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\ndog\n\\(7.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((7.6 - 8.13)^2 = 0.28\\)\n\\((7.6 - 7.34)^2 = 0.07\\)\n\n\ncat\n\\(3.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((3.2 - 4.74)^2 = 2.37\\)\n\\((3.2 - 7.34)^2 = 17.14\\)\n\n\ncat\n\\(2.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((2.2 - 4.74)^2 = 6.45\\)\n\\((2.2 - 7.34)^2 = 26.42\\)\n\n\ncat\n\\(5.4\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((5.4 - 4.74)^2 = 0.44\\)\n\\((5.4 - 7.34)^2 = 3.76\\)\n\n\ncat\n\\(4.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.1 - 4.74)^2 = 0.41\\)\n\\((4.1 - 7.34)^2 = 10.50\\)\n\n\ncat\n\\(4.3\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.3 - 4.74)^2 = 0.19\\)\n\\((4.3 - 7.34)^2 = 9.24\\)\n\n\ncat\n\\(7.9\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((7.9 - 4.74)^2 = 9.99\\)\n\\((7.9 - 7.34)^2 = 0.31\\)\n\n\ncat\n\\(6.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((6.1 - 4.74)^2 = 1.85\\)\n\\((6.1 - 7.34)^2 = 1.54\\)\n\n\nfox\n\\(7.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((7.7 - 9.16)^2 = 2.13\\)\n\\((7.7 - 7.34)^2 = 0.13\\)\n\n\nfox\n\\(8.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.1 - 9.16)^2 = 1.12\\)\n\\((8.1 - 7.34)^2 = 0.58\\)\n\n\nfox\n\\(9.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.1 - 9.16)^2 = 0.00\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\nfox\n\\(9.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.7 - 9.16)^2 = 0.29\\)\n\\((9.7 - 7.34)^2 = 5.57\\)\n\n\nfox\n\\(10.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.6 - 9.16)^2 = 2.07\\)\n\\((10.6 - 7.34)^2 = 10.63\\)\n\n\nfox\n\\(8.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.6 - 9.16)^2 = 0.31\\)\n\\((8.6 - 7.34)^2 = 1.59\\)\n\n\nfox\n\\(10.3\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.3 - 9.16)^2 = 1.30\\)\n\\((10.3 - 7.34)^2 = 8.76\\)\n\n\n\n\n\n\\(74.68\\)\n\\(56.53\\)\n\\(131.21\\)\n\n\n\n\n\nDie ANOVA wird deshalb auch Varianzzerlegung genannt, da die ANOVA versucht den Abstand der Beobachtungen auf die Variablen im Modell zu zerlegen. Also wie viel der Streuung von den Beobachtungen kann von dem Faktor animal erklärt werden? Genau der Abstand von den Gruppenmitteln zu dem globalen Mittelwert.\nDu kannst dir das ungefähr als eine Reise von globalen Mittelwert zu der einzelnen Beobachtung vorstellen. Nehmen wir als Beispiel die kleinste Sprungweite eines Katzenflohs von 2.2 cm und visualisieren wir uns die Reise wie in Abbildung 23.4 zu sehen. Wie kommen wir jetzt numerisch vom globalen Mittel mit \\(7.34\\) zu der Beobachtung? Wir können zum einen den direkten Abstand mit \\(2.2 - 7.34\\) gleich \\(-5.14\\) cm berechnen. Das wäre der total Abstand. Wie sieht es nun aus, wenn wir das Gruppenmittel mit beachten? In dem Fall gehen wir vom globalen Mittel zum Gruppenmittel cat mit \\(\\bar{y}_{cat} - \\bar{y}_{..} = 4.74 -7.34\\) gleich \\(\\beta_{cat} = -2.6\\) cm. Jetzt sind wir aber noch nicht bei der Beobachtung. Wir haben noch einen Rest von \\(y_{cat,2} - \\bar{y}_{cat} = 2.2 - 4.74\\) gleich \\(\\epsilon_{cat, 2} = -2.54\\) cm, die wir noch zurücklegen müssen. Das heißt, wir können einen Teil der Strecke mit dem Gruppenmittelwert erklären. Oder anders herum, wir können die Strecke vom globalen Mittelwert zu der Beobachtung in einen Teil für das Gruppenmittel und einen unerklärten Rest zerlegen.\n\n\n\nAbbildung 23.4— Visualisierung der Varianzzerlegung des Weges vom globalen Mittel zu der einzelnen Beoabchtung. Um zu einer einzelnen Beobachtung zu kommen legen wir den Weg vom globalen Mittelwert über den Abstand vom globalen Mittel zum Gruppenmittel \\(\\beta\\) zurück. Dann fehlt noch der Rest oder Fehler oder Residuum \\(\\epsilon\\).\n\n\n\nWir rechnen also eine ganze Menge an Abständen und quadrieren dann diese Abstände zu den Sum of Squares. Oder eben der Varianz. Dann fragen wir uns, ob der Faktor in unserem Modell einen Teil der Abstände erklären kann. Wir bauen uns dafür eine ANOVA Tabelle. Tabelle 23.6 zeigt eine theoretische, einfaktorielle ANOVA Tabelle. Wir berechnen zuerst die Abstände als \\(SS\\). Nun ist es aber so, dass wenn wir in einer Gruppe viele Level und/oder Beobachtungen haben, wir auch größere Sum of Squares bekommen. Wir müssen also die Sum of Squares in mittlere Abweichungsquadrate (eng. mean squares) mitteln. Abschließend können wir die F Statistik berechnen, indem wir die \\(MS\\) des Faktors durch die \\(MS\\) des Fehlers teilen. Das Verhältnis von erklärter Varianz vom Faktor zu dem unerklärten Rest.\n\n\n\nTabelle 23.6— Einfaktorielle ANOVA in der theoretischen Darstellung. Die sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(k-1\\)\n\\(SS_{animal} = \\sum_{i=1}^{k}n_i(\\bar{y}_{i.} - \\bar{y}_{..})^2\\)\n\\(MS_{animal} = \\cfrac{SS_{animal}}{k-1}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nerror\n\\(n-k\\)\n\\(SS_{error} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{i.})^2\\)\n\\(MS_{error} = \\cfrac{SS_{error}}{N-k}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{..})^2\\)\n\n\n\n\n\n\n\nWir füllen jetzt die Tabelle 23.7 einmal mit den Werten aus. Nachdem wir das getan haben oder aber die Tabelle in R ausgegeben bekommen haben, können wir die Zahlen interpretieren.\n\n\n\nTabelle 23.7— Einfaktorielle ANOVA mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dann direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 74.68\\)\n\\(MS_{animal} = \\cfrac{74.68}{3-1} = 37.34\\)\n\\(F_{calc} = \\cfrac{37.34}{3.14} = 11.89\\)\n\n\nerror\n\\(21-3\\)\n\\(SS_{error} = 56.53\\)\n\\(MS_{error} = \\cfrac{56.53}{18} = 3.14\\)\n\n\n\ntotal\n\\(21-1\\)\n\\(SS_{total} = 131.21\\)\n\n\n\n\n\n\n\nZu erst ist die berechnete F Statistik \\(F_{calc}\\) von Interesse. Wir haben hier eine \\(F_{calc}\\) von 11.89. Wir vergleichen wieder die berechnete F Statistik mit einem kritischen Wert. Der kritische F Wert \\(F_{\\alpha = 5\\%}\\) lautet für die einfaktorielle ANOVA in diesem konkreten Beispiel mit \\(F_{\\alpha = 5\\%} = 3.55\\). Die Entscheidungsregel nach der F Teststatistik lautet, die \\(H_0\\) abzulehnen, wenn \\(F_{calc} > F_{\\alpha = 5\\%}\\).\nWir können also die Nullhypothese \\(H_0\\) in unserem Beispiel ablehnen. Es liegt ein signifikanter Unterschied zwischen den Tiergruppen vor. Mindestens ein Mittelwertsunterschied in den Sprungweiten liegt vor.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(F_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(F_{calc}\\) gilt, wenn \\(F_{calc} \\geq F_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr. Wir nutzen in der praktischen Anwendung den \\(p\\)-Wert.\n\n\n\n23.2.4 Einfaktoriellen ANOVA in R\nUm eine ANOVA zu rechnen nutzen wir zuerst die Funktion lm(), warum das so ist kannst du im Kapitel 32 nachlesen. Du brauchst das Wissen aber hier nicht unbedingt.\nWir rechnen keine ANOVA per Hand sondern nutzen R. Dazu müssen wir als erstes das Modell definieren. Das ist im Falle der einfaktoriellen ANOVA relativ einfach. Wir haben unseren Datensatz fac1_tbl mit einer kontinuierlichen Variable jump_lemgth als \\(y\\) vorliegen sowie einen Faktor animal mit mehr als zwei Leveln als \\(x\\). Wir definieren das Modell in R in der Form jump_length ~ animal. Um das Modell zu rechnen nutzen wir die Funktion lm() - die Abkürzung für linear model. Danach pipen wir die Ausgabe vom lm() direkt in die Funktion anova(). Die Funktion anova() berechnet uns dann die eigentliche einfaktorielle ANOVA. Wir speichern die Ausgabe der ANOVA in fit_1. Schauen wir uns die ANOVA Ausgabe einmal an.\n\nfit_1 <-  lm(jump_length ~ animal, data = fac1_tbl) %>% \n  anova\n\nfit_1\n\nAnalysis of Variance Table\n\nResponse: jump_length\n          Df  Sum Sq Mean Sq F value     Pr(>F)    \nanimal     2 74.6829 37.3414 11.8904 0.00051129 ***\nResiduals 18 56.5286  3.1405                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten die Information was wir gerechnet haben, eine Varianzanalyse. Darunter steht, was das \\(y\\) war nämlich die jump_length. Wir erhalten eine Zeile für den Faktor animal und damit die \\(SS_{animal}\\) und eine Zeile für den Fehler und damit den \\(SS_{error}\\). In R heißen die \\(SS_{error}\\) dann Residuals. Die Zeile für die \\(SS_{total}\\) fehlt.\nNeben der berechneten F Statistik \\(F_{calc}\\) von \\(11.89\\) erhalten wir auch den p-Wert mit \\(0.005\\). Wir ignorieren die F Statistik, da wir in der Anwendung nur den p-Wert berücksichtigen. Die Entscheidung gegen die Nulhypothese lautet, dass wenn der p-Wert kleiner ist als das Signifkanzniveau \\(\\alpha\\) von 5% wir die Nullhypothese ablehnen.\nWir haben hier ein signifikantes Ergebnis vorliegen. Mindestens ein Gruppenmittelerstunterschied ist signifikant. Abbildung 23.5 zeigt nochmal die Daten fac1_tbl als Boxplot. Wir überprüfen visuell, ob das Ergebnis der ANOVA stimmen kann. Ja, die Boxplots und das Ergebnis der ANOVA stimmen überein. Die Boxplots liegen nicht alle auf einer Ebene, so dass hier auch ein signifikanter Unterschied zu erwarten war.\n\n\n\n\nAbbildung 23.5— Boxplot der Sprungweiten [cm] von Hunden-, Katzen- und Fuchsflöhen.\n\n\n\n\nAbschließend können wir noch die Funktion eta_squared() aus dem R Paket effectsize nutzen um einen Effektschätzer für die einfaktorielle ANOVA zu berechnen. Wir können mit \\(\\eta^2\\) abschätzen, welchen Anteil der Faktor animal an der gesamten Varianz erklärt.\n\nfit_1 %>% eta_squared\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nanimal    | 0.57 | [0.27, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDas \\(\\eta^2\\) können wir auch einfach händisch berechnen.\n\\[\n\\eta^2 = \\cfrac{SS_{animal}}{SS_{total}} = \\cfrac{74.68}{131.21} = 0.57 = 57\\%\n\\]\nWir haben nun die Information, das 57% der Varianz der Beobachtungen durch den Faktor animal rklärt wird. Je nach Anwendungsgebiet kann die Relevanz sehr stark variieren. Im Bereich der Züchtung mögen erklärte Varianzen von unter 10% noch sehr relevant sein. Im Bereich des Feldexperiments erwarten wir schon höhere Werte für \\(\\eta^2\\). Immerhin sollte ja unsere Behandlung maßgeblich für die z.B. größeren oder kleineren Pflanzen gesorgt haben."
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac2",
    "href": "stat-tests-anova.html#sec-fac2",
    "title": "23  Die ANOVA",
    "section": "\n23.3 Zweifaktorielle ANOVA",
    "text": "23.3 Zweifaktorielle ANOVA\n\n\n\n\n\n\nEinführung in die zweifaktorielle ANOVA per Video\n\n\n\nwork in progress\nDu findest auf YouTube die zweifaktorielle ANOVA als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\n\n\n\n\n\n\nDie zweifaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität jeweils separat über beide Behandlungsfaktor \\(x_1\\) und \\(x_2\\). Daher alle Level von \\(x_1\\) sollen die gleiche Varianz haben. Ebenso sollen alle Level von \\(x_2\\) die gleiche Varianz haben.\nUnsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x_1\\) und \\(x_2\\) jeweils für sich homogen in den Varianzen sind. Später mehr dazu, wenn wir beides nicht vorliegen haben…\nDie zweifaktorielle ANOVA ist eine wunderbare Methode um herauszufinden, ob zwei Faktoren einen Einfluss auf ein normalverteiltes \\(y\\) haben. Die Stärke der zweifaktoriellen ANOVA ist hierbei, dass die ANOVA beide Effekte der Faktoren auf das \\(y\\) simultan modelliert. Darüber hinaus können wir auch noch einen Interaktionsterm mit in das Modell aufnehmen um zu schauen, ob die beiden Faktoren untereinander auch interagieren. Somit haben wir mit der zweifaktoriellen ANOVA die Auswertungsmehode für ein randomiziertes Blockdesign vorliegen.\n\n23.3.1 Daten für die zweifaktorielle ANOVA\nWir wollen uns nun einen etwas komplexes Modell anschauen mit einem etwas komplizierteren Datensatz flea_dog_cat_fox_site.csv. Wir brauchen hierfür ein normalverteiltes \\(y\\) und sowie zwei Faktoren. Das macht auch soweit Sinn, denn wir wollen ja auch eine zweifaktorielle ANOVA rechnen.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal sowie die Spalte site als \\(x\\). Danach müssen wir noch die Variable animal sowie die Variable site in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac2_tbl <- read_csv2(\"data/flea_dog_cat_fox_site.csv\") %>% \n  select(animal, site, jump_length) %>% \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 23.8 nochmal dargestellt.\n\n\n\n\nTabelle 23.8— Selektierter Datensatz für die zweifaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\nanimal\nsite\njump_length\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.10\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\ncat\ncity\n14.91\n\n\n\n\n\n\nDie Beispieldaten sind in Abbildung 23.6 abgebildet. Wir sehen auf der x-Achse den Faktor animal mit den drei Leveln dog, cat und fox. Jeder dieser Faktorlevel hat nochmal einen Faktor in sich. Dieser Faktor lautet site und stellt dar, wo die Flöhe gesammelt wurden. Die vier Level des Faktors site sind city, smalltown, village und field.\n\n\n\n\nAbbildung 23.6— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nWir bauen dann mit den beiden Variablen bzw. Faktoren animal und site aus dem Objekt fac2_tbl folgendes Modell für die zweifaktorielle ANOVA:\n\\[\njump\\_length \\sim animal + site\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der zweifaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n23.3.2 Hypothesen für die zweifaktorielle ANOVA\nWir haben für jeden Faktor der zweifaktoriellen ANOVA ein Hypothesenpaar. Im Folgenden sehen wir die jeweiligen Hypothesenpaare. Einmal für animal, als Haupteffekt. Wir nennen einen Faktor den Hauptfaktor, weil wir an diesem Faktor am meisten interessiert sind. Wenn wir später einen Posthoc Test durchführen würden, dann würden wir diesen Faktor nehmen. Wir sind primär an dem Unterschied der Sprungweiten in [cm] in Gruppen Hund, Katze und Fuchs interessiert.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für site, als Nebeneffekt oder Blockeffekt oder Clustereffekt. Meist eine Variable, die wir auch erhoben haben und vermutlich auch einen Effekt auf das \\(y\\) haben wird. Oder aber wir haben durch das exprimentelle Design noch eine Aufteilungsvariable wie Block vorliegen. In unserem Beispiel ist es site oder der Ort, wo wir die Hunde-, Katzen, und Fuchsflöhe gefunden haben.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{city} = \\bar{y}_{smalltown} = \\bar{y}_{village} = \\bar{y}_{field}\\\\\nH_A: &\\; \\bar{y}_{city} \\ne \\bar{y}_{smalltown}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{village} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für die Interaktion animal:site - die eigentliche Stärke der zweifaktoriellen ANOVA. Wir können uns anschauen, ob die beiden Faktoren miteinander interagieren. Das heißt, ob eine Interaktion zwischen dem Faktor animal und dem Faktor site vorliegt.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{keine Interaktion}\\\\\nH_A: &\\; \\mbox{eine Interaktion zwischen animal und site}\n\\end{aligned}\n\\]\nWir haben also jetzt die verschiedenen Hypothesenpaare definiert und schauen uns jetzt die ANOVA in R einmal in der Anwendung an.\n\n23.3.3 Zweifaktoriellen ANOVA in R\nBei der einfaktoriellen ANOVA haben wir die Berechnungen der Sum of squares nochmal nachvollzogen. Im Falle der zweifaktoriellen ANOVA verzichten wir darauf. Das Prinzip ist das gleiche. Wir haben nur mehr Mitelwerte und mehr Abweichungen von diesen Mittelwerten, da wir ja nicht nur einen Faktor animal vorliegen haben sondern auch noch den Faktor site. Da wir aber die ANOVA nur Anwenden und dazu R nutzen, müssen wir jetzt nicht per Hand die zweifaktorielle ANOVA rechnen. Du musst aber die R Ausgabe der ANOVA verstehen. Und diese Ausgabe schauen wir uns jetzt einmal ohne und dann mit Interaktionsterm an.\n\n23.3.3.1 Ohne Interaktionsterm\nWir wollen nun einmal die zweifaktorielle ANOVA ohne Interaktionsterm rechnen die in Tabelle 23.9 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen die Funktion anova() in R.\n\n\n\nTabelle 23.9— Zweifaktorielle ANOVA ohne Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nerror\n\\(n-(a-1)(b-1)\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 32 erfahren.\nIm Folgenden sehen wir nochmal das Modell ohne Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel.\n\\[\njump\\_length \\sim animal + site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm().\n\nfit_2 <-  lm(jump_length ~ animal + site, data = fac2_tbl) %>% \n  anova\n\nfit_2\n\nAnalysis of Variance Table\n\nResponse: jump_length\n           Df  Sum Sq Mean Sq  F value         Pr(>F)    \nanimal      2 180.033 90.0165 19.88083 0.000000039196 ***\nsite        3   9.126  3.0419  0.67183        0.57104    \nResiduals 114 516.170  4.5278                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten wiederum die ANOVA Ergebnistabelle. Anstatt nur die Zeile animal für den Effekt des Faktors animal sehen wir jetzt auch noch die Zeile site für den Effekt des Faktors site. Zuerst ist weiterhin der Faktor animal signifikant, da der \\(p\\)-Wert mit \\(0.000000039196\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können von mindestens einem Gurppenunterschied im Faktor animal ausgehen. Im Weiteren ist der Faktor site nicht signifikant. Es scheint keinen Unterschied zwischend den einzelnen Orten und der Sprunglänge von den Hunde-, Katzen- und Fuchsflöhen zu geben.\nNeben der Standausgabe von R können wir auch die tidy Variante uns ausgeben lassen. In dem Fall sieht die Ausgabe etwas mehr aufgeräumt aus.\n\nfit_2 %>% tidy\n\n# A tibble: 3 × 6\n  term         df  sumsq meansq statistic       p.value\n  <chr>     <int>  <dbl>  <dbl>     <dbl>         <dbl>\n1 animal        2 180.    90.0     19.9    0.0000000392\n2 site          3   9.13   3.04     0.672  0.571       \n3 Residuals   114 516.     4.53    NA     NA           \n\n\nAbschließend können wir uns übr \\(\\eta^2\\) auch die erklärten Anteile der Varianz wiedergeben lassen.\n\nfit_2 %>% eta_squared\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\nanimal    |           0.26 | [0.15, 1.00]\nsite      |           0.02 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 26%. Für den Faktor site haben wir nur einen Anteil von 2% der erklärten Varianz. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 23.10 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\nTabelle 23.10— Zweifaktorielle Anova ohne Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dann direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{4.53} = 19.88\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{4.53} = 0.67\\)\n\n\nerror\n\\(120-(3-1)(4-1)\\)\n\\(SS_{error} = 516.17\\)\n\\(MS_{error} = 4.53\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.33\\)\n\n\n\n\n\n\n\n\n23.3.3.2 Mit Interaktionssterm\nWir wollen nun noch einmal die zweifaktorielle ANOVA mit Interaktionsterm rechnen, die in Tabelle 23.11 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen wie immer die Funktion anova() in R.\n\n\n\nTabelle 23.11— Zweifaktorielle ANOVA mit Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nanimal \\(\\times\\) site\n\\((a-1)(b-1)\\)\n\\(SS_{animal \\times site}\\)\n\\(MS_{animal \\times site}\\)\n\\(F_{calc} = \\cfrac{MS_{animal \\times site}}{MS_{error}}\\)\n\n\nerror\n\\(n-ab\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\nIm Folgenden sehen wir nochmal das Modell mit Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel. Einen Interaktionsterm bilden wir durch das : in R ab. Wir können theoretisch auch noch weitere Interaktionsterme bilden, also auch x:y:z. Ich würde aber davon abraten, da diese Interaktionsterme schwer zu interpretieren sind.\n\\[\njump\\_length \\sim animal + site + animal:site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Es wieder das gleich wie schon zuvor. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm(). Auch die Interaktion müssen wir nicht extra in der ANOVA Funktion angeben. Alles wird im Modell des lm() abgebildet.\nDie visuelle Regel zur Überprüfung der Interaktion lautet nun wie folgt. Abbildung 23.7 zeigt die entsprechende Vislualisierung. Wir haben keine Interaktion vorliegen, wenn die Geraden parallel zueinander laufen und die Abstände bei bei jedem Faktorlevel gleich sind. Wir schauen uns im Prinzip die erste Faktorstufe auf der x-Achse an. Wir sehen den Abstand von der roten zu blauen Linie sowie das die blaue Gerade über der roten Gerade liegt. Dieses Muster erwarten wir jetzt auch an dem Faktorlevel B und C. Eine leichte bis mittlere Interaktion liegt vor, wenn sich die Abstände von dem zweiten Faktor über die Faktorstufen des ersten Faktors ändern. Eine starke Interaktion liegt vor, wenn sich die Geraden schneiden.\n\n\n\n\n\n(a) Keine Interaktion\n\n\n\n\n\n\n(b) Leichte bis mittlere Intraktion\n\n\n\n\n\n\n(c) Starke Interaktion\n\n\n\n\nAbbildung 23.7— Darstellung von keiner Interaktion, leichter bis mittler Interaktion und starker Interaktion in einer zweifaktoriellen ANOVA mit einem Faktor mit drei Leveln A, B und C sowie einem Faktor mit zwei Leveln (rot und blau).\n\n\nIn der Abbildung 23.8 sehen wir den Interaktionsplot für unser Beispiel. Auf der y-Achse ist die Sprunglänge abgebildet und auf der x-Achse der Faktor animal. Die einzelnen Farben stellen die Level des Faktor site dar.\n\nggplot(fac2_tbl, aes(x = animal, y = jump_length,\n                     color = site, group = site)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_bw() +\n  scale_color_okabeito()\n\n\n\nAbbildung 23.8— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nWenn sich die Geraden in einem Interaktionsplot schneiden, haben wir eine Interaktion zwischen den beiden Faktoren vorliegen\nWir schauen zur visuellen Überprüfung auf den Faktor animal und das erste level cat. Wir sehen die Ordnung des zweiten Faktors site mit field, village, smalltown und city. Diese Ordnung und die Abstände sind bei zweiten Faktorlevel dog schon nicht mehr gegeben. Die Geraden schneiden sich. Auch liegt bei dem Level fox eine andere ordnung vor. Daher sehen wir hier eine starke Interaktion zwischen den beiden Faktoren animal und site.\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 32 erfahren.\nWir nehmen jetzt auf jeden Fall den Interaktionsterm animal:site mit in unser Modell und schauen uns einmal das Ergebnis der ANOVA an. Das lineare Modell der ANOVA wird erneut über die Funktion lm() berechnet und anschließend in die Funktion anova() gepipt.\n\nfit_3 <-  lm(jump_length ~ animal + site + animal:site, data = fac2_tbl) %>% \n  anova\n\nfit_3\n\nAnalysis of Variance Table\n\nResponse: jump_length\n             Df  Sum Sq Mean Sq  F value            Pr(>F)    \nanimal        2 180.033 90.0165 30.28074 0.000000000036302 ***\nsite          3   9.126  3.0419  1.02327           0.38536    \nanimal:site   6 195.115 32.5191 10.93914 0.000000001709866 ***\nResiduals   108 321.055  2.9727                               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn wir eine signifikante Interaktion vorliegen haben, dann müssen wir den Faktor A getrennt für jedes Levels des Faktors B auswerten.\nDie Ergebnistabelle der ANOVA wiederholt sich. Wir sehen, dass der Faktor animal signifkant ist, da der p-Wert mit \\(0.000000000036\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese ablehnen. Mindestens ein Mittelwertsvergleich unterschiedet sich zwischen den Levels des Faktors animal. Im Weiteren sehen wir, dass der Faktor site nicht signifkant ist, da der p-Wert mit \\(0.39\\) größer ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese nicht ablehnen. Abschließend finden wir die Interaktion zwischen dem Faktor animalund site las signifkant vor. Wenn wir eine signifikante Interaktion vorliegen haben, dann müssen wir den Faktor animal getrennt für jedes Levels des Faktors site auswerten. Wir können keine Aussage über die Sprungweite von Hunde-, Katzen- und Fuchsflöhen unabhängig von der Herkunft site der Flöhe machen.\nIn Kapitel A.4 findest du ein Beispiel für eine signifikante Interaktion und die folgende Auswertung\nWir können wie immer die etwas aufgeräumte Variante der ANOVA Ausgabe mit der Funktion tidy() uns ausgeben lassen.\n\nfit_3 %>% tidy()\n\n# A tibble: 4 × 6\n  term           df  sumsq meansq statistic   p.value\n  <chr>       <int>  <dbl>  <dbl>     <dbl>     <dbl>\n1 animal          2 180.    90.0      30.3   3.63e-11\n2 site            3   9.13   3.04      1.02  3.85e- 1\n3 animal:site     6 195.    32.5      10.9   1.71e- 9\n4 Residuals     108 321.     2.97     NA    NA       \n\n\nIm Folgenden können wir noch die \\(\\eta^2\\) für die ANOVA als Effektschätzer berechnen lassen.\n\nfit_3 %>% eta_squared\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Eta2 (partial) |       95% CI\n-------------------------------------------\nanimal      |           0.36 | [0.24, 1.00]\nsite        |           0.03 | [0.00, 1.00]\nanimal:site |           0.38 | [0.24, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 36%. Für den Faktor site haben wir nur einen Anteil von 3% der erklärten Varianz. Die Interaktion zwischen animal und site erklärt 38% der beobachteten Varianz udn ist somit auch vom Effekt her nicht zu ignorieren. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 23.12 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\nTabelle 23.12— Zweifaktorielle Anova mit Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dan direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{2.97} = 30.28\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{2.97} = 1.02\\)\n\n\nanimal \\(\\times\\) site\n\\((3-1)(4-1)\\)\n\\(SS_{animal \\times site} = 195.12\\)\n\\(MS_{animal \\times site} = 32.52\\)\n\\(F_{calc} = \\cfrac{32.52}{2.97} = 10.94\\)\n\n\nerror\n\\(120 - (3 \\cdot 4)\\)\n\\(SS_{error} = 321.06\\)\n\\(MS_{error} = 2.97\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.34\\)"
  },
  {
    "objectID": "stat-tests-anova.html#und-weiter",
    "href": "stat-tests-anova.html#und-weiter",
    "title": "23  Die ANOVA",
    "section": "\n23.4 Und weiter?",
    "text": "23.4 Und weiter?\nNach einer berechnten ANOVA können wir zwei Fälle vorliegen haben.\nWenn du in deinem Experiment keine signifikanten Ergebnisse findest, ist das nicht schlimm. Du kannst deine Daten immer noch mit der explorativen Datenanalyse auswerten wie in Kapitel 16 beschrieben.\n\nWir habe eine nicht signifkante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) nicht ablehnen und die Mittelwerte über den Faktor sind vermutlich alle gleich. Wir enden hier mit unserer statistischen Analyse.\nWir haben eine signifikante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) ablehnen und mindestens ein Gruppenvergleich über mindestens einen Faktor ist vermutlich unterschiedlich. Wir können dann in Kapitel 31 eine Posthoc Analyse rechnen."
  },
  {
    "objectID": "stat-tests-posthoc.html",
    "href": "stat-tests-posthoc.html",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "",
    "text": "Version vom November 18, 2022 um 08:41:36\nIn diesem Kapitel wollen wir uns mit den multipen Vergleichen beschäftigen. Das heißt, wir wollen statistisch Testen, ob sich die Level eines Faktors voneinander unterscheiden. Eventuell hast du schon eine einfaktorielle ANOVA gerechnet, wie in Kapitel 23.2 beschrieben. Oder aber du hast eine mehrfaktorielle ANOVA gerechnet wie in Kapitel 23.3 gezeigt. In beiden Fällen hast du jetzt einen signifikanten Faktor, der mehr als zwei Level hat. Du willst nun wissen, welche der Gruppenmittelwerte der Level sich signifikant unterscheiden. Hierfür können wir verschiedene Ansätze wählen.\nWenn wir multiple Mittelwertsvergleiche rechnen, dann tritt das Problem des multipen Testens auf. Im Kapitel 20.3 kannst du mehr über die Problematik erfahren und wie wir mit der \\(\\alpha\\) Inflation umgehen. Hier in diesem Kapitel gehe ich jetzt davon aus, dass dir die \\(\\alpha\\) Adjustierung ein Begriff ist.\nDer paarweise Mittelwertsvergleich wird auch gerne Tukey Test genannt. Das heißt, dass der Tukey Test alle Gruppen miteinander vergleicht. Der Tukey Test wird daher auch gerne all pair Vergleich genannt."
  },
  {
    "objectID": "stat-tests-posthoc.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-posthoc.html#genutzte-r-pakete-für-das-kapitel",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.1 Genutzte R Pakete für das Kapitel",
    "text": "31.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom,\n               multcomp, emmeans, ggpubr, multcompView,\n               rstatix, conflicted, see, rcompanion)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-posthoc.html#daten",
    "href": "stat-tests-posthoc.html#daten",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.2 Daten",
    "text": "31.2 Daten\nWir nutzen in diesem Kapitel den Datensatz aus dem Beispiel in Kapitel 6. Wir haben als Outcome die Sprunglänge in [cm] von Flöhen. Die Sprunglänge haben wir an Flöhen von Hunde, Katzen und Füchsen gemessen. Der Datensatz ist also recht übeerschaubar. Wir haben ein normalverteiltes \\(y\\) mit jump_length sowie einen multinomialverteiltes \\(y\\) mit grade und einen Faktor animal mit drei Leveln.\nDu kannst dir komplexere Auswertungen im Kapitel A anschauen. Dort sammelt sich mit der Zeit Auswertungen vom Fachbereich an. Daher finden sich dort auch Beispiele für multiple Vergleiche.\nIm Folgenden laden wir den Datensatz flea_dog_cat_fox.csv und selektieren mit der Funktion select() die benötigten Spalten. Abschließend müssen wir die Spalte animalnoch in einen Faktor umwandeln. Damit ist unsere Vorbereitung des Datensatzes abgeschlossen.\n\nfac1_tbl <- read_csv2(\"data/flea_dog_cat_fox.csv\") %>%\n  select(animal, jump_length, grade) %>% \n  mutate(animal = as_factor(animal))\n\nIn der Tabelle 31.1 ist der Datensatz fac1_tbl nochmal dargestellt.\n\n\n\n\nTabelle 31.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length sowie der multinominalverteilten Variable grade und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\ngrade\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n8.2\n8\n\n\ndog\n5.6\n7\n\n\ndog\n9.1\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\nfox\n7.7\n5\n\n\nfox\n8.1\n4\n\n\nfox\n9.1\n4\n\n\nfox\n9.7\n5\n\n\nfox\n10.6\n4\n\n\nfox\n8.6\n4\n\n\nfox\n10.3\n3\n\n\n\n\n\n\nWir werden nun den Datensatz fac1_tbl in den folgenden Abschnitten immer wieder nutzen.\n\n31.2.1 Hypothesen für multiple Vergleiche\nAls wir eine ANOVA gerechnet hatten, hatten wir nur eine Nullhypothese und eine Alternativehypothese. Wenn wir Nullhypothese abgelehnt hatten, wussten wir nur, dass sich mindestens ein paarweiser Vergleich unterschiedet. Multiple Vergleich lösen nun dieses Problem und führen ein Hypothesenpaar für jeden paarweisen Vergleich ein. Zum einen rechnen wir damit \\(k\\) Tests und haben damit auch \\(k\\) Hypothesenpaare (siehe auch Kapitel 20.3 zur Problematik des wiederholten Testens).\nWenn wir zum Beispiel alle Level des Faktors animal miteinander Vergleichen wollen, dann rechnen wir \\(k=3\\) paarweise Vergleiche. Im Folgenden sind alle drei Hypothesenpaare dargestellt.\n\\[\n\\begin{aligned}\nH_{01}: &\\; \\bar{y}_{cat} = \\bar{y}_{dog}\\\\\nH_{A1}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{02}: &\\; \\bar{y}_{cat} = \\bar{y}_{fox}\\\\\nH_{A2}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{03}: &\\; \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_{A3}: &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\nWenn wir drei Vergleiche rechnen, dann haben wir eine \\(\\alpha\\) Inflation vorliegen. Wir sagen, dass wir für das multiple Testen adjustieren müssen. In R gibt es eine Reihe von Adjustierungsverfahren. Wir nehmen meist Bonferroni oder das Verfahren, was in der jeweiligen Funktion als Standard (eng. default) gesetzt ist.\nWir adjustieren grundsätzlich die \\(p\\)-Werte und erhalten adjustierte \\(p\\)-Werte aus den jeweiligen Funktionen in R. Die adjustierten p-Werte können wir dann mit dem Signifikanzniveau von \\(\\alpha\\) gleich 5% vergleichen."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "href": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.3 Gruppenvergleiche mit pairwise.*.test()\n",
    "text": "31.3 Gruppenvergleiche mit pairwise.*.test()\n\nDie Funktion pairwise.*.test() ist veraltet, wir nutzen das R Paket emmeansoder das R Paket multcomp.\nWenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, dann können wir die Funktion pairwise.*.test() nutzen. Der Stern * steht entweder als Platzhalter für t für den t-Test oder aber für wilcox für den Wilcoxon Test. Die Funktion ist relativ einfach zu nutzen und liefert auch sofort die entsprechenden p-Werte.\nDie Funktion pairwise.*.test() ist in dem Sinne veraltet, da wir keine 95% Konfidenzintervalle generieren können. Da die Funktion aber immer mal wieder angefragt wird, ist die Funktion hier nochmal aufgeführt.\n\n31.3.1 Paarweiser t Test\nWir nutzen den paarweisen t-Test,\n\nwenn wir ein normalverteiltes \\(y\\) vorliegen haben, wie jump_length.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\n\nDie Funktion pairwise.t.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und dhaer auch fehleranfällig.\n\n\nMehr zu mutate_if() erfährst du auf der Hilfeseite von mutate()\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein saubers tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.t.test(fac1_tbl$jump_length, fac1_tbl$animal,\n                p.adjust.method = \"bonferroni\") %>% \n  tidy %>% \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 × 3\n  group1 group2 p.value\n  <chr>  <chr>    <dbl>\n1 cat    dog      0.007\n2 fox    dog      0.876\n3 fox    cat      0.001\n\n\nWir erhalten in einem Tibble die adujstierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wir sehen, dass der Gruppenvergleich cat - dog signifikant ist, der Gruppenvergleich fox - dog nicht signifkant ist und der Gruppenvergleich fox - cat wiederum signifkant ist.\nLeider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket ggpubr anschauen. Die Hilfeseite des R Pakets ggpubr liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\).\nUm die Abbildung 31.1 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen t-Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert einer einfaktoriellen ANOVA als globalen Test ergänzen.\n\nstat_test_obj <- compare_means(\n jump_length ~ animal, data = fac1_tbl,\n method = \"t.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np <- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"jump_length\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(13, 16, 19)) +\n  stat_compare_means(label.y = 20, method = \"anova\")    \n\n\n\nAbbildung 31.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem t-Test und den Bonferroni adjustierten p-Werten.\n\n\n\n\n\n31.3.2 Paarweiser Wilcoxon Test\nWir nutzen den paarweisen Wilxocon Test,\n\nwenn wir ein nicht-normalverteiltes \\(y\\) vorliegen haben, wie grade.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\n\nDie Funktion pairwise.wilcox.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und dhaer auch fehleranfällig.\n\n\nMehr zu mutate_if() erfährst du auf der Hilfeseite von mutate()\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein saubers tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.wilcox.test(fac1_tbl$grade, fac1_tbl$animal,\n                     p.adjust.method = \"bonferroni\") %>% \n  tidy %>% \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 × 3\n  group1 group2 p.value\n  <chr>  <chr>    <dbl>\n1 cat    dog      0.045\n2 fox    dog      0.005\n3 fox    cat      0.011\n\n\nWir erhalten in einem Tibble die adujstierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wir sehen, dass der Gruppenvergleich cat - dog knapp signifikant ist, der Gruppenvergleich fox - dog ebenfalls signifkant ist und der Gruppenvergleich fox - cat auch signifkant ist.\nLeider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket ggpubr anschauen. Die Hilfeseite des R Pakets ggpubr liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\).\nUm die Abbildung 31.2 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen Wilcoxon Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert eines Kruskal Wallis als globalen Test ergänzen.\n\nstat_test_obj <- compare_means(\n grade ~ animal, data = fac1_tbl,\n method = \"wilcox.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np <- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"grade\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(10, 13, 16)) +\n  stat_compare_means(label.y = 20, method = \"kruskal.test\")    \n\n\n\nAbbildung 31.2— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem Wilcoxon Test und den Bonferroni adjustierten p-Werten."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "href": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.4 Gruppenvergleich mit dem multcomp Paket",
    "text": "31.4 Gruppenvergleich mit dem multcomp Paket\nWir drehen hier einmal die Erklärung um. Wir machen erst die Anwendung in R und sollte dich dann noch mehr über die statistischen Hintergründe der Funktionen interessieren, folgt ein Abschnitt noch zur Theorie. Du wirst die Funktionen aus multcomp vermutlich in deiner Abschlussarbeit brauchen. Häufig werden multiple Gruppenvergleiche in Abschlussarbeiten gerechnet.\n\n31.4.1 Gruppenvergleiche mit multcomp in R\n\n\nDie Ausgabe von multcomp können über die Funktion tidy() aufgeräumt werden. Mehr dazu unter der Hilfeseite von tidy() zu multcomp.\nAls erstes brauchen wir ein lineares Modell für die Verwendung von multcomp. Normalerweise verenden wir das gleiche Modell, was wir schon in der ANOVA verwendet haben. Wir nutzen hier ein simples lineares Modell mit nur einem Faktor. Im Prinzip kann das Modell auch größer sein. Du findest immer Beispiel im Kapitel A, die dir eventuell dann nochmal zeigen, wie du deine Daten nutzen musst.\n\nfit_1 <- lm(jump_length ~ animal, data = fac1_tbl)\n\nWir haben das Objeckt fit_1 mit der Funktion lm() erstellt. Im Modell sind jetzt alle Mittelwerte und die entsprechenden Varianzen geschätzt worden. Mit summary(fit_1) kannst du dir gerne das Modell auch nochmal anschauen.\n\n\nWenn wir keinen all-pair Vergleich rechnen wollen, dann können wir auch einen many-to-one Vergleich mit dem Dunnett Kontrast rechnen.\nIm Anschluß nutzen wir die Funktion glht() um den multiplen vergleich zu rechnen. Als erstes musst du wissen, dass wenn wir alle Vergleiche rechnen wollen, wir einen all-pair Vergleich rechnen. In der Statistik heißt dieser Typ von Vergleich Tukey. Wir wollen jetzt als für den Faktor animal einen multiplen Tukey-Vergleich rechnen. Nichts anders sagt mcp(animal = \"Tukey\") aus, dabei steht mcp für multiple comparison procedure. Mit dem hinteren Teil der Funktion weiß jetzt die Funktion glht() was gerechnet werden soll. Wir müssen jetzt der Funktion nur noch mitgeben auf was der multiple vergleich gerehcnet werden soll, mit dem Objekt fit_1. Wir speichern die Ausgabe der Funktion in comp_1_obj.\n\ncomp_1_obj <- glht(fit_1, linfct = mcp(animal = \"Tukey\")) \n\nMit dem Objekt comp_1_fit können wir noch nicht soviel anfangen. Der Inhalt ist etwas durcheinander und wir wollen noch die Konfidenzintervalle haben. Daher pipen wir comp_1_fit erstmal in die Funktion tidy() und alssen mit der Option conf.int = TRUE die simultanen 95% Konfidenzintervalle berechnen. Dann nutzen wir die Funktion select() um die wichtigen Spalten zu selektieren. Abschließend mutieren wir noch alle numerischen Spalten in dem wir auf die dritte Kommastelle runden. Wir speichern alles in das Objekt res_1_obj.\n\nres_1_obj <- comp_1_obj %>% \n  tidy(conf.int = TRUE) %>% \n  select(contrast, estimate, adj.p.value, \n         conf.low, conf.high) %>% \n  mutate_if(is.numeric, round, 3)\n\nWir lassen uns dann den Inhalt von dem Objekt res_1_obj ausgeben.\n\nres_1_obj\n\n# A tibble: 3 × 5\n  contrast  estimate adj.p.value conf.low conf.high\n  <chr>        <dbl>       <dbl>    <dbl>     <dbl>\n1 cat - dog    -3.39       0.006    -5.80     -0.97\n2 fox - dog     1.03       0.535    -1.39      3.44\n3 fox - cat     4.41       0.001     2.00      6.83\n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir cat - dog rechnen. Also wirklich der Mittelwert der Sprungweite der Katzenflöhe minus den Mittelwert der Sprungweite der Hundeflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleichecat - dog mit einem p-Wert von \\(0.006\\) sowie für den Vergleich \\(fox - cat\\) mit einem p-Wert von \\(0.001\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nIn Abbildung 31.3 sind die simultanen 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab. Mit der Funktion factor() kannst du die Sortierung der Level einem Faktor ändern und somit auch Position auf den Achsen.\n\n  ggplot(res_1_obj, aes(contrast, y=estimate, \n                        ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.3— Simultane 95% Konfidenzintervalle für den paarweisen Vergleich der Sprungweiten in [cm] der Hunde-, Katzen- und Fuchsflöhe.\n\n\n\n\nDie Entscheidung gegen die Nullhypothese anhand der simultanen 95% Konfidenzintervalle ist inhaltlich gleich, wie die Entscheidung anhand der p-Werte. Wir entscheiden gegen die Nullhypothese, wenn die 0 nicht mit im Konfindenzintervall enthalten ist. Wir wählen hier die 0 zur Entscheidung gegen die Nullhypothese, weil wir einen Mittelwertsvergleich rechnen.\nFür den Vergleich fox -dog ist die 0 im 95% Konfidenzintervall, wir können daher die Nullhypothese nicht ablehnen. Das 95% Konfidenzintervall ist nicht signifikant. Bei dem Vergleich fox - cat sowie dem Vergleich cat - dog ist jeweils die 0 nicht im 95% Konfidenzintervall enthalten. Beide 95% Konfidenzintervalle sind signifikant, wir können die Nullhypothese ablehnen."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "href": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.5 Gruppenvergleich mit dem emmeans Paket",
    "text": "31.5 Gruppenvergleich mit dem emmeans Paket\n\n\nWir können hier nicht alles erklären und im Detail durchgehen. Hier gibt es noch ein aufwendiges Tutorium zu emmeans: Getting started with emmeans.\nDaneben gibt es auch noch die Einführung mit Theorie auf der Seite des R Paktes\nIm Folgenden wollen wir uns mit einem anderen R Paket beschäftigen was auch multiple Vergleiche rechnen kann. In diesem Kapitel nutzen wir das R Paket emmeans. Im Prinzip kann emmeans das Gleiche wir das R Paket multcomp. Beide Pakete rechnen dir einen multipen Vergleich. Das Paket emmeans kann noch mit nested comparisons umgehen. Deshlb hier nochmal die Vorstellung von emmeans. Du kannst aber für eine simple Auswertung mit nur einem Faktor beide Pakete verwenden.\n\n31.5.1 Gruppenvergleiche mit emmeans in R\n\n\nDie Ausgabe von emmeans können über die Funktion tidy() aufgeräumt werden. Mehr dazu unter der Hilfeseite von tidy() zu emmeans.\nUm den multiplen Vergleich in emmeans durchführen zu können brauchen wir zuerst ein lineares Modell, was uns die notwenidgen Parameter wie Mittelwerte und Standardabweichungen liefert. Wir nutzen in unserem simplen Beispiel ein lineares Modell mit einer Einflussvariable \\(x\\) und nehmen an, dass unser Outcome \\(y\\) normalverteilt ist. Achtung, hier muss natürlich das \\(x\\) ein Faktor sein. Dann können wir ganz einfach die Funktion lm() nutzen. Im Folgenden fitten wir das Modell fit_2 was wir dann auch weiter nutzen werden.\n\nfit_2 <- lm(jump_length ~ animal, data = fac1_tbl)\n\nDer multiple Vergleich in emmeans ist mehrschrittig. Wir pipen unser Modell aus fit_2 in die Funktion emmeans(). Wir geben mit ~ animal an, dass wir über die Level des Faktors animal einen Vergleich rechnen wollen. Wir adjustieren die \\(p\\)-Werte nach Bonferroni. Danach pipen wir weiter in die Funktion contrast() wo der eigentliche Vergleich festgelegt wird. In unserem Fall wollen wir einen many-to-one Vergleich rechnen. Alle Gruppen zu der Gruppe fox. Du kannst mit ref = auch ein anderes Level deines Faktors wählen.\n\ncomp_2_obj <- fit_2 %>% \n  emmeans(~ animal) %>% \n  contrast(method = \"trt.vs.ctrl\", ref = \"fox\", adjust = \"bonferroni\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - fox    -1.03 0.947 18  -1.086  0.5837\n cat - fox    -4.41 0.947 18  -4.660  0.0004\n\nP value adjustment: bonferroni method for 2 tests \n\n\nWir können auch einen anderen Kontrast wählen. Wir überschreiben jetzt das Objekt comp_2_obj mit dem Kontrast all-pair, der alle möglichen Vergleiche rechnet. In emmeans heißt der all-pair Kontrast pairwise.\n\ncomp_2_obj <- fit_2 %>% \n  emmeans(~ animal) %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - cat     3.39 0.947 18   3.574  0.0065\n dog - fox    -1.03 0.947 18  -1.086  0.8756\n cat - fox    -4.41 0.947 18  -4.660  0.0006\n\nP value adjustment: bonferroni method for 3 tests \n\n\nWir können das Ergebnis auch noch mit der Funktion tidy() weiter aufräumen und dann die Spalten selektieren, die wir brauchen. Häufig benötigen wir nicht alle Spalten, die eine Funktion wiedergibt.\n\nres_2_obj <- comp_2_obj %>% \n  tidy(conf.int = TRUE) %>% \n  select(contrast, estimate, adj.p.value, conf.low, conf.high) %>% \n  mutate(across(where(is.numeric), round, 4))\n\nres_2_obj\n\n# A tibble: 3 × 5\n  contrast  estimate adj.p.value conf.low conf.high\n  <chr>        <dbl>       <dbl>    <dbl>     <dbl>\n1 dog - cat     3.39      0.0065    0.886      5.89\n2 dog - fox    -1.03      0.876    -3.53       1.47\n3 cat - fox    -4.41      0.0006   -6.91      -1.91\n\n\nAbschließend wollen wir noch die 95% Konfidenzintervalle in Abbildung 31.4 abbilden. Hier ist es bei emmeans genauso wie bei multcomp. Wir können das Objekt res_2_obj direkt in ggplot() weiterverwenden und uns die 95% Konfidenzintervalle einmal plotten.\n\n  ggplot(res_2_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.4— Die 95% Konfidenzintervalle für den allpair-Vergleich des simplen Datensatzes.\n\n\n\n\nWir wollen uns noch einen etwas komplizierteren Fall anschauen, indem sich emmeans von multcomp in der Anwendung unterscheidet. Wir laden den Datensatz flea_dog_cat_fox_site.csv in dem wir zwei Faktoren haben. Damit können wir dann ein Modell mit einem Interaktionsterm bauen. Wir erinnern uns, dass wir in der zweifaktoriellen ANOAV eine signifikante Interaktion zwischen den beiden Faktoren animal und site festgestelt hatten.\n\nfac2_tbl <- read_csv2(\"data/flea_dog_cat_fox_site.csv\") %>% \n  select(animal, site, jump_length) %>% \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 31.2 nochmal dargestellt.\n\n\n\n\nTabelle 31.2— Selektierter Datensatz mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\nanimal\nsite\njump_length\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.1\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\n…\n…\n…\n\n\nfox\nfield\n16.38\n\n\nfox\nfield\n14.59\n\n\nfox\nfield\n14.03\n\n\nfox\nfield\n13.63\n\n\nfox\nfield\n14.09\n\n\nfox\nfield\n15.52\n\n\n\n\n\n\nIn Abbildung 31.5 sehen wir die Ergebnisse des multiplen Vergleiches nochmal anders als Pairwise P-value plot dargestellt. Wir haben auf der y-Achse zu Abwechselung mal die Gruppen dargestellt und auf der x-Achse die \\(p\\)-Werte. In den Kästchen sind die Effekte der Gruppen nochmal gezeigt. In unserem Fall die Mittelwerte der Sprungweiten für die drei Gruppen. Wir sehen jetzt immer den \\(p\\)-Wert für den jeweiligen Vergleich durch eine farbige Linie miteinander verbunden. So können wir nochmal eine andere Übersicht über das Ergebnis des multiplen Vergleich kriegen.\n\nfit_2 %>% \n  emmeans(~ animal) %>% \n  pwpp(adjust = \"bonferroni\") +\n  theme_bw()\n\n\n\nAbbildung 31.5— Visualisierung der Ergebnisse im Pairwise P-value plot.\n\n\n\n\nAuch haben wir die Möglichkeit un die \\(p\\)-Werte mit der Funktion pwpm() als eine Matrix ausgeben zu lassen. Wir erhalten in dem oberen Triangel die \\(p\\)-Wert für den jeweiligen Vergleich. In dem unteren Triangel die geschätzten Mittelwertsunterschiede. Auf der Diagonalen dann die geschätzten Mittelwerte für die jeweilige Gruppe. So haben wir nochmal alles sehr kompakt zusammen dargestellt.\n\nfit_2 %>% \n  emmeans(~ animal) %>% \n  pwpm(adjust = \"bonferroni\")\n\n       dog    cat    fox\ndog [8.13] 0.0065 0.8756\ncat   3.39 [4.74] 0.0006\nfox  -1.03  -4.41 [9.16]\n\nRow and column labels: animal\nUpper triangle: P values   adjust = \"bonferroni\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later\n\n\nIn Abbildung 31.6 sehen wir nochmal die Daten visualisiert. Wichtig ist hier, dass wir zwei Faktoren vorliegen haben. Den Faktor animal und den Faktor site. Dabei ist der Faktor animal in dem Faktor site genested. Wir messen jedes Level des Faktors animal jeweils in jedem Level des Faktors site.\n\n\n\n\nAbbildung 31.6— Boxplot der Sprungweiten [cm] von Hunden und Katzen gemessen an verschiedenen Orten.\n\n\n\n\nWir rechnen ein multiples lineares Modell mit einem Interaktionsterm. Daher packen wir beide Faktoren in das Modell sowie die Intraktion zwischen den beiden Faktoren. Wir erhalten nach dem fitten des Modells das Objekt fit_3.\n\nfit_3 <- lm(jump_length ~ animal + site + animal:site, data = fac2_tbl)\n\nDer Unterschied zu unserem vorherigen multiplen Vergleich ist nun, dass wir auch einen multiplen Vergleich für animal nested in site rechnen können. Dafür müssen wir den Vergleich in der Form animal | site schreiben. Wir erhalten dann die Vergleiche der Level des faktors animal getrennt für die Level es Faktors site.\n\ncomp_3_obj <- fit_3 %>% \n  emmeans(~ animal | site) %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\ncomp_3_obj\n\nsite = city:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -3.101 0.771 108  -4.022  0.0003\n cat - fox   -6.538 0.771 108  -8.479  <.0001\n dog - fox   -3.437 0.771 108  -4.457  0.0001\n\nsite = smalltown:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -4.308 0.771 108  -5.587  <.0001\n cat - fox   -4.064 0.771 108  -5.271  <.0001\n dog - fox    0.244 0.771 108   0.316  1.0000\n\nsite = village:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -1.316 0.771 108  -1.707  0.2722\n cat - fox   -1.729 0.771 108  -2.242  0.0809\n dog - fox   -0.413 0.771 108  -0.536  1.0000\n\nsite = field:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -0.982 0.771 108  -1.274  0.6167\n cat - fox    1.366 0.771 108   1.772  0.2379\n dog - fox    2.348 0.771 108   3.045  0.0088\n\nP value adjustment: bonferroni method for 3 tests \n\n\nWir können uns das Ergebnis auch etwas schöner ausgeben lassen. Wir nutzen hier noch die Funktion format.pval() um die \\(p\\)-Werte besser zu formatieren. Die \\(p\\)-Wert, die kleiner sind als 0.001 werden als <0.001 ausgegeben und die anderen \\(p\\)-Werte auf zwei Nachstellen nach dem Komma gerundet.\n\ncomp_3_obj %>% \n  summary %>% \n  as_tibble %>% \n  select(contrast, site, p.value) %>% \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2))\n\n# A tibble: 12 × 3\n   contrast  site      p.value\n   <fct>     <fct>     <chr>  \n 1 cat - dog city      <0.001 \n 2 cat - fox city      <0.001 \n 3 dog - fox city      <0.001 \n 4 cat - dog smalltown <0.001 \n 5 cat - fox smalltown <0.001 \n 6 dog - fox smalltown 1.00   \n 7 cat - dog village   0.27   \n 8 cat - fox village   0.08   \n 9 dog - fox village   1.00   \n10 cat - dog field     0.62   \n11 cat - fox field     0.24   \n12 dog - fox field     0.01   \n\n\nIn der Ausgabe können wir erkennen, dass die Vergleich in der Stadt alle signifkant sind. Jedoch erkennen wir keine signifikanten Ergebnisse mehr in dem Dorf und im Feld ist nur der Vergleich dog - fox signifkant. Hier solltest du nochmal beachten, warum wir die Analyse getrennt machen. In der zweifaktoriellen ANOVA haben wir gesehen, dass ein signifkanter Interaktionsterm zwischen den beiden Faktoren animal und site vorliegt.\nWir wollen uns noch über die Funktion confint() die 95% Konfidenzintervalle wiedergeben lassen.\n\nres_3_obj <- comp_3_obj %>% \n  confint() %>% \n  as_tibble() %>% \n  select(contrast, site, estimate, conf.low = lower.CL, conf.high = upper.CL) \n\nres_3_obj\n\n# A tibble: 12 × 5\n   contrast  site      estimate conf.low conf.high\n   <fct>     <fct>        <dbl>    <dbl>     <dbl>\n 1 cat - dog city        -3.10    -4.98     -1.23 \n 2 cat - fox city        -6.54    -8.41     -4.66 \n 3 dog - fox city        -3.44    -5.31     -1.56 \n 4 cat - dog smalltown   -4.31    -6.18     -2.43 \n 5 cat - fox smalltown   -4.06    -5.94     -2.19 \n 6 dog - fox smalltown    0.244   -1.63      2.12 \n 7 cat - dog village     -1.32    -3.19      0.559\n 8 cat - fox village     -1.73    -3.60      0.146\n 9 dog - fox village     -0.413   -2.29      1.46 \n10 cat - dog field       -0.982   -2.86      0.893\n11 cat - fox field        1.37    -0.509     3.24 \n12 dog - fox field        2.35     0.473     4.22 \n\n\nBesonders mit den 95% Konfiendezintervallen sehen wir nochmal den Interaktionseffekt zwischen den beiden Faktoren animal und site. So dreht sich der Effekt von zum Beispiel dog - fox von \\(-3.44\\) in dem Level city zu \\(+2.35\\) in dem Level field. Wir haben eine Interaktion vorliegen und deshalb die Analyse getrennt für jeden Level des Faktors site durchgeführt.\nAbbildung 31.7 zeigt die entsprechenden 95% Konfidenzintervalle. Wir müssen hier etwas mit der position spielen, so dass die Punkte und der geom_errorbar richtig liegen.\n\n  ggplot(res_3_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high,\n                        color = site, group = site)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    scale_color_okabeito() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.7— Die 95% Konfidenzintervalle für den allpair-Vergleich des Models mit Interaktionseffekt."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-ght",
    "href": "stat-tests-posthoc.html#sec-posthoc-ght",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.6 Gruppenvergleich mit dem Games-Howell-Test",
    "text": "31.6 Gruppenvergleich mit dem Games-Howell-Test\nDer Games-Howell-Test ist eine Alternative zu dem Paket multcomp und dem Paket emmeans. Wir nutzen den Games-Howell-Test, wenn die Annahme der Homogenität der Varianzen, der zum Vergleich aller möglichen Kombinationen von Gruppenunterschieden verwendet wird, verletzt ist. Dieser Post-Hoc-Test liefert Konfidenzintervalle für die Unterschiede zwischen den Gruppenmitteln und zeigt, ob die Unterschiede statistisch signifikant sind. Der Test basiert auf der Welch’schen Freiheitsgradkorrektur und adjustiert die \\(p\\)-Werte. Der Test vergleicht also die Differenz zwischen den einzelnen Mittelwertpaaren mit einer Adjustierung für den Mehrfachtest. Es besteht also keine Notwendigkeit, zusätzliche p-Wert-Korrekturen vorzunehmen. Mit dem Games-Howell-Test ist nur ein all-pair Vergleich möglich.\nFür den Games-Howell-Test aus dem Paket rstatix müssen wir kein lineares Modell fitten. Wir schreiben einfach die wie in einem t-Test das Outcome und den Faktor mit den Gruppenleveln in die Funktion games_howell_test(). Wir erhalten dann direkt das Ergebnis des Games-Howell-Test. Wir nutzen in diesem Beispiel die Daten aus dem Objekt fac1_tbl zu sehen in Tabelle 31.1.\n\nfit_4 <- games_howell_test(jump_length ~ animal, data = fac1_tbl) \n\nWir wollen aber nicht mit der Ausgabe arbeiten sondern machen uns noch ein wenig Arbeit und passen die Ausgabe an. Zum einen brauchen wir noch die Kontraste und wir wollen die \\(p\\)-Werte auch ansprechend formatieren. Wir erhalten das Objekt res_4_obj und geben uns die Ausgabe wieder.\n\nres_4_obj <- fit_4 %>% \n  as_tibble %>% \n  mutate(contrast = str_c(group1, \"-\", group2)) %>% \n  select(contrast, estimate, p.adj, conf.low, conf.high) %>% \n  mutate(p.adj = format.pval(p.adj, eps = 0.001, digits = 2))\n\nres_4_obj\n\n# A tibble: 3 × 5\n  contrast estimate p.adj conf.low conf.high\n  <chr>       <dbl> <chr>    <dbl>     <dbl>\n1 dog-cat     -3.39 0.02     -6.28    -0.490\n2 dog-fox      1.03 0.52     -1.52     3.57 \n3 cat-fox      4.41 0.00      2.12     6.71 \n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt, den haben wir uns selber mit der Funktion mutate() und str_c() aus den Spalten group1 und group2 gebaut. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir dog-cat rechnen. Also wirklich den Mittelwert der Sprungweite der Hundeflöhe minus den Mittelwert der Sprungweite der Katzenflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleiche dog - cat mit einem p-Wert von \\(0.02\\) sowie für den Vergleich \\(cat - fox\\) mit einem p-Wert von \\(0.00\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nIn Abbildung 31.8 sind die simultanen 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab.\n\n  ggplot(res_4_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.8— Die 95% Konfidenzintervalle für den allpair-Vergleich des Games-Howell-Test.\n\n\n\n\nDie Entscheidungen nach den 95% Konfidenzintervallen sind die gleichen wie nach dem \\(p\\)-Wert. Da wir hier es mit einem Mittelwertsvergleich zu tun haben, ist die Entscheidung gegen die Nullhypothese zu treffen wenn die 0 im Konfidenzintervall ist."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-compact-letter",
    "href": "stat-tests-posthoc.html#sec-compact-letter",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.7 Compact letter display",
    "text": "31.7 Compact letter display\nIn der Pflanzenernährung ist es nicht unüblich sehr viele Substrate miteinander zu vergleichen. Oder andersherum, wenn wir sehr viele Gruppen haben, dann kann die Darstellung in einem all-pair Vergleich sehr schnell sehr unübersichltich werden. Deshalb wure das compact letter display entwickelt.\n\n\nCompact Letter Display (CLD) - What is it?. Das compact letter display zeigt an, bei welchen Vergleichen der Behandlungen die Nullhypothese gilt. Daher werden die nicht signifikanten Ergebnisse visualisiert.\nSchauen wir uns aber zurerst einmal ein größeres Beispiel mit neun Behandlungen mit jeweils zwanzig Beobachtungen an. Wir erstellen uns den Datensatz in der Form, dass sich die Mittelwerte für die Behandlungen teilweise unterscheiden.\n\nset.seed(20220914)\ndata_tbl <- tibble(trt = gl(n = 9, k = 20, \n                            labels = c(\"pos_crtl\", \"neg_ctrl\", \"treat_A\", \"treat_B\", \n                                       \"treat_C\", \"treat_D\", \"treat_E\", \"treat_F\", \n                                       \"treat_G\")),\n                   rsp = c(rnorm(20, 10, 5), rnorm(20, 20, 5), rnorm(20, 22, 5), rnorm(20, 24, 5),\n                           rnorm(20, 35, 5), rnorm(20, 37, 5), rnorm(20, 40, 5), rnorm(20, 43, 5),\n                           rnorm(20, 50, 5)))\n\nIn der Abbildung 31.9 ist der Datensatz data_tbl nochmal als Boxplot dargestellt.\n\n\n\n\nAbbildung 31.9— Boxplot der Beispieldaten.\n\n\n\n\nWir sehen, dass sich die positive Kontrolle von dem Rest der Behandlungen unterscheidet. Danach haben wir ein Plateau mit der negativen Kontrolle und der Behanldung A und der Behandlung B. Nach diesem Plateau haben wir einen Sprung und sehen einen leicht linearen Anstieg der Mittelwerte der Behandlungen.\nSchauen wir uns zuerst einmal an, wie ein compact letter display aussehen würde, wenn kein Effekt vorliegen würde. Daher die Nullhypothese ist wahr und die Mittelwerte der Gruppen unterscheiden sich nicht. Wir nutzen hier einmal ein kleineres Beispiel mit den Behandlungslevels ctrl, treat_A und treat_B. Alle drei Behandlungslevel haben einen Mittelwert von 10. Es gilt die Nullhypothese und wir erhalten folgendes compact letter display in Tabelle 31.3.\n\n\nTabelle 31.3— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt, es gibt keinen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\\(\\phantom{a}\\)\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n10\na\n\n\n\n\ntreat_B\n10\na\n\n\n\n\n\n\nDas Gegenteil sehen wir in der Tabelle 31.4. Hier haben wir ein compact letter display wo sich alle drei Mittelwerte mit 10, 15 und 20 voneinander klar unterscheiden. Die Nullhypothese gilt für keinen der möglichen paarweisen Vergleiche.\n\n\nTabelle 31.4— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt nicht, es gibt einen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n15\n\nb\n\n\n\ntreat_B\n20\n\\(\\phantom{a}\\)\n\nc\n\n\n\n\nSchauen wir uns nun die Implementierung des compact letter display für die verschiedenen Möglichkeiten der Multiplen Vergleiche einmal an.\n\n31.7.1 … für pairwise.*.test()\n\nWenn wir für die Funktionen pairwise.*.test() das compact letter display berechnen wollen, dann müssen wir etwas ausholen. Denn wir müssen dafür die Funktion multcompLetters() nutzen. Diese Funktion braucht die \\(p\\)-Werte als Matrix und diese Matrix der \\(p\\)-Werte kriegen wir über die Funktion fullPTable(). Am Ende haben wir aber dann das was wir wollten. Ich habe hier nochmal das einfache Beispiel mit den Sprungweiten von oben genommen.\n\npairwise.t.test(fac1_tbl$jump_length, fac1_tbl$animal,\n                p.adjust.method = \"bonferroni\") %>% \n  extract2(\"p.value\") %>% \n  fullPTable() %>% \n  multcompLetters()\n\ndog cat fox \n\"a\" \"b\" \"a\" \n\n\nAls Ergebnis erhalten wir, dass Hund- und Fuchsflöhe gleich weit springen, beide teilen sich den gleichen Buchstaben. Katzenflöhe springen unterschiedlich zu Hunden- und Fuchsflöhen. Das Vorgehen ändert sich dann nicht, wenn wir eine andere Funktion wie pairwise.wilcox.test() nehmen.\n\n31.7.2 … für das Paket multcomp\n\nWir schauen uns zuerst einmal die Implementierung des compact letter display in dem Paket multcomp an. Wir nutzen die Funktion multcompLetters() aus dem Paket multcompView um uns das compact letter display wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Das heist wir müssen aus der Funktion glht() die adjustierten \\(p\\)-Werte extrahieren und dann einen Vektor der Vergleiche bzw. Kontraste in der Form A-B bauen. Also ohne Leerzeichen und in der Beschreibung der Level der Behandlung trt. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nmultcomp_cld <- lm(rsp  ~ trt, data = data_tbl) %>%\n  glht(linfct = mcp(trt = \"Tukey\")) %>% \n  tidy %>% \n  mutate(contrast = str_replace_all(contrast, \"\\\\s\", \"\")) %>% \n  pull(adj.p.value, contrast) %>% \n  multcompLetters() \n\nWir erhalten dann folgendes compact letter display für die paarweisen Vergleiche aus multcomp.\n\nmultcomp_cld \n\nneg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F  treat_G \n     \"a\"      \"a\"      \"a\"      \"b\"     \"bc\"     \"cd\"      \"d\"      \"e\" \npos_crtl \n     \"f\" \n\n\nLeider sind diese Buchstaben in dieser Form schwer zu verstehen. Deshalb gibt es noch die Funktion plot() in dem Paket multcompView um uns die Buchstaben mit den Leveln der Behandlung einmal ausgeben zu lassen. Wir erhalten dann folgende Abbildung.\n\nmultcomp_cld %>% plot\n\n\n\n\nIn dem compact letter display bedeuten gleiche Buchstaben, dass die Behandlungen gleich sind. Es gilt die Nullhypothese für diesen Vergleich.\nWas sehen wir hier? Kombinieren wir einmal das compact letter display mit den Leveln der Behandlung und den Mittelwerten der Behandlungen in einer Tabelle 31.5. Wenn die Mittelwerte gleich sind, dann erhalten die Behandlungslevel den gleichen Buchstaben. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein a. Ebenso sind die MIttelwerte von treat_C und treat_D nahezu gleich, dehalb erhalten beide ein b. Das machen wir immer so weiter und konzentrieren uns also auf die nicht signifikanten Ergebnisse. Denn gleiche Buchstaben bedeuten, dass die Behandlungen gleich sind. Wir sehen hier also, bei welchen Vergleichen die Nullhypothese gilt.\n\n\nTabelle 31.5— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus multcomp. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\nneg_ctrl\n20\na\n\n\n\n\n\n\n\ntreat_A\n22\na\n\n\n\n\n\n\n\ntreat_B\n24\na\n\n\n\n\n\n\n\ntreat_C\n35\n\nb\n\n\n\n\n\n\ntreat_D\n37\n\nb\nc\n\n\n\n\n\ntreat_E\n40\n\n\nc\nd\n\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\npos_crtl\n10\n\n\n\n\n\nf\n\n\n\n\nWir können dann die Buchstaben auch in den Boxplot ergaänzen. Die y-Position kann je nach Belieben dann noch angepasst werden. zum Beispiel könnten hier auch die Mittelwerte aus einer summarise() Funktion ergänzt werden und so die y-Position angepasst werden.\n\nletters_tbl <- multcomp_cld$Letters %>% \n  enframe(\"trt\", \"label\") %>% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 31.10— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n\n31.7.3 … für das Paket emmeans\n\nIn dem Paket emmeans ist das compact letter display ebenfalls implementiert und wir müssen nicht die Funktion multcompLetters() nutzen. Durch die direkte Implementierung ist es etwas einfacher sich das compact letter display anzeigen zu lassen. Das Problem ist dann später sich die Buchstaben zu extrahieren um die Abbildung 31.11 zu ergänzen. Wir nutzen in emmeans die Funktion cld() um das compact letter display zu erstellen.\n\nemmeans_cld <- lm(rsp  ~ trt, data = data_tbl) %>%\n  emmeans(~ trt) %>%\n  cld(Letters = letters, adjust = \"bonferroni\")\n\nWir erhalten dann die etwas besser sortierte Ausgabe für die Behandlungen wieder.\n\nemmeans_cld \n\n trt      emmean   SE  df lower.CL upper.CL .group \n pos_crtl   9.67 1.12 171     6.51     12.8  a     \n neg_ctrl  20.02 1.12 171    16.86     23.2   b    \n treat_A   20.97 1.12 171    17.81     24.1   b    \n treat_B   23.35 1.12 171    20.19     26.5   b    \n treat_C   34.96 1.12 171    31.80     38.1    c   \n treat_D   37.46 1.12 171    34.30     40.6    cd  \n treat_E   40.17 1.12 171    37.01     43.3     de \n treat_F   43.23 1.12 171    40.07     46.4      e \n treat_G   50.51 1.12 171    47.35     53.7       f\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 9 estimates \nP value adjustment: bonferroni method for 36 tests \nsignificance level used: alpha = 0.05 \nNOTE: Compact letter displays can be misleading\n      because they show NON-findings rather than findings.\n      Consider using 'pairs()', 'pwpp()', or 'pwpm()' instead. \n\n\nWie die Ausgabe von cld() richtig anmerkt, können compact letter display irreführend sein weil sie eben Nicht-Unterschiede anstatt von signifikanten Unterschieden anzeigen. Zum Anderen sehen wir aber auch, dass wir 36 statistische Tests gerechnet haben und somit zu einem Signifikanzniveau von \\(\\cfrac{\\alpha}{k} = \\cfrac{0.05}{36} \\approx 0.0014\\) testen. Wir brauchen also schon sehr große Unterschiede oder aber eine sehr kleine Streuung um hier signifikante Effekte nachweisen zu können.\nIn Tabelle 31.6 sehen wir das Ergebnis des compact letter display nochmal mit den Mittelwerten der Behandlungslevel zusammen dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. Wir gehen so alle Vergleiche einmal durch.\n\n\nTabelle 31.6— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus emmeans. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\n\ntreat_E\n40\n\n\n\nd\ne\n\n\n\ntreat_F\n43\n\n\n\n\ne\n\n\n\ntreat_G\n45\n\n\n\n\n\nf\n\n\n\n\nAbschließend können wir die Buchstaben aus dem compact letter display noch in die Abbildung 31.11 ergänzen. Hier müssen wir etwas mehr machen um die Buchstaben aus dem Objekt emmeans_cld zu bekommen. Du kannst dann noch die y-Position anpassen wenn du möchtest.\n\nletters_tbl <- emmeans_cld %>% \n  tidy %>% \n  select(trt, label = .group) %>% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 31.11— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n\n31.7.4 … für den Games-Howell-Test\nAbschließend wollen wir uns die Implementierung des compact letter display für den Games-Howell-Test einmal anschauen. Es gilt vieles von dem in diesem Abschnitt schon gesagtes. Wir nutzen die Funktion multcompLetters() aus dem Paket multcompView um uns das compact letter display aus dem Games-Howell-Test wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nght_cld <- games_howell_test(rsp ~ trt, data = data_tbl) %>% \n  mutate(contrast = str_c(group1, \"-\", group2)) %>% \n  pull(p.adj, contrast) %>% \n  multcompLetters() \n\nDas compact letter display kennen wir schon aus der obigen Beschreibung.\n\nght_cld\n\npos_crtl neg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F \n     \"a\"      \"b\"      \"b\"      \"b\"      \"c\"     \"cd\"      \"d\"      \"d\" \n treat_G \n     \"e\" \n\n\nWir können uns dann auch das compact letter display als übersichtlicheren Plot wiedergeben lassen.\n\nght_cld %>% plot\n\n\n\n\nUm die Zusammenhänge besser zu verstehen ist in Tabelle 31.7 nochmal die Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. In der Form können wir alle Vergleiche einmal durchgehen.\n\n\nTabelle 31.7— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus dem Games-Howell-Test. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\ntreat_E\n40\n\n\n\nd\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\n\nWir können dann auch in Abbildung 31.12 sehen, wie das compact letter display mit den Boxplots verbunden wird.\n\nletters_tbl <- ght_cld$Letters %>% \n  enframe(\"trt\", \"label\") %>% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 31.12— Boxplot der Beispieldaten zusammen mit den compact letter display."
  },
  {
    "objectID": "stat-modeling-sensitivity.html",
    "href": "stat-modeling-sensitivity.html",
    "title": "39  Sensitivitätsanalyse",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:48:37"
  },
  {
    "objectID": "stat-modeling-sensitivity.html#theoretischer-hintergrund",
    "href": "stat-modeling-sensitivity.html#theoretischer-hintergrund",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.1 Theoretischer Hintergrund",
    "text": "39.1 Theoretischer Hintergrund\nWir brauchen die Sensitivitätsanalyse wenn wir Beobachtungen aus unseren Daten entfernt oder aber hinzugefügt haben. Das heißt du hast entweder eine Variablenselektion wie im Kapitel 36 beschrieben durchgeführt. Oder aber du hast fehlende Werte wie in Kapitel 38 beschrieben imputiert. Es kann auch sein, dass du Ausreißer aus den Daten entfernt oder aber imputiert hast, wie es in Kapitel 36 beschrieben ist. Im Prinzip kannst du auch alles drei gemacht haben, aber meistens beschränkt sich die Veränderung der Daten nur auf eins der drei Möglichkeiten.\nWie immer brauchen wir natürlich auch Fallzahl. Eine Sensitivitätsanalyse kannst du nicht auf zwanzig bis fünfzig Beobachtungen machen. Du brauchst schon eine gute dreistellige Anzahl, damit du hier sauber Modellieren und Darstellen kannst. Wenn du weniger Beobachtungen hast, dann ist ganz natürlich das einzelne Werte einen riesigen Einfluss haben müssen. Im Zweifel frag einfach einmal bei mir nach, dann können wir die Sachlage diskutieren.\n\n\n\n\n\n\nDas ist hier natürlich eine Sensitivitätsanalyse für Arme. Wie man es richtig umfangreich macht, findest du in einem sehr gutem und umfangreichen Tutorial zu What Makes a Sensitivity Analysis?\nDieses Kapitel ist relativ übersichtlich. Wir werden die Modelle nach der jeweiligen algorithmischen Veränderung uns nochmal anschauen und dann deskriptive entscheiden, ob wir eine große Veränderung in den Daten sehen. Es gibt zwar auch die Möglichkeit die Modelle untereinander zu vergleichen, aber ist hier die Aussagekraft nicht so stark. Die Idee hinter dem Modellvergleich ist eher die Anzahl an Spalten zu verändern und nicht die Werte in der Datenmatrix. Deshalb machen wir es zwar, genießen die Sache aber mit Vorsicht."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-sensitivity.html#genutzte-r-pakete-für-das-kapitel",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.2 Genutzte R Pakete für das Kapitel",
    "text": "39.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, dlookr, broom, modelsummary,\n               see, performance, ggpubr, factoextra, FactoMineR,\n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#daten",
    "href": "stat-modeling-sensitivity.html#daten",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.3 Daten",
    "text": "39.3 Daten\nIn diesem Beispiel betrachten wir wieder die Gummibärchendaten. Auch hier haben wir echte Daten vorliegen, so dass wir Ausreißer entdecken könnten. Da wir hier auch fehlende Werte in den Daten haben, können wir diese fehlenden Werte auch einfach imputieren und uns dann die Effekte anschauen. Das heißt wir haben also einen idealen Datensatz für unsere Sensitivitätsanalysen.\n\ngummi_tbl <- read_excel(\"data/gummibears.xlsx\")  %>%\n  select(gender, age, height, semester) %>% \n  mutate(gender = as_factor(gender)) \n\nIn der Tabelle 36.2 ist der Datensatz gummi_tbl nochmal für die ersten sieben Zeilen dargestellt. Wir werden später sehen, wie sich die Fallzahl von \\(n = 428\\) immer wieder ändert, je nachdem wie wir mit den fehlenden Daten und den Variablen umgehen.\n\n\n\n\nTabelle 39.1— Auszug aus dem Datensatz gummi_tbl. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\ngender\nage\nheight\nsemester\n\n\n\nm\n35\n193\n10\n\n\nw\n21\n159\n6\n\n\nw\n21\n159\n6\n\n\nw\n36\n180\n10\n\n\nm\n22\n180\n3\n\n\nm\nNA\nNA\nNA\n\n\nm\n22\n180\n3"
  },
  {
    "objectID": "stat-modeling-sensitivity.html#das-modell",
    "href": "stat-modeling-sensitivity.html#das-modell",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.4 Das Modell",
    "text": "39.4 Das Modell\nWir wollen jetzt als erstes das volle Modell schätzen. Das heißt wir packen alle Variablen in das Modell und rechnen dann die lineare Regression. Wir wollen herausfinden in wie weit das Alter, das Geschlecht und das Semester einen Einfluss auf die Körpergröße von Studierenden hat.\n\\[\nheight \\sim gender + age + semester\n\\]\nWir haben nichts an den Daten geändert und somit dient unser volles Modell als Benchmark für die anderen. Wenn sich einige Werte der Modellgüten im Vergleich zum vollen Modell ändern, dann wissen wir, dass etwas nicht stimmt.\n\nfit_full <- lm(height ~ gender + age + semester, data = gummi_tbl)\n\nNeben dem vollen Modell rechnen wir auch noch das Nullmodel. Das Nullmodell beinhaltet nur den Intercept und sonst keine Einflussvariable. Wir wollen schauen, ob es überhaupt was bringt eine unserer Variablen in das Modell zu nehmen oder ob wir es auch gleich lassen können. Im Prinzip unsere Kontrolle für das Modellieren.\n\\[\nheight \\sim 1\n\\]\nIn R fitten wir das Nullmodell in dem wir keine Variablen mit in das Modell nehmen sondern nur eine 1 schreiben. Wir haben dann nur den Intercept mit in dem Modell und sonst nichts. Was wir schon aus den anderen Kapiteln wissen ist, dass das Nullmodell ein schlechtes Modell sein wird.\n\nfit_null <- lm(height ~ 1, data = gummi_tbl)\n\nWir schauen uns die Modelle hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-detektion-von-ausreißer",
    "href": "stat-modeling-sensitivity.html#nach-der-detektion-von-ausreißer",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.5 Nach der Detektion von Ausreißer",
    "text": "39.5 Nach der Detektion von Ausreißer\nTeilweise können wir eine Überprüfung auf Ausreißer nur auf einen Datensatz ohne fehlende Werte durchführen. Hier beißt sich dann die Katze in den Schwanz. Deshalb nutzen wir die Funktion diagnose_outlier(), die intern die fehlenden Werte entfernt. Das ist natürlich kein richtiges Vorgehen! Aber wir nutzen ja diesen Abschnitt nur als Beispiel.\nDu findest die Detektion von Ausreißern im Kapitel 36 beschrieben.\n\ndiagnose_outlier(gummi_tbl) \n\n# A tibble: 3 x 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean without_mean\n  <chr>            <int>          <dbl>         <dbl>     <dbl>        <dbl>\n1 age                 30           7.01          42.3     24.1         22.6 \n2 height               0           0            NaN      176.         176.  \n3 semester            24           5.61          10.3      3.19         2.71\n\n\nWir sehen, dass wir in der Variable age und semester nach der Funktion zu urteilen Ausreißer gefunden haben. Deshalb werden wir jetzt diese Ausreißer durch die Funktion imputate_outlier() entsprechend ersetzen. Mal schauen, ob wir damit eine substanzielle Änderung in der Modellierung erhalten.\n\ngummi_out_imp_tbl <- gummi_tbl %>% \n  mutate(age = imputate_outlier(., age, method = \"capping\"),\n         semester = imputate_outlier(., semester, method = \"capping\"))\n\nNun modellieren wir noch mit unseren ersetzten und angepassten Daten die Körpergröße und erhalten den Modellfit zurück. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.\n\nfit_outlier <- lm(height ~ gender + age + semester, data = gummi_out_imp_tbl)\n\nWir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-imputation-von-fehlenden-werten",
    "href": "stat-modeling-sensitivity.html#nach-der-imputation-von-fehlenden-werten",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.6 Nach der Imputation von fehlenden Werten",
    "text": "39.6 Nach der Imputation von fehlenden Werten\nNehmen wir wieder den Gummibärechendatensatz von neuen und imputieren diesmal die fehlenden Werte mit einer univariaten Imputation. Wir machen uns hier nicht die Mühe ein multivariates Verfahren zu nutzen. Das könnte man tun, aber wir wollen hier ja nur den Weg aufzeigen, wie wir den Vergleich der Modelle zur Sensitivitätsanalyse durchführen.\nDu findest die Imputation von fehlenden Werten im Kapitel 38 beschrieben.\nIn unserem Fall imputieren wir alle numerischen Variablen mit dem Mittelwert und die kategoriale Variable mit der Methode rpart. Damit haben wir dann keine fehlenden Werte mehr in den Daten und somit sollte das jetzt auch unserer größter Datensatz für die lineare Regression sein. Nicht vergessen, sobald wir einen fehlenden Wert bei einer Variable in einem Modell haben, fällt die ganze Beobachtung aus dem Modell heraus.\n\ngummi_imp_tbl <- gummi_tbl %>% \n  mutate(age = imputate_na(., age, method = \"mean\"),\n         gender = imputate_na(., gender, method = \"rpart\"),\n         height = imputate_na(., height, method = \"median\"),\n         semester = imputate_na(., semester, method = \"mode\"))\n\nDann rechnen wir noch schnell das Modell für die imputierten Daten. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.\n\nfit_imp <- lm(height ~ gender + age + semester, data = gummi_imp_tbl)\n\nWir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-variablen-selektion",
    "href": "stat-modeling-sensitivity.html#nach-der-variablen-selektion",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.7 Nach der Variablen Selektion",
    "text": "39.7 Nach der Variablen Selektion\nFür die Variablensleketion machen wir es uns sehr einfach. Wir müssen ja nur eine Spalte aus den Daten werfen, mehr ist ja Variablenselektion auch nicht. Wir machen dort nur eine algorithmengetriebene Auswahl. In diesem Fall entscheide ich einfach zufällig welche Variable aus dem Modell muss.\nDu findest die Variablen Selektion im Kapitel 36 beschrieben.\nSomit nehmen wir an, wir hätten eine Variablenselektion durchgeführt und die Variable semester aus dem Modell entfernt.\n\nfit_var_select <- lm(height ~ gender + age, data = gummi_tbl)\n\nAuch dieses Modell schauen wir nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#modellvergleich",
    "href": "stat-modeling-sensitivity.html#modellvergleich",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.8 Modellvergleich",
    "text": "39.8 Modellvergleich\nKommen wir zu dem eigentlichen Modellvergleich. In Tabelle 39.2 sehen wir den Modellvergleich aller fünf Modelle aus diesem Kapitel. Dazu nutzen wir die Funktion modelsummary() aus dem R Paket modelsummary. Wir vergleichen die Modelle untereinander aber vor allem mit dem vollen Modell. Das volle Modell basiert ja auf den ursprünglichen nicht veränderten Daten. Den Intercept können wir erstmal ignorieren. Spannend ist, dass sich der Effekt von gender auf die Körpergröße durch die Imputation um eine Einheit ändert. Der Effekt des Alters verfünffacht sich durch die Outlieranpassung und verdoppelt sich durch die Imputation. Durch die Imputation wird der Effekt des Semesters abgeschwächt.\nWenn wir auf das \\(R^2_{adj}\\) schauen, dann haben wir eine Verschlechterung durch die Imputation. Sonst bleibt der Wert mehr oder minder konstant. Das ist ein gutes Zeichen, dass wir unser Modell nicht vollkommen an die Wand gefahren haben durch unsere Änderung der Daten. Das \\(AIC\\) wird folglich für die Imputationsdaten sehr viel schlechter und nähert sich dem Nullmodell an. Das ist wirklcih kein gutes Zeichen für die Imputation. Da haben wir mehr kaputt als heile gemacht. Wir sehen keinen Efdekt bei dem Fehler \\(RMSE\\), der noch nach dem Fit des Modell übrig bleibt. Aber das kann passieren. Nicht jede Maßzahl muss sich auch ändern. Deshalb haben wir ja mehrere Maßzahlen vorliegen.\n\nmodelsummary(lst(\"Null Modell\" = fit_null,\n                 \"Volles Modell\" = fit_full,\n                 \"Outlier\" = fit_outlier,\n                 \"Imputation\" = fit_imp,\n                 \"Variablen Selektion\" = fit_var_select),\n             estimate  = \"{estimate}\",\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"))\n\n\n\n\nTabelle 39.2—  Modellvergleich mit den fünf Modellen. Wir schauen in wie weit sich die Koeffizienten und Modelgüten für die einzelnen Modelle im direkten Vergleich zum vollen Modell verändert haben. \n \n   \n    Null Modell \n    Volles Modell \n    Outlier \n    Imputation \n    Variablen Selektion \n  \n\n\n (Intercept) \n    175.639 \n    184.147 \n    185.109 \n    183.071 \n    183.622 \n  \n\n  \n    [174.621, 176.658] \n    [180.683, 187.612] \n    [180.859, 189.360] \n    [180.122, 186.019] \n    [180.795, 186.449] \n  \n\n  \n    s.e. = 0.518 \n    s.e. = 1.762 \n    s.e. = 2.162 \n    s.e. = 1.500 \n    s.e. = 1.438 \n  \n\n  \n    t = 339.055 \n    t = 104.532 \n    t = 85.635 \n    t = 122.030 \n    t = 127.688 \n  \n\n  \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n  \n\n genderw \n     \n    -14.896 \n    -14.858 \n    -13.782 \n    -14.786 \n  \n\n  \n     \n    [-16.428, -13.364] \n    [-16.384, -13.331] \n    [-15.188, -12.376] \n    [-16.241, -13.331] \n  \n\n  \n     \n    s.e. = 0.779 \n    s.e. = 0.776 \n    s.e. = 0.715 \n    s.e. = 0.740 \n  \n\n  \n     \n    t = -19.120 \n    t = -19.137 \n    t = -19.269 \n    t = -19.974 \n  \n\n  \n     \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n  \n\n age \n     \n    -0.010 \n    -0.052 \n    -0.022 \n    -0.015 \n  \n\n  \n     \n    [-0.150, 0.130] \n    [-0.231, 0.128] \n    [-0.135, 0.091] \n    [-0.127, 0.098] \n  \n\n  \n     \n    s.e. = 0.071 \n    s.e. = 0.091 \n    s.e. = 0.057 \n    s.e. = 0.057 \n  \n\n  \n     \n    t = -0.140 \n    t = -0.568 \n    t = -0.385 \n    t = -0.256 \n  \n\n  \n     \n    p = 0.889 \n    p = 0.571 \n    p = 0.700 \n    p = 0.798 \n  \n\n semester \n     \n    -0.192 \n    -0.201 \n    -0.082 \n     \n  \n\n  \n     \n    [-0.472, 0.087] \n    [-0.511, 0.110] \n    [-0.347, 0.182] \n     \n  \n\n  \n     \n    s.e. = 0.142 \n    s.e. = 0.158 \n    s.e. = 0.135 \n     \n  \n\n  \n     \n    t = -1.352 \n    t = -1.270 \n    t = -0.612 \n     \n  \n\n  \n     \n    p = 0.177 \n    p = 0.205 \n    p = 0.541 \n     \n  \n\n Num.Obs. \n    402 \n    371 \n    371 \n    428 \n    398 \n  \n\n R2 \n    0.000 \n    0.505 \n    0.505 \n    0.471 \n    0.506 \n  \n\n R2 Adj. \n    0.000 \n    0.501 \n    0.501 \n    0.467 \n    0.504 \n  \n\n AIC \n    3025.6 \n    2544.5 \n    2544.3 \n    2927.9 \n    2720.7 \n  \n\n BIC \n    3033.6 \n    2564.1 \n    2563.8 \n    2948.2 \n    2736.7 \n  \n\n Log.Lik. \n    -1510.792 \n    -1267.252 \n    -1267.128 \n    -1458.932 \n    -1356.371 \n  \n\n F \n     \n    124.678 \n    124.842 \n    125.766 \n     \n  \n\n RMSE \n    10.37 \n    7.37 \n    7.36 \n    7.31 \n    7.31 \n  \n\n\n\n\n\nDas vergleichen von Modellen, die auf unterschiedlichen Daten basieren ist nicht anzuraten. Wir erhalten auch die passende Warnung von der Funktion compare_performance() aus dem R Paket performance. Dennoch hier einmal der Vergleich. Wir sehen, dass die Modelle mit der Ersetzung der Ausreißer und das volle Modell sich stark ähneln. Das selektierte Modell und das imputierte Modell fallen dagegen ab. Da wir ja hier nicht zeigen wollen, dass sich die Modelle unterscheiden, ist das Ergebnis ähnlich zu der Übersicht. Die Imputation hat so nicht funktioniert.\n\n\n# Comparison of Model Performance Indices\n\nName           | Model |    R2 | R2 (adj.) |   RMSE |  Sigma | AIC weights | BIC weights | Performance-Score\n------------------------------------------------------------------------------------------------------------\nfit_outlier    |    lm | 0.505 |     0.501 |  7.363 |  7.403 |       0.531 |       0.531 |            99.21%\nfit_full       |    lm | 0.505 |     0.501 |  7.366 |  7.406 |       0.469 |       0.469 |            95.29%\nfit_var_select |    lm | 0.506 |     0.504 |  7.308 |  7.336 |    2.52e-39 |    1.55e-38 |            66.67%\nfit_imp        |    lm | 0.471 |     0.467 |  7.314 |  7.348 |    2.67e-84 |    1.86e-84 |            64.19%\nfit_null       |    lm | 0.000 |     0.000 | 10.373 | 10.386 |   1.61e-105 |   5.28e-103 |             0.00%\n\n\nWas ist das Fazit aus der Sensitivitätsanalyse für Arme? Nun wir konnten einmal sehen, dass wir auch mit einfachen Werkzeugen Modelle deskriptiv miteinander vergleichen können und dann einen Schluss über die Güte der Detektion von Ausreißern, der Imputation von fehlenden Werten oder aber der Variablenselektion treffen können. Denk immer dran, die Sensitivitätsanalyse findet nach einer sauberen Detektion, Imputation oder Selektion statt und soll nochmal sicherstellen, dass wir nicht künstliche Effekte der Algorithmen modellieren sondern die Effekte in den Daten sehen.\nSensitivitätsanalysen finden eigentlich in dem Kontext von klinischen Studien statt. Der Trend geht aber natürlich auch nicht an den Agrarwissenschaften vorbei und solltest du den Begriff mal hören, weist du wo Sensitivitätsanalyse hingehören."
  },
  {
    "objectID": "stat-modeling-multinom.html",
    "href": "stat-modeling-multinom.html",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:49:02\nWas machen wir wenn wir ein Outcome haben mit mehr als zwei Kategorien. Wenn wir nur zwei Kategorien hätten, dann würden wir eine logistische Regression rechnen. Wenn wir mehr als zwei Kategorien haben, dann sind wir in dem Fall der multinomialen / ordinalen logistischen Regression. Wir rechnen eine multinomialen Regression, wenn wir keine Ordnung in den Kategorien in dem Outcome haben. Wenn wir eine Ordnung vorliegen haben, dann nutzen wir die ordinale Regression. Wir werden uns erstmal eine ordinale Regression anschauen mit nur drei geordenten Stufen. Dann schauen wir uns einmal wie wir eine ordinale Regression auf Boniturnoten in der Likert-Skala rechnen. Wir machen das getrennt, denn wir sind bei wenigen geordneten Kategorien meistens noch am Effekt zwischen den Kategorien interessiert. Im Gegensatz wollen wir bei einem Outcome mit Boniturnoten einen Gruppenvergleich rechnen. Dann interessiert uns der Unterschied und die Effekte zwischen den Boniturnoten nicht. Deshalb trennen wir das hier etwas auf.\nIm zweiten Teil wollen wir uns dann noch eine multinominale Regression auf ungeordneten Kategorien eines Outcomes anschauen. Korrelterweise tuen wir nur so, als wäre unser vorher geordnetes Outcome dann eben ungeordnet. Das macht dann aber bei deiner Anwendung dann keinen großen Unterschied. Als eine Alternative zur multinationalen Regression stelle ich dann noch die logistsiche Regression vor. Wir können nämlich einfach unsere Daten nach dem Outcome jeweils in kleinere Datensätze mit nur jeweils zwei der Kategorien aufspalten. Das ist zwar nicht schön, aber auch eine Möglichkeit mit einem Problem umzugehen.\nIch gehe hier nicht auf die Theorie hinter der multinomialen / ordinalen logistischen Regression ein. Wenn dich dazu mehr interessiert findest du in den jeweiligen Abschnitten dann noch eine passende Referenz. Da kannst du dann schauen, welche Informationen du noch zusätzlich findest."
  },
  {
    "objectID": "stat-modeling-multinom.html#annahmen-an-die-daten",
    "href": "stat-modeling-multinom.html#annahmen-an-die-daten",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.1 Annahmen an die Daten",
    "text": "42.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Multinomialverteilung.\nIm folgenden Kapitel zu der multinomialen / ordinalen logistischen linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 36 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Multinomialverteilung. Damit finden wir im Outcome im Falle der multinomialen logistischen linearen Regression ungeordnete Kategorien und im Falle der ordinalen logistischen linearen Regression geordnete Kategorien.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen."
  },
  {
    "objectID": "stat-modeling-multinom.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-multinom.html#genutzte-r-pakete-für-das-kapitel",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.2 Genutzte R Pakete für das Kapitel",
    "text": "42.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, gtsummary,\n               ordinal, janitor, MASS, nnet, flextable,\n               emmeans, multcomp, ordinal, see, scales,\n               janitor)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-multinom.html#daten",
    "href": "stat-modeling-multinom.html#daten",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.3 Daten",
    "text": "42.3 Daten\nIm Folgenden wollen wir uns die Daten von den infizierten Ferkeln noch einmal anschauen. Wir nehmen als Outcome die Spalte frailty und damit die Gebrechlichkeit der Ferkel. Die Spalte ordnen wir einmal nach robust, pre-frail und frail. Wobei robust ein gesundes Ferkel beschreibt und frail ein gebrechliches Ferkel. Damit wir später die Richtung des Effekts richtig interpretieren können, müssen wir von gut nach schlecht sortieren. Das brauchen wir nicht, wenn wir Boniturnoten haben, dazu mehr in einem eigenen Abschnitt. Wir bauen uns dann noch einen Faktor mit ebenfalls der Spalte frailty in der wir so tun, als gebe es diese Ordnung nicht. Wir werden dann die ordinale Regression mit dem Outcome frailty_ord rechnen und die multinominale Regression dann mit dem Outcome frailty_fac durchführen.\n\npig_tbl <- read_excel(\"data/infected_pigs.xlsx\") %>%\n  mutate(frailty_ord = ordered(frailty, levels = c(\"robust\", \"pre-frail\", \"frail\")),\n         frailty_fac = as_factor(frailty)) %>% \n  select(-infected)\n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der Tabelle 42.1 an.\n\n\n\n\nTabelle 42.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\nfrailty_ord\nfrailty_fac\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n49.88\n16.94\n3.07\nrobust\nrobust\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n58.2\n17.95\n4.88\nrobust\nrobust\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n56.8\n19.02\n3.98\nrobust\nrobust\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.47\n18.98\n5.18\nrobust\nrobust\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n59.85\n16.57\n6.71\nrobust\nrobust\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n58.1\n18.22\n5.43\nrobust\nrobust\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\nrobust\n57.05\n17.95\n6.16\nrobust\nrobust\n\n\n56\nmale\nwest\n13.91\n20.8\npre-frail\n50.84\n18.02\n6.52\npre-frail\npre-frail\n\n\n57\nmale\nnorthwest\n12.49\n21.95\nrobust\n55.51\n17.73\n3.94\nrobust\nrobust\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n58.5\n18.23\n2.73\nrobust\nrobust\n\n\n59\nfemale\nnorth\n13.13\n20.23\npre-frail\n57.33\n17.21\n5.42\npre-frail\npre-frail\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n55.85\n17.76\n6.18\nrobust\nrobust\n\n\n\n\n\n\nDas wären dann die Daten, die wir für unsere Modelle dann brauchen. Schauen wir mal was wir jetzt bei der ordinalen Regression herausbekommen."
  },
  {
    "objectID": "stat-modeling-multinom.html#sec-ordinal",
    "href": "stat-modeling-multinom.html#sec-ordinal",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.4 Ordinale logistische Regression",
    "text": "42.4 Ordinale logistische Regression\nEs gibt sicherlich einiges an Paketen in R um eine ordinale Regression durchzuführen. Ich nutze gerne die Funktion polr aus dem R Paket MASS. Daneben gibt es auch noch das R Paket ordinal mit der Funktion clm(), die wir dann noch im Anschluss besprechen werden. Ich nutze jetzt erstmal die Funktion polr, da wir hier noch eine externe Referenz haben, die uns noch detailliertere Informationen liefern kann.\n\n\nIch verweise gerne hier auf das tolle Tutorium Ordinal Logistic Regression | R Data Analysis Examples. Hier erfährst du noch mehr über die Analyse der ordinalen logistischen Regression.\nWir schon erwähnt sparen wir usn die mathematischen Details und utzen gleich die Funktion polr auf unserem Outcome frailty. Wir müssen keine Verteilungsfamilie extra angeben, dass haben wir schon mit der Auswahl der Funktion getan. Die Funktion polr kann nur eine ordinale Regression rechnen und wird einen Fehler ausgeben, wenn das Outcome \\(y\\) nicht passt.\n\nologit_fit <- polr(frailty_ord ~ age + sex + location + activity + crp + \n                     bloodpressure + weight + creatinin, \n                   data = pig_tbl)\n\nSchauen wir uns einmal die Ausgabe des Modellfits der ordinalen Regression mit der Funktion summary() an. Wir sehen eine Menge Zahlen und das wichtigste für uns ist ja, dass wir zum einen Wissen, dass wir auch die ordinale Regression auf der \\(link\\)-Funktion rechnen. Wir erhalten also wieder eine Transformation des Zusammenhangs zurück, wie wir es schon bei der Poisson Regression sowie bei der logistischen Regression hatten.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\n\nologit_fit %>% summary()\n\nCall:\npolr(formula = frailty_ord ~ age + sex + location + activity + \n    crp + bloodpressure + weight + creatinin, data = pig_tbl)\n\nCoefficients:\n                      Value Std. Error t value\nage                0.005199    0.02171  0.2395\nsexmale            0.500202    0.28214  1.7729\nlocationnortheast -0.315377    0.27927 -1.1293\nlocationnorthwest -0.469579    0.25166 -1.8659\nlocationwest      -0.046641    0.27381 -0.1703\nactivity          -0.130300    0.07409 -1.7587\ncrp               -0.133107    0.06857 -1.9412\nbloodpressure      0.016181    0.03083  0.5249\nweight             0.047346    0.07080  0.6687\ncreatinin         -0.022933    0.07097 -0.3231\n\nIntercepts:\n                 Value   Std. Error t value\nrobust|pre-frail -2.1188  3.0707    -0.6900\npre-frail|frail  -0.3625  3.0694    -0.1181\n\nResidual Deviance: 776.36 \nAIC: 800.36 \n\n\nUnsere Ausgabe teilt sich in zwei Teile auf. In dem oberen Teil sehen wir die Koeffizienten des Modells zusammen mit dem Fehler und der Teststatistik. Was wir nicht sehen, ist ein \\(p\\)-Wert. Die Funktion rechnet uns keinen Signifikanztest aus. Das können wir aber gleich selber machen. In dem Abschnitt Intercepts finden wir die Werte für die Gruppeneinteilung auf der link-Funktion wieder. Wir transformieren ja unsere drei Outcomekategorien in einen kontinuierliche Zahlenzusammenhang. Trotzdem müssen ja die drei Gruppen auch wieder auftauchen. In dem Abschnitt Intercepts finden wir die Grenzen für die drei Gruppen auf der link-Funktion.\n\n\nWir gibt auch ein Tutorial für How do I interpret the coefficients in an ordinal logistic regression in R?\nBerechnen wir jetzt einmal die \\(p\\)-Werte per Hand. Dafür brauchen wir die absoluten Werte aus der t value Spalte aus der summary des Modellobjekts. Leider ist die Spalte nicht schön formatiert und so müssen wir uns etwas strecken um die Koeffizienten sauber aufzuarbeiten. Wir erhalten dann das Objekt coef_tbl wieder.\n\ncoef_tbl <- summary(ologit_fit) %>% \n  coef %>% \n  as_tibble(rownames = \"term\") %>% \n  clean_names() %>% \n  mutate(t_value = abs(t_value))\n\ncoef_tbl\n\n# A tibble: 12 x 4\n   term                 value std_error t_value\n   <chr>                <dbl>     <dbl>   <dbl>\n 1 age                0.00520    0.0217   0.239\n 2 sexmale            0.500      0.282    1.77 \n 3 locationnortheast -0.315      0.279    1.13 \n 4 locationnorthwest -0.470      0.252    1.87 \n 5 locationwest      -0.0466     0.274    0.170\n 6 activity          -0.130      0.0741   1.76 \n 7 crp               -0.133      0.0686   1.94 \n 8 bloodpressure      0.0162     0.0308   0.525\n 9 weight             0.0473     0.0708   0.669\n10 creatinin         -0.0229     0.0710   0.323\n11 robust|pre-frail  -2.12       3.07     0.690\n12 pre-frail|frail   -0.363      3.07     0.118\n\n\nUm die Fläche rechts von dem \\(t\\)-Wert zu berechnen, können wir zwei Funktionen nutzen. Die Funktion pnorm() nimmt eine Standradnormalverteilung an und die Funktion pt() vergleicht zu einer \\(t\\)-Verteilung. Wenn wir rechts von der Verteilung schauen wollen, dann müssen wir die Option lower.tail = FALSE wählen. Da wir auch zweiseitig statistisch Testen, müssen wir den ausgerechneten \\(p\\)-Wert mal zwei nehmen. Hier einmal als Beispiel für den \\(t\\)-Wert von \\(1.96\\). Mit pnorm(1.96, lower.tail = FALSE) * 2 erhalten wir \\(0.05\\) als Ausgabe. Das ist unser \\(p\\)-Wert. Was uns ja nicht weiter überrascht. Denn rechts neben dem Wert von \\(1.96\\) in einer Standardnormalverteilung ist ja \\(0.05\\). Wenn wir einen \\(t\\)-Test rechnen würden, dann müssten wir noch die Freiheitsgrade df mit angeben. Mit steigendem \\(n\\) nähert sich die \\(t\\)-Verteilung der Standardnormalverteilung an. Wir haben mehr als \\(n = 400\\) Beobachtungen, daher können wir auch df = 400 setzen. Da kommt es auf eine Zahl nicht an. Wir erhalten mit pt(1.96, lower.tail = FALSE, df = 400) * 2 dann eine Ausgabe von \\(0.0507\\). Also fast den gleichen \\(p\\)-Wert.\nIm Folgenden setzte ich die Freiheitsgrade df = 3 dammit wir was sehen. Bei so hohen Fallzahlen wir in unserem beispiel würden wir sonst keine Unterschiede sehen.\n\ncoef_tbl %>% \n  mutate(p_n = pnorm(t_value, lower.tail = FALSE) * 2,\n         p_t = pt(t_value, lower.tail = FALSE, df = 3) * 2) %>% \n  mutate(across(where(is.numeric), round, 3))\n\n# A tibble: 12 x 6\n   term               value std_error t_value   p_n   p_t\n   <chr>              <dbl>     <dbl>   <dbl> <dbl> <dbl>\n 1 age                0.005     0.022   0.239 0.811 0.826\n 2 sexmale            0.5       0.282   1.77  0.076 0.174\n 3 locationnortheast -0.315     0.279   1.13  0.259 0.341\n 4 locationnorthwest -0.47      0.252   1.87  0.062 0.159\n 5 locationwest      -0.047     0.274   0.17  0.865 0.876\n 6 activity          -0.13      0.074   1.76  0.079 0.177\n 7 crp               -0.133     0.069   1.94  0.052 0.148\n 8 bloodpressure      0.016     0.031   0.525 0.6   0.636\n 9 weight             0.047     0.071   0.669 0.504 0.552\n10 creatinin         -0.023     0.071   0.323 0.747 0.768\n11 robust|pre-frail  -2.12      3.07    0.69  0.49  0.54 \n12 pre-frail|frail   -0.363     3.07    0.118 0.906 0.913\n\n\nDamit haben wir einmal händisch uns die \\(p\\)-Werte ausgerechnet. Jetzt könnte man sagen, dass ist ja etwas mühselig. Gibt es da nicht auch einen einfacheren Weg? Ja wir können zum einen die Funktion tidy() nutzen um die 95% Konfidenzintervalle und die exponierten Effektschätzer aus der ordinalen Regresssion zu erhalten. Wir erhalten aber wieder keine \\(p\\)-Werte sondern müssten uns diese \\(p\\)- Werte dann wieder selber berechnen.\n\nologit_fit %>% \n  tidy(conf.int = TRUE, exponentiate = TRUE) %>% \n  select(-coef.type)\n\n# A tibble: 12 x 6\n   term              estimate std.error statistic conf.low conf.high\n   <chr>                <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n 1 age                  1.01     0.0217     0.239    0.963      1.05\n 2 sexmale              1.65     0.282      1.77     0.952      2.88\n 3 locationnortheast    0.730    0.279     -1.13     0.420      1.26\n 4 locationnorthwest    0.625    0.252     -1.87     0.381      1.02\n 5 locationwest         0.954    0.274     -0.170    0.557      1.63\n 6 activity             0.878    0.0741    -1.76     0.758      1.01\n 7 crp                  0.875    0.0686    -1.94     0.765      1.00\n 8 bloodpressure        1.02     0.0308     0.525    0.957      1.08\n 9 weight               1.05     0.0708     0.669    0.913      1.21\n10 creatinin            0.977    0.0710    -0.323    0.850      1.12\n11 robust|pre-frail     0.120    3.07      -0.690   NA         NA   \n12 pre-frail|frail      0.696    3.07      -0.118   NA         NA   \n\n\nUm all dieses Berechnen zu umgehen, können wir dann auch die Funktion model_parameters() nutzen. Hier berechnen wir dann die \\(p\\)-Wert mit \\(df = 400\\) aus einer \\(t\\)-Verteilung. Damit umgehen wir das Problem, dass unser Modellfit keine \\(p\\)-Werte liefert.\n\nologit_fit %>% \n  model_parameters() \n\n# alpha\n\nParameter        | Log-Odds |   SE |        95% CI | t(400) |     p\n-------------------------------------------------------------------\nrobust|pre-frail |    -2.12 | 3.07 | [-8.16, 3.92] |  -0.69 | 0.491\npre-frail|frail  |    -0.36 | 3.07 | [-6.40, 5.67] |  -0.12 | 0.906\n\n# beta\n\nParameter            | Log-Odds |   SE |        95% CI | t(400) |     p\n-----------------------------------------------------------------------\nage                  | 5.20e-03 | 0.02 | [-0.04, 0.05] |   0.24 | 0.811\nsex [male]           |     0.50 | 0.28 | [-0.05, 1.06] |   1.77 | 0.077\nlocation [northeast] |    -0.32 | 0.28 | [-0.87, 0.23] |  -1.13 | 0.259\nlocation [northwest] |    -0.47 | 0.25 | [-0.97, 0.02] |  -1.87 | 0.063\nlocation [west]      |    -0.05 | 0.27 | [-0.59, 0.49] |  -0.17 | 0.865\nactivity             |    -0.13 | 0.07 | [-0.28, 0.01] |  -1.76 | 0.079\ncrp                  |    -0.13 | 0.07 | [-0.27, 0.00] |  -1.94 | 0.053\nbloodpressure        |     0.02 | 0.03 | [-0.04, 0.08] |   0.52 | 0.600\nweight               |     0.05 | 0.07 | [-0.09, 0.19] |   0.67 | 0.504\ncreatinin            |    -0.02 | 0.07 | [-0.16, 0.12] |  -0.32 | 0.747\n\n\nIn Tabelle 42.2 sehen wir nochmal die Ergebnisse der ordinalen Regression einmal anders aufgearbeitet. Wir aber schon bei der Funktion tidy() fehlen in der Tabelle die \\(p\\)-Werte. Wir können aber natürlich auch eine Entscheidung über die 95% Konfidenzintervalle treffen. Wenn die 1 mit im 95% Konfidenzintervall ist, dann können wir die Nullhypothese nicht ablehnen.\n\nologit_fit %>% \n  tbl_regression(exponentiate = TRUE) %>% \n  as_flex_table()\n\n\n\n\n\n\nTabelle 42.2—  Tabelle der Ergebnisse der ordinalen Regression. \n\nCharacteristic\nOR1\n95% CI1\n\n\n\nage\n1.01\n0.96, 1.05\n\n\nsex\n\n\n\n\nfemale\n—\n—\n\n\nmale\n1.65\n0.95, 2.88\n\n\nlocation\n\n\n\n\nnorth\n—\n—\n\n\nnortheast\n0.73\n0.42, 1.26\n\n\nnorthwest\n0.63\n0.38, 1.02\n\n\nwest\n0.95\n0.56, 1.63\n\n\nactivity\n0.88\n0.76, 1.01\n\n\ncrp\n0.88\n0.76, 1.00\n\n\nbloodpressure\n1.02\n0.96, 1.08\n\n\nweight\n1.05\n0.91, 1.21\n\n\ncreatinin\n0.98\n0.85, 1.12\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nWi es im gazen Kapitel schon durchscheint, die Interpreation der \\(OR\\) aus einer ordinalen Regression ist nicht einfach, geschweige den intuitiv. Was wir haben ist der Trend. Wir haben unser Outcome von robust zu frail sortiert und damit von gut nach schlecht. Wir können so die Richtung der Variablen in unserem Modell interpretieren. Das heißt, dass männliche Ferkel eher von einer Gebrechlichkeit betroffen sind als weibliche Ferkel. Oder wir sagen, dass ein ansteigender CRP Wert führt zu weniger Gebrechlichkeit. Auf diesem Niveau lassen sich die \\(OR\\) einer ordinalen Regression gut interpretieren."
  },
  {
    "objectID": "stat-modeling-multinom.html#cumulative-link-models-clm-für-ordinale-daten",
    "href": "stat-modeling-multinom.html#cumulative-link-models-clm-für-ordinale-daten",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.5 Cumulative Link Models (CLM) für ordinale Daten",
    "text": "42.5 Cumulative Link Models (CLM) für ordinale Daten\nJetzt kommen wir nochmal zu dem Fall, dass wir Boniturdaten vorliegen habe. Das heißt, wir bauen uns flux wieder einen Datensatz mit drei Blöcken mit jeweils drei Wiederholungen. Wir haben Weizen angepflanzt und bonitieren die Weizenpflanzen nach der Likert Skala. Dabei bedeutet dann eine 1 ein schlechte Note und eine 9 die best mögliche Note. Wir hätten natürlich hier auch einen Kurskal-Wallis-Test rechnen können und dann im Anschluss einen paarweisen Wilcoxon Test. Nun modellieren wir hier aber die Boniturnoten mal mit einer ordinalen Regression und rechnen den anschließenden Gruppenvergleich dann mit dem R Paket emmeans.\n\n\nWir finden auch ein Tutorial zu Introduction to Cumulative Link Models (CLM) for Ordinal Data.\nUnser Datensatz grade_tbl enthält den Faktor block mit drei Levels sowie den Faktor variety mit fünf Leveln. Jedes Level repränsentiert dabei eine Weizensorte.\n\ngrade_tbl <- tibble(block = rep(c(\"I\", \"II\", \"III\"), each = 3),\n                    A = c(2,3,4,3,3,2,4,2,1),\n                    B = c(7,9,8,9,7,8,9,6,7),\n                    C = c(6,5,5,7,5,6,4,7,6),\n                    D = c(2,3,1,2,1,1,2,2,1),\n                    E = c(4,3,7,5,6,4,5,7,5)) %>%\n  gather(key = variety, value = grade, A:E) %>% \n  mutate(grade_ord = ordered(grade))\n\nWir schauen uns nochmal den Datensatz an und sehen, dass wir einmal die Spalte grade als numerische Spalte vorliegen haben udn einmal als geordneten Faktor. Wir brauchen die numerische Spalte um die Daten besser in ggplot() darstellen zu können.\n\ngrade_tbl\n\n# A tibble: 45 x 4\n   block variety grade grade_ord\n   <chr> <chr>   <dbl> <ord>    \n 1 I     A           2 2        \n 2 I     A           3 3        \n 3 I     A           4 4        \n 4 II    A           3 3        \n 5 II    A           3 3        \n 6 II    A           2 2        \n 7 III   A           4 4        \n 8 III   A           2 2        \n 9 III   A           1 1        \n10 I     B           7 7        \n# ... with 35 more rows\n\n\nIn Abbildung 42.1 sehen wir einmal die Daten als Dotplot dargestellt. Auf der x-Achse sind die Weizensorten und auf der y-Achse die Boniturnoten. Ich habe noch die zusätzlichen Linien für jede einzelne Note mit eingezeichnet.\n\nggplot(grade_tbl, aes(variety, grade, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.6), dotsize = 0.75) +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9)) +\n  scale_fill_okabeito() \n\n\n\nAbbildung 42.1— Dotplot des Datenbeispiels für die Bonitur von fünf Weizensorten.\n\n\n\n\nJetzt können wir schon die Funktion clm() aus dem R Paket ordinal verwenden um die ordinale Regression zu rechnen. Wir haben in dem Paket ordinal noch weitere Modelle zu Verfügung mit denen wir auch komplexere Designs bis hin zu linearen gemischten Modellen für eine ordinale Regresssion rechnen können. Da wir mit Boniturnoten als Outcome arbeiten setzen wir auch die Option threshold = \"symmetric\". Damit teilen wir der Funktion clm() mit, dass wir es mit einer symmetrischen Notenskala zu tun haben. Wenn du das nicht hast, dass kannst du die Option auch of \"flexible\" stellen. Dann wird eine nicht symmetrische Verteilung des Outcomes angenommen.\n\nclm_fit <- clm(grade_ord ~ variety + block, data = grade_tbl,\n               threshold = \"symmetric\")\n\nWir können uns dann den Fit des Modells wieder in der Funktion model_parameters() einmal anschauen. Wir brauchen aber die Ausgabe nicht weiter. Wir werden Koeffizienten des Modells jetzt verwenden um die Gruppenvergleiche zu rechnen. Ich habe hier einmal die \\(OR\\) über die Option exponentiate = TRUE mir ausgeben lassen. Leider sind die \\(OR\\) so alleine nicht zu interpretieren.\n\nclm_fit %>% \n  model_parameters(exponentiate = TRUE)\n\n# Intercept\n\nParameter | Odds Ratio |     SE |          95% CI |    z |      p\n-----------------------------------------------------------------\ncentral 1 |      22.02 |  20.78 | [ 3.47, 139.98] | 3.28 | 0.001 \ncentral 2 |     101.42 | 109.71 | [12.17, 845.18] | 4.27 | < .001\nspacing 1 |       4.30 |   1.89 | [ 1.82,  10.16] | 3.33 | < .001\nspacing 2 |      35.04 |  25.42 | [ 8.46, 145.23] | 4.90 | < .001\nspacing 3 |     169.31 | 142.84 | [32.40, 884.76] | 6.08 | < .001\n\n# Location Parameters\n\nParameter   | Odds Ratio |      SE |             95% CI |     z |      p\n------------------------------------------------------------------------\nvariety [B] |    5917.41 | 9556.45 | [249.73, 1.40e+05] |  5.38 | < .001\nvariety [C] |     137.83 |  161.11 | [ 13.94,  1362.41] |  4.21 | < .001\nvariety [D] |       0.14 |    0.13 | [  0.02,     0.87] | -2.11 | 0.035 \nvariety [E] |      57.06 |   62.60 | [  6.64,   490.03] |  3.69 | < .001\nblock [II]  |       0.95 |    0.64 | [  0.26,     3.52] | -0.07 | 0.942 \nblock [III] |       0.89 |    0.63 | [  0.22,     3.56] | -0.17 | 0.865 \n\n\nEs ist auch möglich auf dem Modellfit eine ANOVA zu rechnen. Wir machen das hier einmal, aber wir erwaten natürlich einen signifikanten Effekt von der Sorte. Die Signifikanz konnten wir ja schon oben im Dortplot sehen.\n\nanova(clm_fit)\n\nType I Analysis of Deviance Table with Wald chi-square tests\n\n        Df   Chisq Pr(>Chisq)    \nvariety  4 37.9927  1.124e-07 ***\nblock    2  0.0292     0.9855    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIch hatte ja eben geschrieben, dass wir die Effektschätzer nicht nutzen. Wir können mit den \\(OR\\) aus dem Modell nichts anfangen. Stattdessen nutzen wir die Schätzer der link-Funktion, also die Log Odds, um den paarweisen Gruppenvergleich zu rechnen. Wir nutzen dafür die Funktion emmeans() und lassen uns das compact letter display über die Funktion cld() wiedergeben. Ich habe dann noch die Ausgabe einmal nach den Sorten sortiert und nicht nach dem compact letter display. Dadurch lassen sich die Buchstaben besser zum Dotplot vergleichen.\n\nclm_fit %>% \n  emmeans(~ variety) %>% \n  cld(Letters = letters) %>% \n  arrange(variety)\n\n variety emmean    SE  df asymp.LCL asymp.UCL .group\n A       -3.912 0.887 Inf    -5.650     -2.17  a    \n B        4.774 0.966 Inf     2.881      6.67    c  \n C        1.014 0.623 Inf    -0.206      2.23   b   \n D       -5.848 1.010 Inf    -7.828     -3.87  a    \n E        0.132 0.639 Inf    -1.120      1.38   b   \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 5 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping letter,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWir erinnern uns, gleiche Buchstaben heißen kein Unterschied zwischen den Weizensorten. Damit ist die Weizensorte A gleich der Weizensorte D. Die Weizensorte C ist gleich der Weizensorte E. Am Ende unterscheidet sich die Weizensorte B von allen anderen Sorten. Wir können aber die Werte in der Spalte emmean nicht als Notenunterschied interpretieren. Dafür müssen wir dann den Median für die Gruppen berechnen. Wir können dann das compact letter display händisch ergänzen.\n\ngrade_tbl %>% \n  group_by(variety) %>% \n  summarise(median(grade)) %>% \n  mutate(cld = c(\"a\", \"  c\", \" b \", \"a  \", \" b \"))\n\n# A tibble: 5 x 3\n  variety `median(grade)` cld  \n  <chr>             <dbl> <chr>\n1 A                     3 \"a\"  \n2 B                     8 \"  c\"\n3 C                     6 \" b \"\n4 D                     2 \"a  \"\n5 E                     5 \" b \"\n\n\nDu könntest dir auch die Buchstaben des compact letter display aus einem Objekt ziehen und nicht händisch übertragen. Ich habe mir hier aber einen Schritt gespart."
  },
  {
    "objectID": "stat-modeling-multinom.html#sec-multinom",
    "href": "stat-modeling-multinom.html#sec-multinom",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.6 Multinomiale logistische Regression",
    "text": "42.6 Multinomiale logistische Regression\nWas machen wir in eine multinomialen logistische Regression? Im Gegensatz zu der ordinalen Regression haben wir in der multinominalen Regression keine Ordnung in unserem Outcome. Das macht die Sache dann schon eine Nummer komplizierter. Und wir lösen dieses Problem indem wir ein Level des Outcomes oder eben eine Kategorie des Outcomes als Referenz definieren. Dann haben wir wieder unsere Ordnung drin. Und die Definition der Referenz ist auch manchmal das schwerste Unterfangen. Wenn ich keine Ordnung in meinem Outcome habe, wie soll ich dann die Referenz bestimmen? Aber das ist dann immer eine Frage an den konkreten Datensatz. Hier basteln wir uns ja die Fragestellung so hin, dass es passt.\n\n\nIch verweise gerne hier auf das tolle Tutorium Multinomial Logistic Regression | R Data Analysis Examples. Hier erfährst du noch mehr über die Analyse der multinominale logistischen Regression.\nUm eine Referenz in dem Outcome zu definieren nutzen wir die Funktion relevel() und setzen als unsere Referenz das Level frail aus unserem Outcome frailty. Wir hätten auch jedes andere Level als Referenz nehmen können. Zu dieser Referenz werden wir jetzt unser Modell anpassen. Ich nehme immer als Referenz das schlechteste im Sinne von nicht gut. In unserem Fall ist das eben das Level frail.\n\npig_tbl <- pig_tbl %>% \n  mutate(frailty_fac = relevel(frailty_fac, ref = \"frail\"))\n\nNachdem wir unsere Referenz definiert haben, können wir wieder recht einfach mit der Funktion multinom() aus dem Paket nnet die multinominalen Regression rechnen. Ich mache keinen Hehl daraus. Ich mag die Funktion nicht, da die Ausgabe der Funktion sehr unsortiert ist und uns nicht gerade die Arbeit erleichtert. Auch schweigt die Funktion nicht, sondern muss immer eine Ausgabe wiedergeben. Finde ich sehr unschön.\n\nmultinom_fit <- multinom(frailty_fac ~ age + sex + location + activity + crp + bloodpressure + weight + creatinin, \n                         data = pig_tbl)\n\n# weights:  36 (22 variable)\ninitial  value 452.628263 \niter  10 value 401.130508\niter  20 value 381.498488\niter  30 value 380.694398\nfinal  value 380.689300 \nconverged\n\n\nDie Standardausgabe von multinom() hat wiederum keine \\(p\\)-Werte und wir könnten uns über die Funktion pnorm() wiederum aus den \\(t\\)-Werten unsere \\(p\\)-Werte berechnen. Leider erspart sich multinom() selbst den Schritt die \\(t\\)-Werte zu berechnen, so dass wir die \\(t\\)-Werte selber berechnen müssen. Nicht das es ein Problem wäre, aber schön ist das alles nicht. Im Folgenden siehst du dann einmal die Berechnung der \\(p\\)-Werte über die Berechnung der Teststatistik.\n\nz_mat <- summary(multinom_fit)$coefficients/summary(multinom_fit)$standard.errors\np_n <- (1 - pnorm(abs(z_mat), 0, 1)) * 2\np_n\n\n          (Intercept)        age   sexmale locationnortheast locationnorthwest\nrobust      0.4775726 0.34659718 0.1394380         0.4904806         0.1320405\npre-frail   0.5263984 0.05534541 0.6300418         0.7989195         0.6659467\n          locationwest  activity        crp bloodpressure    weight  creatinin\nrobust       0.8648791 0.4539323 0.03667107     0.2398979 0.9118323 0.12787728\npre-frail    0.9714730 0.4295601 0.20119551     0.1570281 0.5565101 0.02860694\n\n\nJetzt müssten wir diese \\(pp\\)-Werte aus der Matrix noch mit unseren Koeffizienten verbauen und da hört es dann bei mir auf. Insbesondere da wir ja mit model_parameters() eine Funktion haben, die uns in diesem Fall wirklich gut helfen kann. Wir nehmen hier zwar die \\(t\\)-Verteilung an und haben damit leicht höre \\(p\\)-Werte, aber da wir eine so große Anzahl an Beobachtungen haben, fällt dieser Unterschied nicht ins Gewicht.\n\nmultinom_fit %>% model_parameters(exponentiate = TRUE)\n\n# Response level: robust\n\nParameter            | Odds Ratio |   SE |         95% CI | t(390) |     p\n--------------------------------------------------------------------------\n(Intercept)          |       0.03 | 0.16 | [0.00, 433.73] |  -0.71 | 0.478\nage                  |       1.03 | 0.04 | [0.96,   1.11] |   0.94 | 0.347\nsex [male]           |       0.50 | 0.23 | [0.20,   1.25] |  -1.48 | 0.140\nlocation [northeast] |       1.35 | 0.59 | [0.57,   3.18] |   0.69 | 0.491\nlocation [northwest] |       1.88 | 0.79 | [0.82,   4.27] |   1.51 | 0.133\nlocation [west]      |       1.08 | 0.46 | [0.46,   2.51] |   0.17 | 0.865\nactivity             |       1.09 | 0.13 | [0.86,   1.39] |   0.75 | 0.454\ncrp                  |       1.26 | 0.14 | [1.01,   1.56] |   2.09 | 0.037\nbloodpressure        |       0.94 | 0.05 | [0.86,   1.04] |  -1.18 | 0.241\nweight               |       0.99 | 0.11 | [0.79,   1.23] |  -0.11 | 0.912\ncreatinin            |       1.20 | 0.14 | [0.95,   1.50] |   1.52 | 0.129\n\n# Response level: pre-frail\n\nParameter            | Odds Ratio |   SE |         95% CI | t(390) |     p\n--------------------------------------------------------------------------\n(Intercept)          |       0.04 | 0.20 | [0.00, 945.94] |  -0.63 | 0.527\nage                  |       1.07 | 0.04 | [1.00,   1.16] |   1.92 | 0.056\nsex [male]           |       0.79 | 0.39 | [0.30,   2.08] |  -0.48 | 0.630\nlocation [northeast] |       0.89 | 0.42 | [0.35,   2.24] |  -0.25 | 0.799\nlocation [northwest] |       1.21 | 0.54 | [0.51,   2.90] |   0.43 | 0.666\nlocation [west]      |       1.02 | 0.46 | [0.42,   2.48] |   0.04 | 0.971\nactivity             |       0.90 | 0.12 | [0.70,   1.16] |  -0.79 | 0.430\ncrp                  |       1.16 | 0.14 | [0.92,   1.46] |   1.28 | 0.202\nbloodpressure        |       0.93 | 0.05 | [0.84,   1.03] |  -1.42 | 0.158\nweight               |       1.07 | 0.13 | [0.85,   1.36] |   0.59 | 0.557\ncreatinin            |       1.31 | 0.16 | [1.03,   1.68] |   2.19 | 0.029\n\n\nWas sehen wir? Zuerst haben wir etwas Glück. Den unsere Referenzlevel macht dann doch Sinn. Wir vergleichen ja das Outcomelevel robust zu frail und das Outcomelevel pre-frail zu frail. Dann haben wir noch das Glück, dass durch unsere Ordnung dann auch frail das schlechtere Outcome ist, so dass wir die \\(OR\\) als Risiko oder als protektiv interpretieren können. Nehmen wir als Beispiel einmal die Variable crp. Der CRP Wert höht das Risiko für frail. Das macht schonmal so Sinn. Und zum anderen ist der Effekt bei dem Vergleich von pre-frail zu frail mit \\(1.16\\) nicht so große wie bei robust zu frail mit \\(1.26\\). Das macht auch Sinn. Deshalb passt es hier einigermaßen.\nIn Tabelle 42.3 sehen wir nochmal die Ausgabe von einer multinominalen Regression durch die Funktion tbl_regression() aufgearbeitet.\n\nmultinom_fit %>% \n  tbl_regression(exponentiate = TRUE) %>% \n  as_flex_table()\n\n\n\n\n\n\nTabelle 42.3—  Tabelle der Ergebnisse der multinominalen Regression. \n\nOutcome\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\nrobust\nage\n1.03\n0.96, 1.11\n0.3\n\n\n\nsex\n\n\n\n\n\n\nfemale\n—\n—\n\n\n\n\nmale\n0.50\n0.20, 1.25\n0.14\n\n\n\nlocation\n\n\n\n\n\n\nnorth\n—\n—\n\n\n\n\nnortheast\n1.35\n0.58, 3.17\n0.5\n\n\n\nnorthwest\n1.88\n0.83, 4.26\n0.13\n\n\n\nwest\n1.08\n0.46, 2.51\n0.9\n\n\n\nactivity\n1.09\n0.86, 1.39\n0.5\n\n\n\ncrp\n1.26\n1.01, 1.56\n0.037\n\n\n\nbloodpressure\n0.94\n0.86, 1.04\n0.2\n\n\n\nweight\n0.99\n0.79, 1.23\n>0.9\n\n\n\ncreatinin\n1.20\n0.95, 1.50\n0.13\n\n\npre-frail\nage\n1.07\n1.00, 1.16\n0.055\n\n\n\nsex\n\n\n\n\n\n\nfemale\n—\n—\n\n\n\n\nmale\n0.79\n0.30, 2.07\n0.6\n\n\n\nlocation\n\n\n\n\n\n\nnorth\n—\n—\n\n\n\n\nnortheast\n0.89\n0.35, 2.23\n0.8\n\n\n\nnorthwest\n1.21\n0.51, 2.90\n0.7\n\n\n\nwest\n1.02\n0.42, 2.47\n>0.9\n\n\n\nactivity\n0.90\n0.70, 1.16\n0.4\n\n\n\ncrp\n1.16\n0.92, 1.46\n0.2\n\n\n\nbloodpressure\n0.93\n0.84, 1.03\n0.2\n\n\n\nweight\n1.07\n0.85, 1.36\n0.6\n\n\n\ncreatinin\n1.31\n1.03, 1.68\n0.029\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nLeider wird die Sache mit einer multinominalen Regression sehr unangenehm, wenn wir wirklich nicht sortierbare Level im Outcome haben. Dann haben wir aber noch ein Möglichkeit der multinominalen Regression zu entkommen. Wir rechnen einfach separate logistische Regressionen. Die logistischen Regressionen können wir dann ja separat gut interpretieren."
  },
  {
    "objectID": "stat-modeling-multinom.html#logistische-regression-als-ausweg",
    "href": "stat-modeling-multinom.html#logistische-regression-als-ausweg",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.7 Logistische Regression als Ausweg",
    "text": "42.7 Logistische Regression als Ausweg\n\n\n\n\n\n\nBitte Beachten bei der Berechung über separate logistische Regressionen\n\n\n\nDurch die Verwendung von separaten logistischen Regressionen vermindern wir die Fallzahl je gerechneter Regression, so dass wir größere \\(p\\)-Werte erhalten werden als in einer multinominalen Regression. Oder andersherum, durch die verminderte Fallzahl in den separaten logistischen Regressionen haben wir eine geringere Power einen signifikanten Unterschied nachzuweisen.\n\n\nEs gibt den einen Ring um sich zu knechten. Und das ist die logistische Regression. Gut die logistische Regression hilft jetzt nicht, wenn es mit Boniturnoten zu tun hast, aber wenn wir wenige Level im Outcome haben. In unserem Fall haben wir ja drei Level vorliegen, da können wir dann jeweils ein Level rausschmeißen und haben dann nur noch ein binäres Outcome. Das ist auch die zentrale Idee. Wir entfernen immer alle Level bis wir nur noch zwei Level in unserem Outcome haben und rechnen für diese beiden Level dann eine logistische Regression.\nSchauen wir uns erstmal an, wie sich die Daten über die drei Kategorien in unserem Outcome verteilen. Wenn wir eine Kategorie im Outcome kaum vorliegen haben, könnten wir diese Daten vielleicht mit einer anderen Kategorie zusammenlegen oder aber müssen von unserer Idee hier Abstand nehmen.\n\npig_tbl$frailty_fac %>% tabyl\n\n         .   n   percent\n     frail  53 0.1286408\n    robust 226 0.5485437\n pre-frail 133 0.3228155\n\n\nWir haben nicht so viele Beobachtungen in der Kategorie frail. Wir könnten also auch die beiden Faktorlevel pre-frail und frail zusammenlegen.\nDas R Paket forcats liefert sehr viele Funktion, die dir helfen Faktoren zu kodieren und zu ändern\n\npig_tbl$frailty_fac %>% \n  fct_recode(frail_pre_frail = \"frail\", frail_pre_frail = \"pre-frail\") %>% \n  tabyl\n\n               .   n   percent\n frail_pre_frail 186 0.4514563\n          robust 226 0.5485437\n\n\nDas ist jetzt aber nur eine Demonstration für die Zusammenlegung. Wir wollen jetzt trotzdem unsere drei logistischen Regressionen rechnen. Warum drei? Wir haben ja drei Level in unserem Outcome und wir werden jetzt uns drei Datensätze so bauen, dass in jdem Datensatz unser Outcome immer nur zwei Level hat. Die einzelnen Datensätze speichern wir dann in einer Liste.\n\npig_lst <- list(robust_prefrail = filter(pig_tbl, frailty_fac %in% c(\"robust\", \"pre-frail\")),\n                robust_frail = filter(pig_tbl, frailty_fac %in% c(\"robust\", \"frail\")),\n                prefrail_frail = filter(pig_tbl, frailty_fac %in% c(\"pre-frail\", \"frail\")))\n\nWir können das auch fancy. Und das demonstriere ich dann mal hier. Wenn wir die Funktion combn() nutzen erhalten wir eine Liste mit allen zweier Kombinationen wieder. Diese Liste können wir dann in die Funktion map() stecken, die dann über die Liste unserer Kombinationen iteriert. Pro Liste filtern map() dann den Datensatz für uns heraus. Ja, ist ein wenig over the top, aber ich wollte das mal für mich mit map() ausprobieren und es passte hier so schön.\n\npig_fancy_lst <- combn(c(\"robust\", \"pre-frail\", \"frail\"), 2, simplify = FALSE) %>% \n  map(~filter(pig_tbl, frailty_fac %in% .x)) \n\nEgal wie du auf die Liste gekommen bist, wir müssen noch die überflüssigen Level droppen. Keine Ahnung was das deutsche Wort ist. Vermutlich ist das deutsche Wort dann entfernen. Dann können wir für jeden der Listeneinträge die logistische Regression rechnen. Am Ende lassen wir uns noch die exponierten Modellfits ausgeben. In der letzten Zeile entferne ich noch den Intercept von der Ausgabe des Modells. Den Intercept brauchen wir nun wirklich nicht.\n\npig_lst %>% \n  map(~mutate(.x, frailty_fac = fct_drop(frailty_fac))) %>% \n  map(~glm(frailty_fac ~ age + sex + location + activity + crp + bloodpressure + weight + creatinin, \n           data = .x, family = binomial)) %>% \n  map(model_parameters, exponentiate = TRUE) %>% \n  map(extract, -1, )\n\n$robust_prefrail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.04 | 0.03 | [0.99, 1.09] |  1.52 | 0.127\nsex [male]           |       1.58 | 0.51 | [0.85, 3.00] |  1.44 | 0.151\nlocation [northeast] |       0.66 | 0.22 | [0.35, 1.25] | -1.26 | 0.209\nlocation [northwest] |       0.65 | 0.19 | [0.36, 1.14] | -1.51 | 0.132\nlocation [west]      |       0.96 | 0.31 | [0.51, 1.80] | -0.13 | 0.895\nactivity             |       0.83 | 0.07 | [0.70, 0.98] | -2.19 | 0.028\ncrp                  |       0.93 | 0.07 | [0.79, 1.08] | -0.95 | 0.342\nbloodpressure        |       0.98 | 0.03 | [0.92, 1.06] | -0.44 | 0.657\nweight               |       1.09 | 0.09 | [0.93, 1.28] |  1.05 | 0.295\ncreatinin            |       1.11 | 0.09 | [0.94, 1.31] |  1.23 | 0.220\n\n$robust_frail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.04 | 0.04 | [0.97, 1.12] |  1.09 | 0.275\nsex [male]           |       0.52 | 0.23 | [0.21, 1.24] | -1.46 | 0.144\nlocation [northeast] |       1.26 | 0.55 | [0.54, 3.05] |  0.52 | 0.603\nlocation [northwest] |       1.94 | 0.82 | [0.86, 4.52] |  1.57 | 0.116\nlocation [west]      |       1.06 | 0.46 | [0.45, 2.53] |  0.13 | 0.900\nactivity             |       1.06 | 0.13 | [0.84, 1.34] |  0.47 | 0.638\ncrp                  |       1.28 | 0.14 | [1.03, 1.60] |  2.17 | 0.030\nbloodpressure        |       0.94 | 0.05 | [0.85, 1.04] | -1.22 | 0.223\nweight               |       1.00 | 0.11 | [0.81, 1.24] |  0.01 | 0.992\ncreatinin            |       1.22 | 0.15 | [0.96, 1.55] |  1.64 | 0.100\n\n$prefrail_frail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.06 | 0.04 | [0.99, 1.14] |  1.61 | 0.108\nsex [male]           |       0.77 | 0.41 | [0.27, 2.19] | -0.48 | 0.632\nlocation [northeast] |       1.04 | 0.50 | [0.41, 2.71] |  0.08 | 0.938\nlocation [northwest] |       1.23 | 0.57 | [0.49, 3.10] |  0.44 | 0.660\nlocation [west]      |       1.08 | 0.51 | [0.44, 2.75] |  0.17 | 0.866\nactivity             |       0.95 | 0.13 | [0.73, 1.23] | -0.39 | 0.695\ncrp                  |       1.15 | 0.13 | [0.91, 1.45] |  1.18 | 0.240\nbloodpressure        |       0.93 | 0.05 | [0.82, 1.04] | -1.34 | 0.182\nweight               |       1.05 | 0.13 | [0.82, 1.36] |  0.42 | 0.673\ncreatinin            |       1.26 | 0.16 | [1.00, 1.63] |  1.87 | 0.061\n\n\nEine Sache ist super wichtig zu wissen. Wie oben schon geschrieben, durch die Verwendung von separaten logistischen Regressionen vermindern wir die Fallzahl je Regression, so dass wir größere \\(p\\)-Werte erhalten werden, als in einer multinominalen Regression. Das ist der Preis, den wir dafür bezahlen müssen, dass wir besser zu interpretierende Koeffizienten erhalten. Und das ist auch vollkommen in Ordnung. Ich selber habe lieber Koeffizienten, die ich interpretieren kann, als unklare Effekte mit niedrigen \\(p\\)-Werten.\nSchauen wir einmal auf unseren Goldstandard, der Variable für den CRP-Wert. Die Variable haben wir ja jetzt immer mal wieder in diesem Kapitel interpretiert und uns angeschaut. Die Variable crp passt von dem Effekt jedenfalls gut in den Kontext mit rein. Die Effekte sind ähnlich wie in der multinominalen Regression. Wir haben eben nur größere \\(p\\)-Werte. Jetzt müssen wir entscheiden, wir können vermutlich die getrennten logistischen Regressionen besser beschreiben und interpretieren. Das ist besonders der Fall, wenn wir wirklich Probleme haben eine Referenz in der multinominalen Regression festzulegen. Dann würde ich immer zu den getrennten logistischen Regressionen greifen als eine schief interpretierte multinominale Regression."
  },
  {
    "objectID": "stat-modeling-multinom.html#referenzen",
    "href": "stat-modeling-multinom.html#referenzen",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "stat-modeling-logistic.html",
    "href": "stat-modeling-logistic.html",
    "title": "43  Logistische Regression",
    "section": "",
    "text": "Version vom November 16, 2022 um 13:12:35\nDie logistische Regression ist die Regression, wenn wir rüber in die Medizin schauen. Wohl in keinem Bereich der Wissenschaften wird so viel eine logistische Regression gerechnet wie in der Humanmedizin, Epidemiologie oder Pharmazie. Wir haben in der logistischen Regression ein \\(0/1\\) Outcome als \\(y\\) vorliegen. Also entweder ist eine Beobachtung erkrankt oder nicht. Meistens beschränkt sich die Betrachtung auf erkrankt (\\(1\\), ja) oder eben nicht erkrankt (\\(0\\), nein) bzw. gesund. Wichtig hierbei ist, dass wir eigentlich immer sagen, dass das Schlechte mit \\(1\\) kodiert wird. Wenn du das machst, dann wird dir die Interpretation der Effektschätzer der logistischen Regression leichter fallen.\nGleich zu Beginn dann nochmal wir werden die logistische Regression in den Agrarwissenschaften eher selten sehen. Im Bereich der Pflanzenwissenschaften kommt die logistische Regression kaum bis gar nicht vor. Im Bereich der Tierwissenschaften schon eher, aber dort dann im Bereich der Tiermedizin und eben wieder Erkrankungen.\nWo wir hingegen dann wieder die logistische Regression brauchen, ist bei der Klassifikation oder eben der Vorhersage von einem binären Ereignis. Dafür bietet sich dann die logistische Regression wieder an. Deshalb werden wir am Ende des Kapitels nochmal was zur Klassifikation machen, obwohl das hier eigentlich nur so halb reinpasst. Wenn du nicht Klassifizieren willst, dann lasse den letzten Abschnitt einfach weg."
  },
  {
    "objectID": "stat-modeling-logistic.html#annahmen-an-die-daten",
    "href": "stat-modeling-logistic.html#annahmen-an-die-daten",
    "title": "43  Logistische Regression",
    "section": "\n43.1 Annahmen an die Daten",
    "text": "43.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Binomialverteilung. Damit finden wir im Outcome nur \\(0\\) oder \\(1\\) Werte.\nIm folgenden Kapitel zu der multiplen logistischen linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 36 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Binomailverteilung. Damit finden wir im Outcome nur \\(0\\) oder \\(1\\) Werte. Das ist hier sehr wichtig, denn wir wollen ja eine multiple logistische lineare Regression rechnen.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen, wir du dann das Modell weiterverwendest."
  },
  {
    "objectID": "stat-modeling-logistic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-logistic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "43  Logistische Regression",
    "section": "\n43.2 Genutzte R Pakete für das Kapitel",
    "text": "43.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, gtsummary,\n               tidymodels, cutpointr)\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'cutpointr' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\jokruppa\\AppData\\Local\\Temp\\RtmpKYaU0z\\downloaded_packages\n\n\nWarning: Paket 'cutpointr' wurde unter R Version 4.2.2 erstellt\n\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-logistic.html#daten",
    "href": "stat-modeling-logistic.html#daten",
    "title": "43  Logistische Regression",
    "section": "\n43.3 Daten",
    "text": "43.3 Daten\nIn diesem Kapitel nutzen wir die infizierten Ferkel als Beispieldatensatz. Wir haben in dem Datensatz über vierhundert Ferkel untersucht und festgehalten, ob die Ferkel infiziert sind (\\(1\\), ja) oder nicht infiziert (\\(0\\), nein). Wir haben daneben noch eine ganze Reihe von Risikofaktoren erhoben. Hier sieht man mal wieder wie wirr die Sprache der Statistik ist. Weil wir rausfinden wollen welche Variable das Risiko für die Infektion erhöht, nennen wir diese Variablen Risikofaktoren. Obwohl die Variablen gar keine kategorialen Spalten sin bzw. nicht alle. So ist das dann in der Statistik, ein verwirrender Begriff jagt den Nächsten.\n\npig_tbl <- read_excel(\"data/infected_pigs.xlsx\") \n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der Tabelle 43.1 an.\n\n\n\n\nTabelle 43.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\ninfected\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n49.88\n16.94\n3.07\n1\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n58.2\n17.95\n4.88\n0\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n56.8\n19.02\n3.98\n0\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.47\n18.98\n5.18\n0\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n59.85\n16.57\n6.71\n1\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n58.1\n18.22\n5.43\n1\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\nrobust\n57.05\n17.95\n6.16\n1\n\n\n56\nmale\nwest\n13.91\n20.8\npre-frail\n50.84\n18.02\n6.52\n1\n\n\n57\nmale\nnorthwest\n12.49\n21.95\nrobust\n55.51\n17.73\n3.94\n1\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n58.5\n18.23\n2.73\n1\n\n\n59\nfemale\nnorth\n13.13\n20.23\npre-frail\n57.33\n17.21\n5.42\n1\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n55.85\n17.76\n6.18\n1\n\n\n\n\n\n\nIn dem nächsten Abschnitt werden wir die Daten nutzen um rauszufinden welche Variablen einen Einfluss auf den Infektionsstatus der Ferkel hat."
  },
  {
    "objectID": "stat-modeling-logistic.html#theoretischer-hintergrund",
    "href": "stat-modeling-logistic.html#theoretischer-hintergrund",
    "title": "43  Logistische Regression",
    "section": "\n43.4 Theoretischer Hintergrund",
    "text": "43.4 Theoretischer Hintergrund\nWir schaffen wir es, durch einen \\(0/1\\) Outcome auf der y-Achse eine gerade Linie durch die Punkte zu zeichnen und die Koeffiziente dieser Gerade zu bestimmen? Immerhin gibt es ja gar keine Werte zwischen \\(0\\) und \\(1\\). In Abbildung 43.1 sehen wir beispielhaft den Zusammenhang zwischen dem Infektionsstatus und der Aktivität der Ferkel. Wir haben zwei horizontale Linien. Wie zeichen wir jetzt da eine Gerade durch?\n\nggplot(pig_tbl, aes(x = activity, y = infected)) +\n  theme_bw() +\n  geom_point() \n\n\n\nAbbildung 43.1— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und der Aktivität der Ferkel.\n\n\n\n\nDer Trick hierbei ist wieder die Transformation des Zusammenhangs von \\(y \\sim x\\) auf einen \\(\\log\\)-scale. Das heißt wir Rechnen nicht mit den \\(0/1\\) Werten sondern transformieren den gesamten Zusammenhang. Das ist wichtig, den es gibt einen Unterschied zwischen der Transformation von \\(y\\) und der Transformation die hier gemeint ist. Wir halten fest, wir rechnen also nciht auf der ursprünglichen Skala der Daten sondern auf der \\(\\log\\)-scale. Allgemeiner wird auch von der link-Funktion gesprochen, da wir ja verschiedene Möglichkeiten der Transformation des Zusammenhangs haben.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\nWir gehen wir also vor. Zuerst Modellieren wir die Wahrscheinlichkeit für den Eintritt des Ereignisses. Wir machen also aus unseren binären \\(0/1\\) Daten eine Wahrscheinlichkeit für den Eintritt von 1.\n\\[\nY \\rightarrow Pr(Y = 1)\n\\]\nDamit haben wir schon was erreicht den \\(Pr(Y = 1)\\) liegt zwischen \\(0\\) und \\(1\\). Damit haben wir also schon Werte dazwischen. Wenn wir aber normalverteilte Residuen haben wollen, dann müssen unsere Werte von \\(-\\infty\\) bis \\(+\\infty\\) laufen können. Daher rechnen wir im Weiteren die Chance.\n\\[\n\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\n\\] Die Chance (eng. Odds) für das Eintreten von \\(Y=1\\) ist eben die Wahrscheinlichkeit für das Eintreten geteilt durch die Gegenwahrscheinlichkeit. Das ist schon besser, denn damit liegen unsere transformierten Werte für den Zusammenhang schon zwischen \\(0\\) und \\(+\\infty\\). Wenn wir jetzt noch den \\(\\log\\) von den Chancen rechnen, dann haben wir schon fast alles was wir brauchen.\n\\[\n\\log\\left(\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\\right)\n\\]\nDer Logarithmus der Chance liegt dann zwischen \\(-\\infty\\) und \\(+\\infty\\). Deshalb spricht man auch von den \\(\\log\\)-Odds einer logistischen Regression. Auch sieht man hier woher das logistisch kommt. Wir beschreiben im Namen auch gleich die Transformation mit. Am ende kommen wir somit dann auf folgendes Modell.\n\\[\n\\log\\left(\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\\right) = \\beta_0 + \\beta_1 x_1 + ...  + \\beta_p x_p + \\epsilon\n\\] Vielleicht ist dir der Begriff Wahrscheinlichkeit und der Unterschied zur Chance nicht mehr so präsent. Deshalb hier nochmal als Wiederholung oder Auffrischung.\n\nEine Wahrscheinlichkeit beschreibt dem Anteil an Allen. Zum Beispiel den Anteil Gewinner an allen Teilnehmern. Den Anteil Personen mit Therapieerfolg an allen Studienteilnehmern.\nEine Chance oder (eng. Odds) beschreibt ein Verhältnis. Somit das Verhältnis Gewinner zu Nichtgewinner. Oder das Verhältnis Personen mit Therapieerfolg zu Personen ohne Therapieerfolg\n\nNochmal an einem Zahlenbeispiel. Wenn wir ein Glücksspiel haben, in dem es 2 Kombinationen gibt die gewinnen und drei 3 Kombinationen die verlieren, dann haben wir eine Wahrscheinlichkeit zu gewinnen von \\(2 / 5 = 0.40 = 40\\%\\). Wenn wir die Chance zu gewinnen ausrechnen erhalten wir \\(2:3 = 0.67 = 67\\%\\). Wir sehen es gibt einen deutlichen Unterschied zwischen Chance und Wahrscheinlichkeit. Wenn wir große Fallzahl haben bzw. kleine Wahrscheinlichkeiten, dann ist der Unterschied nicht mehr so drastisch. Aber von einer Gleichkeit von Wahrscheinlichkeit und Chance zu sprechen kann nicht ausgegangen werden.\nWas ist nun das Problem? Wir erhalten aus einer logistischen Regression \\(\\log\\)-Odds wieder. Der Effektchätzer ist also eine Chance. Wir werden aber das Ergebnis wie eine Wahrscheinlichkeit interpretieren. Diese Diskrepanz ist wenigen bekannt und ein Grund, warum wir in der Medizin immer uns daran erinnern müssen, was wir eigentlich mit der logistischen Regression aussagen können."
  },
  {
    "objectID": "stat-modeling-logistic.html#modellierung",
    "href": "stat-modeling-logistic.html#modellierung",
    "title": "43  Logistische Regression",
    "section": "\n43.5 Modellierung",
    "text": "43.5 Modellierung\nDie Modellerierung der logistischen Regression ist sehr einfach. Wir nutzen wieder die Formelschreibweise im glm() um unsere Variablen zu definieren. Wenn unser Outcome nicht binär ist, dann jammert R und gibt uns einen Fehler aus. Ich kann hier nur dringlichst raten, das Outcome in \\(0/1\\) zu kodieren mit dem Schlechten als \\(1\\).\nDas glm() muss dann noch wissen, dass es eine logistische Regression rechnen soll. Das machen wir in dem wir als Verteilungsfamilie die Binomialverteilung auswählen. Wir geben also an family = binomial und schon können wir das volle Modell fitten.\n\nlog_fit <- glm(infected ~ age + sex + location + activity + crp + \n                 frailty + bloodpressure + weight + creatinin, \n               data = pig_tbl, family = binomial)\n\nDas war extrem kurz und scherzlos. Also können wir dann auch ganz kurz schauen, ob das Modell einigermaßen funktioniert hat."
  },
  {
    "objectID": "stat-modeling-logistic.html#performance-des-modells",
    "href": "stat-modeling-logistic.html#performance-des-modells",
    "title": "43  Logistische Regression",
    "section": "\n43.6 Performance des Modells",
    "text": "43.6 Performance des Modells\nNachdem wir das Modell gefittet haben, wollen wir uns nochmal das \\(R^2\\) wiedergeben lassen um zu entscheiden, ob unser Modell einigermaßen funktioniert hat. Dieser Abschnitt ist sehr kurz. Wir haben leider nur sehr wenige Möglichkeiten um ein logistischen Modell zu bewerten.\n\nr2(log_fit)\n\n# R2 for Logistic Regression\n  Tjur's R2: 0.322\n\n\nJa, so viel Varianz erklären wir nicht, aber wenn du ein wenig im Internet suchst, dann wirst du feststellen, dass das Bestimmtheitsmaß so eine Sache in glm()’s ist. Wir sind aber einigermaßen zufrieden. Eventuell würde eine Variablenselektion hier helfen, aber das ist nicht Inhalt dieses Kapitels.\nIn Abbildung 43.2 schauen wir nochmal auf die Residuen und die möglichen Ausreißer. Wieder sehen beide Plots einigermaßen in Ordnung aus. Die Abbildungen sind jetzt nicht die Besten, aber ich würde hier auch anhand der Diagnoseplots nicht die Modellierung verwerfen.\n\ncheck_model(log_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\")) \n\n\n\nAbbildung 43.2— Ausgabe ausgewählter Modelgüteplots der Funktion check_model()."
  },
  {
    "objectID": "stat-modeling-logistic.html#interpretation-des-modells",
    "href": "stat-modeling-logistic.html#interpretation-des-modells",
    "title": "43  Logistische Regression",
    "section": "\n43.7 Interpretation des Modells",
    "text": "43.7 Interpretation des Modells\nZu Interpretation schauen wir uns wie immer nicht die rohe Ausgabe an, sondern lassen uns die Ausgabe mit der Funktion model_parameters() aus dem R Paket parameters wiedergeben. Wir müssen noch die Option exponentiate = TRUE wählen, damit unsere Koeffizienten nicht als \\(\\log\\)-Odds sondern als Odds wiedergeben werden. Korrekterweise erhalten wir die Odds ratio wieder was wir auch als \\(OR\\) angegeben.\n\nmodel_parameters(log_fit, exponentiate = TRUE)\n\nParameter            | Odds Ratio |       SE |       95% CI |     z |      p\n----------------------------------------------------------------------------\n(Intercept)          |   1.31e-13 | 5.81e-13 | [0.00, 0.00] | -6.70 | < .001\nage                  |       1.03 |     0.03 | [0.98, 1.09] |  1.10 | 0.272 \nsex [male]           |       2.75 |     1.00 | [1.36, 5.69] |  2.77 | 0.006 \nlocation [northeast] |       0.56 |     0.20 | [0.28, 1.12] | -1.63 | 0.103 \nlocation [northwest] |       0.66 |     0.22 | [0.34, 1.25] | -1.28 | 0.202 \nlocation [west]      |       0.79 |     0.29 | [0.38, 1.62] | -0.64 | 0.520 \nactivity             |       0.90 |     0.09 | [0.75, 1.09] | -1.08 | 0.281 \ncrp                  |       2.97 |     0.35 | [2.38, 3.78] |  9.25 | < .001\nfrailty [pre-frail]  |       0.64 |     0.27 | [0.28, 1.44] | -1.06 | 0.288 \nfrailty [robust]     |       0.70 |     0.28 | [0.32, 1.51] | -0.89 | 0.376 \nbloodpressure        |       1.12 |     0.04 | [1.03, 1.21] |  2.77 | 0.006 \nweight               |       1.10 |     0.10 | [0.92, 1.30] |  1.05 | 0.293 \ncreatinin            |       1.03 |     0.09 | [0.86, 1.23] |  0.33 | 0.745 \n\n\nWie interpretieren wir nun das \\(OR\\) einer logistischen Regression? Wenn wir darauf gechtet haben, dass wir mit \\(1\\) das Schlechte meinen, dann können wir wir folgt mit dem \\(OR\\) sprechen. Wenn wir ein \\(OR > 1\\) haben, dann haben wir ein Risiko vorliegen. Die Variable mit einem \\(OR\\) größer als \\(1\\) wird die Chance auf den Eintritt des schlechten Ereignisses erhöhen. Wenn wir ein \\(OR < 1\\) haben, dann sprechen wir von einem protektiven Faktor. Die Variable mit einem \\(OR\\) kleiner \\(1\\) wird vor dem Eintreten des schlechten Ereignisses schützen. Schauen wir uns den Zusammenhang mal im Detail für die Ferkeldaten an.\n\n\n(intercept) beschreibt den Intercept der logistischen Regression. Wenn wir mehr als eine simple Regression vorliegen haben, wie in diesem Fall, dann ist der Intercept schwer zu interpretieren. Wir konzentrieren uns auf die Effekte der anderen Variablen.\n\nsex beschreibt den Effekt der männlichen Ferkel zu den weiblichen Ferkeln. Daher haben männliche Ferkel eine \\(2.75\\) höhere Chance infiziert zu werden als weibliche Ferkel.\n\nlocation [northeast], location [northwest] und location [west] beschreibt den Unterschied zur location [north]. Alle Orte haben eine geringere Chance für eine Infektion zum Vergleich der Bauernhöfe im Norden. Zwar ist keiner der Effekte signifikant, aber ein interessantes Ergebnis ist es allemal.\n\nactivity beschreibt den Effekt der Aktivität der Ferkel. Wenn sich die Ferkel mehr bewegen, dann ist die Chance für eine Infektion gemindert.\n\ncrp beschreibt den Effekt des CRP-Wertes auf den Infektionsgrad. Pro Einheit CRP steigt die Chance einer Infektion um \\(2.97\\) an. Das ist schon ein beachtlicher Wert.\n\nfrailty beschreibt die Gebrechlichkeit der Ferkel. Hier müssen wir wieder schauen, zu welchem Level von frailty wir vergleichen. Hier vergleichen wir zu frail. Also dem höchsten Gebrechlichkeitgrad. Ferkel die weniger gebrechlich sind, haben eine niedrigere Chance zu erkranken.\n\nbloodpressure, weight und creatinin sind alles Variablen, mit einem \\(OR\\) größer als \\(1\\) und somit alles Riskovariablen. Hier sind zwar die \\(OR\\) relativ klein, aber das muss erstmal nichts heißen, da die \\(OR\\) ja hier die Änderung für eine Einheit von \\(x\\) beschreiben. Deshalb musst du immer schauen, wie die Einheiten von kontinuierlichen kodiert Variablen sind.\n\nKommen wir nochmal zu den gänigen Tabellen für die Zusammenfassung eines Ergebnisses einer logistischen Regression. Teilweise sind diese Tabellen so generisch und häufiog verwendet, dass wir schon einen Begriff für diese Tabellen haben. In Tabelle 43.2 siehst du die table 1 für die Übersicht aller Risikovariablen aufgeteilt nach dem Infektionsstatus. Diese Art der Tabellendarstellung ist so grundlegend für eine medizinische Veröffentlichung, dass sich eben der Begriff table 1 etabliert hat. Fast jede medizinische Veröffentlichung hat als erste Tabelle diese Art von Tabelle angegeben. Hierbei ist wichtig, dass die \\(p\\)-Werte alle nur aus einem einfachen statistischen Test stammen. Die \\(p\\)-Werte einer multiplen logistischen Regression werden daher immer anders sein.\n\npig_tbl %>% tbl_summary(by = infected) %>% add_p() %>% as_flex_table()\n\n\n\n\n\n\nTabelle 43.2—  Ausgabe der Daten in einer Summary Table oder auch Table 1 genannt. In medizinischen Veröffentlichungen immer die erste Tabelle für die Zusammenfassung der Patienten (hier Ferkel) für jede erhobende Risikovariable. \n\nCharacteristic\n0, N = 1551\n1, N = 2571\np-value2\n\n\n\nage\n60.0 (57.0, 63.0)\n60.0 (57.0, 63.0)\n0.5\n\n\nsex\n\n\n0.3\n\n\nfemale\n66 (43%)\n95 (37%)\n\n\n\nmale\n89 (57%)\n162 (63%)\n\n\n\nlocation\n\n\n0.6\n\n\nnorth\n40 (26%)\n81 (32%)\n\n\n\nnortheast\n33 (21%)\n51 (20%)\n\n\n\nnorthwest\n51 (33%)\n73 (28%)\n\n\n\nwest\n31 (20%)\n52 (20%)\n\n\n\nactivity\n13.35 (12.29, 14.34)\n13.24 (12.25, 14.53)\n0.7\n\n\ncrp\n19.12 (18.17, 19.92)\n20.66 (19.85, 21.46)\n<0.001\n\n\nfrailty\n\n\n0.7\n\n\nfrail\n20 (13%)\n33 (13%)\n\n\n\npre-frail\n54 (35%)\n79 (31%)\n\n\n\nrobust\n81 (52%)\n145 (56%)\n\n\n\nbloodpressure\n56.8 (53.9, 58.6)\n57.0 (54.9, 59.0)\n0.10\n\n\nweight\n18.36 (17.32, 19.34)\n18.33 (17.36, 19.44)\n0.9\n\n\ncreatinin\n4.82 (4.06, 5.88)\n4.97 (3.97, 5.87)\n0.8\n\n\n\n1Median (IQR); n (%)\n2Wilcoxon rank sum test; Pearson's Chi-squared test\n\n\n\n\n\n\nIn Tabelle 43.3 siehst du nochmal für eine Auswahl an Variablen die simplen logistischen Regressionen gerechnet. Du müsst also nicht jede simple logistische Regression selber rechnen, sondern kannst auch die Funktion tbl_uvregression() verwenden. Das R Paket tbl_summary erlaubt weitreichende Formatierungsmöglichkeiten. Am bestes schaust du einmal im Tutorial Tutorial: tbl_regression selber nach was du brauchst oder anpassen willst.\n\npig_tbl%>%\n  select(infected, age, crp, bloodpressure) %>%\n  tbl_uvregression(\n    method = glm,\n    y = infected,\n    method.args = list(family = binomial),\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 2)\n  ) %>% as_flex_table()\n\n\n\n\n\n\nTabelle 43.3—  Simple logistische Regression für eine Auswahl an Einflussvariablen. Für jede Einflussvariable wurde eine simple logistische Regression gerechnet. \n\nCharacteristic\nN\nOR1\n95% CI1\np-value\n\n\n\nage\n412\n1.02\n0.97, 1.06\n0.49\n\n\ncrp\n412\n2.73\n2.23, 3.42\n<0.001\n\n\nbloodpressure\n412\n1.06\n1.00, 1.12\n0.058\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nnun gibt es viele Möglichkeiten sich die logistische Regression wiedergeben zu lassen In Tabelle 43.4 siehst du nochmal die Möglichkeit, die dir das R Paket tbl_summary() bietet. Am Ende ist es dann eine reine Geschmacksfrage, wie wir die Daten dann aufarbeiten wollen.\n\nlog_fit %>% tbl_regression(exponentiate = TRUE) %>% as_flex_table()\n\n\n\n\n\n\nTabelle 43.4—  Ausgabe der multiplen logistischen Regression durch die Funktion tbl_regression(). \n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\nage\n1.03\n0.98, 1.09\n0.3\n\n\nsex\n\n\n\n\n\nfemale\n—\n—\n\n\n\nmale\n2.75\n1.36, 5.69\n0.006\n\n\nlocation\n\n\n\n\n\nnorth\n—\n—\n\n\n\nnortheast\n0.56\n0.28, 1.12\n0.10\n\n\nnorthwest\n0.66\n0.34, 1.25\n0.2\n\n\nwest\n0.79\n0.38, 1.62\n0.5\n\n\nactivity\n0.90\n0.75, 1.09\n0.3\n\n\ncrp\n2.97\n2.38, 3.78\n<0.001\n\n\nfrailty\n\n\n\n\n\nfrail\n—\n—\n\n\n\npre-frail\n0.64\n0.28, 1.44\n0.3\n\n\nrobust\n0.70\n0.32, 1.51\n0.4\n\n\nbloodpressure\n1.12\n1.03, 1.21\n0.006\n\n\nweight\n1.10\n0.92, 1.30\n0.3\n\n\ncreatinin\n1.03\n0.86, 1.23\n0.7\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nZum Abschluss wollen wir uns einmal die Ergebnisse des Modellfits als logistischen Gerade für eine simple lineare Regression mit dem Modell \\(infected \\sim crp\\) anschauen. Wie immer können wir uns den Zusammenhang nur in einem simplen Modell anschauen. Im Fall einer multiplen linearen Regresion können wir nicht so viele Dimensionen in einer Grpahik darstellen. Wir fitten also das Modell log_fit_crp wie im folgenden dargestellt.\n\nlog_fit_crp <- glm(infected ~ crp, data = pig_tbl, family = binomial)\n\nNun können wir uns mit der Funktion predict() die Wert auf der Geraden wiedergeben lassen. Wenn wir predict() nur so aufrufen, dann erhalten wir die Werte für \\(y\\) auf der transformierten \\(link\\)-Scale wieder. Das hilft uns aber nicht weiter, wir haben ja nur 0 und 1 Werte für \\(y\\) vorliegen.\n\npredict(log_fit_crp, type = \"link\") %>% \n  extract(1:10) %>% \n  round(2)\n\n    1     2     3     4     5     6     7     8     9    10 \n 3.03 -0.73 -0.61  0.00  2.22  2.10 -0.39 -0.39  2.54  1.60 \n\n\nDa wir die Werte für die Wahrscheinlichkeit das ein Ferkel infiziert ist, also die Wahrscheinlichkeit \\(Pr(infected = 1)\\), müssen wir noch die Option type = reponse wählen. So erhalten wir die Wahrscheinlichkeiten wiedergegeben.\n\npredict(log_fit_crp, type = \"response\") %>% \n  extract(1:10) %>% \n  round(2)\n\n   1    2    3    4    5    6    7    8    9   10 \n0.95 0.33 0.35 0.50 0.90 0.89 0.40 0.40 0.93 0.83 \n\n\nAbschließend können wir uns die Gerade auch in der Abbildung 43.3 visualisieren lassen. Auf der x-Achse sehen wir die crp-Werte und auf der y-Achse den Infektionsstatus. Auf der \\(reponse\\)-scale sehen wir eine S-Kurve. Auf der \\(link\\)-scale würden wir eine Gerade sehen.\n\nggplot(pig_tbl, aes(x = crp, y = infected)) +\n  theme_bw() +\n  geom_point() +\n  geom_line(aes(y = predict(log_fit_crp, type = \"response\")), color = \"red\") \n\n\n\nAbbildung 43.3— Visualisierung der logistischen Gerade in einer simplen logistischen Regression mit der Variable crp.\n\n\n\n\nNun haben wir das Kapitel zur logistischen Regression fast abgeschlossen. Was noch fehlt ist die Besonderheit der Prädiktion im Kontext des maschinellen Lernens. Das machen wir jetzt im folgenden Abschnitt. Wenn dich die logistische Regression nur interessiert hat um einen kausalen Zusammenhang zwischen Einflussvariablen und dem binären Outcome zu modellieren, dann sind wir hier fertig."
  },
  {
    "objectID": "stat-modeling-logistic.html#prädiktion",
    "href": "stat-modeling-logistic.html#prädiktion",
    "title": "43  Logistische Regression",
    "section": "\n43.9 Prädiktion",
    "text": "43.9 Prädiktion\nDa wir später in dem Kapitel 45 die logistische Regression auch als Vergleich zu maschinellen Lernverfahren in der Klassifikation nutzen werden gehen wir hier auch die Prädiktion einmal für die logistische Regression durch. Wir wollen also eine Klassifikation, also eine Vorhersage, für das Outcome infected mit einer logistischen Regression rechnen. Wir nutzen dazu die Möglichkeiten des R Pakets tidymodels wodurch wir einfacher ein Modell bauen und eine Klassifikation rechnen können. Unsere Fragestellung ist, ob wir mit unseren Einflussvariablen den Infektionsstatus vorhersagen können. Das heißt wir wollen ein Modell bauen mit dem wir zukünftige Ferkel als potenziell krank oder gesund anhand unser erhobenen Daten einordnen bzw. klassifizieren können.\n\n\nMehr zu Rezepten (eng. recipes) kannst du im Kapitel 45 zu den Grundlagen des maschinellen Lernens erfahren.\nDer erste Schritt einer Klassifikation ist immer sicherzustellen, dass unser Outcome auch wirklich aus Kategorien besteht. In R nutzen wir dafür einen Faktor und setzen dann auch gleich die Ordnung fest.\n\npig_tbl <- pig_tbl %>% \n  mutate(infected = factor(infected, levels = c(0, 1)))\n\nNun bauen wir uns ein einfaches Rezept mit der Funktion recipe(). Dafür legen wir das Modell, was wir rechnen wollen einmal fest. Wir nehmen infected als Outcome und den Rest der Vairbalen . aus dem Datensatz pig_tbl als die \\(x\\) Variablen. Dann wollen wir noch alle Variablen, die ein Faktor sind in eine Dummyvariable umwandeln.\n\npig_rec <- recipe(infected ~ ., data = pig_tbl) %>% \n  step_dummy(all_nominal_predictors())\n\nWir wollen jetzt unser Modell definieren. Wir rechnen eine logistsiche Regression und deshalb nutzen wir die Funktion logistic_reg(). Da wir wirklich viele Möglichkeiten hätten die logistische Regression zu rechnen, müssen wir noch den Algorithmus wählen. Das tuen wir mit der Funktion set_engine(). Wir nutzen hier den simplen glm() Algorithmus. Es gebe aber auch andere Implementierungen.\n\nlogreg_mod <- logistic_reg() %>% \n  set_engine(\"glm\")\n\nJetzt müssen wir noch einen Workflow definieren. Wir wollen ein Modell rechnen und zwar mit den Informationen in unserem Rezept. Das bauen wir einmal zusammen und schauen uns die Ausgabe an.\n\npig_wflow <- workflow() %>% \n  add_model(logreg_mod) %>% \n  add_recipe(pig_rec)\n\npig_wflow\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: logistic_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n1 Recipe Step\n\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nDas passt alles soweit. Ja, es ist etwas kompliziert und das ginge sicherlich auch einfacher. Wir werden dann aber noch sehen, dass wir es uns mit dem Ablauf sehr viel einfacher machen, wenn wir kompliziertere Modelle schätzen wollen. Mehr dazu findest du dann im Kapitel 45 zu den maschinellen Lernverfahren.\nJetzt können wir den Workflow nutzen um den Fit zu rechnen. Bis jetzt haben wir nur Informationen gesammelt. Dadurch das wir jetzt das Objekt pig_workflow in die Funktion fit() pipen rechnen wir das Modell.\n\npig_fit <- pig_wflow %>% \n  fit(data = pig_tbl)\n\nDas erhaltende Modell könne wir dann in die Funktion predict() stecken um uns den Inektionsstatus vorhersagen zu lassen.\n\npredict(pig_fit, new_data = pig_tbl)\n\n# A tibble: 412 x 1\n   .pred_class\n   <fct>      \n 1 1          \n 2 0          \n 3 0          \n 4 0          \n 5 1          \n 6 1          \n 7 0          \n 8 0          \n 9 1          \n10 1          \n# ... with 402 more rows\n\n\nIn der Spalte .pred_class finden wir dann die vorhergesagten Werte des Infektionsstatus anhand unseres gefitteten Modells. Eigentlich würden wir ja gerne die vorhergesagten Werte mit unseren Orginalwerten vergleichen. Hier hilft uns die Funktion augment(). Dank der Funktion augment() erhalten wir nicht nur die vorhergesagten Klassen sondern auch die Wahrscheinlichkeit für die Klassenzugehörigkeiten. Daneben dann aber auch die Originalwerte für den Infektionsstatus in der Spalte infected.\n\npig_aug <- augment(pig_fit, new_data = pig_tbl) %>% \n  select(infected, matches(\"^\\\\.\"))\n\npig_aug\n\n# A tibble: 412 x 4\n   infected .pred_class .pred_0 .pred_1\n   <fct>    <fct>         <dbl>   <dbl>\n 1 1        1            0.0977   0.902\n 2 0        0            0.650    0.350\n 3 0        0            0.764    0.236\n 4 0        0            0.587    0.413\n 5 1        1            0.0568   0.943\n 6 1        1            0.0961   0.904\n 7 0        0            0.743    0.257\n 8 0        0            0.561    0.439\n 9 1        1            0.101    0.899\n10 1        1            0.168    0.832\n# ... with 402 more rows\n\n\nWir können dann die Werte aus dem Objekt pig_aug nutzen um uns die ROC Kurve als Güte der Vorhersage wiedergeben zu lassen. Wir nutzen hier die schnelle Variante der Ploterstellung. In dem Kapitel 48 zum Vergleich von Algorithmen gehe ich noch näher auf die möglichen Optionen bei der Erstellung einer ROC Kurve ein. Hier fällt die ROC Kurve dann mehr oder minder vom Himmel. Ich musste noch der Funktion mitgeben, dass das Event bei uns das zweite Level des Faktors infected ist. Sonst ist unsere ROC Kurve einmal an der Diagonalen gespiegelt.\nIn dem Kapitel 28 erfährst du mehr darüber was eine ROC Kurve ist und wie du die ROC Kurve interpretieren kannst.\n\npig_aug %>% \n  roc_curve(truth = infected, .pred_1, event_level = \"second\") %>% \n  autoplot()\n\n\n\nAbbildung 43.5— ROC Kurve für die Vorhersage des Infektionsstatus der Ferkel anhand der erhobenen Daten.\n\n\n\n\nNa das hat doch mal gut funktioniert. Die ROC Kurve verläuft zwar nicht ideal aber immerhin ist die ROC Kurve weit von der Diagnolen entfernt. Unser Modell ist also in der Lage den Infektionsstatus der Ferkel einigermaßen solide vorherzusagen. Schauen wir uns noch die area under the curve (abk. AUC) an.\n\npig_aug %>% \n  roc_auc(truth = infected, .pred_1, event_level = \"second\")\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.830\n\n\nDer beste Wert wäre hier eine AUC von \\(1\\) und damit eine perfekte Vorhersage. Der schlechteste Wert wäre eine AUC von \\(0.5\\) und damit eine nahezu zufällige Zuordnung des Infeketionsstatus zu den Ferkeln von unserem Modell. Mit einer AUC von \\(0.83\\) können wir aber schon gut leben. Immerhin haben wir kaum am Modell rumgeschraubt bzw. ein Tuning betrieben. Wenn du mehr über Tuning und der Optimierung von Modellen zu Klassifikation wissen willst, dan musst du im Kapitel 45 zu den maschinellen Lernverfahren anfangen zu lesen."
  },
  {
    "objectID": "stat-modeling-logistic.html#referenzen",
    "href": "stat-modeling-logistic.html#referenzen",
    "title": "43  Logistische Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "module.html",
    "href": "module.html",
    "title": "Appendix E — Modulbeschreibung",
    "section": "",
    "text": "Wichtige Anmerkung zu den Modulbeschreibungen\n\n\n\nHier finden sich die work in progress Modulbeschreibungen. Diese werden von den offiziellen Modulbeschreibungen abweichen. Es handelt sich hierbei um Entwürfe von Modulbeschreibungen deren langfristiges Ziel es ist in den offiziellen Modulbeschreibungen aufzugehen. Dabei werden sicherlich nicht alle Vorschläge hier übernommen.\nEine inhaltliche Übersicht und die Planung des Vorlesungsverlaufs findest du auf dem Google Spreadsheet zur inhaltlichen Planung."
  },
  {
    "objectID": "module.html#sec-module-statistik",
    "href": "module.html#sec-module-statistik",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.1 Statistik",
    "text": "E.1 Statistik\n\nE.1.1 Inhalte und Qualifikationsziele des Moduls\n\nE.1.1.1 Kurzbeschreibung\nEntwicklung und Qualitätssicherung sind wesentlich getragen durch eine statistische Analyse von Daten. Erhobene und gemessene Daten werden mit Hilfe von statistischen Methoden ausgewertet, dargestellt und interpretiert, um die enthaltenen Informationen zu extrahieren. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt.\n\n\nE.1.1.2 Lehr-Lerninhalte\n\nHypothesenformulierung - Wahl geeigneter Merkmale - Skalenniveaus - Stichprobentheorie - Darstellung und Zusammenfassung der Ergebnisse (beschreibende Statistik) - Überprüfung von Hypothesen (Grundlagen der schließenden Statistik) - statistische Prozesskontrolle - Grundsätze der Versuchsplanung\n\n\n\n\nE.1.2 Kompetenzorientierte Lernergebnisse\n\nE.1.2.1 Wissen und Verstehen\n\nE.1.2.1.1 Wissensverbreiterung\nStudierende kennen die allgemein üblichen statistischen Methoden\n\n\nE.1.2.1.2 Wissensvertiefung\nSie können Hypothesen in adäquate Strategien umwandeln und sie identifizieren die korrekte statistische Methode zur Auswertung der Daten\n\n\nE.1.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Die Studierenden analysieren Daten mit den erlernten Methoden Können - kommunikative Kompetenz Sie analysieren und bewerten fachbezogene Informationen kritisch. Können - systemische Kompetenz Sie können das Risiko von auf Daten gestützten Entscheidungen verdeutlichen und abschätzen\n\n\n\nE.1.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.1.2.2.1 Nutzung und Transfer\n\n\nE.1.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.1.2.3 Kommunikation und Kooperation\n\nE.1.2.3.1 Kommunikation und Kooperation\n\n\n\nE.1.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.1.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.1.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Köhler, Wolfgang, Gabriel Schachtel, and Peter Voleske. Biostatistik: Einführung in die Biometrie für Biologen und Agrarwissenschaftler. Springer-Verlag, 2013.\n\n\n\nE.1.3 Voraussetzungen für die Teilnahme\n\nE.1.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.1.4 Verwendbarkeit des Moduls\n\nE.1.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.1.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.1.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "module.html#sec-module-mathematik",
    "href": "module.html#sec-module-mathematik",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.2 Mathematik und Statistik",
    "text": "E.2 Mathematik und Statistik\n\nE.2.1 Inhalte und Qualifikationsziele des Moduls\n\nE.2.1.1 Kurzbeschreibung\nIn den Biowissenschaften wie auch in Landwirtschaft und Gartenbau werden vielen Prozesse und Phänomene durch mathematische und statistische Modelle beschrieben. Die für Landwirtschaft und Gartenbau relevanten mathematischen und statistischen Verfahren werden dargestellt und diskutiert. Es werden an Fallbeispielen die mathematischen und statistischen Methoden eingeübt.\n\n\nE.2.1.2 Lehr-Lerninhalte\nGrundrechenarten, Zahlen und Mengen, Proportionalität, Prozente, Konzentration und Mischungen, Potenzen, Wurzeln und Logarithmen, Gleichungen, Relationen und wesentliche Funktionen, Vektoren und Matrizen, Folgen, Reihen, Lime, Einführung und praktische Anwendung der Differential- und Integralrechnung Messwerte, Skalenarten, statische Parameter, beschreibende Statistik, Wahrscheinlichkeit, Zufallsvariable und ihre Verteilungen, Schätzen von Parametern, Prüfung von Hypothesen über Mittelwerte, Proportionen und Varianzen, Konfidenzintervalle für Mittelwerte und Varianzen, Einführung in die Regressions- und Varianzanalyse, Einführung in nichtparametrische Teststatistik\n\n\n\nE.2.2 Kompetenzorientierte Lernergebnisse\n\nE.2.2.1 Wissen und Verstehen\n\nE.2.2.1.1 Wissensverbreiterung\nStudierende kennen die grundlegenden mathematischen und statistischen Verfahren, die im weiteren Studium vorausgesetzt werden. Sie können Fallbeispiele selbstständig lösen.\n\n\nE.2.2.1.2 Wissensvertiefung\nSie kennen die grundlegenden Prinzipien der beschreibenden und analytischen Statistik, sie erkennen statistische Probleme und wählen die geeigneten Methoden zu Lösung derselben aus.\n\n\nE.2.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie können Fallbeispiele mithilfe statistischer Software auswerten und die Ergebnisse darstellen. Können - kommunikative Kompetenz Sie können Argumente, Informationen und Ideen, die in dem Lehrgebiet gebräuchlich sind, darstellen und bewerten. Sie können, die aus den Fallbeispielen erhaltenen Ergebnisse analysieren und interpretieren. Können - systemische Kompetenz Sie können die erhaltenen Ergebnisse aus Fallstudien in Beziehung zu den in der Praxis vorhandenen Sachverhalten setzen.\n\n\n\nE.2.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.2.2.2.1 Nutzung und Transfer\n\n\nE.2.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.2.2.3 Kommunikation und Kooperation\n\nE.2.2.3.1 Kommunikation und Kooperation\n\n\n\nE.2.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.2.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.2.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Mathematik für Agrarwissenschaftler, Bartsch, Springer- Verlag\n\n\n\nE.2.3 Voraussetzungen für die Teilnahme\n\nE.2.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.2.4 Verwendbarkeit des Moduls\n\nE.2.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.2.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.2.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "module.html#sec-module-spezielle",
    "href": "module.html#sec-module-spezielle",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.3 Spezielle Statistik und Versuchswesen",
    "text": "E.3 Spezielle Statistik und Versuchswesen\n\nE.3.1 Inhalte und Qualifikationsziele des Moduls\n\nE.3.1.1 Kurzbeschreibung\nIn vielen Bereichen des Gartenbaues und der Landwirtschaft sind vertiefte Kenntnisse in spezifischen statistischen Methoden erforderlich. Die Erlangung von Kenndaten zur Steuerung der Produktion verlangt besondere Kenntnisse über die Planung und Auswertung von Versuchen und über die Datenerfassung, um dann durch eine sachgerechte statistische Auswertung zu korrekten Entscheidungen zu kommen, natürlich unter Berücksichtigung eines gewissen\n\n\nE.3.1.2 Lehr-Lerninhalte\nSubsamplingstruktur, Messwiederholungen; vertiefte Kenntnisse in der Planung, Durchführung und Auswertung von Versuchen im gärtnerisch-landwirtschaftlichen Bereich: Betrachtung wichtiger Versuchsdesigns wie Blockanlage, Lateinisches Quadrat, Spaltanlage, Streifenanlage; Kenntnisse im Umgang mit Software: Auswertung von Versuchsergebnissen mittels bedeutender Statistikprogramme,\n\n\n\nE.3.2 Kompetenzorientierte Lernergebnisse\n\nE.3.2.1 Wissen und Verstehen\n\nE.3.2.1.1 Wissensverbreiterung\nStudierende haben ein fundiertes und umfassendes Wissen über statistische Methoden, die in der Pflanzenproduktion Relevanz haben. Sie haben ein kritisches Verständnis über die Prinzipien, die den statistischen Methoden zu Grunde liegen.\n\n\nE.3.2.1.2 Wissensvertiefung\nSie sind in der Lage gemäß der Versuchsfrage die richtigen statistischen Verfahren auszuwählen, sie verstehen den Zusammenhang zwischen statistischen Methoden und der Versuchsplanung und wählen je nach Problemstellung die geeignetste Versuchsstrategie aus.\n\n\nE.3.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie setzen neben der standardmäßigen statistischen Software auch fortgeschrittene Software ein, die zur Lösung komplexer Probleme notwendig ist, beispielsweise SPSS. Sie erheben, sammeln und übertragen Daten. Können - kommunikative Kompetenz Sie können die in Versuchen erlangten Ergebnisse analysieren und Entscheidungen herbeiführen, diese präsentieren und in praxisrelevante Empfehlungen umsetzen. Können - systemische Kompetenz Sie wenden die Methoden der Datenanalyse auf Fragestellungen der Praxis an.\n\n\n\nE.3.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.3.2.2.1 Nutzung und Transfer\n\n\nE.3.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.3.2.3 Kommunikation und Kooperation\n\nE.3.2.3.1 Kommunikation und Kooperation\n\n\n\nE.3.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.3.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.3.2.5 Literatur\n\n\n\nE.3.3 Voraussetzungen für die Teilnahme\n\nE.3.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.3.4 Verwendbarkeit des Moduls\n\nE.3.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.3.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.3.5.1 Benotete Prüfungsleistung"
  },
  {
    "objectID": "module.html#sec-module-biostatistik",
    "href": "module.html#sec-module-biostatistik",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.4 Biostatistik",
    "text": "E.4 Biostatistik\n\nE.4.1 Inhalte und Qualifikationsziele des Moduls\n\nE.4.1.1 Kurzbeschreibung\nAus den Daten, die sich aus Prozessen und Experimenten ergeben, sollen zuverlässig und objektiv Entscheidungen herbeigeführt werden. Grundvoraussetzung hierzu sind aber vertiefte und umfangreiche Kenntnisse über angewandte statistische Methoden. Deshalb werden die notwendigen wissenschaftlichen und angewandten statistischen Methoden und ihre Prinzipien ausführlich dargestellt und diskutiert. An Fallbeispielen werden die Methoden eingeübt.\n\n\nE.4.1.2 Lehr-Lerninhalte\nWissenschaftliches Arbeiten, Strategien in der Forschung und ihre Beziehungen zu angewandten statistischen Methoden; Population, Merkmalsträger und Messwerte; Wahrscheinlichkeit, Zufallsvariablen und ihre Verteilungen; Stichprobe und Stichprobenverteilung; Interferenz über Mittelwerte und Varianz; allgemeine lineare Modelle; Kontraste und Mittelwertsvergleiche; Schätzen von Varianzkomponenten; Kovarianzanalyse; Nichtparametrische Statistik; Randomisierte Versuchspläne\n\n\n\nE.4.2 Kompetenzorientierte Lernergebnisse\n\nE.4.2.1 Wissen und Verstehen\n\nE.4.2.1.1 Wissensverbreiterung\nDie Absolventen können auf der Grundlage statistischer Methoden Hypothesen aufstellen und prüfen. Sie kennen die wesentlichen verwendeten angewandten statistischen Methoden.\n\n\nE.4.2.1.2 Wissensvertiefung\nSie kennen die Prinzipien, die hinter den angewandten statistischen Methoden stehen und können sich kritisch mit den zur Auswahl stehenden Methoden auseinandersetzen.\n\n\nE.4.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie können Versuchspläne entwickeln, Daten gewinnen und strukturieren, so dass objektive und zuverlässige Entscheidungen getroffen werden können. Können - kommunikative Kompetenz Sie können Daten mithilfe von statistischen Methoden auswerten, aufbereiten, tabellarisch und grafisch darstellen und sind in der Lage, sie in wissenschaftlichen Publikationen zu veröffentlichen. Können - systemische Kompetenz Sie sind in der Lage ihre Ergebnisse in für die Praxis relevanten Empfehlungen umzusetzen.\n\n\n\nE.4.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.4.2.2.1 Nutzung und Transfer\n\n\nE.4.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.4.2.3 Kommunikation und Kooperation\n\nE.4.2.3.1 Kommunikation und Kooperation\n\n\n\nE.4.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.4.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.4.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Köhler, Wolfgang, Gabriel Schachtel, and Peter Voleske. Biostatistik: Einführung in die Biometrie für Biologen und Agrarwissenschaftler. Springer-Verlag, 2013.\n\n\n\nE.4.3 Voraussetzungen für die Teilnahme\n\nE.4.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.4.4 Verwendbarkeit des Moduls\n\nE.4.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.4.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.4.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "module.html#sec-module-angewandte",
    "href": "module.html#sec-module-angewandte",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.5 Angewandte Statistik und Versuchswesen",
    "text": "E.5 Angewandte Statistik und Versuchswesen\n\nE.5.1 Inhalte und Qualifikationsziele des Moduls\n\nE.5.1.1 Kurzbeschreibung\nDer Fortschritt in Pflanzen- und Gartenbau ist wesentlich getragen durch eine intensive Versuchstätigkeit. Um erfolgreich in diesem Bereich tätig zu sein sind neben statistischen Kenntnissen auch solche über die Techniken zur Versuchsdurchführung erforderlich. Messdaten und Beobachtungen aus Erhebungen und Versuchen werden mit Hilfe von statistischen Methoden ausgewertet, dargestellt und interpretiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt.\n\n\nE.5.1.2 Lehr-Lerninhalte\nWeiterführende Kenntnisse in der schließenden Statistik, wissenschaftliche Hypothesenformulierung und -prüfung, Grundlegende Kenntnisse zur Versuchsplanung und Durchführung pflanzenbaulicher Versuche und Auswertung von Versuchsergebnissen mit Hilfe der hierfür relevanten statistischen Methoden\n\n\n\nE.5.2 Kompetenzorientierte Lernergebnisse\n\nE.5.2.1 Wissen und Verstehen\n\nE.5.2.1.1 Wissensverbreiterung\nStudierende kennen die in Landwirtschaft und Gartenbau allgemein üblichen statistischen Methoden, sie haben einen Überblick über die standardmäßig verwendeten Versuchsanlagen.\n\n\nE.5.2.1.2 Wissensvertiefung\nSie können Versuchsfragen in adäquate Versuchspläne und Strategien umwandeln und sie identifizieren die korrekte statistische Methode zur Auswertung der Messdaten.\n\n\nE.5.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie setzen statistische Software zur Auswertung und Darstellung der Daten ein. Können - kommunikative Kompetenz Sie erkennen in ihren Ergebnissen die Sachzusammenhänge und sind in der Lage sie in einem Bericht zu veröffentlichen. Können - systemische Kompetenz Sie können das Risiko von auf Daten gestützten Entscheidungen verdeutlichen und abschätzen.\n\n\n\nE.5.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.5.2.2.1 Nutzung und Transfer\n\n\nE.5.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.5.2.3 Kommunikation und Kooperation\n\nE.5.2.3.1 Kommunikation und Kooperation\n\n\n\nE.5.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.5.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.5.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Köhler, Wolfgang, Gabriel Schachtel, and Peter Voleske. Biostatistik: Einführung in die Biometrie für Biologen und Agrarwissenschaftler. Springer-Verlag, 2013.\n\n\n\nE.5.3 Voraussetzungen für die Teilnahme\n\nE.5.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.5.4 Verwendbarkeit des Moduls\n\nE.5.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.5.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.5.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "module.html#sec-module-bioverfahren",
    "href": "module.html#sec-module-bioverfahren",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.6 Angewandte Statistik für Bioverfahrenstechnik",
    "text": "E.6 Angewandte Statistik für Bioverfahrenstechnik\n\nE.6.1 Inhalte und Qualifikationsziele des Moduls\n\nE.6.1.1 Kurzbeschreibung\nDer wissenschaftliche Fortschritt ist wesentlich getragen durch eine intensive Versuchstätigkeit. Um erfolgreich in diesem Bereich tätig zu sein, sind neben statistischen Kenntnissen auch solche über Versuchsplanung erforderlich. Messdaten und Beobachtungen aus Erhebungen und Versuchen werden mit Hilfe von statistischen Methoden ausgewertet, dargestellt und interpretiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt.\n\n\nE.6.1.2 Lehr-Lerninhalte\n\nHypothesenformulierung - Wahl geeigneter Merkmale - Skalenniveaus - Stichprobentheorie - Darstellung und Zusammenfassung der Ergebnisse (beschreibende Statistik) - Überprüfung von Hypothesen (Grundlagen der schließenden Statistik) - statistische Prozesskontrolle - Versuchsplanung\n\n\n\n\nE.6.2 Kompetenzorientierte Lernergebnisse\n\nE.6.2.1 Wissen und Verstehen\n\nE.6.2.1.1 Wissensverbreiterung\nStudierende kennen die im biologischen Bereich allgemein üblichen statistischen Methoden, sie haben einen Überblick über die standardmäßig verwendeten Versuchsanlagen und kennen die Grundsätze der Versuchsplanung\n\n\nE.6.2.1.2 Wissensvertiefung\nSie können Versuchsfragen in adäquate Versuchspläne und Strategien umwandeln und sie identifizieren die korrekte statistische Methode zur Auswertung der Messdaten,\n\n\nE.6.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie setzen die Standardmethoden zur Darstellung und Auswertung von Daten ein Können - kommunikative Kompetenz Sie erkennen in ihren Ergebnissen die Sachzusammenhänge und sind in der Lage, diese zu präsentieren Können - systemische Kompetenz Sie können das Risiko von auf Daten gestützten Entscheidungen verdeutlichen und abschätzen\n\n\n\nE.6.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.6.2.2.1 Nutzung und Transfer\n\n\nE.6.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.6.2.3 Kommunikation und Kooperation\n\nE.6.2.3.1 Kommunikation und Kooperation\n\n\n\nE.6.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.6.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.6.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Köhler, Wolfgang, Gabriel Schachtel, and Peter Voleske. Biostatistik: Einführung in die Biometrie für Biologen und Agrarwissenschaftler. Springer-Verlag, 2013.\n\n\n\nE.6.3 Voraussetzungen für die Teilnahme\n\nE.6.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.6.4 Verwendbarkeit des Moduls\n\nE.6.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.6.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.6.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "stat-modeling-logistic.html#dichotomisierung-von-y",
    "href": "stat-modeling-logistic.html#dichotomisierung-von-y",
    "title": "43  Logistische Regression",
    "section": "\n43.8 Dichotomisierung von \\(y\\)\n",
    "text": "43.8 Dichotomisierung von \\(y\\)\n\nManchmal ist es so, dass wir eine logistsiche Regression rechnen wollen. Wir fragen nicht, wie ist unser \\(y\\) verteilt und was für eine Regression können wir dann rechnen? Sondern wir wollen mit der logistischen Regression durch die Wand. Wenn wir das wollen, dann können wir unser \\(y\\) dichotomisieren. Das heißt, wir machen aus einer Variable, die mehr als zwei Level hat einen Faktor mit zwei Leveln. Dafür stehen uns verschiedene Möglichkeiten offen. Wie immer gehe ich die Möglichkeiten von simple nach komplex einmal durch."
  },
  {
    "objectID": "stat-modeling-logistic.html#dichotomisierung",
    "href": "stat-modeling-logistic.html#dichotomisierung",
    "title": "43  Logistische Regression",
    "section": "\n43.8 Dichotomisierung",
    "text": "43.8 Dichotomisierung\nManchmal ist es so, dass wir eine logistsiche Regression rechnen wollen. Wir fragen nicht, wie ist unser \\(y\\) verteilt und was für eine Regression können wir dann rechnen? Sondern wir wollen mit der logistischen Regression durch die Wand. Wenn wir das wollen, dann können wir unser \\(y\\) dichotomisieren. Das heißt, wir machen aus einer Variable, die mehr als zwei Level hat einen Faktor mit zwei Leveln. Dafür stehen uns verschiedene Möglichkeiten offen.\nIn dem R Paket dplyr haben wir mit der Funktion recode() die Möglichkeit eine Variable von alt = neu umzukodieren. Dabei müssen wir natürlich darauf achten, dass wir die alten Level der Variable richtig schreiben und bei der neuen Level nur zwei Namen eintragen. Dann sind wir auch schon durch mit der Umbenennung.\n\npig_tbl %>% \n  mutate(frailty = recode(frailty, \n                          \"robust\" = \"robust\", \n                          \"pre-frail\" = \"frail_prefrail\", \n                          \"frail\" = \"frail_prefrail\")) %>% \n  pull(frailty) %>% extract(1:20)\n\n [1] \"robust\"         \"robust\"         \"robust\"         \"robust\"        \n [5] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n [9] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n[13] \"robust\"         \"robust\"         \"robust\"         \"frail_prefrail\"\n[17] \"robust\"         \"frail_prefrail\" \"frail_prefrail\" \"frail_prefrail\"\n\n\nIch finde die Funktion case_when() etwas übersichtlicher. Das ist aber eigentlich nur eine Geschmacksfrage. Am Ende kommt jedenfalls das Gleiche heraus.\n\npig_tbl %>% \nmutate(frailty = case_when(frailty == \"robust\" ~ \"robust\",\n                           frailty == \"pre-frail\" ~ \"frail\",\n                           frailty == \"frail\" ~ \"frail\")) %>% \n  pull(frailty) %>% extract(1:20)\n\n [1] \"robust\" \"robust\" \"robust\" \"robust\" \"robust\" \"robust\" \"frail\"  \"robust\"\n [9] \"robust\" \"robust\" \"frail\"  \"robust\" \"robust\" \"robust\" \"robust\" \"frail\" \n[17] \"robust\" \"frail\"  \"frail\"  \"frail\" \n\n\nHäufig haben wir auch den Fall, dass wir keine kontinuierlichen \\(x\\) in unseren Daten wollen. Alles soll sich in Faktoren verwandeln, so dass wir immer eine 2x2 Tafel haben. Wenn es sein muss, liefert hier cutpointr() die Lösung für dieses Problem. Wir müssen dafür zum einen unser kontinuierliches \\(x\\) angeben und dann mit class unser binäres \\(y\\). Wir erhalten dann für unser \\(y\\) den bestmöglichen Split für unser \\(x\\). Im Beispiel wollen wir einmal die Variable crp für unser Outcome infected in zwei Gruppen aufteilen. Wir wollen eigentlich immer zwei Gruppen, da wir dann in dem Setting eines \\(\\mathcal{X}^2\\)-Test und einer einfacheren Interpretation von dem \\(OR\\) sind.\nWir immer haben wir eine große Bandbreite an Optionen, wie wir den besten Split unseres \\(x\\) kriegen wollen. Ich gehe hier mit den Default-Werten. Damit kommt man eigentlich recht weit. Ich möchte gerne die Summe der Sensivität und der Spezifität sum_sens_spec über alle möglichen Cutpoints maximieren maximize_metric. Der Cutpoint mit der maximalen Summe an Sensivität und der Spezifität wird mir dann wiedergegeben.\n\n\nNatürlich hat das R Paket cutpoint noch viel mehr Optionen. Mehr gibt es in An introduction to cutpointr.\n\ncp_crp <- cutpointr(data = pig_tbl,\n                    x = crp,\n                    class = infected,\n                    method = maximize_metric, \n                    metric = sum_sens_spec) \n\ncp_crp\n\n# A tibble: 1 x 16\n  direction optimal_cutpoint method          sum_sens_spec   acc sensitivity\n  <chr>                <dbl> <chr>                   <dbl> <dbl>       <dbl>\n1 >=                   19.84 maximize_metric       1.49424  0.75    0.758755\n  specificity      AUC pos_class neg_class prevalence outcome  predictor\n        <dbl>    <dbl>     <dbl>     <dbl>      <dbl> <chr>    <chr>    \n1    0.735484 0.811786         1         0   0.623786 infected crp      \n  data               roc_curve                  boot \n  <list>             <list>                     <lgl>\n1 <tibble [412 x 2]> <roc_cutpointr [282 x 10]> NA   \n\n\nIn Abbildung 43.4 sehe wir die Ausgabe der Funktion cutpointr() nochmal visualisiert. Wir sehen, dass der Split einigermaßen die crp-Werte im Sinne von unserem Outcome aufteilt.\n\nplot(cp_crp)\n\n\n\nAbbildung 43.4— Visualisierung des Ergebnisses der Funktion cutpointr für die Variable crp.\n\n\n\n\nWir können uns jetzt noch den optimalen Cutpoint aus der Ausgabe herausziehen, wenn wir den Punkt nicht aus der Ausgabe ablesen wollen.\n\npluck(cp_crp, \"optimal_cutpoint\")\n\n[1] 19.84\n\n\nAm Ende können wir dann über case_when() uns ein binären CRP-Wert zusammenbauen. Wir müssen dann natürlich entscheiden welche Variable wir mit ins Modell nehme, aber meistens machen wir uns ja die Mühen um dann die neue Variable zu verwenden.\n\npig_tbl %>% \nmutate(crp_bin = case_when(crp >= 19.84 ~ \"high\",\n                           crp < 19.84 ~ \"low\")) %>% \nselect(crp, crp_bin)  \n\n# A tibble: 412 x 2\n     crp crp_bin\n   <dbl> <chr>  \n 1  22.4 high   \n 2  18.6 low    \n 3  18.8 low    \n 4  19.4 low    \n 5  21.6 high   \n 6  21.4 high   \n 7  19.0 low    \n 8  19.0 low    \n 9  21.9 high   \n10  21.0 high   \n# ... with 402 more rows\n\n\nDamit haben wir uns dann auch mit dem Problem der Dichotomisierung in der logististischen Regression einmal beschäftigt. Somit bleibt dann noch die Prädiktion übrig."
  },
  {
    "objectID": "stat-tests-theorie.html",
    "href": "stat-tests-theorie.html",
    "title": "20  Die Testtheorie",
    "section": "",
    "text": "Version vom November 16, 2022 um 13:12:21\nIn diesem Kapitel wollen wir uns nochmal tiefer mit der Testtherorie und dem \\(\\alpha\\)-Fehler und der \\(\\beta\\)-Fehler beschäftigen. Was heißt eigentlich einseitig oder zweiseitig Testen? Auch müssen wir nochmal einen Blick auf das mutliple Testen und die \\(\\alpha\\)-Adjustierung werfen.\nWir können auf allen Daten einen statistischen Test rechnen und erhalten statistische Maßzahlen wie eine Teststatistik, einen p-Wert oder ein 95% Konfidenzintervall. Wie wir aus dem vorherigen Kapitel wissen testet jeder statistische Test eine Nullhypothese. Ob diese zu testende Nullhypothese dem Anwender nun bekannt ist oder nicht, ein statistischer Test testet eine Nullhypothese. Daher müssen wir uns immer klar sein, was die entsprechende Nullhypothese zu unserer Fragestellung ist. Manchmal ist das gar nicht so klar und die Nullhypothese zu einer Fragestellung zu finden ist auch nicht einfach. Hier musst du vermutlich etwas überlegen oder dir Hilfe suchen.\nWiederholen wir nochmal was eine statistische Hypothese ist. Eine statistische Hypothese ist eine Aussage über einen Parameter einer Population. Die Nullhypothese \\(H_0\\) nennen wir auch die Null oder Gleichheitshypothese. Die Nullhypothese sagt aus, dass zwei Gruppen gleich sind oder aber kein Effekt zu beobachten ist. In diesem Beispiel sind die beiden Mittelwerte gleich.\n\\[\nH_0: \\bar{y}_{1} = \\bar{y}_{2}\n\\]\nDie Alternativehypothese \\(H_A\\) oder \\(H_1\\) auch Alternative genannt nennen wir auch Unterschiedshypothese. Die Alternativehypothese besagt, dass ein Unterschied vorliegt oder aber ein Effekt vorhanden ist. In diesem Beispiel unterscheiden sich die beiden Mittelwerte.\n\\[\nH_A: \\bar{y}_{1} \\neq \\bar{y}_{2}\n\\]\nEs ist wichtig sich in Erinnerung zu rufen, dass wir nur und ausschließlich Aussagen über die Nullhypothese treffen können. Das frequentistische Hypothesentesten kann nichts anders. Wir kriegen keine Aussage über die Alternativhypothese sondern nur eine Abschätzung der Wahrscheinlichkeit des Auftretens der Daten im durchgeführten Experiment, wenn die Nullhypothese wahr wäre."
  },
  {
    "objectID": "stat-tests-theorie.html#sec-alpha-beta",
    "href": "stat-tests-theorie.html#sec-alpha-beta",
    "title": "20  Die Testtheorie",
    "section": "\n20.1 Der \\(\\alpha\\)-Fehler und der \\(\\beta\\)-Fehler",
    "text": "20.1 Der \\(\\alpha\\)-Fehler und der \\(\\beta\\)-Fehler\nVielleicht ist die Idee der Testtheorie und der Testentscheidung besser mit der Analogie des Rauchmelders zu verstehen. Wir nehmen an, dass der Rauchmelder der statistische Test ist. Der Rauchmelder hängt an der Decke und soll entscheiden, ob es brennt oder nicht. Daher muss der Rauchmelder entscheiden, die Nullhypothese “kein Feuer” abzulehnen oder die Hypothese “kein Feuer” beizubehalten.\n\\[\n\\begin{aligned}\nH_0&: \\mbox{kein Feuer im Haus}  \\\\  \nH_A&: \\mbox{Feuer im Haus}  \\\\   \n\\end{aligned}\n\\]\nWir können jetzt den Rauchmelder so genau einstellen, dass der Rauchmelder bei einer Kerze losgeht. Oder aber wir stellen den Rauchmelder so ein, dass er erst bei einem Stubenbrand ein Piepen von sich gibt. Wie sensibel auf Rauch wollen wir den Rauchmelder einstellen? Soll der Rauchmelder sofort die Nullhypothese ablehnen? Wenn also nur eine Kerze brennt. Soll also der \\(\\alpha\\)-Fehler groß sein? Erinnere dich, mit einem großen \\(\\alpha\\)-Fehler würden wir mehr Nullhypothesen ablehnen oder anders gesprochen leichter die Nullhypothese ablehnen. Wir würden ja den \\(\\alpha\\)-Fehler zum Beispiel von 5% auf 20% setzen können. Das wäre nicht sehr sinnvoll. Die Feuerwehr würde schon bei einer Kerze kommen oder wenn wir mal was anbrennen. Wir dürfen also den \\(\\alpha\\)-Fehler nicht zu groß einstellen.\nIntuitiv würde man meinen, ein sehr kleiner \\(\\alpha\\)-Fehler nun sinnvoll sei. Wenn wir aber den Rauchmelder sehr unsensibel einstellen, also der Rauchmelder erst bei sehr viel Rauch die Nullhypothese ablehnt, könnte das Haus schon unrettbar in Flammen stehen. Dieser Fehler, Haus steht in Flammen und der Rauchmelder geht nicht, wird als \\(\\beta\\)-Fehler bezeichnet. Wie du siehst hängen die beiden Fehler miteinander zusammen. Wichtig hierbei ist immer, dass wir uns einen Zustand vorstellen, das Haus brennt nicht (\\(H_0\\) ist wahr) oder das Haus brennt nicht (\\(H_A\\) ist wahr). An diesem Zustand entscheiden wir dann, wie hoch der Fehler jeweils sein soll diesen Zustand zu übersehen.\n\n\n\n\n\n\nDer \\(\\alpha\\)-Fehler und \\(\\beta\\)-Fehler als Rauchmelderanalogie\n\n\n\nHäufig verwirrt die etwas theoretische Herangehensweise an den \\(\\alpha\\)-Fehler und \\(\\beta\\)-Fehler. Wir versuchen hier nochmal die Analogie eines Rauchmelders und dem Feuer im Haus.\n\n\nAbbildung 20.1— Andere Art der Darstellung des \\(\\alpha\\)-Fehlers als Alarm without fire und dem \\(\\beta\\)-Fehler als Fire without alarm. Je nachdem wie empfindlich wir den Alarm des Rauchmelders (den statistischen Test) über das \\(\\alpha\\) einstellen, desto mehr Alarm bekommen wir ohne das ein Effekt vorhanden wäre. Drehen wir den Alarm zu niedrig, dann kriegen wir kein Feuer mehr angezeigt, den \\(\\beta\\)-Fehler.\n\n\n\n\n\\(\\boldsymbol{\\alpha}\\)-Fehler: Alarm without fire. Der statistische Test schlägt Alarm und wir sollen die \\(H_0\\) ablehnen, obwohl die \\(H_0\\) in Wahrheit gilt und kein Effekt vorhanden ist.\n\n\\(\\boldsymbol{\\beta}\\)-Fehler: Fire without alarm. Der statistische Test schlägt nicht an und wir sollen die \\(H_0\\) beibehalten, obwohl die \\(H_0\\) in Wahrheit nicht gilt und ein Effekt vorhanden ist.\n\n\n\nWie sieht nun die Lösung, erstmal für unseren Rauchmelder, aus? Wir müssen Grenzen für den \\(\\alpha\\) und \\(\\beta\\)-Fehler festlegen bei denen der Rauchmelder angeht und wir die Feuerwehr rufen.\nWir setzen den \\(\\alpha\\)-Fehler auf 5%.\n\nWir setzen den \\(\\alpha\\)-Fehler auf 5%. Somit haben wir in 1 von 20 Fällen das Problem, dass uns der Rauchmelder angeht obwohl gar kein Feuer da ist. Wir lehnen die Nullhypothese ab, obwohl die Nullhypothese gilt.\n\nWir setzen den \\(\\beta\\)-Fehler auf 20%.\n\nAuf der anderen Seite setzen wir den \\(\\beta\\)-Fehler auf 20%. Damit brennt uns die Bude in 1 von 5 Fällen ab ohne das der Rauchmelder einen Pieps von sich gibt. Wir behalten die Nullhypothese bei, obwohl die Nullhypothese nicht gilt.\n\nNachdem wir uns die Testentscheidung mit der Analogie des Rauchmelders angesehen haben, wollen wir uns wieder der Statistik zuwenden. Betrachten wir das Problem nochmal von der theoretischen Seite mit den statistischen Fachbegriffen.\nSoweit haben wir es als gegeben angesehen, dass wir eine Testentscheidung durchführen. Entweder mit der Teststatistik, dem \\(p\\)-Wert oder dem 95% Konfidenzintervall. Immer wenn wir eine Entscheidung treffen, können wir auch immer eine falsche Entscheidung treffen. Wie wir wissen hängt die berechnete Teststatistik \\(T_{calc}\\) nicht nur vom Effekt \\(\\Delta\\) ab sondern auch von der Streuung \\(s\\) und der Fallzahl \\(n\\). Auch können wir den falschen Test wählen oder Fehler im Design des Experiments gemacht haben. Schlussendlich gibt es viele Dinge, die unsere simple mathematischen Formeln beeinflussen können, die wir nicht kennen. Ein frequentistischer Hypothesentest gibt immer nur eine Aussage über die Nullhypothese wieder. Also ob wir die Nullhypothese ablehnen können oder nicht.\nAbbildung 20.2 zeigt die theoretische Verteilung der Nullyhypothese und der Alternativehypothese. Wenn die beiden Verteilungen sehr nahe beieinander sind, wird es schwer für den statistischen Test die Hypothesen klar voneinander zu trennen. Die Verteilungen überlappen. Es gibt einen sehr kleinen Unterschied in den Sprungweiten zwischen Hunde- und Katzenflöhen. In dem Beispiel wurde der gesamte \\(\\alpha\\)-Fehler auf die rechte Seite gelegt. Das macht die Darstellung etwas einfacher.\n\n\n\nAbbildung 20.2— Darstellung der Null- und Alternativehypothese. Mit steigendem \\(T_{calc}\\) wird die Wahrscheinlichkeit für die \\(H_0\\) immer kleiner. Leider ist uns nichts über \\(H_A\\) und deren Lage bekannt. Sollte die \\(H_A\\) Verteilung zu weit nach links ragen, könnten wir die \\(H_0\\) beibehalten, obwohl die \\(H_A\\) gilt.\n\n\n\nAchtung In der Regression wird uns auch wieder das \\(\\beta\\) als Symbol begegnen. In der statistischen Testtheorie ist das \\(\\beta\\) ein Fehler; in der Regression ist das \\(\\beta\\) ein Koeffizient der Regression. Hier ist der Kontext wichtig.\nWir können daher bei statistischen Testen zwei Arten von Fehlern machen. Zum einen den \\(\\alpha\\) Fehler oder auch Type I Fehler genannt. Zum anderen den \\(\\beta\\) Fehler oder auch Type II Fehler genannt. Die Grundidee basiert darauf, dass wir eine Testentscheidung gegen die Nullhypothese machen. Diese Entscheidung kann richtig sein, da in Wirklichkeit die Nullhypothese gilt oder aber falsch sein, da in Wirklichkeit die Nullhypothese nicht gilt. In Abbildung 20.3 wird der Zusammenhang in einer 2x2 Tafel veranschaulicht.\n\n\n\nAbbildung 20.3— Zusammenhang zwischen der Testentscheidung gegen die \\(H_0\\) Hypothese sowie dem Beibehalten der \\(H_0\\) Hypothese und der unbekannten Wahrheit in der die \\(H_0\\) falsch sein kann oder die \\(H_0\\) wahr sein kann. Wir können mit unserer Testenstscheidung richtig liegen oder falsch. Mit welcher Wahrscheinlichkeit geben der \\(\\alpha\\) Fehler und \\(\\beta\\) Fehler wieder. Unten rechts ist der Zusammenhang zu der Abbildung 20.2 gezeigt.\n\n\n\n\n\nDie Diskussion über den \\(p\\)-Wert und dem Vergleich mit dem \\(\\alpha\\)-Fehler wird in der Statistik seit 2019 verstärkt diskutiert (Wasserstein, Schirm, und Lazar 2019). Das Nullritual wird schon lamge kritisiert (Gigerenzer, Krauss, und Vitouch 2004). Siehe dazu auch The American Statistician, Volume 73, Issue sup1 (2019).\nBeide Fehler sind Kulturkonstanten. Das heißt, dass sich diese Zahlen von 5% und 20% so ergeben haben. Es gibt keinen rationalen Grund diese Zahlen so zu nehmen. Prinzipiell schon, aber die Gründe leigen eher in der Anwendbarkeit von feststehenden Tabellen vor der Entwicklung des Computers. Man kann eigentlich sagen, dass die 5% und die 20% eher einem Zufall entsprungen sind, als einer tieferen Rationalen. Wir behalten diese beiden Zahlen bei aus den beiden schlechtesten Gründe überhaupt: i) es wurde schon immer so gemacht und ii) viele machen es so.\nEine weitere wichtige statistische Maßzahl im Kontext der Testtheorie ist die \\(Power\\) oder auch \\(1-\\beta\\). Die \\(Power\\) ist die Gegenwahrscheinlichkeit von dem \\(\\beta\\)-Fehler. In der Analogie des Rauchmelders wäre die \\(Power\\) daher Alarm with fire. Das heißt, wie wahrscheinlich ist es einen wahren Effekt - also einen Unterschied - mit dem statistischen Test auch zu finden. Oder anders herum, wenn wir wüssten, dass die Hunde- und Katzenflöhe unterschiedliche weit springen, mit welcher Wahrscheinlichkeit würde diesen Unterschied ein statistsicher Test auch finden? Mit eben der \\(Power\\), also gut 80%. Tabelle 20.1 zeigt die Abhängigkeit der \\(Power\\) vom Effekt \\(\\Delta\\), der Streuung \\(s\\) und der Fallzahl \\(n\\).\nDie \\(Power\\) ist eine Wahrscheinlichkeit und sagt nichts über die Relevanz des Effektes aus.\n\n\nTabelle 20.1— Abhängigkeit der \\(Power (1-\\beta)\\) vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\). Die \\(Power\\) ist eine Wahrscheinlichkeit und sagt nichts über die Relevanz des Effektes aus.\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{Power (1-\\beta)}\\)\n\n\\(\\boldsymbol{Power (1-\\beta)}\\)\n\n\n\n\\(\\Delta \\uparrow\\)\nsteigt\n\\(\\Delta \\downarrow\\)\nsinkt\n\n\n\\(s \\uparrow\\)\nsinkt\n\\(s \\downarrow\\)\nsteigt\n\n\n\\(n \\uparrow\\)\nsteigt\n\\(n \\downarrow\\)\nsinkt"
  },
  {
    "objectID": "stat-tests-theorie.html#sec-einseitig-zweiseitig",
    "href": "stat-tests-theorie.html#sec-einseitig-zweiseitig",
    "title": "20  Die Testtheorie",
    "section": "\n20.2 Einseitig oder zweiseitig?",
    "text": "20.2 Einseitig oder zweiseitig?\nManchmal kommt die Frage auf, ob wir einseitig oder zweiseitig einen statistischen Test durchführen wollen. Beim Fall des zweiseitigen Testens verteilen wir den \\(\\alpha\\)-Fehler auf beide Seiten der Testverteilung mit jeweils \\(\\cfrac{\\alpha}{2}\\). In dem Fall des einseitigen Tests liegt der gesamte \\(\\alpha\\)-Fehler auf der rechten oder linken Seite der Testverteilung. In Abbildung 20.4 wird der Zusammenhang beispielhaft an der t-Verteilung gezeigt.\n\n\n\nAbbildung 20.4— Zusammenhang zwischen dem einseitigen und zweiseitigen Testen. Im Falle des zweiseitigen Testens teilen wir den \\(\\alpha\\)-Fehler auf beide Seiten der beispielhaften t-Verteilung auf. Im Falle des einseitigen Testen leigt der gesamte \\(\\alpha\\)-Fehler auf der rechten oder der linken Seite der t-Verteilung.\n\n\n\nIn der Anwendung testen wir immer zweiseitig.\nIn der Anwendung testen wir immer zweiseitig. Der Grund ist, dass das Vorzeichen von der Teststatik davon abhängt, welche der beiden Gruppen den größeren Mittelwert hat. Da wir die Mittelwerte vor der Auswertung nicht kennen, können wir auch nicht sagen in welche Richtung der Effekt und damit die Teststatistik laufen wird.\nEs gibt theoretisch Gründe, die für ein einseitiges Testen unter bestimmten Bedingungen sprechen, aber wir nutzen in der Anwendung nur das zweiseite Testen. Wir müssen dazu in R auch nichts weiter angeben. Ein durchgeführter statistischer Test in R testet automatisch immer zweiseitig.\n\n\n\n\n\n\nEinseitig oder zweiseitig im Spiegel der Regulierungsbehörden\n\n\n\nIn den allgemeinen Methoden des IQWiG, einer Regulierungsbehörde für klinische Studien, wird grundsätzlich das zweiseitige Testen empfohlen. Wenn einseitig getestet werden sollte, so soll das \\(\\alpha\\)-Niveau halbiert werden. Was wiederum das gleiche wäre wie zweiseitiges Testen - nur mit mehr Arbeit.\nZur besseren Vergleichbarkeit mit 2-seitigen statistischen Verfahren wird in einigen Guidelines für klinische Studien eine Halbierung des üblichen Signifikanzniveaus von 5 % auf 2,5 % gefordert. – Allgemeine Methoden Version 6.1 vom 24.01.2022, p. 180"
  },
  {
    "objectID": "stat-tests-theorie.html#sec-statistisches-testen-alpha-adjust",
    "href": "stat-tests-theorie.html#sec-statistisches-testen-alpha-adjust",
    "title": "20  Die Testtheorie",
    "section": "\n20.3 Adjustierung für multiple Vergleiche",
    "text": "20.3 Adjustierung für multiple Vergleiche\nDas simultane Testen von mehreren Hypothesen führt zu einer \\(\\alpha\\)-Fehler Inflation\nIm Kapitel 31 werden wir mehrere multiple Gruppenvergleiche durchführen. Das heißt, wir wollen nicht nur die Sprungweite von Hunde- und Katzenflöhen miteinander vergleichen, sondern auch die Sprungweite von Hunde- und Fuchsflöhen sowie Katzen- und Fuchsflöhen. Wir würden also \\(k = 3\\) t-Tests für die Mittelwertsvergleiche rechnen.\nDieses mehrfache Testen führt aber zu einer Inflation des \\(\\alpha\\)-Fehlers oder auch Alphafehler-Kumulierung genannt. Daher ist die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird, nicht mehr durch das Signifikanzniveau \\(\\alpha\\) kontrolliert, sondern kann sehr groß werden.\nGehen wir von einer Situation mit \\(k\\) Null- und Alternativhypothesen aus. Wir rechnen also \\(k\\) statistische Tests und alle Nullhypothesen werden zum lokalen Niveau \\(\\alpha_{local} = 0.05\\) getestet. Im Weiteren nehmen wir an, dass tatsächlich alle Nullhypothesen gültig sind. Wir rechnen also \\(k\\) mal einen t-Test und machen jedes mal einen 5% Fehler Alarm zu geben, obwohl kein Effekt vorhanden ist.\nDie Wahrscheinlichkeit für einen einzelnen Test korrekterweise \\(H_0\\) abzulehnen ist \\((1 − \\alpha)\\). Da die \\(k\\) Tests unabhängig sind, ist die Wahrscheinlichkeit alle \\(k\\) Tests korrekterweise abzulehnen \\((1 − \\alpha)^k\\). Somit ist die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird \\(1-(1-\\alpha)^k\\). In der Tabelle 20.2 wird dieser Zusammenhang nochmal mit Zahlen für verschiedene \\(k\\) deutlich.\n\n\nTabelle 20.2— Inflation des \\(\\alpha\\)-Fehlers. Wenn 50 Hypothesen getestet werden, ist die Wahrscheinlichkeit mindestens eine falsche Testentscheidung zu treffen fast sicher.\n\nAnzahl Test \\(\\boldsymbol{k}\\)\n\n\\(\\boldsymbol{1-(1-\\alpha)^k}\\)\n\n\n\n1\n0.05\n\n\n2\n0.10\n\n\n10\n0.40\n\n\n50\n0.92\n\n\n\n\nAus Tabelle 20.3 können wir entnehmen, dass wenn 100 Hypothesen getestet werden, werden 5 Hypothesen im Schnitt fälschlicherweise abgelehnt. Die Tabelle 20.3 ist nochmal die Umkehrung der vorherigen Tabelle 20.2.\n\n\nTabelle 20.3— Inflation des \\(\\alpha\\)-Fehlers. Erwartete Anzahl fälschlich abgelehnter Nullhypothesen abhängig von der Anzahl der durchgeführten Tests\n\nAnzahl Test \\(\\boldsymbol{k}\\)\n\n\\(\\boldsymbol{\\alpha \\cdot k}\\)\n\n\n\n1\n0.05\n\n\n20\n1\n\n\n100\n5\n\n\n200\n10\n\n\n\n\nNachdem wir verstanden haben, dass wiederholtes statistisches Testen irgendwann immer ein signifikantes Ergebnis produziert, müssen wir für diese \\(\\alpha\\) Inflation unsere Ergebnisse adjustieren. Ich folgenden stelle ich verschiedene Adjustierungsverfahren vor.\nWie können wir nun die p-Werte in R adjustieren? Zum einen passiert dies teilweise automatisch zum anderen müssen wir aber wissen, wo wir Informationen zu den Adjustierungsmethoden finden. Die Funktion p.adjust() ist hier die zentrale Anlaufstelle. Hier finden sich alle implementierten Adjustierungsmethoden in R.\nIm folgenden Code erschaffen wir uns 50 \\(z\\)-Werte von denen 25 aus einer Normalverteilung \\(\\mathcal{N}(0, 1)\\) und 25 aus einer Normalverteilung mit \\(\\mathcal{N}(3, 1)\\) kommen. Die Fläche unter allen Normalverteilungen ist Eins, da die Standatdabweichung Eins ist. Wir berechnen die \\(p-Wert\\) anhand der Fläche rechts von dem \\(z\\)-Wert. Wir testen zweiseitig, deshalb multiplizieren wir die \\(p\\)-Werte mit Zwei. Diese \\(p\\)-Werte können wir nun im Folgenden für die Adjustierung nutzen.\n\nz <- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))\np <- 2*pnorm(sort(-abs(z)))\n\nÜber die eckigen Klammern [] und das : können wir uns die ersten zehn p-Werte wiedergeben lassen.\n\np[1:10] %>% round(5)\n\n [1] 0.00000 0.00001 0.00007 0.00009 0.00027 0.00050 0.00124 0.00251 0.00324\n[10] 0.00442\n\n\nWir sehen, dass die ersten fünf p-Werte hoch signifikant sind. Das würden wir auch erwarten, immerhin haben wir ja auch 25 \\(z\\)-Werte mit einem Mittelwert von Drei. Du kannst dir den \\(z\\)-Wert wie den \\(t\\)-Wert der Teststatistik vorstellen.\n\n20.3.1 Bonferroni Korrektur\nDie Bonferroni Korrektur ist die am weitesten verbreitete Methode zur \\(\\alpha\\) Adjustierung, da die Bonferroni Korrektur einfach durchzuführen ist. Damit die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird beim simultanen Testen von \\(k\\) Hypothesen durch das globale (und multiple) Signifikanzniveau \\(\\alpha = 5\\%\\) kontrolliert ist, werden die Einzelhypothesen zum lokalen Signifikanzniveau \\(\\alpha_{local} = \\tfrac{\\alpha_{5\\%}}{k}\\) getestet.\nDabei ist das Problem der Bonferroni Korrektur, dass die Korrektur sehr konservativ ist. Wir meinen damit, dass das tatsächliche globale (und multiple) \\(\\sum\\alpha_{local}\\) Niveau liegt deutlich unter \\(\\alpha_{5\\%}\\) und somit werden die Nullhypothesen zu oft beibehalten.\n\n\n\n\n\n\nAdjustierung des \\(\\boldsymbol{\\alpha}\\)-Fehlers\n\n\n\n\nDas globale \\(\\alpha\\)-Level wird durch die Anzahl \\(k\\) an durchgeführten statistischen Tests geteilt.\n\n\\(\\alpha_{local} = \\tfrac{\\alpha}{k}\\) für die Entscheidung \\(p < \\alpha_{local}\\)\n\n\n\n\n\n\n\n\n\n\nAdjustierung des \\(\\boldsymbol{p}\\)-Wertes\n\n\n\n\nDie p-Werte werden mit der Anzahl an durchgeführten statistischen Tests \\(k\\) multipliziert.\n\n\\(p_{adjust} = p_{raw} \\cdot k\\) mit \\(k\\) gleich Anzahl der Vergleiche.\nwenn \\(p_{adjust} > 1\\), wird \\(p_{adjust}\\) gleich 1 gesetzt, da \\(p_{adjust}\\) eine Wahrscheinlichkeit ist.\n\n\n\nWir schauen uns die ersten zehn nach Bonferroni adjustierten p-Wert nach der Anwendung der Funktion p.adjust() einmal an.\n\np.adjust(p, \"bonferroni\")[1:10] %>% round(3)\n\n [1] 0.000 0.000 0.004 0.005 0.014 0.025 0.062 0.126 0.162 0.221\n\n\nNach der Adjustierung erhalten wir weniger signifikante \\(p\\)-Werte als vor der Adjustierung. Wir sehen aber, dass wir weit weniger signifikante Ergebnisse haben, als wir eventuell erwarten würden. Wir haben immerhin 25 \\(z\\)-Werte mit einem Mittelwert von Drei. Nach der Bonferroni-Adjustierung hgaben wir nur noch sechs signifikante \\(p\\)-Werte.\n\n20.3.2 Benjamini-Hochberg\nDie Benjamini-Hochberg Adjustierung für den \\(\\alpha\\)-Fehler wird auch Adjustierung nach der false discovery rate (abk. FDR) bezeichnet. Meistens werden beide Namen synoym verwendet, der Trend geht jedoch hin zur Benennung mit der Abkürzung FDR. In der Tabelle 20.4 sehen wir einmal ein Beispiel für die FDR Adjustierung. Die Idee ist, dass wir uns für jeden der \\(m\\) Vergleiche eine eigene lokale Signifikanzschwelle \\(\\alpha_{local}\\) berechnen. Dafür rangieren wir zuerst unsere Vergleiche nach dem \\(p\\)-Wert. Der kleinste \\(p\\)-Wert kommt zuerst und dann der Rest der anderen \\(p\\)-Werte. Wir berechnen jedes lokale Signifkanzniveau mit \\(\\alpha_{local} =(i/m)\\cdot Q\\). Dabei steht das \\(i\\) für den jeweiligen Rang und das \\(m\\) für unsere Anzahl an Vergleichen. In unserem Beispiel haben wir \\(m = 25\\) Vergleiche. Jetzt kommt der eugentlich spannende Teil. Wir können jetzt \\(Q\\) als unsere false discovery rate selber wählen! In unserem Beispiel setzen wir die FDR auf 25%. Es geht aber auch 20% oder 10%. Wie du möchtest.\nIn der letzten Spalte der Tavbelle siehst du die Entscheidung, ob eine Variable noch signifkant ist oder nicht. Wir entscheiden nach folgender Regel. Der fettgedruckte p-Wert für den fertilizer ist der höchste p-Wert, der auch kleiner mit \\(0.042 < 0.050\\) als der kritische Wert ist. Alle darüber liegenden Werte und damit diejenigen mit niedrigeren p-Werten werden hervorgehoben und als signifikant betrachtet, auch wenn diese p-Werte unter den kritischen Werten liegen. Beispielsweise sind N und sun einzeln nicht signifikant, wenn Sie das Ergebnis mit der letzten Spalte vergleichen. Mit der FDR-Korrektur werden sie jedoch als signifikant angesehen.\n\n\nTabelle 20.4— Beispiel für die Benjamini-Hochberg-Prozedur der \\(\\alpha\\)-Fehleradjustierung.\n\n\n\n\n\n\n\n\nVariable\n\\(\\boldsymbol{Pr(D|H_0)}\\)\nRang (\\(\\boldsymbol{i}\\))\n\\(\\boldsymbol{(i/m)\\cdot Q}\\)\n\n\n\n\nfe\n0.001\n1\n\\(1/25 \\cdot 0.25 = 0.01\\)\ns.\n\n\nwater\n0.008\n2\n\\(2/25 \\cdot 0.25 = 0.02\\)\ns.\n\n\nN\n0.039\n3\n\\(3/25 \\cdot 0.25 = 0.03\\)\ns.\n\n\nsun\n0.041\n4\n\\(4/25 \\cdot 0.25 = 0.04\\)\ns.\n\n\nfertilizer\n0.042\n5\n\\(5/25 \\cdot 0.25 = 0.05\\)\ns.\n\n\ninfected\n0.060\n6\n\\(6/25 \\cdot 0.25 = 0.06\\)\nn.s.\n\n\nwind\n0.074\n7\n\\(7/25 \\cdot 0.25 = 0.07\\)\nn.s.\n\n\nS\n0.205\n8\n\\(8/25 \\cdot 0.25 = 0.08\\)\nn.s.\n\n\n…\n…\n…\n…\n…\n\n\nblock\n0.915\n25\n\\(25/25 \\cdot 0.25 = 0.25\\)\nn.s.\n\n\n\n\nDie adjustierten \\(p\\)-Werte nach der Benjamini-Hochberg-Prozedur ist etwas umständlicher, deshalb benutzen wir hier die Funktion p.adjust() in R und erhalten damit die adjustierten \\(p\\)-Werte wieder.\n\np.adjust(p, \"BH\")[1:10] %>% round(3)\n\n [1] 0.000 0.000 0.001 0.001 0.003 0.004 0.009 0.016 0.018 0.022\n\n\nMit der Bonferroni Adjustierung und der FDR Adjustierung haben wir zwei sehr gute Möglichkeiten für das multiple Testen zu korrigieren. Die FDR Korrektur wird häufiger in der Genetik bzw. Bioinformatik eingesetzt. In dem Kontext kommt auch die recht ähnliche Benjamini & Yekutieli (abk. BY) Adjustierung vor. Die Benjamini & Yekutieli unterscheidet sich aber nur in Nuancen unter bestimmten Rahmenbedingungen von der FDR Adjustierung. Wir lassen den statistischen Engel hier mal am Straßenrand stehen und wenden uns der letzten Adjustierung zu.\n\n20.3.3 Dunn-Sidak\nDie Sidak Korrektur berechnet das lokale Signifikanzniveau \\(\\alpha_{local}\\) auf folgende Art und Weise. Wir haben ein globales \\(\\alpha\\) von 5%. Das \\(\\alpha_{local}\\) berechnen wir indem wir die Anzahl der Vergleiche \\(m\\) wie folgt miteinander verbinden.\n\\[\n\\alpha_{local} = 1 - (1 - \\alpha)^{\\tfrac{1}{m}}\n\\]\nDie Sidak Korrektur ist weniger streng als die Bonferroni-Korrektur, aber das auch nur sehr geringfügig. Zum Beispiel beträgt für \\(\\alpha = 0.05\\) und \\(m = 10\\) das Bonferroni-bereinigte lokale Signifikanzniveau \\(\\alpha_{local} = 0.005\\) und das nach Sidak Korrektur ungefähr \\(0.005116\\). Das ist jetzt auch kein so großer Unterschied. Wir finden die Sidak Korrektur dann im R Paket emmeans für die Adjustierung bei den multiplen Vergleichen wieder."
  },
  {
    "objectID": "stat-tests-theorie.html#referenzen",
    "href": "stat-tests-theorie.html#referenzen",
    "title": "20  Die Testtheorie",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nGigerenzer, Gerd, Stefan Krauss, und Oliver Vitouch. 2004. „The null ritual“. The Sage handbook of quantitative methodology for the social sciences, 391–408.\n\n\nWasserstein, Ronald L, Allen L Schirm, und Nicole A Lazar. 2019. „Moving to a world beyond ‚p< 0.05‘“. The American Statistician. Taylor & Francis."
  },
  {
    "objectID": "experimental-design-samplesize.html",
    "href": "experimental-design-samplesize.html",
    "title": "\n48  Fallzahlplanung\n",
    "section": "",
    "text": "Version vom November 16, 2022 um 13:13:01\nIn diesem Kapitel wollen wir uns mit der Fallzahlplanung beschäftigen. Eigentlich stimmt das Wort Planung überhaupt nicht. Wir machen hier eine Fallzahlabschätzung. Es geht hier darum, anhand von Werten aus der Literatur für dein eigenen Experiment die nötige Anzahl an Beobachtungen abzuschätzen. Nun ist es so, dass die Berechnung der benötigten Fallzahl in klinischen Studien oder aber Tierversuchsvorhaben vor dem Beginn des Versuchs durchgeführt werden muss. Es gibt in diesem Zusammenhang das Tierversuchsvorhaben im Falle von Tieren und den Ethikantrag im einer klinischen Studie. Beide Anträge werden faktisch bei einer Behörde gestellt und haben einen regulativen Charakter. Wir machen hier aber kein Jura. Deshalb hat dieses Kapitel auch keinen beratenden Charakter.\nWenn du an einer Institution arbeitest oder forscht, die einen Ethikantrag stellen oder einen Tierversuchsantrag einreichen will, wende dich an deine Vertrauensperson. Es muss jemanden bei dir geben, der für solche Anträge zuständig ist. Dieses Kapitel und ich sind es ausdrücklich nicht.\nEine weitere Besonderheit ist noch eine Studie, die Übrprüfen soll, ob der versuch übrhaupt machbar ist. Der versuch soll die Machbarkeit (eng. feasibility) testen. In diesem Fall brauchen wir keine Fallzahlberechnung. Wir testen ja eine sehr kleine Fallzahl, weil es uns um die technische Umsetzbarkeit des Versuches geht. Erst wenn wir wissen, dass wir den Versuch auch technisch durchführen können, machen wir dann eine statistsiche Fallzahlplanung."
  },
  {
    "objectID": "experimental-design-samplesize.html#theoretischer-hintergrund",
    "href": "experimental-design-samplesize.html#theoretischer-hintergrund",
    "title": "\n48  Fallzahlplanung\n",
    "section": "\n48.1 Theoretischer Hintergrund",
    "text": "48.1 Theoretischer Hintergrund\nManchmal hat man das Gefühl, dass Fallzahlplanung nur ein wildes Gerate ist. Das ist aus der Perspektive eines biologischen Fachlaien auch der Fall. Ich kenne mich sehr wenig in der vielen biologischen Feldern aus. Daher weiß ich wenig darüber was ein großer Effekt ist oder welchen Effekt du überhaupt in deinem Kartoffelexperiment erwarten kannst. Auch ist mir unklar was typische Mittelwertsunterschiede bei Wasserlinsen sind. Du musst sowas aber wissen, es ist ja schließlich dein Experiment. Wenn du also eine Fallzahlplanung durchführen willst, dann heißt es zuerst einmal Literatur wälzen oder mit den Fachkollegen sprechen.\nWir kennen ja schon die Formel für den t-Test. Der t-Test vergleicht die Mittelwerte von zwei normalverteilten Outcomes und gewichtet diesen Mittelwertsunterschied bei der Standardabweichung. Da wir in der Formel des t-Tests auch die Fallzahl inkludiert haben, können wir die Formel nach der Fallzahl umstellen.\n\\[\nT = \\cfrac{\\Delta}{s_p \\cdot \\sqrt{\\cfrac{2}{n_g}}}\n\\]\nDabei nutzen wir die Teststatistik etwas anders. Wir zerlegen die Teststatistik \\(T\\) für in den Wert für den \\(\\alpha\\)-Fehler und den \\(\\beta\\)-Fehler. Damit können wir auch die Power \\(1-\\beta\\) mit in unserer Formel berücksichtigen.\n\\[\nn_g = \\cfrac{2\\cdot(T_{\\alpha = 5\\%} + T_{\\beta = 20\\%})^2}{\\left(\\cfrac{\\Delta}{s_p}\\right)^2}\n\\]\nDabei nutzen wir für \\(T_{\\alpha = 5\\%} = 1.96\\) und \\(T_{\\beta = 20\\%} = 0.84\\) und vereinfachen damit die Formel ziemlich. Eigentlich nutzen wir diese Formel dann in der der Klausur oder aber um wirklich mal eben schnell zu schauen, was wir für eine Fallzahl erwarten.\nJetzt könntest du meinen, dass wir jetzt mit verschiedenen Powerleveln spielen könnten. Aber das ist leider nicht der Fall. Wir sind eigentlich zimelich auf 80% festgelegt. Da gibt es im Rahmen eines Antrags keinen Spielraum. Wir nehmen immer eine Power von 80% an.\n\n\n\n\n\n\nEinseitig oder zweiseitig im Spiegel der Regulierungsbehörden\n\n\n\nIn den allgemeinen Methoden des IQWiG, einer Regulierungsbehörde für klinische Studien, wird grundsätzlich das zweiseitige Testen empfohlen. Wenn einseitig getestet werden sollte, so soll das \\(\\alpha\\)-Niveau halbiert werden. Was wiederum das gleiche wäre wie zweiseitiges Testen - nur mit mehr Arbeit.\nZur besseren Vergleichbarkeit mit 2-seitigen statistischen Verfahren wird in einigen Guidelines für klinische Studien eine Halbierung des üblichen Signifikanzniveaus von 5 % auf 2,5 % gefordert. – Allgemeine Methoden Version 6.1 vom 24.01.2022, p. 180\nFazit des Dokumentes ist dann aber, dass wir immmer zu einem Signifikanzniveau \\(\\alpha\\) von 5% und einer Power von 80% testen."
  },
  {
    "objectID": "experimental-design-samplesize.html#tierversuchsantrag",
    "href": "experimental-design-samplesize.html#tierversuchsantrag",
    "title": "\n48  Fallzahlplanung\n",
    "section": "\n48.2 Tierversuchsantrag",
    "text": "48.2 Tierversuchsantrag\nWenn du einen Tierversuch durchführen willst, dann bist du natürlich hier falsch. Ich kann dir bei dem Ausfüllen von Dokumenten nicht helfen. Was ich aber kann, ist dir einen Überblick über die Inhalte zu geben, so dass du dann nochmal informiert an anderer Stelle Fragen stellen kanst. Schaue gerne einmal mein Video auf YouTube mit dem Kontext zum Tierversuchsvorhaben. Eine wunderbare Übersicht über den Tierversuchsantrag liefert auch Piper u. a. (2022).\n\n\n\n\n\n\nEinführung in den Kontext zu Tierversuchsvorhaben per Video\n\n\n\nDu findest auf YouTube Kontext zu Tierversuchsvorhaben als Video Reihe. Es handelt sich hierbei um ein reines Lehrvideo mit keinem beratenden Anspruch.\n\n\nIn dem Video habe ich dann alles anhand des Tierversuchsvorhaben am LaGeSo in Berlin besprochen. Das hatte den Grund, dass ich zur Zeit des Videos an der Charité beschäftigt war. Da bei einem Tierversuchsantrag jeweils die Bundesländer zuständig sind, musst du bei deiner jeweiligen Ladesbehörde einmal schauen. In Niedersachsen musst du dir die Wenseite zu Tierversuche vom Laves anschauen. Hier findest du dann andere Dokumente und Ausfüllhilfen. Wenn man als Wissenschaftler viel wechselt, wird man leicht wirr."
  },
  {
    "objectID": "experimental-design-samplesize.html#ethikantrag",
    "href": "experimental-design-samplesize.html#ethikantrag",
    "title": "\n48  Fallzahlplanung\n",
    "section": "\n48.3 Ethikantrag",
    "text": "48.3 Ethikantrag\nEben hatten wir uns kurz den Antrag für ein Tierversuchsvorhaben angeschaut. Richtig kompliziert wird es, wenn wir nicht mit Tieren arebiten sondern Versuche am Menschen durchführen. Ein versuch am Menschen beinhaltet schon das Ausfüllen eines Fragebogens! Daher kanns du auch schnell in die Situtaton kommen, dass es eventuell eine ethische Komplikation gibt. Ich habe die Inhalte im Kontext einer klinischen Studie einmal in einem YouTube Video dargestellt und allgemein eingeordnet.\n\n\n\n\n\n\nEinführung in den Kontext zum Ethikantrag per Video\n\n\n\nDu findest auf YouTube Kontext zum Ethikantrag als Video Reihe. Es handelt sich hierbei um ein reines Lehrvideo mit keinem beratenden Anspruch.\n\n\nDa ich in meiner Lehre die klinischen Studie nur am Horizont sehe, gibt es hir auch keine weiteren Links zu dem Thema. In dem Video siehst du noch ein paar öffentliche Quellen. Da es sich aber bei einem Ethikantrag meist um einen internen Prozess einer Universitätsklinik handelt, sind die (aktuellen) Dokumente meist nicht öffentlich zugänglich. Im Zweifel bitte an die zuständigen Gremien an deiner Institution wenden."
  },
  {
    "objectID": "experimental-design-samplesize.html#genutzte-r-pakete-für-das-kapitel",
    "href": "experimental-design-samplesize.html#genutzte-r-pakete-für-das-kapitel",
    "title": "\n48  Fallzahlplanung\n",
    "section": "\n48.4 Genutzte R Pakete für das Kapitel",
    "text": "48.4 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, pwr, readxl, see,\n               effectsize, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "experimental-design-samplesize.html#mittelwertsvergleich-für-zwei-gruppen-in-r",
    "href": "experimental-design-samplesize.html#mittelwertsvergleich-für-zwei-gruppen-in-r",
    "title": "\n48  Fallzahlplanung\n",
    "section": "\n48.5 Mittelwertsvergleich für zwei Gruppen in R",
    "text": "48.5 Mittelwertsvergleich für zwei Gruppen in R\nDa wir ja nur die Formel des t-Tests für die Fallzahlberechnung haben, können wir auch immer nur die Fallzahl für den Vergleich zwischen zwei Gruppen rechnen. Das ist immer erstmal wieder ungewohnt. Aber wir machen das jetzt erstmal nur für zwei Gruppen. Später schauen wir uns an, ws passiert, wenn wir mehr Gruppen miteinander vergleichen wollen. Prinzipiell ist der Kern aber immer ein Zweigruppenvergleich, den wir dann etwas anders Aufbauen.\nWenn du für einen Wilcoxon-Test oder einen anderen nicht-parametrischen Test die Fallzahlplanung machen willst, rechne bitte einen t-Test und addiere \\(+15\\%\\) an Fallzahl drauf.\nFür die Berechnung der Fallzahl wollen wir das R paket pwr nutzen. Wir brauchen in diesem Kapitel nur drei Funktion aus dem Paket, aber es gibt auch weit aus mehr. Im Zweifel einfach einmal die Hilfeseite aufrufen und schauen was es dort noch so gibt.\nWir können mit der Funktion pwr.t.test() die Fallzahl für die Effektstärke nach Cohen’s \\(d\\) berechnen. Mehr über Cohen’s \\(d\\) kannst du im Kapitel 21 erfahren. Wir nutzen hier eine relativ harte Abschätzung. Aber hier wird sowieso alles abgeschätzt, da kommt es jetzt auf künstliche Genauigkeit nicht mehr an. Wir berechnen also Cohen’s \\(d\\) vereinfacht für die Fallzahlberechnung wie folgt.\n\\[\nd = \\cfrac{\\Delta}{s_{\\Delta}}\n\\]\nmit\n\n\n\\(\\Delta\\) als den zu erwartenden Mittelwertsunterschied zwischen den beiden Gruppen. Wir haben den Wert aus der Literatur entnommen.\n\n\\(s_{\\Delta}\\) als der Standardabweichung des Mittelwertsunterschieds. Wir können hier als Vereinfachung mit der Spannweite der Daten mit \\(\\frac{range}{4}\\) als Schätzer für die Standardabweichung rechnen. Ebenfalls haben wir die Werte aus einer Literaturquelle.\n\nEs gäbe auch die Möglichkeit über die Funktion cohen.ES() die Effekte für verschiedene statistische Tests sich wiedergeben zu lassen, wenn wir definieren, wie stark der Effekt zwischen den Gruppen sein soll. Es steht zur Auswahl small, medium und large. Wir erkennen, dass ist nicht gerade viel Abstufung.\n\ncohen.ES(test = \"t\", size=\"medium\") %>% \n  pluck(\"effect.size\")\n\n[1] 0.5\n\n\nDie Fallzahlberechnung geht recht einfach. Wir setzen die Option n = auf NULL, so dass uns die Funktion diese Option berechnet. Wir kriegen also die Fallzahl gegeben von dem Signifikanzniveau, der Power und der Effektstärke wieder. Dann geben wir noch an, dass wir zweiseitig testen. Also eigentlich alles fix, da können wir selber zwar was ändern, aber am Ende wird meist nur die Standardwerte von Dritten akzeptiert.\n\nres_ttest <- pwr.t.test(n = NULL,\n                        sig.level = 0.05, \n                        type = \"two.sample\", \n                        alternative = \"two.sided\", \n                        power = 0.80, \n                        d = 0.8)\nres_ttest\n\n\n     Two-sample t test power calculation \n\n              n = 25.52458\n              d = 0.8\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nBitte immer Aufrunden. Wir brauchen also \\(n_1 = n_2 = 26\\) Beobachtungen je Gruppe, so dass wir für \\(32\\) beobachtungen unseren Versuch planen können. In Abbildung 48.1 sehen die Power abhängig von der verwendeten Fallzahl. Wir sehen, dass wir mit mehr Fallzahl eine höhere Power erhalten würden, aber wir schon sehr nah an der Sättigung sind.\n\nplot(res_ttest) +\n  theme_minimal(base_size = 14) +\n  labs(title = 'Optimierte Fallzahl für den Zweistichproben t-Test.'))\n\n\n\nAbbildung 48.1— Optimierte Fallzahl für den Zweistichproben t-Test."
  },
  {
    "objectID": "experimental-design-samplesize.html#anteilsvergleich-für-zwei-gruppen-in-r",
    "href": "experimental-design-samplesize.html#anteilsvergleich-für-zwei-gruppen-in-r",
    "title": "\n48  Fallzahlplanung\n",
    "section": "\n48.6 Anteilsvergleich für zwei Gruppen in R",
    "text": "48.6 Anteilsvergleich für zwei Gruppen in R\nWann benötigen wir Anteile? Häufig nutzen wir Anteile, wenn wir zum Beispiel infizierte Ferkel untr zwei Behandlungen untersuchen wollen. Wie viel Prozent der Ferkel in der einen Gruppe werden infiziert sein und wieviel Ferkel in der anderen Gruppe. Daher haben wir ein Medikament und wollen schauen, ob sich die Anzahl an infizierten Ferkeln reduziert. Wir nehmen aber nicht die Anzahl als Wert sondern die relative Angabe. Im folgenden Beispiel haben wir \\(95\\%\\) infizierte Ferkel in der einen Gruppe und \\(80\\%\\) infizierte Ferkel in der anderen Gruppe. Wie viel Fallzahl brauchen wir nun, um diesen Untrschied nachzuweisen. Achtung, wir rechnen hier wirklich mit den relativen Zahlen und nicht mit der Differenz. Ist leider so.\nWir können die Funktion ES.h() benutzen um den Effekt zwischen zwei Wahrscheinlichkeiten zu berechnen. Wir geben einfach die beiden Wahrscheinlichkeiten für die zu erwartende Häufigkeit an infizierten Ferkeln ein. Dann berechnen wir den Effekt \\(h\\) und nutzen diesen Wert dann für die Fallzahlberechnung.\n\nES.h(p1 = 0.95, p2 = 0.80) %>% \n  round(2)\n\n[1] 0.48\n\n\nHier kommt es dann auch nicht wieder auf die letzte Prozentzahl an. Wir immer kann man hhier spielen. Aber du hast ja deine Zahlen aus der Literatur und passt diese Zahlen dann deinem Setting und anhand deinem biolologischen Wissen an. Es ist immer eine Gradwanderung, wie genau die Zahlen nun seien sollen. Insbesondere, wenn es dann doch nicht so viel Literatur gibt. Wir setzen die Option n = auf NULL, so dass uns die Funktion diese Option berechnet. Wir kriegen also die Fallzahl gegeben von dem Signifikanzniveau, der Power und der Effektstärke wieder. Dann geben wir noch an, dass wir zweiseitig testen.\n\nres_prop <- pwr.p.test(h = 0.48,\n                       n = NULL,\n                       sig.level = 0.05,\n                       power = 0.80,\n                       alternative = \"two.sided\")\nres_prop\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.48\n              n = 34.06623\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nAm Ende erhalten wir eine Fallzahl von \\(n_1 = n_2 = 35\\) Beobachtungen aus der Fallzahlberechnung. Wir wissen also, wie viele Ferkel wir untersuchen müssten um einen Unterschied von \\(95\\%\\) zu \\(80\\%\\) signifikant nachweisen zu können. In Abbildung 48.2 sehen wir nochml die Sättigungskurve für die Power für verschiedene Fallzahlen. Mit unserer berechneten Fallzahl von \\(n=35\\) pro Gruppe sind wir schon recht nah an der Sättigung der Funktion. Wir können damit die Fallzahl beibehalten und uns übrlegen, ob wir überhaupt das Geld und die Ressourcen haben um den Versuch mit dieser Anzahl an Ferkeln durchzuführen.\n\nplot(res_prop) +\n  theme_minimal(base_size = 14) +\n  labs(title = 'Optimierte Fallzahl für zwei Anteile.'))\n\n\n\nAbbildung 48.2— Optimierte Fallzahl für zwei Anteile."
  },
  {
    "objectID": "experimental-design-samplesize.html#anteil-der-erklärten-varianz-in-r",
    "href": "experimental-design-samplesize.html#anteil-der-erklärten-varianz-in-r",
    "title": "\n48  Fallzahlplanung\n",
    "section": "\n48.7 Anteil der erklärten Varianz in R",
    "text": "48.7 Anteil der erklärten Varianz in R\nNun können wir die Fallzahlplanung auch für eine einfaktorielle ANOVA durchführen. Das ist unsere Basis. Wir würden dann überlegen, wie sich dann die Fallzahl mit weiteren Faktoren ändern würde. Auch hier ein Wort der Warnung. Es gibt häufig so starke Randbedingungen, wie Kosten oder Fläche, dass die Berechnung der Fallzahl absolet wird. Wenn du drei Blöcke hat, dann hast du drei Blöcke. nutze die Blöcke dann auch. Wenn du freeie Wahl hättest und viel, viel Geld, dan kann man sicherlich besser die Fallzahl abschätzen und nutzen. Fallzahlberechnung nur so zum Spaß hat dann ja auch wenig Sinn. Also hier nochmal unser Modell was wir uns mit einer einfaktoriellen ANOVA anschauen.\n\\[\ny \\sim f_1\n\\]\nWir immer brauchen wir auch einen Effekt. In dem Fall der ANOVA ist der Effekt Cohen’s \\(f\\). Wir berechnen Cohen’s \\(f\\) wie folgt aus dem \\(\\eta^2\\). Wir können an dieser Stelle schon die Werte für \\(\\eta^2\\) einsetzen und \\(f\\) berechnen.\n\\[\nf = \\sqrt{\\cfrac{\\eta^2}{1- \\eta^2}}\n\\]\nDu erinnerst dich aus der ANOVA, das \\(\\eta^2\\) beschreibt den Anteil an erklärter Varianz durch den Faktor in der ANOVA. Damit ist \\(\\eta^2\\) wie folgt definiert.\n\\[\n\\eta^2 = \\cfrac{SS_{treat}}{SS_{total}}\n\\]\nDas hilft uns nur so begrenzt weiter. Am besten überlegst du dir, wieviel Varianz wohl die Behandlung erklären kann. Damit hast du dann dein \\(\\eta^2\\). Wenn deine Behandlung vermutlich ca. 70% der Varianz in deinen Daten erklären und somit im Ourtcome erklären kann, dann setzt du \\(\\eta^2 = 0.7\\). Dann berechnest du dein \\(f = \\sqrt{\\tfrac{0.7}{0.3}} = 1.53\\) und hast damit einen sehr großen Effekt. Was dir auch die Funktion interpret_eta_squared() aus dem R Paket effectsize mitteilt.\n\ninterpret_eta_squared(1.53)\n\n[1] \"large\"\n(Rules: field2013)\n\n\nWir können dann Cohen’s \\(f\\) in die Funktion pwr.anova.test() stecken und die Fallzahl pro Gruppe ausrechnen. Wir haben jetzt mal einen Faktor mit drei Behandlunsgleveln angenommen, deshalb ist auch \\(k = 3\\) in der Funktion.\n\nres_anova <- pwr.anova.test(k = 3,\n                            f = 1.5,\n                            n = NULL,\n                            sig.level = 0.05,\n                            power = 0.80)\nres_anova\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 2.713068\n              f = 1.5\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nTja, mit so einem großen Effekt brauchen wir wirklich wenig Wiederholungen um mindestens einen Unterschied nachzuweisen. Stimmt, wir haben natürlich auch nur global über alle Gruppen geschaut. Ich finde die Fallzahlplanung für eine ANOVA relativ eingeschränkt, aber so ist das eben auch bei der Fallzahlplanung. Meistens ist das was möglich ist sehr eingeschränkt."
  },
  {
    "objectID": "experimental-design-samplesize.html#mehr-als-zwei-gruppen",
    "href": "experimental-design-samplesize.html#mehr-als-zwei-gruppen",
    "title": "\n48  Fallzahlplanung\n",
    "section": "\n48.8 Mehr als zwei Gruppen",
    "text": "48.8 Mehr als zwei Gruppen\nWas passiert, wenn wir mehr als zwei Gruppen vorliegen haben? Was eigentlich immer der Fall ist. Also wir haben nicht nur zwei Düngestufen oder zwei Sorten Blumenkohl, die wir miteinander vergleichen wollen, sondern wir haben zehn oer mehr. Wir bauen jetzt nicht so ein großes Beispiel sondern nehmen einmal die Sprungweiten von den Hunde-, Katzen- und Fuchsflöhen.\n\nfleas_tbl <- read_excel(\"data/flea_dog_cat_fox.xlsx\") %>% \n  mutate(animal = as_factor(animal))\n\nIn Abbildung 48.3 sehen wir nochmal die Verteilung der Sprungweiten für die drei Tierarten als Boxplots dargestellt.\n\nggplot(fleas_tbl, aes(animal, jump_length, fill = animal)) + \n  theme_bw() +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito()\n\n\n\nAbbildung 48.3— Boxplot der Sprungweiten für die Hunde-, Katzen- und Fuchsflöhe.\n\n\n\n\nWenn wir jetzt für ein neues Experiment die Fallzahl planen wollen würden, dann brauchen wir die Mittelwerte und die Stanardabweichung der Sprungweiten. Wir haben ja hier unser Pilotexperiment vorliegen, also können wir auch hier die Mittelwerte und die Standardabweichung getrennt für die Tierarten berechnen.\n\nfleas_tbl %>% \n  group_by(animal) %>% \n  summarise(mean = mean(jump_length),\n            sd = sd(jump_length)) %>% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 3 x 3\n  animal  mean    sd\n  <fct>  <dbl> <dbl>\n1 dog     8.13  2.14\n2 cat     4.74  1.9 \n3 fox     9.16  1.1 \n\n\nNatürlich sind wir nicht an den Mittelwerten sondern an den Unterschieden interessiert. Daher rechnen wir nochmal in Tabelle 48.1 alle Mittelwertsdifferenzen aus.\n\n\nTabelle 48.1— Mittelwertsdifferenzen für alle paarweisen Vergleiche.\n\n\n\ncat\nfox\n\n\ndog\n\\(8.13 - 4.74 = 3.39\\)\n\\(8.13 - 9.16 = -1.03\\)\n\n\ncat\n\n\\(4.74 - 9.16 = -4.42\\)\n\n\n\n\nWenn wir die kleinste Differenz in den Mittelwerten mit einer Power von 80% nachweisen können, dann können wir auch alle anderen größeren Mittelwertsdifferenzen mit einer Power größer als 80% nachweisen. Daher brauchen wir die Fallzahlplanung nur für den kleinsten Mittelwertsunterschied durchführen. Wir berechnen noch Cohen’s d mit \\(d = \\tfrac{1.03}{(2.14 + 1.1)/2} \\approx 0.16\\). Ganz schön kleiner Wert, wie uns die Funktion interpret_cohens_d() aus dem R Paket effectsize mitteilt.\n\ninterpret_cohens_d(0.15)\n\n[1] \"very small\"\n(Rules: cohen1988)\n\n\nWeil wir es können berechnen wir auch die Fallzahl und kriegen einen kleinen Schreck. Denn mit einem so kleinen Effekt brauchen wir wirklich viele Flöhe.\n\nres_flea <- pwr.t.test(n = NULL,\n                       sig.level = 0.05, \n                       type = \"two.sample\", \n                       alternative = \"two.sided\", \n                       power = 0.80, \n                       d = 0.16)\nres_flea\n\n\n     Two-sample t test power calculation \n\n              n = 614.1541\n              d = 0.16\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nMit am Ende über \\(2 \\cdot 615 = 1230\\) Flöhen für den Vergleich von Hunde- und Fuchsflöhen sind wir wirklich weit, weit oben was die Fallzahl angeht. Da hilft es dann auch nicht viel, dass wir mit zusätzlich \\(615\\) Katzenflöhen dann auch die anderen paaweisen Vergleichw als signifikant finden würden. Denn Cohen’s d für den Vergleich von den Hunde- und Katrzenflöhen wäre \\(d = \\tfrac{3.39}{(2.14 + 1.9)/2} \\approx 0.42\\). Damit würden wir dann eine Power von \\(0.99999997\\) erhalten. Wir können die Power berechnen indem wir das Feld Power mit NULL belegen und die Fallzahl von \\(n = 615\\) eintragen.\n\nres_flea <- pwr.t.test(n = 615,\n                       sig.level = 0.05, \n                       type = \"two.sample\", \n                       alternative = \"two.sided\", \n                       power = NULL, \n                       d = 0.42)\nres_flea\n\n\n     Two-sample t test power calculation \n\n              n = 615\n              d = 0.42\n      sig.level = 0.05\n          power = 1\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "experimental-design-samplesize.html#gpower-als-alternative",
    "href": "experimental-design-samplesize.html#gpower-als-alternative",
    "title": "\n48  Fallzahlplanung\n",
    "section": "\n48.9 G*Power als Alternative",
    "text": "48.9 G*Power als Alternative\n\n\n\n\n\n\nEinführung in G*Power als Alternative per Video\n\n\n\nDu findest auf YouTube G*Power als Alternative als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nAls Alternative zu R wollen wir uns noch das Standalone Programmm G*Power | Statistical Power Analyses von der Heinrich-Heine-Universität Düsseldorf anschauen. Die Software ist nicht mehr die neuste, wird aber immer noch gewartet und an die aktuellen Versionen von Mac und Windows angepasst. Manchmal ist dann Point und Klick dann doch eine Alternative, wenn man sich ausprobieren will.\nIch werde also im Folgenden ein paar Screenshots zeigen, wie du mi G*Power dir auch die Fallzahl für Mittelwertsunterschiede und Anteilesunterschiede berechnen kannst. Allgemein ist es so das G*Power immer einseitig (eng. Tail(s) one) und zu einer Power von 95% testet. Daher müssen wir immr schauen, dass diese Werte stimmen. Insbeosndere, wenn du viel rumprobierst können auch die Werte mal wieder zurückspringen. Also bitte darauf achten.\nIn Abbildung 48.4 sehen wir die Berechnung der Fallzahl für den t-Test für einen Vergleich zweier Gruppen. Wir müssen darauf achten, dass wir die Testfamilie richtig wählen und dann den korrekten Test auswählen. Du siehst bei der eigenen Verwendung dann, dass es hier eine große Auswahl gibt. Wir nehmen aber den Standard von zwei unabhängigen Gruppen. Wir erhalten dann eine Fallzahl von \\(n = 54\\) für unseren Versuch. Das schöne an G*Power ist, dass du relativ einfach und schnell mit den Zahlen spielen kannst. Das Speichern ist schwerer, so dass ich immer einen Screenshot empfehle. Man vergisst schnell, was alles in en Feldern stand.\n\n\n\nAbbildung 48.4— Die Berechnung der Fallzahl für einen t-Test für zwei Gruppen mit einem Mittelwert +/- Standardabweichung von \\(14 \\pm 2\\) in der einen Gruppe und \\(16 \\pm 3\\) in der anderen Gruppe. Es ergibt sich ein Cohens’ d von \\(0.78\\). Wir müssen darauf achten zweiseitig und zu einer Power von 80% zu testen.\n\n\n\nIn Abbildung 48.5 sehen wir die Berechnung der Fallzahl für zwei Anteile. Wir haben zwei Gruppen vorliegen und in der ersten Gruppe haben wir 60% infizierte Ferkel, In der anderen Gruppe erwarten wir dann 90% infiztierte Ferkel. Um den Unterschied von 30% nachzuweisen, brauchen wir mindestens 180 Ferkel. Leider ist es so, dass wir den Test für Anteile unter dem Reiter Exact finden. Das muss man eben wisen. Achte wieder auf die Power und das zu zweiseitig testen willst.\n\n\n\nAbbildung 48.5— Die Berechung der Fallzahl für einen Anteil in zwei Gruppen von \\(0.6\\) in der einen Gruppe und \\(0.8\\) in der anderen Gruppe. Wir müssen darauf achten zweiseitig und zu eienr Power von 80% zu testen."
  },
  {
    "objectID": "experimental-design-samplesize.html#referenzen",
    "href": "experimental-design-samplesize.html#referenzen",
    "title": "\n48  Fallzahlplanung\n",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nPiper, Sophie K., Dario Zocholl, Ulf Toelch, Robert Roehle, Andrea Stroux, Johanna Hoessler, Anne Zinke, und Frank Konietschke. 2022. „Statistical review of animal trials—A guideline“. Biometrical Journal. https://doi.org/10.1002/bimj.202200061."
  },
  {
    "objectID": "stat-tests-ttest.html",
    "href": "stat-tests-ttest.html",
    "title": "22  Der t-Test",
    "section": "",
    "text": "Version vom November 16, 2022 um 13:12:28\nDer t-Test ist der bedeutende Test, wenn es um das Verständnis der Algorithmen und Konzepte in der Statistik geht. Wir haben den t-Test schon genutzt um die Idee des statistischen Testens zu verstehen und wir werdend den t-Test auch im statistischen Modellieren wiedertreffen. Dort finden wir aber die Teststatistik des t-Tests. Wir werden dort nicht direkt den t-Test rechnen sondern das Konzept des t-Tests wieder nutzen.\nWas macht also der t-Test? Der t-Test vergleicht die Mittelwerte zweier Gruppen miteinander. Das heißt wir haben zwei Gruppen, wie Hunde und Katzen, und wollen nun wissen wie sich die Sprungweiten der Hundeflöhe im Mittel von den Katzenflöhen unterscheiden. In R hätten wir damit einen Faktor mit zwei Leveln vorliegen. Darüber hinaus nimmt der t-Test implizit an, das unser Outcome \\(y\\) normalverteilt ist. Die Varianzen können in beiden Gruppen gleich sein, dann sprechen wir von homogenen Varianzen oder Varianzhomogenität. Wir können den t-Test aber auch mit ungleichen Varianzen in beiden Gruppen rechnen, dann sprechen wir von heterogenen Varianzen oder eben Varianzheterogenität."
  },
  {
    "objectID": "stat-tests-ttest.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-ttest.html#genutzte-r-pakete-für-das-kapitel",
    "title": "22  Der t-Test",
    "section": "\n22.1 Genutzte R Pakete für das Kapitel",
    "text": "22.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, readxl)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-ttest.html#daten-und-modell-für-den-t-test",
    "href": "stat-tests-ttest.html#daten-und-modell-für-den-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.2 Daten und Modell für den t-Test",
    "text": "22.2 Daten und Modell für den t-Test\nWichtig ist, dass wir schon jetzt die Modellschreibweise lernen um die Daten später richtig in R nutzen zu können. Wir werden die Modelschreibweise immer wieder sehen. Die Modellschreibweise ist die Art und Weise wie wir in R eine Abhängigkeit beschreiben. Wir brauchen dieses Konzept in den folgenden Kapiteln. In R heißt y ~ x auch formula, also eine Formelschreibweise.\n\n\nAbbildung 22.1— Modellschreibweise \\(y\\) hängt ab von \\(x\\). Das \\(y\\) repräsentiert eine Spalte im Datensatz und das \\(x\\) repräsentiert ebenso eine Spalte im Datensatz. Wir brauchen also zwei Variablen \\(y\\) und \\(x\\), die natürlich nicht so heißen müssen.\n\n\nEtwas unbefriedigend, dass der t-Test nur zwei Gruppen miteinander Vergleichen kann. Mehr Gruppen gehen in der ANOVA im Kapitel 23\nWas brauchen wir damit wir den t-Test in R rechnen können? Später in der Anwendung nutzt du ja nur die Implementierung des t-Tests in R. Wir rechnen ja unsere Auswertung nicht per Hand. In R brauchen wir für den t-Test eine Spalte \\(y\\) mit kontinuierlichen Zahlen und einer Spalte \\(x\\) in dem wir einen Faktor mit zwei Leveln finden. Jedes Level steht dann für eine der beiden Gruppen. Das war es schon. Schauen wir uns nochmal den Datensatz flea_dog_cat.xlsx in Tabelle 22.1 an und überlegen, wie wir das realisieren können.\n\n\n\n\nTabelle 22.1— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Boniturnote sowie der Infektionsstatus von Hunden und Katzen.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\ncat\n3.2\n12\n7\n1\n\n\ncat\n2.2\n13\n5\n0\n\n\ncat\n5.4\n11\n7\n0\n\n\ncat\n4.1\n12\n6\n0\n\n\ncat\n4.3\n16\n6\n1\n\n\ncat\n7.9\n9\n6\n0\n\n\ncat\n6.1\n7\n5\n0\n\n\n\n\n\n\nIn Abbildung 22.2 sehen wir einmal den Zusammenhang zwischen den Schreibweise \\(y \\sim x\\) und den beiden Variablen jump_length als \\(y\\) und animal als \\(x\\) aus dem Datensatz flea_dog_cat.xlsx. Wir haben also die formula Schreibweise in R als jump_length ~ animal.\n\n\nAbbildung 22.2— Modellschreibweise bzw. formula-Schreibweise in R. Die Variable \\(y\\) hängt ab von \\(x\\) am Beispiel des Datensatzes flea_dog_cat.xlsx mit den beiden Variablen jump_length als \\(y\\) und animal als \\(x\\).\n\n\nWir benötigen für den t-Test ein normalverteiltes \\(y\\) und einen Faktor mit zwei Leveln als \\(x\\). Wir nehmen daher mit select()die Spalte jump_length und animal aus dem Datensatz flea_dog_cat.xlsx. Wichtig ist, dass wir die Spalte animal mit der Funktion as_factor() in einen Faktor umwandeln. Anschließend speichern wir die Auswahl in dem Objekt data_tbl.\n\ndata_tbl <- read_excel(\"data/flea_dog_cat.xlsx\") %>% \n  mutate(animal = as_factor(animal)) %>% \n  select(animal, jump_length)\n\ndata_tbl\n\n# A tibble: 14 x 2\n   animal jump_length\n   <fct>        <dbl>\n 1 dog            5.7\n 2 dog            8.9\n 3 dog           11.8\n 4 dog            8.2\n 5 dog            5.6\n 6 dog            9.1\n 7 dog            7.6\n 8 cat            3.2\n 9 cat            2.2\n10 cat            5.4\n11 cat            4.1\n12 cat            4.3\n13 cat            7.9\n14 cat            6.1\n\n\nWir haben jetzt die Daten richtig vorbereiten und können uns nun mit dem t-Test beschäftigen. Bevor wir den t-Test jedoch rechnen können, müssen wir uns nochmal überlegen, was der t-Test eigentlich testet und uns die Daten einmal visualisieren."
  },
  {
    "objectID": "stat-tests-ttest.html#visualiserung-der-daten",
    "href": "stat-tests-ttest.html#visualiserung-der-daten",
    "title": "22  Der t-Test",
    "section": "\n22.3 Visualiserung der Daten",
    "text": "22.3 Visualiserung der Daten\nBevor wir einen statistischen Test rechnen, wollen wir uns erstmal die Daten, die dem Test zugrundeliegen, visualisieren. Wir schauen uns in Abbildung 22.3 einmal den Boxplot für die Sprungweiten getrennt nach Hund und Katze an.\nWir sehen, dass sich die Boxen nicht überschneiden, ein Indiz für einen signifikanten Unterschied zwischen den beiden Gruppen. Im Weiteren liegt der Median in etwa in der Mitte der beiden Boxen. Die Whisker sind ungefähr gleich bei Hunden und Katzen. Ebenso sehen wir bei beiden Gruppen keine Ausreißer.\nWir schließen daher nach der Betrachtung der Boxplots auf Folgendes:\n\nDie Sprungweite ist für beide Gruppen ist annäherend bzw. approximativ normalverteilt.\nDie Standardabweichungen und damit die Varianzen \\(s^2_{dog} = s^2_{cat}\\) der beiden Gruppen sind gleich. Es liegt somit Varianzhomogenität vor.\n\n\n\n\n\nAbbildung 22.3— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nManchmal ist es etwas verwirrend, dass wir uns in einem Boxplot mit Median und IQR die Daten für einen t-Test anschauen. Immerhin rechnet ja ein t-Test mit den Mittelwerten und der Standardabweichung. Hier vergleichen wir etwas Äpfel mit Birnen. Deshalb in der Abbildung 22.4 der Dotplot mit dem Mittelwert und den entsprechender Standardabweichung als Fehlerbalken.\n\n\n\n\nAbbildung 22.4— Dotplot der Sprungweiten [cm] von Hunden und Katzen zusammen mit dem Mittelwert und der Stanardabweichung als Fehlerbalken.\n\n\n\n\nWir nutzen aber später häufig den Boxplot zur Visualisierung der einzelnen Gruppen. Über den Boxplot können wir auch gut abschätzen, ob wir eine annährende bzw. approximative Normalverteilung vorliegen haben."
  },
  {
    "objectID": "stat-tests-ttest.html#hypothesen-für-den-t-test",
    "href": "stat-tests-ttest.html#hypothesen-für-den-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.4 Hypothesen für den t-Test",
    "text": "22.4 Hypothesen für den t-Test\nOhne eine Hypothese ist das Ergebnis eines statistischen Tests wie auch der t-Test nicht zu interpretieren. Wir berechenen eine Teststatistik und einen p-Wert. Beide statistischen Maßzahlen machen eine Aussage über die beobachteten Daten \\(D\\) unter der Annahme, das die Nullhypothese \\(H_0\\) gilt.\nWie lautet nun das Hypothesenpaar des t-Tests? Der t-Test vergleicht die Mittelwerte von zwei Gruppen. Die Nullhypothese ist auch die Gleichheitshypothese. Die Alternativehypothese haben wir auch als Unterschiedshypothese bezeichnet.\nDaher ergibt sich für unser Beispiel mit den Sprungweiten für Hunde- und Katzenflöhen folgende Hypothesen. Die Nullhypothese sagt, dass die mittleren Sprungweite für die Hundeflöhe gleich der mittleren Sprungweite der Katzenflöhe ist. Die Alternativehypothese sagt aus, dass sich die mittlere Sprungweite von Hunde- und Katzenflöhen unterscheidet.\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{dog} &= \\bar{y}_{cat} \\\\  \nH_A: \\bar{y}_{dog} &\\neq \\bar{y}_{cat} \\\\   \n\\end{aligned}\n\\]\nWir testen grundsätzlich auf ein zweiseitiges \\(\\alpha\\)-Niveau von 5%."
  },
  {
    "objectID": "stat-tests-ttest.html#der-student-t-test",
    "href": "stat-tests-ttest.html#der-student-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.5 Der Student t-Test",
    "text": "22.5 Der Student t-Test\nLiegt ein normalverteiltes \\(y\\) vor und sind die Varianzen für die beiden zu vergleichenden Gruppen homogen \\(s^2_{cat} = s^2_{dog}\\), können wir einen Student t-Test rechnen. Wir nutzen dazu die folgendeFormel des Student t-Tests.\n\n\n\n\n\n\nEigentlich wäre hier folgende Formel richtig…\n\\[\ns_{p} = \\sqrt{\\frac{1}{2} (s^2_{dog} + s^2_{cat})}\n\\] …aber auch hier erwischen wir einen Statistikengel um es etwas einfacher zu machen.\n\\[\nT_{calc} = \\cfrac{\\bar{y}_{dog}-\\bar{y}_{cat}}{s_{p} \\cdot \\sqrt{\\cfrac{2}{n_{group}}}}\n\\]\nmit der vereinfachten Formel für die gepoolte Standardabweichung \\(s_p\\).\n\\[\ns_{p} = \\cfrac{s_{dog} + s_{cat}}{2}\n\\]\nWir wollen nun die Werte für \\(\\bar{y}_{dog}\\), \\(\\bar{y}_{cat}\\) und \\(s_{p}\\) berechnen. Wir nutzen hierfür R auf die etwas komplizierte Art und Weise. Es gibt in R auch die Funktion t.test(), die für uns alles auf einmal macht, aber hier nochaml zu Fuß.\n\nsum_tbl <- data_tbl %>% \n  group_by(animal) %>% \n  summarise(mean = round(mean(jump_length), 2), \n            sd = round(sd(jump_length), 2)) \n\nsum_tbl\n\n# A tibble: 2 x 3\n  animal  mean    sd\n  <fct>  <dbl> <dbl>\n1 dog     8.13  2.14\n2 cat     4.74  1.9 \n\n\n\n\n\nWir erhalten durch die Funktion group_by() den Mittelwert und die Standardabweichung für die Sprungweite getrennt für die Hunde- und Katzenflöhe. Wir können damit die beiden obigen Formeln füllen.\nWir berechnen \\(s_p\\) wie folgt.\n\\[\ns_{pooled} = \\cfrac{2.14 + 1.9}{2} = 2.02\n\\]\nAnschließend können wir jetzt \\(s_p\\) und die Mittelwerte sowie die Gruppengröße \\(n_g = 7\\) in die Formel für den Student t-Test einsetzen und die Teststatistik \\(T_{calc}\\) berechnen.\n\\[\nT_{calc} = \\cfrac{8.13- 4.74}{2.02 \\cdot \\sqrt{\\cfrac{2}{7}}} = 3.14\n\\]\nWir erhalten eine Teststatistik \\(T_{calc} = 3.14\\) die wir mit dem kritischen Wert \\(T_{\\alpha = 5\\%} = 2.17\\) vergleichen können. Da \\(T_{calc} > T_{\\alpha = 5\\%}\\) ist, können wir die Nullhypothese ablehnen. Wir haben ein signifikanten Unterschied zwischen den mittleren Sprungweiten von Hunde- und Katzenflöhen nachgewiesen.\nSoweit für den Weg zu Fuß. Wir rechnen in der Anwendung keinen Student t-Test per Hand. Wir nutzen die Formel t.test(). Da wir den Student t-Test unter der Annahme der Varainzhomogenität nutzen wollen, müssen wir noch die Option var.equal = TRUE wählen.\nDie Funktion t.test() benötigt erst die das \\(y\\) und \\(x\\) in Modellschreibweise mit den Namen, wie die beiden Variablen auch im Datensatz data_tbl stehen. In unserem Fall ist die Modellschreibweise dann jump_length ~ animal. Im Weiteren müssen wir noch den Datensatz angeben den wir verwenden wollen durch die Option data = data_tbl. Dann können wir die Funktion t.test() ausführen.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  jump_length by animal\nt = 3.1253, df = 12, p-value = 0.008768\nalternative hypothesis: true difference in means between group dog and group cat is not equal to 0\n95 percent confidence interval:\n 1.025339 5.746089\nsample estimates:\nmean in group dog mean in group cat \n         8.128571          4.742857 \n\n\nWir erhalten eine sehr lange Ausgabe, die aucb etwas verwirrend aussieht. Gehen wir die Ausgabe einmal durch. Ich gehe nicht auf alle Punkte ein, sondern konzentriere mich hier auf die wichtigsten Aspekte.\n\n\nt = 3.12528 ist die berechnete Teststatistik \\(T_{calc}\\). Der Wert unterscheidet sich leicht von unserem berechneten Wert. Der Unterschied war zu erwarten, wir haben ja auch die t-Test Formel vereinfacht.\n\np-value = 0.0087684 ist der berechnete p-Wert \\(Pr(T_{calc}|H_0)\\) aus der obigen Teststatistik. Daher die Fläche rechts von der Teststatistik.\n\n95 percent confidence interval: 1.0253394 5.7460892 ist das 95% Konfidenzintervall. Die erste Zahl ist die untere Grenze, die zweite Zahl ist die obere Grenze.\n\nWir erhalten hier dreimal die Möglichkeit eine Aussage über die \\(H_0\\) zu treffen. In dem obigen Output von R fehlt der kritische Wert \\(T_{\\alpha = 5\\%}\\). Daher ist die berechnete Teststatistik für die Testentscheidung nicht verwendbar. Wir nutzen daher den p-Wert und vergleichen den p-Wert mit dem \\(\\alpha\\)-Niveau von 5%. Da der p-Wert kleiner ist als das \\(\\alpha\\)-Niveau können wir wie Nullhypothese ablehnen. Wir haben einen signifikanten Unterschied. Die Entscheidung mit dem Konfidentintervall benötigt die Signifikanzschwelle. Da wir hier einen Mittelwertsvergleich vorliegen haben ist die Signifikanzschwelle gleich 0. Wenn die 0 im Konfidenzintervall liegt können wir die Nullhypothese nicht ablehnen. In unserem Fall ist das nicht der Fall. Das Konfidenzintervall läfut von 1.025 bis 5.75. Damit ist die 0 nicht im Konfidenzuntervall enthalten und wir können die Nullhypothese ablehnen. Wir haben ein signifikantes Konfidenznintervall vorliegen.\nWie wir sehen fehlt der Mittelwertsuntschied als Effekt \\(\\Delta\\) in der Standardausgabe des t-Tests in R. Wir können den Mittelwertsunterschied selber berechnen oder aber die Funktion tidy() aus dem R Paket broom nutzen. Da der Funktion tidy() kriegen wir die Informationen besser sortiert und einheitich wiedergegeben. Da tidy eine Funktion ist, die mit vielen statistischen Tests funktioniert müssen wir wissen was die einzelnen estimate sind. Es hilft in diesme Fall sich die Visualisierung der Daten anzuschauen und die Abbildung mit den berechneten Werten abzugleichen.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = TRUE) %>% \n  tidy() \n\n# A tibble: 1 x 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n1     3.39      8.13      4.74      3.13 0.00877        12     1.03      5.75\n# ... with 2 more variables: method <chr>, alternative <chr>\n\n\nWir erkennen als erstes den Mittelwertsunterschied zwischen den beiden Gruppen von 3.39 cm. Danach folgen die einzelnen Mittelwerte der Sprungweiten der Hunde und Katzenflöhe mit jeweils 8.13 cm und 4.74 cm. Darauf folgt noch der p-Wert als p.value mit 0.00891 und die beiden Grenzen des Konfidenzintervalls [1.03; 5.75]."
  },
  {
    "objectID": "stat-tests-ttest.html#der-welch-t-test",
    "href": "stat-tests-ttest.html#der-welch-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.6 Der Welch t-Test",
    "text": "22.6 Der Welch t-Test\nDer t-Test ist auch in der Lage mit Varianzhetrogenität umzugehen. Das heißt, wenn die Varianzen der beiden Gruppen nicht gleich sind. Dadurch ändert sich die Formel für den t-Test wie folgt. Dann nennen wir den statistsichen Test Welch t-Test.\n\\[\nT_{calc} = \\cfrac{\\bar{y_1} - \\bar{y_2}}{\\sqrt{\\cfrac{s^2_{y_1}}{n_1} + \\cfrac{s^2_{y_2}}{n_2}}}\n\\]\nWir sehen, dass sich die Formel etwas andert. Da wir nicht mehr annhemen, dass die Varianzen homogen und daher gleich sind, können wir auch keinen gepoolten Varianzschätzer \\(s_p\\) berechnen. Die Varianzen gehen einzeln in die Formel des Welch t-Tests ein. Ebenso müssen die beiden Gruppen nicht mehr gleich groß sein. Statt einen Wert \\(n_g\\) für die Gruppengröße können wir auch die beiden Gruppengrößen separat angeben.\n\n\nHier muss man noch bedenken, dass die Freiheitsgrade anders berechnte werden Die Freiheitsgrade werden wie folgt berechnet.\n\\[\ndf = \\cfrac{\\left(\\cfrac{s^2_{y_1}}{n} +\n    \\cfrac{s^2_{y_2}}{m}\\right)^2}{\\cfrac{\\left(\\cfrac{s^2_{y_1}}{n}\\right)^2}{n-1} + \\cfrac{\\left(\\cfrac{s^2_{y_2}}{m}\\right)^2}{m-1}}\n\\]\nEs ergibt keinen tieferen Sinn die obige Formel nochmal händisch auszurechnen. Die Zahlen ändern sich leicht, aber konzeptionell erhalten wir hier keinen Mehrwert. Deshalb schauen wir uns gleich die Umsetzung in R an. Wir nutzen erneut die Funtktion t.test() und zwar diesmal mit der Option var.equal = FALSE. Damit geben wir an, dass die Varianzen heterogen zwischen den beiden Gruppen sind. Wir nutzen in unserem Beispiel die gleichen Zahlen und Daten wie schon im obigen Student t-Test Beispiel.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  jump_length by animal\nt = 3.1253, df = 11.831, p-value = 0.008906\nalternative hypothesis: true difference in means between group dog and group cat is not equal to 0\n95 percent confidence interval:\n 1.021587 5.749842\nsample estimates:\nmean in group dog mean in group cat \n         8.128571          4.742857 \n\n\nWir sehen das viele Zahlen nahezu gleich sind. Das liegt auch daran, dass wir in unserem Daten keine große Abweichung von der Varianzhomogenität haben. Wirerhalten die gleichen Aussagen wie auch schon im Student t-Test.\nSchauen wir uns nochmal die Ausgabe der Funkton tidy() an.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = FALSE) %>% \n  tidy() \n\n# A tibble: 1 x 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n1     3.39      8.13      4.74      3.13 0.00891      11.8     1.02      5.75\n# ... with 2 more variables: method <chr>, alternative <chr>\n\n\nFür das Erkennen von Normalverteilung und Varianzhterogenität werden häufig sogenannte Vortest empfohlen. Aber auch hier gilt, bei kleiner Fallzahl liefern die Vortests keine verlässlichen Ergebnisse. In diesem Fall ist weiterhin die Beurteilung über einen Boxplot sinnvoller.\nWir sehen hier etwas besser, dass es kaum Abweichungen gibt. Alles egal? Nicht unbedingt. Das Problem ist eher das Erkennen von Varianzheterogenität in sehr kleinen Datensätzen. Kleine Datensätze meint Datensätze unter 30 Beobachtungen je Gruppe. Erst aber dieser Anzahl lassen sich unverzerrte Histogramme zeichnen und so aussagekräftige Abschätzungen der Varianzhomogenität oder Varianzheterogenität treffen."
  },
  {
    "objectID": "stat-tests-ttest.html#der-verbundene-t-test-paired-t-test",
    "href": "stat-tests-ttest.html#der-verbundene-t-test-paired-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.7 Der verbundene t-Test (Paired t-Test)",
    "text": "22.7 Der verbundene t-Test (Paired t-Test)\nIm folgenden Datenbespiel in Tabelle 22.2 haben wir eine verbundene Stichprobe. Das heißt wir haben nicht zehn Flöhe gemessen sondern fünf Flöhe. Einmal im ungefütterten Zustand unfed und einmal im gefütterten Zustand fed. Wir wollen nun wissen, ob der Fütterungszustand Auswirkungen auf die Sprungweite in [cm] hat.\n\n\n\n\nTabelle 22.2— Tabelle der Sprunglängen [cm] von fünf Flöhen zu zwei Zeitpunkten. Einmal wurde die Sprungweite ungefüttert und einmal gefüttert bestimmt. Die Daten liegen im Wide Format vor.\n\nunfed\nfed\ndiff\n\n\n\n5.2\n6.1\n0.9\n\n\n4.1\n5.2\n1.1\n\n\n3.5\n3.9\n0.4\n\n\n3.2\n4.1\n0.9\n\n\n4.6\n5.3\n0.7\n\n\n\n\n\n\nWir nutzen folgende Formel für den paired t-Test für verbundene Stichproben.\n\\[\nT_{calc} = \\sqrt{n}\\cfrac{\\bar{d}}{s_d}\n\\]\nWir können \\(\\bar{d}\\) als Mittelwert der Differenzen der Variablen diff berechnen. Ebenso verfahren wir mit der Standardabweichung der Differenzen \\(s_d\\).\n\\[\nT_{calc} = \\sqrt{10}\\cfrac{0.8}{0.26} = 6.88\n\\]\nUm den die Funktion t.test()in R mit der Option paired = TRUE für den paired t-Test zu nutzen, müssen wir die Daten nochmal über die Funktion gather() in das Long Format umwandeln. Wir wollen nun wissen, ob der Fütterungszustand food_status Auswirkungen auf die Sprungweite in [cm] hat.\n\nt.test(jump_length ~ food_status, \n       data = paired_tbl, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  jump_length by food_status\nt = 6.7612, df = 4, p-value = 0.002496\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.4714866 1.1285134\nsample estimates:\nmean difference \n            0.8 \n\n\nDie Ausgabe des paired t-Test ähnelt stark der Ausage des Student t-Test. Wir erhalten ebenfalls den wichtigen p-Wert mit 0.0025 sowie das 95% Konfidenzintervall mit [0.47; 1.13]. Zum einen ist \\(0.0025 < \\alpha\\) und somit können wir die Nullhypothese ablehnen, zum anderen ist auch die 0 nicht mit in dem Konfidentintervall, womit wir auch hier die Nullhypothese ablehnen können.\n\nt.test(jump_length ~ food_status, \n       data = paired_tbl, paired = TRUE) %>% \n  tidy() \n\n# A tibble: 1 x 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl> <chr>      <chr>      \n1      0.8      6.76 0.00250         4    0.471      1.13 Paired t-~ two.sided  \n\n\nDie Funktion tidy() gibt uns in diesem Fall keine neuen zusätzlichen Informationen."
  },
  {
    "objectID": "stat-tests-ttest.html#freiheitsgrade-im-t-test",
    "href": "stat-tests-ttest.html#freiheitsgrade-im-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.8 Freiheitsgrade im t-Test",
    "text": "22.8 Freiheitsgrade im t-Test\nDer t-Verteilung der Teststatistiken des t-Tests verhält sich nicht wie eine klassische Normalverteilung, die durch den Mittelwert und die Standardabweichung definiert ist. Die t-Verteilung ist nur durch die Freiheistgrade definiert. Der Freiheitsgrade in einem t-Test mit zwei Stichproben ist gegeben durch \\(df = n_1 + n_2 -2\\). Damit beschreiben die Freiheitsgrade grob die Fallzahl. Je mehr Fallzahl desto großer der Freiheitsgrad eines t-Tests.\nAbbildung 22.5 visualisert diesen Zusammenhang von Freiheitsgraden und der Form der t-Verteilung. Je kleiner die Freiheitgarde und damit die Fallzahl, desto weiter sind die Verteilungsschwänze. Daher benötigen wir auch größere \\(T_{calc}\\) Werte um ein signifikantes Ergebnis zu erhalten. Die Fläche unter der t-Verteilung ist immer gleich.\n\n\nAbbildung 22.5— Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und damit die Fallzahl, desto näher kommt die t-Verteilung einer Normalverteilung nahe. Bei einer geringeren Fallzahl, müssen damit größere \\(T_{calc}\\) Werte erreicht werden um eine signifikantes Ergebnis zu erhalten, da mehr Fläche nach rechts wandert."
  },
  {
    "objectID": "classification-basic.html",
    "href": "classification-basic.html",
    "title": "49  Grundlagen der Klassifikation",
    "section": "",
    "text": "Version vom November 20, 2022 um 19:20:42\nDieses Kapitel dient als Einführung in die Klassifikation mit maschinellen Lernmethoden. Leider müssen wir wieder einiges an Worten lernen, damit wir überhaupt mit den Methoden anfangen können. Vieles dreht sich um die Aufbereitung der Daten, damit wir dann auch mit den Modellen anfangen können zu arbeiten. Ja ich meine wirklich Arbeiten, denn wir werden eher einen Prozess durchführen. Selten rechnet man einmal ein Modell und ist zufrieden. Meistens müssen wir noch die Modelle tunen um mehr aus den Modellen rauszuholen. Wir wollen bessere Vorhersagen mit einem kleineren Fehler erreichen. Das ganze können wir dann aber nicht in einem Schritt machen, sondern brauchen viele Schritte nacheinander. Damit müssen wir auch mir R umgehen können sonst ist der Prozess nicht mehr abzubilden."
  },
  {
    "objectID": "classification-basic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "classification-basic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.1 Genutzte R Pakete für das Kapitel",
    "text": "49.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-basic.html#daten",
    "href": "classification-basic.html#daten",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.2 Daten",
    "text": "49.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt.\n\npig_tbl <- read_excel(\"data/infected_pigs.xlsx\") %>% \n  mutate(pig_id = 1:n()) %>% \n  select(pig_id, infected, everything())  \n\nIn Tabelle 50.1 siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 49.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npig_id\ninfected\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\n\n\n\n1\n1\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n49.88\n16.94\n3.07\n\n\n2\n0\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n58.2\n17.95\n4.88\n\n\n3\n0\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n56.8\n19.02\n3.98\n\n\n4\n0\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.47\n18.98\n5.18\n\n\n5\n1\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n59.85\n16.57\n6.71\n\n\n6\n1\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n58.1\n18.22\n5.43\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\nfemale\nnorth\n11.82\n21.5\nrobust\n57.05\n17.95\n6.16\n\n\n408\n1\n56\nmale\nwest\n13.91\n20.8\npre-frail\n50.84\n18.02\n6.52\n\n\n409\n1\n57\nmale\nnorthwest\n12.49\n21.95\nrobust\n55.51\n17.73\n3.94\n\n\n410\n1\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n58.5\n18.23\n2.73\n\n\n411\n1\n59\nfemale\nnorth\n13.13\n20.23\npre-frail\n57.33\n17.21\n5.42\n\n\n412\n1\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n55.85\n17.76\n6.18\n\n\n\n\n\n\nGehen wir jetzt mal die Wörter und Begrifflichkeiten, die wir für das maschinelle Lernen später brauchen einmal durch."
  },
  {
    "objectID": "classification-basic.html#what-he-says",
    "href": "classification-basic.html#what-he-says",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.3 What he say’s?",
    "text": "49.3 What he say’s?\nIn diesem Teil des Skriptes werden wir wieder mit einer Menge neuer Begriffe konfrontiert. Deshalb steht hier auch eine Menge an neuen Worten drin. Leider ist es aber auch so, dass wir bekanntes neu bezeichnen. Wir tauchen jetzt ab in die Community der Klassifizierer und die haben dann eben die ein oder andere Sache neu benannt.\n\n\nKurze Referenz zu What he says?\nDie gute nachticht zuerst, wir haben ein relativ festes Vokabular. Das heißt, wir springen nicht so sehr zwischen den Begrifflichkeiten wie wir es in den anderen Teilen des Skriptes gemacht haben. Du kennst die Modellbezeichnungen wie folgt.\n\\[\ny \\sim x\n\\]\nmit\n\n\n\\(y\\), als Outcome oder Endpunkt.\n\n\\(x\\), als Covariate oder Einflussvariable.\n\nDas bauen wir jetzt um. Wir nennen in dem Bereich des maschinellen Lernen jetzt das \\(y\\) und das \\(x\\) wie folgt.\n\n\n\\(y\\) ist unser label, dafür gibt es kein deutsches Wort.\n\n\\(x\\) sind unsere features und mehrere Features bilden den feature space, dafür gibt es jeweils auch kein deutsches Wort.\n\nIm folgenden Text werde ich also immer vom Label schreiben und dann damit das \\(y\\) links von dem ~ in der Modellgleichung meinen. Wenn ich dann von den Features schreibe, meine ich alle \\(x\\)-Variablen rechts von dem ~ in der Modellgleichung. Ja, daran muss du dich dann gewöhnen. Es ist wieder ein anderer sprachlicher Akzent in einem anderen Gebiet der Statistik.\nLabel meint also das \\(y\\) oder Outcome. Feature beschreibt das \\(x\\) oder die Einflussvariablen."
  },
  {
    "objectID": "classification-basic.html#klassifikation-vs.-regression",
    "href": "classification-basic.html#klassifikation-vs.-regression",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.4 Klassifikation vs. Regression",
    "text": "49.4 Klassifikation vs. Regression\nWenn mich etwas aus der Bahn geworfen hat, dann waren es die Terme classification und regression im Kontext des maschinellen Lernens. Wenn ich von classification schreibe, dann wollen wir ein kategoriales Label vorhersagen. Das bedeutet wir haben ein \\(y\\) vorliegen, was nur aus Klassen bzw. Kategorien besteht. Im Zweifel haben wir dann ein Label mit \\(0/1\\) einträgen. Wenn mehr Klassen vorliegen, wird auch gerne von multiclass Klassifikation gesprochen.\nDazu steht im Kontrast der Term regression. In dem Kontext vom maschinellen Lernen meint regression die Vorhersage eines numerischen Labels. Das heißt, wir wollen die Körpergröße der Studierenden vorhersagen und nutzen dazu einen regression Klassifikator. Das ist am Anfang immer etwas verwirrend. Wir unterschieden hier nur die Typen der Label, sonst nichts. Wir fassen also wie folgt zusammen.\n\n\nclassification, wir haben ein Label bzw. \\(y\\) mit Kategorien. Nehmen wir einmal unser Ferkelbeispiel. In unserer Spalte infected sind die Ferkel infiziert \\((1)\\) oder nicht-infiziert daher gesund \\((0)\\).\n\nregression, wir haben ein Label bzw. \\(y\\) mit kontinuierlichen Werten. Unsere Ferkel haben ein Gewicht in \\(kg\\) und daher nehmen wir die Spalte weight.\n\nWir brauchen die Begriffe, da wir später in den Algorithmen spezifizieren müssen, welcher Typ die Klassifikation sein soll."
  },
  {
    "objectID": "classification-basic.html#trainingsdatensatz-und-testdatensatz",
    "href": "classification-basic.html#trainingsdatensatz-und-testdatensatz",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.5 Trainingsdatensatz und Testdatensatz",
    "text": "49.5 Trainingsdatensatz und Testdatensatz\nUm zu beginnen, teilen wir diesen einen Datensatz in zwei: einen Trainingssatz und einen Testsatz. Die meisten Zeilen und damit Beobachtungen des Originaldatensatzes werden im Trainingssatz sein. Wir nutzen die Trainingsdaten zum Anpassen des Modells. Wir trainieren das Modell auf den Daten des Trainingsdatensatzes. Wir messen dann das Modell auf den Testdatensatz. Warum machen wir das? Wenn wir auf dem Trainingsdatensatz auch die Modelgüte testen würden, dann könnten wir eine Überanpassung (eg. overfitting) auf die Trainingsdaten beobachten. Das Modell ist so gut an die spezifischen Trainingsdaten angepasst, dass es mit neuen Daten schwer umgehen kann.\n\n\nDas R Paket resample stellt die Common Resampling Patterns nochmal da. Auch findest unter Resampling for Evaluating Performance noch eine Menge mehr Ideen für das Resampling.\nZu diesem Zweck können wir das R Paket rsample verwenden. Wir nutzen dann die Funktion initial_split() um die Daten in einen Trainingsdatensatz und einen Testdatensatz aufzuteilen. Dann müssen wir noch den Trainingsdatensatz und den Testdatensatz einmal getrennt in einem Objekt abspeichern.\n\npig_split <- initial_split(pig_tbl, prop = 3/4)\n\npig_split\n\n<Training/Testing/Total>\n<309/103/412>\n\n\nWie wir sehen, sehen wir gar nichts. Das ist auch so gewollt. Da wir im maschinellen Lernen gerne mal mit Datensätzen mit mehreren tausend Zeilen arbeiten würde es wenig helfen, wenn wir gleich alles auf der R Console ausgegeben kriegen. Die Information wie viel wir in den jeweiligen Gruppen haben, hilft schön genug.\n\ntrain_pig_tbl <- training(pig_split)\ntest_pig_tbl <- testing(pig_split)\n\nNun haben wir die beiden Datensätze jeweils separat und können auf dem Trainingsdatensatz die jeweiligen Algorithmen bzw. Modelle trainieren.\nEs ist schön, wenn wir Funktionen wie initial_split(), die für uns die Arbeit machen. Wir haben dann aber auch sehr schnell das Gefühl mit einer Black Box zu arbeiten. Man weiß gar nicht, was da eigentlich passiert ist. Deshalb hier nochmal der Code, den ich dann auch immer zur Demonstration nutze.\n\npig_train_tbl <- pig_tbl %>% sample_frac(0.75)\npig_test_tbl <- anti_join(pig_tbl,\n                          pig_train_tbl, by = 'pig_id')\n\nWir können dann auch überprüfen, ob wir die gleichen Anteile von den infizierten Ferkeln in den jeweiligen Datensätzen haben. Wir berechnen dafür einfach die relativen Anteile. Ein wenig komplizierter als nötig, aber hier geht es jetzt um die Veranschaulichung.\n\ntable(pig_train_tbl$infected)/sum(table(pig_train_tbl$infected))\n\n\n         0          1 \n0.36245955 0.63754045 \n\ntable(pig_test_tbl$infected)/sum(table(pig_test_tbl$infected))\n\n\n         0          1 \n0.41747573 0.58252427 \n\n\nDu kannst die Generierung häufiger wiederholen und du wirst sehen, dass wir es mit einem Zufallsprozess zu tun haben. Mal sind die Anteile ähnlicher mal eher nicht. Das ist dann auch der Grund warum wir unsere Modelle tunen müssen und Modelle häufig wiederholt rechnen und die Ergebnisse dann zusammenfassen."
  },
  {
    "objectID": "classification-basic.html#validierungsdatensatz",
    "href": "classification-basic.html#validierungsdatensatz",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.6 Validierungsdatensatz",
    "text": "49.6 Validierungsdatensatz\nDie finalen Modelle sollten nur einmal anhand ihres Testdatensatzes evaluieren werden. Das Überpfrüfen auf dem Testdatensatz geschieht nachdem die Optimierung und das Training der Modelle vollständig abgeschlossen ist. Was natürlich für uns nicht so schön ist, wir wollen ja auch zwischendurch mal schauen, ob wir auf dem richtigen Weg mit dem Training sind. Wir solle es auch sonst mit dem Tuning funktionieren? Deshalb ist möglich, zusätzliche Datensätze aus dem Trainingsprozess herauszuhalten, die zur mehrmaligen Evaluierung von Modellen verwendet werden können. Das machen wir dann solange bis wir bereit sind anhand des endgültigen Testsatzes zu evaluieren.\nDiese zusätzlichen, aufgeteilten Datensätze werden oft als Validierungssätze bezeichnet und können in über die Funktion validation_split() erstellt werden.\n\nval_pig_lst <- validation_split(pig_tbl, prop = 0.8)\nval_pig_lst\n\n# Validation Set Split (0.8/0.2)  \n# A tibble: 1 × 2\n  splits           id        \n  <list>           <chr>     \n1 <split [329/83]> validation\n\n\nIn diesem Fall lassen wir den Validierungsdatensatz einmal so in der Liste stehen. Es ist faktisch wider ein Split der Daten, nur das wir jetzt auf diesem Datensatz unser Modell während des Tunings testen."
  },
  {
    "objectID": "classification-basic.html#kreuzvalidierung",
    "href": "classification-basic.html#kreuzvalidierung",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.7 Kreuzvalidierung",
    "text": "49.7 Kreuzvalidierung\nBei der Abstimmung von Hyperparametern und der Modellanpassung ist es oft nützlich, das Modell anhand von mehr als nur einem einzigen Validierungssatz zu bewerten, um eine stabilere Schätzung der Modellleistung zu erhalten. Wir meinen hier mit Hyperparametern die Optionen, die ein Algorithmus hat um diesen Algorithmus zu optimieren. Aus diesem Grund verwenden Modellierer häufig ein Verfahren, das als Kreuzvalidierung bekannt ist und bei dem die Daten mehrfach in Analyse- und Valisierungsdaten aufgeteilt werden.\nDie vielleicht häufigste Methode der Kreuzvalidierung ist die \\(V\\)-fache Kreuzvalidierung. Bei dieser auch als \\(k\\)-fold cross-validation bezeichneten Methode werden \\(V\\) neue Stichproben bzw. Datensätze erstellt, indem die Daten in \\(V\\) Gruppen (auch folds genannt) von ungefähr gleicher Größe aufgeteilt werden. Der Analysesatz jeder erneuten Stichprobe besteht aus \\(V-1\\) Gruppen, wobei die verbleibende Gruppe als Validierungsdatensatz verwendet wird. Insgesamt führen wir dadurch dann den Algorithmus \\(V\\)-mal durch. Auf diese Weise wird jede Beobachtung in Daten in genau einem Beurteilungssatz verwendet.\nIn R können wir dafür die Funktion vfold_cv() nutzen. Im Folgenden einmal Split für \\(V = 5\\). Wir führen also eine \\(5\\)-fache Kreuzvalidierung durch.\n\nvfold_cv(pig_tbl, v = 3)\n\n#  3-fold cross-validation \n# A tibble: 3 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [274/138]> Fold1\n2 <split [275/137]> Fold2\n3 <split [275/137]> Fold3\n\n\nAls ein Nachteil wird oft angesehen, dass die Kreuzvalidierung eine hohe Varianz in den Daten verursacht. Dagegen hilft dann die wiederholte Kreuzvalidierung (eng. repeated cross-validation). Wir bauen in jede Kreuzvalidierung nochmal eine oder mehr Wiederholungen ein. In unserem Fall dann drei Wiederholungen je Kreuzvalidierung \\(V\\).\n\nvfold_cv(pig_tbl, v = 3, repeats = 2)\n\n#  3-fold cross-validation repeated 2 times \n# A tibble: 6 × 3\n  splits            id      id2  \n  <list>            <chr>   <chr>\n1 <split [274/138]> Repeat1 Fold1\n2 <split [275/137]> Repeat1 Fold2\n3 <split [275/137]> Repeat1 Fold3\n4 <split [274/138]> Repeat2 Fold1\n5 <split [275/137]> Repeat2 Fold2\n6 <split [275/137]> Repeat2 Fold3\n\n\nWir sehen das der Split ungefähr immer gleich groß ist. Manchmal haben wir durch die Trennung eine Beobachtung mehr in dem Analysedatensatz mit \\(n = 329\\) oder \\(n = 330\\) Beobachtungen. Dementsprechend hat der Validierungsdatensatz einmal \\(n = 82\\) und einmal \\(n = 83\\) Beobachtungen."
  },
  {
    "objectID": "classification-basic.html#monte-carlo-kreuzvalidierung",
    "href": "classification-basic.html#monte-carlo-kreuzvalidierung",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.8 Monte-Carlo Kreuzvalidierung",
    "text": "49.8 Monte-Carlo Kreuzvalidierung\nWir haben als eine Alternative zur V-fachen Kreuzvalidierung die Monte-Carlo-Kreuzvalidierung vorliegen. Während bei der V-fachen Kreuzvalidierung jede Beobachtung in den Daten einem - und zwar genau einem - Validierungsdatensatz zugewiesen wird, wird bei der Monte-Carlo-Kreuzvalidierung für jeden Validierungsdatensatz eine zufällige Teilmenge der Daten ausgewählt, d. h. jede Beobachtung kann in 0, 1 oder vielen Validierungsdatensätzen verwendet werden. Der Analysesatz besteht dann aus allen Beobachtungen, die nicht ausgewählt wurden. Da jeder Validierungsdatensatz unabhängig ausgewählt wird, können wir diesen Vorgang so oft wie gewünscht wiederholen. Das stimt natürlich nur bedingt, denn irgendwann haben wir auch bei perfekter Permutation dann Wiederholungen der Datensätze.\nDie Funktion mc_cv() liefert uns dann die Datensätze für die Monte-Carlo Kreuzvalidierung. Wir geben dabei an, wieviel der Daten in den jeweiligen Datensatz hinein permutiert werden soll.\n\nmc_cv(pig_tbl, prop = 0.6, times = 3)\n\n# Monte Carlo cross-validation (0.6/0.4) with 3 resamples  \n# A tibble: 3 × 2\n  splits            id       \n  <list>            <chr>    \n1 <split [247/165]> Resample1\n2 <split [247/165]> Resample2\n3 <split [247/165]> Resample3"
  },
  {
    "objectID": "classification-basic.html#bootstraping",
    "href": "classification-basic.html#bootstraping",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.9 Bootstraping",
    "text": "49.9 Bootstraping\nDie letzte Stichprobengenierungsmethode ist der Bootstrap. Eine Bootstrap Stichprobe ist eine Stichprobe des Datensatzes mit der gleichen Größe wie der Datensatz. Nur werden die Bootstrap Stichproben mit Ersetzung gezogen, so dass eine einzelne Beobachtung mehrfach in die Stichprobe aufgenommen werden können. Der Validierungsdatensatz besteht dann aus allen Beobachtungen, die nicht für den Analysesatz ausgewählt wurden. Im Allgemeinen führt das Bootstrap-Resampling zu pessimistischen Schätzungen der Modellgenauigkeit.\nWir können die Funktion bootstraps() für die Generierung der Bootstrap Stichprobe nutzen.\n\npig_boot_tbl <- pig_tbl %>% \n  extract(1:10, 1:5)\n\npig_boot <- bootstraps(pig_boot_tbl, times = 3)\n\nNun haben wir auch die Möglichkeit uns die einzelnen Bootstraps Stichproben mit pluck() rauszuziehen. Hier sehen wir auch, dass einzelene Beobachtungen doppelt in der Bootstrap Stich probe vorkommen.\n\npluck(pig_boot, \"splits\", 1) %>% \n  as_tibble \n\n# A tibble: 10 × 5\n   pig_id infected   age sex    location \n    <int>    <dbl> <dbl> <chr>  <chr>    \n 1      4        0    59 female north    \n 2      2        0    53 male   northwest\n 3      4        0    59 female north    \n 4      4        0    59 female north    \n 5      4        0    59 female north    \n 6      7        0    49 male   west     \n 7      5        1    63 male   northwest\n 8      9        1    58 female west     \n 9      5        1    63 male   northwest\n10     10        1    57 male   northwest"
  },
  {
    "objectID": "classification-basic.html#supervised-vs-unsupervised",
    "href": "classification-basic.html#supervised-vs-unsupervised",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.11 Supervised vs unsupervised",
    "text": "49.11 Supervised vs unsupervised"
  },
  {
    "objectID": "classification-basic.html#bagging",
    "href": "classification-basic.html#bagging",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.12 Bagging",
    "text": "49.12 Bagging\nDas Wort Bagging steht für bootstrap aggregating und ist eine Methode, um Vorhersagen aus verschiedenen Modellen zu kombinieren. Dabei müssen alle Modelle mit dem gleichen Algorithmus laufen, können aber auf verschiedenen Datensätzen oder aber Variablensätzen zugreifen. Häufig haben die Modelle eine hohee Varianz in der Vorhersage und wir nutzen dann Bagging um die Modelle miteinader zu kombinieren und dadurch die Varianz zu verringern. Die Ergebnisse der Modelle werden dann im einfachsten Fall gemittelt. Das Ergebnis jeder Modellvorhersage geht mit gleichem Gewicht in die Vorhersage ein. Wir haben auch noch andere Möglichkeiten, aber du kannst dir Vorstellen wir rechnen verschiedene Modelle \\(k\\)-mal und bilden dann ein finales Modell in dem wir alle \\(k\\)-Modelle zusammenfassen. Wie wir die Zusammenfassung rechnen, ist dann immer wieder von Fall zu Fall unterschiedlich. Wir erhalten am Ende einen Ensemble Klassifizierer, da ja ein Ensemble von Modellen zusammengefasst wird."
  },
  {
    "objectID": "classification-basic.html#boosting",
    "href": "classification-basic.html#boosting",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.15 Boosting",
    "text": "49.15 Boosting\n\n\n\n\nhttps://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5\nhttps://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30\nhttps://howtolearnmachinelearning.com/articles/boosting-in-machine-learning/"
  },
  {
    "objectID": "classification-basic.html#recipes",
    "href": "classification-basic.html#recipes",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.16 Recipes",
    "text": "49.16 Recipes\n\n\nDu findest hier die Introduction to recipes und dann eine Idee wie recipes funktionieren mit Preprocess your data with recipes."
  },
  {
    "objectID": "classification-basic.html#problem-der-fehlenden-werte",
    "href": "classification-basic.html#problem-der-fehlenden-werte",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.17 Problem der fehlenden Werte",
    "text": "49.17 Problem der fehlenden Werte\nEin wichtiger Punkt ist bei der Nutzung von maschinellen Lernen, dass wir keine fehlenden Beobachtungen in den Daten haben dürfen. Es darf kein einzelner Wert fehlen. Dann funktionieren die Algorithmen nicht und wir erhalten eine Fehlermeldung. Deshalb ist es die erste Statistikerpflicht darauf zu achten, dass wir nicht so viele fehlenden Werte in den Daten haben. Das ist natürlich nur begrenzt möglich. Wenn wir auf die Gummibärchendaten schauen, dann wurden die Daten ja von mir mit Erhoben. Dennoch haben wir viele fehlende Daten mit drin, da natürlich Studierende immer was eingetragen haben. Wenn du wissen willst, wie du mit fehlenden Werten umgehst, dann schaue einmal dazu das Kapitel 38 an. Wir gehen hier nicht nochmal auf alle Verfahren ein, werden aber die Verfahren zur Imputation von fehlenden Werten dann am Beispiel der Gummibärchendaten anwenden. Müssen wir ja auch, sonst könnten wir auch die Daten nicht für maschinelle Lernverfahren nutzen."
  },
  {
    "objectID": "classification-basic.html#normalisierung-der-beobachtungen",
    "href": "classification-basic.html#normalisierung-der-beobachtungen",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.16 Normalisierung der Beobachtungen",
    "text": "49.16 Normalisierung der Beobachtungen\nSiehe hierzu das Kapitel 17. Wir gehen hier nicht nochmal auf alle Verfahren ein, sondern konzentrieren uns auf die häufigsten Anwendungen."
  },
  {
    "objectID": "classification-basic.html#needed-r-packages",
    "href": "classification-basic.html#needed-r-packages",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.19 Needed R packages",
    "text": "49.19 Needed R packages\nhttps://parsnip.tidymodels.org/ https://rsample.tidymodels.org/\nR Paket tune\nR Paket yardstick"
  },
  {
    "objectID": "classification-basic.html#visualisation",
    "href": "classification-basic.html#visualisation",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.20 Visualisation",
    "text": "49.20 Visualisation\nhttps://datascienceplus.com/machine-learning-results-one-plot-to-rule-them-all/ https://datascienceplus.com/machine-learning-results-in-r-one-plot-to-rule-them-all-part-2-regression-models/ https://www.r-bloggers.com/2021/04/the-good-the-bad-and-the-ugly-how-to-visualize-machine-learning-data/ https://neptune.ai/blog/visualizing-machine-learning-models https://uc-r.github.io/lime https://cran.r-project.org/web/packages/mboost/vignettes/mboost_tutorial.pdf"
  },
  {
    "objectID": "classification-randomforest.html",
    "href": "classification-randomforest.html",
    "title": "51  Random Forest",
    "section": "",
    "text": "Version vom November 16, 2022 um 11:27:38"
  },
  {
    "objectID": "classification-randomforest.html#rpart",
    "href": "classification-randomforest.html#rpart",
    "title": "51  Random Forest",
    "section": "\n51.1 Rpart",
    "text": "51.1 Rpart"
  },
  {
    "objectID": "classification-randomforest.html#random-forest",
    "href": "classification-randomforest.html#random-forest",
    "title": "51  Random Forest",
    "section": "\n51.2 Random Forest",
    "text": "51.2 Random Forest\n\n\nAbbildung 51.1— Dars.\n\n\n\n\nAbbildung 51.2— Dars.\n\n\n\n\nAbbildung 51.3— Dars.\n\n\n\n\nAbbildung 51.4— Dars."
  },
  {
    "objectID": "classification-knn.html",
    "href": "classification-knn.html",
    "title": "50  \\(k\\) nearest neighbor",
    "section": "",
    "text": "Version vom November 20, 2022 um 19:04:21\nWas macht der \\(k\\) nächste Nachbarn Algorithmus (eng. k nearest neighbor, abk. k-NN) ? Der Algorthmus ist ein sehr einfacher Algorithmus, der auf den Abständen zu den benachbarten Beobachtungen basiert. Wir wollen also für eine neue Beobachtung den Infketionstatus vorhersagen. Um diese Vorhersage zu bewerkstelligen nutzen wir die \\(k\\)-nöchsten Nachbarn zu dieser neuen Beobachtung. Wenn die Mehrzahl der \\(k\\)-nächsten Nachbarn den Infektionsstatus \\(1\\) hat, dann vergeben wir auch der neuen Beoabchtung den Infektionsstatus \\(1\\). Wenn dies nicht der Fall ist, dann erhält die neue Beobachtung den Infektionsstatus \\(0\\)."
  },
  {
    "objectID": "classification-knn.html#kmeans-clustering",
    "href": "classification-knn.html#kmeans-clustering",
    "title": "50  \\(k\\) nearest neighbor",
    "section": "\n50.3 kmeans Clustering",
    "text": "50.3 kmeans Clustering\nhttps://parsnip.tidymodels.org/reference/nearest_neighbor.html\nhttps://recipes.tidymodels.org/reference/step_impute_knn.html\nhttps://rpubs.com/Nilafhiosagam/574373\n\nkmeans(pig_tbl$age, centers = 2)\n\nK-means clustering with 2 clusters of sizes 218, 194\n\nCluster means:\n       [,1]\n1 63.463303\n2 56.159794\n\nClustering vector:\n  [1] 1 2 1 2 1 2 2 2 2 2 1 2 2 1 2 2 1 2 2 1 2 1 2 2 2 1 2 2 1 2 2 1 2 2 1 2 2\n [38] 2 1 2 1 2 2 2 2 2 1 2 1 1 1 2 1 1 2 2 2 2 2 1 2 1 2 2 2 2 2 2 1 2 1 1 1 2\n [75] 2 1 2 2 1 1 2 2 2 1 2 2 1 2 1 1 1 1 1 2 1 1 1 1 1 2\n [ reached getOption(\"max.print\") -- omitted 312 entries ]\n\nWithin cluster sum of squares by cluster:\n[1] 1720.2064 1222.0464\n (between_SS / total_SS =  65.0 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nknn3_fit <- knn3(infected ~ age + sex + location + activity + crp + \n                   frailty + bloodpressure + weight + creatinin, \n                 data = pig_tbl, k = 10)\n\npredict(knn3_fit, newdata = pig_tbl) %>% \n  as_tibble()\n\n# A tibble: 412 × 2\n     `0`   `1`\n   <dbl> <dbl>\n 1   0.3   0.7\n 2   0.5   0.5\n 3   0.6   0.4\n 4   0.5   0.5\n 5   0.2   0.8\n 6   0     1  \n 7   0.6   0.4\n 8   0.3   0.7\n 9   0.1   0.9\n10   0.2   0.8\n# … with 402 more rows\n\nnearest_neighbor(neighbors = 11) %>% \n  set_engine(\"kknn\") %>% \n  set_mode(\"classification\") %>% \n  fit(infected ~ age + sex + location + activity + crp + \n                   frailty + bloodpressure + weight + creatinin,\n      data = pig_tbl)\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = infected ~ age + sex + location +     activity + crp + frailty + bloodpressure + weight + creatinin,     data = data, ks = min_rows(11, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.30582524\nBest kernel: optimal\nBest k: 11\n\ndummies_rec <- pig_tbl %>% \n  recipe(infected ~ sex + age) %>%\n  step_dummy(sex,\n             one_hot = TRUE)  \n\ndummies_rec %>% \n  prep %>% \n  bake(pig_tbl)\n\n# A tibble: 412 × 4\n     age infected sex_female sex_male\n   <dbl> <fct>         <dbl>    <dbl>\n 1    61 1                 0        1\n 2    53 0                 0        1\n 3    66 0                 1        0\n 4    59 0                 1        0\n 5    63 1                 0        1\n 6    55 1                 0        1\n 7    49 0                 0        1\n 8    53 0                 0        1\n 9    58 1                 1        0\n10    57 1                 0        1\n# … with 402 more rows"
  },
  {
    "objectID": "experimental-design-basic.html",
    "href": "experimental-design-basic.html",
    "title": "47  Grundlagen der Versuchsplanung",
    "section": "",
    "text": "Version vom November 16, 2022 um 22:00:15\nIn diesem Kapitel wollen wir uns mit der Auswertung von verschiedenen experiemnetellen Designs beschäftigen. Wir schauen uns dafür jeweils eine mögliche Visualisierung an und bauen uns dann die Daten künstlich nach. Warum eigentlich künstliche Daten? Das heist wir erschaffen uns Daten wo wir genau wissen, wie der Mittelwert und die Standardabweichungen in den einzelnen Gruppen sind. Warum ist das hilfreich? Dadurch das wir wissen, dass der Mittelwertsunterschied zwischen Gruppe \\(A\\) und Gruppe \\(B\\) mit dem Effekt von \\(\\Delta_{A-B} = 5\\) erschaffen wurde, können wir dann auch die Ausgaben der Funktionen besser bewerten."
  },
  {
    "objectID": "experimental-design-basic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "experimental-design-basic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "47  Grundlagen der Versuchsplanung",
    "section": "\n47.1 Genutzte R Pakete für das Kapitel",
    "text": "47.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, emmeans, multcomp, scales, performance,\n               effectsize, parameters)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "experimental-design-basic.html#sec-crd",
    "href": "experimental-design-basic.html#sec-crd",
    "title": "47  Grundlagen der Versuchsplanung",
    "section": "\n47.2 Complete randomized design (CRD)",
    "text": "47.2 Complete randomized design (CRD)\nDas komplette randomizierte Design (eng. complete randomized design) ist das simpleste Felddesign was wir anzubieten haben. Wir haben einen Stall oder ein Feld oder einen Tisch und unterteilen diesen Raum zufällig in Untereinheiten. Auf oder in jeder Untereinheit bringen wir dann eine Behandlung aus.\n\n\nAbbildung 47.1— Visualisierung des complete randomized design mit einer Behandlung und vier Behandlungsleveln.\n\n\nWir haben einen Tisch und stellen Töpfe mit Pflanzen auf den Tisch. Jeder Topf erhält zufällig eine Behandlung. Wir haben gleich viele Töpfe mit Pflanzen für jede Behandlung.\nWir haben einen Stall mit Buchten für Schweine. Jede Bucht erhält eine zufällige Behandlung. Wir haben gleich viele Buchten für jede Behandlung.\nWir haben ein Feld und erschaffen Parzellen auf dem Feld. Auf jeder Parzelle wird zufällig eine Variante ausgebracht. Wir haben geich viele Parzellen für jede Variante.\nSchauen wir uns das Complete randomized design einmal an einem konkreten Beispiel an. Wir nutzen dafür einen Faktor mit der Behandlung. Die Behandlung hat vier Level mit den einzelnen Leveln \\(A\\), \\(B\\), \\(C\\) und \\(D\\).\n\n47.2.1 Visualisierung\nIn Abbildung 47.2 sehen wir die Visualisierung unseres Versuches. Wir haben einen großen Raun in dem sich zufällig die Level der Behandlung drauf verteilen. Hierbei ist es wichtig zu verstehen, dass die Anordnung rein zufällig ist. Wir sehen, dass jedes Level der Behandlung mit \\(n = 5\\) auf das Feld aufgebracht wurde. Wir haben also ein balanciertes Design mit \\(N = 20\\) Beobachtungen. Wir könnten hier auch einen Tisch mit \\(n=20\\) Pflanzentöpfen vorliegen haben oder einen Stall mit \\(n = 20\\) Buchten.\n\n\nAbbildung 47.2— Visualisierung des complete randomized design mit einer Behandlung und vier Behandlungsleveln.\n\n\n\n47.2.2 Daten\nIm Folgenden bauen wir uns die Daten für das Complete randomized design. Dafür nuten wir die Funktion rnorm(). Die Funktion rnorm() erlaubt es aus einer Normalverteilung n Beobachtungen mit einem Mittelwert mean und einer Standardabweichung sd zu ziehen. Wir erschaffen uns so vier Behandlungsgruppen \\(A\\) bis \\(D\\) mit jeweils unterschiedlichen Mittelwerten von \\(\\bar{y}_A = 10\\), \\(\\bar{y}_B = 12\\), \\(\\bar{y}_C = 16\\) und \\(\\bar{y}_D = 20\\) sowie homogenen Varianzen mit \\(s_A = s_B = s_C = s_D = 2\\). Jede Behandlung hat \\(n = 5\\) Beobachtungen. Wir haben also ein balanziertes Design vorliegen.\n\nset.seed(20220916)\ncrd_tbl <- tibble(A = rnorm(n = 5, mean = 10, sd = 2),\n                  B = rnorm(n = 5, mean = 12, sd = 2),\n                  C = rnorm(n = 5, mean = 16, sd = 2),\n                  D = rnorm(n = 5, mean = 20, sd = 2)) %>% \n  gather(key = trt, value = rsp) %>% \n  mutate(trt = as_factor(trt))\n\nSchauen wir uns einmal die Daten an, die wir in R erhalten. Das Objekt crd_tbl ist ein tibble in Long-Format nach der Anwendung der Funktion gather(). Wir haben auch die Spalte trt für die Behanldung als Faktor umgewandelt.\n\ncrd_tbl\n\n# A tibble: 20 × 2\n   trt     rsp\n   <fct> <dbl>\n 1 A     12.3 \n 2 A     11.0 \n 3 A     13.4 \n 4 A     13.2 \n 5 A      4.27\n 6 B     12.2 \n 7 B     11.4 \n 8 B     15.4 \n 9 B      9.51\n10 B     11.9 \n11 C     13.1 \n12 C     15.4 \n13 C     16.3 \n14 C     19.0 \n15 C     19.4 \n16 D     20.9 \n17 D     17.8 \n18 D     20.6 \n19 D     24.6 \n20 D     17.5 \n\n\nWir haben also \\(N = 20\\) Beobachtungen vorliegen. Wir immer ist es schwer eine Datentabelle zu erfasen. Daher schauen wir uns die Daten einmal in Abbildung 47.3 als Boxplots an. Wir wolllen uns noch die Punkte zusätzlich anzeigen lassen. bei der geringen Anzahl an Beobachtungen wäre ein Dotplot oder ein Scatterplot auch eine Möglichkeit.\n\nggplot(crd_tbl, aes(trt, rsp, fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, shape = 4, size = 3) +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung 47.3— Boxplots der Behandlungsgruppen zufällig aus einer Normalverteilung mit Varianzhomogenität generierten Daten.\n\n\n\n\nWir erinnern uns, dass die Daten alle varianzhomogen und normalverteilt sind. Wir haben die Daten so erschaffen. Dennoch wirken die Boxplots so, als würde teilweise eine schiefe Verteilung vorliegen. Bei so wenigen Beobachtungen ist es immer schwer, für oder gegen eine Verteilung zu argumentieren. Wir bleiben bei einer Normalverteilung, wenn wir glauben, dass das \\(y\\) approimativ normalverteilt ist. Wir schreiben dann, dass wir ein normalverteiltes \\(y\\) annehmen.\n\n47.2.3 Modellierung\nIm Folgenden wollen wir die Daten modellieren. Das heist wir wollen eine Linie durch eine multidimensionale Punktewolke zeichnen. Daher auch lineares Modell oder eben durch die Funktion lm() in R für linear model. Wir nutzen das Paket parameters und die Funktion model_parameters() um uns die Parameter des Modells auszugeben. Wir könnten auch die Funktion tidy() nutzen, aber wir erhalten durch die Funktion model_parameters() etwas mehr Informationen und bessere Spaltenüberschriften.\nWir bauen das Modell in folgender Form. Wir haben ein numerisches Outcome \\(y\\) sowie einen Faktor \\(f_1\\).\n\\[\ny \\sim f_1\n\\]\nNun können wir das abstrakte Modell in die Daten übersetzen und erhalten folgendes Modell.\n\\[\nrsp \\sim trt\n\\]\nDas heist, unsere numerische Variable rsp hängt ab von unserer faktoriellen Variable trt. Wir müssen immer wissen, wie die Spaltennamen in unserem Datensatz crd_tbl lauten sonst kann R die Spalten nicht finden.\n\nfit_crd <- lm(rsp ~ trt, crd_tbl)\n\nfit_crd %>%  model_parameters()\n\nParameter   | Coefficient |   SE |         95% CI | t(16) |      p\n------------------------------------------------------------------\n(Intercept) |       10.83 | 1.30 | [ 8.07, 13.59] |  8.33 | < .001\ntrt [B]     |        1.25 | 1.84 | [-2.65,  5.15] |  0.68 | 0.506 \ntrt [C]     |        5.80 | 1.84 | [ 1.90,  9.70] |  3.15 | 0.006 \ntrt [D]     |        9.47 | 1.84 | [ 5.57, 13.37] |  5.15 | < .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nÜberlege mal, was die Spalte Coefficient aussagen möchte. Wir erhalten den (Intercept) mit \\(10.38\\) und damit den MIttelwert der Gruppe \\(A\\). In den folgenden Zeilen sind die Änderungen zu dem (Intercept) und damit zu der Gruppe \\(A\\) dargestellt. Da wir nur eine sehr kleine Anzhl an Beoabchtungen haben, haben wir hier auch Abweichungen zu den voreingestellten Mittelwerten und Standardabweichungen. Wir schauen uns ja auch nur eine Realisierung von möglichen Daten \\(D\\) an. Wir sehen, dass alle Koeffizienten signifikant und damit unterschiedlich von der Null sind. Der \\(p\\)-Wert ist kleiner als das Signiifkanzniveau von \\(\\alpha\\) gleich 5%.\nWir können jetzt nochmal überprüfen, ob die Residuen die Annahme der Varianzhomogenität erfüllen.\n\nfit_crd %>% check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.737).\n\n\nSowie ob die Residuen normalverteilt sind.\n\nfit_crd %>% check_normality()\n\nOK: residuals appear as normally distributed (p = 0.620).\n\n\nDa wir ja hiermit nur eine Zeile Text produziert haben und darübr hinaus wir gerne uns Dinge anschauen, können wir auch die Residuen einmal visualisieren. In Abbildung 47.4 sehen wir den QQ-Plot der Residuen sowie die Verteilung unserer Residuen in einem Desnityplot. Wir sehen, dass die Residuen einer Normalverteilung folgen.\n\ncheck_model(fit_crd, check = c(\"qq\", \"normality\"))\n\n\n\nAbbildung 47.4— QQ-Plot und Densityplot der Residuen aus dem lineare Modell.\n\n\n\n\nWunderbar. Wir können jetzt eine Varianzanalyse und dann eine Mittelwertsvergleich durchführen. Achtung, wir können uns hier auch etwas in die Ecke testen. Wenn wir nur lange genug neue Daten generieren, werden wir irgendwann auch einen Datensatz finden, der die Varianzhomogenität und die Normalverteilung ablehnt. Das liegt in der Theorie des statistischen Testens sowie der kleinen Fallzahl verborgen. Deshalb können wir im Zweifel gerne einmal deine Vortests in dem R Tutorium oder in einer statistischen Beratung diskutieren.\n\n47.2.4 Varianzanalyse und Mittelwertsvergleich\nDie einfaktorielle Varianzanalyse ist ziemlich einfach und ergibt sich fast von alleine. Wir nehmen das Objekt des Modells und pipen das Modell in die Funktion anova(). Wir lassen uns dann wieder die Modellparameter der ANOVA widergeben.\n\nres_anova <- fit_crd %>% \n  anova() \n\nres_anova %>% model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |     F |      p\n-----------------------------------------------------------\ntrt       |      283.24 |  3 |       94.41 | 11.16 | < .001\nResiduals |      135.35 | 16 |        8.46 |       |       \n\nAnova Table (Type 1 tests)\n\n\nWir sehen, dass der Faktor Behandlung signifkant ist, da der \\(p\\)-Wert kleiner ist als das Signifkanzniveau \\(\\alpha\\) gleich 5%. Wir können damit die Nullhypothese ablehnen, wir haben zumindestens einen paarweisen Gruppenunterschied in der Behandlung. Welchen wissen wir nicht, dafür machen wir dann die paarweisen Vergleiche. Eigentlich können wir uns in diesem simplen Fall die ANOVA schhenken und gleich den Mittelwertsvergleich rechnen. Aber das es Usus ist und auch in vielen Abschlussarbeiten verlangtt wird, machen wir hier es einfach mal gleich mit.\nJetzt brauchen wir nur noch die Effektstärke der ANOVA, also wieviel Varianz eigentlich der Faktor Behandlung erklärt. Dfür nutzen wir die Funktion eta_squared() aus dem Paket effectsize.\n\nres_anova %>% eta_squared(partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 |       95% CI\n-------------------------------\ntrt       | 0.68 | [0.38, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nMit einem \\(\\eta^2\\) von \\(0.86\\) wissen wir, dass 86% der Varianz von dem Faktor Behandlung erklärt wird. Das wundert uns nicht, denn wir haben ja nur den Faktor Behandlung in unseren Daten aus denen sich unser Outcome ergibt.\nNachdem wir kurz die ANOVA gerechnet haben, wollen wir noch den Mittelwertsvergleich rechnen. Wir nutzen dazu das Paket emmeans. Wir müssen der Funktion emmeans() ein Objekt aus einem Modell übergeben und der Funktion mitteilen, was der Faktor ist mit dem der Vergleich gerechnet werden soll. Wir haben hier den Faktor trt vorliegen und wollen einen parweisen Vergleich über alle Level des Faktors rechnen.\n\nres_crd <- fit_crd %>% \n  emmeans(~ trt) \n\nWir haben die Ausgabe der Funktion emmeans() in dem Objekt res_crd gespeichert und nutzen das Objekt zuerst um einmal die Ausgabe für das comapct letter display zu erhalten. Als Adjustierung des \\(\\alpha\\) Fehlers nutzen wir die Adjustierung nach Bonferroni. Es sind auch andere Adjustierungen möglich, aber aus Gründen der Einfachheit nehmen wir hier mal den Klassiker der Adjustierung. Je nach Fragestellung gibt es sicherlich auch eine bessere Alternative für Bonferroni.\n\nres_crd_cld <- res_crd %>% \n  cld(adjust = \"bonferroni\", Letters = letters) %>% \n  tidy() %>% \n  select(trt, estimate, conf.low, conf.high, .group) %>% \n  mutate(across(where(is.numeric), round, 2))\n\nNachdem wir noch ein wenig gerundet haben und die Spalten passend gewählt, erhalten wir dann folgende Ausgabe.\n\nres_crd_cld \n\n# A tibble: 4 × 5\n  trt   estimate conf.low conf.high .group\n  <chr>    <dbl>    <dbl>     <dbl> <chr> \n1 A         10.8     7.17      14.5 \" a  \"\n2 B         12.1     8.42      15.7 \" ab \"\n3 C         16.6    13.0       20.3 \"  bc\"\n4 D         20.3    16.6       24.0 \"   c\"\n\n\nWir nutzen die Ausgabe res_crd_cld direkt in der Abbildung 47.5 um uns das compact letter display zusammen mit den Daten und den entsprechenden 95% konfidenzintervallen anzeigen zu lassen. Der Code ist etwas länger, da wir hier verschiedene Schichten von einem geom übereinander legen müssen.\n\nggplot() +\n  theme_bw() +\n  geom_point(data = crd_tbl, aes(x = trt, y = rsp, fill = trt)) +\n  geom_text(data = res_crd_cld, \n            aes(x = trt , y = estimate, label = .group),\n            position = position_nudge(x = 0.2), color = \"red\") +\n  geom_errorbar(data = res_crd_cld,\n                aes(ymin = conf.low, ymax = conf.high, x = trt),\n                color = \"red\", width = 0.1,\n                position = position_nudge(x = 0.1)) +\n  geom_point(data = res_crd_cld, \n             aes(x = trt , y = estimate),\n             position = position_nudge(x = 0.1), color = \"red\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"Behandlung\", y = \"Gewicht [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die Rohdaten dar.\n       Rote Punkte und Fehlerbalken stellen bereinigte Mittelwerte mit 95% Konfidenzgrenzen pro Behandlung dar.\n       Mittelwerte, mit einem gemeinsamen Buchstaben, sind nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 47.5— Scatterplot der Behandlungsgruppen zusammen mit den 95% Konfidenzintervall und dem compact letter display.\n\n\n\n\nWi sehen an dem compact letter display, dass sich die Behandlung \\(A\\) von der Behandlung \\(B\\), \\(C\\) und \\(D\\) unterscheidet. Die Behandlung \\(B\\) und \\(C\\) sind gleich. Die Behandlung \\(C\\) unterschdeit sich von all den anderen Behandlungen. Wir erinnern uns, wenn die Buchstaben in dem compact letter display gleich sind, dann können wie die Nullhypothese für diese Vergleiche nicht ablehnen. Wir haben keinen signifikanten Unterschied vorliegen.\nNun ist es so, dass wir meistens noch die \\(p\\)-Werte für die paarweisen Vergleich sowie die 95% Konfidenzintervalle darstellen wollen. Wir nutzen dafür die Funktion contrast() aus dem Paket emmeans. Danach müssen wir noch Spalten auswählen und die \\(p\\)-Werte über die Funktion pvalue() aus dem Paket scales schöner formatieren. Wir erhalten dann das Objekt res_crd_tbl.\n\nres_crd_tbl <- res_crd %>% \n  contrast(method = \"pairwise\") %>% \n  tidy(conf.int = TRUE) %>% \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) %>% \n  select(contrast, estimate, p.value,\n         conf.low, conf.high) \n\nIn dem Objekt res_crd_tbl finden wir dann die \\(p\\)-Werte für alle paarweisen Vergleiche sowie die 95% Konfidenzintevalle.\n\nres_crd_tbl\n\n# A tibble: 6 × 5\n  contrast estimate p.value conf.low conf.high\n  <chr>       <dbl> <chr>      <dbl>     <dbl>\n1 A - B       -1.25 0.903      -6.51      4.01\n2 A - C       -5.8  0.028     -11.1      -0.54\n3 A - D       -9.47 <0.001    -14.7      -4.21\n4 B - C       -4.55 0.103      -9.81      0.71\n5 B - D       -8.22 0.002     -13.5      -2.96\n6 C - D       -3.67 0.231      -8.93      1.59\n\n\nHier sehen wir dann die \\(p\\)-Werte für alle paarweisen Vergleiche und können dann die Entscheidung gegen die Nullhypothese für jeden der Kontraste einmal durchführen. Wir sehen, dass wir für alle Vergleiche die Nullhypothese ablehnen können, bis auf den Vergleich zwischen der Behandlung \\(B\\) und der Behandlung \\(C\\).\nIn der Abbildung 47.6 sehen wir die 95% Konfidenzintervalle für alle Vergleiche einmal dargestellt. Da wir es hier mit einem Mittelwertsvergleich zu tun haben, ist die Entscheidungsregel gegen die Nullhyppthese, dass wir ein signifikantes Konfidenzintervall vorliegen haben, wenn die Null nicht im Konfidenzintervall enthalten ist.\n\nggplot(res_crd_tbl, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1) + \n  geom_point() +\n  coord_flip() +\n  theme_bw()  +\n  labs(x = \"Vergleich\", y = \"Mittelwertsunterschied des Gewichtes [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die bereinigten Mittelwertsunterschiede mit 95% Konfidenzgrenzen dar.\n       Enthält ein 95% Konfidenzintervalle die 0 ist es nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 47.6— Abbildung der 95% Konfidenzintervallefür alle paarweisen Vergleiche der Behandlungsgruppen."
  },
  {
    "objectID": "experimental-design-basic.html#sec-rcbd",
    "href": "experimental-design-basic.html#sec-rcbd",
    "title": "47  Grundlagen der Versuchsplanung",
    "section": "\n47.3 Randomized complete block design (RCBD)",
    "text": "47.3 Randomized complete block design (RCBD)\nDas randomisierte, vollständige Blockdesign (eng. randomized complete block design) ist das Design, wenn es darum geht für verschiedene Räume die Varianz zu adjustieren bzw. zu modellieren. Was meinen wir mit Räumen? Wir meinen damit verschiedene Ställe, verschiedene Felder oder aber verschiedene Tische. Wir nennen diese zusätzlichen Beobachtungsräume auch Block.\n\n\n\n\n\n(a) Visualisierung des Randomized complete block design mit einer Behandlung und vier Behandlungsleveln. In jedem Block findet sich nur ein Behandlungslevel randomisiert wieder.\n\n\n\n\n\n\n(b) Visualisierung des Randomized complete block design mit einer Behandlung und vier Behandlungsleveln. In jedem Block finden wir mehrfach die Level der Behandlung. Im Prinzip ein Complete randomized design in mehreren Wiederholungen.\n\n\n\n\nAbbildung 47.7— Visualisierung der zwei Möglichkeiten ein Randomized complete block design zu konstruieren.\n\n\nWichtig ist zu unterschieden, wir pro Block nur einmal ein Level der Behandlung vorliegen haben. Dann hätten wir nämlich nur einen Topf mit Behandlung pro Block wie in Abbildung 47.7 (a) dargestellt. Damit haben wir den Block als Wiederholung. Oder wir haben ein Complete randomized design in Blöcken wiederholen vorliegen. Dann haben wir nämlich pro Block mehrere Wiederholungen der Behandlung wie in Abbildung 47.7 (b) veranschaulicht. Wir schauen uns erstmal den ersten Fall an. Das heist im Prinzip, dass unser Block die Wiederholung ist.\nHier ein paar Beispiele in Prosa, wie so ein Randomized complete block design konstruiert sein könnte.\nWir haben drei Tische und auf jeden der Tische steht zufällig vier ein Töpfe mit je einer Behandlung\nWir haben drei Ställe und in jedem Stall werden vier Buchten mit jeweils einer Behandlung genutzt.\nWir haben drei Felder mit jeweils vier Parzellen die zufällig mit jeweils einer der Behandlungen versehen werden.\nWir können natürlich auch auf den Tischen mehrere Wiederholungen einer Behandlung haben. Dann wird der Datensatz nur größer, aber die Auswertung unterschiedet sich nicht. Wir haben dann mehr Beobachtungen pro Block und Behandlung.\n\n47.3.1 Visualisierung\nIn der Abbildung 47.8 sehen wir eine Realisierung des Randomized complete block design. Wir haben insgesamt drei Blöcke vorliegen mit Block I, Block II und Block III. In jedem Block haben wir die Behandlungen \\(A\\), \\(B\\), \\(C\\) und \\(D\\) zufällig randomisiert. In jedem Block haben wir genau einmal ein Level der Behandlung vorliegen.\n\n\nAbbildung 47.8— Visualisierung des complete randomized design mit einer Behandlung und vier Behandlungsleveln.\n\n\n\n47.3.2 Daten\nIm Folgenden generieren wir uns die Daten für das Randomized complete block design. Wir wissen, dass in jedem Block die Behandlung genau einmal vorkommt. Um diese Datenstruktur mit zwei Faktoren nachzubauen, können wir die Funktion expand_grid() nutzen. Wir definieren zuerst, dass wir vier Behandlungslevel wollen und für jedes Behandlungslevel dann die drei Level des Blocks. Hier muss ich auch immer wieder rumspielen und probieren, bis ich die Daten dann zu dem Design passend habe. Wir erstellen uns so das Objekt factor_tbl.\n\nset.seed(20221001)\nfactor_tbl <- expand_grid(trt = 1:4, block = 1:3) %>% \n  mutate(trt = factor(trt, labels = c(\"A\", \"B\", \"C\", \"D\")),\n         block = factor(block, labels = as.roman(1:3))) \n\nfactor_tbl\n\n# A tibble: 12 × 2\n   trt   block\n   <fct> <fct>\n 1 A     I    \n 2 A     II   \n 3 A     III  \n 4 B     I    \n 5 B     II   \n 6 B     III  \n 7 C     I    \n 8 C     II   \n 9 C     III  \n10 D     I    \n11 D     II   \n12 D     III  \n\n\nWir sehen, dass jede Behandlung in allen drei Level des Blocks hat. Das entspricht unser Abbildung 47.8 und somit können wir uns darum kümmern, den Leveln der Behandlung und des Blocks einen Effekt zuzuweisen. Dafür brauchen wir die Modellmatrix, die beschreibt, wie sich für jede Beobachtung die Effekte zum Outcome rsp aufsummieren. Nicht jede Beobachtung ist in jedem Block in jeder Behandlung vertreten. Genau genommen hat jede Beobachtung nur eine einzige Behandlung/Block-Kombintation. Wir sehen diese Kombination dann in der Modellmatrix.\n\nmodel_mat <- factor_tbl %>% \n  model_matrix(~ trt + block) %>% \n  as.matrix()\n\nmodel_mat\n\n      (Intercept) trtB trtC trtD blockII blockIII\n [1,]           1    0    0    0       0        0\n [2,]           1    0    0    0       1        0\n [3,]           1    0    0    0       0        1\n [4,]           1    1    0    0       0        0\n [5,]           1    1    0    0       1        0\n [6,]           1    1    0    0       0        1\n [7,]           1    0    1    0       0        0\n [8,]           1    0    1    0       1        0\n [9,]           1    0    1    0       0        1\n[10,]           1    0    0    1       0        0\n[11,]           1    0    0    1       1        0\n[12,]           1    0    0    1       0        1\n\n\nWir sehen in der Modellmarix in jeder Zeile eine zukünftige Beobachtung. In den Spalten wird angegeben zu welchen Faktorleveln die Beobachtung gehört. Dabei bedeutet eine 1 ein Ja und eine 0 ein Nein. Die Beobachtung in der Zeile 5 wird zu Behandlungslevel \\(B\\) und Block \\(II\\) gehören.\nWir legen jetzt folgende Effekte für die einzelnen Behandlungslevel fest. Für den Intercept und damit auch für die Behandlung \\(A\\) auf \\(\\beta_{0} = \\beta_{A} = 20\\). Das Behandlunsglevel \\(B\\) wird auf \\(\\beta_{B} = 15\\), die Behandlung \\(C\\) auf \\(\\beta_{C} = 10\\) sowie die Behandlung \\(D\\) auf \\(\\beta_{D} = 5\\) gesetzt. Um die Sachlage zu vereinfachen setzen wir die Effekte der Blöcke auf \\(\\beta_{0} = \\beta_{I} = 0\\) sowie \\(\\beta_{II} = 0\\) und \\(\\beta_{III} = 0\\). Wir haben also faktisch keinen Effekt der Blöcke. Es ist egal welchen Tisch wir benutzen, die Effekte der Behandlung sind immer die Gleichen. Wenn wir die Daten so bauen würden, dann erhalten wir die Spalte rsp_eff in dem Datensatz rcbd_tbl. Wir haben keine Varianz. Deshalb müssen wir noch die Residuen mit \\(\\epsilon \\sim \\mathcal{N}(0, 2)\\) auf die Werte in der Spalte rsp_eff addieren. Wir erhalten die Spalte rsp für die Auswertung.\n\nrcbd_tbl <- factor_tbl %>% \n  mutate(rsp_eff = as.numeric(model_mat %*% c(20, 15, 10, 5, 0, 0)),\n         rsp = rsp_eff + rnorm(n(), 0, 2))\n\nrcbd_tbl\n\n# A tibble: 12 × 4\n   trt   block rsp_eff   rsp\n   <fct> <fct>   <dbl> <dbl>\n 1 A     I          20  19.8\n 2 A     II         20  17.6\n 3 A     III        20  18.2\n 4 B     I          35  34.4\n 5 B     II         35  33.9\n 6 B     III        35  32.3\n 7 C     I          30  28.4\n 8 C     II         30  29.5\n 9 C     III        30  29.5\n10 D     I          25  23.6\n11 D     II         25  24.9\n12 D     III        25  23.0\n\n\nIn Tabelle 47.1 sehen wir nochmal den Zusammenhang zwischen den generierten Daten und den entsprechenden berechneten Mittelwerten je Behandlungsgruppe. Wir berechnen den Mittelwert auf der Spalte rsp_eff. Wir sehen, dass wir die voreingestellten Mittelwerte in den Daten widerfinden.\n\n\n\n\nTabelle 47.1— Vergleich der Mittlwerte aus den Daten und den voreingestellten Effekten für die Generierung der Daten.\n\nFactor trt\nMean of level\nDifference to level A\nBeta\n\n\n\nA\n20\n0\n20\n\n\nB\n35\n15\n15\n\n\nC\n30\n10\n10\n\n\nD\n25\n5\n5\n\n\n\n\n\n\nAbschließend wollen wir uns die generierten Daten nochmal als einen Dotplot anschauen. Wir wollen dafür einen Dotplot nutzen, da wir mit drei Beobachtungen pro Level der Behandlung keinen sinnvollen Boxplot zeichnen können.\n\nggplot(rcbd_tbl, aes(trt, rsp, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", \n               position = position_dodge(width = 0.4)) +\n  ylim(15, 40) +\n  scale_fill_okabeito() +\n  labs(fill = \"Block\", x = \"Behandlung\", y = \"Outcome\")\n\n\n\nAbbildung 47.9— Dotplot der Level der Behandlungen aufgeteilt für die Level des Blocks.\n\n\n\n\nWir können die Daten aus dem Datensatz rcbd_tbl jetzt für die Varianzanalyse und Mittelwertsvergleich nutzen.\n\n47.3.3 Modellierung\nIm Folgenden wollen wir die Daten modellieren. Das heist wir wollen eine Linie durch eine multidimensionale Punktewolke zeichnen. Daher auch lineares Modell oder eben durch die Funktion lm() in R für linear model. Wir nutzen das Paket parameters und die Funktion model_parameters() um uns die Parameter des Modells auszugeben. Wir könnten auch die Funktion tidy() nutzen, aber wir erhalten durch die Funktion model_parameters() etwas mehr Informationen und bessere Spaltenüberschriften.\nWir bauen das Modell in folgender Form. Wir haben ein numerisches Outcome \\(y\\) sowie einen Faktor \\(f_1\\) sowie einem Faktor für den Block \\(b_1\\).\n\\[\ny \\sim f_1 + b_1\n\\]\nNun können wir das abstrakte Modell in die Daten übersetzen und erhalten folgendes Modell.\n\\[\nrsp \\sim trt + block\n\\]\nDas heist, unsere numerische Variable rsp hängt ab von unserer faktoriellen Variable trt und der faktoriellen Blockvariable block. Wir müssen immer wissen, wie die Spaltennamen in unserem Datensatz crd_tbl lauten sonst kann R die Spalten nicht finden.\n\nfit_rcbd <- lm(rsp ~ trt + block, rcbd_tbl)\n\nfit_rcbd %>%  model_parameters()\n\nParameter   | Coefficient |   SE |         95% CI |  t(6) |      p\n------------------------------------------------------------------\n(Intercept) |       18.79 | 0.70 | [17.08, 20.51] | 26.75 | < .001\ntrt [B]     |       15.03 | 0.81 | [13.04, 17.01] | 18.53 | < .001\ntrt [C]     |       10.57 | 0.81 | [ 8.59, 12.56] | 13.03 | < .001\ntrt [D]     |        5.29 | 0.81 | [ 3.30,  7.27] |  6.52 | < .001\nblock [II]  |       -0.05 | 0.70 | [-1.77,  1.67] | -0.08 | 0.942 \nblock [III] |       -0.77 | 0.70 | [-2.49,  0.94] | -1.10 | 0.313 \n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nWir sehen, dass wir die Koeffizienten, die wir vorher eingestellt haben, auch hier wiederfinden. Alle Steigungen der Behandlungslevel sind signifikant. Das hilft uns aber noch nicht so richtig weiter. Wir werden gleich das Modell in einer zweifaktoriellen ANOVA und einem Mittelwertsvergleich anschauen. Vorher wollen wir einmal statistisch Testen, ob die Varianzen homogens sind. Wir können die Varianzen aber nicht über das volle Modell testen, da wir nur eine Beobachtung per Behandlung/Block-Kombintation vorliegen haben.\n\nfit_rcbd %>% check_homogeneity()\n\nError in bartlett.test.default(x = mf[[1L]], g = mf[[2L]]) :  there must be at least 2 observations in each group\nDaher schauen wir uns nur die Varianzen für die Behandlung an und nehmen an, dass die Varanzen über die Blöcke homogen sind. Wir können nur einen Faktor testen und deshalb nehmen wir den für uns wichtigeren Faktor die Behandlung.\n\nlm(rsp ~ trt, rcbd_tbl) %>% check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.907).\n\n\nAbschließend schauen wir nochmal auf die Normalverteilung der Residuen.\n\nfit_rcbd %>% check_normality()\n\nOK: residuals appear as normally distributed (p = 0.391).\n\n\nIn der Abbildung 47.10 sehen wir den QQ-Plot und die Verteilung der Residuen im Densityplot. Auch die Visualisierung zeigt keine Aufälligkeiten. Wir sehen, dass die Residuen einer Normalverteilung folgen.\n\ncheck_model(fit_rcbd, check = c(\"qq\", \"normality\"))\n\n\n\nAbbildung 47.10— QQ-Plot und Densityplot der Residuen aus dem lineare Modell.\n\n\n\n\nWir können jetzt eine Varianzanalyse und dann eine Mittelwertsvergleich durchführen. Achtung, wir können uns hier auch etwas in die Ecke testen. Wenn wir nur lange genug neue Daten generieren, werden wir irgendwann auch einen Datensatz finden, der die Varianzhomogenität und die Normalverteilung ablehnt. Besonders in dem Fall, dass wir wenige Blöcke haben. Das liegt in der Theorie des statistischen Testens sowie der kleinen Fallzahl verborgen. Deshalb können wir im Zweifel gerne einmal deine Vortests in dem R Tutorium oder in einer statistischen Beratung diskutieren.\n\n47.3.4 Varianzanalyse und Mittelwertsvergleich\nAls erstes Rechnen wir eine zweifaktroielle ANOVA, da unser Modell zwei Faktoren hat. In R müssen wir dazu nur das Modell fit_rcbd in die Funktion anova() pipen. Wir erhalten dann die Ergebnisse aus der ANOVA mit der Funktion model_parameters() aus dem Paket parameters besser aufgearbeitet wieder. Die Mittelwertsunterschiede der Level der Behandlung haben wir bewusst sehr hoch angesetzt, so dass wir auf jeden Fall eine signifikante ANOVA erhalten sollen.\n\nres_anova <- fit_rcbd %>% \n  anova() \n\nres_anova %>% model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |      F |      p\n------------------------------------------------------------\ntrt       |      381.24 |  3 |      127.08 | 128.72 | < .001\nblock     |        1.50 |  2 |        0.75 |   0.76 | 0.509 \nResiduals |        5.92 |  6 |        0.99 |        |       \n\nAnova Table (Type 1 tests)\n\n\nAls Ergebnis haben wir einen signifikanten Faktor Behandlung trt sowie einen nicht signifikanten Faktor Block block. Wir können die Signifkanz an dem \\(p\\)-Wert bestimmen. Liegt der \\(p\\)-Wert unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5% so können wir die Nullhypothese ablehnen. Wir haben dann mindestens einen signifikanten paarweisen Mittelwertsunterschied vorliegen.\nSchauen wir uns nun noch den Anteil der erklärten Varianz an. Wir nutzen dafür den Effektschätzer \\(\\eta^2\\).\n\nres_anova %>% eta_squared(partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter |     Eta2 |       95% CI\n-----------------------------------\ntrt       |     0.98 | [0.93, 1.00]\nblock     | 3.85e-03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass durch den Faktor trt mit 92% der Varianz erklärt werden. Der Faktor Block erklärt nur ca. 2% der Varianz. Beides war so zu erwarten, denn wir haben ja auch den Datensatz in dieser Form gebaut. Die Behandlung hat einen starken Effekt und der Block hat gar keinen Effekt.\nSchauen wir nun auf den Mittelwertsvergleich. Wir nutzen dafür die Funktion emmeans() aus dem R Paket emmeans. Wichtig ist hier, dass wir uns jetzt die Vergleiche der Gruppen bzw. Level der Behandlung anschauen wollen.\n\nres_rcbd <- fit_rcbd %>% \n  emmeans(~ trt) \n\nAls erstes nutzen wir die Ausagbe der Funktion emmeans um uns das compact letter display wiedergeben zu lassen. Wir wollen wieder die Ausgaben runden und nutzen die Adjustierung der \\(p\\)-Werte für multiple Vergleiche nach Bonferroni. Nochmal als Erinnerung, das compact letter display gibt uns keine \\(p\\)-Werte wieder sondern wir Entscheiden anhand der vergebenen Buchstaben und deren Gleichheit über ein signifikantes Ergebnis oder ein nicht signifikantes Ergebnis.\n\nres_rcbd_cld <- res_rcbd %>% \n  cld(adjust = \"bonferroni\", Letters = letters) %>% \n  tidy() %>% \n  select(trt, estimate, conf.low, conf.high, .group) %>% \n  mutate(across(where(is.numeric), round, 2))\n\nres_rcbd_cld \n\n# A tibble: 4 × 5\n  trt   estimate conf.low conf.high .group \n  <chr>    <dbl>    <dbl>     <dbl> <chr>  \n1 A         18.5     16.5      20.5 \" a   \"\n2 D         23.8     21.8      25.8 \"  b  \"\n3 C         29.1     27.1      31.1 \"   c \"\n4 B         33.6     31.5      35.6 \"    d\"\n\n\nAn dem compact letter display sehen wir, dass sich alle Mittelwerte der Level der Behandlungen signifikant unterscheiden. In Abbildung 47.11 sehen wir die Daten zusammen mit dem compact letter display in einer Abbildung. Wir ändern hier das geom_point() zu geom_jitter() um ein Overplotting zu vermeiden. So können wir alle Beobachtungen als Punkte erkennen.\n\nggplot() +\n  theme_bw() +\n  geom_jitter(data = rcbd_tbl, aes(x = trt, y = rsp, fill = trt),\n              width = 0.05) +\n  geom_text(data = res_rcbd_cld, \n            aes(x = trt , y = estimate, label = .group),\n            position = position_nudge(x = 0.2), color = \"red\") +\n  geom_errorbar(data = res_rcbd_cld,\n                aes(ymin = conf.low, ymax = conf.high, x = trt),\n                color = \"red\", width = 0.1,\n                position = position_nudge(x = 0.1)) +\n  geom_point(data = res_rcbd_cld, \n             aes(x = trt , y = estimate),\n             position = position_nudge(x = 0.1), color = \"red\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"Behandlung\", y = \"Gewicht [kg/ha]\",\n       caption = \"Schwarze Punkte stellen Rohdaten dar.\n       Rote Punkte und Fehlerbalken stellen bereinigte Mittelwerte mit 95% Konfidenzgrenzen pro Behandlung dar.\n       Mittelwerte, mit einem gemeinsamen Buchstaben, sind nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 47.11— Scatterplot der Behandlungsgruppen zusammen mit den 95% Konfidenzintervall und dem compact letter display.\n\n\n\n\nHäufig wollen wir nicht nur das compact letter display sehen sondern auch die dazugehörigen \\(p\\)-Werte und die entsprechenden 95% Konfidenzintervalle. Wir berechnen im Folgenden alle paarweisen Vergleiche bzw. Kontraste und lassen uns die adjustierten sowie formatierten \\(p\\)-Werte ausgeben. Wir runden wieder die Ausgabe.\n\nres_rcbd_tbl <- res_rcbd %>% \n  contrast(method = \"pairwise\") %>% \n  tidy(conf.int = TRUE) %>% \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) %>% \n  select(contrast, estimate, p.value,\n         conf.low, conf.high) \n\nres_rcbd_tbl\n\n# A tibble: 6 × 5\n  contrast estimate p.value conf.low conf.high\n  <chr>       <dbl> <chr>      <dbl>     <dbl>\n1 A - B      -15.0  <0.001    -17.8     -12.2 \n2 A - C      -10.6  <0.001    -13.4      -7.76\n3 A - D       -5.29 0.003      -8.1      -2.48\n4 B - C        4.46 0.006       1.65      7.26\n5 B - D        9.74 <0.001      6.93     12.6 \n6 C - D        5.29 0.003       2.48      8.09\n\n\nAuch hier passen die \\(p\\)-Werte zu dem compact letter display. Alle Vergleiche sind signifikant. Das haben wir noch dem compact letter display auch so erwartet. Auch sehen wir das gleiche Ergebnis in Abbildung 47.12 für die 95% Konfidenzintervalle. Wir betrachten Mittelwertsunterschiede und kein Konfidenzintervall beinhaltet die Null somit sind alle Konfidenzintervalle signifikant.\n\nggplot(res_rcbd_tbl, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1) + \n  geom_point() +\n  coord_flip() +\n  theme_bw()  +\n  labs(x = \"Vergleich\", y = \"Mittelwertsunterschied des Gewichtes [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die bereinigten Mittelwertsunterschiede mit 95% Konfidenzgrenzen dar.\n       Enthält ein 95% Konfidenzintervalle die 0 ist es nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 47.12— Abbildung der 95% Konfidenzintervallefür alle paarweisen Vergleiche der Behandlungsgruppen."
  },
  {
    "objectID": "experimental-design-basic.html#sec-lsd",
    "href": "experimental-design-basic.html#sec-lsd",
    "title": "47  Grundlagen der Versuchsplanung",
    "section": "\n47.4 Latin square design (LSD)",
    "text": "47.4 Latin square design (LSD)\n\n47.4.1 Visualisierung\n\n\nAbbildung 47.13— Visualisierung des latin square design mit einer Behandlung und vier Behandlungsleveln.\n\n\n\n47.4.2 Daten\n\nexpand_grid(trt = 1:4, block = 1:4)\n\n# A tibble: 16 × 2\n     trt block\n   <int> <int>\n 1     1     1\n 2     1     2\n 3     1     3\n 4     1     4\n 5     2     1\n 6     2     2\n 7     2     3\n 8     2     4\n 9     3     1\n10     3     2\n11     3     3\n12     3     4\n13     4     1\n14     4     2\n15     4     3\n16     4     4\n\n\n\n47.4.3 Modellierung\n\n47.4.4 Varianzanalyse und Mittelwertsvergleich"
  },
  {
    "objectID": "experimental-design-basic.html#sec-alpha",
    "href": "experimental-design-basic.html#sec-alpha",
    "title": "47  Grundlagen der Versuchsplanung",
    "section": "\n47.5 Alpha design",
    "text": "47.5 Alpha design\n\n47.5.1 Visualisierung\n\n\n\nAbbildung 47.14— Visualisierung des alpha design mit einer Behandlung und vier Behandlungsleveln und zwölf unvollständigen Blöcken sowie vier Wiederholungen.\n\n\n\n\n47.5.2 Daten\n\n47.5.3 Modellierung\n\n47.5.4 Varianzanalyse und Mittelwertsvergleich"
  },
  {
    "objectID": "experimental-design-basic.html#sec-augment",
    "href": "experimental-design-basic.html#sec-augment",
    "title": "47  Grundlagen der Versuchsplanung",
    "section": "\n47.8 Augmented design",
    "text": "47.8 Augmented design"
  },
  {
    "objectID": "experimental-design-basic.html#sec-split",
    "href": "experimental-design-basic.html#sec-split",
    "title": "47  Grundlagen der Versuchsplanung",
    "section": "\n47.7 Split plot design",
    "text": "47.7 Split plot design\n\n47.7.1 Visualisierung\n\n\n\nAbbildung 47.15— Visualisierung des split plot design mit einer Behandlung und vier Behandlungsleveln sowie einer zweiten Behandlung mit fünf Behandlungsleveln. Die erste Behandlung ist über die drei Blöcke randomisiert.\n\n\n\n\n47.7.2 Daten\n\ndata_tbl <- expand_grid(trt = 1:4, block = 1:4, rep = 1:5) %>% \n    mutate(rsp = 20 + 2.5 * trt + 1.5 * block + rnorm(n(), 0, 1),\n           trt = factor(trt, labels = c(\"ctrl\", \"A\", \"B\", \"C\")),\n           block = factor(block, labels = as.roman(1:4)),\n           rep = as_factor(rep))\n\n\n47.7.3 Modellierung\n\n47.7.4 Varianzanalyse und Mittelwertsvergleich"
  },
  {
    "objectID": "classification-basic.html#weitere-valdierungen",
    "href": "classification-basic.html#weitere-valdierungen",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.10 Weitere Valdierungen",
    "text": "49.10 Weitere Valdierungen\nNeben den hier vorgestellten Varianten gibt es noch weitere Möglichkeiten in dem R Paket rsample sich Stichprobendatensätze zu generieren. Wir gehen jetzt hier nicht mehr im Detail auf die verschiedenen Möglichkeiten ein. Dafür dann einfach die Links auf die rsample Hilfeseite nutzen.\n\n\nStratifiziertes Resampling nutzen wir, wenn wir eine Gruppe in den Daten haben, die nicht gleichmäßig über die Daten verteilt ist. Das heißt, wir haben ein nicht balanciertes Design. Kann plaktiv wäre das der Fall, wenn wir fast nur Frauen oder Männer in unseren Daten vorliegen hätten. Hier kann es dann passieren, dass wir zufällig Datensätze ziehen, die nur Frauen oder nur Männer beinhalten. Das wollen wir natürlich verhindern.\n\nGruppiertes Resampling nutzen wir, wenn wir korrelierte Beobachtungen haben. Oft sind einige Beobachtungen in deinen Daten ähnlicher als es der Zufall vermuten ließe, z. B. weil sie wiederholte Messungen desselben Probanden darstellen oder alle an einem einzigen Ort gesammelt wurden. Dann müssen wir eventuell auch hierfür das Resampling anpassen.\n\nZeitpunkt basiertes Resampling sind in dem Sinne eine besonderheit, da wir natürlich berücksichtigen müssen, wann eine Beobachtung im zeitlichen Verlauf gemacht wurde. Hier hat die Zeit einen Einfluss auf das Resampling.\n\nAm Ende musst du entscheiden, welche der Resamplingmethoden für dich am besten geeignet ist. Wir müssen eben einen Trainingsdatensatz und einen Testdatensatz haben. Der Rest dient dann zum Tuning deiner Modelle."
  },
  {
    "objectID": "classification-basic.html#supervised-vs.-unsupervised",
    "href": "classification-basic.html#supervised-vs.-unsupervised",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.11 Supervised vs. unsupervised",
    "text": "49.11 Supervised vs. unsupervised\nDer Unterschied zwischen einer suprvised Lernmethode oder Algorithmus ist, dass das Label bekannt ist. Das heißt, dass wir in unseren Daten eine \\(y\\) Spalte haben an der wir unser Modell dann trainieren können. Das Modell weiß also an was es sich optimieren soll. In Tabelle 49.2 sehen wir einen kleinen Datensatz in einem supervised Setting. Wir haben ein \\(y\\) in den Daten und können an diesem Label unser Modell optimieren.\n\n\nTabelle 49.2— Beispieldatensatz für supervised learning. Unsere Daten haben eine Spalte \\(y\\), die wir als Label in unserem Modell nutzen können.\n\n\\(y\\)\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n1\n0.2\n1.3\n1.2\n\n\n0\n0.1\n0.8\n0.6\n\n\n1\n0.3\n2.3\n0.9\n\n\n1\n0.2\n9.1\n1.1\n\n\n\n\nIn der Tabelle 49.3 sehen wir als Beispiel einen Datensatz ohne eine Spalte, die wir als Label nutzen können. Nazürlich haben wir in echt dann keine freie Spalte. Ich habe das einmal so gebaut, damit du den Unterschied besser erkennen kannst. Beim unsuoervised Lernen muss der Algorithmus sich das Label selber bauen. Wir müssen meist vorgeben, wie viele Gruppen wir im Label erwarten würden. Dann können wir den Algorithmus starten.\n\n\nTabelle 49.3— Beispieldatensatz für unsupervised learning. Unsere Daten haben keine Spalte \\(y\\), die wir als Label in unserem Modell nutzen können.\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n\\(\\phantom{0}\\)\n0.2\n1.3\n1.2\n\n\n\n0.1\n0.8\n0.6\n\n\n\n0.3\n2.3\n0.9\n\n\n\n0.2\n9.1\n1.1\n\n\n\n\nWir haben sehr oft eine superised Setting vorliegen. Aber wie immer, du wirst vielleicht auch Cluster bilden wollen und dann ist das unsupervised Lernen eine Methode, die du gut nutzen kannst."
  },
  {
    "objectID": "classification-basic.html#normalisierung",
    "href": "classification-basic.html#normalisierung",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.18 Normalisierung",
    "text": "49.18 Normalisierung\nUnter Normalisierung der Daten fassen wir eigentlich ein preprocessing der Daten zusammen. Wir haben ja unsere Daten in einer ursprünglichen Form vorliegen. Häufig ist diese Form nicht geeignet um einen maschinellen Lernalgorithmus auf diese ursprüngliche Form der Daten anzuwenden. Deshalb müssen wir die Daten vorher einmal anpassen und in eine geleiche Form über alle Variablen bringen. Was meine ich so kryptisch damit? Schauen wir uns einmal in der Tabelle 49.7 ein Beispiel für zu normalisierende Daten an.\n\n\nTabelle 49.7— Beispieldatensatz für einen Datensatz der nomiert werden muss. Die einzelenen Spalten haben sehr unterschiedliche Wertebereiche eingetragen.\n\n\\(y\\)\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n1\n0.2\n1430\n23.54\n\n\n0\n0.1\n1096\n18.78\n\n\n1\n0.4\n2903\n16.89\n\n\n1\n0.2\n7861\n12.98\n\n\n\n\nWarum müssen diese Daten normalisiert werden? Wir haben mit \\(x_1\\) eine Variable vorliegen, die im Iterval \\([0;1]\\) liegt. Die Variable \\(x_2\\) liegt in einem zehntausendfach größeren Wertebereich. Die Werte der Variable \\(x_3\\) ist auch im Vergleich immer noch hundertfach im Wertebereich unterschiedlich. Dieser großen Unterschiede im Wertebereich führen zu fehlern bei Modellieren. Wir können hierzu das Kapitel 17 betrachten. Dort werden gängige Transformationen einmal erklärt. Wir gehen hier nicht nochmal auf alle Verfahren ein, sondern konzentrieren uns auf die häufigsten Anwendungen."
  },
  {
    "objectID": "classification-basic.html#bias-vs.-varianz",
    "href": "classification-basic.html#bias-vs.-varianz",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.14 Bias vs. Varianz",
    "text": "49.14 Bias vs. Varianz\n\\[\nerror = variance + bias + \\epsilon\n\\]\nhttps://towardsdatascience.com/quick-bias-variance-trade-off-d4895b126b08"
  },
  {
    "objectID": "classification-knn.html#genutzte-r-pakete-für-das-kapitel",
    "href": "classification-knn.html#genutzte-r-pakete-für-das-kapitel",
    "title": "50  \\(k\\) nearest neighbor",
    "section": "\n50.1 Genutzte R Pakete für das Kapitel",
    "text": "50.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, conflicted,\n               caret, kknn)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\nconflict_prefer(\"fit\", \"parsnip\")\nconflict_prefer(\"contr.dummy\", \"kknn\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-knn.html#daten",
    "href": "classification-knn.html#daten",
    "title": "50  \\(k\\) nearest neighbor",
    "section": "\n50.2 Daten",
    "text": "50.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt.\n\npig_tbl <- read_excel(\"data/infected_pigs.xlsx\") %>% \n  mutate(pig_id = 1:n(),\n         infected = as_factor(infected)) %>% \n  select(pig_id, infected, everything())  \n\nIn Tabelle 50.1 siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 50.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npig_id\ninfected\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\n\n\n\n1\n1\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n49.88\n16.94\n3.07\n\n\n2\n0\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n58.2\n17.95\n4.88\n\n\n3\n0\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n56.8\n19.02\n3.98\n\n\n4\n0\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.47\n18.98\n5.18\n\n\n5\n1\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n59.85\n16.57\n6.71\n\n\n6\n1\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n58.1\n18.22\n5.43\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\nfemale\nnorth\n11.82\n21.5\nrobust\n57.05\n17.95\n6.16\n\n\n408\n1\n56\nmale\nwest\n13.91\n20.8\npre-frail\n50.84\n18.02\n6.52\n\n\n409\n1\n57\nmale\nnorthwest\n12.49\n21.95\nrobust\n55.51\n17.73\n3.94\n\n\n410\n1\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n58.5\n18.23\n2.73\n\n\n411\n1\n59\nfemale\nnorth\n13.13\n20.23\npre-frail\n57.33\n17.21\n5.42\n\n\n412\n1\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n55.85\n17.76\n6.18\n\n\n\n\n\n\nGehen wir jetzt mal die Wörter und Begrifflichkeiten, die wir für das maschinelle Lernen später brauchen einmal durch.\n\n\nAbbildung 50.1— Dars.\n\n\n\n\nAbbildung 50.2— Dars.\n\n\n\n\nAbbildung 50.3— Dars.\n\n\n\n\nAbbildung 50.4— Dars."
  },
  {
    "objectID": "app-example-analysis.html",
    "href": "app-example-analysis.html",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "",
    "text": "Version vom November 20, 2022 um 17:21:13\nWorum geht es in diesem Kapitel? Ich rechne hier fröhlich Dinge und präsentiere dann die Ergebnisse. Das heißt, du findest hier beispielhafte Auswertungen, die eventuell auch deine Problemstellung betreffen.\nJe weiter du nach unten in diesem Kapitel kommst, desto wilder wird der R Code. Ich werde noch eine Zeit brauchen, bis ich alles wieder schon mit Text hier verarbeitet habe. Es wird aber immer mal wieder etwas messy aussehen. Hier wird eben auch gearbeitet."
  },
  {
    "objectID": "app-example-analysis.html#genutzte-r-pakete-für-das-kapitel",
    "href": "app-example-analysis.html#genutzte-r-pakete-für-das-kapitel",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.1 Genutzte R Pakete für das Kapitel",
    "text": "A.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, \n               broom, multcomp, emmeans, \n               conflicted, effectsize, report,\n               see, metR, parameters,\n               modelsummary)\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "app-example-analysis.html#isoplethendiagramm-für-münster-osnabrück",
    "href": "app-example-analysis.html#isoplethendiagramm-für-münster-osnabrück",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.2 Isoplethendiagramm für Münster & Osnabrück",
    "text": "A.2 Isoplethendiagramm für Münster & Osnabrück\nIm Folgenden zeige ich ein Beispiel für die Nutzung der entgeltfreien Informationen auf der DWD-Website. Wir finden dort auf der Seite die Klimadaten für Deutschland und natürlich auch die Daten für Münster/Osnabrück. Ich habe mir flux die Tageswerte runtergeladen und noch ein wenig den Header der txt-Datei angepasst. Du findest die Datei day_values_osnabrueck.txt wie immer auf meiner GitHub Seite. Du musst dir für andere Orte die Daten nur entsprechend zusammenbauen. Am Ende brauchen wir noch die Informationen zu den Tages- und Monatswerten damit wir auch verstehen, was wir uns da von der DWD runtergeladen haben. Ich nutze gleich nur einen Ausschnitt aus den Daten.\n\n\nWenn wir Geocomputation with R machen wollen, dann haben wir natürlich noch viele andere Möglichkeiten. Das verlinkte Buch hilft da weiter.\nDann lesen wir die Daten einmal ein und müssen dann eine Winkelzüge machen, damit wir aus dem Datum JJJJMMDD dann jeweils den Monat und den Tag extrahiert kriegen. Dann müssen wir die Monatszahl und die Tageszahl noch in eine Zahl umwandeln. Sonst geht es schlecht mit dem Zeichnen des Konturplots. Wir nehmen dann die Temperaturen TG, TN, TM und TX um diese Temperaturen in vier Konturplots zu zeigen.\n\nweather_tbl <- read_table(\"data/day_values_osnabrueck.txt\") %>% \n  mutate(JJJJMMDD = as.Date(as.character(JJJJMMDD), \"%Y%m%d\"),\n         day = as.numeric(format(JJJJMMDD, \"%d\")), \n         month = as.numeric(format(JJJJMMDD, \"%m\")), \n         year = as.numeric(format(JJJJMMDD, \"%Y\"))) %>% \n  select(month, day, TG, TN, TM, TX) %>% \n  na.omit() %>% \n  gather(temp, grad, TG:TX) %>% \n  mutate(temp = factor(temp, \n                       labels = c(\"Minimum der Temperatur in 5 cm (TG)\",\n                                  \"Minimum der Temperatur in 2 m (TN)\",\n                                  \"Mittel der Temperatur in 2 m (TM)\",\n                                  \"Maximum der Temperatur in 2 m (TX)\")))\n\nNachdem wir ordentlich an den Daten geschraubt haben können wir jetzt in Abbildung A.1 die vier Konturplots sehen. Wir mussten noch das Spektrum der Farben einmal drehen, damit es auch mit den Temperaturfarben passt und wir haben noch ein paar Hilfslinien miteingezeichnet.\n\nggplot(weather_tbl, aes(month, day, z = grad)) +\n  theme_minimal() +\n  geom_contour_filled(bins = 13) +\n  geom_contour(binwidth = 2, color = \"black\") +\n  facet_wrap(~ temp, ncol = 2) + \n  scale_fill_brewer(palette = \"Spectral\", direction = -1) +\n  scale_x_continuous(breaks = 1:12) +\n  geom_vline(xintercept = 1:12, alpha = 0.9, linetype = 2) +\n  geom_hline(yintercept = c(5, 10, 15, 20, 25, 30), \n             alpha = 0.9, linetype = 2)\n\n\n\nAbbildung A.1— Konturplot der verschiedenen Temperaturen."
  },
  {
    "objectID": "app-example-analysis.html#analyse-von-anzahlen",
    "href": "app-example-analysis.html#analyse-von-anzahlen",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.3 Analyse von Anzahlen",
    "text": "A.3 Analyse von Anzahlen\nIn dieser sehr simplen Analyse haben wir zwei Gruppen vorliegen. Die Gruppe 1 ist hat zwei Level oder Behandlungen abgekürzt mit I und II. Die Gruppe 2 hat insgesamt vier Level oder eben Behandlungen, die wir mit A, B, C und D bezeichnen. Wir haben jetzt für die jeweiligen Kombinationen auf dem Feld etwas gezählt. Wir haben also für jede dieser Kombinationen nur eine Zahl. Es ergbit sich somit die folgende Matrix an Zahlen.\n\nrel_mat <- matrix(c(45, 14, 4, 0,\n                    25, 32, 5, 1), nrow = 2, byrow = TRUE,\n                  dimnames = list(c(\"I\", \"II\"), c(\"A\", \"B\", \"C\", \"D\")))\nrel_mat\n\n    A  B C D\nI  45 14 4 0\nII 25 32 5 1\n\n\nNun können wir den \\(\\mathcal{X}^2\\)-Test nutzen, um zu testen, ob die Zahlen in der Matrix bzw. auf unseren Feld gelcihverteilt sind. Die Nullhypothese lautet, dass es keinen Zusammenhang zwischen der Gruppe 1 und der Gruppe 2 auf dem Feld gibt. Die Zahlen sind also rein zufällig in dieser Anordnung.\n\nchisq.test(rel_mat)\n\n\n    Pearson's Chi-squared test\n\ndata:  rel_mat\nX-squared = 13.8689, df = 3, p-value = 0.0030892\n\n\nWir erhalten einen sehr kleinen \\(p\\)-Wert mit \\(0.003\\). Wir können daher die Nullhypothese ablehnen, da der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) mit 5%. Wir haben ein signifikantes Ergebnis. Wir können von einen Zusamenhang zwischen den beiden Gruppen ausgehen.\nMit Cramers V können wir auch noch die Effektstärke für einen \\(\\mathcal{X}^2\\)-Test berechnen.\n\ncramers_v(rel_mat) \n\nCramer's V |       95% CI\n-------------------------\n0.33       | [0.15, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDer Effekt ist mit \\(0.33\\) nicht besonders stark. Du kannst Cramers V wie die Korrelation interpretieren. Ein V von 0 bedeutet keinen Zusammenhang und ein V von 1 einen maximalen Zusammenhang. Wir wollen uns die Daten dann nochmal in einer Abbidlung anschauen. Dafür müssen wir die Matrix erstmal in einen Datensatz umwandeln und die Gruppen zu Faktoren machen.\n\nplot_tbl <- rel_mat %>% \n  as_tibble(rownames = \"group1\") %>% \n  gather(A:D, key = \"group2\", value = \"value\") %>% \n  mutate(group1 = as_factor(group1),\n         group2 = as_factor(group2))\n\nIn Abbildung A.2 sehen wir die Matrix der Zähldaten für die beiden Gruppen nochmal visualisiert. Beim betrachten fällt auf, dass die beiden Level C und D kaum Zähldaten enthalten. Hier wäre zu überlegen die beiden Level aus der Analyse herauszunehmen und einen klassischen \\(\\mathcal{X}^2\\)-Test auf einer 2x2 Kreuztabelle zu rechnen.\n\nggplot(plot_tbl, aes(x = group2, y = value, fill = group1)) +\n  theme_bw() +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Gruppe 2\", y = \"Anzahl\", fill = \"Gruppe 1\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung A.2— Barplot der Zähldaten aus der Matrix."
  },
  {
    "objectID": "app-example-analysis.html#auswertung-zweifaktorielle-anova-mit-interaktion",
    "href": "app-example-analysis.html#auswertung-zweifaktorielle-anova-mit-interaktion",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.4 Auswertung zweifaktorielle ANOVA mit Interaktion",
    "text": "A.4 Auswertung zweifaktorielle ANOVA mit Interaktion\n\nlight_tbl <- read_excel(\"data/light_intensity_data.xlsx\") %>% \n  mutate(rack = factor(rack, labels = c(\"left\", \"middle\", \"right\")),\n         layer = factor(layer, labels = c(\"1st\", \"2nd\", \"3rd\")),\n         light_intensity = factor(light_intensity, labels = c(\"low\", \"mid\", \"high\")),\n         growth = as.numeric(growth))\n\n\nggplot(light_tbl, aes(light_intensity, growth, fill = rack)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito()\n\n\n\n\n\nggplot(light_tbl, aes(light_intensity, growth, fill = layer)) +\n  theme_bw() +\n  geom_boxplot()  +\n  scale_fill_okabeito()\n\n\n\n\n\nggplot(light_tbl, aes(light_intensity, growth, fill = rack)) +\n  theme_bw() +\n  geom_boxplot() +\n  facet_wrap(~ layer)  +\n  scale_fill_okabeito()\n\n\n\n\n\nfit_1 <- lm(growth ~ light_intensity + layer + light_intensity:layer, \n            data = light_tbl)\nfit_1 %>% model_parameters()\n\nParameter                            | Coefficient |   SE |           95% CI | t(45) |      p\n---------------------------------------------------------------------------------------------\n(Intercept)                          |       11.62 | 2.80 | [  5.97,  17.26] |  4.14 | < .001\nlight intensity [mid]                |        1.43 | 3.96 | [ -6.55,   9.42] |  0.36 | 0.719 \nlight intensity [high]               |        3.70 | 3.96 | [ -4.28,  11.68] |  0.93 | 0.356 \nlayer [2nd]                          |        5.68 | 3.96 | [ -2.30,  13.67] |  1.43 | 0.159 \nlayer [3rd]                          |       14.05 | 3.96 | [  6.07,  22.03] |  3.54 | < .001\nlight intensity [mid] * layer [2nd]  |       -3.33 | 5.61 | [-14.62,   7.96] | -0.59 | 0.555 \nlight intensity [high] * layer [2nd] |       -5.92 | 5.61 | [-17.21,   5.37] | -1.06 | 0.297 \nlight intensity [mid] * layer [3rd]  |       -9.60 | 5.61 | [-20.89,   1.69] | -1.71 | 0.094 \nlight intensity [high] * layer [3rd] |      -25.90 | 5.61 | [-37.19, -14.61] | -4.62 | < .001\n\n\n\ncomp_1_obj <- fit_1 %>% \n  emmeans(specs = ~ light_intensity | layer) %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\n\ncomp_1_obj %>% \n  summary %>% \n  as_tibble %>% \n  select(contrast, layer, p.value) %>% \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2))\n\n# A tibble: 9 × 3\n  contrast   layer p.value\n  <fct>      <fct> <chr>  \n1 low - mid  1st   1.0000 \n2 low - high 1st   1.0000 \n3 mid - high 1st   1.0000 \n4 low - mid  2nd   1.0000 \n5 low - high 2nd   1.0000 \n6 mid - high 2nd   1.0000 \n7 low - mid  3rd   0.1355 \n8 low - high 3rd   <0.001 \n9 mid - high 3rd   0.0028 \n\n\n\nci_obj <- comp_1_obj %>% \n  confint() %>% \n  as_tibble() %>% \n  select(contrast, layer, estimate, conf.low = lower.CL, conf.high = upper.CL) \n\nci_obj\n\n# A tibble: 9 × 5\n  contrast   layer estimate conf.low conf.high\n  <fct>      <fct>    <dbl>    <dbl>     <dbl>\n1 low - mid  1st     -1.43    -11.3       8.42\n2 low - high 1st     -3.70    -13.6       6.16\n3 mid - high 1st     -2.27    -12.1       7.59\n4 low - mid  2nd      1.90     -7.96     11.8 \n5 low - high 2nd      2.22     -7.64     12.1 \n6 mid - high 2nd      0.317    -9.54     10.2 \n7 low - mid  3rd      8.17     -1.69     18.0 \n8 low - high 3rd     22.2      12.3      32.1 \n9 mid - high 3rd     14.0       4.18     23.9 \n\n\n\nggplot(ci_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high,\n                   color = layer, group = layer)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n  geom_point(position = position_dodge(0.5)) +\n  scale_color_okabeito() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\nfit_1 %>% \n  emmeans(specs = ~ light_intensity | layer)  %>%\n  cld(Letters = letters, adjust = \"bonferroni\") \n\nlayer = 1st:\n light_intensity emmean  SE df lower.CL upper.CL .group\n low              11.62 2.8 45     4.65     18.6  a    \n mid              13.05 2.8 45     6.08     20.0  a    \n high             15.32 2.8 45     8.35     22.3  a    \n\nlayer = 2nd:\n light_intensity emmean  SE df lower.CL upper.CL .group\n high             15.08 2.8 45     8.11     22.1  a    \n mid              15.40 2.8 45     8.43     22.4  a    \n low              17.30 2.8 45    10.33     24.3  a    \n\nlayer = 3rd:\n light_intensity emmean  SE df lower.CL upper.CL .group\n high              3.47 2.8 45    -3.50     10.4  a    \n mid              17.50 2.8 45    10.53     24.5   b   \n low              25.67 2.8 45    18.70     32.6   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: bonferroni method for 3 tests \nsignificance level used: alpha = 0.05 \nNOTE: Compact letter displays can be misleading\n      because they show NON-findings rather than findings.\n      Consider using 'pairs()', 'pwpp()', or 'pwpm()' instead."
  },
  {
    "objectID": "app-example-analysis.html#auswertung-von-gewichten",
    "href": "app-example-analysis.html#auswertung-von-gewichten",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.5 Auswertung von Gewichten",
    "text": "A.5 Auswertung von Gewichten\n\n\n\n\n\n\n\ntrt\nblock\nrep\nrsp\n\n\n\nlow\nI\n1\n13.19\n\n\nlow\nI\n2\n13.61\n\n\nlow\nI\n3\n11.61\n\n\nlow\nI\n4\n13.11\n\n\nlow\nII\n1\n17.94\n\n\nlow\nII\n2\n14.46\n\n\nlow\nII\n3\n12.45\n\n\nlow\nII\n4\n16.67\n\n\nlow\nIII\n1\n14.52\n\n\nlow\nIII\n2\n13.88\n\n\nlow\nIII\n3\n15.87\n\n\nlow\nIII\n4\n13.31\n\n\nmid\nI\n1\n16.23\n\n\nmid\nI\n2\n15.02\n\n\nmid\nI\n3\n16.59\n\n\nmid\nI\n4\n15.12\n\n\nmid\nII\n1\n16.81\n\n\nmid\nII\n2\n10.40\n\n\nmid\nII\n3\n16.45\n\n\nmid\nII\n4\n13.42\n\n\nmid\nIII\n1\n16.89\n\n\nmid\nIII\n2\n15.38\n\n\nmid\nIII\n3\n17.77\n\n\nmid\nIII\n4\n15.82\n\n\nhigh\nI\n1\n15.21\n\n\nhigh\nI\n2\n13.87\n\n\nhigh\nI\n3\n16.88\n\n\nhigh\nI\n4\n17.70\n\n\nhigh\nII\n1\n15.39\n\n\nhigh\nII\n2\n16.94\n\n\nhigh\nII\n3\n17.70\n\n\nhigh\nII\n4\n19.68\n\n\nhigh\nIII\n1\n17.42\n\n\nhigh\nIII\n2\n20.82\n\n\nhigh\nIII\n3\n17.80\n\n\nhigh\nIII\n4\n17.55\n\n\n\n\n\n\nA.5.1 Explorative Datenanalyse (EDA)\n\nggplot(data_tbl, aes(trt, rsp, color = block)) +\n  geom_boxplot()\n\n\n\n\n\nstat_tbl <- data_tbl %>% \n  group_by(trt, block) %>% \n  summarise(mean = mean(rsp),\n            sd = sd(rsp),\n            se = sd/sqrt(n()))\n\nggplot(stat_tbl, aes(x = trt, y = mean, fill = block)) + \n    geom_bar(position = position_dodge(), stat = \"identity\") +\n    geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                  width = 0.2,\n                  position = position_dodge(.9))\n\n\n\n\n\nA.5.2 Lineares Modell\n\nfit_1 <- lm(rsp ~ trt + block, data = data_tbl)\n\n\nA.5.3 ANOVA\n\nfit_1 %>% anova\n\nAnalysis of Variance Table\n\nResponse: rsp\n          Df   Sum Sq  Mean Sq F value   Pr(>F)   \ntrt        2  55.4758 27.73791 8.39882 0.001217 **\nblock      2  14.8976  7.44882 2.25544 0.121759   \nResiduals 31 102.3804  3.30260                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nA.5.4 Gruppenvergleich mit dem multcomp Paket\nhttps://broom.tidymodels.org/reference/tidy.glht.html\n\nfit_1 %>% \n  glht(linfct = mcp(trt = \"Tukey\")) %>% \n  tidy %>% \n  select(contrast, estimate, adj.p.value) %>% \n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 3 × 3\n  contrast   estimate adj.p.value\n  <chr>         <dbl>       <dbl>\n1 mid - low      1.27      0.215 \n2 high - low     3.03      0.0008\n3 high - mid     1.75      0.0619\n\n\n\nA.5.5 Gruppenvergleich mit der emmeans Paket\nhttps://broom.tidymodels.org/reference/tidy.emmGrid.html\n\nfit_1 %>% \n  emmeans(\"trt\") %>% \n  contrast(method = \"pairwise\") %>% \n  tidy %>% \n  select(contrast, estimate, adj.p.value) %>% \n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 3 × 3\n  contrast   estimate adj.p.value\n  <chr>         <dbl>       <dbl>\n1 low - mid     -1.27      0.215 \n2 low - high    -3.03      0.0008\n3 mid - high    -1.75      0.0618"
  },
  {
    "objectID": "app-example-analysis.html#auswertung-von-boniturnoten",
    "href": "app-example-analysis.html#auswertung-von-boniturnoten",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.6 Auswertung von Boniturnoten",
    "text": "A.6 Auswertung von Boniturnoten\n\n\n\n\n\n\n\nvariety\nblock\nrating\n\n\n\nA\nI\n2\n\n\nA\nI\n3\n\n\nA\nI\n3\n\n\nA\nI\n4\n\n\nA\nI\n1\n\n\nA\nII\n3\n\n\nA\nII\n2\n\n\nA\nII\n2\n\n\nA\nII\n4\n\n\nA\nII\n4\n\n\nA\nIII\n2\n\n\nA\nIII\n2\n\n\nA\nIII\n3\n\n\nA\nIII\n1\n\n\nA\nIII\n2\n\n\nB\nI\n8\n\n\nB\nI\n9\n\n\nB\nI\n8\n\n\nB\nI\n9\n\n\nB\nI\n7\n\n\nB\nII\n7\n\n\nB\nII\n7\n\n\nB\nII\n8\n\n\nB\nII\n8\n\n\nB\nII\n7\n\n\nB\nIII\n8\n\n\nB\nIII\n9\n\n\nB\nIII\n7\n\n\nB\nIII\n9\n\n\nB\nIII\n8\n\n\nC\nI\n6\n\n\nC\nI\n5\n\n\nC\nI\n5\n\n\nC\nI\n6\n\n\nC\nI\n4\n\n\nC\nII\n4\n\n\nC\nII\n5\n\n\nC\nII\n3\n\n\nC\nII\n6\n\n\nC\nII\n4\n\n\nC\nIII\n7\n\n\nC\nIII\n6\n\n\nC\nIII\n4\n\n\nC\nIII\n6\n\n\nC\nIII\n4\n\n\nD\nI\n2\n\n\nD\nI\n4\n\n\nD\nI\n1\n\n\nD\nI\n2\n\n\nD\nI\n2\n\n\nD\nII\n2\n\n\nD\nII\n4\n\n\nD\nII\n4\n\n\nD\nII\n1\n\n\nD\nII\n3\n\n\nD\nIII\n3\n\n\nD\nIII\n4\n\n\nD\nIII\n2\n\n\nD\nIII\n1\n\n\nD\nIII\n3\n\n\nE\nI\n4\n\n\nE\nI\n4\n\n\nE\nI\n2\n\n\nE\nI\n7\n\n\nE\nI\n5\n\n\nE\nII\n4\n\n\nE\nII\n3\n\n\nE\nII\n4\n\n\nE\nII\n7\n\n\nE\nII\n7\n\n\nE\nIII\n5\n\n\nE\nIII\n5\n\n\nE\nIII\n4\n\n\nE\nIII\n6\n\n\nE\nIII\n6\n\n\n\n\n\n\nA.6.1 Explorative Datenanalyse (EDA)\n\nggplot(data_tbl, aes(variety, rating, color = block)) +\n  geom_boxplot() +\n  geom_dotplot(aes(fill = block), binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.8))  \n\n\n\n\n\nggplot(data_tbl, aes(variety, rating, fill = block)) +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.8)) +\n  stat_summary(fun = median, fun.min = median, fun.max = median,\n               geom = \"crossbar\", width = 0.5, \n               position=position_dodge(0.8)) \n\n\n\n\n\nA.6.2 Friedman Test\n\n#friedman.test(rating ~ variety | block, data = data_tbl)\n\ndata_tbl <- tibble(Block = 1:4,\n                   Sorte_1 = c(2,3,4,3),\n                   Sorte_2 = c(7,9,8,9),\n                   Sorte_3 = c(6,5,4,7),\n                   Sorte_4 = c(2,4,1,2),\n                   Sorte_5 = c(4,5,3,7)) %>%\n  gather(key, value, Sorte_1:Sorte_5)\n\nfriedman.test(value ~ key | Block, data = data_tbl)\n\n\n    Friedman rank sum test\n\ndata:  value and key and Block\nFriedman chi-squared = 13.5263, df = 4, p-value = 0.0089709"
  },
  {
    "objectID": "app-example-analysis.html#auswertung-von-infektionsstatus",
    "href": "app-example-analysis.html#auswertung-von-infektionsstatus",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.7 Auswertung von Infektionsstatus",
    "text": "A.7 Auswertung von Infektionsstatus"
  },
  {
    "objectID": "app-example-analysis.html#sec-app-example-iso",
    "href": "app-example-analysis.html#sec-app-example-iso",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.2 Isoplethendiagramm für Münster & Osnabrück",
    "text": "A.2 Isoplethendiagramm für Münster & Osnabrück\nIm Folgenden zeige ich ein Beispiel für die Nutzung der entgeltfreien Informationen auf der DWD-Website. Wir finden dort auf der Seite die Klimadaten für Deutschland und natürlich auch die Daten für Münster/Osnabrück. Ich habe mir flux die Tageswerte runtergeladen und noch ein wenig den Header der txt-Datei angepasst. Du findest die Datei day_values_osnabrueck.txt wie immer auf meiner GitHub Seite. Du musst dir für andere Orte die Daten nur entsprechend zusammenbauen. Am Ende brauchen wir noch die Informationen zu den Tages- und Monatswerten damit wir auch verstehen, was wir uns da von der DWD runtergeladen haben. Ich nutze gleich nur einen Ausschnitt aus den Daten.\n\n\nWenn wir Geocomputation with R machen wollen, dann haben wir natürlich noch viele andere Möglichkeiten. Das verlinkte Buch hilft da weiter.\nDann lesen wir die Daten einmal ein und müssen dann eine Winkelzüge machen, damit wir aus dem Datum JJJJMMDD dann jeweils den Monat und den Tag extrahiert kriegen. Dann müssen wir die Monatszahl und die Tageszahl noch in eine Zahl umwandeln. Sonst geht es schlecht mit dem Zeichnen des Konturplots. Wir nehmen dann die Temperaturen TG, TN, TM und TX um diese Temperaturen in vier Konturplots zu zeigen.\n\nweather_tbl <- read_table(\"data/day_values_osnabrueck.txt\") %>% \n  mutate(JJJJMMDD = as.Date(as.character(JJJJMMDD), \"%Y%m%d\"),\n         day = as.numeric(format(JJJJMMDD, \"%d\")), \n         month = as.numeric(format(JJJJMMDD, \"%m\")), \n         year = as.numeric(format(JJJJMMDD, \"%Y\"))) %>% \n  select(month, day, TG, TN, TM, TX) %>% \n  na.omit() %>% \n  gather(temp, grad, TG:TX) %>% \n  mutate(temp = factor(temp, \n                       labels = c(\"Minimum der Temperatur in 5 cm (TG)\",\n                                  \"Minimum der Temperatur in 2 m (TN)\",\n                                  \"Mittel der Temperatur in 2 m (TM)\",\n                                  \"Maximum der Temperatur in 2 m (TX)\")))\n\nNachdem wir ordentlich an den Daten geschraubt haben können wir jetzt in Abbildung A.1 die vier Konturplots sehen. Wir mussten noch das Spektrum der Farben einmal drehen, damit es auch mit den Temperaturfarben passt und wir haben noch ein paar Hilfslinien miteingezeichnet.\n\nggplot(weather_tbl, aes(month, day, z = grad)) +\n  theme_minimal() +\n  geom_contour_filled(bins = 13) +\n  geom_contour(binwidth = 2, color = \"black\") +\n  facet_wrap(~ temp, ncol = 2) + \n  scale_fill_brewer(palette = \"Spectral\", direction = -1) +\n  scale_x_continuous(breaks = 1:12) +\n  geom_vline(xintercept = 1:12, alpha = 0.9, linetype = 2) +\n  geom_hline(yintercept = c(5, 10, 15, 20, 25, 30), \n             alpha = 0.9, linetype = 2)\n\n\n\nAbbildung A.1— Konturplot der verschiedenen Temperaturen."
  },
  {
    "objectID": "app-example-analysis.html#sec-app-example-number-groups",
    "href": "app-example-analysis.html#sec-app-example-number-groups",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.3 Analyse von Anzahlen in zwei Gruppen",
    "text": "A.3 Analyse von Anzahlen in zwei Gruppen\nIn dieser sehr simplen Analyse haben wir zwei Gruppen vorliegen. Die Gruppe 1 ist hat zwei Level oder Behandlungen abgekürzt mit I und II. Die Gruppe 2 hat insgesamt vier Level oder eben Behandlungen, die wir mit A, B, C und D bezeichnen. Wir haben jetzt für die jeweiligen Kombinationen auf dem Feld etwas gezählt. Wir haben also für jede dieser Kombinationen nur eine Zahl. Es ergbit sich somit die folgende Matrix an Zahlen.\n\nrel_mat <- matrix(c(45, 14, 4, 0,\n                    25, 32, 5, 1), nrow = 2, byrow = TRUE,\n                  dimnames = list(c(\"I\", \"II\"), c(\"A\", \"B\", \"C\", \"D\")))\nrel_mat\n\n    A  B C D\nI  45 14 4 0\nII 25 32 5 1\n\n\nNun können wir den \\(\\mathcal{X}^2\\)-Test nutzen, um zu testen, ob die Zahlen in der Matrix bzw. auf unseren Feld gelcihverteilt sind. Die Nullhypothese lautet, dass es keinen Zusammenhang zwischen der Gruppe 1 und der Gruppe 2 auf dem Feld gibt. Die Zahlen sind also rein zufällig in dieser Anordnung.\n\nchisq.test(rel_mat)\n\n\n    Pearson's Chi-squared test\n\ndata:  rel_mat\nX-squared = 13.8689, df = 3, p-value = 0.0030892\n\n\nWir erhalten einen sehr kleinen \\(p\\)-Wert mit \\(0.003\\). Wir können daher die Nullhypothese ablehnen, da der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) mit 5%. Wir haben ein signifikantes Ergebnis. Wir können von einen Zusamenhang zwischen den beiden Gruppen ausgehen.\nMit Cramers V können wir auch noch die Effektstärke für einen \\(\\mathcal{X}^2\\)-Test berechnen.\n\ncramers_v(rel_mat) \n\nCramer's V |       95% CI\n-------------------------\n0.33       | [0.15, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDer Effekt ist mit \\(0.33\\) nicht besonders stark. Du kannst Cramers V wie die Korrelation interpretieren. Ein V von 0 bedeutet keinen Zusammenhang und ein V von 1 einen maximalen Zusammenhang. Wir wollen uns die Daten dann nochmal in einer Abbidlung anschauen. Dafür müssen wir die Matrix erstmal in einen Datensatz umwandeln und die Gruppen zu Faktoren machen.\n\nplot_tbl <- rel_mat %>% \n  as_tibble(rownames = \"group1\") %>% \n  gather(A:D, key = \"group2\", value = \"value\") %>% \n  mutate(group1 = as_factor(group1),\n         group2 = as_factor(group2))\n\nIn Abbildung A.2 sehen wir die Matrix der Zähldaten für die beiden Gruppen nochmal visualisiert. Beim betrachten fällt auf, dass die beiden Level C und D kaum Zähldaten enthalten. Hier wäre zu überlegen die beiden Level aus der Analyse herauszunehmen und einen klassischen \\(\\mathcal{X}^2\\)-Test auf einer 2x2 Kreuztabelle zu rechnen.\n\nggplot(plot_tbl, aes(x = group2, y = value, fill = group1)) +\n  theme_bw() +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Gruppe 2\", y = \"Anzahl\", fill = \"Gruppe 1\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung A.2— Barplot der Zähldaten aus der Matrix."
  },
  {
    "objectID": "app-example-analysis.html#sec-app-example-anova-inter",
    "href": "app-example-analysis.html#sec-app-example-anova-inter",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.4 Auswertung zweifaktorielle ANOVA mit Interaktion",
    "text": "A.4 Auswertung zweifaktorielle ANOVA mit Interaktion\nHier kommt jetzt ein schönes Beispiel für eine Auswertung von einem dreifaktoriellen Design mit einer ANOVA. Passenderweise haben wir auch einen Interaktionsterm vorliegen. Unser dreifaktoriellen Design ist auch kein echtes dreifaktorielles Design. Wir müssen uns hier entscheiden, welcher der zwei Blockfaktoren nun unsere Wiederholung sein soll. Aber schreiben wir erstmal unser Modell auf, bevor wir das Modell mit Inhalt füllen.\n\\[\ny \\sim  f_1 + b_1 + b_2\n\\]\nIn unserem Beispiel schauen wir uns das Pflanzenwachstum growth in einer Klimmakammer mit verschiedenen Belichtungsstufen light_intensity sowie der Position der Pflanze in der Lichtkammer. Die Pflanze hat eine Position im rack und dann im layer. So ergibt sich dann für uns folgendes ausgeschriebenes Modell.\n\\[\ngrowth \\sim light\\_intensity + layer + rack\n\\] In dieser Form wird unser Modell aber leider nicht funktionieren. Wir hätten dann keine Wiederholungen mehr. Jede Pflanze würe dann exakt durch eine Faktorkombination beschrieben. Wir sehen gleich das Problem visualisiert. Vorher müssen wir uns aber einmal die Daten einlesen und eine Menge Faktoren erschaffen. Achtung, das Erschaffen der Faktoren ist hier sehr wichtig! Im Orginaldatensatz stehen nur Zahlen für die Faktoren. Wir kriegen dann ein echtes Problem.\n\nlight_tbl <- read_excel(\"data/light_intensity_data.xlsx\") %>% \n  mutate(rack = factor(rack, labels = c(\"left\", \"middle\", \"right\")),\n         layer = factor(layer, labels = c(\"1st\", \"2nd\", \"3rd\")),\n         light_intensity = factor(light_intensity, labels = c(\"low\", \"mid\", \"high\")),\n         growth = as.numeric(growth))\n\nNachdem wir die Daten eingelesen haben, schauen wir uns den Sachverhalt einmal für die drei Faktoren über die Level der einzelnen Faktoren an. Wir nutzen dafür die Funktion datasummary_crosstab() aus dem R Paket modelsummary. Wir können uns hier die Anzahl der Beobachtungen je Faktorlevelkombination einmal anschauen.\n\ndatasummary_crosstab(light_intensity ~ layer * rack, data = light_tbl,\n                     statistic = NULL)\n\n\n\n\n\n\n1st\n2nd\n3rd\n\n\n light_intensity \n    left \n    middle \n    right \n    left \n    middle \n    right \n    left \n    middle \n    right \n  \n\n\n\n low \n    6 \n    0 \n    0 \n    0 \n    6 \n    0 \n    0 \n    0 \n    6 \n  \n\n mid \n    0 \n    0 \n    6 \n    6 \n    0 \n    0 \n    0 \n    6 \n    0 \n  \n\n high \n    0 \n    6 \n    0 \n    0 \n    0 \n    6 \n    6 \n    0 \n    0 \n  \n\n\n\n\nWir sehen eine Menge Nullen. Das heißt, dass diese Faktorlevelkombinationen keine Beobachtungen haben. Dann können wir auch über diese Kombinationen keine Aussage treffen. Wenn wir entweder rack oder layer entfernen, sieht die Sache schon besser aus. Wir haben jetzt alle Faktorlevelkombinationen belegt. Wir müssen uns dann nur noch entscheiden, welchen Faktor wir ins Modell nehmen wollen.\n\ndatasummary_crosstab(light_intensity ~ layer, data = light_tbl,\n                     statistic = NULL)\ndatasummary_crosstab(light_intensity ~ rack, data = light_tbl,\n                     statistic = NULL)\n\n\n\n\n\n\n light_intensity \n    1st \n    2nd \n    3rd \n  \n\n\n low \n    6 \n    6 \n    6 \n  \n\n mid \n    6 \n    6 \n    6 \n  \n\n high \n    6 \n    6 \n    6 \n  \n\n\n\n\n\n\n\n light_intensity \n    left \n    middle \n    right \n  \n\n\n low \n    6 \n    6 \n    6 \n  \n\n mid \n    6 \n    6 \n    6 \n  \n\n high \n    6 \n    6 \n    6 \n  \n\n\n\n\n\n\n\n\nDas R Paket modelsummary bietet hier eine sehr große Auswahl an tollen Funktionen an um seine Daten übersichtlich zu gestalten.\nFür die Entscheidung welcher der beiden Faktoren rack oder layer mit ins Modekll soll, schauen wir uns einmal die Boxplots für die jeweiligen Fakoten an. In Abbildung A.3 sehen wir einmal die Boxplots aufgeteilt nach rack.\n\nggplot(light_tbl, aes(light_intensity, growth, fill = rack)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito()\n\n\n\nAbbildung A.3— Boxplots des Pflanzenwachstums aufgeteilt nach rack.\n\n\n\n\nUnd wir sehen schon, da stimmt was nicht. Die Annahme der ANOVA ist, dass sich der Trend im ersten Faktorlevel für alle im Faktor über die anderen Faktoren gleicht. Das liest sich kryptisch, aber verdeutlichen wir es mal. Im Level low steigen alle Level des Faktors rack an. Wenn keine Interaktion vorliegen würde, dann müssten dieses Muster in dem Level mid und high ebenfalls annährend zu beobachten sein. Tut es aber nicht. Wir haben eine Interaktion zwischen light_intensity und rack visuell bestätigt.\n\nggplot(light_tbl, aes(light_intensity, growth, fill = layer)) +\n  theme_bw() +\n  geom_boxplot()  +\n  scale_fill_okabeito()\n\n\n\nAbbildung A.4— Boxplots des Pflanzenwachstums aufgeteilt nach layer.\n\n\n\n\nDieses wirre Muster sehen wir dann auch in Abbildung A.4. Hier passen die Trends des Faktors layer über die Faktorlevel low, mid und high auch wieder nicht. Schauen wir uns jetzt nochmal die ganze Sache aufgeteilt nach rack und layer an. Vielelicht werden wir dann etwas schlauer oder das Problem wird noch klarer.\n\nggplot(light_tbl, aes(light_intensity, growth, fill = rack)) +\n  theme_bw() +\n  geom_boxplot() +\n  facet_wrap(~ layer)  +\n  scale_fill_okabeito()\n\n\n\nAbbildung A.5— Boxplots des Pflanzenwachstums aufgeteilt nach rack und layer.\n\n\n\n\nJetzt sehen wir etwas mehr. Im 1st Level liegen alle rack-Level auf einer Ebene. Ebenso liegen alle rack-Level auf einer Ebene im 2n Level. Das ganze Problem der Interaktion entsteht im 3rd Level. Hier ging etwas drunter und drüber im Pflanzenwachstum. Wir wissen jetzt, dass das dritte Layer anscheinend defekt war oder irgendwas dort mit den Racks nicht gestimmt hat.\nWir könnten jetzt das dritte Layer aus der Analyse werfen. Das wäre aber nur eine Möglichkeit. Wenn wir das tuen würden, dann würde wir auch die Interaktion los werden. Das wollen wir hier aber nicht, wir ziehen jetzt die Analyse einmal mit der Interaktion durch. Dafür bauen wir uns jetzt das lineare Modell und schauen uns einmal die ANOVA an.\n\nfit_1 <- lm(growth ~ light_intensity + layer + light_intensity:layer, \n            data = light_tbl)\nfit_1 %>% model_parameters()\n\nParameter                            | Coefficient |   SE |           95% CI | t(45) |      p\n---------------------------------------------------------------------------------------------\n(Intercept)                          |       11.62 | 2.80 | [  5.97,  17.26] |  4.14 | < .001\nlight intensity [mid]                |        1.43 | 3.96 | [ -6.55,   9.42] |  0.36 | 0.719 \nlight intensity [high]               |        3.70 | 3.96 | [ -4.28,  11.68] |  0.93 | 0.356 \nlayer [2nd]                          |        5.68 | 3.96 | [ -2.30,  13.67] |  1.43 | 0.159 \nlayer [3rd]                          |       14.05 | 3.96 | [  6.07,  22.03] |  3.54 | < .001\nlight intensity [mid] * layer [2nd]  |       -3.33 | 5.61 | [-14.62,   7.96] | -0.59 | 0.555 \nlight intensity [high] * layer [2nd] |       -5.92 | 5.61 | [-17.21,   5.37] | -1.06 | 0.297 \nlight intensity [mid] * layer [3rd]  |       -9.60 | 5.61 | [-20.89,   1.69] | -1.71 | 0.094 \nlight intensity [high] * layer [3rd] |      -25.90 | 5.61 | [-37.19, -14.61] | -4.62 | < .001\n\n\nErstmal sehen wir an den Modellparameters, dass hier wieder etwas nicht stimmt. Wir würden erwarten, dass der Effekt des Layers immer gleich ist. Hier ist der Effekt von dem 2nd Layer zu dem 3rd Layer fast dreimal so stark. Und eigentlich sollten die Layer den gleichen Effekt haben. Nämlich eigentlich keinen oder einen Effekt weit unter dem von der Lichtintensität. Das Layer ist eine technische Komponente.\n\nfit_1 %>% anova() %>% model_parameters()\n\nParameter             | Sum_Squares | df | Mean_Square |    F |      p\n----------------------------------------------------------------------\nlight_intensity       |      433.15 |  2 |      216.57 | 4.60 | 0.015 \nlayer                 |       70.92 |  2 |       35.46 | 0.75 | 0.477 \nlight_intensity:layer |     1138.80 |  4 |      284.70 | 6.04 | < .001\nResiduals             |     2120.85 | 45 |       47.13 |      |       \n\nAnova Table (Type 1 tests)\n\n\nWir sehen die visuelle Interaktion auch in der ANOVA Ausgabe als hoch signifikanten Term light_intensity:layer mit dem \\(p\\)-Wert \\(<0.001\\). Im Anschluss rechnen wir jetzt die paarweisen Vergleiche mit der Funktion emmeans(). Mit dem | geben wir an, dass wir die paarweisen Vergleiche für die Level von light_intensity getrennt für die Level vom layer rechnen wollen. Wenn du keine Adjustierung des \\(\\alpha\\)-Niveaus für die multiplen Vergleiche möchtest, dann wähle einfach die Option adjust = \"none\". Wir nutzen dann die Ausgabe nicht direkt sondern werden noch die Ausgabe etwas aufhübschen.\n\ncomp_1_obj <- fit_1 %>% \n  emmeans(specs = ~ light_intensity | layer) %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\nIn dem Objekt comp_1_obj sind eine Menge Informationen enthalten. Ich kürze mir immer die Informationen und sortiere nochmal die Ergebnisse. Wir erhalten dann eine saubere Wiedergabe.\n\ncomp_1_obj %>% \n  summary %>% \n  as_tibble %>% \n  select(contrast, layer, p.value) %>% \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2))\n\n# A tibble: 9 × 3\n  contrast   layer p.value\n  <fct>      <fct> <chr>  \n1 low - mid  1st   1.0000 \n2 low - high 1st   1.0000 \n3 mid - high 1st   1.0000 \n4 low - mid  2nd   1.0000 \n5 low - high 2nd   1.0000 \n6 mid - high 2nd   1.0000 \n7 low - mid  3rd   0.1355 \n8 low - high 3rd   <0.001 \n9 mid - high 3rd   0.0028 \n\n\nNach der Adjustierung für die multiplen Vergleiche haben wir nur noch einen Effekt in dem 3rd Layer. Sonst haben die Lichtintensitäten keinen Einfluss auf die Wuchshöhe der Pflanzen. Da wir wissen, dass das 3rd Layer auch das defekte Layer war, sehen wir hier schon, dass wir keinen wirklichen Effekt durch das Licht vorliegen haben. Alles was wir gefunden haben, ist eben ein defektes 3rd Layer.\nDie 95% Konfidenzintervalle erhalten wir mit der Funktion confint(). Die Ergebnisse sind natürlich die gleichen. Wir sehen wieder keinen Unterschied zwischen den Lichtintensitäten außer in dem 3rd Layer.\n\nci_obj <- comp_1_obj %>% \n  confint() %>% \n  as_tibble() %>% \n  select(contrast, layer, estimate, conf.low = lower.CL, conf.high = upper.CL) \n\nci_obj\n\n# A tibble: 9 × 5\n  contrast   layer estimate conf.low conf.high\n  <fct>      <fct>    <dbl>    <dbl>     <dbl>\n1 low - mid  1st     -1.43    -11.3       8.42\n2 low - high 1st     -3.70    -13.6       6.16\n3 mid - high 1st     -2.27    -12.1       7.59\n4 low - mid  2nd      1.90     -7.96     11.8 \n5 low - high 2nd      2.22     -7.64     12.1 \n6 mid - high 2nd      0.317    -9.54     10.2 \n7 low - mid  3rd      8.17     -1.69     18.0 \n8 low - high 3rd     22.2      12.3      32.1 \n9 mid - high 3rd     14.0       4.18     23.9 \n\n\nIn der Abbildung A.6 sehen wir dann die berechneten 95% Konfidenzintervalle nochmal visualisiert. Wenn wir einen Effekt haben, dann im 3rd Layer. In den restlichen 95% Konfidenzintervallen ist die Null mit enthalten, wir können also die Nullhypothese auf Gleichheit des Gruppenvergleiches nicht ablehnen.\n\nggplot(ci_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high,\n                   color = layer, group = layer)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n  geom_point(position = position_dodge(0.5)) +\n  scale_color_okabeito() +\n  coord_flip() +\n  theme_classic()\n\n\n\nAbbildung A.6— Die 95% Konfidenzintervalle für die paarweisen Vergleiche aufgeteilt layer.\n\n\n\n\nNeben der Darstellung mit 95% Konfidenzintervallen ist auch die Darstellung mit dem compact letter display sehr beliebt. Wir nutzen dafür dann die Funktion cld(). Wir adjustieren uns wieder die Vergleiche nach Bonferroni. Im Weiteren trenne wir die Vergleiche auch wieder nach den Leveln für den Faktor layer auf.\n\ncld_obj <- fit_1 %>% \n  emmeans(specs = ~ light_intensity | layer)  %>%\n  cld(Letters = letters, adjust = \"bonferroni\") \n\ncld_obj\n\nlayer = 1st:\n light_intensity emmean  SE df lower.CL upper.CL .group\n low              11.62 2.8 45     4.65     18.6  a    \n mid              13.05 2.8 45     6.08     20.0  a    \n high             15.32 2.8 45     8.35     22.3  a    \n\nlayer = 2nd:\n light_intensity emmean  SE df lower.CL upper.CL .group\n high             15.08 2.8 45     8.11     22.1  a    \n mid              15.40 2.8 45     8.43     22.4  a    \n low              17.30 2.8 45    10.33     24.3  a    \n\nlayer = 3rd:\n light_intensity emmean  SE df lower.CL upper.CL .group\n high              3.47 2.8 45    -3.50     10.4  a    \n mid              17.50 2.8 45    10.53     24.5   b   \n low              25.67 2.8 45    18.70     32.6   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: bonferroni method for 3 tests \nsignificance level used: alpha = 0.05 \nNOTE: Compact letter displays can be misleading\n      because they show NON-findings rather than findings.\n      Consider using 'pairs()', 'pwpp()', or 'pwpm()' instead. \n\n\nWir sehen wieder, dass wir nur in dem 3rd Layer Buchstabenunterschiede haben. Daher haben wir auch nur im 3rd Layer signifikante Ergebnisse. Wichtig ist, dass wir die Buchstaben nur pro Level des Layers vergleichen können, aber auf keinen Fall über die Layer hinweg. Das geht dann leider nicht. Die Ausgabe der Funktion emmeans() schlägt noch andere Darstellungsformen für die Vergleiche vor, du kannst gerne einmal die Funktionen pairs(), pwpp() oder pwpm() ausprobieren und schauen, ob dir die Visualisierung mehr sagt. Im Kapitel 31.5 gehe ich nochmal auf die verschiedene Darstellungsformen in emmeans ein.\nWenn wir das compact letter display mit deinem Barplot verbinden wollen, müssen wir uns etwas strecken. Zuerst sortieren wir die Ausgabe von cld_obj wieder in die korrekte Reihenfolge der Faktorenlevel. Dann können wir die Spalte .group direkt in ggplot() verwenden.\n\ncld_sort_obj <- cld_obj %>% \n  as_tibble() %>% \n  select(light_intensity, layer, .group) %>% \n  arrange(layer, light_intensity)\n\nIn Abbildung A.7 sehen wir die Ausgabe des Barplots für die Daten und dann an die Balken geschrieben das compact letter display. Wichtig ist hier, dass die Buchstaben immer nur für ein Layer gelten. Wir können wegen der Interaktion nicht die Layer untereinander mit den Buchstaben vergleichen. Wir sehen wiederum, dass wir keine relevanten signifikanten Ergebnisse aus dem Experiment mitnehmen können.\n\nstat_tbl <- light_tbl %>% \n  group_by(light_intensity, layer) %>% \n  summarise(mean = mean(growth),\n            sd = sd(growth))\n\nggplot(stat_tbl, aes(x = layer, y = mean, group = light_intensity, \n                     fill = light_intensity)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2, position = position_dodge(0.9)) +\n  annotate(\"text\", \n           x = c(0.7, 1, 1.3, 1.7, 2, 2.3, 2.7, 3, 3.3), \n           y = c(22, 21, 23, 24, 20, 23, 39, 25, 9), \n           label = pluck(cld_sort_obj, \".group\")) +\n  theme_bw() +\n  labs(fill = \"Behandlung\")  +\n  scale_fill_okabeito()\n\n\n\nAbbildung A.7— Barplots für die Mittelwerte mit den entsprechenden Standardabweichungen und dem compact letter display für die paarweisen Vergleiche. Achtung die letter gelten nur in einem Level des Layers."
  },
  {
    "objectID": "classification-basic.html#dummy-codierung",
    "href": "classification-basic.html#dummy-codierung",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.13 Dummy Codierung",
    "text": "49.13 Dummy Codierung\n\n\nTabelle 49.4— Test\n\ninfected\nage\nsex\nfrailty\n\n\n\n1\n24\nmale\nrobust\n\n\n0\n36\nmale\npre-frail\n\n\n0\n21\nfemale\nfrail\n\n\n1\n34\nfemale\nrobust\n\n\n1\n27\nmale\nfrail\n\n\n\n\n\n\nTabelle 49.5— Test\n\ninfected\nage\nsex_male\n\n\n\n1\n24\n1\n\n\n0\n36\n1\n\n\n0\n21\n0\n\n\n1\n34\n0\n\n\n1\n27\n1\n\n\n\n\n\n\nTabelle 49.6— Test\n\ninfected\nage\nfrailty_robust\nfrailty_pre-frail\n\n\n\n1\n24\n1\n0\n\n\n0\n36\n0\n1\n\n\n0\n21\n0\n0\n\n\n1\n34\n1\n0\n\n\n1\n27\n0\n0"
  },
  {
    "objectID": "classification-basic.html#dummycodierung-von-x",
    "href": "classification-basic.html#dummycodierung-von-x",
    "title": "49  Grundlagen der Klassifikation",
    "section": "\n49.13 Dummycodierung von \\(X\\)\n",
    "text": "49.13 Dummycodierung von \\(X\\)\n\nWir werden immer häufiger davon sprechen, dass wir alle kategorialen Daten in Dummies überführen müssen. Das heißt, wir dürfen keine Faktoren mehr in unseren Daten haben. Wir wandeln daher alle Variablen, die ein Faktor sind, in Dummyspalten um. Die Idee von der Dummyspalte ist die gleiche wie bei der multiplen Regression. Da ich aber nicht davon ausgehe, dass du dir alles hier durchgelesen hast, kommt hier die kurze Einführung zur Dummycodierung.\nDie Dummycodierung wird nur auf den Features durcgeführt. Dabei werden nur Spalten erschaffen, die \\(0/1\\), für Level vorhanden oder Level nicht vorhanden, beinhalten. Wir werden also nur alle \\(x\\) in Dummies umwandeln, die einem Faktor entsprechen. Dafür nutzen wir dann später eine Funktion, hier machen wir das einmal zu Veranschaulichung per Hand. In Tabelle 49.4 haben wir einen kleinen Ausschnitt unser Schweinedaten gegeben. Wir wollen zuerst die Spalte sex in eine Dummycodierung umwandeln.\n\n\nTabelle 49.4— Beispieldatensatz für die Dummycodierung. Wir wollen die Spalten sex und frailty als Dummyspalten haben.\n\ninfected\nage\nsex\nfrailty\n\n\n\n1\n24\nmale\nrobust\n\n\n0\n36\nmale\npre-frail\n\n\n0\n21\nfemale\nfrail\n\n\n1\n34\nfemale\nrobust\n\n\n1\n27\nmale\nfrail\n\n\n\n\nIn der Tabelle 49.5 sehen wir das Ergebnis für die Dummycodierung der Spalte sex in die Dummyspalte sex_male. Wir haben in der Dummyspalte nur noch die Information, ob das Ferkel mänlich ist oder nicht. Wenn wir eine Eins in der Spalte finden, dann ist das Ferkel männlich. Wenn wir eine Null vorfinden, dann ist das Ferkel nicht männlich also weiblich. Das Nicht müssen wir uns dann immer merken.\n\n\nTabelle 49.5— Ergebnis der Dummycodierung der Spalte sex zu der Spalte sex_male.\n\ninfected\nage\nsex_male\n\n\n\n1\n24\n1\n\n\n0\n36\n1\n\n\n0\n21\n0\n\n\n1\n34\n0\n\n\n1\n27\n1\n\n\n\n\nIn der Tabelle 49.6 betrachten wir einen kompleren Fall. Wenn wir eine Spalte vorliegen haben mit mehr als zwei Leveln, wie zum Beispiel die Spalte frailty, dann erhalten wir zwei Spalten wieder. Die Spalte frailty_robust beschreibt das Vorhandensein des Levels robust und die Spalte frailty_pre-frail das Vorhandensein des Levels pre-frail. Und was ist mit dem Level frail? Das Level wir durch das Nichtvorhandesein von robust und dem Nichtvorhandensein von pre-frail abgebildet. Beinhalten beide Spalten die Null, so ist das Ferkel frail.\n\n\nTabelle 49.6— Ergebnis der Dummycodierung für eine Spalte mit mehr als zwei Leveln.\n\ninfected\nage\nfrailty_robust\nfrailty_pre-frail\n\n\n\n1\n24\n1\n0\n\n\n0\n36\n0\n1\n\n\n0\n21\n0\n0\n\n\n1\n34\n1\n0\n\n\n1\n27\n0\n0\n\n\n\n\nWenn wir einen Faktor mit \\(l\\) Leveln haben, erhalten wir immer \\(l-1\\) Spalten nach der Dummycodierung wieder."
  }
]