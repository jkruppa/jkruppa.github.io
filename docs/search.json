[
  {
    "objectID": "stat-modeling-basic.html",
    "href": "stat-modeling-basic.html",
    "title": "33  Multiple lineare Regression",
    "section": "",
    "text": "Version vom September 14, 2022 um 09:00:39\nEin multiples lineare Modell hat mehrere \\(x\\). Daher haben wir auf der linken Seite ein \\(y\\) und auf der rechten Seite \\(p\\)-mal ein \\(x\\). Wir können daher ein multiples lineares Modell daher wie folgt in R schreiben.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nOder konkreter können wir sagen, dass jump_length von animal, sex und weight abhängt und wir diesen Zusammenhang modellieren wollen.\n\\[\njump\\_length \\sim animal + sex + weight\n\\]\nDas hilft uns jetzt nur bedingt, denn wir wollen ja aus einer Modellierung in R die Koeffizienten der Regression wiederbekommen. Dafür müssen wir uns nochmal klar werden, was die Koeffizienten einer Regression sind. Das sind zum einen der y-Achsenabschnitt \\(\\beta_0\\) und die Steigung der einzelnen Variablen mit \\(\\beta_1\\) bis \\(\\beta_p\\). Wir erhalten auch bei einem multiplen Regressionsmodell nur einen Vektor mit den Residuen wieder.\n\\[\ny \\sim \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon\n\\]\nDennoch sind die Residuen \\(\\epsilon\\) normalverteilt. Wir haben im Mittel einen Abstand der \\(\\epsilon\\)’s zur geraden von 0 und eine Streuung von \\(s^2_{\\epsilon}\\).\n\\[\n\\epsilon \\sim \\mathcal{N}(0, s^2_{\\epsilon})\n\\]\nWir zeichen im Prinzip eine Gerade durch den \\(p\\)-dimensioneln Raum."
  },
  {
    "objectID": "stat-modeling-basic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-basic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.1 Genutzte R Pakete für das Kapitel",
    "text": "33.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance, car)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-basic.html#die-modelmatrix-in-r",
    "href": "stat-modeling-basic.html#die-modelmatrix-in-r",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.2 Die Modelmatrix in R",
    "text": "33.2 Die Modelmatrix in R\nAls erstes wollen wir verstehen, wie ein Modell in R aussieht. Dann können wir auch besser verstehen, wie die eigentlichen Koeffizienten aus dem Modell entstehen. Wir bauen uns dafür ein sehr simplen Datensatz.\n\nsnake_tbl <- tibble(\n  svl = c(40, 45, 39, 51, 52, 57, 58, 49),\n  mass = c(6, 8, 5, 7, 9, 11, 12, 10),\n  region = as_factor(c(\"west\",\"west\", \"west\", \"nord\",\"nord\",\"nord\",\"nord\",\"nord\")),\n  color = as_factor(c(\"schwarz\", \"schwarz\", \"rot\", \"rot\", \"rot\", \"blau\", \"blau\", \"blau\"))\n) \n\nIn der Tabelle 33.1 ist der Datensatz snake_tbl nochmal dargestellt. Wir haben die Schlangenlänge svl als Outcome \\(y\\) sowie das Gewicht der Schlangen mass, die Sammelregion region und die Farbe der Schlangen color. Dabei ist mass eine kontinuierliche Variable, region eine kategorielle Variable als Faktor mit zwei Leveln und color eine kategorielle Variable als Faktor mit drei Leveln.\n\n\n\n\nTabelle 33.1— Datensatz zu Schlangen ist entlehnt und modifiiert nach Kéry (2010, p. 77)\n\n\nsvl\nmass\nregion\ncolor\n\n\n\n40\n6\nwest\nschwarz\n\n\n45\n8\nwest\nschwarz\n\n\n39\n5\nwest\nrot\n\n\n51\n7\nnord\nrot\n\n\n52\n9\nnord\nrot\n\n\n57\n11\nnord\nblau\n\n\n58\n12\nnord\nblau\n\n\n49\n10\nnord\nblau\n\n\n\n\n\n\nWir wollen uns nun einmal anschauen, wie ein Modell in R sich zusammensetzt.\n\n33.2.1 Kontinuierliches \\(x\\)\n\nIm ersten Schritt wollen wir uns einmal das Modell mit einem kontenuierlichen \\(x\\) anschauen. Wir bauen uns ein Modell mit der Variable mass. Die Funktion model.matrix() gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ mass, data = snake_tbl) %>% as_tibble\n\n# A tibble: 8 × 2\n  `(Intercept)`  mass\n          <dbl> <dbl>\n1             1     6\n2             1     8\n3             1     5\n4             1     7\n5             1     9\n6             1    11\n7             1    12\n8             1    10\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte mass als kontinuierliche Variable.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 6 \\\\\n  1 & 8 \\\\\n  1 & 5 \\\\\n  1 & 7 \\\\\n  1 & 9 \\\\\n  1 & 11\\\\\n  1 & 12\\\\\n  1 & 10\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_{mass}\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen. Die Funktion residuals() gibt uns die Residuen der Geraden wieder.\n\nfit_1 <- lm(svl ~ mass, data = snake_tbl) \nfit_1 %>% coef %>% round(2)\n\n(Intercept)        mass \n      26.71        2.61 \n\nfit_1 %>% residuals() %>% round(2)\n\n    1     2     3     4     5     6     7     8 \n-2.36 -2.57 -0.75  6.04  1.82  1.61  0.00 -3.79 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 26.71\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für mass mit \\(\\beta_{mass}=2.61\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  26.71 & \\phantom{0}6 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}8 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}5 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}7 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}9 \\cdot 2.61\\\\\n  26.71 & 11\\cdot 2.61\\\\\n  26.71 & 12\\cdot 2.61\\\\\n  26.71 & 10\\cdot 2.61\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_{mass}\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  -2.36\\\\\n  -2.57\\\\\n  -0.75 \\\\\n  +6.04\\\\\n  +1.82\\\\\n  +1.61\\\\\n  \\phantom{+}0.00\\\\\n  -3.79\\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(26.71 +  6*2.61 - 2.36,\n  26.71 +  8*2.61 - 2.57,\n  26.71 +  5*2.61 - 0.75,\n  26.71 +  7*2.61 + 6.04,\n  26.71 +  9*2.61 + 1.82,\n  26.71 + 11*2.61 + 1.61,\n  26.71 + 12*2.61 + 0.00,\n  26.71 + 10*2.61 - 3.79) %>% round() \n\n[1] 40 45 39 51 52 57 58 49\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat.\n\n33.2.2 Kategorielles \\(x\\) mit 2 Leveln\nIm diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen \\(x\\) mit 2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable region``. Die Funktionmodel.matrix()` gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ region, data = snake_tbl) %>% as_tibble\n\n# A tibble: 8 × 2\n  `(Intercept)` regionnord\n          <dbl>      <dbl>\n1             1          0\n2             1          0\n3             1          0\n4             1          1\n5             1          1\n6             1          1\n7             1          1\n8             1          1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte regionnord. In dieser Spalte steht die Dummykodierung für die Variable region. Die ersten drei Schlangen kommen nicht aus der Region nord und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region nord und erhalten daher eine 1 in der Spalte.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 &  0  \\\\\n  1 &  0 \\\\\n  1 &  0\\\\\n  1 &  1\\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta^{region}_{nord} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen. Die Funktion residuals() gibt uns die Residuen der Geraden wieder.\n\nfit_2 <- lm(svl ~ region, data = snake_tbl) \nfit_2 %>% coef %>% round(2)\n\n(Intercept)  regionnord \n      41.33       12.07 \n\nfit_2 %>% residuals() %>% round(2)\n\n    1     2     3     4     5     6     7     8 \n-1.33  3.67 -2.33 -2.40 -1.40  3.60  4.60 -4.40 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 41.33\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für die Region und das Level nord mit \\(\\beta^{region}_{nord} = 12.07\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  -1.33\\\\\n  +3.67 \\\\\n  -2.33 \\\\\n  -2.40 \\\\\n  -1.40 \\\\\n  +3.60 \\\\\n  +4.60 \\\\\n  -4.40 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(41.33 + 0*12.07 - 1.33,\n  41.33 + 0*12.07 + 3.67,\n  41.33 + 0*12.07 - 2.33,\n  41.33 + 1*12.07 - 2.40,\n  41.33 + 1*12.07 - 1.40,\n  41.33 + 1*12.07 + 3.60,\n  41.33 + 1*12.07 + 4.60,\n  41.33 + 1*12.07 - 4.40) %>% round() \n\n[1] 40 45 39 51 52 57 58 49\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat.\n\n33.2.3 Kategorielles \\(x\\) mit >2 Leveln\nIm diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen \\(x\\) mit >2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable color. Die Funktion model.matrix() gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ color, data = snake_tbl) %>% as_tibble\n\n# A tibble: 8 × 3\n  `(Intercept)` colorrot colorblau\n          <dbl>    <dbl>     <dbl>\n1             1        0         0\n2             1        0         0\n3             1        1         0\n4             1        1         0\n5             1        1         0\n6             1        0         1\n7             1        0         1\n8             1        0         1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalten für color. Die Spalten colorrot und colorblau geben jeweils an, ob die Schlange das Level rot hat oder blau oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 0 & 0 \\\\\n  1 & 0 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 0 & 1\\\\\n  1 & 0 & 1\\\\\n  1 & 0 & 1\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta^{color}_{rot} \\\\\n  \\beta^{color}_{blau} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen. Die Funktion residuals() gibt uns die Residuen der Geraden wieder.\n\nfit_3 <- lm(svl ~ color, data = snake_tbl) \nfit_3 %>% coef %>% round(2)\n\n(Intercept)    colorrot   colorblau \n      42.50        4.83       12.17 \n\nfit_3 %>% residuals() %>% round(2)\n\n    1     2     3     4     5     6     7     8 \n-2.50  2.50 -8.33  3.67  4.67  2.33  3.33 -5.67 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 25\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für die Farbe und das Level rot mit \\(\\beta^{color}_{rot} = 4.83\\) und für die Farbe und das Level blau mit \\(\\beta^{color}_{blau} = 12.17\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  42.50 & 0 \\cdot 4.83& 0 \\cdot 12.17 \\\\\n  42.50 & 0 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  -2.50 \\\\\n  +2.50 \\\\\n  -8.33 \\\\\n  +3.67 \\\\\n  +4.67 \\\\\n  +2.33 \\\\\n  +3.33 \\\\\n  -5.67 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(42.50 + 0*4.83 + 0*-12.17 - 2.50,\n  42.50 + 0*4.83 + 0*-12.17 + 2.50,\n  42.50 + 1*4.83 + 0*-12.17 - 8.33,\n  42.50 + 1*4.83 + 0*-12.17 + 3.67,\n  42.50 + 1*4.83 + 0*-12.17 + 4.67,\n  42.50 + 0*4.83 + 1*-12.17 + 2.33,\n  42.50 + 0*4.83 + 1*-12.17 + 3.33,\n  42.50 + 0*4.83 + 1*-12.17 - 5.67) %>% round()\n\n[1] 40 45 39 51 52 33 34 25\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat.\n\n33.2.4 Das volle Modell\nIm letzten Schritt wollen wir uns einmal das volle Modell anschauen. Wir bauen uns ein Modell mit allen Variablen in dem Datensatz snake_tbl. Die Funktion model.matrix() gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ mass + region + color, data = snake_tbl) %>% as_tibble() \n\n# A tibble: 8 × 5\n  `(Intercept)`  mass regionnord colorrot colorblau\n          <dbl> <dbl>      <dbl>    <dbl>     <dbl>\n1             1     6          0        0         0\n2             1     8          0        0         0\n3             1     5          0        1         0\n4             1     7          1        1         0\n5             1     9          1        1         0\n6             1    11          1        0         1\n7             1    12          1        0         1\n8             1    10          1        0         1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte mass als kontenuierliche Variable. In der Spalte regionnord steht die Dummykodierung für die Variable region. Die ersten drei Schlangen kommen nicht aus der Region nord und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region nord und erhalten daher eine 1 in der Spalte. Die nächsten beiden Spalten sind etwas komplizierter. Die Spalten colorrot und colorblau geben jeweils an, ob die Schlange das Level rot hat oder blau oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 6 & 0 & 0 & 0 \\\\\n  1 & 8 & 0 & 0 & 0\\\\\n  1 & 5 & 0 & 1 & 0\\\\\n  1 & 7 & 1 & 1 & 0\\\\\n  1 & 9 & 1 & 1 & 0\\\\\n  1 & 11& 1 & 0 & 1\\\\\n  1 & 12& 1 & 0 & 1\\\\\n  1 & 10& 1 & 0 & 1\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_{mass} \\\\\n  \\beta^{region}_{nord} \\\\\n  \\beta^{color}_{rot} \\\\\n  \\beta^{color}_{blau} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen. Die Funktion residuals() gibt uns die Residuen der Geraden wieder.\n\nfit_4 <- lm(svl ~ mass + region + color, data = snake_tbl) \nfit_4 %>% coef %>% round(2)\n\n(Intercept)        mass  regionnord    colorrot   colorblau \n      25.00        2.50        5.00        1.50       -2.83 \n\nfit_4 %>% residuals() %>% round(2)\n\n    1     2     3     4     5     6     7     8 \n 0.00  0.00  0.00  2.00 -2.00  2.33  0.83 -3.17 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 25\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für mass mit \\(\\beta_{mass}=2.5\\), für Region und das Level nord mit \\(\\beta^{region}_{nord} = 5\\), für die Farbe und das Level rot mit \\(\\beta^{color}_{rot} = 1.5\\) und für die Farbe und das Level blau mit \\(\\beta^{color}_{blau} = -2.83\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  25 & \\phantom{0}6 \\cdot 2.5 & 0 \\cdot 5 & 0 \\cdot 1.5& 0 \\cdot -2.83 \\\\\n  25 & \\phantom{0}8 \\cdot 2.5 & 0 \\cdot 5 & 0 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}5 \\cdot 2.5 & 0 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}7 \\cdot 2.5 & 1 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}9 \\cdot 2.5 & 1 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & 11\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n  25 & 12\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n  25 & 10\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\phantom{+}0.00 \\\\\n  \\phantom{+}0.00 \\\\\n  \\phantom{+}0.00 \\\\\n  +2.00 \\\\\n  -2.00 \\\\\n  +2.33 \\\\\n  +0.83 \\\\\n  -3.17 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(25 +  6*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,\n  25 +  8*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,\n  25 +  5*2.5 + 0*5 + 1*1.5 + 0*-2.83 + 0.00,\n  25 +  7*2.5 + 1*5 + 1*1.5 + 0*-2.83 + 2.00,\n  25 +  9*2.5 + 1*5 + 1*1.5 + 0*-2.83 - 2.00,\n  25 + 11*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 2.33,\n  25 + 12*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 0.83,\n  25 + 10*2.5 + 1*5 + 0*1.5 + 1*-2.83 - 3.17) \n\n[1] 40 45 39 51 52 57 58 49\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat. Was haben wir gelernt?\n\nIn einem Modell gibt es immer ein Faktorlevel weniger als ein Faktor Level hat. Die Information des alphanumerisch ersten Levels steckt dann mit in dem Intercept.\nIn einem Modell geht eine kontinuierliche Variable als eine Spalte mit ein.\nIn einem Modell gibt es immer nur eine Spalte für die Residuen und damit nur eine Residue für jede Beobachtung, egal wie viele Variablen ein Modell hat."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-interpret-x",
    "href": "stat-modeling-basic.html#sec-interpret-x",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.3 Interpretation von \\(x\\)\n",
    "text": "33.3 Interpretation von \\(x\\)\n\n\n33.3.1 Kontinuierliches \\(x\\)\n\n\nset.seed(20137937)\ncont_tbl <- tibble(x = seq(from = 1, to = 7, by = 1),\n                   y = 5 + 1.5 * x + rnorm(length(x), 0, 1))\ncont_tbl\n\n# A tibble: 7 × 2\n      x     y\n  <dbl> <dbl>\n1     1  5.93\n2     2  8.06\n3     3  8.95\n4     4 12.1 \n5     5 11.9 \n6     6 14.9 \n7     7 16.3 \n\n\n\nmodel.matrix(y ~ x, data = cont_tbl)\n\n  (Intercept) x\n1           1 1\n2           1 2\n3           1 3\n4           1 4\n5           1 5\n6           1 6\n7           1 7\nattr(,\"assign\")\n[1] 0 1\n\n\n\nlm(y ~ x, data = cont_tbl) %>% \n  tidy() %>% \n  select(term, estimate)\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)     4.32\n2 x               1.71\n\n\n\nggplot(cont_tbl, aes(x = x, y = y)) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, fullrange = TRUE) +\n  scale_x_continuous(expand = c(0, 0), limits = c(0, 8), breaks = 0:8) + \n  scale_y_continuous(expand = c(0, 0), limits = c(0, 17), breaks = 0:17) +\n  annotate(\"text\", x = 4, y = 9, label = \"y ==  4.32 + 1.71 %.% x\", parse = TRUE,\n           hjust = 0)\n\n\n\nAbbildung 33.1— foo\n\n\n\n\n\n33.3.2 Kategorielles \\(x\\) mit 2 Leveln\n\nset.seed(20339537)\ncat_two_tbl <- tibble(A = rnorm(n = 7, mean = 10, sd = 1),\n                      B = rnorm(n = 7, mean = 15, sd = 1)) %>% \n  gather(key = x, value = y) %>% \n  mutate(x = as_factor(x))\ncat_two_tbl\n\n# A tibble: 14 × 2\n   x         y\n   <fct> <dbl>\n 1 A     10.0 \n 2 A     10.8 \n 3 A     10.7 \n 4 A     11.0 \n 5 A      9.25\n 6 A      8.98\n 7 A      9.71\n 8 B     15.1 \n 9 B     16.0 \n10 B     14.5 \n11 B     15.1 \n12 B     14.2 \n13 B     16.8 \n14 B     15.6 \n\n\n\nmodel.matrix(y ~ x, data = cat_two_tbl)\n\n   (Intercept) xB\n1            1  0\n2            1  0\n3            1  0\n4            1  0\n5            1  0\n6            1  0\n7            1  0\n8            1  1\n9            1  1\n10           1  1\n11           1  1\n12           1  1\n13           1  1\n14           1  1\nattr(,\"assign\")\n[1] 0 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$x\n[1] \"contr.treatment\"\n\n\n\nlm(y ~ x, data = cat_two_tbl) %>% \n  tidy() %>% \n  select(term, estimate)\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    10.1 \n2 xB              5.27\n\n\n\n\n\n\nTabelle 33.2— Datensatz mit mehreren Outcomes zu Flöhen auf verschiedenen Tierarten.\n\nFactor x\nMean of level\nDifference to level A\n\n\n\nA\n10.07\n0.00\n\n\nB\n15.33\n5.27\n\n\n\n\n\n\n\n\n\n\nAbbildung 33.2— foo\n\n\n\n\n\n33.3.3 Kategorielles \\(x\\) mit >2 Leveln\n\nset.seed(20339537)\ncat_three_tbl <- tibble(A = rnorm(n = 7, mean = 10, sd = 1),\n                        B = rnorm(n = 7, mean = 15, sd = 1),\n                        C = rnorm(n = 7, mean = 3, sd = 1)) %>% \n  gather(key = x, value = y) %>% \n  mutate(x = as_factor(x))\ncat_three_tbl\n\n# A tibble: 21 × 2\n   x         y\n   <fct> <dbl>\n 1 A     10.0 \n 2 A     10.8 \n 3 A     10.7 \n 4 A     11.0 \n 5 A      9.25\n 6 A      8.98\n 7 A      9.71\n 8 B     15.1 \n 9 B     16.0 \n10 B     14.5 \n# … with 11 more rows\n\n\n\nmodel.matrix(y ~ x, data = cat_three_tbl)\n\n   (Intercept) xB xC\n1            1  0  0\n2            1  0  0\n3            1  0  0\n4            1  0  0\n5            1  0  0\n6            1  0  0\n7            1  0  0\n8            1  1  0\n9            1  1  0\n10           1  1  0\n11           1  1  0\n12           1  1  0\n13           1  1  0\n14           1  1  0\n15           1  0  1\n16           1  0  1\n17           1  0  1\n18           1  0  1\n19           1  0  1\n20           1  0  1\n21           1  0  1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$x\n[1] \"contr.treatment\"\n\n\n\nlm(y ~ x, data = cat_three_tbl) %>% \n  tidy() %>% \n  select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    10.1 \n2 xB              5.27\n3 xC             -7.41\n\n\n\n\n\n\nTabelle 33.3— Datensatz mit mehreren Outcomes zu Flöhen auf verschiedenen Tierarten.\n\nFactor x\nMean of level\nDifference to level A\n\n\n\nA\n10.07\n0.00\n\n\nB\n15.33\n5.27\n\n\nC\n2.66\n-7.41\n\n\n\n\n\n\n\n\n\n\nAbbildung 33.3— foo"
  },
  {
    "objectID": "stat-modeling-basic.html#adjustierung-für-confounder",
    "href": "stat-modeling-basic.html#adjustierung-für-confounder",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.4 Adjustierung für Confounder",
    "text": "33.4 Adjustierung für Confounder\n\nmodel_tbl <- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %>%\n  mutate(animal = as_factor(animal),\n         sex = as_factor(sex),\n         log_hatch_time = round(log(hatch_time), 2))\n\nIn der Tabelle 37.1 ist der Datensatz model_tbl nochmal dargestellt.\n\n\n\n\nTabelle 33.4— Datensatz mit mehreren Outcomes zu Flöhen auf verschiedenen Tierarten.\n\n\n\n\n\n\n\n\n\n\nanimal\nsex\nweight\njump_length\nflea_count\nhatch_time\nlog_hatch_time\n\n\n\ncat\nmale\n6.02\n15.79\n5\n483.6\n6.18\n\n\ncat\nmale\n5.99\n18.33\n1\n82.56\n4.41\n\n\ncat\nmale\n8.05\n17.58\n1\n296.73\n5.69\n\n\ncat\nmale\n6.71\n14.09\n3\n140.9\n4.95\n\n\ncat\nmale\n6.19\n18.22\n1\n162.2\n5.09\n\n\ncat\nmale\n8.18\n13.49\n1\n167.47\n5.12\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\nfox\nfemale\n8.04\n27.81\n4\n424.46\n6.05\n\n\nfox\nfemale\n9.03\n24.02\n1\n349.48\n5.86\n\n\nfox\nfemale\n7.42\n24.53\n3\n151.43\n5.02\n\n\nfox\nfemale\n9.26\n24.35\n1\n182.68\n5.21\n\n\nfox\nfemale\n8.85\n24.36\n3\n104.89\n4.65\n\n\nfox\nfemale\n7.89\n22.13\n2\n62.99\n4.14\n\n\n\n\n\n\nAbbildung 33.4 (a)\nAbbildung 33.4 (b)\nAbbildung 33.4 (c)\n\n\n\n\n\n(a) jump_length ~ weight\n\n\n\n\n\n\n(b) jump_length ~ weight + animal\n\n\n\n\n\n\n(c) jump_length ~ weight + animal + sex\n\n\n\n\nAbbildung 33.4— Darstellung des counfounder Effekts anhand des Zusammenhangs der Sprungweite in [cm] und dem Gewicht von Flöhen [mg]."
  },
  {
    "objectID": "stat-modeling-basic.html#variance-inflation-factor-vif",
    "href": "stat-modeling-basic.html#variance-inflation-factor-vif",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.5 Variance inflation factor (VIF)",
    "text": "33.5 Variance inflation factor (VIF)\nVIF kann nicht für kategoriale Daten verwendet werden. Statistisch gesehen würde es keinen Sinn machen.\n\nmodel <- lm(mpg ~ disp + hp + wt + drat, data = mtcars)\n\nmodel <- lm(jump_length ~ sex + weight, data = model_tbl)\n\nvif(model)\n\n      sex    weight \n2.1161355 2.1161355 \n\ncheck_model(model, check = \"vif\")"
  },
  {
    "objectID": "stat-modeling-basic.html#sec-model-basic-compare",
    "href": "stat-modeling-basic.html#sec-model-basic-compare",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.6 Vergleich von Modellen",
    "text": "33.6 Vergleich von Modellen\n\nfit_1 <- lm(jump_length ~ animal, data = model_tbl)\nfit_2 <- lm(jump_length ~ animal + sex, data = model_tbl)\nfit_3 <- lm(jump_length ~ animal + sex + weight, data = model_tbl)\nfit_4 <- lm(jump_length ~ animal + sex + sex:weight + animal:weight, data = model_tbl)\nfit_5 <- lm(log(jump_length) ~ animal + sex, data = model_tbl)\n\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\n\\[\n\\Delta_i = AIC_i - AIC_{min}\n\\]\n\nwenn \\(\\Delta_i < 2\\), dann gibt es eine deutliche Unterstützung für das \\(i\\)-te Modell;;\nwenn \\(2 < \\Delta_i < 4\\), dann gibt es eine starke Unterstützung für das \\(i\\)-te Modell;\nwenn \\(4 < \\Delta_i < 7\\), dann gibt es deutlich weniger Unterstützung für das \\(i\\)-te Modell;\nModelle mit \\(\\Delta_i > 10\\) haben im Wesentlichen keine Unterstützung.\n\n\\(AIC_1 = AIC_{min} = 100\\) und \\(AIC_2\\) ist \\(100,7\\). Dann ist \\(\\Delta_2=0,7<2\\), so dass es keinen wesentlichen Unterschied zwischen den Modellen gibt. \\(AIC_1 = AIC_{min} = 100000\\) und \\(AIC_2\\) ist \\(100700\\). Dann ist \\(\\Delta_2 = 700 \\gg 10\\), also gibt es keine Unterstützung für das \\(2\\)-te Modell.\nJe kleiner das AIC ist, desto besser ist das AIC.\nJe kleiner das BIC ist, desto besser ist das BIC.\n\\[\np_i = \\exp\\left(\\cfrac{-\\Delta_i}{2}\\right)\n\\]\nDas \\(p_i\\) ist die relative (im Vergleich zu \\(AIC_{min}\\)) Wahrscheinlichkeit, dass das \\(i\\)-te Modell den AIC minimiert. Zum Beispiel entspricht \\(\\Delta_i = 1.5\\) einem \\(p_i\\) von \\(0.47\\) (ziemlich hoch) und ein \\(\\Delta_ = 15\\) entspricht einem \\(p_i =0.0005\\) (ziemlich niedrig). Im ersten Fall besteht eine Wahrscheinlichkeit von 47%, dass das \\(i\\)-te Modell tatsächlich eine bessere Beschreibung ist als das Modell, das \\(AIC_{min}\\) ergibt, und im zweiten Fall beträgt diese Wahrscheinlichkeit nur 0,05%.\n\nmodel_performance(fit_1) %>% \n  as_tibble() %>% \n  select(AIC, BIC) %>% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 1 × 2\n    AIC   BIC\n  <dbl> <dbl>\n1 3076. 3093.\n\n\n\ncomp_res <- compare_performance(fit_1, fit_2, fit_3, fit_4, fit_5, rank = TRUE)\n\ncomp_res\n\n# Comparison of Model Performance Indices\n\nName  | Model |    R2 | R2 (adj.) |  RMSE | Sigma | AIC weights | BIC weights | Performance-Score\n-------------------------------------------------------------------------------------------------\nfit_2 |    lm | 0.739 |     0.738 | 1.926 | 1.933 |       0.717 |       0.961 |            79.80%\nfit_5 |    lm | 0.731 |     0.729 | 0.099 | 0.099 |    2.88e-09 |    3.85e-09 |            66.01%\nfit_3 |    lm | 0.739 |     0.737 | 1.926 | 1.935 |       0.265 |       0.039 |            53.28%\nfit_4 |    lm | 0.739 |     0.736 | 1.925 | 1.938 |       0.018 |    3.66e-06 |            46.82%\nfit_1 |    lm | 0.316 |     0.314 | 3.118 | 3.126 |   6.04e-126 |   7.29e-125 |             0.00%\n\n\n\nplot(comp_res)\n\n\n\n\n\ntest_vuong(fit_1, fit_2, fit_3, fit_4, fit_5)\n\nName  | Model | Omega2 | p (Omega2) |      LR | p (LR)\n------------------------------------------------------\nfit_1 |    lm |        |            |         |       \nfit_2 |    lm |   0.41 |     < .001 |  -18.46 | < .001\nfit_3 |    lm |   0.41 |     < .001 |  -18.45 | < .001\nfit_4 |    lm |   0.41 |     < .001 |  -18.46 | < .001\nfit_5 |    lm |   0.47 |     < .001 | -123.94 | < .001\nEach model is compared to fit_1.\n\n\n\n#pacman::p_load(report)\n\n#report(fit_1)\n\nWar die Transformation sinnvoll?\n\nfit_1 <- lm(hatch_time ~ animal + sex, data = model_tbl)\nfit_2 <- lm(log_hatch_time ~ animal + sex, data = model_tbl)\n\n\ncomp_res <- compare_performance(fit_1, fit_2, rank = TRUE)\n\nWarning: When comparing models, please note that probably not all models were fit\n  from same data.\n\ncomp_res\n\n# Comparison of Model Performance Indices\n\nName  | Model |    R2 | R2 (adj.) |    RMSE |   Sigma | AIC weights | BIC weights | Performance-Score\n-----------------------------------------------------------------------------------------------------\nfit_2 |    lm | 0.006 |     0.001 |   0.992 |   0.995 |        1.00 |        1.00 |            66.67%\nfit_1 |    lm | 0.008 |     0.003 | 789.441 | 792.086 |    0.00e+00 |    0.00e+00 |            33.33%"
  },
  {
    "objectID": "stat-modeling-basic.html#generalisierung-von-lm-zu-glm-und-glmer",
    "href": "stat-modeling-basic.html#generalisierung-von-lm-zu-glm-und-glmer",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.7 Generalisierung von lm() zu glm() und [g]lmer()\n",
    "text": "33.7 Generalisierung von lm() zu glm() und [g]lmer()\n\n\nDie Funktion lm() nutzen wir, wenn das Outcome \\(y\\) einer Normalverteilung folgt.\nDie Funktion glm() nutzen wir, wenn das Outcome \\(y\\) einer andere Verteilung folgt.\nDie Funktion lmer() nutzen wir, wenn das Outcome \\(y\\) einer Normalverteilung folgt und wir noch einen Block- oder Clusterfaktor vorliegen haben.\nDie Funktion glmer() nutzen wir, wenn das Outcome \\(y\\) einer andere Verteilung folgt und wir noch einen Block- oder Clusterfaktor vorliegen haben."
  },
  {
    "objectID": "stat-modeling-basic.html#referenzen",
    "href": "stat-modeling-basic.html#referenzen",
    "title": "33  Multiple lineare Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nKéry, Marc. 2010. Introduction to WinBUGS for ecologists: Bayesian approach to regression, ANOVA, mixed models and related analyses. Academic Press."
  }
]