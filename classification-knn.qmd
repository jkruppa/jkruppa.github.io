```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# $k$ nearest neighbor {#sec-knn}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

Was macht der $k$ nächste Nachbarn Algorithmus (eng. *k nearest neighbor*, abk. *k-NN*) ? Der Algorthmus ist ein sehr einfacher Algorithmus, der auf den Abständen zu den benachbarten Beobachtungen basiert. Wir wollen also für eine neue Beobachtung den Infketionstatus vorhersagen. Um diese Vorhersage zu bewerkstelligen nutzen wir die $k$-nöchsten Nachbarn zu dieser neuen Beobachtung. Wenn die Mehrzahl der $k$-nächsten Nachbarn den Infektionsstatus $1$ hat, dann vergeben wir auch der neuen Beoabchtung den Infektionsstatus $1$. Wenn dies nicht der Fall ist, dann erhält die neue Beobachtung den Infektionsstatus $0$.

## Genutzte R Pakete für das Kapitel

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, conflicted,
               caret, kknn)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
conflict_prefer("fit", "parsnip")
conflict_prefer("contr.dummy", "kknn")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

In dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt.

```{r}
pig_tbl <- read_excel("data/infected_pigs.xlsx") %>% 
  mutate(pig_id = 1:n(),
         infected = as_factor(infected)) %>% 
  select(pig_id, infected, everything())  
```

In @tbl-ml-basic-pig siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-ml-basic-pig
#| tbl-cap: Auszug aus dem Daten zu den kranken Ferkeln.
#| column: page

pig_raw_tbl <- pig_tbl %>% 
  mutate(infected = as.character(infected))

rbind(head(pig_raw_tbl),
      rep("...", times = ncol(pig_raw_tbl)),
      tail(pig_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

Gehen wir jetzt mal die Wörter und Begrifflichkeiten, die wir für das maschinelle Lernen später brauchen einmal durch.

## $k$-NN theoretisch

Im Folgenden betrachten wir uns den $k$-NN Algorithmus einmal ganz simpel. Dafür nutzen wir die @fig-class-knn-01 als Startpunkt. Wir haben dort 11 Beobachtungen im Trainingsdatensatz dargestellt. Wir finden in dem Trainingsdatensatz acht infizierte Personen soiwe drei gesunde Personen. Darüber hinaus eine neue rote Beobachtung. Gegeben den Traingsdaten, welchen Status wollen wir der neuen roten Beobachtung geben?

![Darstellung von 11 Beobachtungen aus dem Traingsdatensatz und einer neuen roten Beobachtung aus den Testdaten. Die schwarzen Kugeln stellen kranke Personen und die grünen die gesunde Personen dar.](images/class-knn-01.png){#fig-class-knn-01 fig-align="center" width="70%"}

In der @fig-class-knn-02 sehen wir die Klassifizierung nach $k = 1$. Wir nehmen daher die $k = 1$ nächsten Beobachtungen und bestimmen daran den neuen Status der roten Beobachtung. Wenn wir nur die eine nächste Beobachtung als Nachbarn betrachten, so setzen wir den Status unser neuen Beobachtung auf grün und daher gesund.

![Wir nehmen mit $k=1$ nur die nächste Beobachtung zu unserer neuen Beobachtung hinzu und bestimmen die neue Beobachtung als grün.](images/class-knn-02.png){#fig-class-knn-02 fig-align="center" width="70%"}

Nun können wir das Spiel weiterspielen und wählen in der @fig-class-knn-03 die $k = 2$ nächsten Nachbarn zu unser neuen Beobachtung aus. Wir erhalten jetzt ein Unentschieden. Wir haben eine schwarze Beobachtung und eine grüne Beobachtung als $k=2$ nächste Nachbarn. Wir können hier keine Entscheidung treffen. Eine gerade Anzahl an nächsten Nachbarn ist prinzipiell nicht anzuraten. Ich empfehle immer eine ungerade Anzhl. Auch wenn es natürlich auch für eine gerade Anzahl eine algorithmische Lösung gibt. Das ist aber weit über die Anwendung hinaus und geht in die Tiefe des Algorithmus, die wir hier nicht behandeln wollen.

![Mit $k = 2$ nächste Nachbarn haben wir ein Patt vorliegen. Wir können uns nicht entscheiden, ob wir die neue Beobachtung als grün oder schwarz klassifizieren.](images/class-knn-03.png){#fig-class-knn-03 fig-align="center" width="70%"}

In der @fig-class-knn-04 sehen wir, dass wir jetzt $k = 3$ Nachbarn betrachten. Damit haben wir auf jeden Fall wieder eine Entscheidung. Wenn auch hier nur sehr knapp, da wir ja zwei schwarze und einen grünen Nachbarn haben. Wir klassifizieren dennoch die neue Beobachtung als schwarz.

![Die Klassifizierung mit $k = 3$ nächsten Nachbarn. Wir erhalten hier eine , wenn auch knappe, Entscheidung für den schwarzen Status und damit krank.](images/class-knn-04.png){#fig-class-knn-04 fig-align="center" width="70%"}

Soweit so gut. Und wie entscheide ich jetzt was weit weg ist? Wenn wir uns mit dem $k$-NN Algorithmus näher beschäftigen würden, dann werden wir feststellen, dass es eine Vielzahl an Abstandsmaßen gibt. Wir du dir vorstellen kannst, kann man die Entfernung zwischen zwei Punkten als den absoluten Abstand messen. Oder aber als den quadratischen Abstand. Es wäre auch möglich einen gewichteten Abstand einzuführen, so dass nähere Beobachtungen einen größeren Einfluss auf die Vorhersage haben als weiter entfernte Beobachtungen. Dann würden wir auch das Problem von geraden $k$ Anzahlen lösen. Du musst dann leider in den jeweiligen R Paketen schauen, welche Optionen es dort geben mag. Wir werden uns hier auf eins der R Pakete mit `kknn` konzentrieren.m

## kmeans Clustering

```{r}
kmeans(pig_tbl$age, centers = 2)
```

## Imputation von fehlenden Werten

https://recipes.tidymodels.org/reference/step_impute_knn.html

## Klassifikation

https://rpubs.com/Nilafhiosagam/574373

https://parsnip.tidymodels.org/reference/nearest_neighbor.html

```{r}





knn3_fit <- knn3(infected ~ age + sex + location + activity + crp + 
                   frailty + bloodpressure + weight + creatinin, 
                 data = pig_tbl, k = 10)

predict(knn3_fit, newdata = pig_tbl) %>% 
  as_tibble()

nearest_neighbor(neighbors = 11) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") %>% 
  fit(infected ~ age + sex + location + activity + crp + 
                   frailty + bloodpressure + weight + creatinin,
      data = pig_tbl)


dummies_rec <- pig_tbl %>% 
  recipe(infected ~ sex + age) %>%
  step_dummy(sex,
             one_hot = TRUE)  

dummies_rec %>% 
  prep %>% 
  bake(pig_tbl)

```
