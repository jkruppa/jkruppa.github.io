```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, see)
```

# Simple lineare Regression {#sec-modeling-simple-stat}

*Letzte Änderung am `r format(fs::file_info("stat-linear-reg-basic.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"Farewell to the Fairground; These rides aren't working anymore." --- White Lies, Farewell to the Fairground*

in diesem Kapitel wollen wir uns mit den Grundlagen der linearen Regression beschäftigen. Damit meine ich erstmal die Idee eine Gerade durch eine Punktewolke zu zeichnen. Das ist erstmal die simpleste Anwendung. Wir lernen hier einmal die Grundbegriffe und erweitern diese dann auf komplexere Modelle.

Du kanst duch aber davon gedanklich lösen, dass die lineare Regression *nur* eine Methode ist um eine Gerade durch eine Punktewolke zu legen. Die lineare Regression und damit auch das statistische Modellieren kann viel mehr.

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, conflicted, broom,
               readxl, quantreg)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Wir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die lineare Regression auf sehr großen Datensätzen anwenden, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit $n=7$ Beobachtungen. In der @tbl-model-0 ist der Datensatz `simplel_tbl` dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in \[cm\] und dem Gewicht in \[mg\] für sieben Beobachtungen modellieren.

```{r}
#| message: false

simple_tbl <- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),
                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))
```

```{r}
#| message: false
#| echo: false
#| tbl-cap: Datensatz mit einer normalverteilten Variable `jump_length` und der normalverteilten Variable `weight`. 
#| label: tbl-model-0

simple_tbl <- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),
                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))

simple_tbl %>% kable(align = "c", "pipe")
```

In @fig-scatter-lin-01 sehen wir die Visualisierung der Daten `simple_tbl` in einem Scatterplot mit einer geschätzen Gerade.

```{r}
#| echo: false
#| message: false
#| label: fig-scatter-lin-01
#| fig-align: center
#| fig-height: 4
#| fig-width: 4
#| fig-cap: "Scatterplot der Beobachtungen der Sprungweite in \\[cm\\] und dem Gewicht in \\[mg\\]. Die Gerade verläuft mittig durch die Punkte."

ggplot(simple_tbl, aes(weight, jump_length)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  theme_bw() +
  xlim(0, 3.5) + ylim(0, 3.5)
```

Wir schauen uns in diesem Kapitel nur eine *simple* lineare Regression mit einem $x_1$ an. In unserem Fall ist das $x_1$ gleich dem `weight`. Später schauen wir dann *multiple* lineare Regressionen mit mehreren $x_1,..., x_p$ an.

Bevor wir mit dem Modellieren anfangen können, müssen wir verstehen, wie ein *simples* Modell theoretisch aufgebaut ist. Danach können wir uns das lineare Modell in R anschauen.

## Simple lineare Regression theoretisch

Wir haben nun die ersten sieben Beobachtungen in dem Objekt `simple_tbl` vorliegen. Wie sieht nun theoretisch eine lineare Regression aus? Wir wollen eine Grade durch Punkte legen, wie wie wir es in @fig-scatter-lin-01 sehen. Die blaue Gerade wir durch eine Geradengleichung beschreiben. Du kenst vermutlich noch die Form $y = mx + b$. In der Statistik beschreiben wir eine solche Gerade aber wie folgt.

$$
y \sim \beta_0 + \beta_1 x_1 + \epsilon
$$

mit

-   $\beta_0$ als den y-Achsenabschnitt.
-   $\beta_1$ als der Steigung der Geraden.
-   $\epsilon$ als Residuen oder die Abweichungen von den $y$-Werten auf Geraden zu den einzelnen $y$-Werten der Beobachtungen.

In @tbl-reg-deu-eng siehst du nochmal in einer Tabelle den Vergleich von der Schreibweise der linearen Regression in der Schule und in der Statistik. Darüber hinaus sind die deutschen Begriffe den englischen Begriffen gegenüber gestellt. Warum schreiben wir die Gleichung in der Form? Damit wir später noch weitere $\beta_px_p$-Paare ergänzen könen und so *multiple* Modelle bauen können.

| $\boldsymbol{y = mx +b}$ | $\boldsymbol{y \sim \beta_0 + \beta_1 x_1 + \epsilon}$ |      Deutsch      |  Englisch   |
|:------------------------:|:------------------------------------------------------:|:-----------------:|:-----------:|
|           $m$            |                       $\beta_1$                        |     Steigung      |    Slope    |
|           $x$            |                         $x_1$                          | Einflussvariable  | Risk factor |
|           $b$            |                       $\beta_0$                        | y-Achsenabschnitt |  Intercept  |
|                          |                       $\epsilon$                       |     Residuen      |  Residual   |

: Vergleich und Übersicht der schulischen vs. statistischen Begriffe in den linearen Regression sowie die deutschen und englischen Begriffe. {#tbl-reg-deu-eng}

In @fig-lin-reg-01 sehen wir die Visualisierung der Gleichung in einer Abbildung. Die Gerade läuft durch die Punktewolke und wird durch die statistischen Maßzahlen bzw. Parameter $\beta_0$, $\beta_1$ sowie den $\epsilon$ beschrieben. Wir sehen, dass das $\beta_0$ den *Intercept* darstellt und das $\beta_1$ die Steigung der Geraden. Wenn wir $x$ um 1 Einheit erhöhen $x+1$, dann steigt der $y$ Wert um den Wert von $\beta_1$. Die einzelnen Abweichungen der beobachteten $y$-Wert zu den $y$-Werten auf der Gerade ($\hat{y}$) werden als Residuen oder auch $\epsilon$ bezeichnet.

![Visualisierung der linearen Regression. Wir legen eine Gerade durch eine Punktewolke. Die Gerade wird durch die statistischen Maßzahlen bzw. Parameter $\beta_0$, $\beta_1$ sowie den $\epsilon$ beschrieben.](images/statistical_modeling_lm){#fig-lin-reg-01 fig-align="center" width="100%"}

[In R werden die $\hat{y}$ auch *fitted values* genannt. Die $\epsilon$ Werte werden dann *residuals* bezeichnet.]{.aside}

Schauen wir uns einmal den Zusammenhang von $y$, den beobachteten Werten, und $\hat{y}$, den geschätzen Werten auf der Gerade in unserem Beispiel an. In @tbl-lin-reg-epsilon sehen wir die Berechnung der einzelnen Residuen für die Gerade aus der @fig-scatter-lin-01. Wir nehmen jedes beobachtete $y$ und ziehen den Wert von $y$ auf der Gerade, bezeichnet als $\hat{y}$, ab. Diesen Schritt machen wir für jedes Wertepaar $(y_i; \hat{y}_i)$.

::: column-page
|  x  |  y  | $\boldsymbol{\hat{y}}$ | Residuen ($\boldsymbol{\epsilon}$) |               Wert                |
|:---:|:---:|:----------------------:|:----------------------------------:|:---------------------------------:|
| 0.8 | 1.2 |          1.38          |   $\epsilon_1 = y_1 - \hat{y}_1$   | $\epsilon_1 = 1.2 - 1.38 = -0.18$ |
| 1.0 | 1.8 |          1.48          |   $\epsilon_2 = y_2 - \hat{y}_2$   | $\epsilon_2 = 1.8 - 1.48 = +0.32$ |
| 1.2 | 1.3 |          1.58          |   $\epsilon_3 = y_3 - \hat{y}_3$   | $\epsilon_3 = 1.3 - 1.58 = -0.28$ |
| 1.9 | 1.7 |          1.94          |   $\epsilon_4 = y_4 - \hat{y}_4$   | $\epsilon_4 = 1.7 - 1.94 = -0.24$ |
| 2.0 | 2.6 |          1.99          |   $\epsilon_5 = y_5 - \hat{y}_5$   | $\epsilon_5 = 2.6 - 1.99 = +0.61$ |
| 2.7 | 1.8 |          2.34          |   $\epsilon_6 = y_6 - \hat{y}_6$   | $\epsilon_6 = 1.8 - 2.34 = -0.54$ |
| 2.8 | 2.7 |          2.40          |   $\epsilon_7 = y_7 - \hat{y}_7$   | $\epsilon_7 = 2.7 - 2.40 = +0.30$ |

: Zusammenhang zwischen den $y$, den beobachteten Werten, und $\hat{y}$, den geschätzen Werten auf der Gerade. Wir nennen den Abstand $y_i - \hat{y}_i$ auch Residuum oder *Epsilon* $\epsilon$. {#tbl-lin-reg-epsilon}
:::

```{r}
#| echo: false

epsilon <- c(-0.18, 0.32, -0.28, -0.24, 0.61, -0.54, 0.30)
mean_e <- round(mean(epsilon), 2)
var_e <- round(var(epsilon), 2)

```

[In R wird in Modellausgaben die Standardabweichung der Residuen $s_{\epsilon}$ als `sigma` bezeichnet.]{.aside}

Die Abweichungen $\epsilon$ oder auch Residuen genannt haben einen Mittelwert von $\bar{\epsilon} = `r mean_e`$ und eine Varianz von $s^2_{\epsilon} = `r var_e`$. Wir schreiben, dass die Residuen normalverteilt sind mit $\epsilon \sim \mathcal{N}(0, s^2_{\epsilon})$. Wir zeichnen die Gerade also so durch die Punktewolke, dass die Abstände zu den Punkten, die Residuen, im Mittel null sind. Die Optimierung erreichen wir in dem wir die Varianz der Residuuen minimieren. Folglich modellieren wir die Varianz.

## Simples lineare Regression in R

Im Allgemeinen können wir ein Modell in R wie folgt schreiben. Wir brauchen *ein* y auf der linken Seite und in der simplen linearen Regressione ein $x$ auf der rechten Seite der Gleichung. Wir brauchen also zwei Variablen $y$ und $x$, die natürlich nicht im Datensatz in R so heißen müssen.

![Modellschreibweise $y$ hängt ab von $x$. Das $y$ repräsentiert eine Spalte im Datensatz und das $x$ repräsentiert ebenso eine Spalte im Datensatz.](images/statistical_modeling_0.png){#fig-lin-reg-01 fig-align="center" width="30%"}

Konkret würden wir in unserem Beispiel das Modell wie folgt benennen. Das $y$ wird zu `jump_length` und das $x$ wird zu `weight`. Wir haben dann das Modell in der simplesten Form definiert.

![Modellschreibweise bzw. `formula` Schreibweise in R. Die Variable $y$ hängt ab von $x$ am Beispiel des Datensatzes `simple_tbl` mit den beiden Variablen `jump_length` als $y$ und `weight` als $x$.](images/statistical_modeling_2.png){#fig-lin-reg-22 fig-align="center" width="80%"}

Nachdem wir das Modell definiert haben, setzen wir dieses Modell `jump_length ~ weight` in die Funktion `lm()` ein um das lineare Modell zu rechnen. Wie immer müssen wir auch festlegen aus welcher Datei die Spalten genommen werden sollen. Das machen wir mit der Option `data = simple_tbl`. Wir speichern dann die Ausgabe der Funktion `lm()` in dem Objekt `fit_1` damit wir die Ausgabe noch in andere Funktionen pipen können.

```{r}
fit_1 <- lm(jump_length ~ weight, data = simple_tbl)
```

[An dieser Stelle kannst du schnell in das Problem der Antwort auf alles kommen: "42"]{.aside}

Wir können jetzt mir dem Modell drei Dinge tun. Abhängig von der Fragestellung liefert uns natürlich jedes der drei Möglichkeiten eine andere Antwort.

-   Wir rechnen mit dem Fit des Modells eine ANOVA (siehe @sec-anova)
-   Wir rechnen ein kausales Modell, uns interessieren die Effekte (siehe @sec-simple-kausal)
-   Wir rechnen ein prädiktives Modell, uns interessiert der Wert *neuer* Werte (siehe @sec-simple-pred)

### Kausales Modell {#sec-simple-kausal}

Im Folgenden rechnen wir ein kausales Modell, da wir an dem Effekt des $x$ interessiert sind. Wenn also das $x_1$ um eine Einheit ansteigt, um wie viel verändert sich dann das $y$? Der Schätzer $\beta_1$ gibt uns also den Einfluss oder den kausalen Zusammenhang zwischen $y$ und $x_1$ wieder.

[Die Funktion `summary()` gibt dir das Ergebnis eines kausalen Modells wieder]{.aside}

Im ersten Schritt schauen wir uns die Ausgabe der Funktion `lm()` in der Funktion `summary()` an. Daher pipen wir das Objekt `fit_1` in die Funktion `summary()`.

```{r}
#| eval: false

fit_1 %>% summary
```

Wir erhalten folgende Ausgabe dargestellt in @fig-lin-reg-3.

![Die `summary()` Ausgabe des Modells `fit_1`.](images/statistical_modeling_3.png){#fig-lin-reg-3 fig-align="center" width="100%"}

Was sehen wir in der Ausgabe der `summary()` Funktion? Als erstes werden uns die Residuen wiedergegeben. Wenn wir nur wenige Beobachtungen haben, dann werden uns die Residuen direkt wiedergegeben, sonst die Verteilung der Residuen. Mit der Funktion `augment()` aus dem R Paket `broom` können wir uns die Residuen wiedergeben lassen. Die Residuen schauen wir uns aber nochmal im @sec-lin-reg-quality genauer an.

```{r}
fit_1 %>% augment
```

Im zweiten Block erhalten wir die Koeffizienten (eng. *coefficients*) der linearen Regression. Das heißt, wir kriegen dort $\beta_0$ als y-Achsenabschnitt sowie die Steigung $\beta_1$ für das Gewicht. Dabei ist wichtig zu wissen, dass immer als erstes der y-Achsenabschnitt `(Intercept)` auftaucht. Dann die Steigungen der einzelnen $x$ in dem Modell. Wir haben nur *ein* kontinuierliches $x$, daher ist die Interpretation der Ausgabe einfach. Wir können die Gradengleichung wie folgt formulieren.

$$
jump\_length \sim 0.97 + 0.51 \cdot weight
$$

Was heißt die Gleichung nun? Wenn wir das $x$ um eine Einheit erhöhen dann verändert sich das $y$ um den Wert von $\beta_1$. Wir haben hier eine Steigung von $0.51$ vorliegen. Ohne Einheit keine Interpretation! Wir wissen, dass das Gewicht in \[mg\] gemessen wurde und die Sprungweite in \[cm\]. Damit können wir aussagen, dass wenn ein Floh 1 mg mehr wiegt der Floh 0.51 cm weiter springen würde.

Schauen wir nochmal in die *saubere* Ausgabe der `tidy()` Funktion. Wir sehen nämlich noch einen $p$-Wert für den Intercept und die Steigung von `weight`.

```{r}
fit_1 %>% tidy
```

Wenn wir einen $p$-Wert sehen, dann brauchen wir eine Nullhypothese, die wir dann eventuell mit der Entscheidung am Signifikanzniveau $\alpha$ von 5% ablehnen können. Die Nullhypothese ist die Gleichheitshypothese. Wenn es also keinen Effekt von dem Gewicht auf die Sprungweite gebe, wie groß wäre dann $\beta_1$? Wir hätten dann keine Steigung und die Grade würde parallel zur x-Achse laufen. Das $\beta_1$ wäre dann gleich null.

$$
\begin{align*} 
H_0: \beta_i &= 0\\  
H_A: \beta_i &\neq 0 \\   
\end{align*}
$$

Wir haben für jedes $\beta_i$ ein eigenes Hypothesenpaar. Meistens interessiert uns der Intercept nicht. Ob der Intercept nun durch die Null geht oder nicht ist eher von geringem Interessen.

Spannder ist aber wie sich der $p$-Wert berechnet. Der $p$-Wert basiert auf einer t-Statistik, also auf dem t-Test. Wir rechnen für jeden Koeffizienten $\beta_i$ einen t-Test. Das machen wir in dem wir den Koeffizienten `estimate` durch den Fehler des Koeffizienten `std.error` teilen.

$$
\begin{align*} 
T_{(Intercept)} &= \cfrac{\mbox{estimate}}{\mbox{std.error}}  = \cfrac{0.969}{0.445} = 2.18\\  
T_{weight} &= \cfrac{\mbox{estimate}}{\mbox{std.error}}  = \cfrac{0.510}{0.232} = 2.20\\   
\end{align*}
$$

Wir sehen in diesem Fall, dass weder der Intercept noch die Steigung von `weight` signifikant ist, da die $p$-Werte mit $0.081$ und $0.079$ leicht über dem Signifikanzniveau von $\alpha$ gleich 5% liegen. Wir haben aber einen starkes Indiz gegen die Nullhypothese, da die Wahrscheinlichkeit die Daten zu beobachten sehr gering ist unter der Annahme das die Nullhypothese gilt.

Zun Abschluß noch die Funktion `glance()` ebenfalls aus dem R Paket `broom`, die uns erlaubt noch die Qualitätsmaße der linearen Regression zu erhalten. Wir müssen nämlich noch schauen, ob die Regression auch funktioniert hat. Die Überprüfung geht mit einem $x$ sehr einfach. Wir können uns die Grade ja anschauen. Das geht dann mit einem Model mit mehreren $x$ nicht mehr und wir brauchen andere statistsiche Maßzahlen.

```{r}
fit_1 %>% glance 
```

### Prädiktives Modell {#sec-simple-pred}

Neben dem kausalen Modell gibt es auch die Möglichkeit ein prädiktives Modell zu rechnen. Im Prinzip ist die Sprache hier etwas ungenau. Wir verwenden das gefittete Modell nur anders. Anstatt das Modell `fit_1` in die Funktion `summary()` zu pipen, pipen wir die das Modell in die Funktion `predict()`. Die Funktion `predict()` kann dann für neue Daten über die Option `newdata =` das $y$ vorhersagen.

In unserem Fall müssen wir uns deshalb ein `tibble` mit einer Spalte bauen. Wir haben ja oben im Modell auch nur ein $x_1$ mit aufgenommen. Später können wir natürlich auch für multiple Modelle die Vorhersage machen. Wichtig ist, dass die Namen gleich sind. Das heißt in dem neuen Datensatz müssen die Spalten *exakt* so heißen wir in dem alten Datensatz in dem das Modell gefittet wurde.

```{r}

simple_new_tbl <- tibble(weight = c(1.7, 1.4, 2.1, 3.0)) 

predict(fit_1, newdata = simple_new_tbl) %>% round(2)
```

Wie wir sehen ist die Anwendung recht einfach. Wir haben die vier `jump_length` Werte vorhergesagt bekommen, die sich mit dem Fit des Modells mit den neuen `weight` Werten ergeben.

In @fig-scatter-lin-pred sehen wir die Visualisierung der vier vorhergesagten Werte. Die Werte müssen auf der Geraden liegen.

```{r}
#| echo: false
#| message: false
#| label: fig-scatter-lin-pred
#| fig-align: center
#| fig-height: 4
#| fig-width: 5.5
#| fig-cap: "Scatterplot der *alten* Beobachtungen der Sprungweite in \\[cm\\] und dem Gewicht in \\[mg\\]. Sowie der *neuen* vorhergesagten Beobachtungen auf der Geraden."

pred_tbl <- bind_rows(mutate(simple_tbl, status = "beobachtet"),
                      tibble(weight = c(1.7, 1.4, 2.1, 3.0),
                             jump_length = predict(fit_1, newdata = tibble(weight)),
                             status = "vorhergesagt"))

ggplot(pred_tbl, aes(weight, jump_length, color = status, shape = status)) +
  stat_smooth(method = "lm", se = FALSE, fullrange = TRUE, 
              color = "black") +
  labs(color = "", shape = "") +
  geom_point(size = 4) +
  theme_bw() +
  scale_color_okabeito() +
  xlim(0, 3.5) + ylim(0, 3.5)
```

Wir werden später in der Klassiifkation, der Vorhersage von $0/1$-Werten, sowie in der multiplen Regression noch andere Prädktionen und deren Maßzahlen kennen lernen. Im Rahmen der simplen Regression soll dies aber erstmal hier genügen.
