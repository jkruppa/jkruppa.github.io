# Verteilung von Daten {#sec-distribution}

*Letzte Änderung am `r format(fs::file_info("eda-distribution.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"I'm your density. I mean - your destiny." --- George McFly in Zurück in die Zukunft*

```{r}
#| warning: false
#| echo: false
pacman::p_load(tidyverse, readxl, knitr, kableExtra, see, conflicted,
               ggridges)
cbbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", 
                "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

In diesem Kapitel wollen wir uns mit Verteilungen beschäftigen. @dormann2013parametrische liefert eine weitreichende Übersicht über verschiedene Verteilungen. Natürlich ist diese Übersicht auch nicht abschließend. Es gibt eine sehr große Anzahl an Verteilungen, aber wir werden uns nur mit einer kleinen Auswahl beschäftigen. Die folgenden Verteilungen haben eine praktische Verwendung in der Data Science. Wir wollen uns in diesem Kapitel mit folgenden Verteilungen beginnen.

-   der *Normalverteilung*, die Glockenkurve oder auch *Gaussian* im englischen Sprachgebrauch genannt, die kontinuierliche Zahlen repräsentiert.
-   die *Standardnormalverteilung*, als eine spezielle Form der Normalverteilung mit einer Fläche von Eins unter der Kurve.
-   die *t-Verteilung*, eine abgeleitete Verteilung von der Standardnormalverteilung, die ähnliche Eigenschaften wie die Standardnormalverteilung hat.
-   der *Poissonverteilung*, die diskrete Zähldaten repräsentiert.
-   die *Binomialverteilung*, die $0/1$ Zahlen und damit das Eintreten eines Ereignisses repräsentiert. Bekannt aus den Würfel und Münzwurfbeispielen.
-   die *Uniformverteilung*, eine Sockelverteilung, die über einen Zahlenraum nur einen Wert annimmt.

Wir wollen uns jetzt die verschiedenen Verteilungen einmal in der Anwendung anschauen. Dabei lassen wir viel Mathematik recht und links liegen. Du kannst bei @dormann2013parametrische mehr zu dem Thema statistische Verteilungen anlesen. Dort gibt es auch nochmal mehr Informationen zu den einzelnen Eigenschaften, die eine Verteilung noch so haben kann. Wir konzentrieren uns hier auf die Lageparameter und die Streuung der Verteilungen.

::: callout-tip
## R Shiny App zu verschiedenen Verteilungen

Wir besuchen gerne die R Shiny App [The distribution zoo](https://ben18785.shinyapps.io/distribution-zoo/) um mehr über die verschiedenen Verteilungen und deren Parameter zu erfahren.
:::

In diesem Kapitel geht es erstmal um das Grundverständnis, das Daten einer Verteilung folgen. Oder noch konkreter, dass unser Outcome $y$ einer Verteilung folgt. Wir müssen später unseren Algorithmen sagen, welcher Verteilung $y$ entspringt, sonst können wir keine *korrekte* Analyse unser Daten rechnen.

::: {layout="[15,85]" layout-valign="center"}
![](images/angel_01.png){fig-align="center" width="100%"}

> Wir halten den mathematischen Teil zu den Verteilungen sehr kurz oder überspringen den Teil ganz. Wir brauchen die Idee der Verteilungen, weil wir später den Methoden sagen müssen wie unser Outcome $y$ verteilt ist. Nur dann können wir die Daten richtig auswerten.
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
pacman::p_load(tidyverse, magrittr, see, readxl, ggbeeswarm, conflicted)
conflicts_prefer(dplyr::filter)
```

An der Seite des Kapitels findest du den Link *Quellcode anzeigen*, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.

## Daten

Damit wir uns auch eine Verteilung anschauen können brauchen wir *viele* Beobachtungen. Wir haben das ja schon bei den Histogrammen gesehen, wenn wir ein aussagekräftiges Histogramm erstellen wollen, dann brauchen wir mehr als zwanzig Beobachtungen. Daher nehmen wir für dieses Kapitel einmal den Gummibärchendatensatz und schauen uns dort die Variablen `gender`, `height`, `age` ,`count_bears` und `count_color` einmal genauer an. Wie immer nutzen wir die Funktion `select()` um die Spalten zu selektieren. Abschließend verwandeln wir das Geschlecht `gender` und das `module` noch in einen Faktor.

```{r}
#| message: false

gummi_tbl <- read_excel("data/gummibears.xlsx")  |>
  select(year, module, gender, height, age, count_bears, 
         count_color, most_liked ) |> 
  mutate(gender = as_factor(gender),
         module = as_factor(module)) |> 
  na.omit()

```

Wir erhalten das Objekt `gummi_tbl` mit dem Datensatz in @tbl-data-dist-gummi nochmal dargestellt. Wir brauchen nicht alle Spalten aus dem ursprünglichen Datensatz und somit ist die Tabelle etwas übersichtlicher.

```{r}
#| message: false
#| echo: false
#| tbl-cap: Auszug aus den selektierten Daten zu den Gummibärchendaten.
#| label: tbl-data-dist-gummi

gummi_print_tbl <- gummi_tbl |> 
  mutate(gender = as.character(gender),
         module = as.character(module))
rbind(head(gummi_print_tbl),
      rep("...", times = ncol(gummi_print_tbl)),
      tail(gummi_print_tbl)) |> 
  kable(align = "c", "pipe")
```

```{r}
#| echo: false
most_comm_mat <- read_excel("data/gummibears.xlsx") |> 
  select(year, gender, height, most_liked, age) |> 
  mutate(string = str_c(gender, height, most_liked, age, sep = "-")) |> 
  na.omit() |> 
  group_by(year, gender) |> 
  reframe(janitor::tabyl(string)) |> 
  group_by(gender) |> 
  arrange(desc(n)) |> 
  filter(n == max(n)) |> 
  pull(string) |> 
  str_split("-", simplify = TRUE)

col_switch <- c(green = "grün", none = "keine", darkred = "dunkelrot", lightred = "hellrot", 
                white = "weiß", yellow = "gelb", orange = "orange")
```

Gibt es eigentlich eine typische weibliche Person oder männliche Person in den Daten? Ja, gibt es. Unsere meist vertretende Frau ist `r most_comm_mat[2, 2]`cm groß, `r most_comm_mat[2, 4]` Jahre alt und mag am liebsten Gummibärchen der Farbe `r col_switch[most_comm_mat[2, 3]]`. Auf der anderen Seite ist unser meist vertretender Mann `r most_comm_mat[1, 2]`cm groß, `r most_comm_mat[1, 4]` Jahre alt und mag am liebsten Gummibärchen der Farbe `r col_switch[most_comm_mat[1, 3]]`.

## Die Normalverteilung {#sec-normal}

[Wir sprechen in der Statistik auch von Verteilungs*familien*. Daher schreiben wir in R auch `family = gaussian`, wenn wir sagen wollen, dass unsere Daten einer Normalverteilung entstammen.]{.aside}

Wenn wir von de Normalverteilung sprechen, dann schreiben wir ein $\mathcal{N}$ Symbol - also ein großes N mit Serifen. Die Normalverteilung sieht aus wie eine Glocke, deshalb wird die Normalverteilung auch Glockenkurve genannt. Im englischen Sprachgebrauch und auch in R nutzen wir dagegen die Bezeichnung nach dem "Entdecker" der Normalverteilung, Carl Friedrich Gauß (1777 - 1985). Wir nennen daher die Normalverteilung auch Gaussian-Verteilung.

[*Parameter* sind Zahlen, die eine Verteilungskurve beschreiben.]{.aside}

Eine Normalverteilung wird ruch zwei Verteilungs*parameter* definiert. Eine Verteilung hat Parameter. Parameter sind die Eigenschaften einer Verteilung, die notwendig sind um eine Verteilung vollständig zu beschreiben. Im Falle der Normalverteilung brauchen wir zum einen den Mittelwert $\bar{y}$, der den höchsten Punkt unserer Glockenkurve beschreibt. Zum anderen brauchen wir auch die Standardabweichung $s^2_y$, die die Ausbreitung oder Breite der Glockenkurve bestimmt. Wir beschreiben eine Normalverteilung für eine Stichprobe mit $\bar{y}$ und $s^2_y$ wie folgt.

$$
\mathcal{N}(\bar{y}, s^2_y)
$$

Oder mit mehr Details in folgender Form. Wir können hier Verallgemeinern und schreiben in der Grundgesamtheit mit $\mu = \bar{y}$ und $\sigma^2 = s^2_y$. Das heißt, wenn wir unendlich viele Beobachtungen vorliegen hätten, dann wüssetn wir auch den wahren Mittelwert $\mu$ und die wahre Varianz $\sigma^2$ der Daten.

$$
f(y \mid\mu,\sigma^2)=\cfrac{1}{\sqrt{2\pi\sigma^2}} e^{-\cfrac{(y-\mu)^2}{2\sigma^2}}\quad -\infty<y<\infty
$$

Im Falle der Normalverteilung brauchen wir einen Paramter für den höchsten Punkt der Kurve, sowie einen Parameter für die Ausbreitung, also wie weit geht die Kurve nach links und nach rechts. Je nach $\bar{y}$ und $s^2_y$ können wir verschiedenste Normalverteilungen vorliegen haben. Eine Sammlung von Verteilungen nennen wir auch Familie (eng. *family*).

[Wir haben Varianzhomogenität vorliegen, wenn $s^2_{1} = s^2_{2} = s^2_{3}$ sind. Wir haben Varianzheterogenität vorliegen, wenn $s^2_{1} \neq s^2_{2} \neq s^2_{3}$ sind.]{.aside}

In @fig-normal-02 sehen wir verschiedene Normalverteilungen mit unterschiedlichen Mittelwerten. In @fig-normal-02-1 sehen wir eine Varianzhomogenität vorliegen, da die Varianzen in allen drei Normalverteilungen gleich sind. Wir können auch schreiben, dass $s^2_{1} = s^2_{2} = s^2_{3} = 2$. In @fig-normal-02-2 haben wir Varianzheterogenität vorliegen, da die Varianzen der Normalverteilungen ungleich sind. Wir können hier dann schreiben, dass $s^2_{1} = 6 \neq s^2_{2} = 1 \neq s^2_{3} = 3$ sind. Häufig gehen statistische Verfahren davon aus, dass wir Varianzhomogenität über die Gruppen und daher auch die Normalverteilungen vorliegen haben. Konkret, wenn wir die Sprungweiten in\[cm\] von Hunde- und Katzenflöhen mit einander vergleichen wollen, dann gehen wir erstmal davon aus, dass die Mittelwerte verschieden sind, aber die Varianzen gleich sind.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-normal-02
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm verschiedener Normalverteilungen mit unterschiedlichen Mittelwerten."
#| fig-subcap: 
#|   - "Drei Normalverteilungen mit Varianzhomogenität."
#|   - "Drei Normalverteilungen unter Varianzheterogenität."
#| layout-nrow: 1
#| column: page


set.seed(20220789)
norm_tbl <- tibble(n1 = rnorm(10000, 10, 2),
                   n2 = rnorm(10000, 20, 2),
                   n3 = rnorm(10000, 0, 2)) |> 
  gather()

ggplot(data = norm_tbl, aes(x = value, fill = key)) +
  theme_minimal() +
  geom_histogram(position = position_identity(), alpha = 0.75, color = "black") +
  labs(x = "", y = "Anzahl") +
  xlim(-15, 30) +
  theme(legend.position = "none") +
  scale_fill_okabeito() +
  annotate("text", 0, 3200, label = expression(paste("N(0,2)")),
           color = cbbPalette[4]) +
  annotate("text", 10, 3200, label = expression(paste("N(10,2)")),
           color = cbbPalette[2]) +
  annotate("text", 20, 3200, label = expression(paste("N(20,2)")),
           color = cbbPalette[3]) 

set.seed(20220789)
norm_tbl <- tibble(n1 = rnorm(10000, 10, 1),
                   n2 = rnorm(10000, 20, 3),
                   n3 = rnorm(10000, 0, 6)) |> 
  gather()

ggplot(data = norm_tbl, aes(x = value, fill = key)) +
  theme_minimal() +
  geom_histogram(binwidth = 1, position = position_identity(), alpha = 0.75, color = "black") +
  labs(x = "", y = "Anzahl") +
  xlim(-15, 30) +
  theme(legend.position = "none") +
  scale_fill_okabeito() +
  annotate("text", 0, 900, label = expression(paste("N(0,6)")),
           color = cbbPalette[4]) +
  annotate("text", 10, 4100, label = expression(paste("N(10,1)")),
           color = cbbPalette[2]) +
  annotate("text", 20, 1500, label = expression(paste("N(20,3)")),
           color = cbbPalette[3]) 

```

[In einer Normalverteilung liegen 68% der Werte innerhalb $\bar{y}\pm 1 \cdot s_y$ und 95% der Werte innerhalb $\bar{y}\pm 2 \cdot s_y$]{.aside}

Wenn wir eine Normalverteilung vorliegen haben, dann liegen 68% der Werte plus/minus einer Standardabweichung vom Mittelwert. Ebenso liegen 95% der Werte plus/minus zwei Standabweichungen vom Mittelwert. Über 99% der Werte befinden sich innerhalb von drei Standardabweichungen vom Mittelwert. Diese Eigenschaft einer Normalverteilung können wir später noch nutzen um abzuschätzen, ob wir einen relevanten Gruppenunterschied vorliegen haben oder aber ob unsere Daten *unnatürlich* breit streuen.

[Wir nutzen das Wort *approximativ* wenn wir sagen wollen, dass ein Outcome näherungsweise normalverteilt ist.]{.aside}

Schauen wir uns die Normalverteilung einmal am Beispiel unserer Gummibärchendaten und der Körpergröße der Studierenden an. Wir färben das Histogramm nach dem Geschlecht ein. In @fig-normal-01 sehen wir das Ergebnis einmal als Histogramm und einmal als Densityplot dargestellt. Wir können annehmen, dass die Größe *approximativ* normalverteilt ist.

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| label: fig-normal-01
#| fig-cap: "Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Die Farben repräsentieren die jeweiligen Geschlechter. Die Männer sind blau und die Frauen in lila dargestellt."
#| fig-subcap: 
#|   - "Histogramm."
#|   - "Densityplot."
#| layout-nrow: 1
#| column: page

gummi_1_tbl <- gummi_tbl |> 
  select(height, gender) |> 
  na.omit()

mean_tbl <- gummi_1_tbl |> 
  group_by(gender) |> 
  summarise(mean = round(mean(height), 1)) 

ggplot(data = gummi_1_tbl, aes(x = height, fill = gender)) +
  theme_minimal() +
  geom_histogram(position = "dodge", color = "black") +
  labs(x = "", y = "Anzahl", fill = "Geschlecht") +
  scale_x_continuous(breaks = seq(150, 210, by = 5)) +
  scale_fill_okabeito(order = c(2, 7), 
                      labels = c("männlich", "weiblich"))  +
  theme(legend.position = "none") #c(0.85, 0.8))

ggplot(data = gummi_1_tbl, aes(x = height, fill = gender)) +
  theme_minimal() +
  geom_density(alpha = 0.75) +
  labs(x = "", y = "", fill = "Geschlecht") +
  scale_fill_okabeito(order = c(2, 7), 
                      labels = c("männlich", "weiblich")) +
  scale_x_continuous(breaks = seq(145, 210, by = 5), limits = c(145, 210)) +
  scale_y_continuous(breaks = seq(0, 0.055, 0.01), limits = c(0, 0.055)) +
  geom_vline(xintercept = mean_tbl$mean, color = cbbPalette[c(6, 8)],
             size = 1) +
  annotate("label", x = mean_tbl$mean, y = 0.055, label = mean_tbl$mean) +
  theme(legend.position = "none") #c(0.85, 0.8))

```

::: {.callout-note collapse="true"}
## Gibt es einen signifikanten Unterschied für die Körpergröße?

Wenn wir testen wollen, ob es einen signifikanten Unterschied zwischen der Körpergröße zwischen Männern und Frauen gibt, dann können wir die Funktion `t.test()` nutzen, die zwei Mittelwerte einer Normalverteilung vergleicht.

```{r}
t.test(height ~ gender, data = gummi_tbl)
```

Mehr gibt es dann in dem [Kapitel zum t-test](#sec-ttest). Dort findest du dann auch mehr Informationen zu der Interpretation des Ergebnisses.
:::

Wir können die Funktion `rnorm()` nutzen um uns zufällige Zahlen aus der Normalverteilung ziehen zu lassen. Dazu müssen wir mit `n =` spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert `mean =` und die gewünschte Standardabweichung mit `sd =` angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion `rnorm()` mit zehn Werten.

```{r}
rnorm(n = 10, mean = 5, sd = 2) |> round(2)
```

Du kannst ja mal den Mittelwert und die Standardabweichung der zehn Zahlen ausrechnen. Da wir es hier mit einer Stichprobe mit zehn Beobachtungen zu tun haben, wird der Mittelwert $\bar{y}$ und die Standardabweichung $s_y$ sich von den vorher definierten Mittelwert $\mu_y = 5$ und Standardabweichung $\sigma_y = 2$ der Grundgesamtheit unterscheiden.

Wir können auch aus unseren Gummibärchendaten für die Körpergröße in \[cm\] jeweils den Mittelwert und die Standardabweichung getrennt für die Geschlechter berechnen und dann die theoretische Normalverteilung zeichenen. In @fig-normal-03-2 und @fig-normal-03-4 sehen wir die Verteilung der theoretischen Werte, wenn wir die Mittelwerte und die Standardabweichung aus den Verteilungen in @fig-normal-03-1 schätzen. Spannderweise bildet sich den *zufällig* gezogenen Daten auch eine leichte Schulter bei der Verteilung der Körpergrößen. Auch $n = `r nrow(gummi_1_tbl)`$ vollständige Beobachtungen bedeuten nicht, dass wir eine perfekte Normalverteilung erhalten.

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| label: fig-normal-03
#| fig-cap: "Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte. Einmal dargestellt als Histogramm und einmal als Densityplot. Die Farben repräsentieren die jeweiligen Geschlechter. Die Männer sind blau und die Frauen in lila dargestellt."
#| fig-subcap: 
#|   - "Verteilung der beobachteten Werte."
#|   - "Verteilung der theoretischen Werte."
#| layout-nrow: 2
#| column: page

stat_tbl <- gummi_tbl |> 
  select(gender, height) |> 
  na.omit() |> 
  group_by(gender) |> 
  summarise(mean = mean(height),
            sd = sd(height),
            n = n())

gummi_theo_tbl <- tibble(height = c(with(filter(stat_tbl, gender == "m"), rnorm(n, mean, sd)),
                                    with(filter(stat_tbl, gender == "w"), rnorm(n, mean, sd))),
                         gender = rep(c("m", "w"), c(stat_tbl$n[1], stat_tbl$n[2])))

y_max <- 45

ggplot(data = gummi_1_tbl, aes(x = height, fill = gender)) +
  theme_minimal() +
  geom_histogram(position = "dodge", color = "black") +
  labs(x = "", y = "Anzahl", fill = "Geschlecht") +
  scale_x_continuous(breaks = seq(150, 215, by = 5)) +
  scale_fill_okabeito(order = c(2, 7), 
                      labels = c("männlich", "weiblich")) +
  scale_y_continuous(breaks = seq(0, y_max, 5), limits = c(0, y_max)) +
  theme(legend.position = "none") #c(0.85, 0.8))

ggplot(data = gummi_theo_tbl, aes(x = height, fill = gender)) +
  theme_minimal() +
  geom_histogram(position = "dodge", color = "black") +
  labs(x = "", y = "", fill = "Geschlecht") +
  scale_fill_okabeito(order = c(2, 7), 
                      labels = c("männlich", "weiblich")) +
  scale_x_continuous(breaks = seq(150, 215, by = 5)) +
  scale_y_continuous(breaks = seq(0, y_max, 5), limits = c(0, y_max)) +
  theme(legend.position = "none") #c(0.85, 0.8))

ggplot(data = gummi_1_tbl, aes(x = height, fill = gender)) +
  theme_minimal() +
  geom_density(alpha = 0.75) +
  labs(x = "", y = "", fill = "Geschlecht") +
  scale_fill_okabeito(order = c(2, 7), 
                      labels = c("männlich", "weiblich")) +
  scale_x_continuous(breaks = seq(145, 210, by = 5), limits = c(145, 210)) +
  scale_y_continuous(breaks = seq(0, 0.06, 0.01), limits = c(0, 0.06)) +
  theme(legend.position = "none") #c(0.85, 0.8))

ggplot(data = gummi_theo_tbl, aes(x = height, fill = gender)) +
  theme_minimal() +
  geom_density(alpha = 0.75) +
  labs(x = "", y = "", fill = "Geschlecht") +
  scale_fill_okabeito(order = c(2, 7), 
                      labels = c("männlich", "weiblich")) +
  scale_x_continuous(breaks = seq(145, 210, by = 5), limits = c(145, 210)) +
  scale_y_continuous(breaks = seq(0, 0.06, 0.01), limits = c(0, 0.06)) +
  theme(legend.position = "none") #c(0.85, 0.8))

```

In der @fig-beeswarm-dist-0 ist nochmal der Beeswarm für das Alter und die Körpergröße nach Geschlecht dargestellt. Manchmal sieht man in so einem Plot nochmal mehr als in einem reinen Histogramm oder Densityplot. Ich finde es jedenfalls spannend die Teilnehmerinnen vom Girl's Day klar zu erkennen sowie die Teilnehmer:innen meiner Weiterbildungen. Auch spannend ist die Größenverteilung der Männer, die einen klaren Trend zu der Größe von 1.80 haben, aber dafür dann weniger zur Größe 1.79 oder 1.81 zu haben scheinen. Es ist immer wieder spannend was du dann in den unterschiedlichen Abbildungen erkennen kannst.

```{r }
#| echo: false
#| message: false
#| label: fig-beeswarm-dist-0
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Der Beeswarm ist ein Dotplot für eine große Anzahl an Beobachtungen. Hier schauen wir uns einmal das Alter und die Körpergröße aufgeteilt nach Geschlecht an. Bei sehr vielen Beobachtungen kommt dann auch ein Bienenschwarm an die Grenze."
#| fig-subcap: 
#|   - "Alter nach Geschlecht"
#|   - "Körpergröße nach Geschlecht"
#| layout-nrow: 1
#| column: page

na.omit(gummi_tbl) |> 
  mutate(gender = factor(gender, labels = c("männlich", "weiblich"))) |> 
  ggplot(aes(x = gender, y = age,
                             color = gender)) +
  geom_beeswarm(size = 1) +
  theme_minimal() +
  scale_color_okabeito(order = c(2, 7), 
                    labels = c("männlich", "weiblich")) +
  labs(x = "Geschlecht", y = "Alter in Jahren") +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = seq(10, 70, 5))

na.omit(gummi_tbl) |> 
  mutate(gender = factor(gender, labels = c("männlich", "weiblich"))) |> 
  ggplot(aes(x = gender, y = height,
                             color = gender)) +
  geom_beeswarm(size = 1) +
  theme_minimal() +
  scale_color_okabeito(order = c(2, 7), 
                    labels = c("männlich", "weiblich")) +
  labs(x = "Geschlecht", y = "Körpergröße in [cm]") +
  theme(legend.position = "none") +
  scale_y_continuous(breaks = seq(138, 214, 4))

```

Dann schauen wir uns einmal in der @fig-gummi-height-gender die verschiedenen Kurse im Bezug auf die Körpergröße aufgeteilt nach Männern und Frauen an. Gibt es in den einzelnen Veranstaltungen einen Unterschied in der Verteilung? Oder sehen die Verteilungen der Körpergrößen eher gleich aus? Dann habe ich noch die durchschnittlichen Körpergrößen aus dem [statistischen Bundesamt](https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Gesundheit/Gesundheitszustand-Relevantes-Verhalten/Tabellen/liste-koerpermasse.html#) mit $167.5cm$ für 20 bis 25 jährige Frauen sowie $181.4cm$ für 20 bis 25 jährige Männer als gestrichelte Linie ergänzt. Die durchgezogene Linie stellt die Körpergrößen in unseren Daten über alle Beobachtungen dar. Wir haben hier anscheinend im Schnitt etwas größere Personen als zu erwarten wäre.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-gummi-height-gender
#| fig-align: center
#| fig-height: 5
#| fig-width: 7
#| fig-cap: "Densityplot der Körpergrößen für Männer und Frauen aufgeteilt nach einer Auswahl von Lehrveranstaltungen. Die schwarze Linie zeigt den aktuellen Durchschnitt der Körpergröße über alle Beobachtungen. Die gestrichelte Linie zeigt die durchschnittiche Körpergröße nach dem statistsischen Bundesamt mit $167.5cm$ für 20 bis 25 jährige Frauen sowie $181.4cm$ für 20 bis 25 jährige Männer."

gummi_3_tbl <- gummi_tbl |> 
  select(height, gender, year, module) |> 
  mutate(module = factor(module, 
                         labels = c("Bioinformatik", "Statistik für\n Bioverfahrenstechnik", 
                                    "Statistik & Versuchswesen",
                                    "Biostatistik", "Fakultätsinformationstag", "Mathematik & Statistik",
                                    "Statistik", "Girl's Day", "PhD Borstel", "PhD Osnabrück"))) |> 
  filter(module %in% c("Bioinformatik", "Statistik für\n Bioverfahrenstechnik", 
                                    "Biostatistik", "Mathematik & Statistik",
                                    "Statistik", "PhD Borstel")) |> 
  na.omit() |> 
  droplevels()

count_tbl <- gummi_3_tbl |> 
  pull(module) |> 
  janitor::tabyl()

ggplot(data = gummi_3_tbl, aes(x = height, y = fct_rev(as_factor(module)),
                             fill = gender)) +
  theme_minimal() +
  stat_density_ridges(alpha = 0.8) +
  scale_x_continuous(breaks = seq(140, 220, by = 10), limits = c(140, 215)) +
  labs(x = "Körpegröße in [cm]", y = "") +
  theme(legend.position = "none") +
  #geom_vline(xintercept = c(7, 8, 9, 10, 11, 12, 13, 14), linetype = 2, alpha = 0.5) +
  scale_fill_okabeito(order = c(2, 7)) +
  annotate("label", x = 205, y = c(1, 2, 3, 4, 5, 6) + 0.7, label = str_c("n = ", rev(count_tbl$n))) +
  scale_y_discrete(expand = expansion(mult = c(0.11, 0.35))) +
  geom_vline(xintercept = mean_tbl$mean, color = "black",  #cbbPalette[c(6, 8)],
             size = 0.75) +
  geom_vline(xintercept = c(167.5, 181.4), color = "black", # cbbPalette[c(8, 6)],
             size = 0.75, linetype = 2) +
  annotate("label", x = mean_tbl$mean, y = 7.5, label = mean_tbl$mean) +
  annotate("label", x = c(167.5, 181.4), y = 0.7, label = c(167.5, 181.4), fill = "gray") 

```

## Die Standardnormalverteilung

Es gibt viele Normalverteilungen. Eiegntlich gibt es unednlich viele Normalverteilunge, da wir für die Parameter Mittelwert $\bar{y}$ und die Standardabweichung $s_y$ beliebige Zahlen einsetzen können. Aber es gibt eine besondere Normalverteilung, so dass diese Verteilung einen eigenen Namen hat. Wir sprechen von der Standardnormalverteilung, wenn der Mittelwert gleich Null ist und die Standardabweichung gleich Eins. Du siehst hier nochmal die Standardnormalverteilung ausgeschrieben.

$$
\mathcal{N}(0, 1)
$$

Folgende Eigenschaften sind der Standardnormalverteilung gegeben. Die Standardnormalverteilung hat eine Fläche von $A = 1$ unter der Kurve. Darüber hinaus liegen 95% der Werte zwischen $\approx -2$ und $\approx 2$. Die einzelnen Werte einer Standardnormalverteilung nennen wir $z$-Werte. Wenn wir eine beliebige Normalverteilung in eine Standardnormalverteilung überführen wollen so machen wir die Umwandlung mit der $z$-Transformation. Und jetzt fahren wir wieder in die Doppeldeutigkeit in R.

![Darstellung von dem Zusammenhang von `pnorm(q = 1.96)` und `qnorm(p = 0.025)`. Mit der Option `lower.tail` bestimmen wir auf welche Seite der Verteilung wir sein wollen.](images/eda/distribution-t.png){#fig-dist-t-01 fig-align="center" width="100%"}

In @fig-dist-t-01 haben wir eine Standardnormalverteilung gegeben. Können jetzt verschiedene Werte auf der $x$-Achse und die Flächen links und rechts von diesen Werten berechnen. Wir nutzen die Funktion `pnorm()` wenn wir die Fläche rechts oder links von einem Wert $q$ berechnen wollen.

```{r}
pnorm(q = 1.96, mean = 0, sd = 1, lower.tail = FALSE) |> 
  round(3)
```

Wir berechen die Fläche links von $q$ und damit auch die Wahrscheinlichkeit $Pr(X \leq q)$ mit `lower.tail = TRUE`. Warum ist die Fläche jetzt eine Wahrscheinlichkeit? Wir haben unter der Kurve der Standardnormalverteilung eine Fläche von $A = 1$. Damit ist jede Fläche auch gleich einer Wahrscheinlichkeit. Wenn wir an der Fläche rechts von $q$ interessiert sind und damit auch an der Wahrscheinlichkeit $Pr(X > q)$ nutzen wir die Option `lower.tail = FALSE`. Das ist erstmal immer etwas verwirrend, aber schau dir den Zusammenhang nochmal in der @fig-dist-t-01 an. Wir brauchen diese Idee von der *Fläche ist auch gleich Wahrscheinlichkeit* im @sec-stat-theorie zum statistischen Testen.

Wir können die Berechnung von $q$ zu $p$ auch umdrehen. Wir geben eine Fläche vor und wollen wissen wie der Wert auf der x-Achse zu der entsprechenden Fläche ist. In diesem Fall will ich die Werte zu den Flächen von $p = 0.025$ und $p = 0.05$. Da wir `lower.tail = FALSE` ausgewählt haben, sind wir auf der rechten Seite der Verteilung.

```{r}
qnorm(p = c(0.025, 0.05), mean = 0, sd = 1, lower.tail = FALSE) |> 
  round(3)
```

Und hier einmal als Gegenprobe mit der Option `lower.tail = TRUE`. Wir springen dann damit auf die linke Seite der Verteilung und wie zu erwarten erhlaten wir dann auch den negativen Wert für die Fläche von $p = 0.05$.

```{r}
qnorm(p = 0.05, mean = 0, sd = 1, lower.tail = TRUE) |> 
  round(3)
```

Die ganzen Berechnungen funktionieren natürlich auch, wenn wir nicht die Fläche $A=1$ unterhalb der Standardnormalverteilung hätten. Aber wir nutzen hier eben den Zusammenhang von Fläche zu Wahrscheinlichkeit um mit der Verteilung zu rechnen und Wahrscheinlichkeiten abzuschätzen.

## Die t-Verteilung {#sec-t-dist}

Die t-Verteilung ist eine Abwandlung der Standardnormalverteilung. Wir haben wieder eine Fläche $A = 1$ unter der Verteilungskurve. Wir benötigen die t-Verteilung, als eine künstliche Verteilung, im @sec-ttest zum statistischen Testen mit dem t-Test. Wir bezeichnen die t-Verteilung als eine künstliche Verteilung, da wir in der Biologie nichts beobachten können, was t-verteilt ist. Wir nutzen die t-Verteilung nur im statistischen Kontext und in diesem Kontekt nur um uns klar zu machen wie statistisches Testen konzeptionell funktioniert. Anwenden werden wir die Verteilung nicht.

Der Unterschied ist die Form der t-Verteilung. Wir geben mit der Option `df =` die Freiheitsgrade der Verteilung an. Hier soll es reichen, dass mit $\lim_{df \to \infty}$ sich die t-Verteilung der Standardnormalverteilung fast gleicht. Bei niedrigeren Freiheitsgraden ist die t-Verteilung nicht mehr so hoch und daher sind die Verteilungsenden weiter nach außen geschoben. Die t-Verteilung ist gestaucht wie wir in @fig-tdist-01 etwas überspitzt gezeichnet sehen. Die Freiheitsgrade hängen direkt an der beobachteten Fallzahl mit $df = n_1 + n_2 - 2$.

![Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und damit die Fallzahl, desto näher kommt die t-Verteilung einer Normalverteilung nahe.](images/eda/t-verteilung_06.png){#fig-tdist-01 fig-align="center"}

Wie auch bei der Standardnormalverteilung gilt folgender Zusammenhang, wenn wir die Flächen anhand eines gegebenen t-Wertes berechnen wollen. Wenn wir die Fläche links von dem t-Wert berechnen wollen, also die Wahrscheinlichkeit $Pr(X \leq t)$, dann nutzen wir die Option `lower.tail = TRUE`. Wenn wir die Fläche auf der rechten Seite von unserem t-Wert berechnen wollen, dann nutzen wir mit $Pr(X > t)$ die Option `lower.tail = FALSE`. In der Funktion `pt()` ist das `q=` als `t=` zu lesen. Das macht das Verständnis vielleicht leichter.

```{r}
pt(q = 2.571, df = 5, lower.tail = FALSE) |> 
  round(3)
```

Neben der Berechnung der Wahrscheinlichkeit rechts und links eines gegebenen Wertes $t$ können wir auch $t$ berechnen, wenn wir eine Fläche vorgeben. Das kann uns dann die Funktion `qt()` liefern. Wir sehen, dass mit steigender Fallzahl und damit steigenden Freiheitsgrad sich der berechnete Wert sich dem Wert der Standardnormalverteilung von $1.96$ für $p = 0.05$ annähert.

```{r}
qt(p = c(0.025), df = c(5, 10, 20, 100, 1000), lower.tail = FALSE) |> 
  round(3)
```

Wir haben gelernt, dass der Zusammenhang zwischen der Standardnormalverteilung und der t-Verteilung ziemlich stark ist. Nutzen werden wir die t-Verteilung aber nur im Rahmen des statistischen Testens.

## Die Poissonverteilung {#sec-poisson}

Eine weitere wichtige Verteilung ist die Poissonverteilung. Die Poissonverteilung ist eine diskrete Verteilung. Daher kommen nur ganze Zahlen vor. Damit bildet die Poissonverteilung die Zähldaten ab. Wenn wir also etwas Zählen, dann ist diese Variable mit den gezählten Ergebnissen poissonverteilt. Im Folgenden sehen wir die Poissonverteilung einmal dargestellt.

$$
\mathcal{Pois}(\lambda)
$$

Oder mit mehr Details in folgender Form.

$$
P_\lambda (k) = \frac{\lambda^k}{k!}\, \mathrm{e}^{-\lambda}
$$

Die Poisson-Verteilung gibt dann die Wahrscheinlichkeit einer bestimmten Ereignisanzahl $k$ im Einzelfall an, wenn die mittlere Ereignisrate $\lambda$ bekannt ist. Im Gegensatz zur Normalverteilung hat die Poissonverteilung nur einen Parameter. Den Lageparameter $\lambda$ ausgedrückt durch den griechischen Buchstaben Lambda. Eine Poissonverteilung mit $\mathcal{Pois}(4)$ hat den höchsten Punkt bei vier. Nun hat die Poissonverteilung hat mehrere Besonderheiten. Da die Poissonverteilung keinen Streuungsparameter hat, steigt mit dem $\lambda$ auch die Streuung. Daher haben Poissonverteilungen mit einem großen $\lambda$ auch eine große Streuung. ie Ausbreitung der Kurve ist eine Funktion von $\lambda$ und steigt mit $\lambda$ an. Du kannst diesen Zusammenhang in @fig-pois-00 beobachten.

Darüber hinaus kann eine Poissonverteilung nicht negativ werden. Es kann keine kleinere Zahl als die Null geben. Durch die diskreten Zahlen haben wir auch immer mal Lücken zwischen den Balken der Poissonverteilung. Das passiert besonders, wenn wir eine kleine Anzahl an Beobachtungen haben. Abschließend konvergiert die Poissonverteilung bei großen $\lambda$ hin zu einer Normalverteilung.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pois-00
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm verschiedener Poissonverteilungen."

set.seed(20220789)
pois_tbl <- tibble(lambda1 = rpois(1000, 6),
              lambda2 = rpois(1000, 10),
              lambda3 = rpois(1000, 1)) |> 
  gather()

ggplot(data = pois_tbl, aes(x = value, fill = key)) +
  theme_minimal() +
  geom_histogram(binwidth = 1, position = position_dodge(0.95), color = "black") +
  labs(x = "", y = "Anzahl") +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = 0:25, limits = c(0,NA)) +
  scale_fill_okabeito() +
  annotate("text", 4, 250, label = expression(paste("Pois(", lambda, " = 1)")),
           color = cbbPalette[4]) +
  annotate("text", 6, 180, label = expression(paste("Pois(", lambda, " = 6)")),
           color = cbbPalette[2]) +
  annotate("text", 10, 150, label = expression(paste("Pois(", lambda, " = 10)")),
           color = cbbPalette[3]) 
```

Schauen wir uns nun einmal die Poissonverteilung im Beispiel an. In @fig-pois-01 sehen wir die Histogramme der Anzahl an Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte. Da wir es hier mit Zähldaten zu tun haben, könnte es sich um eine Poissonverteilung handeln. Wie müssen uns nun die Frage stellen, ob die Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte *wirklich* eine zufällige Realistierung sind. Daher eine zufällige Stichprobe der Grundgesamtheit. Wir können diese Annahme überprüfen in dem wir die theoretischen Werte für die beiden Poissonverteilung mit $\mathcal{Pois}(10)$ und $\mathcal{Pois}(5)$ genieren.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-pois-01
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramme der Anzahl an Gummibärchen und die Anzahl an Farben in einer Tüte. Es gibt nicht mehr als sechs Farben."
#| fig-subcap: 
#|   - "Anzahl an Bärchen"
#|   - "Anzahl an Farben"
#| layout-nrow: 1
#| column: page

gummi_2_tbl <- gummi_tbl |> 
  select(count_bears, count_color) |> 
  na.omit()

height_max <- 375

ggplot(data = gummi_2_tbl, aes(x = count_bears)) +
  theme_minimal() +
  geom_bar(position = "dodge", color = "black", fill = cbbPalette[2]) +
  scale_x_continuous(breaks = seq(5, 15, by = 1), limits = c(5, 15)) +
  labs(x = "", y = "Anzahl") 

ggplot(data = gummi_2_tbl, aes(x = count_color)) +
  theme_minimal() +
  geom_bar(position = "dodge", color = "black", fill = cbbPalette[3]) +
  #scale_x_continuous(breaks = seq(1, 6, by = 1), limits = c(1, 6)) +
  labs(x = "", y = "Anzahl") +
  scale_y_continuous(breaks = seq(0, height_max, 50), limits = c(0, height_max))
```

Wir können die Funktion `rpois()` nutzen um uns zufällige Zahlen aus der Poissonverteilung ziehen zu lassen. Dazu müssen wir mit `n =` spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert `lambda =` angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion `rpois()` mit zehn Werten.

```{r}
rpois(n = 10, lambda = 5)
```

[Es gibt neben der Poissonverteilung auch die negative Binomialverteilung sowie die Quasi-Poissonverteilung, die es erlauben einen Streuungsparameter für die Poissonverteilung zu schätzen.]{.aside}

Wir können nun auch aus unseren Gummibärchendaten für die Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte die theoretische Poissonverteilung berechnen. In @fig-pois-03 sehen wir die Verteilung der beobachteten Werte für Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte und deren theoretischen Verteilung nach dem geschätzen $\lambda = 10$ und $\lambda = 5$. Wir sehen ganz klar, dass die beide Variablen *keine* Zufallsrealisierung sind. Zum einen haben wir das auch nicht erwartet, es gibt nicht mehr als sechs Farben und zum anderen ist zu vermuten, dass Haribo technisch in den Auswahlprozess eingreift. Wir haben auf jeden Fall eine sehr viel kleinere Streuung als bei einer *klassischen* Poissonverteilung anzunehmen wäre.

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| label: fig-pois-03
#| fig-cap: "Darstellung Anzahl an Bärchen und Anzahl an Farben. Es gibt nicht mehr als sechs Farben. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte."
#| fig-subcap: 
#|   - "Verteilung der beobachteten Anzahl an Bärchen."
#|   - "Verteilung der theoretischen Anzahl an Bärchen."
#|   - "Verteilung der beobachteten Anzahl an Farben."
#|   - "Verteilung der theoretischen Anzahl an Farben."
#| layout-nrow: 2
#| column: page


stat_color_tbl <- gummi_2_tbl |> 
  summarise(mean = mean(count_color),
            n = n())

stat_bears_tbl <- gummi_2_tbl |> 
  summarise(mean = mean(count_bears),
            n = n())

height_max <- 375

ggplot(data = gummi_2_tbl, aes(x = count_bears)) +
  theme_minimal() +
  geom_bar(position = "dodge", color = "black", fill = cbbPalette[5]) +
  labs(x = "", y = "Anzahl") +
  scale_x_continuous(breaks = seq(1, 20, by = 1), limits = c(1, 20)) +
  scale_y_continuous(breaks = seq(0, height_max, 10), limits = c(0, height_max))

set.seed(20220831)
ggplot(data = tibble(count_bears = rpois(stat_bears_tbl$n, stat_bears_tbl$mean)), 
       aes(x = count_bears)) +
  theme_minimal() +
  geom_bar(position = "dodge", color = "black", fill = cbbPalette[6]) +
  labs(x = "", y = "Anzahl") +
  scale_x_continuous(breaks = seq(1, 20, by = 1), limits = c(1, 20)) +
  scale_y_continuous(breaks = seq(0, height_max, 10), limits = c(0, height_max))

ggplot(data = gummi_2_tbl, aes(x = count_color)) +
  theme_minimal() +
  geom_bar(position = "dodge", color = "black", fill = cbbPalette[7]) +
  labs(x = "", y = "Anzahl") +
  scale_x_continuous(breaks = seq(0, 11, by = 1), limits = c(0, 11)) +
  scale_y_continuous(breaks = seq(0, height_max, 10), limits = c(0, height_max))

set.seed(20220831)
ggplot(data = tibble(count_color = rpois(stat_color_tbl$n, stat_color_tbl$mean)), 
       aes(x = count_color)) +
  theme_minimal() +
  geom_bar(position = "dodge", color = "black", fill = cbbPalette[8]) +
  labs(x = "", y = "Anzahl") +
  scale_x_continuous(breaks = seq(0, 11, by = 1), limits = c(0, 11)) +
  scale_y_continuous(breaks = seq(0, height_max, 10), limits = c(0, height_max))

```

In @fig-gummi-years-count schauen wir uns nochmal an in wie weit sich die Füllung der Tütchen im Laufe der Jahre entwickelt hat. Die Daten werden ja schon seit 2018 erhoben. Wir schauen uns daher die Densityplot einmal aufgetrennt für die Jahre 2018 bis heute an. Das Jahr 2020 fehlt, da bedingt durch die Coronapandemie keine Präsenslehre stattfand. Wir sehen, dass sich die Verteilung anscheinend in dem Jahr 2022 langsam nach links zu weniger Bärchen in einer Tüte bewegt. Wir bleiben gespannt auf den weiteren Trend.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-gummi-years-count
#| fig-align: center
#| fig-height: 6
#| fig-width: 8
#| fig-cap: "Densityplot der Anzahl an Bärchen in einer Tüte aufgetrennt nach den Jahren der Erhebung. Das Jahr 2020 fehlt bedingt durch die Coronapandemie."

gummi_3_tbl <- gummi_tbl |> 
  select(count_bears, year) |> 
  na.omit()

count_tbl <- gummi_3_tbl |> 
  pull(year) |> 
  janitor::tabyl()

ggplot(data = gummi_3_tbl, aes(x = count_bears, y = fct_rev(as_factor(year)),
                             fill = fct_rev(as_factor(year)))) +
  theme_minimal() +
  stat_density_ridges() +
  scale_x_continuous(breaks = seq(6, 15, by = 1), limits = c(6, 15)) +
  labs(x = "Anzahl an Bärchen in einer Tüte", y = "Jahr") +
  theme(legend.position = "none") +
  geom_vline(xintercept = c(7, 8, 9, 10, 11, 12, 13, 14), linetype = 2, alpha = 0.5) +
  scale_fill_okabeito() +
  annotate("label", x = 11.5, y = c(1, 2, 3, 4, 5, 6) + 0.7, label = str_c("n = ", rev(count_tbl$n)))

```

In @fig-gummi-mostliked betrachten wir die Verteilung der am meisten gemochten Gummibärchen aufgeteilt nach dem angegebenen Geschlecht im Vergleich zu den Gummibärchen in den Tütchen. Wir sehen, dass Haribo die Tütchen sehr gleichmäßig verteilt und auf die Geschmäcker keinerlei Rücksicht nimmt. Entweder weiß Haribo nichts von den Vorlieben seiner Käufer:innen oder aber es ist dann doch zu viel Aufwand die Produktion anzupassen.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-gummi-mostliked
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramme der am liebsten gemochten Gummibärchchen im Vergleich zum Inhalt der Tütchen."
#| fig-subcap: 
#|   - "Anzahl am liebsten gemochten Gummibärchen aufgeteilt nach Geschlecht."
#|   - "Anzahl der Gummibärchen aufgeteilt nach der Farbe."
#| layout-nrow: 1
#| column: page

gummi_4_tbl <- gummi_tbl |> 
  select(most_liked, gender) |> 
  mutate(most_liked = factor(most_liked, 
                             levels = c("darkred", "green", "white", "lightred",
                                        "yellow", "orange", "none"))) |> 
  na.omit()


ggplot(data = gummi_4_tbl, aes(x = most_liked, fill = gender)) +
  theme_minimal() +
  geom_bar(position = "dodge") +
  #scale_x_continuous(breaks = seq(5, 15, by = 1), limits = c(5, 15)) +
  labs(x = "", y = "Anzahl", fill = "Geschlecht") +
  theme(legend.position = c(0.8, 0.8)) +
  geom_label(stat='count', aes(label=..count.., group = gender), 
             size = 3, fill = "white", position = position_dodge(0.9)) +
  scale_fill_okabeito(order = c(2, 7), 
                      labels = c("männlich", "weiblich"))


gummi_color_tbl <- read_excel("data/gummibears.xlsx") |> 
  select(darkred:white) |> 
  gather(color, count)

count_bag_tbl <- rep(gummi_color_tbl$color, gummi_color_tbl$count) |> 
  as_tibble() |> 
  mutate(value = factor(value, 
                        levels = c("darkred", "green", "white", "lightred",
                                   "yellow", "orange", "none")))

ggplot(count_bag_tbl, aes(value)) +
  theme_minimal() +
  geom_bar(position = "dodge", color = "black", 
           fill = c("brown3", "darkolivegreen3", "white", "coral1", "yellow", "orange")) +
  labs(x = "", y = "Anzahl")  +
  geom_label(stat='count', aes(label=..count..), size = 5) 


```

::: {.callout-note collapse="true"}
## Gibt es einen signifikanten Unterschied für weiße Gummibärchen?

Wenn wir testen wollen, ob es einen signifikanten Unterschied zwischen den Vorlieben zwischen Männern und Frauen für den Geschmack von weißen Gummibärchen gibt, dann können wir die Funktion `prop.test()` nutzen, die zwei Anteile vergleicht. Wir geben hier nur nicht die Anteile $p_1 = 37/86$ und $p2 = 49/86$ als Wahrscheinlichkeiten sondern eben als absolute Zahlen in die Funktion.

```{r}
prop.test(x = c(38, 51), n = c(89, 89))
```

Mehr gibt es dann in dem Kapitel zum [Vergleich zweier Anteile $p_1$ und $p_2$](#sec-chisquare-prop-test). Dort findest du dann auch Möglichkeiten alle Geschmacksrichtungen für die beiden Geschlechter zu verglichen.
:::

## Die Binominalverteilung {#sec-binom}

Die Binomialverteilung wird uns vor allem später in den logistischen Regression wieder begegnen. An dieser Stelle ist es wichtig zu wissen, dass wir es bei der Binomialverteilung mit binären Ereignissen zu tun haben. Wir haben nur Erfolg oder nicht. Daher haben wir nur das Ergebnis $0/1$ daher Null oder Eins. Dieses Ergebnis ist im Prinzip auch die Beschreibung eines Patienten, ob dieser krank oder nicht krank ist. Deshalb finden wir die Binomialverteilung auch häufig in einem medizinischen Kontext.

::: column-margin
Es gibt auch ein schönes Tutorial zur [Binomial Distribution](https://mse.redwoods.edu/darnold/math15/spring2013/R/Activities/binomial1.html) von David Arnold.
:::

Schauen wir uns die Formel für die Binomialverteilung einmal genauer an. Wichtig ist, dass wir etwas $n$-mal wiederholen und uns dann fragen, wie *exakt* $k$-oft haben wir Erfolg.

$$
B(k\mid p,n)=
\begin{cases}
  \binom nk p^k (1-p)^{n-k} &\text{falls} \quad k\in\left\{0,1,\dots,n\right\}\\
  0            & \text{sonst.}
  \end{cases}
$$

mit

-   $n$ gleich der Anzahl an Versuchen (eng. *trails*)
-   $k$ gleich der Anzahl an Erfolgen
-   $p$ gleich der Wahrscheinlichkeit für einen Erfolg.

Bevor wir mit dem Beispiel beginnen können brauchen wir noch etwas mehr für die Berechnung der Formel. Wir brauchen noch für die Berechnung der Binomalverteilung den Binomialkoeffizienten $\tbinom {n}{k}$, den wir wie folgt bestimmen können. Dabei bedeutet das $!$, dass wir eine Zahl aufmultiplizieren. Daher müssen wir für $4!$ dann wie folgt rechnen $4! = 1 \cdot 2 \cdot 3 \cdot 4 = 24$.

$$
\binom nk = \cfrac{n!}{k! \cdot (n-k)!}
$$

Nehmen wir dafür einmal ein Beispiel mit 5 über 3 und schauen uns die Rechnung einmal an. Wir erhalten den Binomialkoeffizienten $\tbinom {5}{3}$ wie folgt.

$$
\binom 5 3 = \frac{5!}{3! \cdot (5-3)!} = \frac{5!}{3! \cdot 2!} = \frac{1\cdot 2\cdot 3\cdot 4\cdot 5}{(1\cdot 2\cdot 3) \cdot (1\cdot 2)} = \frac{4\cdot 5}{1\cdot 2} = 10
$$

Viele Taschenrechner können den Binomialkoeffizienten flott ausrechnen. Wenn wir keinen Taschenrechner haben, dann können wir auch das Pascalsche (oder Pascal'sche) Dreieck nutzen. Das Pascalsche Dreieck ist eine Form der grafischen Darstellung der Binomialkoeffizienten $\tbinom {n}{k}$. Wir sehen einmal in @fig-pascal den Zusammenhang mit dem Binomialkoeffizienten dargestellt. Mit dem Pascalsche Dreieck können wir auch ohne Taschenrechner den Binomialkoeffizienten bestimmen.

![Darstellung des Pascalsche (oder Pascal'sche) Dreieckes im Zusammenhang zum Binomialkoeffizienten.](images/eda/distribution-binom.png){#fig-pascal fig-align="center" width="80%"}

Somit können wir auch einmal ein erweitertes Beispiel der Binomialverteilung rechnen. Was ist die Wahrscheinlichkeit bei $n = 5$ Münzwürfen *genau* dann $k = 2$ Erfolge zu erzielen, wenn die Münze fair ist und damit gilt $p = 0.5$?

$$
\begin{aligned}
Pr(Y = 3) &= \binom {5}{3} 0.5^{3} (1-0.5)^{5-3} \\  
&= 10 \cdot 0.5^3 \cdot 0.5^2 \\
&= 0.31
\end{aligned}
$$

Wir immer können wir die ganze Rechnung dann auch in R durchführen. Dank der Funktion `choose()` können wir schnell den Binomialkoeffizienten berechnen. Der Rest ist dann nur noch das Einsetzen.

```{r}
choose(5,3) * 0.5^3 * 0.5^2
```

Auch hier geht es natürlich auch in R noch einen Schritt schneller. Leider heißt dann wieder alles anders. Wir wollen `x = k = 3` Erfolge aus `size = n = 5` Versuchen mit einer Erfolgswahrscheinlichkeit von `prob = 0.5`. Daran muss man sich dann gewöhnen, dass sich die Begrifflichkeiten dann doch immer mal wieder ändern.

```{r}
dbinom(x = 3, size = 5, prob = 0.5)
```

Was wäre wenn wir jetzt die Wahrscheinlichkeit $Pr(Y \leq 3)$ berechnen wollen? Also nicht *exakt* die Wahrscheinlichkeit für $k=3$ Erfolge sondern eben $k$ Erfolge oder weniger $k \leq 3$. Dann müssen wir die Wahrscheinlichkeiten für $Pr(Y = 0)$, $Pr(Y = 1)$, $Pr(Y = 2 )$ und $Pr(Y = 3)$ berechnen und diese Wahrscheinlichkeiten aufaddieren.

```{r}
dbinom(0, 5, 0.5) + dbinom(1, 5, 0.5) + dbinom(2, 5, 0.5) + dbinom(3, 5, 0.5)
```

Oder wir rechen einfach die Fläche und damit die Wahrscheinlichkeit links von $k = 3$ aus. Dafür haben wir dann die Funktion `pbinom()`. Es geht dann eben doch etwas flotter. Wie immer können wir dann über die Option `lower.tail =` entscheiden, auf welche Seite der Verteilung wir schauen wollen.

```{r}
pbinom(3, 5, 0.5, lower.tail = TRUE)
```

Angenommen, eine Münze wird so gewichtet, dass sie in 60 % der Fälle Kopf ergibt. Wie hoch ist die Wahrscheinlichkeit, dass Sie nach 50 Würfen 25 oder mehr Köpfe erhalten? Dafür können wir dann auch die Funktion `pbinom()` einmal nutzen. Da wir *mehr* wollen, also "größer als", müssen wir rechts von dem berechneten Wert schauen, also auswählen, dass `lower.tail = FALSE` ist.

```{r}
pbinom(25, 50, 0.6, lower.tail = FALSE)
```

Zum Abschluss schauen wir nochmal in unseren Gummibärchendaten, wie dort ein Histogramm einer binären Variable mit nur zwei Ausprägungen aussehen würde. In @fig-gummi-binom sehen wir einmal das Geschlecht als Balkendiagramm dargestellt. Mehr gibt es zu diesem Diagramm erstmal nicht zu berichten. Bei einer Variable bei einem unbekannten $p$ für eine der Kategorien, ist schwer etwas zu bewerten. Wir sehen aber, dass wir eine sehr schöne Gleichverteilung von den Geschlechtern in den Daten haben.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-gummi-binom
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Beispiel für eine Binomialverteilung anhand des Geschlechts. Die fehlenden Angaben wurden entfernt."

na.omit(gummi_tbl) |> 
  mutate(gender = factor(gender, labels = c("männlich", "weiblich"))) |> 
  ggplot(aes(gender, fill = gender)) +
  theme_minimal() +
  geom_bar() +
  labs(x = "", y = "Anzahl") +
  theme(legend.position = "none") +
  scale_fill_okabeito(order = c(2, 7)) +
  geom_label(stat='count', aes(label=..count..), size = 5, fill = "white") +
  scale_y_continuous(limits = c(0, 370), breaks = c(0, 50, 100, 150, 200, 250, 300, 350))

```

## Die Uniformverteilung {#sec-uniform}

Die Gleichverteilung oder Uniformverteilung brauchen wir in der Statistik eher selten. Da wir aber hin und wieder mal auf die Gleichverteilung in technischen Prozessen stoßen, wollen wir uns die Gleichverteilung nochmal anschauen. Wenn wir eine Gleichverteilung vorliegen haben, dann sind alle Kategorien gleich häufig vertreten. Es ergibt sich dann folgende Verteilung als Plateau. Das Eintreten jedes Ereignisses ist gleich wahrscheinlich.

$$
f(y)=
\begin{cases}
  \cfrac 1{b-a} & a \le y \le b\\
  0            & \text{sonst.}
\end{cases}
$$

![Darstellung der Uniformverteilung zwischen den beiden Punkten $a$ und $b$.](images/eda/distribution-uni.png){#fig-dist-uni fig-align="center" width="60%"}

Da gibt es auch sonst wenig mehr zu berichten. Nehmen wir daher nochmal ein technisches Beispiel aus unseren Gummibärchendaten. Wir würden je Farbe $`r floor(nrow(count_bag_tbl)/6)`$ Gummibärchen erwarten. Warum ist das so? Wir haben insgesamt $`r nrow(count_bag_tbl)`$ ausgezählt. Wenn jede der sechs Kategorien mit der gleichen Wahrscheinlichkeit auftritt, dann erwarten wir jeweils $1/6$ von der Gesamtzahl. Wir erkennen, dass wir etwas zu wenig grüne Bärchen haben. Ebenso sind die hellroten Bärchen unterrepräsentiert. Dafür haben wir dann zwangsweise etwas mehr an gelben und orangen Gummibärchen. Dennoch würde ich hier von einer Gleichverteilung ausgehen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-gummi-uniform
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Beispiel für eine uniforme Verteilung anhand der Anzahl der Gummibärchen pro Tüte nach Farbe"
#| fig-subcap: 
#|   - "Verteilung der beobachteten Anzahl der Gummibärchen pro Tüte nach Farbe."
#|   - "Verteilung der theoretischen Anzahl der Gummibärchen pro Tüte nach Farbe."
#| layout-nrow: 1
#| column: page

ggplot(count_bag_tbl, aes(value)) +
  theme_minimal() +
  geom_bar(position = "dodge", color = "black", 
           fill = c("brown3", "darkolivegreen3", "white", "coral1", "yellow", "orange")) +
  ylim(c(0, 1500)) +
  labs(x = "", y = "Anzahl") +
  geom_hline(yintercept = floor(nrow(count_bag_tbl)/6), 
             linetype = 2, size = 1) +
  geom_label(stat='count', aes(label=..count..), size = 5)

tibble(y = rep(nrow(count_bag_tbl)/6, 6),
       grp = as_factor(c("darkred", "green", "white", "lightred", "yellow", "orange"))) |> 
  ggplot(aes(y = y, x = grp)) +
  theme_minimal() +
  ylim(c(0, 1500)) +
  geom_bar(position = "dodge", color = "black", stat = "identity", 
           fill = c("brown3", "darkolivegreen3", "white", "coral1", "yellow", "orange")) +
  labs(x = "", y = "Anzahl") +
  geom_hline(yintercept = floor(nrow(count_bag_tbl)/6), 
             linetype = 2, size = 1) 

```

## Weitere Verteilungen

::: column-margin
Wir besuchen gerne die R Shiny App [The distribution zoo](https://ben18785.shinyapps.io/distribution-zoo/) um mehr über die verschiedenen Verteilungen und deren Parameter zu erfahren.
:::

Weitere Beispiele finden sich unter [Basic Probability Distributions in R](https://rstudio-pubs-static.s3.amazonaws.com/100906_8e3a32dd11c14b839468db756cee7400.html). Im Weiteren liefert @dormann2013parametrische eine gute Übersicht über verschiedene Verteilungen und deren Repräsentation in R. Das ist nur eine Auswahl an möglichen Verteilungen. Bitte hier nicht ins *rabbit hole* der Verteilungen gehen. Wir benötigen in unserer täglichen Arbeit nur einen kleinen Teil der Verteilungen. Es reicht, wenn du eine Vorstellungen der Verteilungen in diesem Kapitel hat.

## Referenzen {.unnumbered}
