```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra)
```

# Simple lineare Regression {#sec-modeling-simple-stat}

in diesem Kapitel wollen wir uns mit den Grundlagen der linearen Regression beschäftigen. Damit meine ich erstmal die Idee eine Gerade durch eine Punktewolke zu zeichnen. Das ist erstmal die simpleste Anwendung. Wir lernen hier einmal die Grundbegriffe und erweitern diese dann auf komplexere Modelle.

Du kanst duch aber davon gedanklich lösen, dass die lineare Regression *nur* eine Methode ist um eine Gerade durch eine Punktewolke zu legen. Die lineare Regression und damit auch das statistische Modellieren kann viel mehr.

## Genutzte R Pakete für das Kapitel

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, conflicted, broom,
               readxl)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Wir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die lineare Regression auf sehr großen Datensätzen anwenden, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit $n=7$ Beobachtungen. In der @tbl-model-0 ist der Datensatz `simplel_tbl` dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in \[cm\] und dem Gewicht in \[mg\] für sieben Beobachtungen modellieren.

```{r}
#| message: false
#| echo: false
#| tbl-cap: Datensatz mit einer normalverteilten Variable `jump_length` und der normalverteilten Variable `weight`. 
#| label: tbl-model-0

simple_tbl <- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),
                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))

simple_tbl %>% kable(align = "c", "pipe")
```

In @fig-scatter-lin-01 sehen wir die Visualisierung der Daten `simple_tbl` in einem Scatterplot mit einer geschätzen Gerade.

```{r}
#| echo: false
#| message: false
#| label: fig-scatter-lin-01
#| fig-align: center
#| fig-height: 4
#| fig-width: 4
#| fig-cap: "Scatterplot der Beobachtungen der Sprungweite in \\[cm\\] und dem Gewicht in \\[mg\\]. Die Gerade verläuft mittig durch die Punkte."

ggplot(simple_tbl, aes(weight, jump_length)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  theme_bw() +
  xlim(0, 3.5) + ylim(0, 3.5)
```

Wir schauen uns in diesem Kapitel nur eine *simple* lineare Regression mit einem $x_1$ an. In unserem Fall ist das $x_1$ gleich dem `weight`. Später schauen wir dann *multiple* lineare Regressionen mit mehreren $x_1,..., x_p$ an.

Bevor wir mit dem Modellieren anfangen können, müssen wir verstehen, wie ein *simples* Modell theoretisch aufgebaut ist. Danach können wir uns das lineare Modell in R anschauen.

## Simple lineare Regression theoretisch

Wir haben nun die ersten sieben Beobachtungen in dem Objekt `simple_tbl` vorliegen. Wie sieht nun theoretisch eine lineare Regression aus? Wir wollen eine Grade durch Punkte legen, wie wie wir es in @fig-scatter-lin-01 sehen. Die blaue Gerade wir durch eine Geradengleichung beschreiben. Du kenst vermutlich noch die Form $y = mx + b$. In der Statistik beschreiben wir eine solche Gerade aber wie folgt.

$$
y \sim \beta_0 + \beta_1 x_1 + \epsilon
$$

mit

-   $\beta_0$ als den y-Achsenabschnitt.
-   $\beta_1$ als der Steigung der Geraden.
-   $\epsilon$ als Residuen oder die Abweichungen von den $y$-Werten auf Geraden zu den einzelnen $y$-Werten der Beobachtungen.

In @tbl-reg-deu-eng siehst du nochmal in einer Tabelle den Vergleich von der Schreibweise der linearen Regression in der Schule und in der Statistik. Darüber hinaus sind die deutschen Begriffe den englischen Begriffen gegenüber gestellt. Warum schreiben wir die Gleichung in der Form? Damit wir später noch weitere $\beta_px_p$-Paare ergänzen könen und so *multiple* Modelle bauen können.

| $\boldsymbol{y = mx +b}$ | $\boldsymbol{y \sim \beta_0 + \beta_1 x_1 + \epsilon}$ |      Deutsch      |  Englisch   |
|:------------------------:|:------------------------------------------------------:|:-----------------:|:-----------:|
|           $m$            |                       $\beta_1$                        |     Steigung      |    Slope    |
|           $x$            |                         $x_1$                          | Einflussvariable  | Risk factor |
|           $b$            |                       $\beta_0$                        | y-Achsenabschnitt |  Intercept  |
|                          |                       $\epsilon$                       |     Residuen      |  Residual   |

: Vergleich und Übersicht der schulischen vs. statistischen Begriffe in den linearen Regression sowie die deutschen und englischen Begriffe. {#tbl-reg-deu-eng}

In @fig-lin-reg-01 sehen wir die Visualisierung der Gleichung in einer Abbildung. Die Gerade läuft durch die Punktewolke und wird durch die statistischen Maßzahlen bzw. Parameter $\beta_0$, $\beta_1$ sowie den $\epsilon$ beschrieben. Wir sehen, dass das $\beta_0$ den *Intercept* darstellt und das $\beta_1$ die Steigung der Geraden. Wenn wir $x$ um 1 Einheit erhöhen $x+1$, dann steigt der $y$ Wert um den Wert von $\beta_1$. Die einzelnen Abweichungen der beobachteten $y$-Wert zu den $y$-Werten auf der Gerade ($\hat{y}$) werden als Residuen oder auch $\epsilon$ bezeichnet.

![Visualisierung der linearen Regression. Wir legen eine Gerade durch eine Punktewolke. Die Gerade wird durch die statistischen Maßzahlen bzw. Parameter $\beta_0$, $\beta_1$ sowie den $\epsilon$ beschrieben.](images/statistical_modeling_lm){#fig-lin-reg-01 fig-align="center" width="100%"}

[In R werden die $\hat{y}$ auch *fitted values* genannt. Die $\epsilon$ Werte werden dann *residuals* bezeichnet.]{.aside}

Schauen wir uns einmal den Zusammenhang von $y$, den beobachteten Werten, und $\hat{y}$, den geschätzen Werten auf der Gerade in unserem Beispiel an. In @tbl-lin-reg-epsilon sehen wir die Berechnung der einzelnen Residuen für die Gerade aus der @fig-scatter-lin-01. Wir nehmen jedes beobachtete $y$ und ziehen den Wert von $y$ auf der Gerade, bezeichnet als $\hat{y}$, ab. Diesen Schritt machen wir für jedes Wertepaar $(y_i; \hat{y}_i)$.

::: column-page
|  x  |  y  | $\boldsymbol{\hat{y}}$ | Residuen ($\boldsymbol{\epsilon}$) |               Wert                |
|:---:|:---:|:----------------------:|:----------------------------------:|:---------------------------------:|
| 0.8 | 1.2 |          1.38          |   $\epsilon_1 = y_1 - \hat{y}_1$   | $\epsilon_1 = 1.2 - 1.38 = -0.18$ |
| 1.0 | 1.8 |          1.48          |   $\epsilon_2 = y_2 - \hat{y}_2$   | $\epsilon_2 = 1.8 - 1.48 = +0.32$ |
| 1.2 | 1.3 |          1.58          |   $\epsilon_3 = y_3 - \hat{y}_3$   | $\epsilon_3 = 1.3 - 1.58 = -0.28$ |
| 1.9 | 1.7 |          1.94          |   $\epsilon_4 = y_4 - \hat{y}_4$   | $\epsilon_4 = 1.7 - 1.94 = -0.24$ |
| 2.0 | 2.6 |          1.99          |   $\epsilon_5 = y_5 - \hat{y}_5$   | $\epsilon_5 = 2.6 - 1.99 = +0.61$ |
| 2.7 | 1.8 |          2.34          |   $\epsilon_6 = y_6 - \hat{y}_6$   | $\epsilon_6 = 1.8 - 2.34 = -0.54$ |
| 2.8 | 2.7 |          2.40          |   $\epsilon_7 = y_7 - \hat{y}_7$   | $\epsilon_7 = 2.7 - 2.40 = +0.30$ |

: Zusammenhang zwischen den $y$, den beobachteten Werten, und $\hat{y}$, den geschätzen Werten auf der Gerade. Wir nennen den Abstand $y_i - \hat{y}_i$ auch Residuum oder *Epsilon* $\epsilon$. {#tbl-lin-reg-epsilon}
:::

```{r}
#| echo: false

epsilon <- c(-0.18, 0.32, -0.28, -0.24, 0.61, -0.54, 0.30)
mean_e <- round(mean(epsilon), 2)
var_e <- round(var(epsilon), 2)

```

Die Abweichungen $\epsilon$ oder auch Residuen genannt haben einen Mittelwert von $\bar{\epsilon} = `r mean_e`$ und eine Varianz von $s^2_{\epsilon} = `r var_e`$. Wir schreiben, dass die Residuen normalverteilt sind mit $\epsilon \sim \mathcal{N}(0, s^2_{\epsilon})$. Wir zeichnen die Gerade also so durch die Punktewolke, dass die Abstände zu den Punkten, die Residuen, im Mittel null sind. Die Optimierung erreichen wir in dem wir die Varianz der Residuuen minimieren. Folglich modellieren wir die Varianz.

## Simples lineare Regression in R

Im Allgemeinen können wir ein Modell in R wie folgt schreiben. Wir brauchen *ein* y auf der linken Seite und in der simplen linearen Regressione ein $x$ auf der rechten Seite der Gleichung. Wir brauchen also zwei Variablen $y$ und $x$, die natürlich nicht im Datensatz in R so heißen müssen.

![Modellschreibweise $y$ hängt ab von $x$. Das $y$ repräsentiert eine Spalte im Datensatz und das $x$ repräsentiert ebenso eine Spalte im Datensatz.](images/statistical_modeling_0.png){#fig-lin-reg-01 fig-align="center" width="30%"}

Konkret würden wir in unserem Beispiel das Modell wie folgt benennen. Das $y$ wird zu `jump_length` und das $x$ wird zu `weight`. Wir haben dann das Modell in der simplesten Form definiert.

![Modellschreibweise bzw. `formula` Schreibweise in R. Die Variable $y$ hängt ab von $x$ am Beispiel des Datensatzes `simple_tbl` mit den beiden Variablen `jump_length` als $y$ und `weight` als $x$.](images/statistical_modeling_2.png){#fig-lin-reg-22 fig-align="center" width="80%"}

Nachdem wir das Modell definiert haben, setzen wir dieses Modell `jump_length ~ weight` in die Funktion `lm()` ein um das lineare Modell zu rechnen. Wie immer müssen wir auch festlegen aus welcher Datei die Spalten genommen werden sollen. Das machen wir mit der Option `data = simple_tbl`. Wir speichern dann die Ausgabe der Funktion `lm()` in dem Objekt `fit_1` damit wir die Ausgabe noch in andere Funktionen pipen können.

```{r}
fit_1 <- lm(jump_length ~ weight, data = simple_tbl)
```

Im ersten Schritt schauen wir uns die Ausgabe der Funktion `lm()` in der Funktion `summary()` an. Daher pipen wir das Objekt `fit_1` in die Funktion `summary()`.

```{r}
#| eval: false

fit_1 %>% summary
```

Wir erhalten folgende Ausgabe dargestellt in @fig-lin-reg-3.

![Die `summary()` Ausgabe des Modells `fit_1`.](images/statistical_modeling_3.png){#fig-lin-reg-3 fig-align="center" width="100%"}

Was sehen wir in der Ausgabe der `summary()` Funktion? Als erstes werden uns die Residuen wiedergegeben. Wenn wir nur wenige Beobachtungen haben, dann werden uns die Residuen direkt wiedergegeben, sonst die Verteilung der Residuen. Mit der Funktion `augment()` aus dem R Paket `broom` können wir uns die Residuen wiedergeben lassen. Die Residuen schauen wir uns aber nochmal im @sec-lin-reg-quality genauer an.

```{r}
fit_1 %>% augment
```

Im zweiten Block erhalten wir die Koeffizienten (eng. *coefficients*) der linearen Regression. Das heißt, wir kriegen dort $\beta_0$ als y-Achsenabschnitt sowie die Steigung $\beta_1$ für das Gewicht. Dabei ist wichtig zu wissen, dass immer als erstes der y-Achsenabschnitt `(Intercept)` auftaucht. Dann die Steigungen der einzelnen $x$ in dem Modell. Wir haben nur *ein* kontinuierliches $x$, daher ist die Interpretation der Ausgabe einfach. Wir können die Gradengleichung wie folgt formulieren.

$$
jump\_length \sim 0.97 + 0.51 \cdot weight
$$

Was heißt die Gleichung nun? Wenn wir das $x$ um eine Einheit erhöhen dann verändert sich das $y$ um den Wert von $\beta_1$. Wir haben hier eine Steigung von $0.51$ vorliegen. Ohne Einheit keine Interpretation! Wir wissen, dass das Gewicht in \[mg\] gemessen wurde und die Sprungweite in \[cm\]. Damit können wir aussagen, dass wenn ein Floh 1 mg mehr wiegt der Floh 0.51 cm weiter springen würde.

Schauen wir nochmal in die *saubere* Ausgabe der `tidy()` Funktion. Wir sehen nämlich noch einen $p$-Wert für den Intercept und die Steigung von `weight`.

```{r}
fit_1 %>% tidy
```

Wenn wir einen $p$-Wert sehen, dann brauchen wir eine Nullhypothese, die wir dann eventuell mit der Entscheidung am Signifikanzniveau $\alpha$ von 5% ablehnen können. Die Nullhypothese ist die Gleichheitshypothese. Wenn es also keinen Effekt von dem Gewicht auf die Sprungweite gebe, wie groß wäre dann $\beta_1$? Wir hätten dann keine Steigung und die Grade würde parallel zur x-Achse laufen. Das $\beta_1$ wäre dann gleich null.

```{=latex}
\begin{align*} 
H_0: \beta_i &= 0\\  
H_A: \beta_i &\neq 0 \\   
\end{align*}
```
Wir haben für jedes $\beta_i$ ein eigenes Hypothesenpaar. Meistens interessiert uns der Intercept nicht. Ob der Intercept nun durch die Null geht oder nicht ist eher von geringem Interessen.

Spannder ist aber wie sich der $p$-Wert berechnet. Der $p$-Wert basiert auf einer t-Statistik, also auf dem t-Test. Wir rechnen für jeden Koeffizienten $\beta_i$ einen t-Test. Das machen wir in dem wir den Koeffizienten `estimate` durch den Fehler des Koeffizienten `std.error` teilen.

```{=latex}
\begin{align*} 
T_{(Intercept)} &= \cfrac{\mbox{estimate}}{\mbox{std.error}}  = \cfrac{0.96864}{0.44470} = 2.1782\\  
T_{weight} &= \cfrac{\mbox{estimate}}{\mbox{std.error}}  = \cfrac{0.50964}{0.23155} = 2.2010\\   
\end{align*}
```
Zun Abschluß noch die Funktion `glance()` ebenfalls aus dem R Paket `broom`, die uns erlaubt noch die Qualitätsmaße der linearen Regression zu erhalten. Wir müssen nämlich noch schauen, ob die Regression auch funktioniert hat. Die Überprüfung geht mit einem $x$ sehr einfach. Wir können uns die Grade ja anschauen. Das geht dann mit einem Model mit mehreren $x$ nicht mehr und wir brauchen andere statistsiche Maßzahlen.

```{r}
fit_1 %>% glance 
```
