```{r echo = FALSE}
#| message: false
#| warning: false
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc, quantreg,
               multcomp, emmeans, ggpubr, multcompView, nlme, tinytable,
               see, patchwork, conflicted)
conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
```

```{r}
#| echo: false
#| message: false
#| warning: false
source("images/R/eda-transform.R")
```

# Transformieren von Daten {#sec-eda-transform}

*Letzte Änderung am `r format(fs::file_info("eda-transform.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"It's morphin' time! I never get tired of that! Now, go, go Power Rangers!" --- Power Rangers morphing phrase*

![](images/caution.png){fig-align="center" width="100%"}

::: {.callout-caution appearance="simple"}
## Stand des Kapitels: Baustelle (seit 06.2025)

Dieses Kapitel wird überarbeitet, da ich nochmal an den Kern der Transformation ran möchte. Daher kann es sein, dass es in den nächsten Wochen hier zu Problemen und nicht funktionierenden Code kommt. Das repariere ich dann aber meistens in den folgenden Tagen. Ziel ist es bis zum Start des WiSe 2025/26 das Kapitel neu aufgestellt zu haben.
:::

text

## Allgemeiner Hintergrund

Warum müssen wir Daten transformieren? Meistens hat dies drei Hauptgründe.

#### Das Modell {.unnumbered .unlisted}

1)  Wir wollen eine ANOVA oder eine Gaussian lineare Regression rechen und benötigen ein normalverteiltes Outcome $y$.
2)  Wir wollen einen Algorithmus zur Prädiktion (deu. *Vorhersage*) nutzen und haben sehr viele Einflussvariablen $x$ in sehr unterschiedlichen Einheiten.
3)  Wir wollen eine komplexere Analyse wie die Hauptkomponentenanalyse oder Clusteranalyse rechnen und brauchen Variablen, die alle eine ähnliche Spannweite haben.

Im ersten Fall wollen wir meist unsere Daten $log$-Transformieren um aus einem *nicht*-normalverteilten Outcome $y$ ein $log$-normalverteiltes $y$ zu erschaffen. Im zweiten Fall wollen wir unsere Daten Standardisieren oder Normalisieren. Wir brauchen normalisierte Daten später beim Klassifizieren im Rahmen von maschinellen Lernverfahren. Bitte beachte auch, dass die Transformationen hier eher für kleine Datensätze geeignet sind. Im [Kapitel zur Klassifikation](#sec-pre-processing) gehe ich nochmal auf die Automatisierung über mehrere Variablen ein. Wenn du also einen großen Datensatz hast, den du vielleicht oft bearbeiten musst, dann mag dir dort mehr geholfen sein.

| Col1 | Col2 |
|------|------|
|      |      |
|      |      |
|      |      |

: test {#tbl-tukey-ladder}

Ein Wort zum R Paket `{recipes}`

Wir wollen uns nun die Verfahren zur Transformation von Daten in den folgenden Abschnitten einmal näher anschauen.

::: callout-tip
## Weitere Tutorien für das Transformieren von Daten.

Wir oben schon erwähnt, kann dieses Kapitel nicht die umfangreiche Literatur der Transformation von Daten abarbeiten. Daher präsentiere ich hier eine Liste von Literatur und Links, die mich für dieses Kapitel hier inspiriert haben. Nicht alles habe ich genutzt, aber vielleicht ist für dich was dabei.

-   Ich verweise hier auch nochmal auf das tolle Tutorium von Matus Seci auf dem [Coding Club - Transforming and scaling data: Understand the fundamental concepts of manipulating data distributions for modelling and visualization](https://ourcodingclub.github.io/tutorials/data-scaling/). Du findest auch dort mehr Informationen zu der Rücktransformation und der Anwendung von Transformationen auf einen komplexeren Datensatz.
-   [Transformations](https://onlinestatbook.com/2/transformations/contents.html)
-   [Data Transformation mit `dlookr`](https://choonghyunryu.github.io/dlookr/articles/transformation.html#standardization-and-resolving-skewness)
-   [Transforming Data](https://rcompanion.org/handbook/I_12.html)
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r}
#| message: false
#| warning: false
#| echo: true

pacman::p_load(tidyverse, magrittr, scales, see, MASS, bestNormalize,
               dlookr, conflicted)
conflicts_prefer(MASS::boxcox)
conflicts_prefer(dplyr::select)
conflicts_prefer(dplyr::filter)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

Zwischenzeitlich musste ich das R Paket `{dlookr}` aus dem Code entfernt, da das Paket extra installieren musstest. Das R Paket war nicht mehr bei CRAN gelistet. Das führt bei mir zu Schluckauf im Code, so dass ich die Funktionen jetzt in einem Extraabschnitt bearbeite. Sollte das Paket sich mal nicht installieren lassen, dann gibt es hier noch einen alternativen Weg.

::: callout-caution
## Alternative Installation von `{dlookr}`

Ist das R Paket `{dlookr}` mal nicht per Standard aus RStudio zu installieren kannst du das R Paket auch über GitHub installieren. Dafür einmal das R Paket `{devtools}` installieren und dann folgenden Code ausführen.

```{r}
#| eval: false
devtools::install_github("choonghyunryu/dlookr")
```
:::

An der Seite des Kapitels findest du den Link *Quellcode anzeigen*, über den du Zugang zum gesamten R-Code dieses Kapitels erhältst.

## Daten

Wir wollen uns in diesem Kapitel mit der normalverteilten Variable `jump_length` gemessen in \[cm\] und der nicht-normalverteilten Variable `hatch_time` gemessen in \[h\] aus dem Datensatz `flea_dog_cat_length_weight.csv"` beschäftigen. Wir wählen über die Funktion `select()` nur die beiden Spalten aus dem Datensatz, die wir benötigen.

```{r}
#| message: false

fac2_tbl <- read_excel("data/flea_dog_cat_length_weight.xlsx") |>
  select(jump_length, hatch_time) |> 
  filter(hatch_time <= 1500)
```

In der @tbl-trans-1 ist der Datensatz `fac2_tbl` nochmal dargestellt. Wir zeigen hier nur die ersten sieben zeilen des Datensatzes.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-trans-1
#| tbl-cap: "Selektierter Datensatz mit einer normalverteilten Variable `jump_length` und der nicht-normalverteilten Variable `hatch_time`. Wir betrachten die ersten sieben Zeilen des Datensatzes."

fac2_raw_tbl <- read_excel("data/flea_dog_cat_length_weight.xlsx") |>
  select(animal, jump_length, hatch_time) 

rbind(head(fac2_raw_tbl, n = 3),
      rep("...", times = ncol(fac2_raw_tbl)),
      tail(fac2_raw_tbl, n = 3)) |> 
  tt(width = 2/3, align = "c", theme = "striped")
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-data-intro
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "foo. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(fac2_tbl, aes(jump_length)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Sprungweiten in [cm]", y = "") +
  xlim(8, 33)

p2 <- ggplot(fac2_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in sqrt(h)", y = "") +
  xlim(-100, 1500)

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

Im Folgenden nutzen wir oft die Funktion `mutate()`. Schau dir im Zweifel nochmal im Kapitel zu Programmierung die Funktion `mutate()` an.

## Transformationen

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pretest-rank
#| fig-align: center
#| fig-height: 4
#| fig-width: 8
#| fig-cap: "Zusammenhang vom Mittelwert, Median und dem Modus zur Feststellung einer Normalverteilung. Der Modus ist hierbei der häufigste Wert. **(A)** Linksschiefe Verteilung. Der Modus ist größer als der Median ist größer als der Mittelwert. **(B)** Symmetrische Normalverteilung. Der Mittelwert und Median sowie Modus sind gleich. **(C)** Rechtsschiefe Verteilung der Mittelwert ist größer als der Median ist größer als der Modus. *[Zum Vergrößern anklicken]*"

p2kurt + p1kurt + p3kurt +
  plot_layout(ncol = 3) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))
```

Quadratwurzel moderat schiefe Messwerte:

-   `sqrt(y)` für positiv, schiefe Messwerte
-   `sqrt(max(y+1) - y)` für negative, schiefe Messwerte

Logarithmus für starke schiefe Messwerte:

-   `log10(y)` für positiv, schiefe Messwerte
-   `log10(max(y+1) - y)` für negative, schiefe Messwerte

Die Inverse für extrem, schiefe Messwerte:

-   `1/y` für positiv, schiefe Messwerte
-   `1/(max(y+1) - y)` für negative, schiefe Messwerte

Die Transformation mit Rängen `rank()`

Schnelle Übersicht...

::: panel-tabset
## `sqrt(y)`

```{r}
sqrt_tbl <- fac2_tbl |> 
  mutate(sqrt_hatch_time = sqrt(hatch_time))
```

```{r}
#| message: false
#| echo: false
#| label: fig-log-pretest-2
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $sqrt$-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(sqrt_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA)

p2 <- ggplot(sqrt_tbl, aes(sqrt_hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in sqrt(h)", y = "")

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

## `log(y)`

```{r}
log_tbl <- fac2_tbl |> 
  mutate(log_hatch_time = log(hatch_time))
```

```{r}
#| message: false
#| echo: false
#| label: fig-log-pretest-1
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $log$-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(log_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(log_tbl, aes(log_hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```

## `1/y`

```{r}
inverse_tbl <- fac2_tbl |> 
  mutate(inverse_hatch_time = 1/hatch_time)
```

```{r}
#| message: false
#| echo: false
#| label: fig-inverse-pretest-1
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 8
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten. **(A)** Nicht transformierte, rohe Daten. **(B)** $inverse$-transformierte Daten. *[Zum Vergrößern anklicken]*"
#| layout-nrow: 1

p1 <- ggplot(inverse_tbl, aes(hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[2], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "") +
  xlim(-100, NA) 

p2 <- ggplot(inverse_tbl, aes(inverse_hatch_time)) +
  theme_minimal() +
  geom_density(fill = cbbPalette[3], color = "black") +
  labs(x = "Zeit bis zum Schlüpfen in (1/h)", y = "")  

p1 + p2 +
  theme(panel.grid.minor.x = element_blank()) +
  plot_annotation(tag_levels = 'A', tag_prefix = '(', tag_suffix = ')') &
  theme(plot.tag = element_text(size = 16, face = "bold"))

```
:::

### Quadratwurzel

Die Quadratwurzel-Transformationen ist eine etwas seltenere Transformation. Meist wird die Quadratwurzel-Transformationen als die schwächere $log$-Transformation bezeichnet. Wir sehen in @fig-log-scale-2-2 den Grund dafür. Aber zuerst müssen wir aber über die Funktion `sqrt()` unsere Daten transformieren. Wir können auch die Funktion `transform()` aus dem R Paket `{dlookr}` verwenden und haben eine große Auswahl an möglichen Transformationen. Einfach mal die Hilfeseite von `transform()` aufrufen und nachschauen.

```{r}

sqrt_tbl <- fac2_tbl |> 
  mutate(sqrt_hatch_time = sqrt(hatch_time))

```

In @fig-log-scale-2-1 sehen wir die nicht transformierte, rohe Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam nach rechts aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. @fig-log-scale-2-2 zeigt die Wurzel-transmutierten Daten. Unser Ziel besser normalverteilte Daten vorliegen zu haben, haben wir aber mit der Quadratwurzel-Transformationen nicht erreicht. Die Daten sind immer noch rechtsschief. Wir würden also die $log$-Transformation bevorzugen.

```{r}
#| message: false
#| echo: false
#| label: fig-log-scale-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten."
#| fig-subcap: 
#|   - "Nicht transformierte, rohe Daten"
#|   - "Wurzel-transformierte Daten."
#| layout-nrow: 1

ggplot(sqrt_tbl, aes(hatch_time)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "Anzahl")

ggplot(sqrt_tbl, aes(sqrt_hatch_time)) +
  geom_histogram(fill = cbbPalette[3], color = "black") +
  theme_minimal() +
  labs(x = expression("Zeit bis zum Schlüpfen in" ~ sqrt(h)), y = "Anzahl")
```

### Logarithmus

Wir nutzen die $log$-Transformation, wenn wir aus einem nicht-normalverteiltem Outcome $y$ ein approxomativ normalverteiltes Outcome $y$ machen wollen. Dabei ist wichtig, dass wir natürlich auch die Einheit mit $log$-transformieren.

Im Folgenden sehen wir die $log$-Transformation der Variable `hatch_time` mit der Funktion `log()`. Wir erschaffen eine neue Spalte im `tibble` damit wir die beiden Variable vor und nach der $log$-Transformation miteinander vergleichen können.

Das [R Paket `{dlookr}`](https://yuzar-blog.netlify.app/posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data/#standardization-and-resolving-skewness) hat eine große Auswahl an implementierten Funktionen für $y$-Transformationen.

```{r}

log_tbl <- fac2_tbl |> 
  mutate(log_hatch_time = log(hatch_time))

```

Wir können dann über ein Histogramm die beiden Verteilungen anschauen. In @fig-log-scale-1-1 sehen wir die nicht transformierte, rohe Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. @fig-log-scale-1-2 zeigt die $log$-transmutierten Daten. In diesem Fall sehen wir normalverteilte Daten. Wir haben also ein $log$ normalverteiltes Outcome $y$ mit dem wir jetzt weiterechnen können.

```{r}
#| message: false
#| echo: false
#| label: fig-log-scale-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten."
#| fig-subcap: 
#|   - "Nicht transformierte, rohe Daten"
#|   - "$log$-transformierte Daten."
#| layout-nrow: 1

ggplot(log_tbl, aes(hatch_time)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = "Zeit bis zum Schlüpfen in [h]", y = "Anzahl")

ggplot(log_tbl, aes(log_hatch_time)) +
  geom_histogram(fill = cbbPalette[3], color = "black") +
  theme_minimal() +
  labs(x = "Zeit bis zum Schlüpfen in log(h)", y = "Anzahl")

```

In der Abbildung @fig-log-scale-ggplot siehst du nochmal die Anwendung der $\log$-Skala in `ggplot` auf die Daten. Aber Achtung, die ursprünglichen Daten sind *nicht* transformiert. Du schaust dir hier nur an, wie die Daten transformiert aussehen würden. Es gibt auch hierzu ein kleines Tutorium zu [ggplot log scale transformation](https://www.datanovia.com/en/blog/ggplot-log-scale-transformation/). Da kannst du dann auch einmal nachschauen, wie die $\log_2$-Skala in `ggplot` funktioniert.

```{r}
#| message: false
#| echo: true
#| label: fig-log-scale-ggplot
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm der nicht transfomierten Daten einmal auf der $log_{10}$-Skala in `ggplot` dargestellt. Achtung, hier ist nur die Darstellung transformiert. Die ursprünglichen Daten bleiben von der Transformation *in* `ggplot` unberührt."

ggplot(log_tbl, aes(hatch_time)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = expression("Zeit bis zum Schlüpfen in" ~ log[10](h)), y = "Anzahl") +
  ## here starts the log scale
  scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x), # <1>
                labels = trans_format("log10", math_format(10^.x))) + # <2>
  annotation_logticks(sides = "b")  # <3>

```

1.  Hier wird die $x$-Achse einmal auf die $\log$-Skala gestellt und die `breaks` entsprechend angepasst.
2.  Wir ergänzen noch die richtige Schreibweise.
3.  Die Ticks werden auch richtig gezeichnet. Du kannst mit `l` auch auf der $y$-Achse $\log$-Ticks anzeigen lassen.

### Inverse

### Inverse Normalverteilung

[Inverse normal transformation -- What is it and how to do it?](https://yingji15.github.io/2019-08-17-inverse-normal-transformation/)

```{r}
invers_norm <- function(x) qnorm((rank(x, na.last="keep") - 0.5)/sum(!is.na(x)))
```

[What is the inverse normal transformation (INT) and what are the reasons behind using it?](https://stats.stackexchange.com/questions/588992/what-is-the-inverse-normal-transformation-int-and-what-are-the-reasons-behind)

[Rank-Based Inverse Normal Transformations are Increasingly Used, But are They Merited?](https://pmc.ncbi.nlm.nih.gov/articles/PMC2921808/) von @beasley2009rank

## Skalierung

### Standardisierung

Die Standardisierung wird auch $z$-Transformation genannt. In dem Fall der Standardisierung schieben wir die Daten auf den Ursprung, in dem wir von jedem Datenpunkt $y_i$ den Mittelwert $\bar{y}$ abziehen. Dann setzen wir noch die Standardabweichung auf Eins in dem wir durch die Standardabweichung $y_s$ teilen. Unser standardisiertes $y$ ist nun standardnormalverteilt mit $\mathcal{N(0,1)}$. Wir nutzen für die Standardisierung folgende Formel.

$$
y_z = \cfrac{y_i - \bar{y}}{s_y} 
$$

In R können wir für die Standardisierung die Funktion `scale()` verwenden. Wir müssen auch nichts weiter in den Optionen von `scale()` angeben. Die Standardwerte der Funktion sind so eingestellt, dass eine Standardnormalverteilung berechnet wird. Wir können auch die Funktion `transform()` aus dem R Paket `{dlookr}` verwenden. Die Option wäre dann `zscore`.

```{r}
scale_tbl <- fac2_tbl |> 
  mutate(scale_jump_length = scale(jump_length))
```

In @fig-log-scale-3-1 sehen wir nochmal die nicht transformierten, rohen Daten. Wir haben in diesem Beispiel die normalvertielte Variable `jump_length` gewählt. Der Mittelwert von `jump_length` ist `r round(mean(fac2_tbl$jump_length), 2)` und die Standardabweichung ist `r round(sd(fac2_tbl$jump_length), 2)`. Ziehen wir nun von jedem Wert von `jump_length` den Mittelwert mit 19.3 ab, so haben wir einen neuen Schwerpunkt bei Null. Teilen wir dann jede Zahl durch 3.36 so haben wir eine reduzierte Spannweite der Verteilung. Es ergibt sich die @fig-log-scale-3-2 als Standardnormalverteilung. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die Sprungweite auf der $z$-Skala nicht mehr biologisch interpretieren.

```{r}
#| message: false
#| echo: false
#| label: fig-log-scale-3
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten."
#| fig-subcap: 
#|   - "Nicht transformierte, rohe Daten"
#|   - "$z$-transformierte Daten."
#| layout-nrow: 1


ggplot(scale_tbl, aes(jump_length)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = "Sprungweite in [cm]", y = "Anzahl")

ggplot(scale_tbl, aes(scale_jump_length)) +
  geom_histogram(fill = cbbPalette[3], color = "black") +
  theme_minimal() +
  labs(x = "Sprungweite auf der z-Skala", y = "Anzahl")

```

### Normalisierung

Abschließend wollen wir uns nochmal die Normalisierung anschauen. In diesem Fall wollen wir die Daten so transformieren, dass die Daten nur noch in der Spannweite 0 bis 1 vorkommen. Egal wie die Einheiten vorher waren, alle Variablen haben jetzt nur noch eine Ausprägung von 0 bis 1. Das ist besonders wichtig wenn wir viele Variablen haben und anhand der Variablen eine Vorhersage machen wollen. Uns interessieren die Werte in den Variablen an sich nicht, sondern wir wollen ein Outcome vorhersagen. Wir brauchen die Normalisierung später für das maschinelle Lernen und die Klassifikation. Die Formel für die Normalisierung lautet wie folgt.

$$
y_n = \cfrac{y_i - \min(y)}{\max(y) - \min(y)} 
$$

In R gibt es die Normalisierungsfunktion nicht direkt. Wir könnten hier ein extra Paket laden, aber bei so einer simplen Formel können wir auch gleich die Berechnung in der Funktion `mutate()` machen. Wir müssen nur etwas mit den Klammern aufpassen. Wir können auch die Funktion `transform()` aus dem R Paket `{dlookr}` mit der Option `minmax` verwenden.

```{r}
norm_tbl <- fac2_tbl |> 
  mutate(norm_jump_length = (jump_length - min(jump_length))/(max(jump_length) - min(jump_length)))
```

In @fig-log-scale-4-1 sehen wir nochmal die nicht transformierten, rohen Daten. In @fig-log-scale-4-2 sehen wir die normalisierten Daten. Hier fällt dann auf, dass die normalisierten Sprungweiten nur noch Werte zwischen Null und Eins annehmen. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die normalisierten Sprungweiten nicht mehr biologisch interpretieren.

```{r}
#| message: false
#| echo: false
#| label: fig-log-scale-4
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm der nicht transfomierten und transformierten Daten."
#| fig-subcap: 
#|   - "Nicht transformierte, rohe Daten"
#|   - "Normalisierte Daten"
#| layout-nrow: 1

ggplot(norm_tbl, aes(jump_length)) +
  geom_histogram(fill = cbbPalette[2], color = "black") +
  theme_minimal() +
  labs(x = "Sprungweite in [cm]", y = "Anzahl")

ggplot(norm_tbl, aes(norm_jump_length)) +
  geom_histogram(fill = cbbPalette[3], color = "black") +
  theme_minimal() +
  labs(x = "Normalisierte Sprungweite", y = "Anzahl")

```

## Automatische Transformationen...

[Normalisierung vs. Standardisierung: Wie man den Unterschied erkennt](https://www.datacamp.com/de/tutorial/normalization-vs-standardization)

Die Standardisierung versucht auch, die Symmetrie mit dem Mittelwert 0 und der Standardabweichung 1 zu erzwingen. Bei der Normalisierung ist die Wahrscheinlichkeit einer Schiefe größer, da Ihre Skalierungsfaktoren die beiden extremsten Datenpunkte sind.

### ... mit `{bestNormalize}`

Dann sind wir bis hierher gekommen und fragen uns nun welche ist den jetzt die beste Transformation für unsere Daten um einigermaßen eine Normalverteilung hinzubekommen? Da hilft uns jetzt das [R Paket `{bestNormalize}`](https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html). Die [Kurzanleitung zum Paket](https://petersonr.github.io/bestNormalize/index.html) erlaubt es automatisiert auf einem Datenvektor die beste Transformation zu finden um die Daten in eine Normalverteilung zu verwandeln. Wie immer geht noch mehr, aber wir machen hier einmal den schnellen Durchlauf. Wir bauen uns dazu einmal einen Datenvektor `x` mit 1000 Beobachtungen, die einer Gamma-Verteilung folgen.

```{r}
x <- rgamma(1000, 1, 1)
```

Jetzt fragst du dich, wie sieht so eine Gamma-Verteilung aus? Zuerst natürlich nicht wie eine Normalverteilung. Ich habe dir in der @fig-trans-bestNorm-1 einmal das Histogramm der x-Werte dargestellt. Wir haben also eine sehr schiefe Verteilung vorliegen.

```{r}
#| message: false
#| echo: false
#| label: fig-trans-bestNorm-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 7
#| fig-cap: "Histogramm der Gamma-Verteilung."
ggplot(as_tibble(x), aes(x)) +
   geom_histogram(fill = cbbPalette[4], color = "black") +
  theme_minimal() 
```

Jetzt können wir die Funktion `bestNormalize()` nutzen um uns die beste Transformationsmethode wiedergeben zu lassen. Das ist super praktisch in der Anwendung. Wir können uns theoretisch noch verschiedene Gütekriterien aussuchen aber für hier reicht die Standardimplementierung. Wichtig ist noch, dass wir hier eine Kreuzvalidierung durchführen, so dass die Ergebnisse und die Auswahl des Algorithmus robust sein sollte.

```{r}
bn_obj <- bestNormalize(x)
bn_obj
```

Laut dem Algorithmus sollen wir eine `Box Cox Transformation` durchführen. Okay, wie machen wir das jetzt? Wir hätten zwar die Parameter in der Ausgabe angegeben und die könnten wir dann auch in einer Veröffentlichung angeben, aber wenn es schneller gehen soll, dann können wir die Funktion `predict()` nutzen, die uns die transformierten `x` Werte wiedergibt.

```{r}
x_trans <- predict(bn_obj)
```

Dann nochmal schnell gucken, ob das auch mit der Rücktransformation klappen würde.

```{r}
x_trans_back <- predict(bn_obj, newdata = x_trans, inverse = TRUE)
```

Einmal dann der Vergleich ob alle `x_trans_back` Werte den ursprünglichen `x` Werten entsprechen. Ja, das sieht gut aus.

```{r}
all.equal(x_trans_back, x)
```

Und zum Abschluss nochmal eine Abbildung der transformierten `x` Werte. In der @fig-trans-bestNorm-2 siehst du dann einmal das entsprechende Histogramm. Das sieht doch sehr gut aus und wir mussten nicht zig verschiedene Algorithmen selber testen.

```{r}
#| message: false
#| echo: false
#| label: fig-trans-bestNorm-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 7
#| fig-cap: "Histogramm der transformierten `x` Werte mit der Box-Cos Transformation aus dem R Paket `{bestNormalize}`."
ggplot(as_tibble(x_trans), aes(x_trans)) +
   geom_histogram(fill = cbbPalette[4], color = "black") +
  theme_minimal() 
```

### ... mit `{rcompanion}`

`transformTukey()` aus dem R Paket `{rcompanion}`

### ... mit `{LambertW}`

`Gaussianize()` aus dem [R Paket `{LambertW}`](https://cran.r-project.org/web/packages/LambertW/index.html)

### ... mit `{MASS}`

Die Box-Cox-Transformation ist ein statistisches Verfahren zur Umwandlung von nicht normalverteilten Daten in eine Normalverteilung. Die Transformation ist nicht so einfach wie die logarithmische Transformation oder die Quadratwurzeltransformation und erfordert etwas mehr Erklärung. Beginnen wir zunächst die Gleichung zu verstehen, die die Transformation beschreibt. Die grundsätzliche Idee ist, dass wir unser $y$ als Outcome mit einem $\lambda$-Exponenten transformieren. Die Frage ist jetzt, welches $\lambda$ produziert die besten normalverteilten Daten? Wenn wir ein $\lambda$ von Null finden sollten, dann rechnen wir einfach eine $\log$-Transformation. Die Idee ist also recht simpel.

$$
y(\lambda)=\left\{\begin{matrix} \dfrac{y^{\lambda}-1} {\lambda} & \quad  \mathrm{f\ddot ur\;\;}\lambda \ne 0 \\[10pt] \log(y) &\quad  \mathrm{f\ddot ur\;\;}\lambda = 0\end{matrix}\right.
$$

Wir werden natürlich jetzt nicht händisch alle möglichen $\lambda$ durchprobieren bis wir das beste $\lambda$ gefunden haben. Dafür gibt es die Funktion `boxcox` aus dem R Paket `{MASS}`, die ein lineares Modell benötigt. Daher bauen wir usn erst unser lineares Modell und dann stecken wir das Modell in die Funktion `boxcox()`.

```{r}
jump_mod <- lm(jump_length ~ 1, data = fac2_tbl)
```

Jetzt einmal die Funktion `boxcox()` ausführen und danach das $\lambda$ extrahieren. Hier ist es etwas umständlicher, da das $\lambda$ in der Ausgabe der Funktion etwas vergraben ist. Dafür ist das R Paket `{MASS}` einfach nicht mehr das jüngste Paket und hat keine so guten Funktionen zum Erhalten von wichtigen Parametern. Ich möchte die Abbildung nicht haben, daher die Option `plotit = FALSE`.

```{r}
bc_obj <- boxcox(jump_mod, plotit = FALSE)
lambda <- bc_obj$x[which.max(bc_obj$y)]
lambda
```

Wir erhalten also ein $\lambda$ von `r lambda` wieder. Dieses $\lambda$ können wir dann nutzen um unsere Daten nach Box-Cox zu transformieren. Dafür übersetzen wir dann die obige Matheformel einmal in R Code.

```{r}
fac2_tbl <- fac2_tbl |>
  mutate(jump_boxcox = ((jump_length ^ lambda - 1)/lambda))
```

In der @fig-boxcox-ggplot sehen wir dann einmal das Ergebnis der Transformation. Sieht gar nicht mal so schlecht aus und noch besser als die reine $\log$-Transformation. Wie immer musst du aber auch hier rumprobieren, was dann am besten einer Normalverteilung folgt.

```{r}
#| message: false
#| echo: true
#| label: fig-boxcox-ggplot
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Histogramm der Box-Cox transfomierten Daten in `ggplot` dargestellt."
ggplot(data = fac2_tbl) +
  theme_minimal() +
  geom_histogram(aes(x = jump_boxcox), alpha = 0.9,
                 fill = cbbPalette[3], color = "black") +
  labs(x = 'Box-Cox transformierte Sprungweiten', y = 'Anzahl')

```

### ... mit `{dlookr}`

::: callout-warning
## Achtung, bitte beachten!

Das R Paket `{dlookr}` lies sich eine Zeit nicht über CRAN einfach installieren.
:::

::: {layout="[15,85]" layout-valign="top"}
![](images/personal_opinion.png){fig-align="center" width="100%"}

> *"foo." --- Jochen Kruppa-Scheetz, meiner bescheidener Meinung nach.*
:::

### ... mit `{trafo}`

[The R Package trafo for Transforming Linear Regression Models](https://cran.r-project.org/web/packages/trafo/vignettes/vignette_trafo.pdf)

## Referenzen {.unnumbered}
