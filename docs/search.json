[
  {
    "objectID": "stat-modeling-poisson.html",
    "href": "stat-modeling-poisson.html",
    "title": "41  Poisson Regression",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:48:48\nIn diesem Kapitel wollen wir eine Poisson Regression rechnen. Wir müssen uns hier wieder überlegen, was ist eigentlich unser Outcome \\(y\\) und was sind unsere Einflussvariablen \\(x\\). Die Poisson Regression ist je nach Hintergurnd des Anwenders eher selten. In der Ökologie, wo gerne mal gezaählt wird, wie oft etwas vorkommt, ist die Poisson Regression häufig vertreten. Sonst fristet die Poisson Regresson eher ein unbekanntes Dasein.\nEin häufig unterschätzter Vorteil der Poisson Regression ist, dass wir auch auch \\(0/1\\) Daten eine Poisson Regression rechnen können. Moment, wirst du jetzt vielleicht denken, das machen wir doch mit der logistsichen Regression. Ja, das stimmt, aber wir können auf Zahlen viel rechnen. Wenn wir auf ein \\(0/1\\) Outcome eine Poisson Regression rechnen, dann kriegen wir nicht Odds Ratios \\(OR\\) als Effektschätzer sondern Risk Ratios \\(RR\\). Wir erhalten also keine Chancen sondern Wahrscheinlichkeiten. Unter der Annahme, dass das Modell auch konvergiert und wir sinnvolle Zahlen erhalten.\nEin weiteres Problem sind die zu vielen Nullen in dem Outcome \\(y\\). Daherher wir zählen über die Maßen viel Nichts. Wir nennen diesen Fall zero inflation und beschreiben damit die zu vielen Nullen in den Daten. Hier muss dann noch speziell modelliert werden. Eine Poisson Regression hat schon so seine speziellen Tücken."
  },
  {
    "objectID": "stat-modeling-poisson.html#annahmen-an-die-daten",
    "href": "stat-modeling-poisson.html#annahmen-an-die-daten",
    "title": "41  Poisson Regression",
    "section": "\n41.1 Annahmen an die Daten",
    "text": "41.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Poissonverteilung.\nIm folgenden Kapitel zu der multiplen Poisson linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 36 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Poissonverteilung. Das ist hier sehr wichtig, denn wir wollen ja eine multiple Poisson lineare Regression rechnen.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen."
  },
  {
    "objectID": "stat-modeling-poisson.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-poisson.html#genutzte-r-pakete-für-das-kapitel",
    "title": "41  Poisson Regression",
    "section": "\n41.2 Genutzte R Pakete für das Kapitel",
    "text": "41.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, MASS, pscl, see,\n               modelsummary, scales)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-poisson.html#daten",
    "href": "stat-modeling-poisson.html#daten",
    "title": "41  Poisson Regression",
    "section": "\n41.3 Daten",
    "text": "41.3 Daten\nIm folgenden schauen wir uns ein Datenbeispiel mit Hechten an. Es handelt sich um langnasige Hechte in nordamerikanischen Flüssen. Wir haben uns insgesamt \\(n = 68\\) Flüsse einmal angesehen und dort die Anzahl an Hechten gezählt. Im Weiteren haben wir dann noch andere Flussparameter erhoben und fragen uns nun, welche dieser Parameter einen Einfluss auf die Anzahl an Hechten in den Flussarmen haben. In Kapitel 10.2 findest du nochmal mehr Informationen zu den Daten. Wir entfernen hier die Informationen zu den Flüssen, die brauchen wir in dieser Analyse nicht.\n\n\nDie Daten zu den langnasigen Hechten stammt von Salvatore S. Mangiafico - An R Companion for the Handbook of Biological Statistics.\n\nlongnose_tbl <- read_csv2(\"data/longnose.csv\") %>% \n  select(-stream)\n\n\n\n\n\nTabelle 41.1— Auszug aus dem Daten zu den langnasigen Hechten.\n\nlongnose\narea\ndo2\nmaxdepth\nno3\nso4\ntemp\n\n\n\n13\n2528\n9.6\n80\n2.28\n16.75\n15.3\n\n\n12\n3333\n8.5\n83\n5.34\n7.74\n19.4\n\n\n54\n19611\n8.3\n96\n0.99\n10.92\n19.5\n\n\n19\n3570\n9.2\n56\n5.44\n16.53\n17\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n26\n1450\n7.9\n60\n2.96\n8.84\n18.6\n\n\n20\n4106\n10\n96\n2.62\n5.45\n15.4\n\n\n38\n10274\n9.3\n90\n5.45\n24.76\n15\n\n\n19\n510\n6.7\n82\n5.25\n14.19\n26.5\n\n\n\n\n\n\nIm Folgenden werden wir die Daten nur für das Fitten eines Modells verwenden. In den anderen oben genannten Kapiteln nutzen wir die Daten dann anders. In Abbildung 41.1 sehen wir nochmal die Verteilung der Anzahl der Hechte in den Flüssen.\n\nggplot(longnose_tbl, aes(longnose)) +\n  theme_bw() +\n  geom_histogram()\n\n\n\nAbbildung 41.1— Histogramm der Verteilung der Hechte in den beobachteten Flüssen."
  },
  {
    "objectID": "stat-modeling-poisson.html#fit-des-modells",
    "href": "stat-modeling-poisson.html#fit-des-modells",
    "title": "41  Poisson Regression",
    "section": "\n41.4 Fit des Modells",
    "text": "41.4 Fit des Modells\nIn diesem Abschnitt wollen wir verschiedene Modelle für Zähldaten schätzen. Die Poissonverteilung hat keinen eignen Parameter für die Streung wie die Normalverteilung. Die Poissonverteilung ist mit \\(\\mathcal{Pois}(\\lambda)\\) definiert und hat somit die Eigenschaft das die Varianz eins zu eins mit dem Mittelwert \\(\\lambda\\) der Poissonverteilung ansteigt. Es kann aber sein, dass wir in den Daten nicht diesen ein zu eins Zusammenhang von Mittelwert und Varianz vrliegen haben. Häufig ist die Varianz viel größer und steigt schneller an. Wenn die Varianz in Wirklichkeit sehr viel größer ist, dann würden wir die Varianz in unseren Modell unterschätzen.\n\nEin klassisches Poissonmodell glm(..., familiy = poisson) mit der Annahme keiner Overdisperison.\nEin Quasi-Poissonmodell glm(..., family = quasipoisson) mit der Möglichkeit der Berücksichtigung einer Overdispersion.\nEin negative Binomialmodell glm.nb(...) ebenfalls mit der Berücksichtigung einer Overdispersion.\n\nBeginnen wollen wir aber mit einer klassischen Poissonregression ohne die Annahme von einer Overdispersion in den Daten. Wir nutzen dafür die Funktion glm() und spezifizieren die Verteilungsfamilie als poisson. Wir nehmen wieder alle Variablen in das Modell auf der rechten Seite des ~. Auf der linken Seite des ~ kommt dann unser Outcome longnose was die Anzahl an Hechten erhält.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\nWir müssen für die Possionregression noch beachten, dass die Zähldaten von \\(0\\) bis \\(+\\infty\\) laufen. Damit wir normalverteilte Residuen erhalten und einen lineren Zusammenhang, werden wir das Modell auf dem \\(\\log\\)-scale fitten. Das heißt, wir werden den Zusammenhang von \\(y\\) und \\(x\\) logarithmieren. Wichtig ist hierbei der Zusammenhang. Wir transformieren nicht einfach \\(y\\) und lassen den Rest unberührt. Das führt dazu, dass wir am Ende die Koeffizienten der Poissonregression exponieren müssen. Das können die gängigen Funktionen, wir müssen das Exponieren aber aktiv durchführen. Deshalb hier schon mal erwähnt.\n\npoisson_fit <- glm(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                    longnose_tbl, family = poisson)\n\nWir schauen uns die Ausgabe des Modells einmal mit der summary() Funktion an, da wir hier einmal händisch schauen wollen, ob eine Overdispersion vorliegt. Sonst könnten wir auch die Funktion model_parameters() nehmen. Die nutzen wir später für die Interpretation des Modells, hier wollen wir erstmal sehen, ob alles geklappt hat.\n\npoisson_fit %>% summary\n\n\nCall:\nglm(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, family = poisson, data = longnose_tbl)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.234  -4.086  -1.662   1.771  14.362  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.564e+00  2.818e-01  -5.551 2.83e-08 ***\narea         3.843e-05  2.079e-06  18.480  < 2e-16 ***\ndo2          2.259e-01  2.126e-02  10.626  < 2e-16 ***\nmaxdepth     1.155e-02  6.688e-04  17.270  < 2e-16 ***\nno3          1.813e-01  1.068e-02  16.974  < 2e-16 ***\nso4         -6.810e-03  3.622e-03  -1.880   0.0601 .  \ntemp         7.854e-02  6.530e-03  12.028  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2766.9  on 67  degrees of freedom\nResidual deviance: 1590.0  on 61  degrees of freedom\nAIC: 1936.9\n\nNumber of Fisher Scoring iterations: 5\n\n\nWir schauen in die Summary-Ausgabe des Poissonmodells und sehen, dass dort steht, dass Dispersion parameter for poisson family taken to be 1. Wir modellieren also einen eins zu eins Zusammenhang von Mittelwert und Varianz. Wenn dieser Zusammenhang nicht in unseren Daten existiert, dann haben wir eine Overdispersion vorliegen.\nWir können die Overdispersion mit abschätzen indem wir die Residual deviance durch die Freiheitsgrade der Residual deviance teilen. Daher erhalten wir eine Overdispersion von \\(\\cfrac{1590.04}{61} \\approx 26.1\\). Damit haben wir eine eindeutige Overdispersion vorliegen. Damit steigt die Varianz in einem Verhältnis von ca. 1 zu 26. Wir können auch die Funktion check_overdispersion() aus dem R Paket performance nutzen um die Overdispersion zu berechnen. Die Funktion kann das schneller und ist auch in der Abfolge einer Analyse besser geeignet.\n\npoisson_fit %>% check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =   29.403\n  Pearson's Chi-Squared = 1793.599\n                p-value =  < 0.001\n\n\nOverdispersion detected.\n\n\nWenn wir Overdispersion vorliegen haben und damit die Varianz zu niedrig schätzen, dann erhalten wir viel mehr signifikante Ergebnisse als es in den Daten zu erwarten wäre. Schauen wir uns nochmal die Parameter der Poissonverteilung und die \\(p\\)-Werte einmal an.\n\npoisson_fit %>% model_parameters()\n\nParameter   |  Log-Mean |       SE |         95% CI |     z |      p\n--------------------------------------------------------------------\n(Intercept) |     -1.56 |     0.28 | [-2.12, -1.01] | -5.55 | < .001\narea        |  3.84e-05 | 2.08e-06 | [ 0.00,  0.00] | 18.48 | < .001\ndo2         |      0.23 |     0.02 | [ 0.18,  0.27] | 10.63 | < .001\nmaxdepth    |      0.01 | 6.69e-04 | [ 0.01,  0.01] | 17.27 | < .001\nno3         |      0.18 |     0.01 | [ 0.16,  0.20] | 16.97 | < .001\nso4         | -6.81e-03 | 3.62e-03 | [-0.01,  0.00] | -1.88 | 0.060 \ntemp        |      0.08 | 6.53e-03 | [ 0.07,  0.09] | 12.03 | < .001\n\n\nIn der Spalte p finden wir die \\(p\\)-Werte für alle Variablen. Wir sehen, dass fast alle Variablen signifikant sind und das wir eine sehr niedrige Varianz in der Spalte SE sehen. Das heißt unser geschätzer Fehler ist sehr gering. Das ahnten wir ja schon, immerhin haben wir eine Overdisperson vorliegen. Das Modell ist somit falsch. Wir müssen uns ein neues Modell suchen, was Overdispersion berückscihtigen und modellieren kann.\nDie Quasi-Poisson Verteilung hat einen zusätzlichen, unabhänigen Parameter um die Varianz der Verteilung zu schätzen. Daher können wir die Overdispersion mit einer Quasi-Poisson Verteilung berückscihtigen. Wir können eine Quasi-Poisson Verteilung auch mit der Funktion glm() schätzen nur müssen wir als Verteilungsfamilie quasipoisson angeben.\n\nquasipoisson_fit <- glm(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                        data = longnose_tbl, family = quasipoisson)\n\nNach dem Modellti können wir nochmal in der summary() Funktion schauen, ob wir die Overdispersion richtig berücksichtigt haben.\n\nquasipoisson_fit %>% summary\n\n\nCall:\nglm(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, family = quasipoisson, data = longnose_tbl)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.234  -4.086  -1.662   1.771  14.362  \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.564e+00  1.528e+00  -1.024  0.30999   \narea         3.843e-05  1.128e-05   3.408  0.00116 **\ndo2          2.259e-01  1.153e-01   1.960  0.05460 . \nmaxdepth     1.155e-02  3.626e-03   3.185  0.00228 **\nno3          1.813e-01  5.792e-02   3.130  0.00268 **\nso4         -6.810e-03  1.964e-02  -0.347  0.73001   \ntemp         7.854e-02  3.541e-02   2.218  0.03027 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 29.40332)\n\n    Null deviance: 2766.9  on 67  degrees of freedom\nResidual deviance: 1590.0  on 61  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nAn der Zeile Dispersion parameter for quasipoisson family taken to be 29.403319 in der Summary-Ausgabe sehen wir, dass das Modell der Quasi-Possion Verteilung die Overdispersion korrekt berücksichtigt hat. Wir können uns nun einmal die Modellparameter anschauen. Die Interpretation machen wir am Ende des Kapitels.\n\nquasipoisson_fit %>% model_parameters()\n\nParameter   |  Log-Mean |       SE |        95% CI | t(61) |      p\n-------------------------------------------------------------------\n(Intercept) |     -1.56 |     1.53 | [-4.57, 1.41] | -1.02 | 0.306 \narea        |  3.84e-05 | 1.13e-05 | [ 0.00, 0.00] |  3.41 | < .001\ndo2         |      0.23 |     0.12 | [ 0.00, 0.45] |  1.96 | 0.050 \nmaxdepth    |      0.01 | 3.63e-03 | [ 0.00, 0.02] |  3.18 | 0.001 \nno3         |      0.18 |     0.06 | [ 0.07, 0.29] |  3.13 | 0.002 \nso4         | -6.81e-03 |     0.02 | [-0.05, 0.03] | -0.35 | 0.729 \ntemp        |      0.08 |     0.04 | [ 0.01, 0.15] |  2.22 | 0.027 \n\n\nJetzt sieht unser Modell und die \\(p\\)-Werte zusammen mit dem Standardfehler SE schon sehr viel besser aus. Wir können also diesem Modell erstmal von der Seite der Overdispersion vertrauen.\nAm Ende wollen wir nochmal das Modell mit der negativen Binomialverteilung rechnen. Die negativen Binomialverteilung erlaubt auch eine Unabhängigkeit von dem Mittelwert zu der Varianz. Wir können hier auch für die Overdispersion adjustieren. Wir rechnen die negativen Binomialregression mit der Funktion glm.nb() aus dem R Paket MASS. Wir müssen keine Verteilungsfamilie angeben, die Funktion glm.nb() kann nur die negative Binomialverteilung modellieren.\n\nnegativebinomial_fit <- glm.nb(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                               data = longnose_tbl)\n\nAuch hier schauen wir mit der Funktion summary() einmal, ob die Overdisprsion richtig geschätzt wurde oder ob hier auch eine Unterschätzung des Zusammenhangs des Mittelwerts und der Varianz vorliegt.\n\nnegativebinomial_fit %>% summary()\n\n\nCall:\nglm.nb(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, data = longnose_tbl, init.theta = 1.666933879, link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4601  -0.9876  -0.4426   0.4825   2.2776  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -2.946e+00  1.305e+00  -2.256 0.024041 *  \narea         4.651e-05  1.300e-05   3.577 0.000347 ***\ndo2          3.419e-01  1.050e-01   3.256 0.001130 ** \nmaxdepth     9.538e-03  3.465e-03   2.752 0.005919 ** \nno3          2.072e-01  5.627e-02   3.683 0.000230 ***\nso4         -2.157e-03  1.517e-02  -0.142 0.886875    \ntemp         9.460e-02  3.315e-02   2.854 0.004323 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.6669) family taken to be 1)\n\n    Null deviance: 127.670  on 67  degrees of freedom\nResidual deviance:  73.648  on 61  degrees of freedom\nAIC: 610.18\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.667 \n          Std. Err.:  0.289 \n\n 2 x log-likelihood:  -594.175 \n\n\nAuch hier sehen wir, dass die Overdispersion mit dem Parameter \\(\\theta\\) berücksichtigt wird. Wir können die Zahl \\(1.67\\) nicht direkt mit der Overdispersion aus einer Poissonregression verglechen, aber wir sehen dass das Verhältnis von Residual deviance zu den Freiheitsgraden mit \\(\\cfrac{73.65}{61} \\approx 1.20\\) fast bei 1:1 liegt. Wir könnten also auch eine negative Binomialverteilung für das Modellieren nutzen.\n\nnegativebinomial_fit %>% model_parameters()\n\nParameter   |  Log-Mean |       SE |         95% CI |     z |      p\n--------------------------------------------------------------------\n(Intercept) |     -2.95 |     1.31 | [-5.85, -0.10] | -2.26 | 0.024 \narea        |  4.65e-05 | 1.30e-05 | [ 0.00,  0.00] |  3.58 | < .001\ndo2         |      0.34 |     0.11 | [ 0.11,  0.58] |  3.26 | 0.001 \nmaxdepth    |  9.54e-03 | 3.47e-03 | [ 0.00,  0.02] |  2.75 | 0.006 \nno3         |      0.21 |     0.06 | [ 0.10,  0.32] |  3.68 | < .001\nso4         | -2.16e-03 |     0.02 | [-0.03,  0.03] | -0.14 | 0.887 \ntemp        |      0.09 |     0.03 | [ 0.03,  0.16] |  2.85 | 0.004 \n\n\n\n\nWie immer gibt es reichtlich Tipps & Tricks welches Modell du nun nehmen solltest. How to deal with overdispersion in Poisson regression: quasi-likelihood, negative binomial GLM, or subject-level random effect? und das Tutorial Modeling Count Data. Auch ich mus immer wieder schauen, was am besten konkret in der Anwendung passen könnte und würde.\nWelches Modell nun das beste Modell ist, ist schwer zu sagen. Wenn du Overdisperion vorliegen hast, dann ist natürlich nur das Quasi-Poissonmodell oder das negative Binomialmodell möglich. Welche der beiden dann das bessere ist, hängt wieder von der Fragestellung ab. Allgemein gesprochen ist das Quasi-Poissonmodell besser wenn dich die Zusammenhänge von \\(y\\) zu \\(x\\) am meisten interessieren. Und das ist in unserem Fall hier die Sachlage. Daher gehen wir mit den Quasi-Poissonmdell dann weiter."
  },
  {
    "objectID": "stat-modeling-poisson.html#performance-des-modells",
    "href": "stat-modeling-poisson.html#performance-des-modells",
    "title": "41  Poisson Regression",
    "section": "\n41.5 Performance des Modells",
    "text": "41.5 Performance des Modells\nIn diesem kurzen Abschnitt wollen wir uns einmal anschauen, ob das Modell neben der Overdispersion auch sonst aus statistischer Sicht in Ordnung ist. Wir wollen ja mit dem Modell aus dem Fit quasipoisson_fit weitermachen. Also schauen wir uns einmal das pseudo-\\(R^2\\) für die Poissonregression an. Da wir es mit einem GLM zu tun haben, ist das \\(R^2\\) mit vorsicht zu genießen. In einer Gaussianregression können wir das \\(R^2\\) als Anteil der erklärten Varianz durch das Modell interpretieren. Im Falle von GLM’s müssen wir hier vorsichtiger sein. In GLM’s gibt es ja keine Varianz sondern eine Deviance.\n\nr2_efron(quasipoisson_fit)\n\n[1] 0.3257711\n\n\nMit einem pseudo-\\(R^2\\) von \\(0.33\\) erklären wir ca. 33% der Varianz in der Anzahl der Hechte. Das ist zwar keine super gute Zahl, aber dafür, dass wir nur eine handvoll von Parametern erfasst haben, ist es dann auch wieder nicht so schlecht. Die Anzahl an Hechten wird sicherlich an ganz vielen Parametern hängen, wir konnten immerhin einige wichtige Stellschrauben vermutlich finden.\nIn Abbildung 41.2 schauen wir uns nochmal die Daten in den Modelgüteplots an. Wir sehen vorallem, dass wir vielelicht doch einen Ausreißer mit der Beobachtung 17 vorliegen haben. Auch ist der Fit nicht so super, wie wir an dem QQ-Plot sehen. Die Beobachtungen fallen in dem QQ-Plot nicht alle auf eine Linie. Auch sehen wir dieses Muster in dem Residualplot. Hiererwarten wir eine gerade blaue Linie und auch hier haben wir eventuell Ausreißer mit in den Daten.\n\ncheck_model(quasipoisson_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\", \"pp_check\", \"homogeneity\")) \n\n\n\nAbbildung 41.2— Ausgabe ausgewählter Modelgüteplots der Funktion check_model()."
  },
  {
    "objectID": "stat-modeling-poisson.html#interpretation-des-modells",
    "href": "stat-modeling-poisson.html#interpretation-des-modells",
    "title": "41  Poisson Regression",
    "section": "\n41.6 Interpretation des Modells",
    "text": "41.6 Interpretation des Modells\nUm die Effektschätzer einer Poissonregression oder aber einer Quasipoisson-Regression interpretieren zu können müssen wir uns einmal einen Beispieldatensatz mit bekannten Effekten zwischen den Gruppen bauen. Im Folgenden bauen wir uns einen Datensatz mit zwei Gruppen. Einmal einer Kontrollgruppe mit einer mittleren Anzahl an \\(15\\) und einer Behandlungsgruppe mit einer um \\(\\beta_1 = 10\\) höheren Anzahl. Wir haben also in der Kontrolle im Mittel eine Anzahl von \\(15\\) und in der Behandlungsgruppe eine mittlere Anzahl von \\(25\\).\n\nsample_size <- 100\nlongnose_small_tbl <- tibble(grp = rep(c(0, 1), each = sample_size),\n                             count = 15 + 10 * grp + rnorm(2 * sample_size, 0, 1)) %>%\n  mutate(count = round(count),\n         grp = factor(grp, labels = c(\"ctrl\", \"trt\")))\n\nIn Tabelle 41.2 sehen wir nochmal die Daten als Ausschnitt dargestellt.\n\n\n\n\nTabelle 41.2— How much is the fish? Der Datensatz über \\(n = 1000\\) Beobachtungen an dem wir überlegen wollen wie wir die Effektschätzer einer Poissonregression zu interpretieren haben.\n\ngrp\ncount\n\n\n\nctrl\n15\n\n\nctrl\n14\n\n\nctrl\n15\n\n\nctrl\n13\n\n\n…\n…\n\n\ntrt\n25\n\n\ntrt\n26\n\n\ntrt\n26\n\n\ntrt\n25\n\n\n\n\n\n\nDa sich die Tabelle schlecht liest hier nochmal der Boxplot in Abbildung 41.3. Wir sehen den Grupenunterschied von \\(10\\) sowie die unterschiedlichen mittleren Anzahlen für die Kontrolle und die Behandlung.\n\nggplot(longnose_small_tbl, aes(x = grp, y = count, fill = grp)) +\n  theme_bw() +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\nggplot(data = longnose_small_tbl, aes(x = count, fill = grp)) +\n  theme_bw() +\n  geom_density(alpha = 0.75) +\n  labs(x = \"\", y = \"\", fill = \"Gruppe\") +\n  scale_fill_okabeito() +\n  scale_x_continuous(breaks = seq(10, 30, by = 5), limits = c(10, 30)) \n\n\n\n\n\n(a) Verteilung der Werte als Boxplot.\n\n\n\n\n\n\n\n\n(b) Verteilung der Werte als Densityplot.\n\n\n\n\nAbbildung 41.3— How much is the fish? Der Boxplot über \\(n = 1000\\) Beobachtungen an dem wir überlegen wollen wie wir die Effektschätzer einer Poissonregression zu interpretieren haben.\n\n\n\nJetzt fitten wir einmal das simple Poissonmodell mit der Anzahl als Outcome und der Gruppe mit den zwei Leveln als \\(x\\). Wir pipen dann das Ergebnis des Fittes gleich in die Funktion model_parameters() weiter um die Ergebnisse des Modellierens zu erhalten.\n\nglm(count ~ grp, data = longnose_small_tbl, family = poisson) %>%\n  model_parameters(exponentiate = TRUE)\n\nParameter   |   IRR |   SE |         95% CI |      z |      p\n-------------------------------------------------------------\n(Intercept) | 14.95 | 0.39 | [14.20, 15.72] | 104.58 | < .001\ngrp [trt]   |  1.69 | 0.06 | [ 1.58,  1.80] |  16.06 | < .001\n\n\nAls erstes fällt auf, dass wir die Ausgabe des Modells exponieren müssen. Um einen linearen Zusamenhang hinzukriegen bedient sich die Poissonregression den Trick, das der Zusammenhang zwischen dem \\(y\\) und dem \\(x\\) transformiert wird. Wir rechnen unsere Regression nicht auf den echten Daten sondern auf dem \\(\\log\\)-scale. Daher müssen wir die Koeffizienten der Poissonregression wieder zurücktransfomieren, wenn wir die Koeffizienten interpretieren wollen. Das können wir mit der Option exponentiate = TRUE durchführen.\nGut soweit, aber was heißen den jetzt die Zahlen? Wir haben einen Intercept von \\(14.99\\) das entspricht der mittleren Anzahl in der Kontrollgruppe. Und was sagt jetzt die \\(1.67\\) vom Level trt des Faktors grp? Wenn wir \\(14.99 \\cdot 1.67\\) rechnen, dann erhalten wir als Ergebnis \\(25.03\\), also die mittlere Anzahl in der Behandlungsgruppe. Was sagt uns das jetzt aus? Wir erhalten aus der Poissonregression eine Wahrscheinlichkeit oder aber ein Risk Ratio. Wir können sagen, dass die Anzahl in der Behandlungsgruppe \\(1.67\\)-mal so groß ist wie in der Kontrollgruppe.\nSchauen wir uns nochmal das volle Modell an und interpretieren die Effekte der einzelnen Variablen.\n\nquasipoisson_fit %>% \n  model_parameters(exponentiate = TRUE) \n\nParameter   |  IRR |       SE |       95% CI | t(61) |      p\n-------------------------------------------------------------\n(Intercept) | 0.21 |     0.32 | [0.01, 4.11] | -1.02 | 0.306 \narea        | 1.00 | 1.13e-05 | [1.00, 1.00] |  3.41 | < .001\ndo2         | 1.25 |     0.14 | [1.00, 1.57] |  1.96 | 0.050 \nmaxdepth    | 1.01 | 3.67e-03 | [1.00, 1.02] |  3.18 | 0.001 \nno3         | 1.20 |     0.07 | [1.07, 1.34] |  3.13 | 0.002 \nso4         | 0.99 |     0.02 | [0.95, 1.03] | -0.35 | 0.729 \ntemp        | 1.08 |     0.04 | [1.01, 1.16] |  2.22 | 0.027 \n\n\nSo schön auch die Funktion model_parameters() ist, so haben wir aber hier das Problem, dass wir den Effekt von area nicht mehr richtig sehen. Wir kriegen hier eine zu starke Rundung auf zwei Nachkommastellen. Wir nutzen jetzt mal die Funktion tidy() um hier Abhilfe zu leisten. Ich muss hier noch die Spalte estimate mit num(..., digits = 5) anpassen, damit du in der Ausgabe auf der Webseite auch die Nachkommastellen siehst.\n\nquasipoisson_fit %>% \n  tidy(exponentiate = TRUE, digits = 5) %>% \n  select(term, estimate, p.value) %>% \n  mutate(p.value = pvalue(p.value),\n         estimate = num(estimate, digits = 5))\n\n# A tibble: 7 x 3\n  term         estimate p.value\n  <chr>       <num:.5!> <chr>  \n1 (Intercept)   0.20922 0.310  \n2 area          1.00004 0.001  \n3 do2           1.25342 0.055  \n4 maxdepth      1.01162 0.002  \n5 no3           1.19879 0.003  \n6 so4           0.99321 0.730  \n7 temp          1.08171 0.030  \n\n\nSchauen wir uns die Effekte der Poissonregression einmal an und versuchen die Ergebnisse zu interpretieren. Dabei ist wichtig sich zu erinnern, dass kein Effekt eine 1 bedeutet. Wir schauen hier auf einen Faktor. Wenn wir eine Anzahl mal Faktor 1 nehmen, dann ändert sich nichts an der Anzahl.\n\n\n(Intercept) beschreibt den Intercept der Poissonregression. Wenn wir mehr als eine simple Regression vorliegen haben, wie in diesem Fall, dann ist der Intercept schwer zu interpretieren. Wir konzentrieren uns auf die Effekte der anderen Variablen.\n\narea, beschreibt den Effekt der Fläche. Steigt die Fläche um ein Quadratmeter an, so erhöht sich die Anzahl an Fischen um den \\(1.00001\\). Daher würde man hier eher sagen, erhöht sich die Fläche um jeweils 1000qm so erhöht sich die Anzahl an Fischen um den Faktor \\(1.1\\). Dann haben wir auch einen besser zu interpretierenden Effektschätzer. Die Signifikanz bleibt hier davon unbetroffen.\n\ndo2, beschreibt den Partzialdruck des Sauerstoffs. Steigt dieser um eine Einheit an, so sehen wie eine Erhöhung der Anzahl an Fischen um den Faktor \\(1.25\\). Der Effekt ist gerade nicht signifikant.\n\nmaxdepth, beschreibt die maximale Tiefe. Je tiefer ein Fluß, desto mehr Hechte werden wir beobachten. Der Effekt von \\(1.01\\) pro Meter Tiefe ist signifikant.\n\nno3, beschreibt den Anteil an Nitrat in den Flüssen. Je mehr Nitrat desto signifiant mehr Hechte werden wir beobachten. Hier steigt der Faktor auch um \\(1.20\\).\n\nso4, beschreibt den Schwefelgehalt und mit steigenden Schwefelgehalt nimmt die Anzahl an Fischen leicht ab. Der Effekt ist aber überhauot nicht signifikant.\n\ntemp, beschreibt die Temperatur der Flüsse. Mit steigender Tempertaur erwarten wir mehr Hechte zu beobachten. Der Effekt von \\(1.08\\) Fischen pro Grad Erhöhung ist signifikant.\n\nWas nehmen wir aus der Poissonregression zu den langnasigen Hechten mit? Zum einen haben die Fläche, die Tiefe und der Nitratgehalt einen signifikanten Einfluss auf die Anzahl an Hechten. Auch führt eine höhere Temperatur zu mehr gefundenen Hechten. Die erhöhte Temperatur steht etwas im Widerspuch zu dem Sauerstoffpartizaldruck. Denn je höher die Temperatur desto weniger Sauerstoff wird in dem Wasser gelöst sein. Auch scheint die Oberfläche mit der Tiefe korreliert. Allgemein scheinen Hechte große Flüße zu mögen. Hier bietet sich also noch eine Variablenselektion oder eine Untersuchung auf Ausreißer an um solche Effekte nochmal gesondert zu betrachten."
  },
  {
    "objectID": "stat-modeling-poisson.html#zeroinflation",
    "href": "stat-modeling-poisson.html#zeroinflation",
    "title": "41  Poisson Regression",
    "section": "\n41.7 Zeroinflation",
    "text": "41.7 Zeroinflation\nSo eine Poissonregression hat schon einiges an Eigenheiten. Neben dem Problem der Overdispersion gibt es aber noch eine weitere Sache, die wir beachten müssen. Wir können bei einer Poissonregression auch eine Zeroinflation vorliegen haben. Das heißt, wir beobachten viel mehr Nullen in den Daen, als wir aus der Poissonverteilung erwarten würden. Es gibt also einen biologischen oder künstlichn Prozess, der uns Nullen produziert. Häufig wissen wir nicht, ob wir den Prozess, der uns die Nullen in den Daten produziert, auch abbilden. Das heißt, es kann sein, dass wir einfach nichts Zählen, weil dort nichts ist oder aber es gibt dafür einen Grund. Diesen Grund müssten wir dann irgendwie in unseren Daten erfasst haben, aber meistens haben wir das nicht.\nSchauen wir usn dafür einmal ein Datenbeispiel von Eidechsen in der Lüneburgerheide an. Wir haben Eidechsen lizard in zwei verschiedenen Habitaten grp gezählt. Einmal, ob die Eidechsen eher im offenen Gelände oder eher im bedeckten Gelände zu finden waren. Im Weiteren haben wir geschaut, ob der Boden keinen Regen erhalten hatte, trocken war oder gar feucht. Mit trocken ist hier eine gewisse Restfeuchte gemeint. Am Ende haben wir noch bestimmt, ob wir eher nah an einer Siedlung waren oder eher weiter entfernt. Du kannst dir den Daten satz in der Datei lizards.csv nochmal anschauen. In Tabelle 41.3 sind die Daten nochmal dargestellt.\n\n\n\n\nTabelle 41.3— Ausschnitt aus den Eidechsendaten für die zwei Habitate unter verschiedenen Feuchtigkeitsbedingungen und Nähe zur nächsten Siedlung.\n\ngrp\nrain\npop\nlizard\n\n\n\nopen\nno\nnear\n0\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n0\n\n\nopen\nno\nfar\n2\n\n\nopen\nno\nfar\n4\n\n\n\n\n\n\nIn Abbildung 41.4 sehen wir die Zähldaten der Eidechsen nochmal als Histogramm dargestellt. Wenn wir an einem Punkt keine Eidechsen gefunden haben, dann haben wir keine fehlenden Werte eingetragen, sondern eben, dass wir keine Eidechsen gezählt haben. Wir sehen das wir sehr viele Nullen in unseren Daten haben. Ein Indiz für eine Inflation an Nullen oder eben einer Zeroinflation.\n\nggplot(lizard_zero_tbl, aes(lizard)) +\n  theme_bw() +\n  geom_histogram() +\n  labs(x = \"Anzahl der gefundenen Eidechsen\", y = \"Anzahl\") +\n  scale_x_continuous(breaks = 0:7)\n\n\n\nAbbildung 41.4— Histogramm der Verteilung der Hechte in den beobachteten Flüssen.\n\n\n\n\nUm zu überprüfen, ob wir eine Zeroinflation in den Daten vorliegen haben, werden wir erstmal eine ganz normale Poissonregression auf den Daten rechnen. Wir ignorieren auch eine potenzielle Overdispersion. Das schauen wir uns dann in den Daten später nochmal an.\n\nlizard_fit <- glm(lizard ~ grp + rain + pop, data = lizard_zero_tbl,\n                  family = poisson)\n\nWie immer nutzen wir die Funktion model_parameters() um uns die exponierten Koeffizienten aus dem Modell wiedergeben zu lassen. Das Modell dient uns jetzt nur als Ausgangsmodell und wir werden das Poissonmodell jetzt nicht weiter tiefer verwenden.\n\nlizard_fit %>% model_parameters(exponentiate = TRUE)\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.29 | [0.60, 1.77] |  0.20 | 0.840 \ngrp [cover] | 1.88 | 0.46 | [1.18, 3.07] |  2.61 | 0.009 \nrain [dry]  | 0.31 | 0.09 | [0.17, 0.53] | -4.12 | < .001\nrain [wet]  | 0.13 | 0.05 | [0.06, 0.28] | -4.98 | < .001\npop [far]   | 2.41 | 0.61 | [1.49, 4.04] |  3.47 | < .001\n\n\nWir sehen, dass wir in der Variable rain eine starke Reduzierung der Anzahl an Eidechsen sehen. Vielleicht ist dies eine Variable, die zu viele Nullen produziert. Auch hat die Variable pop, die für die Nähe an einer Siedlung kodiert, einen starken positiven Effekt auf unsere Anzahl an Eidechsen. Hier wollen wir also einmal auf eine Zeroinflation überprüfen. Wir nutzen dazu die Funktion check_zeroinflation() aus dem R Paket performance. Die Funktion läuft nur auf einem Modellfit.\n\ncheck_zeroinflation(lizard_fit)\n\n# Check for zero-inflation\n\n   Observed zeros: 31\n  Predicted zeros: 27\n            Ratio: 0.87\n\n\nDie Funktion gibt uns wieder, dass wir vermutlich eine Zeroinflation vorliegen haben. Das können wir aber Modellieren. Um eine Zeroinflation ohne Overdispersion zu modellieren nutzen wir die Funktion zeroinfl() aus dem R Paket pscl. Der erste Teil der Funktion ist leicht erkläret. Wir bauen uns wieder unswer Model zusammen, was wir fitten wollen. Dann kommt aber ein | und mit diesem Symbol | definieren wir, ob wir wissen, woher die Nullen kommen oder aber ob wir die Nullen mit einem zufälligen Prozess modellieren wollen.\nWenn wir das Modell in der Form y ~ f1 + f2 | 1 schreiben, dann nehmen wir an, dass das Übermaß an Nullen in unseren Daten rein zufällig entstanden sind. Wir haben keine Spalte in de Daten, die uns eine Erklärung für die zusätzlichen Nullen liefern würde.\nWir können auch y ~ f1 + f2 | x3 schreiben. Dann haben wir eine Variable x3 in den Daten von der wir glauben ein Großteil der Nullen herrührt. Wir könnten also in unseren Daten annehmen, dass wir den Überschuss an Nullen durch den Regen erhalten haben und damit über die Spalte rain den Exzess an Nullen modellieren.\nMan sollte immer mit dem einfachsten Modell anfangen, deshalb werden wir jetzt einmal ein Modell fitten, dass annimmt, dass die Nullen durch einen uns unbekannten Zufallsprozess entstanden sind.\n\nlizard_zero_infl_intercept_fit <- zeroinfl(lizard ~ grp + pop + rain | 1, \n                                           data = lizard_zero_tbl) \n\nWir schauen uns das Modell dann wieder einmal an und sehen eine Zweiteilung der Ausgabe. In dem oberen Teil der Ausgabe wird unsere Anzahl an Eidechsen modelliert. In dem unteren Teil wird der Anteil der Nullen in den Daten modelliert. Daher können wir über Variablen in dem Zero-Inflation Block keine Aussagen über die Anzahl an Eidechsen treffen. Variablen tauchen nämlich nur in einem der beiden Blöcke auf.\n\nlizard_zero_infl_intercept_fit %>% \n  model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.31 | [0.60, 1.87] |  0.22 | 0.830 \ngrp [cover] | 2.03 | 0.51 | [1.25, 3.31] |  2.84 | 0.005 \npop [far]   | 2.59 | 0.67 | [1.56, 4.31] |  3.67 | < .001\nrain [dry]  | 0.31 | 0.10 | [0.17, 0.56] | -3.82 | < .001\nrain [wet]  | 0.14 | 0.06 | [0.06, 0.31] | -4.73 | < .001\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |   SE |       95% CI |     z |     p\n--------------------------------------------------------------\n(Intercept) |       0.11 | 0.11 | [0.02, 0.74] | -2.26 | 0.024\n\n\nAls erstes beobachten wir einen größeren Effekt der Variable grp. Das ist schon mal ein spannender Effekt. An der Signifikanz hat scih nicht viel geändert. Wir werden am Ende des Kapitels einmal alle Modell für die Modellierung der Zeroinflation vergleichen.\nNun könnte es auch sein, dass der Effekt der vielen Nullen in unserer Variable rain verborgen liegt. Wenn es also regnet, dann werden wir viel weniger Eidechsen beoabchten. Nehmen wir also rain als ursächliche Variable mit in das Modell für die Zeroinflation.\n\nlizard_zero_infl_rain_fit <- zeroinfl(lizard ~ grp + pop | rain, \n                                      data = lizard_zero_tbl)\n\nWieder schauen wir uns einmal die Ausgabe des Modells einmal genauer an.\n\nlizard_zero_infl_rain_fit %>% model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |    z |     p\n-------------------------------------------------------\n(Intercept) | 1.13 | 0.34 | [0.63, 2.03] | 0.42 | 0.677\ngrp [cover] | 1.60 | 0.42 | [0.95, 2.67] | 1.77 | 0.077\npop [far]   | 1.84 | 0.51 | [1.07, 3.18] | 2.20 | 0.028\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |     SE |          95% CI |     z |     p\n-------------------------------------------------------------------\n(Intercept) |       0.04 |   0.08 | [0.00,    2.09] | -1.60 | 0.109\nrain [dry]  |      27.94 |  59.06 | [0.44, 1760.08] |  1.58 | 0.115\nrain [wet]  |      83.71 | 178.43 | [1.28, 5459.08] |  2.08 | 0.038\n\n\nEs ändert sich einiges. Zum einen erfahren wir, dass der Regen anscheined doch viele Nullen in den Daten produziert. Wir haben ein extrem hohes \\(OR\\) für die Variable rain. Die Signifikanz ist jedoch eher gering. Wir haben nämlich auch eine sehr hohe Streuung mit den großen \\(OR\\) vorliegen. Au der anderen Seite verlieren wir jetzt auch die Signifikanz von unseren Habitaten und dem Standort der Population. Nur so mäßig super dieses Modell.\nWir können jetzt natürlich auch noch den Standort der Population mit in den Prozess für die Entstehung der Nullen hineinnehmen. Wir schauen uns dieses Modell aber nicht mehr im Detail an, sondern dann nur im Vergleich zu den anderen Modellen.\n\nlizard_zero_infl_rain_pop_fit <- zeroinfl(lizard ~ grp | rain + pop, \n                                          data = lizard_zero_tbl)\n\nDie Gefahr besteht immer, das man sich an die Wand modelliert und vor lauter Modellen die Übersicht verliert. Neben der Zeroinflation müssen wir ja auch schauen, ob wir eventuell eine Overdispersion in den Daten vorliegen haben. Wenn das der Fall ist, dann müsen wir nochmal überlegen, was wir dann machen. Wir testen nun auf Ovrdisprsion in unserem ursprünglichen Poissonmodell mit der Funktion check_overdispersion().\n\ncheck_overdispersion(lizard_fit)\n\n# Overdispersion test\n\n       dispersion ratio =  1.359\n  Pearson's Chi-Squared = 74.743\n                p-value =  0.039\n\n\nTja, und so erfahren wir, dass wir auch noch Overdispersion in unseren Daten vorliegen haben. Wir müsen also beides Modellieren. Einmal modellieren wir die Zeroinflation und einmal die Overdispersion. Wir können beides in einem negativen binominalen Modell fitten. Auch hier hilft die Funktion zeroinfl() mit der Option dist = negbin. Mit der Option geben wir an, dass wir eine negative binominal Verteilungsfamilie wählen. Damit können wir dann auch die Ovrdispersion in unseren Daten modellieren.\n\nlizard_zero_nb_intercept_fit <- zeroinfl(lizard ~ grp + rain + pop | 1, \n                                         dist = \"negbin\", data = lizard_zero_tbl)\n\nDann schauen wir usn einmal das Modell an. Zum einen sehen wir, dass der Effekt ähnlich groß ist, wie bei dem Intercept Modell der Funktion zeroinfl. Auch bleiben die Signifikanzen ähnlich.\n\nlizard_zero_nb_intercept_fit %>% model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.31 | [0.60, 1.87] |  0.22 | 0.830 \ngrp [cover] | 2.03 | 0.51 | [1.25, 3.31] |  2.84 | 0.005 \nrain [dry]  | 0.31 | 0.10 | [0.17, 0.56] | -3.82 | < .001\nrain [wet]  | 0.14 | 0.06 | [0.06, 0.31] | -4.73 | < .001\npop [far]   | 2.59 | 0.67 | [1.56, 4.31] |  3.67 | < .001\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |   SE |       95% CI |     z |     p\n--------------------------------------------------------------\n(Intercept) |       0.11 | 0.11 | [0.02, 0.74] | -2.26 | 0.024\n\n\nNun haben wir vier Modelle geschätzt und wolen jetzt wissen, was ist das beste Modell. Dafür hilft usn dann eine Gegenüberstellung der Modelle mit der Funktion modelsummary(). Wir könnten die Modelle auch gegeneinander statistsich Testen, aber hier behalten wir uns einmal den beschreibenden Vergleich vor. In Tabelle 41.4 sehen wir einmal die vier Modelle nebeneinander gestellt. Für eine bessere Übrsicht, habe ich aus allen Modellen den Intercept entfernt.\n\nmodelsummary(lst(\"ZeroInfl Intercept\" = lizard_zero_infl_intercept_fit,\n                 \"ZeroInfl rain\" = lizard_zero_infl_rain_fit,\n                 \"ZeroInfl rain+pop\" = lizard_zero_infl_rain_pop_fit,\n                 \"NegBinom intercept\" = lizard_zero_nb_intercept_fit),\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"),\n             coef_omit = \"Intercept\", \n             exponentiate = TRUE)\n\n\n\n\nTabelle 41.4—  Modellvergleich mit den vier Modellen. Wir schauen in wie weit sich die Koeffizienten und Modelgüten für die einzelnen Modelle im direkten Vergleich zum vollen Modell verändert haben. \n \n   \n    ZeroInfl Intercept \n    ZeroInfl rain \n    ZeroInfl rain+pop \n    NegBinom intercept \n  \n\n\n count_grpcover \n    2.031 \n    1.595 \n    1.611 \n    2.031 \n  \n\n  \n    [1.245, 3.313] \n    [0.951, 2.675] \n    [0.912, 2.845] \n    [1.245, 3.313] \n  \n\n  \n    s.e. = 0.507 \n    s.e. = 0.421 \n    s.e. = 0.468 \n    s.e. = 0.507 \n  \n\n  \n    t = 2.839 \n    t = 1.771 \n    t = 1.642 \n    t = 2.839 \n  \n\n  \n    p = 0.005 \n    p = 0.077 \n    p = 0.101 \n    p = 0.005 \n  \n\n count_popfar \n    2.591 \n    1.844 \n     \n    2.591 \n  \n\n  \n    [1.558, 4.310] \n    [1.069, 3.183] \n     \n    [1.558, 4.310] \n  \n\n  \n    s.e. = 0.673 \n    s.e. = 0.513 \n     \n    s.e. = 0.673 \n  \n\n  \n    t = 3.667 \n    t = 2.199 \n     \n    t = 3.667 \n  \n\n  \n    p = <0.001 \n    p = 0.028 \n     \n    p = <0.001 \n  \n\n count_raindry \n    0.308 \n     \n     \n    0.308 \n  \n\n  \n    [0.168, 0.564] \n     \n     \n    [0.168, 0.564] \n  \n\n  \n    s.e. = 0.095 \n     \n     \n    s.e. = 0.095 \n  \n\n  \n    t = -3.816 \n     \n     \n    t = -3.816 \n  \n\n  \n    p = <0.001 \n     \n     \n    p = <0.001 \n  \n\n count_rainwet \n    0.135 \n     \n     \n    0.135 \n  \n\n  \n    [0.059, 0.310] \n     \n     \n    [0.059, 0.310] \n  \n\n  \n    s.e. = 0.057 \n     \n     \n    s.e. = 0.057 \n  \n\n  \n    t = -4.726 \n     \n     \n    t = -4.726 \n  \n\n  \n    p = <0.001 \n     \n     \n    p = <0.001 \n  \n\n zero_raindry \n     \n    27.939 \n    98.196 \n     \n  \n\n  \n     \n    [0.443, 1760.078] \n    [0.0001, 76101776.131] \n     \n  \n\n  \n     \n    s.e. = 59.059 \n    s.e. = 679.400 \n     \n  \n\n  \n     \n    t = 1.575 \n    t = 0.663 \n     \n  \n\n  \n     \n    p = 0.115 \n    p = 0.507 \n     \n  \n\n zero_rainwet \n     \n    83.713 \n    402.409 \n     \n  \n\n  \n     \n    [1.284, 5459.081] \n    [0.0004, 418712524.080] \n     \n  \n\n  \n     \n    s.e. = 178.434 \n    s.e. = 2844.677 \n     \n  \n\n  \n     \n    t = 2.077 \n    t = 0.848 \n     \n  \n\n  \n     \n    p = 0.038 \n    p = 0.396 \n     \n  \n\n zero_popfar \n     \n     \n    0.148 \n     \n  \n\n  \n     \n     \n    [0.022, 1.000] \n     \n  \n\n  \n     \n     \n    s.e. = 0.144 \n     \n  \n\n  \n     \n     \n    t = -1.960 \n     \n  \n\n  \n     \n     \n    p = 0.050 \n     \n  \n\n Num.Obs. \n    60 \n    60 \n    60 \n    60 \n  \n\n R2 \n    0.620 \n    0.477 \n    0.454 \n    0.620 \n  \n\n R2 Adj. \n    0.585 \n    0.449 \n    0.435 \n    0.585 \n  \n\n AIC \n    157.3 \n    167.2 \n    167.4 \n    159.3 \n  \n\n BIC \n    169.8 \n    179.8 \n    180.0 \n    173.9 \n  \n\n RMSE \n    1.27 \n    1.27 \n    1.32 \n    1.27 \n  \n\n\n\n\n\nDie beiden Intercept Modelle haben die kleinsten \\(AIC\\)-Werte der vier Modelle. Darüber hinaus haben dann beide Modelle auch die höchsten \\(R^2_{adj}\\) Werte. Beide Modelle erklären also im Verhältnis viel Varianz mit 58.5%. Auch ist der \\(RMSE\\) Wert als Fehler bei beiden Modellen am kleinsten. Damit haben wir die Qual der Wahl, welches Modell wir nehmen. Ich würde das negative binominal Modell nehmen. Wir haben ins unseren Daten vermutlich eine Zeroinflation sowie eine Overdispersion vorliegen. Daher bietest es sich an, beides in einer negativen binominalen Regression zu berücksichtigen. Zwar sind die beiden Intercept Modelle in diesem Beispielfall von den Koeffizienten fast numerisch gleich, aber das hat eher mit dem reduzierten Beispiel zu tun, als mit dem eigentlichen Modell. In unserem Fall ist die Overdispersion nicht so extrem.\nWie sehe den unser negative binominal Modell aus, wenn wir mit dem Modell einmal die zu erwartenden Eidechsen vorhersagen würden? Auch das kann helfen um abzuschätzen, ob das Modelle einigermaßen funktioniert hat. Wir haben ja hier den Vorteil, dass wir nur mit kategorialen Daten arbeiten. Wir haben keine kontiniuerlichen Variablen vorliegen und darüber hinaus auch nicht so viele Variablen insgesamt.\nDaher bauen wir uns mit expand_grid() erstmal einen Datensatz, der nur aus den Faktorkombinationen besteht. Wir haben also nur eine Beobachtung je Faktorkombination. Danach nutzen wir die Daten einmal in der Funktion predict() um uns die vorhergesagten Eidechsen nach dem gefitten Modell wiedergeben zu lassen.\n\nnewdata_tbl <- expand_grid(grp = factor(1:2, labels = c(\"open\", \"cover\")),\n                           rain = factor(1:3, labels = c(\"no\", \"dry\", \"wet\")),\n                           pop = factor(1:2, labels = c(\"near\", \"far\")))\n\npred_lizards <- predict(lizard_zero_nb_intercept_fit, newdata = newdata_tbl) \n  \nnewdata_tbl <- newdata_tbl %>% \n  mutate(lizard = pred_lizards)\n\nNachdem wir in dem Datensatz newdata_tbl nun die vorhergesagten Eidechsen haben, können wir uns jetzt in der Abbildung 41.5 die Zusammenhänge nochmal anschauen.\n\nggplot(newdata_tbl, aes(x = rain, y = lizard, colour = grp, group = grp)) +\n  theme_bw() +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ pop) +\n  labs(x = \"Feuchtigkeit nach Regen\", y = \"Anzahl der gezählten Eidechsen\"\",\n       color = \"Gruppe\") +\n  scale_color_okabeito()\n\n\n\nAbbildung 41.5— Scatterplot der vorhergesagten Eidechsen in den Habitaten (grp), der Feuchtigkeit des Bodens nach Regen und dem Abstand zur nächsten Ortschaft.\n\n\n\n\nWir erkennen, dass mit der Erhöhung der Feuchtigkeit die Anzahl an aufgefundenen Eidechsen sinkt. Der Effekt ist nicht mehr so stark, wenn es schon einmal geregnet hat. Ebenso macht es einen Unterschied, ob wir nahe einer Siedlung sind oder nicht. Grundsätzlich finden wir immer mehr Eidechsen in geschützten Habitaten als in offenen Habitaten."
  },
  {
    "objectID": "stat-modeling-poisson.html#referenzen",
    "href": "stat-modeling-poisson.html#referenzen",
    "title": "41  Poisson Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "eda-distribution.html",
    "href": "eda-distribution.html",
    "title": "\n18  Verteilung von Daten\n",
    "section": "",
    "text": "Version vom November 14, 2022 um 17:26:17\nIn diesem Kapitel wollen wir uns mit Verteilungen beschäftigen. Dormann (2013) liefert eine weitreichende Übersicht über verschiedene Verteilungen. Wir wollen uns in diesem Kapitel mit folgenden Verteilungen beginnen.\nWir wollen uns jetzt die verschiedenen Verteilungen einmal in der Anwendung anschauen. Dabei lassen wir viel Mathematik recht und links liegen. Du kannst bei Dormann (2013) mehr zu dem Thema statistische Verteilungen anlesen.\nIn diesem Kapitel geht es erstmal um das Grundverständnis, das Daten einer Verteilung folgen. Oder noch konkreter, dass unser Outcome \\(y\\) einer Verteilung folgt. Wir müssen später unseren Alogrithmen sagen, welcher Verteilung \\(y\\) entspringt, sonst können wir keine korrekte Analyse unser Daten rechnen."
  },
  {
    "objectID": "eda-distribution.html#genutzte-r-pakete-für-das-kapitel",
    "href": "eda-distribution.html#genutzte-r-pakete-für-das-kapitel",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.1 Genutzte R Pakete für das Kapitel",
    "text": "18.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, see, readxl)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "eda-distribution.html#daten-für-verteilungen",
    "href": "eda-distribution.html#daten-für-verteilungen",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.2 Daten für Verteilungen",
    "text": "18.2 Daten für Verteilungen\nDamit wir uns auch eine Verteilung anschauen können bruachen wir viele Beobachtungen. Wir haben das ja schon bei den Histogrammen gesehen, wenn wir ein aussagekräftiges Histogramm erstellen wollen, dann brauchen wir viele Beobachtungen. Daher nehmen wir für dieses Kapitel einmal den Gummibärchendatensatz und schauen uns dort die Variablen gender, height, count_bears und count_color einmal genauer an. Wie immer nutzen wir die Funktion select() um die Spalten zu selektieren. Abschließend verwandeln wir das Geschlecht gender und das module noch in einen Faktor.\n\ngummi_tbl <- read_excel(\"data/gummibears.xlsx\")  %>%\n  select(year, module, gender, height, count_bears, count_color,\n         most_liked) %>% \n  mutate(gender = as_factor(gender),\n         module = as_factor(module))\n\nWir erhalten das Objekt gummi_tbl mit dem Datensatz in Tabelle 18.1 nochmal dargestellt.\n\n\n\n\nTabelle 18.1— Auszug aus den selektierten Daten zu den Gummibärchendaten.\n\n\n\n\n\n\n\n\n\n\nyear\nmodule\ngender\nheight\ncount_bears\ncount_color\nmost_liked\n\n\n\n2018\nFU Berlin\nm\n193\n9\n3\nlightred\n\n\n2018\nFU Berlin\nw\n159\n10\n5\nyellow\n\n\n2018\nFU Berlin\nw\n159\n9\n6\nwhite\n\n\n2018\nFU Berlin\nw\n180\n10\n5\nwhite\n\n\n2018\nFU Berlin\nm\n180\n10\n6\nwhite\n\n\n2018\nFU Berlin\nm\nNA\n10\n5\nwhite\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n2022\nStatistik\nm\n197\n9\n4\nwhite\n\n\n2022\nStatistik\nm\n180\n10\n3\ngreen\n\n\n2022\nStatistik\nm\n187\n11\n5\ndarkred\n\n\n2022\nStatistik\nm\n186\n10\n5\ngreen\n\n\n2022\nStatistik\nm\nNA\n9\n4\ndarkred\n\n\n2022\nStatistik\nNA\nNA\n10\n6\nNA\n\n\n\n\n\n\nWir nutzen jetzt die Daten einmal um uns die Normalverteilung und die Poissonverteilung am Beispiel näher anzuschauen."
  },
  {
    "objectID": "eda-distribution.html#sec-normal",
    "href": "eda-distribution.html#sec-normal",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.3 Die Normalverteilung",
    "text": "18.3 Die Normalverteilung\nWir sprechen in der Statistik auch von Verteilungsfamilien. Daher schreiben wir in R auch family = gaussian, wenn wir sagen wollen, dass unsere Daten einer Normalverteilung entstammen.\nWenn wir von de Normalverteilung sprechen, dann schreiben wir ein \\(\\mathcal{N}\\) Symbol - also ein großes N mit Serifen. Die Normalverteilung sieht aus wie eine Glocke, deshalb wird die Normalverteilung auch Glockenkurve genannt. Im englischen Sprachgebrauch und auch in R nutzen wir dagegen die Bezeichnung nach dem “Entdecker” der Normalverteilung, Carl Friedrich Gauß (1777 - 1985). Wir nennen daher die Normalverteilung auch Gaussian-Verteilung.\nParameter sind Zahlen, die eine Verteilungskurve beschreiben.\nEine Normalverteilung wird ruch zwei Verteilungsparameter definiert. Eine Verteilung hat Parameter. Parameter sind die Eigenschaften einer Verteilung, die notwendig sind um eine Verteilung vollständig zu beschreiben. Im Falle der Normalverteilung brauchen wir zum einen den Mittelwert \\(\\bar{y}\\), der den höchsten Punkt unserer Glockenkurve beschreibt. Zum anderen brauchen wir auch die Standardabweichung \\(s^2_y\\), die die Ausbreitung oder Breite der Glockenkurve bestimmt. Wir beschreiben eine Normalverteilung für eine Stichprobe mit \\(\\bar{y}\\) und \\(s^2_y\\) wie folgt.\n\\[\n\\mathcal{N}(\\bar{y}, s^2_y)\n\\]\nOder mit mehr Details in folgender Form. Wir können hier Verallgemeinern und schreiben in der Grundgesamtheit mit \\(\\mu = \\bar{y}\\) und \\(\\sigma^2 = s^2_y\\). Das heißt, wenn wir unendlich viele Beobachtungen vorliegen hätten, dann wüssetn wir auch den wahren Mittelwert \\(\\mu\\) und die wahre Varianz \\(\\sigma^2\\) der Daten.\n\\[\nf(y \\mid\\mu,\\sigma^2)=\\cfrac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\cfrac{(y-\\mu)^2}{2\\sigma^2}}\\quad -\\infty<y<\\infty\n\\]\nIm Falle der Normalverteilung brauchen wir einen Paramter für den höchsten Punkt der Kurve, sowie einen Parameter für die Ausbreitung, also wie weit geht die Kurve nach links und nach rechts. Je nach \\(\\bar{y}\\) und \\(s^2_y\\) können wir verschiedenste Normalverteilungen vorliegen haben. Eine Sammlung von Normalverteilungen nennen wir auch Familie (eng. family).\nWir haben Varianzhomogenität vorliegen, wenn \\(s^2_{1} = s^2_{2} = s^2_{3}\\) sind. Wir haben Varianzheterogenität vorliegen, wenn \\(s^2_{1} \\neq s^2_{2} \\neq s^2_{3}\\) sind.\nIn Abbildung 18.1 sehen wir verschiedene Normalverteilungen mit unterschiedlichen Mittelwerten. In Abbildung 18.1 (a) sehen wir eine Varianzhomogenität vorliegen, da die Varianzen in allen drei Normalverteilungen gleich sind. Wir können auch schreiben, dass \\(s^2_{1} = s^2_{2} = s^2_{3} = 2\\). In Abbildung 18.1 (b) haben wir Varianzheterogenität vorliegen, da die Varianzen der Normalverteilungen ungleich sind. Wir können hier dann schreiben, dass \\(s^2_{1} = 6 \\neq s^2_{2} = 1 \\neq s^2_{3} = 3\\) sind. Häufig gehen statistische Verfahren davon aus, dass wir Varianzhomogenität über die Gruppen und daher auch die Normalverteilungen vorliegen haben. Konkret, wenn wir die Sprungweiten in[cm] von Hunde- und Katzenflöhen mit einander vergleichen wollen, dann gehen wir erstmal davon aus, dass die Mittelwerte verschieden sind, aber die Varianzen gleich sind.\n\n\n\n\n\n(a) Drei Normalverteilungen mit Varianzhomogenität.\n\n\n\n\n\n\n(b) Drei Normalverteilungen unter Varianzheterogenität.\n\n\n\n\nAbbildung 18.1— Histogramm verschiedener Normalverteilungen mit unterschiedlichen Mittelwerten.\n\n\nIn einer Normalverteilung liegen 68% der Werte innerhalb \\(\\bar{y}\\pm 1 \\cdot s_y\\) und 95% der Werte innerhalb \\(\\bar{y}\\pm 2 \\cdot s_y\\)\nWenn wir eine Normalverteilung vorliegen haben, dann liegen 68% der Werte plus/minus einer Standardabweichung vom Mittelwert. Ebenso liegen 95% der Werte plus/minus zwei Standabweichungen vom Mittelwert. Über 99% der Werte befinden sich innerhalb von drei Standardabweichungen vom Mittelwert. Diese Eigenschaft einer Normalverteilung können wir später noch nutzen um abzuschätzen, ob wir einen relevanten Gruppenunterschied vorliegen haben oder aber ob unsere Daten unnatürlich breit streuen.\nWir nutzen das Wort approximativ wenn wir sagen wollen, dass ein Outcome näherungsweise normalverteilt ist.\nSchauen wir uns die Normalverteilung einmal am Beispiel unserer Gummibärchendaten und der Körpergröße der Studierenden an. Wir färben das Histogramm nach dem Geschlecht ein. In Abbildung 18.2 sehen wir das Ergebnis einmal als Histogramm und einmal als Densityplot dargestellt. Wir können annehmen, dass die Größe approximativ normalverteilt ist.\n\n\n\n\n\n(a) Histogramm.\n\n\n\n\n\n\n(b) Densityplot.\n\n\n\n\nAbbildung 18.2— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt.\n\n\nWir können die Funktion rnorm() nutzen um uns zufällige Zahlen aus der Normalverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert mean = und die gewünschte Standardabweichung mit sd = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rnorm() mit zehn Werten.\n\nrnorm(n = 10, mean = 5, sd = 2) %>% round(2)\n\n [1] 6.97 6.24 5.59 5.27 6.30 4.91 4.13 4.92 3.67 4.42\n\n\nDu kannst ja mal den Mittelwert und die Standardabweichung der zehn Zahlen ausrechnen. Da wir es hier mit einer Stichprobe mit zehn Beobachtungen zu tun haben, wird der Mittelwert \\(\\bar{y}\\) und die Standardabweichung \\(s_y\\) sich von den vorher definierten Mittelwert \\(\\mu_y = 5\\) und Standardabweichung \\(\\sigma_y = 2\\) der Grundgesamtheit unterscheiden.\nWir können auch aus unseren Gummibärchendaten für die Körpergröße in [cm] jeweils den Mittelwert und die Standardabweichung getrennt für die Geschlechter berechnen und dann die theoretische Normalverteilung zeichenen. In Abbildung 18.3 (b) und Abbildung 18.3 (d) sehen wir die Verteilung der theoretischen Werte, wenn wir die Mittelwerte und die Standardabweichung aus den Verteilungen in Abbildung 18.3 (a) schätzen.\n\n\n\n\n\n(a) Verteilung der beobachteten Werte.\n\n\n\n\n\n\n(b) Verteilung der theoretischen Werte.\n\n\n\n\n\n\n\n\n(c) Verteilung der beobachteten Werte.\n\n\n\n\n\n\n(d) Verteilung der theoretischen Werte.\n\n\n\n\nAbbildung 18.3— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte. Einmal dargestellt als Histogramm und einmal als Densityplot."
  },
  {
    "objectID": "eda-distribution.html#die-standardnormalverteilung",
    "href": "eda-distribution.html#die-standardnormalverteilung",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.4 Die Standardnormalverteilung",
    "text": "18.4 Die Standardnormalverteilung\nEs gibt viele Normalverteilungen. Aber es gibt eine besondere Normalverteilung, so dass diese Verteilung einen eigenen Namen hat. Wir sprechen von der Standardnormalverteilung, wenn der Mittelwert gleich Null ist und die Standardabweichung gleich Eins. Du siehst hier nochmal die Standardnormalverteilung ausgeschrieben.\n\\[\n\\mathcal{N}(0, 1)\n\\]\nFolgende Eigenschaften sind der Standardnormalverteilung gegeben. Die Standardnormalverteilung hat eine Fläche von \\(A = 1\\) unter der Kurve. Darüber hinaus liegen 95% der Werte zwischen -2 und 2. Die einzelnen Werte einer Standardnormalverteilung nennen wir \\(z\\)-Werte. Wenn wir eine beliebige Normalverteilung in eine Standardnormalverteilung überführen wollen so machen wir die Umwandlung mit der \\(z\\)-Transformation.\n\\(Pr(X \\leq x)\\) mit lower.tail = TRUE\n\\(Pr(X > x)\\) mit lower.tail = FALSE\n\npnorm(q = 1.96, mean = 0, sd = 1, lower.tail = FALSE) %>% round(3)\n\n[1] 0.025\n\n\n\nqnorm(p = c(0.025, 0.05), mean = 0, sd = 1, lower.tail = FALSE) %>% round(3)\n\n[1] 1.960 1.645\n\n\n\nqnorm(p = 0.05, mean = 0, sd = 1, lower.tail = TRUE) %>% round(3)\n\n[1] -1.645"
  },
  {
    "objectID": "eda-distribution.html#sec-poisson",
    "href": "eda-distribution.html#sec-poisson",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.6 Die Poissonverteilung",
    "text": "18.6 Die Poissonverteilung\nEine weitere wichtige Verteilung ist die Poissonverteilung. Die Poissonverteilung ist eine diskrete Verteilung. Daher kommen nur ganze Zahlen vor. Damit bildet die Poissonverteilung die Zähldaten ab. Wenn wir also etwas Zählen, dann ist diese Variable mit den gezählten Ergebnissen poissonverteilt. Im Folgenden sehen wir die Poissonverteilung einmal dargestellt.\n\\[\n\\mathcal{Pois}(\\lambda)\n\\]\nOder mit mehr Details in folgender Form.\n\\[\nP_\\lambda (k) = \\frac{\\lambda^k}{k!}\\, \\mathrm{e}^{-\\lambda}\n\\]\nDie Poisson-Verteilung gibt dann die Wahrscheinlichkeit einer bestimmten Ereignisanzahl \\(k\\) im Einzelfall an, wenn die mittlere Ereignisrate \\(\\lambda\\) bekannt ist. Im Gegensatz zur Normalverteilung hat die Poissonverteilung nur einen Parameter. Den Lageparameter \\(\\lambda\\) ausgedrückt durch den griechischen Buchstaben Lambda. Eine Poissonverteilung mit \\(\\mathcal{Pois}(4)\\) hat den höchsten Punkt bei vier. Nun hat die Poissonverteilung hat mehrere Besonderheiten. Da die Poissonverteilung keinen Streuungsparameter hat, steigt mit dem \\(\\lambda\\) auch die Streuung. Daher haben Poissonverteilungen mit einem großen \\(\\lambda\\) auch eine große Streuung. ie Ausbreitung der Kurve ist eine Funktion von \\(\\lambda\\) und steigt mit \\(\\lambda\\) an. Du kannst diesen Zusammenhang in Abbildung 18.4 beobachten.\nDarüber hinaus kann eine Poissonverteilung nicht negativ werden. Es kann keine kleinere Zahl als die Null geben. Durch die diskreten Zahlen haben wir auch immer mal Lücken zwischen den Balken der Poissonverteilung. Das passiert besonders, wenn wir eine kleine Anzahl an Beobachtungen haben. Abschließend konvergiert die Poissonverteilung bei großen \\(\\lambda\\) hin zu einer Normalverteilung.\n\n\n\n\nAbbildung 18.4— Histogramm verschiedener Poissonverteilungen.\n\n\n\n\nSchauen wir uns nun einmal die Poissonverteilung im Beispiel an. In Abbildung 18.5 sehen wir die Histogramme der Anzahl an Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte. Da wir es hier mit Zähldaten zu tun haben, könnte es sich um eine Poissonverteilung handeln. Wie müssen uns nun die Frage stellen, ob die Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte wirklich eine zufällige Realistierung sind. Daher eine zufällige Stichprobe der Grundgesamtheit. Wir können diese Annahme überprüfen in dem wir die theoretischen Werte für die beiden Poissonverteilung mit \\(\\mathcal{Pois}(10)\\) und \\(\\mathcal{Pois}(5)\\) genieren.\n\n\n\n\n\n(a) Anzahl an Bärchen\n\n\n\n\n\n\n(b) Anzahl an Farben\n\n\n\n\nAbbildung 18.5— Histogramme der Anzahl an Gummibärchen und die Anzahl an Farben in einer Tüte. Es gibt nicht mehr als sechs Farben.\n\n\nWir können die Funktion rpois() nutzen um uns zufällige Zahlen aus der Poissonverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert lambda = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rpois() mit zehn Werten.\n\nrpois(n = 10, lambda = 5)\n\n [1] 3 5 4 5 3 3 9 3 5 5\n\n\nEs gibt neben der Poissonverteilung auch die negative Binomialverteilung sowie die Quasi-Poissonverteilung, die es erlauben einen Streuungsparameter für die Poissonverteilung zu schätzen.\nWir können nun auch aus unseren Gummibärchendaten für die Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte die theoretische Poissonverteilung berechnen. In Abbildung 18.6 sehen wir die Verteilung der beobachteten Werte für Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte und deren theoretischen Verteilung nach dem geschätzen \\(\\lambda = 10\\) und \\(\\lambda = 5\\). Wir sehen ganz klar, dass die beide Variablen keine Zufallsrealisierung sind. Zum einen haben wir das auch nicht erwartet, es gibt nicht mehr als sechs Farben und zum anderen ist zu vermuten, dass Haribo technisch in den Auswahlprozess eingreift. Wir haben auf jeden Fall eine sehr viel kleinere Streuung als bei einer klassischen Poissonverteilung anzunehmen wäre.\n\n\n\n\n\n(a) Verteilung der beobachteten Anzahl an Bärchen.\n\n\n\n\n\n\n(b) Verteilung der theoretischen Anzahl an Bärchen.\n\n\n\n\n\n\n\n\n(c) Verteilung der beobachteten Anzahl an Farben.\n\n\n\n\n\n\n(d) Verteilung der theoretischen Anzahl an Farben.\n\n\n\n\nAbbildung 18.6— Darstellung Anzahl an Bärchen und Anzahl an Farben. Es gibt nicht mehr als sechs Farben. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte.\n\n\nIn Abbildung 18.7 schauen wir uns nochmal an in wie weit sich die Füllung der Tütchen im Laufe der Jahre entwickelt hat. Die Daten werden ja schon seit 2018 erhoben. Wir schauen uns daher die Densityplot einmal aufgetrennt für die Jahre 2018 bis heute an. Das Jahr 2020 fehlt, da bedingt durch die Coronapandemie keine Präsenslehre stattfand. Wir sehen, dass sich die Verteilung anscheinend in dem Jahr 2022 langsam nach links zu weniger Bärchen in einer Tüte bewegt. Wir bleiben gespannt auf den weiteren Trend.\n\n\n\n\nAbbildung 18.7— Densityplot der Anzahl an Bärchen in einer Tüte aufgetrennt nach den Jahren der Erhebung. Das Jahr 2020 fehlt bedingt durch die Coronapandemie.\n\n\n\n\nIn Abbildung 18.8 betrachten wir die Verteilung der am meisten gemochten Gummibärchen aufgeteilt nach dem angegebenen Geschlecht im Vergeich zu den Gummibärchen in den Tütchen. Wir sehen, dass Haribo die Tütchen sehr gleichmäßig verteilt und auf die Geschmäcker keinerlei Rücksicht nimmt. Entweder weiß Haribo nichts von den Vorlieben seiner Käufer:innen oder aber es ist dann doch zu viel Aufwand die Produktion anzupassen.\n\n\n\n\n\n(a) Anzahl am liebsten gemochten Gummibärchen aufgeteilt nach Geschlecht.\n\n\n\n\n\n\n(b) Anzahl der Gummibärchen pro Tüte nach Farbe.\n\n\n\n\nAbbildung 18.8— Histogramme der am liebsten gemochten Gummibärchchen im Vergleich zum Inhalt der Tütchen."
  },
  {
    "objectID": "eda-distribution.html#weitere-verteilungen",
    "href": "eda-distribution.html#weitere-verteilungen",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.9 Weitere Verteilungen",
    "text": "18.9 Weitere Verteilungen\n\n\nWir besuchen gerne die R Shiny App The distribution zoo um mehr über die verschiedenen Verteilungen und deren Parameter zu erfahren.\nWeitere Beispiele finden sich unter Basic Probability Distributions in R. Im Weiteren liefert Dormann (2013) eine gute Übersicht über verschiedene Verteilungen und deren Repräsentation in R. Das ist nur eine Auswahl an möglichen Verteilungen. Bitte hier nicht ins rabbit hole der Verteilungen gehen. Wir benötigen in unserer täglichen Arbeit nur einen kleinen Teil der Verteilungen. Es reicht, wenn du eine Vorstellungen der Verteilungen in diesem Kapitel hat."
  },
  {
    "objectID": "eda-distribution.html#referenzen",
    "href": "eda-distribution.html#referenzen",
    "title": "\n18  Verteilung von Daten\n",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "eda-distribution.html#sec-t-dist",
    "href": "eda-distribution.html#sec-t-dist",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.5 Die t-Verteilung",
    "text": "18.5 Die t-Verteilung\n\\(Pr(X \\leq x)\\) mit lower.tail = TRUE\n\\(Pr(X > x)\\) mit lower.tail = FALSE\n\npt(q = 2.571, df = 5, lower.tail = FALSE) %>% round(3)\n\n[1] 0.025\n\n\n\nqt(p = c(0.025), df = c(5, 10, 20, 100), lower.tail = FALSE) %>% round(3)\n\n[1] 2.571 2.228 2.086 1.984"
  },
  {
    "objectID": "eda-distribution.html#sec-uniform",
    "href": "eda-distribution.html#sec-uniform",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.8 Die Uniformverteilung",
    "text": "18.8 Die Uniformverteilung\n\\[\nf(x)=\\begin{cases}\n  \\frac 1{b-a} & a \\le x \\le b\\\\\n  0            & \\text{sonst.}\n\\end{cases}\n\\]\n\n\n\n\nAbbildung 18.10— Beispiel für eine uniforme Verteilung anhand der Anzahl der Gummibärchen pro Tüte nach Farbe"
  },
  {
    "objectID": "eda-distribution.html#sec-binom",
    "href": "eda-distribution.html#sec-binom",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.7 Die Binominalverteilung",
    "text": "18.7 Die Binominalverteilung\n\\[\nB(k\\mid p,n)=\\begin{cases}\n  \\binom nk p^k (1-p)^{n-k} &\\text{falls} \\quad k\\in\\left\\{0,1,\\dots,n\\right\\}\\\\\n  0            & \\text{sonst.}\\end{cases}\n\\]\n\n\n\n\nAbbildung 18.9— Beispiel für eine Binomialverteilung anhand des Geschlechts. Die fehlenden Angaben wurden entfernt."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skript Bio Data Science",
    "section": "",
    "text": "Auf den folgenden Seiten wirst du eine Menge über Statistik oder Data Science lernen. Du musst dafür nicht eine meiner Veranstaltungen besuchen. Gerne kannst du hier und dort einmal schauen, ob etwas für dich dabei ist. Das Skript wird fortlaufend von mir ergänzt. Neben dem Skript gibt es auch noch die erklärenden YouTube Videos. Ich freue mich, dass du Lust hast hier etwas zu lernen… oder aber du musst – da bald eine Klausur ansteht. Wie auch immer – schau dich einfach mal um. Im Anhang findest du auch einen kleinen Leitfaden für das Schreiben einer Abschlussarbeit. Vielleicht hilft dir die Anleitung ja beim Schreiben.\n\n\n\n\n\n\nGesammelte Klausurfragen in der Bio Data Science\n\n\n\n\n\nDu findest die gesammelten Klausurfragen auf GitHub oder auf ILIAS in dem entsprechenden Modul. Die Klausurfragen zu den einzelnen Vorlesungen in einem Modul werden in den entsprechenden Übungen des Moduls besprochen. Bitte komme in die Übungen.\nDu brauchst dir die Fragen nicht alle auszudrucken. Wir besprechen die Fragen teilweise in den Übungen.\nDie finale Version für die Klausur veröffentliche ich Ende November für das Wintersemester bzw. Ende Mai für das Sommersemester.\n\n\n\n\n\n\n\n\n\nInformationen zu einer Bachelorarbeit in der Bio Data Science\n\n\n\n\n\nWenn du dich fragst, wie die Rahmenbedingungen einer Bachelorarbeit bei mir sind findest du hier die Informationen zu einer Bachelorarbeit in der Bio Data Science. Oder du fragst mich einfach unverbindlich per Mail oder in einer meiner Vorlesungen. Wie es dir am besten passt.\n\n\n\n\n\nDu liest hier gerade das Skript für meine Vorlesungen an der Hochschule Osnabrück an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Wie immer Leben kannst du auf verschiedene Arten und Weisen den Stoff, den ich vermitteln will, lernen. Daher gibt es noch zwei andere Möglichkeiten. Zum einen Lernen auf YouTube, mit meinen Lernvideos oder du schaust dir das Material auf GitHub an. Auf GitHub habe ich auch Informationen, die du vielleicht brauchen kannst. Ebenso findest du im Kapitel 2 noch andere Literaturempfehlungen.\n\n\n\n\n\n\n\nWenn du möchtest kannst du auf YouTube unter https://www.youtube.com/c/JochenKruppa noch einige Lehrvideos als Ergänzung schauen. In den Videos wiederhole ich Inhalte und du kannst auf Pause drücken um nochmal Programmierschritte nachverfolgen zu können.\n\n\n\n\n\n\n\n\nAlle Materialien von mir findest du immer auf GitHub unter https://github.com/jkruppa/teaching. Selbst wenn du nicht mehr in einem meiner Kurse bist, kannst du so auf die Lehrinhalte immer nochmal zugreifen und die aktuellen Versionen haben. Auf GitHub liegt auch immer eine semesteraktuelle Version der gesammelten Klausurfragen für meine Module.\n\n\n\n\nWie erreichst du mich? Am einfachsten über die gute, alte E-Mail. Bitte bachte, dass gerade kurz vor den Prüfungen ich mehr E-Mails kriege. Leider kann es dann einen Tick dauern.\n\n\n\n\n\nEinfach an j.kruppa@hs-osnabrueck.de schreiben. Du findest hier auch eine kurze Formulierungshilfe. Einfach auf den Ausklapppfeil klicken.\nBitte gib immer in deiner E-Mail dein Modul - was du belegst - mit an. Pro Semester unterrichte ich immer drei sehr ähnlich klingende Module. Daher schau nochmal hier in der Liste, wenn du unsicher bist.\n\n\n\n\n\n\nE-Mailvorlage mit beispielhafter Anrede\n\n\n\n\n\nHallo Herr Kruppa,\n… ich belege gerade Ihr Modul Modulname und hätte eine Bitte/Frage/Anregung…\n… ich benötige Hilfe bei der Planung/Auswertung meiner Bachelorarbeit…\nMit freundlichen Grüßen\nM. Muster"
  },
  {
    "objectID": "organisation.html",
    "href": "organisation.html",
    "title": "\n1  Organisation\n",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:47:40\nDen Teil kannst du hier überspringen, wenn es dich nicht so richtig interessiert, was ich alles an Vorlesungen an der Hochschule Osnabrück anbiete. Wenn es dir um statistische Inhalte geht, dann gehe einfach weiter in die nächsten Kapitel. In diesem Kapitel geht es nochmal Orientierung über meine Vorlesungen zu geben, wenn dich noch mehr als nur der Pflichtkurs interessiert."
  },
  {
    "objectID": "organisation.html#statistische-beratung",
    "href": "organisation.html#statistische-beratung",
    "title": "\n1  Organisation\n",
    "section": "\n1.1 Statistische Beratung",
    "text": "1.1 Statistische Beratung\nNeben der klassischen Vorlesung biete ich auch Termine für die statistische Beratung von Abschlussarbeiten sowie Projekten an. Dieses Angebot gilt es für alle Mitglieder der Hochschule Osnabrück. Primär für Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL), aber ntürlich auch für alle anderen Fakultäten. Dafür musst du mir einfach nur eine E-Mail schreiben und dann erhälst du einen Termin innerhalb der nächsten zwei Wochen.\nDie Beratung ist grundsätzlich anonym und vertraulich. Wenn du willst kannst du gerne noch dein:e Betreuer:in mitbringen. Das ist aber keine Voraussetzung oder Notwendigkeit. Meistens finden mehrere Besprechungen statt, wir versuchen aber natürlich zusammen zügig dein Problem zu lösen. Ziel ist der Beratung ist es dich in die Lage zu versetzen selbstständig deine Analyse zu rechnen."
  },
  {
    "objectID": "organisation.html#sec-r-tutorium",
    "href": "organisation.html#sec-r-tutorium",
    "title": "\n1  Organisation\n",
    "section": "\n1.2 R Tutorium",
    "text": "1.2 R Tutorium\nZusätzlich zu der statistischen Beratung bieten wir auch ein R Tutorium für alle Mitglieder der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) an. Theoretisch können auch hier andere Mitglieder der anderen Faklutäten vorbeischauen, der Ort ist aber aktuell ein Raum auf dem Gelände in Haste. Die aktuellen Termine findest du in Tabelle 1.1.\nIm R Tutorium besprechen wir aktuelle Themen der Teilnehmer:innen. Meist sind dies aktuelle Fragen zu den Bachelorarbeiten. Auch wenn du kein dringendes Problem hast, kannst du gerne kommen und dir die Fragestellungen anhören. Meistens ist es auch interessant mal die Fragestellungen der anderen Studierenden sich anzuhören oder aber schon mal zu Üben ein anderes Experiment zu verstehen.\nBitte beachte folgende Hinweise zu den Terminen.\n\n\nHier findest du den Lage- und Gebäudeplan vom Standort Haste.\n\n\n\n\n\n\nHinweise zu dem R Tutorium\n\n\n\nDas R Tutorium findet nicht im Prüfungszeitraum der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) statt.\nDas R Tutorium findet nicht im Februar und März statt.\nDas R Tutorium findet nicht im August und September statt.\n\n\n\n\n\nTabelle 1.1— Aktuelle Termine des R Tutoriums im Semester. Hier findest du den Lage- und Gebäudeplan vom Standort Haste.\n\nTermin\nUhrzeit\nRaum\nAnmerkung\n\n\n\nDienstag, den 15. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 22. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 29. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 06. Dezember 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 13. Dezember 2022\n9:45 - 11:15\nHM 0115\n\n\n\nOnlinetermine ab jetzt…\n\n\n\n\n\nDienstag, den 20. Dezember 2022\n9:45 - 11:15\n[online]\nohne Kruppa\n\n\nWeihnachtswoche\n\n\n\n\n\nDienstag, den 03. Januar 2023\n9:45 - 11:15\n[online]\n\n\n\nDienstag, den 10. Januar 2023\n9:45 - 11:15\n[online]\n\n\n\nDienstag, den 17. Januar 2023\n9:45 - 11:15\n[online]\nOptionaler Termin\n\n\nDienstag, den 23. Januar 2023\n9:45 - 11:15\n[online]\nOptionaler Termin"
  },
  {
    "objectID": "organisation.html#sec-vorlesungen-hs",
    "href": "organisation.html#sec-vorlesungen-hs",
    "title": "\n1  Organisation\n",
    "section": "\n1.3 Vorlesungen an der Hochschule Osnabrück",
    "text": "1.3 Vorlesungen an der Hochschule Osnabrück\nVon mir angebotene Vorlesungen werden an der Hochschule Osnabrück an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) in ILIAS verwaltet. Alle notwendigen Informationen und Materialien sind auf ILIAS unter https://lms.hs-osnabrueck.de/ zu finden. Wenn du in dem Kurs nicht angemeldet bist, dann kontaktiere mich bitte per Mail. Auch die Kommunikation erfolgt von meiner Seite aus über ILIAS.\nAuf ILIAS findest du alle aktuellen Kursinformationen und erhälst auch die Mails, wenn Änderungen im Kursablauf stattfinden.\nWenn du nicht in der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) studierst oder aber in einem Studiengang, der meine Module nicht anbietet, steht es dir natürlich frei, sich in meine Vorlesungen zu setzten. Du findest in Tabelle 1.2 eine Übersicht der angebotenen Module und auch die inhaltliche Ordnung nach Lernstufe. Bitte informiere dich in deinem Studierendensekretariat über die Modalitäten zur Prüfungsteilnahme.\n\n\n\n\n\n\nAktuelle Entwürfe der Modulbeschreibungen\n\n\n\nDu findest die aktuellen Entwürfe der Modulbeschreibungen im Kapitel E. Im Rahmen der Weiterentwicklung der Studiengänge arbeite ich immer parallel zu den offiziellen Modulbeschreibungen an den Inhalten der Vorlesungen weiter. Dieses Entwickeln findet Niederschlag in den Entwürfe der Modulbeschreibungen.\nEine inhaltliche Übersicht und die Planung des Vorlesungsverlaufs findest du auf dem Google Spreadsheet zur inhaltlichen Planung.\n\n\n\n\n\nTabelle 1.2— Angebotene Statistik Module an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Die Stufe gibt das Lernniveau an.\n\nStufe\nLandwirtschaft; Angewandte Pflanzenbiologie – Gartenbau, Pflanzentechnologie\nWirtschafts- ingenieurwesen Agrar / Lebensmittel\nBioverfahrenstechnik in Agrar- und Lebensmittelwirtschaft\nAngewandte Nutztier- und Pflanzenwissenschaften\n\n\n\n1\nMathematik und Statistik\nStatistik\nAngewandte Statistik für Bioverfahrenstechnik\n\n\n\n2\nAngewandte Statistik und Versuchswesen\nAngewandte Statistik und Versuchswesen\n\n\n\n\n3\nSpezielle Statistik und Versuchswesen\n\n\nBiostatistik"
  },
  {
    "objectID": "organisation.html#sec-bachelorarbeit",
    "href": "organisation.html#sec-bachelorarbeit",
    "title": "\n1  Organisation\n",
    "section": "\n1.4 Bachelorarbeit",
    "text": "1.4 Bachelorarbeit\nHier findest sich eine aktuelle Struktursammlung für die Bachelorarbeit. Hier findest du keine Themen. Dafür musst du mich bitte ansprechen oder eine E-Mail schreiben. Die Themen finden sich dann etwa mit Kooperationspartern oder aber eher methodisch ohne echte Daten. Das müssen wir dann aber Absprechen.\nBitte halte Rücksprache, wenn dir Teile der Regeln für die Bachelorarbeit unklar sind.\n\nIch empfehle die Arbeit in engischer Sprache zu verfassen.\nDie Bachelorarbeit umfasst einen Zeitraum von 12 Wochen. Bei Unsicherheit über das Thema kann einmalig eine 4-wöchige Einarbeitungsphase vereinbart werden. Danach wird das Thema der Bachelorarbeit konkretisiert.\nDer Umfang sollte die 30 Seiten nicht überschreiten.\nDie Arbeit umfasst ca. 30 Referenzen, davon sind die meisten aktuelleren Datums. Internetseiten zählen ausdrücklich nicht als Referenz.\nEin Bewertungsbogen für die Bachelorarbeit steht zu Beginn der Arbeit zu Verfügung und kann jederzeit eingesehen werden.\nIm Rahmen der Betreuung finden mindestens jede Wochen ein kurzes Zoom-Treffen statt in dem der aktuelle Fortschritt der Arbeit besprochen wird.\nIn der 4-ten Woche wird eine kurze Präsentation der bisherigen erarbeiteten Inhalte gegeben. Diese Präsentation kann in der 8-ten Woche erneut erfolgen.\nMit einem methodischen Thema wird die Arbeit in Quarto in R oder in LaTeX in Overleaf geschreiben.\n\nIm Folgenden siehst du nochmal den groben zeitlichen Ablauf in Abbildung 1.1. Der Ablauf dient der groben Orientierung, damit du auch weißt, wo du etwa stehst. Nach vier Wochen solltest du gut 3500 Wörter geschrieben haben und nach 8 Wochen ca. 7000 Worte. Damit solltest du dann am Ende auf die 30 Seiten mit ca. 10500 Worten kommen.\n\n\nAbbildung 1.1— Grober zeitlicher Ablauf einer Bachelorarbeit. Die Präsentation umfasst ca. 10 Slides und folgt ebenfalls dem IMRAD Schema. Ich rechne mit 350 Worten pro deutscher Standardseite.\n\n\nBitte bachte auch die Hilfestellungen und die Erfahrungsberichte in dem Kapitel C, wo ich nochmal über Writing principles etwas aufgeschrieben habe. Vielleicht hilft dir das dann auch.\n\n\n\n\n\n\nKorrekte Schreibweise von einer Formel\n\n\n\n\n\n\\[\ny \\sim x_1 + x_2\n\\]\nmit\n\n\n\\(y\\) gleich dem gemessenen Frischgewicht in [kg/ha],\n\n\\(x_1\\) als der kontinuierliche Einflussvariable 1,\n\n\\(x_2\\) als der Einflussvariable 2 als Faktor mit den Leveln \\(a\\), \\(b\\) und \\(c\\).\n\n\n\n\n\n\n\n\n\n\nKorrekte Beschriftung und Referenzierung einer Abbildung\n\n\n\n\n\n\n\n\n\nAbbildung 1.2— Boxplots der Sprungweiten in [cm] getrennt für Hunde- und Katzenflöhe.\n\n\n\n\nIn Abbildung 1.2 sind die Sprungweiten in [cm] von Hunde- und Katzenflöhen als Boxplots dargestellt.\n\n\n\n\n\n\n\n\n\nKorrekte Beschriftung und Referenzierung einer Tabelle\n\n\n\n\n\n\n\n\n\nTabelle 1.3— Tabelle der Sprunglängen in [cm] von Hunde- und Katzenflöhen. Es wurden sieben Hundeflöhe und sieben Katzenflöhe gemessen (\\(n= 14\\)).\n\nanimal\njump_length\n\n\n\ndog\n5.7\n\n\ndog\n8.9\n\n\ndog\n11.8\n\n\ndog\n8.2\n\n\ndog\n5.6\n\n\ndog\n9.1\n\n\ndog\n7.6\n\n\ncat\n3.2\n\n\ncat\n2.2\n\n\ncat\n5.4\n\n\ncat\n4.1\n\n\ncat\n4.3\n\n\ncat\n7.9\n\n\ncat\n6.1\n\n\n\n\n\n\nIn Tabelle 1.3 sind die Sprunglängen in [cm] in der Spalte jump_length von Hunde- und Katzenflöhen in der Spalte animal dargestellt. Insgesamt wurden \\(n = 14\\) Flöhe gemessen davon sieben Hundeflöhe und sieben Katzenflöhe. Wir haben ein balanciertes Design vorliegen."
  },
  {
    "objectID": "stat-tests-basic.html",
    "href": "stat-tests-basic.html",
    "title": "19  Die Testentscheidung",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:47:58\nDu erfährst im diesem Kapitel mehr zur statistischen Testentscheidung und welche Konzepte wir beim statistischen Testen nutzen:"
  },
  {
    "objectID": "stat-tests-basic.html#sec-hypothesen",
    "href": "stat-tests-basic.html#sec-hypothesen",
    "title": "19  Die Testentscheidung",
    "section": "\n19.1 Die Hypothesen",
    "text": "19.1 Die Hypothesen\nWir können auf allen Daten einen statistischen Test rechnen und erhalten statistische Maßzahlen wie eine Teststatistik oder einen p-Wert. Nur leider können wir mit diesen statistischen Maßzahlen nicht viel anfangen ohne die Hypothesen zu kennen. Jeder statistische Test testet eine Nullhypothese. Ob diese Hypothese dem Anwender nun bekannt ist oder nicht, ein statistischer Test testet eine Nullhypothese. Daher müssen wir uns immer klar sein, was die entsprechende Nullhypothese zu unserer Fragestellung ist. Wenn du hier stockst, ist das ganz normal. Eine Fragestellung mit einer statistischen Hypothese zu verbinden ist nicht immer so einfach gemacht.\n\n\n\n\n\n\nDie Nullhypothese \\(H_0\\) und die Alternativhypothese \\(H_A\\)\n\n\n\nDie Nullhypothese \\(H_0\\) nennen wir auch die Null oder Gleichheitshypothese. Die Nullhypothese sagt aus, dass zwei Gruppen gleich sind oder aber kein Effekt zu beobachten ist.\n\\[\nH_0: \\bar{y}_{1} = \\bar{y}_{2}\n\\]\nDie Alternativhypothese \\(H_A\\) oder \\(H_1\\) auch Alternative genannt nennen wir auch Unterschiedshypothese. Die Alternativhypothese besagt, dass ein Unterschied vorliegt oder aber ein Effekt vorhanden ist.\n\\[\nH_A: \\bar{y}_{1} \\neq \\bar{y}_{2}\n\\]\n\n\nAls Veranschaulichung nehmen wir das Beispiel aus der unterschiedlichen Sprungweiten in [cm] für Hunde- und Katzenflöhe. Wir formulieren als erstes die Fragestellung. Eine Fragestellung endet mit einem Fragezeichen.\nLiegt ein Unterschied zwischen den Sprungweiten von Hunde- und Katzenflöhen vor?\nWir können die Frage auch anders formulieren.\nSpringen Hunde- und Katzenflöhe unterschiedlich weit?\nWichtig ist, dass wir eine Fragestellung formulieren. Wir können auch mehrere Fragen an einen Datensatz haben. Das ist auch vollkommen normal. Nur hat jede Fragestellung ein eigenes Hypothesenpaar. Wir bleiben aber bei dem simplen Beispiel mit den Sprungweiten von Hunde- und Katzenflöhen.\nEine statistische Hypothese ist eine Aussage über einen Parameter einer Population.\nWie sieht nun die statistische Hypothese in diesem Beispiel aus? Wir wollen uns die Sprungweite in [cm] anschauen und entscheiden, ob die Sprungweite für Hunde- und Katzenflöhen sich unterscheidet. Eine statistische Hypothese ist eine Aussage über einen Parameter einer Population. Wir entscheiden jetzt, dass wir die mittlere Sprungweite der Hundeflöhe \\(\\bar{y}_{dog}\\) mit der mittleren Sprungweite der Katzenflöhe \\(\\bar{y}_{cat}\\) vergleichen wollen. Es ergibt sich daher folgendes Hypothesenpaar.\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{dog} &= \\bar{y}_{cat} \\\\  \nH_A: \\bar{y}_{dog} &\\neq \\bar{y}_{cat} \\\\   \n\\end{aligned}\n\\]\nDas Falisifkationsprinzip - wir können nur Ablehnen - kommt hier zusammen mit der frequentistischen Statistik in der wir nur eine Wahrscheinlichkeitsaussage über das Auftreten der Daten \\(D\\) - unter der Annahme \\(H_0\\) gilt - treffen können.\nEs ist wichtig sich in Erinnerung zu rufen, dass wir nur und ausschließlich Aussagen über die Nullhypothese treffen werden. Das frequentistische Hypothesentesten kann nichts anders. Wir kriegen keine Aussage über die Alternativhypothese sondern nur eine Abschätzung der Wahrscheinlichkeit des Auftretens der Daten im durchgeführten Experiment, wenn die Nullhypothese wahr wäre. Wenn die Nullhypothese war ist, dann liegt kein Effekt oder Unterschied vor."
  },
  {
    "objectID": "stat-tests-basic.html#die-testentscheidung",
    "href": "stat-tests-basic.html#die-testentscheidung",
    "title": "19  Die Testentscheidung",
    "section": "\n19.2 Die Testentscheidung…",
    "text": "19.2 Die Testentscheidung…\nIn den folgenden Kapiteln werden wir verschiedene statistische Tests kennenlernen. Alle statistischen Tests haben gemein, dass ein Test eine Teststatistik \\(T_{calc}\\) berechnet. Darüber hinaus liefert jeder Test auch einen p-Wert (eng. p-value). Manche statistischen Test geben auch ein 95% Konfidenzintervall wieder. Eine Testentscheidung gegen die Nullhypothese \\(H_0\\) kann mit jedem der drei statistischen Maßzahlen - Teststatistik, $p$-Wert und Konfidenzintervall - durchgeführt werden. Die Regel für die Entscheidung, ob die Nullhypothese \\(H_0\\) abgelehnt werden kann, ist nur jeweils anders. In Tabelle 19.1 sind die Entscheidungsregeln einmal zusammengefasst.\n\n\n\nTabelle 19.1— Zusammenfassung der statistischen Testentscheidung unter der Nutzung der Teststatistik, dem p-Wert und dem 95% Konfidenzintervall. Die Entscheidung nach der Teststatistik ist veraltet und dient nur dem konzeptionellen Verständnisses. In der Forschung angewandt wird der \\(p\\)-Wert und das 95% Konfidenzintervall. Im Fall des 95% Konfidenzintervalls müssen wir noch unterschieden, ob wir einen Mittelwertsunterschied \\(\\Delta_{A-B}\\) oder aber einen Anteilsunterschied \\(\\Delta_{A/B}\\) betrachten.\n\n\n\n\n\n\n\n\nTeststatistik\np-Wert\n95% Konfidenzintervall\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(\\boldsymbol{KI_{1-\\alpha}}\\)\n\n\nH\\(_0\\) ablehnen\n\\(T_{calc} \\geq T_{\\alpha = 5\\%}\\)\n\\(Pr(\\geq T_{calc}| H_0) \\leq \\alpha\\)\n\n\\(\\Delta_{A-B}\\): enthält nicht 0\n\n\n\nH\\(_0\\) ablehnen\n\n\n\n\\(\\Delta_{A/B}\\): enthält nicht 1\n\n\n\n\n\n\nWir wollen in den folgenden Abschnitten die jeweiligen Entscheidungsregeln eines statistisches Tests einmal durchgehen.\n\nDie Testentscheidung gegen die Nullhypothese anhand der Teststatistik in Kapitel 19.2.1\n\nDie Testentscheidung gegen die Nullhypothese anhand dem p-Wert in Kapitel 19.2.2\n\nDie Testentscheidung gegen die Nullhypothese anhand des 95% Konfidenzintervall in Kapitel 19.2.3\n\n\n\n\n\n\n\n\nStreng genommen gilt die Regel \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) nur für eine Auswahl an statistischen Tests siehe dazu auch Kapitel 19.2.1. Bei manchen statistischen Tests ist die Entscheidung gedreht. Hier lassen wir das aber mal so stehen…\n\n19.2.1 … anhand der Teststatistik\n\n\n\n\n\n\nPrinzip des statistischen Testens I - Die Teststatistik\n\n\n\nDu findest auf YouTube Prinzip des statistischen Testens I - Die Teststatistik als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nWir wollen uns dem frequentistischen Hypothesentesten über die Idee der Teststatistik annähern. Im folgenden sehen wir die Formel für den t-Test. Den t-Test werden wir im Kapitel 22 uns nochmal detaillierter anschauen. Hier nutzen wir die vereinfachte Formel um das Konzept der Teststatistik \\(T\\) zu verstehen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_2}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nmit\n\n\n\\(\\bar{y}_1\\) dem Mittelwert für die erste Gruppe.\n\n\\(\\bar{y}_2\\) dem Mittelwert für die zweite Gruppe.\n\n\\(s_{p}\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{s_A + s_B}{2}\\).\n\n\\(n_g\\) der Gruppengröße der gruppen. Wir nehmen an beide Gruppen sind gleich groß.\n\nWir benötigen also zwei Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) und deren gepoolte Standardabweichung \\(s_p\\) sowie die Anzahl der Beobachtungen je Gruppe \\(n_g\\). Wenden wir die Formel des t-Tests einmal auf den folgenden Beispieldatensatz zu den Sprunglängen in [cm] von jeweils vier Hunde- und Kätzenflohen an. In Tabelle 19.2 ist das Datenbeispiel gegeben.\n\n\n\n\nTabelle 19.2— Beispiel für die Berechnung von einem Mittelwertseffekt an der Sprunglänge [cm] von Hunde und Katzenflöhen.\n\nanimal\njump_length\n\n\n\ncat\n8.5\n\n\ncat\n9.9\n\n\ncat\n8.9\n\n\ncat\n9.4\n\n\ndog\n8.0\n\n\ndog\n7.2\n\n\ndog\n8.4\n\n\ndog\n7.5\n\n\n\n\n\n\nNun berechnen wir die Mittelwerte und die Standardabweichungen aus der obigen Datentabelle für die Sprungweiten getrennt für die Hunde- und Katzenflöhe. Die Werte setzen wir dann in die Formel ein und berechnen die Teststatistik \\(T_{calc}\\).\n\\[\nT_{calc}=\\cfrac{9.18 - 7.78}{\\cfrac{(0.61 + 0.53)}{2} \\cdot \\sqrt{2/4}} = 3.47\n\\]\nmit\n\n\n\\(\\bar{y}_{cat} = 9.18\\) dem Mittelwert für die Gruppe cat.\n\n\\(\\bar{y}_{dog} = 7.78\\) dem Mittelwert für die Gruppe dog.\n\n\\(s_p = 0.57\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.61 + 0.53}{2}\\).\n\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nWir haben nun die Teststatistik \\(T_{calc} = 3.47\\) berechnet. In der ganzen Rechnererei verliert man manchmal den Überblick. Erinnern wir uns, was wir eigentlich wollten. Die Frage war, ob sich die mittleren Sprungweiten der Hunde- und Katzenflöhe unterschieden. Wenn die \\(H_0\\) wahr wäre, dann wäre der Unterschied \\(\\Delta\\) der beiden Mittelwerte der Hunde- und Katzenflöhe gleich null. Oder nochmal in der Analogie der t-Test Formel, dann wäre im Zähler \\(\\Delta = \\bar{y}_{cat} - \\bar{y}_{dog} = 0\\). Wenn die Mittelwerte der Sprungweite [cm] der Hunde- und Katzenflöhe gleich wäre, dann wäre die berechnete Teststatistik \\(T_{calc} = 0\\), da im Zähler Null stehen würde. Die Differenz von zwei gleichen Zahlen ist Null.\nJe größer die berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die beiden Mittelwerte per Zufall gleich sind. Wie groß muss nun die berechnete Teststatistik \\(T_{calc}\\) werden damit wir die Nullhypothese ablehnen können?\n\n\n\nAbbildung 19.1— Die t-Verteilung aller möglichen \\(T_{calc}\\) wenn die Nullhypothese wahr ist. Der Mittelwert der t-Verteilung ist \\(T=0\\). Wenn wir keinen Effekt erwarten würden dann wären die beiden Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) gleich groß. Die Differenz wäre 0. Je größer der \\(T_{calc}\\) wird desto weniger können wir davon ausgehen, dass die beiden Mittelwerte gleich sind. Liegt der \\(T_{calc}\\) über dem kritischen Wert von \\(T_{\\alpha = 5\\%}\\) dann wir die Nullhypothese abgelehnt.\n\n\n\nIn Abbildung 19.1 ist die Verteilung aller möglichen \\(T_{calc}\\) Werte unter der Annahme, dass die Nullhypothese wahr ist, dargestellt. Wir sehen, dass die t-Verteilung den Gipfel bei \\(T_{calc} = 0\\) hat und niedrigere Werte mit steigenden Werten der Teststatistik annimmt. Wenn \\(T = 0\\) ist, dann sind auch die Mittelwerte gleich. Je größer unsere berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die Nullhypothese gilt.\nDie t-Verteilug ist so gebaut, dass die Fläche \\(A\\) unter der Kurve gleich \\(A=1\\) ist. Wir können nun den kritschen Wert \\(T_{\\alpha = 5\\%}\\) berechnen an dem rechts von dem Wert eine Fläche von 0.05 oder 5% liegt. Somit liegt dann links von dem kritischen Wert die Fläche von 0.95 oder 95%. Den kritischen Wert \\(T_{\\alpha = 5\\%}\\) können wir statistischen Tabellen entnehmen. Oder wir berechnen den kritischen Wert direkt in R mit \\(T_{\\alpha = 5\\%} = 2.78\\).\nKommen wir zurück zu unserem Beispiel. Wir haben in unserem Datenbeispiel für den Vergleich von der Sprungweite in [cm] von Hunde- und Katzenflöhen eine Teststatistik von \\(T_{calc} = 3.47\\) berechnet. Der kritische Wert um die Nullhypothese abzulehnen liegt bei \\(T_{\\alpha = 5\\%} = 2.78\\). Wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. In unserem Fall ist \\(3.47 \\geq 2.78\\). Wir können die Nullhypothese ablehnen. Es gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen.\n\n\n\n\n\n\nWoher kommt die Testverteilung von \\(T\\), wenn \\(H_0\\) gilt?\n\n\n\n\n\nIn diesem Exkurs wollen wir einmal überlegen, woher die Testverteilung \\(T\\) herkommt, wenn die \\(H_0\\) gilt. Wir wollen die Verteilung der Teststatistik einmal in R herleiten. Zuerst gehen wir davon aus, dass die Mittelwerte der Sprungweite der Hunde- und Katzenflöhe gleich sind \\(\\bar{y}_{cat} = \\bar{y}_{dog} = (9.18 + 7.78)/2 = 8.48\\). Daher nehmen wir an, dass die Mittelwerte aus der gleichen Normalverteilung kommen. Wir ziehen also vier Sprungweiten jeweils für die Hunde- und Katzenflöhe aus einer Normalverteilung mit \\(\\mathcal{N}(8.48, 0.57)\\). Wir nutzen dafür die Funktion rnorm(). Anschließend berechnen wir die Teststatistik. Diesen Schritt wiederholen wir eintausend Mal.\n\nset.seed(20201021)\nT_vec <- map_dbl(1:1000, function(...){\n  dog_vec <- rnorm(n = 4, mean = 8.48, sd = 0.57)\n  cat_vec <- rnorm(n = 4, mean = 8.48, sd = 0.57)\n  s_p <- (sd(cat_vec) + sd(dog_vec))/2 \n  T_calc <- (mean(cat_vec) - mean(dog_vec))/(s_p * sqrt(2/4)) \n  return(T_calc)  \n}) %>% round(2)\n\nNachdem wir eintausend Mal die Teststatistik unter der \\(H_0\\) berechnet haben, schauen wir uns die sortierten ersten 100 Werte der Teststatistik einmal an. Wir sehen, dass extrem kleine Teststatistiken bis sehr große Teststatistiken zufällig auftreten können, auch wenn die Mittelwerte für das Ziehen der Zahlen gleich waren.\n\nT_vec %>% magrittr::extract(1:100) %>% sort()  \n\n  [1] -5.19 -3.48 -3.29 -2.65 -2.40 -2.10 -1.48 -1.35 -1.30 -1.29 -1.29 -1.27\n [13] -1.24 -1.22 -1.10 -1.03 -1.02 -1.02 -0.91 -0.87 -0.84 -0.79 -0.79 -0.76\n [25] -0.76 -0.76 -0.73 -0.66 -0.63 -0.63 -0.62 -0.61 -0.57 -0.56 -0.55 -0.52\n [37] -0.52 -0.50 -0.48 -0.48 -0.43 -0.35 -0.33 -0.32 -0.26 -0.26 -0.22 -0.21\n [49] -0.20 -0.18 -0.17 -0.17 -0.14 -0.14 -0.12 -0.12 -0.10 -0.06  0.04  0.10\n [61]  0.14  0.16  0.17  0.31  0.34  0.41  0.45  0.50  0.50  0.51  0.55  0.63\n [73]  0.63  0.68  0.73  0.73  0.77  0.89  0.92  0.95  0.99  1.07  1.07  1.09\n [85]  1.12  1.16  1.22  1.33  1.33  1.76  2.11  2.16  2.51  2.79  2.87  3.24\n [97]  3.48  3.56  3.60  6.56\n\n\nUnsere berechnete Teststatistik war \\(T_{calc} = 3.47\\). Wenn wir diese Zahl mit den ersten einhundert, sortierten Teststatistiken vergleichen, dann sehen wir, dass nur 4 von 100 Zahlen größer sind als unsere berechnete Teststatistik. Wir beobachten also sehr seltene Daten wie in Tabelle 19.2, wenn wir davon ausgehen, dass kein Unterschied zwischen der Sprungweite der Hunde- und Katzenflöhe vorliegt.\nIn Abbildung 19.2 sehen wir die Verteilung der berechneten eintausend Verteilungen nochmal als ein Histogramm dargestellt. Wiederum sehen wir, dass unsere berechnete Teststatistik - dargestellt als rote Linie - sehr weit rechts am Rand der Verteilung liegt.\n\nggplot(as_tibble(T_vec), aes(x = value)) +\n  theme_bw() +\n  labs(x = \"Teststatistik\", y = \"Anzahl\") +\n  geom_histogram() +\n  geom_vline(xintercept = 3.47, color = \"red\")\n\n\n\nAbbildung 19.2— Histogramm der 1000 gerechneten Teststaistiken \\(T_{calc}\\), wenn die \\(H_0\\) war wäre und somit kein Unterschied zwischen den Mittelwerten der Sprungweiten der Hunde- und Katzenflöhe vorliegen würde.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEs gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen. Die Aussage ist statistisch falsch. Wir können im frequentistischen Hypothesentesten keine Aussage über die \\(H_A\\) treffen. Im Sinne der Anwendbarkeit soll es hier so stehen bleiben.\nNun ist es leider so, dass jeder statistische Test seine eigene Teststatistik \\(T\\) hat. Daher ist es etwas mühselig sich immer neue und andere kritische Werte für jeden Test zu merken. Es hat sich daher eingebürgert, sich nicht die Teststatistik für die Testentscheidung gegen die Nullhypothese zu nutzen sondern den \\(p\\)-Wert. Den \\(p\\)-Wert wollen wir uns in dem folgenden Abschnitt anschauen.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik\n\n\n\nBei der Entscheidung mit der Teststatistik müssen wir zwei Fälle unterschieden.\n\nBei einem t-Test und einem \\(\\mathcal{X}^2\\)-Test gilt, wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nBei einem Wilcoxon-Mann-Whitney-Test gilt, wenn \\(T_{calc} < T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\n\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.\n\n\n\n19.2.2 … anhand dem p-Wert\n\n\n\n\n\n\nPrinzip des statistischen Testens II - Der p-Wert\n\n\n\nDu findest auf YouTube Prinzip des statistischen Testens II - Der p-Wert als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nIn dem vorherigen Abschnitt haben wir gelernt, wie wir zu einer Entscheidung gegen die Nullhypothese anhand der Teststatistik kommen. Wir haben einen kritischen Wert \\(T_{\\alpha = 5\\%}\\) definiert bei dem rechts von dem Wert 5% der Werte liegen. Anstatt nun den berechneten Wert \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) zu vergleichen, vergleichen wir jetzt die Flächen rechts von den jeweiligen Werten. Wir machen es uns an dieser Stelle etwas einfacher, denn wir nutzen immer den absoluten Wert der Teststatistik.\nWir schreiben \\(\\boldsymbol{Pr}\\) und meinen damit eine Wahrscheinlichkeit (eng. probability). Häufig wird auch nur das \\(P\\) verwendet, aber dann kommen wir wieder mit anderen Konzepten in die Quere.\nIn Abbildung 19.1 sind die Flächen auch eingetragen. Da die gesamte Fläche unter der t-Verteilung mit \\(A = 1\\) ist, können wir die Flächen auch als Wahrscheinlichkeiten lesen. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert genannt. Die gesamte Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%. Wir können also die Teststatistiken oder den p-Wert mit dem \\(\\alpha\\)-Niveau von 5% vergleichen.\n\n\nTabelle 19.3— Zusammenhang zwischen der Teststatistik \\(T\\) und der Fläche \\(A\\) rechts von der Teststatistik. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T|H_0)\\) oder \\(p\\)-Wert genannt. Die Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%.\n\nTeststatistik \\(T\\)\n\nFläche \\(A\\)\n\n\n\n\n\\(T_{calc}\\)\n\n\\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert\n\n\n\\(T_{\\alpha = 5\\%}\\)\n\\(\\alpha\\)\n\n\n\n\nIn der folgenden Abbildung 19.3 ist dann nochmal der Zusammenhang aus der Tabelle als eine Abbildung visualisiert. Mit dem \\(p\\)-Wert entscheiden wir anhand von Flächen. Wir schauen uns in diesem Fall die beiden Seiten der Testverteilung mit jeweils \\(T_{\\alpha = 2.5\\%}\\) für \\(-T_K\\) und \\(T_K\\) an und vergleichen die Flächen rechts neben der berechneten Teststatistik \\(T_{calc}\\).\n\n\n\nAbbildung 19.3— Die Flächen links und rechts von \\(T_{\\alpha = 2.5\\%}\\) nochmal separat dargestellt. Wir vergleichen bei der Entscheidung mit dem \\(p\\)-Wert nicht die berechnete Teststatistik \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) sondern die Flächen rechts von den jeweiligen Teststatistiken mit \\(A_K = 5\\%\\) und \\(A_{calc}\\) als den \\(p\\)-Wert. An dem Flächenvergleich machen wir dann die Testentscheidung fest.\n\n\n\nDer p-Wert oder \\(Pr(T|H_0)\\) ist eine Wahrscheinlichkeit. Eine Wahrscheinlichkeit kann die Zahlen von 0 bis 1 annehmen. Dabei sind die Grenzen einfach zu definieren. Eine Wahrscheinlichkeit von \\(Pr(A) = 0\\) bedeutet, dass das Ereignis A nicht auftritt; eine Wahrscheinlichkeit von \\(Pr(A) = 1\\) bedeutet, dass das Ereignis A eintritt. Der Zahlenraum dazwischen stellt jeden von uns schon vor große Herausforderungen. Der Unterschied zwischen 40% und 60% für den Eintritt des Ereignisses A sind nicht so klar zu definieren, wie du auf den ersten Blick meinen magst.\nEin frequentistischer Hypothesentest beantwortet die Frage, mit welcher Wahrscheinlichkeit \\(Pr\\) die Teststatistik \\(T\\) aus dem Experiment mit den Daten \\(D\\) zu beobachten wären, wenn es keinen Effekt gäbe (\\(H_0\\) ist wahr).\nLikelihood heißt Plausibilität und Probability heißt Wahrscheinlichkeit.\nIn anderen Büchern liest man an dieser Stelle auch gerne etwas über die Likelihood, nicht so sehr in deutschen Büchern, schon aber in englischen Veröffentlichungen. Im Englischen gibt es die Begrifflichkeiten einer Likelihood und einer Probability. Meist wird beides ins Deutsche ungenau mit Wahrscheinlichkeit übersetzt oder wir nutzen einfach Likelihood. Was aber auch nicht so recht weiterhilft, wenn wir ein Wort mit dem gleichen Wort übersetzen. Es handelt sich hierbei aber um zwei unterschiedliche Konzepte. Deshalb Übersetzen wir Likelihood mit Plausibilität und Probability mit Wahrscheinlichkeit.\nIm Folgenden berechnen wir den \\(p\\)-Wert in R mit der Funktion t.test(). Mehr dazu im Kapitel 22, wo wir den t-Test und deren Anwendung im Detail besprechen. Hier fällt der \\(p\\)-Wert etwas aus den Himmel. Wir wollen aber nicht per Hand Flächen unter einer Kurve berechnen sondern nutzen für die Berechnung von \\(p\\)-Werten statistische Tests in R.\n\n\n# A tibble: 1 x 2\n  statistic p.value\n      <dbl>   <dbl>\n1      3.47  0.0133\n\n\nWir sagen, dass wir ein signifikantes Ergebnis haben, wenn der \\(p\\)-Wert kleiner ist als die Signifikanzschwelle \\(\\alpha\\) von 5%.\nWir erhalten einen \\(p\\)-Wert von 0.013 und vergleichen diesen Wert zu einem \\(\\alpha\\) von 5%. Ist der \\(p\\)-Wert kleiner als der \\(\\alpha\\)-Wert von 5%, dann können wir die Nullhypothese ablehnen. Da 0.013 kleiner ist als 0.05 können wir die Nullhypothese und damit die Gleichheit der mittleren Sprungweiten in [cm] ablehnen. Wir sagen, dass wir ein signifikantes Ergebnis vorliegen haben.\n\n\n\n\n\n\nEntscheidung mit dem p-Wert\n\n\n\nWenn der p-Wert \\(\\leq \\alpha\\) dann wird die Nullhypothese (H\\(_0\\)) abgelehnt. Das Signifikanzniveau \\(\\alpha\\) wird als Kulturkonstante auf 5% oder 0.05 gesetzt. Die Nullhypothese (H\\(_0\\)) kann auch Gleichheitshypothese gesehen werden. Wenn die H\\(_0\\) gilt, liegt kein Unterschied zwischen z.B. den Behandlungen vor.\n\n\n\n19.2.3 … anhand des 95% Konfidenzintervall\nEin statistischer Test der eine Teststatistik \\(T\\) berechnet liefert auch immer einen \\(p\\)-Wert. Nicht alle statistischen Tests ermöglichen es ein 95% Konfidenzintervall zu berechnen. Abbildung 19.4 zeigt ein 95% Konfidenzintervall.\n\n\nAbbildung 19.4— Ein 95% Konfidenzintervall. Der Punkt in der Mitte entspricht dem Unterschied oder Effekt \\(\\Delta\\).\n\n\nMit p-Werten haben wir Wahrscheinlichkeitsaussagen und damit über die Signifikanz. Damit haben wir noch keine Aussage über die Relevanz des beobachtenten Effekts.\nMit der Teststatistik \\(T\\) und dem damit verbundenen \\(p\\)-Wert haben wir uns Wahrscheinlichkeiten angeschaut und erhalten eine Wahrscheinlichkeitsaussage. Eine Wahrscheinlichkeitsaussage sagt aber nichts über den Effekt \\(\\Delta\\) aus. Also wie groß ist der mittlere Sprungunterschied zwischen Hunde- und Katzenflöhen.\nDie Idee von 95% Kondifenzintervallen ist es jetzt den Effekt mit der Wahrscheinlichkeitsaussage zusammenzubringen und beides in einer Visualisierung zu kombinieren. Im Folgenden sehen wir die vereinfachte Formel für das 95% Konfidenzintervall eines t-Tests.\n\\[\n\\left[\n(\\bar{y}_1-\\bar{y}_2) -\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}}; \\;\n(\\bar{y}_1-\\bar{y}_2) +\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}};\n\\right]\n\\]\nDie Formel ist ein wenig komplex, aber im Prinzip einfach, wenn du ein wenig die Formel auf dich wirken lässt. Der linke und der rechte Teil neben dem Semikolon sind fast gleich, bis auf das Plus- und Minuszeichen. Abbildung 19.5 visualisert die Formel einmal. Wir sehen Folgendes in der Formel und dann in der entsprechenden Abbildung:\n\n\n\\((\\bar{y}_{1}-\\bar{y}_{2})\\) ist der Effekt \\(\\Delta\\). In diesem Fall der Mittelwertsunterschied. Wir finden den Effekt als Punkt in der Mitte des Intervals.\n\n\\(T_{\\alpha = 5\\%} \\cdot \\frac {s}{\\sqrt{n}}\\) ist der Wert, der die Arme des Intervals bildet. Wir vereinfachen die Formel mit \\(s_p\\) für die gepoolte Standardabweichung und \\(n_g\\) für die Fallzahl der beiden Gruppen. Wir nehmen an das beide Gruppen die gleiche Fallzahl \\(n_1 = n_2\\) haben.\n\n\n\nAbbildung 19.5— Zusammenhang zwischen der vereinfachten Formel für das 95% Konfidenzintervall und der Visualisierung des 95% Konfidenzintervalls. Der Effektschätzer wird als Punkt in der Mitte des Intervalls dargestellt. Der Effektschäter \\(\\Delta\\) kann entweder ein Mittelwertsunterschied sein oder ein Anteilsunterschied. Bei einem Mittelwertsunterschied kann die Nullhypothese abgelehnt werden, wenn die 0 nicht im Konfidenzintervall ist; bei einem Anteilsunterschied wenn die 1 nicht im Konfidenzintervall ist. Die Arme werden länger oder kürzer je nachdem wie sich die statistischen Maßzahlen \\(s\\) und \\(n\\) verändern.\n\n\nDie Funktion factor() in R erlaubt es dir die Level eines Faktors zu sortieren und so festzulegen ob Level cat minus Level dog oder umgekehrt von R gerechnet wird.\nWir können eine biologische Relevanz definieren, dadurch das ein 95% Konfidenzintervall die Wahrscheinlichkeitsaussage über die Signifkanz, daher ob die Nullhypothese abgelehnt werden kann, mit dem Effekt zusammenbringt. Wo die Signifikanzschwelle klar definiert ist, hängt die Relevanzschwelle von der wissenschaftlichen Fragestellung und weiteren externen Faktoren ab. Die Signifikanzschwelle liegt bei 0, wenn wir Mittelwerte miteinander vergleichen und bei 1, wenn wir Anteile vergleichen. Abbildung 19.6 zeigt fünf 95% Konfidenzintervalle (a-e), die sich anhand der Signifikanz und Relevanz unterscheiden. Bei der Relevanz ist es wichtig zu wissen in welche Richtung der Effekt gehen soll. Erwarten wir einen positiven Effekt wenn wir die Differenz der beiden Gruppen bilden oder einen negativen Effekt?\n\n\nAbbildung 19.6— Verschiedene signifikante und relevante Konfidenzintervalle: (a) signifikant und nicht relevant; (b) nicht signifikant und nicht relevant; (c) signifikant und relevant; (d) signifikant und nicht relevant, der Effekt ist zu klein; (e) signifikant und potenziell relevant, Effekt zeigt in eine unerwartete Richtung gegeben der Relevanzschwelle.\n\n\nWir wollen uns nun einmal anschauen, wie sich ein 95% Konfidenzintervall berechnet. Wir nehmen dafür die vereinfachte Formel und setzen die berechneten statistischen Maßzahlen ein. In der Anwendung werden wir die Konfidenzintervalle nicht selber berechnen. Wenn ein statistisches Verfahren konfidenzintervalle berechnen kann, dann liefert die entsprechende Funktion in R das Konfidenzintervall.\nEs ergibt sich Folgende ausgefüllte, vereinfachte Formel für das 95% Konfidenzintervalls eines t-Tests für das Beispiel des Sprungweitenunterschieds [cm] zwischen Hunde- und Katzenflöhen.\n\n\n\n\n\n\nWir nutzen hier eine vereinfachte Formel für das Konfidenzintervall um das Konzept zu verstehen. Später berechnen wir das Konfidenzintervall in R.\n\\[\n\\left[\n(9.18-7.78) -\n2.78 \\cdot \\frac {0.57}{\\sqrt{4}}; \\;\n(9.18-7.78) +\n2.78 \\cdot \\frac {0.57}{\\sqrt{4}};\n\\right]\n\\]\nmit\n\n\n\\(\\bar{y}_{cat} = 9.18\\) dem Mittelwert für die Gruppe cat.\n\n\\(\\bar{y}_{dog} = 7.78\\) dem Mittelwert für die Gruppe dog.\n\n\\(T_{\\alpha = 5\\%} = 2.78\\) dem kritischen Wert.\n\n\\(s_p = 0.57\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.61 + 0.53}{2}\\).\n\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nLösen wir die Formel auf, so ergibt sich folgendes 95% Konfidenzintervall des Mittelwertsunterschiedes der Hunde- und Katzenflöhe.\n\\[[0.64; 2.16]\\]\nWir können sagen, dass mit 95% Wahrscheinlichkeit das Konfidenzintervall den wahren Effektunterschied \\(\\Delta\\) überdeckt. Oder etwas mehr in Prosa, dass wir eine Sprungweitenunterschied von 0.64 cm bis 2.16 cm zwischen Hunde- und Katzenflöhen erwarten würden.\nDie Entscheidung gegen die Nullhypothese bei einem Mittelwertsunterschied erfolgt bei einem 95% Konfidenzintervall danach ob die Null mit im Konfidenzintervall liegt oder nicht. In dem Interval \\([0.64; 2.16]\\) ist die Null nicht enthalten, also können wir die Nullhypothese ablehnen. Es ist mit einem Unterschied zwischen den mittleren Sprungweiten von Hunde- und Katzenflöhen auszugehen.\nIn unserem Beispiel, könnten wir die Relevanzschwelle für den mittleren Sprungweitenunterschied zwischen Hund- und Katzenflöhen auf 2 cm setzen. In dem Fall würden wir entscheiden, dass der mittlere Sprungweitenunterschied nicht relevant ist, da die 2 cm im Konfidenzintervall enthalten sind. Was wäre wenn wir die Relevanzschwelle auf 4 cm setzen? Dann wäre zwar die Relevanzschwelle nicht mehr im Konfidenzintervall, aber wir hätten Fall (d) in der Abbildung 19.6 vorliegen. Der Effekt ist einfach zu klein, dass der Effekt relevant sein könnte.\nWir können dann die 95% Konfidenzintervall des Mittelwertsunterschiedes der Hunde- und Katzenflöhe auch nochmal richtig in R berechnen. Wir haben ja oben eine einfachere Formel für die gepoolte Standardabweichung genutzt. Wenn wir also ganz genau rechnen wollen, dann sind die 95% Konfidenzintervall wie folgt. Wir nutzen auch hier die Funktion t.test(). Mehr dazu im Kapitel 22, wo wir den t-Test und deren Anwendung im Detail besprechen.\n\n\n# A tibble: 1 x 2\n  conf.low conf.high\n     <dbl>     <dbl>\n1    0.412      2.39\n\n\n\n\n\n\n\n\nEntscheidung mit dem 95% Konfidenzintervall\n\n\n\nBei der Entscheidung mit dem 95% Konfidenzinterval müssen wir zwei Fälle unterscheiden.\n\nEntweder schauen wir uns einen Mittelwertsunterschied (\\(\\Delta_{y_1-y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 0 im 95% Konfidenzinterval ist.\nOder wir schauen uns einen Anteilsunterschied (\\(\\Delta_{y_1/y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 1 im 95% Konfidenzinterval ist."
  },
  {
    "objectID": "stat-tests-basic.html#sec-delta-n-s",
    "href": "stat-tests-basic.html#sec-delta-n-s",
    "title": "19  Die Testentscheidung",
    "section": "\n19.3 Auswirkung des Effektes, der Streuung und der Fallzahl",
    "text": "19.3 Auswirkung des Effektes, der Streuung und der Fallzahl\nWir wollen einmal den Zusammenhang zwischen dem Effekt \\(\\Delta\\), der Streuung als Standardabweichung \\(s\\) und Fallzahl \\(n\\) uns näher anschauen. Wir können die Formel des t-Tests wie folgt vereinfachen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_1}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nFür die Betrachtung der Zusammenhänge wandeln wir \\(\\sqrt{2/n_g}\\) in \\(1/n\\) um. Dadurch wandert die Fallzahl \\(n\\) in den Zähler. Die Standardabweichung verallgemeinern wir zu \\(s\\) und damit allgemein zur Streuung. Abschließend betrachten wir \\(\\bar{y}_A-\\bar{y}_B\\) als den Effekt \\(\\Delta\\). Es ergibt sich folgende vereinfachte Formel.\n\\[\nT_{calc} = \\cfrac{\\Delta \\cdot n}{s}\n\\]\nWir können uns nun die Frage stellen, wie ändert sich die Teststatistik \\(T_{calc}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\) in den Daten. Die Tabelle 19.4 zeigt die Zusammenhänge auf. Die Aussagen in der Tabelle lassen sich generalisieren. So bedeutet eine steigende Fallzahl meist mehr signifikante Ergebnisse. Eine stiegende Streuung reduziert die Signifikanz eines Vergleichs. Ein Ansteigen des Effektes führt zu mehr signifikanten Ergebnissen. Ebenso verschiebt eine Veränderung des Effekt das 95% Konfidenzintervall, eine Erhöhung der Streuung macht das 95% Konfidenzintervall breiter, eine sinkende Streuung macht das 95% Konfidenzintervall schmaler. bei der Fallzahl verhält es sich umgekehrt. Eine Erhöhung der Fallzahl macht das 95% Konfidenzintervall schmaler und eine sinkende Fallzahl das Konfidenzintervall breiter.\n\n\n\nTabelle 19.4— Zusammenhang von der Teststatistik \\(T_{calc}\\) und dem p-Wert \\(Pr(\\geq T_{calc}|H_0)\\) sowie dem \\(KI_{1-\\alpha}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\n\n\\(\\Delta \\uparrow\\)\nsteigt\nsinkt\nverschoben\n\\(\\Delta \\downarrow\\)\nsinkt\nsteigt\nverschoben\n\n\n\\(s \\uparrow\\)\nsinkt\nsteigt\nbreiter\n\\(s \\downarrow\\)\nsteigt\nsinkt\nschmaler\n\n\n\\(n \\uparrow\\)\nsteigt\nsinkt\nschmaler\n\\(n \\downarrow\\)\nsinkt\nsteigt\nbreiter"
  },
  {
    "objectID": "stat-tests-anova.html",
    "href": "stat-tests-anova.html",
    "title": "23  Die ANOVA",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:48:07\nDie ANOVA (eng. analysis of variance) ist wichtig. Was für ein schöner Satz um anzufangen. Wir brauchen die ANOVA aus mehreren Gründen. Die Hochzeiten der ANOVA sind eigentlich vorbei, wir haben in der Statistik für viele Fälle mittlerweile besser Werkzeuge, aber als Allrounder ist die ANOVA immer noch nutzbar.\nWofür brauchen wir die ANOVA?\nWir sehen also, dass die ANOVA zum einen alt ist, aber auch heute noch viel verwendet wird. Daher werden wir in diesem langem Kapitel uns einmal mit der ANOVA ausgiebig beschäftigen. Fangen wir also an, dieses großartige Schwerzeitaschenmesser der Statistik besser zu verstehen."
  },
  {
    "objectID": "stat-tests-anova.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-anova.html#genutzte-r-pakete-für-das-kapitel",
    "title": "23  Die ANOVA",
    "section": "\n23.1 Genutzte R Pakete für das Kapitel",
    "text": "23.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, effectsize, ggpubr)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac1",
    "href": "stat-tests-anova.html#sec-fac1",
    "title": "23  Die ANOVA",
    "section": "\n23.2 Einfaktorielle ANOVA",
    "text": "23.2 Einfaktorielle ANOVA\nDie einfaktorielle ANOVA ist die simpelste Form der ANOVA. Wir nutzen einen Faktor mit mehr als zwei Leveln. Im Rahmen der einfaktoriellen ANOVA wollen wir usn auch die ANOVA theoretisch einmal anschauen. Danach wie die einfaktorielle ANOVA in R genutzt wird. Ebenso wie wir die einfaktorielle ANOVA visualsieren. Abschließend müssen wir uns noch überlegen, ob es einen Effektschätzer für die einfaktorielle ANOVA gibt.\n\n\n\n\n\n\nDie einfaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität über den Behandlungsfaktor \\(x\\). Daher alle Level von \\(x\\) sollen die gleiche Varianz haben.\nUnsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x\\) homogen in den Varianzen sind. Später mehr dazu, wenn wir beides nicht vorliegen haben…\n\n23.2.1 Daten für die einfaktorielle ANOVA\nWir wollen uns nun erstmal den einfachsten Fall anschauen mit einem simplen Datensatz. Wir nehmen ein normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Hätten wir nur zwei Level, dann können wir auch einen t-Test rechnen können.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac1_tbl <- read_csv2(\"data/flea_dog_cat_fox.csv\") %>%\n  select(animal, jump_length) %>% \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt fac1_tbl mit dem Datensatz in Tabelle 23.1 nochmal dargestellt.\n\n\n\n\nTabelle 23.1— Selektierter Datensatz für die einfaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\n\n\n\ndog\n5.7\n\n\ndog\n8.9\n\n\ndog\n11.8\n\n\ndog\n8.2\n\n\ndog\n5.6\n\n\ndog\n9.1\n\n\ndog\n7.6\n\n\ncat\n3.2\n\n\ncat\n2.2\n\n\ncat\n5.4\n\n\ncat\n4.1\n\n\ncat\n4.3\n\n\ncat\n7.9\n\n\ncat\n6.1\n\n\nfox\n7.7\n\n\nfox\n8.1\n\n\nfox\n9.1\n\n\nfox\n9.7\n\n\nfox\n10.6\n\n\nfox\n8.6\n\n\nfox\n10.3\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt fac1_tbl folgendes Modell für später:\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der einfaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n23.2.2 Hypothesen für die einfaktorielle ANOVA\nDie ANOVA betrachtet die Mittelwerte und nutzt die Varianzen um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mittelwerte jedes Levels des Faktors animal gleich sind.\n\\[\nH_0: \\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Mittelwerten unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mittelwerte unterschieden oder eben nur ein Paar. Wenn eine ANOVA die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mittelwerte sich unterscheiden.\n\\[\n\\begin{aligned}\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nWir schauen uns jetzt einmal die ANOVA theoretisch an bevor wir uns mit der Anwendung der ANOVA in R beschäftigen.\n\n23.2.3 Einfaktoriellen ANOVA theoretisch\nKommen wir zurück zu den Daten in Tabelle 23.1. Wenn wir die ANOVA per Hand rechnen wollen, dann ist nicht das Long Format die beste Wahl sondern das Wide Format. Wir haben ein balanciertes Design vorliegen, dass heißt in jeder Level sind die gleiche Anzahl Beobachtungen. Wir schauen uns jeweils sieben Flöhe von jeder Tierart an. Für eine ANOVA ist aber ein balanciertes Design nicht notwendig, wir können auch mit ungleichen Gruppengrößen eine ANOVA rechnen.\nStatt einer einfaktoriellen ANOVA könnten wir auch gleich einen pairwise.t.test()rechnen. Historisch ebtrachtet ist die einfaktorielle ANOVA die Visualsierung des paarweisen t-Tests.\nEien einfaktorielle ANOVA macht eigentlich keinen großen Sinn, wenn wir anschließend sowieso paarweise Vergleich, wie in Kapitel 31 beschrieben, rechnen. Aus der Hisotrie stellte sich die Frage, ob es sich lohnt die ganze Arbeit für die paarweisen t-Tests per Hand zu rechnen. Daher wurde die ANOVA davorgeschaltet. War die ANOVA nicht signifikant, dann konnte man sich dann auch die Rechnerei für die paaweisen t-Tests sparen.\nIn Tabelle 23.2 sehen wir die Daten einmal als Wide Format dargestellt.\n\n\nTabelle 23.2— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal.\n\nj\ndog\ncat\nfox\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\n\nWir können jetzt für jedes de Level den Mittelwert über all \\(j=7\\) Beobachtungen berechnen.\n\\[\n\\begin{aligned}\n\\bar{y}_{dog} &= 8.13 \\\\\n\\bar{y}_{cat} &= 4.74 \\\\\n\\bar{y}_{fox} &= 9.16 \\\\\n\\end{aligned}\n\\]\nWir tuen jetzt für einen Moment so, als gebe es den Faktor animal nicht in den Daten und schauen uns die Verteilung der einzelnen Beobachtungen in Abbildung 23.1 (a) einmal an. Wir sehen das sich die Beobachtungen von ca. 2.2cm bis 11 cm streuen. Woher kommt nun diese Streuung bzw. Varianz? Was ist die Quelle der Varianz? In Abbildung 23.1 (b) haben wir die Punkte einmal nach dem Faktor animal eingefärbt. Wir sehen, dass die blauen Beobachtungen eher weitere Sprunglängen haben als die grünen Beobachtungen. Wir gruppieren die Beobachtungen in Abbildung 23.1 (c) nach dem Faktor animal und sehen, dass ein Teil der Varianz der Daten von dem Faktor animal ausgelöst wird.\n\n\n\n\n\n(a) Die Sprungweite in [cm] ohne den Faktor animal betrachtet.\n\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal eingefärbt.\n\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal eingefärbt und gruppiert.\n\n\n\n\nAbbildung 23.1— Die Spungweite in [cm] in Abhängigkeit von dem Faktor animal dargestellt.\n\n\nGehen wir einen Schritt weiter und zeichnen einmal das globale Mittel in die Abbildung 23.2 (a) von \\(\\bar{y}_{..} = 7.34\\) und lassen die Beobachtungen gruppiert nach dem Faktor animal. Wir sehen, dass die Level des Faktors animal um das globale Mittel streuen. Was ja auch bei einem Mittelwert zu erwarten ist. Wir können jetzt in Abbildung 23.2 (b) die lokalen Mittel für die einzelnen Level dog, catund fox ergänzen. Und abschließend in Abbildung 23.2 (c) die Abweichungen \\(\\\\beta_i\\) zwischen dem globalen Mittel \\(\\bar{y}_{..} = 7.34\\) und den einzelnen lokalen Mittel berechnen. Die Summe der Abweichungen \\(\\\\beta_i\\) ist \\(0.79 + (-2.6) + 1.81 \\approx 0\\). Das ist auch zu erwarten, den das globale Mittel muss ja per Definition als Mittelwert gleich großen Abstand “nach oben” wie “nach unten” haben.\n\n\n\n\n\n(a) Die Sprungweite in [cm] mit den Faktor animal gruppiert und das globale Mittel \\(\\bar{y}_{..} = 7.34\\) ergänzt.\n\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die lokalen Mittel \\(\\bar{y}_{i.}\\) für jedes Level ergänzt.\n\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die Abweichungen \\(\\beta_i\\) ergänzt.\n\n\n\n\nAbbildung 23.2— Dotplot der Spungweite in [cm] in Abhängigkeit von dem Faktor animal.\n\n\nWir tragen die Werte der lokalen Mittlwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\) noch in die Tabelle 23.3 ein. Wir sehen in diesem Beispiel warum das Wide Format besser ist, wenn wir die lokalen Mittelwerte und die Abweichungen per Hand berechnen. Da wir in der Anwendung aber nie die ANOVA per Hand rechnen, liegen unsere Daten immer in R als Long Format vor.\n\n\nTabelle 23.3— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal. Wir ergänzen die lokalen Mittlwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\).\n\nj\ndog\ncat\nfox\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\\(\\bar{y}_{i.}\\)\n\\(8.13\\)\n\\(4.74\\)\n\\(9.16\\)\n\n\n\\(\\beta_i\\)\n\\(-2.6\\)\n\\(0.79\\)\n\\(1.81\\)\n\n\n\n\nWie kriegen wir nun die ANOVA rechnerisch auf die Straße? Schauen wir uns dazu einmal die Abbildung 23.3 an. Auf der linken Seiten sehen wir vier Gruppen, die keinen Effekt haben. Die Gruppen liegen alle auf der gleichen Höhe. Es ist mit keinem Unterschied zwischen den Gruppen zu rechnen. Alle Gruppenmittel liegen auf dem globalen Mittel. Die Abweichungen der einzelnen Gruppenmittel zum globalen Mittel ist damit gleich null. Auf der rechten Seite sehen wir vier Gruppen mit einem Effekt. Die Gruppen unterscheiden sich in ihren Gruppenmitteln. Dadurch unterscheide sich aber auch die Gruppenmittel von dem globalen Mittel.\n\n\n\n\n\n(a) Kein Effekt\n\n\n\n\n\n\n(b) Leichter bis mittlerer Effekt\n\n\n\n\nAbbildung 23.3— Darstellung von keinem Effekt und leichtem bis mittleren Effekt in einer einfaktoriellen ANOVA mit einem Faktor mit vier Leveln A - D.\n\n\nWir können daher wie in Tabelle 23.4 geschrieben die Funktionsweise der ANOVA zusammenfassen.\n\n\nTabelle 23.4— Zusammenfassung der ANOVA Funktionsweise.\n\n\n\n\n\n\nAll level means are equal.\n=\nThe differences between level means and the total mean are small.\n\n\n\nNun kommen wir zum eigentlichen Schwenk und warum eigentlich die ANOVA meist etwas verwirrt. Wir wollen eine Aussage über die Mittelwerte machen. Die Nullhypothese lautet, dass alle Mittelwerte gleich sind. Wie wir in Tabelle 23.4 sagen, heißt alle Mittelwerte gleich auch, dass die Abweichungen von den Gruppenmitteln zum globalen Mittel klein ist.\nWie weit die Gruppenmittel von dem globalen Mittel weg sind, dazu nutzt die ANOVA die Varianz. Die ANOVA vergleicht somit\n\ndie Varianz der einzelnen Mittelwerte der (Gruppen)Level zum globalen Mittel (eng. variability between levels)\nund die Varianz der Beobachtungen zu den einzelnen Mittelwerten der Level (eng. variability within one level)\n\nDie sum of squares sind nichts anderes als die Varianz. Wir nennen das hier nur einmal anders…\nWir berechnen also wie die Beobachtungen jeweils um das globale Mittel streuen (\\(SS_{total}\\)), die einzelnen Beobachtungen um die einzelnen Gruppenmittel \\(SS_{error}\\) und die Streuung der Gruppenmittel um das globale Mittel (\\(SS_{animal}\\)). Wir nennen die Streuung Abstandquadrate (eng. sum of squares) und damit sind die Sum of Square \\((SS)\\) nichts anderes als die Varianz. Die Tabelle 23.5 zeigt die Berechnung des Anteils jeder einzlenen Beobachtung an den jeweiligen Sum of Squares.\n\n\n\nTabelle 23.5— Berechnung der \\(SS_{animal}\\), \\(SS_{error}\\) und \\(SS_{total}\\) anhand der einzelnen gemessenen Werte \\(y\\) für durch die jeweiligen Gruppenmittel \\(\\bar{y}_{i.}\\) und dem globalen Mittel \\(\\bar{y}_{..}\\) über alle Beobachtungen\n\n\n\n\n\n\n\n\n\nanimal (x)\njump_length (y)\n\\(\\boldsymbol{\\bar{y}_{i.}}\\)\nSS\\(_{\\boldsymbol{animal}}\\)\n\nSS\\(_{\\boldsymbol{error}}\\)\n\nSS\\(_{\\boldsymbol{total}}\\)\n\n\n\n\ndog\n\\(5.7\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.7 - 8.13)^2 = 5.90\\)\n\\((5.7 - 7.34)^2 = 2.69\\)\n\n\ndog\n\\(8.9\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.9 - 8.13)^2 = 0.59\\)\n\\((8.9 - 7.34)^2 = 2.43\\)\n\n\ndog\n\\(11.8\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((11.8 - 8.13)^2 = 13.47\\)\n\\((11.8 - 7.34)^2 = 19.89\\)\n\n\ndog\n\\(8.2\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.2 - 8.13)^2 = 0.00\\)\n\\((8.2 - 7.34)^2 = 0.74\\)\n\n\ndog\n\\(5.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.6 - 8.13)^2 = 6.40\\)\n\\((5.6 - 7.34)^2 = 3.03\\)\n\n\ndog\n\\(9.1\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((9.1 - 8.13)^2 = 0.94\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\ndog\n\\(7.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((7.6 - 8.13)^2 = 0.28\\)\n\\((7.6 - 7.34)^2 = 0.07\\)\n\n\ncat\n\\(3.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((3.2 - 4.74)^2 = 2.37\\)\n\\((3.2 - 7.34)^2 = 17.14\\)\n\n\ncat\n\\(2.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((2.2 - 4.74)^2 = 6.45\\)\n\\((2.2 - 7.34)^2 = 26.42\\)\n\n\ncat\n\\(5.4\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((5.4 - 4.74)^2 = 0.44\\)\n\\((5.4 - 7.34)^2 = 3.76\\)\n\n\ncat\n\\(4.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.1 - 4.74)^2 = 0.41\\)\n\\((4.1 - 7.34)^2 = 10.50\\)\n\n\ncat\n\\(4.3\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.3 - 4.74)^2 = 0.19\\)\n\\((4.3 - 7.34)^2 = 9.24\\)\n\n\ncat\n\\(7.9\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((7.9 - 4.74)^2 = 9.99\\)\n\\((7.9 - 7.34)^2 = 0.31\\)\n\n\ncat\n\\(6.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((6.1 - 4.74)^2 = 1.85\\)\n\\((6.1 - 7.34)^2 = 1.54\\)\n\n\nfox\n\\(7.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((7.7 - 9.16)^2 = 2.13\\)\n\\((7.7 - 7.34)^2 = 0.13\\)\n\n\nfox\n\\(8.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.1 - 9.16)^2 = 1.12\\)\n\\((8.1 - 7.34)^2 = 0.58\\)\n\n\nfox\n\\(9.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.1 - 9.16)^2 = 0.00\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\nfox\n\\(9.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.7 - 9.16)^2 = 0.29\\)\n\\((9.7 - 7.34)^2 = 5.57\\)\n\n\nfox\n\\(10.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.6 - 9.16)^2 = 2.07\\)\n\\((10.6 - 7.34)^2 = 10.63\\)\n\n\nfox\n\\(8.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.6 - 9.16)^2 = 0.31\\)\n\\((8.6 - 7.34)^2 = 1.59\\)\n\n\nfox\n\\(10.3\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.3 - 9.16)^2 = 1.30\\)\n\\((10.3 - 7.34)^2 = 8.76\\)\n\n\n\n\n\n\\(74.68\\)\n\\(56.53\\)\n\\(131.21\\)\n\n\n\n\n\nDie ANOVA wird deshalb auch Varianzzerlegung genannt, da die ANOVA versucht den Abstand der Beoabchtungen auf die Variablen im Modell zu zerlegen. Also wieviel der Streuung von den Beobachtungen kann von dem Faktor animal erklärt werden? Genau der Abstand von den Gruppenmitteln zu dem globalen Mittlelwert.\nDu kannst dir das ungefähr als eine Reise von globalen Mittelwert zu der einzelnen Beobachtung vorstellen. Nehmen wir als Beispiel die kleinste Sprungweite eines Katzenflöhes von 2.2 cm und visualisieren wir uns die Reise wie in Abbildung 23.4 zu sehen. Wie kommen wir jetzt numerisch vom globalen Mittel mit \\(7.34\\) zu der Beobachtung? Wir können zum einen den direkten Abstand mit \\(2.2 - 7.34\\) gleich \\(-5.14\\) cm berechnen. Das wäre der total Abstand. Wie sieht es nun aus, wenn wir das Gruppenmittel mit beachten? In dem Fall gehen wir vom globalen Mittel zum Gruppenmittel cat mit \\(\\bar{y}_{cat} - \\bar{y}_{..} = 4.74 -7.34\\) gleich \\(\\beta_{cat} = -2.6\\) cm. Jetzt sind wir aber noch nicht bei der Beobachtung. Wir haben noch einen Rest von \\(y_{cat,2} - \\bar{y}_{cat} = 2.2 - 4.74\\) gleich \\(\\epsilon_{cat, 2} = -2.54\\) cm, die wir noch zurücklegen müssen. Das heißt, wir können einen Teil der Strecke mit dem Gruppenmittelwert erklären. Oder anders herum, wir können die Strecke vom globalen Mittelwert zu der Beobachtung in einen Teil für das Gruppenmittel und einen unerklärten Rest zerlegen.\n\n\n\nAbbildung 23.4— Visualisierung der Varianzzerlegung des Weges vom globalen Mittel zu der einzelnen Beoabchtung. Um zu einer einzelnen Beobachtung zu kommen legen wir den Weg vom globalen Mittelwert über den Abstand vom globalen Mittel zum Gruppenmittel \\(\\beta\\) zurück. Dann fehlt noch der Rest oder Fehler oder Residuum \\(\\epsilon\\).\n\n\n\nWir rechnen also eine ganze Menge an Abständen und quadrieren dann diese Absatände zu den Sum of Squares. Oder eben der Varianz. Dann fragen wir uns, ob der Faktor in unserem Modell einen Teil der Abstände erklären kann. Wir bauen uns dafür eine ANOVA Tabelle. Tabelle 23.6 zeigt eine theoretische, einfaktorielle ANOVA Tabelle. Wir berechnen zuerst die Abstände als \\(SS\\). Nun ist es aber so, dass wenn wir in einer Gruppe viele Level und/oder Beoabchtungen haben, wir auch größere Sum of Squares bekommen. Wir müssen also die Sum of Squares in mittlere Abweichungsqudrate (eng. mean squares) mitteln. Abschließend können wir die F Statistik berechnen, indem wir die \\(MS\\) des Faktors durch die \\(MS\\) des Fehlers teilen. Das Verhältnis von erklärter Varianz vom Faktor zu dem unerklärten Rest.\n\n\n\nTabelle 23.6— Einfaktorielle ANOVA in der theoretischen Darstellung. Die sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(k-1\\)\n\\(SS_{animal} = \\sum_{i=1}^{k}n_i(\\bar{y}_{i.} - \\bar{y}_{..})^2\\)\n\\(MS_{animal} = \\cfrac{SS_{animal}}{k-1}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nerror\n\\(n-k\\)\n\\(SS_{error} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{i.})^2\\)\n\\(MS_{error} = \\cfrac{SS_{error}}{N-k}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{..})^2\\)\n\n\n\n\n\n\n\nWir füllen jetzt die Tabelle 23.7 einmal mit den Werten aus. Nachdem wir das getan haben oder aber die Tabelle in R ausgegeben bekommen haben, können wir die Zahlen interpretieren.\n\n\n\nTabelle 23.7— Einfaktorielle ANOVA mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dan direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 74.68\\)\n\\(MS_{animal} = \\cfrac{74.68}{3-1} = 37.34\\)\n\\(F_{calc} = \\cfrac{37.34}{3.14} = 11.89\\)\n\n\nerror\n\\(21-3\\)\n\\(SS_{error} = 56.53\\)\n\\(MS_{error} = \\cfrac{56.53}{18} = 3.14\\)\n\n\n\ntotal\n\\(21-1\\)\n\\(SS_{total} = 131.21\\)\n\n\n\n\n\n\n\nZu erst ist die berechnete F Statistik \\(F_{calc}\\) von Interesse. Wir haben hier eine \\(F_{calc}\\) von 11.89. Wir vergleichen wieder die berechnete F Statistik mit einem kritsichen Wert. Der kritische F Wert \\(F_{\\alpha = 5\\%}\\) lautet für die einfaktorielle ANOVA in diesem konkreten Beispiel mit \\(F_{\\alpha = 5\\%} = 3.55\\). Die Enstscheidungsregel nach der F Testatitik lautet, die \\(H_0\\) abzulehnen, wenn \\(F_{calc} > F_{\\alpha = 5\\%}\\).\nWir können also die Nullhypothese \\(H_0\\) in unserem Beispiel ablehnen. Es liegt ein signifikanter Unterschied zwischen den Tiergruppen vor. Mindestens ein Mittelwertsunterschied in den Sprungweiten liegt vor.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(F_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(F_{calc}\\) gilt, wenn \\(F_{calc} \\geq F_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.\n\n\n\n23.2.4 Einfaktoriellen ANOVA in R\nUm eine ANOVA zu rechnen nutzen wir zuerst die Funktion lm(), warum das so ist kannst du im Kapitel 31 nachlesen. Du brauchst das Wissen aber hier nicht unbedingt.\nWir rechnen keine ANOVA per Hand sondern nutzen R. Dazu müssen wir als erstes das Modell definieren. Das ist im Falle der infaktoriellen ANOVA relativ einfach. Wir haben unseren Datensatz fac1_tbl mit einer kontinuierlichen Variable jump_lemgth als \\(y\\) vorliegen sowie einen Faktor animal mit mehr als zwei Leveln als \\(x\\). Wir definieren das Modell in R in der Form jump_length ~ animal. Um das Modell zu rechnen nutzen wir die Funktion lm() - die Abkürzung für linear model. Danach pipen wir die Ausgabe vom lm() direkt in die Funktion anova(). Die Funktion anova berechnet uns dann die eigentliche einfaktorielle ANOVA. Wir speichern die Ausgabe der ANOVA in fit_1. Schauen wir uns die ANOVA Ausgabe einmal an.\n\nfit_1 <-  lm(jump_length ~ animal, data = fac1_tbl) %>% \n  anova\n\nfit_1\n\nAnalysis of Variance Table\n\nResponse: jump_length\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nanimal     2 74.683  37.341   11.89 0.0005113 ***\nResiduals 18 56.529   3.140                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten die Information was wir gerechnet haben, eine Varianzanalyse. Darunter steht, was das \\(y\\) war nämlich die jump_length. Wir erhalten eine Zeile für den Faktor animal und damit die \\(SS_{animal}\\) und eine Zeile für den Fehler und damit den \\(SS_{error}\\). In R heißen die \\(SS_{error}\\) dann Residuals. Die Zeile für die \\(SS_{total}\\) fehlt.\nNeben der berechneten F Statistik \\(F_{calc}\\) von \\(11.89\\) erhalten wir auch den p-Wert mit \\(0.005\\). Wir ignorieren die F Statistik, da wir in der Anwendung nur den p-Wert berücksichtigen. Die Entscheidung gegen die Nulhypothese lautet, dass wenn der p-Wert kleiner ist als das Signifkanzniveau \\(\\alpha\\) von 5% wir die Nullhypothese ablehnen.\nWir haben hier ein signifikantes Ergebnis vorliegen. Mindestens ein Gruppenmittelerstunterschied ist signifikant. Abbildung 23.5 zeigt nochmal die Daten fac1_tbl als Boxplot. Wir überprüfen visuell, ob das Ergebnis der ANOVA stimmen kann. Ja, die Boxplots und das Ergebnis der ANOVA stimmen überein. Die Boxplots liegen nicht alle auf einer Ebene, so dass hier auch ein signifikanter Unterschied zu erwarten war.\n\n\n\n\nAbbildung 23.5— Boxplot der Sprungweiten [cm] von Hunden-, Katzen- und Fuchsflöhen.\n\n\n\n\nAbschließend können wir noch die Funktion eta_squared() aus dem R Paket effectsize nutzen um einen Effektschätzer für die einfaktorielle ANOVA zu berechnen. Wir können mit \\(\\eta^2\\) abschätzen, welchen Anteil der Faktor animal an der gesamten Varianz erklärt.\n\nfit_1 %>% eta_squared\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nanimal    | 0.57 | [0.27, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDas \\(\\eta^2\\) können wir auch einfach händisch berechnen.\n\\[\n\\eta^2 = \\cfrac{SS_{animal}}{SS_{total}} = \\cfrac{74.68}{131.21} = 0.57 = 57\\%\n\\]\nWir haben nun die Information, das 57% der Varianz der Beobachtungen durch den Faktor animal rklärt wird. Je nach Anwendungsgebiet kann die Relevanz sehr stark variieren. Im Bereich der Züchtung mögen erklärte Varianzen von unter 10% noch sehr relevant sein. Im Bereich des Feldexperiments erwarten wir schon höhere Werte für \\(\\eta^2\\). Immerhin sollte ja unsere Behandlung maßgeblich für die z.B. größeren oder kleineren Pflanzen gesorgt haben."
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac2",
    "href": "stat-tests-anova.html#sec-fac2",
    "title": "23  Die ANOVA",
    "section": "\n23.3 Zweifaktorielle ANOVA",
    "text": "23.3 Zweifaktorielle ANOVA\n\n\n\n\n\n\nDie zweifaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität jeweils separat über beide Behandlungsfaktor \\(x_1\\) und \\(x_2\\). Daher alle Level von \\(x_1\\) sollen die gleiche Varianz haben. Ebenso sollen alle Level von \\(x_2\\) die gleiche Varianz haben.\nUnsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x_1\\) und \\(x_2\\) jewiels für sich homogen in den Varianzen sind. Später mehr dazu, wenn wir beides nicht vorliegen haben…\nDie zweifaktorielle ANOVA ist eine wunderbare Methode um herauszufinden, ob zwei Faktoren einen Einfluss auf ein normalverteiltes \\(y\\) haben. Die Stärke der zweifaktoriellen ANOVA ist hierbei, dass die ANOVA beide Effekte der Faktoren auf das \\(y\\) simultan modelliert. Darüber hinaus können wir auch noch einen Interaktionsterm mit in das Modell aufnehmen um zu schauen, ob die beiden Faktoren untereinander auch interagieren. Somit haben wir mit der zweifaktoriellen ANOVA die Auswertungsmehode für ein randomiziertes Blockdesign vorliegen.\n\n23.3.1 Daten für die zweifaktorielle ANOVA\nWir wollen uns nun einen etwas komplexes Modell anschauen mit einem etwas komplizierteren Datensatz flea_dog_cat_fox_site.csv. Wir brauchen hierfür ein normalverteiltes \\(y\\) und sowie zwei Faktoren. Das macht auch soweit Sinn, denn wir wollen ja auch eine zweifaktorielle ANOVA rechnen.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal sowie die Spalte site als \\(x\\). Danach müssen wir noch die Variable animal sowie die Variable site in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac2_tbl <- read_csv2(\"data/flea_dog_cat_fox_site.csv\") %>% \n  select(animal, site, jump_length) %>% \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 23.8 nochmal dargestellt.\n\n\n\n\nTabelle 23.8— Selektierter Datensatz für die zweifaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\nanimal\nsite\njump_length\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.10\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\ncat\ncity\n14.91\n\n\n\n\n\n\nDie Beispieldaten sind in Abbildung 23.6 abgebildet. Wir sehen auf der x-Achse den Faktor animal mit den drei Leveln dog, cat und fox. Jeder dieser Faktorlevel hat nochmal einen Faktor in sich. Dieser Faktor lautet site und stellt dar, wo die Flöhe gesammelt wurden. Die vier Level des Faktors site sind city, smalltown, village und field.\n\n\n\n\nAbbildung 23.6— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nWir bauen dann mit den beiden Variablen bzw. Faktoren animal und site aus dem Objekt fac2_tbl folgendes Modell für die zweifaktorielle ANOVA:\n\\[\njump\\_length \\sim animal + site\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der zweifaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n23.3.2 Hypothesen für die zweifaktorielle ANOVA\nWir haben für jeden Faktor der zweifaktoriellen ANOVA ein Hypothesenpaar. Im Folgenden sehen wir die jeweiligen Hypothesenpaare.\nEinmal für animal, als Haupteffekt. Wir nennen einen Faktor den Hauptfaktor, weil wir an diesem Faktor am meisten interessiert sind. Wenn wir später einen Posthoc Test durchführen würden, dann würden wir diesen Faktor nehmen. Wir sind primär an dem Unterschied der Sprungweiten in [cm] in Gruppen Hund, Katze und Fuchs interessiert.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für site, als Nebeneffekt oder Blockeffekt oder Clustereffekt. Meist eine Variable, die wir auch erhoben haben und vermutlich auch einen Effekt auf das \\(y\\) haben wird. Oder aber wir haben durch das exprimentelle Design noch eine Aufteilungsvariable wie Block vorliegen. In unserem Beispiel ist es site oder der Ort, wo wir die Hunde-, Katzen, und Fuchsflöhe gefunden haben.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{city} = \\bar{y}_{smalltown} = \\bar{y}_{village} = \\bar{y}_{field}\\\\\nH_A: &\\; \\bar{y}_{city} \\ne \\bar{y}_{smalltown}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{village} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für die Interaktion animal:site - die eigentliche Stärke der zweifaktoriellen ANOVA. Wir können uns anschauen, ob die beiden Faktoren miteinander interagieren. Das heißt, ob eine Interaktion zwischen dem Faktor animal und dem Faktor site vorliegt.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{keine Interaktion}\\\\\nH_A: &\\; \\mbox{eine Interaktion zwischen animal und site}\n\\end{aligned}\n\\]\nWir haben also jetzt die verschiedenen Hypothesenpaare definiert und schauen uns jetzt die ANOVA in R einmal in der Anwendung an.\n\n23.3.3 Zweifaktoriellen ANOVA in R\nBei der einfaktoriellen ANOVA haben wir die Berechnungen der Sum of squares nochmal nachvollzogen. Im Falle der zweifaktoriellen ANOVA verzichten wir darauf. Das Prinzip ist das gleiche. Wir haben nur mehr Mitelwerte und mehr Abweichungen von diesen Mittelwerten, da wir ja nicht nur einen Faktor animal vorliegen haben sondern auch noch den Faktor site. Da wir aber die ANOVA nur Anwenden und dazu R nutzen, müssen wir jetzt nicht per Hand die zweifaktorielle ANOVA rechnen. Du musst aber die R Ausgabe der ANOVA verstehen. Und diese Ausgabe schauen wir uns jetzt einmal ohne und dann mit Interaktionsterm an.\n\n23.3.3.1 Ohne Interaktionsterm\nWir wollen nun einmal die zweifaktorielle ANOVA ohne Interaktionsterm rechnen die in Tabelle 23.9 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen die Funktion anova() in R.\n\n\n\nTabelle 23.9— Zweifaktorielle ANOVA ohne Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nerror\n\\(n-(a-1)(b-1)\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 31 erfahren.\nIm Folgenden sehen wir nochmal das Modell ohne Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel.\n\\[\njump\\_length \\sim animal + site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm().\n\nfit_2 <-  lm(jump_length ~ animal + site, data = fac2_tbl) %>% \n  anova\n\nfit_2\n\nAnalysis of Variance Table\n\nResponse: jump_length\n           Df Sum Sq Mean Sq F value   Pr(>F)    \nanimal      2 180.03  90.017 19.8808 3.92e-08 ***\nsite        3   9.13   3.042  0.6718    0.571    \nResiduals 114 516.17   4.528                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten wiederrum die ANOVA Ergebnistabelle. Ansatt nur die Zeile animal für den Effekt des Faktors animal sehen wir jetzt auch noch die Zeile site für den Effekt des Faktors site. Zuerst ist weiterhin der Faktor animal signifikant, da der \\(p\\)-Wert mit \\(0.000000039196\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können von mindestens einem Gurppenunterschied im Faktor animal ausgehen. Im Weiteren ist der Faktor site nicht signifikant. Es scheint keinen Untrschied zwischend den einzelnen Orten und der Sprunglänge von den Hunde-, Katzen- und Fuchsflöhen zu geben.\nNeben der Standausgabe von R können wir auch die tidy Variante uns ausgeben lassen. In dem Fall sieht die Ausgabe etwas mehr aufgeräumt aus.\n\nfit_2 %>% tidy\n\n# A tibble: 3 x 6\n  term         df  sumsq meansq statistic       p.value\n  <chr>     <int>  <dbl>  <dbl>     <dbl>         <dbl>\n1 animal        2 180.    90.0     19.9    0.0000000392\n2 site          3   9.13   3.04     0.672  0.571       \n3 Residuals   114 516.     4.53    NA     NA           \n\n\nAbschließend können wir uns übr \\(\\eta^2\\) auch die erklärten Anteile der Varainz wiedergeben lassen.\n\nfit_2 %>% eta_squared\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\nanimal    |           0.26 | [0.15, 1.00]\nsite      |           0.02 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 26%. Für den Faktor site haben wir nur einen Anteil von 2% der erklärten Varianz. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 23.10 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\nTabelle 23.10— Zweifaktorielle Anova ohne Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dan direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{4.53} = 19.88\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{4.53} = 0.67\\)\n\n\nerror\n\\(120-(3-1)(4-1)\\)\n\\(SS_{error} = 516.17\\)\n\\(MS_{error} = 4.53\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.33\\)\n\n\n\n\n\n\n\n\n23.3.3.2 Mit Interaktionssterm\nWir wollen nun noch einmal die zweifaktorielle ANOVA mit Interaktionsterm rechnen, die in Tabelle 23.11 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen wie immer die Funktion anova() in R.\n\n\n\nTabelle 23.11— Zweifaktorielle ANOVA mit Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nanimal \\(\\times\\) site\n\\((a-1)(b-1)\\)\n\\(SS_{animal \\times site}\\)\n\\(MS_{animal \\times site}\\)\n\\(F_{calc} = \\cfrac{MS_{animal \\times site}}{MS_{error}}\\)\n\n\nerror\n\\(n-ab\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\nIm Folgenden sehen wir nochmal das Modell mit Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel. Einen Interaktionsterm bilden wir durch das : in R ab. Wir können theoretisch auch noch weitere Interaktionsterme bilden, also auch x:y:z. Ich würde aber davon abraten, da diese Interaktionsterme schwer zu interpretieren sind.\n\\[\njump\\_length \\sim animal + site + animal:site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Es wieder das gleich wie schon zuvor. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm(). Auch die Interaktion müssen wir nicht extra in der ANOVA Funktion angeben. Alles wird im Modell des lm() abgebildet.\nDie visuelle Regel zur Überprüfung der Interaktion lautet nun wie folgt. Abbildung 23.7 zeigt die entsprechende Vislualisierung. Wir haben keine Interaktion vorliegen, wenn die Geraden parallel zueinander laufen und die Abstände bei bei jedem Faktorlevel gleich sind. Wir schauen uns im Prinzip die erste Faktorstufe auf der x-Achse an. Wir sehen den Abstand von der roten zu blauen Linie sowie das die blaue Gerade über der roten Gerade liegt. Dieses Muster erwarten wir jetzt auch an dem Faktorlevel B und C. Eine leichte bis mittlere Interaktion liegt vor, wenn sich die Abstaände von dem zweiten Fakotr über die Faktorstufen des ersten Faktors ändern. Eine starke Interaktion liegt vor, wenn sich die Geraden schneiden.\n\n\n\n\n\n(a) Keine Interaktion\n\n\n\n\n\n\n(b) Leichte bis mittlere Intraktion\n\n\n\n\n\n\n(c) Starke Interaktion\n\n\n\n\nAbbildung 23.7— Darstellung von keiner Interaktion, leichter bis mittler Interaktion und starker Interaktion in einer zweifaktoriellen ANOVA mit einem Faktor mit drei Leveln A, B und C sowie einem Faktor mit zwei Leveln (rot und blau).\n\n\nIn der Abbildung 23.8 sehen wir den Interaktionsplot für unser Beispiel. Auf der y-Achse ist die Sprunglänge abgebildet und auf der x-Achse der Faktor animal. Die einzelnen Farben stellen die Level des Faktor site dar.\n\nggplot(fac2_tbl, aes(x = animal, y = jump_length,\n                     color = site, group = site)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_bw()\n\n\n\nAbbildung 23.8— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nWenn sich die Geraden in einem Interaktionsplot schneiden, haben wir eine Interaktion zwischen den beiden Faktoren vorliegen\nWir schauen zur visuellen Überprüfung auf den Faktor animal und das erste level cat. Wir sehen die Ordnung des zweiten Faktors site mit field, village, smalltown und city. Diese Ordnung und die Abstände sind bei zweiten Faktorlevel dog schon nicht mehr gegeben. Die Geraden schneiden sich. Auch liegt bei dem Level fox eine andere ordnung vor. Daher sehen wir hier eine starke Interaktion zwischen den beiden Faktoren animal und site.\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 31 erfahren.\nWir nehmen jetzt auf jeden Fall den Interaktionsterm animal:site mit in unser Modell und schauen uns einmal das Ergebnis der ANOVA an. Das lineare Modell der ANOVA wird erneut über die Funktion lm() berechnet und anschließend in die Funktion anova() gepipt.\n\nfit_3 <-  lm(jump_length ~ animal + site + animal:site, data = fac2_tbl) %>% \n  anova\n\nfit_3\n\nAnalysis of Variance Table\n\nResponse: jump_length\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nanimal        2 180.03  90.017 30.2807 3.63e-11 ***\nsite          3   9.13   3.042  1.0233   0.3854    \nanimal:site   6 195.11  32.519 10.9391 1.71e-09 ***\nResiduals   108 321.05   2.973                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn wir eine signifikante Interaktion vorliegen haben, dann müssen wir den Faktor A getrennt für jedes Levels des Faktors B auswerten.\nDie Ergebnistabelle der ANOVA wiederholt sich. Wir sehen, dass der Faktor animal signifkant ist, da der p-Wert mit \\(0.000000000036\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese ablehnen. Mindestens ein Mittelwertsvergleich unterschiedet sich zwischen den Levels des Faktors animal. Im Weiteren sehen wir, dass der Faktor site nicht signifkant ist, da der p-Wert mit \\(0.39\\) größer ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese nicht ablehnen. Abschließend finden wir die Interaktion zwischen dem Faktor animalund site las signifkant vor. Wenn wir eine signifikante Interaktion vorliegen haben, dann müssen wir den Faktor animal getrennt für jedes Levels des Faktors site auswerten. Wir können keine Aussage über die Sprungweite von Hunde-, Katzen- und Fuchsflöhen unabhängig von der Herkunft site der Flöhe machen.\nIn Kapitel XX findest du ein Beispiel für eine signifikante Interaktion und die folgende Auswertung\nWir können wie immer die etwas aufgeräumte Variante der ANOVA Ausgabe mit der Funktion tidy() uns ausgeben lassen.\n\nfit_3 %>% tidy()\n\n# A tibble: 4 x 6\n  term           df  sumsq meansq statistic   p.value\n  <chr>       <int>  <dbl>  <dbl>     <dbl>     <dbl>\n1 animal          2 180.    90.0      30.3   3.63e-11\n2 site            3   9.13   3.04      1.02  3.85e- 1\n3 animal:site     6 195.    32.5      10.9   1.71e- 9\n4 Residuals     108 321.     2.97     NA    NA       \n\n\nIm Folgenden können wir noch die \\(\\eta^2\\) für die ANOVA als Effektschätzer berechnen lassen.\n\nfit_3 %>% eta_squared\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Eta2 (partial) |       95% CI\n-------------------------------------------\nanimal      |           0.36 | [0.24, 1.00]\nsite        |           0.03 | [0.00, 1.00]\nanimal:site |           0.38 | [0.24, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 36%. Für den Faktor site haben wir nur einen Anteil von 3% der erklärten Varianz. Die Interaktion zwischen animal und site erklärt 38% der beobachteten Varianz udn ist somit auch vom Effekt her nicht zu ignorieren. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 23.12 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\nTabelle 23.12— Zweifaktorielle Anova mit Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dan direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{2.97} = 30.28\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{2.97} = 1.02\\)\n\n\nanimal \\(\\times\\) site\n\\((3-1)(4-1)\\)\n\\(SS_{animal \\times site} = 195.12\\)\n\\(MS_{animal \\times site} = 32.52\\)\n\\(F_{calc} = \\cfrac{32.52}{2.97} = 10.94\\)\n\n\nerror\n\\(120 - (3 \\cdot 4)\\)\n\\(SS_{error} = 321.06\\)\n\\(MS_{error} = 2.97\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.34\\)"
  },
  {
    "objectID": "stat-tests-anova.html#und-weiter",
    "href": "stat-tests-anova.html#und-weiter",
    "title": "23  Die ANOVA",
    "section": "\n23.4 Und weiter?",
    "text": "23.4 Und weiter?\nNach einer berechnten ANOVA können wir zwei Fälle vorliegen haben.\nWenn du in deinem Experiment keine signifikanten Ergebnisse findest, ist das nicht schlimm. Du kannst deine Daten immer noch mit der explorativen Datenanalyse auswerten wie in Kapitel 16 beschrieben.\n\nWir habe eine nicht signifkante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) nicht ablehnen und die Mittelwerte über den Faktor sind vermutlich alle gleich. Wir enden hier mit unserer statistischen Analyse.\nWir haben eine signifikante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) ablehnen und mindestens ein Gruppenvergleich über mindestens einen Faktor ist vermutlich unterschiedlich. Wir können dann in Kapitel 31 eine Posthoc Analyse rechnen."
  },
  {
    "objectID": "stat-tests-posthoc.html",
    "href": "stat-tests-posthoc.html",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:48:19\nIn diesem Kapitel wollen wir uns mit den multipen Vergleichen beschäftigen. Das heißt, wir wollen statistisch Testen, ob sich die Level eines Faktors voneinander unterscheiden. Eventuell hast du schon eine einfaktorielle ANOVA gerechnet, wie in Kapitel 23.2 beschrieben. Oder aber du hast eine mehrfaktorielle ANOVA gerechnet wie in Kapitel 23.3 gezeigt. In beiden Fällen hast du jetzt einen signifikanten Faktor, der mehr als zwei Level hat. Du willst nun wissen, welche der Gruppenmittelwerte der Level sich signifikant unterscheiden. Hierfür können wir verschiedene Ansätze wählen.\nWenn wir multiple Mittelwertsvergleiche rechnen, dann tritt das Problem des multipen Testens auf. Im Kapitel 20.3 kannst du mehr über die Problematik erfahren und wie wir mit der \\(\\alpha\\) Inflation umgehen. Hier in diesem Kapitel gehe ich jetzt davon aus, dass dir die \\(\\alpha\\) Adjustierung ein Begriff ist.\nDer paarweise Mittelwertsvergleich wird auch gerne Tukey Test genannt. Das heißt, dass der Tukey Test alle Gruppen miteinander vergleicht. Der Tukey Test wird daher auch gerne all pair Vergleich genannt."
  },
  {
    "objectID": "stat-tests-posthoc.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-posthoc.html#genutzte-r-pakete-für-das-kapitel",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.1 Genutzte R Pakete für das Kapitel",
    "text": "31.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom,\n               multcomp, emmeans, ggpubr, multcompView,\n               rstatix, conflicted, see, rcompanion)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-posthoc.html#daten",
    "href": "stat-tests-posthoc.html#daten",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.2 Daten",
    "text": "31.2 Daten\nWir nutzen in diesem Kapitel den Datensatz aus dem Beispiel in Kapitel 6. Wir haben als Outcome die Sprunglänge in [cm] von Flöhen. Die Sprunglänge haben wir an Flöhen von Hunde, Katzen und Füchsen gemessen. Der Datensatz ist also recht übeerschaubar. Wir haben ein normalverteiltes \\(y\\) mit jump_length sowie einen multinomialverteiltes \\(y\\) mit grade und einen Faktor animal mit drei Leveln.\nDu kannst dir komplexere Auswertungen im Kapitel A anschauen. Dort sammelt sich mit der Zeit Auswertungen vom Fachbereich an. Daher finden sich dort auch Beispiele für multiple Vergleiche.\nIm Folgenden laden wir den Datensatz flea_dog_cat_fox.csv und selektieren mit der Funktion select() die benötigten Spalten. Abschließend müssen wir die Spalte animalnoch in einen Faktor umwandeln. Damit ist unsere Vorbereitung des Datensatzes abgeschlossen.\n\nfac1_tbl <- read_csv2(\"data/flea_dog_cat_fox.csv\") %>%\n  select(animal, jump_length, grade) %>% \n  mutate(animal = as_factor(animal))\n\nIn der Tabelle 31.1 ist der Datensatz fac1_tbl nochmal dargestellt.\n\n\n\n\nTabelle 31.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length sowie der multinominalverteilten Variable grade und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\ngrade\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n8.2\n8\n\n\ndog\n5.6\n7\n\n\ndog\n9.1\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\nfox\n7.7\n5\n\n\nfox\n8.1\n4\n\n\nfox\n9.1\n4\n\n\nfox\n9.7\n5\n\n\nfox\n10.6\n4\n\n\nfox\n8.6\n4\n\n\nfox\n10.3\n3\n\n\n\n\n\n\nWir werden nun den Datensatz fac1_tbl in den folgenden Abschnitten immer wieder nutzen.\n\n31.2.1 Hypothesen für multiple Vergleiche\nAls wir eine ANOVA gerechnet hatten, hatten wir nur eine Nullhypothese und eine Alternativehypothese. Wenn wir Nullhypothese abgelehnt hatten, wussten wir nur, dass sich mindestens ein paarweiser Vergleich unterschiedet. Multiple Vergleich lösen nun dieses Problem und führen ein Hypothesenpaar für jeden paarweisen Vergleich ein. Zum einen rechnen wir damit \\(k\\) Tests und haben damit auch \\(k\\) Hypothesenpaare (siehe auch Kapitel 20.3 zur Problematik des wiederholten Testens).\nWenn wir zum Beispiel alle Level des Faktors animal miteinander Vergleichen wollen, dann rechnen wir \\(k=3\\) paarweise Vergleiche. Im Folgenden sind alle drei Hypothesenpaare dargestellt.\n\\[\n\\begin{aligned}\nH_{01}: &\\; \\bar{y}_{cat} = \\bar{y}_{dog}\\\\\nH_{A1}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{02}: &\\; \\bar{y}_{cat} = \\bar{y}_{fox}\\\\\nH_{A2}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{03}: &\\; \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_{A3}: &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\nWenn wir drei Vergleiche rechnen, dann haben wir eine \\(\\alpha\\) Inflation vorliegen. Wir sagen, dass wir für das multiple Testen adjustieren müssen. In R gibt es eine Reihe von Adjustierungsverfahren. Wir nehmen meist Bonferroni oder das Verfahren, was in der jeweiligen Funktion als Standard (eng. default) gesetzt ist.\nWir adjustieren grundsätzlich die \\(p\\)-Werte und erhalten adjustierte \\(p\\)-Werte aus den jeweiligen Funktionen in R. Die adjustierten p-Werte können wir dann mit dem Signifikanzniveau von \\(\\alpha\\) gleich 5% vergleichen."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "href": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.3 Gruppenvergleiche mit pairwise.*.test()\n",
    "text": "31.3 Gruppenvergleiche mit pairwise.*.test()\n\nDie Funktion pairwise.*.test() ist veraltet, wir nutzen das R Paket emmeansoder das R Paket multcomp.\nWenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, dann können wir die Funktion pairwise.*.test() nutzen. Der Stern * steht entweder als Platzhalter für t für den t-Test oder aber für wilcox für den Wilcoxon Test. Die Funktion ist relativ einfach zu nutzen und liefert auch sofort die entsprechenden p-Werte.\nDie Funktion pairwise.*.test() ist in dem Sinne veraltet, da wir keine 95% Konfidenzintervalle generieren können. Da die Funktion aber immer mal wieder angefragt wird, ist die Funktion hier nochmal aufgeführt.\n\n31.3.1 Paarweiser t Test\nWir nutzen den paarweisen t-Test,\n\nwenn wir ein normalverteiltes \\(y\\) vorliegen haben, wie jump_length.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\n\nDie Funktion pairwise.t.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und dhaer auch fehleranfällig.\n\n\nMehr zu mutate_if() erfährst du auf der Hilfeseite von mutate()\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein saubers tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.t.test(fac1_tbl$jump_length, fac1_tbl$animal,\n                p.adjust.method = \"bonferroni\") %>% \n  tidy %>% \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 x 3\n  group1 group2 p.value\n  <chr>  <chr>    <dbl>\n1 cat    dog      0.007\n2 fox    dog      0.876\n3 fox    cat      0.001\n\n\nWir erhalten in einem Tibble die adujstierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wir sehen, dass der Gruppenvergleich cat - dog signifikant ist, der Gruppenvergleich fox - dog nicht signifkant ist und der Gruppenvergleich fox - cat wiederum signifkant ist.\nLeider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket ggpubr anschauen. Die Hilfeseite des R Pakets ggpubr liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\).\nUm die Abbildung 31.1 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen t-Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert einer einfaktoriellen ANOVA als globalen Test ergänzen.\n\nstat_test_obj <- compare_means(\n jump_length ~ animal, data = fac1_tbl,\n method = \"t.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np <- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"jump_length\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(13, 16, 19)) +\n  stat_compare_means(label.y = 20, method = \"anova\")    \n\n\n\nAbbildung 31.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem t-Test und den Bonferroni adjustierten p-Werten.\n\n\n\n\n\n31.3.2 Paarweiser Wilcoxon Test\nWir nutzen den paarweisen Wilxocon Test,\n\nwenn wir ein nicht-normalverteiltes \\(y\\) vorliegen haben, wie grade.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\n\nDie Funktion pairwise.wilcox.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und dhaer auch fehleranfällig.\n\n\nMehr zu mutate_if() erfährst du auf der Hilfeseite von mutate()\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein saubers tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.wilcox.test(fac1_tbl$grade, fac1_tbl$animal,\n                     p.adjust.method = \"bonferroni\") %>% \n  tidy %>% \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 x 3\n  group1 group2 p.value\n  <chr>  <chr>    <dbl>\n1 cat    dog      0.045\n2 fox    dog      0.005\n3 fox    cat      0.011\n\n\nWir erhalten in einem Tibble die adujstierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wir sehen, dass der Gruppenvergleich cat - dog knapp signifikant ist, der Gruppenvergleich fox - dog ebenfalls signifkant ist und der Gruppenvergleich fox - cat auch signifkant ist.\nLeider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket ggpubr anschauen. Die Hilfeseite des R Pakets ggpubr liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\).\nUm die Abbildung 31.2 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen Wilcoxon Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert eines Kruskal Wallis als globalen Test ergänzen.\n\nstat_test_obj <- compare_means(\n grade ~ animal, data = fac1_tbl,\n method = \"wilcox.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np <- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"grade\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(10, 13, 16)) +\n  stat_compare_means(label.y = 20, method = \"kruskal.test\")    \n\n\n\nAbbildung 31.2— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem Wilcoxon Test und den Bonferroni adjustierten p-Werten."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "href": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.4 Gruppenvergleich mit dem multcomp Paket",
    "text": "31.4 Gruppenvergleich mit dem multcomp Paket\nWir drehen hier einmal die Erklärung um. Wir machen erst die Anwendung in R und sollte dich dann noch mehr über die statistischen Hintergründe der Funktionen interessieren, folgt ein Abschnitt noch zur Theorie. Du wirst die Funktionen aus multcomp vermutlich in deiner Abschlussarbeit brauchen. Häufig werden multiple Gruppenvergleiche in Abschlussarbeiten gerechnet.\n\n31.4.1 Gruppenvergleiche mit multcomp in R\n\n\nDie Ausgabe von multcomp können über die Funktion tidy() aufgeräumt werden. Mehr dazu unter der Hilfeseite von tidy() zu multcomp.\nAls erstes brauchen wir ein lineares Modell für die Verwendung von multcomp. Normalerweise verenden wir das gleiche Modell, was wir schon in der ANOVA verwendet haben. Wir nutzen hier ein simples lineares Modell mit nur einem Faktor. Im Prinzip kann das Modell auch größer sein. Du findest immer Beispiel im Kapitel A, die dir eventuell dann nochmal zeigen, wie du deine Daten nutzen musst.\n\nfit_1 <- lm(jump_length ~ animal, data = fac1_tbl)\n\nWir haben das Objeckt fit_1 mit der Funktion lm() erstellt. Im Modell sind jetzt alle Mittelwerte und die entsprechenden Varianzen geschätzt worden. Mit summary(fit_1) kannst du dir gerne das Modell auch nochmal anschauen.\n\n\nWenn wir keinen all-pair Vergleich rechnen wollen, dann können wir auch einen many-to-one Vergleich mit dem Dunnett Kontrast rechnen.\nIm Anschluß nutzen wir die Funktion glht() um den multiplen vergleich zu rechnen. Als erstes musst du wissen, dass wenn wir alle Vergleiche rechnen wollen, wir einen all-pair Vergleich rechnen. In der Statistik heißt dieser Typ von Vergleich Tukey. Wir wollen jetzt als für den Faktor animal einen multiplen Tukey-Vergleich rechnen. Nichts anders sagt mcp(animal = \"Tukey\") aus, dabei steht mcp für multiple comparison procedure. Mit dem hinteren Teil der Funktion weiß jetzt die Funktion glht() was gerechnet werden soll. Wir müssen jetzt der Funktion nur noch mitgeben auf was der multiple vergleich gerehcnet werden soll, mit dem Objekt fit_1. Wir speichern die Ausgabe der Funktion in comp_1_obj.\n\ncomp_1_obj <- glht(fit_1, linfct = mcp(animal = \"Tukey\")) \n\nMit dem Objekt comp_1_fit können wir noch nicht soviel anfangen. Der Inhalt ist etwas durcheinander und wir wollen noch die Konfidenzintervalle haben. Daher pipen wir comp_1_fit erstmal in die Funktion tidy() und alssen mit der Option conf.int = TRUE die simultanen 95% Konfidenzintervalle berechnen. Dann nutzen wir die Funktion select() um die wichtigen Spalten zu selektieren. Abschließend mutieren wir noch alle numerischen Spalten in dem wir auf die dritte Kommastelle runden. Wir speichern alles in das Objekt res_1_obj.\n\nres_1_obj <- comp_1_obj %>% \n  tidy(conf.int = TRUE) %>% \n  select(contrast, estimate, adj.p.value, \n         conf.low, conf.high) %>% \n  mutate_if(is.numeric, round, 3)\n\nWir lassen uns dann den Inhalt von dem Objekt res_1_obj ausgeben.\n\nres_1_obj\n\n# A tibble: 3 x 5\n  contrast  estimate adj.p.value conf.low conf.high\n  <chr>        <dbl>       <dbl>    <dbl>     <dbl>\n1 cat - dog    -3.39       0.006    -5.80     -0.97\n2 fox - dog     1.03       0.535    -1.39      3.44\n3 fox - cat     4.41       0.001     2.00      6.83\n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir cat - dog rechnen. Also wirklich der Mittelwert der Sprungweite der Katzenflöhe minus den Mittelwert der Sprungweite der Hundeflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleichecat - dog mit einem p-Wert von \\(0.006\\) sowie für den Vergleich \\(fox - cat\\) mit einem p-Wert von \\(0.001\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nIn Abbildung 31.3 sind die simultanen 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab. Mit der Funktion factor() kannst du die Sortierung der Level einem Faktor ändern und somit auch Position auf den Achsen.\n\n  ggplot(res_1_obj, aes(contrast, y=estimate, \n                        ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.3— Simultane 95% Konfidenzintervalle für den paarweisen Vergleich der Sprungweiten in [cm] der Hunde-, Katzen- und Fuchsflöhe.\n\n\n\n\nDie Entscheidung gegen die Nullhypothese anhand der simultanen 95% Konfidenzintervalle ist inhaltlich gleich, wie die Entscheidung anhand der p-Werte. Wir entscheiden gegen die Nullhypothese, wenn die 0 nicht mit im Konfindenzintervall enthalten ist. Wir wählen hier die 0 zur Entscheidung gegen die Nullhypothese, weil wir einen Mittelwertsvergleich rechnen.\nFür den Vergleich fox -dog ist die 0 im 95% Konfidenzintervall, wir können daher die Nullhypothese nicht ablehnen. Das 95% Konfidenzintervall ist nicht signifikant. Bei dem Vergleich fox - cat sowie dem Vergleich cat - dog ist jeweils die 0 nicht im 95% Konfidenzintervall enthalten. Beide 95% Konfidenzintervalle sind signifikant, wir können die Nullhypothese ablehnen."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "href": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.5 Gruppenvergleich mit dem emmeans Paket",
    "text": "31.5 Gruppenvergleich mit dem emmeans Paket\n\n\nWir können hier nicht alles erklären und im Detail durchgehen. Hier gibt es noch ein aufwendiges Tutorium zu emmeans: Getting started with emmeans.\nDaneben gibt es auch noch die Einführung mit Theorie auf der Seite des R Paktes\nIm Folgenden wollen wir uns mit einem anderen R Paket beschäftigen was auch multiple Vergleiche rechnen kann. In diesem Kapitel nutzen wir das R Paket emmeans. Im Prinzip kann emmeans das Gleiche wir das R Paket multcomp. Beide Pakete rechnen dir einen multipen Vergleich. Das Paket emmeans kann noch mit nested comparisons umgehen. Deshlb hier nochmal die Vorstellung von emmeans. Du kannst aber für eine simple Auswertung mit nur einem Faktor beide Pakete verwenden.\n\n31.5.1 Gruppenvergleiche mit emmeans in R\n\n\nDie Ausgabe von emmeans können über die Funktion tidy() aufgeräumt werden. Mehr dazu unter der Hilfeseite von tidy() zu emmeans.\nUm den multiplen Vergleich in emmeans durchführen zu können brauchen wir zuerst ein lineares Modell, was uns die notwenidgen Parameter wie Mittelwerte und Standardabweichungen liefert. Wir nutzen in unserem simplen Beispiel ein lineares Modell mit einer Einflussvariable \\(x\\) und nehmen an, dass unser Outcome \\(y\\) normalverteilt ist. Achtung, hier muss natürlich das \\(x\\) ein Faktor sein. Dann können wir ganz einfach die Funktion lm() nutzen. Im Folgenden fitten wir das Modell fit_2 was wir dann auch weiter nutzen werden.\n\nfit_2 <- lm(jump_length ~ animal, data = fac1_tbl)\n\nDer multiple Vergleich in emmeans ist mehrschrittig. Wir pipen unser Modell aus fit_2 in die Funktion emmeans(). Wir geben mit ~ animal an, dass wir über die Level des Faktors animal einen Vergleich rechnen wollen. Wir adjustieren die \\(p\\)-Werte nach Bonferroni. Danach pipen wir weiter in die Funktion contrast() wo der eigentliche Vergleich festgelegt wird. In unserem Fall wollen wir einen many-to-one Vergleich rechnen. Alle Gruppen zu der Gruppe fox. Du kannst mit ref = auch ein anderes Level deines Faktors wählen.\n\ncomp_2_obj <- fit_2 %>% \n  emmeans(~ animal, adjust = \"bonferroni\") %>% \n  contrast(method = \"trt.vs.ctrl\", ref = \"fox\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - fox    -1.03 0.947 18  -1.086  0.4682\n cat - fox    -4.41 0.947 18  -4.660  0.0004\n\nP value adjustment: dunnettx method for 2 tests \n\n\nWir können auch einen anderen Kontrast wählen. Wir überschreiben jetzt das Objekt comp_2_obj mit dem Kontrast all-pair, der alle möglichen Vergleiche rechnet. In emmeans heißt der all-pair Kontrast pairwise.\n\ncomp_2_obj <- fit_2 %>% \n  emmeans(~ animal, adjust = \"bonferroni\") %>% \n  contrast(method = \"pairwise\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - cat     3.39 0.947 18   3.574  0.0058\n dog - fox    -1.03 0.947 18  -1.086  0.5347\n cat - fox    -4.41 0.947 18  -4.660  0.0005\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nWir können das Ergebnis auch noch mit der Funktion tidy() weiter aufräumen und dann die Spalten selektieren, die wir brauchen. Häufig benötigen wir nicht alle Spalten, die eine Funktion wiedergibt.\n\nres_2_obj <- comp_2_obj %>% \n  tidy(conf.int = TRUE) %>% \n  select(contrast, estimate, adj.p.value, conf.low, conf.high) %>% \n  mutate(across(where(is.numeric), round, 4))\n\nres_2_obj\n\n# A tibble: 3 x 5\n  contrast  estimate adj.p.value conf.low conf.high\n  <chr>        <dbl>       <dbl>    <dbl>     <dbl>\n1 dog - cat     3.39      0.0058    0.968      5.80\n2 dog - fox    -1.03      0.535    -3.45       1.39\n3 cat - fox    -4.41      0.0005   -6.83      -2.00\n\n\nAbschließend wollen wir noch die 95% Konfidenzintervalle in Abbildung 31.4 abbilden. Hier ist es bei emmeans genauso wie bei multcomp. Wir können das Objekt res_2_obj direkt in ggplot() weiterverwenden und uns die 95% Konfidenzintervalle einmal plotten.\n\n  ggplot(res_2_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.4— Die 95% Konfidenzintervalle für den allpair-Vergleich des simplen Datensatzes.\n\n\n\n\nWir wollen uns noch einen etwas komplizierteren Fall anschauen, indem sich emmeans von multcomp in der Anwendung unterscheidet. Wir laden den Datensatz flea_dog_cat_fox_site.csv in dem wir zwei Faktoren haben. Damit können wir dann ein Modell mit einem Interaktionsterm bauen. Wir erinnern uns, dass wir in der zweifaktoriellen ANOAV eine signifikante Interaktion zwischen den beiden Faktoren animal und site festgestelt hatten.\n\nfac2_tbl <- read_csv2(\"data/flea_dog_cat_fox_site.csv\") %>% \n  select(animal, site, jump_length) %>% \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 31.2 nochmal dargestellt.\n\n\n\n\nTabelle 31.2— Selektierter Datensatz mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\nanimal\nsite\njump_length\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.1\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\n…\n…\n…\n\n\nfox\nfield\n16.38\n\n\nfox\nfield\n14.59\n\n\nfox\nfield\n14.03\n\n\nfox\nfield\n13.63\n\n\nfox\nfield\n14.09\n\n\nfox\nfield\n15.52\n\n\n\n\n\n\nIn Abbildung 31.5 sehen wir die Ergebnisse des multiplen Vergleiches nochmal anders als Pairwise P-value plot dargestellt. Wir haben auf der y-Achse zu Abwechselung mal die Gruppen dargestellt und auf der x-Achse die \\(p\\)-Werte. In den Kästchen sind die Effekte der Gruppen nochmal gezeigt. In unserem Fall die Mittelwerte der Sprungweiten für die drei Gruppen. Wir sehen jetzt immer den \\(p\\)-Wert für den jeweiligen Vergleich durch eine farbige Linie miteinander verbunden. So können wir nochmal eine andere Übersicht über das Ergebnis des multiplen Vergleich kriegen.\n\nfit_2 %>% \n  emmeans(~ animal) %>% \n  pwpp(adjust = \"bonferroni\") +\n  theme_bw()\n\n\n\nAbbildung 31.5— Visualisierung der Ergebnisse im Pairwise P-value plot.\n\n\n\n\nAuch haben wir die Möglichkeit un die \\(p\\)-Werte mit der Funktion pwpm() als eine Matrix ausgeben zu lassen. Wir erhalten in dem oberen Triangel die \\(p\\)-Wert für den jeweiligen Vergleich. In dem unteren Triangel die geschätzten Mittelwertsunterschiede. Auf der Diagonalen dann die geschätzten Mittelwerte für die jeweilige Gruppe. So haben wir nochmal alles sehr kompakt zusammen dargestellt.\n\nfit_2 %>% \n  emmeans(~ animal) %>% \n  pwpm(adjust = \"bonferroni\")\n\n       dog    cat    fox\ndog [8.13] 0.0065 0.8756\ncat   3.39 [4.74] 0.0006\nfox  -1.03  -4.41 [9.16]\n\nRow and column labels: animal\nUpper triangle: P values   adjust = \"bonferroni\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later\n\n\nIn Abbildung 31.6 sehen wir nochmal die Daten visualisiert. Wichtig ist hier, dass wir zwei Faktoren vorliegen haben. Den Faktor animal und den Faktor site. Dabei ist der Faktor animal in dem Faktor site genested. Wir messen jedes Level des Faktors animal jeweils in jedem Level des Faktors site.\n\n\n\n\nAbbildung 31.6— Boxplot der Sprungweiten [cm] von Hunden und Katzen gemessen an verschiedenen Orten.\n\n\n\n\nWir rechnen ein multiples lineares Modell mit einem Interaktionsterm. Daher packen wir beide Faktoren in das Modell sowie die Intraktion zwischen den beiden Faktoren. Wir erhalten nach dem fitten des Modells das Objekt fit_3.\n\nfit_3 <- lm(jump_length ~ animal + site + animal:site, data = fac2_tbl)\n\nDer Unterschied zu unserem vorherigen multiplen Vergleich ist nun, dass wir auch einen multiplen Vergleich für animal nested in site rechnen können. Dafür müssen wir den Vergleich in der Form animal | site schreiben. Wir erhalten dann die Vergleiche der Level des faktors animal getrennt für die Level es Faktors site.\n\ncomp_3_obj <- fit_3 %>% \n  emmeans(~ animal | site, adjust = \"bonferroni\") %>% \n  contrast(method = \"pairwise\") \n\ncomp_3_obj\n\nsite = city:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -3.101 0.771 108  -4.022  0.0003\n cat - fox   -6.538 0.771 108  -8.479  <.0001\n dog - fox   -3.437 0.771 108  -4.457  0.0001\n\nsite = smalltown:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -4.308 0.771 108  -5.587  <.0001\n cat - fox   -4.064 0.771 108  -5.271  <.0001\n dog - fox    0.244 0.771 108   0.316  0.9463\n\nsite = village:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -1.316 0.771 108  -1.707  0.2073\n cat - fox   -1.729 0.771 108  -2.242  0.0687\n dog - fox   -0.413 0.771 108  -0.536  0.8540\n\nsite = field:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -0.982 0.771 108  -1.274  0.4131\n cat - fox    1.366 0.771 108   1.772  0.1840\n dog - fox    2.348 0.771 108   3.045  0.0082\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nWir können uns das Ergebnis auch etwas schöner ausgeben lassen. Wir nutzen hier noch die Funktion format.pval() um die \\(p\\)-Werte besser zu formatieren. Die \\(p\\)-Wert, die kleiner sind als 0.001 werden als <0.001 ausgegeben und die anderen \\(p\\)-Werte auf zwei Nachstellen nach dem Komma gerundet.\n\ncomp_3_obj %>% \n  summary %>% \n  as_tibble %>% \n  select(contrast, site, p.value) %>% \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2))\n\n# A tibble: 12 x 3\n   contrast  site      p.value\n   <fct>     <fct>     <chr>  \n 1 cat - dog city      <0.001 \n 2 cat - fox city      <0.001 \n 3 dog - fox city      <0.001 \n 4 cat - dog smalltown <0.001 \n 5 cat - fox smalltown <0.001 \n 6 dog - fox smalltown 0.95   \n 7 cat - dog village   0.21   \n 8 cat - fox village   0.07   \n 9 dog - fox village   0.85   \n10 cat - dog field     0.41   \n11 cat - fox field     0.18   \n12 dog - fox field     0.01   \n\n\nIn der Ausgabe können wir erkennen, dass die Vergleich in der Stadt alle signifkant sind. Jedoch erkennen wir keine signifikanten Ergebnisse mehr in dem Dorf und im Feld ist nur der Vergleich dog - fox signifkant. Hier solltest du nochmal beachten, warum wir die Analyse getrennt machen. In der zweifaktoriellen ANOVA haben wir gesehen, dass ein signifkanter Interaktionsterm zwischen den beiden Faktoren animal und site vorliegt.\nWir wollen uns noch über die Funktion confint() die 95% Konfidenzintervalle wiedergeben lassen.\n\nres_3_obj <- comp_3_obj %>% \n  confint() %>% \n  as_tibble() %>% \n  select(contrast, site, estimate, conf.low = lower.CL, conf.high = upper.CL) \n\nres_3_obj\n\n# A tibble: 12 x 5\n   contrast  site      estimate conf.low conf.high\n   <fct>     <fct>        <dbl>    <dbl>     <dbl>\n 1 cat - dog city        -3.10    -4.93     -1.27 \n 2 cat - fox city        -6.54    -8.37     -4.71 \n 3 dog - fox city        -3.44    -5.27     -1.60 \n 4 cat - dog smalltown   -4.31    -6.14     -2.48 \n 5 cat - fox smalltown   -4.06    -5.90     -2.23 \n 6 dog - fox smalltown    0.244   -1.59      2.08 \n 7 cat - dog village     -1.32    -3.15      0.516\n 8 cat - fox village     -1.73    -3.56      0.103\n 9 dog - fox village     -0.413   -2.25      1.42 \n10 cat - dog field       -0.982   -2.81      0.850\n11 cat - fox field        1.37    -0.466     3.20 \n12 dog - fox field        2.35     0.516     4.18 \n\n\nBesonders mit den 95% Konfiendezintervallen sehen wir nochmal den Interaktionseffekt zwischen den beiden Faktoren animal und site. So dreht sich der Effekt von zum Beispiel dog - fox von \\(-3.44\\) in dem Level city zu \\(+2.35\\) in dem Level field. Wir haben eine Interaktion vorliegen und deshalb die Analyse getrennt für jeden Level des Faktors site durchgeführt.\nAbbildung 31.7 zeigt die entsprechenden 95% Konfidenzintervalle. Wir müssen hier etwas mit der position spielen, so dass die Punkte und der geom_errorbar richtig liegen.\n\n  ggplot(res_3_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high,\n                        color = site, group = site)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    scale_color_okabeito() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.7— Die 95% Konfidenzintervalle für den allpair-Vergleich des Models mit Interaktionseffekt."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-ght",
    "href": "stat-tests-posthoc.html#sec-posthoc-ght",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.6 Gruppenvergleich mit dem Games-Howell-Test",
    "text": "31.6 Gruppenvergleich mit dem Games-Howell-Test\nDer Games-Howell-Test ist eine Alternative zu dem Paket multcomp und dem Paket emmeans. Wir nutzen den Games-Howell-Test, wenn die Annahme der Homogenität der Varianzen, der zum Vergleich aller möglichen Kombinationen von Gruppenunterschieden verwendet wird, verletzt ist. Dieser Post-Hoc-Test liefert Konfidenzintervalle für die Unterschiede zwischen den Gruppenmitteln und zeigt, ob die Unterschiede statistisch signifikant sind. Der Test basiert auf der Welch’schen Freiheitsgradkorrektur und adjustiert die \\(p\\)-Werte. Der Test vergleicht also die Differenz zwischen den einzelnen Mittelwertpaaren mit einer Adjustierung für den Mehrfachtest. Es besteht also keine Notwendigkeit, zusätzliche p-Wert-Korrekturen vorzunehmen. Mit dem Games-Howell-Test ist nur ein all-pair Vergleich möglich.\nFür den Games-Howell-Test aus dem Paket rstatix müssen wir kein lineares Modell fitten. Wir schreiben einfach die wie in einem t-Test das Outcome und den Faktor mit den Gruppenleveln in die Funktion games_howell_test(). Wir erhalten dann direkt das Ergebnis des Games-Howell-Test. Wir nutzen in diesem Beispiel die Daten aus dem Objekt fac1_tbl zu sehen in Tabelle 31.1.\n\nfit_4 <- games_howell_test(jump_length ~ animal, data = fac1_tbl) \n\nWir wollen aber nicht mit der Ausgabe arbeiten sondern machen uns noch ein wenig Arbeit und passen die Ausgabe an. Zum einen brauchen wir noch die Kontraste und wir wollen die \\(p\\)-Werte auch ansprechend formatieren. Wir erhalten das Objekt res_4_obj und geben uns die Ausgabe wieder.\n\nres_4_obj <- fit_4 %>% \n  as_tibble %>% \n  mutate(contrast = str_c(group1, \"-\", group2)) %>% \n  select(contrast, estimate, p.adj, conf.low, conf.high) %>% \n  mutate(p.adj = format.pval(p.adj, eps = 0.001, digits = 2))\n\nres_4_obj\n\n# A tibble: 3 x 5\n  contrast estimate p.adj conf.low conf.high\n  <chr>       <dbl> <chr>    <dbl>     <dbl>\n1 dog-cat     -3.39 0.02     -6.28    -0.490\n2 dog-fox      1.03 0.52     -1.52     3.57 \n3 cat-fox      4.41 0.00      2.12     6.71 \n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt, den haben wir uns selber mit der Funktion mutate() und str_c() aus den Spalten group1 und group2 gebaut. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir dog-cat rechnen. Also wirklich den Mittelwert der Sprungweite der Hundeflöhe minus den Mittelwert der Sprungweite der Katzenflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleiche dog - cat mit einem p-Wert von \\(0.02\\) sowie für den Vergleich \\(cat - fox\\) mit einem p-Wert von \\(0.00\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nIn Abbildung 31.8 sind die simultanen 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab.\n\n  ggplot(res_4_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.8— Die 95% Konfidenzintervalle für den allpair-Vergleich des Games-Howell-Test.\n\n\n\n\nDie Entscheidungen nach den 95% Konfidenzintervallen sind die gleichen wie nach dem \\(p\\)-Wert. Da wir hier es mit einem Mittelwertsvergleich zu tun haben, ist die Entscheidung gegen die Nullhypothese zu treffen wenn die 0 im Konfidenzintervall ist."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-compact-letter",
    "href": "stat-tests-posthoc.html#sec-compact-letter",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.7 Compact letter display",
    "text": "31.7 Compact letter display\nIn der Pflanzenernährung ist es nicht unüblich sehr viele Substrate miteinander zu vergleichen. Oder andersherum, wenn wir sehr viele Gruppen haben, dann kann die Darstellung in einem all-pair Vergleich sehr schnell sehr unübersichltich werden. Deshalb wure das compact letter display entwickelt.\n\n\nCompact Letter Display (CLD) - What is it?. Das compact letter display zeigt an, bei welchen Vergleichen der Behandlungen die Nullhypothese gilt. Daher werden die nicht signifikanten Ergebnisse visualisiert.\nSchauen wir uns aber zurerst einmal ein größeres Beispiel mit neun Behandlungen mit jeweils zwanzig Beobachtungen an. Wir erstellen uns den Datensatz in der Form, dass sich die Mittelwerte für die Behandlungen teilweise unterscheiden.\n\nset.seed(20220914)\ndata_tbl <- tibble(trt = gl(n = 9, k = 20, \n                            labels = c(\"pos_crtl\", \"neg_ctrl\", \"treat_A\", \"treat_B\", \n                                       \"treat_C\", \"treat_D\", \"treat_E\", \"treat_F\", \n                                       \"treat_G\")),\n                   rsp = c(rnorm(20, 10, 5), rnorm(20, 20, 5), rnorm(20, 22, 5), rnorm(20, 24, 5),\n                           rnorm(20, 35, 5), rnorm(20, 37, 5), rnorm(20, 40, 5), rnorm(20, 43, 5),\n                           rnorm(20, 50, 5)))\n\nIn der Abbildung 31.9 ist der Datensatz data_tbl nochmal als Boxplot dargestellt.\n\n\n\n\nAbbildung 31.9— Boxplot der Beispieldaten.\n\n\n\n\nWir sehen, dass sich die positive Kontrolle von dem Rest der Behandlungen unterscheidet. Danach haben wir ein Plateau mit der negativen Kontrolle und der Behanldung A und der Behandlung B. Nach diesem Plateau haben wir einen Sprung und sehen einen leicht linearen Anstieg der Mittelwerte der Behandlungen.\nSchauen wir uns zuerst einmal an, wie ein compact letter display aussehen würde, wenn kein Effekt vorliegen würde. Daher die Nullhypothese ist wahr und die Mittelwerte der Gruppen unterscheiden sich nicht. Wir nutzen hier einmal ein kleineres Beispiel mit den Behandlungslevels ctrl, treat_A und treat_B. Alle drei Behandlungslevel haben einen Mittelwert von 10. Es gilt die Nullhypothese und wir erhalten folgendes compact letter display in Tabelle 31.3.\n\n\nTabelle 31.3— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt, es gibt keinen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\\(\\phantom{a}\\)\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n10\na\n\n\n\n\ntreat_B\n10\na\n\n\n\n\n\n\nDas Gegenteil sehen wir in der Tabelle 31.4. Hier haben wir ein compact letter display wo sich alle drei Mittelwerte mit 10, 15 und 20 voneinander klar unterscheiden. Die Nullhypothese gilt für keinen der möglichen paarweisen Vergleiche.\n\n\nTabelle 31.4— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt nicht, es gibt einen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n15\n\nb\n\n\n\ntreat_B\n20\n\\(\\phantom{a}\\)\n\nc\n\n\n\n\nSchauen wir uns nun die Implementierung des compact letter display für die verschiedenen Möglichkeiten der Multiplen Vergleiche einmal an.\n\n31.7.1 … für pairwise.*.test()\n\nWenn wir für die Funktionen pairwise.*.test() das compact letter display berechnen wollen, dann müssen wir etwas ausholen. Denn wir müssen dafür die Funktion multcompLetters() nutzen. Diese Funktion braucht die \\(p\\)-Werte als Matrix und diese Matrix der \\(p\\)-Werte kriegen wir über die Funktion fullPTable(). Am Ende haben wir aber dann das was wir wollten. Ich habe hier nochmal das einfache Beispiel mit den Sprungweiten von oben genommen.\n\npairwise.t.test(fac1_tbl$jump_length, fac1_tbl$animal,\n                p.adjust.method = \"bonferroni\") %>% \n  extract2(\"p.value\") %>% \n  fullPTable() %>% \n  multcompLetters()\n\ndog cat fox \n\"a\" \"b\" \"a\" \n\n\nAls Ergebnis erhalten wir, dass Hund- und Fuchsflöhe gleich weit springen, beide teilen sich den gleichen Buchstaben. Katzenflöhe springen unterschiedlich zu Hunden- und Fuchsflöhen. Das Vorgehen ändert sich dann nicht, wenn wir eine andere Funktion wie pairwise.wilcox.test() nehmen.\n\n31.7.2 … für das Paket multcomp\n\nWir schauen uns zuerst einmal die Implementierung des compact letter display in dem Paket multcomp an. Wir nutzen die Funktion multcompLetters() aus dem Paket multcompView um uns das compact letter display wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Das heist wir müssen aus der Funktion glht() die adjustierten \\(p\\)-Werte extrahieren und dann einen Vektor der Vergleiche bzw. Kontraste in der Form A-B bauen. Also ohne Leerzeichen und in der Beschreibung der Level der Behandlung trt. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nmultcomp_cld <- lm(rsp  ~ trt, data = data_tbl) %>%\n  glht(linfct = mcp(trt = \"Tukey\")) %>% \n  tidy %>% \n  mutate(contrast = str_replace_all(contrast, \"\\\\s\", \"\")) %>% \n  pull(adj.p.value, contrast) %>% \n  multcompLetters() \n\nWir erhalten dann folgendes compact letter display für die paarweisen Vergleiche aus multcomp.\n\nmultcomp_cld \n\nneg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F  treat_G \n     \"a\"      \"a\"      \"a\"      \"b\"     \"bc\"     \"cd\"      \"d\"      \"e\" \npos_crtl \n     \"f\" \n\n\nLeider sind diese Buchstaben in dieser Form schwer zu verstehen. Deshalb gibt es noch die Funktion plot() in dem Paket multcompView um uns die Buchstaben mit den Leveln der Behandlung einmal ausgeben zu lassen. Wir erhalten dann folgende Abbildung.\n\nmultcomp_cld %>% plot\n\n\n\n\nIn dem compact letter display bedeuten gleiche Buchstaben, dass die Behandlungen gleich sind. Es gilt die Nullhypothese für diesen Vergleich.\nWas sehen wir hier? Kombinieren wir einmal das compact letter display mit den Leveln der Behandlung und den Mittelwerten der Behandlungen in einer Tabelle 31.5. Wenn die Mittelwerte gleich sind, dann erhalten die Behandlungslevel den gleichen Buchstaben. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein a. Ebenso sind die MIttelwerte von treat_C und treat_D nahezu gleich, dehalb erhalten beide ein b. Das machen wir immer so weiter und konzentrieren uns also auf die nicht signifikanten Ergebnisse. Denn gleiche Buchstaben bedeuten, dass die Behandlungen gleich sind. Wir sehen hier also, bei welchen Vergleichen die Nullhypothese gilt.\n\n\nTabelle 31.5— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus multcomp. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\nneg_ctrl\n20\na\n\n\n\n\n\n\n\ntreat_A\n22\na\n\n\n\n\n\n\n\ntreat_B\n24\na\n\n\n\n\n\n\n\ntreat_C\n35\n\nb\n\n\n\n\n\n\ntreat_D\n37\n\nb\nc\n\n\n\n\n\ntreat_E\n40\n\n\nc\nd\n\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\npos_crtl\n10\n\n\n\n\n\nf\n\n\n\n\nWir können dann die Buchstaben auch in den Boxplot ergaänzen. Die y-Position kann je nach Belieben dann noch angepasst werden. zum Beispiel könnten hier auch die Mittelwerte aus einer summarise() Funktion ergänzt werden und so die y-Position angepasst werden.\n\nletters_tbl <- multcomp_cld$Letters %>% \n  enframe(\"trt\", \"label\") %>% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 31.10— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n\n31.7.3 … für das Paket emmeans\n\nIn dem Paket emmeans ist das compact letter display ebenfalls implementiert und wir müssen nicht die Funktion multcompLetters() nutzen. Durch die direkte Implementierung ist es etwas einfacher sich das compact letter display anzeigen zu lassen. Das Problem ist dann später sich die Buchstaben zu extrahieren um die Abbildung 31.11 zu ergänzen. Wir nutzen in emmeans die Funktion cld() um das compact letter display zu erstellen.\n\nemmeans_cld <- lm(rsp  ~ trt, data = data_tbl) %>%\n  emmeans(~ trt) %>%\n  cld(Letters = letters, adjust = \"bonferroni\")\n\nWir erhalten dann die etwas besser sortierte Ausgabe für die Behandlungen wieder.\n\nemmeans_cld \n\n trt      emmean   SE  df lower.CL upper.CL .group \n pos_crtl   9.67 1.12 171     6.51     12.8  a     \n neg_ctrl  20.02 1.12 171    16.86     23.2   b    \n treat_A   20.97 1.12 171    17.81     24.1   b    \n treat_B   23.35 1.12 171    20.19     26.5   b    \n treat_C   34.96 1.12 171    31.80     38.1    c   \n treat_D   37.46 1.12 171    34.30     40.6    cd  \n treat_E   40.17 1.12 171    37.01     43.3     de \n treat_F   43.23 1.12 171    40.07     46.4      e \n treat_G   50.51 1.12 171    47.35     53.7       f\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 9 estimates \nP value adjustment: bonferroni method for 36 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping letter,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWie die Ausgabe von cld() richtig anmerkt, können compact letter display irreführend sein weil sie eben Nicht-Unterschiede anstatt von signifikanten Unterschieden anzeigen. Zum Anderen sehen wir aber auch, dass wir 36 statistische Tests gerechnet haben und somit zu einem Signifikanzniveau von \\(\\cfrac{\\alpha}{k} = \\cfrac{0.05}{36} \\approx 0.0014\\) testen. Wir brauchen also schon sehr große Unterschiede oder aber eine sehr kleine Streuung um hier signifikante Effekte nachweisen zu können.\nIn Tabelle 31.6 sehen wir das Ergebnis des compact letter display nochmal mit den Mittelwerten der Behandlungslevel zusammen dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. Wir gehen so alle Vergleiche einmal durch.\n\n\nTabelle 31.6— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus emmeans. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\n\ntreat_E\n40\n\n\n\nd\ne\n\n\n\ntreat_F\n43\n\n\n\n\ne\n\n\n\ntreat_G\n45\n\n\n\n\n\nf\n\n\n\n\nAbschließend können wir die Buchstaben aus dem compact letter display noch in die Abbildung 31.11 ergänzen. Hier müssen wir etwas mehr machen um die Buchstaben aus dem Objekt emmeans_cld zu bekommen. Du kannst dann noch die y-Position anpassen wenn du möchtest.\n\nletters_tbl <- emmeans_cld %>% \n  tidy %>% \n  select(trt, label = .group) %>% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 31.11— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n\n31.7.4 … für den Games-Howell-Test\nAbschließend wollen wir uns die Implementierung des compact letter display für den Games-Howell-Test einmal anschauen. Es gilt vieles von dem in diesem Abschnitt schon gesagtes. Wir nutzen die Funktion multcompLetters() aus dem Paket multcompView um uns das compact letter display aus dem Games-Howell-Test wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nght_cld <- games_howell_test(rsp ~ trt, data = data_tbl) %>% \n  mutate(contrast = str_c(group1, \"-\", group2)) %>% \n  pull(p.adj, contrast) %>% \n  multcompLetters() \n\nDas compact letter display kennen wir schon aus der obigen Beschreibung.\n\nght_cld\n\npos_crtl neg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F \n     \"a\"      \"b\"      \"b\"      \"b\"      \"c\"     \"cd\"      \"d\"      \"d\" \n treat_G \n     \"e\" \n\n\nWir können uns dann auch das compact letter display als übersichtlicheren Plot wiedergeben lassen.\n\nght_cld %>% plot\n\n\n\n\nUm die Zusammenhänge besser zu verstehen ist in Tabelle 31.7 nochmal die Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. In der Form können wir alle Vergleiche einmal durchgehen.\n\n\nTabelle 31.7— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus dem Games-Howell-Test. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\ntreat_E\n40\n\n\n\nd\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\n\nWir können dann auch in Abbildung 31.12 sehen, wie das compact letter display mit den Boxplots verbunden wird.\n\nletters_tbl <- ght_cld$Letters %>% \n  enframe(\"trt\", \"label\") %>% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 31.12— Boxplot der Beispieldaten zusammen mit den compact letter display."
  },
  {
    "objectID": "stat-modeling-sensitivity.html",
    "href": "stat-modeling-sensitivity.html",
    "title": "39  Sensitivitätsanalyse",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:48:37"
  },
  {
    "objectID": "stat-modeling-sensitivity.html#theoretischer-hintergrund",
    "href": "stat-modeling-sensitivity.html#theoretischer-hintergrund",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.1 Theoretischer Hintergrund",
    "text": "39.1 Theoretischer Hintergrund\nWir brauchen die Sensitivitätsanalyse wenn wir Beobachtungen aus unseren Daten entfernt oder aber hinzugefügt haben. Das heißt du hast entweder eine Variablenselektion wie im Kapitel 36 beschrieben durchgeführt. Oder aber du hast fehlende Werte wie in Kapitel 38 beschrieben imputiert. Es kann auch sein, dass du Ausreißer aus den Daten entfernt oder aber imputiert hast, wie es in Kapitel 36 beschrieben ist. Im Prinzip kannst du auch alles drei gemacht haben, aber meistens beschränkt sich die Veränderung der Daten nur auf eins der drei Möglichkeiten.\nWie immer brauchen wir natürlich auch Fallzahl. Eine Sensitivitätsanalyse kannst du nicht auf zwanzig bis fünfzig Beobachtungen machen. Du brauchst schon eine gute dreistellige Anzahl, damit du hier sauber Modellieren und Darstellen kannst. Wenn du weniger Beobachtungen hast, dann ist ganz natürlich das einzelne Werte einen riesigen Einfluss haben müssen. Im Zweifel frag einfach einmal bei mir nach, dann können wir die Sachlage diskutieren.\n\n\n\n\n\n\nDas ist hier natürlich eine Sensitivitätsanalyse für Arme. Wie man es richtig umfangreich macht, findest du in einem sehr gutem und umfangreichen Tutorial zu What Makes a Sensitivity Analysis?\nDieses Kapitel ist relativ übersichtlich. Wir werden die Modelle nach der jeweiligen algorithmischen Veränderung uns nochmal anschauen und dann deskriptive entscheiden, ob wir eine große Veränderung in den Daten sehen. Es gibt zwar auch die Möglichkeit die Modelle untereinander zu vergleichen, aber ist hier die Aussagekraft nicht so stark. Die Idee hinter dem Modellvergleich ist eher die Anzahl an Spalten zu verändern und nicht die Werte in der Datenmatrix. Deshalb machen wir es zwar, genießen die Sache aber mit Vorsicht."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-sensitivity.html#genutzte-r-pakete-für-das-kapitel",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.2 Genutzte R Pakete für das Kapitel",
    "text": "39.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, dlookr, broom, modelsummary,\n               see, performance, ggpubr, factoextra, FactoMineR,\n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#daten",
    "href": "stat-modeling-sensitivity.html#daten",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.3 Daten",
    "text": "39.3 Daten\nIn diesem Beispiel betrachten wir wieder die Gummibärchendaten. Auch hier haben wir echte Daten vorliegen, so dass wir Ausreißer entdecken könnten. Da wir hier auch fehlende Werte in den Daten haben, können wir diese fehlenden Werte auch einfach imputieren und uns dann die Effekte anschauen. Das heißt wir haben also einen idealen Datensatz für unsere Sensitivitätsanalysen.\n\ngummi_tbl <- read_excel(\"data/gummibears.xlsx\")  %>%\n  select(gender, age, height, semester) %>% \n  mutate(gender = as_factor(gender)) \n\nIn der Tabelle 36.2 ist der Datensatz gummi_tbl nochmal für die ersten sieben Zeilen dargestellt. Wir werden später sehen, wie sich die Fallzahl von \\(n = 428\\) immer wieder ändert, je nachdem wie wir mit den fehlenden Daten und den Variablen umgehen.\n\n\n\n\nTabelle 39.1— Auszug aus dem Datensatz gummi_tbl. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\ngender\nage\nheight\nsemester\n\n\n\nm\n35\n193\n10\n\n\nw\n21\n159\n6\n\n\nw\n21\n159\n6\n\n\nw\n36\n180\n10\n\n\nm\n22\n180\n3\n\n\nm\nNA\nNA\nNA\n\n\nm\n22\n180\n3"
  },
  {
    "objectID": "stat-modeling-sensitivity.html#das-modell",
    "href": "stat-modeling-sensitivity.html#das-modell",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.4 Das Modell",
    "text": "39.4 Das Modell\nWir wollen jetzt als erstes das volle Modell schätzen. Das heißt wir packen alle Variablen in das Modell und rechnen dann die lineare Regression. Wir wollen herausfinden in wie weit das Alter, das Geschlecht und das Semester einen Einfluss auf die Körpergröße von Studierenden hat.\n\\[\nheight \\sim gender + age + semester\n\\]\nWir haben nichts an den Daten geändert und somit dient unser volles Modell als Benchmark für die anderen. Wenn sich einige Werte der Modellgüten im Vergleich zum vollen Modell ändern, dann wissen wir, dass etwas nicht stimmt.\n\nfit_full <- lm(height ~ gender + age + semester, data = gummi_tbl)\n\nNeben dem vollen Modell rechnen wir auch noch das Nullmodel. Das Nullmodell beinhaltet nur den Intercept und sonst keine Einflussvariable. Wir wollen schauen, ob es überhaupt was bringt eine unserer Variablen in das Modell zu nehmen oder ob wir es auch gleich lassen können. Im Prinzip unsere Kontrolle für das Modellieren.\n\\[\nheight \\sim 1\n\\]\nIn R fitten wir das Nullmodell in dem wir keine Variablen mit in das Modell nehmen sondern nur eine 1 schreiben. Wir haben dann nur den Intercept mit in dem Modell und sonst nichts. Was wir schon aus den anderen Kapiteln wissen ist, dass das Nullmodell ein schlechtes Modell sein wird.\n\nfit_null <- lm(height ~ 1, data = gummi_tbl)\n\nWir schauen uns die Modelle hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-detektion-von-ausreißer",
    "href": "stat-modeling-sensitivity.html#nach-der-detektion-von-ausreißer",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.5 Nach der Detektion von Ausreißer",
    "text": "39.5 Nach der Detektion von Ausreißer\nTeilweise können wir eine Überprüfung auf Ausreißer nur auf einen Datensatz ohne fehlende Werte durchführen. Hier beißt sich dann die Katze in den Schwanz. Deshalb nutzen wir die Funktion diagnose_outlier(), die intern die fehlenden Werte entfernt. Das ist natürlich kein richtiges Vorgehen! Aber wir nutzen ja diesen Abschnitt nur als Beispiel.\nDu findest die Detektion von Ausreißern im Kapitel 36 beschrieben.\n\ndiagnose_outlier(gummi_tbl) \n\n# A tibble: 3 x 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean without_mean\n  <chr>            <int>          <dbl>         <dbl>     <dbl>        <dbl>\n1 age                 30           7.01          42.3     24.1         22.6 \n2 height               0           0            NaN      176.         176.  \n3 semester            24           5.61          10.3      3.19         2.71\n\n\nWir sehen, dass wir in der Variable age und semester nach der Funktion zu urteilen Ausreißer gefunden haben. Deshalb werden wir jetzt diese Ausreißer durch die Funktion imputate_outlier() entsprechend ersetzen. Mal schauen, ob wir damit eine substanzielle Änderung in der Modellierung erhalten.\n\ngummi_out_imp_tbl <- gummi_tbl %>% \n  mutate(age = imputate_outlier(., age, method = \"capping\"),\n         semester = imputate_outlier(., semester, method = \"capping\"))\n\nNun modellieren wir noch mit unseren ersetzten und angepassten Daten die Körpergröße und erhalten den Modellfit zurück. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.\n\nfit_outlier <- lm(height ~ gender + age + semester, data = gummi_out_imp_tbl)\n\nWir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-imputation-von-fehlenden-werten",
    "href": "stat-modeling-sensitivity.html#nach-der-imputation-von-fehlenden-werten",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.6 Nach der Imputation von fehlenden Werten",
    "text": "39.6 Nach der Imputation von fehlenden Werten\nNehmen wir wieder den Gummibärechendatensatz von neuen und imputieren diesmal die fehlenden Werte mit einer univariaten Imputation. Wir machen uns hier nicht die Mühe ein multivariates Verfahren zu nutzen. Das könnte man tun, aber wir wollen hier ja nur den Weg aufzeigen, wie wir den Vergleich der Modelle zur Sensitivitätsanalyse durchführen.\nDu findest die Imputation von fehlenden Werten im Kapitel 38 beschrieben.\nIn unserem Fall imputieren wir alle numerischen Variablen mit dem Mittelwert und die kategoriale Variable mit der Methode rpart. Damit haben wir dann keine fehlenden Werte mehr in den Daten und somit sollte das jetzt auch unserer größter Datensatz für die lineare Regression sein. Nicht vergessen, sobald wir einen fehlenden Wert bei einer Variable in einem Modell haben, fällt die ganze Beobachtung aus dem Modell heraus.\n\ngummi_imp_tbl <- gummi_tbl %>% \n  mutate(age = imputate_na(., age, method = \"mean\"),\n         gender = imputate_na(., gender, method = \"rpart\"),\n         height = imputate_na(., height, method = \"median\"),\n         semester = imputate_na(., semester, method = \"mode\"))\n\nDann rechnen wir noch schnell das Modell für die imputierten Daten. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.\n\nfit_imp <- lm(height ~ gender + age + semester, data = gummi_imp_tbl)\n\nWir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-variablen-selektion",
    "href": "stat-modeling-sensitivity.html#nach-der-variablen-selektion",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.7 Nach der Variablen Selektion",
    "text": "39.7 Nach der Variablen Selektion\nFür die Variablensleketion machen wir es uns sehr einfach. Wir müssen ja nur eine Spalte aus den Daten werfen, mehr ist ja Variablenselektion auch nicht. Wir machen dort nur eine algorithmengetriebene Auswahl. In diesem Fall entscheide ich einfach zufällig welche Variable aus dem Modell muss.\nDu findest die Variablen Selektion im Kapitel 36 beschrieben.\nSomit nehmen wir an, wir hätten eine Variablenselektion durchgeführt und die Variable semester aus dem Modell entfernt.\n\nfit_var_select <- lm(height ~ gender + age, data = gummi_tbl)\n\nAuch dieses Modell schauen wir nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#modellvergleich",
    "href": "stat-modeling-sensitivity.html#modellvergleich",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.8 Modellvergleich",
    "text": "39.8 Modellvergleich\nKommen wir zu dem eigentlichen Modellvergleich. In Tabelle 39.2 sehen wir den Modellvergleich aller fünf Modelle aus diesem Kapitel. Dazu nutzen wir die Funktion modelsummary() aus dem R Paket modelsummary. Wir vergleichen die Modelle untereinander aber vor allem mit dem vollen Modell. Das volle Modell basiert ja auf den ursprünglichen nicht veränderten Daten. Den Intercept können wir erstmal ignorieren. Spannend ist, dass sich der Effekt von gender auf die Körpergröße durch die Imputation um eine Einheit ändert. Der Effekt des Alters verfünffacht sich durch die Outlieranpassung und verdoppelt sich durch die Imputation. Durch die Imputation wird der Effekt des Semesters abgeschwächt.\nWenn wir auf das \\(R^2_{adj}\\) schauen, dann haben wir eine Verschlechterung durch die Imputation. Sonst bleibt der Wert mehr oder minder konstant. Das ist ein gutes Zeichen, dass wir unser Modell nicht vollkommen an die Wand gefahren haben durch unsere Änderung der Daten. Das \\(AIC\\) wird folglich für die Imputationsdaten sehr viel schlechter und nähert sich dem Nullmodell an. Das ist wirklcih kein gutes Zeichen für die Imputation. Da haben wir mehr kaputt als heile gemacht. Wir sehen keinen Efdekt bei dem Fehler \\(RMSE\\), der noch nach dem Fit des Modell übrig bleibt. Aber das kann passieren. Nicht jede Maßzahl muss sich auch ändern. Deshalb haben wir ja mehrere Maßzahlen vorliegen.\n\nmodelsummary(lst(\"Null Modell\" = fit_null,\n                 \"Volles Modell\" = fit_full,\n                 \"Outlier\" = fit_outlier,\n                 \"Imputation\" = fit_imp,\n                 \"Variablen Selektion\" = fit_var_select),\n             estimate  = \"{estimate}\",\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"))\n\n\n\n\nTabelle 39.2—  Modellvergleich mit den fünf Modellen. Wir schauen in wie weit sich die Koeffizienten und Modelgüten für die einzelnen Modelle im direkten Vergleich zum vollen Modell verändert haben. \n \n   \n    Null Modell \n    Volles Modell \n    Outlier \n    Imputation \n    Variablen Selektion \n  \n\n\n (Intercept) \n    175.639 \n    184.147 \n    185.109 \n    183.071 \n    183.622 \n  \n\n  \n    [174.621, 176.658] \n    [180.683, 187.612] \n    [180.859, 189.360] \n    [180.122, 186.019] \n    [180.795, 186.449] \n  \n\n  \n    s.e. = 0.518 \n    s.e. = 1.762 \n    s.e. = 2.162 \n    s.e. = 1.500 \n    s.e. = 1.438 \n  \n\n  \n    t = 339.055 \n    t = 104.532 \n    t = 85.635 \n    t = 122.030 \n    t = 127.688 \n  \n\n  \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n  \n\n genderw \n     \n    -14.896 \n    -14.858 \n    -13.782 \n    -14.786 \n  \n\n  \n     \n    [-16.428, -13.364] \n    [-16.384, -13.331] \n    [-15.188, -12.376] \n    [-16.241, -13.331] \n  \n\n  \n     \n    s.e. = 0.779 \n    s.e. = 0.776 \n    s.e. = 0.715 \n    s.e. = 0.740 \n  \n\n  \n     \n    t = -19.120 \n    t = -19.137 \n    t = -19.269 \n    t = -19.974 \n  \n\n  \n     \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n    p = <0.001 \n  \n\n age \n     \n    -0.010 \n    -0.052 \n    -0.022 \n    -0.015 \n  \n\n  \n     \n    [-0.150, 0.130] \n    [-0.231, 0.128] \n    [-0.135, 0.091] \n    [-0.127, 0.098] \n  \n\n  \n     \n    s.e. = 0.071 \n    s.e. = 0.091 \n    s.e. = 0.057 \n    s.e. = 0.057 \n  \n\n  \n     \n    t = -0.140 \n    t = -0.568 \n    t = -0.385 \n    t = -0.256 \n  \n\n  \n     \n    p = 0.889 \n    p = 0.571 \n    p = 0.700 \n    p = 0.798 \n  \n\n semester \n     \n    -0.192 \n    -0.201 \n    -0.082 \n     \n  \n\n  \n     \n    [-0.472, 0.087] \n    [-0.511, 0.110] \n    [-0.347, 0.182] \n     \n  \n\n  \n     \n    s.e. = 0.142 \n    s.e. = 0.158 \n    s.e. = 0.135 \n     \n  \n\n  \n     \n    t = -1.352 \n    t = -1.270 \n    t = -0.612 \n     \n  \n\n  \n     \n    p = 0.177 \n    p = 0.205 \n    p = 0.541 \n     \n  \n\n Num.Obs. \n    402 \n    371 \n    371 \n    428 \n    398 \n  \n\n R2 \n    0.000 \n    0.505 \n    0.505 \n    0.471 \n    0.506 \n  \n\n R2 Adj. \n    0.000 \n    0.501 \n    0.501 \n    0.467 \n    0.504 \n  \n\n AIC \n    3025.6 \n    2544.5 \n    2544.3 \n    2927.9 \n    2720.7 \n  \n\n BIC \n    3033.6 \n    2564.1 \n    2563.8 \n    2948.2 \n    2736.7 \n  \n\n Log.Lik. \n    -1510.792 \n    -1267.252 \n    -1267.128 \n    -1458.932 \n    -1356.371 \n  \n\n F \n     \n    124.678 \n    124.842 \n    125.766 \n     \n  \n\n RMSE \n    10.37 \n    7.37 \n    7.36 \n    7.31 \n    7.31 \n  \n\n\n\n\n\nDas vergleichen von Modellen, die auf unterschiedlichen Daten basieren ist nicht anzuraten. Wir erhalten auch die passende Warnung von der Funktion compare_performance() aus dem R Paket performance. Dennoch hier einmal der Vergleich. Wir sehen, dass die Modelle mit der Ersetzung der Ausreißer und das volle Modell sich stark ähneln. Das selektierte Modell und das imputierte Modell fallen dagegen ab. Da wir ja hier nicht zeigen wollen, dass sich die Modelle unterscheiden, ist das Ergebnis ähnlich zu der Übersicht. Die Imputation hat so nicht funktioniert.\n\n\n# Comparison of Model Performance Indices\n\nName           | Model |    R2 | R2 (adj.) |   RMSE |  Sigma | AIC weights | BIC weights | Performance-Score\n------------------------------------------------------------------------------------------------------------\nfit_outlier    |    lm | 0.505 |     0.501 |  7.363 |  7.403 |       0.531 |       0.531 |            99.21%\nfit_full       |    lm | 0.505 |     0.501 |  7.366 |  7.406 |       0.469 |       0.469 |            95.29%\nfit_var_select |    lm | 0.506 |     0.504 |  7.308 |  7.336 |    2.52e-39 |    1.55e-38 |            66.67%\nfit_imp        |    lm | 0.471 |     0.467 |  7.314 |  7.348 |    2.67e-84 |    1.86e-84 |            64.19%\nfit_null       |    lm | 0.000 |     0.000 | 10.373 | 10.386 |   1.61e-105 |   5.28e-103 |             0.00%\n\n\nWas ist das Fazit aus der Sensitivitätsanalyse für Arme? Nun wir konnten einmal sehen, dass wir auch mit einfachen Werkzeugen Modelle deskriptiv miteinander vergleichen können und dann einen Schluss über die Güte der Detektion von Ausreißern, der Imputation von fehlenden Werten oder aber der Variablenselektion treffen können. Denk immer dran, die Sensitivitätsanalyse findet nach einer sauberen Detektion, Imputation oder Selektion statt und soll nochmal sicherstellen, dass wir nicht künstliche Effekte der Algorithmen modellieren sondern die Effekte in den Daten sehen.\nSensitivitätsanalysen finden eigentlich in dem Kontext von klinischen Studien statt. Der Trend geht aber natürlich auch nicht an den Agrarwissenschaften vorbei und solltest du den Begriff mal hören, weist du wo Sensitivitätsanalyse hingehören."
  },
  {
    "objectID": "stat-modeling-multinom.html",
    "href": "stat-modeling-multinom.html",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "",
    "text": "Version vom November 14, 2022 um 13:49:02\nWas machen wir wenn wir ein Outcome haben mit mehr als zwei Kategorien. Wenn wir nur zwei Kategorien hätten, dann würden wir eine logistische Regression rechnen. Wenn wir mehr als zwei Kategorien haben, dann sind wir in dem Fall der multinomialen / ordinalen logistischen Regression. Wir rechnen eine multinomialen Regression, wenn wir keine Ordnung in den Kategorien in dem Outcome haben. Wenn wir eine Ordnung vorliegen haben, dann nutzen wir die ordinale Regression. Wir werden uns erstmal eine ordinale Regression anschauen mit nur drei geordenten Stufen. Dann schauen wir uns einmal wie wir eine ordinale Regression auf Boniturnoten in der Likert-Skala rechnen. Wir machen das getrennt, denn wir sind bei wenigen geordneten Kategorien meistens noch am Effekt zwischen den Kategorien interessiert. Im Gegensatz wollen wir bei einem Outcome mit Boniturnoten einen Gruppenvergleich rechnen. Dann interessiert uns der Unterschied und die Effekte zwischen den Boniturnoten nicht. Deshalb trennen wir das hier etwas auf.\nIm zweiten Teil wollen wir uns dann noch eine multinominale Regression auf ungeordneten Kategorien eines Outcomes anschauen. Korrelterweise tuen wir nur so, als wäre unser vorher geordnetes Outcome dann eben ungeordnet. Das macht dann aber bei deiner Anwendung dann keinen großen Unterschied. Als eine Alternative zur multinationalen Regression stelle ich dann noch die logistsiche Regression vor. Wir können nämlich einfach unsere Daten nach dem Outcome jeweils in kleinere Datensätze mit nur jeweils zwei der Kategorien aufspalten. Das ist zwar nicht schön, aber auch eine Möglichkeit mit einem Problem umzugehen.\nIch gehe hier nicht auf die Theorie hinter der multinomialen / ordinalen logistischen Regression ein. Wenn dich dazu mehr interessiert findest du in den jeweiligen Abschnitten dann noch eine passende Referenz. Da kannst du dann schauen, welche Informationen du noch zusätzlich findest."
  },
  {
    "objectID": "stat-modeling-multinom.html#annahmen-an-die-daten",
    "href": "stat-modeling-multinom.html#annahmen-an-die-daten",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.1 Annahmen an die Daten",
    "text": "42.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Multinomialverteilung.\nIm folgenden Kapitel zu der multinomialen / ordinalen logistischen linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 36 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Multinomialverteilung. Damit finden wir im Outcome im Falle der multinomialen logistischen linearen Regression ungeordnete Kategorien und im Falle der ordinalen logistischen linearen Regression geordnete Kategorien.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen."
  },
  {
    "objectID": "stat-modeling-multinom.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-multinom.html#genutzte-r-pakete-für-das-kapitel",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.2 Genutzte R Pakete für das Kapitel",
    "text": "42.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, gtsummary,\n               ordinal, janitor, MASS, nnet, flextable,\n               emmeans, multcomp, ordinal, see, scales,\n               janitor)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-multinom.html#daten",
    "href": "stat-modeling-multinom.html#daten",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.3 Daten",
    "text": "42.3 Daten\nIm Folgenden wollen wir uns die Daten von den infizierten Ferkeln noch einmal anschauen. Wir nehmen als Outcome die Spalte frailty und damit die Gebrechlichkeit der Ferkel. Die Spalte ordnen wir einmal nach robust, pre-frail und frail. Wobei robust ein gesundes Ferkel beschreibt und frail ein gebrechliches Ferkel. Damit wir später die Richtung des Effekts richtig interpretieren können, müssen wir von gut nach schlecht sortieren. Das brauchen wir nicht, wenn wir Boniturnoten haben, dazu mehr in einem eigenen Abschnitt. Wir bauen uns dann noch einen Faktor mit ebenfalls der Spalte frailty in der wir so tun, als gebe es diese Ordnung nicht. Wir werden dann die ordinale Regression mit dem Outcome frailty_ord rechnen und die multinominale Regression dann mit dem Outcome frailty_fac durchführen.\n\npig_tbl <- read_excel(\"data/infected_pigs.xlsx\") %>%\n  mutate(frailty_ord = ordered(frailty, levels = c(\"robust\", \"pre-frail\", \"frail\")),\n         frailty_fac = as_factor(frailty)) %>% \n  select(-infected)\n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der Tabelle 42.1 an.\n\n\n\n\nTabelle 42.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\nfrailty_ord\nfrailty_fac\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n49.88\n16.94\n3.07\nrobust\nrobust\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n58.2\n17.95\n4.88\nrobust\nrobust\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n56.8\n19.02\n3.98\nrobust\nrobust\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.47\n18.98\n5.18\nrobust\nrobust\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n59.85\n16.57\n6.71\nrobust\nrobust\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n58.1\n18.22\n5.43\nrobust\nrobust\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\nrobust\n57.05\n17.95\n6.16\nrobust\nrobust\n\n\n56\nmale\nwest\n13.91\n20.8\npre-frail\n50.84\n18.02\n6.52\npre-frail\npre-frail\n\n\n57\nmale\nnorthwest\n12.49\n21.95\nrobust\n55.51\n17.73\n3.94\nrobust\nrobust\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n58.5\n18.23\n2.73\nrobust\nrobust\n\n\n59\nfemale\nnorth\n13.13\n20.23\npre-frail\n57.33\n17.21\n5.42\npre-frail\npre-frail\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n55.85\n17.76\n6.18\nrobust\nrobust\n\n\n\n\n\n\nDas wären dann die Daten, die wir für unsere Modelle dann brauchen. Schauen wir mal was wir jetzt bei der ordinalen Regression herausbekommen."
  },
  {
    "objectID": "stat-modeling-multinom.html#sec-ordinal",
    "href": "stat-modeling-multinom.html#sec-ordinal",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.4 Ordinale logistische Regression",
    "text": "42.4 Ordinale logistische Regression\nEs gibt sicherlich einiges an Paketen in R um eine ordinale Regression durchzuführen. Ich nutze gerne die Funktion polr aus dem R Paket MASS. Daneben gibt es auch noch das R Paket ordinal mit der Funktion clm(), die wir dann noch im Anschluss besprechen werden. Ich nutze jetzt erstmal die Funktion polr, da wir hier noch eine externe Referenz haben, die uns noch detailliertere Informationen liefern kann.\n\n\nIch verweise gerne hier auf das tolle Tutorium Ordinal Logistic Regression | R Data Analysis Examples. Hier erfährst du noch mehr über die Analyse der ordinalen logistischen Regression.\nWir schon erwähnt sparen wir usn die mathematischen Details und utzen gleich die Funktion polr auf unserem Outcome frailty. Wir müssen keine Verteilungsfamilie extra angeben, dass haben wir schon mit der Auswahl der Funktion getan. Die Funktion polr kann nur eine ordinale Regression rechnen und wird einen Fehler ausgeben, wenn das Outcome \\(y\\) nicht passt.\n\nologit_fit <- polr(frailty_ord ~ age + sex + location + activity + crp + \n                     bloodpressure + weight + creatinin, \n                   data = pig_tbl)\n\nSchauen wir uns einmal die Ausgabe des Modellfits der ordinalen Regression mit der Funktion summary() an. Wir sehen eine Menge Zahlen und das wichtigste für uns ist ja, dass wir zum einen Wissen, dass wir auch die ordinale Regression auf der \\(link\\)-Funktion rechnen. Wir erhalten also wieder eine Transformation des Zusammenhangs zurück, wie wir es schon bei der Poisson Regression sowie bei der logistischen Regression hatten.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\n\nologit_fit %>% summary()\n\nCall:\npolr(formula = frailty_ord ~ age + sex + location + activity + \n    crp + bloodpressure + weight + creatinin, data = pig_tbl)\n\nCoefficients:\n                      Value Std. Error t value\nage                0.005199    0.02171  0.2395\nsexmale            0.500202    0.28214  1.7729\nlocationnortheast -0.315377    0.27927 -1.1293\nlocationnorthwest -0.469579    0.25166 -1.8659\nlocationwest      -0.046641    0.27381 -0.1703\nactivity          -0.130300    0.07409 -1.7587\ncrp               -0.133107    0.06857 -1.9412\nbloodpressure      0.016181    0.03083  0.5249\nweight             0.047346    0.07080  0.6687\ncreatinin         -0.022933    0.07097 -0.3231\n\nIntercepts:\n                 Value   Std. Error t value\nrobust|pre-frail -2.1188  3.0707    -0.6900\npre-frail|frail  -0.3625  3.0694    -0.1181\n\nResidual Deviance: 776.36 \nAIC: 800.36 \n\n\nUnsere Ausgabe teilt sich in zwei Teile auf. In dem oberen Teil sehen wir die Koeffizienten des Modells zusammen mit dem Fehler und der Teststatistik. Was wir nicht sehen, ist ein \\(p\\)-Wert. Die Funktion rechnet uns keinen Signifikanztest aus. Das können wir aber gleich selber machen. In dem Abschnitt Intercepts finden wir die Werte für die Gruppeneinteilung auf der link-Funktion wieder. Wir transformieren ja unsere drei Outcomekategorien in einen kontinuierliche Zahlenzusammenhang. Trotzdem müssen ja die drei Gruppen auch wieder auftauchen. In dem Abschnitt Intercepts finden wir die Grenzen für die drei Gruppen auf der link-Funktion.\n\n\nWir gibt auch ein Tutorial für How do I interpret the coefficients in an ordinal logistic regression in R?\nBerechnen wir jetzt einmal die \\(p\\)-Werte per Hand. Dafür brauchen wir die absoluten Werte aus der t value Spalte aus der summary des Modellobjekts. Leider ist die Spalte nicht schön formatiert und so müssen wir uns etwas strecken um die Koeffizienten sauber aufzuarbeiten. Wir erhalten dann das Objekt coef_tbl wieder.\n\ncoef_tbl <- summary(ologit_fit) %>% \n  coef %>% \n  as_tibble(rownames = \"term\") %>% \n  clean_names() %>% \n  mutate(t_value = abs(t_value))\n\ncoef_tbl\n\n# A tibble: 12 x 4\n   term                 value std_error t_value\n   <chr>                <dbl>     <dbl>   <dbl>\n 1 age                0.00520    0.0217   0.239\n 2 sexmale            0.500      0.282    1.77 \n 3 locationnortheast -0.315      0.279    1.13 \n 4 locationnorthwest -0.470      0.252    1.87 \n 5 locationwest      -0.0466     0.274    0.170\n 6 activity          -0.130      0.0741   1.76 \n 7 crp               -0.133      0.0686   1.94 \n 8 bloodpressure      0.0162     0.0308   0.525\n 9 weight             0.0473     0.0708   0.669\n10 creatinin         -0.0229     0.0710   0.323\n11 robust|pre-frail  -2.12       3.07     0.690\n12 pre-frail|frail   -0.363      3.07     0.118\n\n\nUm die Fläche rechts von dem \\(t\\)-Wert zu berechnen, können wir zwei Funktionen nutzen. Die Funktion pnorm() nimmt eine Standradnormalverteilung an und die Funktion pt() vergleicht zu einer \\(t\\)-Verteilung. Wenn wir rechts von der Verteilung schauen wollen, dann müssen wir die Option lower.tail = FALSE wählen. Da wir auch zweiseitig statistisch Testen, müssen wir den ausgerechneten \\(p\\)-Wert mal zwei nehmen. Hier einmal als Beispiel für den \\(t\\)-Wert von \\(1.96\\). Mit pnorm(1.96, lower.tail = FALSE) * 2 erhalten wir \\(0.05\\) als Ausgabe. Das ist unser \\(p\\)-Wert. Was uns ja nicht weiter überrascht. Denn rechts neben dem Wert von \\(1.96\\) in einer Standardnormalverteilung ist ja \\(0.05\\). Wenn wir einen \\(t\\)-Test rechnen würden, dann müssten wir noch die Freiheitsgrade df mit angeben. Mit steigendem \\(n\\) nähert sich die \\(t\\)-Verteilung der Standardnormalverteilung an. Wir haben mehr als \\(n = 400\\) Beobachtungen, daher können wir auch df = 400 setzen. Da kommt es auf eine Zahl nicht an. Wir erhalten mit pt(1.96, lower.tail = FALSE, df = 400) * 2 dann eine Ausgabe von \\(0.0507\\). Also fast den gleichen \\(p\\)-Wert.\nIm Folgenden setzte ich die Freiheitsgrade df = 3 dammit wir was sehen. Bei so hohen Fallzahlen wir in unserem beispiel würden wir sonst keine Unterschiede sehen.\n\ncoef_tbl %>% \n  mutate(p_n = pnorm(t_value, lower.tail = FALSE) * 2,\n         p_t = pt(t_value, lower.tail = FALSE, df = 3) * 2) %>% \n  mutate(across(where(is.numeric), round, 3))\n\n# A tibble: 12 x 6\n   term               value std_error t_value   p_n   p_t\n   <chr>              <dbl>     <dbl>   <dbl> <dbl> <dbl>\n 1 age                0.005     0.022   0.239 0.811 0.826\n 2 sexmale            0.5       0.282   1.77  0.076 0.174\n 3 locationnortheast -0.315     0.279   1.13  0.259 0.341\n 4 locationnorthwest -0.47      0.252   1.87  0.062 0.159\n 5 locationwest      -0.047     0.274   0.17  0.865 0.876\n 6 activity          -0.13      0.074   1.76  0.079 0.177\n 7 crp               -0.133     0.069   1.94  0.052 0.148\n 8 bloodpressure      0.016     0.031   0.525 0.6   0.636\n 9 weight             0.047     0.071   0.669 0.504 0.552\n10 creatinin         -0.023     0.071   0.323 0.747 0.768\n11 robust|pre-frail  -2.12      3.07    0.69  0.49  0.54 \n12 pre-frail|frail   -0.363     3.07    0.118 0.906 0.913\n\n\nDamit haben wir einmal händisch uns die \\(p\\)-Werte ausgerechnet. Jetzt könnte man sagen, dass ist ja etwas mühselig. Gibt es da nicht auch einen einfacheren Weg? Ja wir können zum einen die Funktion tidy() nutzen um die 95% Konfidenzintervalle und die exponierten Effektschätzer aus der ordinalen Regresssion zu erhalten. Wir erhalten aber wieder keine \\(p\\)-Werte sondern müssten uns diese \\(p\\)- Werte dann wieder selber berechnen.\n\nologit_fit %>% \n  tidy(conf.int = TRUE, exponentiate = TRUE) %>% \n  select(-coef.type)\n\n# A tibble: 12 x 6\n   term              estimate std.error statistic conf.low conf.high\n   <chr>                <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n 1 age                  1.01     0.0217     0.239    0.963      1.05\n 2 sexmale              1.65     0.282      1.77     0.952      2.88\n 3 locationnortheast    0.730    0.279     -1.13     0.420      1.26\n 4 locationnorthwest    0.625    0.252     -1.87     0.381      1.02\n 5 locationwest         0.954    0.274     -0.170    0.557      1.63\n 6 activity             0.878    0.0741    -1.76     0.758      1.01\n 7 crp                  0.875    0.0686    -1.94     0.765      1.00\n 8 bloodpressure        1.02     0.0308     0.525    0.957      1.08\n 9 weight               1.05     0.0708     0.669    0.913      1.21\n10 creatinin            0.977    0.0710    -0.323    0.850      1.12\n11 robust|pre-frail     0.120    3.07      -0.690   NA         NA   \n12 pre-frail|frail      0.696    3.07      -0.118   NA         NA   \n\n\nUm all dieses Berechnen zu umgehen, können wir dann auch die Funktion model_parameters() nutzen. Hier berechnen wir dann die \\(p\\)-Wert mit \\(df = 400\\) aus einer \\(t\\)-Verteilung. Damit umgehen wir das Problem, dass unser Modellfit keine \\(p\\)-Werte liefert.\n\nologit_fit %>% \n  model_parameters() \n\n# alpha\n\nParameter        | Log-Odds |   SE |        95% CI | t(400) |     p\n-------------------------------------------------------------------\nrobust|pre-frail |    -2.12 | 3.07 | [-8.16, 3.92] |  -0.69 | 0.491\npre-frail|frail  |    -0.36 | 3.07 | [-6.40, 5.67] |  -0.12 | 0.906\n\n# beta\n\nParameter            | Log-Odds |   SE |        95% CI | t(400) |     p\n-----------------------------------------------------------------------\nage                  | 5.20e-03 | 0.02 | [-0.04, 0.05] |   0.24 | 0.811\nsex [male]           |     0.50 | 0.28 | [-0.05, 1.06] |   1.77 | 0.077\nlocation [northeast] |    -0.32 | 0.28 | [-0.87, 0.23] |  -1.13 | 0.259\nlocation [northwest] |    -0.47 | 0.25 | [-0.97, 0.02] |  -1.87 | 0.063\nlocation [west]      |    -0.05 | 0.27 | [-0.59, 0.49] |  -0.17 | 0.865\nactivity             |    -0.13 | 0.07 | [-0.28, 0.01] |  -1.76 | 0.079\ncrp                  |    -0.13 | 0.07 | [-0.27, 0.00] |  -1.94 | 0.053\nbloodpressure        |     0.02 | 0.03 | [-0.04, 0.08] |   0.52 | 0.600\nweight               |     0.05 | 0.07 | [-0.09, 0.19] |   0.67 | 0.504\ncreatinin            |    -0.02 | 0.07 | [-0.16, 0.12] |  -0.32 | 0.747\n\n\nIn Tabelle 42.2 sehen wir nochmal die Ergebnisse der ordinalen Regression einmal anders aufgearbeitet. Wir aber schon bei der Funktion tidy() fehlen in der Tabelle die \\(p\\)-Werte. Wir können aber natürlich auch eine Entscheidung über die 95% Konfidenzintervalle treffen. Wenn die 1 mit im 95% Konfidenzintervall ist, dann können wir die Nullhypothese nicht ablehnen.\n\nologit_fit %>% \n  tbl_regression(exponentiate = TRUE) %>% \n  as_flex_table()\n\n\n\n\n\n\nTabelle 42.2—  Tabelle der Ergebnisse der ordinalen Regression. \n\nCharacteristic\nOR1\n95% CI1\n\n\n\nage\n1.01\n0.96, 1.05\n\n\nsex\n\n\n\n\nfemale\n—\n—\n\n\nmale\n1.65\n0.95, 2.88\n\n\nlocation\n\n\n\n\nnorth\n—\n—\n\n\nnortheast\n0.73\n0.42, 1.26\n\n\nnorthwest\n0.63\n0.38, 1.02\n\n\nwest\n0.95\n0.56, 1.63\n\n\nactivity\n0.88\n0.76, 1.01\n\n\ncrp\n0.88\n0.76, 1.00\n\n\nbloodpressure\n1.02\n0.96, 1.08\n\n\nweight\n1.05\n0.91, 1.21\n\n\ncreatinin\n0.98\n0.85, 1.12\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nWi es im gazen Kapitel schon durchscheint, die Interpreation der \\(OR\\) aus einer ordinalen Regression ist nicht einfach, geschweige den intuitiv. Was wir haben ist der Trend. Wir haben unser Outcome von robust zu frail sortiert und damit von gut nach schlecht. Wir können so die Richtung der Variablen in unserem Modell interpretieren. Das heißt, dass männliche Ferkel eher von einer Gebrechlichkeit betroffen sind als weibliche Ferkel. Oder wir sagen, dass ein ansteigender CRP Wert führt zu weniger Gebrechlichkeit. Auf diesem Niveau lassen sich die \\(OR\\) einer ordinalen Regression gut interpretieren."
  },
  {
    "objectID": "stat-modeling-multinom.html#cumulative-link-models-clm-für-ordinale-daten",
    "href": "stat-modeling-multinom.html#cumulative-link-models-clm-für-ordinale-daten",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.5 Cumulative Link Models (CLM) für ordinale Daten",
    "text": "42.5 Cumulative Link Models (CLM) für ordinale Daten\nJetzt kommen wir nochmal zu dem Fall, dass wir Boniturdaten vorliegen habe. Das heißt, wir bauen uns flux wieder einen Datensatz mit drei Blöcken mit jeweils drei Wiederholungen. Wir haben Weizen angepflanzt und bonitieren die Weizenpflanzen nach der Likert Skala. Dabei bedeutet dann eine 1 ein schlechte Note und eine 9 die best mögliche Note. Wir hätten natürlich hier auch einen Kurskal-Wallis-Test rechnen können und dann im Anschluss einen paarweisen Wilcoxon Test. Nun modellieren wir hier aber die Boniturnoten mal mit einer ordinalen Regression und rechnen den anschließenden Gruppenvergleich dann mit dem R Paket emmeans.\n\n\nWir finden auch ein Tutorial zu Introduction to Cumulative Link Models (CLM) for Ordinal Data.\nUnser Datensatz grade_tbl enthält den Faktor block mit drei Levels sowie den Faktor variety mit fünf Leveln. Jedes Level repränsentiert dabei eine Weizensorte.\n\ngrade_tbl <- tibble(block = rep(c(\"I\", \"II\", \"III\"), each = 3),\n                    A = c(2,3,4,3,3,2,4,2,1),\n                    B = c(7,9,8,9,7,8,9,6,7),\n                    C = c(6,5,5,7,5,6,4,7,6),\n                    D = c(2,3,1,2,1,1,2,2,1),\n                    E = c(4,3,7,5,6,4,5,7,5)) %>%\n  gather(key = variety, value = grade, A:E) %>% \n  mutate(grade_ord = ordered(grade))\n\nWir schauen uns nochmal den Datensatz an und sehen, dass wir einmal die Spalte grade als numerische Spalte vorliegen haben udn einmal als geordneten Faktor. Wir brauchen die numerische Spalte um die Daten besser in ggplot() darstellen zu können.\n\ngrade_tbl\n\n# A tibble: 45 x 4\n   block variety grade grade_ord\n   <chr> <chr>   <dbl> <ord>    \n 1 I     A           2 2        \n 2 I     A           3 3        \n 3 I     A           4 4        \n 4 II    A           3 3        \n 5 II    A           3 3        \n 6 II    A           2 2        \n 7 III   A           4 4        \n 8 III   A           2 2        \n 9 III   A           1 1        \n10 I     B           7 7        \n# ... with 35 more rows\n\n\nIn Abbildung 42.1 sehen wir einmal die Daten als Dotplot dargestellt. Auf der x-Achse sind die Weizensorten und auf der y-Achse die Boniturnoten. Ich habe noch die zusätzlichen Linien für jede einzelne Note mit eingezeichnet.\n\nggplot(grade_tbl, aes(variety, grade, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.6), dotsize = 0.75) +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9)) +\n  scale_fill_okabeito() \n\n\n\nAbbildung 42.1— Dotplot des Datenbeispiels für die Bonitur von fünf Weizensorten.\n\n\n\n\nJetzt können wir schon die Funktion clm() aus dem R Paket ordinal verwenden um die ordinale Regression zu rechnen. Wir haben in dem Paket ordinal noch weitere Modelle zu Verfügung mit denen wir auch komplexere Designs bis hin zu linearen gemischten Modellen für eine ordinale Regresssion rechnen können. Da wir mit Boniturnoten als Outcome arbeiten setzen wir auch die Option threshold = \"symmetric\". Damit teilen wir der Funktion clm() mit, dass wir es mit einer symmetrischen Notenskala zu tun haben. Wenn du das nicht hast, dass kannst du die Option auch of \"flexible\" stellen. Dann wird eine nicht symmetrische Verteilung des Outcomes angenommen.\n\nclm_fit <- clm(grade_ord ~ variety + block, data = grade_tbl,\n               threshold = \"symmetric\")\n\nWir können uns dann den Fit des Modells wieder in der Funktion model_parameters() einmal anschauen. Wir brauchen aber die Ausgabe nicht weiter. Wir werden Koeffizienten des Modells jetzt verwenden um die Gruppenvergleiche zu rechnen. Ich habe hier einmal die \\(OR\\) über die Option exponentiate = TRUE mir ausgeben lassen. Leider sind die \\(OR\\) so alleine nicht zu interpretieren.\n\nclm_fit %>% \n  model_parameters(exponentiate = TRUE)\n\n# Intercept\n\nParameter | Odds Ratio |     SE |          95% CI |    z |      p\n-----------------------------------------------------------------\ncentral 1 |      22.02 |  20.78 | [ 3.47, 139.98] | 3.28 | 0.001 \ncentral 2 |     101.42 | 109.71 | [12.17, 845.18] | 4.27 | < .001\nspacing 1 |       4.30 |   1.89 | [ 1.82,  10.16] | 3.33 | < .001\nspacing 2 |      35.04 |  25.42 | [ 8.46, 145.23] | 4.90 | < .001\nspacing 3 |     169.31 | 142.84 | [32.40, 884.76] | 6.08 | < .001\n\n# Location Parameters\n\nParameter   | Odds Ratio |      SE |             95% CI |     z |      p\n------------------------------------------------------------------------\nvariety [B] |    5917.41 | 9556.45 | [249.73, 1.40e+05] |  5.38 | < .001\nvariety [C] |     137.83 |  161.11 | [ 13.94,  1362.41] |  4.21 | < .001\nvariety [D] |       0.14 |    0.13 | [  0.02,     0.87] | -2.11 | 0.035 \nvariety [E] |      57.06 |   62.60 | [  6.64,   490.03] |  3.69 | < .001\nblock [II]  |       0.95 |    0.64 | [  0.26,     3.52] | -0.07 | 0.942 \nblock [III] |       0.89 |    0.63 | [  0.22,     3.56] | -0.17 | 0.865 \n\n\nEs ist auch möglich auf dem Modellfit eine ANOVA zu rechnen. Wir machen das hier einmal, aber wir erwaten natürlich einen signifikanten Effekt von der Sorte. Die Signifikanz konnten wir ja schon oben im Dortplot sehen.\n\nanova(clm_fit)\n\nType I Analysis of Deviance Table with Wald chi-square tests\n\n        Df   Chisq Pr(>Chisq)    \nvariety  4 37.9927  1.124e-07 ***\nblock    2  0.0292     0.9855    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIch hatte ja eben geschrieben, dass wir die Effektschätzer nicht nutzen. Wir können mit den \\(OR\\) aus dem Modell nichts anfangen. Stattdessen nutzen wir die Schätzer der link-Funktion, also die Log Odds, um den paarweisen Gruppenvergleich zu rechnen. Wir nutzen dafür die Funktion emmeans() und lassen uns das compact letter display über die Funktion cld() wiedergeben. Ich habe dann noch die Ausgabe einmal nach den Sorten sortiert und nicht nach dem compact letter display. Dadurch lassen sich die Buchstaben besser zum Dotplot vergleichen.\n\nclm_fit %>% \n  emmeans(~ variety) %>% \n  cld(Letters = letters) %>% \n  arrange(variety)\n\n variety emmean    SE  df asymp.LCL asymp.UCL .group\n A       -3.912 0.887 Inf    -5.650     -2.17  a    \n B        4.774 0.966 Inf     2.881      6.67    c  \n C        1.014 0.623 Inf    -0.206      2.23   b   \n D       -5.848 1.010 Inf    -7.828     -3.87  a    \n E        0.132 0.639 Inf    -1.120      1.38   b   \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 5 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping letter,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWir erinnern uns, gleiche Buchstaben heißen kein Unterschied zwischen den Weizensorten. Damit ist die Weizensorte A gleich der Weizensorte D. Die Weizensorte C ist gleich der Weizensorte E. Am Ende unterscheidet sich die Weizensorte B von allen anderen Sorten. Wir können aber die Werte in der Spalte emmean nicht als Notenunterschied interpretieren. Dafür müssen wir dann den Median für die Gruppen berechnen. Wir können dann das compact letter display händisch ergänzen.\n\ngrade_tbl %>% \n  group_by(variety) %>% \n  summarise(median(grade)) %>% \n  mutate(cld = c(\"a\", \"  c\", \" b \", \"a  \", \" b \"))\n\n# A tibble: 5 x 3\n  variety `median(grade)` cld  \n  <chr>             <dbl> <chr>\n1 A                     3 \"a\"  \n2 B                     8 \"  c\"\n3 C                     6 \" b \"\n4 D                     2 \"a  \"\n5 E                     5 \" b \"\n\n\nDu könntest dir auch die Buchstaben des compact letter display aus einem Objekt ziehen und nicht händisch übertragen. Ich habe mir hier aber einen Schritt gespart."
  },
  {
    "objectID": "stat-modeling-multinom.html#sec-multinom",
    "href": "stat-modeling-multinom.html#sec-multinom",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.6 Multinomiale logistische Regression",
    "text": "42.6 Multinomiale logistische Regression\nWas machen wir in eine multinomialen logistische Regression? Im Gegensatz zu der ordinalen Regression haben wir in der multinominalen Regression keine Ordnung in unserem Outcome. Das macht die Sache dann schon eine Nummer komplizierter. Und wir lösen dieses Problem indem wir ein Level des Outcomes oder eben eine Kategorie des Outcomes als Referenz definieren. Dann haben wir wieder unsere Ordnung drin. Und die Definition der Referenz ist auch manchmal das schwerste Unterfangen. Wenn ich keine Ordnung in meinem Outcome habe, wie soll ich dann die Referenz bestimmen? Aber das ist dann immer eine Frage an den konkreten Datensatz. Hier basteln wir uns ja die Fragestellung so hin, dass es passt.\n\n\nIch verweise gerne hier auf das tolle Tutorium Multinomial Logistic Regression | R Data Analysis Examples. Hier erfährst du noch mehr über die Analyse der multinominale logistischen Regression.\nUm eine Referenz in dem Outcome zu definieren nutzen wir die Funktion relevel() und setzen als unsere Referenz das Level frail aus unserem Outcome frailty. Wir hätten auch jedes andere Level als Referenz nehmen können. Zu dieser Referenz werden wir jetzt unser Modell anpassen. Ich nehme immer als Referenz das schlechteste im Sinne von nicht gut. In unserem Fall ist das eben das Level frail.\n\npig_tbl <- pig_tbl %>% \n  mutate(frailty_fac = relevel(frailty_fac, ref = \"frail\"))\n\nNachdem wir unsere Referenz definiert haben, können wir wieder recht einfach mit der Funktion multinom() aus dem Paket nnet die multinominalen Regression rechnen. Ich mache keinen Hehl daraus. Ich mag die Funktion nicht, da die Ausgabe der Funktion sehr unsortiert ist und uns nicht gerade die Arbeit erleichtert. Auch schweigt die Funktion nicht, sondern muss immer eine Ausgabe wiedergeben. Finde ich sehr unschön.\n\nmultinom_fit <- multinom(frailty_fac ~ age + sex + location + activity + crp + bloodpressure + weight + creatinin, \n                         data = pig_tbl)\n\n# weights:  36 (22 variable)\ninitial  value 452.628263 \niter  10 value 401.130508\niter  20 value 381.498488\niter  30 value 380.694398\nfinal  value 380.689300 \nconverged\n\n\nDie Standardausgabe von multinom() hat wiederum keine \\(p\\)-Werte und wir könnten uns über die Funktion pnorm() wiederum aus den \\(t\\)-Werten unsere \\(p\\)-Werte berechnen. Leider erspart sich multinom() selbst den Schritt die \\(t\\)-Werte zu berechnen, so dass wir die \\(t\\)-Werte selber berechnen müssen. Nicht das es ein Problem wäre, aber schön ist das alles nicht. Im Folgenden siehst du dann einmal die Berechnung der \\(p\\)-Werte über die Berechnung der Teststatistik.\n\nz_mat <- summary(multinom_fit)$coefficients/summary(multinom_fit)$standard.errors\np_n <- (1 - pnorm(abs(z_mat), 0, 1)) * 2\np_n\n\n          (Intercept)        age   sexmale locationnortheast locationnorthwest\nrobust      0.4775726 0.34659718 0.1394380         0.4904806         0.1320405\npre-frail   0.5263984 0.05534541 0.6300418         0.7989195         0.6659467\n          locationwest  activity        crp bloodpressure    weight  creatinin\nrobust       0.8648791 0.4539323 0.03667107     0.2398979 0.9118323 0.12787728\npre-frail    0.9714730 0.4295601 0.20119551     0.1570281 0.5565101 0.02860694\n\n\nJetzt müssten wir diese \\(pp\\)-Werte aus der Matrix noch mit unseren Koeffizienten verbauen und da hört es dann bei mir auf. Insbesondere da wir ja mit model_parameters() eine Funktion haben, die uns in diesem Fall wirklich gut helfen kann. Wir nehmen hier zwar die \\(t\\)-Verteilung an und haben damit leicht höre \\(p\\)-Werte, aber da wir eine so große Anzahl an Beobachtungen haben, fällt dieser Unterschied nicht ins Gewicht.\n\nmultinom_fit %>% model_parameters(exponentiate = TRUE)\n\n# Response level: robust\n\nParameter            | Odds Ratio |   SE |         95% CI | t(390) |     p\n--------------------------------------------------------------------------\n(Intercept)          |       0.03 | 0.16 | [0.00, 433.73] |  -0.71 | 0.478\nage                  |       1.03 | 0.04 | [0.96,   1.11] |   0.94 | 0.347\nsex [male]           |       0.50 | 0.23 | [0.20,   1.25] |  -1.48 | 0.140\nlocation [northeast] |       1.35 | 0.59 | [0.57,   3.18] |   0.69 | 0.491\nlocation [northwest] |       1.88 | 0.79 | [0.82,   4.27] |   1.51 | 0.133\nlocation [west]      |       1.08 | 0.46 | [0.46,   2.51] |   0.17 | 0.865\nactivity             |       1.09 | 0.13 | [0.86,   1.39] |   0.75 | 0.454\ncrp                  |       1.26 | 0.14 | [1.01,   1.56] |   2.09 | 0.037\nbloodpressure        |       0.94 | 0.05 | [0.86,   1.04] |  -1.18 | 0.241\nweight               |       0.99 | 0.11 | [0.79,   1.23] |  -0.11 | 0.912\ncreatinin            |       1.20 | 0.14 | [0.95,   1.50] |   1.52 | 0.129\n\n# Response level: pre-frail\n\nParameter            | Odds Ratio |   SE |         95% CI | t(390) |     p\n--------------------------------------------------------------------------\n(Intercept)          |       0.04 | 0.20 | [0.00, 945.94] |  -0.63 | 0.527\nage                  |       1.07 | 0.04 | [1.00,   1.16] |   1.92 | 0.056\nsex [male]           |       0.79 | 0.39 | [0.30,   2.08] |  -0.48 | 0.630\nlocation [northeast] |       0.89 | 0.42 | [0.35,   2.24] |  -0.25 | 0.799\nlocation [northwest] |       1.21 | 0.54 | [0.51,   2.90] |   0.43 | 0.666\nlocation [west]      |       1.02 | 0.46 | [0.42,   2.48] |   0.04 | 0.971\nactivity             |       0.90 | 0.12 | [0.70,   1.16] |  -0.79 | 0.430\ncrp                  |       1.16 | 0.14 | [0.92,   1.46] |   1.28 | 0.202\nbloodpressure        |       0.93 | 0.05 | [0.84,   1.03] |  -1.42 | 0.158\nweight               |       1.07 | 0.13 | [0.85,   1.36] |   0.59 | 0.557\ncreatinin            |       1.31 | 0.16 | [1.03,   1.68] |   2.19 | 0.029\n\n\nWas sehen wir? Zuerst haben wir etwas Glück. Den unsere Referenzlevel macht dann doch Sinn. Wir vergleichen ja das Outcomelevel robust zu frail und das Outcomelevel pre-frail zu frail. Dann haben wir noch das Glück, dass durch unsere Ordnung dann auch frail das schlechtere Outcome ist, so dass wir die \\(OR\\) als Risiko oder als protektiv interpretieren können. Nehmen wir als Beispiel einmal die Variable crp. Der CRP Wert höht das Risiko für frail. Das macht schonmal so Sinn. Und zum anderen ist der Effekt bei dem Vergleich von pre-frail zu frail mit \\(1.16\\) nicht so große wie bei robust zu frail mit \\(1.26\\). Das macht auch Sinn. Deshalb passt es hier einigermaßen.\nIn Tabelle 42.3 sehen wir nochmal die Ausgabe von einer multinominalen Regression durch die Funktion tbl_regression() aufgearbeitet.\n\nmultinom_fit %>% \n  tbl_regression(exponentiate = TRUE) %>% \n  as_flex_table()\n\n\n\n\n\n\nTabelle 42.3—  Tabelle der Ergebnisse der multinominalen Regression. \n\nOutcome\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\nrobust\nage\n1.03\n0.96, 1.11\n0.3\n\n\n\nsex\n\n\n\n\n\n\nfemale\n—\n—\n\n\n\n\nmale\n0.50\n0.20, 1.25\n0.14\n\n\n\nlocation\n\n\n\n\n\n\nnorth\n—\n—\n\n\n\n\nnortheast\n1.35\n0.58, 3.17\n0.5\n\n\n\nnorthwest\n1.88\n0.83, 4.26\n0.13\n\n\n\nwest\n1.08\n0.46, 2.51\n0.9\n\n\n\nactivity\n1.09\n0.86, 1.39\n0.5\n\n\n\ncrp\n1.26\n1.01, 1.56\n0.037\n\n\n\nbloodpressure\n0.94\n0.86, 1.04\n0.2\n\n\n\nweight\n0.99\n0.79, 1.23\n>0.9\n\n\n\ncreatinin\n1.20\n0.95, 1.50\n0.13\n\n\npre-frail\nage\n1.07\n1.00, 1.16\n0.055\n\n\n\nsex\n\n\n\n\n\n\nfemale\n—\n—\n\n\n\n\nmale\n0.79\n0.30, 2.07\n0.6\n\n\n\nlocation\n\n\n\n\n\n\nnorth\n—\n—\n\n\n\n\nnortheast\n0.89\n0.35, 2.23\n0.8\n\n\n\nnorthwest\n1.21\n0.51, 2.90\n0.7\n\n\n\nwest\n1.02\n0.42, 2.47\n>0.9\n\n\n\nactivity\n0.90\n0.70, 1.16\n0.4\n\n\n\ncrp\n1.16\n0.92, 1.46\n0.2\n\n\n\nbloodpressure\n0.93\n0.84, 1.03\n0.2\n\n\n\nweight\n1.07\n0.85, 1.36\n0.6\n\n\n\ncreatinin\n1.31\n1.03, 1.68\n0.029\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nLeider wird die Sache mit einer multinominalen Regression sehr unangenehm, wenn wir wirklich nicht sortierbare Level im Outcome haben. Dann haben wir aber noch ein Möglichkeit der multinominalen Regression zu entkommen. Wir rechnen einfach separate logistische Regressionen. Die logistischen Regressionen können wir dann ja separat gut interpretieren."
  },
  {
    "objectID": "stat-modeling-multinom.html#logistische-regression-als-ausweg",
    "href": "stat-modeling-multinom.html#logistische-regression-als-ausweg",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.7 Logistische Regression als Ausweg",
    "text": "42.7 Logistische Regression als Ausweg\n\n\n\n\n\n\nBitte Beachten bei der Berechung über separate logistische Regressionen\n\n\n\nDurch die Verwendung von separaten logistischen Regressionen vermindern wir die Fallzahl je gerechneter Regression, so dass wir größere \\(p\\)-Werte erhalten werden als in einer multinominalen Regression. Oder andersherum, durch die verminderte Fallzahl in den separaten logistischen Regressionen haben wir eine geringere Power einen signifikanten Unterschied nachzuweisen.\n\n\nEs gibt den einen Ring um sich zu knechten. Und das ist die logistische Regression. Gut die logistische Regression hilft jetzt nicht, wenn es mit Boniturnoten zu tun hast, aber wenn wir wenige Level im Outcome haben. In unserem Fall haben wir ja drei Level vorliegen, da können wir dann jeweils ein Level rausschmeißen und haben dann nur noch ein binäres Outcome. Das ist auch die zentrale Idee. Wir entfernen immer alle Level bis wir nur noch zwei Level in unserem Outcome haben und rechnen für diese beiden Level dann eine logistische Regression.\nSchauen wir uns erstmal an, wie sich die Daten über die drei Kategorien in unserem Outcome verteilen. Wenn wir eine Kategorie im Outcome kaum vorliegen haben, könnten wir diese Daten vielleicht mit einer anderen Kategorie zusammenlegen oder aber müssen von unserer Idee hier Abstand nehmen.\n\npig_tbl$frailty_fac %>% tabyl\n\n         .   n   percent\n     frail  53 0.1286408\n    robust 226 0.5485437\n pre-frail 133 0.3228155\n\n\nWir haben nicht so viele Beobachtungen in der Kategorie frail. Wir könnten also auch die beiden Faktorlevel pre-frail und frail zusammenlegen.\nDas R Paket forcats liefert sehr viele Funktion, die dir helfen Faktoren zu kodieren und zu ändern\n\npig_tbl$frailty_fac %>% \n  fct_recode(frail_pre_frail = \"frail\", frail_pre_frail = \"pre-frail\") %>% \n  tabyl\n\n               .   n   percent\n frail_pre_frail 186 0.4514563\n          robust 226 0.5485437\n\n\nDas ist jetzt aber nur eine Demonstration für die Zusammenlegung. Wir wollen jetzt trotzdem unsere drei logistischen Regressionen rechnen. Warum drei? Wir haben ja drei Level in unserem Outcome und wir werden jetzt uns drei Datensätze so bauen, dass in jdem Datensatz unser Outcome immer nur zwei Level hat. Die einzelnen Datensätze speichern wir dann in einer Liste.\n\npig_lst <- list(robust_prefrail = filter(pig_tbl, frailty_fac %in% c(\"robust\", \"pre-frail\")),\n                robust_frail = filter(pig_tbl, frailty_fac %in% c(\"robust\", \"frail\")),\n                prefrail_frail = filter(pig_tbl, frailty_fac %in% c(\"pre-frail\", \"frail\")))\n\nWir können das auch fancy. Und das demonstriere ich dann mal hier. Wenn wir die Funktion combn() nutzen erhalten wir eine Liste mit allen zweier Kombinationen wieder. Diese Liste können wir dann in die Funktion map() stecken, die dann über die Liste unserer Kombinationen iteriert. Pro Liste filtern map() dann den Datensatz für uns heraus. Ja, ist ein wenig over the top, aber ich wollte das mal für mich mit map() ausprobieren und es passte hier so schön.\n\npig_fancy_lst <- combn(c(\"robust\", \"pre-frail\", \"frail\"), 2, simplify = FALSE) %>% \n  map(~filter(pig_tbl, frailty_fac %in% .x)) \n\nEgal wie du auf die Liste gekommen bist, wir müssen noch die überflüssigen Level droppen. Keine Ahnung was das deutsche Wort ist. Vermutlich ist das deutsche Wort dann entfernen. Dann können wir für jeden der Listeneinträge die logistische Regression rechnen. Am Ende lassen wir uns noch die exponierten Modellfits ausgeben. In der letzten Zeile entferne ich noch den Intercept von der Ausgabe des Modells. Den Intercept brauchen wir nun wirklich nicht.\n\npig_lst %>% \n  map(~mutate(.x, frailty_fac = fct_drop(frailty_fac))) %>% \n  map(~glm(frailty_fac ~ age + sex + location + activity + crp + bloodpressure + weight + creatinin, \n           data = .x, family = binomial)) %>% \n  map(model_parameters, exponentiate = TRUE) %>% \n  map(extract, -1, )\n\n$robust_prefrail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.04 | 0.03 | [0.99, 1.09] |  1.52 | 0.127\nsex [male]           |       1.58 | 0.51 | [0.85, 3.00] |  1.44 | 0.151\nlocation [northeast] |       0.66 | 0.22 | [0.35, 1.25] | -1.26 | 0.209\nlocation [northwest] |       0.65 | 0.19 | [0.36, 1.14] | -1.51 | 0.132\nlocation [west]      |       0.96 | 0.31 | [0.51, 1.80] | -0.13 | 0.895\nactivity             |       0.83 | 0.07 | [0.70, 0.98] | -2.19 | 0.028\ncrp                  |       0.93 | 0.07 | [0.79, 1.08] | -0.95 | 0.342\nbloodpressure        |       0.98 | 0.03 | [0.92, 1.06] | -0.44 | 0.657\nweight               |       1.09 | 0.09 | [0.93, 1.28] |  1.05 | 0.295\ncreatinin            |       1.11 | 0.09 | [0.94, 1.31] |  1.23 | 0.220\n\n$robust_frail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.04 | 0.04 | [0.97, 1.12] |  1.09 | 0.275\nsex [male]           |       0.52 | 0.23 | [0.21, 1.24] | -1.46 | 0.144\nlocation [northeast] |       1.26 | 0.55 | [0.54, 3.05] |  0.52 | 0.603\nlocation [northwest] |       1.94 | 0.82 | [0.86, 4.52] |  1.57 | 0.116\nlocation [west]      |       1.06 | 0.46 | [0.45, 2.53] |  0.13 | 0.900\nactivity             |       1.06 | 0.13 | [0.84, 1.34] |  0.47 | 0.638\ncrp                  |       1.28 | 0.14 | [1.03, 1.60] |  2.17 | 0.030\nbloodpressure        |       0.94 | 0.05 | [0.85, 1.04] | -1.22 | 0.223\nweight               |       1.00 | 0.11 | [0.81, 1.24] |  0.01 | 0.992\ncreatinin            |       1.22 | 0.15 | [0.96, 1.55] |  1.64 | 0.100\n\n$prefrail_frail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.06 | 0.04 | [0.99, 1.14] |  1.61 | 0.108\nsex [male]           |       0.77 | 0.41 | [0.27, 2.19] | -0.48 | 0.632\nlocation [northeast] |       1.04 | 0.50 | [0.41, 2.71] |  0.08 | 0.938\nlocation [northwest] |       1.23 | 0.57 | [0.49, 3.10] |  0.44 | 0.660\nlocation [west]      |       1.08 | 0.51 | [0.44, 2.75] |  0.17 | 0.866\nactivity             |       0.95 | 0.13 | [0.73, 1.23] | -0.39 | 0.695\ncrp                  |       1.15 | 0.13 | [0.91, 1.45] |  1.18 | 0.240\nbloodpressure        |       0.93 | 0.05 | [0.82, 1.04] | -1.34 | 0.182\nweight               |       1.05 | 0.13 | [0.82, 1.36] |  0.42 | 0.673\ncreatinin            |       1.26 | 0.16 | [1.00, 1.63] |  1.87 | 0.061\n\n\nEine Sache ist super wichtig zu wissen. Wie oben schon geschrieben, durch die Verwendung von separaten logistischen Regressionen vermindern wir die Fallzahl je Regression, so dass wir größere \\(p\\)-Werte erhalten werden, als in einer multinominalen Regression. Das ist der Preis, den wir dafür bezahlen müssen, dass wir besser zu interpretierende Koeffizienten erhalten. Und das ist auch vollkommen in Ordnung. Ich selber habe lieber Koeffizienten, die ich interpretieren kann, als unklare Effekte mit niedrigen \\(p\\)-Werten.\nSchauen wir einmal auf unseren Goldstandard, der Variable für den CRP-Wert. Die Variable haben wir ja jetzt immer mal wieder in diesem Kapitel interpretiert und uns angeschaut. Die Variable crp passt von dem Effekt jedenfalls gut in den Kontext mit rein. Die Effekte sind ähnlich wie in der multinominalen Regression. Wir haben eben nur größere \\(p\\)-Werte. Jetzt müssen wir entscheiden, wir können vermutlich die getrennten logistischen Regressionen besser beschreiben und interpretieren. Das ist besonders der Fall, wenn wir wirklich Probleme haben eine Referenz in der multinominalen Regression festzulegen. Dann würde ich immer zu den getrennten logistischen Regressionen greifen als eine schief interpretierte multinominale Regression."
  },
  {
    "objectID": "stat-modeling-multinom.html#referenzen",
    "href": "stat-modeling-multinom.html#referenzen",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "stat-modeling-logistic.html",
    "href": "stat-modeling-logistic.html",
    "title": "43  Logistische Regression",
    "section": "",
    "text": "Version vom November 14, 2022 um 19:51:46\nDie logistische Regression ist die Regression, wenn wir rüber in die Medizin schauen. Wohl in keinem Bereich der Wissenschaften wird so viel eine logistische Regression gerechnet wie in der Humanmedizin, Epidemiologie oder Pharmazie. Wir haben in der logistischen Regression ein \\(0/1\\) Outcome als \\(y\\) vorliegen. Also entweder ist eine Beobachtung erkrankt oder nicht. Meistens beschränkt sich die Betrachtung auf erkrankt (\\(1\\), ja) oder eben nicht erkrankt (\\(0\\), nein) bzw. gesund. Wichtig hierbei ist, dass wir eigentlich immer sagen, dass das Schlechte mit \\(1\\) kodiert wird. Wenn du das machst, dann wird dir die Interpretation der Effektschätzer der logistischen Regression leichter fallen.\nGleich zu Beginn dann nochmal wir werden die logistische Regression in den Agrarwissenschaften eher selten sehen. Im Bereich der Pflanzenwissenschaften kommt die logistische Regression kaum bis gar nicht vor. Im Bereich der Tierwissenschaften schon eher, aber dort dann im Bereich der Tiermedizin und eben wieder Erkrankungen.\nWo wir hingegen dann wieder die logistische Regression brauchen, ist bei der Klassifikation oder eben der Vorhersage von einem binären Ereignis. Dafür bietet sich dann die logistische Regression wieder an. Deshalb werden wir am Ende des Kapitels nochmal was zur Klassifikation machen, obwohl das hier eigentlich nur so halb reinpasst. Wenn du nicht Klassifizieren willst, dann lasse den letzten Abschnitt einfach weg."
  },
  {
    "objectID": "stat-modeling-logistic.html#annahmen-an-die-daten",
    "href": "stat-modeling-logistic.html#annahmen-an-die-daten",
    "title": "43  Logistische Regression",
    "section": "\n43.1 Annahmen an die Daten",
    "text": "43.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Binomialverteilung. Damit finden wir im Outcome nur \\(0\\) oder \\(1\\) Werte.\nIm folgenden Kapitel zu der multiplen logistischen linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 37 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Binomailverteilung. Damit finden wir im Outcome nur \\(0\\) oder \\(1\\) Werte. Das ist hier sehr wichtig, denn wir wollen ja eine multiple logistische lineare Regression rechnen.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen, wir du dann das Modell weiterverwendest."
  },
  {
    "objectID": "stat-modeling-logistic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-logistic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "43  Logistische Regression",
    "section": "\n43.2 Genutzte R Pakete für das Kapitel",
    "text": "43.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, gtsummary,\n               tidymodels, cutpointr)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-logistic.html#daten",
    "href": "stat-modeling-logistic.html#daten",
    "title": "43  Logistische Regression",
    "section": "\n43.3 Daten",
    "text": "43.3 Daten\nIn diesem Kapitel nutzen wir die infizierten Ferkel als Beispieldatensatz. Wir haben in dem Datensatz über vierhundert Ferkel untersucht und festgehalten, ob die Ferkel infiziert sind (\\(1\\), ja) oder nicht infiziert (\\(0\\), nein). Wir haben daneben noch eine ganze Reihe von Risikofaktoren erhoben. Hier sieht man mal wieder wie wirr die Sprache der Statistik ist. Weil wir rausfinden wollen welche Variable das Risiko für die Infektion erhöht, nennen wir diese Variablen Risikofaktoren. Obwohl die Variablen gar keine kategorialen Spalten sin bzw. nicht alle. So ist das dann in der Statistik, ein verwirrender Begriff jagt den Nächsten.\n\npig_tbl <- read_excel(\"data/infected_pigs.xlsx\") \n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der Tabelle 43.1 an.\n\n\n\n\nTabelle 43.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\ninfected\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n49.88\n16.94\n3.07\n1\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n58.2\n17.95\n4.88\n0\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n56.8\n19.02\n3.98\n0\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.47\n18.98\n5.18\n0\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n59.85\n16.57\n6.71\n1\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n58.1\n18.22\n5.43\n1\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\nrobust\n57.05\n17.95\n6.16\n1\n\n\n56\nmale\nwest\n13.91\n20.8\npre-frail\n50.84\n18.02\n6.52\n1\n\n\n57\nmale\nnorthwest\n12.49\n21.95\nrobust\n55.51\n17.73\n3.94\n1\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n58.5\n18.23\n2.73\n1\n\n\n59\nfemale\nnorth\n13.13\n20.23\npre-frail\n57.33\n17.21\n5.42\n1\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n55.85\n17.76\n6.18\n1\n\n\n\n\n\n\nIn dem nächsten Abschnitt werden wir die Daten nutzen um rauszufinden welche Variablen einen Einfluss auf den Infektionsstatus der Ferkel hat."
  },
  {
    "objectID": "stat-modeling-logistic.html#theoretischer-hintergrund",
    "href": "stat-modeling-logistic.html#theoretischer-hintergrund",
    "title": "43  Logistische Regression",
    "section": "\n43.4 Theoretischer Hintergrund",
    "text": "43.4 Theoretischer Hintergrund\nWir schaffen wir es, durch einen \\(0/1\\) Outcome auf der y-Achse eine gerade Linie durch die Punkte zu zeichnen und die Koeffiziente dieser Gerade zu bestimmen? Immerhin gibt es ja gar keine Werte zwischen \\(0\\) und \\(1\\). In Abbildung 43.1 sehen wir beispielhaft den Zusammenhang zwischen dem Infektionsstatus und der Aktivität der Ferkel. Wir haben zwei horizontale Linien. Wie zeichen wir jetzt da eine Gerade durch?\n\nggplot(pig_tbl, aes(x = activity, y = infected)) +\n  theme_bw() +\n  geom_point() \n\n\n\nAbbildung 43.1— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und der Aktivität der Ferkel.\n\n\n\n\nDer Trick hierbei ist wieder die Transformation des Zusammenhangs von \\(y \\sim x\\) auf einen \\(\\log\\)-scale. Das heißt wir Rechnen nicht mit den \\(0/1\\) Werten sondern transformieren den gesamten Zusammenhang. Das ist wichtig, den es gibt einen Unterschied zwischen der Transformation von \\(y\\) und der Transformation die hier gemeint ist. Wir halten fest, wir rechnen also nciht auf der ursprünglichen Skala der Daten sondern auf der \\(\\log\\)-scale. Allgemeiner wird auch von der link-Funktion gesprochen, da wir ja verschiedene Möglichkeiten der Transformation des Zusammenhangs haben.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\nWir gehen wir also vor. Zuerst Modellieren wir die Wahrscheinlichkeit für den Eintritt des Ereignisses. Wir machen also aus unseren binären \\(0/1\\) Daten eine Wahrscheinlichkeit für den Eintritt von 1.\n\\[\nY \\rightarrow Pr(Y = 1)\n\\]\nDamit haben wir schon was erreicht den \\(Pr(Y = 1)\\) liegt zwischen \\(0\\) und \\(1\\). Damit haben wir also schon Werte dazwischen. Wenn wir aber normalverteilte Residuen haben wollen, dann müssen unsere Werte von \\(-\\infty\\) bis \\(+\\infty\\) laufen können. Daher rechnen wir im Weiteren die Chance.\n\\[\n\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\n\\] Die Chance (eng. Odds) für das Eintreten von \\(Y=1\\) ist eben die Wahrscheinlichkeit für das Eintreten geteilt durch die Gegenwahrscheinlichkeit. Das ist schon besser, denn damit liegen unsere transformierten Werte für den Zusammenhang schon zwischen \\(0\\) und \\(+\\infty\\). Wenn wir jetzt noch den \\(\\log\\) von den Chancen rechnen, dann haben wir schon fast alles was wir brauchen.\n\\[\n\\log\\left(\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\\right)\n\\]\nDer Logarithmus der Chance liegt dann zwischen \\(-\\infty\\) und \\(+\\infty\\). Deshalb spricht man auch von den \\(\\log\\)-Odds einer logistischen Regression. Auch sieht man hier woher das logistisch kommt. Wir beschreiben im Namen auch gleich die Transformation mit. Am ende kommen wir somit dann auf folgendes Modell.\n\\[\n\\log\\left(\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\\right) = \\beta_0 + \\beta_1 x_1 + ...  + \\beta_p x_p + \\epsilon\n\\] Vielleicht ist dir der Begriff Wahrscheinlichkeit und der Unterschied zur Chance nicht mehr so präsent. Deshalb hier nochmal als Wiederholung oder Auffrischung.\n\nEine Wahrscheinlichkeit beschreibt dem Anteil an Allen. Zum Beispiel den Anteil Gewinner an allen Teilnehmern. Den Anteil Personen mit Therapieerfolg an allen Studienteilnehmern.\nEine Chance oder (eng. Odds) beschreibt ein Verhältnis. Somit das Verhältnis Gewinner zu Nichtgewinner. Oder das Verhältnis Personen mit Therapieerfolg zu Personen ohne Therapieerfolg\n\nNochmal an einem Zahlenbeispiel. Wenn wir ein Glücksspiel haben, in dem es 2 Kombinationen gibt die gewinnen und drei 3 Kombinationen die verlieren, dann haben wir eine Wahrscheinlichkeit zu gewinnen von \\(2 / 5 = 0.40 = 40\\%\\). Wenn wir die Chance zu gewinnen ausrechnen erhalten wir \\(2:3 = 0.67 = 67\\%\\). Wir sehen es gibt einen deutlichen Unterschied zwischen Chance und Wahrscheinlichkeit. Wenn wir große Fallzahl haben bzw. kleine Wahrscheinlichkeiten, dann ist der Unterschied nicht mehr so drastisch. Aber von einer Gleichkeit von Wahrscheinlichkeit und Chance zu sprechen kann nicht ausgegangen werden.\nWas ist nun das Problem? Wir erhalten aus einer logistischen Regression \\(\\log\\)-Odds wieder. Der Effektchätzer ist also eine Chance. Wir werden aber das Ergebnis wie eine Wahrscheinlichkeit interpretieren. Diese Diskrepanz ist wenigen bekannt und ein Grund, warum wir in der Medizin immer uns daran erinnern müssen, was wir eigentlich mit der logistischen Regression aussagen können."
  },
  {
    "objectID": "stat-modeling-logistic.html#modellierung",
    "href": "stat-modeling-logistic.html#modellierung",
    "title": "43  Logistische Regression",
    "section": "\n43.5 Modellierung",
    "text": "43.5 Modellierung\nDie Modellerierung der logistischen Regression ist sehr einfach. Wir nutzen wieder die Formelschreibweise im glm() um unsere Variablen zu definieren. Wenn unser Outcome nicht binär ist, dann jammert R und gibt uns einen Fehler aus. Ich kann hier nur dringlichst raten, das Outcome in \\(0/1\\) zu kodieren mit dem Schlechten als \\(1\\).\nDas glm() muss dann noch wissen, dass es eine logistische Regression rechnen soll. Das machen wir in dem wir als Verteilungsfamilie die Binomialverteilung auswählen. Wir geben also an family = binomial und schon können wir das volle Modell fitten.\n\nlog_fit <- glm(infected ~ age + sex + location + activity + crp + \n                 frailty + bloodpressure + weight + creatinin, \n               data = pig_tbl, family = binomial)\n\nDas war extrem kurz und scherzlos. Also können wir dann auch ganz kurz schauen, ob das Modell einigermaßen funktioniert hat."
  },
  {
    "objectID": "stat-modeling-logistic.html#performance-des-modells",
    "href": "stat-modeling-logistic.html#performance-des-modells",
    "title": "43  Logistische Regression",
    "section": "\n43.6 Performance des Modells",
    "text": "43.6 Performance des Modells\nNachdem wir das Modell gefittet haben, wollen wir uns nochmal das \\(R^2\\) wiedergeben lassen um zu entscheiden, ob unser Modell einigermaßen funktioniert hat. Dieser Abschnitt ist sehr kurz. Wir haben leider nur sehr wenige Möglichkeiten um ein logistischen Modell zu bewerten.\n\nr2(log_fit)\n\n# R2 for Logistic Regression\n  Tjur's R2: 0.322\n\n\nJa, so viel Varianz erklären wir nicht, aber wenn du ein wenig im Internet suchst, dann wirst du feststellen, dass das Bestimmtheitsmaß so eine Sache in glm()’s ist. Wir sind aber einigermaßen zufrieden. Eventuell würde eine Variablenselektion hier helfen, aber das ist nicht Inhalt dieses Kapitels.\nIn Abbildung 43.2 schauen wir nochmal auf die Residuen und die möglichen Ausreißer. Wieder sehen beide Plots einigermaßen in Ordnung aus. Die Abbildungen sind jetzt nicht die Besten, aber ich würde hier auch anhand der Diagnoseplots nicht die Modellierung verwerfen.\n\ncheck_model(log_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\")) \n\n\n\nAbbildung 43.2— Ausgabe ausgewählter Modelgüteplots der Funktion check_model()."
  },
  {
    "objectID": "stat-modeling-logistic.html#interpretation-des-modells",
    "href": "stat-modeling-logistic.html#interpretation-des-modells",
    "title": "43  Logistische Regression",
    "section": "\n43.7 Interpretation des Modells",
    "text": "43.7 Interpretation des Modells\nZu Interpretation schauen wir uns wie immer nicht die rohe Ausgabe an, sondern lassen uns die Ausgabe mit der Funktion model_parameters() aus dem R Paket parameters wiedergeben. Wir müssen noch die Option exponentiate = TRUE wählen, damit unsere Koeffizienten nicht als \\(\\log\\)-Odds sondern als Odds wiedergeben werden. Korrekterweise erhalten wir die Odds ratio wieder was wir auch als \\(OR\\) angegeben.\n\nmodel_parameters(log_fit, exponentiate = TRUE)\n\nParameter            | Odds Ratio |       SE |       95% CI |     z |      p\n----------------------------------------------------------------------------\n(Intercept)          |   1.31e-13 | 5.81e-13 | [0.00, 0.00] | -6.70 | < .001\nage                  |       1.03 |     0.03 | [0.98, 1.09] |  1.10 | 0.272 \nsex [male]           |       2.75 |     1.00 | [1.36, 5.69] |  2.77 | 0.006 \nlocation [northeast] |       0.56 |     0.20 | [0.28, 1.12] | -1.63 | 0.103 \nlocation [northwest] |       0.66 |     0.22 | [0.34, 1.25] | -1.28 | 0.202 \nlocation [west]      |       0.79 |     0.29 | [0.38, 1.62] | -0.64 | 0.520 \nactivity             |       0.90 |     0.09 | [0.75, 1.09] | -1.08 | 0.281 \ncrp                  |       2.97 |     0.35 | [2.38, 3.78] |  9.25 | < .001\nfrailty [pre-frail]  |       0.64 |     0.27 | [0.28, 1.44] | -1.06 | 0.288 \nfrailty [robust]     |       0.70 |     0.28 | [0.32, 1.51] | -0.89 | 0.376 \nbloodpressure        |       1.12 |     0.04 | [1.03, 1.21] |  2.77 | 0.006 \nweight               |       1.10 |     0.10 | [0.92, 1.30] |  1.05 | 0.293 \ncreatinin            |       1.03 |     0.09 | [0.86, 1.23] |  0.33 | 0.745 \n\n\nWie interpretieren wir nun das \\(OR\\) einer logistischen Regression? Wenn wir darauf gechtet haben, dass wir mit \\(1\\) das Schlechte meinen, dann können wir wir folgt mit dem \\(OR\\) sprechen. Wenn wir ein \\(OR > 1\\) haben, dann haben wir ein Risiko vorliegen. Die Variable mit einem \\(OR\\) größer als \\(1\\) wird die Chance auf den Eintritt des schlechten Ereignisses erhöhen. Wenn wir ein \\(OR < 1\\) haben, dann sprechen wir von einem protektiven Faktor. Die Variable mit einem \\(OR\\) kleiner \\(1\\) wird vor dem Eintreten des schlechten Ereignisses schützen. Schauen wir uns den Zusammenhang mal im Detail für die Ferkeldaten an.\n\n\n(intercept) beschreibt den Intercept der logistischen Regression. Wenn wir mehr als eine simple Regression vorliegen haben, wie in diesem Fall, dann ist der Intercept schwer zu interpretieren. Wir konzentrieren uns auf die Effekte der anderen Variablen.\n\nsex beschreibt den Effekt der männlichen Ferkel zu den weiblichen Ferkeln. Daher haben männliche Ferkel eine \\(2.75\\) höhere Chance infiziert zu werden als weibliche Ferkel.\n\nlocation [northeast], location [northwest] und location [west] beschreibt den Unterschied zur location [north]. Alle Orte haben eine geringere Chance für eine Infektion zum Vergleich der Bauernhöfe im Norden. Zwar ist keiner der Effekte signifikant, aber ein interessantes Ergebnis ist es allemal.\n\nactivity beschreibt den Effekt der Aktivität der Ferkel. Wenn sich die Ferkel mehr bewegen, dann ist die Chance für eine Infektion gemindert.\n\ncrp beschreibt den Effekt des CRP-Wertes auf den Infektionsgrad. Pro Einheit CRP steigt die Chance einer Infektion um \\(2.97\\) an. Das ist schon ein beachtlicher Wert.\n\nfrailty beschreibt die Gebrechlichkeit der Ferkel. Hier müssen wir wieder schauen, zu welchem Level von frailty wir vergleichen. Hier vergleichen wir zu frail. Also dem höchsten Gebrechlichkeitgrad. Ferkel die weniger gebrechlich sind, haben eine niedrigere Chance zu erkranken.\n\nbloodpressure, weight und creatinin sind alles Variablen, mit einem \\(OR\\) größer als \\(1\\) und somit alles Riskovariablen. Hier sind zwar die \\(OR\\) relativ klein, aber das muss erstmal nichts heißen, da die \\(OR\\) ja hier die Änderung für eine Einheit von \\(x\\) beschreiben. Deshalb musst du immer schauen, wie die Einheiten von kontinuierlichen kodiert Variablen sind.\n\nKommen wir nochmal zu den gänigen Tabellen für die Zusammenfassung eines Ergebnisses einer logistischen Regression. Teilweise sind diese Tabellen so generisch und häufiog verwendet, dass wir schon einen Begriff für diese Tabellen haben. In Tabelle 43.2 siehst du die table 1 für die Übersicht aller Risikovariablen aufgeteilt nach dem Infektionsstatus. Diese Art der Tabellendarstellung ist so grundlegend für eine medizinische Veröffentlichung, dass sich eben der Begriff table 1 etabliert hat. Fast jede medizinische Veröffentlichung hat als erste Tabelle diese Art von Tabelle angegeben. Hierbei ist wichtig, dass die \\(p\\)-Werte alle nur aus einem einfachen statistischen Test stammen. Die \\(p\\)-Werte einer multiplen logistischen Regression werden daher immer anders sein.\n\npig_tbl %>% tbl_summary(by = infected) %>% add_p() %>% as_flex_table()\n\n\n\n\n\n\nTabelle 43.2—  Ausgabe der Daten in einer Summary Table oder auch Table 1 genannt. In medizinischen Veröffentlichungen immer die erste Tabelle für die Zusammenfassung der Patienten (hier Ferkel) für jede erhobende Risikovariable. \n\nCharacteristic\n0, N = 1551\n1, N = 2571\np-value2\n\n\n\nage\n60.0 (57.0, 63.0)\n60.0 (57.0, 63.0)\n0.5\n\n\nsex\n\n\n0.3\n\n\nfemale\n66 (43%)\n95 (37%)\n\n\n\nmale\n89 (57%)\n162 (63%)\n\n\n\nlocation\n\n\n0.6\n\n\nnorth\n40 (26%)\n81 (32%)\n\n\n\nnortheast\n33 (21%)\n51 (20%)\n\n\n\nnorthwest\n51 (33%)\n73 (28%)\n\n\n\nwest\n31 (20%)\n52 (20%)\n\n\n\nactivity\n13.35 (12.29, 14.34)\n13.24 (12.25, 14.53)\n0.7\n\n\ncrp\n19.12 (18.17, 19.92)\n20.66 (19.85, 21.46)\n<0.001\n\n\nfrailty\n\n\n0.7\n\n\nfrail\n20 (13%)\n33 (13%)\n\n\n\npre-frail\n54 (35%)\n79 (31%)\n\n\n\nrobust\n81 (52%)\n145 (56%)\n\n\n\nbloodpressure\n56.8 (53.9, 58.6)\n57.0 (54.9, 59.0)\n0.10\n\n\nweight\n18.36 (17.32, 19.34)\n18.33 (17.36, 19.44)\n0.9\n\n\ncreatinin\n4.82 (4.06, 5.88)\n4.97 (3.97, 5.87)\n0.8\n\n\n\n1Median (IQR); n (%)\n2Wilcoxon rank sum test; Pearson's Chi-squared test\n\n\n\n\n\n\nIn Tabelle 43.3 siehst du nochmal für eine Auswahl an Variablen die simplen logistischen Regressionen gerechnet. Du müsst also nicht jede simple logistische Regression selber rechnen, sondern kannst auch die Funktion tbl_uvregression() verwenden. Das R Paket tbl_summary erlaubt weitreichende Formatierungsmöglichkeiten. Am bestes schaust du einmal im Tutorial Tutorial: tbl_regression selber nach was du brauchst oder anpassen willst.\n\npig_tbl%>%\n  select(infected, age, crp, bloodpressure) %>%\n  tbl_uvregression(\n    method = glm,\n    y = infected,\n    method.args = list(family = binomial),\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 2)\n  ) %>% as_flex_table()\n\n\n\n\n\n\nTabelle 43.3—  Simple logistische Regression für eine Auswahl an Einflussvariablen. Für jede Einflussvariable wurde eine simple logistische Regression gerechnet. \n\nCharacteristic\nN\nOR1\n95% CI1\np-value\n\n\n\nage\n412\n1.02\n0.97, 1.06\n0.49\n\n\ncrp\n412\n2.73\n2.23, 3.42\n<0.001\n\n\nbloodpressure\n412\n1.06\n1.00, 1.12\n0.058\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nnun gibt es viele Möglichkeiten sich die logistische Regression wiedergeben zu lassen In Tabelle 43.4 siehst du nochmal die Möglichkeit, die dir das R Paket tbl_summary() bietet. Am Ende ist es dann eine reine Geschmacksfrage, wie wir die Daten dann aufarbeiten wollen.\n\nlog_fit %>% tbl_regression(exponentiate = TRUE) %>% as_flex_table()\n\n\n\n\n\n\nTabelle 43.4—  Ausgabe der multiplen logistischen Regression durch die Funktion tbl_regression(). \n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\nage\n1.03\n0.98, 1.09\n0.3\n\n\nsex\n\n\n\n\n\nfemale\n—\n—\n\n\n\nmale\n2.75\n1.36, 5.69\n0.006\n\n\nlocation\n\n\n\n\n\nnorth\n—\n—\n\n\n\nnortheast\n0.56\n0.28, 1.12\n0.10\n\n\nnorthwest\n0.66\n0.34, 1.25\n0.2\n\n\nwest\n0.79\n0.38, 1.62\n0.5\n\n\nactivity\n0.90\n0.75, 1.09\n0.3\n\n\ncrp\n2.97\n2.38, 3.78\n<0.001\n\n\nfrailty\n\n\n\n\n\nfrail\n—\n—\n\n\n\npre-frail\n0.64\n0.28, 1.44\n0.3\n\n\nrobust\n0.70\n0.32, 1.51\n0.4\n\n\nbloodpressure\n1.12\n1.03, 1.21\n0.006\n\n\nweight\n1.10\n0.92, 1.30\n0.3\n\n\ncreatinin\n1.03\n0.86, 1.23\n0.7\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nZum Abschluss wollen wir uns einmal die Ergebnisse des Modellfits als logistischen Gerade für eine simple lineare Regression mit dem Modell \\(infected \\sim crp\\) anschauen. Wie immer können wir uns den Zusammenhang nur in einem simplen Modell anschauen. Im Fall einer multiplen linearen Regresion können wir nicht so viele Dimensionen in einer Grpahik darstellen. Wir fitten also das Modell log_fit_crp wie im folgenden dargestellt.\n\nlog_fit_crp <- glm(infected ~ crp, data = pig_tbl, family = binomial)\n\nNun können wir uns mit der Funktion predict() die Wert auf der Geraden wiedergeben lassen. Wenn wir predict() nur so aufrufen, dann erhalten wir die Werte für \\(y\\) auf der transformierten \\(link\\)-Scale wieder. Das hilft uns aber nicht weiter, wir haben ja nur 0 und 1 Werte für \\(y\\) vorliegen.\n\npredict(log_fit_crp, type = \"link\") %>% \n  extract(1:10) %>% \n  round(2)\n\n    1     2     3     4     5     6     7     8     9    10 \n 3.03 -0.73 -0.61  0.00  2.22  2.10 -0.39 -0.39  2.54  1.60 \n\n\nDa wir die Werte für die Wahrscheinlichkeit das ein Ferkel infiziert ist, also die Wahrscheinlichkeit \\(Pr(infected = 1)\\), müssen wir noch die Option type = reponse wählen. So erhalten wir die Wahrscheinlichkeiten wiedergegeben.\n\npredict(log_fit_crp, type = \"response\") %>% \n  extract(1:10) %>% \n  round(2)\n\n   1    2    3    4    5    6    7    8    9   10 \n0.95 0.33 0.35 0.50 0.90 0.89 0.40 0.40 0.93 0.83 \n\n\nAbschließend können wir uns die Gerade auch in der Abbildung 43.3 visualisieren lassen. Auf der x-Achse sehen wir die crp-Werte und auf der y-Achse den Infektionsstatus. Auf der \\(reponse\\)-scale sehen wir eine S-Kurve. Auf der \\(link\\)-scale würden wir eine Gerade sehen.\n\nggplot(pig_tbl, aes(x = crp, y = infected)) +\n  theme_bw() +\n  geom_point() +\n  geom_line(aes(y = predict(log_fit_crp, type = \"response\")), color = \"red\") \n\n\n\nAbbildung 43.3— Visualisierung der logistischen Gerade in einer simplen logistischen Regression mit der Variable crp.\n\n\n\n\nNun haben wir das Kapitel zur logistischen Regression fast abgeschlossen. Was noch fehlt ist die Besonderheit der Prädiktion im Kontext des maschinellen Lernens. Das machen wir jetzt im folgenden Abschnitt. Wenn dich die logistische Regression nur interessiert hat um einen kausalen Zusammenhang zwischen Einflussvariablen und dem binären Outcome zu modellieren, dann sind wir hier fertig."
  },
  {
    "objectID": "stat-modeling-logistic.html#prädiktion",
    "href": "stat-modeling-logistic.html#prädiktion",
    "title": "43  Logistische Regression",
    "section": "\n43.9 Prädiktion",
    "text": "43.9 Prädiktion\nDa wir später in dem Kapitel 49 die logistische Regression auch als Vergleich zu maschinellen Lernverfahren in der Klassifikation nutzen werden gehen wir hier auch die Prädiktion einmal für die logistische Regression durch. Wir wollen also eine Klassifikation, also eine Vorhersage, für das Outcome infected mit einer logistischen Regression rechnen. Wir nutzen dazu die Möglichkeiten des R Pakets tidymodels wodurch wir einfacher ein Modell bauen und eine Klassifikation rechnen können. Unsere Fragestellung ist, ob wir mit unseren Einflussvariablen den Infektionsstatus vorhersagen können. Das heißt wir wollen ein Modell bauen mit dem wir zukünftige Ferkel als potenziell krank oder gesund anhand unser erhobenen Daten einordnen bzw. klassifizieren können.\n\n\nMehr zu Rezepten (eng. recipes) kannst du im Kapitel 49 zu den Grundlagen des maschinellen Lernens erfahren.\nDer erste Schritt einer Klassifikation ist immer sicherzustellen, dass unser Outcome auch wirklich aus Kategorien besteht. In R nutzen wir dafür einen Faktor und setzen dann auch gleich die Ordnung fest.\n\npig_tbl <- pig_tbl %>% \n  mutate(infected = factor(infected, levels = c(0, 1)))\n\nNun bauen wir uns ein einfaches Rezept mit der Funktion recipe(). Dafür legen wir das Modell, was wir rechnen wollen einmal fest. Wir nehmen infected als Outcome und den Rest der Vairbalen . aus dem Datensatz pig_tbl als die \\(x\\) Variablen. Dann wollen wir noch alle Variablen, die ein Faktor sind in eine Dummyvariable umwandeln.\n\npig_rec <- recipe(infected ~ ., data = pig_tbl) %>% \n  step_dummy(all_nominal_predictors())\n\nWir wollen jetzt unser Modell definieren. Wir rechnen eine logistsiche Regression und deshalb nutzen wir die Funktion logistic_reg(). Da wir wirklich viele Möglichkeiten hätten die logistische Regression zu rechnen, müssen wir noch den Algorithmus wählen. Das tuen wir mit der Funktion set_engine(). Wir nutzen hier den simplen glm() Algorithmus. Es gebe aber auch andere Implementierungen.\n\nlogreg_mod <- logistic_reg() %>% \n  set_engine(\"glm\")\n\nJetzt müssen wir noch einen Workflow definieren. Wir wollen ein Modell rechnen und zwar mit den Informationen in unserem Rezept. Das bauen wir einmal zusammen und schauen uns die Ausgabe an.\n\npig_wflow <- workflow() %>% \n  add_model(logreg_mod) %>% \n  add_recipe(pig_rec)\n\npig_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nDas passt alles soweit. Ja, es ist etwas kompliziert und das ginge sicherlich auch einfacher. Wir werden dann aber noch sehen, dass wir es uns mit dem Ablauf sehr viel einfacher machen, wenn wir kompliziertere Modelle schätzen wollen. Mehr dazu findest du dann im Kapitel 49 zu den maschinellen Lernverfahren.\nJetzt können wir den Workflow nutzen um den Fit zu rechnen. Bis jetzt haben wir nur Informationen gesammelt. Dadurch das wir jetzt das Objekt pig_workflow in die Funktion fit() pipen rechnen wir das Modell.\n\npig_fit <- pig_wflow %>% \n  fit(data = pig_tbl)\n\nDas erhaltende Modell könne wir dann in die Funktion predict() stecken um uns den Inektionsstatus vorhersagen zu lassen.\n\npredict(pig_fit, new_data = pig_tbl)\n\n# A tibble: 412 × 1\n   .pred_class\n   <fct>      \n 1 1          \n 2 0          \n 3 0          \n 4 0          \n 5 1          \n 6 1          \n 7 0          \n 8 0          \n 9 1          \n10 1          \n# … with 402 more rows\n\n\nIn der Spalte .pred_class finden wir dann die vorhergesagten Werte des Infektionsstatus anhand unseres gefitteten Modells. Eigentlich würden wir ja gerne die vorhergesagten Werte mit unseren Orginalwerten vergleichen. Hier hilft uns die Funktion augment(). Dank der Funktion augment() erhalten wir nicht nur die vorhergesagten Klassen sondern auch die Wahrscheinlichkeit für die Klassenzugehörigkeiten. Daneben dann aber auch die Originalwerte für den Infektionsstatus in der Spalte infected.\n\npig_aug <- augment(pig_fit, new_data = pig_tbl) %>% \n  select(infected, matches(\"^\\\\.\"))\n\npig_aug\n\n# A tibble: 412 × 4\n   infected .pred_class .pred_0 .pred_1\n   <fct>    <fct>         <dbl>   <dbl>\n 1 1        1            0.0977   0.902\n 2 0        0            0.650    0.350\n 3 0        0            0.764    0.236\n 4 0        0            0.587    0.413\n 5 1        1            0.0568   0.943\n 6 1        1            0.0961   0.904\n 7 0        0            0.743    0.257\n 8 0        0            0.561    0.439\n 9 1        1            0.101    0.899\n10 1        1            0.168    0.832\n# … with 402 more rows\n\n\nWir können dann die Werte aus dem Objekt pig_aug nutzen um uns die ROC Kurve als Güte der Vorhersage wiedergeben zu lassen. Wir nutzen hier die schnelle Variante der Ploterstellung. In dem Kapitel 55 zum Vergleich von Algorithmen gehe ich noch näher auf die möglichen Optionen bei der Erstellung einer ROC Kurve ein. Hier fällt die ROC Kurve dann mehr oder minder vom Himmel. Ich musste noch der Funktion mitgeben, dass das Event bei uns das zweite Level des Faktors infected ist. Sonst ist unsere ROC Kurve einmal an der Diagonalen gespiegelt.\nIn dem Kapitel 29 erfährst du mehr darüber was eine ROC Kurve ist und wie du die ROC Kurve interpretieren kannst.\n\npig_aug %>% \n  roc_curve(truth = infected, .pred_1, event_level = \"second\") %>% \n  autoplot()\n\n\n\nAbbildung 43.5— ROC Kurve für die Vorhersage des Infektionsstatus der Ferkel anhand der erhobenen Daten.\n\n\n\n\nNa das hat doch mal gut funktioniert. Die ROC Kurve verläuft zwar nicht ideal aber immerhin ist die ROC Kurve weit von der Diagnolen entfernt. Unser Modell ist also in der Lage den Infektionsstatus der Ferkel einigermaßen solide vorherzusagen. Schauen wir uns noch die area under the curve (abk. AUC) an.\n\npig_aug %>% \n  roc_auc(truth = infected, .pred_1, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.830\n\n\nDer beste Wert wäre hier eine AUC von \\(1\\) und damit eine perfekte Vorhersage. Der schlechteste Wert wäre eine AUC von \\(0.5\\) und damit eine nahezu zufällige Zuordnung des Infeketionsstatus zu den Ferkeln von unserem Modell. Mit einer AUC von \\(0.83\\) können wir aber schon gut leben. Immerhin haben wir kaum am Modell rumgeschraubt bzw. ein Tuning betrieben. Wenn du mehr über Tuning und der Optimierung von Modellen zu Klassifikation wissen willst, dan musst du im Kapitel 49 zu den maschinellen Lernverfahren anfangen zu lesen."
  },
  {
    "objectID": "stat-modeling-logistic.html#referenzen",
    "href": "stat-modeling-logistic.html#referenzen",
    "title": "43  Logistische Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "module.html",
    "href": "module.html",
    "title": "Appendix E — Modulbeschreibung",
    "section": "",
    "text": "Wichtige Anmerkung zu den Modulbeschreibungen\n\n\n\nHier finden sich die work in progress Modulbeschreibungen. Diese werden von den offiziellen Modulbeschreibungen abweichen. Es handelt sich hierbei um Entwürfe von Modulbeschreibungen deren langfristiges Ziel es ist in den offiziellen Modulbeschreibungen aufzugehen. Dabei werden sicherlich nicht alle Vorschläge hier übernommen.\nEine inhaltliche Übersicht und die Planung des Vorlesungsverlaufs findest du auf dem Google Spreadsheet zur inhaltlichen Planung."
  },
  {
    "objectID": "module.html#sec-module-statistik",
    "href": "module.html#sec-module-statistik",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.1 Statistik",
    "text": "E.1 Statistik\n\nE.1.1 Inhalte und Qualifikationsziele des Moduls\n\nE.1.1.1 Kurzbeschreibung\nEntwicklung und Qualitätssicherung sind wesentlich getragen durch eine statistische Analyse von Daten. Erhobene und gemessene Daten werden mit Hilfe von statistischen Methoden ausgewertet, dargestellt und interpretiert, um die enthaltenen Informationen zu extrahieren. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt.\n\n\nE.1.1.2 Lehr-Lerninhalte\n\nHypothesenformulierung - Wahl geeigneter Merkmale - Skalenniveaus - Stichprobentheorie - Darstellung und Zusammenfassung der Ergebnisse (beschreibende Statistik) - Überprüfung von Hypothesen (Grundlagen der schließenden Statistik) - statistische Prozesskontrolle - Grundsätze der Versuchsplanung\n\n\n\n\nE.1.2 Kompetenzorientierte Lernergebnisse\n\nE.1.2.1 Wissen und Verstehen\n\nE.1.2.1.1 Wissensverbreiterung\nStudierende kennen die allgemein üblichen statistischen Methoden\n\n\nE.1.2.1.2 Wissensvertiefung\nSie können Hypothesen in adäquate Strategien umwandeln und sie identifizieren die korrekte statistische Methode zur Auswertung der Daten\n\n\nE.1.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Die Studierenden analysieren Daten mit den erlernten Methoden Können - kommunikative Kompetenz Sie analysieren und bewerten fachbezogene Informationen kritisch. Können - systemische Kompetenz Sie können das Risiko von auf Daten gestützten Entscheidungen verdeutlichen und abschätzen\n\n\n\nE.1.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.1.2.2.1 Nutzung und Transfer\n\n\nE.1.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.1.2.3 Kommunikation und Kooperation\n\nE.1.2.3.1 Kommunikation und Kooperation\n\n\n\nE.1.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.1.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.1.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Köhler, Wolfgang, Gabriel Schachtel, and Peter Voleske. Biostatistik: Einführung in die Biometrie für Biologen und Agrarwissenschaftler. Springer-Verlag, 2013.\n\n\n\nE.1.3 Voraussetzungen für die Teilnahme\n\nE.1.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.1.4 Verwendbarkeit des Moduls\n\nE.1.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.1.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.1.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "module.html#sec-module-mathematik",
    "href": "module.html#sec-module-mathematik",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.2 Mathematik und Statistik",
    "text": "E.2 Mathematik und Statistik\n\nE.2.1 Inhalte und Qualifikationsziele des Moduls\n\nE.2.1.1 Kurzbeschreibung\nIn den Biowissenschaften wie auch in Landwirtschaft und Gartenbau werden vielen Prozesse und Phänomene durch mathematische und statistische Modelle beschrieben. Die für Landwirtschaft und Gartenbau relevanten mathematischen und statistischen Verfahren werden dargestellt und diskutiert. Es werden an Fallbeispielen die mathematischen und statistischen Methoden eingeübt.\n\n\nE.2.1.2 Lehr-Lerninhalte\nGrundrechenarten, Zahlen und Mengen, Proportionalität, Prozente, Konzentration und Mischungen, Potenzen, Wurzeln und Logarithmen, Gleichungen, Relationen und wesentliche Funktionen, Vektoren und Matrizen, Folgen, Reihen, Lime, Einführung und praktische Anwendung der Differential- und Integralrechnung Messwerte, Skalenarten, statische Parameter, beschreibende Statistik, Wahrscheinlichkeit, Zufallsvariable und ihre Verteilungen, Schätzen von Parametern, Prüfung von Hypothesen über Mittelwerte, Proportionen und Varianzen, Konfidenzintervalle für Mittelwerte und Varianzen, Einführung in die Regressions- und Varianzanalyse, Einführung in nichtparametrische Teststatistik\n\n\n\nE.2.2 Kompetenzorientierte Lernergebnisse\n\nE.2.2.1 Wissen und Verstehen\n\nE.2.2.1.1 Wissensverbreiterung\nStudierende kennen die grundlegenden mathematischen und statistischen Verfahren, die im weiteren Studium vorausgesetzt werden. Sie können Fallbeispiele selbstständig lösen.\n\n\nE.2.2.1.2 Wissensvertiefung\nSie kennen die grundlegenden Prinzipien der beschreibenden und analytischen Statistik, sie erkennen statistische Probleme und wählen die geeigneten Methoden zu Lösung derselben aus.\n\n\nE.2.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie können Fallbeispiele mithilfe statistischer Software auswerten und die Ergebnisse darstellen. Können - kommunikative Kompetenz Sie können Argumente, Informationen und Ideen, die in dem Lehrgebiet gebräuchlich sind, darstellen und bewerten. Sie können, die aus den Fallbeispielen erhaltenen Ergebnisse analysieren und interpretieren. Können - systemische Kompetenz Sie können die erhaltenen Ergebnisse aus Fallstudien in Beziehung zu den in der Praxis vorhandenen Sachverhalten setzen.\n\n\n\nE.2.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.2.2.2.1 Nutzung und Transfer\n\n\nE.2.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.2.2.3 Kommunikation und Kooperation\n\nE.2.2.3.1 Kommunikation und Kooperation\n\n\n\nE.2.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.2.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.2.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Mathematik für Agrarwissenschaftler, Bartsch, Springer- Verlag\n\n\n\nE.2.3 Voraussetzungen für die Teilnahme\n\nE.2.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.2.4 Verwendbarkeit des Moduls\n\nE.2.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.2.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.2.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "module.html#sec-module-spezielle",
    "href": "module.html#sec-module-spezielle",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.3 Spezielle Statistik und Versuchswesen",
    "text": "E.3 Spezielle Statistik und Versuchswesen\n\nE.3.1 Inhalte und Qualifikationsziele des Moduls\n\nE.3.1.1 Kurzbeschreibung\nIn vielen Bereichen des Gartenbaues und der Landwirtschaft sind vertiefte Kenntnisse in spezifischen statistischen Methoden erforderlich. Die Erlangung von Kenndaten zur Steuerung der Produktion verlangt besondere Kenntnisse über die Planung und Auswertung von Versuchen und über die Datenerfassung, um dann durch eine sachgerechte statistische Auswertung zu korrekten Entscheidungen zu kommen, natürlich unter Berücksichtigung eines gewissen\n\n\nE.3.1.2 Lehr-Lerninhalte\nSubsamplingstruktur, Messwiederholungen; vertiefte Kenntnisse in der Planung, Durchführung und Auswertung von Versuchen im gärtnerisch-landwirtschaftlichen Bereich: Betrachtung wichtiger Versuchsdesigns wie Blockanlage, Lateinisches Quadrat, Spaltanlage, Streifenanlage; Kenntnisse im Umgang mit Software: Auswertung von Versuchsergebnissen mittels bedeutender Statistikprogramme,\n\n\n\nE.3.2 Kompetenzorientierte Lernergebnisse\n\nE.3.2.1 Wissen und Verstehen\n\nE.3.2.1.1 Wissensverbreiterung\nStudierende haben ein fundiertes und umfassendes Wissen über statistische Methoden, die in der Pflanzenproduktion Relevanz haben. Sie haben ein kritisches Verständnis über die Prinzipien, die den statistischen Methoden zu Grunde liegen.\n\n\nE.3.2.1.2 Wissensvertiefung\nSie sind in der Lage gemäß der Versuchsfrage die richtigen statistischen Verfahren auszuwählen, sie verstehen den Zusammenhang zwischen statistischen Methoden und der Versuchsplanung und wählen je nach Problemstellung die geeignetste Versuchsstrategie aus.\n\n\nE.3.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie setzen neben der standardmäßigen statistischen Software auch fortgeschrittene Software ein, die zur Lösung komplexer Probleme notwendig ist, beispielsweise SPSS. Sie erheben, sammeln und übertragen Daten. Können - kommunikative Kompetenz Sie können die in Versuchen erlangten Ergebnisse analysieren und Entscheidungen herbeiführen, diese präsentieren und in praxisrelevante Empfehlungen umsetzen. Können - systemische Kompetenz Sie wenden die Methoden der Datenanalyse auf Fragestellungen der Praxis an.\n\n\n\nE.3.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.3.2.2.1 Nutzung und Transfer\n\n\nE.3.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.3.2.3 Kommunikation und Kooperation\n\nE.3.2.3.1 Kommunikation und Kooperation\n\n\n\nE.3.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.3.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.3.2.5 Literatur\n\n\n\nE.3.3 Voraussetzungen für die Teilnahme\n\nE.3.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.3.4 Verwendbarkeit des Moduls\n\nE.3.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.3.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.3.5.1 Benotete Prüfungsleistung"
  },
  {
    "objectID": "module.html#sec-module-biostatistik",
    "href": "module.html#sec-module-biostatistik",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.4 Biostatistik",
    "text": "E.4 Biostatistik\n\nE.4.1 Inhalte und Qualifikationsziele des Moduls\n\nE.4.1.1 Kurzbeschreibung\nAus den Daten, die sich aus Prozessen und Experimenten ergeben, sollen zuverlässig und objektiv Entscheidungen herbeigeführt werden. Grundvoraussetzung hierzu sind aber vertiefte und umfangreiche Kenntnisse über angewandte statistische Methoden. Deshalb werden die notwendigen wissenschaftlichen und angewandten statistischen Methoden und ihre Prinzipien ausführlich dargestellt und diskutiert. An Fallbeispielen werden die Methoden eingeübt.\n\n\nE.4.1.2 Lehr-Lerninhalte\nWissenschaftliches Arbeiten, Strategien in der Forschung und ihre Beziehungen zu angewandten statistischen Methoden; Population, Merkmalsträger und Messwerte; Wahrscheinlichkeit, Zufallsvariablen und ihre Verteilungen; Stichprobe und Stichprobenverteilung; Interferenz über Mittelwerte und Varianz; allgemeine lineare Modelle; Kontraste und Mittelwertsvergleiche; Schätzen von Varianzkomponenten; Kovarianzanalyse; Nichtparametrische Statistik; Randomisierte Versuchspläne\n\n\n\nE.4.2 Kompetenzorientierte Lernergebnisse\n\nE.4.2.1 Wissen und Verstehen\n\nE.4.2.1.1 Wissensverbreiterung\nDie Absolventen können auf der Grundlage statistischer Methoden Hypothesen aufstellen und prüfen. Sie kennen die wesentlichen verwendeten angewandten statistischen Methoden.\n\n\nE.4.2.1.2 Wissensvertiefung\nSie kennen die Prinzipien, die hinter den angewandten statistischen Methoden stehen und können sich kritisch mit den zur Auswahl stehenden Methoden auseinandersetzen.\n\n\nE.4.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie können Versuchspläne entwickeln, Daten gewinnen und strukturieren, so dass objektive und zuverlässige Entscheidungen getroffen werden können. Können - kommunikative Kompetenz Sie können Daten mithilfe von statistischen Methoden auswerten, aufbereiten, tabellarisch und grafisch darstellen und sind in der Lage, sie in wissenschaftlichen Publikationen zu veröffentlichen. Können - systemische Kompetenz Sie sind in der Lage ihre Ergebnisse in für die Praxis relevanten Empfehlungen umzusetzen.\n\n\n\nE.4.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.4.2.2.1 Nutzung und Transfer\n\n\nE.4.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.4.2.3 Kommunikation und Kooperation\n\nE.4.2.3.1 Kommunikation und Kooperation\n\n\n\nE.4.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.4.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.4.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Köhler, Wolfgang, Gabriel Schachtel, and Peter Voleske. Biostatistik: Einführung in die Biometrie für Biologen und Agrarwissenschaftler. Springer-Verlag, 2013.\n\n\n\nE.4.3 Voraussetzungen für die Teilnahme\n\nE.4.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.4.4 Verwendbarkeit des Moduls\n\nE.4.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.4.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.4.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "module.html#sec-module-angewandte",
    "href": "module.html#sec-module-angewandte",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.5 Angewandte Statistik und Versuchswesen",
    "text": "E.5 Angewandte Statistik und Versuchswesen\n\nE.5.1 Inhalte und Qualifikationsziele des Moduls\n\nE.5.1.1 Kurzbeschreibung\nDer Fortschritt in Pflanzen- und Gartenbau ist wesentlich getragen durch eine intensive Versuchstätigkeit. Um erfolgreich in diesem Bereich tätig zu sein sind neben statistischen Kenntnissen auch solche über die Techniken zur Versuchsdurchführung erforderlich. Messdaten und Beobachtungen aus Erhebungen und Versuchen werden mit Hilfe von statistischen Methoden ausgewertet, dargestellt und interpretiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt.\n\n\nE.5.1.2 Lehr-Lerninhalte\nWeiterführende Kenntnisse in der schließenden Statistik, wissenschaftliche Hypothesenformulierung und -prüfung, Grundlegende Kenntnisse zur Versuchsplanung und Durchführung pflanzenbaulicher Versuche und Auswertung von Versuchsergebnissen mit Hilfe der hierfür relevanten statistischen Methoden\n\n\n\nE.5.2 Kompetenzorientierte Lernergebnisse\n\nE.5.2.1 Wissen und Verstehen\n\nE.5.2.1.1 Wissensverbreiterung\nStudierende kennen die in Landwirtschaft und Gartenbau allgemein üblichen statistischen Methoden, sie haben einen Überblick über die standardmäßig verwendeten Versuchsanlagen.\n\n\nE.5.2.1.2 Wissensvertiefung\nSie können Versuchsfragen in adäquate Versuchspläne und Strategien umwandeln und sie identifizieren die korrekte statistische Methode zur Auswertung der Messdaten.\n\n\nE.5.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie setzen statistische Software zur Auswertung und Darstellung der Daten ein. Können - kommunikative Kompetenz Sie erkennen in ihren Ergebnissen die Sachzusammenhänge und sind in der Lage sie in einem Bericht zu veröffentlichen. Können - systemische Kompetenz Sie können das Risiko von auf Daten gestützten Entscheidungen verdeutlichen und abschätzen.\n\n\n\nE.5.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.5.2.2.1 Nutzung und Transfer\n\n\nE.5.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.5.2.3 Kommunikation und Kooperation\n\nE.5.2.3.1 Kommunikation und Kooperation\n\n\n\nE.5.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.5.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.5.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Köhler, Wolfgang, Gabriel Schachtel, and Peter Voleske. Biostatistik: Einführung in die Biometrie für Biologen und Agrarwissenschaftler. Springer-Verlag, 2013.\n\n\n\nE.5.3 Voraussetzungen für die Teilnahme\n\nE.5.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.5.4 Verwendbarkeit des Moduls\n\nE.5.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.5.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.5.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "module.html#sec-module-bioverfahren",
    "href": "module.html#sec-module-bioverfahren",
    "title": "Appendix E — Modulbeschreibung",
    "section": "E.6 Angewandte Statistik für Bioverfahrenstechnik",
    "text": "E.6 Angewandte Statistik für Bioverfahrenstechnik\n\nE.6.1 Inhalte und Qualifikationsziele des Moduls\n\nE.6.1.1 Kurzbeschreibung\nDer wissenschaftliche Fortschritt ist wesentlich getragen durch eine intensive Versuchstätigkeit. Um erfolgreich in diesem Bereich tätig zu sein, sind neben statistischen Kenntnissen auch solche über Versuchsplanung erforderlich. Messdaten und Beobachtungen aus Erhebungen und Versuchen werden mit Hilfe von statistischen Methoden ausgewertet, dargestellt und interpretiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt.\n\n\nE.6.1.2 Lehr-Lerninhalte\n\nHypothesenformulierung - Wahl geeigneter Merkmale - Skalenniveaus - Stichprobentheorie - Darstellung und Zusammenfassung der Ergebnisse (beschreibende Statistik) - Überprüfung von Hypothesen (Grundlagen der schließenden Statistik) - statistische Prozesskontrolle - Versuchsplanung\n\n\n\n\nE.6.2 Kompetenzorientierte Lernergebnisse\n\nE.6.2.1 Wissen und Verstehen\n\nE.6.2.1.1 Wissensverbreiterung\nStudierende kennen die im biologischen Bereich allgemein üblichen statistischen Methoden, sie haben einen Überblick über die standardmäßig verwendeten Versuchsanlagen und kennen die Grundsätze der Versuchsplanung\n\n\nE.6.2.1.2 Wissensvertiefung\nSie können Versuchsfragen in adäquate Versuchspläne und Strategien umwandeln und sie identifizieren die korrekte statistische Methode zur Auswertung der Messdaten,\n\n\nE.6.2.1.3 Wissensverständnis\nAltdaten aus MoPPS2. Bitte diese Daten auf die Felder Wissensverständnis, Nutzung und Transfer, Wissenschaftliche Innovation, Kommunikation und Kooperation, Wissenschaftliches Selbstverständnis aufteilen und ggf. erweitern/abändern!\nKönnen - instrumentale Kompetenz Sie setzen die Standardmethoden zur Darstellung und Auswertung von Daten ein Können - kommunikative Kompetenz Sie erkennen in ihren Ergebnissen die Sachzusammenhänge und sind in der Lage, diese zu präsentieren Können - systemische Kompetenz Sie können das Risiko von auf Daten gestützten Entscheidungen verdeutlichen und abschätzen\n\n\n\nE.6.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nE.6.2.2.1 Nutzung und Transfer\n\n\nE.6.2.2.2 Wissenschaftliche Innovation\n\n\n\nE.6.2.3 Kommunikation und Kooperation\n\nE.6.2.3.1 Kommunikation und Kooperation\n\n\n\nE.6.2.4 Wissenschaftliches Selbstverständnis / Professionalität\n\nE.6.2.4.1 Wissenschaftliches Selbstverständnis / Professionalität\n\n\n\nE.6.2.5 Literatur\nSkript als Video unter https://www.youtube.com/c/JochenKruppa Dormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013. Wickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/] Köhler, Wolfgang, Gabriel Schachtel, and Peter Voleske. Biostatistik: Einführung in die Biometrie für Biologen und Agrarwissenschaftler. Springer-Verlag, 2013.\n\n\n\nE.6.3 Voraussetzungen für die Teilnahme\n\nE.6.3.1 Empfohlene Vorkenntnisse\n\n\n\nE.6.4 Verwendbarkeit des Moduls\n\nE.6.4.1 Zusammenhang mit anderen Modulen\n\n\n\nE.6.5 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nE.6.5.1 Benotete Prüfungsleistung\nKlausur\nmündliche Prüfung\nReferat (mit schriftlicher Ausarbeitung)"
  },
  {
    "objectID": "stat-modeling-logistic.html#dichotomisierung-von-y",
    "href": "stat-modeling-logistic.html#dichotomisierung-von-y",
    "title": "43  Logistische Regression",
    "section": "\n43.8 Dichotomisierung von \\(y\\)\n",
    "text": "43.8 Dichotomisierung von \\(y\\)\n\nManchmal ist es so, dass wir eine logistsiche Regression rechnen wollen. Wir fragen nicht, wie ist unser \\(y\\) verteilt und was für eine Regression können wir dann rechnen? Sondern wir wollen mit der logistischen Regression durch die Wand. Wenn wir das wollen, dann können wir unser \\(y\\) dichotomisieren. Das heißt, wir machen aus einer Variable, die mehr als zwei Level hat einen Faktor mit zwei Leveln. Dafür stehen uns verschiedene Möglichkeiten offen. Wie immer gehe ich die Möglichkeiten von simple nach komplex einmal durch."
  },
  {
    "objectID": "stat-modeling-logistic.html#dichotomisierung",
    "href": "stat-modeling-logistic.html#dichotomisierung",
    "title": "43  Logistische Regression",
    "section": "\n43.8 Dichotomisierung",
    "text": "43.8 Dichotomisierung\nManchmal ist es so, dass wir eine logistsiche Regression rechnen wollen. Wir fragen nicht, wie ist unser \\(y\\) verteilt und was für eine Regression können wir dann rechnen? Sondern wir wollen mit der logistischen Regression durch die Wand. Wenn wir das wollen, dann können wir unser \\(y\\) dichotomisieren. Das heißt, wir machen aus einer Variable, die mehr als zwei Level hat einen Faktor mit zwei Leveln. Dafür stehen uns verschiedene Möglichkeiten offen.\n\npig_tbl %>% \n  mutate(frailty = recode(frailty, \"robust\" = \"robust\", \n                          \"pre-frail\" = \"frail_prefrail\", \n                          \"frail\" = \"frail_prefrail\")) %>% \n  pull(frailty) %>% extract(1:20)\n\n [1] \"robust\"         \"robust\"         \"robust\"         \"robust\"        \n [5] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n [9] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n[13] \"robust\"         \"robust\"         \"robust\"         \"frail_prefrail\"\n[17] \"robust\"         \"frail_prefrail\" \"frail_prefrail\" \"frail_prefrail\"\n\n\n\npig_tbl %>% \nmutate(frailty = case_when(frailty == \"robust\" ~ \"robust\",\n                           frailty == \"pre-frail\" ~ \"frail\",\n                           frailty == \"frail\" ~ \"frail\")) %>% \n  pull(frailty) %>% extract(1:20)\n\n [1] \"robust\" \"robust\" \"robust\" \"robust\" \"robust\" \"robust\" \"frail\"  \"robust\"\n [9] \"robust\" \"robust\" \"frail\"  \"robust\" \"robust\" \"robust\" \"robust\" \"frail\" \n[17] \"robust\" \"frail\"  \"frail\"  \"frail\" \n\n\nHäufig haben wir auch den Fall, dass wir keine kontinuierlichen \\(x\\) in unseren Daten wollen. Alles soll sich in Faktoren verwandeln, so dass wir immer eine 2x2 Tafel haben. Wenn es sein muss, liefert hier cutpointr() die Lösung für dieses Problem.\nAbbildung 43.4\n\ncp_crp <- cutpointr(data = pig_tbl,\n                    x = crp,\n                    class = infected,\n                    method = maximize_metric, \n                    metric = sum_sens_spec) \n\ncp_crp\n\n# A tibble: 1 × 16\n  direction optimal_cutpoint method          sum_sens_spec   acc sensitivity\n  <chr>                <dbl> <chr>                   <dbl> <dbl>       <dbl>\n1 >=                   19.84 maximize_metric       1.49424  0.75    0.758755\n  specificity      AUC pos_class neg_class prevalence outcome  predictor\n        <dbl>    <dbl>     <dbl>     <dbl>      <dbl> <chr>    <chr>    \n1    0.735484 0.811786         1         0   0.623786 infected crp      \n  data               roc_curve                  boot \n  <list>             <list>                     <lgl>\n1 <tibble [412 × 2]> <roc_cutpointr [282 × 10]> NA   \n\n\n\nplot(cp_crp)\n\n\n\nAbbildung 43.4— Visualisierung der logistischen Gerade in einer simplen logistischen Regression mit der Variable crp.\n\n\n\n\n\npluck(cp_crp, \"optimal_cutpoint\")\n\n[1] 19.84\n\n\n\npig_tbl %>% \nmutate(crp_bin = case_when(crp >= 19.84 ~ \"high\",\n                           crp < 19.84 ~ \"low\")) %>% \nselect(crp, crp_bin)  \n\n# A tibble: 412 × 2\n     crp crp_bin\n   <dbl> <chr>  \n 1  22.4 high   \n 2  18.6 low    \n 3  18.8 low    \n 4  19.4 low    \n 5  21.6 high   \n 6  21.4 high   \n 7  19.0 low    \n 8  19.0 low    \n 9  21.9 high   \n10  21.0 high   \n# … with 402 more rows\n\n\n\n\nAn introduction to cutpointr"
  }
]