```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra)
```

# Clusteranalysen {#sec-cluster-analysis}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

> *"Cluster together like stars!" --- Henry Miller*

In diesem Kapitel wollen wir uns mit der Clusteranalyse beschäftigen. Zuerst was verstehen wir unter einem Cluster? Ein Cluster ist ein Zusammenschluss von ähnlichen Beobachtungen. Nun stellt sich zuerst die Frage, was heißt ähnlich? Wir brauchen also Maßzahlen für die Ähnlichkeit zwischen zwei und mehreren Beobachtungen. Im Weiteren haben wir in erster Linie kein Outcome $y$. Wir nehmen alle Spalten $x$ aus unseren Daten und versuchen anhand der Spalten Gruppen über die Beobachtungen in den Zeilen zu bilden.

-   Die **Dendrogramme** über hierarchisches Clustern in @sec-clust-dendro
-   Das **k-NN** oder nächste Nachbarn Clustern in @sec-clust-knn
-   Die **Hauptkomponentenanalyse** in @sec-clust-pca
-   Die **Heatmaps** verbreitet in der genetischen Analys in @sec-clust-heat
-   Einmal das hierarchisches Clustern und k-NN über das R Paket `tidyclust` in @sec-clust-tidyclust

Teilweise haben wir Überlagerungen mit dem Kapitel @sec-outlier

https://www.datanovia.com/en/lessons/cluster-analysis-example-quick-start-r-code/

https://www.r-bloggers.com/2021/04/cluster-analysis-in-r/

https://www.datanovia.com/en/lessons/clustering-distance-measures/#data-preparation

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, palmerpenguins, readxl,
               ggdendro, broom, cluster, factoextra,
               pheatmap, tidyclust, dlookr, conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflicts_prefer(dlookr::transform)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Woher

[Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/index.html)

```{r}
penguins_raw_tbl <- penguins %>% 
  select(species, bill_length_mm, bill_depth_mm, body_mass_g) %>% 
  na.omit()
```

```{r}
penguins_tbl <- penguins_raw_tbl %>% 
  select(bill_length_mm, bill_depth_mm, body_mass_g)
```

```{r}
penguins_species <- penguins_raw_tbl %>% 
  pull(species)
```

Schauen

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-meta-drymatter
#| tbl-cap: "Daten zu den Weizenerträgen nach der Gabe von einer Eisendosis mit $10\\mu mol$. In allen Studien wurde die gleiche Dosis auf die $n$ Pflanzen gegeben."

penguins_raw_tbl %>% 
  head(7) %>% 
  kable(align = "lrrr", "pipe")
```

Im Weiteren betrachten wir noch das Beispiel der Gummibärchendaten. Auch hier haben wir echte Daten vorliegen, so dass wir eventuell Ausreißer entdecken könnten. Da wir hier fehlende Werte in den Daten haben, entfernen wir alle fehlenden Werte mit der Funktion `na.omit()`. Damit löschen wir jede Zeile in den Daten, wo mindestens ein fehlender Wert auftritt. Da wir hier mittlerweile sehr viele Daten vorliegen haben, wollen wir das Problem auf die beiden Quellen *FU Berlin* und dem *Girls and Boys Day* eingrenzen.

```{r}
#| message: false

gummi_tbl <- read_excel("data/gummibears.xlsx")  %>%
  filter(module %in% c("FU Berlin", "Girls and Boys Day")) %>% 
  select(gender, age, height, semester, most_liked) %>% 
  mutate(gender = as_factor(gender),
         most_liked = as_factor(most_liked)) %>% 
  na.omit()
```

Auch

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-meta-sunflower
#| tbl-cap: "Daten zu den mit Mehltau infizierten Sonnenblumensamen nach der Behandlung mit MoldEx."

gummi_tbl %>% 
  head(7) %>% 
  kable(align = "lrr", "pipe")
```

Häufig

## Daten preprocessing

Wir können die Daten so wir wie sie vorliegen haben in einem Clusteralgorithmus verwenden. Wir führen also keine Transformation der Daten durch, wir nutzen die Daten untransformiert. Dieses untransfomierte Verwenden der Daten führt aber meist dazu, dass Variablen nicht im gleichen Maße berücksichtigt werden. Es macht eben einen Unterschied, ob wir wie bei dem Alter sehr viele verschiedene Werte haben als beim Geschlecht. Es macht auch einen Unterschied, ob das Alter numerische Werte von 20 bis 60 haben kann und das Semester nur numerische Werte von 1 bis 10.

In dem @sec-eda-transform findest du die hier verwendeten Standardfunktionen in R aus dem Paket `dlookr` für die Normalisierung sowie Standardisierung mit der Funktion `transform()`. Wenn es komplexer wird, dann empfehle ich den Workflow, wie er im @sec-pre-processing für die Klassifikation von Daten vorgestellt wird.

### Normalisieren

Die Normalisierung von Variablen in ein Intervall von $[0;1]$. Es gehen natürlich auch andere Intervalle, aber das Intervall von 0 bis 1 ist wohl das häufigste Intervall was genutzt wird.

```{r}
norm_pen_tbl <- penguins_tbl %>% 
  mutate(bill_length_mm = transform(bill_length_mm, "minmax"),
         bill_depth_mm = transform(bill_depth_mm, "minmax"),
         body_mass_g = transform(body_mass_g, "minmax")) 
norm_pen_tbl
```

Hier helfen natürlich auch die Funktionen von dem R Paket `dplyr` und der [Hilfsseite von `across()`](https://dplyr.tidyverse.org/reference/across.html) um mehrere Spalten schneller in `mutate` zu transformieren.

### Standardisieren

Die Standardisierung von Variablen zu einer $\mathcal{N(0,1)}$ Standardnormalverteilung

```{r}
scale_gummi_tbl <- gummi_tbl %>% 
  mutate(gender = as_factor(gender),
         age = transform(age, "zscore"),
         height = transform(height, "zscore"),
         semester = transform(semester, "zscore"),
         most_liked = as_factor(most_liked))
scale_gummi_tbl
```

Wie beim Normalisieren helfen hier natürlich auch die Funktionen von dem R Paket `dplyr` und der [Hilfsseite von `across()`](https://dplyr.tidyverse.org/reference/across.html) um mehrere Spalten schneller in `mutate` zu transformieren.

## Distanzmaße

Wir betrachten im Folgenden immer die Distanzen zwischen den Zeilen des Datensatzes. Das heißt, wir wollen immer die Distanzen zwischen den Beobachtungen berechnen. Wie nah oder fern sind sich zwei Beobachtungen gegeben den Spalten? Wir schauen uns einmal zwei sehr intuitive Distanzmaße mit der euklidischen sowie der manhattan Distanz an.

Euklidische Distanz

:   $$
    d_E(p,q) = \sqrt{(p-q)^2}
    $$

Manhattan Distanz

:   $$
    d_M(p,q) = \lvert p-q \rvert
    $$

-   `dist()` als Standardfunktion: Akzeptiert nur numerische Daten als Eingabe, das zu verwendende Abstandsmaß muss eines der Folgenden sein: `euclidean`, `maximum`, `manhattan`, `canberra`, `binary` oder `minkowski`. Die Hilfeseite `?dist()` liefert mehr Informationen über die Distanzmaße.
-   `get_dist()` aus dem R Paket `factoextra`: Akzeptiert nur numerische Daten als Eingabe. Im Vergleich zur Standardfunktion dist() unterstützt sie korrelationsbasierte Abstandsmaße einschließlich der Methoden `pearson`, `kendall` und `spearman`.
-   `daisy()` aus dem R Paket `cluster`: Kann mit anderen Variablentypen umgehen als numerisch. Also auch mit Kategorien und Faktoren. In diesem Fall wird automatisch der Gower-Koeffizient als Metrik verwendet. Er ist eines der beliebtesten Näherungsmaße für gemischte Datentypen. Weitere Einzelheiten findest du auf der Hilfeseite der Funktion `?daisy`.

Durch die Standardisierung werden die Abstandsmaße - Euklidisch, Manhattan, Korrelation - ähnlicher, als sie es bei nicht transformierten Daten wären.

Die Funktion `fviz_dist()` aus dem R Paket `factorextra` erlaubt hier die Distanzmatrix zu visualisieren.

## Algorithmen fürs Clustern

### Dendrogramm {#sec-clust-dendro}

```{r}
h.cluster <- penguins_tbl  %>% 
  dist(., method = "euclidean") %>% 
  hclust(., method = "ward.D")
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-01
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."

ggdendrogram(h.cluster)
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-02
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."

fviz_nbclust(penguins_tbl, kmeans, method = "gap_stat")
```

```{r}
set.seed(123) # for reproducibility
km.res <- kmeans(penguins_tbl, 3, nstart = 25)
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-03
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."
fviz_cluster(km.res, data = penguins_tbl, palette = "jco",
             ggtheme = theme_minimal())
```

```{r}
res.hc <- hclust(dist(penguins_tbl),  method = "ward.D2")
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-04
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."
fviz_dend(res.hc, cex = 0.5, k = 4, palette = "jco") 
```

```{r}
p.cluster <- penguins_tbl %>% kmeans(., 3)
p.cluster$cluster <- as.factor(penguins_species)
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-05
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."
ggplot(penguins_tbl, aes(bill_length_mm, bill_depth_mm, label = penguins_species)) + 
  scale_fill_discrete(name = "Cluster") + 
  geom_label(aes(fill = p.cluster$cluster), colour = "white", 
  fontface = "bold", size=2)
```

```{r}
multi.clust <- data.frame(k = 1:6) %>% group_by(k) %>% do(clust = kmeans(penguins_tbl, .$k))
sumsq.clust <- multi.clust %>% group_by(k) %>% do(glance(.$clust[[1]]))
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-06
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."
ggplot(sumsq.clust, aes(k, tot.withinss)) + geom_line() + geom_point()
```

```{r}
multi.clust <- data.frame(k = 1:6) %>% group_by(k) %>% do(clust = kmeans(penguins_tbl, .$k))
multi.k <- multi.clust %>% group_by(k) %>% do(augment(.$clust[[1]], penguins_tbl))
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-07
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."
ggplot(multi.k, aes(bill_length_mm, bill_depth_mm)) + geom_point(aes(color = .cluster)) + 
  facet_wrap(~k)
```

Um das Ergebnis der Gruppenfindung zu beurteilen, eignet sich ein Silhouettenplot. Ein Silhouettenplot zeigt für jede Beobachtung i die Silhouettenbreite $s_i$, welche definiert ist als normierte Differenz der kleinsten Distanz zu den Beobachtungen außerhalb der eigenen Gruppe und dem Mittelwert der Distanzen innerhalb einer Gruppe. Die Silhouettenbreite $s_i$ kann jeden Wert im Intervall \[-1, 1\] annehmen und wird folgendermaßen interpretiert. - $s_i = 1$ Die Beobachtung ist dem "richtigen" Cluster zugeordnet. - $s_i = 0$ Die Beobachtung hätte ebenso gut einer anderen Gruppe zugeordnet werden können. - $s_i = -1$ Die Beobachtung ist schlecht zugeordnet. Es kann darüber hinaus die durchschnittliche Silhouettenbreite über alle Beobachtungen berechnet werden, womit sich die Gruppenbildung als Ganzes beurteilen lässt. Die durchschnittliche Silhouettenbreite wird analog interpretiert.

Was ist mit tidyclust::silhouette?

```{r}
data(ruspini)
pr4 <- pam(ruspini, 4)
```

```{r}
si2 <- cluster::silhouette(pr4$clustering, dist(ruspini, "canberra"))
sil.data <- data.frame(cluster = factor(si2[, 1]), sil_width = si2[, 3])
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-08
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."

ggplot(sil.data, aes(x = row.names(sil.data), y = sil_width, fill = cluster)) +
  geom_bar(stat = "identity", width = 0.5) + coord_flip() + labs(x = "") +
  scale_x_discrete(limits = row.names(sil.data[order(sil.data$cluster, 
  sil.data$sil_width), ])) 
```

### Heatmap {#sec-clust-heat}

gdata Packae genetik data?

https://bioconductor.org/packages/release/bioc/vignettes/heatmaps/inst/doc/heatmaps.html https://jokergoo.github.io/ComplexHeatmap-reference/book/index.html

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-09
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."

pheatmap(t(penguins_tbl), cutree_cols = 4)
```

### k-means Clusteranalyse {#sec-clust-knn}

### Hauptkomponentenanalyse {#sec-clust-pca}

## Datenanalyse mit `tidyclust` {#sec-clust-tidyclust}

https://tidyclust.tidymodels.org/index.html

Im Folgenden analysieren wir die Gummibärchendaten einmal mit dem R Paket `tidyclust`.

### Hierarchical Clustering

::: column-margin
[tidyclust Hilfeseite für das Hierarchical Clustering](https://tidyclust.tidymodels.org/articles/hier_clust.html)
:::

Es gibt vier gängige Ansätze für die Cluster-Cluster-Distanzierung, auch "Linkage" genannt:

-   *single linkage*: Der Abstand zwischen zwei Clustern ist der Abstand zwischen den beiden nächstgelegenen Beobachtungen.
-   *average linkage*: Der Abstand zwischen zwei Clustern ist der Durchschnitt aller Abstände zwischen den Beobachtungen in einem Cluster und den Beobachtungen im anderen Cluster.
-   *complete linkage*: Der Abstand zwischen zwei Clustern ist der Abstand zwischen den beiden am weitesten entfernten Beobachtungen.
-   *centroid method*: Der Abstand zwischen zwei Clustern ist der Abstand zwischen ihren Zentroiden (geometrisches Mittel oder Median).
-   *Ward-Methode*: Der Abstand zwischen zwei Clustern ist proportional zur Zunahme der Fehlerquadratsumme (ESS), die sich aus der Verbindung der beiden Cluster ergeben würde. Die ESS wird als Summe der quadrierten Abstände zwischen den Beobachtungen in einem Cluster und dem Schwerpunkt des Clusters berechnet.

```{r}
hc_spec <- hier_clust(num_clusters = 3,
                       linkage_method = "average")
```

```{r}
hc_fit <- hc_spec %>%
  fit(~ gender + age + height + semester + most_liked,
      data = gummi_tbl)
```

```{r}
hc_fit %>%
  summary()
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:  fig-cluster-10
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."
hc_fit$fit %>% plot()
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label:   fig-cluster-11
#| fig-align: center
#| fig-height: 4
#| fig-width: 6
#| fig-cap: "."

ggdendrogram(hc_fit$fit, rotate = FALSE, size = 2)

```

```{r}
hc_summary <- hc_fit %>% extract_fit_summary()

hc_summary %>% 
  pluck("cluster_assignments")
```

### k-means Clustering

::: column-margin
[tidyclust Hilfeseite für das k-means Clustering](https://tidyclust.tidymodels.org/articles/k_means.html)
:::

```{r}
kmeans_spec <- k_means(num_clusters = 3)

kmeans_spec
```

```{r}
kmeans_spec_lloyd <- k_means(num_clusters = 3) %>%
  parsnip::set_engine("stats", algorithm = "Lloyd")
```

```{r}
kmeans_fit <- kmeans_spec %>%
  fit(~ age + height + semester,
      data = gummi_tbl)
```

```{r}
kmeans_fit$fit
```

```{r}
kmeans_fit %>%
  extract_cluster_assignment()
```

```{r}
kmeans_summary <- kmeans_fit %>%
  extract_fit_summary()
```

```{r}
tibble(
  orig_labels = kmeans_summary$orig_labels,
  standard_labels = kmeans_summary$cluster_assignments
)
```

```{r}
kmeans_fit %>%
  extract_centroids()
```

```{r}
kmeans_fit %>%
  tidyclust::silhouette_avg(select(gummi_tbl, age, height, semester))
```

## Referenzen {.unnumbered}
