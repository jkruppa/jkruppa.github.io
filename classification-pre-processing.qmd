```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Data preprocessing {#sec-pre-processing}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

Die Vorverarbeitung von Daten (eng. *preprocessing*) für die Klassifikation grundlegend. Wir können nicht auf unseren Daten so wir wie die Daten erhoben haben eine Klassifikation rechnen. Dafür sind die Algorithmen der Klassifikation weder ausgelegt noch gedacht. Zum anderen wollen wir ja gar keine Aussagen über mögliche Effekte von den Einflussvariablen auf das Outcome treffen. Uns ist vollkommen egal, ob eine Variable *signifikant* ist. Wir wollen nur wissen, ob eine Variable wichtig für die Voerhersage von unserem Label $y$ ist.

::: column-margin
Du findest auch noch im Appendix von [Tidy Modeling with R](https://www.tmwr.org/) die [Recommended Preprocessing](https://www.tmwr.org/pre-proc-table.html) Schritte für viele Algorithmen.
:::

Du findest auf der [Referenzseite von recipes](https://recipes.tidymodels.org/reference/index.html#step-functions-discretization) eine *große* Auswahl an Vorverarbeitungsschritten. Ich kann dir hier nur eine Auswahl präsentieren und konzentriere mich auf die häufigst genutzen Algorithmen. Du solltest aber für deinen Anwendungsfallauf jeden Fall nochmal selber schauen, ob du was passenderes findest.

-   Die Dummycodierung findest du in @sec-preprocess-dummy. Wir verwandeln dabei kategorielle Spalten in mehrere $0/1$ kodierte Spalten.
-   Die Zero Varianz können wir in @sec-preprocess-zv überprüfen. Mit der Zero Varianz meine ich, dass wir Spalten haben mit nur einem Eintrag und daher natürlich keine Varianz. Du kannst auch schauen, ob du eine Spalte hast in der nahezu eine Varianz von Null vorliegt.
-   Die Standardisierung von Variablen zu einer $\mathcal{N(0,1)}$ Standardnormalverteilung in @sec-preprocess-standard.
-   Die Normalisierung von Variablen in ein Intervall von $[0;1]$ in @sec-preprocess-normal. Es gehen natürlich auch andere Intervalle, aber das \$\[0;1\]\$\$ Intervall ist wohl das häufigste Intervall was genutzt wird.
-   Die Imputation von fehlenden Werte in dem @sec-preprocess-impute. Das ist hier aber nur eine Auswahl. Es gibt sehr viel mehr Möglichkeiten. Du musst auf jeden Fall hier nochmal auf die Referenzseite von `recipes` nachschauen.
-   Die Erstellung von Kategorien aus einer numerischen Variable in @sec-preprocess-discrete. Dann könnten wir auch die neuen kategorialen Variablen dann auch wieder in eine Dummycodierung überführen.
-   Die Entfernung von hoch korrelierten Variabeln in @sec-preprocess-corr. Hier wäre ich vorsichtig und würde mir den Schritt gerne einaml vorher selber anschauen und dann eventuell Variablen per Hand vor dem *preprocessing* entfernen.

Einige Vorverarbeitungsschritte kannst du auch in den vorherigen Kapiteln nachlesen. Im Kapitel zur [Transformation von Daten](#sec-eda-transform) oder zur [Imputation von fehlenden Werten](#sec-missing) findest du noch tiefer greifende Informationen zu den Themen. In diesem Kapitel zeige ich nur, wie du die Verfahren anwendest und gehe nochmal eher oberflächig auf mögliche Probleme ein.

## Genutzte R Pakete für das Kapitel

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, 
               janitor,
               conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

In dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt. Wir wählen auch nur ein kleines Subset aus den Daten aus, da wir in diesem Kapitel nur Funktion demonstrieren und nicht die Ergebnisse interpretieren.

```{r}
pig_tbl <- read_excel("data/infected_pigs.xlsx") %>% 
  mutate(pig_id = 1:n()) %>% 
  select(pig_id, infected, age, crp, sex, frailty) %>% 
  select(pig_id, infected, everything())  
```

In @tbl-ml-basic-pig siehst du nochmal einen Ausschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-ml-basic-pig
#| tbl-cap: Auszug aus dem Daten zu den kranken Ferkeln.

rbind(head(pig_tbl),
      rep("...", times = ncol(pig_tbl)),
      tail(pig_tbl)) %>% 
  kable(align = "c", "pipe")
```

Gehen wir jetzt mal die Preproessing Schritte, die wir für das maschinelle Lernen später brauchen einmal durch. Am Ende des Kapitels schauen wir uns dann die Anwendung nochmal im Ganzen auf den Gummibärchendaten einmal an.

## Das Rezept mit `recipe()`

In dem Einführungskapitel zur Klassifikation haben wir uns ja mit dem Rezept und dem Workflow schonmal beschäftigt. Hier möchte ich dann nochmal etwas mehr auf das Rezept eingehen und zeigen, wie das Rezept für Daten dann mit den Daten zusammenkommt. Wir bauen uns wie immer mit der Funktion `recipe()` das Datenrezept in R zusammen. Ich empfehle grundsätzlich vorab einen `select()` Schritt durchzuführen und nur die Variablen in den Daten zu behalten, die wir wirklich brauchen. Dann können wir auch mit dem `.` einfach alle Spalten ohne das Outcome als Prädiktoren definieren.

```{r}
pig_rec <- recipe(infected ~ ., data = pig_tbl) %>% 
  update_role(pig_id, new_role = "ID")

pig_rec %>% summary()
```

Nachdem wir dann unser Rezept definiert haben, können wir auch noch Rollen vergeben. Die Rollen sind nützlich, wenn wir später auf bestimmten Variablen etwas rechnen wollen oder eben nicht. Wir können die Rollen selber definieren und diese Rollen dann auch über die Funktion `has_role()` ein- oder ausschließen. Neben dieser Möglichkeit gezielt Variablen nach der Rolle anzusprechen, können wir auch *alle* Prädiktoren oder *alle* Outcomes auswählen.

Wir haben Funktionen, die die Rolle der Variablen festlegen:

-   `all_predictors()` wendet den Schritt nur auf die Prädiktorvariablen an, daher auf die Features.
-   `all_outcomes()` wendet den Schritt nur auf die Outcome-Variable(n) an, daher auf die Label.

Un wir haben Funktionen, die den Typ der Variablen angeben:

-   `all_nominal()` wendet den Schritt auf alle Variablen an, die nominal (kategorisch) sind.
-   `all_numeric()` wendet den Schritt auf alle Variablen an, die numerisch sind.

Und natürlich deren Kombination wie `all_nominal_predictors()` oder `all_numeric_predictors()`, die dann eben auf die Prädiktoren, die nominal also Faktoren oder Gruppen repräsentieren oder eben numerischen Variablen, angewendet werden. Du wirst die Anwendung gleich später in den Rezeptschritten sehen, da macht die Sache dann sehr viel mehr Sinn.

Nun ist es aber auch so, dass es bei dem Rezept auf die Reihenfolge der einzelnen Schritte ankommt. Die Reihenfolge der Zutaten und damit der Rezeptschritte sind ja auch beim Kuchenbacken sehr wichtig! Da das Rezept wirklich in der Reihenfolge durchgeführt wird, wie du die einzelnen Schritte angibst, empfiehlt sich folgende Reihenfolge. Du musst natürlich nicht jeden dieser Schritte auch immer durchführen.

::: column-margin
Bitte die Hinweise zur Ordnung der Schritte eines Rezeptes beachten: [Ordering of steps](https://recipes.tidymodels.org/articles/Ordering.html)
:::

1)  Imputation von fehlenden Werten in den Daten.
2)  Individuelle Transformationen auf einzelnen Spalten.
3)  Umwandeln von einzelnen numerischen Variablen in eine diskrete Variable.
4)  Erstellung der Dummyvariablen für jede diskrete Variable.
5)  Eventuell Berücksichtigung der Interaktion zwischen Variablen.
6)  Transformation der numerischen Variablen mit zum Beispiel der Standarisierung oder Normalisierung.
7)  Multivariate Transformationen über alle Spalten hinweg wie zum Beispiel PCA.

Am Ende wollen wir dann natürlich auch die Daten wiederhaben. Das heißt, wir bauen ja das Rezept auf einem Datensatz. Wenn wir dann das fertige Rezept in die Funktion `prep()` pipen können wir über die Funktion `juice()` den ursprünglichen jetzt aber transformierten Datensatz wieder erhalten. Wenn wir das Rezept auf einen *neuen* Datensatz anwenden wollen, dann nutzen wir die Funktion `bake()`. Mit einem neuen Datensatz meine ich natürlich einen [Split in Training- und Testdaten](#sec-data-splitting) von dem ursprünglichen Datensatz. In dem neuen Datensatz müssen natürlich alle Spaltennamen auch enthalten sein, sonst macht die Sache recht wenig Sinn.

## Dummycodierung von $X$ {#sec-preprocess-dummy}

::: column-margin
[Create Traditional Dummy Variables](https://recipes.tidymodels.org/reference/step_dummy.html)
:::

Wir werden immer häufiger davon sprechen, dass wir alle kategorialen Daten in *Dummies* überführen müssen. Das heißt, wir dürfen keine Faktoren mehr in unseren Daten haben. Wir wandeln daher alle Variablen, die ein Faktor sind, in Dummyspalten um. Die Idee von der Dummyspalte ist die gleiche wie bei der multiplen Regression. Da ich aber nicht davon ausgehe, dass du dir alles hier durchgelesen hast, kommt hier die kurze Einführung zur Dummycodierung.

Die Dummycodierung wird nur auf den Features durchgeführt. Dabei werden nur Spalten erschaffen, die $0/1$, für Level vorhanden oder Level nicht vorhanden, beinhalten. Wir werden also nur alle $x$ in Dummies umwandeln, die einem Faktor entsprechen. Dafür nutzen wir dann später eine Funktion, hier machen wir das einmal zu Veranschaulichung per Hand. In @tbl-class-basic-dummy-01 haben wir einen kleinen Ausschnitt unser Schweinedaten gegeben. Wir wollen zuerst die Spalte `sex` in eine Dummycodierung umwandeln.

| infected | age |  sex   |  frailty  |
|:--------:|:---:|:------:|:---------:|
|    1     | 24  |  male  |  robust   |
|    0     | 36  |  male  | pre-frail |
|    0     | 21  | female |   frail   |
|    1     | 34  | female |  robust   |
|    1     | 27  |  male  |   frail   |

: Beispieldatensatz für die Dummycodierung. Wir wollen die Spalten `sex` und `frailty` als Dummyspalten haben. {#tbl-class-basic-dummy-01}

In der @tbl-class-basic-dummy-02 sehen wir das Ergebnis für die Dummycodierung der Spalte `sex` in die Dummyspalte `sex_male`. Wir haben in der Dummyspalte nur noch die Information, ob das Ferkel mänlich ist oder nicht. Wenn wir eine Eins in der Spalte finden, dann ist das Ferkel männlich. Wenn wir eine Null vorfinden, dann ist das Ferkel nicht männlich also weiblich. Das *Nicht* müssen wir uns dann immer merken.

| infected | age | sex_male |
|:--------:|:---:|:--------:|
|    1     | 24  |    1     |
|    0     | 36  |    1     |
|    0     | 21  |    0     |
|    1     | 34  |    0     |
|    1     | 27  |    1     |

: Ergebnis der Dummycodierung der Spalte `sex` zu der Spalte `sex_male`. {#tbl-class-basic-dummy-02}

In der @tbl-class-basic-dummy-03 betrachten wir einen komplexeren Fall. Wenn wir eine Spalte vorliegen haben mit mehr als zwei Leveln, wie zum Beispiel die Spalte `frailty`, dann erhalten wir zwei Spalten wieder. Die Spalte `frailty_robust` beschreibt das Vorhandensein des Levels `robust` und die Spalte `frailty_pre-frail` das Vorhandensein des Levels `pre-frail`. Und was ist mit dem Level `frail`? Das Level wir durch das Nichtvorhandensein von `robust` und dem Nichtvorhandensein von `pre-frail` abgebildet. Beinhalten beide Spalten die Null, so ist das Ferkel `frail`.

| infected | age | frailty_robust | frailty_pre-frail |
|:--------:|:---:|:--------------:|:-----------------:|
|    1     | 24  |       1        |         0         |
|    0     | 36  |       0        |         1         |
|    0     | 21  |       0        |         0         |
|    1     | 34  |       1        |         0         |
|    1     | 27  |       0        |         0         |

: Ergebnis der Dummycodierung für eine Spalte mit mehr als zwei Leveln. {#tbl-class-basic-dummy-03}

Wenn wir einen Faktor mit $l$ Leveln haben, erhalten wir immer $l-1$ Spalten nach der Dummycodierung wieder.

```{r}
pig_dummy_rec <- pig_rec %>% 
  step_dummy(all_nominal_predictors()) 

pig_dummy_rec 
```

Daten generieren

```{r}
pig_dummy_rec %>%
  prep() %>%
  juice() 
```

## Zero Variance Spalten {#sec-preprocess-zv}

::: column-margin
[Zero Variance Filter](https://recipes.tidymodels.org/reference/step_zv.html)

[Near-Zero Variance Filter](https://recipes.tidymodels.org/reference/step_nzv.html)
:::

```{r}
pig_zero_rec <- pig_rec %>% 
  step_zv(all_predictors()) %>% 
  step_nzv(all_predictors())


pig_zero_rec
```

## Standardisieren $\mathcal{N}(0,1)$ {#sec-preprocess-standard}

::: column-margin
[Scaling Numeric Data](https://recipes.tidymodels.org/reference/step_scale.html)

[Centering Numeric Data](https://recipes.tidymodels.org/reference/step_center.html)

[Center and Scale Numeric Data](https://recipes.tidymodels.org/reference/step_normalize.html)
:::

[Transformation von Daten](#sec-eda-transform)

```{r}
pig_scale_center_rec <- pig_rec %>% 
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) 

pig_scale_center_rec 
```

```{r}
pig_scale_center_rec <- pig_rec %>% 
  step_normalize(all_numeric_predictors()) 

pig_scale_center_rec 
```

Daten generieren

```{r}
pig_scale_center_rec %>%
  prep() %>%
  juice() %>% 
  mutate(across(where(is.numeric), round, 2))
```

## Normalisieren $[0; 1]$ {#sec-preprocess-normal}

::: column-margin
[Scaling Numeric Data to a Specific Range](https://recipes.tidymodels.org/reference/step_range.html)
:::

[Transformation von Daten](#sec-eda-transform)

```{r}
pig_range_rec <- pig_rec %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1) 

pig_range_rec 
```

Daten generieren

```{r}
pig_range_rec %>%
  prep() %>%
  juice() %>% 
  mutate(across(where(is.numeric), round, 2))
```

## Imputieren von fehlenden Werten {#sec-preprocess-impute}

::: column-margin
[Step Functions - Imputation](https://recipes.tidymodels.org/reference/index.html#step-functions-imputation)
:::

oder zur [Imputation von fehlenden Werten](#sec-missing)

```{r}
pig_imp_rec <- pig_rec %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_mode(all_nominal_predictors())

pig_imp_rec 
```

Daten generieren

```{r}
pig_imp_rec  %>%
  prep() %>%
  juice() %>% 
  mutate(across(where(is.numeric), round, 2))
```

## Kategorisierung {#sec-preprocess-discrete}

::: column-margin
[Step Functions - Discretization](https://recipes.tidymodels.org/reference/index.html#step-functions-discretization)
:::

```{r}
pig_discrete_rec <- pig_rec %>%
  step_discretize(crp, age, min_unique = 3)

pig_discrete_rec 
```

Daten generieren

```{r}
pig_discrete_tbl <- pig_discrete_rec  %>%
  prep() %>%
  juice() 

pig_discrete_tbl %>% pull(crp) %>% tabyl()
pig_discrete_tbl %>% pull(age) %>% tabyl()
```

Das [R Paket `embed`](https://cran.r-project.org/web/packages/embed/embed.pdf) bietet noch eine Vielzahl an weiteren Funktionen für die Erstellung von kategorialen Variablen.

## Korrelation zwischen Variablen {#sec-preprocess-corr}

::: column-margin
[High Correlation Filter](https://recipes.tidymodels.org/reference/step_corr.html)
:::

[Korrelation von Daten](#sec-lin-reg-corr)

```{r}
pig_corr_rec <- pig_rec %>% 
  step_corr(all_numeric_predictors(), threshold = 0.5)

pig_corr_rec 
```

Daten generieren

```{r}
pig_corr_tbl <- pig_corr_rec  %>%
  prep() %>%
  juice() %>% 
  mutate(across(where(is.numeric), round, 2)) 

```

## Beispiel Gummibärchendaten

```{r}
gummi_tbl <- read_excel("data/gummibears.xlsx") %>% 
  mutate(gender = as_factor(gender),
         most_liked = as_factor(most_liked),
         student_id = 1:n()) %>% 
  select(student_id, gender, most_liked, age, semester, height)  

```

In @tbl-gummi-prepro

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-gummi-prepro
#| tbl-cap: Auszug aus dem Daten zu den Gummibärchendaten.

gummi_raw_tbl <- gummi_tbl %>% 
  mutate(gender = as.character(gender),
         most_liked = as.character(most_liked))

rbind(head(gummi_raw_tbl),
      rep("...", times = ncol(gummi_raw_tbl)),
      tail(gummi_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

```{r}
gummi_rec <- recipe(gender ~ ., data = gummi_tbl) %>% 
  update_role(student_id, new_role = "ID")

gummi_rec %>% summary()

```

```{r}
gummi_full_rec <- gummi_rec %>% 
  step_naomit(all_outcomes()) %>% 
  step_impute_mean(all_numeric_predictors(), -has_role("ID")) %>% 
  step_impute_bag(all_nominal_predictors(), -has_role("ID")) %>% 
  step_discretize(semester, num_breaks = 3, min_unique = 4) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1, -has_role("ID")) %>% 
  step_dummy(all_nominal_predictors(), -has_role("ID")) %>% 
  step_nzv(all_predictors(), -has_role("ID"))

gummi_full_rec
```

Daten generieren

```{r}
gummi_class_tbl <- gummi_full_rec %>%
  prep() %>%
  juice() %>% 
  mutate(across(where(is.numeric), round, 2)) 

```

In der @tbl-gummi-prepro-trans können wir uns die transformierten Daten einmal anschauen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-gummi-prepro-trans
#| tbl-cap: Der transformierte Gummibärchendatensatz nach der Anwendung des Rezepts. 
#| column: page

gummi_class_tbl <- gummi_class_tbl %>% 
  mutate(gender = as.character(gender),
         student_id = 1:n())

rbind(head(gummi_class_tbl),
      rep("...", times = ncol(gummi_class_tbl)),
      tail(gummi_class_tbl)) %>% 
  kable(align = "c", "pipe")
```

Bis hierher haben wir jetzt die Rezepte nur genutzt um uns die Daten aufzuarbeiten. Das ist eigentlich nur ein Schritt in der Klassifikation. Mit der Funktion `workflow()` können wir dann Rezepte mit Algorithmen verbinden. Dann nutzen wir die Funktion `fit()` um verschiedene Daten auf den Workflow anzuwenden. Das musst du aber nicht tun. Du kannst die Rezepte hier auch verwenden um deine Daten einfach aufzuarbeiten und dann eben doch ganz *normale* Statistik drauf zu rechnen.
