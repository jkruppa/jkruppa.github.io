```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Support vector machines {#sec-svm}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, 
               janitor, vip, rpart.plot, see,
               xgboost, conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
##
set.seed(2025429)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Bei dem vorherigen Beispielen haben wir immer unseren Datensatz zu den infizierten Ferkeln genutzt. In diesem Kapitel wolle wir uns aber mal auf einen echten Datensatz anschauen. Wir nutzen daher einmal den Gummibärchendatensatz. Als unser Label und daher als unser Outcome nehmen wir das Geschlecht `gender`. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir nur die Spalte Geschlecht sowie als Prädiktoren die Spalten `most_liked`, `age`, `semester`, und `height`.

```{r}
gummi_tbl <- read_excel("data/gummibears.xlsx") %>% 
  mutate(gender = as_factor(gender),
         most_liked = as_factor(most_liked)) %>% 
  select(gender, most_liked, age, semester, height) %>% 
  drop_na(gender)

```

Wir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein `NA` in der Variable `gender` haben. Wir haben dann insgesamt $n = `r nrow(gummi_tbl)`$ Beobachtungen vorliegen. In @tbl-gummi-prepro sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-gummi-model-compare
#| tbl-cap: Auszug aus dem Daten zu den Gummibärchendaten.

gummi_raw_tbl <- gummi_tbl %>% 
  mutate(gender = as.character(gender),
         most_liked = as.character(most_liked))

rbind(head(gummi_raw_tbl),
      rep("...", times = ncol(gummi_raw_tbl)),
      tail(gummi_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

Unsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Um die Klassifikation mit Entscheidungsbäumen rechnen zu können brauchen wir wie bei allen anderen Algorithmen auch einen Trainings- und Testdatensatz. Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Der Traingsdatensatz ist dabei immer der größere Datensatz. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.

[Im maschinellen Lernen sind alle Datensätze, die weniger als tausend Beobachtungen vorliegen haben, klein.]{.aside}

```{r}
gummi_data_split <- initial_split(gummi_tbl, prop = 3/4)
```

Wir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluß einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.

```{r}
gummi_train_data <- training(gummi_data_split)
gummi_test_data  <- testing(gummi_data_split)
```

Nachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht `gender` als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem `.` Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Es gibt wie immer noch andere Imputationsmöglichkeiten, ich habe mich jetzt aus praktischen Gründen für dies beiden Verfahren entschieden. Ich überspringe hier auch die Diagnose der Imputation, also ob das jetzt eine gute und sinnvolle Imputation der fehlenden Werte war oder nicht. Die Diagnoseschritte müsstest du im Anwendungsfall nochmal im [Kapitel zur Imputation](#sec-missing) nachlesen und anwenden. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.

```{r}
gummi_rec <- recipe(gender ~ ., data = gummi_train_data) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_bag(all_nominal_predictors()) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors())

gummi_rec
```

Alles in allem haben wir ein sehr kleines Modell. Wir haben ja nur ein Outcome und vier Prädiktoren. Trotzdem sollte dieser Datensatz reichen um zu erklären wie Entscheidungsbäume funktionieren.

## Theoretischer Hintergrund

::: column-margin
[Support vector machines](https://www.jeremyjordan.me/support-vector-machines/)
:::

Wir haben 2 Farben von Bällen auf dem Tisch, die wir trennen wollen.

![Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen kranke Personen und die rote die gesunde Personen dar.](images/svm/svm1.png){#fig-class-knn-01 fig-align="center" width="70%"}

Wir nehmen einen Stock und legen ihn auf den Tisch, das funktioniert doch ganz gut, oder?

![Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen kranke Personen und die rote die gesunde Personen dar.](images/svm/svm2.png){#fig-class-knn-01 fig-align="center" width="70%"}

Ein Bösewicht kommt und legt weitere Kugeln auf den Tisch, es funktioniert irgendwie, aber eine der Kugeln ist auf der falschen Seite und es gibt wahrscheinlich einen besseren Platz, um den Stock jetzt zu platzieren.

![Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen kranke Personen und die rote die gesunde Personen dar.](images/svm/svm3.png){#fig-class-knn-01 fig-align="center" width="70%"}

SVMs versuchen, den Knüppel an der bestmöglichen Stelle zu platzieren, indem sie auf beiden Seiten des Knüppels einen möglichst großen Abstand einhalten.

![Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen kranke Personen und die rote die gesunde Personen dar.](images/svm/svm4.png){#fig-class-knn-01 fig-align="center" width="70%"}

Wenn der Bösewicht nun zurückkehrt, ist der Stock immer noch in einer ziemlich guten Position.

![Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen kranke Personen und die rote die gesunde Personen dar.](images/svm/svm5.png){#fig-class-knn-01 fig-align="center" width="70%"}

Es gibt einen weiteren Trick im SVM-Werkzeugkasten, der noch wichtiger ist. Nehmen wir an, der Bösewicht hat gesehen, wie gut du mit einem Stock umgehen kannst, und stellt dich vor eine neue Herausforderung.

![Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen kranke Personen und die rote die gesunde Personen dar.](images/svm/svm6.png){#fig-class-knn-01 fig-align="center" width="70%"}

Es gibt keinen Stock auf der Welt, mit dem man die Kugeln gut spalten kann, also was tun Sie? Du drehst den Tisch natürlich um! Du wirfst die Bälle in die Luft. Dann schnappst du dir mit deinen Profi-Ninja-Fähigkeiten ein Blatt Papier und schiebst es zwischen die Bälle.

![Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen kranke Personen und die rote die gesunde Personen dar.](images/svm/svm7.png){#fig-class-knn-01 fig-align="center" width="70%"}

Wenn man die Bälle von der Position des Bösewichts aus betrachtet, sehen sie so aus, als wären sie durch eine kurvige Linie geteilt.

![Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen kranke Personen und die rote die gesunde Personen dar.](images/svm/svm8.png){#fig-class-knn-01 fig-align="center" width="70%"}

## SVM Algorithm

```{r}
#| message: false
#| warning: false

svm_lin_mod <- svm_linear(cost = 1, margin = 0.1) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification") 
```

degree: The polynomial degree.

```{r}
#| message: false
#| warning: false

svm_poly_mod <- svm_poly(cost = 1, margin = 0.1, degree = 4) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification") 
```

```{r}
#| message: false
#| warning: false

svm_rbf_mod <- svm_rbf(cost = 1, margin = 0.1) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification") 
```

```{r}
#| message: false
#| warning: false

svm_aug_lst <- lst(svm_lin_mod,
                   svm_poly_mod,
                   svm_rbf_mod) %>% 
  map(~workflow(gummi_rec, .x)) %>% 
  map(~fit(.x, gummi_train_data)) %>% 
  map(~augment(.x, gummi_test_data)) %>% 
  map(~select(.x, gender, matches("pred")))

```

::: callout-note
## Kann ich auch eine Kreuzvalidierung und Tuning für die Support Vector Machines durchführen?

Ja, kannst du. Wenn du *nur* eine Kreuzvalidierung durchführen willst, findest du alles im @sec-knn für den $k$-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf den Support Vector Machine Algorithmus anwenden. Wenn du den Support Vector Machine Algorithmus auch tunen willst, dann schaue einfach nochmal im @sec-xgboost zum Tuning von xgboost rein.
:::

```{r}
svm_cm <- svm_aug_lst %>%
  map(~conf_mat(.x, gender, .pred_class))
svm_cm
```

```{r}
svm_cm %>% 
  map(summary)  %>% 
  map(~select(.x, .metric, .estimate)) %>% 
  reduce(left_join, by = ".metric") %>% 
  set_names(c("metric", "linear", "poly", "radial")) %>% 
  mutate(across(where(is.numeric), round, 3))
```

```{r}
roc_tbl <- svm_aug_lst %>% 
  map(~roc_curve(.x, gender, .pred_w, event_level = "second")) %>% 
  bind_rows(.id = "model")
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| label: fig-roc-svm
#| fig-cap: "Darstellung der Vorhersagegüte der drei Modelle linear, polynomial und radial."

roc_tbl %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  theme_minimal() +
  geom_path() +
  geom_abline(lty = 3) + 
  scale_color_okabeito()

```
