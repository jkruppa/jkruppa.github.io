```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, patchwork)
```

# Beta Regression {#sec-beta}

*Letzte Änderung am `r format(fs::file_info("stat-modeling-beta.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

link zu test auf anteile

wirkungsgrad?

Selten n

Im folgenden Kapitel

::: callout-note
## Moment, ich möchte nur zwei Wahrscheinlichkeiten $p_1$ und $p_2$ vergleichen!

Wenn du nur wissen willst, ob sich zwei Wahrscheinlichkeiten unterscheiden, dann musst du einmal in dem Kapitel [Vergleich zweier Anteile $p_1$ und $p_2$](#sec-chisquare-prop-test) nachschauen. Dort zeige ich dir wie du dann einen statistischen Test rechnen kannst um rauszufinden ob sich $0.5$ signifikant von $0.7$ unterscheidet. In diesem Kapitel schauen wir uns dann pro Faktor sehr viele Wahrscheinlichkeiten als Outcome an.
:::

Daher sieht unser Modell wie folgt aus. Wir haben ein $y$ und $p$-mal $x$. Wobei $p$ für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren ist unser $y$ normalverteilt. Das ist hier sehr wichtig, denn wir wollen ja eine multiple gaussian lineare Regression rechnen.

$$
y \sim x_1 + x_2 + ... + x_p 
$$

::: callout-tip
## Weitere Tutorien für die Beta Regression

Wie immer gibt es auch für die Frage nach dem Tutorium für die Beta Regression verschiedene Quellen. Ich kann noch folgende Informationen und Hilfen empfehlen.

-   [A guide to modeling proportions with Bayesian beta and zero-inflated beta regression models](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide) hilft besonders, wenn du tiefer in die Matrie einsteigen willst. Du erhälst bei dem Tutorium einen vollständigen Überblick über die Möglichkeiten. Weit mehr als ich hier mache.
-   Das Tutorium [Beta Regression for Percent and Proportion Data](https://rcompanion.org/handbook/J_02.html) leidet etwas unter dem Mangel an erklärenden Text. Hier wurde anscheinend erst der R Code generiert und der Text sollte folgen. Das schneit hier aber (noch) nicht der Fall zu sein. Als Überblick lohnt sich das Tutorium aber dennoch.
-   [What is the intuition behind beta distribution?](https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution) erklärt nochmal was die Idee der Beta Regression eigentlich ist und was eine Betaverteilung eigentlich beschreibt.
-   Das R Paket `{betareg}` und die entsprechende Vignette [Beta Regression in R](https://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf) liefert wichtige Informationen über die Umsetzung der Beta Regression in R.
-   [Causal inference with beta regression](https://solomonkurz.netlify.app/blog/2023-06-25-causal-inference-with-beta-regression/) liefert eine sehr umfangreiche Überbick über die Beta Regression und das Testen mit der ANOVA. Dann aber auch nicht in dem klassischen Ansatz, den ich hier normalerweise rechne, sondern als bayesianische Variante.
-   Genauso betrachtet das Tutorium [Model Estimation by Example - Bayesian Beta Regression](https://m-clark.github.io/models-by-example/bayesian-beta-regression.html) auch die bayesianische Variante der Beta Regression, so dass du hier vermutlich eher weniger fündig wirst. Ich fand den Überblick aber gut und schön zu lesen -- vorallem war er auch nicht so lang.
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, broom, betareg, car,
               see, performance, parameters, agridat, mfp,
               emmeans, multcomp, rcompanion, ggbeeswarm,
               marginaleffects, conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflicts_prefer(dplyr::summarise)
cb_pal <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
            "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Wie immer schauen wir uns verschiedene Datensätze an, Visualisieren die Zusammenhänge und rechnen dann verschiedene Modelle, die passen könnten. Beginnen möchte ich mit einem Datensatz zu dem Jagederfolg in \[%\] von Schneefüchsen in verschiedenen Habitaten. Die Daten sind etwas gekürzt, wir haben nur den Jagederfolg und keine Informationen zu den Habitaten. Des Weiteren wollen wir schauen, ob der Jagderfolg der Eisfüche von der standardisierten Schneehöhe in \[cm\] abhängt. Wir haben hier mehr oder minder die Schneehöhe in dem Habit gemittelt. Der Eisfuchs jagt ja nicht immer an der perfekt gleichen Stelle, wo wir die Schneehöhe kennen.

```{r}
hunting_tbl <- read_excel("data/hunting_fox.xlsx") %>% 
  mutate(proportion = round(success/attempts, 2))
```

In der @tbl-beta-fox siehst du einen Auszug aus den Daten. Wir haben die Schneehöhe gemessen und geschaut von wie vielen Anläufen `attempts` eine Maus unter dem Schnee zu fangen erfolgreich war `success` oder eben ein Fehlschlag `fail`. Daraud haben wir dann die Erfolgsrate `proportion` berechnet. Wir haben einfach den Anteil der Erfolge eine Maus zu fangen an den gesamten Versuchen berechnet.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-beta-fox
#| tbl-cap: "Auszug aus dem Daten zu dem Jagderfolg in [%] von Eisfüchsen in abhängigkeit von der Schneehöhe in [cm]."

hunting_raw_tbl <- hunting_tbl %>% 
  mutate_all(as.character)
  

rbind(head(hunting_raw_tbl, 4),
      rep("...", times = ncol(hunting_raw_tbl)),
      tail(hunting_raw_tbl, 4)) %>% 
  kable(align = "c", "pipe")

```

Im Weiteren schauen wir uns einen Datensatz zu Brokkoli an. Wir wollen hier einmal schauen, ob wir das Zielgewicht von $500g$ erreichen. Wir sind aber daran interessiert die Rate von untergewichtigen Brokkoli möglichst klein zu halten. Deshalb schauen wir uns in dieser Auswertung den Anteil von Brokkoli unter der Zielmarke von $500g$ für zwei Düngezeitpunkte sowie drei Düngestufen an. Wir müssen hier jetzt die Daten etwas mehr aufbereiten, da wir mehr Informationen in den Daten haben als wir wirklich brauchen.

```{r}
#| message: false
broc_tbl <- read_excel("data/broccoli_weight.xlsx") %>% 
  filter(fert_time %in% c("early", "late")) %>% 
  mutate(fert_time = factor(fert_time, levels = c("early", "late")),
         fert_amount = as_factor(fert_amount),
         block = as_factor(block)) %>%
  select(fert_time, fert_amount, block, weight) %>% 
  filter(weight < 500) %>% 
  mutate(proportion = weight/500) %>% 
  select(-weight)
```

In der @tbl-beta-broc siehst du einmal den Auszug aus den Brokkolidaten. Wir wollen jetzt sehen, ob wir in den Behandlungsfaktoren einen Unterschied bezüglich der Anteile der untergewichtigen Brokkoliköpfe finden. Tendenziell wollen wir eine Kombination finden, die uns natürlich möglichst schwere Köpfe beschert.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-beta-broc
#| tbl-cap: "Auszug aus dem Daten zu den Zielgewichten von Brokkoli zu zwei Düngezeitpunkten und drei Düngestufen."

broc_raw_tbl <- broc_tbl %>% 
  mutate_all(as.character)
  
rbind(head(broc_raw_tbl, 4),
      rep("...", times = ncol(broc_raw_tbl)),
      tail(broc_raw_tbl, 4)) %>% 
  kable(align = "c", "pipe")

```

Abschließend schauen wir nochmal in das R Paket `{agridat}` und nehmen von dort den Datensatz `salmon.bunt` welcher eine Pilzinfektion von Weizenlinien beschreibt. Mehr dazu dann auf der Hilfeseite [Fungus infection in varieties of wheat](https://kwstat.github.io/agridat/reference/salmon.bunt.html) in der Vignette zum R Paket. Ich möchte später die Faktoren `gen` für die genetischen Linien und die Pilzarten `bunt` für die Anteile der Pilzinfektionen sortiert haben. Das mache ich dann einmal mit der Funktion `fct_reorder()` welche mir erlaubt einen Faktor nach einer anderen Variable zu sortieren. Wir haben zwei Wiederholungen `rep`, die auch so nicht helfen. Deshalb mittlere ich mit `summarise()` über die beiden Wiederholungen die Prozente der Pilzinfektionen des Weizen.

```{r}
#| message: false
#| warning: false
data(salmon.bunt)
fungi_tbl <- salmon.bunt %>% 
  as_tibble() %>% 
  select(gen, bunt, rep, percent = pct) %>% 
  mutate(gen = fct_reorder(gen, percent),
         bunt = fct_reorder(bunt, percent),
         percent = percent/100 + 0.001) 
```

In der @tbl-beta-fungi siehst du dann einmal den Auszug aus unseren Weizendaten mit einer Pilzinfektion. Wir haben 10 genetische Linien sowie 20 Pilzarten vorliegen. Daher ist der Datensatz ziemlich groß, was die Möglichkeiten der Faktorkombinationen angeht.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-beta-fungi
#| tbl-cap: "Auszug aus dem Daten zu den zehn Weizenlinien und den zwanzig Pilzarten. Es wurde der Anteil an infizierten Weizen [%] gemessen."

fungi_raw_tbl <- fungi_tbl %>% 
  mutate_all(as.character) 
  
rbind(head(fungi_raw_tbl, 4),
      rep("...", times = ncol(fungi_raw_tbl)),
      tail(fungi_raw_tbl, 4)) %>% 
  kable(align = "c", "pipe")

```

Damit habe wir dann einige spannende Datensätze vorliegen, die wir nutzen können um die verschiedenen Aspekte der Beta Regression anzuschauen. Nicht immer muss es ja eine Beta Regression sein, wir haben auch die Möglichkeit unsere Fragestellung mit anderen Modellen eventuell anders oder gar besser zu beantworten.

## Visualisierung

Auch hier beginnen wir einmal mit der Visualisierung der Daten. Zuerst schauen wir uns einmal die Daten zu dem Jagderfolg der Eisfüchse in verschiedenen Habitaten in Abhängigkeit zu der Schneehöhe an. Wenn der Schnee zu hoch liegt, werden die Füchse weniger Jagderfolg haben. In der @fig-beta-fox-scatter sehen wir einmal den Jagerfolg von der Schneehöhe aufgetragen. Wir erkennen, dass wir zwar anfänglich eher einen linearen Zusammenhang haben könnten, aber bei höheren Schneedichten dann sehr schnell einen Abfall des Jagderfolges beobachten. Wir schauen uns dann gleich mal verschiedene Modelle an um eine Kurve durch die Punkte zu legen.

```{r}
#| warning: false
#| message: false
#| label: fig-beta-fox-scatter
#| fig-align: center
#| fig-height: 4
#| fig-width: 5
#| fig-cap: "Zusammenhang zwischen dem Jagederfolg von Eisfüchsen und der Schneehöhe in den jeweiligen beobachteten Habitaten."

hunting_tbl %>% 
  ggplot(aes(snow_height, proportion)) +
  theme_minimal() +
  geom_point() +
  labs(y = "Jagderfolg [%]", x = "Standardisierte Schneehöhe [cm]") +
  ylim(0, 1)
```

In der @fig-beta-broc sehen wir einmal die Verteilung unser untergewichtigen Brokkoli für die beiden Düngezeitpunkte und Düngemengen. Wir könnten annehmen, dass wir tendenziell bei einer höheren Düngemenge einen größeren Anteil an erreichtem Zielgewicht erhalten. Global betrachtet scheint es aber nicht so große Effekte zu geben. Wir schauen uns diese Beispiel dann einmal für den Gruppenvergleich an.

```{r}
#| warning: false
#| message: false
#| label: fig-beta-broc
#| fig-align: center
#| fig-height: 4
#| fig-width: 5
#| fig-cap: "Zusammenhang zwischen erreichten Zielgewicht von $500g$ bei Brokkoli [%] und der Düngermenge sowiw dem Düngezeitpunkt."

broc_tbl %>% 
  ggplot(aes(x = fert_amount, y = proportion, color = fert_time)) +
  theme_minimal() + 
  labs(y = "[%] erreichtes Zielgewicht", x = "Düngemenge [mg/l]",
       color = "Düngezeitpunkt") +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +
  geom_beeswarm(dodge.width = 0.8) +
  theme(legend.position = "top") +
  scale_color_okabeito()
```

Abschließend schauen wir uns in der @fig-beta-fungi die Heatmap der genetischen Linien und der Art des Pilzes an. Ich habe die Heatmap so erstellt, dass eine viel Infektion rot dargestellt wird und wenig Infektion blau. Durch die Sortierung der Faktoren nach dem Infektionsgrad können wir sehr schön die Linien voneinander unterscheiden. Teilweise werden einige Linien von dem Pilz förmlich aufgefressen während andere Weizenlinien kaum befallen werden. Auch scheinen einige Arten des Pilzen mehr Weizenlinien befallen zu können als andere Pilzarten. So ist die Pilzart B189 extrem erfolgreich bei einer großen Anzahl an Linien. Die Art B1 hingegen kann mehr oder minder nur zwei Weizensorten befallen.

```{r}
#| warning: false
#| message: false
#| label: fig-beta-fungi
#| fig-align: center
#| fig-height: 4
#| fig-width: 8
#| fig-cap: "Sortierte Heatmap der genetischen Linien und der Art des Pilzes. Farbig dargestellt sind die Anteile an infizierten Weizen in [%]. Nach dem Grad der Infektion wurden die Faktoren sortiert um eine bessere Übersicht zu erhalten."


fungi_tbl %>% 
  group_by(bunt, gen) %>% 
  summarise(percent = mean(percent)) %>% 
  ggplot(aes(x = gen, y = bunt, fill = percent)) +
  theme_minimal() +
  geom_tile() +
  scale_fill_gradientn(colors = c("#375997", "gray", "firebrick"),
                       breaks = seq(0, 1, 0.1), 
                       limits = c(0, 1)) + 
  labs(y = "Art des Pilzes", x = "Genetische Linie des Weizens",
       fill = "[%] infiziert")

```

## Fit des Modells

Dann haben wir jetzt unsere Daten und wissen auch grob was in den Daten stecken könnte. Jetzt wollen wir einmal die verschieden Datensätze auswerten. Je nach Fragestellung können wir da verschiedene Modelle nutzen. Wie immer stelle ich auch Alternativen zu der Beta Regression vor. Wir schauen uns zum einen eine einfache Gaussian Regression an, die passt zwar nicht so richtig zu dem Outcome, aber unter bestimmten Voraussetzungen kann die Gaussian Regression Sinn machen. Wenn wir Erfolg/Misserfolg in unseren Daten als Outcome haben, dann können wir auch eine logistische Regression rechnen. Dementsprechend zeige ich die Anwendung auch einmal auf den Eisfuchsdaten, wo wir ja wissen wie oft ein Erfolg und ein Fehlschlag vorgekommen ist.

### ... mit der Gaussian Regression

Fangen wir also einmal mit der etwas groben Variante an. Wir rechnen einfach eine lineare Regression unter der Annahme das unser Outcome normalverteilt ist. Das stimmt zwar nur begrenzt für eine Wahrscheinlichkeit, die zwischen 0 und 1 liegt, aber rechnen können wir ja erstmal viel. Besonders wenn du Wahrscheinlichkeiten berechnet hast, die nicht sehr viele Nullen und Einsen beinhalten sondern mehr um die 0.5 streuen, dann kann deine Auswertung auch mit einer Gaussian Regression funktionieren. Wie immer kommt es dann auf den Einzelfall an, aber die Gaussian Regression liefert dann eben auch einen sehr gut zu verstehenden und interpretierenden Effektschätzer.

Wir wenden jetzt also einfach mal die Gaussian Regression mit der Funktion `lm()` auf unsere Daten zu dem Jagderfolg der Eisfüchse an. Wenn du dir nochmal die @fig-beta-fox-scatter anschaust, dann siehst du, dass wir nicht so viele Beobachtungen nah der Eins und der Null haben. Darüber hinaus ist eine wage Linearität zu erkennen. Oder anderherum, die Daten sehen jetzt nicht so schlimm aus, dass wir nicht eine Gerade durch die Punkte legen könnten.

```{r}
hunting_lm_fit <- lm(proportion ~ snow_height, data = hunting_tbl)
```

Dann schauen wir uns einmal die Koeffizienten des Modells einmal an. Wir haben einen `Intercept` von über Eins, was natürlich keinen Sinn ergibt. Wir können keinen Jagederfolg von über Eins bei einer Schneehöhe von Null haben. Hier sieht man schon, dass das Modell nicht so gut für die Randbereiche funktioniert. Dennoch haben wir einen Abfall durch die Steigung der Schneehöhe vorliegen. Wir erkennen, dass pro Zentimeter mehr Schnee der Jagderfolg um $0.0156$ oder eben $1.56\%$ signifikant zurückgeht. Das ist ein Ergebnis mit dem wir leben könnten.

```{r}
hunting_lm_fit 
```

Wir können uns auch den Effekt der Schneehöhe auf den Jagderfolg einmal mit der Funktion `avg_slopes()` aus dem R Paket `{marginaleffects}` wiedergeben lassen. Für eine so simple Gaussion Regression ist der marginale Effekt gleich der Steigung, aber wenn wir gleich ein anderes Modell nehmen, dann wird es schon komplizierter. Mehr dazu dann auf der Hilfeseite zu [Marginal Effects Zoo - Slopes](https://marginaleffects.com/vignettes/slopes.html) wo du dann auch mehr über *marginal effects* lernen kannst. Wie du dann siehst, erhalten wir hier den gleichen Wert für die Steigung.

```{r}
#| message: false
#| warning: false
avg_slopes(hunting_lm_fit)
```

Schauen wir einmal wie das Bestimmtheitsmaß $R^2$ aussieht. Hier haben wir einen Wert von $0.758$ und damit können wir durch die Gerade gut $75\%$ der Varianz erklären. Das ist jetzt nicht der beste Werte und wir schauen uns am Ende nochmal in der @fig-beta-fox-predict wie die Gerade durch die Punkte läuft.

```{r}
hunting_lm_fit %>% r2()
```

Das einmal als sehr schneller und kurzer Einwurf der Gaussian Regression auf einem Outcome mit Prozenten. Es ist nicht ideal und weit weg von der Empfehlung. Aber wenn du einen statistischen Engel anfahren willst und mit der Interpretation ganz gut leben kannst, dann ist eine lineare Modellierung nicht so dramatisch. Achtung eben an den Rändern. Du erhälst eben auch schnell mal vorhergesagte Werte außerhalb von den Grenzen einer Wahrscheinlichkeit.

### ... mit einer logistischen Regression

Ja, auch dieses Problem können wir mit einer logistischen Regression angehen. Wenn du auch mal in den anderen Kapiteln geschaut hast, dann wundert es dich vermutlich nicht mehr, dass die logistische Regression außerhalb der Agarwissenschaften zu einer der beliebtesten Analysewerkzeugen gehört. Hier müssen wir aber unser Outcome etwas anders der Funktion `glm()` für die logistische Regression übergeben. Wir nutzen nämlich dafür das [Wilkinson-Rogers Format](https://edgarhassler.com/posts/wilkinson-rogers-notation/) welches dann den Anteil an Erfolgen an Fehlschlägen beschreibt. Wir schreiben aber in das Modell die konkrete Anzahl an Erfolgen und Fehlschlägen.

$$
(Success|Failure) \sim x_1 + x_2 + ... + x_p 
$$

In R würden wir dann die *zwei* Spalten mit der Anzahl an Erfolgen und Fehlschlägen mit `cbind()` zusammenfassen und in `glm()` ergänzen. Dieses Format haben wir dann auch bei unseren Eisfüchsen vorliegen. In unseren Daten haben wir ja die Spalte `success`, welche die Anzahl Jagderfolge beschreibt sowie die Anzahl der Fehlschläge in der Spalte `fail`. Wichtig ist hier, dass wir wirklich die beiden Spalten mit den jeweiligen Anzahlen haben. Daher würden wir dann in R wie folgt schreiben.

```{r}
hunting_log_fit <- glm(cbind(success, fail) ~ snow_height, 
                       data = hunting_tbl, family = binomial)
hunting_log_fit
```

Damit haben wir dann auch unseren Koeffizienten des Intercept und der Steigung. Da die logistische Regression auf dem Logit-Link rechnet, können wir die Steigung von $-0.08326$ nicht direkt interpretieren. Daher müssen wir die Steigung wieder von dem Logit-Link auf unsere ursprüngliche Skala (eng. *response*) zurückrechnen. Dafür gibt es einen einfachen Trick mit der "Teile durch Vier" Regel oder aber eine entsprechende Funktion im R Paket `{marginaleffects}`. Wichtig ist auch hier, uns interessieren nicht die Odds Ratios aus einer logistischen Regression, da wir an dem Effekt der Steigung interessiert sind. Wir wollen ja wissen, wie sich der Jagderfolg in \[%\] durch die steigende Schneeschöhe verändert.

::: callout-note
## "Teile durch Vier" Regel

Die "Teile durch Vier" Regel (eng. *Divide by 4 Rule*) erlaubt uns von dem Koeffizienten aus der logistischen Regression auf die wahre Steigung der Geraden zu schließen. Da die logistische Regression auf dem Logit-Link rechnet, ist unsere $-0.08$ *nicht* die Steigung der Geraden. Es gibt hier die Daumenregel, den Koeffizienten durch Vier zu teilen und so einen annähernd korrekten Wert zu erhalten. In unserem Fall also $-0.08326/4 = -0.021$. Damit haben wir nicht den exakten Wert der Steigung, aber eine recht guten Wert.
:::

Mit der Funktion `avg_slopes()` können wir uns aus dem Modell die Steigung der Gerade berechnen lassen. Wir haben dann nicht mehr die Logit-Skala vorliegen sondern sind wieder auf unserer ursprünglichen Skala der Prozente. Im Gegensatz zur "Teile durch Vier" Regel ist der Wert von $-0.15$ natürlich genauer. Wir sehen, dass wir fast einen ähnlichen Wert für die Reduzierung des Jagderfolges der Eisfüchse wie bei der Gaussian Regression erhalten.

```{r}
#| message: false
#| warning: false
avg_slopes(hunting_log_fit)
```

Leider gibt es für eine logistische Regression mit einem $(Success/Failure)$-Outcome kein kein *echtes* Bestimmtheitsmaß $R^2$. Hier greifen wir auf das R Paket `{rcompanion}` zurück. Wir rechnen auch dabei auch hier kein *echtes* Bestimmtheitsmaß $R^2$ aus, sondern den Vergleich zu einem Null-Modell in dem wir gar keine Variable als Einfluss mit ins Modell nehmen. Das folgende Bestimmtheitsmaß $R^2$ beantwortet also eher die Frage, ob wir besser mit unserem Modell mit der Schneehöhe sind, also mit einem Modell ohne die Schneehöhe.

```{r}
hunting_log_fit %>% 
  nagelkerke() %>% 
  pluck("Pseudo.R.squared.for.model.vs.null")
```

Hm, am Ende würde ich es lassen. Keine der Bestimmtheitsmaße $R^2$ ist wirklich sinnig. Das erste Bestimmtheitsmaß ist viel zu niedrig, dafür das wir so nahe an den Koeffizienten der Gaussian Regression sind. Dafür sind die anderen beiden Bestimmtheitsmaße viel zu optimistisch, wie wir gleich in der @fig-beta-fox-predict sehen werden. Aber damit haben wir auch gesehen, dass wir die Analyse auch mit einer logistischen Regression rechnen können.

### ... mit dem R Paket `{betareg}`

Jetzt haben wir uns durch andere Modellierungen durchgearbeitet und wollen uns jetzt einmal die Beta Regression anschauen. Die Beta Regression ist in dem R Paket `{betareg}` implementiert und kann über die Funktion `betareg()` angewendet werden. Wir haben hier noch den Sonderfall, dass wir das Modell mit einem `|` schreiben können. Wir können nämlich bei der Beta Regression auch die Varianz global schätzen oder aber für eine Variable adjustieren. Wir wählen hier einmal beide Schreibweisen in den folgenden Tabs und schauen was dann herauskommt. Sonst müssen wir bei der Beta Regression erstmal nichts beachten - wie immer schauen wir dann nochmal, ob unser Modell auch gut funktioniert hat.

::: panel-tabset
## Mit `~ snow_height`

Das simpleste Modell wäre hier, dass wir einfach den Jagderfolg in \[%\] durch die Schneehöhe modellieren. Wir nehmen dann mehr oder minder Varianzhomogenität an und erlauben keinen gesonderten Varianzterm. Das kann gut funktionieren, aber meistens variiert die Varianz der $y$-Werte über die $x$-Werte. Wir haben meist eine größere Varianz bei größeren $x$-Werten.

```{r}
hunting_beta_fit <- betareg(proportion ~ snow_height, data = hunting_tbl)
hunting_beta_fit
```

Wir sehen, dass wir unseren Koeffizienten erhalten, der fast unserer logistischen Regression entspricht. Jedenfalls numerisch. Aber auch hier dürfen wir uns nicht blenden lassen, wir rechnen auch in einer Beta Regression auf dem Logit-Link. Deshalb müssen wir die Steigung der Schneehöhe dann erst wieder zurückrechnen. Auch hier hilft die Funktion `avg_slopes()` aus dem R Paket `{marginaleffects}`.

```{r}
#| message: false
#| warning: false
avg_slopes(hunting_beta_fit)
```

Wir sehen, dass mit jedem Zentimeter mehr Schnee der Jagderfolg um $-0.0149$ oder $-1.49\%$ zurückgeht. Das sind ähnliche Zahlen wie auch schon bei der logistischen Regression sowie der Gaussian Regression.

## Mit `~ snow_height | snow_height`

Wenn wir erlauben, dass sich die Varianz der $y$-Werte über den Verlauf der $x$-Werte ändern darf, dann schreiben wir nochmal die Variable, die die Varianz verursacht hinter das Symbol `|`. Wir haben hier nur die Schneehöhe vorliegen, so dass unser Modell sich dann von alleine ergibt. Meistens macht es mehr Sinn die Varianz auch über eine Variable mit zu modellieren. Schauen wir mal, ob wir mit der separaten Modellierung der Varianz der Schneehöhe mehr erreichen.

```{r}
hunting_beta_phi_fit <- betareg(proportion ~ snow_height | snow_height, data = hunting_tbl)
hunting_beta_phi_fit
```

Aber auch hier dürfen wir uns nicht blenden lassen, wir rechnen auch in einer Beta Regression auf dem Logit-Link. Deshalb müssen wir die Steigung der Schneehöhe dann erst wieder zurückrechnen. Auch hier hilft die Funktion `avg_slopes()` aus dem R Paket `{marginaleffects}`.

```{r}
#| message: false
#| warning: false
avg_slopes(hunting_beta_phi_fit)
```

Spannenderweise ist hier der Effekt der Schneehöhe noch einen Tick geringer mit $-0.0132$ pro Zentimeter mehr Schnee. Dann müssen wir uns gleich einmal anschauen welches Modell den niedrigeren AIC-Wert hat.
:::

Dann berechnen wir einmal die AIC-Werte für die beiden Modelle. Ein niedriger AIC-Wert ist besser. Je kleiner oder negativer ein AIC-Wert eines Modells ist, desto besser ist das Modell im Vergleich zu einem anderen Modell. Das AIC ist ein Maß dafür, wie gut die Daten durch das Modell erklärt werden, korrigiert um die Komplexität des Modells. Wir berechnen mit der Funktion `AIC()` einmal die AIC-Werte der beiden Beta Regressionsmodellen.

```{r}
AIC(hunting_beta_fit)
AIC(hunting_beta_phi_fit)
```

Die absoluten Zahlen vom AIC sind nicht von Bedeutung. Erstmal sehen wir, dass unser Modell mit der Berücksichtigung der Varianz durch die Schneehöhe `hunting_beta_phi_fit` besser ist als das reine Modell ohne Berücksichtigung. Wir wollen aber immer die Differenzen von AIC betrachten. Du findest in der wissenschaftlichen Veröffentlichung [Multimodel Inference: Understanding AIC and BIC in Model Selection](https://faculty.washington.edu/skalski/classes/QERM597/papers_xtra/Burnham%20and%20Anderson.pdf) mehr Informationen zu den Entscheidungen [@burnham2004multimodel, pp. 270-271]. Wichtig ist hier, wenn die Differenz größer ist als 2, dann haben wir einen signifikanten Unterschied zwischen den Modellen und wir sollten das Modell mit dem niedrigeren AIC nehmen.

```{r}
AIC(hunting_beta_fit) - AIC(hunting_beta_phi_fit)
```

Wir nehmen dann mal auf jeden Fall das Modell mit der Berücksichtigung der Varianz durch die Schneehöhe `hunting_beta_phi_fit`. Damit würden wir einen Effekt der Schneehöhe von $-0.0132$ berichten. Berechnen wir jetzt nochmal als Vergleich das Bestimmtheitsmaß $R^2$ und sehen, dass der Wert schon besser ist. Damit können wir dann auf jeden Fall leben.

```{r}
hunting_beta_phi_fit %>% r2()
```

In der @fig-beta-fox-diagnostic sehen wir nochmal die Abbildungen der Modelldiagnostik. Wie du sehen kannst, sind die Ergebnisse zufriedenstellend. Der Residualplot sieht aus, wie wir ihn erwarten würden. Unsere Beobachtungen streuen um die Gerade. Auch haben wir keine Ausreißer vorliegen. Unsere Werte der *Cook's distance* sind okay. Der Grenzwert wäre hier $4/n = 4/21 = 0.19$. Unsere erste Beobachtung würde den Grenzwert reißen, aber das ist noch okay so. Das passt auch zu der Abbildung der Residuen. Auch die *Generalized leverage* deutet auf einen Ausreißer hin. Wir haben hier den Grenzwert von $3(k+1)/n$ mit $k$ gleich der Anzahl an Variablen im Modell. Somit liegt unser Grenzwert bei $3(1+1)/21 = 0.29$. Auch der abschließende Residualplot sieht so aus, wie wir ihn erwarten würden. Als Fazit lässt sich ziehen, das wir zwar eine Beobachtung drin haben, die etwas am Rand liegt, was hier aber für mich noch nicht fürs Entfernen spricht.

```{r}
#| warning: false
#| message: false
#| label: fig-beta-fox-diagnostic
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Ausgabe ausgewählter Modelgüteplots der Funktion `plot()`. Im Gegensatz zu der Funktion `check_model()` musst du wissen, was du erwarten würdest."
#| fig-subcap: 
#|   - "Residualplot I"
#|   - "Cooks distance"
#|   - "Generalized leverage"
#|   - "Residualplot II"
#| layout-nrow: 2

plot(hunting_beta_phi_fit)
```

Damit hätten wir höchstens einen Ausreißer, aber das kann bei echten Daten schon mal vorkommen, dass nicht alle Datenpunkte perfekt zu einem Modell passen. In der @fig-beta-fox-predict siehst du nochmal die Daten der Jagderfolge der Eisfüche im Zusammenhang mit der Schneehöhe dargestellt. Ich sehe da keinen eindeutigen Ausreißer und deshalb lasse ich alle Beobachtungen im Modell. Im Weiteren siehst du einmal die Modelle, die ich gerechnet habe, jeweils als Kurve dargestellt. Das beste Modell ist das `betareg_phi` Modell, was auch ziemlich gut durch die Punkte läuft. Du siehst besonders gut, wie das lineare Modell `lm` leider die Punkte nur sehr unzureichend an den Rändern trifft. Kann ja das Modell auch nicht anders, es muss ja eine Linie sein. Beachte aber vor allem die Unterschiede in den Effekten. Das lineare Modell mit der Gaussian Regression hat einen viel größeren Effekt berechnet als das beste Modell mit der Beta Regression.

```{r}
#| warning: false
#| message: false
#| label: fig-beta-fox-predict
#| fig-align: center
#| fig-height: 6
#| fig-width: 7
#| fig-cap: "Zusammenhang zwischen dem Jagederfolg von Eisfüchsen und der Schneehöhe in den jeweiligen beobachteten Habitaten zusammen mit den vier vorgestellten Modellen. Das Modell `betareg_phi` ist dabei das Modell, was die Daten am besten beschreibt."

hunting_tbl %>% 
  ggplot(aes(snow_height, proportion)) +
  theme_minimal() +
  geom_point() +
  geom_line(aes(y = predict(hunting_log_fit, type = "response"), color = "log")) +
  geom_line(aes(y = predict(hunting_lm_fit, type = "response"), color = "lm")) +
  geom_line(aes(y = predict(hunting_beta_fit, type = "response"), color = "betareg")) +
  geom_line(aes(y = predict(hunting_beta_phi_fit, type = "response"), color = "betareg_phi")) +
  scale_color_manual(name = "Modell", values = cb_pal[2:5])
```

::: callout-note
## Wo ist die mathematische Formel?

Jetzt haben wir zwar schön das Modell mit der Beta Regression geschätzt aber leider keine mathematische Formel erhalten. Das ist jetzt ja eigentlich auch eine andere Fragestellung. Daher nutzen wir für die Erstellung der mathematischen Formel auch nicht die Beta Regression sondern die nicht linear Regression. Prinzipiell ginge natürlich auch die lineare Regression, aber da wissen wir ja schon, dass die nicht so super funktioniert. Hier jetzt also der Weg um die Koeffizienten einer nicht lineare Regression zu bestimmen, die durch die Punkte eine Kurve legt.

Auch hier haben wir die Wahl zwischen der Funktion `nls()` aus dem Standardpaket in R oder aber der Funktion `mfp()` aus dem gleichnamigen R Paket `{mfp}`. In den beiden Tabs zeige ich dir einmal die schnelle Anwendung. Wenn du mehr lesen willst dann kannst du nochmal in dem [Kapitel zur nicht linearen Regression](#sec-non-linear) reinschauen. Wie immer führe ich hier den Code mehr aus als ihn dann zu erklären.

::: panel-tabset
## Mit `nls()`

Die Funktion `nls()` hat die Herausforderung, dass wir Startwerte für die Koeffizienten unserer mathematischen Formel übergeben müssen. Darüber hinaus müssen wir auch in der Funktion `nls()` eine Formel vordefinieren, die dann eben mit den Werten der Koeffizienten gefüllt wird. Wenn es dir auch eher schwer fällt in einer Punktewolke eine mathematische Funktion zu sehen, dann ist die Funktion `nls()` eine Herausforderung. Es gibt zwar den einen oder anderen Trick, aber am Ende müssen wir schauen, ob es dann passt mit dem Ergebnis.

Wir brauchen erstmal Startwerte für unsere mathematische Formel $y = a - x^b$. Ein Trick ist, erstmal eine lineare Regression mit `lm()` zu rechnen und die Koeffizienten dann als Startwerte in `nls()` zu nutzen.

```{r}
lm(log(proportion) ~ snow_height, hunting_tbl)
```

Mit unseren Startwerten können wir dann einmal schauen, ob unser Modell konvergiert. Dann haben wir auch die Werte für die Koeffizienten $a$ und $b$ und können das Modell als mathematische Formel aufschreiben.

```{r}
nls(proportion ~ a - I(snow_height^b), data = hunting_tbl, 
    start = c(a = exp(1.0324), b = -0.0354),
    control = nls.control(maxiter = 1000))
```

Wir können auch nochmal eine $e$-Funktion mit $y = a - e^{b \cdot x}$ nutzen und schauen, ob wir damit etwas besser an die Daten näherkommen. Hier müssen wir dann wirklich ausprobieren, was teilweise echt nervig ist. Dafür schauen wir uns gleich nochmal die Funktion `mfp()` an.

```{r}
nls(proportion ~ a - exp(b * snow_height), data = hunting_tbl, 
    start = c(a = 1, b = 0),
    control = nls.control(maxiter = 1000))
```

Hat jeweils geklappt und dann können wir uns auch schon die Gleichung zusammenbauen. Die Frage ist natürlich, ob die Werte gut zu unseren Daten passen. Das werden wir dann gleich nochmal in der @fig-beta-fox-predict-non überprüfen. Hier dann einmal die erste Gleichung für das Polynom und dann die zweite Gleichung mit der $e$-Funktion.

$$
proportion = 3.196 - snow\_height^{0.248}
$$

$$
proportion = 2.268 - e^{0.0101 \cdot snow\_height}
$$

Welche dann die beste mathematische Formel ist sehen wir dann gleich. Bitte schaue dir aber noch den anderen Tab mit der Funktion `mfp()` an, denn mit der Funktion `nls()` ist es dann doch manchmal etwas Glücksspiel, ob man die richtige mathematische Formel mit den richtigen Startwerten trifft.

## Mit `mfp()`

Nachdem wir den langen Weg mit der Funktion `nls()` gegeangen sind, machen wir es jetzt etwas kürzer mit der Funktion `mfp()`. Wir müssen nur angeben welche Variable als Polynom modelliert werden soll. Den Rest macht dann die Funktion `mfp()` für uns. Wir erhalten dann auch die Formel für den Zusammenhang der folgenden Form. Wir erhalten dann die Werte für $\beta_0$, $\beta_1$ sowie dem Polynom $p$ und der möglichen Transformation von $x$.

$$
proportion = \beta_0 - \beta_1 \cdot \left(snow\_height\right)^p
$$

Dann rechnen wir mal die Funktion und schauen welche Werte wir erhalten. Die Kunst ist hier die Werte für die Koeffizienten aus der Ausgabe abzulesen. Aber das mache ich dir hier ja einmal vor.

```{r}
mfp(proportion ~ fp(snow_height), data = hunting_tbl)
```

Dann können wir die Werte aus der Ausgabe auch schon in unsere Gleichung einsetzen. Der Vorteil ist wirklich, dass ich mir nicht überlegen muss, welche mathematische Formel ich nutzen will. Das schränkt mich zwar ein, macht mir das Leben aber auch einfacher. Wir erhalten dann eben eine Polynom, was auch einfach einzusetzen ist.

$$
proportion = 0.9741 -2.4216 \cdot \left(\cfrac{snow\_height}{100}\right)^3
$$

Dann schauen wir mal gleich, welche der drei mathematischen Formeln am besten zu unseren Daten in der @fig-beta-fox-predict-non passen.
:::

Im ersten Schritt bauen wir uns einmal in R die drei mathematischen Formeln einmal nach. Wir nutzen dafür die Funktion `\(x){...}` und setzen die Zahlen der Koeffizienten ein. Hier musst du nur mit den Klammern aufpassen, sonst bauen sich die Formeln einfach.

```{r}
nls_poly_func <- \(x){3.196 - x^(0.248)}
nls_exp_func <- \(x){2.268 - exp(0.0101 * x)}
mfp_func <- \(x){0.9741 - 2.4216 * (x/100)^3}
```

In der @fig-beta-fox-predict-non siehst du einmal den Vergleich der drei nicht linearen Regressionen zu den Datenpunkten. Was will man sagen, dass Modell aus der Funktion `mfp()` bei weitem das beste Modell liefert um den Verlauf der Punkte durch eine Kurve zu beschreiben. Wenn es dir also nur darum ginge die Werte für die Jagderfolge nach der Schneehöhe vorherzusagen, dann sieht das `mfp()`-Modell sehr gut aus.

```{r}
#| warning: false
#| message: false
#| label: fig-beta-fox-predict-non
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Zusammenhang zwischen dem Jagederfolg von Eisfüchsen und der Schneehöhe in den jeweiligen beobachteten Habitaten zusammen mit den drei nicht linearen Regressionen aus der Funktion `nls()` und `mfp()`."

hunting_tbl %>% 
  ggplot(aes(snow_height, proportion)) +
  theme_minimal() +
  geom_point() +
  geom_function(fun = nls_poly_func, aes(color = "nls_poly")) +
  geom_function(fun = nls_exp_func, aes(color = "nls_exp")) +
  geom_function(fun = mfp_func, aes(color = "mfp")) +
  scale_color_manual(name = "Modell", values = cb_pal[2:4])
```
:::

## Gruppenvergleich {#sec-mult-comp-beta-reg}

### Untergewichtige Brokkoli

```{r}
broc_fit <- betareg(proportion ~ fert_time + fert_amount + fert_time:fert_amount + block, data = broc_tbl)
```

```{r}
broc_fit %>% 
  Anova() %>% 
  model_parameters()
```

```{r}
#| warning: false
#| message: false
emm_obj <- broc_fit %>% 
  emmeans(~ fert_time * fert_amount) 
emm_obj
```

```{r}
#| eval: false
emm_obj %>% 
  pairs(adjust = "bonferroni")
```

```{r}
emm_obj %>% 
  cld(Letters = letters)
```

```{r}
#| echo: true
#| warning: false
#| message: false
#| label: fig-bar-beta-broc
#| fig-align: center
#| fig-height: 4
#| fig-width: 5
#| fig-cap: "Säulendigramm der mittleren Brokkoligewichte aus einer Gaussian Regression. Das `lm()`-Modell berechnet das mittler Gewicht des Brokkoli in jeder Faktorkombination. Das *compact letter display* wird dann in `{emmeans}` generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre. Wir haben noch ein Mindestgewicht von 500g ergänzt."

emm_obj %>% 
  cld(Letters = letters) %>% 
  as_tibble() %>% 
  ggplot(aes(x = fert_amount, y = emmean, fill = fert_time)) +
  theme_minimal() + 
  labs(y = "[%] erreichtes Zielgewicht", x = "Düngemenge [mg/l]",
       color = "Düngezeitpunkt") +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1)) +
  geom_bar(stat = "identity", width = 0.7,
           position = position_dodge(width = 0.9, preserve = "single")) +
  geom_text(aes(label = .group, y = emmean + SE + 0.01),  
            position = position_dodge(width = 0.9), vjust = -0.25) +
  geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE),
                width = 0.2,  
                position = position_dodge(width = 0.9, preserve = "single")) +
  theme(legend.position = "top") +
  scale_fill_okabeito()
```

### Pilze auf Weizen

```{r}
fungi_tbl <- fungi_tbl %>% 
  filter(bunt %in% c("B1", "B12", "B157", "B51", "B189", "B4")) %>% 
  filter(gen %in% c("Oro", "Albit", "Turkey", "Hybrid128")) 
```

```{r}
fungi_fit <- betareg(percent ~ gen + bunt + gen:bunt, data = fungi_tbl)
```

```{r}
fungi_fit %>% 
  Anova() %>% 
  model_parameters()
```

```{r}
#| warning: false
#| message: false
emm_obj <- fungi_fit %>% 
  emmeans(~ gen * bunt) 
emm_obj 
```

```{r}
#| eval: false
emm_obj %>% 
  pairs(adjust = "bonferroni")
```

```{r}
emm_obj %>% 
  cld(Letters = letters) 
```

```{r}
#| echo: true
#| warning: false
#| message: false
#| label: fig-bar-beta-fungi
#| fig-align: center
#| fig-height: 4
#| fig-width: 5
#| fig-cap: "Säulendigramm der mittleren Brokkoligewichte aus einer Gaussian Regression. Das `lm()`-Modell berechnet das mittler Gewicht des Brokkoli in jeder Faktorkombination. Das *compact letter display* wird dann in `{emmeans}` generiert. Wir nutzen hier den Standardfehler, da die Standardabweichung mit der großen Fallzahl rießig wäre. Wir haben noch ein Mindestgewicht von 500g ergänzt."

emm_obj %>% 
  cld(Letters = letters) %>% 
  as_tibble() %>% 
  ggplot(aes(x = bunt, y = emmean, fill = gen)) +
  theme_minimal() + 
  labs(y = "[%] erreichtes Zielgewicht", x = "Düngemenge [mg/l]",
       color = "Düngezeitpunkt") +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1)) +
  geom_bar(stat = "identity", width = 0.7,
           position = position_dodge(width = 0.9, preserve = "single")) +
  geom_text(aes(label = .group, y = emmean + SE + 0.01),  
            position = position_dodge(width = 0.9), vjust = -0.25) +
  geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE),
                width = 0.2,  
                position = position_dodge(width = 0.9, preserve = "single")) +
  theme(legend.position = "top") +
  scale_fill_okabeito()
```

## Referenzen {.unnumbered}
