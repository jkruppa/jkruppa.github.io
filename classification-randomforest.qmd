```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Decision trees {#sec-class-tree}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

::: column-margin
![](images/angel_01.png){fig-align="center" width="50%"}

Wir werden uns hier mit der Anwendung beschäftigen. Wie immer lassen wir daher *tiefere* mathematische Überlegungen weg.
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, 
               janitor, vip, rpart.plot, see,
               conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
##
set.seed(2025429)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

```{r}
gummi_tbl <- read_excel("data/gummibears.xlsx") %>% 
  mutate(gender = as_factor(gender),
         most_liked = as_factor(most_liked),
         student_id = 1:n()) %>% 
  select(student_id, gender, most_liked, age, semester, height) %>% 
  drop_na(gender)

```

In @tbl-gummi-prepro

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-gummi-model-compare
#| tbl-cap: Auszug aus dem Daten zu den Gummibärchendaten.

gummi_raw_tbl <- gummi_tbl %>% 
  mutate(gender = as.character(gender),
         most_liked = as.character(most_liked))

rbind(head(gummi_raw_tbl),
      rep("...", times = ncol(gummi_raw_tbl)),
      tail(gummi_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

```{r}
gummi_data_split <- initial_split(gummi_tbl, prop = 3/4)

gummi_train_data <- training(gummi_data_split)
gummi_test_data  <- testing(gummi_data_split)
```

```{r}
gummi_rec <- recipe(gender ~ ., data = gummi_train_data) %>% 
  update_role(student_id, new_role = "ID") %>% 
  step_naomit(all_outcomes()) %>% 
  step_impute_mean(all_numeric_predictors(), -has_role("ID")) %>% 
  step_impute_bag(all_nominal_predictors(), -has_role("ID")) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1, -has_role("ID")) %>% 
  step_dummy(all_nominal_predictors(), -has_role("ID")) %>% 
  step_nzv(all_predictors(), -has_role("ID"))

gummi_rec %>% summary()

```

## Rpart {#sec-rpart}

Mit Abbildung

https://www.tidymodels.org/start/tuning/

```{r}
rpart_mod <- decision_tree(tree_depth = 5, min_n = 10, cost_complexity = 0.001) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
```

```{r}
rpart_wflow <- workflow() %>% 
  add_model(rpart_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
rpart_fit <- rpart_wflow %>% 
  parsnip::fit(gummi_train_data)
```

```{r}
rpart_aug <- augment(rpart_fit, gummi_test_data ) 
```

```{r}
rpart_cm <- rpart_aug %>% 
  conf_mat(gender, .pred_class)

rpart_cm
```

```{r}
rpart_cm %>% summary()
```

```{r}
rpart_aug %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

Es gibt viele Möglichkeiten sich einen Entscheidungsbaum anzuschauen. Wir nutzen hier das R Paket `rpart.plot` und die gleichnamige Funktion `rpart.plot()`. Die vielen Möglichkeiten der Darstellung und der Optionen findest in der Vignette [Plotting rpart trees with the rpart.plot package.](http://127.0.0.1:52037/help/library/rpart.plot/doc/prp.pdf). Wir gehen hier einmal auf die Variante `extra = 101` ein. Es gibt insgesamt elf verschiedene Arten plus eben noch die Möglichkeit 100 zu einer der elf genannten Varianten hinzufügen, um auch den Prozentsatz der Beobachtungen im Knoten anzuzeigen. Zum Beispiel zeigt `extra = 101` die Anzahl und den Prozentsatz der Beobachtungen in dem Knoten an.

```{r}
rpart_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE, extra = 101)
```

## Random Forest {#sec-class-rf}

![Dars.](images/class-rf-01.png){#fig-class-rf-01 fig-align="center" width="100%"}

![Dars.](images/class-rf-02.png){#fig-class-rf-02 fig-align="center" width="100%"}

![Dars.](images/class-rf-03.png){#fig-class-rf-03 fig-align="center" width="100%"}

![Dars.](images/class-rf-04.png){#fig-class-rf-04 fig-align="center" width="100%"}

Das Wort *Bagging* steht für *bootstrap aggregating* und ist eine Methode, um Vorhersagen aus verschiedenen Modellen zu kombinieren. Dabei müssen alle Modelle mit dem gleichen Algorithmus laufen, können aber auf verschiedenen Datensätzen oder aber Variablensätzen zugreifen. Häufig haben die Modelle eine hohee Varianz in der Vorhersage und wir nutzen dann Bagging um die Modelle miteinader zu kombinieren und dadurch die Varianz zu verringern. Die Ergebnisse der Modelle werden dann im einfachsten Fall gemittelt. Das Ergebnis jeder Modellvorhersage geht mit gleichem Gewicht in die Vorhersage ein. Wir haben auch noch andere Möglichkeiten, aber du kannst dir Vorstellen wir rechnen verschiedene Modelle $k$-mal und bilden dann ein finales Modell in dem wir alle $k$-Modelle zusammenfassen. Wie wir die Zusammenfassung rechnen, ist dann immer wieder von Fall zu Fall unterschiedlich. Wir erhalten am Ende einen *Ensemble* Klassifizierer, da ja ein Ensemble von Modellen zusammengefasst wird.

::: {.callout-caution collapse="true"}
## Parallele CPU Nutzung

```{r}
cores <- parallel::detectCores()
cores
```

Und dann in `set_engine("ranger", num.threads = cores)`
:::

```{r}
ranger_mod <- rand_forest(mtry = 5, min_n = 10, trees = 1000) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")
```

```{r}
ranger_wflow <- workflow() %>% 
  add_model(ranger_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
ranger_fit <- ranger_wflow %>% 
  parsnip::fit(gummi_train_data)

```

```{r}
ranger_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)
```

```{r}
ranger_aug <- augment(ranger_fit, gummi_test_data ) 
```

```{r}
ranger_cm <- ranger_aug %>% 
  conf_mat(gender, .pred_class)

ranger_cm
```

```{r}
ranger_cm %>% summary()
```

```{r}
ranger_aug %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

## xgboost {#sec-xgboost}

https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5

https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30

https://howtolearnmachinelearning.com/articles/boosting-in-machine-learning/

![Darstellung von adaptive Boosting](images/class-xgboost.png){#fig-class-adaboost fig-align="center" width="100%"}

![Darstellung von adaptive Boosting](images/class-xgboost-2.png){#fig-class-gradientboost fig-align="center" width="100%"}

```{r}
xgboost_mod <- boost_tree(mtry = 5, min_n = 10, trees = 1000) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```

```{r}
xgboost_wflow <- workflow() %>% 
  add_model(xgboost_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
xgboost_fit <- xgboost_wflow %>% 
  parsnip::fit(gummi_train_data)
```

```{r}
xgboost_aug <- augment(xgboost_fit, gummi_test_data ) 
```

```{r}
xgboost_cm <- xgboost_aug %>% 
  conf_mat(gender, .pred_class)

xgboost_cm
```

```{r}
xgboost_cm %>% summary()
```

```{r}
xgboost_aug %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

## Tuning

```{r}
tune_spec <-  boost_tree(mtry = tune(), 
                         min_n = tune(), 
                         trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

tune_spec
```

Ich nutze hier `levels = 2` damit hier die Ausführung nicht so lange läuft. Das ist natürlich etwas zu wenig. Fange am besten mit `levels = 5` an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert.

`mtry` kennt nicht die Spaltenzahl

```{r}
gummi_grid <- grid_regular(mtry(range = c(1, 4)),
                           trees(),
                           min_n(),
                           levels = 5)
```

Eigentlich ist eine 10-fache Kreuzvalidierung mit $v=10$ besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur $v=5$ gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.

```{r}
gummi_folds <- vfold_cv(gummi_train_data, v = 5)
```

```{r}
gummi_tune_wflow <- workflow() %>% 
  add_model(tune_spec) %>% 
  add_recipe(gummi_rec)
```

Wenn du `control_grid(verbose = TRUE)` wählst, dann erhälst du eine Ausgabe wie weit das Tuning gerade ist.

```{r}
#| eval: false
gummi_tune_res <- gummi_tune_wflow %>% 
   tune_grid(resamples = gummi_folds,
             grid = gummi_grid,
             control = control_grid(verbose = FALSE))
```

```{r}
#| eval: false
#| echo: false

## write_rds(gummi_tune_res, "data/gummi_xboost_tune_res.rds")
```

```{r}
#| echo: false

gummi_tune_res <- read_rds("data/gummi_xboost_tune_res.rds")
```

```{r}
gummi_tune_res %>%
  collect_metrics() %>%
  mutate(trees = as_factor(trees),
         min_n = as_factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n, linetype = trees)) +
  theme_minimal() +
  geom_line(alpha = 0.6) +
  geom_point() +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_okabeito()
```

```{r}
gummi_tune_res %>%
  show_best("accuracy")
```

```{r}
best_xgboost <- gummi_tune_res %>%
  select_best("accuracy")

best_xgboost
```

```{r}
final_gummi_wf <- gummi_tune_wflow %>% 
  finalize_workflow(best_xgboost)

final_gummi_wf 
```

```{r}
final_fit <- final_gummi_wf %>%
  last_fit(gummi_data_split) 

final_fit %>%
  collect_metrics()

final_fit %>%
  collect_predictions() %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```
