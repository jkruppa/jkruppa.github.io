```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Neural networks & deep learning {#sec-neural}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

So, das war ein ganz schönes Brett, TensorFlow oder Keras auf dem Rechner zu installieren. Es gibt zwar einen [Quick start um Tensorflow zu installieren](https://tensorflow.rstudio.com/install/) aber dann hatte ich das schöne Problem der GPU auf dem macOS mit M1 Chip. Die Lösung für die [Local GPU](https://tensorflow.rstudio.com/install/local_gpu) hat mich auf dem macOS einen Tag Nerven gekostet. Das mag dann auf einem Windows Rechner anders sein bzw. andere Probleme verursachen. Schlussendlich ist die Nutzung von *neural networks* auf keinen Laptops vieleicht auch nicht so die beste Idee. Wir würden die Algorithmen eher auf Hochleistungsrechner durchführen und dann vermutich eine Linuxdistribution verwenden. Dennoch werde ich hier einmal Tensorflow in R vorstellen. Die Packete für die Integration von dem eigenständigen Algorithmus Tensorflow gibt es und wenn es dann mal installiert ist, funktioniert auch alles super. Da Tensorflow in Phyton programmiert ist, muss auch Phyton auf dem Rechner installiert sein. Du siehst also, es ist einiges einzurichten, damit wir Deep learning betreiben können. Hier möchte ich dann auch gerne auf @mueller2019deep verweisen, der zu dem Thema Deep learning einen guten Einstieg liefert.

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, 
               janitor, keras, tensorflow, see,
               lime,
               conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

In diesem Kapitel wolle wir uns aber mal auf einen echten Datensatz anschauen. Wir nutzen daher einmal den Gummibärchendatensatz. Als unser Label und daher als unser Outcome nehmen wir das Geschlecht `gender`. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir nur die Spalte Geschlecht sowie als Prädiktoren die Spalten `most_liked`, `age`, `semester`, und `height`.

```{r}
gummi_tbl <- read_excel("data/gummibears.xlsx") %>% 
  mutate(gender = as_factor(gender),
         most_liked = as_factor(most_liked)) %>% 
  select(gender, most_liked, age, semester, height) %>% 
  drop_na(gender)

```

Wir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein `NA` in der Variable `gender` haben. Wir haben dann insgesamt $n = `r nrow(gummi_tbl)`$ Beobachtungen vorliegen. In @tbl-gummi-prepro sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-gummi-model-compare
#| tbl-cap: Auszug aus dem Daten zu den Gummibärchendaten.

gummi_raw_tbl <- gummi_tbl %>% 
  mutate(gender = as.character(gender),
         most_liked = as.character(most_liked))

rbind(head(gummi_raw_tbl),
      rep("...", times = ncol(gummi_raw_tbl)),
      tail(gummi_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

Unsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Um die Klassifikation mit Entscheidungsbäumen rechnen zu können brauchen wir wie bei allen anderen Algorithmen auch einen Trainings- und Testdatensatz. Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Der Traingsdatensatz ist dabei immer der größere Datensatz. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.

```{r}
gummi_data_split <- initial_split(gummi_tbl, prop = 3/4)
```

Wir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluss einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.

```{r}
gummi_train_data <- training(gummi_data_split)
gummi_test_data  <- testing(gummi_data_split)
```

Nachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht `gender` als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem `.` Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.

```{r}
gummi_rec <- recipe(gender ~ ., data = gummi_train_data) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_bag(all_nominal_predictors()) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors())

gummi_rec %>% summary()

```

Alles in allem haben wir ein sehr kleines Modell. Wir haben ja nur ein Outcome und vier Prädiktoren. Trotzdem sollte dieser Datensatz reichen um zu erklären wie Keras oder Tensorflow funktionieren. Am Ende muss man sich aber auch ehrlich machen und sagen, dass ein Datensatz mit unter tausend Beobachtungen eigentlich keinen großen Sinn für ein neuronales Netz macht. Deshalb ist das hier eher eine Demonstration des Algorithmus.

## Keras / Tensorflow

Deeplearning in R

Nochmal genauer anschauen:

https://colorado.rstudio.com/rsc/churn/modeling/tensorflow-w-r.nb.html

https://parsnip.tidymodels.org/reference/details_linear_reg_keras.html

https://camrongodbout.medium.com/tensorflow-in-a-nutshell-part-one-basics-3f4403709c9d

https://medium.com/analytics-vidhya/neural-networks-in-a-nutshell-bb013f40197d

```{r}
keras_mod <- mlp() %>% 
  set_engine("keras") %>% 
  set_mode("classification")
```

```{r}
keras_wflow <- workflow() %>% 
  add_model(keras_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
#| message: false
#| warning: false

keras_fit <- keras_wflow %>% 
  parsnip::fit(gummi_train_data)

```

```{r}
keras_aug <- augment(keras_fit, gummi_test_data ) 
```

```{r}
keras_cm <- keras_aug %>% 
  conf_mat(gender, .pred_class)

keras_cm
```

```{r}
keras_cm %>% summary()
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-class-keras-01
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "ROC Kurve für den `keras` Algorithmus."

keras_aug %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

::: callout-note
## Kann ich auch eine Kreuzvalidierung für Keras / Tensorflow durchführen?

Ja, kannst du. Wenn du *nur* eine Kreuzvalidierung durchführen willst, findest du alles im @sec-knn für den $k$-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf Keras / Tensorflow Algorithmus anwenden.
:::

## Tuning

```{r}
cores <- parallel::detectCores()
```

```{r}
tune_spec <- mlp(hidden_units = tune(),
                 penalty = tune(), 
                 epochs = tune()) %>% 
  set_engine("keras", num.threads = cores) %>% 
  set_mode("classification") 

tune_spec
```

Ich nutze hier `levels = 2` damit hier die Ausführung nicht so lange läuft. Das ist natürlich etwas zu wenig. Fange am besten mit `levels = 5` an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert.

```{r}
gummi_grid <- grid_regular(hidden_units(range = c(1, 100)),
                           penalty(),
                           epochs(range = c(10, 200)),
                           levels = 5)
```

Eigentlich ist eine 10-fache Kreuzvalidierung mit $v=10$ besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur $v=5$ gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.

```{r}
gummi_folds <- vfold_cv(gummi_train_data, v = 5)
```

```{r}
gummi_tune_wflow <- workflow() %>% 
  add_model(tune_spec) %>% 
  add_recipe(gummi_rec)
```

```{r}
#| eval: false
gummi_tune_res <- gummi_tune_wflow %>% 
   tune_grid(resamples = gummi_folds,
             grid = gummi_grid,
             control = control_grid(verbose = FALSE))
```

```{r}
#| eval: false
#| echo: false

## write_rds(gummi_tune_res, "data/gummi_keras_tune_res.rds")
```

```{r}
#| echo: false

gummi_tune_res <- read_rds("data/gummi_keras_tune_res.rds")
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-class-keras-02
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Tuning Kurven für den `keras` Algorithmus."

gummi_tune_res %>%
  collect_metrics() %>%
  mutate(hidden_units = as_factor(hidden_units),
         penalty = as_factor(penalty)) %>%
  ggplot(aes(epochs, mean, color = hidden_units, linetype = penalty)) +
  theme_minimal() +
  geom_line(alpha = 0.6) +
  geom_point() +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_okabeito()
```

```{r}
gummi_tune_res %>%
  show_best("accuracy")
```

```{r}
best_keras <- gummi_tune_res %>%
  select_best("accuracy")

best_keras
```

```{r}
final_gummi_wf <- gummi_tune_wflow %>% 
  finalize_workflow(best_keras)

final_gummi_wf 
```

```{r}
#| cache: true
#| message: false
#| warning: false

final_fit <- final_gummi_wf %>%
  last_fit(gummi_data_split) 
```

```{r}
final_fit %>%
  collect_metrics()
```

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-class-keras-03
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "ROC Kurve für den `keras` Algorithmus nach der Kreuvalidierung und dem Tuning."

final_fit %>%
  collect_predictions() %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

## Referenzen {.unnumbered}
