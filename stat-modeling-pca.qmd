```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra)
```

# Hauptkomponentenanalyse {#sec-pca-main}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

Die Hauptkomponentenanalyse (eng. *Principle Component Analysis*, abk. PCA) ist ein bedeutendes Verfahren zur Dimensionsreduktion. Wir haben also einen sehr großen Datensatz mit sehr vielen Spalten und wollen Muster in den Beobachtungen finden, die durch die Zeilen definiert sind. Dieses Kapitel umfasst dabei aber nur die notwendigen Verfahren und Anwendungen. Das hat vor allem damit zu tun, dass die Hauptkomponentenanalyse in den Sozialwissenschaften sehr weitreichend genutzt wird. Wir machen hier aber Agrarwissenschaften, also ist es eher ein nebensächlicher Fokus. Ebenso ist die Auswertung von Fragebögen allgemein ein Schwerpunkt der Hauptkomponentenanalyse. Auch werden Fragebögen eher in meinem Feld ausgewertet, deshalb hier nur das eine Kapitel, wenn man auch sicherlich viel mehr schreiben könnte.

Das folgende Kapitel basiert zum Teilen auf den [Articles - Principal Component Methods in R: Practical Guide](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/). Diese Sammlung an Tutorien geben einen wunderbaren Überblick über alle möglichen Methoden zu der Hauptkomponentenanalyse und deren verwandten Algorithmen. Wenn dich dazu mehr interessiert kann ich das Buch von @kassambara2017practical dir sehr ans Herz legen. Dort findest du eine tolle Übersicht über die Hauptkomponentenanalyse in R.

::: column-margin
Neben viele anderen Tutorien gibt es hier noch ein weiteres zu den [PCA - Principal Component Analysis Essentials](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/).
:::

Was ist grob die Idee der Hauptkomponentenanalyse? Wir wollen unseren Daten, also die ganze Daten*matrix* einmal so transformieren, dass wir neue Komponenten aus den Daten extrahieren, die die Daten auf einer anderen Dimension beschreiben. Klingt etwas kryptisch, aber im Prinzip handelt es sich bei der Hauptkomponentenanalyse um eine komplexere Transformation der Daten. Wir nutzen dabei die Varianzstruktur und die Varianz/Kovarianzmatrix als unsere Distanzmatrix für die Variablen und Beobachtungen. Im Prinzip also die Korrelation zwischen den einzelnen Variablen in dem Datensatz. Über diese Korrelation zwischen den Spalten und Zeilen wollen wir die Daten reduzieren. Wird die gesamte Varianz der Daten vielleicht nur von wenigen Spalten verursacht? Dann brauchen wir ja nur diese Hauptkomponenten weiter betrachten. Wir nutzen also die Varianz/Kovarianzmatrix als unsere Ähnlichkeitsmatrix, wie schon bei den Clusteranalysen, um hier unbekannte Zusammenhänge zwischen den Spalten und damit dann Hauptkomponenten aufzuklären.

::: callout-note
## Hauptkomponente, Eigenwert und Varianz kurz zusammengefasst

In einer Hauptkomponentenanalyse ersetzen wir die ursprünglichen Spalten eines Datensatzes durch Hauptkomponenten. Die Hauptkomponenten haben so viele Dimensionen wie es Spalten im ursprünglichen Datensatz gibt. Jede Hauptkomponente hat einen Eigenwert (eng. *eigenvalue*), der den Anteil der erklärten Varianz der Hauptkomponente in den Daten beschreibt. Wir können die Beobachtungen oder Individuen (abk. *ind*) in den Zeilen betrachten oder aber die Variablen (abk. *var*) in den Spalten.
:::

Neben der PCA existiert noch das *Multidimensional Scaling* (abk. MDS). Das MDS ist im Prinzip eine Spezialform der PCA. Im Unterschied zur PCA wird die MDS auf einer Distanzmatrix gerechnet. In einer MDS können wir nicht einfach so unsere Daten reinstecken sondern müssen zuerst die Daten in eine Distanzmatrix umrechnen. Dafür gibt es die Funktion `dist()` oder `as.dist()`, wenn wir schon Distanzen vorliegen haben. Daher ist die Anwendung einer MDS nicht besonders komplizierter.

Wenn wir eine PCA in R rechnen wollen, dann haben wir zuerst die Wahl zwischen den Funktionen `prcomp()` und `princomp()`. Laut der R-Hilfe hat die Funktion `prcomp()` eine etwas bessere numerische Genauigkeit. Daher ist die Funktion `prcomp()` gegenüber `princomp()` vorzuziehen. Es gibt aber noch eine *neuere* Implementierung der Funktionalität in dem R Paket `FactoMineR` und der Funktion `PCA()`. In diesem Kapitel nutzen wir also das R Paket `factoextra` um sich Faktoranalysen super anzuschauen und durchzuführen. Dur kannst mehr auf der Webseite [Factoextra R Package: Easy Multivariate Data Analyses and Elegant Visualization](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization) mehr über das Paket erfahren.

Es gibt eine *natürlich* große Anzahl an Quellen wie du in R eine PCA oder ein MDS durchführst. In der folgenden Box findest du eine Sammlung an Tutorien und R Code, der dir als Inspiration dienen mag. Ich werde teile von den Tutorien in der Folge verwenden, kann aber natürlich nichts alles nochmal machen.

::: {.callout-tip collapse="true"}
## Weitere Tutorien für die Principal Component Analysis

Wie immer gibt es eine Vielzahl an tollen Tutorien, die die PCA gut erklären. Ich habe hier einmal eine Auswahl zusammengestellt und du kannst dich da ja mal vertiefend mit beschäftigen, wenn du willst. Teile der Tutorien findest du vermutlich hier im Haupttext wieder.

-   [Principal Component Analysis (PCA) For Dummies](http://www.billconnelly.net/?p=697)
-   [Principal Component Analysis 4 Dummies: Eigenvectors, Eigenvalues and Dimension Reduction](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)
-   [The most gentle introduction to Principal Component Analysis](https://towardsdatascience.com/the-most-gentle-introduction-to-principal-component-analysis-9ffae371e93b)
:::

Wir wollen uns jetzt die Hauptkomponentenanalyse an zwei Spieldaten anschauen. Eigentlich werden ja auch gerne Fragebögen mit der Hauptkomponentenanalyse ausgewertet, aber hier muss ich nochmal warten bis ich ein gutes Beispiel in den Beratungen hatte. Dann ergänze ich ein Beispiel bei dem Skrippt zu den [Beispielhaften Auswertungen](https://jkruppa.github.io/application/).

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
set.seed(20230727)
pacman::p_load(tidyverse, magrittr, readxl, patchwork,
               factoextra, FactoMineR, ggpubr,
               janitor, corrplot, conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflicts_prefer(magrittr::set_names)
conflicts_prefer(dlookr::transform)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Beginnen wir mit einem normierten Datensatz aus dem R Paket `cluster`. Der Datensatz `animals` wurde von mir noch mit ein paar Tieren ergänzt und schaut sich sechs Eigenschaften von 23 Tieren an. Wir wollen im Folgenden nun herausfinden, ob wir anhand der Eigenschaften in den Spalten die Tiere in den Zeilen in Gruppen einordnen können. Einige der Tiere sind ja näher miteinander verwandt als andere Tiere. Die ursprünglichen Daten liefen noch auf einem $1/2$-System, das ändern wir dann zu $0/1$ damit wir dann auch besser mit den Daten arbeiten können. Für die Algorithmen ist es egal, aber ich habe lieber $1$ gleich ja und $0$ gleich nein.

```{r}
animals_tbl <- read_excel("data/cluster_animal.xlsx", sheet = 1) %>% 
  clean_names() %>% 
  mutate(across(where(is.numeric), \(x) x - 1))
```

Schauen wir uns einmal den Datensatz in der @tbl-cluster-01 an. Wir sehen, dass wir noch einige fehlende Werte in den Daten vorliegen haben. Das ist manchmal ein Problem, deshalb werden wir im Laufe der Analyse die `NA` Werte mit `na.omit()` entfernen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-cluster-01
#| tbl-cap: "Übersicht über die 23 Tiere mit den sechs Eigenschaften in den Spalten. Eine 1 bedeutet, dass die Eigenschaft vorliegt; eine 0 das die Eigenschaft nicht vorliegt."

animals_tbl %>% 
  kable(align = "c", "pipe")
```

Der Tierdatensatz ist schön, da wir es hier nur mit 0/1 Werten zu tun haben. Wir werden später in dem preprocessing der Daten sehen, dass wir alle Spalten in der gleichen Spannweite der Werte wollen. Das klingt immer etwas kryptisch, aber der nächste Datensatz über verschiedene Kreaturen macht es deutlicher.

::: column-margin
Eine andere Art die Daten zu Gruppieren kannst du im Tutorium [Clustering Creatures](https://rpubs.com/askieswe/clustering) nochmal nachvollziehen.
:::

Im Folgenen einmal der Datensatz, den wir dann in der gleichen Exceldatei finden nur eben auf dem zweiten Tabellenblatt. Wir reinigen noch die Namen und setzen die `creature`-Spalte auf Klein geschrieben. Wie du siehst, haben wir dann nur 15 Kreaturen und drei Spalten mit dem Gewicht, der Herzrate und dem maximalen möglichen Alter.

```{r}
creature_tbl <- read_excel("data/cluster_animal.xlsx", sheet = 2) %>% 
  clean_names() %>% 
  mutate(creature = tolower(creature))

```

In der @tbl-cluster-02 sehen wir nochmal die Daten dargestellt und hier erkennst du auch gut, wo das Problem liegt. Die Masse der Tiere reicht von $6g$ beim Hamster bis $120000000g$ beim Wal. Diese Spannweiten in einer Spalte und zwischen den Spalten führt dann zu Problemen bei den Algorithmen. Deshalb müssen wir hier Daten nochmal normalisieren oder aber standardisieren. Je nachdem was da besser passt.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-cluster-02
#| tbl-cap: "Übersicht über die 15 Kreaturen mit den drei Eigenschaften in den Spalten. Wir haben hier sehr große Unterschiede in den Datenwerten. Daher müssen wir vor dem Clustern nochmal normalisieren."

creature_tbl %>% 
  kable(align = "c", "pipe")
```

## Daten standardisieren

Die Standardisierung zwingt Variablen in eine $\mathcal{N(0,1)}$ Standardnormalverteilung. Das heißt, wir transformieren alle Variablen auf einen Mittelwert von $0$ und einer Standardabweichung von $1$. Hier nochmal die Formel für die Standardisierung oder auch $z$-Transformation.

$$
z = \cfrac{x_i - \bar{x}}{s_x}
$$ Wie wir hier sehen ziehen wir von jeder $i$-ten Beobachtung den Mittlwert von allen Beobachtungen ab. Dann teilen wir noch durch die Standardabweichung alle Beobachtungen. Am Ende ist dann damit unser Mittelwert auf $0$ und unsere Standardabweichung auf $1$.

Die Standardisierung macht dann auch die Daten sehr schon gleichförmig. Hier nutzen wir auch die Funktion `transform()` aus dem R Paket `dlookr` mit der Option `zscore`. Damit wir auch auf jeden Fall sicher gehen, dass wir die richtige Funktion nutzen, schreiben wir `dlookr::transform()` und damit ist sichergestellt, dass wir auch die Funktion `transform()` aus dem R Paket `dlookr` nutzen.

```{r}
std_creature_tbl <- creature_tbl %>% 
  mutate(mass_grams = dlookr::transform(mass_grams, "zscore"),
         heart_rate_bpm = dlookr::transform(heart_rate_bpm, "zscore"),
         longevity_years = dlookr::transform(longevity_years, "zscore")) 
std_creature_tbl
```

Die Funktion `PCA()`, die wir im Folgenden verwenden wollen, wird zwar auch die Daten intern von sich aus standardisieren, wenn die nicht standardisiert wurden. Ich mag es aber nicht, wenn wichtige Schritte in Funktionen begraben werden, deshlab hier nochmal das Beispiel, wie man es macht.

## Das `data.frame()` Problem

Leider ist es so, dass fast alle Pakete im Kontext der Clusteranalyse mit den Zeilennamen bzw. `row.names()` eines `data.frame()` arbeiten. Das hat den Grund, dass wir gut das Label in den Zeilennamen parken können, ohne das uns eine Spalte in den Auswertungen stört. Meistens ist das Label ja ein `character` und soll gar nicht in den Clusteralgorithmus mit rein. Deshalb müssen wir hier einmal unsere `tibble()` in einen `data.frame()` umwandeln. Die `tibble()` haben aus gutem Grund keine Zeilennamen, die Zeilennamen sind ein Ärgernis und Quelle von Fehlern und aus gutem Grund nicht in einem `tibble()` drin. Hier brauchen wir die Zeilennamen aber.

Wir bauen uns also einmal einen `data.frame()` für unseren Tierdatensatz und setzen die Tiernamen als Zeilennamen bzw. `row.names()`. Wir entfernen dann auch noch schnell alle fehlenden Werte, denn wir wollen usn hier nicht noch mit der Imputation von fehlenden Werten beschäftigen.

```{r}
animals_df <- animals_tbl %>% 
  na.omit() %>% 
  as.data.frame() %>% 
  set_rownames(.$animal) %>% 
  select(-animal)
```

Das Ganze machen wir dann auch noch einmal für die normalisierten Kreaturendaten. Wir wollen dann ja nur auf den normalisierten Daten weitermachen.

```{r}
std_creature_df <- std_creature_tbl %>% 
  as.data.frame() %>% 
  set_rownames(.$creature) %>% 
  select(-creature)
```

## Hauptkomponentenanalyse

Es gibt viele Implementierungen der Hauptkomponentenanalyse in R. Wir nutzen die Funktion `PCA()` aus dem R Paket `FactoMineR`. Denk immer daran, es kann maximal nur so viele Hauptkomponenten geben, wie wir auch Spalten in den Daten vorliegen haben. Folgende wichtige Funktionen werden wir jetzt einmal nutzen. Es gibt noch mehr, aber das übersteigt die einfache Hauptkomponentenanalyse, die wir uns hier anschauen wollen.

-   `get_eigenvalue()`: Extrahiert die Eigenwerte/Varianzen der Hauptkomponenten
-   `fviz_eig()`: Visualisierung der Eigenwerte
-   `get_pca_ind()`, `get_pca_var(res.pca)`: Extrahiert die Ergebnisse für Individuen bzw. Variablen.
-   `fviz_pca_ind()`, `fviz_pca_var(res.pca)`: Visualisierung der Ergebnisse für Individuen bzw. Variablen.
-   `fviz_pca_biplot()`: Erstellt einen Biplot der Individuen und Variablen.

Die zentrale Funktion ist aber die Funktion `PCA()` womit wir die eigentliche Hauptkomponentenanalyse durchführen. Die Funktion nimmt die Daten und berechnet dann intern die Varianz/Kovarianzmatrix. Die Varianz/Kovarianzmatrix wird dann als Distanzmatrix genutzt um die Hauptkomponenten und deren Eigenwerte zu berechnen.

Bei der Nutzung der Funktion `PCA()` ist für mich wichtig, dass wir nicht gleich irgendwelche Abbildungen erhalten, deshalb ist hier im Skrippt `graph = FALSE` gewählt. Mit der Option `scale.unit = TRUE` musst du die Daten selber nicht standadisieren sondern die Funktion `PCA()` macht das für dich. Manchmal sind Fragebögen sehr groß mit mehr als dutzenden von Fragen, da macht es Sinn sich nicht die Anzahl an Fragen als Hauptkomponenten wiedergeben zu lassen. Die Option `ncp = 5` zum Beispiel schränkt hier die Anzahl auf 5 Hauptkomponenten ein. Bei den Kreaturen macht es keinen Sinn, da erhalten wir natürlich nur drei Hauptkomponenten, da wir nur drei Spalten in dem Datensatz haben.

```{r}
pca_animals <- PCA(animals_df, scale.unit = TRUE, ncp = 5, graph = FALSE)
```

```{r}
pca_creature <- PCA(std_creature_df, scale.unit = TRUE, ncp = 5, graph = FALSE)
```

Im Folgenden betrachten wir dann die Hauptkomponentenanalyse auf drei Ebenen.

1)  *Auf der Ebene der Eigenwerte:* Wir entscheiden, wie viele Hauptkomponenten wir eigentlich in die weitere Analyse nehmen wollen.
2)  *Auf der Ebene der Variablen:* Wir betrachten die Spalten und schauen wie sich die Spalten zueinander und den Beobachtungen verhalten. In der R Welt enden dann die Funktionen `*_var`.
3)  *Auf der Eben der Individuen*: Wir schauen uns einmal die Zeilen an und versuchen zu verstehen, wie sich die einzelenen Beobachtungen oder Individuen verhalten. Gibt es hier Auffälligkeiten? In der R Welt enden dann die Funktionen `*_ind`.

### Ebene der Eigenwerte

Die Eigenwerte messen die Menge der von jeder Hauptkomponente beibehaltenen Variation. Damit repräsentieren die Eigenwerte die Varianz/Kovarianzmatrix der Daten. Die Eigenwerte sind für die ersten Hauptkomponenten grundsätzlich groß und für die nachfolgenden Hauptkomponente immer kleiner. Das heißt, die ersten Hauptkomponente beschreiben Variablen mit der größten Variation im Datensatz. Wie schon gesagt, wir erschaffen neue Variablen aus den Daten, die wir Hauptkomponenten nennen. Jede Hauptkomponente hat einen Eigenwert, der beschreibt, wie viel Varianz die Hauptkomponente in den Daten erklären kann.

::: callout-note
## Auf welcher Matrix basieren die Eigenwerte der Hauptkomponeten?

Wenn du dich ein wenig mit Eigenwerten auskennst, dann weißt du, dass Eigenwerte immer auf einer Matrix berechnet werden. In unserem Fall werden die Eigenwerte der Hauptkomponeten auf der Varianz/Kovarianzmatrix der standardisierten Daten berechnet. Im Folgenden siehst du einmal die Varianz/Kovarianzmatrix $\boldsymbol{\Sigma_x}$ für die Datenmatrix $x$. Wie du siehst, kann diese Matrix sehr groß werden. Du musst dir vorstellen, dass jedes $x_1$ bis $x_n$ jeweils eine Spalte in deiner Datenmatrix $\boldsymbol{x}$ entspricht. Damit hätte unser Tierdatensatz zum Beispiel $x_1$ bis $x_5$ und die Varianz/Kovarianzmatrix hätte fünf Zeilen und Spalten.

$$
\boldsymbol{\Sigma_x} = \begin{pmatrix}\operatorname{Var}(x_1) & \operatorname{Cov}(x_1,x_2) & \cdots & \operatorname{Cov}(x_1,x_n) \\ \\
 \operatorname{Cov}(x_2,x_1)  & \operatorname{Var}(x_2) & \cdots & \operatorname{Cov}(x_2,x_n) \\ \\
 \vdots & \vdots & \ddots & \vdots \\ \\
\operatorname{Cov}(x_n,x_1) & \operatorname{Cov}(x_n,x_2) & \cdots & \operatorname{Var}(x_n)
\end{pmatrix} \\
$$

Hier einmal das Beispiel der Varianz/Kovarianzmatrix für den Datensatz `animals_df`. Wie du siehst liegen auf der Diagonalen die Varianzen. In dem Rest der Matrix findest du dann immer die Kovarainz zwischen den Variablen. Achtung, die Kovarianz ist nicht die Korrelation. Zwar bedeutet eine Kovarianz von 0, dass die beiden Variablen nichts miteinander zu tun haben, aber die Kovarianz ist nicht auf 1 skaliert. Wir können auch sehr viel größere Werte für die Kovarianz beobachten.

```{r}
#| echo: true
#| message: false
#| warning: false
#| eval: false
animals_df %>% 
  cov() %>% 
  round(2) 
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| eval: true
animals_df %>% 
  cov() %>% 
  round(2) %>% 
  kable()
```

Für den Fall von zwei Variablen $x_1$ und $x_2$ können wir die Idee der Kovarianz nochmal nachvollziehen. Später rechnen wir dann die Hauptkomponentenanalyse auf der großen Matrix, aber ich kann dir den Zusammenhang leider nur für zwei Variablen erklären, den zweidimensional kriege ich noch hin. Wir können mit folgender Formel die Kovarianzen zwischen den beiden Variablen $x_1$ und $x_2$ berechnen.

$$
\operatorname{Cov}(x_2,x_1) = \sum_{i=1}^n(x_{1i}-\bar{x_1})(x_{2i}-\bar{x_2})
$$

Nochmal als Erinnerung, die Formel berechnet die quadrierten Abweichung der Beobachtungen von $x_1$ zum Mittelwert $\bar{x}_1$ und somit die Varianz $\operatorname{Var}(x_1)$ von $x_1$.

$$
\operatorname{Var}(x_1) = \sum_{i=1}^n(x_{1i}-\bar{x}_1)^2
$$

Ebenso berechnet diese Formel die quadrierten Abweichung der Beobachtungen von $x_2$ zum Mittelwert $\bar{x}_2$ und damit die Varianz $\operatorname{Var}(x_2)$ von $x_2$.

$$
\operatorname{Var}(x_2) = \sum_{i=1}^n(x_{2i}-\bar{x}_2)^2
$$

Das Ganze ist natürlich sehr trocken. Deshalb füttern wir einmal die Variablen $x_1$ und $x_2$ mit echten Daten und erschaffen uns den Datensatz `cov_tbl`. In @tbl-pca-corr-example ist der Zusammenhang nochmal Schritt für Schritt aufgeschlüsselt wie sich die Zahlen grob berechnen. Ich habe ein, zwei Schritte ausgelassen,a ber die ergänzt du fix selber.

```{r}
cov_tbl <- tibble(x_1 = c(0.8, 1.0, 1.2, 1.9, 2.0, 2.7, 2.8),
                  x_2 = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7))
```

Dann können wir uns einmal den Mittelwert und die Varianz für die beiden Variablen $x_1$ und $x_2$ berechnen und dann direkt in der @fig-pca-cov verwenden.

```{r}
cov_tbl %>% 
  gather %>% 
  group_by(key) %>% 
  summarise(mean = mean(value), var = var(value))
```

In der folgenden @fig-pca-cov siehst du einmal die Konzepte der Varianz für $x_1$ in Subplot A und $x_2$ in Subplot B dargestellt. Die durchgezogene Linie stellt dabei den Mittelwert für die beiden Variablen dar. Die Varianz berechnet sich jetzt als der quadrierte Abstand von den Beobachtungen zu den Mittelwerten. Der Abstand ist als gestrichelte Linie dargestellt. Faktisch addierst du die sich ergebenden Quadrate auf. Bei der Kovarianz sind es keine Quadrate, sondern Rechtecke. Die berechnest nämlich einmal den Abstand einer Beobachtung zum Mittelwert von $x_1$ und einmal den Abstand zum Mittelwert von $x_2$. Die beiden Abstände $(x_{1i}-\bar{x_1})$ und $(x_{2i}-\bar{x_2})$ multiplizierst du und addierst dann diese Rechtecke auf.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-pca-cov
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Berechnung der Abstände für die Varianz von $x_1$ sowie $x_2$ in der oberen Zeile. In der unteren Zeile die Rechtecke der Berechnung der Kovarianz von $x_1$ und $x_2$."


p1 <- ggplot(cov_tbl, aes(x_1, x_2)) +
  theme_bw() +
  geom_point() +
  geom_vline(xintercept = 1.771429, color = "blue") +
  geom_segment(aes(x = x_1, 
                   xend = rep(1.771429, 7), 
                   y = x_2, yend = x_2), linetype = 2, color = "blue") +
  labs(x = expression(x[1]), y = expression(x[2]))


p2 <- ggplot(cov_tbl, aes(x_1, x_2)) +
  theme_bw() +
  geom_point() +
  geom_hline(yintercept = 1.871429, color = "red") +
  geom_segment(aes(x = x_1, 
                   xend = x_1, 
                   y = x_2, 
                   yend = rep(1.871429, 7)), linetype = 2, color = "red") +
  labs(x = expression(x[1]), y = expression(x[2]))

p3 <- ggplot(cov_tbl, aes(x_1, x_2)) +
  theme_bw() +
  geom_point() +
  geom_hline(yintercept = 1.871429, color = "red") +
  geom_segment(aes(x = x_1, 
                   xend = x_1, 
                   y = x_2, 
                   yend = rep(1.871429, 7)), linetype = 2, color = "red") +
  geom_vline(xintercept = 1.771429, color = "blue") +
  geom_segment(aes(x = x_1, 
                   xend = rep(1.771429, 7), 
                   y = x_2, yend = x_2), linetype = 2, color = "blue") +
  labs(x = expression(x[1]), y = expression(x[2]))

p1 + p2 + p3 + 
  plot_layout(nrow = 2) + 
  plot_annotation(tag_levels = 'A')

```

In der folgenden @tbl-pca-corr-example siehst du dann das Vorgehen nochmal numerisch. Wichtig ist hierbei, dass wir am Ende die Varianz und die Kovarianz berechnen können indem wir die Summen $\sum$ durch $n-1$ gleich 6 teilen.

| $\boldsymbol{x_1}$ | $\boldsymbol{x_2}$  | $\boldsymbol{(x_{2i}-\bar{x}_2)^2}$ | $\boldsymbol{(x_{1i}-\bar{x}_1)^2}$ | $\boldsymbol{(x_{1i}-\bar{x}_1)(x_{2i}-\bar{x}_2)}$ |
|:------------------:|:-------------------:|:-----------------------------------:|:-----------------------------------:|:---------------------------------------------------:|
|        1.2         |         0.8         |                0.94                 |                0.45                 |                        0.65                         |
|        1.8         |         1.0         |                0.60                 |                0.01                 |                        0.06                         |
|        1.3         |         1.2         |                0.33                 |                0.33                 |                        0.33                         |
|        1.7         |         1.9         |                0.02                 |                0.03                 |                        -0.02                        |
|        2.6         |         2.0         |                0.05                 |                0.53                 |                        0.17                         |
|        1.8         |         2.7         |                0.86                 |                0.03                 |                        -0.07                        |
|        2.7         |         2.8         |                1.06                 |                0.69                 |                        0.85                         |
|                    |       $\sum$        |                3.86                 |                2.05                 |                        1.97                         |
|                    | $\cfrac{\sum}{n-1}$ |                0.64                 |                0.34                 |                        0.33                         |

: Tabelle zur Berechnung der Varianz sowie der Kovarianz von $x_1$ und $x_2$. {#tbl-pca-corr-example}

Schauen wir mal, ob wir richtig gerechnet haben und die Varianz für $x_1$ mit 0.64, die Varianz von $x_2$ mit 0.34 sowie die Kovarianz von $x_1$ und $x_2$ auch R wiederfinden. Wir nutzen die Funktion `cov()` um uns die Varianz/Kovarianzmatrix wiedergeben zu lassen.

```{r}
cov_tbl %>% 
  cov() %>% 
  round(2)
```

Wie wir sehen können wir die Werte in der Varianz/Kovarianzmatrix wiederfinden. Das ist ja mal ein Erfolg. Wir nutzen also die Varianz/Kovarianzmatrix als unsere Ähnlichkeitsmatrix, wie schon bei den Clusteranalysen, um hier unbekannte Zusammenhänge zwischen den Spalten und damit dann Hauptkomponenten aufzuklären.
:::

Mit Hilfe der Eigenwerte lässt sich die Anzahl der Hauptkomponenten bzw. Dimensionen bestimmen, die nach der PCA beibehalten werden sollen. Wir wollen selten alle Hauptkomponenten berücksichtigen. Es geht hier ja auch darum die Dimensionen der Daten zu reduzieren. Wenn wir alle Hauptkomponenten weiterverwenden würden, dann könnten wir auch den ursprünglichen Datensatz nutzen. Die Eigenwerte und der Anteil der Varianzen, die von den Hauptkomponenten beibehalten werden, können mit der Funktion `get_eigenvalue()` extrahiert werden.

Dabei bedeutet ein Eigenwert \> 1, dass die Hauptkomponenten mehr Varianz erklären als eine der ursprünglichen Variablen in den standardisierten Daten. Dies wird üblicherweise als Grenzwert für die Beibehaltung der Hauptkomponenten verwendet. Dies trifft nur zu, wenn die Daten standardisiert sind.

```{r}
eig_animals <- get_eigenvalue(pca_animals) %>% 
  as_tibble()
eig_animals 
```

Die Summe aller Eigenwerte für die Tiere ergibt eine Gesamtvarianz von `r sum(eig_animals$eigenvalue)`. Jetzt können wir ganz einfach den Anteil der erklärten Varianz von jedem Eigenwert berechnen. In der zweiten Spalte finden wir dann die Werte der Eigenwerte geteilt durch die Gesamtvarianz. Daher ist $42.35$ gleich $2.54$ geteilt durch $6$. Der kumulative Prozentsatz der erklärten Variation wird durch Addition der aufeinander folgenden Anteile der erklärten Variation ermittelt.

```{r}
eig_creature <- get_eigenvalue(pca_creature) %>% 
  as_tibble()
eig_creature
```

Die Summe aller Eigenwerte für die Kreaturen ergibt eine Gesamtvarianz von `r sum(eig_creature$eigenvalue)`. Damit können wir dann auch einfach die anderen Werte in den Spalten nachvollziehen.

Jetzt stellt sich natürlich die Frage, wie viele der Hauptkomponenten sollen den jetzt zukünftig berücksichtigt werden? Leider gibt es keine allgemein anerkannte objektive Methode, um zu entscheiden, wie viele Hauptkomponenten ausreichend sind. Dies hängt von dem jeweiligen Anwendungsbereich und dem jeweiligen Datensatz ab. In der Praxis neigen wir dazu, die ersten paar Hauptkomponenten zu betrachten, um interessante Muster in den Daten zu finden. Dafür nutzen wir den Scree Plot und entscheiden anhand der *Beuge* in dem Plot. Wenn wir nur wenige Variablen in den Daten haben, dann kann es sein, dass wir nur wenige Hauptkomponenten raus schmeißen. In der @fig-cluster-pca-3 siehst du einmal die beiden Scree Plots für die beiden Datensätze.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-cluster-pca-3
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix."
#| fig-subcap: 
#|   - "Richtung der Variablen (Spalten)"
#|   - "Verteilung der Beobachtungen (Zeilen)."
#| layout-nrow: 1
#| column: page

fviz_eig(pca_animals, addlabels = TRUE)
fviz_eig(pca_creature, addlabels = TRUE)
```

Wir sehen für die Tierdaten, dass die erste Hauptkomponente gut 42% der Varianz in den Daten erklärt, die folgende Hauptkomponente dann nur noch 20% und so weiter. Hier sehen wir dann auch die Beuge und könnten schließen, dass die ersten beiden Hauptkomponenten ausreichen um den Datensatz zu beschreiben. Bei den Kreaturendaten sieht es so aus, als ob wir die Datan alleinig mit der ersten Hauptkomponente erklären könnten. Wir sehen ja, dass gut 62% der Varianz durch die erste Hauptkomponente erklärt wird.

### Ebene der Variablen

Schauen wir jetzt einmal die Informationen der Variablen also Spalten der Hauptkomponentenanalyse an. Wir erhalten jetzt also die Informationen zu den einzelnen Hauptkomponenten, die ja die Variablen bzw. Spalten der Daten repräsentieren. Wir erhalten die Informationen über die Funktion `get_pca_var()`.

```{r}
var_animals <- get_pca_var(pca_animals)
var_creature <- get_pca_var(pca_creature)
var_creature
```

Schauen wir uns einmal an was wir mit den Informationen über die Variablen durch die Funktion `get_pca_var()` machen können.

-   `coord`: Koordinaten der Variablen zur Erstellung eines Streudiagramms. Wir nutzen meistens nur die ersten beiden Hauptkomponenten, da wir sonst kein zweidimensionalen Scatterplot machen können.
-   `cor`: Die Korrelation zwischen den ursprünglichen Variablenwerten in den Zeilen und den neuen Hauptkomponenten in den Spalten.
-   `cos2`: stellt die Qualität der Darstellung der Variablen auf der Faktorkarte dar. Es wird berechnet als die quadrierten Koordinaten: `cos2 = coord * coord`
-   `contrib:` enthält die Beiträge (in Prozent) der Variablen zu den Hauptkomponenten. Der Beitrag einer Variablen zu einer bestimmten Hauptkomponente ist (in Prozent): (`cos2` \* 100) / (gesamter `cos2` der Komponente).

Im Folgenden bewerten wir die Qualität einer Variable nach den `cos2`-Werten oder ihren Beitragswerten zu den Hauptkomponenten dargestellt durch `contrib`.

Die Qualität der Darstellung der Variablen auf der Faktorkarte wird als `cos2` (quadratischer Kosinus, quadratische Koordinaten) bezeichnet. Sie können auf den `cos2` wie folgt zugreifen:

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-pca-corrplot-cos2
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix."
#| fig-subcap: 
#|   - "Richtung der Variablen (Spalten)"
#|   - "Verteilung der Beobachtungen (Zeilen)."
#| layout-nrow: 1
#| column: page

corrplot(var_animals$cos2, is.corr = FALSE)
corrplot(var_creature$cos2, is.corr = FALSE)
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-pca-fviz-cos2
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix."
#| fig-subcap: 
#|   - "Richtung der Variablen (Spalten)"
#|   - "Verteilung der Beobachtungen (Zeilen)."
#| layout-nrow: 1
#| column: page

fviz_cos2(pca_animals, choice = "var", axes = 1:2)
fviz_cos2(pca_creature, choice = "var", axes = 1:2)

```

Beachten Sie das,

-   Ein hoher cos2-Wert deutet auf eine gute Darstellung der Variablen auf der Hauptkomponente hin. In diesem Fall ist die Variable nahe am Umfang des Korrelationskreises positioniert.
-   Ein niedriger cos2-Wert zeigt an, dass die Variable nicht perfekt durch die PC's repräsentiert ist. In diesem Fall befindet sich die Variable in der Nähe der Kreismitte.
-   Für eine bestimmte Variable ist die Summe des cos2 aller Hauptkomponenten gleich eins.

Wenn eine Variable nur durch zwei Hauptkomponenten (Dim.1 & Dim.2) perfekt repräsentiert wird, ist die Summe des cos2 auf diesen beiden PC gleich eins. In diesem Fall werden die Variablen auf dem Korrelationskreis positioniert. In @fig-cluster-pca-2 wären die Namen direkt auf dem Korrelationskreis. Für einige der Variablen sind möglicherweise mehr als 2 Komponenten erforderlich, um die Daten perfekt zu repräsentieren. In diesem Fall werden die Variablen innerhalb des Korrelationskreises positioniert, was wir eher in der @fig-cluster-pca-2 sehen.

Die cos2-Werte werden verwendet, um die Qualität der Darstellung abzuschätzen Je näher eine Variable am Korrelationskreis liegt, desto besser ist ihre Darstellung auf der Faktorkarte (und desto wichtiger ist es, diese Komponenten zu interpretieren) Variablen, die sich in der Nähe der Mitte des Diagramms befinden, sind für die ersten Komponenten weniger wichtig.

Es ist möglich, Variablen nach ihren cos2-Werten zu färben, indem man das Argument col.var = "cos2" verwendet. Dies erzeugt einen Farbverlauf. In diesem Fall kann das Argument gradient.cols verwendet werden, um eine benutzerdefinierte Farbe anzugeben. Zum Beispiel bedeutet gradient.cols = c("weiß", "blau", "rot"), dass:

Variablen mit niedrigen cos2-Werten werden in "weiß" eingefärbt Variablen mit mittleren cos2-Werten werden in "blau" eingefärbt Variablen mit hohen cos2-Werten werden in "rot" eingefärbt

In der @fig-cluster-pca-2 sehen wir das Korrelationsdiagramm der Variablen. Das Korrelationsdiagramm zeigt die Beziehungen zwischen allen Variablen und kann wie folgt interpretiert werden:

-   Positiv korrelierte Variablen sind zusammen gruppiert.
-   Negativ korrelierte Variablen befinden sich auf gegenüberliegenden Seiten des Ursprungs der Grafik oder auch den gegenüberliegende Quadranten.
-   Der Abstand zwischen den Variablen und dem Ursprung misst die Qualität der Variablen im Bezug auf den erklärenden Anteil. Variablen, die vom Ursprung entfernt sind, sind bedeutender als Variablen nahe des Ursprungs.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-cluster-pca-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Korrelationsdiagramm der Variablen eingefärbt nach den cos2-Werten."
#| fig-subcap: 
#|   - "Tierdaten"
#|   - "Kreaturendaten."
#| layout-nrow: 1
#| column: page

fviz_pca_var(pca_animals, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )

fviz_pca_var(pca_creature, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )

```

```{r}
res.desc <- dimdesc(pca_animals, axes = c(1,2), proba = 0.05)
# Description of dimension 1
res.desc$Dim.1
```

### Ebene der Individuen

```{r}
ind_animals <- get_pca_ind(pca_animals)
ind_animals
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-cluster-pca-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix."
#| fig-subcap: 
#|   - "Verteilung der Beobachtungen (Zeilen)."
#|   - "Komibination Variablen und Individuen."
#| layout-nrow: 1
#| column: page

fviz_pca_ind(pca_animals,
             col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE) +
  scale_x_continuous(expand = expansion(add = c(0.5, 1))) +
  scale_y_continuous(expand = expansion(add = c(0.5, 0.5))) 

fviz_pca_biplot(pca_animals) +
  scale_x_continuous(expand = expansion(add = c(0.5, 1))) +
  scale_y_continuous(expand = expansion(add = c(0.5, 0.5))) 

```

Wir müssen uns jetzt leider etwas von dem `tibble` verabschieden. Für die PCA brauchen wir einen Datensatz, in dem nur Zahlen oder Faktoren stehen. Daher schieben wir die Namen der Beobachtungen oder die ID in die Zeilennamen. Eigentlich keine gute Idee für die Arbeit mit Daten, aber für die PCA passt es. Wir haben dann also den `data.frame()` als `longnose_pca_df` vorliegen. Mit diesem Datensatzobjekt können wir dann in die PCA starten.

```{r}
#| message: false
#| eval: false
#| echo: true
#| warning: false

longnose_pca_df <- longnose_tbl %>%
  select(-stream) %>% 
  as.data.frame() %>% 
  set_rownames(longnose_tbl$stream)
```

Wenn wir die Daten von jeglichen `character` Spalten gereinigt haben, dann können wir die Funktion `PCA()` nutzen. Wir wollen uns die Visualisierung gleich selber nochmal nachbauen, deshalb hier die Option mit `graph = FALSE`. Im folgenden schauen wir uns nur eine Auswahl an möglichen Abbildungen an. Davon natürlich die wichtigsten Abbildungen, aber das [Factoextra R Package: Easy Multivariate Data Analyses and Elegant Visualization](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization) kann natürlich noch viel mehr.

```{r}
#| message: false
#| eval: false
#| echo: true
#| warning: false

pca_res <- PCA(longnose_pca_df,  graph = FALSE)
```

Nachdem wir die PCA durchgeführt haben, schauen wir uns einmal an, ob es überhaupt irgendwas gebracht hat, dass wir die PCA durchgeführt haben. Wir haben ja unsere Daten transformiert und erhalten pro Variable eine neue Dimension wieder. In einem Scree Plot wie in @fig-pca-1 wird die erklärte Varianz pro Hauptkomponente gezeigt.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-pca-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Scree Plot der PCA mit der erklärten Varianz pro Hauptkomponente."
#| eval: false


fviz_screeplot(pca_res, addlabels = TRUE, ylim = c(0, 50))
```

Wir erkennen, dass unsere Hauptkomponenten teilweise viel Varianz erklären, aber wir keine Hauptkomponente gefunden haben, die sehr viel Varianz erklärt. Es gibt also in unseren erhobenen Daten keine Variable, die "alles" erklärt. Mit "alles" ist dann natürlich die Varianz gemeint. In @fig-pca-2-1 sehen wir das Diagramm der Variablen, also der Spalten in der Datenmatrix. Positiv korrelierte Variablen zeigen auf dieselbe Seite des Diagramms. Negativ korrelierte Variablen zeigen auf die gegenüberliegenden Seiten des Diagramms. In @fig-pca-2-2 ist das Diagramm der Beobachtungen, also den Zeilen in der Datenmatrix, dargestellt. Beobachtungen mit einem ähnlichen *Muster* über die Zeilen werden in Gruppen zusammengefasst. In der @fig-pca-2-3 sehen wir nochmal die beiden Abbilungen zusammen und übereinander.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-pca-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix."
#| fig-subcap: 
#|   - "Richtung der Variablen (Spalten)"
#|   - "Verteilung der Beobachtungen (Zeilen)."
#|   - "Komibination Variablen und Individuen."
#| layout-nrow: 1
#| column: page
#| eval: false

fviz_pca_var(pca_res, col.var = "black")

fviz_pca_ind(pca_res,
             col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

fviz_pca_biplot(pca_res)

```

Gut, und was sollte das ganze jetzt? Wenn wir uns die Abbildungen ansehen, dann erkennen wir zuerst, dass es zwar Variablen gibt, die sich sehr ähnlich sind. Die Variable `area` und `maxdepth` sind stark miteinander korreliert. Die Pfeile zeigen beide in die gleiche Richtung. Ebenso scheint es einen negativen Zusammenhang zwischen `so4` und `do2` zu geben. Die Richtung ist egal, wir können die Dimensionen nicht direkt interpretieren, aber die Zusammenhänge zwischen den Variablen. Abschließend sehen wir, dass wir eine zu erwartende Aufteilung der Variablen haben. Immerhin messen wir Werte eines Flusses, da sollten die Variablen was miteinander zu tun haben. Auch ist die Kausalität gegeben. Wir erwarten bei einer hohen Temperatur weniger Sauerstoff und umgekehrt.

Bei der Aufteilung der Beobachtungen sehen wir auch keine Auffälligkeiten. Wir sehen eine große Wolke mit keinen separaten Gruppen. Das heißt, obwohl wir weiter oben Ausreißer gefunden haben, würde ich hier alle Beoachtungen in den Daten lassen, wenn ich die PCA sehe. Es ist dann immer eine Abschätzung. Wir sehen aber keine sonderlich auffälligen Beobachtungen. Wir könnten noch überlegen, ob wir das Outcome `longnose` nicht doch lieber aus der PCA nehmen und festhalten, dass es im Outcome keine Ausreißer gibt. Kontroverse Entscheidung, dir wir uns überlegen müssten. Beobachtungen aus einem Datensatz zu entfernen, den man nicht selber erschaffen hat, ist immer eine sehr schwere Sache.

## Multi Dimensional Scaling (MDS)

Eine besondere Form der Hauptkomponentenanalyse ist das *Multidimensional Scaling* (abk. MDS). Im Prinzip sind die Mechanismen sehr ähnlich. Der Hauptunterschied ist aber, das wir für die MDS eine Distanzmatrix benötigen. Wir können dafür die Funktion `dist()` oder `as.dist()` nehmen, wenn wir schon Distanzen vorliegen haben. Nehmen wir als plakatives Beispiel einmal die Distanzen von europäischen Städten zueinander. Wir haben die Daten in der Exceldatei `distance.xlsx` vorliegen. Wir lesen die Daten einmal ein und schauen uns die ersten fünf Spalten und die ersten fünf Zeilen des Daten*satzes* einmal an.

```{r}
#| message: false
#| echo: true
#| warning: false

distance_tbl <- read_excel("data/distance.xlsx")

distance_tbl[1:5, 1:5]
```

Wenn wir jetzt auf diesem Datensatz jetzt ein MDS rechnen wollen, dann müssen wir zum einen alle Spalten mit einem `character` entfernen. Wir haben dann nur noch einen Datensatz bzw. Datenmatrix mit den Distanzen vorliegen. Dann kann wir das `tibble` in einen `dist`-Objekt mit der Funktion `as.dist()` umwandeln. Die eigentliche Berechnung für das *Multidimensional Scaling* findet in der Funktion `cmdscale()` statt. Mit der Option `k = 2` legen wir fest, dass wir nur zwei Hauptkomponenten bzw. Dimensionen bestimmen wollen. Wir machen also aus unserem 37x37 großen Datenmatrix durch *Multidimensional Scaling* eine Reduktion auf zwei Dimensionen bzw. Spalten.

```{r}
#| message: false
#| echo: true
#| warning: false

mds <- distance_tbl %>%
  select(-city) %>% 
  as.dist() %>%          
  cmdscale(k = 2) %>%
  as_tibble() %>% 
  mutate(V1 = -V1,
         V2 = -V2)
colnames(mds) <- c("Dim.1", "Dim.2")

```

In @fig-mds-1 sehen wir das Ergebnis der Dimensionsreduktion auf zwei Dimensionen. Wir erhalten die Zusammenhänge bzw. Distanzen aus der Datenmatrix in einem Scatterplot. Ein Scatterplot ist ja nichts anders als die Darstellung von zwei Dimensionen. Wie wir sehen können nimmt die Anordnung der Orte in etwa die Positionen von den Orten auf der Landkarte in Europa ein. Natürlich stimmen die Relationen nicht perfekt, aber das Abbild ist schon recht nahe dran. Wir können also auf diese Art und Weise auch Ausreißer bestimmen.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-mds-1
#| fig-align: center
#| fig-height: 6
#| fig-width: 7
#| fig-cap: "Scatterplot der zwei Dimensionen nach dem *Multidimensional Scaling* für den Abstand europäischer Städte."

ggscatter(mds, x = "Dim.1", y = "Dim.2", 
          label = distance_tbl$city,
          size = 1,
          repel = TRUE)
```

Wenn wir keine Distanzmatrix wie im obigen Beispiel zu den Entfernungen der europäischen Städte vorliegen haben, dann können wir uns die Distanzen auch mit der Funktion `dist()` berechnen lassen. Wir nutzen jetzt mal als Echtdaten die Daten der Gummibärchen. Mal sehen, ob wir hier irgendwelche Gruppen erkennen. Die Hilfeseite der Funktion `?dist` zeigt welche mathematischen Distanzmaße wir auf die Daten anwenden können. In unseren Fall berechnen wir die euklidische Distanz zwischen den Beobachtungen. Dann rufen wir über die Funkion `cmdsscale` das *Multidimensional Scaling* auf.

```{r}
#| message: false
#| echo: true
#| warning: false

mds <- animals_df %>%
  dist(method = "euclidean") %>%          
  cmdscale(k = 2) %>%
  as_tibble() %>% 
  set_names(c("Dim.1", "Dim.2"))
```

Das Ergebnis des *Multidimensional Scaling* hat keine Bedeutung für uns. Wir können die Zahlen nicht interpretieren. Was wir können ist das Ergebnis in einem Scatterplot wie in @fig-mds-2 zu visualisieren.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-mds-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Scatterplot der zwei Dimensionen nach dem *Multidimensional Scaling* für den Gummibärchendatensatz."

ggscatter(mds, x = "Dim.1", y = "Dim.2", 
          label = rownames(animals_df),
          size = 1,
          repel = TRUE)

```

Na endlich, wir sehen mal eine Gruppe von Beobachtungen oder Ausreißern, die nicht in der Wolke aller Beobachtungen liegen. Beobachtungen mit einem Wert kleiner $-20$ in der 2. Dimension könnten wir dann als Ausreißer entfernen. Der Rest bildet dann eine recht homogene Gruppe. Wir können uns aber auch das $k$-NN Verfahren aus dem @sec-knn nutzen um Cluster in den Daten zu finden. Das heißt wir nutzen das maschinelle Lernverfahren $k-NN$ um uns $k$ Cluster bestimmen zu lassen. Dafür nutzen wir die Funktion `kmeans()` und ziehen uns über die Funktion `pluck()` die Cluster raus. Daher erhalten wir einen Vektor mit Zahlen, die beschreiben in welchem Cluster die jeweilige $i$-te Beobachtung ist.

```{r}
# K-means clustering
clust <- kmeans(mds, centers = 5) %>%
  pluck("cluster") %>% 
  as.factor()
```

Wir wollen jetzt unser MDS Ergebnis von den Gummibärchen um eine Spalte für die Clusterergebnisse von $k$-NN ergänzen.

```{r}
mds <- mds %>%
  mutate(groups = clust)
```

Nun sehen in @fig-mds-3 die gleiche Abbildung wie oben nur ergänzt um die farbliche Hinterlegung der $k=5$ Clustern aus dem $k$-NN Algorithmus. Wir können jetzt die Ausreißer numerisch feststellen und dann aus den Daten entfernen, wenn wir dies wollen würden. Entweder machen wir das über die Clusterzuordnung vom gelben Cluster 2 über die Funktion `filter()` oder aber wir suchen uns die Beobachtungen und damit Zeilen raus, die wir nicht mehr in den Daten wollen. Die Nummern stehen ja dabei.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-mds-3
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Scatterplot der zwei Dimensionen nach dem *Multidimensional Scaling* für den Gummibärchendatensatz mit den $k=5$ Clustern aus dem $k$-NN Algorithmus."

ggscatter(mds, x = "Dim.1", y = "Dim.2", 
          label = rownames(animals_df),
          color = "groups",
          palette = "jco",
          size = 1, 
          ellipse = TRUE,
          ellipse.type = "convex",
          repel = TRUE)
```

## Referenzen {.unnumbered}
