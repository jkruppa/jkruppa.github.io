```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Neural networks & deep learning {#sec-neural}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

So, das war ein ganz schönes Brett, TensorFlow oder Keras auf dem Rechner zu installieren. Es gibt zwar einen [Quick start um Tensorflow zu installieren](https://tensorflow.rstudio.com/install/) aber dann hatte ich das schöne Problem der GPU auf dem macOS mit M1 Chip. Die Lösung für die [Local GPU](https://tensorflow.rstudio.com/install/local_gpu) hat mich auf dem macOS einen Tag Nerven gekostet. Das mag dann auf einem Windows Rechner anders sein bzw. andere Probleme verursachen. Schlussendlich ist die Nutzung von *neural networks* auf keinen Laptops vieleicht auch nicht so die beste Idee. Wir würden die Algorithmen eher auf Hochleistungsrechner durchführen und dann vermutich eine Linuxdistribution verwenden. Dennoch werde ich hier einmal Tensorflow in R vorstellen. Die Packete für die Integration von dem eigenständigen Algorithmus Tensorflow gibt es und wenn es dann mal installiert ist, funktioniert auch alles super. Da Tensorflow in Phyton programmiert ist, muss auch Phyton auf dem Rechner installiert sein.

@mueller2019deep

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, 
               janitor, keras, tensorflow,
               conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

```{r}
gummi_tbl <- read_excel("data/gummibears.xlsx") %>% 
  mutate(gender = as_factor(gender),
         most_liked = as_factor(most_liked),
         student_id = 1:n()) %>% 
  select(student_id, gender, most_liked, age, semester, height) %>% 
  drop_na(gender)

```

In @tbl-gummi-prepro

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-gummi-model-compare
#| tbl-cap: Auszug aus dem Daten zu den Gummibärchendaten.

gummi_raw_tbl <- gummi_tbl %>% 
  mutate(gender = as.character(gender),
         most_liked = as.character(most_liked))

rbind(head(gummi_raw_tbl),
      rep("...", times = ncol(gummi_raw_tbl)),
      tail(gummi_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

```{r}
gummi_data_split <- initial_split(gummi_tbl, prop = 3/4)

gummi_train_data <- training(gummi_data_split)
gummi_test_data  <- testing(gummi_data_split)
```

```{r}
gummi_rec <- recipe(gender ~ ., data = gummi_train_data) %>% 
  update_role(student_id, new_role = "ID") %>% 
  step_impute_mean(all_numeric_predictors(), -has_role("ID")) %>% 
  step_impute_bag(all_nominal_predictors(), -has_role("ID")) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1, -has_role("ID")) %>% 
  step_dummy(all_nominal_predictors(), -has_role("ID")) %>% 
  step_nzv(all_predictors(), -has_role("ID"))

gummi_rec %>% summary()

```

## Keras / Tensorflow

Deeplearning in R

Nochmal genauer anschauen:

https://colorado.rstudio.com/rsc/churn/modeling/tensorflow-w-r.nb.html

https://parsnip.tidymodels.org/reference/details_linear_reg_keras.html

https://camrongodbout.medium.com/tensorflow-in-a-nutshell-part-one-basics-3f4403709c9d

https://medium.com/analytics-vidhya/neural-networks-in-a-nutshell-bb013f40197d

```{r}
keras_mod <- mlp() %>% 
  set_engine("keras") %>% 
  set_mode("classification")
```

```{r}
keras_wflow <- workflow() %>% 
  add_model(keras_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
#| message: false
#| warning: false

keras_fit <- keras_wflow %>% 
  parsnip::fit(gummi_train_data)

```

```{r}
keras_aug <- augment(keras_fit, gummi_test_data ) 
```

```{r}
keras_cm <- keras_aug %>% 
  conf_mat(gender, .pred_class)

keras_cm
```

```{r}
keras_cm %>% summary()
```

```{r}
keras_aug %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

## Resampling

```{r}
#| eval: false
folds <- vfold_cv(gummi_train_data, v = 10)
folds
```

```{r}
#| eval: false
gummi_cv_fit <- keras_wflow %>% 
  fit_resamples(folds)
```

```{r}
#| eval: false
collect_metrics(gummi_cv_fit)
```

## Tuning

```{r}
cores <- parallel::detectCores()
```

```{r}
tune_spec <- mlp(hidden_units = tune(),
                 penalty = tune(), 
                 epochs = tune()) %>% 
  set_engine("keras", num.threads = cores) %>% 
  set_mode("classification") 

tune_spec
```

Ich nutze hier `levels = 2` damit hier die Ausführung nicht so lange läuft. Das ist natürlich etwas zu wenig. Fange am besten mit `levels = 5` an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert.

```{r}
gummi_grid <- grid_regular(hidden_units(range = c(1, 100)),
                           penalty(),
                           epochs(range = c(10, 200)),
                           levels = 5)
```

Eigentlich ist eine 10-fache Kreuzvalidierung mit $v=10$ besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur $v=5$ gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.

```{r}
gummi_folds <- vfold_cv(gummi_train_data, v = 5)
```

```{r}
gummi_tune_wflow <- workflow() %>% 
  add_model(tune_spec) %>% 
  add_recipe(gummi_rec)
```

```{r}
#| eval: false
gummi_tune_res <- gummi_tune_wflow %>% 
   tune_grid(resamples = gummi_folds,
             grid = gummi_grid,
             control = control_grid(verbose = FALSE))
```

```{r}
#| eval: false
#| echo: false

## write_rds(gummi_tune_res, "data/gummi_keras_tune_res.rds")
```

```{r}
#| echo: false

gummi_tune_res <- read_rds("data/gummi_keras_tune_res.rds")
```

```{r}
gummi_tune_res %>%
  collect_metrics() %>%
  mutate(hidden_units = as_factor(hidden_units),
         penalty = as_factor(penalty)) %>%
  ggplot(aes(epochs, mean, color = hidden_units, linetype = penalty)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

```{r}
gummi_tune_res %>%
  show_best("accuracy")
```

```{r}
best_keras <- gummi_tune_res %>%
  select_best("accuracy")

best_keras
```

```{r}
final_gummi_wf <- gummi_tune_wflow %>% 
  finalize_workflow(best_keras)

final_gummi_wf 
```

```{r}
#| cache: true
#| message: false
#| warning: false

final_fit <- final_gummi_wf %>%
  last_fit(gummi_data_split) 
```

```{r}
final_fit %>%
  collect_metrics()

final_fit %>%
  collect_predictions() %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

## Referenzen {.unnumbered}
