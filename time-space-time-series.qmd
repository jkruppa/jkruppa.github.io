```{r echo = FALSE}
#| message: FALSE
pacman::p_load(tidyverse, readxl, knitr, kableExtra, openxlsx)
pacman::p_load(tidyverse, magrittr, janitor, see, readxl,
               xgboost, tidymodels, modeltime, forecast,
               lubridate, plotly, zoo, timetk, xts,
               corrplot, GGally, conflicted)
conflicts_prefer(dplyr::filter)
conflicts_prefer(magrittr::set_names)
conflicts_prefer(plyr::mutate)
conflicts_prefer(dplyr::slice)
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

# Zeitreihen (eng. *time series*) {#sec-time-series}

*Letzte Änderung am `r format(fs::file_info("time-space-time-series.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"Die Vergangenheit ist geschrieben, aber die Zukunft ist noch nicht in Stein gemeißelt." --- Jean-Luc Picard, Star Trek: The Next Generation*

![](images/caution.png){fig-align="center" width="50%"}

::: callout-important
## Bitte bei der Analyse von Zeitreihen gleich von Anfang an beachten!

Die Analyse von Zeitreihen kann sehr quälend sein, wenn dein [Datumsformat nicht richtig](#sec-time-data-format) ist. Bitte achte darauf, dass du in der Spalte mit dem Datum immer das gleiche Format vorliegen hast. Dann können wir das Format später richtig transformieren. Danach geht dann alles einfacher...
:::

In diesem Kapitel wollen wir uns mit Zeitreihen (eng. *time series*) beschäftigen. Was ja auch irgendwie zu erwarten war, denn so heißt ja auch das Kapitel hier. Im Gegensatz zu dem vorherigen Kapitel, wollen wir uns hier aber mit den "echten" Zeitreihen beschäftigen. Mit echten Zeitreihen meine ich Zeitreihen, die im statistischen Sinne eine Reihe von Zeiten sind. Wir haben somit auf der $x$-Achse einer potenziellen Visualisierung die Zeit dargestellt. Wir können aber auch nur eine Reihe von zeitlichen Abständen vorliegen haben. Wir wollen dann auswerten, ob es über den zeitlichen Verlauf einen Trend gibt oder wir ein gutes Modell für den Verlauf der Beobachtungen anpassen können. Ziel ist es dabei den *zukünftigen* Verlauf vorherzusagen. Es geht hier also weniger um statistisches Testen als um eine Modell für eine Vorhersage. Also wir suchen ein Modell für den zukünftigen Verlauf eines Messwertes oder aber zeitlichen Abständen.

In der @fig-time-intro sehen wir eine typische Zeitreihe, die in der klassischen Analyse von Zeitreihen erwartet wird. Mit erwartet wird meine ich, dass wir hier einen Messwert über eine längere Zeit beobachten und wir dabei eine zyklische Abfolge der Messwerte über die Zeit sehen. Die Idee der Zeitreihenanalyse ist unn den weiteren Verlauf der Zeitreihe in der Zukunft vorherzusagen. Wie würden die Messwerte für die zukünftigen Zeitpunkte aussehen? Was können wir erwarten? Diese Fragen wollen wir in diesem Kapitel einmal näher betrachten.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-intro
#| fig-align: center
#| fig-height: 3
#| fig-width: 7
#| fig-cap: "Eine Zeitreihe, wie sie in der klassischen Analyse von Zeitreihen erwartet wird. Wir haben über einen längeren Zeitraum ein zyklisches Verhalten von Messwerten vorliegen. Nicht immer liegt so eine Form der Zeitreihe vor. Häufig ist der Zeitraum zu kurz gewählt."
tibble(erupt = c(78, 74, 68, 76, 80, 84, 50, 93, 55, 76, 58, 74, 75, 80, 56, 80, 69, 57,
           90, 42, 91, 51, 79, 53, 82, 51, 76, 82, 84, 53, 86, 51, 85, 45, 88, 51,
           80, 49, 82, 75, 73, 67, 68, 86, 72, 75, 75, 66, 84, 70, 79, 60, 86, 71,
           67, 81, 76, 83, 76, 55, 73, 56, 83, 57, 71, 72, 77, 55, 75, 73, 70, 83,
           50, 95, 51, 82, 54, 83, 51, 80, 78, 81, 53, 89, 44, 78, 61, 73, 75, 73,
           76, 55, 86, 48, 77, 73, 70, 88, 75, 83, 61, 78, 61, 81, 51, 80, 79),
       day = 1:length(erupt)) %>% 
  ggplot(aes(day, erupt)) +
  geom_point() +
  geom_line() +
  labs(x = "Zeitpunkte", y = "Messwerte") +
  theme_bw()

```

Was bei anderen Themen gilt, gilt natürlich auch für Zeitreihen. Für die Analyse von Zeitreihen wurden und werden ganze Bücher geschrieben. Damit kann ich in diesem Übersichtskapitel nicht dienen. Aber das ist ja auch hier nicht das Ziel. So tiefgreifend kann ich hier nicht die Zusammenhänge darstellen. Aber vermutlich so ausreichend dargestellt, dass du dann in anderen Büchern weiterlesen kannst. Was sollst du also lesen, wenn du mehr wissen willst als hier in dem Kapitel steht? Ich würde dir [Forecasting: Principles and Practice](https://otexts.com/fpp3/) als erstes Buch empfehlen. Du hast hier sogar YouTube Videos für die wichtigsten Inhalte. Dann würde ich den Kurs [STAT 510: Applied Time Series Analysis](https://online.stat.psu.edu/stat510/) als umfangreichen, theoretischen Hintergrund empfehlen. Du findest dort auch teilweise R Code etwas in den Skript vergraben. Wenn du noch mehr lesen willst, dann kann ich dir folgende Literatur empfehlen. @robert2006time liefert eine gute Übersicht über die Anwendung in R, ist aber schon etwas älter. Das Gleiche gilt dann auch für das Buch von @chan2008time und @cowpertwait2009introductory. Dennoch bilden alle drei Bücher die Grundlagen der Analysen von Zeitreihen super ab. Für eine Abschlussarbeit sollten die Quellen also allemal reichen.

Es gibt reichlich R Pakete, die ich teilweise tiefer vorstellen werden. Mit den R Paketen `{tktime}`, `{fable}` und `{modeltime}` gibt es neben den built-in Paketen einiges zur Auswahl. Ich werde hier aber auch alle drei Pakete einmal vorstellen und diskutieren. Mehr geht natürlich immer deshalb kann ich dir da noch die Übersicht auf dem [Big Book of R -- Time Series Analysis and Forecasting](https://www.bigbookofr.com/time-series-analysis-and-forecasting) empfehlen. Dort findest du dann noch mehr frei verfügbare Literatur und auch das ein oder andere R Paket, was genau in der entsprechenden Literatur genutzt und besprochen wird.

Wenn es um Zeitreihen geht, dann ist die Formatierung der Spalte mit dem Datum eigentlich so ziemlich das aufwendigste. Achte bitte darauf, dass du eine einheitlich formatierte Datumsspalte hast, die sich nicht im Laufe der Zeilen ändert. Wenn das der Fall ist, dann musst du meist händisch nochmal die Daten anpassen und das ist meistens sehr aufwendig. Das R Paket `{timetk}` liefert dankenswerterweise Funktionen für die Konvertierung von verschiedenen Zeitformaten in R. Deshalb schaue einmal in die Hilfeseite [Time Series Class Conversion -- Between ts, xts, zoo, and tbl](https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html#introduction) und dann dort speziell der Abschnitt [Conversion Methods](https://business-science.github.io/timetk/articles/TK00_Time_Series_Coercion.html#conversion-methods). Leider ist Zeit in R wirklich relativ.

::: callout-tip
## Weitere Tutorien für die Analyse von Zeitreihen

Wir oben schon erwähnt, kann dieses Kapitel nicht die umfangreiche Analyse von Zeitreihen abarbeiten. Daher präsentiere ich hier eine Liste von Literatur und Links, die mich für dieses Kapitel hier inspiriert haben. Nicht alles habe ich genutzt, aber vielleicht ist für dich was dabei.

-   [Forecasting: Principles and Practice](https://otexts.com/fpp3/) -- Wenn ich keine Zeit hätte mich durch sehr viele Tutorien zu arbeiten, dann würde ich vermutlich mit diesem Online-Buch starten. Es ist alles drin was man braucht um die Zeitreihenanalyse tiefer zu verstehen, als dass ich es hier aufarbeiten kann. Wenn du dann noch das R Paket `{tktime}` nutzt, passt dann wieder alles.
-   [STAT 510: Applied Time Series Analysis](https://online.stat.psu.edu/stat510/) -- Ein umfangreicher Kurs der Penn State - Eberly College of Science. Hier ist dann der R Anteil sehr eingeschränkt bzw. schon etwas älter aber dafür sind die Erklärungen umfangreicher als in diesem Kapitel. Es lohnt sich also nochmal dort für weitere Beispiele einmal reinzuschauen.
-   [Welcome to a Little Book of R for Time Series!](https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/) -- Du findest hier eine sehr gute Übersicht über die Möglichkeiten einer Zeitreihenanalyse mit der Möglichkeit Vorhersagen durchzuführen. Teilweise sind die Funktionen etwas veraltet, ich würde da eher das R Paket `{timetk}` empfehlen. Der Text um die Funktionen herum ist immer noch lesenswert.
-   [timetk for R](https://business-science.github.io/timetk/) -- Hier hast du das aktuelle R Paket, welches ich auch in diesem Kapitel teilweise vorstelle. Leider gibt es hier aber nicht sehr viel Erklärtext, daher musst du dich dann auf andere Quellen verlassen. Die Funktionen und die Anwendbarkeit ist aber sehr schön.
-   [14 Time Series Analysis](https://rc2e.com/timeseriesanalysis) -- Du findest hier neben der klassischen Analyse von Zeitreihen noch Informationen zu anderen Möglichkeiten der Analyse von Zeitreihen. Eingebettet ist das Kapitel zu Zeitreihen in ein umfangreiches Buch über die Analyse von Daten und der Programmierung.
-   [Analysing Time Series Data -- Modelling, Forecasting and Data Formatting in R](https://ourcodingclub.github.io/tutorials/time/) -- Du findest hier nochmal ein gutes und in sich kompaktes Tutorium zu der Zeitreihenanalyse. Hier findest du dann auch mehr erklärenden Text. Auch werden die Datumsformate nochmal genauer auseinander genommen und erklärt. Für den Einstieg sicherlich eine gute Quelle.
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, janitor, see, readxl,
               xgboost, tidymodels, modeltime, forecast,
               lubridate, plotly, zoo, timetk, xts,
               corrplot, GGally, conflicted)
conflicts_prefer(dplyr::filter)
conflicts_prefer(magrittr::set_names)
conflicts_prefer(plyr::mutate)
conflicts_prefer(dplyr::slice)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten & Visualisierung

In diesem Abschnitt wollen wir uns einmal ein paar "echte" Zeitreihen anschauen, die den Anforderungen der klassischen Zeitreihenanalyse an eine Zeitreihe genügen. Du kennst ja schon aus dem vorherigen Kapitel die Möglichkeiten der Visualisierung, deshalb packe ich hier auch gleich zu den Daten die entsprechenden Abbildungen. Dann wird dir der Zusammenhang auch schneller bewusst. Wenn die Daten nochmal wegen dem Datum formatiert werden mussten, dann habe ich das auch gleich hier in einem Abwasch erledigt. Wenn da noch Fragen sind, dann schaue gerne nochmal in das vorherige Kapitel, da findest du dann noch mehr Beispiele. Hier soll es mehr um die Verläufe und die Vorhersage gehen.

Fangen wir also mit dem ersten Datensatz an. Im Sommer 1987 maßen die Ranger des Yellowstone-Nationalparks die Zeit zwischen den Ausbrüchen des Old Faithful Geysirs. Dieser Geysir ist für seine relativ regelmäßigen Ausbrüche bekannt, aber wie du dir vorstellen kannst, ist der Geysir keine Uhr. Ein Ziel bei der Erfassung der Zeiten war es, eine Möglichkeit zu finden, den Zeitpunkt des nächsten Ausbruchs vorherzusagen, um den Touristen, die auf einen Ausbruch warten, die Wartezeit zu erleichtern. Die Daten in Minuten für $n=107$ fast aufeinanderfolgende Wartezeiten lauten dann wie folgt.

```{r}
erupt <- c(78, 74, 68, 76, 80, 84, 50, 93, 55, 76, 58, 74, 75, 80, 56, 80, 69, 57,
           90, 42, 91, 51, 79, 53, 82, 51, 76, 82, 84, 53, 86, 51, 85, 45, 88, 51,
           80, 49, 82, 75, 73, 67, 68, 86, 72, 75, 75, 66, 84, 70, 79, 60, 86, 71,
           67, 81, 76, 83, 76, 55, 73, 56, 83, 57, 71, 72, 77, 55, 75, 73, 70, 83,
           50, 95, 51, 82, 54, 83, 51, 80, 78, 81, 53, 89, 44, 78, 61, 73, 75, 73,
           76, 55, 86, 48, 77, 73, 70, 88, 75, 83, 61, 78, 61, 81, 51, 80, 79)
```

Wir haben hier also eine echte Zeitreihe vorliegen. Wir haben die zeitlichen Abstände zwischen zwei Ausbrüchen. Das ist dann auch die einfachste mögliche Zeitreihe. Dennoch können wir dann an diesem Beispiel sehr viel erklären und uns die gängigen Konzepte der klassischen Zeitreihenanalyse veranschaulichen. In der @fig-time-intro-2 siehst du dann nochmal den Verlauf der Eruptionen über die Zeit. Dir kommt die Abbildung sicherlich aus der Einleitung des Kapitels bekannt vor. Dort habe ich die Abbildung einmal für ein generelles Beispiel genutzt, hier ist dann der richtige Kontext.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-intro-2
#| fig-align: center
#| fig-height: 3
#| fig-width: 7
#| fig-cap: "Dauer zwischen zwei Eruptionen des Old Faithful Geysirs. Eine klare zyklische Zeitreihe lässt sich aus der Abbildung ableiten."
tibble(erupt = c(78, 74, 68, 76, 80, 84, 50, 93, 55, 76, 58, 74, 75, 80, 56, 80, 69, 57,
           90, 42, 91, 51, 79, 53, 82, 51, 76, 82, 84, 53, 86, 51, 85, 45, 88, 51,
           80, 49, 82, 75, 73, 67, 68, 86, 72, 75, 75, 66, 84, 70, 79, 60, 86, 71,
           67, 81, 76, 83, 76, 55, 73, 56, 83, 57, 71, 72, 77, 55, 75, 73, 70, 83,
           50, 95, 51, 82, 54, 83, 51, 80, 78, 81, 53, 89, 44, 78, 61, 73, 75, 73,
           76, 55, 86, 48, 77, 73, 70, 88, 75, 83, 61, 78, 61, 81, 51, 80, 79),
       day = 1:length(erupt)) %>% 
  ggplot(aes(day, erupt)) +
  geom_point() +
  geom_line() +
  labs(x = "Ausbruch", y = "Zeit bis zum Ausbruch") +
  theme_bw()

```

Als nächstes Datenbeispiel möchte ich den schon fast ikonischen Anstieg an $CO_2$ in der Atmosphäre einmal betrachten. Ich habe die Daten von dem [Global Monitoring Laboratory](https://gml.noaa.gov/ccgg/trends/data.html) heruntergeladen. Du kannst dir auch gern einmal die Daten für $CH_4$ $N_2O$ und $SF_6$ anschauen. Auch hier liegen Verläufe vor, die du dann einmal visualisieren und vorhersagen kannst. Wenn du noch mehr Beispiele von echten Daten haben möchtest, dann besuch doch die Webseite [Our World in Data](https://ourworldindata.org/).

Ich muss mich hier etwas strecken um die Daten richtig sauber einzulesen. Zum einen nutze ich die $CO_2$ Daten für die Monate und dann einmal für die Tage. Als erste habe ich das Problem, dass ich erstmal einiges an zeilen überspringen muss, bis die echten Daten kommen. Leider hat das Datumsformat auch eine echt schlimme Kodierung in den Daten. Daher nutze ich die Funktion `parse_date_time()` um mir ein Datum wiedergeben zu lassen. Vorher muss ich die Monate noch so umbauen, dass die Monate immer zwei Zeichen lang sind. Leider braucht `as.Date()` immer einen Tag, was bei den monatlichen Daten nicht gegeben ist. In den folgenden Analysen nutze ich dann beide Datensätz um mal zu schauen, wie deatiliert die Informationen vorliegen müssen. Reicht ein Wert pro Monat oder aber macht es ein Wert pro Tag besser?

```{r}
#| message: false
#| warning: false
co2_monthly_tbl <- read_csv("data/co2_mm_mlo.csv", skip = 40) %>% 
  mutate(month = str_pad(month, 2, pad = "0", side = "left"),
         date = parse_date_time(str_c(year, month), "ym"))

co2_daily_tbl <- read_csv("data/co2_daily_mlo.csv", skip = 32, col_names = FALSE) %>% 
  set_names(c("year", "month", "day", "decimal date", "average")) %>% 
  mutate(month = str_pad(month, 2, pad = "0", side = "left"),
         date = parse_date_time(str_c(year, month, day), "ymd"))
```

In der @fig-time-co2-1 siehst du einmal die Verläufe des $CO_2$ Anstiegs in der Atmosphäre. Beachte hier, dass die beiden Zeitreihen unterschiedlich lang sind. Die monatlichen Daten beginnen in den 1960ziger Jahren wobei wir die Daten für die täglichen Messungen erst ab den späten 70zigern vorliegen haben. Ich habe hier jetzt einmal die Funktion `plot_time_series()` aus dem R Paket `{timetk}` genutzt. Es gibt natürlich auch noch andere Möglichkeiten, aber ich mag die interaktive Funktion, die du sonst über den Umweg über `{plotly}` nutzen müsstest. Aber da gibt es dann in dem vorherigen Kapitel genug Beispiele für. Mehr findest du auch auf der Seite von [R Coder - Evolution charts](https://r-charts.com/evolution/).

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-co2-1
#| fig-align: center
#| fig-height: 3.5
#| fig-width: 6
#| fig-cap: "Verlauf des $CO_2$ Anstiegs in der Atmosphäre. Hier einmal die Darstellung mit der Funktion `plot_time_series()` aus dem R Paket `{timetk}`. Die beiden Datensätze haben aber nicht die gleiche Zeitspanne beobachtet."
#| fig-subcap: 
#|   - "Gemittelt über den Monat."
#|   - "Gemittelt über den Tag."
#| layout-ncol: 1


co2_monthly_tbl %>% 
    plot_time_series(date, average, .interactive = TRUE) 

co2_daily_tbl %>% 
    plot_time_series(date, average, .interactive = TRUE)

```

Einen weiteren Datensatz, den wir uns anschauen wollen, ist ein Datensatz zu der Milchleistung von Kühen stammt aus dem Tutorium [Analysing Time Series Data -- Modelling, Forecasting and Data Formatting in R](https://ourcodingclub.github.io/tutorials/time/). Wir haben hier ein idealisierten Datensatz vorliegen, so dass wir uns nicht mit dem Datumsformat quälen müssen. Der Datensatz wurde auch für die Analysen künstlich erstellt. Daher ist die Milchleistung auch nicht als echt anzusehen. Wir haben es hier im Prinzip mit simulierten Daten zu tun.

```{r}
#| message: false
#| warning: false
milk_tbl <- read_csv("data/monthly_milk.csv") 
```

In der @tbl-time-milk siehst du nochmal einen Auszug aus den Milchdaten. An jedem Tag haben wir die Milchleistung für eine Kuh aufgetragen. Ich würde hier davon ausgehen, dass es sich um die mittlere Leistung handelt. In Wirklichkeit sind die Daten vermutlich etwas komplizierter und wir haben nicht nur eine Leistungsbewertung pro Tag für eine Kuh. Aber für diese Übersicht soll es reichen.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-time-milk
#| tbl-cap: "Auszug aus den Daten zu der Milchleistung von Kühen."

raw_milk_tbl <- milk_tbl %>% 
  mutate_all(as.character)

rbind(head(raw_milk_tbl , n = 4),
      rep("...", times = ncol(raw_milk_tbl)),
      tail(raw_milk_tbl, n = 4)) %>% 
  kable(align = "c", "pipe")
```

Um die Milchdaten in der @fig-time-milk-1 darzustellen nutzen wir die Funktion `plot_time_series()` aus dem R Paket `{timetk}`. Eigentlich ist es ein Zusammenschluss von `{ggplot}` und `{plotly}`. Wenn du die Option `.interactive = TRUE` wie ich setzt, dann bekommst du einen semi-interaktiven Plot durch `{plotly}` wiedergegeben. Mehr Informationen erhälst du dann auf der Hilfeseite von `timetk` zu [Visualizing Time Series](https://business-science.github.io/timetk/articles/TK04_Plotting_Time_Series.html). Wie immer wenn du so generische Funktionen nutzt, musst du schauen, ob dir die Abbildung so gefällt. Du verlierst hier etwas Flexibilität und erhälst dafür aber schneller deine Abbildungen.

Wir erkennen ganz gut, dass wir hier einen Effekt der Saison oder aber der Jahreszeit haben. Wir haben zyklische Peaks der Milchleistung über das Jahr verteilt. Gegen Ende unserer Zeitreihe sehen wir aber eine Art Plateau der Milchleistung. In der folgenden Analyse wollen wir einmal schauen, ob wir die zukünftige Milchleistung anhand der bisherigen Daten vorhersagen können.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-milk-1
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "Die Darstellung der Milchdaten durch das R Paket `{timetk}` und der Funktion `plot_time_series()`, die durch die Option `.interactive = TRUE` intern dann `{plotly}` aufruft."
milk_tbl %>%
  plot_time_series(month, milk_prod_per_cow_kg, .interactive = TRUE)

```

## Analysen von Zeitreihen

Bis jetzt haben wir uns die Visualisierung von Zeitreihen angeschaut. Häufig reicht die Visualisierung auch aus, wenn es um die Darstellung von Temperaturverläufen in einer Abschlussarbeit geht. Wir gehen jetzt aber einen Schritt weiter und wollen unsere Zeitreihe einmal statistisch Analysieren. Die Zeitreihenanalyse oder auch Zeitreihenprognose ist die Verwendung eines Modells zur Vorhersage künftiger Werte auf der Grundlage zuvor beobachteter Werte. Für dich bedeutet dies, dass Prognosen darüber erstellt werden, wie sich solche Zeitreihen in der Zukunft entwickeln werden. Wie in vielen anderen Tutorien auch, werde ich mir hier mal die zwei häufigsten Modelle anschauen: Auto-Regression (AR) und gleitender Durchschnitt (MA) und deren Kombination als ARIMA Modell.

<https://rpubs.com/JSHAH/481706> und [https://medium.com/\@vaibhav1403/ar-model-vs-ma-model-427ee28587a](https://medium.com/@vaibhav1403/ar-model-vs-ma-model-427ee28587a){.uri}

und <https://towardsdatascience.com/time-series-analysis-with-auto-arima-in-r-2b220b20e8ab>

### Definitionen und Überblick

In dem folgenden Abschnitt möchte ich gerne einmal einen Überblick über die wichtigsten Begriffe in der Analyse von Zeitreihen geben. Jetzt geht es also los mit der "richtigen" Zeitreihenanalyse. Teilweise sind es etwas speziellere Begriffe, so dass ich hier erstmal etwas zu den Begriffen schreibe und dann die einzelnen Begriffe nochmal tiefer erkläre. Als vertiefenden Einstieg kann ich hier auch das Buch "Forecasting: principles and practice" von @hyndman2018forecasting empfehlen. Hier gibt es dann auch eine [Webseite mit Videos zu dem Buch](https://otexts.com/fpp3/).

Stationarität (eng. *stationarity*)

:   Eine gängige Annahme bei vielen Zeitreihenverfahren ist, dass die Daten stationär sind. Das klingt etwas seltsam, denn eigentlich soll sich doch was über die Zeit *verändern*. Wie kann dann eine Vorbedingung an Zeitreihen sein, dass Zeitreihen stationär sind? Solange die Zeitreihe nicht stationär ist, können wir kein Zeitreihenmodell erstellen. Insbesondere sind AR oder MA nicht auf nicht-stationäre Reihen anwendbar. Es gibt drei grundlegende Kriterien, damit eine Reihe als stationäre Reihe eingestuft wird:

1.  Der Mittelwert der Reihe darf keine Funktion der Zeit sein, sondern muss konstant sein. Zeitpunkte in der Zukunft haben keine größeren Werte.
2.  Die Varianz der Reihe sollte keine Funktion der Zeit sein. Diese Eigenschaft wird als Homoskedastizität bezeichnet. Die Varianz steigt nicht mit der Zeit an.
3.  Die Kovarianz des $i$-ten Messwerts und des ($i + m$)-ten Messwert sollte keine Funktion der Zeit sein. Die Zeitpunkte untereinander zeigen nur eine eingeschränkte Korrelation.

Ein stationärer Prozess hat die Eigenschaft, dass sich der Mittelwert, die Varianz und die Autokorrelationsstruktur im Laufe der Zeit nicht ändern. Wir sprechen hier also von statistischen Eigenschaften über den Zeitverlauf.

Lag (deu. *Zeitverzögerung*)

:   Mit Lag (deu. *Zeitverzögerung*, abk. *p*) ist im Wesentlichen eine Verzögerung gemeint. Das Konzept des Lag ist zentral für das Verständnis der Zeitreihenanalyse. Betrachte dafür eine Folge von Zeitpunkten. Bei einem Lag von 1 vergleichst du die Zeitreihe $t$ mit einer verzögerten Zeitreihe $t-1$. Du verschiebst die Zeitreihe um einen Zeitpunkt, bevor du sie mit sich selbst vergleichst. So gehst du dann für gesamte Länge der Zeitreihe vor. Wir haben nun eine Autokorrelationsfunktion für das Lag 1 vorliegen. Wenn wir also eine Zeitreihe $t$ vorliegen haben und das Lag 1 berechnen sollen, dann entfernen wir die erste Beobachtung und haben eine $t-1$ *gelaggte* Zeitreihe. Wenn $t$ für heute steht und wir wöchentliche Werte haben, dann steht $t-1$ für die letzte Woche. $y_{t-1}$ stellt also den Wert dar, der vor einer Woche aufgezeichnet wurde. Wenn du das Lag 2 berechnest, dann entfernst du die ersten beiden Zeitpunkte aus den Daten. Wir schreiben anstatt des 1 Lag auch gerne $p(1)$ als Lag 1. Ordnung. Das 2 Lag wäre dann $p(2)$.

Differenz

:   Die Differenz (abk. *d*) zwischen zwei Zeitpunkten in einer Zeitreihe wird auch häufig benötigt um einen stattionäre Zeitreihe zu erreichen. Dabei wird häufig auch von der Ordnung (eng. *order*) geschrieben. Die Ordnung gibt nur an, die wievielte Differenz wir berechnet haben. Klingt wild, ist aber nichts anders als immer wieder die Differenz zwischen zwei Zahlen zu berechnen. Die Differenz 1. Ordnung $d(1)$ zwischen den Zahlen $y = {2, 6, 7}$ ist $d(1) = {4, 1}$. Die Differenz 2. Ordnung $d(2)$ ist dann nur noch die Differenz in der 1. Ordnung und damit $d(2) = 3$.

Autokorrelation

:   Wenn wir die Zeitreihen $t_1$ und $t_2$ vorliegen haben, so zeigt die normale Korrelation $\rho(t_1, t_2)$, wie sehr sich die *zwei* Zeitreihen ähneln. Die Autokorrelation hingegen beschreibt, wie ähnlich die Zeitreihe $t_1$ oder $t_2$ *sich selbst* ist. Damit beschreibt die Autokorrelation die inhärente Ähnlichkeit einer Zeitreihe $t$. Anhand der Werte der Autokorrelationsfunktion können wir erkennen, wie stark sie mit sich selbst korreliert. Dafür nutzen wir ein sogenanntes Lag (deu. *Verzögerung*) um aus einer Zeitreihe $t$ eine zweite Zeitreihe zu bauen. Bei jeder Zeitreihe ist die Korrelation bei einem Lag von 0 perfekt, da man dieselben Werte miteinander vergleicht. Wenn du nun eine Zeitreihe verschiebst, wirst du feststellen, dass die Korrelationswerte zwischen den Lags abnehmen. Wenn die Zeitreihe aus völlig zufälligen Werten besteht, gibt es nur eine Korrelation bei Lag gleich 0, aber keine Korrelation überall sonst. Bei den meisten Zeitreihen ist dies nicht der Fall, da die Werte im Laufe der Zeit abnehmen und somit eine gewisse Korrelation bei niedrigen Lag-Werten besteht. Damit kann die Autokorrelationsfunktion die Frequenzkomponenten einer Zeitreihe aufzeigen.

AR Modell

:   Das AR-Modell (*autoregressive model*) setzt den aktuellen Wert der Reihe in Beziehung zu ihren vergangenen Werten. Es geht davon aus, dass die Vergangenheitswerte in einem linearen Verhältnis zum aktuellen Wert stehen. Deshalb brauchen wir auch eine Art stationären Zeitverlauf.

MA Modell

:   Das MA-Modell (*moving average model*) setzt den aktuellen Wert der Reihe mit dem weißen Rauschen oder den Fehlertermen der Vergangenheit in Beziehung. Es erfasst die Schocks oder unerwarteten Ereignisse der Vergangenheit, die sich noch immer auf die Reihe auswirken. Wir betrachten hier also hier den Fehler aus einer Regression und nicht die tatsächlichen Werte.

ARIMA Model l

:   Wenn wir die beiden Modell kombinieren erhalten wir das ARIMA Modell (abk. *autoregressive integrated moving average*, deu. *autoregressiver gleitender Durchschnitt*). Das ARIMA Modell ist dabei eine Erweiterung schon existierender Modelle und wird sehr häufig für die Analyse von Zeitreihen genutzt. Als wichtigste Anwendung gilt die kurzfristige Vorhersage. Das ARIMA Modell besitzt einen autoregressiven Teil (AR-Modell) und einen gleitenden Mittelwertbeitrag (MA-Modell). Das ARIMA Modell erfordern eigentlich stationäre Zeitreihen. Eine stationäre Zeitreihe bedeutet, dass sich die Randbedingungen einer Zeitreihe nicht verändern. Die zugrunde liegende Verteilungsfunktion der gemessenen Werte über die Zeitreihe muss zeitlich konstant sein. Das heißt konkret, dass die Mittelwerte und die Varianz zu jeder Zeit gleich sind. Gewisse Trends lassen sich durch ein ARIMA Modell herausfiltern.

Exponential smoothing

:   Alternative zu ARIMA <https://otexts.com/fpp3/expsmooth.html>

Cross Correlation Functions and Lagged Regressions

:   Hier nochmal schauen: <https://online.stat.psu.edu/stat510/lesson/8/8.2>

### Grundlagen der Modellierung von Zeitreihen

Im Folgenden wollen wir einmal den englischen Begriff *lag* (deu. *Zeitverzögerung*) verstehen. Dann arbeiten wir uns zu der Autokorrelation bzw. dem autoregressiven Prozess (abk. *AR-Prozess*) vor. Wir brauchen die Idee des Lag um eine Autokorrelation und daraus dann auch einen AR-Prozess ableiten zu können. Zum Verständis des Lag nutzen wir eine [Zeitreihe in Minuten aus dem Yellowstone-Nationalpark](https://math.stackexchange.com/questions/2548314/what-is-lag-in-a-time-serie).

Was ist nun ein Lag? Ein Lag ist einfach nur eine Verschiebung der Daten um einen Wert. Wir schauen uns also für Lag 1 die Werte ohne den ersten Wert an. Bei dem Lag 2 löschen wir die ersten beiden Werte. Und dann ist schon fast klar was wir bei dem Lag 3 machen, wir löschen die ersten drei Werte. Wenn wir dann dennoch die paarweise Korrelation berechnen, dann berechnen wir nicht die Korrelation mit sich selber, das wäre bei Lag 0, sondern eben die Korrelation mit sich selbst verschoben um den Lag. Deshalb nennt sich das dann auch Autokorrelation.

Dann berechnen wir einmal mit der Funktion `lag` aus dem R Paket `{dplyr}` die Lags 1, 2, und 3. Alle Lags packen wir dann mit den originalen Daten zusammen in einen Datensatz und haben somit auch gleich einen Vergleich.

```{r}
erupt_tbl <- tibble(erupt, 
                    erupt_p1 = dplyr::lag(erupt, 1),
                    erupt_p2 = dplyr::lag(erupt, 2),
                    erupt_p3 = dplyr::lag(erupt, 3))
erupt_tbl
```

Wie du siehst, haben wir immer eine Verschiebung um einen Wert. Daher können wir jetzt trotzdem eine lineare Regression auf die verschiedenen Lags und den originalen Daten rechnen. Dabei fleigen natürlich alle Zeilen aus den Daten wo wir einen fehlenden Wert haben, aber das ist ja dann auch gewünscht. In den folgenden Tabs kannst du einmal sehen, wie ich für verschiedene Lags die lineare Regression gerechnet habe. Nichts anderes ist dann auch ein AR-Modell. Wir berechnen über die lineare Regression die Korrelation für jedes Lag zu den originalen Daten.

::: panel-tabset
## Lag 1 oder p(1)

```{r}
lm(erupt_p1 ~ erupt, erupt_tbl) %>% 
  coef()
```

## Lag 2 oder p(2)

```{r}
lm(erupt_p2 ~ erupt, erupt_tbl) %>% 
  coef()
```

## Lag 3 oder p(3)

```{r}
lm(erupt_p3 ~ erupt, erupt_tbl) %>% 
  coef()
```
:::

In der @fig-time-lag siehst du nochmal die verschobenen Daten auf der $y$-Achse und die originalen Daten auf der $x$-Achse. Durch die Verschiebung ändert sich immer die Punktewolke und damit dann auch die Regression sowie die berechnete Korrelation als "Steigung der Geraden". Die Idee ist eben, einen Zyklus in den Eruptionen zu finden. Wenn wir also immer kurz/lang Wartezeiten hätten, dann würde sich auch immer die Korrelation ändern, wenn wir um einen Wert verschieben. Das trifft natürlich nur zu, wenn es eben der Rhythmus um *einen* Zeitpunkt verschoben ist. Häufig ist aber nicht *ein* Zeitpunkt sondern een *zwei* oder mehr. Das müssen wir dann durch eine Modellierung herausfinden.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-lag
#| fig-align: center
#| fig-height: 4
#| fig-width: 4
#| fig-cap: "Dastellung der orginalen Eruptionszeiten des Old Faithful Geysirs sowie die entsprechenden Daten für die Lag 1 bis 3. Die Regressiongleichung wurde für alle Zusammenhänge ergänzt."
#| fig-subcap: 
#|   - "p(1) vs. Orginal Eruptionszeiten."
#|   - "p(2) vs. Orginal Eruptionszeiten."
#|   - "p(3) vs. Orginal Eruptionszeiten."
#| layout-nrow: 1

erupt_tbl %>% 
  ggplot(aes(erupt, erupt_p1)) +
  theme_bw() +
  geom_point2(color = cbbPalette[7]) +
  geom_function(fun = \(x){119.49 - 0.68 * x}, 
                color = cbbPalette[7]) +
  annotate("text", x = 50, y = 60, label = "r = -0.69", size = 6) +
  labs(x = "Orginal Eruptionszeiten", y =  "Lag 1 Eruptionszeiten")

erupt_tbl %>% 
  ggplot(aes(erupt, erupt_p2)) +
  theme_bw() +
  geom_point2(color = cbbPalette[6]) +
  geom_function(fun = \(x){31.81 + 0.55 * x}, 
                color = cbbPalette[6]) +
  annotate("text", x = 50, y = 90, label = "r = 0.55", size = 6) +
  labs(x = "Orginal Eruptionszeiten", y =  "Lag 2 Eruptionszeiten")

erupt_tbl %>% 
  ggplot(aes(erupt, erupt_p3)) +
  theme_bw() +
  geom_point2(color = cbbPalette[2]) +
  geom_function(fun = \(x){101.28 - 0.42 * x}, 
                color = cbbPalette[2]) +
  annotate("text", x = 50, y = 60, label = "r = 0.55", size = 6) +
  labs(x = "Orginal Eruptionszeiten", y =  "Lag 3 Eruptionszeiten")

```

Bevor wir aber mit der Modellierung von Zeitreihen beginnen, hier nochmal die vollständige Korrelationsmatrix für alle Lags $p$ und den originalen Daten. Ich habe den Zusammenhang einmal in der @fig-time-corrplot-1 dargestellt. Wenn du mehr über die Visualisierung der Korrelation erfahren möchtest, dann besuche das [Kapitel zur Korrelation](#sec-linear-corr-visu). Hier sehen wir dann gut, wie mit jedem Schritt im Lag die Korrelation im Vorzeichen flippt. Ein Zeichen, dass wir es hier mit einer zyklischen Zeitreihe zu tun haben und wir etwas in den Daten entdecken könnten.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-corrplot-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Korrelation zwischen den originalen Daten und den Lags, auch als $p$ bezeichnet. Im unteren Bereich des Korrelationsplot sind die Scatterplost mit der Regressionsgraden eingezeichnet. Im oberen Bereich finden sich die berechneten Korrelationskoeffizienten $\\rho$ für die paarweisen Vergleiche."

cor_func <- function(data, mapping, method, symbol, ...){
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  corr <- cor(x, y, method=method, use='complete.obs')
  ggally_text(
    label = paste(symbol, as.character(round(corr, 2))), 
    mapping = aes(),
    xP = 0.5, yP = 0.5,
    color = 'black', size = 6
  ) 
}

lower_fun <- function(data, mapping) {
  ggplot(data = data, mapping = mapping) +
    geom_point2(alpha = 0.7) + 
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE)
}

ggpairs(erupt_tbl, 
        upper = list(continuous = wrap(cor_func, method = 'pearson', 
                                       symbol = expression('\u03C1 ='))),
        lower = list(continuous = wrap(lower_fun)),
        columnLabels = c("Original", "p(1)", "p(2)", "p(3)"),
        axisLabels = "internal") +
  theme_bw()
```

Das war jetzt die händische Darstellung. Wir können uns die Autokorrelation auch über eine Zeitreihe anschauen. Wir nutzen hier die etwas primitive Funktion `acf()`, da nur die "alten" Funktionen nur mit einem Vektor von Zeiten klar kommen. Eigentlich brauchen wir ja zu jedem Wert ein Datum. Das haben wir hier aber nicht, deshalb müssen wir hier zu der alten Implementierung greifen.

```{r}
erupt_ts <- tk_ts(erupt)
```

In der Abbildung @fig-time-correlogram-1 sehen wir die Autokorrelation zwischen den Orginaldaten des Geysirs und den entsprechenden Korrelationen zu den Lags 1 bis 5. Wenn du nochmal weiter oben schaust, dann haben wir für die Korrelationen von den Orginaldaten zu den entsprechenden Lags folgende Korrelationen berechnet. Wir hatten eine Korrelation von $\rho = -0.68$ zu Lag 1, eine Korrelation von $\rho = 0.55$ zu Lag 2 und ein $\rho = -0.43$ zum Lag 3 beobachtet. Die Korrelationen findest du dann als Striche auch in der Abbildung wieder.

Nun ist es aber so, dass natürlich die Lags untereinander auch korreliert sind. Diese Korrelation untereinander wollen wir dann einmal raus rechnen, so dass wir nur die partielle Korrelation haben, die zu den jeweiligen Lags gehört. Dabei entsteht natürlich eine Ordnung. Das Lag 1 wird vermutlich die meiste Korrelation erklären und dann folgen die anderen Lags. Deshalb nennen wir diese Art der Korrelation auch *partial* Autokorrelation (deu. *partielle*). Du siehst die Anteile der partiellen Korrelation zu den jeweiligen Lags dann in der @fig-time-correlogram-2. Wir werden dann später bei `{tktime}` eine bessere Art und Weise sehen die Abbildungen zu erstellen. Zum Verstehen sind die Abbildungen gut, aber schön sind die Abbildungen nicht.

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-correlogram
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Korrelationsabbildungen mit der Option `lag.max = 5`. Daher werden nur die ersten fün Lags betrachtet. Die Abbildungen dienen der Veranschaulichung vom Lag. Für eine Veröffentlichung bitte die Funktionen aus `{tktime}` verwenden. Beim Correlogram ist das Lag 0 entfernt, da die Korrelation mit sich selbst immer 1 ist."
#| fig-subcap: 
#|   - "Correlogram."
#|   - "Partial Correlogram."
#| layout-nrow: 1
#| column: page

erupt_ts %>% 
  acf(main = "Correlogram", lag.max = 5, 
      ylim = c(-1, 1), xlim = c(1, 5))

erupt_ts %>% 
  pacf(main = "Partial Correlogram", lag.max = 5, 
       ylim = c(-1, 1))

```

In der folgenden @fig-time-ar-1 siehst du nochmal verschiedene Beispiele für eine Autokorrelation bzw. einen autoregressiven Prozess (abk. *AR*). Der einfachste AR-Prozess ist ein Prozess der Ordnung 0 und daher schreiben wir dann auch AR(0). Bei einem AR(0) liegt keine Abhängigkeit zwischen den Zeitpunkten vor. Nur der Fehlerterm trägt zum Output des Prozesses bei, so dass AR(0) in der Abbildung weißem Rauschen entspricht.

Bei einem AR(1)-Prozess mit einem positiven $\varphi$ tragen nur der vorherige Term des Prozesses und der Rauschterm zum Output bei. Wenn $\varphi$ nahe bei 0 liegt, dann sieht der Prozess immer noch wie weißes Rauschen aus, aber wenn $\varphi$ sich 1 nähert, erhält die Ausgabe einen größeren Beitrag des vorhergehenden Terms im Verhältnis zum Rauschen. Dies führt zu einer "Glättung" oder Integration der Ausgabe, ähnlich wie bei einem Tiefpassfilter.

Bei einem AR(2)-Prozess tragen die beiden vorhergehenden Terme und der Rauschtext zur Ausgabe bei. Wenn beide $\varphi_1$ und $\varphi_2$ positiv sind, ähnelt das Ergebnis einem Tiefpassfilter, bei dem der Hochfrequenzanteil des Rauschens verringert wird. $\varphi_1$ positiv ist, während $\varphi_2$ negativ ist, dann begünstigt der Prozess Vorzeichenwechsel zwischen den Termen des Prozesses. Die Ausgabe oszilliert. Dies kann mit der Erkennung von Kanten oder Richtungsänderungen verglichen werden.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-ar-1
#| fig-align: center
#| fig-height: 6
#| fig-width: 8
#| fig-cap: "AR(0); AR(1) mit AR-Parameter 0.3; AR(1) mit AR-Parameter 0.9; AR(2) mit AR-Parameter 0.3 und 0.3; und AR(2) mit AR-Parameter 0.9 und -0.8."

set.seed(202318080)

plot_tbl <- map(lst(0.01, 0.3, 0.9, c(0.3, 0.3), c(0.2, 0.7)), \(x) {
  arima.sim(model = list(ar = x), n = 100) %>% 
    as_tibble() 
}) %>% 
  bind_cols() %>% 
  set_names(c("AR(0) 0", "AR(1) 0.3", "AR(1) 0.9", "AR(2) 0.3; 0.3", "AR(2) 0.9; -0.8")) %>% 
  mutate(index = 1:100) %>% 
  pivot_longer(cols = matches("AR"),
               values_to = "value",
               names_to = "key") %>% 
  mutate(key = as_factor(key))

ggplot(plot_tbl, aes(index, value, fill = key)) +
  theme_bw() +
  facet_wrap(~ key, nrow = 5, strip.position="right") + 
  geom_line() +
  geom_hline(yintercept = 0) +
  geom_ribbon(aes(ymin = ifelse(value <= 0, value, 0), 
                  ymax = ifelse(value >= 0, value, 0)), 
              alpha=0.5) +
  ylim(-5, 5) +
  labs(x = "", y = "") +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        legend.position = "none", 
        strip.text = element_text(size = 10))

```

::: panel-tabset
## Orginal

```{r}
erupt[1:10]
```

## Differenz 1

```{r}
diff(erupt[1:10], differences = 1)
```

## Differenz 2

```{r}
diff(erupt[1:10], differences = 2)
```
:::

```{r}
diff_tbl <- tibble(erupt,
                   erupt_d1 = c(NA, diff(erupt, 1)),
                   erupt_d2 = c(NA, NA, diff(erupt, 2)))
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-diff-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 5
#| fig-cap: "Korrelation zwischen den originalen Daten und den Lags. Im unteren Bereich des Korrelationsplot sind die Scatterplost mit der Regressionsgraden eingezeichnet. Im oberen Bereich finden sich die berechneten Korrelationskoeffizienten $\\rho$ für die paarweisen Vergleiche."

ggpairs(diff_tbl, 
        upper = list(continuous = wrap(cor_func, method = 'pearson', 
                                       symbol = expression('\u03C1 ='))),
        lower = list(continuous = wrap(lower_fun)),
        columnLabels = c("Original", "d(1)", "d(2)"),
        axisLabels = "internal") +
  theme_bw()
```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-time-diff-milk-1
#| fig-align: center
#| fig-height: 3
#| fig-width: 5
#| fig-cap: "Stationäre Abbildungen der CO$_2$-Daten sowie der Milchdaten nach der Differenzbildung mit $d(1)$. Wir sehen, dass wir einen stationären Verlauf erreichen."
#| fig-subcap: 
#|   - "CO$_2$ Daten."
#|   - "Milch Daten."
#| layout-nrow: 1

co2_monthly_tbl %>% 
  mutate(diff = c(NA, diff(average, 1))) %>% 
  plot_time_series(date, diff, .interactive = TRUE) 

milk_tbl %>% 
  mutate(diff = c(NA, diff(milk_prod_per_cow_kg, 1))) %>% 
  plot_time_series(month, diff, .interactive = TRUE)
```

[Time Series as Features](https://www.kaggle.com/code/ryanholbrook/time-series-as-features)

[Time series forecasting](https://medium.com/analytics-vidhya/time-series-forecasting-d611fa8ae6ca)

The coefficient ϕ1 is a numeric constant by which we multiply the lagged variable (Xt-1). You can interpret it as the part of the previous value which remains in the future. It's good to note that these coefficients should always be between -1 and 1.

Let me explain why.

If the absolute value of the coefficient is greater than 1, then over time, it would blow up immeasurably.

This idea can seem confusing at first. So let's take a look at a mathematical example.

Say, we have a time-series with 1000 observations and ϕ1 = 1.3 and C=0.

Then, X2 = 0 + 1.3 X1

## Vorhersagen mit...

### ...`{ts}` und `{zoo}`

Eigentlich ist `{ts}` kein eigens Paket sondern die bulit-in Lösung von R, aber ich möchte hier dann doch `{ts}` als Paket schreiben, damit hier mehr Ordnung drin ist.

Wenn wir eine Vorhersage auf einem zeitlichen Verlauf rechnen wollen, dann brauchen wir als aller erstes einen Datensatz, der auch eine *echte* Zeitreihe über mehrere zeitliche Zyklen enthält. Das ist dann meistens die Herausforderung so eine Zeitreihe in einer Abschlussarbeit zu erzeugen. In ein paar Monaten einen zyklischen Verlauf zu finden ist schon eine echte Leistung. Deshalb nehmen wir hier als Beispiel einmal unsere künstlichen Daten zur Milchleistung von Kühen. Wie du in der obigen @fig-time-milk-1 klar erkennen kannst, haben wir hier Zyklen über die einzelnen Jahre hinweg. Es liegt ein stetiger, zyklischer Anstieg der Milchleistung über die beobachteten Jahre vor. Wir wollen jetzt den Verlauf modellieren und einen zukünftigen Verlauf vorhersagen. Oder andersherum, wie die zukünftigen Zyklen aussehen könnten.

In diesem Beispiel nutzen wir das R Paket `{zoo}` und die Funktion `ts()` für die Standardimplemetierung von Zeitreihen in R. Das hat immer ein paar Nachteile, da wir hier die veralteten Speicherformen für eine Zeitreihe nutzen. Auf der anderen Seite sind viele Tutorien im Internet noch genau auf diese Funktionen ausgerichtet. Deshalb auch hier einmal die Erklärung der etwas älteren Funktionen. Später schauen wir dann auch die neuere Implementierung in dem R Paket `{tktime}` einmal an. Wir wandeln also erstmal unsere Milchdaten mit der Funktion `tk_ts()` in ein `ts`-Zeitobjekt um. Dafür müssen wir angeben von wann bis wann die Jahre laufen und wie viele Beobachtungen jedes Jahr hat. Glücklicherweise müssen nicht alle Jahre gefüllt werden und die Funktion erlaubt auch einen anderen Startmonat als `Jan`. Wie du gleich siehst, dann haben wir eine Art Matrix als Ausgabe.

```{r}
milk_ts <- milk_tbl %$%
  tk_ts(milk_prod_per_cow_kg, start = 1962, end = 1975, frequency = 12)
milk_ts
```

```{r}
#| eval: false
co2_monthly_ts <- co2_monthly_tbl  %$%
  tk_ts(average, start = c(1958, 3), end = 2023, frequency = 12)

co2_month_arima_obj <- auto.arima(co2_monthly_ts)

arima_mdl <- forecast(co2_month_arima_obj, 12*6)

autoplot(arima_mdl) +
  theme_bw()

```

Ein ARIMA Modell setzt sich aus drei Komponenten wie folgt zusammen.

1)  Dem verwendeten Lag $p$ der Zeitreihe (eng. *autoregressiv model* oder AR-Modell), die wir für die Berechnung der Autokorrelation der Zeitreihe nutzen.
2)  Der verwendeten Differenz $d$, mit der wir unsere Zeitreihe anpassen.
3)  Der laufenden Durchschnitt $q$ (eng. *moving average model* oder MA-Modell). Hiermit ist nicht das glättende Mittel gemeint. Wir modellieren hier etwas anderes.

In der folgenden @fig-time-ts-3 siehst du die Zeitverzögerung (eng. *lag*).

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-ts-3
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "foo."

milk_ts %>% 
  acf(main = 'Correlogram')
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-ts-4
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "foo."

milk_ts %>% 
  pacf(main = 'Partial Correlogram' )

```

```{r}
decomp <- decompose(milk_ts)
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-ts-1
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "foo."
plot(decomp)
```

```{r}
#| cache: true
#| warning: false
#| message: false
#| label: arima_milk
milk_arima_obj <- auto.arima(milk_ts)
```

```{r}
arima_mdl <- forecast(milk_arima_obj, 24)
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-ts-2
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "foo."

autoplot(arima_mdl) +
  theme_bw()



```

### ... `{tktime}`

[{timetk}](https://business-science.github.io/timetk/)

### ... `{fable}` und `{feasts}`

[{fable}](https://fable.tidyverts.org/articles/fable.html) [{feasts}](https://feasts.tidyverts.org/index.html)

### ... `{modeltime}`

Am Ende hier nochmal eine Möglichkeit sehr effizient Zeitreihen zu modellieren und eine Vorhersage für zukünftige Verläufe zu machen. Ich empfehle hier die Hilfeseite [Getting Started with Modeltime](https://business-science.github.io/modeltime/articles/getting-started-with-modeltime.html). Auf der Seite erfährst du dann die Grundlagen für die Anwendung von dem R Paket `{modeltime}`. Hier kannst du dann tief in den Kaninchenbau reingehen. Neben dem R Paket `{modeltime}` findest du im Ökosystem `{modeltime}` noch andere spannende R Pakete, die dir weitere und vertiefende Modellierungen zur Vorhersage erlauben. Wenn dir die Begriffe zu der Vorhersage und der Klassifikation etwas komisch vorkommen, dann kannst du in dem Kapitel [Grundlagen der Klassifikation](#sec-class-basic) reinschauen und nachlesen wo du dann noch Lücken hast. Ich gehe hier dann nicht mehr so tief auf die einzelnen Punkte ein, sondern führe hier eher grob durch den Code.

```{r}
#| warning: false
#| message: false
splits <- initial_time_split(milk_tbl, prop = 0.9)
```

```{r}
#| cache: true
#| warning: false
#| message: false
#| label: arima_modeltime
model_fit_arima_no_boost <- arima_reg() %>%
    set_engine(engine = "auto_arima") %>%
    fit(milk_prod_per_cow_kg ~ month, data = training(splits))
```

```{r}
#| warning: false
#| message: false
model_fit_ets <- exp_smoothing() %>%
    set_engine(engine = "ets") %>%
    fit(milk_prod_per_cow_kg ~ month, data = training(splits))
```

```{r}
#| warning: false
#| message: false

milk_tbl %>% 
  mutate(month_fac = factor(month(month, label = TRUE), ordered = FALSE),
         month_num = as.numeric(month)) %>% 
  head(7)
```

```{r}
#| warning: false
#| message: false
model_fit_lm <- linear_reg() %>%
    set_engine("lm") %>%
    fit(milk_prod_per_cow_kg ~ as.numeric(month) + factor(month(month, label = TRUE), ordered = FALSE),
        data = training(splits))
```

```{r}
#| warning: false
#| message: false
models_tbl <- modeltime_table(
    model_fit_arima_no_boost,
    model_fit_ets,
    model_fit_lm
)
```

```{r}
#| warning: false
#| message: false
calibration_tbl <- models_tbl %>%
    modeltime_calibrate(new_data = testing(splits))
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-modeltime-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Eigenwerte für die beiden Datensätze."

calibration_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = milk_tbl
    ) %>%
    plot_modeltime_forecast(
      .legend_max_width = 25, # For mobile screens
      .interactive      = FALSE
    )
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: tbl-time-modeltime-1
#| tbl-cap: "Eigenwerte für die beiden Datensätze."

calibration_tbl %>%
    modeltime_accuracy() %>%
    table_modeltime_accuracy(
        .interactive = FALSE
    )

```

-   MAE - Mean absolute error, [`mae()`](https://yardstick.tidymodels.org/reference/mae.html)
-   MAPE - Mean absolute percentage error, [`mape()`](https://yardstick.tidymodels.org/reference/mape.html)
-   MASE - Mean absolute scaled error, [`mase()`](https://yardstick.tidymodels.org/reference/mase.html)
-   SMAPE - Symmetric mean absolute percentage error, [`smape()`](https://yardstick.tidymodels.org/reference/smape.html)
-   RMSE - Root mean squared error, [`rmse()`](https://yardstick.tidymodels.org/reference/rmse.html)
-   RSQ - R-squared, [`rsq()`](https://yardstick.tidymodels.org/reference/rsq.html)

```{r}
#| warning: false
#| message: false
refit_tbl <- calibration_tbl %>%
    modeltime_refit(data = milk_tbl)
```

```{r}
#| message: false
#| echo: true
#| warning: false
#| label: fig-time-modeltime-2
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Eigenwerte für die beiden Datensätze."
refit_tbl %>%
    modeltime_forecast(h = "3 years", actual_data = milk_tbl) %>%
    plot_modeltime_forecast(
      .legend_max_width = 25, # For mobile screens
      .interactive      = FALSE
    )
```

## Referenzen {.unnumbered}
