```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Random Forest {#sec-class-rf}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

::: column-margin
![](images/angel_01.png){fig-align="center" width="50%"}

Wir werden uns hier mit der Anwendung beschäftigen. Wie immer lassen wir daher *tiefere* mathematische Überlegungen weg.
:::

## Rpart

## Random Forest

![Dars.](images/class-rf-01.png){#fig-class-rf-01 fig-align="center" width="100%"}

![Dars.](images/class-rf-02.png){#fig-class-rf-02 fig-align="center" width="100%"}

![Dars.](images/class-rf-03.png){#fig-class-rf-03 fig-align="center" width="100%"}

![Dars.](images/class-rf-04.png){#fig-class-rf-04 fig-align="center" width="100%"} \## Bagging

Das Wort *Bagging* steht für *bootstrap aggregating* und ist eine Methode, um Vorhersagen aus verschiedenen Modellen zu kombinieren. Dabei müssen alle Modelle mit dem gleichen Algorithmus laufen, können aber auf verschiedenen Datensätzen oder aber Variablensätzen zugreifen. Häufig haben die Modelle eine hohee Varianz in der Vorhersage und wir nutzen dann Bagging um die Modelle miteinader zu kombinieren und dadurch die Varianz zu verringern. Die Ergebnisse der Modelle werden dann im einfachsten Fall gemittelt. Das Ergebnis jeder Modellvorhersage geht mit gleichem Gewicht in die Vorhersage ein. Wir haben auch noch andere Möglichkeiten, aber du kannst dir Vorstellen wir rechnen verschiedene Modelle $k$-mal und bilden dann ein finales Modell in dem wir alle $k$-Modelle zusammenfassen. Wie wir die Zusammenfassung rechnen, ist dann immer wieder von Fall zu Fall unterschiedlich. Wir erhalten am Ende einen *Ensemble* Klassifizierer, da ja ein Ensemble von Modellen zusammengefasst wird.

## Boosting

![](images/caution.png){fig-align="center" width="50%"}

https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5

https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30

https://howtolearnmachinelearning.com/articles/boosting-in-machine-learning/
