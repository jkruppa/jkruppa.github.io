```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, Hmisc)
```

# Decision trees {#sec-class-tree}

*Version vom `r format(Sys.time(), '%B %d, %Y um %H:%M:%S')`*

![](images/caution.png){fig-align="center" width="50%"}

In diesem Kapitel wollen wir uns mit Entscheidungsbäumen (eng. *decision trees*) beschäftigen. Wie oft gibt es auch bei der Anwendung von Entscheidungsbäumen eine Menge Varianten. Wir wollen uns in diesem Kapitel eine erste Übersicht geben und du kannst dann ja schauen, welche Varianten es noch von den Entscheidungsbäumen gibt. Wichtig ist zu wissen, unsere Bäume spalten sich immer nur in zwei Äste auf.

::: column-margin
![](images/angel_01.png){fig-align="center" width="50%"}

Wir werden uns hier mit der Anwendung beschäftigen. Wie immer lassen wir daher *tiefere* mathematische Überlegungen weg.
:::

-   In @sec-rpart schauen wir uns *einen* Entscheidungsbaum an. Wir teilen nacheinander unsere Beobachtungen immer weiter anhand unser Prädiktoren in zwei Gruppen auf, bis wir nur noch Gruppen haben, die fast nur noch aus einer Klasse des Labels bestehen.
-   In @sec-rf schauen wir uns ein Ensemble von Entscheidungsbäumen an. Wir lassen daher nicht einen sondern hunderte von Entscheidungsbäumen wachsen. Damit wir nicht immer den gleichen Baum auf den gleichen Daten wachsen lassen, wählen zufällig Beobachtungen und Variablen aus, die wir zum Erstellen der Bäume nutzen. Wir lassen einen Random Forest wachsen.
-   In @sec-xgboost betrachten wir eine komplexere Implementierung der Random Forest. Wir nutzen hier Gradient Boosting um die Bäume noch besser anhand unseres Trainingsdatensatzes wachsen zu lassen.

Alle drei Algorithmen gehen wir jetzt einmal durch. Dabei können wir bei einem Entscheidunsgbaum noch recht gut nachvollziehen, was dort eigentlich passiert. Bei mehreren Bäumen zusammen, können wir nur noch schematisch nachvollziehen was die einzelnen Schritte in der Modellbildung sind.

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, tidymodels, magrittr, 
               janitor, vip, rpart.plot, see,
               conflicted)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
conflict_prefer("extract", "magrittr")
##
set.seed(2025429)
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Bei dem vorherigen Beispielen haben wir immer unseren Datensatz zu den infizierten Ferkeln genutzt. In diesem Kapitel wolle wir uns aber mal auf einen echten Datensatz anschauen. Wir nutzen daher einmal den Gummibärchendatensatz. Als unser Label und daher als unser Outcome nehmen wir das Geschlecht `gender`. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir nur die Spalte Geschlecht sowie als Prädiktoren die Spalten `most_liked`, `age`, `semester`, und `height`.

```{r}
gummi_tbl <- read_excel("data/gummibears.xlsx") %>% 
  mutate(gender = as_factor(gender),
         most_liked = as_factor(most_liked)) %>% 
  select(gender, most_liked, age, semester, height) %>% 
  drop_na(gender)

```

Wir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein `NA` in der Variable `gender` haben. Wir haben dann insgesamt $n = `r nrow(gummi_tbl)`$ Beobachtungen vorliegen. In @tbl-gummi-prepro sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: tbl-gummi-model-compare
#| tbl-cap: Auszug aus dem Daten zu den Gummibärchendaten.

gummi_raw_tbl <- gummi_tbl %>% 
  mutate(gender = as.character(gender),
         most_liked = as.character(most_liked))

rbind(head(gummi_raw_tbl),
      rep("...", times = ncol(gummi_raw_tbl)),
      tail(gummi_raw_tbl)) %>% 
  kable(align = "c", "pipe")
```

Unsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Um die Klassifikation mit Entscheidungsbäumen rechnen zu können brauchen wir wie bei allen anderen Algorithmen auch einen Trainings- und Testdatensatz. Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Der Traingsdatensatz ist dabei immer der größere Datensatz. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.

[Im maschinellen Lernen sind alle Datensätze, die weniger als tausend Beobachtungen vorliegen haben, klein.]{.aside}

```{r}
gummi_data_split <- initial_split(gummi_tbl, prop = 3/4)
```

Wir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluß einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.

```{r}
gummi_train_data <- training(gummi_data_split)
gummi_test_data  <- testing(gummi_data_split)
```

Nachdem wir die Daten vorbereitet haben, müssen wir noch das Rezeot mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht `gender` als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem `.` Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Es gibt wie immer noch andere Imputationsmöglichkeiten, ich habe mich jetzt aus praktischen Gründen für dies beiden Verfahren entschieden. Ich überspringe hier auch die Diagnose der Imputation, also ob das jetzt eine gute und sinnvolle Imputation der fehlenden Werte war oder nicht. Die Diagnoseschritte müsstest du im Anwendungsfall nochmal im [Kapitel zur Imputation](#sec-missing) nachlesen und anwenden. Dann müssen wir noch alle numerischen Vriablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.

```{r}
gummi_rec <- recipe(gender ~ ., data = gummi_train_data) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_impute_bag(all_nominal_predictors()) %>% 
  step_range(all_numeric_predictors(), min = 0, max = 1) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors())

gummi_rec
```

Alles in allem haben wir ein sehr kleines Modell. Wir haben ja nur ein Outcome und vier Prädiktoren. Trotzdem sollte dieser Datensatz reichen um zu erklären wie Entscheidungsbäume funktionieren.

## Rpart {#sec-rpart}

Wie funktioniert nun ein Entscheidungsbaum? Ein Entscheidungsbaum besteht aus Knoten (eng. *nodes*) und Ästen (eng. *edge*). Dabei hat immer ein Knoten zwei Äste. Die Beobachtungen in einem Knoten fallen nach einer Entscheidungsregel anhand eines Prädiktors in entlang zweier Äste in zwei separate Knoten. So können wir unsere $n = `r nrow(gummi_tbl)`$ zum Beispiel anhand des Alters in zwei Gruppen aufteilen. Wir legen willkürlich die Altersgrenze bei 22 fest.

```{r}
gummi_tbl %>% 
  mutate(grp = if_else(age >= 22, 1, 0)) %>% 
  pull(grp) %>% 
  tabyl()
```

Wir erhalten mit diesem Split zwei Gruppen mit je $n_0 = 207$ und $n_1 = 259$ Beobachtungen. Wir haben jetzt diesen Split willkürlich gewählt. In dem Algorithmus für die Entscheidungsbäume wird dieser Schritt intern optimiert, so dass wir den besten Wert für den Alterssplit finden, der uns möglichst reine Knoten im Bezug auf das Label liefert. Wir wollen ja am Ende einen Algorithmus trainieren, der uns die Geschlechter bestmöglich auftrennt, so dass wir eine neue Beobachtung bestmöglich vorhersagen können. Wenn keine Aufteilungen in einem Knoten mehr möglich sind, dann nennen wir diesen Knoten einen Terminalknoten.

In @fig-class-rf-01

![Darstellung des Anwachsen des Entscheidungsbaumes. Links sind die beiden Prädiktoren $X_1$ und $X_2$ als Koordinatensysten dargestellt. Die Punkte stllen die Beobachtungen mit den jeweiligen Label weiß und schwarz dar. Rechts ist der Knoten $t_1$ dargestellt, der alle Beobachtungen beinhaltet..](images/class-rf-01.png){#fig-class-rf-01 fig-align="center" width="100%"}

In @fig-class-rf-02

![Darstellung des ersten Splits anhand des Prädiktors $X_1$. Wir wählen den Wert $c_1$ für den Split so, dass wir möglichst reine Knoten produzieren. Wir erhalten zwei neue Knoten $t_2$ und $t_3$. Der Knoten $t_3$ ist maximal rein und wird daher zu einem Terminalknoten.](images/class-rf-02.png){#fig-class-rf-02 fig-align="center" width="100%"}

In @fig-class-rf-03

![Darstellung des zweiten Splits anhand des Prädiktors $X_2$. Wir wählen wiederum den Wert $c_2$ für den Split so, dass wir möglichst reine Knoten erhalten. So erhalten wir zwei neue Knoten $t_4$ und $t_5$. Da nun $t_4$ ebenfalls ein reiner Knoten ist, wird der Knoten $t_4$ ebenfalls zu einem Terminalknoten. Wir stoppen hier das Wachstum, da mir eine mindest Anzahl von vier Beobachtungen in den Knoten erreicht haben.](images/class-rf-03.png){#fig-class-rf-03 fig-align="center" width="100%"}

In @fig-class-rf-04

![Darstellung der Vorhersage einer neuen Beobachtung mit Werten für die Prädiktoren $X_1$ und $X_2$. Unsere neue Beobachtung `?` fällt in den Terminalknoten $t_5$. Dort zählen wir die schwarzen Kreise. Wir stellen fest, dass die neue Beobachtung mit 25% Wahrscheinlichkeit ein Fall und damit schwarz ist. Daher ist die neue Beobachtung weiß.](images/class-rf-04.png){#fig-class-rf-04 fig-align="center" width="100%"}

```{r}
rpart_mod <- decision_tree(tree_depth = 5, min_n = 10, cost_complexity = 0.001) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
```

```{r}
rpart_wflow <- workflow() %>% 
  add_model(rpart_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
rpart_fit <- rpart_wflow %>% 
  parsnip::fit(gummi_train_data)
```

```{r}
rpart_aug <- augment(rpart_fit, gummi_test_data ) 
```

```{r}
rpart_cm <- rpart_aug %>% 
  conf_mat(gender, .pred_class)

rpart_cm
```

```{r}
rpart_cm %>% summary()
```

```{r}
rpart_aug %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

Es gibt viele Möglichkeiten sich einen Entscheidungsbaum anzuschauen. Wir nutzen hier das R Paket `rpart.plot` und die gleichnamige Funktion `rpart.plot()`. Die vielen Möglichkeiten der Darstellung und der Optionen findest in der Vignette [Plotting rpart trees with the rpart.plot package.](http://127.0.0.1:52037/help/library/rpart.plot/doc/prp.pdf). Wir gehen hier einmal auf die Variante `extra = 101` ein. Es gibt insgesamt elf verschiedene Arten plus eben noch die Möglichkeit 100 zu einer der elf genannten Varianten hinzufügen, um auch den Prozentsatz der Beobachtungen im Knoten anzuzeigen. Zum Beispiel zeigt `extra = 101` die Anzahl und den Prozentsatz der Beobachtungen in dem Knoten an.

```{r}
rpart_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE, extra = 101)
```

## Random Forest {#sec-rf}

Wir wählen zufällig eine Anzahl an Zeilen und Spalten aus.

-   Zufällige Auswahl der Beobachtungen mit Zurücklegen. Wir haben also einzelne Zeilen der Daten und damit Beobachtungen mehrfach in den Daten.
-   Zufällige Auswahl eines Sets an Variablen. Wir nutzen nicht immer alle Variablen in unserem Modell sondern nur ein Set an Spalten.

Das Wort *Bagging* steht für *bootstrap aggregating* und ist eine Methode, um Vorhersagen aus verschiedenen Modellen zu kombinieren. Dabei müssen alle Modelle mit dem gleichen Algorithmus laufen, können aber auf verschiedenen Datensätzen oder aber Variablensätzen zugreifen. Häufig haben die Modelle eine hohee Varianz in der Vorhersage und wir nutzen dann Bagging um die Modelle miteinader zu kombinieren und dadurch die Varianz zu verringern. Die Ergebnisse der Modelle werden dann im einfachsten Fall gemittelt. Das Ergebnis jeder Modellvorhersage geht mit gleichem Gewicht in die Vorhersage ein. Wir haben auch noch andere Möglichkeiten, aber du kannst dir Vorstellen wir rechnen verschiedene Modelle $k$-mal und bilden dann ein finales Modell in dem wir alle $k$-Modelle zusammenfassen. Wie wir die Zusammenfassung rechnen, ist dann immer wieder von Fall zu Fall unterschiedlich. Wir erhalten am Ende einen *Ensemble* Klassifizierer, da ja ein Ensemble von Modellen zusammengefasst wird.

::: {.callout-caution collapse="true"}
## Parallele CPU Nutzung

```{r}
cores <- parallel::detectCores()
cores
```

Und dann in `set_engine("ranger", num.threads = cores)`
:::

```{r}
ranger_mod <- rand_forest(mtry = 5, min_n = 10, trees = 1000) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")
```

```{r}
ranger_wflow <- workflow() %>% 
  add_model(ranger_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
ranger_fit <- ranger_wflow %>% 
  parsnip::fit(gummi_train_data)

```

```{r}
ranger_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20) +
  theme_minimal()
```

```{r}
ranger_aug <- augment(ranger_fit, gummi_test_data ) 
```

```{r}
ranger_cm <- ranger_aug %>% 
  conf_mat(gender, .pred_class)

ranger_cm
```

```{r}
ranger_cm %>% summary()
```

```{r}
ranger_aug %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

::: callout-note
## Kann ich auch eine Kreuzvalidierung und Tuning für Random Forest durchführen?

Ja, kannst du. Wenn du *nur* eine Kreuzvalidierung durchführen willst, findest du alles im @sec-knn für den $k$-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf den Random Forest Algorithmus anwenden. Wir nutzen gleich die Kreuzvalidierung in Kombination mit dem Tuning vom xgboost Algorithmus.

Wenn du also den Random Forest Algorithmus auch tunen willst, dann schaue einfach weiter unten nochmal bei dem Tuning des xgboost Algorithmus rein. Es ändert sich nur kaum was für die [Tuning Parameter vom Random Forest Algorithmus](https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html).
:::

## xgboost {#sec-xgboost}

https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5

https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30

https://howtolearnmachinelearning.com/articles/boosting-in-machine-learning/

![Darstellung von adaptive Boosting](images/class-xgboost.png){#fig-class-adaboost fig-align="center" width="100%"}

![Darstellung von adaptive Boosting](images/class-xgboost-2.png){#fig-class-gradientboost fig-align="center" width="100%"}

```{r}
xgboost_mod <- boost_tree(mtry = 5, min_n = 10, trees = 1000) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```

```{r}
xgboost_wflow <- workflow() %>% 
  add_model(xgboost_mod) %>% 
  add_recipe(gummi_rec)
```

```{r}
xgboost_fit <- xgboost_wflow %>% 
  parsnip::fit(gummi_train_data)
```

```{r}
xgboost_aug <- augment(xgboost_fit, gummi_test_data ) 
```

```{r}
xgboost_cm <- xgboost_aug %>% 
  conf_mat(gender, .pred_class)

xgboost_cm
```

```{r}
xgboost_cm %>% summary()
```

```{r}
xgboost_aug %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

::: callout-note
## Kann ich auch eine Kreuzvalidierung für xgboost durchführen?

Ja, kannst du. Wenn du *nur* eine Kreuzvalidierung durchführen willst, findest du alles im @sec-knn für den $k$-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf den xgboost Algorithmus anwenden. Wir nutzen gleich die Kreuzvalidierung in Kombination mit dem Tuning vom xgboost Algorithmus.
:::

## Tuning

Was heißt Tuning? Wie bei einem Auto können wir an verschiedenen Stellschrauben bei einem mathematischen Algorithmus schrauben. Welche Schrauben und Teile das sind, hängt dann wieder vom Algorithmus ab. Im Falle des xgboost Algorithmus können wir an folgenden Parametern drehen und jeweils schauen, was dann mit unserer Vorhersage passiert. Insgesamt hat der [xgboost Algorithmus acht Tuningparameter](https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html), wir wählen jetzt für uns hier drei aus. Ich nehme hier auch nur drei Parameter, da sich dann drei Parameter noch sehr gut visuell darstellen lassen. In der Anwendung wäre dann natürlich besser alle Parameter zu tunen, aber das dauert dann auch lange.

-   `mtry`, zufällig ausgewählte Anzahl an Variablen für jeden Baum. Das heißt, für jeden Baum werden von unseren Variablen die Anzahl `mtry` zufällig ausgewählt und auf diesem kleineren Datensatz der Baum erstellt.
-   `min_n`, kleinste Knotengröße, die noch akzeptiert wird. Wenn ein Knoten unter `min_n` fällt, dann endet hier das Wachstum des Baumes.
-   `trees`, Anzahl der Bäume die in einem xgboost Algorithmus erstellt werden.

Nun ist es so, dass wir natürlich nicht händisch alle möglichen Kombinationen von der Anzahl der ausgewählten Variablen pro Baum, der kleinsten Knotengröße und der Anzahl der Bäume berechnen wollen. Das sind ziemlich viele Kombinationen und wir kommen dann vermutlich schnell durcheinander. Deshalb gibt es die Funktion `tune()` aus dem R Paket `tune`, die uns einen Prozess anbietet, das Tuning automatisiert durchzuführen.

Als erstes müssen wir uns ein Objekt bauen, das aussieht wie ein ganz normales Modell in der Klassifikation. Aber wir ergänzen jetzt noch hinter jeder zu tunenden Option noch die Funktion `tune()`. Das sind die Parameter des Algorithmus, die wir später tunen wollen.

```{r}
tune_spec <-  boost_tree(mtry = tune(), 
                         min_n = tune(), 
                         trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

tune_spec
```

Jetzt bauen wir uns den Workflow indem wir statt unserem Modell, die Tuninganweisung in den Workflow reinnehmen. Echt simpel und straightforward. Das Rezept bleibt ja das Gleiche.

```{r}
gummi_tune_wflow <- workflow() %>% 
  add_model(tune_spec) %>% 
  add_recipe(gummi_rec)
```

Jetzt müssen wir noch alle Kombinationen aus den drei Parametern `mtry`, `min_n` und `trees` ermitteln. Das macht die Funktion `grid_regular()`. Es gibt da noch andere Funktionen in dem R Paket `tune`, aber ich konzentriere mich hier auf die einfachste. Jetzt müssen wir noch die Anzahl an Kombinationen festlegen. Ich möchte für jeden Parameter fünf Werte tunen. Daher nutze ich hier die Option `levels = 5` auch damit hier die Ausführung nicht so lange läuft. Fange am besten mit `levels = 5` an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert. Dann kannst du die Levels noch hochschrauben. Beachte aber, dass mehr Level nur mehr *Zwischenschritte* bedeutet. Jede Option hat eine Spannweite `range`, die du dann anpassen musst, wenn du *höhere* Werte haben willst. Mehr Level würden nur mehr Zwischenschritte bedeuten. In unserem Fall weiß zum Beispiel die Funktion `mtry()` nicht, wie viele Variablen in dem Datensatz sind. Wir müssen also die `range` für die Anzahl an ausgewählten Variablen selber setzen. Ich wähle daher eine Variable bis vier Variablen.

```{r}
gummi_grid <- grid_regular(mtry(range = c(1, 4)),
                           trees(),
                           min_n(),
                           levels = 5)
```

Das Tuning nur auf dem Trainingsdatensatz durchzuführen ist nicht so eine gute Idee. Deshalb nutzen wir hier auch die Kreuzvalidierung. Eigentlich ist eine 10-fache Kreuzvalidierung mit $v=10$ besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur $v=5$ gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.

```{r}
gummi_folds <- vfold_cv(gummi_train_data, v = 5)
```

Nun bringen wir den Workflow zusammen mit dem Tuninggrid und unseren Sets der Kreuzvaidierung. Daher pipen wir den Workflow in die Funktion `tune_grid()`. Als Optionen brauchen wir die Kreuzvaldierungsdatensätze und das Tuninggrid. Wenn du `control_grid(verbose = TRUE)` wählst, dann erhälst du eine Ausgabe wie weit das Tuning gerade ist. **Achtung!**, das Tuning dauert seine Zeit. Im Falle des xgboost Algorithmus dauert das Tuning zwar nicht so lange, aber immer noch ein paar Minuten. Wenn du dann alle acht Parameter des xgboost Algorithmustunen wollen würdest, dann würde die Berechnung sehr viel länger dauern. Du kannst das Ergebnis des simpleren Tunings auch in der Datei `gummi_xgboost_tune_res.rds` finden.

```{r}
#| eval: false
gummi_tune_res <- gummi_tune_wflow %>% 
   tune_grid(resamples = gummi_folds,
             grid = gummi_grid,
             control = control_grid(verbose = FALSE))
```

```{r}
#| eval: false
#| echo: false

## write_rds(gummi_tune_res, "data/gummi_xboost_tune_res.rds")
```

Damit du nicht das Tuning durchlaufen lassen musst, habe ich das Tuning in die Datei `gummi_xgboost_tune_res.rds` abgespeichert und du kannst dann über die Funktion `read_rds()` wieder einlesen. Dann kannst du den R Code hier wieder weiter ausführen.

```{r}
#| echo: false

gummi_tune_res <- read_rds("data/gummi_xboost_tune_res.rds")
```

Nachdem das Tuning durchgelaufen ist, können wir uns über die Funktion `collect_metrics()`, die Ergebnisse des Tunings für jede Kombination der drei Parameter `mtry`, `min_n` und `trees` wiedergeben lassen. Diese Ausgabe ist super unübersichtlich. Ich habe mich ja am Anfange des Abschnitts auch für drei Tuningparameter entschieden, da sich dann diese drei Parameter noch gut visualisieren lassen. Deshalb einmal die Abbildung der mittleren Accuarcy und der mittleren AUC-Werte über alle Kreuzvalidierungen.

```{r}
gummi_tune_res %>%
  collect_metrics() %>%
  mutate(trees = as_factor(trees),
         min_n = as_factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n, linetype = trees)) +
  theme_minimal() +
  geom_line(alpha = 0.6) +
  geom_point() +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_okabeito()
```

Damit wir nicht händisch uns die beste Kombination raussuchen müssen, können wir die Funktion `show_best()` nutzen. Wir wählen hier die beste Accuarcy und erhalten dann die sortierten Ergebnisse nach der Accuarcy des Tunings.

```{r}
gummi_tune_res %>%
  show_best("accuracy")
```

Das war die Funktion `show_best()` aber wir können uns auch die gleich die besten Parameter nach der Accuracy raus ziehen. Das Rausziehen der besten Parameter macht für uns die Funktion `select_best()`.

```{r}
best_xgboost <- gummi_tune_res %>%
  select_best("accuracy")

best_xgboost
```

Wir sehen, dass wir `mtry = 3` wählen sollten. Dann müssen wir als Anzahl der Bäume `trees = 1000` nutzen. Die minimale Anzahl an Beobachtungen pro Knoten ist dann `11`. Müssen wir jetzt die Zahlen wieder in ein Modell eingeben? Nein, müssen wir nicht. Mit der Funktion `finalize_workflow()` können wir dann die besten Parameter aus unserem Tuning gleich mit dem Workflow kombinieren. Dann haben wir unseren finalen, getunten Workflow. Du siehst dann auch in der Ausgabe, dass die neuen Parameter in dem xgboost Algorithmus übernommen wurden

```{r}
final_gummi_wf <- gummi_tune_wflow %>% 
  finalize_workflow(best_xgboost)

final_gummi_wf 
```

Jetzt bleibt uns nur noch der letzte Fit übrig. Wir wollen unseren finalen, getunten Workflow auf die Testdaten anwenden. Dafür gibt es dann auch die passende Funktion. Das macht für uns die Funktion `last_fit()`, die sich dann die Informationen für die Trainings- und Testdaten aus unserem Datensplit von ganz am Anfang extrahiert.

```{r}
final_fit <- final_gummi_wf %>%
  last_fit(gummi_data_split) 
```

Da wir immer noch eine Kreuzvaldierung rechnen, müssen wir dann natürlich wieder alle Informationen über alle Kreuzvaldierungsdatensätze einsammeln. Dann erhalten wir unsere beiden Gütekriterien für die Klassifikation des Geschlechts unser Studierenden nach dem xgboost Algorithmus. Die Zahlen sind schon gut für echte Daten. Eine Accuracy von 84% bedeutet das wir über acht von zehn Studierenden richtig klassifizieren. Die AUC ist auch schon fast hervorragend, wir bringen kaum Label durcheinander.

```{r}
final_fit %>%
  collect_metrics()
```

Dann bleibt uns nur noch die ROC Kurve zu visualisieren. Da wir wieder etwas faul sind, nutzen wir die Funktion `autoplot()`. Als Alternative geht natürlich auch das [R Paket `pROC`](https://web.expasy.org/pROC/screenshots.html), was eine Menge mehr Funktionen und Möglichkeiten bietet.

```{r}
final_fit %>%
  collect_predictions() %>% 
  roc_curve(gender, .pred_w, event_level = "second") %>% 
  autoplot()
```

Eine gute ROC Kurve würde senkrecht nach oben gehen und dann waagrecht nach rechts. Dann hätten wir eine AUC von 1 und eine perfekte Separation der beiden Label durch unseren Algorithmus. Unser Algorithmus würde jedem weiblichen Studierenden in dem Testdatensatz korrekt dem Geschlecht `w` zuweisen. Da wir eine ROC Kurve hier vorliegen haben, die sehr weit weg von der Diagonalen ist, haben wir sehr viele richtig vorhergesagte Studierende in unseren Testdaten. Unser Modell funktioniert um das Geschlecht von Studierenden anhand unserer Gummibärchendaten vorherzusageb.
