[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skript Bio Data Science",
    "section": "",
    "text": "Auf den folgenden Seiten wirst du eine Menge über Statistik oder Data Science lernen. Du musst dafür nicht eine meiner Veranstaltungen besuchen. Gerne kannst du hier und dort einmal schauen, ob etwas für dich dabei ist. Das Skript wird fortlaufend von mir ergänzt. Neben dem Skript gibt es auch noch die erklärenden YouTube Videos. Ich freue mich, dass du Lust hast hier etwas zu lernen… oder aber du musst – da bald eine Klausur ansteht. Wie auch immer – schau dich einfach mal um. Im Anhang findest du auch einen kleinen Leitfaden für das Schreiben einer Abschlussarbeit. Vielleicht hilft dir die Anleitung ja beim Schreiben.\n\n\n\n\n\n\nGesammelte Klausurfragen Bio Data Science\n\n\n\n\n\nDu findest die gesammelten Klausurfragen auf GitHub oder auf ILIAS in dem entsprechenden Modul. Die Klausurfragen zu den einzelnen Vorlesungen in einem Modul werden in den entsprechenden Übungen des Moduls besprochen. Bitte komme in die Übungen.\nDu brauchst dir die Fragen nicht alle auszudrucken. Wir besprechen die Fragen teilweise in den Übungen.\nDie finale Version für die Klausur veröffentliche ich Ende Dezember für das Wintersemester bzw. Ende Juni für das Sommersemester.\n\n\n\n\n\nDu liest hier gerade das Skript für meine Vorlesungen an der Hochschule Osnabrück an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Wie immer Leben kannst du auf verschiedene Arten und Weisen den Stoff, den ich vermitteln will, lernen. Daher gibt es noch zwei andere Möglichkeiten. Zum einen Lernen auf YouTube, mit meinen Lernvideos oder du schaust dir das Material auf GitHub an. Auf GitHub habe ich auch Informationen, die du vielleicht brauchen kannst. Ebenso findest du im Kapitel 2 noch andere Literaturempfehlungen.\n\n\n\n\n\n\n\nWenn du möchtest kannst du auf YouTube unter https://www.youtube.com/c/JochenKruppa noch einige Lehrvideos als Ergänzung schauen. In den Videos wiederhole ich Inhalte und du kannst auf Pause drücken um nochmal Programmierschritte nachverfolgen zu können.\n\n\n\n\n\n\n\n\nAlle Materialien von mir findest du immer auf GitHub unter https://github.com/jkruppa/teaching. Selbst wenn du nicht mehr in einem meiner Kurse bist, kannst du so auf die Lehrinhalte immer nochmal zugreifen und die aktuellen Versionen haben. Auf GitHub liegt auch immer eine semesteraktuelle Version der gesammelten Klausurfragen für meine Module.\n\n\n\n\nWie erreichst du mich? Am einfachsten über die gute, alte E-Mail. Bitte bachte, dass gerade kurz vor den Prüfungen ich mehr E-Mails kriege. Leider kann es dann einen Tick dauern.\n\n\n\n\n\nEinfach an j.kruppa@hs-osnabrueck.de schreiben. Du findest hier auch eine kurze Formulierungshilfe. Einfach auf den Ausklapppfeil klicken.\nBitte gib immer in deiner E-Mail dein Modul - was du belegst - mit an. Pro Semester unterrichte ich immer drei sehr ähnlich klingende Module. Daher schau nochmal hier in der Liste, wenn du unsicher bist.\n\n\n\n\n\n\nE-Mailvorlage mit beispielhafter Anrede\n\n\n\n\n\nHallo Herr Kruppa,\n… ich belege gerade Ihr Modul Modulname und hätte eine Bitte/Frage/Anregung…\n… ich benötige Hilfe bei der Planung/Auswertung meiner Bachelorarbeit…\nMit freundlichen Grüßen\nM. Muster"
  },
  {
    "objectID": "organisation.html",
    "href": "organisation.html",
    "title": "\n1  Organisation\n",
    "section": "",
    "text": "Version vom October 12, 2022 um 18:19:08\nDen Teil kannst du hier überspringen, wenn es dich nicht so richtig interessiert, was ich alles an Vorlesungen an der Hochschule Osnabrück anbiete. Wenn es dir um statistische Inhalte geht, dann gehe einfach weiter in die nächsten Kapitel. In diesem Kapitel geht es nochmal Orientierung über meine Vorlesungen zu geben, wenn dich noch mehr als nur der Pflichtkurs interessiert."
  },
  {
    "objectID": "organisation.html#statistische-beratung",
    "href": "organisation.html#statistische-beratung",
    "title": "\n1  Organisation\n",
    "section": "\n1.1 Statistische Beratung",
    "text": "1.1 Statistische Beratung\nNeben der klassischen Vorlesung biete ich auch Termine für die statistische Beratung von Abschlussarbeiten sowie Projekten an. Dieses Angebot gilt es für alle Mitglieder der Hochschule Osnabrück. Primär für Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL), aber ntürlich auch für alle anderen Fakultäten. Dafür musst du mir einfach nur eine E-Mail schreiben und dann erhälst du einen Termin innerhalb der nächsten zwei Wochen.\nDie Beratung ist grundsätzlich anonym und vertraulich. Wenn du willst kannst du gerne noch dein:e Betreuer:in mitbringen. Das ist aber keine Voraussetzung oder Notwendigkeit. Meistens finden mehrere Besprechungen statt, wir versuchen aber natürlich zusammen zügig dein Problem zu lösen. Ziel ist der Beratung ist es dich in die Lage zu versetzen selbstständig deine Analyse zu rechnen."
  },
  {
    "objectID": "organisation.html#sec-r-tutorium",
    "href": "organisation.html#sec-r-tutorium",
    "title": "\n1  Organisation\n",
    "section": "\n1.2 R Tutorium",
    "text": "1.2 R Tutorium\nZusätzlich zu der statistischen Beratung bieten wir auch ein R Tutorium für alle Mitglieder der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) an. Theoretisch können auch hier andere Mitglieder der anderen Faklutäten vorbeischauen, der Ort ist aber aktuell ein Raum auf dem Gelände in Haste. Die aktuellen Termine findest du in Tabelle 1.1.\nIm R Tutorium besprechen wir aktuelle Themen der Teilnehmer:innen. Meist sind dies aktuelle Fragen zu den Bachelorarbeiten. Auch wenn du kein dringendes Problem hast, kannst du gerne kommen und dir die Fragestellungen anhören. Meistens ist es auch interessant mal die Fragestellungen der anderen Studierenden sich anzuhören oder aber schon mal zu Üben ein anderes Experiment zu verstehen.\nBitte beachte folgende Hinweise zu den Terminen.\n\n\nHier findest du den Lage- und Gebäudeplan vom Standort Haste.\n\n\n\n\n\n\nHinweise zu dem R Tutorium\n\n\n\nDas R Tutorium findet nicht im Prüfungszeitraum der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) statt.\nDas R Tutorium findet nicht im Februar und März statt.\nDas R Tutorium findet nicht im August und September statt.\n\n\n\n\n\nTabelle 1.1— Aktuelle Termine des R Tutoriums im Semester. Hier findest du den Lage- und Gebäudeplan vom Standort Haste.\n\nTermin\nUhrzeit\nRaum\nAnmerkung\n\n\n\nDienstag, den 11. Oktober 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 18. Oktober 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 25. Oktober 2022\n9:45 - 11:15\nHM 0115\nBlockwoche, ohne Kruppa\n\n\n\nDienstag, den 01. November 2022\n9:45 - 11:15\nHM 0115\nohne Kruppa\n\n\nDienstag, den 08. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 15. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 22. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 29. November 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 06. Dezember 2022\n9:45 - 11:15\nHM 0115\n\n\n\nDienstag, den 13. Dezember 2022\n9:45 - 11:15\nHM 0115\n\n\n\nOnlinetermine ab jetzt…\n\n\n\n\n\nDienstag, den 20. Dezember 2022\n9:45 - 11:15\n[online]\nohne Kruppa\n\n\nWeihnachtswoche\n\n\n\n\n\nDienstag, den 03. Januar 2023\n9:45 - 11:15\n[online]\n\n\n\nDienstag, den 10. Januar 2023\n9:45 - 11:15\n[online]\n\n\n\nDienstag, den 17. Januar 2023\n9:45 - 11:15\n[online]\nOptionaler Termin\n\n\nDienstag, den 23. Januar 2023\n9:45 - 11:15\n[online]\nOptionaler Termin"
  },
  {
    "objectID": "organisation.html#sec-vorlesungen-hs",
    "href": "organisation.html#sec-vorlesungen-hs",
    "title": "\n1  Organisation\n",
    "section": "\n1.3 Vorlesungen an der Hochschule Osnabrück",
    "text": "1.3 Vorlesungen an der Hochschule Osnabrück\nVon mir angebotene Vorlesungen werden an der Hochschule Osnabrück an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) in ILIAS verwaltet. Alle notwendigen Informationen und Materialien sind auf ILIAS unter https://lms.hs-osnabrueck.de/ zu finden. Wenn du in dem Kurs nicht angemeldet bist, dann kontaktiere mich bitte per Mail. Auch die Kommunikation erfolgt von meiner Seite aus über ILIAS.\nAuf ILIAS findest du alle aktuellen Kursinformationen und erhälst auch die Mails, wenn Änderungen im Kursablauf stattfinden.\nWenn du nicht in der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) studierst oder aber in einem Studiengang, der meine Module nicht anbietet, steht es dir natürlich frei, sich in meine Vorlesungen zu setzten. Du findest in Tabelle 1.2 eine Übersicht der angebotenen Module und auch die inhaltliche Ordnung nach Lernstufe. Bitte informiere dich in deinem Studierendensekretariat über die Modalitäten zur Prüfungsteilnahme.\nEine inhaltliche Übersicht findet sich auf dem Google Spreadsheet. Die Planung ist aktuell (Stand Herbst/Winter 2022) noch nicht abgeschlossen. Im Zweifel einfach bei mir einmal per Mail anfragen.\n\n\n\nTabelle 1.2— Angebotene Statistik Module an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Die Stufe gibt das Lernniveau an.\n\nStufe\nLandwirtschaft; Angewandte Pflanzenbiologie – Gartenbau, Pflanzentechnologie\nWirtschafts- ingenieurwesen Agrar / Lebensmittel\nBioverfahrenstechnik in Agrar- und Lebensmittelwirtschaft\nAngewandte Nutztier- und Pflanzenwissenschaften\n\n\n\n1\nMathematik und Statistik\nStatistik\nAngewandte Statistik für Bioverfahrenstechnik\n\n\n\n2\nAngewandte Statistik und Versuchswesen\nAngewandte Statistik und Versuchswesen\n\n\n\n\n3\nSpezielle Statistik und Versuchswesen\n\n\nBiostatistik"
  },
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "2  Literatur",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:36:34\nWas ist gute Literatur? Immer schwer zu beurteilen. Im Folgenden liste ich einige Literaturquellen auf. Zum einen basiert eine Menge von dem R Code auf Wickham (2016) zum Anderen möchtest du dich vielleicht nochmal rechts oder links weiter bilden. Du musst aber nicht um die Klausur bestehen zu können. Siehe es eher als ein Angebot.\nNeben diesem Modul musst du vermutlich noch andere Module belegen. Deshalb hier eine Auswahl Literatur, die dir helfen mag. Zum einen ist die Literatur anders geschrieben und zum anderen sind dort andere Inhalte."
  },
  {
    "objectID": "literature.html#parametrische-statistik",
    "href": "literature.html#parametrische-statistik",
    "title": "2  Literatur",
    "section": "\n2.1 Parametrische Statistik",
    "text": "2.1 Parametrische Statistik\n\n\n\n\nDormann (2013) liefert ein tolles deutsches Buch für die Vertiefung in die Statistik. Insbesondere wenn du wissenschaftlich Arbeiten willst weit über die Bachelorarbeit hinaus. Dormann baut in seinem Buch eine hervorragende Grundlage auf. Das Buch ist an der Hochschule Osnabrück kostenlos über den Link zu erhalten."
  },
  {
    "objectID": "literature.html#experimental-methods-in-agriculture",
    "href": "literature.html#experimental-methods-in-agriculture",
    "title": "2  Literatur",
    "section": "\n2.2 Experimental methods in agriculture",
    "text": "2.2 Experimental methods in agriculture\n\n\n\n\nOnofri und Sacco (2021) haben das Buch Experimental methods in agriculture geschrieben. Wir werden auf dieses englische Buch ab und zu mal verweisen. Insbesondere der Einleitungstext zur Wissenschaft und dem Design von Experiementen ist immer wieder lesenswert. Spätere Teile des Buches sind etwas mathematischer und nicht für den Einstieg unbedingt geeignet. Aber schaue es dir selber an."
  },
  {
    "objectID": "literature.html#r-for-data-science",
    "href": "literature.html#r-for-data-science",
    "title": "2  Literatur",
    "section": "\n2.3 R for Data Science",
    "text": "2.3 R for Data Science\n\n\n\n\nWickham (2016) ist die Grundlage für die R Programmierung. Das Material von Wickahm findet sich kostenlos online unter https://r4ds.had.co.nz/ und https://www.tidyverse.org/. Wir werden uns hauptsächlich mit R wie es Wickham lehrt beschäftigen. Somit ist Wickham unsere Grundlage für R."
  },
  {
    "objectID": "literature.html#practical-statistics-for-data-scientists",
    "href": "literature.html#practical-statistics-for-data-scientists",
    "title": "2  Literatur",
    "section": "\n2.4 Practical Statistics for Data Scientists",
    "text": "2.4 Practical Statistics for Data Scientists\n\n\n\n\nBruce (2020) schreibt ein Buch für den Anwender. Ohne Vorkenntnisse ist das Buch vermutlich etwas schwer zu lesen. Dafür bietet das Buch aber nach einem Statistikkurs sehr gute Anknüpfungspunkte Richtung maschinelles Lernen und somit der Klassifikation. Das Buch ist auch hier in der englischen Version und hier in der deutschen Version zu erhalten. Beide Links benötigen den Zugang über die Hochschule Osnabrück."
  },
  {
    "objectID": "literature.html#data-science-for-agriculture-in-r",
    "href": "literature.html#data-science-for-agriculture-in-r",
    "title": "2  Literatur",
    "section": "\n2.5 Data Science for Agriculture in R",
    "text": "2.5 Data Science for Agriculture in R\n\n\n\n\nSchmidt liefert auf der Webseite https://schmidtpaul.github.io/DSFAIR/index.html eine tolle Sammlung an experimentellen Designs bzw. Versuchsanlagen samt der Auswertung in R. Ohne Vorkenntnisse schwer zu verstehen. Sollte aber nach einem Kurs Statistik dann möglich sein. Gerne hier auch mich fragen, dann können wir gemeinsam das passende Design raussuchen und besprechen."
  },
  {
    "objectID": "literature.html#odds-ends",
    "href": "literature.html#odds-ends",
    "title": "2  Literatur",
    "section": "\n2.6 Odds & Ends",
    "text": "2.6 Odds & Ends\n\n\n\n\nAm Ende dann noch eine Mathebuch von Weisberg zu finden unter https://jonathanweisberg.org/vip/. Eigentlich eher ein Buch über Wahrscheinlichkeiten und wenn ein Buch am Ende stehen muss, dann ist es dieses Buch. Ich finde es sehr spannend zu lesen, aber das ist dann vermutlich special intrest."
  },
  {
    "objectID": "literature.html#referenzen",
    "href": "literature.html#referenzen",
    "title": "2  Literatur",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nBruce, Peter, Andrew Bruce, und Peter Gedeck. 2020. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media.\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer.\n\n\nWickham, Hadley, und Garrett Grolemund. 2016. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "3  Einführung",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:36:35\nIn diesem Kapitel nenne ich die wichtigsten Lernziele, die nach dem Lesen des Skriptes im Rahmen deiner Lehrveranstaltung von dir erreicht worden sein sollten. Je nach besuchten Kurs kann natürlich nicht alles geschafft worden sein. Viele Kapitel haben noch eine Abschnitt in dem du mehr über die Klausur erfährst. So sehe diese Übersicht als Einführung für das was später an Lehrinhalten kommt. Wenn du die Lernziele hier verstehst, dann hast du eine gute und solide Grundlage in Statistik und Bio Data Science. Damit solltest du dann auch gut durch deine Bachelorarbeit kommen."
  },
  {
    "objectID": "preface.html#ein-wort-der-warnung",
    "href": "preface.html#ein-wort-der-warnung",
    "title": "3  Einführung",
    "section": "Ein Wort der Warnung…",
    "text": "Ein Wort der Warnung…\nWenn du dieses Bild eines niedergeschlagenen Engels der Statistik siehst…\n\n\n\n\n… dann bedeutet der niedergeschlagene Engel der Statistik:\n\nWir opfern Genauigkeit für Anwendbarkeit. Ja, manchmal ist es eben statstisch nicht richtig was hier steht, aber aus Gründen der Anwendung fahren wir mal über den Engel drüber. Schade.\nWir sind hier Anfänger und Anwender. Später kannst du noch tiefer ins Detail gehen. Hier wollen wir die Grundlagen lernen. Das hat dann einen Preis an Richtigkeit.\nWir wollen fertig werden. Durch geschicktes Manövrieren können wir an einen Punkt kommen, wo kein statistischer Test mehr passt. Das wollen wir nicht. Deshalb zahlen wir hier auch einen Preis. Passt aber.\n\nDeshalb konzentrieren wir uns auf einige wichtige Lernziele, die wir jetzt einmal nacheinander durchgehen."
  },
  {
    "objectID": "preface.html#lernziel-1-eine-explorative-datananalyse-durchführen",
    "href": "preface.html#lernziel-1-eine-explorative-datananalyse-durchführen",
    "title": "3  Einführung",
    "section": "\n3.1 Lernziel 1: Eine explorative Datananalyse durchführen",
    "text": "3.1 Lernziel 1: Eine explorative Datananalyse durchführen\nGleich zu Beginn R Code zu zeigen und eine entsprechende Abbildung ist vielleicht ungewöhnlich, aber wir wollen zu dieser Abbildung 3.1 hin. In Abbildung 3.1 siehst du einen Boxplot. Und wie wir aus den Daten flea_dog_cat.xlsx einen Boxplot erstellen, das soll uns in den nächsten Kapitel beschäftigen. Dafür müssen wir nämlich eine Menge in dem Codeblock verstehen und dann auch Anwenden können. Und natürlich lernen was eigentlich ein Boxplot ist und was in einem Boxplot eigentlich dargestellt ist.\n\n\n\n\nAbbildung 3.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nHier ist der Codeblock der in R die Abbildung 3.1 erstellt.\n\n## Einlesen von Daten aus Excel\ndata_tbl <- read_excel(\"data/flea_dog_cat.xlsx\")\n\n## Umformen der <chr> Spalte in einen Factor <fct>\ndata_tbl <- data_tbl %>% \n  mutate(animal = as_factor(animal))\n\n## Auswählen der wichtigen Spalten für den Boxplot\ndata_tbl <- data_tbl %>% \n  select(animal, jump_length) \n\n## Generieren des Boxplots in ggplot()\nggplot(data_tbl, aes(x = animal, y = jump_length, \n                     fill = animal)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, shape = 4, size = 4) +\n  labs(x = \"Tierart\", y = \"Sprungweite in [cm]\", \n       fill = \"Tierart\") +\n  scale_x_discrete(labels = c(\"Hund\", \"Katze\")) +\n  theme_bw()\n\nWir müssen nun folgende Dinge lernen um den Codeblock zu verstehen:\n\nWir müssen das Datenbeispiel verstehen. Was sind das eigentlich für Daten, die wir da abbilden? Was sind überhaupt Daten im Sinne der Statistik bzw. für R.\nWir müssen den R Code verstehen. Von einzelnen wichtigen Operatoren wie -> und %\\>% zu dem den Unterschieden von Worten und Objekten.\nWie kriegen wir Daten aus Excel in R hinein? Wir können die Daten ja nicht einfach in R eintragen sondern haben die Daten ja meist in einer (Excel) Datei wie flea_dog_cat.xlsx.\nWas ist eigentlich ein Boxplot und welche statistischen Maßzahlen werden hier eigentlich abgebildet?\nWie funktioniert eigentlich die Funktionalen ggplot() mit der wir den Boxplot erstellt haben?\n\nAll diese Fragen und weitere Fragen, die sich diesen Fragen anschließen, wollen wir uns in den nächsten Kapitel anschauen. Leider kann ich hier nur linear schreiben. Deshalb musst du eventuell mal ein Kapitel wiederholen oder etwas quer lesen. Du kannst dir ja auch nicht immer alles auf einmal merken."
  },
  {
    "objectID": "preface.html#lernziel-2-rstudio-und-r",
    "href": "preface.html#lernziel-2-rstudio-und-r",
    "title": "3  Einführung",
    "section": "\n3.2 Lernziel 2: RStudio und R",
    "text": "3.2 Lernziel 2: RStudio und R\n\n\n\n\n\n\nWas ist eigentlich RStudio und woher kriege ich das?\n\n\n\nDu findest auf YouTube Einführung in R - Teil 01 - Installation von RStudio und R als Video. Ich gehe in dem Video einmal alle wichtigen Schritte durch und so kannst du dir Rstudio und R installieren.\n\n\nUm Data Science durchführen zu können musst du etwas Programmieren können. Wir programmieren in R und nutzen die Software um Abbildungen zu erstellen und Analysen zu rechnen.\nWir arbeiten in R und nutzen dafür das RStudio. Führe einfach folgende Schritte aus um erst R zu installieren und dann das RStudio.\n\nR installieren unter https://cran.rstudio.com/\n\nRStudio installieren unter https://www.rstudio.com/products/rstudio/download/#download\n\n\nBitte die Reihenfolge beachten. Beide Schritte kannst du dir auch nochmals im Video anschauen oder aber du kommst in das R Tutorium was regelmäßig an der Hochschule Osnabrück von mir angeboten wird. Die Termine findest du im Kapitel 1.2."
  },
  {
    "objectID": "preface.html#lernziel-3-falsifikationsprinzip",
    "href": "preface.html#lernziel-3-falsifikationsprinzip",
    "title": "3  Einführung",
    "section": "\n3.3 Lernziel 3: Falsifikationsprinzip",
    "text": "3.3 Lernziel 3: Falsifikationsprinzip\n\n\n\n\n\n\nGrundlagen der Wissenschaft und Falsifikationsprinzip\n\n\n\nDu findest auf YouTube Grundlagen der Wissenschaft und Falsifikationsprinzip als Video Reihe.\n\n\nWie funktioniert ein statistischer Versuch? Ich könnte auch wissenschaftliches Experiment schreiben, aber ein wissenschaftliches Experiment ist sehr abstrakt. Wir wollen ja einen Versuch durchführen und danach - ja was eigentlich? Was wollen wir nach dem Versuch haben? Meistens eine neue Erkenntnis. Um diese Erkenntnis zu validieren oder aber abzusichern nutzen wir Statistik. Dazu musst du noch wissen, dass wir eine spezielle Form der Statistik nutzen: die frequentistische Statistik.\nEine biologische Wiederholung beinhaltet ein neues Tier, Pflanze oder Mensch. Eine technische Wiederholung ist die gleiche Messung an dem gleichen Tier, Pflanze oder Mensch.\nWir nennen das Outcome auch Endpunkt, Response oder kurz \\(y\\).\nDie frequentistische Statistik basiert - wie der Name andeutet - auf Wiederholungen in einem Versuch. Daher der Name frequentistisch. Also eine Frequenz von Beobachtungen. Ist ein wenig gewollt, aber daran gewöhnen wir uns schon mal. Konkret, ein Experiment welches wir frequentistisch Auswerten wollen besteht immer aus biologischen Wiederholungen. Wir müssen also ein Experiment planen in dem wir wiederholt ein Outcome an vielen Tieren, Pflanzen oder Menschen messen. Auf das Outcome gehen wir noch später ein. Im Weiteren konzentrieren wir uns hier auf die parametrische Statistik. Die parametrische Statistik beschäftigt sich mit Parametern von Verteilungen.\n\n\n\n\n\n\nWie gehen wir nun vor, wenn wir ein Experiment durchführen wollen?\n\n\n\n\nWir müssen auf jeden Fall wiederholt ein Outcome an verschiedenen Tieren, Pflanzen oder Menschen messen.\nWir überlegen uns aus welcher Verteilungsfamilie unser Outcome stammt, damit wir dann die entsprechende Verfahren zur Analyse nehmen können.\n\n\n\nWenn wir nun ein Experiment durchführen dann erheben wir einmalig Daten \\(D_1\\). Wir könnten das Experiment wiederholen und erneut Daten \\(D_2\\) erheben. Wir können das Experiment \\(j\\)-mal wiederholen und haben dann Daten von \\(D_1,..., D_j\\). Dennoch werden wir nie alle Daten erheben können, die mit einem Experiment verbunden sind.\nStrukturgleichkeit erreichen wir durch Randomisierung.\nIn Tabelle 3.1 sehen wir einmal mögliche Quellen für die Verwirrung und die Möglichkeiten des experimentellen Design etwas gegen diese Quellen der Verwirrung zu unternehmen. Wir können hier Quelle der Verwirrung auch als Quelle der Varianz deuten. Eine detailierte Diskussion findet sich in Dormann (2013) und Hurlbert (1984). Wichtig ist hier mitzunehmen, dass wir häufig eine Kontrolle brauchen um überhaupt die Stärke des Effektes messen zu können. Sonst können wir die Frage, ob die Behandlung besser ist nicht quantifizieren.\n\n\nTabelle 3.1— In Dormann (2013) und Hurlbert (1984) finden wir eine Zusammenfassung von Quellen der Verwirrung also eigentlich der Varianz und deren mögliche Lösung um die Varianz zu beherrschen.\n\n\n\n\n\n\n\nQuelle der Verwirrung\nMerkmal des experimentellen Designs um die Verwirrung zu reduziert oder aufzulösen\n\n\n\n1.\nZeitliche Veränderung\nKontrollgruppe\n\n\n2.\nArtefakte in der Behandlung\nKontrollgruppe\n\n\n3.\nVoreingenommenheit des Forschenden (Bias)\nRandomisierte Zuordnung der Versuchseinheiten zu den Behandlungen; generelle Randomisierung bei allen möglichen Prozessen; Verblindete Prozeduren\n\n\n4.\nVom Forschenden induzierte Variabilität (zufälliger Fehler)\nWiederholungen der Behandlungen (und Kontrolle)\n\n\n5.\nAnfängliche oder beinhaltende Variabilität zwischen den Versuchseinheiten\nWiederholungen der Behandlungen (und Kontrolle); Durchmischen der Behandlungen; Begleitbeobachtungen (Positive Kontrolle)\n\n\n6.\nNicht-dämonische Einflüsse\nWiederholung und Durchmischung der Behandlungen (und Kontrolle)\n\n\n7.\nDämonische Eingriffe\nEwige Wachsamkeit - siehe dazu auch Feynman (1998); Geisteraustreibung; Menschenopfer\n\n\n\n\nNehmen wir das Beispiel, dass wir die Sprungweite von Hunde- und Katzenflöhen vergleichen wollen. Wir können nicht alle Hunde- und Katzenflöhe messen. Wir können nur eine Stichprobe an Daten \\(D_1\\) erheben. Über diese Daten \\(D_1\\) können wir dann später durch statistische Algorithmen eine Aussage treffen. Wichtig ist hier sich zu merken, dass wir eine Grundgesamtheit haben aus der wir eine Stichprobe ziehen. Wir müssen darauf achten, dass die Stichprobe repräsentativ ist und damit strukturgleich zur Grundgesamtheit ist. Die Strukturgleichkeit erreichen wir durch Randomisierung. Wir veranschaulichen diesen Zusammenhang in Abbildung 3.2. Ein Rückschluß von der Stichprobe ist nur möglich, wenn die Stichprobe die Grundgesamtheit repräsentiert. Auch eine Randomisierung mag dieses Ziel nicht immer erreichen. Im Beispiel der Hundeflöhe könnte wir eine Art an Flöhen übersehen und diese Flohart nicht mit in die Stichprobe aufnehmen. Ein Rückschluß auf diese Flohart wäre dann mit unserem Experiment nicht möglich.\n\n\nAbbildung 3.2— Abbildung über die Grundgesamtheit und die Stichprobe(n) \\(D_1\\) bis \\(D_j\\). Durch Randomisierung wird Sturkturgleichheit erreicht, die dann einen Rückschluß von der Stichprobe auf die Grundgesamtheit erlaubt. Jede Stichprobe ist anders und nicht jede Randomisierung ist erfolgreich was die Strukturgleicheit betrifft.\n\n\nTabelle 3.2 zeigt nochmal die Zusammenfassung von der Grundgesamtheit un der Stichprobe im Vergleich. Wichtig ist zu merken, dass wir mit unserem kleinen Experiment Daten \\(D\\) generieren mit denen wir einen Rückschluß und somit eine Verallgemeinerung erreichen wollen.\n\n\nTabelle 3.2— Vergleich von Grundgesamtheit und Stichprobe.\n\n\n\n\n\nGrundgesamtheit\nStichprobe\n\n\n\n… \\(n\\) ist riesig bis unfassbar.\n… \\(n_1\\) von \\(D_1\\) ist klein.\n\n\n… der Mittelwert wird mit \\(\\mu_y\\) beschrieben.\n… der Mittelwert wird mit \\(\\bar{y}\\) beschrieben.\n\n\n… die Varianz wird mit \\(\\sigma^2\\) beschrieben.\n… die Varianz wird mit \\(s^2\\) beschrieben.\n\n\n… die Standardabweichung wird mit \\(\\sigma\\) beschrieben.\n… die Standardabweichung wird mit \\(s\\) beschrieben."
  },
  {
    "objectID": "preface.html#referenzen",
    "href": "preface.html#referenzen",
    "title": "3  Einführung",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer.\n\n\nFeynman, Richard P. 1998. „Cargo cult science“. In The art and science of analog circuit design, 55–61. Elsevier.\n\n\nHurlbert, Stuart H. 1984. „Pseudoreplication and the design of ecological field experiments“. Ecological monographs 54 (2): 187–211."
  },
  {
    "objectID": "example-preface.html",
    "href": "example-preface.html",
    "title": "Datenbeispiele",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:36:41\nWir brauchen am Anfang erstmal ein simples Beispiel. Konkrete Zahlen mit denen wir arbeiten können und Grundlagen aufbauen können. Was liegt da näher als sich einmal am Kopf zu kratzen und zu fragen, was juckt den da? Genau! Flöhe. Wir schauen uns einmal Flöhe auf Hunden und Katzen an. Daran können wir viel über Zahlen und Buchstaben in der Statistik und dann im Programmieren lernen."
  },
  {
    "objectID": "example-preface.html#von-flöhen-und-hunden",
    "href": "example-preface.html#von-flöhen-und-hunden",
    "title": "Datenbeispiele",
    "section": "Von Flöhen und Hunden",
    "text": "Von Flöhen und Hunden\nIn unserem ersten Beispiel in Kapitel 4 geht es darum einmal ein Gefühl für Daten zu kriegen. Also was sind diese Zahlen und Buchstaben eigentlich? Wie sind Daten aufgebaut und wie musst du Daten bauen, so dass wir auch mit den Daten arbeiten können? Wir schauen uns dafür einmal Flöhe auf Hunden an und fragen uns welche Typen von Zahlen können wir erheben?"
  },
  {
    "objectID": "example-preface.html#von-flöhen-hunden-und-katzen",
    "href": "example-preface.html#von-flöhen-hunden-und-katzen",
    "title": "Datenbeispiele",
    "section": "Von Flöhen, Hunden und Katzen",
    "text": "Von Flöhen, Hunden und Katzen\nIn unserem zweiten Beispiel in Kapitel 5 erweitern wir unserer erstes Beispiel um die Katzen. Das heist, dass eigentlich alles gleich bleibt. Wir schauen usn zusätlich noch als zweite Gruppe die Katzen an. Nun können wir die Frage stellen, unterscheiden sich Flöhe auf Hunden und Katzen gegeben von gemessenen Eigenschaften?"
  },
  {
    "objectID": "example-preface.html#von-flöhen-auf-tieren",
    "href": "example-preface.html#von-flöhen-auf-tieren",
    "title": "Datenbeispiele",
    "section": "Von Flöhen auf Tieren",
    "text": "Von Flöhen auf Tieren\nIn unserem dritten Beispiel in Kapitel 6 erweitern wir das Beispiel um den Fuchs mit einem weiteren Tier. Dadurch haben wir nicht mehr einen Faktor mit zwei Leveln vorliegen sondern einen mit drei Leveln. Die Fragestrellung erweitert sich jetzt auf einen multiplen Gruppenvergleich. Wir vergleichen nicht mehr nur noch zwei Gruppen miteinander sondern drei."
  },
  {
    "objectID": "example-preface.html#von-flöhen-auf-tieren-in-habitaten",
    "href": "example-preface.html#von-flöhen-auf-tieren-in-habitaten",
    "title": "Datenbeispiele",
    "section": "Von Flöhen auf Tieren in Habitaten",
    "text": "Von Flöhen auf Tieren in Habitaten\nIn unserem vierten Beispiel in Kapitel 7 schauen wir uns zusätzlich zu dem dritten Beispiel noch verschiedene Habitate (eng. site) an. Wir haben nämlich die Hunde-, Katzen-, und Fuchsflöhe nicht nur an einem Ort sondern an verschiedenen Orten gesammelt und gemessen. Wir haben einen zweiten Faktor vorliegen."
  },
  {
    "objectID": "example-preface.html#von-vielen-flöhen-auf-hunden-und-katzen",
    "href": "example-preface.html#von-vielen-flöhen-auf-hunden-und-katzen",
    "title": "Datenbeispiele",
    "section": "Von vielen Flöhen auf Hunden und Katzen",
    "text": "Von vielen Flöhen auf Hunden und Katzen\nIm fünften Beispiel in Kapitel 8 schauen wir uns wiederum nur noch zwei Tierarten an: Hunde und Katzen. Dafür aber eine große Anzahl an Tieren. Wir schauen uns hier die Daten von 400 Tiere an. Auf diesen Tierarten messen wir mehrere Variablen unn wollen uns diese Daten später in der Regression anschauen."
  },
  {
    "objectID": "example-preface.html#gummibärchen",
    "href": "example-preface.html#gummibärchen",
    "title": "Datenbeispiele",
    "section": "Gummibärchen",
    "text": "Gummibärchen\nIm Beispiel mit den Gummibärchen in Kapitel 9 geht es um die Darstellung verschiedener Verteilungen. Wir brauchen den Datensatz um zu verstehen, wie Daten verteilt sind. Sonst können wir den Datensatz auch gut nutzen um einmal in R zu filtern und zu selektieren. Auch für die Erstellung von Abbilungen eignet sich der Datensatz sehr gut."
  },
  {
    "objectID": "example-fleas-dogs.html",
    "href": "example-fleas-dogs.html",
    "title": "4  Von Flöhen und Hunden",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:36:44\n\n\n\nIn unserem ersten Beispiel wollen wir uns verschiedene Daten \\(D\\) von Hunden und Hundeflöhen anschauen. Unter anderem sind dies die Sprungweite, die Anzahl an Flöhen, die Boniturnoten auf einer Hundemesse sowie der Infektionsstatus. Hier nochmal detailiert, was wir uns im Folgenden immer wieder anschauen wollen.\n\nSprungweite in [cm] von verschiedenen Flöhen \\[\nY_{jump} = \\{5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6\\}.\n\\]\nAnzahl an Flöhen auf verschiedenen Hunden \\[\n  Y_{count} = \\{18, 22, 17, 12, 23, 18, 21\\}.\n  \\]\nBoniturnoten [1 = schlechteste bis 9 = beste Note] von verschiedenen Hunden \\[\n  Y_{grade} = \\{8, 8, 6, 8, 7, 7, 9\\}.\n  \\]\nInfektionstatus [0 = gesund, 1 = infiziert] mit Flöhen von verschiedenen Hunden \\[\n  Y_{infected} = \\{0, 1, 1, 0, 1, 0, 0\\}.\n  \\]\n\nJe nachdem was wir messen, nimmt \\(Y\\) andere Zahlenräume an. Wir sagen, \\(Y\\) folgt einer Verteilung. Die Sprungweite ist normalverteilt, die Anzahl an Flöhen folgt einer Poisson Verteilung, die Boniturnoten sind multinominal/ordinal bzw. kategorial verteilt. Der Infektionsstatus ist binomial verteilt. Wir werden uns später die Verteilungen anschauen und visualisieren. Das können wir hier aber noch nicht. Wichtig ist, dass du schon mal gehört hast, dass \\(Y\\) unterschiedlich verteilt ist, je nachdem welche Dinge wir messen.\nTabelle 4.1 zeigt dir die Darstellung der Daten von oben in einer einzigen Tabelle. Bitte beachte, dass genau eine Zeile für eine Beobachutng, in diesem Fall einem Hund, vorgesehen ist.\n\n\n\n\nTabelle 4.1— Sprunglängen [cm] für Hundeflöhe. Die Tabelle ist im Long-Format dargestellt.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\n\n\n\n\n\n\n\n\n\n\nDatei für von Flöhen und Hunden\n\n\n\nDu findest die Datei flea_dog.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-fleas-dogs-cats.html",
    "href": "example-fleas-dogs-cats.html",
    "title": "5  Von Flöhen, Hunden und Katzen",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:36:47\n\n\n\nWir wollen jetzt das Beispiel von den Hunden und Flöhen um eine Spezies erweitern. Wir nehmen noch die Katzen mit dazu und fragen uns, wie sieht es mit der Sprungfähigkeit von Katzen und Hundeflöhen aus? Konzentrieren wir uns hier einmal auf die Sprungweite. Wir können wie in dem vorherigen Beispiel mit den Hundeflöhen die Sprungweiten [cm] der Katzenflöhe wieder in der gleichen Weise aufschreiben:\n\\[\nY_{jump} = \\{3.2, 2.2, 5.4, 4.1, 4.3, 7.9, 6.1\\}.\n\\]\nWenn wir jetzt die Sprungweiten der Hundeflöhe mit den Katzenflöhen vergleichen wollen haben wir ein Problem. Beide Zahlenvektoren heißen gleich, nämlich \\(Y_{jump}\\). Wir könnten jeweils in die Indizes noch \\(dog\\) und \\(cat\\) schreiben als \\(Y_{jump,\\, dog}\\) und \\(Y_{jump,\\, cat}\\) und erhalten folgende Vektoren.\n\\[\nY_{jump,\\, dog} = \\{5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6\\}\n\\]\n\\[\nY_{jump,\\, cat} = \\{3.2, 2.2, 5.4, 4.1, 4.3, 7.9, 6.1\\}\n\\]\nDadurch werden die Indizes immer länger und unübersichtlicher. Auch das \\(Y\\) einfach \\(Y_{dog}\\) oder \\(Y_{cat}\\) zu nennen ist keine Lösung - wir wollen uns vielleicht später nicht nur die Sprungweite vergleichen, sondern vielleicht auch die Anzahl an Flöhen oder den Infektionsstatus. Dann ständen wir wieder vor dem Problem die \\(Y\\) für die verschiedenen Outcomes zu unterscheiden. Daher erstellen wir uns die Tabelle 5.1. Wir haben jetzte eine Datentabelle.\n\n\n\n\nTabelle 5.1— Sprunglängen [cm] für Hunde- und Katzenflöhe. Die Tabelle ist im Wide-Format dargestellt.\n\ndog\ncat\n\n\n\n5.7\n3.2\n\n\n8.9\n2.2\n\n\n11.8\n5.4\n\n\n8.2\n4.1\n\n\n5.6\n4.3\n\n\n9.1\n7.9\n\n\n7.6\n6.1\n\n\n\n\n\n\nIntuitiv ist die Tabelle 5.1 übersichtlich und beinhaltet die Informationen die wir wollten. Dennoch haben wir das Probem, das wir in dieser Tabelle 5.1 nicht noch weitere Outcomes angeben können. Wir können die Anzahl an Flöhen auf den Hunde und Katzen nicht darstellen. Als Lösung ändern wir die Tabelle 5.1 in das Long-Format. Dargestellt in Tabelle 5.2. Jede Beobachtung belegt nun eine Zeile. Dies ist sehr wichtig im Kopf zu behalten, wenn du eigene Daten in z.B. Excel einstellst.\n\n\n\n\nTabelle 5.2— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Boniturnote sowie der Infektionsstatus von Hunde- und Katzenflöhe. Die Tabelle ist im Long-Format dargestellt.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\ncat\n3.2\n12\n7\n1\n\n\ncat\n2.2\n13\n5\n0\n\n\ncat\n5.4\n11\n7\n0\n\n\ncat\n4.1\n12\n6\n0\n\n\ncat\n4.3\n16\n6\n1\n\n\ncat\n7.9\n9\n6\n0\n\n\ncat\n6.1\n7\n5\n0\n\n\n\n\n\n\n\n\n\n\n\n\nDatei für von Flöhen, Hunden und Katzen\n\n\n\nDu findest die Datei flea_dog_cat.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-fleas-dogs-cats-foxes.html",
    "href": "example-fleas-dogs-cats-foxes.html",
    "title": "6  Von Flöhen auf Tieren",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:36:50\n\n\n\nWir wollen jetzt das Beispiel von den Hunde- und Katzenflöhen um eine weitere Spezies erweitern. Warum machen wir das? Später wollen wir uns anschauen, wie sich verschiedene Gruppen oder Behandlungen voneinander unterscheiden. Wir brauchen alos mehr Spezies. Wir nehmen noch die Füchse mit dazu und fragen uns, wie sieht es mit der Sprungfähigkeit von Hunde-, Katzen- und Fuchsflöhen aus?\n\n\n\n\nTabelle 6.1— Sprunglängen [cm] für Hunde-, Katzen- und Fuchsflöhe.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\ncat\n3.2\n12\n7\n1\n\n\ncat\n2.2\n13\n5\n0\n\n\ncat\n5.4\n11\n7\n0\n\n\ncat\n4.1\n12\n6\n0\n\n\ncat\n4.3\n16\n6\n1\n\n\ncat\n7.9\n9\n6\n0\n\n\ncat\n6.1\n7\n5\n0\n\n\nfox\n7.7\n21\n5\n1\n\n\nfox\n8.1\n25\n4\n1\n\n\nfox\n9.1\n31\n4\n1\n\n\nfox\n9.7\n12\n5\n1\n\n\nfox\n10.6\n28\n4\n0\n\n\nfox\n8.6\n18\n4\n1\n\n\nfox\n10.3\n19\n3\n0\n\n\n\n\n\n\nDer Datensatz in Tabelle 6.1 beginnt schon recht groß zu werden. Deshalb brauchen wir auch R als Werkzeug um große Datensätze auswerten zu können.\n\n\n\n\n\n\nDatei für von Flöhen auf Tieren\n\n\n\nDu findest die Datei flea_dog_cat_fox.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-fleas-dogs-cats-foxes-site.html",
    "href": "example-fleas-dogs-cats-foxes-site.html",
    "title": "7  Von Flöhen auf Tieren in Habitaten",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:36:53\nWir schauen uns in diesem Beispiel wiederum drei Tierarten an: Hunde, Katzen und Füchse. Auf diesen Tierarten messen wir die Sprunglänge von jeweils zehn Tieren. Im Vergleich zu dem vorherigen Beispiel erweitern wir die Daten um eine Spalte site in der wir vier verschiedene Messorte protokollieren. Es ergibt sich folgende Tabelle 7.1 und die dazugehörige Abbildung 7.1.\n\n\n\n\n\n\n\nTabelle 7.1— Sprunglängen [cm] für Hunde-, Katzen- und Fuchsflöhe in verschiedenen Habitaten.\n\nanimal\nsite\nrep\njump_length\n\n\n\ncat\ncity\n1\n12.04\n\n\ncat\ncity\n2\n11.98\n\n\ncat\ncity\n3\n16.10\n\n\ncat\ncity\n4\n13.42\n\n\ncat\ncity\n5\n12.37\n\n\ncat\ncity\n6\n16.36\n\n\ncat\ncity\n7\n14.91\n\n\ncat\ncity\n8\n11.17\n\n\ncat\ncity\n9\n12.38\n\n\ncat\ncity\n10\n15.06\n\n\ncat\nsmalltown\n1\n15.24\n\n\ncat\nsmalltown\n2\n13.36\n\n\ncat\nsmalltown\n3\n15.08\n\n\ncat\nsmalltown\n4\n12.83\n\n\ncat\nsmalltown\n5\n14.68\n\n\ncat\nsmalltown\n6\n10.73\n\n\ncat\nsmalltown\n7\n13.35\n\n\ncat\nsmalltown\n8\n14.54\n\n\ncat\nsmalltown\n9\n12.99\n\n\ncat\nsmalltown\n10\n14.51\n\n\ncat\nvillage\n1\n17.59\n\n\ncat\nvillage\n2\n11.24\n\n\ncat\nvillage\n3\n12.44\n\n\ncat\nvillage\n4\n13.63\n\n\ncat\nvillage\n5\n14.92\n\n\ncat\nvillage\n6\n17.43\n\n\ncat\nvillage\n7\n18.30\n\n\ncat\nvillage\n8\n16.35\n\n\ncat\nvillage\n9\n16.34\n\n\ncat\nvillage\n10\n14.23\n\n\ncat\nfield\n1\n13.70\n\n\ncat\nfield\n2\n15.13\n\n\ncat\nfield\n3\n17.99\n\n\ncat\nfield\n4\n14.60\n\n\ncat\nfield\n5\n16.16\n\n\ncat\nfield\n6\n14.26\n\n\ncat\nfield\n7\n15.39\n\n\ncat\nfield\n8\n16.85\n\n\ncat\nfield\n9\n19.02\n\n\ncat\nfield\n10\n18.76\n\n\ndog\ncity\n1\n19.35\n\n\ndog\ncity\n2\n17.10\n\n\ndog\ncity\n3\n19.85\n\n\ndog\ncity\n4\n15.33\n\n\ndog\ncity\n5\n15.15\n\n\ndog\ncity\n6\n19.57\n\n\ndog\ncity\n7\n15.44\n\n\ndog\ncity\n8\n16.09\n\n\ndog\ncity\n9\n15.91\n\n\ndog\ncity\n10\n13.01\n\n\ndog\nsmalltown\n1\n17.72\n\n\ndog\nsmalltown\n2\n17.11\n\n\ndog\nsmalltown\n3\n17.57\n\n\ndog\nsmalltown\n4\n17.12\n\n\ndog\nsmalltown\n5\n16.02\n\n\ndog\nsmalltown\n6\n22.61\n\n\ndog\nsmalltown\n7\n16.49\n\n\ndog\nsmalltown\n8\n18.64\n\n\ndog\nsmalltown\n9\n17.21\n\n\ndog\nsmalltown\n10\n19.90\n\n\ndog\nvillage\n1\n16.60\n\n\ndog\nvillage\n2\n15.28\n\n\ndog\nvillage\n3\n16.91\n\n\ndog\nvillage\n4\n15.08\n\n\ndog\nvillage\n5\n18.56\n\n\ndog\nvillage\n6\n16.34\n\n\ndog\nvillage\n7\n17.61\n\n\ndog\nvillage\n8\n14.80\n\n\ndog\nvillage\n9\n17.52\n\n\ndog\nvillage\n10\n16.93\n\n\ndog\nfield\n1\n15.78\n\n\ndog\nfield\n2\n17.02\n\n\ndog\nfield\n3\n15.41\n\n\ndog\nfield\n4\n15.61\n\n\ndog\nfield\n5\n19.87\n\n\ndog\nfield\n6\n19.24\n\n\ndog\nfield\n7\n17.65\n\n\ndog\nfield\n8\n18.83\n\n\ndog\nfield\n9\n17.60\n\n\ndog\nfield\n10\n14.67\n\n\nfox\ncity\n1\n19.50\n\n\nfox\ncity\n2\n18.49\n\n\nfox\ncity\n3\n19.78\n\n\nfox\ncity\n4\n19.45\n\n\nfox\ncity\n5\n21.56\n\n\nfox\ncity\n6\n21.37\n\n\nfox\ncity\n7\n18.64\n\n\nfox\ncity\n8\n20.08\n\n\nfox\ncity\n9\n21.62\n\n\nfox\ncity\n10\n20.68\n\n\nfox\nsmalltown\n1\n19.81\n\n\nfox\nsmalltown\n2\n17.78\n\n\nfox\nsmalltown\n3\n19.65\n\n\nfox\nsmalltown\n4\n16.38\n\n\nfox\nsmalltown\n5\n17.46\n\n\nfox\nsmalltown\n6\n17.02\n\n\nfox\nsmalltown\n7\n19.38\n\n\nfox\nsmalltown\n8\n15.89\n\n\nfox\nsmalltown\n9\n17.15\n\n\nfox\nsmalltown\n10\n17.43\n\n\nfox\nvillage\n1\n15.32\n\n\nfox\nvillage\n2\n17.59\n\n\nfox\nvillage\n3\n15.70\n\n\nfox\nvillage\n4\n18.58\n\n\nfox\nvillage\n5\n16.85\n\n\nfox\nvillage\n6\n18.25\n\n\nfox\nvillage\n7\n18.75\n\n\nfox\nvillage\n8\n16.96\n\n\nfox\nvillage\n9\n13.38\n\n\nfox\nvillage\n10\n18.38\n\n\nfox\nfield\n1\n16.85\n\n\nfox\nfield\n2\n13.55\n\n\nfox\nfield\n3\n13.89\n\n\nfox\nfield\n4\n15.67\n\n\nfox\nfield\n5\n16.38\n\n\nfox\nfield\n6\n14.59\n\n\nfox\nfield\n7\n14.03\n\n\nfox\nfield\n8\n13.63\n\n\nfox\nfield\n9\n14.09\n\n\nfox\nfield\n10\n15.52\n\n\n\n\n\n\nÜber die explorative Datenanalyse erfährst du mehr im Kapitel 15\nDie Datentabelle ist in dieser Form schon fast nicht mehr überschaubar. Daher hilft hier die explorative Datenanalyse weiter. Wir schauen uns daher die Daten einmal als einen Boxplot in Abbildung 7.1 an. Wir sehen hier, dass wir drei Tierarten an vier Orten die Sprungweite in [cm] gemessen haben.\n\n\n\n\nAbbildung 7.1— Boxplot der Sprungweiten [cm] für Hunde-, Katzen- und Fuchsflöhe in verschiedenen Habitaten.\n\n\n\n\n\n\n\n\n\n\nDatei für von Flöhen auf Tieren in Habitaten\n\n\n\nDu findest die Datei flea_dog_cat_fox_site.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-fleas-dogs-cats-length-weight.html",
    "href": "example-fleas-dogs-cats-length-weight.html",
    "title": "8  Von vielen Flöhen auf Hunden und Katzen",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:36:57\nWir schauen uns in diesem Beispiel wiederum nur zwei Tierarten an: Hunde und Katzen. Auf diesen Tierarten messen wir wieder die Sprunglänge in [cm] von jeweils 400 Tieren. Im Vergleich zu dem vorherigen Beispiel erweitern wir die Daten um eine Spalte jump_weight in [mg] sowie sex [male, female]. Bei Versuch wurde noch in der Variable hatch_time gemessen, wie lange die Flöhe in Stunden zum Schlümpfen brauchen. Es ergibt sich folgende Tabelle 8.1 mit den ersten zehn Beobachtungen und die dazugehörige Abbildung 8.1.\n\n\n\n\n\n\n\nTabelle 8.1— Sprunglängen [cm], Gewichte [mg], Geschecht [sex] und Schlüpfzeit [h] für Hunde- und Katzenflöhe.\n\nanimal\nsex\nweight\njump_length\nflea_count\nhatch_time\n\n\n\ncat\nmale\n6.02\n15.79\n5\n483.60\n\n\ncat\nmale\n5.99\n18.33\n1\n82.56\n\n\ncat\nmale\n8.05\n17.58\n1\n296.73\n\n\ncat\nmale\n6.71\n14.09\n3\n140.90\n\n\ncat\nmale\n6.19\n18.22\n1\n162.20\n\n\ncat\nmale\n8.18\n13.49\n1\n167.47\n\n\ncat\nmale\n7.46\n16.28\n1\n291.20\n\n\ncat\nmale\n5.58\n14.54\n0\n112.58\n\n\ncat\nmale\n6.19\n16.36\n1\n143.97\n\n\ncat\nmale\n7.53\n15.08\n1\n766.31\n\n\n\n\n\n\nÜber die explorative Datenanalyse erfährst du mehr im Kapitel 15\nDie Datentabelle ist in dieser Form schon fast nicht mehr überschaubar. Daher hilft hier die explorative Datenanalyse weiter. Wir schauen uns daher die Daten einmal als einen Scatterplot in Abbildung 8.1 an. Wir sehen hier, dass wir das mit dem Gewicht [mg] der Flöhe auch die Sprungweite in [cm] steigt.\n\n\n\n\nAbbildung 8.1— Scatterplot der Sprunglängen [cm] und Gewichte [mg] für Hunde- und Katzenflöhe.\n\n\n\n\n\n\n\n\n\n\nDatei für von vielen Flöhen auf Hunden und Katzen\n\n\n\nDu findest die Datei flea_dog_cat_length_weight.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-gummi-bears.html",
    "href": "example-gummi-bears.html",
    "title": "9  Gummibärchen",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:36:59\nIm Folgenden sehen wir in Tabelle 9.1 den Gummibärchen Datensatz, der im Laufe der letzten Jahre entstanden ist. Dabei wächst der Datensatz von Semester zu Semester immer ein wenig weiter. Jedes Semester darf Tütchen aufreißen und schauen was da so drin ist. Wir sind jetzt aktuell bei \\(N = 428\\) Gummibärchentütchen mit insgesamt \\(4087\\) ausgezählen Gummibärchen.\nWir erheben folgende Variablen im Datensatz. Dabei unterscheiden wir einmal für Variablen, die technischer Natur sind. Wir schreiben NA für eng. not available, wenn ein Eintrag fehlt.\nDann wollen wir aber auch noch etwas über den Studierenden wissen, der die Tüte aufgemacht hat. Wir erheben hier noch einge demographische Informationen:\nAktuell hat der Datensatz \\(N = 428\\) Beobachtungen also auch Personen, die mitgemacht haben. Da der Datensatz aber immer weiter wächst brauchen wir wirklich R dazu um den Datensatz uns anschauen zu können."
  },
  {
    "objectID": "example-gummi-bears.html#referenzen",
    "href": "example-gummi-bears.html#referenzen",
    "title": "9  Gummibärchen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nKruppa, Jochen, und Björn Kiehne. 2019. „Statistik lebendig lehren durch Storytelling und forschungsbasiertes Lernen“. Beiträge zu Praxis, Praxisforschung und Forschung, 501.\n\n\nKruppa, Jochen, und Miriam Sieg. 2021. „Spielerisch Daten reinigen“. In Zeig mir Health Data Science!, 93–103. Springer."
  },
  {
    "objectID": "programing-preface.html",
    "href": "programing-preface.html",
    "title": "Programmieren in R",
    "section": "",
    "text": "Einführung in R per Video\n\n\n\nDu findest auf YouTube Grundlagen in R als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nUm eien Sprache zu lernen brauchst du Vokabeln und Grammatik. Beides lernt sich alleine immer recht sinnlos. Aber zusammen bringen kann man beides erst, wenn man beides gelernt hat.\nWas solltest du nun zuerst Lesen um Programmieren in R zu verstehen und zu lernen? Es ist sehr schwierig die Programmierung exakt so zu schreiben, dass das Programmieren linear verständlich ist. Du brauchst im Prinzip das Wissen aus Kapitel 11 Operatoren, Funktionen und Pakete um die Grundlagen von Operatoren und Funktionen in R zu verstehen.\nAuf der anderen Seite fehlt dir vielleicht noch das Verständnis von Buchstaben und Zahlen in R. Diesen Zusammenhang zwischen Buchstaben und Zahlen erkläre ich als erstes im folgenden Kapitel 10 Von Buchstaben und Zahlen. Vielleicht musst du beide Kapitel jeweils nochmal lesen. Oder aber in der Anwendung sehen. Stell dir vor es ist wie eine Sprache zu lernen. Ohne Vokabeln keine Sätze aber ohne Grammatik kein Sinn.\nBevor wir uns weiter mit statistischen Kennzahlen beschäftigen, wollen wir uns einmal die Realisierung einer Datentabelle mit den Hunde- und Katzenflöhen in R anschauen. Dabei wollen wir auch Eigenschaften von Zahlen und Buchstaben lernen, die notwendig sind um mit einem Programm wie R kommunizieren zu können. Wir wollen später R nutzen um die explorative Datenanalyse anzuwenden.\nFangen wir also an die Grammatik und die Vokabeln in R zu verstehen um dann mit dem Rechner kommunizieren zu können."
  },
  {
    "objectID": "programing-letters-numbers.html",
    "href": "programing-letters-numbers.html",
    "title": "10  Von Buchstaben und Zahlen",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:37:04\nIn den vorherigen Beispielen haben wir uns die Sprungweiten und andere Eigenschaften von Hunden und Katzen angeschaut. Bevor wir uns weiter mit statistischen Kennzahlen beschäftigen, wollen wir uns einmal die Realisierung des Beispiels in R anschauen. Das heist, wie ist eine Tabelle in R aufgebaut und was sehen wir da eigentlich?\nHier siehst du nochmal die Tabelle, wie wir die Tabelle in Micorsoft Word darstellen würden.\nWir wollen jetzt an der Datentabelle die Besonderheiten von Zahlen und Buchstaben in R verstehen. Dabei wollen wir insbesondere die Eigenschaften von Zahlen und Buchstaben lernen, die notwendig sind um mit einem Programm wie R kommunizieren zu können. Nun haben wir in der Tabelle 10.1 mit Daten zu verschiedenen Oucomes, wie Sprungweite [cm], Anzahl an Flöhen auf Hunden und Katzen, die Boniturnoten oder aber den Infektionsstatus. Die Tabelle ist zwar nicht groß aber auch nicht wirklich klein. Wir wollen uns nun damit beschäftigen, die Zahlen sinnvoll in R darzustellen. Wir wollen mit der Darstellung einer Datentabelle in R beginnen, einem tibble()."
  },
  {
    "objectID": "programing-letters-numbers.html#daten-in-r-sind-tibble",
    "href": "programing-letters-numbers.html#daten-in-r-sind-tibble",
    "title": "10  Von Buchstaben und Zahlen",
    "section": "\n10.1 Daten in R sind tibble()\n",
    "text": "10.1 Daten in R sind tibble()\n\nIm Folgenden sehen wir die Daten aus der Tabelle 10.1 in R als tibble dargestellt. Was ist nun ein tibble? Ein tibble ist zu aller erst ein Speicher für Daten in R. Das heist wir haben Spalten und Zeilen. Jede Spalte repräsentiert eine Messung oder Variable und die Zeilen jeweils eine Beobachtung.\n\n\n# A tibble: 14 × 5\n   animal jump_length flea_count grade infected\n   <chr>        <dbl>      <int> <dbl> <lgl>   \n 1 dog            5.7         18     8 FALSE   \n 2 dog            8.9         22     8 TRUE    \n 3 dog           11.8         17     6 TRUE    \n 4 dog            8.2         12     8 FALSE   \n 5 dog            5.6         23     7 TRUE    \n 6 dog            9.1         18     7 FALSE   \n 7 dog            7.6         21     9 FALSE   \n 8 cat            3.2         12     7 TRUE    \n 9 cat            2.2         13     5 FALSE   \n10 cat            5.4         11     7 FALSE   \n11 cat            4.1         12     6 FALSE   \n12 cat            4.3         16     6 TRUE    \n13 cat            7.9          9     6 FALSE   \n14 cat            6.1          7     5 FALSE   \n\n\nAls erstes erfahren wir, dass wir einen A tibble: 14 x 5 vorliegen haben. Das heist, wir haben 14 Zeile und 5 Spalten. In einem tibble wird immer in der ersten Zeile angezeigt wieviele Beobachtungen wir in dem Datensatz haben. Wenn das tibble zu groß wird, werden wir nicht mehr das ganze tibble sehen sondern nur noch einen Ausschnitt. Im Weiteren hat jede Spalte noch eine Eigenschaft unter dem Spaltennamen:\n\n\n<chr> bedeutet character. Wir haben also hier Worte vorliegen.\n\n<dbl> bedeutet double. Ein double ist eine Zahl mit Kommastellen.\n\n<int> bedeutet integer. Ein integer ist eine ganze Zahl ohne Kommastellen.\n\n<lgl> bedeutet logical oder boolean. Hier gibt es nur die Ausprägung wahr oder falsch. Somit TRUE oder FALSE. Statt den Worten TRUE oder FALSE kann hier auch 0 oder 1 stehen.\n\n<str> bedeutet string der aus verschiedenen character besteht kann, getrennt durch Leerzeichen."
  },
  {
    "objectID": "programing-letters-numbers.html#faktoren-als-wörter-zu-zahlen",
    "href": "programing-letters-numbers.html#faktoren-als-wörter-zu-zahlen",
    "title": "10  Von Buchstaben und Zahlen",
    "section": "\n10.2 Faktoren als Wörter zu Zahlen",
    "text": "10.2 Faktoren als Wörter zu Zahlen\nEin Faktor ist eine Variable mit mehrern Faktorstufen oder Leveln. Für uns sieht der Faktor wie ein Wort aus, hinter jedem Wort steht aber eine Zahl mit der gerechnet werden kann.\nEin Computer und somit auch eine Programmsprache wie R kann keine Buchstaben verrechnen. Ein Programm kann nur mit Zahlen rechnen. Wir haben aber in der Tabelle 10.1 in der Spalte animal Buchstaben stehen. Da wir hier einen Kompromiss eingehen müssen führen wir Faktoren ein. Ein Faktor kombiniert Buchstaben mit Zahlen. Wir als Anwender sehen die Buchstaben, die Wörter bilden. Intern steht aber jedes Wort für eine Zahl, so dass R mit den Zahlen rechnen kann. Klingt ein wenig kryptisch, aber wir schauen uns einen factor einmal an.\n\ndata_tbl$animal[1:8]\n\n[1] \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\"\n\n\nÜber das $ Symbol kannst du im Kapitel 11.7 mehr erfahren.\nWas haben wir gemacht? Als erstes haben wir die Spalte animal aus dem Datensatz data_tbl mit dem Dollarzeichen $ herausgezogen. Mit dem $ Zeichen können wir uns eine einzelne Spalte aus dem Datensatz data_tbl rausziehen. Du kannst dir das $ wie einen Kleiderbügel und das data_tbl als einen Schrank für Kleiderbügel verstellen. An dem Kleiderbügel hängen dann die einzelnen Zahlen und Worte. Wir nehmen aber nicht den ganzen Vektor sondern nur die Zahlen 1 bis 8, dargestellt durch [1:8]. Die Gänsefüße \" um dog zeigen uns, dass wir hier Wörter oder charactervorliegen haben. Schauen wir auf das Ergebnis, so erhalten wir sieben Mal dog und einmal cat. Insgesamt die ersten acht Einträge der Datentabelle. Wir wollen diesen Vektor uns nun einmal als Faktor anschauen. Wir nutzen die Funktion as_factor().\nÜber Funktionen kannst du im Kapitel 11.4 mehr erfahren.\n\nas.factor(data_tbl$animal[1:8])\n\n[1] dog dog dog dog dog dog dog cat\nLevels: cat dog\n\n\nIm direkten vergleich verschwinden die Gänsefüße \" um dog und zeigen uns, dass wir hier keine character mehr vorliegen haben. Darüber hinaus sehen wir auch, dass die der Faktor jetzt Levels hat. Exakt zwei Stück. Jeweils einen für dog und einen für cat. Wir werden später Faktoren benötigen, wenn wir zum Beispiel eine einfaktorielle ANOVA rechnen. Hier siehst du schon den Begriff Faktor wieder."
  },
  {
    "objectID": "programing-letters-numbers.html#von-wörtern-und-objekten",
    "href": "programing-letters-numbers.html#von-wörtern-und-objekten",
    "title": "10  Von Buchstaben und Zahlen",
    "section": "\n10.3 Von Wörtern und Objekten",
    "text": "10.3 Von Wörtern und Objekten\nDas mag etwas verwirrend sein, denn es gibt in R Wörter string <str> oder character <chr>. Wörter sind was anderes als Objekte. Streng genommen sind beides Wörter, aber in Objekten werden Dinge gespeichert wohin gegen das Wort einfach ein Wort ist. Deshalb kennezeichnen wir Wörter auch mit Gänsefüßchen als win \"wort\" und zeigen damit, dass es sich hier um einen String handelt.\nWir tippen \"animal\" in R und erhalten \"animal\" als Wort zurück. Das sehen wir auch an dem Ausdruck mit den Gänsefüßchen.\n\n\"animal\"\n\n[1] \"animal\"\n\n\nÜber den Zuweisungspfeil <- kannst du im Kapitel 11.5 mehr erfahren.\nWir tippen animal ohne die Anführungszeichen in R und erhalten den Inhalt von animal ausgegeben. Dafür müssen wir aber das Objekt animal erst einmal über den Zuweisungspfeil <-erschaffen.\n\nanimal <- c(\"dog\", \"cat\", \"fox\")\nanimal\n\n[1] \"dog\" \"cat\" \"fox\"\n\n\nSollte es das Objekt animal nicht geben, also nicht über den Zuweisungspfeil <- erschaffen worden, dann wird eine Fehlermeldung von R ausgegeben:\nFehler in eval(expr, envir, enclos) : Objekt 'animal' nicht gefunden"
  },
  {
    "objectID": "programing-letters-numbers.html#zusammenfassung",
    "href": "programing-letters-numbers.html#zusammenfassung",
    "title": "10  Von Buchstaben und Zahlen",
    "section": "\n10.4 Zusammenfassung",
    "text": "10.4 Zusammenfassung\nVariablennamen meint hier immer den Namen der Spalte im Datensatz bzw. tibble\nTabelle 10.2 zeigt eine Übersicht wie einzelne Variablennamen und deren zugehörigen Beispielen sowie den Namen in R, der Informatik allgemein, als Skalenniveau und welcher Verteilungsfamilie die Variable angehören würde. Leider ist es so, dass wieder gleiche Dinge unterschiedliche benannt werden. Aber an dieses doppelte Benennen können wir uns in der Statistik schonmal gewöhnen.\n\n\n\nTabelle 10.2— Zusammenfassung und Übersicht von Variablennamen und deren Bennung in R, in der Informatik allgemein, als Skalenniveau und die dazugehörige Verteilungsfamilie.\n\n\n\n\n\n\n\n\n\nVariablenname\nBeispiel\nR\nInfomatik\nSkalenniveau\nVerteilungsfamilie\n\n\n\nweight\n12.3, 12.4, 5.4, 21.3, 13.4\nnumeric\ndouble\ncontinuous\nGaussian\n\n\ncount\n5, 0, 12, 23, 1, 4, 21\ninteger\ninteger\ndiscrete\nPoisson\n\n\ndosis\nlow, mid, high\nordered\n\ncategorical / ordinal\nOrdinal\n\n\nfield\nmainz, berlin, kiel\nfactor\n\ncategorical\nMultinomial\n\n\ncancer\n0, 1\nfactor\n\ndichotomous / binary / nominal\nBinomial\n\n\ntreatment\n“placebo”, “aspirin”\ncharacter\ncharacter/string\ndichotomous / binary / nominal\nBinomial\n\n\nbirth\n2001-12-02, 2005-05-23\ndate"
  },
  {
    "objectID": "programing-basics.html",
    "href": "programing-basics.html",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:37:07\nEs ist immer schwierig, wann die Grundlagen von R einmal gelehrt werden sollte. Wenn du nichts von Programmierung bis jetzt gehört hast, dann mag es keinen Sinn ergeben mit Operatoren, wie dem Zuweisungspfeil <- und der Pipe %>% zu beginnen. Wir brauchen aber für die Programmierung folgende zentrale Konzepte.\nNicht alle Konzepte brauchst du unmittelbar aber ich nutze diese Konzepte wiederholt in allen Kapiteln, so dass du hier immer wieder mal schauen kannst, was die Grundlagen sind."
  },
  {
    "objectID": "programing-basics.html#sec-R-packages",
    "href": "programing-basics.html#sec-R-packages",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "\n11.1 Pakete und library()\n",
    "text": "11.1 Pakete und library()\n\n\n\n\n\n\n\nUnterschied von Packages und Libraries in R\n\n\n\nDu findest auf YouTube Einführung in R - Teil 03 - Unterschied Packages und Libraries in R als Video. Hier erkläre ich nochmal den Ablauf zwischen Installieren eines Paketes und dem Laden eines Paketes.\n\n\nAls Vanilla beschreibt man in der Informatikerwelt ein Programm, was keine zusätzlichen Pakete geladen hat. Also die reinst Form ohne zusätzlichen Geschmack.\nIn der Vanilla-Variante hat R sehr wenige Funktionen. Ohne zusätzliche Pakete ist R mehr ein sehr potenter Taschenrechner. Leider mit der Funktionalität aus den 90’zigern, was die Programmierumgebung und die Funktionen angeht. Das wollen wir aber nicht. Wir wollen auf den aktuellen Stand der Technik und auch Sprache programmieren. Daher nutzen wir zusätzliche R Pakete.\n\n\nAbbildung 11.1— Auf den Reiter Packages klicken und dann Install. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\nIn Abbildung 11.1 wird gezeigt wie du ein zusätzliches Paket installieren kannst. Hierbei ist nochmal wichtig den semantischen Unterschied zu wissen. Es gibt das Paket tidyverse was wir viel nutzen. Wir installieren einmalig Pakete der Funktion install.packages() oder eben wie in Abbildung 11.1 gezeigt. Wir nutzen die Funktion library() um ein Paket in R zu laden. Ja, es müsste anders heisen, tut es aber nicht.\n\n## Das Paket tidyverse installieren - einmalig\ninstall.packages(tidyverse)\n\n## Das Paket tidyverse laden - jedes Mal\nlibrary(tidyverse)\n\nNun muss man sich immer merken, ob das Paket schon installiert ist oder man schreibt relativ viele library() untereinander. Das passiert schnell, wenn du viele Pakete laden willst. Dafür erlaubt dir das Paket pacman eine Vereinfachung. Die Funktion p_load() installiert Pakete, wenn die Pakete nicht installiert sind. Sollten die Pakete installiert sein, so werden die Pakete geladen. Du musst nur einmal install.packages(pacman) ausführen um das Paket pacman zu installieren.\n\npacman::p_load(tidyverse, magrittr, readxl)"
  },
  {
    "objectID": "programing-basics.html#anordnung-der-fenster-im-rstudio",
    "href": "programing-basics.html#anordnung-der-fenster-im-rstudio",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "\n11.2 Anordnung der Fenster im RStudio",
    "text": "11.2 Anordnung der Fenster im RStudio\nWie dir sicherlich aufgefallen ist, sind in meinen Videos die einzelnen Kacheln im RStudio anders angeordnet. Der Grund ist einfach. Wir sind die meiste Zeit in dem Skript auf der linken Seite und schicken dann den R Code auf die rechte Seite. Normalerweise sind das Skript und die R Console links untereinander angeordnet. Das finde ich aber disfunktional. In Abbildung 11.2 und Abbildung 11.3 kannst du nachvollziehen, wie du die Anordnung der Kacheln im R Studio ändern kannst.\n\n\nAbbildung 11.2— Auf den Reiter Tools klicken und dann Global Options…. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\n\nAbbildung 11.3— Auf den Reiter Pane Layout klicken und dann die Kacheln so anordnen wie du sie hier siehst. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\n\nDu kannst vieles in den Global Options… anpassen - unter anderem auch das Aussehen (eng. Appearance)."
  },
  {
    "objectID": "programing-basics.html#sec-R-vector",
    "href": "programing-basics.html#sec-R-vector",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "\n11.3 Einen Vektor bauen c()\n",
    "text": "11.3 Einen Vektor bauen c()\n\nWir können mit der Funktion c() Zahlen und Wörter zu einem Vektor kombinieren.\n\nc(\"dog\", \"dog\", \"cat\", \"cat\", \"fox\", \"fox\")\n\n[1] \"dog\" \"dog\" \"cat\" \"cat\" \"fox\" \"fox\"\n\n\nHier werden die Wörter “dog”, “cat” und “fox” miteinader in einen Vektor kombiniert. Wir erinnern uns an das $ Zeichen, was uns erlaubt eine Variable als Vektor aus einem tibble()herauszuziehen.\nWir können auch Zahlen zusammenbauen oder aber ganze Bereiche mit dem : definieren. Wir lesen den : als “von bis”.\n\nc(1, 8, 4, 5)\n\n[1] 1 8 4 5\n\n\nDie Zahlen von 1 bis 5 werden durch den : ausgegeben.\n\nc(1:5)\n\n[1] 1 2 3 4 5"
  },
  {
    "objectID": "programing-basics.html#sec-R-function",
    "href": "programing-basics.html#sec-R-function",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "\n11.4 Funktionen",
    "text": "11.4 Funktionen\nWir haben schon einige Funktion nebenbei in R kennengelernt. Zum einen as.factor() um einen Faktor zu erstellen oder aus dem Kapitel 11.1, wo wir die Funktion install.packages() nutzen um ein Paket zu installieren oder aber die Funktion library() um ein Paket in R zu laden.\nFunktionen sehen aus wie Wörter. Haben aber keine Gänsefüßchen und beinhalten auch keine Daten oder Vektoren. Funktionen können mit Daten und Vektoren rechnen und geben das Berechnete dann wieder. Nehmen wir als Beispiel die Funktion mean(), die den Mittelwert von einer Reihe Zahlen berechnet.\n\ny <- c(1.2, 3.4, 2.1, 6, 4.3)\nmean(y)\n\n[1] 3.4\n\n\nEigentlich müssen in der Programmierung Objekte erst deklariert und somit erschaffen werden. Erst dann können Objekte initalisiert und somit befüllt bzw. etwas zugewiesen werden.\nWir sehen, dass wir mit der Funktion c() die Zahlen \\(1.2, 3.4, 2.1, 6, 4.3\\) zusammenkleben. Danach speichern wir die Zahlen in den Objekt y als einen Vektor ab. Wir müssen y nicht erst erschaffen, das Erschaffen und Speichern passiert in R in einem Schritt. Wir stecken nun den Vektor y in die Funktion mean() und erhalten den Mittelwert von \\(3.4\\) der Zahlen wiedergegeben."
  },
  {
    "objectID": "programing-basics.html#sec-R-pfeil",
    "href": "programing-basics.html#sec-R-pfeil",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "\n11.5 Zuweisungspfeil <-\n",
    "text": "11.5 Zuweisungspfeil <-\n\nMit dem Zuweisungspfeil speichern wir Dinge in Objekte. Das heist wir speichern damit intern in R Datensätze und viele andere Sachen, die wir dan später wieder verwenden wollen. Schauen wir uns das einmal im Beispiel an. Schrieben wir nur den Vektor c() mit Hunden und Katzen darin, so erscheint eine Ausgabe in R.\n\nc(\"dog\", \"dog\", \"cat\", \"cat\", \"fox\", \"fox\")\n\n[1] \"dog\" \"dog\" \"cat\" \"cat\" \"fox\" \"fox\"\n\n\nSchreiben wir den gleichen Vektor und nutzen den Zuweisungspfeil, dann wird der Vektor in dem Objekt animal gespeichert. Wenn du Strg Enter drückst, dann erstellt das RStudio automatisch den Zuweisungspfeil <-.\n\nanimal <- c(\"dog\", \"dog\", \"cat\", \"cat\", \"fox\", \"fox\")\n\nWie kommen wir jetzt an die Sachen, die in animal drin sind? Wir können einfach animal in R schreiben und dann wird uns der Inhalt von animal ausgegeben.\n\nanimal\n\n[1] \"dog\" \"dog\" \"cat\" \"cat\" \"fox\" \"fox\"\n\n\nDer Zuweisungspfeil <- ist zentral für die Nutzung von R. Wenn du Strg Enter drückst, dann erstellt das RStudio automatisch den Zuweisungspfeil <-.\nWir nutzen den Zuweisungspfeil <- ist zentral für die Nutzung von R. Wir brauchen den Zuweisungspfeil <- um Objekte in R zu erschaffen und Ergebnisse intern abzuspeichern. Zusammen mit Funktionen nutzen wir nur noch die Pipe %>% öfter."
  },
  {
    "objectID": "programing-basics.html#sec-R-pipe",
    "href": "programing-basics.html#sec-R-pipe",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "\n11.6 Pipe %>%\n",
    "text": "11.6 Pipe %>%\n\n\n\n\n\n\n\nPipes in R\n\n\n\nDu findest auf YouTube Einführung in R - Teil 11 - Pipes in R als Video. Hier erkläre ich den Zusammenhang nochmal in einem Video.\n\n\nIm Weiteren nutzen wir den Pipe Operator dargestellt als %\\>%. Du kannst dir den Pipe Operator als eine Art Röhre vorstellen in dem die Daten verändert werden und dann an die nächste Funktion weitergeleitet werden. Im folgenden siehst du viele Funktionen, die aneinander über Objekte miteinander verbunden werden. Im Kapitel 13 erfährst du mehr über die Funktionen select()und filter().\n\ndata_tbl <- read_excel(\"data/flea_dog_cat.xlsx\")\nanimal_1_tbl <- select(data_tbl, animal, jump_length)\nanimal_2_tbl <- filter(animal_1_tbl, jump_length >= 4)\nsort(animal_2_tbl$jump_length)\n\n [1]  4.1  4.3  5.4  5.6  5.7  6.1  7.6  7.9  8.2  8.9  9.1 11.8\n\ndata_tbl %>% \n  select(animal, jump_length) %>% \n  filter(jump_length >= 4) %>% \n  pull(jump_length) %>% \n  sort\n\n [1]  4.1  4.3  5.4  5.6  5.7  6.1  7.6  7.9  8.2  8.9  9.1 11.8\n\n\nIm unteren Beispiel siehst du die Nutzung des Pipe Operators %>%. Das Ergebnis ist das gleiche, aber der Code ist einfacher zu lesen. Wir nehmen den Datensatz data_tbl leiten den Datensatz in den Funktion select() und wählen die Spalten animal sowie jump_length. Dann filtern wir noch nach jump_lengthgrößer als 4 cm. Dann ziehen wir uns mit der Funktion pull() die Spalte jump_length aus dem Datensatz. Den Vektor leiten wir dann weiter in die Funktion sort() und erhalten die sortierten Sprunglängen zurück.\nIn Abbildung 11.4 und Abbildung 11.5 sehen wir, wie wir den Shortcut für das Erstellen des Pipe Operators umdefinieren. Danach können wir einfach den Shortcut nutzen und müssen nicht immer händisch den Pipe Operator eingeben.\n\n\nAbbildung 11.4— Auf den Reiter Modify Keyboard Shortcuts klicken. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\n\nAbbildung 11.5— Im Suchfeld pipe eingeben und dann in das Feld mit dem Shortcut klicken. Danach Alt und . klicken. Danach wird der Pipe Operator mit dem Shortcut Alt . gesetzt. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein."
  },
  {
    "objectID": "programing-basics.html#sec-dollar",
    "href": "programing-basics.html#sec-dollar",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "\n11.7 Spalte extrahieren $\n",
    "text": "11.7 Spalte extrahieren $\n\nWir nutzen eigentlich die Funktion pull() um eine Spalte bzw. Vektor aus einem Datensatz zu extrahieren.\n\ndata_tbl %>% \n  pull(animal)\n\n [1] \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\" \"cat\" \"cat\" \"cat\" \"cat\"\n[13] \"cat\" \"cat\"\n\n\nManche Funktionen in R, besonders die älteren Funktionen, benötigen keinen Datensatz sondern meist zwei bis drei Vektoren. Das heißt, wir können nicht einfach einen Datensatz in eine Funktion über data = data_tbl stecken sondern müssen der Funktion Vektoren übergeben. Dafür nutzen wir den $ Operator.\n\ndata_tbl$animal\n\n [1] \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\" \"cat\" \"cat\" \"cat\" \"cat\"\n[13] \"cat\" \"cat\"\n\ndata_tbl$jump_length\n\n [1]  5.7  8.9 11.8  8.2  5.6  9.1  7.6  3.2  2.2  5.4  4.1  4.3  7.9  6.1\n\n\nWir werden versuchen diese Schreibweise zu vermeiden, aber manchmal ist es sehr nützlich die Möglichkeit zu haben auf diese Weise eine Spalte zu extrahieren."
  },
  {
    "objectID": "programing-basics.html#sec-formula",
    "href": "programing-basics.html#sec-formula",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "\n11.8 Modelle definieren mit formula\n",
    "text": "11.8 Modelle definieren mit formula\n\nWir müssen später Modelle in R definieren um zum Beispiel den t Test oder aber eine lineare Regression rechnen zu können. Wir nutzen dazu in R die formula Syntax. Das heißt links von der Tilde ~ steht das \\(y\\), also der Spaltenname aus dem Datensatz data = den wir nutzen, der das Outcome repräsentiert. Rechts von der Tilde ~ stehen alle \\(x_1, ..., x_p\\), also alle Spalten aus dem Datensatz data = den wir nutzen, der die Einflussfaktoren repräsentiert.\nIn unserem Beispiel mit den Hunde- und Katzenflöhen aus Kapitel 5 wäre das \\(y\\) die Spalte jump_length und das \\(x\\) der Faktor animal. Wir erstellen mit der Funktion formula() das Modell in R. Wir brauchen später die Funktion formula nur implizit, aber hier ist es gut, das du einmal siehst, wie so eine Formula in R aussieht.\n\nformula(jump_length ~ animal)\n\njump_length ~ animal\n\n\nWenn die Formel sehr lang wird bzw. wir die Namen der Spalten aus anderen Funktionen haben, können wir auch die Funktion reformulate() nutzen. Wir brauchen die Funktion aber eher im Bereich des maschinellen Lernens. Hier ist die Funktion reformulate() aufgeführt, da es inhaltlich passt.\n\nreformulate(termlabels = c(\"animal\", \"sex\", \"site\"),\n            response = \"jump_length\",\n            intercept = TRUE)\n\njump_length ~ animal + sex + site"
  },
  {
    "objectID": "programing-basics.html#sec-R-help",
    "href": "programing-basics.html#sec-R-help",
    "title": "11  Operatoren, Funktionen und Pakete",
    "section": "\n11.9 Hilfe mit ?\n",
    "text": "11.9 Hilfe mit ?\n\nDas Fragezeichen ? vor einem Funktionsnamen erlaubt die Hilfeseite zu öffnen. Die Hilfsseiten findest du auch in einem der Reiter im RStudio.\n\n\nAbbildung 11.6— Neben den Paketen in R findet sich auch der Reiter Help, wo du Hilfe für die einzelnen Funktionen findets.."
  },
  {
    "objectID": "programing-import.html",
    "href": "programing-import.html",
    "title": "12  Daten einlesen",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:37:12\nDie Daten aus unserem Experiment müssen rein in R. Das heißt, wir haben meist unsere Daten in einer Exceldatei vorliegen und wollen diese Daten nun in R einlesen. Leider gibt es nur sehr wenig Spielraum wie die Exceldatei aussehen darf. Das führt meistens zu einem wilden Formatieren. Deshalb hier schon mal die goldenen Regeln in einer Exceldatei…\nGängige Fehler beim Einlesen von Dateien in R sind folgende Probleme. Wir wollen diese Probleme nacheinander einmal durchgehen. Aber keine Sorge, das Einlesen von Daten in R ist immer am Anfang etwas frickelig. Du kannst gerne in das R Tutorium kommen, dann können wir dir da beim Einlesen der Daten helfen."
  },
  {
    "objectID": "programing-import.html#genutzte-r-pakete-für-das-kapitel",
    "href": "programing-import.html#genutzte-r-pakete-für-das-kapitel",
    "title": "12  Daten einlesen",
    "section": "\n12.1 Genutzte R Pakete für das Kapitel",
    "text": "12.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, janitor)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "programing-import.html#sec-format",
    "href": "programing-import.html#sec-format",
    "title": "12  Daten einlesen",
    "section": "\n12.2 Dateiformat",
    "text": "12.2 Dateiformat\n\n\nDas Buch Cookbook for R stellt auch Beispiele für die Funktion gather() zu Verfügung für die Umwandlung von Wide zu Long Format: Converting data between wide and long format\nWir unterschieden bei Datenformaten zwischen den Wide Format und dem Long Format. Meistens gibst du die Daten intuitv im Wide Format in Excel ein. Das ist in Excel auch übersichtlicher. R und später die Funktion ggplot() zur Visualisierung der Daten kann aber nur mit dem Long Format arbeiten. Wir können aber mit der Funktion gather() das Wide Format in das Long Format umwandeln.\n\n12.2.1 Wide Format\nIn Tabelle 12.1 sehen wir eine typische Datentabelle in einem Wide Format. Die Spalten egeben jeweils die Tierart wieder und die Einträge in den Spalten sind die Sprungweiten in [cm].\n\n\nTabelle 12.1— Eine Datentabelle mit Sprungweiten in [cm] von Hunde- und Katzenflöhen im Wide Format.\n\ndog\ncat\n\n\n\n5.2\n10.1\n\n\n4.9\n9.4\n\n\n12.1\n11.8\n\n\n8.2\n6.7\n\n\n5.6\n8.2\n\n\n9.1\n9.1\n\n\n7.4\n7.1\n\n\n\n\nWir können diese Datentablle auch in R erstellen und uns als tibble() wiedergeben lassen.\n\njump_wide_tbl <- tibble(dog = c(5.2, 4.9, 12.1, 8.2, 5.6, 9.1, 7.4),\n                        cat = c(10.1, 9.4, 11.8, 6.7, 8.2, 9.1, 7.1))\njump_wide_tbl\n\n# A tibble: 7 × 2\n    dog   cat\n  <dbl> <dbl>\n1   5.2  10.1\n2   4.9   9.4\n3  12.1  11.8\n4   8.2   6.7\n5   5.6   8.2\n6   9.1   9.1\n7   7.4   7.1\n\n\nWenn du schon Daten hast, dann macht es eventuell mehr Sinn eine neue Exceldatei anzulegen in der du dann die Daten in das Long Format kopierst.\nWir können aber mit einem Wide-Format nicht mit ggplot() die Daten aus der Tabelle 12.1 visualisieren. Deshalb müssen wir entweder das Wide Format in das Long Format umwandeln oder die Daten gleich in Excel im Long Format erstellen.\n\n12.2.2 Long Format\nWenn du Daten erstellst ist es wichtig, dass du die Daten in Excel im Long-Format erstellst. Dabei muss eine Beobachtung eine Zeile sein. Du siehst in Abbildung 12.1 ein Beispiel für eine Tabelle in Excel, die dem Long Format folgt.\n\n\nAbbildung 12.1— Beispiel für eine Exceldatentabelle in Long Format.\n\n\nIm Folgenden sehen wir einmal wie die Funktion gather() das tibble() in Wide Format in ein tibble() in Long Format umwandelt. Wir müssen dafür noch die Spalte benennen mit der Option key = in die die Namen der Spalten aus dem Wide Format geschrieben werden sowie den Spaltennamen für die eigentlichen Messwerte mit der Option value =.\n\njump_tbl <- tibble(dog = c(5.2, 4.9, 12.1, 8.2, 5.6, 9.1, 7.4),\n                   cat = c(10.1, 9.4, 11.8, 6.7, 8.2, 9.1, 7.1)) %>%\n  gather(key = \"animal\", value = \"jump_length\")\njump_tbl\n\n# A tibble: 14 × 2\n   animal jump_length\n   <chr>        <dbl>\n 1 dog            5.2\n 2 dog            4.9\n 3 dog           12.1\n 4 dog            8.2\n 5 dog            5.6\n 6 dog            9.1\n 7 dog            7.4\n 8 cat           10.1\n 9 cat            9.4\n10 cat           11.8\n11 cat            6.7\n12 cat            8.2\n13 cat            9.1\n14 cat            7.1\n\n\nWir sehen, dass ein Long Format viel mehr Paltz benötigt. Das ist aber in R kein Problem. Wir sehen die Daten kaum sondern nutzen Funktionen wie ggplot() um die Daten zu visualisieren. Wichtig ist, dass du die Daten in Excel sauber abgelegt hast."
  },
  {
    "objectID": "programing-import.html#beispiel-in-excel",
    "href": "programing-import.html#beispiel-in-excel",
    "title": "12  Daten einlesen",
    "section": "\n12.3 Beispiel in Excel…",
    "text": "12.3 Beispiel in Excel…\nSchauen wir uns den Fall nochmal als Beispiel in einer Exceldatei an. Du findest die Beispieldatei germination_data.xlsx auf GitHub zum Herunterladen. Eventuell muss du bei dir den Pfad ändern oder aber die Importfunktion des RStudios nutzen. Dafür siehe einfach den nächsten Abschnitt.\nWir haben in der Beispieldatei germination_data.xlsx zum einen Messwiederholungen, gekenntzeichnet durch die Spalten t1 bis t4 sowie einmal gemessene Spalten wie freshmatter, drymatter, count_small_leaf und count_large_leaf.\n\n12.3.1 …ohne Messwiederholung\nWir schauen uns erstmal die Spalten ohne Messwiederholung an. Wenn du also keine Messwiederholungen hast, also due hast das Frischegewicht nur einmal an einer Pflanze gemessen, dann sieht deine Datei so aus wie in Abbildung 12.2. Ich zeige hier nur die Spalten A und F bis I aus der Datei germination_data.xlsx.\n\n\nAbbildung 12.2— Beispiel für eine Exceldatentabelle ohne Messwiederholungen.\n\n\nWir können dann die Datei auch über die Funktion read_excel() einlesen. Ich nutze noch die Funktion select() um die Spalten auszuwählen, die wir auch oben in der Abbildung sehen. Durch den Doppelpunkt : kann ich zusammenhängende Spalten auswählen und muss die Namen nicht einzeln eingeben.\n\nread_excel(\"data/germination_data.xlsx\") %>% \n  select(treatment, freshmatter:count_large_leaf)\n\n# A tibble: 20 × 5\n   treatment freshmatter drymatter count_small_leaf count_large_leaf\n   <chr>           <dbl>     <dbl>            <dbl>            <dbl>\n 1 control          13.7      0.98                2               12\n 2 control          18.1      1.31                4               12\n 3 control          14.4      1.01                4               12\n 4 control          10.7      0.74                2               12\n 5 control          15.9      1.11                2               12\n 6 low              26.4      1.9                10               18\n 7 low              24.3      1.68                8               14\n 8 low              27.1      1.87                6               18\n 9 low              27.2      1.8                 4               18\n10 low              18.2      1.22                6               18\n11 mid              20.9      1.31                6                8\n12 mid              19.4      1.41                4                8\n13 mid              21.5      1.44                4               14\n14 mid              24        1.56                6               11\n15 mid              25.8      1.77                6               11\n16 high             30.9      2.22                6               24\n17 high             36.3      2.52                2               19\n18 high             25.6      1.82                4               22\n19 high             33.3      2.27                6               22\n20 high             30.4      2.14                6               22\n\n\nWir könnten jetzt die Ausgabe auch in ein Objekt schreiben und dann mit der eingelesenen Datei weiterarbeiten.\n\n12.3.2 … mit Messwiederholung\nIn Abbildung 12.3 siehst du ein Datenbeispiel für eine Behandlung mit Messwiederholungen. Das heist wir haben immer noch eine Pflanze pro Zeile, aber die Pflanze wurde zu den Zeitpunkten t1 bis t4 gemessen. In dieser Form ist es viel einfacher aufzuschreiben, aber wir brauchen einen Faktor time_point mit vier Leveln t1 bis t4.\n\n\nAbbildung 12.3— Beispiel für eine Exceldatentabelle mit Messwiederholungen.\n\n\nWir nutzen die Funktion gather() um die Spalten t1 bis t4 zusammenzufassen und untereinander zu kleben. Die Spalte treatment wird dann einfach viermal wiederholt. Wir müssen dann noch die Spalte für den Faktor benennen und die Spalte für die eigentlichen Messwerte. Beides machen wir einmal über die Option key = und value =. Wir haben dann im Anschluss einen Datensatz im Long-Format mit dem wir dann weiterarbeiten können.\n\nread_excel(\"data/germination_data.xlsx\") %>% \n  select(treatment, t1:t4) %>% \n  gather(key = time_point, value = weight, t1:t4)\n\n# A tibble: 80 × 3\n   treatment time_point weight\n   <chr>     <chr>       <dbl>\n 1 control   t1             16\n 2 control   t1             17\n 3 control   t1             16\n 4 control   t1              9\n 5 control   t1             17\n 6 low       t1             18\n 7 low       t1             17\n 8 low       t1             15\n 9 low       t1             19\n10 low       t1             17\n# … with 70 more rows\n\n\nWir könnten jetzt die Ausgabe auch in ein Objekt schreiben und dann mit der eingelesenen Datei weiterarbeiten."
  },
  {
    "objectID": "programing-import.html#importieren-mit-rstudio",
    "href": "programing-import.html#importieren-mit-rstudio",
    "title": "12  Daten einlesen",
    "section": "\n12.4 Importieren mit RStudio",
    "text": "12.4 Importieren mit RStudio\nWir können das RStudio nutzen um Daten mit Point-and-Klick rein zuladen und dann den Code wieder in den Editor kopieren. Im Prinzip ist dieser Weg der einfachste um einmal zu sehen, wie ein pfad funktioniert und der Code lautet. Später benötigt man diese ‘Krücke’ nicht mehr. Wir nutzen dann direkt den Pfad zu der Datei. Abbildung 12.4 zeigt einen Ausschnitt, wo wir im RStudio die Import Dataset Funktionalität finden.\n\n\nAbbildung 12.4— Auf den Reiter Einviroment klicken und dann Import Dataset. In der deutschen version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\n\n\n\n\n\nImportieren mit RStudio als Video\n\n\n\nDu findest auf YouTube Einführung in R - Teil 21.0 - Daten importieren mit RStudio - Point and Klick als Video. Point and Klick ist als Video einfacher nachzuvollziehen als Screenshots in einem Fließtext."
  },
  {
    "objectID": "programing-import.html#sec-pfad",
    "href": "programing-import.html#sec-pfad",
    "title": "12  Daten einlesen",
    "section": "\n12.5 Importieren per Pfad",
    "text": "12.5 Importieren per Pfad\nIn Abbildung 12.5 können wir sehen wie wir den Pfad zu unserer Excel Datei flea_dog_cat.xlsx finden. Natürlich kannst du den Pfad auch anders herausfinden bzw. aus dem Explorer oder Finder kopieren.\n\n\nAbbildung 12.5— Durch den Rechts-Klick auf die Eigenschaften einer Datei kann man sich den Pfad zur Datei anzeigen lassen. Achtung! Unter Windows muss der Slash \\ noch in den Backslash / gedreht werden.\n\n\nNachdem wir den Pfad gefunden haben, können wir den Pfad in die Funktion read_excel() kopieren und die Datei in das Objekt data_tbl einlesen. Ja, es wird nichts in der R Console ausgegeben, da sich die Daten jetzt in dem Object data_tbl befinden.\n\n## Ganzer Pfad zur Datei flea_dog_cat.xlsx\ndata_tbl <- read_excel(\"data/flea_dog_cat.xlsx\")\n\n\n\n\n\n\n\nUnterschied zwischen \\ in Windows und / in R\n\n\n\nAchte einmal auf den Slash im Pfad in R und einem im Pfsd in Windows. Einmal ist es der Slash \\ im Dateipfad und einmal der Backslash /. Das ist sehr ärgerlich, aber dieses Problem geht zurück in die 80’ziger. Bill hat entschieden für sein Windows / zu nutzen und Steve (und Unix) eben /. Und mit dieser Entscheidung müssen wir jetzt leben…"
  },
  {
    "objectID": "programing-import.html#sec-umlaute",
    "href": "programing-import.html#sec-umlaute",
    "title": "12  Daten einlesen",
    "section": "\n12.6 Auf ein englisches Wort in Dateien",
    "text": "12.6 Auf ein englisches Wort in Dateien\nEin großes Problem in Datein sind Umlaute (ä,ö,ü) oder aber andere (Sonder)zeichen (ß, ?, oder #). Als dies sollte vermieden werden. Eine gute Datei für R beinhaltet nur ganze Wörter, Zahlen oder aber leere Felder. Ein leeres Feld ist ein fehlender Wert. Abbildung 12.6 zeigt eine gute Exceldatentablle. Wir schreiben jump_length mit Unterstrich um den Namen besser zu lesen zu können. Sonst ist auch alles in Englisch geschrieben. Wir vermeiden durch die neglische Schreibweise aus versehen einen Umlaut oder anderweitig problematische Zeichen zu verwenden. Später können wir alles noch für Abbildungen anpassen.\n\n\nAbbildung 12.6— Beispiel für eine gute (Excel)Datentabelle. Keine Umlaute sind vorhanden und die Spaltennamen haben keine Leerzeichen oder Sonderzeichen."
  },
  {
    "objectID": "programing-import.html#sec-spalten",
    "href": "programing-import.html#sec-spalten",
    "title": "12  Daten einlesen",
    "section": "\n12.7 Spaltennamen in der (Excel)-Datei",
    "text": "12.7 Spaltennamen in der (Excel)-Datei\nDie Funktion clean_names() aus dem R Paket janitor erlaubt es die Spaltennamen einer eingelesenen Datei in eine für R gute Form zu bringen.\n\nKeine Leerzeichen in den Spaltennamen.\nAlle Spaltennamen sind klein geschrieben.\n\n\ndata_tbl %>% \n  clean_names()\n\n# A tibble: 14 × 5\n   animal jump_length flea_count grade infected\n   <chr>        <dbl>      <dbl> <dbl>    <dbl>\n 1 dog            5.7         18     8        0\n 2 dog            8.9         22     8        1\n 3 dog           11.8         17     6        1\n 4 dog            8.2         12     8        0\n 5 dog            5.6         23     7        1\n 6 dog            9.1         18     7        0\n 7 dog            7.6         21     9        0\n 8 cat            3.2         12     7        1\n 9 cat            2.2         13     5        0\n10 cat            5.4         11     7        0\n11 cat            4.1         12     6        0\n12 cat            4.3         16     6        1\n13 cat            7.9          9     6        0\n14 cat            6.1          7     5        0"
  },
  {
    "objectID": "programing-dplyr.html",
    "href": "programing-dplyr.html",
    "title": "13  Daten bearbeiten",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:37:16\nWir haben in dem vorherigen Kapitel Daten eingelesen. Jetzt wollen wir die Daten aufräumen (eng. tidy). Es ist notwendig, dass wir die Daten so aufarbeiten, dass R damit umgehen kann. Insbesondere das Erstellen von Faktoren ist wichtig, wenn die Spalte ein Faktor ist. R muss wissen was für Eigenschaften eine Spalte hat. Sonst funktionieren spätere Anwendungen in R nicht richtig oder geben einen Fehler wieder.\nEs gibt zwei Möglichkeiten wie du mit deinen Daten umgehst:\nIm Folgenden wollen wir den Datensatz data_tbl in R bearbeiten. Das heißt wir wollen Spalten auswählen mit select() oder Zeilen auswählen mit filter(). Schlussendlich wollen wir auch die Eigenschaften von Spalten mit der Funktion mutate ändern. Wir laden also den Datensatz flea_dog_cat.xlsx einmal in R.\nEs ergibt sich folgende Tabelle 13.1, die wir schon aus vorherigen Kapiteln kennen."
  },
  {
    "objectID": "programing-dplyr.html#genutzte-r-pakete-für-das-kapitel",
    "href": "programing-dplyr.html#genutzte-r-pakete-für-das-kapitel",
    "title": "13  Daten bearbeiten",
    "section": "\n13.1 Genutzte R Pakete für das Kapitel",
    "text": "13.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, readxl, magrittr, janitor)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "programing-dplyr.html#spalten-wählen-mit-select",
    "href": "programing-dplyr.html#spalten-wählen-mit-select",
    "title": "13  Daten bearbeiten",
    "section": "\n13.2 Spalten wählen mit select()\n",
    "text": "13.2 Spalten wählen mit select()\n\n\n\n\n\n\n\nYouTube - Spalten auswählen mit select()\n\n\n\nDu findest auf YouTube Einführung in R - Teil 12 - Spalten auswählen mit select() als Video zum nochmal anschauen.\n\n\nWir nutzen die Funktion select()um Spalten zu wählen.\nDer Datensatz, den wir im Experiment erschaffen, ist meist riesig. Jetzt könnten wir natürlich eine Exceltabelle mit unterschiedlichen Sheets bzw. Reitern erstellen oder aber die Spalten die wir brauchen in R selektieren. Wir nutzen die Funktion select()um Spalten zu wählen. Im folgenden Codeblock wählen wir die Spalten animal, jump_length und flea_count.\n\ndata_tbl %>% \n  select(animal, jump_length, flea_count)\n\n# A tibble: 21 × 3\n   animal jump_length flea_count\n   <chr>        <dbl>      <dbl>\n 1 dog            5.7         18\n 2 dog            8.9         22\n 3 dog           11.8         17\n 4 dog            8.2         12\n 5 dog            5.6         23\n 6 dog            9.1         18\n 7 dog            7.6         21\n 8 cat            3.2         12\n 9 cat            2.2         13\n10 cat            5.4         11\n# … with 11 more rows\n\n\nWir können die Spalten beim selektieren auch umbenennen und in eine andere Reihenfolge bringen.\n\ndata_tbl %>% \n  select(Sprungweite = jump_length, flea_count, animal)\n\n# A tibble: 21 × 3\n   Sprungweite flea_count animal\n         <dbl>      <dbl> <chr> \n 1         5.7         18 dog   \n 2         8.9         22 dog   \n 3        11.8         17 dog   \n 4         8.2         12 dog   \n 5         5.6         23 dog   \n 6         9.1         18 dog   \n 7         7.6         21 dog   \n 8         3.2         12 cat   \n 9         2.2         13 cat   \n10         5.4         11 cat   \n# … with 11 more rows\n\n\nDu findest auf der englischen Hilfeseite für select() noch weitere Beispiele für die Nutzung."
  },
  {
    "objectID": "programing-dplyr.html#zeilen-wählen-mit-filter",
    "href": "programing-dplyr.html#zeilen-wählen-mit-filter",
    "title": "13  Daten bearbeiten",
    "section": "\n13.3 Zeilen wählen mit filter()\n",
    "text": "13.3 Zeilen wählen mit filter()\n\n\n\n\n\n\n\nYouTube - Zeilen auswählen mit filter()\n\n\n\nDu findest auf YouTube Einführung in R - Teil 13 - Zeilen auswählen mit filter() als Video zum nochmal anschauen.\n\n\nWir nutzen die Funktion filter() um Zeilen nach Kriterien zu wählen.\nWährend wir die Auswahl an Spalten gut und gerne auch in Excel durchführen können, so ist dies bei der Auswahl der Zeilen nicht so einfach. Wir können in R hier auf die Funktion filter() zurückgreifen. Wir nutzen die Funktion filter() um Zeilen nach Kriterien zu wählen.\nIm folgenden Codeblock wählen wir die Zeilen aus in denen die Worte dog und fox stehen. Wir nutzen dazu den Operator %in% um auszudrücken, dass wir alle Einträge in der Spalte animal wollen die in dem Vektor c(\"dog\", \"fox\") beschrieben sind.\n\ndata_tbl %>% \n  filter(animal %in% c(\"dog\", \"fox\"))\n\n# A tibble: 14 × 5\n   animal jump_length flea_count grade infected\n   <chr>        <dbl>      <dbl> <dbl>    <dbl>\n 1 dog            5.7         18     8        0\n 2 dog            8.9         22     8        1\n 3 dog           11.8         17     6        1\n 4 dog            8.2         12     8        0\n 5 dog            5.6         23     7        1\n 6 dog            9.1         18     7        0\n 7 dog            7.6         21     9        0\n 8 fox            7.7         21     5        1\n 9 fox            8.1         25     4        1\n10 fox            9.1         31     4        1\n11 fox            9.7         12     5        1\n12 fox           10.6         28     4        0\n13 fox            8.6         18     4        1\n14 fox           10.3         19     3        0\n\n\nEs stehen dir Folgende logische Operatoren zu Verfügung wie in Tabelle 13.2 gezeigt. Am Anfang ist es immer etwas schwer sich in den logischen Operatoren zurechtzufinden. Daher kann ich dir nur den Tipp geben einmal die Operatoren selber auszuprobieren und zu schauen, was du da so rausfilterst.\n\n\nTabelle 13.2— Logische Opertairen und R und deren Beschreibung\n\n\n\n\n\nLogischer Operator\nBeschreibung\n\n\n\n<\nkleiner als (eng. less than)\n\n\n<=\nkleiner als oder gleich (eng. less than or equal to)\n\n\n>\ngrößer als (eng. greater than)\n\n\n>=\ngrößer als oder gleich (eng. greater than or equal to)\n\n\n==\nexact gleich (eng. exactly equal to)\n\n\n!=\nnicht gleich (eng. not equal to)\n\n\n!x\nnicht (eng. not x)\n\n\nx | y\noder (eng. x or y)\n\n\nx & y\nund (eng. x and y)\n\n\n\n\nHier ein paar Beispiele. Probiere gerne auch mal Operatoren selber aus. Im folgenden Codeblock wollen wir nur die Zeilen haben, die eine Anzahl an Flöhen größer von 15 haben.\n\ndata_tbl %>% \n  filter(flea_count > 15)\n\n# A tibble: 13 × 5\n   animal jump_length flea_count grade infected\n   <chr>        <dbl>      <dbl> <dbl>    <dbl>\n 1 dog            5.7         18     8        0\n 2 dog            8.9         22     8        1\n 3 dog           11.8         17     6        1\n 4 dog            5.6         23     7        1\n 5 dog            9.1         18     7        0\n 6 dog            7.6         21     9        0\n 7 cat            4.3         16     6        1\n 8 fox            7.7         21     5        1\n 9 fox            8.1         25     4        1\n10 fox            9.1         31     4        1\n11 fox           10.6         28     4        0\n12 fox            8.6         18     4        1\n13 fox           10.3         19     3        0\n\n\nWir wollen nur die infizierten Tiere haben.\n\ndata_tbl %>% \n  filter(infected == TRUE)\n\n# A tibble: 10 × 5\n   animal jump_length flea_count grade infected\n   <chr>        <dbl>      <dbl> <dbl>    <dbl>\n 1 dog            8.9         22     8        1\n 2 dog           11.8         17     6        1\n 3 dog            5.6         23     7        1\n 4 cat            3.2         12     7        1\n 5 cat            4.3         16     6        1\n 6 fox            7.7         21     5        1\n 7 fox            8.1         25     4        1\n 8 fox            9.1         31     4        1\n 9 fox            9.7         12     5        1\n10 fox            8.6         18     4        1\n\n\nWir wollen nur die infizierten Tiere haben UND die Tiere mit einer Flohanzahl größer als 20.\n\ndata_tbl %>% \n  filter(infected == TRUE & flea_count > 20)\n\n# A tibble: 5 × 5\n  animal jump_length flea_count grade infected\n  <chr>        <dbl>      <dbl> <dbl>    <dbl>\n1 dog            8.9         22     8        1\n2 dog            5.6         23     7        1\n3 fox            7.7         21     5        1\n4 fox            8.1         25     4        1\n5 fox            9.1         31     4        1\n\n\nDu findest auf der englischen Hilfeseite für filter() noch weitere Beispiele für die Nutzung."
  },
  {
    "objectID": "programing-dplyr.html#spalten-ändern-mit-mutate",
    "href": "programing-dplyr.html#spalten-ändern-mit-mutate",
    "title": "13  Daten bearbeiten",
    "section": "\n13.4 Spalten ändern mit mutate()\n",
    "text": "13.4 Spalten ändern mit mutate()\n\n\n\n\n\n\n\nYouTube - Eigenschaften von Variablen ändern mit mutate()\n\n\n\nDu findest auf YouTube Einführung in R - Teil 14 - Eigenschaften von Variablen ändern mit mutate() als Video zum nochmal anschauen.\n\n\n\n\nWir nutzen die Funktion mutate() um die Eigenschaften von Spalten daher Variablen zu ändern.\nDie Reihenfolge der Funktionen ist wichtig um unliebsame Effekte zu vermeiden.\n\nErst wählen wir die Spalten mit select()\n\nDann filtern wir die Zeilen mit filter()\n\nAbschließend ändern wir die Eigenschaften der Spalten mit mutate()\n\n\nNachdem wir die Spalten mit select() udn eventuell die Zeieln mit filter() gewählt haben. müssen wir jetzt noch die Eigenschaften der Spalten ändern. Das Ändern müssen wir nicht immer tun, aber häufig müssen wir noch einen Faktor erschaffen. Wir nutzen noch die Funktion pull() um uns die Spalte animal aus dem Datensatz zu ziehen. Nur so sehen wir die vollen Eigenschaften des Faktors. Später nutzen wir pull seltener und nur um zu kontrollieren, was wir gemacht haben.\nIm folgenden Codeblock verwandeln wir die Variable animal in einen Faktor durch die Funktion as_factor. Wir sehen, dass die Level des Faktoes so sortiert sind, wie das Auftreten in der Spalte animal.\n\ndata_tbl %>% \n  mutate(animal = as_factor(animal)) %>% \n  pull(animal)\n\n [1] dog dog dog dog dog dog dog cat cat cat cat cat cat cat fox fox fox fox fox\n[20] fox fox\nLevels: dog cat fox\n\n\nWollen wir die Sortierung der Level ändern, können wir die Funktion factor() nutzen. Wir ändern die Sortierung des Faktors zu fox, dog und cat.\n\ndata_tbl %>% \n  mutate(animal = factor(animal, levels = c(\"fox\", \"dog\", \"cat\"))) %>% \n  pull(animal)\n\n [1] dog dog dog dog dog dog dog cat cat cat cat cat cat cat fox fox fox fox fox\n[20] fox fox\nLevels: fox dog cat\n\n\nWir können auch die Namen (eng. labels) der Level ändern. Hier musst du nur aufpassen wie du die alten Labels überschreibst. Wenn ich gleichzeitig die Level und die Labels ändere komme ich häufig durcheinander. Da muss du eventuell nochmal schauen, ob auch alles so geklappt hat wie du wolltest.\n\ndata_tbl %>% \n  mutate(animal = factor(animal, labels = c(\"Hund\", \"Katze\", \"Fuchs\"))) %>% \n  pull(animal)\n\n [1] Katze Katze Katze Katze Katze Katze Katze Hund  Hund  Hund  Hund  Hund \n[13] Hund  Hund  Fuchs Fuchs Fuchs Fuchs Fuchs Fuchs Fuchs\nLevels: Hund Katze Fuchs\n\n\nDu findest auf der englischen Hilfeseite für mutate() noch weitere Beispiele für die Nutzung. Insbesondere die Nutzung von mutate() über mehrere Spalten gleichzeitig erlaubt sehr effiezientes Programmieren. Aber das ist für den Anfang etwas viel.\n\n\n\n\n\n\nDie Funktionen select(), filter() und mutate() in R\n\n\n\nBitte schaue dir auch die Hilfeseiten der Funktionen an. In diesem Skript kann ich nicht alle Funktionalitäten der Funktionen zeigen. Oder du kommst in das R Tutorium welches ich anbiete und fragst dort nach den Möglichkeiten Daten in R zu verändern."
  },
  {
    "objectID": "programing-dplyr.html#gruppieren-mit-group_by",
    "href": "programing-dplyr.html#gruppieren-mit-group_by",
    "title": "13  Daten bearbeiten",
    "section": "\n13.5 Gruppieren mit group_by()\n",
    "text": "13.5 Gruppieren mit group_by()\n\nSobald wir einen Faktor erschaffen haben, können wir die Daten in R auch nach dem Faktor gruppieren. Das heißt wir nutzen die Funktion group_by() um R mitzuteilen, dass nun folgende Funktionen getrennt für die einzelen Gruppen erfolgen sollen. Im folgenden Codeblock siehst du die Anwendung.\n\ndata_tbl %>% \n  mutate(animal = as_factor(animal)) %>% \n  group_by(animal)\n\n# A tibble: 21 × 5\n# Groups:   animal [3]\n   animal jump_length flea_count grade infected\n   <fct>        <dbl>      <dbl> <dbl>    <dbl>\n 1 dog            5.7         18     8        0\n 2 dog            8.9         22     8        1\n 3 dog           11.8         17     6        1\n 4 dog            8.2         12     8        0\n 5 dog            5.6         23     7        1\n 6 dog            9.1         18     7        0\n 7 dog            7.6         21     9        0\n 8 cat            3.2         12     7        1\n 9 cat            2.2         13     5        0\n10 cat            5.4         11     7        0\n# … with 11 more rows\n\n\nAuf den ersten Blick ändert sich nicht viel. Es entsteht aber die Zeile # Groups: animal [3]. Wir wissen nun, dass wir nach der Variable animal mit drei Gruppen die Datentabelle gruppiert haben. Die Anwendung siehst du in Kapitel 14.10 bei der Berechung von deskriptiven Maßzahlen."
  },
  {
    "objectID": "programing-dplyr.html#mehr-informationen-durch-glimpse-und-str",
    "href": "programing-dplyr.html#mehr-informationen-durch-glimpse-und-str",
    "title": "13  Daten bearbeiten",
    "section": "\n13.6 Mehr Informationen durch glimpse() und str()\n",
    "text": "13.6 Mehr Informationen durch glimpse() und str()\n\nAm Ende noch zwei Funktionen zur Kontrolle, was wir hier eigentlich gerade tun. Mit der Funktion glimpse() können wir uns einen Einblick in die Daten geben lassen. Wir sehen dann nochmal kurz und knapp wieviel Zeieln und Spalten wir haben und welche Inhalte in den Spalten stehen. Die gleichen Informationen erhalten wir auch durch die Funktion str(). Die Funktion str()geht aber noch einen Schritt weiter und nennt uns auch Informationen zu dem Objekt. Daher wir wissen jetzt, dass es sich beim dem Objekt data_tbl um ein tibble() handelt.\n\nglimpse(data_tbl)\n\nRows: 21\nColumns: 5\n$ animal      <chr> \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"cat\", \"c…\n$ jump_length <dbl> 5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6, 3.2, 2.2, 5.4, 4.1, 4.…\n$ flea_count  <dbl> 18, 22, 17, 12, 23, 18, 21, 12, 13, 11, 12, 16, 9, 7, 21, …\n$ grade       <dbl> 8, 8, 6, 8, 7, 7, 9, 7, 5, 7, 6, 6, 6, 5, 5, 4, 4, 5, 4, 4…\n$ infected    <dbl> 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1…\n\nstr(data_tbl)\n\ntibble [21 × 5] (S3: tbl_df/tbl/data.frame)\n $ animal     : chr [1:21] \"dog\" \"dog\" \"dog\" \"dog\" ...\n $ jump_length: num [1:21] 5.7 8.9 11.8 8.2 5.6 9.1 7.6 3.2 2.2 5.4 ...\n $ flea_count : num [1:21] 18 22 17 12 23 18 21 12 13 11 ...\n $ grade      : num [1:21] 8 8 6 8 7 7 9 7 5 7 ...\n $ infected   : num [1:21] 0 1 1 0 1 0 0 1 0 0 ..."
  },
  {
    "objectID": "programing-dplyr.html#sec-stringr",
    "href": "programing-dplyr.html#sec-stringr",
    "title": "13  Daten bearbeiten",
    "section": "\n13.7 Einen character mit stringr bearbeiten",
    "text": "13.7 Einen character mit stringr bearbeiten\n\n\n\n\nstringr\n\ndata_tbl <- tibble(animal = c(\"cat\", \"cat\", \"cat\", \"dog\", \"dog\", \"bird\"),\n                   site = c(\"village\", \"village\", \"town\", \"town\", \"town\", \"city\"),\n                   time = c(\"t1_1\", \"t2_2\", \"t3_3\", \"t3_4\", \"t3_5\", \"t3_6\"))\n\n\ndata_tbl %>% \n  mutate(animal_site = str_c(animal, \"-\", site))\n\n# A tibble: 6 × 4\n  animal site    time  animal_site\n  <chr>  <chr>   <chr> <chr>      \n1 cat    village t1_1  cat-village\n2 cat    village t2_2  cat-village\n3 cat    town    t3_3  cat-town   \n4 dog    town    t3_4  dog-town   \n5 dog    town    t3_5  dog-town   \n6 bird   city    t3_6  bird-city  \n\n\n\ndata_tbl %>% \n  mutate(animal_pad = str_pad(animal, pad = \"0\", width = 5, side = \"left\"))\n\n# A tibble: 6 × 4\n  animal site    time  animal_pad\n  <chr>  <chr>   <chr> <chr>     \n1 cat    village t1_1  00cat     \n2 cat    village t2_2  00cat     \n3 cat    town    t3_3  00cat     \n4 dog    town    t3_4  00dog     \n5 dog    town    t3_5  00dog     \n6 bird   city    t3_6  0bird     \n\n\n\ndata_tbl %>% \n  separate(time, into = c(\"time\", \"rep\"), sep = \"_\", convert = TRUE)\n\n# A tibble: 6 × 4\n  animal site    time    rep\n  <chr>  <chr>   <chr> <int>\n1 cat    village t1        1\n2 cat    village t2        2\n3 cat    town    t3        3\n4 dog    town    t3        4\n5 dog    town    t3        5\n6 bird   city    t3        6"
  },
  {
    "objectID": "programing-dplyr.html#sec-regex",
    "href": "programing-dplyr.html#sec-regex",
    "title": "13  Daten bearbeiten",
    "section": "\n13.8 Regular expressions",
    "text": "13.8 Regular expressions\n\n\n\n\nRegular expressions in stingr\nRegular expressions"
  },
  {
    "objectID": "programing-dplyr.html#sec-purrr",
    "href": "programing-dplyr.html#sec-purrr",
    "title": "13  Daten bearbeiten",
    "section": "\n13.9 Mit purrr wiederholt Dinge tun",
    "text": "13.9 Mit purrr wiederholt Dinge tun\n\n\n\n\nIteration\nR Paket purrr"
  },
  {
    "objectID": "eda-preface.html",
    "href": "eda-preface.html",
    "title": "Explorative Datenanalyse",
    "section": "",
    "text": "Wir kürzen die explorative Datenanalyse häufig als EDA ab.\nWir haben die Daten jetzt in R Eingelesen und im Zweifel noch angepasst. Nun wollen wir uns die Daten einmal angucken. Nicht in dem Sinne, dass wir auf die Datentabelle schauen. Sondern wir wollen die Daten visualisieren. Wir erstellen Abbildungen von den Daten und versuchen so mehr über die Daten zu erfahren. Sehen wir Zusammenhänge zwischen verschiedenen Variablen bzw. Spalten? Wir führen eine explorative Datenanalyse durch. Über die explorative Datenanalyse wollen wir uns in diesem Kapitel einmal Gedanken machen.\n\n\n\n\n\n\nEinführung in R per Video\n\n\n\nDu findest auf YouTube Grundlagen in R als Video Reihe. Du musst die Grundlagen in R verstanden haben, damit du dem R Code folgen kannst."
  },
  {
    "objectID": "eda-descriptive.html",
    "href": "eda-descriptive.html",
    "title": "14  Deskriptive Statistik",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:37:24\nWir nutzen die deskriptive Statistik um Zahlen zusammenzufassen. Das heißt wir haben einen Datensatz vorliegen wie zum Beispiel den Datensatz im Beispiel zu den Sprungweiten der Hundeflöhe. Wir wollen jetzt den großen Datensatz in wenigen Zahlen wiedergeben. Warum wenige Zahlen? Wenn wir das Ergebnis präsentieren wollen, dann müssen es wenige Zahlen sein, die den Datensatz gut zusammenfassen. Daher ist es wichtig zu wissen, dass wir dutzende bis hunderte Zahlen durch meist eine oder zwei Zahlen beschreiben wollen. Wir brauchen die statistischen Maßzahlen aus diesem Kapitel später um teilweise noch extrem größere Datensätze darstellen zu können. Ebenso werden wir die Maßzahlen aus diesem Kapitel dafür verwenden statistische Tests und Modelle zu rechnen.\nNehmen wir nun als Beispiel die Sprungweiten in [cm] von Hundeflöhen. Wir messen sieben Sprungweiten von sieben Hundeflöhen und messen dabei folgende Werte in [cm]: 5.7, 8.9, 11.8, 8.2, 5.6, 9.1 und 7.6. Wir schreiben diese Sprungweiten nun als \\(y\\) in einen Vektor in der Form\n\\[\ny = \\{5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6\\}.\n\\]\nIn R würde der Vektor wie etwas anders aussehen.\nWir wollen nun die Sprungweiten in \\(y\\) beschrieben und durch wenige andere Zahlen zusammenfassen. Einige dieser statistischen Maßzahlen sind dir vermutlich schon bekannt, andere eher neu."
  },
  {
    "objectID": "eda-descriptive.html#mittelwert",
    "href": "eda-descriptive.html#mittelwert",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.1 Mittelwert",
    "text": "14.1 Mittelwert\nDer Mittelwert einer Zahlenreihe beschreibt den Schwerpunkt der Zahlen. Der Mittelwert wird auch als Lageparameter benannt.  Wir schreiben den Mittelwert mit einem Strich über den Vektor, der die Zahlen enthält. Im folgenden ist die Formel für den Mittelwert der Sprungweite in [cm] der Hunde gezeigt. Der Mittelwert ist in dem Sinne eine künstliche Zahl, da der Mittlwert häufig nicht in den beobachteten Zahlen vorkommt.Der Mittelwert und der Median sind zwei Lageparameter einer Verteilung. Beide beschreiben die Stelle, wo die Verteilungskurve am höchsten ist.\n\n\n\n\n\n\nWir werden immer mal wieder Formeln vereinfachen. Zum Beispiel nur \\(\\sum\\) schreiben anstatt \\(\\sum_i^n\\), wenn wir einen Vektor aufsummieren und uns die Indizes sparen…\n\\[\n\\bar{y} = \\sum_{i=1}^{n}\\cfrac{x_i}{n} =\n\\cfrac{5.7 + 8.9 + 11.8 + 8.2 + 5.6 + 9.1 + 7.6}{7} =\n8.13\n\\]\nIm Durchschnitt oder im Mittel springen Hundeflöhe 8.13 cm weit. In der Abbildung 14.1 wollen wir die Formel nochmal visualisieren. Vielleicht fällt dir dann der Zusammenhang von dem Index \\(i\\) und der gesamten Fallzahl \\(n\\) leichter.\n\n\nAbbildung 14.1— Zusamenhang zwischen \\(y\\) sowie dem Index \\(i\\) in der Formel für den Mittelwert.\n\n\nIn R können wir den Mittelwert einfach mit der Funktion mean() berechnen. Wir wollen dann den Mittelwert noch auf die zweite Kommastelle runden. Das machen wir dann mit der Funktion round().\nDu findest in Kapitel 11 den Einstieg für die Programmierung in R. Da findest du auch die Erklärung für den Pipe Operator %>%.\n\ny %>% mean %>% round(2)\n\n[1] 8.13\n\n\nWir erhalten das gleiche Ergebnis wie oben in unserer händischen Rechnung. Die Hundeflöhe springen im Mittel 8.13 cm weit.\nDer Mittelwert ist eine bedeutende Maßzahl der Normalverteilung. Daher merken wir uns hier schon mal, dass wir den Mittelwert brauchen werden. Auch wenn wir darüber nachdenken ob sich zwei Gruppen unterscheiden, so nutzen wir hierzu den Mitelwert. Unterscheiden sich die mittleren Sprungweiten in [cm] von Hunde- und Katzenflöhen?"
  },
  {
    "objectID": "eda-descriptive.html#spannweite",
    "href": "eda-descriptive.html#spannweite",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.2 Spannweite",
    "text": "14.2 Spannweite\nDie Spannweite erlaubt uns zu überprüfen was die kleinste Zahl und die größte Zahl ist. Also uns das Minimum und das Maximum einer Zahlenreihe anzuschauen. Auf den ersten Blick mag das nicht so sinnig sein, aber wenn wir uns hunderte von Beobachtungen anschauen, wollen wir wissen, ob wir nicht einen Fehler bei Eintragen der Daten gemacht haben. Wir wissen eigentlich, dass z.B keine negativen Zuwachsraten auftreten können.\nDie Spannweite dient dazu in einem Datensatz zu übersprüfen ob die Spalte, oder auch Variable genannt, den richtigen Zahlenraum aufweist. Das machen wir durch die Funktion range().\n\\[\ny_{range} = y_{max} - y_{min} = 12.1 - 4.9 = 7.2\n\\] Die Hundeflöhe springen in einer Spannweite von 7.2 cm. Das kommt einem normal vor. Die Spannweite ist nicht übertrieben groß. Der minimale Wert ist 4.9 und der maximale Wert it 12.1 und somit sind beide Zahlen in Ordnung. Keine der beiden Zahlen ist übertrieben groß oder gar negativ.\nIn R können wir die Spannweite mit range() wie folgt berechnen. Wir erhalten den minmialen udn maximalen Wert.\n\nrange(y) \n\n[1]  5.6 11.8\n\n\nWir merken uns, dass die Spannweite eine Maßzahl für die Validität der Daten ist. Hat das Experiment geklappt oder kamen da nur komische Zahlen bei raus, die wir so in der Realität nicht erwarten würden. Zum Beispiel negative Sprungweiten, weil wir einmalauf das Minuszeichen gekommen sind."
  },
  {
    "objectID": "eda-descriptive.html#varianz",
    "href": "eda-descriptive.html#varianz",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.3 Varianz",
    "text": "14.3 Varianz\nBis jetzt können wir mit dem Mittelwert \\(\\bar{y}\\) die Lage oder den Mittelpunkt unserer Zahlenreihe beschreiben. Uns fehlt damit aber die Information über die Streuung der Zahlen. Sind die Zahlen alle eher gleich oder sehr verschieden? Liegen die Zahlen daher alle bei dem Mittelwert oder sind die Zahlen weit um den Mittelwert gestreut.\nDie Streuung der Zahlen um den Mittelwert beschreibt die Varianz oder auch \\(s^2\\). Wir berechnen die Varianz indem wir von jeder Zahl den Mittelwert aller Zahlen abziehen und dann das Ergebnis quadrieren. Das machen wir für alle Zahlen und addieren dann die Summe auf. Wir erhalten die Quadratsumme von \\(y\\).\nAbweichungsquadrate sind ein wichtiges Konzept in der Statistik. Wenn wir wissen wollen, wie groß eine Abweichung von einer Zahl zu einer anderen ist, dann nutzen wir immer das Quadrat der Abweichung und bilden die Quadratsumme.\n\\[\ns^2 = \\sum_{i=1}^n\\cfrac{(y_i - \\bar{y})^2}{n-1} = \\cfrac{(5.7 -\n8.13)^2 + ... + (7.6 - 8.13)^2}{7-1} = 4.6\n\\]\nDie Varianz berschreibt also die Streuung der Zahlen im Quadrat um den Mittelwert. Das heißt in unserem Beispiel, dass die Sprungweite eine Varianz von 4.6 cm\\(^2\\) hat. Wir können Quadratzentimeter schlecht interpretieren. Deshalb führen wir gleich die Wuzel der Varianz ein: die Standardabweichung.\nIn R lässt sich die Varianz einfach durch die Funktion var() berechnen.\n\ny %>% var %>% round(2) \n\n[1] 4.6\n\n\nWir benötigen die Varianz häufig nur als Zwischenschritt um die Standardabweichung zu berechnen. Das Konzept der Abweichungsquadrate benötigen wir aber in der Varianzanalyse (ANOVA) und für die Beschreibung einer Normalverteilung."
  },
  {
    "objectID": "eda-descriptive.html#standardabweichung",
    "href": "eda-descriptive.html#standardabweichung",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.4 Standardabweichung",
    "text": "14.4 Standardabweichung\nDie Standardabweichung ist die Wurzel der Varianz. Wo die Varianz die Abweichung der Sprungweite in [cm\\(^2\\)] beschreibt, beschreibt die Standardabweichung die Streung der Sprungweite in [cm].\n\\[\ns = \\sqrt{s^2} = \\sqrt{4.6} = 2.14\n\\] Wir schreiben immer den Mittelwert plusminus die Standardabweichung. Also immer \\(\\bar{y} \\pm s\\).\nWir können also schreiben, dass die Flöhe im Mittel 8.13 \\(\\pm\\) 2.14cm weit springen. Somit haben wir die Lage und die Streuung der Zahlenreihe \\(y\\) der Sprungweite in [cm] mit zwei Zahlen beschrieben.\nIn R können wir die Standardabweichung einfach mit der Funktion sd() berechnen.\n\ny %>% sd %>% round(2) \n\n[1] 2.14"
  },
  {
    "objectID": "eda-descriptive.html#mittelwert-und-varianz---eine-herleitung",
    "href": "eda-descriptive.html#mittelwert-und-varianz---eine-herleitung",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.5 Mittelwert und Varianz - eine Herleitung",
    "text": "14.5 Mittelwert und Varianz - eine Herleitung\nWas ist der Mitelwert und die Varianz genau? Schauen wir uns das einmal in Abbildung 14.2 an. Die graue Linie oder Grade beschreibt den Mittelwert der fünf Beobachtungen. Die fünf Beobachtungen sind als blaue Punkt dargestellt. Auf der x-Achse ist nur der Index des Punktes. Das heißt \\(y_1\\) ist der erste Punkte, das der Index \\(i\\) gleich 1 ist.\n\n\nAbbildung 14.2— Die graue Linie beschreibt den Mittelwert der genau so durch die blauen Punkte geht, dass die Abstände der Punkte oberhalb und unterhalb zu Null aufaddieren. Die Linie liegt in der Mitte der Punkte. Die quadrierten Abstände sind die Varainz der blauen Punkte. Auf der x-Achse ist der Index des Punktes eingetragen.\n\n\nWenn wir die Summe der Abweichungen von \\(y_1\\) bis \\(y_5\\) zu dem Mittelwert bilden, so wird diese Summe 0 sein. Der Mittelwert liegt genau in der Mitte der Punkte. In unserem Beispiel ist der Mittelwert \\(\\bar{y} = 5.8\\). Wi können jetzt die Abstände wie in der folgenden Tabelle 14.1 berechnen.\n\n\nTabelle 14.1— Zusammenhang von den Werten von \\(y\\), dem Mittelwert sowie die Abweichung vom Mittelwert \\(\\epsilon\\)\n\n\n\n\n\n\n\n\n\nIndex \\(i\\)\n\ny\n\\(\\boldsymbol{y_i - \\bar{y}}\\)\nWert\n\\(\\boldsymbol{\\epsilon}\\)\n\n\n\n1\n5.7\n\\(y_1 - \\bar{y}\\)\n\\(5.7 - 8.13 = -2.43\\)\n\\(\\epsilon_1\\)\n\n\n2\n8.9\n\\(y_2 - \\bar{y}\\)\n\\(8.9 - 8.13 = 0.77\\)\n\\(\\epsilon_2\\)\n\n\n3\n11.8\n\\(y_3 - \\bar{y}\\)\n\\(11.8 - 8.13 = 3.67\\)\n\\(\\epsilon_3\\)\n\n\n4\n8.2\n\\(y_4 - \\bar{y}\\)\n\\(8.2 - 8.13 = 0.07\\)\n\\(\\epsilon_4\\)\n\n\n5\n5.6\n\\(y_5 - \\bar{y}\\)\n\\(5.6 - 8.13 = -2.53\\)\n\\(\\epsilon_5\\)\n\n\n6\n9.1\n\\(y_4 - \\bar{y}\\)\n\\(9.1 - 8.13 = 0.97\\)\n\\(\\epsilon_4\\)\n\n\n7\n7.6\n\\(y_5 - \\bar{y}\\)\n\\(7.6 - 8.13 = -0.53\\)\n\\(\\epsilon_5\\)\n\n\n\n\n\n\n\nWir nennen die Abstände \\(y_i - \\bar{y}\\) nach dem griechischen Buchstaben Epsilon \\(\\epsilon\\). Das \\(\\epsilon\\) soll an das \\(e\\) von Error erinnern. So meint dann Error eben auch Abweichung. Ja, es gibt hier viele Worte für das gleiche Konzept.\nWir berechnen einen Mittelwert von den Epsilons mit \\(\\bar{\\epsilon} = 0\\). Ein Mittelwert nahe Null bzw. von Null wundert uns nicht. Wir haben die Gerade ja so gebaut, das nach oben und unten die gleichen Abstände sind. Die Varianz \\(s^2\\) der \\(y\\) ist \\(s_y^2 = 4.599\\) und die Varianz von \\(\\epsilon\\) ist \\(s_{\\epsilon}^2 = 4.599\\). In beiden Fällen ist die Zahl gleich."
  },
  {
    "objectID": "eda-descriptive.html#standardfehler-oder-standard-error-se",
    "href": "eda-descriptive.html#standardfehler-oder-standard-error-se",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.6 Standardfehler oder Standard Error (SE)",
    "text": "14.6 Standardfehler oder Standard Error (SE)\nWenn wir den Mittelwert der Sprungweiten berichten dann gehört die Standardabweichung der Sprungweiten mit als beschreibendes Maß dazu. Wir berichten keinen Mittelwert ohne Standardabweichung.\nNun ist es aber so, dass der Mittelwert und die Standardabweichung von der Fallzahl abhängen. Je mehr Fallzahl bzw. Beoabchtungen wir haben, desto genauer wird der Mittelwert sein. Oder anders ausgedrückt \\(\\bar{y}\\) wird sich \\(\\mu_y\\) annähern. Das gleiche gilt auch für die Standardabweichung \\(s_y\\), die sich \\(\\sigma_y\\) mit steigender Fallzahl annähert.\nAus diesem Grund brauchen wir noch einen Fehler bzw. eine Maßzahl für die Streuung, die unabhängig von der Fallzahl ist. Wir skalieren also die Standardabweichung mit der Fallzahl indem wir die Standardbweichung durch die Wurzel der Fallzahl teilen.\n\\[\nSE = \\cfrac{s}{\\sqrt{n}} = \\cfrac{2.14}{2.65} = 0.81\n\\]\nWir müssten ein Paket in R laden um den Standardfehler zu berechnen. Das Laden von zusätzlichen Paketen wollen wir hier aber vermeiden; wir können den Standardfehler auch einfach selber berechnen.\n\nse <- sd(y)/sqrt(length(y))\nse %>% round(2)\n\n[1] 0.81\n\n\nWir erhalten einen Standardfehler von 0.81. Diese Zahl ist in dem Sinne nicht zu interpretieren, da wir hier nur Experimente losgelöst von deren Fallzahl miteinander vergleichen können. Auf der anderen Seite können wir ohne die berichtete Fallzahl nicht vom Standardfehler auf die Standardabweichung schließen.\nWir berichten den Standardfehler immer zusammen mit der Fallzahl, so dass die Standardabweichung berechnet werden kann.\nWir benötigen den Standardfehler eigentlich nicht zum Berichten von Ergebnissen. Der Standardfehler ist nicht als Zahl interpretierbar und somit eine reine statistische Größe. Tabelle 14.2 zeigt die Zusammenfassung und den Vergleich von Standardabweichung und Standardfehler.\n\n\nTabelle 14.2— Zusammenfassung und Vergleich von Standardabweichung und Standardfehler\n\n\n\n\n\nStandardabweichung\nStandardfehler\n\n\n\n… ist eine Aussage über die Streuung der erhobenen Werte einer Stichprobe.\n… ist eine Aussage über die Genauigkeit des Mittelwertes einer Stichprobe.\n\n\n… hängt von der biologischen Variabilität ab.\n… abhängig von der Messgenauigkeit\n\n\n… ist ein beschreibendes Maß.\n… ist ein statistisches Maß.\n\n\n… ist nur wenig durch die Größe der Stichprobe beineinflussbar.\n… steht im direkten Verhältnis zur Größe der Stichprobe.\n\n\n\n\n\n\nDer Standardfehler oder Standard Error (SE) oder Standard Error of the Mean (SEM) wird uns wieder beim statistischen Testen und dem t-Test begegnen.\n\\[\nT_{calc} = \\cfrac{\\bar{y_1} - \\bar{y_2}}{s_p \\cdot \\sqrt{\\tfrac{2}{n}}} \\approx \\cfrac{\\bar{y_1} - \\bar{y_2}}{SEM}\n\\]\nDer Nenner beim t-Test kann als Standardfehler gesehen werden. Wir benötigen den Standardfehler also im Kontext des statistischen Testen als eine statististische Maßzahl.\n\n\n\n\n\n\nStandardfehler wird in der Metaanalyse genutzt\n\n\n\nDer Standardfehler ist bedeutend in der Metaanalyse. Also dem gemeinsamen Auswerten von mehreren klinischen Studien. Du kannst im Buch Doing Meta-Analysis with R: A Hands-On Guide mehr darüber erfahren. Wir nutzen keine Metaanalysen in den Grundlagenveranstaltungen."
  },
  {
    "objectID": "eda-descriptive.html#median",
    "href": "eda-descriptive.html#median",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.7 Median",
    "text": "14.7 Median\nWir wollen uns jetzt noch eine andere Art der Zusammenfassung von Zahlen anschauen. Anstatt mit den Zahlen zu rechnen, sortieren wir jetzt die Zahlen aus dem Vektor \\(y = \\{5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6\\}\\) nach dem Rang. Wir rechnen dann mit den Rängen. Die kleinste Zahl kriegt den kleinsten Rang. Wir können R nutzen über due Funktion sort() um den Vektor \\(y\\) zu sortieren.\n\ny %>% sort()\n\n[1]  5.6  5.7  7.6  8.2  8.9  9.1 11.8\n\n\nDer Median \\(\\tilde{y}\\) ist die mittlere Zahl eines Zahlenvektors. Wir haben hier sieben Zahlen, also ist der Median die vierte Zahl. Wir müssen hier aber zwischen einr ungeraden Anzahl und einer geraden Anzahl unterscheiden.\n\n\nUngerade Anzahl von Zahlen, der Median ist die mittlere Zahl des Vektors \\(y\\): \\[\n5.6,  5.7,  7.6,  \\underbrace{8.2,}_{Median}  8.9,  9.1, 11.8\n\\]\n\n\nIn R können wir den Median einfach mit der Funktion median()berechnen.\n\nmedian(y) \n\n[1] 8.2\n\n\n\n\nGerade Anzahl von Zahlen, der Median ist der Mittelwert der beiden mittleren Zahlen des Vektors \\(y\\): \\[\n5.6,  5.7,  7.6,  \\underbrace{8.2, 8.9,}_{Median = \\tfrac{8.2+8.9}{2}=8.55} 9.1, 11.8, \\color{blue}{13.1}\n\\]\n\n\nIn R können wir den Median wieder einfach mit der Funktion median()berechnen. Wir müssen nur die Zahl 13.1 zu dem Vektor y mit der Funktion c() hinzufügen.\n\nc(y, 13.1) %>% median() \n\n[1] 8.55\n\n\nWenn der Mittelwert stark von dem Median abweicht, deutet dies auf eine schiefe Verteilung oder aber Ausreißer in den Daten hin. Wir müssen dann in der explorativen Datenanalyse der Sachlage nachgehen\nDer Median ist eine Alternative zu dem Mitelwert. Insbesondere in Fällen, wo es sehr große Zahlen gibt, die den Mittelwert in der Aussage verzerren, kann der Median sinnvoll sein.\n\n\n\n\n\n\nMedian versus Mittelwert\n\n\n\nZur Veranschaulichung des Unterschiedes zwischen Median und Mittelwert nehmen wir die Mietpreise in New York. Der mittlere Mietpreis für eine 2-Zimmerwohnung in Manhattan liegt bei 5000$ pro Monat. In den mittleren Mietpreis gehen aber auch die Mieten der Billionaires’ Row mit ein. Der mediane Mietpreis liegt bei 4000$. Die hohen Mieten ziehen den Mittelwert nach rechts."
  },
  {
    "objectID": "eda-descriptive.html#quantile-und-quartile",
    "href": "eda-descriptive.html#quantile-und-quartile",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.8 Quantile und Quartile",
    "text": "14.8 Quantile und Quartile\nBei dem Mittelwert beschreibt die Standardabweichung die Streuung der Daten um den Mitelwert. Bei dem Median sind dies die Quartile. Die Quartile beschreiben die Streuung der Daten um den Median. Um die Quartile bestimmen zu können, teilen wir die Daten in 100 Quantile. Du kannst dir Quantile wie Prozente vorstellen. Wir schneiden die Daten also in 100 Scheiben. Das geht natürlich erst wirklich, wenn wir hundert Zahlen haben. Deshalb hilft man sich mit Quartilen - von Quarta, ein Viertel - aus. Tabelle 14.3 zeigt den Zusammenhang.\n\n\nTabelle 14.3— Zusammenfassung und Vergleich von Quantilen, Quartilen und Median\n\nQuantile\nQuartile\nMedian\n\n\n\n25% Quantile\n1\\(^{st}\\) Quartile\n\n\n\n50% Quantile\n2\\(^{nd}\\) Quartile\nMedian\n\n\n75% Quantile\n3\\(^{rd}\\) Quartile\n\n\n\n\n\nWir bestimmen die Quartile wie den Median. Wir müssen unterscheiden, ob wir eine ungerade Anzahl an Zahlen oder eine gerade Anzahl an Zahlen vorliegen haben.\n\nUngerade Anzahl von Zahlen, das 1\\(^{st}\\) Quartile ist die mittlere Zahl des unteren Mittels und das 3\\(^{rd}\\) Quartile ist die mittlere Zahl des oberen Mittels des Vektors \\(y\\): \\[\n5.6,  \\underbrace{5.7,}_{1st\\ Quartile}  7.6,  8.2,  8.9,  \\underbrace{9.1,}_{3rd\\ Quartile} 11.8\n\\]\nGerade Anzahl von Zahlen, das 1\\(^{st}\\) Quartile ist der Mittelwert der beiden mittleren Zahl des unteren Mittels und das 3\\(^{rd}\\) Quartile ist der Mitelwert der beiden mittleren Zahlen des oberen Mittels des Vektors \\(y\\): \\[\n5.6,  \\underbrace{5.7, 7.6,}_{1st\\ Quartile = \\tfrac{5.7+7.6}{2}=6.65}    8.2,  8.9,  \\underbrace{9.1, 11.8}_{3rd\\ Quartile = \\tfrac{9.1+11.8}{2}=10.45} \\color{blue}{13.1}\n\\]\n\nDas 95% Quantile und das 97.25% Quantile werden wir später nochmal im statistischen Testen brauchen. Auch hier ist die Idee, dass wir die Daten in hundert Teile schneiden und uns dann die extremen Zahlen anschauen.\nIn R können wir den Median einfach mit der Funktion quantile() berechnen. Wir berechnen hier das 25% Quantile also das 1\\(^{st}\\) Quartile sowie das 50% Quantile also den Median und das 75% Quantile also das 3\\(^{rd}\\) Quartile.\n\ny %>% quantile(probs = c(0.25, 0.5, 0.75)) %>% round(2)\n\n 25%  50%  75% \n6.65 8.20 9.00 \n\nc(y, 13.1) %>% quantile(probs = c(0.25, 0.5, 0.75)) %>% round(2) \n\n 25%  50%  75% \n7.12 8.55 9.77 \n\n\nWarum unterscheiden sich die händisch berechneten Quartile von den Quartilen aus R? Es gibt verschiedene Arten der Berechnung. In der Klausur nutzen wir die Art und Weise wie die händische Berechnung hier beschrieben ist. Später in der Anwendung nehmen wir die Werte, die R ausgibt. Die Abweichungen sind so maginal, dass wir diese Abweichungen in der praktischen Anwendung ignorieren wollen."
  },
  {
    "objectID": "eda-descriptive.html#interquartilesabstand-iqr",
    "href": "eda-descriptive.html#interquartilesabstand-iqr",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.9 Interquartilesabstand (IQR)",
    "text": "14.9 Interquartilesabstand (IQR)\nDer Interquartilesabstand (IQR) beschreibt den Abstand zwischen dem 1\\(^{st}\\) Quartile und dem 3\\(^{rd}\\) Quartile. Daher ist der Interquartilesabstand (IQR) ähnlich der Spannweite zwischen dem maximalen und minimalen Wert. Wir benötigen das Interquartilesabstand (IQR) in der explorativen Datenanalyse wenn wir einen Boxplot erstellen wollen.\n\\[\nIQR = 3^{rd}\\,\\mbox{Quartile} - 1^{st}\\,\\mbox{Quartile} = 9.1 - 5.7 = 3.4\n\\]\nWir verwenden das IQR als Zahl eher selten.\n\n\n\n\n\n\nParametrik versus Nicht-Parametrik\n\n\n\nWenn wir einen Zahlenvektor wie durch \\(y = \\{5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6\\}\\) beschrieben zusammenfassen wollen, haben wir zwei Möglichkeiten.\n\nDie parametrische Variante indem wir mit den Zahlen rechnen und deskriptive Maßzahlen wie Mittelwert, Varianz und Standardabweichung berechnen. Diese Maßzahlen kommen aber in den Zahlen nicht vor.\nDie nicht-parametrische Variante indem wir die Zahlen in Ränge umwandeln, also sortieren, und mit den Rängen der Zahlen rechnen. Die deskriptiven Maßzahlen wären dann Median, Quantile und Quartile."
  },
  {
    "objectID": "eda-descriptive.html#sec-desc-group-by",
    "href": "eda-descriptive.html#sec-desc-group-by",
    "title": "14  Deskriptive Statistik",
    "section": "\n14.10 Zusammenfassen von Daten per Faktor",
    "text": "14.10 Zusammenfassen von Daten per Faktor\nGut und soll ich jetzt für jeden Faktorlevel überall den Mittelwert mit mean() berechnen? Geht das nicht einfacher? Ja, geht es. Im folgenden siehst du, wie du den verschiedene deskriptive Maßzahlen in einem Rutsch berechnen kannst.\n\n## Einlesen der Daten aus der Datei flea_dog_cat.xlsx\ndata_tbl <- read_excel(\"data/flea_dog_cat.xlsx\")\n\n## Berechnen der deskriptiven Statistiken \n## getrennt für beide Tierarten\ndata_tbl %>%\n  mutate(animal = as_factor(animal)) %>%\n  group_by(animal) %>%\n  summarise(mean = mean(jump_length),\n            sd = sd(jump_length),\n            median = median(jump_length),\n            quantiles = quantile(jump_length, \n                                 probs = c(0.25, 0.5, 0.75))) %>% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 6 × 5\n# Groups:   animal [2]\n  animal  mean    sd median quantiles\n  <fct>  <dbl> <dbl>  <dbl>     <dbl>\n1 dog     8.13  2.14    8.2      6.65\n2 dog     8.13  2.14    8.2      8.2 \n3 dog     8.13  2.14    8.2      9   \n4 cat     4.74  1.9     4.3      3.65\n5 cat     4.74  1.9     4.3      4.3 \n6 cat     4.74  1.9     4.3      5.75"
  },
  {
    "objectID": "eda-ggplot.html",
    "href": "eda-ggplot.html",
    "title": "15  Visualisierung von Daten",
    "section": "",
    "text": "Version vom October 13, 2022 um 15:09:28\nEin wichtiger Teil in der Analyse von Daten ist die Visualisierung. Wir glauben keine Auswertung eines mathematischen Algorithmus, wenn wir nicht die Bestätigung in einer Abbildung sehen. Daher ist die Visualisierung die Grundlage für ein fundiertes, wissenschaftliches Arbeiten. In diesem Kapitel stelle ich dir verschiedene Abbilungen vor, die uns helfen werden zu Verstehen ob es einen Zusammenhang zwischen Y und X gibt. Wir haben ein \\(y\\) vorliegen, was wir auf die y-Achse eines Graphen legen und daneben dann mehrere Variablen bzw. Spalten die wir \\(x\\) nennen. Eine der Variablen legen wir auf die x-Achse des Graphen. Nach den anderen \\(x\\) färben wir die Abbildung ein."
  },
  {
    "objectID": "eda-ggplot.html#genutzte-r-pakete-für-das-kapitel",
    "href": "eda-ggplot.html#genutzte-r-pakete-für-das-kapitel",
    "title": "15  Visualisierung von Daten",
    "section": "\n15.1 Genutzte R Pakete für das Kapitel",
    "text": "15.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, ggmosaic, \n               janitor, see, patchwork)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "eda-ggplot.html#grundlagen-in-ggplot",
    "href": "eda-ggplot.html#grundlagen-in-ggplot",
    "title": "15  Visualisierung von Daten",
    "section": "\n15.2 Grundlagen in ggplot()",
    "text": "15.2 Grundlagen in ggplot()\nIm Gegensatz zu dem Pipe-Operator %>% nutzt ggplot den Operator + um die verschiedenen ggplot Funktionen (geom_) miteinander zu verbinden.\nWir nutzen in R das R Paket ggplot2 um unsere Daten zu visualisieren. Die zentrale Idee von ggplot2 ist, dass wir uns eine Abbildung wie ein Sandwich bauen. Zuerst legen wir eine Scheibe Brot hin und legen uns dann Scheibe für Scheibe weitere Schichten übereinander. Oder die Idee eines Bildes, wo wir erst die Leinwand definieren und dann Farbschicht über Farbschicht auftragen. Das Konzept von ggplot2ist schlecht zu beschreiben deshalb habe ich auch noch zwei Videos hierfür gemacht. Um den Prozess von ggplot2 zu visualisieren…\n\n\n\n\n\n\nGrundlagen von ggplot() im Video\n\n\n\nDu findest auf YouTube Einführung in R - Teil 16.0 - Trockenübung ggplot2 simpel und einfach erklärt als Video.\nSowie auch auf YouTube Einführung in R - Teil 16.1 - Abbildungen mit ggplot in R erstellen. Idee und Konzept von ggplot als Video. Also alles nochmal als Video - vielleicht einfacher nachzuvollziehen als in einem Fließtext.\n\n\nDie Funktion ggplot() ist die zentrale Funktion, die die Leinwand erschafft auf der wir dann verschiedene Schichten aufbringen werden. Diese Schichten heißen geom. Es gibt nicht nur ein geom sondern mehrere. Zum Beispiel das geom_boxplot für die Erstellung von Boxplots, das geom_histogram für die Erstellung von Histogrammen. Die Auswahl ist riesig. Die einzelnen Schichten werden dann über den Operator + miteinander verbunden. Soviel erstmal zur Trockenübung. Schauen wir uns das ganze einmal an einem Beispiel an.\n\n15.2.1 Datenbeispiel\nWir importieren den Datensatz flea_cat_dog.xlsx und wollen einzelne Variablen visualisieren. Wir kennen den Datensatz schon aus dem Kapitel 5. Dennoch nochmal hier der Datensatz in Tabelle 15.1.\n\nflea_dog_cat_tbl <- read_excel(\"data/flea_dog_cat.xlsx\") %>% \n  mutate(animal = as_factor(animal))\n\nSpaltennamen sind in Englisch und haben keine Leerzeichen. Die Funktion clean_names() aus dem R Paket janitor ist hier eine Hilfe.\nIm folgenden ist es wichtig, dass du dir die Spaltennamen merkst. Wir können nur die exakten, wortwörtlichen Spaltennamen verwenden. Sonst erhalten wir einen Fehler. Deshalb haben wir auch keine Leerzeichen in den Spaltennamen.\n\n\n\n\nTabelle 15.1— Beispieldatensatz für Eigenschaften von Flöhen von zwei Tierarten.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\ncat\n3.2\n12\n7\n1\n\n\ncat\n2.2\n13\n5\n0\n\n\ncat\n5.4\n11\n7\n0\n\n\ncat\n4.1\n12\n6\n0\n\n\ncat\n4.3\n16\n6\n1\n\n\ncat\n7.9\n9\n6\n0\n\n\ncat\n6.1\n7\n5\n0\n\n\n\n\n\n\n\n15.2.2 Erste Abbildung in ggplot()\nDer folgende R Code erstellt die Leinwand in der Abbildung 15.1 für die folgende, zusätzliches Schichten (geom).\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal , y = jump_length))\n\nWir schauen uns einmal den Code im Detail an.\n\n\nggplot ruft die Funktion auf. Die Funktion ist dafür da den Plot zu zeichnen.\n\ndata = flea_dog_cat_tbl bennent den Datensatz aus dem der Plot gebaut werden soll.\n\naes()ist die Abkürzung für aesthetics und beschreibt, was auf die x-Achse soll, was auf die y-Achse soll sowie ob es noch andere Faktoren in den Daten gibt.\n\n\nx braucht den Spaltennamen für die Variable auf der x-Achse.\n\ny braucht den Spaltennamen für die Variable auf der y-Achse.\n\n\n\nFaktoren meint hier andere Gruppenvariablen. Variablen sind ein anderes Wort für Spalten. Also Variablen die wir mit as_factorerschaffen haben.\n\n\n\n\nAbbildung 15.1— Leere ggplot() Leinwand mit den Spalten animal und jump_length aus dem Datensatz flea_dog_cat_tbl.\n\n\n\n\nWir sehen, dass wir nichts sehen in Abbildung 15.1. Der Grund ist, dass wir noch kein geom hinzugefügt haben. Das geom beschreibt nun wie die Zahlen in der Datentabelle flea_dog_cat_tbl visualisiert werden sollen."
  },
  {
    "objectID": "eda-ggplot.html#häufig-verwendete-abbildungen",
    "href": "eda-ggplot.html#häufig-verwendete-abbildungen",
    "title": "15  Visualisierung von Daten",
    "section": "\n15.3 Häufig verwendete Abbildungen",
    "text": "15.3 Häufig verwendete Abbildungen\nIn diesem Kapitel wollen wir durch die häufigsten und wichtigsten Abbildungen in der explorativen Datenanalyse durchghen. Das wären im folgenden diese Abbildungen:\n\n\nHistogramm in Kapitel 15.3.1 für mehr als 20 Beobachtungen (pro Gruppe). Wir nutzen ein Histogramm um die Verteilung einer Variable zu visualisieren.\n\nBoxplot in Kapitel 15.3.3 für 5 bis 20 Beobachtungen (pro Gruppe). Ebenso wie bei einem Histogramm, geht es bei einem Boxplot auch um die Verteilung der einer Variable.\n\nBarplot in Kapitel 15.3.4 für 5 und mehr Beobachtungen (pro Gruppe). Der Barplot oder das Balkendiagramm stellt den Mitelwert und die Standardabweichung da.\n\nDotplot in Kapitel 15.3.5 für 3 bis 5 Beobachtungen (pro Gruppe). Hier geht es weniger um die Verteilung der Variable, sondern darum die wenigen Beobachtungen zu visualisieren.\n\nScatterplot in Kapitel 15.3.6 für zwei kontinuierliche Variablen. Auch xy-Plot genannt. Die Abbildung, die dir bekannt sein müsste. Wir zeichnen hier eine Grade durch eine Punktewolke.\n\nMosaicplot in Kapitel 15.3.7 für zwei diskrete Variablen. Eine etwas seltene Abbildung, wenn wir Variablen abbilden wollen, die diskret sind bzw. aus Kategorien bestehen.\n\nKonkret ist eine Variable gleich einer Spalte in einem Datensatz.\n\n\n\n\n\n\nHistogramm, Boxplot, Scatterplot und Mosaicplot im Video\n\n\n\nDu findest auf YouTube Einführung in R - Teil 16.2 - Histogramm, Boxplot, Scatterplot und Mosaicplot mit ggplot in R als Video. Weitere Videos werden dann noch folgen und ergänzt.\n\n\n\n15.3.1 Histogramm\nWir nutzen für die Erstellung eines Histogramms den Datensatz dog_fleas_hist.csv. Wir brauchen für ein anständiges Histogramm, wo du auch was erkennen kannst, mindestens 20 Beobachtung. Am besten mehr noch mhr Beobachtungen. Deshalb schauen wir uns jetzt einmal 39 Hunde an und zählen wieviele Flöhe die Hunde jeweils haben, dargestellt in der Spalteflea_count. Darüber hinaus bestimmen wir auch noch das mittlere Gewicht der Flöhe auf dem jeweiligen Hund, dargestellt in der Spalte flea_weight.\n\ndog_fleas_hist_tbl <- read_csv(\"data/dog_fleas_hist.csv\")\n\n\n\n\n\nTabelle 15.2— Beispieldatensatz für die Anzahl an Flöhen auf 39 Hunden. Gezählt wurde die Anzahl an Flöhen flea_count und das gemittelte Gewicht der Flöhe flea_weight.\n\nflea_count\nflea_weight\n\n\n\n0\n0.00\n\n\n1\n7.43\n\n\n4\n21.04\n\n\n2\n20.07\n\n\n1\n21.90\n\n\n0\n0.00\n\n\n2\n24.96\n\n\n1\n27.08\n\n\n5\n16.58\n\n\n1\n19.92\n\n\n0\n0.00\n\n\n0\n0.00\n\n\n2\n24.63\n\n\n4\n21.64\n\n\n3\n20.97\n\n\n1\n23.15\n\n\n0\n0.00\n\n\n3\n14.91\n\n\n1\n19.39\n\n\n2\n17.66\n\n\n1\n19.15\n\n\n1\n25.10\n\n\n2\n26.38\n\n\n2\n19.33\n\n\n2\n13.29\n\n\n1\n17.81\n\n\n0\n0.00\n\n\n2\n23.56\n\n\n1\n18.64\n\n\n1\n15.64\n\n\n3\n19.88\n\n\n1\n18.40\n\n\n1\n25.17\n\n\n0\n0.00\n\n\n0\n0.00\n\n\n\n\n\n\nTabelle 15.2 zeigt den Datensatz dog_fleas_hist.csv. Wir wollen jetzt die Variable flea_count und flea_weight jeweils abbilden. Wir beginnen mit der diskreten Variable flea_count. Im Gegensatz zu der Variable flea_weight haben wir bei der Anzahl gleiche Zahlen vorliegen, die wir dann zusammen darstellen können. Abbildung 15.2 zeigt die Darstellung der Tabelle. Auf der x-Achse ist die Anzahl an Flöhen dargestellt. Auf der y-Achse die Anzahl der jeweiligen Anzahl an Flöhen. Das klingt jetzt etwas schief, aber schauen wir uns die Abbilung näher an.\n\n\n\n\nAbbildung 15.2— Die Anzahl von Flöhen auf 39 Hunden. Jeder Punkt entspricht einem Hund und der entsprechenden Anzahl an Flöhen auf dem Hund.\n\n\n\n\nWir sehen in Abbildung 15.2 das acht Hunde keine Flöhe hatten - also eine Anzahl an Flöhen von 0. Auf der anderen Seite hatten zwei Hunde vier Flöhe und ein Hund hatte sogar fünf Flöhe. Wir sehen also die Verteilung der Anzahl an Flöhen über alle unsere 39 Hundebeobachtungen.\nWir schauen uns aber die Verteilung der Anzahl an Flöhen meist nicht in der Form von gestapelten Punkten an, sondern in der Form eines Histogramms also einem Balkendiagramm. Abbildung 15.3 zeigt das Histogramm für die Anzahl der Flöhe.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_count)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Anzahl Flöhe\", y = \"Anzahl\") \n\n\n\nAbbildung 15.3— Histogramm der Anzahl von Flöhen auf 39 Hunden.\n\n\n\n\nWas sehen wir in der Abbildung 15.3? Anstatt von gestapelten Punkten sehen wir jetzt Balken, die die jeweilige Anzahl an Flöhen zusammenfassen. Der Unterschied ist bei einer diskreten Variable wie der Anzahl (eng. count) relativ gering.\nAnders sieht es für kontenuierliche Variablen mit Kommazahlen aus. Schauen wir uns das Gewicht der Flöhe an, so sehen wir, dass es sehr viele Zahlen gibt, die nur einmal vorkomen. Abbildung 15.4 zeigt das Histogramm für das Geicht der Flöhe.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_weight)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Gewicht [mg]\", y = \"Anzahl\") \n\n\n\nAbbildung 15.4— Histogramm des Gewichts von Flöhen auf 39 Hunden.\n\n\n\n\nWie entsteht nun ein Hisotgramm für konetnierliche Zahlen? Schauen wir uns dafür einmal ein kleineres Datenbeispiel an, in dem wir nur Flöhe mit einem Gewicht größer als 11 und kleiner als 19 wäheln. Wir nutzen dazu die Funktion filter(flea_weight > 11 & flea_weight < 19). Wir erhalten folgende Zahlen und das entsprechende Histogramm.\n\n\n[1] 13.29 14.91 15.64 16.58 17.66 17.81 18.40 18.64\n\n\n\n\nAbbildung 15.5— Zusammenhang zwischen den einzelnen Beobachtungen und der Höhe der einzelnen Balken am Beispiel von acht Hunden.\n\n\n\n\nAbbildung 15.5 zeigt das Histogramm der reduzierten Daten. Die roten vertikalen Linien zeigen die Position der einzelnen Flohgewichte auf der x-Achse. Die blauen Hilfslinien machen nochmal klarer, wie hoch die einzelnen Balken sind sowie welche Beobachtungen auf der x-Achse in den jeweiligen Balken mit eingehen. Wir sehen, dass wir einen Hund mit Flöhen haben, die zwischen 12.5 und 13.5 wiegen - der entsprechende Balken erhält die Anzahl von eins. Auf der anderen Seite sehen wir, dass es drei Hunde mit Flöhen, die zwischen 17.5 und 18.5 wiegen. Daher wächst der Balken auf eine Anzahl von drei.\nWir können mit der Option binwidth in dem geom_histogram() einstellen, wie breit auf der x-Achse die jeweiligen Balken sein sollen. Hier empfiehlt es sich verschiedene Zahlen für binwidthauszuprobieren.\n\n15.3.2 Density Plot\nEine weitere Möglichkeit sich eine Verteilung anzuschauen, ist die Daten nicht als Balkendiagramm sondern als Densityplot - also Dichteverteilung - anzusehen. Im Prinzip verwandeln wir die Balken in eine Kurve. Damit würden wir im Prinzip unterschiedliche Balkenhöhen ausgleichen udn eine “glattere” Darstellung erreichen. Wir wir aber gleich sehen werden, benötigen wir dazu eine Menge an Beoabchtungen und auch dann ist das Ergebnis eventuell nicht gut zu interpretieren.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_count)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Anzahl Flöhe\", y = \"Anzahl\")\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_count)) +\n  geom_density(fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Anzahl Flöhe\", y = \"Häufigkeit\") \n\n\n\n\n\n(a) Histogramm\n\n\n\n\n\n\n(b) Densityplot\n\n\n\n\nAbbildung 15.6— Zusammenhang von Histogramm und Densityplot an der Anzahl der Flöhe auf 39 Hunden.\n\n\n\nAbbildung 15.6 zeigt auf der linken Seite erneut die Abbildung des Histogramms als Balkendiagramm für die Anzahl der Flöhe auf den 39 Hunden. Auf der rechten Seite die entsprechenden gleichen Daten als Denistyplot. Klar ist die Wellenbewegung des Densityplots zu erkennen. Hier leigen zu wenige Beobachtungen und Kategorien auf der x-Achse vor, so dass der Densityplot nicht zu empfehlen ist.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_weight)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Gewicht [mg]\", y = \"Anzahl\") \n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_weight)) +\n  geom_density(fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Gewicht [mg]\", y = \"Häufigkeit\") \n\n\n\n\n\n(a) Histogramm\n\n\n\n\n\n\n(b) Densityplot\n\n\n\n\nAbbildung 15.7— Zusammenhang von Histogramm und Densityplot am Gewicht der Flöhe auf 39 Hunden.\n\n\n\nAbbildung 15.7 zeigt auf der linken Seite erneut die Abbildung des Histogramms als Balkendiagramm für das Gewicht der Flöhe auf den 39 Hunden. Insbesondere bei dieser Abbildung erkennst du die Nachteile des Densityplot. Dadurch das es einen Peak von acht Hunden mit einem Flohgewicht von 0 gibt, zeigt der Densityplot eine seltsame Wellenform. Es emppfielt sich daher die Daten zuerst als Histogramm zu betrachten.\n\n15.3.3 Boxplot\nIn Kapitel 14 haben wir den Median und die Quartile kennengelernt. Mit dem Boxplot können wir den Median und die Quartile visualisieren. In Abbildung 15.8 sehen wir einen Boxplot, der den Median und die Quartile visualisiert. Die Box wird aus dem IQR gebildet. Der Median wird als Strich in der Box gezeigt. Die Schnurrhaare (eng. Whiskers) sind das 1.5 fache des IQR. Punkte die außerhalb der Schnurrhaare liegen werden als einzelne Punkte dargestellt. Diese einzelnen Punkte werden auch als Ausreißer (eng. Outlier) bezeichnet.\n\n\n\nAbbildung 15.8— Ein Boxplot der die statistischen Maßzahlen Median und Quartile visualisiert. Die Box wird aus dem IQR gebildet. Der Median wird als Strich in der Box gezeigt. Die Schnurrhaare sind das 1.5 fache des IQR. Punkte die außerhalb der Schnurrhaare liegen werden als einzele Punkte dargestellt.\n\n\n\nIn Abbildung 15.9 sehen wir den Zusammenhang zwischen einem Histogramm, Densityplot und dem Boxplot. Der Median \\(\\tilde{y}\\) im Boxplot zeigt die höchste Stelle des Densityplots an. Durch einen Boxplot kann die Verteilung der entsprechenden Zahlen abgeschätzt werden.\n\n\nAbbildung 15.9— Der Zusammenhang von Histogram, Densityplot und Boxplot.\n\n\nDie “liegende” Darstellung des Boxplots dient nur der Veranschaulichung und dem Verständnis des Zusammenhangs von Histogramm und Boxplot. In der Abbildung 15.10 sehen wir drei Boxplots für einen Faktor mit drei Leveln. Jedes Level wird duch einen Boxplot dargestellt. Zum Beispiel eine Düngerbehandlung mit drei Konzentrationen. Auf der x-Achse würden wir die Behandelung finden und auf der y-Achse das Trockengewicht in [kg/ha].\n\n\nAbbildung 15.10— Typische Darstellung von drei Gruppen jeweils dargestellt durch einen Boxplot. Boxplots werden in der Anwendung stehtend dargestellt. Insbesondere wenn die Boxplots mehrere Gruppen repräsentieren.\n\n\nWie erstellen wir nun einen Boxplot in R? Zuerst laden wir die Daten mit der Funktion read_excel() in R, wenn du die Daten als .xlsx Datei vorliegen hast. Im XX kannst du nochmal das Importieren von Daten wiederholen.\n\nflea_dog_cat_tbl <- read_excel(\"data/flea_dog_cat.xlsx\")\n\n\n\n\n\nAbbildung 15.11— An 39 Hunden wurde die Anzahl an Flöhen gezählt.\n\n\n\n\nIn Abbildung 15.11 ist der Boxplot für die Daten aus der Datei flea_dog_cat.xlsx dargestellt. Auf der x-Achse finden wir die Tierart als cat und dog. Auf der y-Achse ist die Sprungweite in [cm] dargestellt.\nWir erkennen auf einen Blick, dass die Sprungweite von den Hundeflöhen weiter ist als die Sprungweite der Katzenflöhe. Im Weiteren können wir abschätzen, dass die Streuung etwa gleich groß ist. Die Boxen sind in etwa gleich groß und die Whiskers in etwa gleich lang.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = jump_length,\n                                    fill = animal)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.25, shape = 1) +\n  theme_bw() +\n  labs(x = \"Tierart\", y = \"Sprungweite [cm]\") \n\n\n\nAbbildung 15.12— An 39 Hunden wurde die Anzahl an Flöhen gezählt.\n\n\n\n\nWir neigen dazu die Boxplots überzuinterpretieren, wenn die Anzahl der Beobachtungen klein ist. Deshalb können wir mit dem geom_jitter() noch die Beobachtungen zu den Boxplot ergänzen, dargestellt in Abbildung 15.12. Die Funktion geom_jitter() streut die Punkte zufällig, so dass keine Punkte übereinanderliegen. Wir haben hier die Streuuweite durch die Option width = 0.25 etwas eingeschränkt. Darüber hinaus habe wir das Aussehen der Punkte mit shape = 1 geändert, so dass wir die Jitter-Punkte von den potenziellen Ausreißer-Punkten unterscheiden können. Du kannst auch andere Zahlen hinter shape eintragen um verschiedene Punktesymbole durchzuprobieren. Eine Übersicht an shapes findest du auch hier unter Cookbook for R > Graphs > Shapes and line types.\n\n15.3.4 Barplot oder Balkendiagramm\nDer Barplot oder das Balkendiagramm ist eigentlich veraltet. Wir haben mit dem Boxplot eine viel bessere Methode um eine Verteilung und gleichzeitig auch die Gruppenunterschiede zu visualisieren. Warum nutzen wir jetzt so viel den Braplot? Das hat damit zu tun, dass früher - oder besser bis vor kurzem - in Excel kein Boxplot möglich war. Daher nutzte jeder der mit Excel seine Daten auswertet den Barplot. Und was der Bauer nicht kennt…\n\n\n\nAlles was es schon gab, als Du geboren wurdest, ist normal und gewöhnlich. Diese Dinge werden als natürlich wahrgenommen und halten die Welt am Laufen.\nAlles was zwischen Deinem 16ten und 36ten Lebensjahr erfunden wird ist neu, aufregend und revoltionär. Und vermutlich kannst Du in dem Bereich sogar Karriere machen.\nAlles was nach dem 36ten Lebensjahr erfunden wird ist gegen die natürliche Ordnung der Dinge.\n\n– Douglas Adams aus Per Anhalter durch die Galaxis\nDeshalb ist hier auch der Barplot dargestellt. Ich persönlich mag den Barplot übrhaupt nicht. Der Barplot ist einfach schlechter als der Boxplot. Aber gut, häufog musst du den Barplot in deiner Abschlussarbeit machen. Also dann hier der Barplot.\nWie erstellen wir nun einen Barplot in R? Zuerst laden wir die Daten mit der Funktion read_excel() in R, wenn du die Daten als .xlsx Datei vorliegen hast. Im Kapitel 12 kannst du nochmal das Importieren von Daten wiederholen.\n\nflea_dog_cat_tbl <- read_excel(\"data/flea_dog_cat.xlsx\")\n\nWir müssen jetzt für ggplot() noch den Mittelwert und die Streuung für die Gruppen berechnen. Ein komplexeres Beispiel für einen Barplot findets du in Kapitel A. Du kanst als Streuung die Standardabweichung oder den Standardfehler nehmen. Ich würde die Standardabweichung bei kleinen Fallzahlen kleiner als 20 Beobachtungen nehmen.\n\nstat_tbl <- flea_dog_cat_tbl %>% \n  group_by(animal) %>% \n  summarise(mean = mean(jump_length),\n            sd = sd(jump_length),\n            se = sd/sqrt(n()))\n\nWir nutzen nun das Objekt stat_tbl um den Barplot mit der Funktion ggplot() zu erstellen. Dabei müssen wir zum einen schauen, dass die Balken nicht übereinander angeordnet sind. Nebeneinander angeordnete Balken kriegen wir mit der Option stat = \"identity\" in dem geom_bar(). Dann müssen wir noch die Fehlerbalken ergänzen mit dem geom_errorbar. Hier kann nochmal mit der Option width = an der Länge der Fehlerenden gedreht werden.\n\nggplot(stat_tbl, aes(x = animal, y = mean, fill = animal)) + \n    geom_bar(stat = \"identity\") +\n    geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                  width = 0.2)\n\n\n\n\nIm Zweifel muss du nochmal googlen und schauen welche Form dir am besten zusgat. Es gibt sehr viele Möglichkeiten einen Barplot zu erstellen. Daher komm im Zweifel einmal ins R Tutorium.\n\n15.3.5 Dotplot\nWenn wir weniger als fünf Beobachtungen haben, dann ist meist ein Boxplot verzerrend. Wir sehen eine Box und glauben, dass wir viele Datenpunkte vorliegen haben. Bei 3 bis 7 Beobachtungen je Gruppe bietet sich der Dotplot als eine Lösung an. Wir stellen hier alle Beobachtungen als einzelne Punkte dar.\nWie erstellen wir nun einen Dotplot in R? Zuerst laden wir die Daten mit der Funktion read_excel() in R, wenn du die Daten als .xlsx Datei vorliegen hast. Im XX kannst du nochmal das Importieren von Daten wiederholen.\n\nflea_dog_cat_tbl <- read_excel(\"data/flea_dog_cat.xlsx\")\n\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = grade,\n                                    fill = animal)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  theme_bw() +\n  labs(x = \"Tierart\", y = \"Boniturnote [1-9]\") \n\n\n\nAbbildung 15.13— Der Dotplot für die Anzahl der Flöhe für die beiden Tierarten Hund und Katze.\n\n\n\n\nIn Abbildung 15.13 sehen wir den Dotplot aus der Datei flea_dog_cat.xlsx. Auf der x-Achse sind die Level des Faktors animal dargestellt und auf der y-Achse die Notenbewertung grade der einzelnen Hunde und Katzen. Die Funktion geom_dotplot() erschafft das Layer für die Dots bzw. Punkte. Wir müssen in der Funktion noch zwei Dinge angeben, damit der Plot so aussieht, dass wir den Dotplot gut interpretieren können. Zum einen müssen wir die Option binaxis = y wählen, damit die Punkte horizontal geordent werden. Zum anderen wollen wir auch, dass die Punkte zentriert sind und nutzen dafür die Option stackdir = center.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = grade,\n                            fill = animal)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  stat_summary(fun = median, fun.min = median, fun.max = median,\n               geom = \"crossbar\", width = 0.5) +\n  theme_bw() +\n  labs(x = \"Tierart\", y = \"Boniturnote [1-9]\") \n\n\n\nAbbildung 15.14— Der Dotplot für die Anzahl der Flöhe für die beiden Tierarten Hund und Katze. Die schwarze Linie stelt den Median für die beiden Tierarten dar.\n\n\n\n\nNun macht es wenig Sinn bei sehr wenigen Beobachtungen noch statistische Maßzahlen mit in den Plot zu zeichnen. Sonst hätten wir auch gleich einen Boxplot als Visualisierung der Daten wählen können. In Abbildung 15.14 sehen wir die Ergänzung des Medians. Hier müssen wir etwas mehr angeben, aber immerhin haben wir so eine Idee, wo die “meisten” Beobachtungen wären. Aber auch hier ist Vorsicht geboten. Wir haben sehr wenige Beobachtungen, so dass eine Beobachtung mehr oder weniger große Auswirkungen auf den Median und die Interpretation hat.\n\n15.3.6 Scatterplot\nDer Scatterplot wird auch xy-Plot genannt. Wir stellen in einem Scatterplot zwei kontenuierliche Variablen dar. Dann wollen wir eine Linie durch die Punkte legen. Im Prinzip fragen wir uns, wie hänge die Werte auf der y-Achse von den Werten auf der x-Achse ab? Wenn sich also die Werte auf der x-Achse erhöhen, wie verhalten sich dann die Werte auf der y-Achse?\n\nggplot(data = flea_dog_cat_tbl, aes(x = flea_count, y = jump_length)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  labs(x = \"Anzahl der Flöhe\", y = \"Sprungweite in [cm]\") \n\n\n\nAbbildung 15.15— Zusammenhang zwischen der Sprungweite in [cm] und der Anzahl an Flöhen auf den 39 Hunden. Jeder Punkt stellt einen Hund dar.\n\n\n\n\nWie du die Gerade richtig mit den statistischen Maßzahlen beschriftest findest du dann im Kapitel 30 zur linearen Regression.\nAbbildung 15.15 zeigt den Scatterplot für die Spalte flea_count auf der x-Achse und jump_length auf der y-Achse. Mit der Funktion geom_point() können wir die Punktepaare für jede Beobachtung zeichnen. In unserem Fall zeichnen wir mit der Funktion stat_smooth() noch die entsprechende Grade durch die Punkte. Es handelt sich hierbei um eine Regression. Du kannst im Kapitel 30 mehr darüber erfahren.\n\n15.3.7 Mosaic Plot\nWenn wir zwei Spalten visualisieren wollen, die aus zwei Faktoren bestehen mit jeweils zwei Leveln, dann nutzen wir den Mosaic Plot. Wir nutzen den Datensatz flea_dog_cat.xlsx mit vierzehn Beobachtungen. Zuerst drehen wir nochmal die Ordnung der Level in dem Faktor animal.\n\nflea_dog_cat_mosaic_tbl <- flea_dog_cat_tbl %>%\n  mutate(animal = factor(animal, levels = c(\"dog\", \"cat\"))) \n\nSchauen wir uns jetzt einmal die 2x2 Kreuztabelle der beiden Spalten animal and infected an. Um die 2x2 Tabelle in R in der richtigen Orientierung vorliegen zu haben, müssen wir nochmal einen kleinen Klimmzug über mutate() nehmen. Wir wandeln die Variable infected in einen Faktor um und sortieren die Level entsprechend, so dass wir die richtige Ordnung wie später im Mosaic Plot haben. Dieser Umweg hat nur didaktische Gründe, später plotten wir den Mosaic Plot direkt und schauen uns vorher nicht die 2x2 Tabelle in R an. Hier also die 2x2 Kreuztablle aus R.\n\nflea_dog_cat_mosaic_tbl %>% \n  mutate(infected = factor(infected, levels = c(1, 0))) %>% \n  tabyl(infected, animal) \n\n infected dog cat\n        1   3   2\n        0   4   5\n\n\nWir sehen in der Tabelle, dass wir mehr uninfizierte Tiere (n = 9) als infizierte Tiere haben (n = 5). Die Aufteilung zwischen den beiden Tierarten ist nahezu gleich. Im folgenden wollen wir diese Tabelle durch einen Mosaic Plot einmal visualisieren.\nUm jetzt einen Mosaic Plot zeichnen zu können müssen wir die relativen Anteile pro Spalte bzw. für jedes Level von \\(x\\) berechnen. In unserem Fall ist \\(x\\) die Variable animal und die Level sind dog und cat. In der folgenden 2x2 Kreutablle werden die relativen Anteile für die Hunde- und Katzenflöhe für den Infektionsstatus berechnet.\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal\n\n\n\n\n\n\nDog\nCat\n\n\n\nInfected\nYes (1)\n\\(\\cfrac{3}{7} = 0.43\\)\n\\(\\cfrac{2}{7} = 0.29\\)\n\\(\\mathbf{5}\\)\n\n\n\nNo (0)\n\\(\\cfrac{4}{7} = 0.57\\)\n\\(\\cfrac{5}{7} = 0.71\\)\n\\(\\mathbf{9}\\)\n\n\n\n\n\\(\\mathbf{7}\\)\n\\(\\mathbf{7}\\)\n\\(n = 14\\)\n\n\n\nWir können jetzt die relativen Anteile in den Mosaic Plot übertragen und erhalten die Abbildung 15.16. Wir müssen also zuerst die absoluten Anteile bestimmen um dann die relativen Anteile für die Spalten berechnen zu können. Abschließend zeichnen wir dann den Mosaic Plot. Wir nutzen dafür das R Paket ggmosaic mit der Funktion geom_mosaic().\n\nggplot(data = flea_dog_cat_mosaic_tbl) +\n  geom_mosaic(aes(x = product(infected, animal), fill = animal)) +\n  annotate(\"text\", x = c(0.25, 0.25, 0.75, 0.75), \n                   y = c(0.25, 0.75, 0.25, 0.85), \n           label = c(\"0.57\", \"0.43\", \"0.71\", \"0.29\"), size = 7) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 15.16— Visualisierung einer 2x2 Tabelle als Mosaic Plot. Die unterschiedlich großen Flächen geben die Verhältnisse per Spalte wieder.\n\n\n\n\nAbbildung 15.16 zeigt den Mosaic Plot für die Variable animal and infected. Die untrschiedlich großen Flächen bilden die Verhältnisse der 2x2 Tabelle ab. So sehen wir, dass es mehr uninfizierte Tiere als infizierte Tiere gibt. Am meisten gibt es uninfizierte Katzen. Am wenigstens treten infizierte Katzen auf."
  },
  {
    "objectID": "eda-ggplot.html#überschriften-achsen-und-legenden",
    "href": "eda-ggplot.html#überschriften-achsen-und-legenden",
    "title": "15  Visualisierung von Daten",
    "section": "\n15.4 Überschriften, Achsen und Legenden",
    "text": "15.4 Überschriften, Achsen und Legenden\nWenn du mehr machen willst, also die Überschriften anpassen oder aber die Achsenbeschriftung ändern, dann gibt es hier global Hilfe im ggplot Manual. Die Webseite R Cookbook hat auch spezielle Hilfe für ggplot().\n\nÜberschriften von Abbildungen\nAchsenbeschriftung\nLegende\nFarben\n\nIm Kapitel 1.2 findest du Informationen zum R Tutorium, wann und wo es stattfindet.\nIn Abbildung 15.17 siehst du eine Abbildung mit Titel und veränderten Beschriftungen. Die Möglichkeiten sind nahezu unbegrenzt und sprengen auch hier den Rahmen. Im Zweifel im R Tutorium vorbeischauen oder aber in der Vorlesung fragen.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = jump_length,\n                                    fill = animal)) +\n  geom_boxplot() +\n  labs(title = \"Frischgewicht in Abhängigkeit von der Behandlung\",\n       x = \"Behandlung\", y = \"Frischgewicht in kg/ha\") +\n  scale_x_discrete(labels = c(\"Katze\", \"Hund\")) +\n  scale_fill_discrete(name = \"Behandlung\", labels = c(\"Katze\", \"Hund\")) +\n  theme_bw() \n\n\n\nAbbildung 15.17— Beispielhafte Abbildung mit Titel und geänderter Achsenbeschrittung"
  },
  {
    "objectID": "eda-ggplot.html#die-okabe-ito-farbpalette",
    "href": "eda-ggplot.html#die-okabe-ito-farbpalette",
    "title": "15  Visualisierung von Daten",
    "section": "\n15.5 Die Okabe-Ito Farbpalette",
    "text": "15.5 Die Okabe-Ito Farbpalette\n\n\nMehr zum R Paket see auf der Hilfeseite des Paketes\nNeben den klassischen Farben im R Paket ggplotgibt es noch weit, weit mehr Farbpaletten. Wir nutzen in der Folge immer wieder die Okabe-Ito Farbpalette aus dem R Paket see. Die Okabe-Ito Farbpalette ist speziell so gebaut, dass die Farben sich gut für farbenblinde Personen unterscheiden. Der Kontrast zwischen den Farben ist sehr gut. Wenn du eine andere Farbpalette nutzen willst, findest du hier noch andere Color Scales.\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal, y = jump_length,\n           fill = animal)) +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  theme_bw()\n\n\n\nAbbildung 15.18— Beispielhafte Abbildung der Okabe-Ito Farbpalette für Boxplots.\n\n\n\n\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal, y = jump_length,\n           color = animal)) +\n  geom_point() +\n  scale_color_okabeito() +\n  theme_bw()\n\n\n\nAbbildung 15.19— Beispielhafte Abbildung der Okabe-Ito Farbpalette für Punkte."
  },
  {
    "objectID": "eda-ggplot.html#abbildungen-nebeneinander",
    "href": "eda-ggplot.html#abbildungen-nebeneinander",
    "title": "15  Visualisierung von Daten",
    "section": "\n15.6 Abbildungen nebeneinander",
    "text": "15.6 Abbildungen nebeneinander\nDas R Paket patchwork erlaubt es mehrere ggplot Abbildungen nebeneinander oder in einem beliebigen Layout miteinander zu verbinden. Das tolle ist, dass die Idee sehr intutiv ist. Wir nutzen wieder das + um verschiedene Plots miteinander zu verbinden.\nIm Folgenden erschaffen wir uns zwei ggplots und speichern die Plots in den Objekten p1 und p2. Das ist wie wir es bisher kennen, nur das jetzt keine Abbildung erscheint sondern beide Plots in zwei Objekten gespeichert sind.\n\np1 <- ggplot(data = flea_dog_cat_tbl, \n             aes(x = flea_count, y = jump_length,\n                 color = animal)) +\n  geom_point() +\n  scale_color_okabeito() +\n  theme_bw()\n\np2 <- ggplot(data = flea_dog_cat_tbl, \n                aes(x = animal, y = jump_length,\n                    color = animal)) +\n  geom_point() +\n  scale_color_okabeito() +\n  theme_bw()\n\nWie können wir nun die beiden Abbildungen nebeneinander zeichnen? Wir nutzen einfach das + Symbol.\n\np1 + p2\n\n\n\nAbbildung 15.20— Beispielhafte Abbildung der zweier Plots nebeneinander.\n\n\n\n\nAuf der Seite des R Paket patchwork findest du viel mehr Möglichkeiten das Layout anzupassen und auch die einzelnen Subplots zu beschriften."
  },
  {
    "objectID": "eda-transform.html",
    "href": "eda-transform.html",
    "title": "16  Transformieren von Daten",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:37:39\nWarum müssen wir Daten transformieren? Meistens hat dies zwei Hauptgründe.\nIm ersten Fall wollen wir meist unsere Daten \\(log\\)-Transformieren um aus einem nicht-normalverteilten Outcome \\(y\\) ein \\(log\\)-normalverteiltes \\(y\\) zu erschaffen. Im zweiten Fall wollen wir unsere Daten Standardisieren oder Normalisieren. Wir brauchen normalisierte Daten später beim Klassifizieren im Rahmen von maschinellen Lernverfahren.\nWir wollen uns nun die Verfahren zur Transformation von Daten in den folgenden Abschnitten einmal alle anschauen."
  },
  {
    "objectID": "eda-transform.html#genutzte-r-pakete-für-das-kapitel",
    "href": "eda-transform.html#genutzte-r-pakete-für-das-kapitel",
    "title": "16  Transformieren von Daten",
    "section": "\n16.1 Genutzte R Pakete für das Kapitel",
    "text": "16.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "eda-transform.html#daten",
    "href": "eda-transform.html#daten",
    "title": "16  Transformieren von Daten",
    "section": "\n16.2 Daten",
    "text": "16.2 Daten\nWir wollen uns in diesem Kapitel mit der normalverteilten Variable jump_length gemessen in [cm] und der nicht-normalverteilten Variable hatch_time gemessen in [h] aus dem Datensatz flea_dog_cat_length_weight.csv\" beschäftigen. Wir wählen über die Funktion select() nur die beiden Spalten aus dem Datensatz, die wir benötigen.\n\ndata_tbl <- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %>%\n  select(jump_length, hatch_time)\n\nIn der Tabelle 16.1 ist der Datensatz data_tbl nochmal dargestellt. Wir zeigen hier nur die ersten sieben zeilen des Datensatzes.\n\n\n\n\nTabelle 16.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length und der nicht-normalverteilten Variable hatch_time. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\njump_length\nhatch_time\n\n\n\n15.79\n483.60\n\n\n18.33\n82.56\n\n\n17.58\n296.73\n\n\n14.09\n140.90\n\n\n18.22\n162.20\n\n\n13.49\n167.47\n\n\n16.28\n291.20\n\n\n\n\n\n\nIm Folgenden nutzen wir oft die Funktion mutate(). Schau dir im Zweifel nochmal im Kapitel zu Programmierung die Funktion mutate() an."
  },
  {
    "objectID": "eda-transform.html#log-transformation",
    "href": "eda-transform.html#log-transformation",
    "title": "16  Transformieren von Daten",
    "section": "\n16.3 \\(log\\)-Transformation",
    "text": "16.3 \\(log\\)-Transformation\nWir nutzen die \\(log\\)-Transformation, wenn wir aus einem nicht-normalverteiltem Outcome \\(y\\) ein approxomativ normalverteiltes Outcome \\(y\\) machen wollen. Dabei ist wichtig, dass wir natürlich auch die Einheit mit \\(log\\)-transformieren.\nIm Folgenden sehen wir die \\(log\\)-Transformation der Variable hatch_time mit der Funktion log(). Wir erschaffen eine neue Spalte im tibble damit wir die beiden Variable vor und nach der \\(log\\)-Transformation miteinander vergleichen können.\n\nlog_tbl <- data_tbl %>% \n  mutate(log_hatch_time = log(hatch_time))\n\nWir können dann über ein Histogramm die beiden Verteilungen anschauen. In Abbildung 16.1 (a) sehen wir die nicht transformierte, rohe Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Abbildung 16.1 (b) zeigt die \\(log\\)-transmutierten Daten. In diesem Fall sehen wir normalverteilte Daten. Wir haben also ein \\(log\\) normalverteiltes Outcome \\(y\\) mit dem wir jetzt weiterechnen können.\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n\n(b) \\(log\\)-transformierte Daten.\n\n\n\n\nAbbildung 16.1— Histogramm der nicht transfomierten und transformierten Daten."
  },
  {
    "objectID": "eda-transform.html#quadratwurzel-transformationen",
    "href": "eda-transform.html#quadratwurzel-transformationen",
    "title": "16  Transformieren von Daten",
    "section": "\n16.4 Quadratwurzel-Transformationen",
    "text": "16.4 Quadratwurzel-Transformationen\nDie Quadratwurzel-Transformationen ist eine etwas seltenere Transformation. Meist wird die Quadratwurzel-Transformationen als die schwächere \\(log\\)-Transformation bezeichnet. Wir sehen in Abbildung 16.2 (b) den Grund dafür. Aber zuerst müssen wir aber über die Funktion sqrt() unsere Daten transformieren.\n\nsqrt_tbl <- data_tbl %>% \n  mutate(sqrt_hatch_time = sqrt(hatch_time))\n\nIn Abbildung 16.2 (a) sehen wir die nicht transformierte, rohe Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam nach rechts aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Abbildung 16.2 (b) zeigt die Wurzel-transmutierten Daten. Unser Ziel besser normalverteilte Daten vorliegen zu haben, haben wir aber mit der Quadratwurzel-Transformationen nicht erreicht. Die Daten sind immer noch rechtsschief. Wir würden also die \\(log\\)-Transformation bevorzugen.\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n\n(b) Wurzel-transformierte Daten.\n\n\n\n\nAbbildung 16.2— Histogramm der nicht transfomierten und transformierten Daten."
  },
  {
    "objectID": "eda-transform.html#standardisierung",
    "href": "eda-transform.html#standardisierung",
    "title": "16  Transformieren von Daten",
    "section": "\n16.5 Standardisierung",
    "text": "16.5 Standardisierung\nDie Standardisierung wird auch \\(z\\)-Transformation genannt. In dem Fall der Standardisierung schieben wir die Daten auf den Ursprung, in dem wir von jedem Datenpunkt \\(y_i\\) den Mittelwert \\(\\bar{y}\\) abziehen. Dann setzen wir noch die Standardabweichung auf Eins in dem wir durch die Standardabweichung \\(y_s\\) teilen. Unser standardisiertes \\(y\\) ist nun standard normalverteilt mit \\(\\mathcal{N(0,1)}\\). Wir nutzen für die Standardisiwerung folgende Formel.\n\\[\ny_z = \\cfrac{y_i - \\bar{y}}{s_y}\n\\] In R können wir für die Standardisierung die Funktion scale() verwenden. Wir müssen auch nichts weiter in den Optionen von scale() angeben. Die Standardwerte der Funktion sind so eingestellt, dass eine Stanardnormalverteilung berechnet wird.\n\nscale_tbl <- data_tbl %>% \n  mutate(scale_jump_length = scale(jump_length))\n\nIn Abbildung 16.3 (a) sehen wir nochmal die nicht transformierten, rohen Daten. Wir haben in diesem Beispiel die normalvertielte Variable jump_length gewählt. Der Mittelwert von jump_length ist 20.51 und die Standardabweichung ist 3.77. Ziehen wir nun von jedem Wert von jump_length den Mittelwert mit 19.3 ab, so haben wir einen neuen Schwerpunkt bei Null. Teilen wir dann jede Zahl durch 3.36 so haben wir eine reduzierte Spannweite der Verteilung. Es ergibt sich die Abbildung 16.3 (b) als Standardnormalverteilung. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die Sprungweite auf der \\(z\\)-Skala nicht mehr biologisch interpretieren.\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n\n(b) \\(z\\)-transformierte Daten.\n\n\n\n\nAbbildung 16.3— Histogramm der nicht transfomierten und transformierten Daten."
  },
  {
    "objectID": "eda-transform.html#normalisierung",
    "href": "eda-transform.html#normalisierung",
    "title": "16  Transformieren von Daten",
    "section": "\n16.6 Normalisierung",
    "text": "16.6 Normalisierung\nAbschließend wollen wir uns nochmal die Normalisierung anschauen. In diesem Fall wollen wir die Daten so transformieren, dass die Daten nur noch in der Spannweite 0 bis 1 vorkommen. Egal wie die Einheiten vorher waren, alle Variablen haben jetzt nur noch eine Ausprägung von 0 bis 1. Das ist besonders wichtig wenn wir viele Variablen haben und anhand der Variablen eine Vorhersage machen wollen. Uns interessieren die Werte in den Variablen an sich nicht, sondern wir wollen ein Outcome vorhersagen. Wir brauchen die Normalisierung später für das maschinelle Lernen und die Klassifikation. Die Formel für die Normalisierung lautet wie folgt.\n\\[\ny_n = \\cfrac{y_i - \\min(y)}{\\max(y) - \\min(y)}\n\\]\nIn R gibt es die Normalisierungsfunktion nicht direkt. Wir könnten hier ein extra Paket laden, aber bei so einer simplen Formel können wir auch gleich die Berechnung in der Funktion mutate() machen. Wir müssen nur etwas mit den Klammern aufpassen.\n\n\nnorm_tbl <- data_tbl %>% \n  mutate(norm_jump_length = (jump_length - min(jump_length))/(max(jump_length) - min(jump_length)))\n\n\nIn Abbildung 16.4 (a) sehen wir nochmal die nicht transformierten, rohen Daten. In Abbildung 16.4 (b) sehen wir die normalisierten Daten. Hier fällt dann auf, dass die normalisierten Sprungweiten nur noch Werte zwischen Null und Eins annehmen. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die normalisierten Sprungweiten nicht mehr biologisch interpretieren.\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n\n(b) Normalisierte Daten\n\n\n\n\nAbbildung 16.4— Histogramm der nicht transfomierten und transformierten Daten."
  },
  {
    "objectID": "eda-distribution.html",
    "href": "eda-distribution.html",
    "title": "\n17  Verteilung von Daten\n",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:37:44\nIn diesem Kapitel wollen wir uns mit Verteilungen beschäftigen. Dormann (2013) liefert eine weitreichende Übersicht über verschiedene Verteilungen. Wir wollen uns in diesem Kapitel mit folgenden Verteilungen beginnen.\nWir wollen uns jetzt die verschiedenen Verteilungen einmal in der Anwendung anschauen. Dabei lassen wir viel Mathematik recht und links liegen. Du kannst bei Dormann (2013) mehr zu dem Thema statistische Verteilungen anlesen.\nIn diesem Kapitel geht es erstmal um das Grundverständnis, das Daten einer Verteilung folgen. Oder noch konkreter, dass unser Outcome \\(y\\) einer Verteilung folgt. Wir müssen später unseren Alogrithmen sagen, welcher Verteilung \\(y\\) entspringt, sonst können wir keine korrekte Analyse unser Daten rechnen."
  },
  {
    "objectID": "eda-distribution.html#genutzte-r-pakete-für-das-kapitel",
    "href": "eda-distribution.html#genutzte-r-pakete-für-das-kapitel",
    "title": "\n17  Verteilung von Daten\n",
    "section": "\n17.1 Genutzte R Pakete für das Kapitel",
    "text": "17.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, see, readxl)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "eda-distribution.html#daten-für-verteilungen",
    "href": "eda-distribution.html#daten-für-verteilungen",
    "title": "\n17  Verteilung von Daten\n",
    "section": "\n17.2 Daten für Verteilungen",
    "text": "17.2 Daten für Verteilungen\nDamit wir uns auch eine Verteilung anschauen können bruachen wir viele Beobachtungen. Wir haben das ja schon bei den Histogrammen gesehen, wenn wir ein aussagekräftiges Histogramm erstellen wollen, dann brauchen wir viele Beobachtungen. Daher nehmen wir für dieses Kapitel einmal den Gummibärchendatensatz und schauen uns dort die Variablen gender, height, count_bears und count_color einmal genauer an. Wie immer nutzen wir die Funktion select() um die Spalten zu selektieren. Abschließend verwandeln wir das Geschlecht gender und das module noch in einen Faktor.\n\ngummi_tbl <- read_excel(\"data/gummibears.xlsx\")  %>%\n  select(year, module, gender, height, count_bears, count_color,\n         most_liked) %>% \n  mutate(gender = as_factor(gender),\n         module = as_factor(module))\n\nWir erhalten das Objekt gummi_tbl mit dem Datensatz in Tabelle 17.1 nochmal dargestellt.\n\n\n\n\nTabelle 17.1— Auszug aus den selektierten Daten zu den Gummibärchendaten.\n\n\n\n\n\n\n\n\n\n\nyear\nmodule\ngender\nheight\ncount_bears\ncount_color\nmost_liked\n\n\n\n2018\nFU Berlin\nm\n193\n9\n3\nlightred\n\n\n2018\nFU Berlin\nw\n159\n10\n5\nyellow\n\n\n2018\nFU Berlin\nw\n159\n9\n6\nwhite\n\n\n2018\nFU Berlin\nw\n180\n10\n5\nwhite\n\n\n2018\nFU Berlin\nm\n180\n10\n6\nwhite\n\n\n2018\nFU Berlin\nm\nNA\n10\n5\nwhite\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n2022\nStatistik\nm\n197\n9\n4\nwhite\n\n\n2022\nStatistik\nm\n180\n10\n3\ngreen\n\n\n2022\nStatistik\nm\n187\n11\n5\ndarkred\n\n\n2022\nStatistik\nm\n186\n10\n5\ngreen\n\n\n2022\nStatistik\nm\nNA\n9\n4\ndarkred\n\n\n2022\nStatistik\nNA\nNA\n10\n6\nNA\n\n\n\n\n\n\nWir nutzen jetzt die Daten einmal um uns die Normalverteilung und die Poissonverteilung am Beispiel näher anzuschauen."
  },
  {
    "objectID": "eda-distribution.html#sec-normal",
    "href": "eda-distribution.html#sec-normal",
    "title": "\n17  Verteilung von Daten\n",
    "section": "\n17.3 Die Normalverteilung",
    "text": "17.3 Die Normalverteilung\nWir sprechen in der Statistik auch von Verteilungsfamilien. Daher schreiben wir in R auch family = gaussian, wenn wir sagen wollen, dass unsere Daten einer Normalverteilung entstammen.\nWenn wir von de Normalverteilung sprechen, dann schreiben wir ein \\(\\mathcal{N}\\) Symbol - also ein großes N mit Serifen. Die Normalverteilung sieht aus wie eine Glocke, deshalb wird die Normalverteilung auch Glockenkurve genannt. Im englischen Sprachgebrauch und auch in R nutzen wir dagegen die Bezeichnung nach dem “Entdecker” der Normalverteilung, Carl Friedrich Gauß (1777 - 1985). Wir nennen daher die Normalverteilung auch Gaussian-Verteilung.\nParameter sind Zahlen, die eine Verteilungskurve beschreiben.\nEine Normalverteilung wird ruch zwei Verteilungsparameter definiert. Eine Verteilung hat Parameter. Parameter sind die Eigenschaften einer Verteilung, die notwendig sind um eine Verteilung vollständig zu beschreiben. Im Falle der Normalverteilung brauchen wir zum einen den Mittelwert \\(\\bar{y}\\), der den höchsten Punkt unserer Glockenkurve beschreibt. Zum anderen brauchen wir auch die Standardabweichung \\(s^2_y\\), die die Ausbreitung oder Breite der Glockenkurve bestimmt. Wir beschreiben eine Normalverteilung wie folgt.\n\\[\n\\mathcal{N}(\\bar{y}, s^2_y)\n\\] Im Falle der Normalverteilung brauchen wir einen Paramter für den höchsten Punkt der Kurve, sowie einen Parameter für die Ausbreitung, also wie weit geht die Kurve nach links und nach rechts. Je nach \\(\\bar{y}\\) und \\(s^2_y\\) können wir verschiedenste Normalverteilungen vorliegen haben. Eine Sammlung von Normalverteilungen nennen wir auch Familie (eng. family).\nWir haben Varianzhomogenität vorliegen, wenn \\(s^2_{1} = s^2_{2} = s^2_{3}\\) sind. Wir haben Varianzheterogenität vorliegen, wenn \\(s^2_{1} \\neq s^2_{2} \\neq s^2_{3}\\) sind.\nIn Abbildung 17.1 sehen wir verschiedene Normalverteilungen mit unterschiedlichen Mittelwerten. In Abbildung 17.1 (a) sehen wir eine Varianzhomogenität vorliegen, da die Varianzen in allen drei Normalverteilungen gleich sind. Wir können auch schreiben, dass \\(s^2_{1} = s^2_{2} = s^2_{3} = 2\\). In Abbildung 17.1 (b) haben wir Varianzheterogenität vorliegen, da die Varianzen der Normalverteilungen ungleich sind. Wir können hier dann schreiben, dass \\(s^2_{1} = 6 \\neq s^2_{2} = 1 \\neq s^2_{3} = 3\\) sind. Häufig gehen statistische Verfahren davon aus, dass wir Varianzhomogenität über die Gruppen und daher auch die Normalverteilungen vorliegen haben. Konkret, wenn wir die Sprungweiten in[cm] von Hunde- und Katzenflöhen mit einander vergleichen wollen, dann gehen wir erstmal davon aus, dass die Mittelwerte verschieden sind, aber die Varianzen gleich sind.\n\n\n\n\n\n(a) Drei Normalverteilungen mit Varianzhomogenität.\n\n\n\n\n\n\n(b) Drei Normalverteilungen unter Varianzheterogenität.\n\n\n\n\nAbbildung 17.1— Histogramm verschiedener Normalverteilungen mit unterschiedlichen Mittelwerten.\n\n\nIn einer Normalverteilung liegen 68% der Werte innerhalb \\(\\bar{y}\\pm 1 \\cdot s_y\\) und 95% der Werte innerhalb \\(\\bar{y}\\pm 2 \\cdot s_y\\)\nWenn wir eine Normalverteilung vorliegen haben, dann liegen 68% der Werte plus/minus einer Standardabweichung vom Mittelwert. Ebenso liegen 95% der Werte plus/minus zwei Standabweichungen vom Mittelwert. Über 99% der Werte befinden sich innerhalb von drei Standardabweichungen vom Mittelwert. Diese Eigenschaft einer Normalverteilung können wir später noch nutzen um abzuschätzen, ob wir einen relevanten Gruppenunterschied vorliegen haben oder aber ob unsere Daten unnatürlich breit streuen.\nWir nutzen das Wort approximativ wenn wir sagen wollen, dass ein Outcome näherungsweise normalverteilt ist.\nSchauen wir uns die Normalverteilung einmal am Beispiel unserer Gummibärchendaten und der Körpergröße der Studierenden an. Wir färben das Histogramm nach dem Geschlecht ein. In Abbildung 17.2 sehen wir das Ergebnis einmal als Histogramm und einmal als Densityplot dargestellt. Wir können annehmen, dass die Größe approximativ normalverteilt ist.\n\n\n\n\n\n(a) Histogramm.\n\n\n\n\n\n\n(b) Densityplot.\n\n\n\n\nAbbildung 17.2— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt.\n\n\nWir können die Funktion rnorm() nutzen um uns zufällige Zahlen aus der Normalverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert mean = und die gewünschte Standardabweichung mit sd = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rnorm() mit zehn Werten.\n\nrnorm(n = 10, mean = 5, sd = 2) %>% round(2)\n\n [1] 6.97 6.24 5.59 5.27 6.30 4.91 4.13 4.92 3.67 4.42\n\n\nDu kannst ja mal den Mittelwert und die Standardabweichung der zehn Zahlen ausrechnen. Da wir es hier mit einer Stichprobe mit zehn Beobachtungen zu tun haben, wird der Mittelwert \\(\\bar{y}\\) und die Standardabweichung \\(s_y\\) sich von den vorher definierten Mittelwert \\(\\mu_y = 5\\) und Standardabweichung \\(\\sigma_y = 2\\) der Grundgesamtheit unterscheiden.\nWir können auch aus unseren Gummibärchendaten für die Körpergröße in [cm] jeweils den Mittelwert und die Standardabweichung getrennt für die Geschlechter berechnen und dann die theoretische Normalverteilung zeichenen. In Abbildung 17.3 (b) und Abbildung 17.3 (d) sehen wir die Verteilung der theoretischen Werte, wenn wir die Mittelwerte und die Standardabweichung aus den Verteilungen in Abbildung 17.3 (a) schätzen.\n\n\n\n\n\n(a) Verteilung der beobachteten Werte.\n\n\n\n\n\n\n(b) Verteilung der theoretischen Werte.\n\n\n\n\n\n\n\n\n(c) Verteilung der beobachteten Werte.\n\n\n\n\n\n\n(d) Verteilung der theoretischen Werte.\n\n\n\n\nAbbildung 17.3— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte. Einmal dargestellt als Histogramm und einmal als Densityplot."
  },
  {
    "objectID": "eda-distribution.html#die-standardnormalverteilung",
    "href": "eda-distribution.html#die-standardnormalverteilung",
    "title": "\n17  Verteilung von Daten\n",
    "section": "\n17.4 Die Standardnormalverteilung",
    "text": "17.4 Die Standardnormalverteilung\nEs gibt viele Normalverteilungen. Aber es gibt eine besondere Normalverteilung, so dass diese Verteilung einen eigenen Namen hat. Wir sprechen von der Standardnormalverteilung, wenn der Mittelwert gleich Null ist und die Standardabweichung gleich Eins. Du siehst hier nochmal die Standardnormalverteilung ausgeschrieben.\n\\[\n\\mathcal{N}(0, 1)\n\\]\nFolgende Eigenschaften sind der Standardnormalverteilung gegeben. Die Standardnormalverteilung hat eine Fläche von \\(A = 1\\) unter der Kurve. Darüber hinaus liegen 95% der Werte zwischen -2 und 2. Die einzelnen Werte einer Standardnormalverteilung nennen wir \\(z\\)-Werte. Wenn wir eine beliebige Normalverteilung in eine Standardnormalverteilung überführen wollen so machen wir die Umwandlung mit der \\(z\\)-Transformation."
  },
  {
    "objectID": "eda-distribution.html#sec-poisson",
    "href": "eda-distribution.html#sec-poisson",
    "title": "\n17  Verteilung von Daten\n",
    "section": "\n17.5 Die Poissonverteilung",
    "text": "17.5 Die Poissonverteilung\nEine weitere wichtige Verteilung ist die Poissonverteilung. Die Poissonverteilung ist eine diskrete Verteilung. Daher kommen nur ganze Zahlen vor. Damit bildet die Poissonverteilung die Zähldaten ab. Wenn wir also etwas Zählen, dann ist diese Variable mit den gezählten Ergebnissen poissonverteilt. Im Folgenden sehen wir die Poissonverteilung einmal dargestellt.\n\\[\n\\mathcal{Pois}(\\lambda)\n\\]\nIm Gegensatz zur Normalverteilung hat die Poissonverteilung nur einen Parameter. Den Lageparameter \\(\\lambda\\) ausgedrückt durch den griechischen Buchstaben Lambda. Eine Poissonverteilung mit \\(\\mathcal{Pois}(4)\\) hat den höchsten Punkt bei vier. Nun hat die Poissonverteilung hat mehrere Besonderheiten. Da die Poissonverteilung keinen Streuungsparameter hat, steigt mit dem \\(\\lambda\\) auch die Streuung. Daher haben Poissonverteilungen mit einem großen \\(\\lambda\\) auch eine große Streuung. ie Ausbreitung der Kurve ist eine Funktion von \\(\\lambda\\) und steigt mit \\(\\lambda\\) an. Du kannst diesen Zusammenhang in Abbildung 17.4 beobachten.\nDarüber hinaus kann eine Poissonverteilung nicht negativ werden. Es kann keine kleinere Zahl als die Null geben. Durch die diskreten Zahlen haben wir auch immer mal Lücken zwischen den Balken der Poissonverteilung. Das passiert besonders, wenn wir eine kleine Anzahl an Beobachtungen haben. Abschließend konvergiert die Poissonverteilung bei großen \\(\\lambda\\) hin zu einer Normalverteilung.\n\n\n\n\nAbbildung 17.4— Histogramm verschiedener Poissonverteilungen.\n\n\n\n\nSchauen wir uns nun einmal die Poissonverteilung im Beispiel an. In Abbildung 17.5 sehen wir die Histogramme der Anzahl an Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte. Da wir es hier mit Zähldaten zu tun haben, könnte es sich um eine Poissonverteilung handeln. Wie müssen uns nun die Frage stellen, ob die Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte wirklich eine zufällige Realistierung sind. Daher eine zufällige Stichprobe der Grundgesamtheit. Wir können diese Annahme überprüfen in dem wir die theoretischen Werte für die beiden Poissonverteilung mit \\(\\mathcal{Pois}(10)\\) und \\(\\mathcal{Pois}(5)\\) genieren.\n\n\n\n\n\n(a) Anzahl an Bärchen\n\n\n\n\n\n\n(b) Anzahl an Farben\n\n\n\n\nAbbildung 17.5— Histogramme der Anzahl an Gummibärchen und die Anzahl an Farben in einer Tüte. Es gibt nicht mehr als sechs Farben.\n\n\nWir können die Funktion rpois() nutzen um uns zufällige Zahlen aus der Poissonverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert lambda = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rpois() mit zehn Werten.\n\nrpois(n = 10, lambda = 5)\n\n [1] 3 5 4 5 3 3 9 3 5 5\n\n\nEs gibt neben der Poissonverteilung auch die negative Binomialverteilung sowie die Quasi-Poissonverteilung, die es erlauben einen Streuungsparameter für die Poissonverteilung zu schätzen.\nWir können nun auch aus unseren Gummibärchendaten für die Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte die theoretische Poissonverteilung berechnen. In Abbildung 17.6 sehen wir die Verteilung der beobachteten Werte für Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte und deren theoretischen Verteilung nach dem geschätzen \\(\\lambda = 10\\) und \\(\\lambda = 5\\). Wir sehen ganz klar, dass die beide Variablen keine Zufallsrealisierung sind. Zum einen haben wir das auch nicht erwartet, es gibt nicht mehr als sechs Farben und zum anderen ist zu vermuten, dass Haribo technisch in den Auswahlprozess eingreift. Wir haben auf jeden Fall eine sehr viel kleinere Streuung als bei einer klassischen Poissonverteilung anzunehmen wäre.\n\n\n\n\n\n(a) Verteilung der beobachteten Anzahl an Bärchen.\n\n\n\n\n\n\n(b) Verteilung der theoretischen Anzahl an Bärchen.\n\n\n\n\n\n\n\n\n(c) Verteilung der beobachteten Anzahl an Farben.\n\n\n\n\n\n\n(d) Verteilung der theoretischen Anzahl an Farben.\n\n\n\n\nAbbildung 17.6— Darstellung Anzahl an Bärchen und Anzahl an Farben. Es gibt nicht mehr als sechs Farben. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte.\n\n\nIn Abbildung 17.7 schauen wir uns nochmal an in wie weit sich die Füllung der Tütchen im Laufe der Jahre entwickelt hat. Die Daten werden ja schon seit 2018 erhoben. Wir schauen uns daher die Densityplot einmal aufgetrennt für die Jahre 2018 bis heute an. Das Jahr 2020 fehlt, da bedingt durch die Coronapandemie keine Präsenslehre stattfand. Wir sehen, dass sich die Verteilung anscheinend in dem Jahr 2022 langsam nach links zu weniger Bärchen in einer Tüte bewegt. Wir bleiben gespannt auf den weiteren Trend.\n\n\n\n\nAbbildung 17.7— Densityplot der Anzahl an Bärchen in einer Tüte aufgetrennt nach den Jahren der Erhebung. Das Jahr 2020 fehlt bedingt durch die Coronapandemie.\n\n\n\n\nIn Abbildung 17.8 betrachten wir die Verteilung der am meisten gemochten Gummibärchen aufgeteilt nach dem angegebenen Geschlecht im Vergeich zu den Gummibärchen in den Tütchen. Wir sehen, dass Haribo die Tütchen sehr gleichmäßig verteilt und auf die Geschmäcker keinerlei Rücksicht nimmt. Entweder weiß Haribo nichts von den Vorlieben seiner Käufer:innen oder aber es ist dann doch zu viel Aufwand die Produktion anzupassen.\n\n\n\n\n\n(a) Anzahl am liebsten gemochten Gummibärchen aufgeteilt nach Geschlecht.\n\n\n\n\n\n\n(b) Anzahl der Gummibärchen pro Tüte nach Farbe.\n\n\n\n\nAbbildung 17.8— Histogramme der am liebsten gemochten Gummibärchchen im Vergleich zum Inhalt der Tütchen."
  },
  {
    "objectID": "eda-distribution.html#weitere-verteilungen",
    "href": "eda-distribution.html#weitere-verteilungen",
    "title": "\n17  Verteilung von Daten\n",
    "section": "\n17.6 Weitere Verteilungen",
    "text": "17.6 Weitere Verteilungen\n\n\n\n\n\n\nWir besuchen gerne die R Shiny App The distribution zoo um mehr über die verschiedenen Verteilungen und deren Parameter zu erfahren.\nIn der nächsten Zeit werden noch weitere gängige Verteilungen ergänzt. Bis dahin können die Basic Probability Distributions in R nochmal extern nachgeschaut werden.\nIm Weiteren liefert Dormann (2013) eine gute Übersicht über verschiedene Verteilungen."
  },
  {
    "objectID": "eda-distribution.html#referenzen",
    "href": "eda-distribution.html#referenzen",
    "title": "\n17  Verteilung von Daten\n",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "stat-tests-preface.html",
    "href": "stat-tests-preface.html",
    "title": "Frequentistische Hypothesentests",
    "section": "",
    "text": "Version vom October 12, 2022 um 18:19:10\nDas statistische Testen - eine Geschichte voller Missverständnisse. Wir wollen uns in den folgenden Kapiteln mit den Grundlagen des frequentistischen Hypothesentestens beschäftigen. Wenn ich hier einen Unterschied mache, dann muss es ja auch noch ein anderes Hypothesentesten geben. Ja, das nennt man dann bayesianische Statistik und kommt eventuell mal später. Wir konzentrieren uns aber zuerst auf frequentistische Hypothesentesten was seit gut hundert Jahren genutzt wird. Ich werde hier textlich nur einen kurzen Einstieg liefern. Vielleicht wird es in den folgenden Jahren länger aber aktuell (Ende 2022) bleiben wir hier bei einem kurzen Einstieg.\nBeginnen wir mit der Logik der Forschung oder allgemeiner formuliert, als die Grundlage der Wissenschaft. Wir basieren all unsere Entscheidungen in der Wissenschaft auf dem Falsifikationsprinzip. Also bitte merken, wir können nur ablehnen (eng. reject).\nWir wollen hier auf keinen Fall die Leistungen von Altvorderen schmälern. Dennoch hatten Ronald Fischer (1890 - 1962), als der Begründer der Statistik, andere Vorraussetzungen als wir heutzutage. Als wichtigster Unterschied sei natürlich das Gerät genannt, an dem du gerade diese Zeilen liest: dem Computer. Selbst die Erstellung einfachster Abbildungen war sehr, sehr zeitaufwendig. Die Berechnung von Zahlen lohnte sich mehr, als die Zahlen zu visualisieren. Insbesondere wenn wir die Explorative Datenanalyse nach John Tukey (1915 - 2000) durchführen. Undenkbar zu den Zeiten von Ronald Fischer mehrere Abbildungen unterschiedlich nach Faktoren einzufärben und sich die Daten anzugucken.\nNeben dieser Begrenzung von moderner Rechenkapazität um 1900 gab es noch eine andere ungünstige Entwicklung. Stark vereinfacht formuliert entwickelte Ronald Fischer statistische Werkzeuge um abzuschätzen wir wahrscheinlich die Nullhypothese unter dem Auftreten der beobachteten Daten ist. Nun ist es aber so, dass wir ja auch eine Entscheidung treffen wollen. Nach der Logik der Forschung wollen wir ja eine Hypothese falsifizieren, in unserem Fall die Nullhypothese. Die Entscheidungsregeln, also die statistische Testtheorie, kommen nun von Jerzy Neyman (1894 - 1981) und Egon Pearson (1895 - 1980), beide als die Begründer der frequentistischen Hypothesentests.\nSchlussendlich gibt es noch eine andere Strömung in der Statistik, die auf den mathematischen Formeln von Thomas Bayes (1701 - 1761) basieren. In sich eine geschlossene Theorie, die auf der inversen Wahrscheinlichkeit basiert. Das klingt jetzt etwas schräg, aber eigentlich ist die bayesianische Statistik die Statistik, die die Fragen um die Alternativehypothese beantwortet. Der Grund warum die bayesianische Statistik nicht angewendet wurde, war der Bedarf an Rechenleistung. Die bayesiansiche Statistik lässt sich nicht händisch in endlicher Zeit lösen. Dieses technische Problem haben wir aber nicht mehr. Eigentlich könnten wir also die bayesiansiche Statistik verwenden. Wir wollen hier aber (noch) nicht auf die bayesianische Statistik eingehen, das werden wir später tun.\nWenn du allgemein Interesse hast an der Geschichte der Statistik dann sei auf Salsburg (2001) verwiesen. Ein sehr schönes Buch, was die geschichtlichen Zusammenhänge nochmal aufzeigt.\nKommen wir aber nun zu den wichtigeren Punkten. Die folgenden Kapitel ist sehr umfangreich und enthalten viele Informationen, die wir teilweise später nochmal brauchen. Darüber hinaus müssen wir noch das Lernen und Verstehen von der Anwendung unterscheiden. Wir teilen dabei die Testentscheidung und die Testtheorie in zwei Kapitel auf.\nDu erfährst im Kapitel 18 mehr zur Testentscheidung und welche Konzepte wir dort nutzen:\nDu erfährst im Kapitel 19 mehr zur Testtheorie und welche Konzepte wir dort nutzen:"
  },
  {
    "objectID": "stat-tests-preface.html#referenzen",
    "href": "stat-tests-preface.html#referenzen",
    "title": "Frequentistische Hypothesentests",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nSalsburg, David. 2001. The lady tasting tea: How statistics revolutionized science in the twentieth century. Macmillan."
  },
  {
    "objectID": "stat-tests-basic.html",
    "href": "stat-tests-basic.html",
    "title": "18  Die Testentscheidung",
    "section": "",
    "text": "Version vom October 12, 2022 um 18:27:00\nDu erfährst im diesem Kapitel mehr zur Testentscheidung und welche Konzepte wir dort nutzen:"
  },
  {
    "objectID": "stat-tests-basic.html#sec-hypothesen",
    "href": "stat-tests-basic.html#sec-hypothesen",
    "title": "18  Die Testentscheidung",
    "section": "\n18.1 Die Hypothesen",
    "text": "18.1 Die Hypothesen\n\n\nIm Anhang A findest du verschiedene Beispiele zu Auswertungen von Datenbeispielen.\nWir können auf allen Daten einen statistischen Test rechnen und erhalten statistische Maßzahlen wie eine Teststatistik oder einen p-Wert. Nur leider können wir mit diesen statistischen Maßzahlen nicht viel anfangen ohne die Hypothesen zu kennen. Jeder statistische Test testet eine Nullhypothese. Ob diese Hypothese dem Anwender nun bekannt ist oder nicht, ein statistischer Test testet eine Nullhypothese. Daher müssen wir uns immer klar sein, was die entsprechende Nullhypothese zu unserer Fragestellung ist. Wenn du hier stockst, ist das ganz normal. Eine Fragestellung mit einer statistischen Hypothese zu verbinden ist nicht immer so einfach gemacht.\n\n\n\n\n\n\nDie Nullhypothese \\(H_0\\) und die Alternativehypothese \\(H_A\\)\n\n\n\nDie Nullhypothese \\(H_0\\) nennen wir auch die Null oder Gleichheitshypothese. Die Nullhypothese sagt aus, dass zwei Gruppen gleich sind oder aber kein Effekt zu beobachten ist.\n\\[\nH_0: \\bar{y}_{1} = \\bar{y}_{2}\n\\]\nDie Alternativehypothese \\(H_A\\) oder \\(H_1\\) auch Alternative genannt nennen wir auch Unterschiedshypothese. Die Alternativehypothese besagt, dass ein Unterschied vorliegt oder aber ein Effekt vorhanden ist.\n\\[\nH_A: \\bar{y}_{1} \\neq \\bar{y}_{2}\n\\]\n\n\nAls Veranschaulichung nehmen wir das Beispiel aus der unterschiedlichen Sprungweiten in [cm] für Hunde- und Katzenflöhe. Wir formulieren als erstes die Fragestellung. Eine Fragestellung endet mit einem Fragezeichen.\nLiegt ein Unterschied zwischen den Sprungweiten von Hunde- und Katzenflöhen vor?\nWir können die Frage auch anders formulieren.\nSpringen Hunde- und Katzenflöhe unterschiedlich weit?\nWichtig ist, dass wir eine primäre Fragestellung formulieren. Wir können auch mehrere Fragen an einen Datensatz haben. Das ist auch vollkommen normal. Nur hat jede Fragestellung ein eigenes Hypothesenpaar. Wir bleiben aber bei dem simplen Beispiel.\nWie sieht nun die statistische Hypothese in diesem Beispiel aus? Wir wollen uns die Sprungweite in [cm] anschauen. In diesem Fall wollen wir die mittlere Sprungweite der Hundeflöhe \\(\\bar{y}_{dog}\\) mit der mittleren Sprungweite der Katzenflöhe \\(\\bar{y}_{cat}\\) vergleichen. Es ergibt sich folgendes Hypothesenpaar.\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{dog} &= \\bar{y}_{cat} \\\\  \nH_A: \\bar{y}_{dog} &\\neq \\bar{y}_{cat} \\\\   \n\\end{aligned}\n\\]\nDas Falisifkationsprinzip - wir können nur Ablehnen - kommt hier zusammen mit der frequentistischen Statistik in der wir nur eine Wahrscheinlichkeitsaussage über das Auftreten der Daten \\(D\\) - unter der Annahme \\(H_0\\) gilt - treffen können.\nEs ist wichtig sich in Erinnerung zu rufen, dass wir nur und ausschließlich Aussagen über die Nullhypothese treffen werden. Das frequentistische Hypothesentesten kann nichts anders. Wir kriegen keine Aussage über die Alternativhypothese sondern nur eine Abschätzung der Wahrscheinlichkeit des Auftretens der Daten im durchgeführten Experiment, wenn die Nullhypothese wahr wäre."
  },
  {
    "objectID": "stat-tests-basic.html#die-testentscheidung",
    "href": "stat-tests-basic.html#die-testentscheidung",
    "title": "18  Die Testentscheidung",
    "section": "\n18.2 Die Testentscheidung…",
    "text": "18.2 Die Testentscheidung…\nIn den folgenden Kapiteln werden wir verschiedene statistische Tests kennenlernen. Alle statistischen Tests haben gemein, dass ein Test eine Teststatistik \\(T_{calc}\\) berechnet. Darüber hinaus liefert jeder Test auch einen p-Wert (eng. p-value). Manche statistischen Test geben auch ein 95% Konfidenzintervall wieder. Eine Testentscheidung gegen die Nullhypothese \\(H_0\\) kann mit jedem der drei statistischen Maßzahlen durchgeführt werden. Die Regel für die Entscheidung, ob die Nullhypothese \\(H_0\\) abgelehnt werden kann, ist nur jeweils anders. In Tabelle 18.1 sind die Entscheidungsregeln einmal zusammengefasst.\n\n\n\nTabelle 18.1— Zusammenfassung der statistischen Testentscheidung unter der Nutzung der Teststatistik, dem p-Wert und dem 95% Konfidenzintervall. Die Entscheidung nach der Teststatistik ist veraltet und dient nur dem konzeptionellen Verständnisses. In der Forschung angewandt wird der p-Wert und das 95% Konfidenzintervall. Im Fall des 95% Konfidenzintervalls müssen wir noch unterschieden, ob wir einen Mittelwertsunterschied \\(\\Delta_{A-B}\\) oder aber einen Anteilsunterschied \\(\\Delta_{A/B}\\) betrachten.\n\n\n\n\n\n\n\n\nTeststatistik\np-Wert\n95% Konfidenzintervall\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(\\boldsymbol{KI_{1-\\alpha}}\\)\n\n\nH\\(_0\\) ablehnen\n\\(T_{calc} \\geq T_{\\alpha = 5\\%}\\)\n\\(Pr(\\geq T_{calc}| H_0) \\leq \\alpha\\)\n\n\\(\\Delta_{A-B}\\): enthält nicht 0\n\n\n\nH\\(_0\\) ablehnen\n\n\n\n\\(\\Delta_{A/B}\\): enthält nicht 1\n\n\n\n\n\n\nWir wollen in den folgenden Abschnitten die jeweiligen Entscheidungsregeln eines statistisches Tests einmal durchgehen.\n\nDie Testentscheidung gegen die Nullhypothese anhand der Teststatistik in Kapitel 18.2.1\n\nDie Testentscheidung gegen die Nullhypothese anhand dem p-Wert in Kapitel 18.2.2\n\nDie Testentscheidung gegen die Nullhypothese anhand des 95% Konfidenzintervall in Kapitel 18.2.3\n\n\n\n\n\n\n\n\nStreng genommen gilt die Regel \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) nur für eine Auswahl an statistischen Tests siehe dazu auch Kapitel 18.2.1. Bei manchen statistischen Tests ist die Entscheidung gedreht. Hier lassen wir das aber mal so stehen…\n\n18.2.1 … anhand der Teststatistik\n\n\n\n\n\n\nPrinzip des statistischen Testens I - Die Teststatistik\n\n\n\nDu findest auf YouTube Prinzip des statistischen Testens I - Die Teststatistik als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nWir wollen uns dem frequentistischen Hypothesentesten über die Idee der Teststatistik annähern. Im folgenden sehen wir die Formel für den t-Test. Den t-Test werden wir im Kapitel 21 uns nochmal detaillierter anschauen. Hier nutzen wir die vereinfachte Formel um das Konzept zu verstehen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_2}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nmit\n\n\n\\(\\bar{y}_1\\) dem Mittelwert für die erste Gruppe.\n\n\\(\\bar{y}_2\\) dem Mittelwert für die zweite Gruppe.\n\n\\(s_{p}\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{s_A + s_B}{2}\\).\n\n\\(n_g\\) der Gruppengröße der gruppen. Wir nehmen an beide Gruppen sind gleich groß.\n\nWir benötigen also zwei Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) und deren gepoolte Standardabweichung \\(s_p\\) sowie die Anzahl der Beobachtungen je Gruppe \\(n_g\\). Wenden wir die Formel des t-Tests einmal auf den folgenden Beispieldatensatz an. In Tabelle 18.2 ist eine Datenbeispiel gegeben.\n\n\n\n\nTabelle 18.2— Beispiel für die Berechnung von einem Mittelwertseffekt an der Sprunglänge [cm] von Hunde und Katzenflöhen.\n\nanimal\njump_length\n\n\n\ncat\n8.5\n\n\ncat\n9.9\n\n\ncat\n8.9\n\n\ncat\n9.4\n\n\ndog\n8.0\n\n\ndog\n7.2\n\n\ndog\n8.4\n\n\ndog\n7.5\n\n\n\n\n\n\nWir berechnen nun die Mittelwerte und die Standardabweichungen aus der obigen Datentabelle. Die Werte setzen wir dann in die Formel ein.\n\\[\nT_{calc}=\\cfrac{9.2 - 7.8}{\\cfrac{(0.6 + 0.5)}{2} \\cdot \\sqrt{2/4}} = 3.6\n\\]\nmit\n\n\n\\(\\bar{y}_{cat} = 9.2\\) dem Mittelwert für die Gruppe cat.\n\n\\(\\bar{y}_{dog} = 7.8\\) dem Mittelwert für die Gruppe dog.\n\n\\(s_p = 0.55\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.6 + 0.5}{2}\\).\n\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nWir haben nun die Teststatistik \\(T_{calc} = 3.6\\) berechnet. In der ganzen Rechnererei verliert man manchmal den Überblick. Erinnern wir uns, was wir eigentlich wollten. Die Frage war, ob sich die mittleren Sprungweiten der Hunde- und Katzenflöhe unterschieden. Wenn die \\(H_0\\) wahr wäre, dann wäre der Unterschied \\(\\Delta\\) der beiden Mittelwerte der Hunde- und Katzenflöhe gleich null. Oder nochmal in der Analogie der t-Test Formel, dann wäre im Zähler \\(\\Delta = \\bar{y}_{cat} - \\bar{y}_{dog} = 0\\). Wenn die Mittelwerte der Sprungweite [cm] der Hunde- und Katzenflöhe gleich wäre, dann wäre die berechnete Teststatistik \\(T_{calc} = 0\\), da im Zähler Null stehen würde. Die Differenz von zwei gleichen Zahlen ist Null.\nJe größer die berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die beiden Mittelwerte per Zufall gleich sind. Wie groß muss nun die berechnete Teststatistik \\(T_{calc}\\) werden damit wir die Nullhypothese ablehnen können?\n\n\n\nAbbildung 18.1— Die t-Verteilung aller möglichen \\(T_{calc}\\) wenn die Nullhypothese wahr ist. Der Mittelwert der t-Verteilun ist \\(T=0\\). Wenn wir keinen Effekt erwarten würden dann wären die beiden Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) gleich groß. Die Differenz wäre 0. Je größer der \\(T_{calc}\\) wird desto weniger können wir davon ausgehen, dass die beiden Mittelwerte gleich sind. Liegt der \\(T_{calc}\\) über dem kritischen Wert von \\(T_{\\alpha = 5\\%}\\) dann wir die Nullhypothese abgelehnt.\n\n\n\nIn Abbildung 18.1 ist die Verteilung aller möglichen \\(T_{calc}\\) Werte unter der Annahme, dass die Nullhypothese wahr ist, dargestellt. Wir sehen, dass die t-Verteilung am höchsten bei \\(T_{calc} = 0\\) ist und niedrigeren Werte mit steigenden t-Werten annimmt. Wenn \\(T = 0\\) dann sind auch die Mittelwerte gleich. Je größer unsere berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die Nullhypothese gilt. Die t-Verteilug ist so gebaut, dass die Fläche \\(A\\) unter der Kurve gleich \\(A=1\\) ist. Wir können nun den kritschen Wert \\(T_{\\alpha = 5\\%}\\) berechnen an dem rechts von dem Wert eine Fläche von 0.05 oder 5% liegt. Sommit liegt dann links von dem kritischen Wert die Fläche von 0.95 oder 95%. Den kritischen Wert \\(T_{\\alpha = 5\\%}\\) können wir statistischen Tabellen entnehmen. Oder wir berechnen den kritischen Wert direkt in R mit \\(T_{\\alpha = 5\\%} = 2.78\\).\nKommen wir zurück zu unserem Beispiel. Wir haben in unserem Datenbeispiel für den Vergleich von der Sprungweite in [cm] von Hunde- und Katzenflöhen eine Teststatistik von \\(T_{calc} = 3.6\\) berechnet. Der kritische Wert um die Nullhypothese abzulehnen liegt bei \\(T_{\\alpha = 5\\%} = 2.78\\). Wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. In unserem Fall ist \\(3.6 \\geq 2.78\\). Wir können die Nullhypothese ablehnen. Es gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen.\n\n\n\n\n\n\nEs gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen. Die Aussage ist statistisch falsch. Wir können im frequentistischen Hypothesentesten keine Aussage über die \\(H_A\\) treffen. Im Sinne der Anwendbarkeit soll es hier so stehen bleiben.\nNun ist es leider so, dass jeder statistische Test seine eigene Teststatistik \\(T\\) hat. Daher ist es etwas mühselig sich immer neue und andere kritische Werte für jeden Test zu merken. Es hat sich daher eingebürgert, sich nicht die Teststatistik für die Testentscheidung gegen die Nullhypothese zu nutzen sondern den p-Wert. Den p-Wert wollen wir uns in dem folgenden Abschnitt anschauen.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik\n\n\n\nBei der Entscheidung mit der Teststatistik müssen wir zwei Fälle unterschieden.\n\nBei einem t-Test und einem \\(\\mathcal{X}^2\\)-Test gilt, wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nBei einem Wilcoxon-Mann-Whitney-Test gilt, wenn \\(T_{calc} < T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\n\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.\n\n\n\n18.2.2 … anhand dem p-Wert\n\n\n\n\n\n\nPrinzip des statistischen Testens II - Der p-Wert\n\n\n\nDu findest auf YouTube Prinzip des statistischen Testens II - Der p-Wert als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nIn dem vorherigen Abschnitt haben wir gelernt, wie wir zu einer Entscheidung gegen die Nullhypothese anhand der Teststatistik kommen. Wir haben einen kritischen Wert \\(T_{\\alpha = 5\\%}\\) definiert bei dem rechts von dem Wert 5% der Werte liegen. Anstatt nun den berechneten Wert \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) zu vergleichen, vergleichen wir jetzt die Flächen rechts von den jeweiligen Werten.\nWir schreiben \\(\\boldsymbol{Pr}\\) und meinen damit eine Wahrscheinlichkeit (eng. probability). Häufig wird auch nur das \\(P\\) verwendet, aber dann kommen wir wieder mit anderen Konzepten in die Quere.\nIn Abbildung 18.1 sind die Flächen auch eingetragen. Da die gesamte Fläche unter der t-Verteilung mit \\(A = 1\\) ist, können wir die Flächen auch als Wahrscheinlichkeiten lesen. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert genannt. Die gesamte Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%. Wir können also die Teststatistiken oder den p-Wert mit dem \\(\\alpha\\)-Niveau von 5% vergleichen.\n\n\nTabelle 18.3— Zusammenhang zwischen der Teststatistik \\(T\\) und der Fläche \\(A\\) rechts von der Teststatistik. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T|H_0)\\) oder \\(p\\)-Wert genannt. Die Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%.\n\nTeststatistik \\(T\\)\n\nFläche \\(A\\)\n\n\n\n\n\\(T_{calc}\\)\n\n\\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert\n\n\n\\(T_{\\alpha = 5\\%}\\)\n\\(\\alpha\\)\n\n\n\n\nDer p-Wert oder \\(Pr(T|H_0)\\) ist eine Wahrscheinlichkeit. Eine Wahrscheinlichkeit kann die Zahlen von 0 bis 1 annehmen. Dabei sind die Grenzen einfach zu definieren. Eine Wahrscheinlichkeit von \\(Pr(A) = 0\\) bedeutet, dass das Ereignis A nicht auftritt; eine Wahrscheinlichkeit von \\(Pr(A) = 1\\) bedeutet, dass das Ereignis A eintritt. Der Zahlenraum dazwischen stellt jeden von uns schon vor große Herausforderungen. Der Unterschied zwischen 40% und 60% für den Eintritt des Ereignisses A sind nicht so klar zu definieren, wie du auf den ersten Blick meinen magst.\nEin frequentistischer Hypothesentest beantwortet die Frage, mit welcher Wahrscheinlichkeit \\(Pr\\) die Teststatistik \\(T\\) aus dem Experiment mit den Daten \\(D\\) zu beobachten wären, wenn es keinen Effekt gäbe (\\(H_0\\) ist wahr).\nLikelihood heißt Plausibilität und Probability heißt Wahrscheinlichkeit.\nIm Englischen gibt es die Begrifflichkeiten einer Likelihood und einer Probability in der Statistik. Meist wird beides ins Deutsche ungenau mit Wahrscheinlichkeit übersetzt oder wir nutzen einfach Likelihood. Was aber auch nicht so recht weiterhilft. Es handelt sich hierbei aber um zwei unterschiedliche Konzepte. Deshalb Übersetzen wir Likelihood mit Plausibilität und Probability mit Wahrscheinlichkeit.\nIm Folgenden berechnen wir den \\(p\\)-Wert in R mit der Funktion t.test(). Mehr dazu im Kapitel 21, wo wir den t-Test und deren Anwendung im Detail besprechen.\n\n\n# A tibble: 1 × 2\n  statistic p.value\n      <dbl>   <dbl>\n1      2.81  0.0309\n\n\nWir sagen, dass wir ein signifikantes Ergebnis haben, wenn der \\(p\\)-Wert kleiner ist als die Signifikanzschwelle \\(\\alpha\\) von 5%.\nWir erhalten einen \\(p\\)-Wert von 0.031 und vergleichen diesen Wert zu einem \\(\\alpha\\) von 5%. Ist der \\(p\\)-Wert kleiner als der \\(\\alpha\\)-Wert von 5%, dann können wir die Nullhypothese ablehnen. Da 0.031 kleiner ist als 0.05 können wir die Nullhypothese und damit die Gleichheit der mittleren Sprungweiten in [cm] ablehnen. Wir sagen, dass wir ein signifikantes Ergebnis vorliegen haben.\n\n\n\n\n\n\nEntscheidung mit dem p-Wert\n\n\n\nWenn der p-Wert \\(\\leq \\alpha\\) dann wird die Nullhypothese (H\\(_0\\)) abgelehnt. Das Signifikanzniveau \\(\\alpha\\) wird als Kulturkonstante auf 5% oder 0.05 gesetzt. Die Nullhypothese (H\\(_0\\)) kann auch Gleichheitshypothese gesehen werden. Wenn die H\\(_0\\) gilt, liegt kein Unterschied zwischen z.B. den Behandlungen vor.\n\n\n\n18.2.3 … anhand des 95% Konfidenzintervall\nEin statistischer Test der eine Teststatistik \\(T\\) berechnet liefert auch immer einen \\(p\\)-Wert. Nicht alle statistischen Tests ermöglichen es ein 95% Konfidenzintervall zu berechnen. Abbildung 18.2 zeigt ein 95% konfidenzintervall.\n\n\nAbbildung 18.2— Ein 95% Konfidenzintervall. Der Punkt in der Mitte entspricht dem Unterschied oder Effekt \\(\\Delta\\).\n\n\nMit p-Werten haben wir Wahrscheinlichkeitsaussagen und damit über die Signifikanz. Damit haben wir noch keine Aussage über die Relevanz des beobachtenten Effekts.\nMit der Teststatistik \\(T\\) und dem damit verbundenen \\(p\\)-Wert haben wir uns Wahrscheinlichkeiten angeschaut und erhalten eine Wahrscheinlichkeitsaussage. Eine Wahrscheinlichkeitsaussage sagt aber nichts über den Effekt \\(\\Delta\\) aus. Also wie groß ist der mittlere Sprungunterschied zwischen Hunde- und Katzenflöhen.\nDie Idee von 95% Kondifenzintervallen ist es jetzt den Effekt mit der Wahrscheinlichkeitsaussage zusammenzubringen und beides in einer Visualisierung zu kombinieren. Im Folgenden sehen wir die vereinfachte Formel für das 95% Konfidenzintervall eines t-Tests.\n\\[\n\\left[\n(\\bar{y}_1-\\bar{y}_2) -\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}}; \\;\n(\\bar{y}_1-\\bar{y}_2) +\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}};\n\\right]\n\\]\nDie Formel ist ein wenig komplex, aber im Prinzip einfach. Der linke und der rechte Teil neben dem Semikolon sind fast gleich, bis auf das Plus- und Minuszeichen. Abbildung 18.3 visualisert die Formel einmal. Wir sehen Folgendes in der Formel und dann in der entsprechenden Abbildung:\n\n\n\\((\\bar{y}_{1}-\\bar{y}_{2})\\) ist der Effekt \\(\\Delta\\). In diesem Fall der Mittelwertsunterschied. Wir finden den Effekt als Punkt in der Mitte des Intervals.\n\n\\(T_{\\alpha = 5\\%} \\cdot \\frac {s}{\\sqrt{n}}\\) ist der Wert, der die Arme des Intervals bildet. Wir vereinfachen die Formel mit \\(s_p\\) für die gepoolte Standardabweichung und \\(n_g\\) für die Fallzahl der beiden Gruppen. Wir nehmen an das beide Gruppen die gleiche Fallzahl \\(n_1 = n_2\\) haben.\n\n\n\nAbbildung 18.3— Zusammenhang zwischen der vereinfachten Formel für das 95% Konfidenzintervall und der Visualisierung des 95% Konfidenzintervalls. Der Effektschätzer wird als Punkt in der Mitte des Intervalls dargestellt. Der Effektschäter \\(\\Delta\\) kann entweder ein Mittelwertsunterschied sein oder ein Anteilsunterschied. Bei einem Mittelwertsunterschied kann die Nullhypothese abgelehnt werden, wenn die 0 nicht im Konfidenzintervall ist; bei einem Anteilsunterschied wenn die 1 nicht im Konfidenzintervall ist. Die Arme werden länger oder kürzer je nachdem wie sich die statistischen Maßzahlen \\(s\\) und \\(n\\) verändern.\n\n\nDie Funktion factor() in R erlaubt es dir die Level eines Faktors zu sortieren und so festzulegen ob Level cat minus Level dog oder umgekehrt von R gerechnet wird.\nWir können eine biologische Relevanz definieren, dadurch das ein 95% Konfidenzintervall die Wahrscheinlichkeitsaussage über die Signifkanz, daher ob die Nullhypothese abgelehnt werden kann, mit dem Effekt zusammenbringt. Wo die Signifikanzschwelle klar definiert ist, hängt die Relevanzschwelle von der wissenschaftlichen Fragestellung und weiteren externen Faktoren ab. Die Signifikanzschwelle liegt bei 0, wenn wir Mittelwerte miteinander vergleichen und bei 1, wenn wir Anteile vergleichen. Abbildung 18.4 zeigt fünf 95% Konfidenzintervalle (a-e), die sich anhand der Signifikanz und Relevanz unterscheiden. Bei der Relevanz ist es wichtig zu wissen in welche Richtung der Effekt gehen soll. Erwarten wir einen positiven Effekt wenn wir die Differenz der beiden Gruppen bilden oder einen negativen Effekt?\n\n\nAbbildung 18.4— Verschiedene signifikante und relevante Konfidenzintervalle: (a) nicht signifikant und nicht relevant; (b) signifikant und nicht relevant; (c) signifikant und relevant; (d) signifikant und nicht relevant, der Effekt ist zu klein; (e) signifikant und potenziell relevant, Effekt zeigt in eine unerwartete Richtung gegeben der Relevanzschwelle.\n\n\nWir wollen uns nun einmal anschauen, wie sich ein 95% Konfidenzintervall berechnet. Wir nehmen dafür die vereinfachte Formel und setzen die berechneten statistischen Maßzahlen ein. In der Anwendung werden wir die Konfidenzintervalle nicht selber berechnen. Wenn ein statistisches Verfahren konfidenzintervalle berechnen kann, dann liefert die entsprechende Funktion in R das Konfidenzintervall.\nEs ergibt sich Folgende ausgefüllte, vereinfachte Formel für das 95% Konfidenzintervalls eines t-Tests für das Beispiel des Sprungweitenunterschieds [cm] zwischen Hunde- und Katzenflöhen.\n\n\n\n\n\n\nWir nutzen hier eine vereinfachte Formel für das Konfidenzintervall um das Konzept zu verstehen. Später berechnen wir das Konfidenzintervall in R.\n\\[\n\\left[\n(9.2-7.8) -\n2.78 \\cdot \\frac {0.55}{\\sqrt{4}}; \\;\n(9.2-7.8) +\n2.78 \\cdot \\frac {0.55}{\\sqrt{4}};\n\\right]\n\\]\nmit\n\n\n\\(\\bar{y}_{cat} = 9.2\\) dem Mittelwert für die Gruppe cat.\n\n\\(\\bar{y}_{dog} = 7.8\\) dem Mittelwert für die Gruppe dog.\n\n\\(T_{\\alpha = 5\\%} = 2.78\\) dem kritischen Wert.\n\n\\(s_p = 0.55\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.6 + 0.5}{2}\\).\n\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nLösen wir die Formel auf, so ergibt sich folgendes 95% Konfidenzintervall des Mittelwertsunterschiedes der Hunde- und Katzenflöhe.\n\\[[0.39; 2.20]\\]\nWir können sagen, dass mit 95% Wahrscheinlichkeit das Konfidenzintervall den wahren Effektunterschied \\(\\Delta\\) überdeckt. Oder etwas mehr in Prosa, dass wir eine Sprungweitenunterschied von 0.39 cm bis 2.20 cm zwischen Hunde- und Katzenflöhen erwarten würden.\nDie Entscheidung gegen die Nullhypothese bei einem Mittelwertsunterschied erfolgt bei einem 95% Konfidenzintervall danach ob die Null mit im Konfidenzintervall liegt oder nicht. In dem Interval \\([0.39; 2.20]\\) ist die Null nicht enthalten, also können wir die Nullhypothese ablehnen. Es ist mit einem Unterschied zwischen den mittleren Sprungweiten von Hunde- und Katzenflöhen auszugehen.\nIn unserem Beispiel, könnten wir die Relevanzschwelle für den mittleren Sprungweitenunterschied zwischen Hund- und Katzenflöhen auf 2 cm setzen. In dem Fall würden wir entscheiden, dass der mittlere Sprungweitenunterschied nicht relevant ist, da die 2 cm im Konfidenzintervall enthalten sind. Was wäre wenn wir die Relevanzschwelle auf 4 cm setzen? Dann wäre zwar die Relevanzschwelle nicht mehr im Konfidenzintervall, aber wir hätten Fall (d) in der Abbildung 18.4 vorliegen. Der Effekt ist einfach zu klein, dass der Effekt relevant sein könnte.\n\n\n\n\n\n\nEntscheidung mit dem 95% Konfidenzintervall\n\n\n\nBei der Entscheidung mit dem 95% Konfidenzinterval müssen wir zwei Fälle unterscheiden.\n\nEntweder schauen wir uns einen Mittelwertsunterschied (\\(\\Delta_{y_1-y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 0 im 95% Konfidenzinterval ist.\nOder wir schauen uns einen Anteilsunterschied (\\(\\Delta_{y_1/y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 1 im 95% Konfidenzinterval ist."
  },
  {
    "objectID": "stat-tests-basic.html#sec-delta-n-s",
    "href": "stat-tests-basic.html#sec-delta-n-s",
    "title": "18  Die Testentscheidung",
    "section": "\n18.3 Auswirkung des Effektes, der Streuung und der Fallzahl",
    "text": "18.3 Auswirkung des Effektes, der Streuung und der Fallzahl\nWir wollen einmal den Zusammenhang zwischen dem Effekt \\(\\Delta\\), der Streuung als Standardabweichung \\(s\\) und Fallzahl \\(n\\) uns näher anschauen. Wir können die Formel des t-Tests wie folgt vereinfachen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_1}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nFür die Betrachtung der Zusammenhänge wandeln wir \\(\\sqrt{2/n_g}\\) in \\(1/n\\) um. Dadurch wandert die Fallzahl \\(n\\) in den Zähler. Die Standardabweichung verallgemeinern wir zu \\(s\\) und damit allgemein zur Streuung. Abschließend betrachten wir \\(\\bar{y}_A-\\bar{y}_B\\) als den Effekt \\(\\Delta\\). Es ergibt sich folgende vereinfachte Formel.\n\\[\nT_{calc} = \\cfrac{\\Delta \\cdot n}{s}\n\\]\nWir können uns nun die Frage stellen, wie ändert sich die Teststatistik \\(T_{calc}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\) in den Daten. Die Tabelle 18.4 zeigt die Zusammenhänge auf. Die Aussagen in der Tabelle lassen sich generalisieren. So bedeutet eine steigende Fallzahl meist mehr signifikante Ergebnisse. Eine stiegende Streuung reduziert die Signifikanz eines Vergleichs. Ein Ansteigen des Effektes führt zu mehr signifikanten Ergebnissen. Ebenso verschiebt eine Veränderung des Effekt das Konfidenzintervall, eine Erhöhung der Streuung macht das konfidenzintervall breiter, eine sinkende Streeung macht das konfidenzintervall schmaller. bei der Fallzahl verhält es sich umgekehrt. Eine Erhöhung der Fallzahl macht das Konfidenzintervall schmaller und eine sinkende Fallzahl das Konfidenzintervall breiter.\n\n\n\nTabelle 18.4— Zusammenhang von der Teststatistik \\(T_{calc}\\) und dem p-Wert \\(Pr(\\geq T_{calc}|H_0)\\) sowie dem \\(KI_{1-\\alpha}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\n\n\\(\\Delta \\uparrow\\)\nsteigt\nsinkt\nverschoben\n\\(\\Delta \\downarrow\\)\nsinkt\nsteigt\nverschoben\n\n\n\\(s \\uparrow\\)\nsinkt\nsteigt\nbreiter\n\\(s \\downarrow\\)\nsteigt\nsinkt\nschmaller\n\n\n\\(n \\uparrow\\)\nsteigt\nsinkt\nschmaller\n\\(n \\downarrow\\)\nsinkt\nsteigt\nbreiter"
  },
  {
    "objectID": "stat-tests-theorie.html",
    "href": "stat-tests-theorie.html",
    "title": "19  Die Testtheorie",
    "section": "",
    "text": "Version vom Oktober 12, 2022 um 14:12:58\nWir können auf allen Daten einen statistischen Test rechnen und erhalten statistische Maßzahlen wie eine Teststatistik, einen p-Wert oder ein 95% Konfidenzintervall. Wie wir aus dem vorherigen Kapitel wissen testet jeder statistische Test eine Nullhypothese. Ob diese Hypothese dem Anwender nun bekannt ist oder nicht, ein statistischer Test testet eine Nullhypothese. Daher müssen wir uns immer klar sein, was die entsprechende Nullhypothese zu unserer Fragestellung ist.\nIn diesem Kapitel wollen wir uns nochmal tiefer mit der Testtherorie und dem \\(\\alpha\\)-Fehler und der \\(\\beta\\)-Fehler beschäftigen. Was heißt eigentlich einseitig oder zweiseitig Testen? Auch müssen wir nochmal einen Blick auf das mutliple Testen und die \\(\\alpha\\)-Adjustierung werfen.\nWir wieder holen nochmal. Die Nullhypothese \\(H_0\\) nennen wir auch die Null oder Gleichheitshypothese. Die Nullhypothese sagt aus, dass zwei Gruppen gleich sind oder aber kein Effekt zu beobachten ist.\n\\[\nH_0: \\bar{y}_{1} = \\bar{y}_{2}\n\\]\nDie Alternativehypothese \\(H_A\\) oder \\(H_1\\) auch Alternative genannt nennen wir auch Unterschiedshypothese. Die Alternativehypothese besagt, dass ein Unterschied vorliegt oder aber ein Effekt vorhanden ist.\n\\[\nH_A: \\bar{y}_{1} \\neq \\bar{y}_{2}\n\\]\nEs ist wichtig sich in Erinnerung zu rufen, dass wir nur und ausschließlich Aussagen über die Nullhypothese treffen können. Das frequentistische Hypothesentesten kann nichts anders. Wir kriegen keine Aussage über die Alternativhypothese sondern nur eine Abschätzung der Wahrscheinlichkeit des Auftretens der Daten im durchgeführten Experiment, wenn die Nullhypothese wahr wäre."
  },
  {
    "objectID": "stat-tests-theorie.html#sec-alpha-beta",
    "href": "stat-tests-theorie.html#sec-alpha-beta",
    "title": "19  Die Testtheorie",
    "section": "\n19.1 Der \\(\\alpha\\)-Fehler und der \\(\\beta\\)-Fehler",
    "text": "19.1 Der \\(\\alpha\\)-Fehler und der \\(\\beta\\)-Fehler\nVielleicht ist die Idee der Testtheorie und der Testentscheidung besser mit der Analogie des Rauchmelders zu verstehen. Wir nehmen an, dass der Rauchmelder der statistische Test ist. Der Rauchmelder hängt an der Decke und soll entscheiden, ob es brennt oder nicht. Daher muss der Rauchmelder entscheiden, die Nullhypothese “kein Feuer” abzulehnen oder die Hypothese “kein Feuer” beizubehalten.\n\\[\n\\begin{aligned}\nH_0&: \\mbox{kein Feuer im Haus}  \\\\  \nH_A&: \\mbox{Feuer im Haus}  \\\\   \n\\end{aligned}\n\\]\nWir können jetzt den Rauchmelder einstellen, so dass der Rauchmelder bei einer Kerze losgeht oder erst bei einem Stubenbrand. Wie sensibel auf Rauch wollen wir den Rauchmelder einstellen? Soll der Rauchmelder sofort die Nullhypothese ablehnen? Wenn also nur eine Kerze brennt. Soll also der \\(\\alpha\\)-Fehler groß sein? Das wäre nicht sehr sinnvoll. Due Feuerwehr würde schon bei einer Kerze kommen oder wenn wir mal was anbrennen. Wir dürfen also den \\(\\alpha\\)-Fehler nicht zu groß einstellen.\nIntuitiv würde man meinen, ein sehr kleiner \\(\\alpha\\)-Fehler nun sinnvoll sei. Wenn wir aber den Rauchmelder sehr unsensibel einstellen, also der Rauchmelder erst bei sehr viel Rauch die Nullhypothese ablehnt, könnte das Haus schon unrettbar in Flammen stehen. Dieser Fehler, Haus steht in Flammen und der Rauchmelder geht nicht, wird als \\(\\beta\\)-Fehler bezeichnet. Wie du siehst hängen die beiden Fehler miteinander zusammen. Wichtig hierbei ist immmer, dass wir uns einen Zustand vorstellen, das Haus brent nicht (\\(H_0\\) ist wahr) oder das Haus brennt nicht (\\(H_A\\) ist wahr). An diesem Zustand entscheiden wir dann, wie hoch der Fehler jeweils sein soll diesen Zustand zu übersehen.\n\n\n\n\n\n\nDer \\(\\alpha\\)-Fehler und \\(\\beta\\)-Fehler als Rauchmelderanalogie\n\n\n\nHäufig verwirrt die etwas theoretische Herangehensweise an den \\(\\alpha\\)-Fehler und \\(\\beta\\)-Fehler. Wir versuchen hier nochmal die Analogie eines Rauchmelders und dem Feuer im Haus.\n\n\nAbbildung 19.1— Andere Art der Darstellung des \\(\\alpha\\)-Fehlers als Alarm without fire und dem \\(\\beta\\)-Fehler als Fire without alarm. Je nachdem wie empfindlich wir den Alarm des Rauchmelders (den statistischen Test) über das \\(\\alpha\\) einstellen, desto mehr Alarm bekommen wir ohne das ein Effekt vorhanden wäre. Drehen wir den Alarm zu niedrig, dann kriegen wir kein Feuer mehr angezeigt, den \\(\\beta\\)-Fehler.\n\n\n\n\n\\(\\boldsymbol{\\alpha}\\)-Fehler: Alarm without fire. Der statistische Test schlägt Alarm und wir sollen die \\(H_0\\) ablehnen, obwohl die \\(H_0\\) in Wahrheit gilt und kein Effekt vorhanden ist.\n\n\\(\\boldsymbol{\\beta}\\)-Fehler: Fire without alarm. Der statistische Test schlägt nicht an und wir sollen die \\(H_0\\) beibehalten, obwohl die \\(H_0\\) in Wahrheit nicht gilt und ein Effekt vorhanden ist.\n\n\n\nWie sieht nun die Lösung, erstmal für unseren Rauchmelder, aus? Wir müssen Grenzen für den \\(\\alpha\\) und \\(\\beta\\)-Fehler festlegen.\nWir setzen den \\(\\alpha\\)-Fehler auf 5%.\n\nWir setzen den \\(\\alpha\\)-Fehler auf 5%. Somit haben wir in 1 von 20 Fällen das Problem, dass uns der Rauchmelder angeht obwohl gar kein Feuer da ist. Wir lehnen die Nullhypothese ab, obwohl die Nullhypothese gilt.\n\nWir setzen den \\(\\beta\\)-Fehler auf 20%.\n\nAuf der anderen Seite setzen wir den \\(\\beta\\)-Fehler auf 20%. Damit brennt uns die Bude in 1 von 5 Fällen ab ohne das der Rauchmelder einen Pieps von sich gibt. Wir behalten die Nullhypothese bei, obwohl die Nullhypothese nicht gilt.\n\nNachdem wir uns die Testentscheidung mit der Analogie des Rauchmelders angesehen haben, wollen wir uns wieder der Statistik zuwenden. Betrachten wir das Problem nochmal von der theoretischen Seite mit den statistischen Fachbegriffen.\nSoweit haben wir es als gegeben angesehen, dass wir eine Testentscheidung durchführen. Entweder mit der Teststatistik, dem \\(p\\)-Wert oder dem 95% Konfidenzintervall. Immer wenn wir eine Entscheidung treffen, können wir auch immer eine falsche Entscheidung treffen. Wie wir wissen hängt die berechnete Teststatistik \\(T_{calc}\\) nicht nur vom Effekt \\(\\Delta\\) ab sondern auch von der Streuung \\(s\\) und der Fallzahl \\(n\\). Auch können wir den falschen Test wählen oder Fehler im Design des Experiments gemacht haben. Schlussendlich gibt es viele Dinge, die unsere simple mathematischen Formeln beeinflussen können, die wir nicht kennen. Ein frequentistischer Hypothesentest gibt immer nur eine Aussage über die Nullhypothese wieder. Also ob wir die Nullhypothese ablehnen können oder nicht.\nAbbildung 19.2 zeigt die theoretische Verteilung der Nullyhypothese und der Alternativehypothese. Wenn die beiden Verteilungen sehr nahe beieinander sind, wird es schwer für den statistischen Test die Hypothesen klar voneinander zu trennen. Die Verteilungen überlappen. Es gibt einen sehr kleinen Unterschied in den Sprungweiten zwischen Hunde- und Katzenflöhen.\n\n\n\nAbbildung 19.2— Darstellung der Null- und Alternativehypothese. Mit steigendem \\(T_{calc}\\) wird die Wahrscheinlichkeit für die \\(H_0\\) immer kleiner. Leider ist uns nichts über \\(H_A\\) und deren Lage bekannt. Sollte die \\(H_A\\) Verteilung zu weit nach links ragen, könnten wir die \\(H_0\\) beibehalten, obwohl die \\(H_A\\) gilt.\n\n\n\nAchtung In der Regression wird uns auch wieder das \\(\\beta\\) als Symbol begegnen. In der statistischen Testtheorie ist das \\(\\beta\\) ein Fehler; in der Regression ist das \\(\\beta\\) ein Koeffizient der Regression. Hier ist der Kontext wichtig.\nWir können daher bei statistischen Testen zwei Arten von Fehlern machen. Zum einen den \\(\\alpha\\) Fehler oder auch Type I Fehler genannt. Zum anderen den \\(\\beta\\) Fehler oder auch Type II Fehler genannt. Die Grundidee basiert darauf, dass wir eine Testentscheidung gegen die Nullhypothese machen. Diese Entscheidung kann richtig sein, da in Wirklichkeit die Nullhypothese gilt oder aber falsch sein, da in Wirklichkeit die Nullhypothese nicht gilt. In Abbildung 19.3 wird der Zusammenhang in einer 2x2 Tafel veranschaulicht.\n\n\n\nAbbildung 19.3— Zusammenhang zwischen der Testentscheidung gegen die \\(H_0\\) Hypothese sowie dem Beibehalten der \\(H_0\\) Hypothese und der unbekannten Wahrheit in der die \\(H_0\\) falsch sein kann oder die \\(H_0\\) wahr sein kann. Wir können mit unserer Testenstscheidung richtig liegen oder falsch. Mit welcher Wahrscheinlichkeit geben der \\(\\alpha\\) Fehler und \\(\\beta\\) Fehler wieder. Unten rechts ist der Zusammenhang zu der Abbildung 19.2 gezeigt.\n\n\n\n\n\nDie Diskussion über den \\(p\\)-Wert und dem Vergleich mit dem \\(\\alpha\\)-Fehler wird in der Statistik seit 2019 verstärkt diskutiert (Wasserstein, Schirm, und Lazar 2019). Das Nullritual wird schon lamge kritisiert (Gigerenzer, Krauss, und Vitouch 2004). Siehe dazu auch The American Statistician, Volume 73, Issue sup1 (2019).\nBeide Fehler sind Kulturkonstanten. Das heißt, dass sich diese Zahlen von 5% und 20% so ergeben haben. Es gibt keinen rationalen Grund diese Zahlen so zu nehmen. Man kann eigentlich sagen, dass die 5% und die 20% eher einem Zufall entsprungen sind, als einer tieferen Rationalen. Wir behalten diese beiden Zahlen bei aus den beiden schlechtesten Gründe überhaupt: i) es wurde schon immer so gemacht und ii) viele machen es so.\nEine weitere wichtige statistische Maßzahl im Kontext der Testtheorie ist die \\(Power\\) oder auch \\(1-\\beta\\). Die \\(Power\\) ist die Gegenwahrscheinlichkeit von dem \\(\\beta\\)-Fehler. In der Analogie des Rauchmelders wäre die \\(Power\\) daher Alarm with fire. Das heißt, wie wahrscheinlich ist es einen wahren Effekt - also einen Unterschied - mit dem statistischen Test auch zu finden. Oder anders herum, wenn wir wüssten, dass die Hunde- und Katzenflöhe unterschiedliche weit springen, mit welcher Wahrscheinlichkeit würde diesen Unterschied ein statistsicher Test auch finden? Mit eben der \\(Power\\), also gut 80%. Tabelle 19.1 zeigt die Abhängigkeit der \\(Power\\) vom Effekt \\(\\Delta\\), der Streuung \\(s\\) und der Fallzahl \\(n\\).\nDie \\(Power\\) ist eine Wahrscheinlichkeit und sagt nichts über die Relevanz des Effektes aus.\n\n\nTabelle 19.1— Abhängigkeit der \\(Power (1-\\beta)\\) vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\). Die \\(Power\\) ist eine Wahrscheinlichkeit und sagt nichts über die Relevanz des Effektes aus.\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{Power (1-\\beta)}\\)\n\n\\(\\boldsymbol{Power (1-\\beta)}\\)\n\n\n\n\\(\\Delta \\uparrow\\)\nsteigt\n\\(\\Delta \\downarrow\\)\nsinkt\n\n\n\\(s \\uparrow\\)\nsinkt\n\\(s \\downarrow\\)\nsteigt\n\n\n\\(n \\uparrow\\)\nsteigt\n\\(n \\downarrow\\)\nsinkt"
  },
  {
    "objectID": "stat-tests-theorie.html#sec-einseitig-zweiseitig",
    "href": "stat-tests-theorie.html#sec-einseitig-zweiseitig",
    "title": "19  Die Testtheorie",
    "section": "\n19.2 Einseitig oder zweiseitig?",
    "text": "19.2 Einseitig oder zweiseitig?\nManchmal kommt die Frage auf, ob wir einseitig oder zweiseitig einen statistischen Test durchführen wollen. Beim Fall des zweiseitigen Testens verteilen wir den \\(\\alpha\\)-Fehler auf beide Seiten der Testverteilung mit jeweils \\(\\cfrac{\\alpha}{2}\\). In dem Fall des einseitigen Tests liegt der gesamte \\(\\alpha\\)-Fehler auf der rechten oder linken Seite der Testverteilung. In Abbildung 19.4 wird der Zusammenhang beispielhaft an der t-Verteilung gezeigt.\n\n\n\nAbbildung 19.4— Zusammenhang zwischen dem einseitigen und zweiseitigen Testen. Im Falle des zweiseitigen Testens teilen wir den \\(\\alpha\\)-Fehler auf beide Seiten der beispielhaften t-Verteilung auf. Im Falle des einseitigen Testen leigt der gesamte \\(\\alpha\\)-Fehler auf der rechten oder der linken Seite der t-Verteilung.\n\n\n\nIn der Anwendung testen wir immer zweiseitig.\nIn der Anwendung testen wir immer zweiseitig. Der Grund ist, dass das Vorzeichen von der Teststatik davon abhängt, welche der beiden Gruppen den größeren Mittelwert hat. Da wir die Mittelwerte vor der Auswertung nicht kennen, können wir auch nicht sagen in welche Richtung der Effekt und damit die Teststatistik laufen wird.\nEs gibt theoretisch Gründe, die für ein einseitiges Testen unter bestimmten Bedingungen sprechen, aber wir nutzen in der Anwendung nur das zweiseite Testen. Wir müssen dazu in R auch nichts weiter angeben. Ein durchgeführter statistischer Test in R testet automatisch immer zweiseitig.\n\n\n\n\n\n\nEinseitig oder zweiseitig im Spiegel der Regulierungsbehörden\n\n\n\nIn den allgemeinen Methoden des IQWiG, einer Regulierungsbehörde für klinische Studien, wird grundsätzlich das zweiseitige Testen empfohlen. Wenn einseitig getestet werden sollte, so soll das \\(\\alpha\\)-Niveau halbiert werden. Was wiederum das gleiche wäre wie zweiseitiges Testen - nur mit mehr Arbeit.\nZur besseren Vergleichbarkeit mit 2-seitigen statistischen Verfahren wird in einigen Guidelines für klinische Studien eine Halbierung des üblichen Signifikanzniveaus von 5 % auf 2,5 % gefordert. – Allgemeine Methoden Version 6.1 vom 24.01.2022, p. 180"
  },
  {
    "objectID": "stat-tests-theorie.html#sec-statistisches-testen-alpha-adjust",
    "href": "stat-tests-theorie.html#sec-statistisches-testen-alpha-adjust",
    "title": "19  Die Testtheorie",
    "section": "\n19.3 Adjustierung für multiple Vergleiche",
    "text": "19.3 Adjustierung für multiple Vergleiche\nDas simultane Testen von mehreren Hypothesen führt zu einer \\(\\alpha\\)-Fehler Inflation\nIm Kapitel 29 werden wir mehrere multiple Gruppenvergleiche durchführen. Das heißt, wir wollen nicht nur die Sprungweite von Hunde- und Katzenflöhen miteinander vergleichen, sondern auch die Sprungweite von Hunde- und Fuchsflöhen sowie Katzen- und Fuchsflöhen. Wir würden also \\(k = 3\\) t-Tests für die Mittelwertsvergleiche rechnen.\nDieses mehrfache Testen führt aber zu einer Inflation des \\(\\alpha\\)-Fehlers oder auch Alphafehler-Kumulierung genannt. Daher ist die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird, nicht mehr durch das Signifikanzniveau \\(\\alpha\\) kontrolliert, sondern kann sehr groß werden.\nGehen wir von einer Situation mit \\(k\\) Null- und Alternativhypothesen aus. Wir rechnen also \\(k\\) statistische Tests und alle Nullhypothesen werden zum lokalen Niveau \\(\\alpha_{local} = 0.05\\) getestet. Im Weiteren nehmen wir an, dass tatsächlich alle Nullhypothesen gültig sind. Wir rechnen also \\(k\\) mal einen t-Test und machen jedes mal einen 5% Fehler Alarm zu geben, obwohl kein Effekt vorhanden ist.\nDie Wahrscheinlichkeit für einen einzelnen Test korrekterweise \\(H_0\\) abzulehnen ist \\((1 − \\alpha)\\). Da die \\(k\\) Tests unabhängig sind, ist die Wahrscheinlichkeit alle \\(k\\) Tests korrekterweise abzulehnen \\((1 − \\alpha)^k\\). Somit ist die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird \\(1-(1-\\alpha)^k\\). In der Tabelle 19.2 wird dieser Zusammenhang nochmal mit Zahlen für verschiedene \\(k\\) deutlich.\n\n\nTabelle 19.2— Inflation des \\(\\alpha\\)-Fehlers. Wenn 50 Hypothesen getestet werden, ist die Wahrscheinlichkeit mindestens eine falsche Testentscheidung zu treffen fast sicher.\n\nAnzahl Test \\(\\boldsymbol{k}\\)\n\n\\(\\boldsymbol{1-(1-\\alpha)^k}\\)\n\n\n\n1\n0.05\n\n\n2\n0.10\n\n\n10\n0.40\n\n\n50\n0.92\n\n\n\n\nAus Tabelle 19.3 können wir entnehmen, dass wenn 100 Hypothesen getestet werden, werden 5 Hypothesen im Schnitt fälschlicherweise abgelehnt. Die Tabelle 19.3 ist nochmal die Umkehrung der vorherigen Tabelle 19.2.\n\n\nTabelle 19.3— Inflation des \\(\\alpha\\)-Fehlers. Erwartete Anzahl fälschlich abgelehnter Nullhypothesen abhängig von der Anzahl der durchgeführten Tests\n\nAnzahl Test \\(\\boldsymbol{k}\\)\n\n\\(\\boldsymbol{\\alpha \\cdot k}\\)\n\n\n\n1\n0.05\n\n\n20\n1\n\n\n100\n5\n\n\n200\n10\n\n\n\n\nNachdem wir verstanden haben, dass wiederholtes statistisches Testen irgendwann immer ein signifikantes Ergebnis produziert, müssen wir für diese \\(\\alpha\\) Inflation unsere Ergebnisse adjustieren. Ich folgenden stelle ich verschiedene Adjustierungsverfahren vor.\nWie können wir nun die p-Werte in R adjustieren? Zum einen passiert dies teilweise automatisch zum anderen müssen wir aber wissen, wo wir Informationen zu den Adjustierungsmethoden finden. Die Funktion p.adjust() ist hier die zentrale Anlaufstelle. Hier finden sich alle implementierten Adjustierungsmethoden in R.\nIm folgenden Code erschaffen wir uns 50 \\(z\\)-Werte von denen 25 aus einer Normalverteilung \\(\\mathcal{N}(0, 1)\\) und 25 aus einer Normalverteilung mit \\(\\mathcal{N}(3, 1)\\) kommen. Die Fläche unter allen Normalverteilungen ist Eins, da die Standatdabweichung Eins ist. Wir berechnen die \\(p-Wert\\) anhand der Fläche rechts von dem \\(z\\)-Wert. Wir testen zweiseitig, deshalb multiplizieren wir die \\(p\\)-Werte mit Zwei. Diese \\(p\\)-Werte können wir nun im Folgenden für die Adjustierung nutzen.\n\nz <- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))\np <- 2*pnorm(sort(-abs(z)))\n\nÜber die eckigen Klammern [] und das : können wir uns die ersten zehn p-Werte wiedergeben lassen.\n\np[1:10] %>% round(5)\n\n [1] 0.00000 0.00001 0.00007 0.00009 0.00027 0.00050 0.00124 0.00251 0.00324\n[10] 0.00442\n\n\nWir sehen, dass die ersten fünf p-Werte hoch signifikant sind. Das würden wir auch erwarten, immerhin haben wir ja auch 25 \\(z\\)-Werte mit einem Mittelwert von Drei. Du kannst dir den \\(z\\)-Wert wie den \\(t\\)-Wert der Teststatistik vorstellen.\n\n19.3.1 Bonferroni Korrektur\nDie Bonferroni Korrektur ist die am weitesten verbreitete Methode zur \\(\\alpha\\) Adjustierung, da die Bonferroni Korrektur einfach durchzuführen ist. Damit die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird beim simultanen Testen von \\(k\\) Hypothesen durch das globale (und multiple) Signifikanzniveau \\(\\alpha = 5\\%\\) kontrolliert ist, werden die Einzelhypothesen zum lokalen Signifikanzniveau \\(\\alpha_{local} = \\tfrac{\\alpha_{5\\%}}{k}\\) getestet.\nDabei ist das Problem der Bonferroni Korrektur, dass die Korrektur sehr konservativ ist. Wir meinen damit, dass das tatsächliche globale (und multiple) \\(\\sum\\alpha_{local}\\) Niveau liegt deutlich unter \\(\\alpha_{5\\%}\\) und somit werden die Nullhypothesen zu oft beibehalten.\n\n\n\n\n\n\nAdjustierung des \\(\\boldsymbol{\\alpha}\\)-Fehlers\n\n\n\n\nDas globale \\(\\alpha\\)-Level wird durch die Anzahl \\(k\\) an durchgeführten statistischen Tests geteilt.\n\n\\(\\alpha_{local} = \\tfrac{\\alpha}{k}\\) für die Entscheidung \\(p < \\alpha_{local}\\)\n\n\n\n\n\n\n\n\n\n\nAdjustierung des \\(\\boldsymbol{p}\\)-Wertes\n\n\n\n\nDie p-Werte werden mit der Anzahl an durchgeführten statistischen Tests \\(k\\) multipliziert.\n\n\\(p_{adjust} = p_{raw} \\cdot k\\) mit \\(k\\) gleich Anzahl der Vergleiche.\nwenn \\(p_{adjust} > 1\\), wird \\(p_{adjust}\\) gleich 1 gesetzt, da \\(p_{adjust}\\) eine Wahrscheinlichkeitist.\n\n\n\nWir schauen uns die ersten zehn nach Bonferroni adjustierten p-Wert nach der Anwendung der Funktion p.adjust() einmal an.\n\np.adjust(p, \"bonferroni\")[1:10] %>% round(3)\n\n [1] 0.000 0.000 0.004 0.005 0.014 0.025 0.062 0.126 0.162 0.221\n\n\nNach der Adjustierung erhalten wir weniger signifikante \\(p\\)-Werte als vor der Adjustierung. Wir sehen aber, dass wir weit weniger signifikante Ergebnisse haben, als wir eventuell erwarten würden. Wir haben immerhin 25 \\(z\\)-Werte mit einem Mittelwert von Drei. Nach der Bonferroni-Adjustierung hgaben wir nur noch sechs signifikante \\(p\\)-Werte.\n\n19.3.2 Benjamini-Hochberg\n\n\n\n\nhttp://www.biostathandbook.com/multiplecomparisons.html\n\np.adjust(p, \"BH\")[1:10] %>% round(3)\n\n [1] 0.000 0.000 0.001 0.001 0.003 0.004 0.009 0.016 0.018 0.022\n\n\n\n19.3.3 Dunn-Sidak\n\\(\\alpha_{local} = 1 - (1 - \\alpha)^{\\tfrac{1}{k}}\\)"
  },
  {
    "objectID": "stat-tests-theorie.html#referenzen",
    "href": "stat-tests-theorie.html#referenzen",
    "title": "19  Die Testtheorie",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nGigerenzer, Gerd, Stefan Krauss, und Oliver Vitouch. 2004. „The null ritual“. The Sage handbook of quantitative methodology for the social sciences, 391–408.\n\n\nWasserstein, Ronald L, Allen L Schirm, und Nicole A Lazar. 2019. „Moving to a world beyond ‚p< 0.05‘“. The American Statistician. Taylor & Francis."
  },
  {
    "objectID": "stat-tests-effect.html",
    "href": "stat-tests-effect.html",
    "title": "20  Der Effektschätzer",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:38:04\nDer Effektschätzer. Ein seltsames Kapitel, denn ich tue mich sehr schwer, dieses Kapitel irgendwo in die Linearität dieses Buches hier einzuordnen. Deshalb ist dieses Kapitel eigentlich immer an der falschen Stelle. Entweder hast du schon die statistischen Tests gelesen und du wüsstest gerne was die Effektschätzer sind oder du suchst hier nochmal die Beschreibung der Effektschätzer zum Beispiel aus der multiple Regression heraus. Also steht jetzt dieses Kapitel hier im Raum und du musst schauen, was du wirklich brauchst. Oder ob du dieses Kapitel erst überspringst und dann später nochmal hier liest.\nWenn wir einen der vielen Effektschätzer berechnen wollen, dann nutzen wir dafür die Effektschätzer aus dem R Paket effectsize. Das R Paket effectsize liefert Effektschätzer für fast alle statistischen Gelegenheiten. Wir werden hier wie immer nur den groben Überblick abdecken. Vermutlich wird das Kapitel dann noch Anwachsen. Streng genommen gehört das Kapitel 27 zu den diagnostischen Tests auf einer 2x2 Kreuztabelle auch irgendwie zu Effektschätzern. Wenn du Spezifität und Sensitivität suchst bist du in dem Kapitel zu diagnostischen Tests richtig.\nWir unterscheiden hier erstmal grob in zwei Arten von Effektschätzern:\nDaneben gibt es wie noch die Korrelation wie in Kapitel 32 beschrieben. Die Korrelation wollen wir aber in diesem Kapitel nicht vorgreifen bzw. wiederholen.\nAm Ende muss du immer den Effekt im Kontext der Fragestellung bzw. des Outcomes \\(y\\) bewerten. Der numerische Unterschied von \\(0.1\\) cm kann in einem Kontext viel sein. Das Wachstum von Bakterienkolonien kann ein Unterschied von \\(0.1\\) cm viel sein. Oder aber sehr wenig, wenn wir uns das Wachstum von Bambus pro Tag anschauen. Hier bist du gefragt, den Effekt in den Kontext richtig einzuordnen. Ebenso stellt sich die Frage, ob ein Unterschied von 6% viel oder wenig ist."
  },
  {
    "objectID": "stat-tests-effect.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-effect.html#genutzte-r-pakete-für-das-kapitel",
    "title": "20  Der Effektschätzer",
    "section": "\n20.1 Genutzte R Pakete für das Kapitel",
    "text": "20.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, see, effectsize)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-effect.html#unterschied-zweier-mittelwerte",
    "href": "stat-tests-effect.html#unterschied-zweier-mittelwerte",
    "title": "20  Der Effektschätzer",
    "section": "\n20.2 Unterschied zweier Mittelwerte",
    "text": "20.2 Unterschied zweier Mittelwerte\nWir berechnen zwei Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\). Wenn wir wissen wollen wie groß der Effekt zwischen den beiden Mittelwerten ist, dann bilden wir die Differenz. Wir berechnen das \\(\\Delta_{y_1-y_2}\\) für \\(y_1\\) und \\(y_2\\) indem wir die beiden Mittelwerte voneinander abziehen.\n\\[\n\\Delta_{y_1-y_2} = \\bar{y}_1 - \\bar{y}_2\n\\]\nWarum schreiben wir hier vermutlich? Ein statistischer Test ist eine Funktion von \\(\\Delta\\), \\(s\\) und \\(n\\). Wir können auch mit kleinem \\(\\Delta\\) die Nullhypothese ablehnen, wenn \\(s\\) und \\(n\\) eine passende Teststatistik generieren. Siehe dazu auch das Kapitel 18.3.\nWenn es keinen Unterschied zwischen den beiden Mittelwerten \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) gibt, dann ist die Differenz \\(\\Delta_{y_1-y_2} = \\bar{y}_1 - \\bar{y}_2\\) gleich 0. Wir sagen, die Nullhypothese vermutlich gilt, wenn die Differenz klein ist. Was wir besser annehmen können ist, dass die Relevanz klein ist. Effekt mit einem geringen Mittelwertsunterschied sind meistens nicht relevant. Aber diese Einschätzung hängt stark von der Fragestellung ab.\n\\[\nH_0: \\Delta_{y_1-y_2} = \\bar{y}_1 - \\bar{y}_2 = 0\n\\]\nIn Tabelle 20.1 ist nochmal ein sehr simples Datenbeispiel gegeben an dem wir den Zusammenhang nochmal nachvollziehen wollen.\n\n\n\n\nTabelle 20.1— Beispiel für die Berechnung von einem Mittelwertseffekt an der Sprunglänge [cm] von Hunde und Katzenflöhen.\n\nanimal\njump_length\n\n\n\ncat\n8.0\n\n\ncat\n7.9\n\n\ncat\n8.3\n\n\ncat\n9.1\n\n\ndog\n8.0\n\n\ndog\n7.8\n\n\ndog\n9.2\n\n\ndog\n7.7\n\n\n\n\n\n\nNehmen wir an, wir berechnen für die Sprungweite [cm] der Hundeflöhe einen Mittelwert von \\(\\bar{y}_{dog} = 8.2\\) und für die Sprungweite [cm] der Katzenflöhe einen Mittelwert von \\(\\bar{y}_{cat} =8.3\\). Wie große ist nun der Effekt? Oder anders gesprochen, welchen Unterschied in der Sprungweite macht es aus ein Hund oder eine Katze zu sein? Was ist also der Effekt von animal? Wir rechnen \\(\\bar{y}_{dog} - \\bar{y}_{cat} = 8.2 - 8.3 = -0.1\\). Zum einen wissen wir jetzt “die Richtung”. Da wir ein Minus vor dem Mittelwertsunterschied haben, müssen die Katzenflöhe weiter springen als die Hundeflöhe, nämlich 0.1 cm. Dennoch ist der Effekt sehr klein.\n\n20.2.1 Cohen’s d\nDa der Mittlwertsunterschied alleine nnur eine eingeschränkte Aussage über den Effekt erlaubt, gibt es noch Effektschätzer, die den Mittelwertsunterschied \\(\\Delta_{y_1-y_2}\\) mit der Streuung \\(s^2\\) sowie der Fallzahl zusammenbringt. Der bekannteste Effektschätzer für einen Mittelwertsunterschied bei großer Fallzahl mit mehr als 20 Beobachtungen ist Cohen’s d. Wir können Cohen’s d wie folgt berechnen.\n\\[\n|d| = \\cfrac{\\bar{y}_1-\\bar{y}_2}{\\sqrt{\\cfrac{s_1^2+s_2^2}{2}}}\n\\]\nWenn wir die berechneten Mittelwerte und die Varianz der beiden Gruppen in die Formel einsetzten ergibt sich ein absolutes Cohen’s d von 0.24 für den Gruppenvergleich.\n\\[\n|d| = \\cfrac{8.2 - 8.3}{\\sqrt{(0.5^2+0.3^2) /2}} = \\cfrac{-0.1}{0.41} = \\lvert-0.24\\rvert\n\\]\n\n\nMehr Informationen zu Cohen’s d gibt es auf der Hilfeseite von effectsize: Interpret standardized differences\nWas denn nun Cohen’s d exakt aussagt, kann niemand sagen. Aber wir haben einen Wust an möglichen Grenzen. Hier soll die Grenzen von Cohen (1988) einmal angegeben werden. Cohen (1988) hat in seiner Arbeit folgende Grenzen in Tabelle 20.2 für die Interpretation von \\(d\\) vorgeschlagen.\n\n\nTabelle 20.2— Interpretation der Effektstärke nach Cohen (1988).\n\nCohen’s d\nInterpretation des Effekts\n\n\n\n\\(d < 0.2\\)\nSehr klein\n\n\n\\(0.2 \\leq d < 0.5\\)\nKlein\n\n\n\\(0.5 \\leq d < 0.8\\)\nMittel\n\n\n\\(d \\geq 0.8\\)\nStark\n\n\n\n\nWir können auch über die Funktion cohens_d() Cohen’s d einfach in R berechnen. Die Funktion cohens_d() akzeptiert die Formelschreibweise. Die 95% Konfidenzintervalle sind mit Vorsicht zu interpretieren. Denn die Nullhypothese ist hier nicht so klar formuliert. Wir lassen also die 95% Konfidenzintervalle erstmal hier so stehen.\n\ncohens_d(jump_length ~ animal, data = data_tbl, pooled_sd = TRUE)\n\nCohen's d |        95% CI\n-------------------------\n0.24      | [-1.16, 1.62]\n\n- Estimated using pooled SD.\n\n\nDankenswerterweise gibt es noch die Funktion interpret_cohens_d, die es uns erlaubt auszusuchen nach welche Literturquelle wir den Wert von Cohen’s d interpretieren wollen. Ob dieser Effekt relevant zur Fragestellung ist musst du selber entscheiden.\n\ninterpret_cohens_d(0.24, rules = \"cohen1988\")\n\n[1] \"small\"\n(Rules: cohen1988)\n\n\n\n20.2.2 Hedges’ g\nSoweit haben wir uns mit sehr großen Fallzahlen beschäftigt. Cohen’s d ist dafür auch sehr gut geeigent und wenn wir mehr als 20 Beobachtungen haben, können wir Cohen’s d auch gut anwenden. Wenn wir weniger Fallzahl vorliegen haben, dann können wir Hedges’ g nutzen. Hedges’ g bietet eine Verzerrungskorrektur für kleine Stichprobengrößen (\\(N < 20\\)) sowie die Möglichkeit auch für unbalanzierte Gruppengrößen einen Effektschätzeer zu berechnen. Die Formel sieht mit dem Korrekturterm recht mächtig aus.\n\\[\ng = \\cfrac{\\bar{y}_1 - \\bar{y}_2}{s^*} \\cdot \\left(\\cfrac{N-3}{N-2.25}\\right) \\cdot \\sqrt{\\cfrac{N-2}{N}}\n\\]\nmit\n\\[\ns^* = \\sqrt{\\cfrac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\n\\]\nWir können aber einfach die Mittelwerte und die Varianzen aus unserem beispiel einsetzen. Da unsere beiden Gruppen gleich groß sind \\(n_1 = n_2\\) und damit ein balanziertes Design vorliegt, sind Cohen’s d und Hedges’ g numerisch gleich. Wir können dann noch für die geringe Fallzahl korrigieren und erhalten ein händisches \\(g = 0.18\\).\n\\[\ng = \\cfrac{8.2 - 8.3}{0.41} \\cdot \\left(\\cfrac{8-3}{8-2.25}\\right) \\cdot \\sqrt{\\cfrac{8-2}{8}} = \\lvert-0.24\\rvert \\cdot 0.87 \\cdot 0.87 \\approx 0.18\n\\]\nmit\n\\[\ns^* = \\sqrt{\\cfrac{(4-1)\\cdot0.5^2 + (4-1)\\cdot0.3^2}{4+4-2}} = \\sqrt{\\cfrac{0.75 + 0.27}{6}} = 0.41\n\\]\nIn R gibt es die Funktion hedges_g() die uns erlaubt in der Formelschreibweise direkt Hedges’ g zu berechnen. Wir sehen hier eine Abweichung von unserer händischen Rechnung. Das ist aber in soweit nicht ungewöhnlich, da es noch eine Menge Varianten der Anpassung für die geringe Fallzahl gibt. In der Anwendung nutzen wir die Funktion aus dem Paket effectsize wie hier durchgeführt.\nWir ignorieren wie auch bei Cohen’s d das 95% Konfidenzintervall, da die Interpretation ohne die Nullhypothese nicht möglich ist. Die Nullhypothese ist in diesem Fall komplexer. Wir lassen daher das 95% Konfidenzintervall erstmal einfach hier so stehen.\n\nhedges_g(jump_length ~ animal, data = data_tbl, pooled_sd = TRUE)\n\nHedges' g |        95% CI\n-------------------------\n0.21      | [-1.01, 1.41]\n\n- Estimated using pooled SD.\n\n\nAuch für Hedges’ g gibt es die Möglichkeit sich über die Funktion interpret_hedges_g() den Wert von \\(g=0.21\\) interpretieren zu lassen. Nach Sawilowsky (2009) haben wir hier einen kleinen Effekt vorliegen. Ob dieser Effekt relevant zur Fragestellung ist musst du selber entscheiden.\n\ninterpret_hedges_g(0.21, rules = \"sawilowsky2009\")\n\n[1] \"small\"\n(Rules: sawilowsky2009)\n\n\nDie Hilfeseite zu dem Paket effectsize bietet eine Liste an möglichen Referenzen für die Wahl der Interpretation der Effektstärke. Du musst dann im Zweifel schauen, welche der Quellen und damit Grenzen du nutzen willst."
  },
  {
    "objectID": "stat-tests-effect.html#unterschied-zweier-anteile",
    "href": "stat-tests-effect.html#unterschied-zweier-anteile",
    "title": "20  Der Effektschätzer",
    "section": "\n20.3 Unterschied zweier Anteile",
    "text": "20.3 Unterschied zweier Anteile\nEine Wahrscheinlichkeit und eine Chance sind nicht das Gleiche. Mehr in diesem Abschnitt.\nNeben den Unterschied zweier Mittelwerte ist auch häufig das Interesse an dem Unterschied zwischen zwei Anteilen. Nun unterscheiden wir zwischen Wahrscheinlichkeiten und Chancen. Beide Maßzahlen, die Wahrscheinlichkeit wie auch die Chance, beschreiben einen Anteil. Hier tritt häufig Verwirrung auf, daher hier zuerst ein Beispiel.\nWir behandelt \\(n = 65\\) Hunde mit dem Antiflohmittel FleaEx. Um die Wirkung von FleaEx auch bestimmen zu können haben wir uns zwei Gruppen von Hunden ausgesucht. Wir haben Hunde, die mit Flöhe infiziert sind und Hunde, die nicht mit Flöhen infiziert sind. Wir schauen nun in wie weit FleaEx gegen Flöhe hilft im Vergleich zu einer Kontrolle.\n\n\nTabelle 20.3— Eine 2x2 Tabelle als Beispiel für unterschiedliche Flohinfektionen bei nach einer Behandlung mit FleaEx für die Berechnung von Effektschätzern eines Anteils.\n\n\n\n\nGroup\n\n\n\n\n\nFleaEx\nControl\n\n\nInfected\nYes (1)\n\\(18_{\\;\\Large a}\\)\n\\(23_{\\;\\Large b}\\)\n\n\n\nNo (0)\n\\(14_{\\;\\Large c}\\)\n\\(10_{\\;\\Large d}\\)\n\n\n\n\nAus der Tabelle 20.3 können wir entnehmen, dass 18 behandelte Hunde mit Flöhen infiziert sind und 14 Hunde keine Infektion aufweisen. Bei den Hunden aus der Kontrolle haben wir 23 infizierte und 10 gesunde Tiere beobachtet.\nEs gibt verschiedene Typen von klinischen Studien, also Untersuchungen an Menschen. Einige Studien liefern nur \\(OR\\) wieder andere Studientypen liefern \\(RR\\).\nWir können nun zwei Arten von Anteilen berechnen um zu beschreiben, wie sich der Anteil an infizierten Hunden verhält. Das bekanntere ist die Frequenz oder Wahrscheinlichkeit oder Risk Ratio (\\(RR\\)). Das andere ist das Chancenverhältnis oder Odds Ratio (\\(OR\\)). Beide kommen in der Statistik vor und sind unterschiedlich zu interpretieren.\nUm die die Odds Ratio und die Risk Ratios auch in R berechnen zu können müssen wir einmal die 2x2 Kreuzabelle in R nachbauen. Wir nutzen dafür die Funktion matrix() und müssen schauen, dass die Zahlen in der 2x2 Kreuztabelle in R dann auch so sind, wie in der Datentabelle. Das ist jetzt ein schöner Codeblock, ist aber dafür da um sicherzustellen, dass wir die Zahlen richtig eintragen.\n\ncross_mat <- matrix(c(18, 23, 14, 10),\n  nrow = 2, byrow = TRUE,\n  dimnames = list(\n    Infected = c(\"Yes\", \"No\"),\n    Group = c(\"FleaEx\", \"Control\")\n  )\n)\n\ncross_mat\n\n        Group\nInfected FleaEx Control\n     Yes     18      23\n     No      14      10\n\n\n\n\nGeorge, Stead, und Ganti (2020) liefert eine gute Übersicht über What’s the risk: differentiating risk ratios, odds ratios, and hazard ratios?\nSpäter werden wir das \\(OR\\) und \\(RR\\) wieder treffen. Das \\(OR\\) kommt in der logistsichen Regression als Effektschätzer vor. Wir nutzen das \\(RR\\) als Effektschätzer in der Poissonregression.\n\n20.3.1 Wahrscheinlichkeitsverhältnis oder Risk Ratio (RR)\nWir berechnen wir nun das Wahrscheinlichkeitsverhältnis oder Risk Ratio (RR)? Das Risk Ratio ist das Verhältnis von den infizierten Hunden in der Behandlung (\\(a\\)) zu allen infizierten Hunden (\\(a+c\\)) zu dem Verhältnis der gesunden Hunde in der Behandlung (\\(b\\)) zu allen gesunden Hunden (\\(b+d\\)). Das klingt jetzt etwas wirr, deshlab helfen manchaml wirklich Formeln, den Zusammenhang besser zu verstehen.\n\\(Pr(\\mbox{FleaEx}|\\mbox{infected})\\) ist die Wahrscheinlichkeit infiziert zu sein, wenn der Hund mit FleaEx behandelt wurde.\n\\[\nPr(\\mbox{FleaEx}|\\mbox{infected}) = \\cfrac{a}{a+c} = \\cfrac{18}{18+14} \\approx 0.56\n\\]\n\\(Pr(\\mbox{Control}|\\mbox{infected})\\) ist die Wahrscheinlichkeit infiziert zu sein, wenn der Hund mit in der Kontrolle war.\n\\[\nPr(\\mbox{Control}|\\mbox{infected}) = \\cfrac{b}{b+d} = \\cfrac{23}{23 + 10} \\approx 0.70\n\\]\nDas Risk Ratio ist mehr oder minder das Verhältnis von der beiden Spalten der Tabelle 20.3 für die Behandlung. Wir erhalten also ein \\(RR\\) von \\(0.76\\). Damit mindert die Gabe von FleaEx die Wahrscheinlichkeit sich mit Flöhen zu infizieren.\n\\[\n\\Delta_{y_1/y_2} = RR = \\cfrac{Pr(\\mbox{FleaEx}|\\mbox{infected})}{Pr(\\mbox{Control}|\\mbox{infected})} =  \\cfrac{0.56}{0.70} \\approx 0.80\n\\]\nWir überprüfen kurz mit der Funktion riskratio() ob wir richtig gerechnet haben. Das 95% Konfidenzintervall können wir interpretieren, dafür brauchen wir aber noch einmal eine Idee was “kein Effekt” bei einem Risk Ratio heist.\n\nriskratio(cross_mat)\n\nRisk ratio |       95% CI\n-------------------------\n0.81       | [0.32, 2.01]\n\n\nWann liegt nun kein Effekt bei einem Anteil wie dem RR vor? Wenn der Anteil in der einen Gruppe genauso groß ist wie der Anteil der anderen Gruppe.\n\\[\nH_0: RR = \\cfrac{Pr(\\mbox{dog}|\\mbox{infected})}{Pr(\\mbox{cat}|\\mbox{infected})} = 1\n\\]\nWir interpretieren das \\(RR\\) nun wie folgt. Unter der Annahme, dass ein kausaler Effekt zwischen der Behandlung und dem Outcome besteht, können die Werte des relativen Risikos auf folgende Art und Weise interpretiert werden:\n\n\n\\(RR = 1\\) bedeutet, dass die Behandlung keinen Einfluss auf das Outcome hat\n\n\\(RR < 1\\) bedeutet, dass das Risiko für das Outcome durch die Behandlung verringert wird, was ein “Schutzfaktor” ist\n\n\\(RR > 1\\) bedeutet, dass das Risiko für das Outcome durch die Behandlung erhöht wird, was ein “Risikofaktor” ist.\n\nDas heist in unserem Fall, dass wir mit einem RR von \\(0.80\\) eine protektive Behandlung vorliegen haben. Die Gabe von FleaEx reduziert das Risiko mit Flöhen infiziert zu werden. Durch das 95% Konfidenzintervall wissen wir auch, dass das \\(RR\\) nicht signifikant ist, da die 1 im 95% Konfidenzintervall enthalten ist.\n\n20.3.2 Chancenverhältnis oder Odds Ratio (OR)\nNeben dem Risk Ratio gibt es noch das Odds Ratio. Das Odds Ratio ist ein Chancenverhältnis. Wenn der Mensch an sich schon Probleme hat für sich Wahrscheinlichkeiten richtig einzuordnen, scheitert man allgemein an der Chance vollkommen. Dennoch ist das Odds Ratio eine gute Maßzahl um abzuschätzen wie die Chancen stehen, einen infizierten Hund vorzufinden, wenn der Hund behandelt wurde.\nScaheun wir uns einmal die Formeln an. Im Gegensatz zum Risk Ratio, welches die Spalten miteinander vergleicht, vergleicht das Odds Ratio die Zeilen. Als erstes berechnen wir die Chance unter der Gabe von FleaEx infiziert zu sein wie folgt.\n\\[\nOdds(\\mbox{FleaEx}|\\mbox{infected}) = a:b = 18:23 = \\cfrac{18}{23} = 0.78\n\\] Dann berechnen wir die Chance in der Kontrollgruppe infiziert zu sein wie folgt.\n\\[\nOdds(\\mbox{Control}|\\mbox{infected}) = c:d = 14:10 = \\cfrac{14}{10} \\approx 1.40\n\\] Abschließend bilden wir das Chancenverhältnis der Chance unter der Gabe von FleaEx infiziert zu sein zu der Chance in der Kontrollgruppe infiziert zu sein. Es ergbit sich das Odds Ratio wie folgt.\n\\[\n\\Delta_{y_1/y_2} = OR =  \\cfrac{Odds(\\mbox{Flea}|\\mbox{infected})}{Odds(\\mbox{Control}|\\mbox{infected})} = \\cfrac{a \\cdot d}{b \\cdot c} = \\cfrac{0.78}{1.40} \\approx 0.56\n\\]\nWir überprüfen kurz mit der Funktion oddsratio() ob wir richtig gerechnet haben. Das 95% Konfidenzintervall können wir interpretieren, dafür brauchen wir aber noch einmal eine Idee was “kein Effekt” bei einem Odds Ratio heist.\n\noddsratio(cross_mat)\n\nOdds ratio |       95% CI\n-------------------------\n0.56       | [0.20, 1.55]\n\n\nWann liegt nun kein Effekt bei einem Anteil wie dem OR vor? Wenn der Anteil in der einen Gruppe genauso groß ist wie der Anteil der anderen Gruppe.\n\\[\nH_0: OR =  \\cfrac{Odds(\\mbox{dog}|\\mbox{infected})}{Odds(\\mbox{cat}|\\mbox{infected})} = 1\n\\]\nWir interpretieren das \\(OR\\) nun wie folgt. Unter der Annahme, dass ein kausaler Effekt zwischen der Behandlung und dem Outcome besteht, können die Werte des Odds Ratio auf folgende Art und Weise interpretiert werden:\n\n\n\\(OR = 1\\) bedeutet, dass die Behandlung keinen Einfluss auf das Outcome hat\n\n\\(OR < 1\\) bedeutet, dass sich die Chance das Outcome zu bekommen durch die Behandlung verringert wird, was ein “Schutzfaktor” ist\n\n\\(OR > 1\\) bedeutet, dass sich die Chance das Outcome zu bekommen durch die Behandlung erhöht wird, was ein “Risikofaktor” ist.\n\nDas heist in unserem Fall, dass wir mit einem OR von \\(0.56\\) eine protektive Behandlung vorliegen haben. Die Gabe von FleaEx reduziert die Chance mit Flöhen infiziert zu werden. Durch das 95% Konfidenzintervall wissen wir auch, dass das \\(OR\\) nicht signifikant ist, da die 1 im 95% Konfidenzintervall enthalten ist.\n\n20.3.3 Odds Ratio (OR) zu Risk Ratio (RR)\n\n\nGrant (2014) gibt nochmal eine wissenschaftliche Diskussion des Themas zur Konvertierung von OR zu RR.\nWenn wir das OR berechnet haben, wollen wir eventuell das \\(OR\\) in dem Sinne eines Riskoverhältnisses berichten. Leider ist es nun so, dass wir das nicht einfach mit einem \\(OR\\) machen können. Ein \\(OR\\) von 3.5 ist ein großes Chancenverhältnis. Aber ist es auch 3.5-mal so wahrscheinlich? Nein so einfach können wir das OR nicht interpretieren. Wir können aber das \\(OR\\) in das \\(RR\\) umrechnen. Dafür brauchen wir aber das \\(p_0\\). Dabei ist das \\(p_0\\) das Basisrisiko also die Wahrscheinlichkeit des Ereignisses ohne die Intervention. Wenn wir nichts tun würden, wie wahrscheinlich wäre dann das Auftreten des Ereignisses? Es ergibt sich dann die folgende Formel für die Umrechnung des \\(OR\\) in das \\(RR\\).\n\\[\nRR = \\cfrac{OR}{(1 - p_0 + (p_0 \\cdot OR))}\n\\]\nSchauen wir uns das einmal in einem Beispiel an. Wir nutzen für die Umrechnung die Funktion oddsratio_to_riskratio() aus dem R Paket effectsize. Wenn wir ein \\(OR\\) von 3.5 haben, so hängt das \\(RR\\) von dem Basisriskio ab. Wenn das Basisirisko für die Erkrankung ohne die Behandlung sehr hoch ist mit \\(p_0 = 0.85\\), dann ist das \\(RR\\) sehr klein.\n\nOR <- 3.5\nbaserate <- 0.85\n\noddsratio_to_riskratio(OR, baserate) %>% round(2)\n\n[1] 1.12\n\n\nAuf der anderen Seite nähert sich das \\(OR\\) dem \\(RR\\) an, wenn das Basisriskio für die Erkrankung mit \\(p_0 = 0.04\\) sehr klein ist.\n\nOR <- 3.5\nbaserate <- 0.04\n\noddsratio_to_riskratio(OR, baserate) %>% round(2)\n\n[1] 3.18\n\n\nWeil wir natürlich das Basisrisiko nur abschätzen können, verbleibt hier eine gewisse Unsicherheit, wie das \\(RR\\) zu einem gegebenen \\(OR\\) aussieht."
  },
  {
    "objectID": "stat-tests-effect.html#referenzen",
    "href": "stat-tests-effect.html#referenzen",
    "title": "20  Der Effektschätzer",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nGeorge, Andrew, Thor S Stead, und Latha Ganti. 2020. „What’s the risk: differentiating risk ratios, odds ratios, and hazard ratios?“ Cureus 12 (8).\n\n\nGrant, Robert L. 2014. „Converting an odds ratio to a range of plausible relative risks for better communication of research findings“. Bmj 348."
  },
  {
    "objectID": "stat-tests-ttest.html",
    "href": "stat-tests-ttest.html",
    "title": "21  Der t-Test",
    "section": "",
    "text": "Version vom October 13, 2022 um 14:38:36\nDer t-Test ist der bedeutende Test, wenn es um das Verständnis der Algorithmen und Konzepte in der Statistik geht. Wir haben den t-test schon genutzt um die Idee des statistischen Testens zu verstehen und wir werdend den t-Test auch im statistischen Modellieren wiedertreffen.\nWas macht also der t-Test? Der t-Test vergleicht die Mitfellwerte zweier Gruppen miteinander. Das heißt wir haben zwei Gruppen, wie Hunde und Katzen, und wollen nun wissen wie sich die Sprungweiten der Hundeflöhe im Mittel von den Katzenflöhen unterscheiden."
  },
  {
    "objectID": "stat-tests-ttest.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-ttest.html#genutzte-r-pakete-für-das-kapitel",
    "title": "21  Der t-Test",
    "section": "\n21.1 Genutzte R Pakete für das Kapitel",
    "text": "21.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, readxl)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-ttest.html#daten-für-den-t-test",
    "href": "stat-tests-ttest.html#daten-für-den-t-test",
    "title": "21  Der t-Test",
    "section": "\n21.2 Daten für den t-Test",
    "text": "21.2 Daten für den t-Test\nWichtig ist, dass wir schon jetzt die Modellschreibweise lernen um die Daten richtig nutzen zu können. Wir werden die Modelschreibweise immer wieder sehen und diese Art eine Abhängigkeit zu beschreiben ist sehr wichtig in den folgenden Kapiteln.\n\n\nAbbildung 21.1— Modellschreibweise \\(y\\) hängt ab von \\(x\\). Das \\(y\\) repräsentiert eine Spalte im Datensatz und das \\(x\\) repräsentiert ebenso eine Spalte im Datensatz. Wir brauchen also zwei Variablen \\(y\\) und \\(x\\), die natürlich nicht so heißen müssen.\n\n\nEtwas unbefriedigend, dass der t-Test nur zwei Gruppen miteinander Vergleichen kann. Mehr Gruppen gehen in der ANOVA im Kapitel 22\nWas brauchen wir dafür in R? Wir brauchen dafür eine Spalte \\(y\\) mit kontinuierlichen Zahlen und einer Spalte \\(x\\) in dem wir einen Faktor mit zwei Leveln finden. Jedes Level steht dann für eine der beiden Gruppen. Das war es schon. Schauen wir uns nochmal den Datensatz flea_dog_cat.xlsx in Tabelle 21.1 an und überlegen, wie wir das realisieren können.\n\n\n\n\nTabelle 21.1— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Boniturnote sowie der Infektionsstatus von Hunden und Katzen.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\ncat\n3.2\n12\n7\n1\n\n\ncat\n2.2\n13\n5\n0\n\n\ncat\n5.4\n11\n7\n0\n\n\ncat\n4.1\n12\n6\n0\n\n\ncat\n4.3\n16\n6\n1\n\n\ncat\n7.9\n9\n6\n0\n\n\ncat\n6.1\n7\n5\n0\n\n\n\n\n\n\nIn Abbildung 21.2 sehen wir einmal den Zusammenhang zwischen den Schreibweise \\(y \\sim x\\) und den beiden Variablen jump_length als \\(y\\) und animal als \\(x\\) aus dem Datensatz flea_dog_cat.xlsx. Wir haben also die formula Schreibweise in R als jump_length ~ animal.\n\n\nAbbildung 21.2— Modellschreibweise bzw. formula Schreibweise in R. Die Variable \\(y\\) hängt ab von \\(x\\) am Beispiel des Datensatzes flea_dog_cat.xlsx mit den beiden Variablen jump_length als \\(y\\) und animal als \\(x\\).\n\n\nWir benötigen für den t-Test ein normalverteiltes \\(y\\) und einen Faktor mit zwei Leveln als \\(x\\). Wir nehmen daher mit select()die Spalte jump_length und animal aus dem Datensatz flea_dog_cat.xlsx. Wichtig ist, dass wir die Spalte animal mit der Funktion as_factor() in einen Faktor umwandeln. Anschließend speichern wir die Auswahl in dem Objekt data_tbl.\n\ndata_tbl <- read_excel(\"data/flea_dog_cat.xlsx\") %>% \n  mutate(animal = as_factor(animal)) %>% \n  select(animal, jump_length)\n\ndata_tbl\n\n# A tibble: 14 × 2\n   animal jump_length\n   <fct>        <dbl>\n 1 dog            5.7\n 2 dog            8.9\n 3 dog           11.8\n 4 dog            8.2\n 5 dog            5.6\n 6 dog            9.1\n 7 dog            7.6\n 8 cat            3.2\n 9 cat            2.2\n10 cat            5.4\n11 cat            4.1\n12 cat            4.3\n13 cat            7.9\n14 cat            6.1\n\n\nWir haben jetzt die Daten richtig vorbereiten und können uns nun mit dem t-Test beschäftigen. Bevor wir den t-Test jedoch rechnen können, müssen wir uns nochmal überlegen, was der t-Test eigentlich testet und uns die Daten einmal visualisieren."
  },
  {
    "objectID": "stat-tests-ttest.html#visualiserung-der-daten",
    "href": "stat-tests-ttest.html#visualiserung-der-daten",
    "title": "21  Der t-Test",
    "section": "\n21.3 Visualiserung der Daten",
    "text": "21.3 Visualiserung der Daten\nBevor wir einen statistischen Test rechnen, wollen wir uns erstmal die Daten, die dem Test zugrundeliegen, visualisieren. Wir schauen uns in Abbildung 21.3 einmal den Boxplot für die Sprungweiten getrennt nach Hund und Katze an.\nWir sehen, dass sich die Boxen nicht überschneiden, ein Indiz für einen signifikanten Unterschied zwischen den beiden Gruppen. Im Weiteren liegt der Median in etwa in der Mitte der beiden Boxen. Die Whisker sind ungefähr gleich bei Hunden und Katzen. Ebenso sehen wir bei beiden Gruppen keine Ausreißer.\nWir schließen daher nach der Betrachtung der Boxplots auf Folgendes:\n\nDie Sprungweite ist für beide Gruppen ist annäherend bzw. approximativ normalverteilt.\nDie Standardabweichungen und damit die Varianzen \\(s^2_{dog} = s^2_{cat}\\) der beiden Gruppen sind gleich. Es liegt somit Varianzhomogenität vor.\n\n\n\n\n\nAbbildung 21.3— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nManchmal ist es etwas verwirrend, dass wir uns in einem Boxplot mit Median und IQR die Daten für einen t-Test anschauen. Immerhin rechnet ja ein t-Test mit den Mittelwerten und der Standardabweichung. Hier vergleichen wir etwas Äpfel mit Birnen. Deshalb in der Abbildung 21.4 der Dotplot mit dem Mittelwert und den entsprechender Standardabweichung als Fehlerbalken.\n\n\n\n\nAbbildung 21.4— Dotplot der Sprungweiten [cm] von Hunden und Katzen zusammen mit dem Mittelwert und der Stanardabweichung als Fehlerbalken.\n\n\n\n\nWir nutzen aber später häufig den Boxplot zur Visualisierung der einzelnen Gruppen. Über den Boxplot können wir auch gut abschätzen, ob wir eine annährende bzw. approximative Normalverteilung vorliegen haben."
  },
  {
    "objectID": "stat-tests-ttest.html#hypothesen-für-den-t-test",
    "href": "stat-tests-ttest.html#hypothesen-für-den-t-test",
    "title": "21  Der t-Test",
    "section": "\n21.4 Hypothesen für den t-Test",
    "text": "21.4 Hypothesen für den t-Test\nOhne eine Hypothese ist das Ergebnis eines statistischen Tests wie auch der t-Test nicht zu interpretieren. Wir berechenen eine Teststatistik und einen p-Wert. Beide statistischen Maßzahlen machen eine Aussage über die beobachteten Daten \\(D\\) unter der Annahme, das die Nullhypothese \\(H_0\\) gilt.\nWie lautet nun das Hypothesenpaar des t-Tests? Der t-Test vergleicht die Mittelwerte von zwei Gruppen. Die Nullhypothese ist auch die Gleichheitshypothese. Die Alternativehypothese haben wir auch als Unterschiedshypothese bezeichnet.\nDaher ergibt sich für unser Beispiel mit den Sprungweiten für Hunde- und Katzenflöhen folgende Hypothesen. Die Nullhypothese sagt, dass die mittleren Sprungweite für die Hundeflöhe gleich der mittleren Sprungweite der Katzenflöhe ist. Die Alternativehypothese sagt aus, dass sich die mittlere Sprungweite von Hunde- und Katzenflöhen unterscheidet.\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{dog} &= \\bar{y}_{cat} \\\\  \nH_A: \\bar{y}_{dog} &\\neq \\bar{y}_{cat} \\\\   \n\\end{aligned}\n\\]\nWir testen grundsätzlich auf ein zweiseitiges \\(\\alpha\\)-Niveau von 5%."
  },
  {
    "objectID": "stat-tests-ttest.html#der-student-t-test",
    "href": "stat-tests-ttest.html#der-student-t-test",
    "title": "21  Der t-Test",
    "section": "\n21.5 Der Student t-Test",
    "text": "21.5 Der Student t-Test\nLiegt ein normalverteiltes \\(y\\) vor und sind die Varianzen für die beiden zu vergleichenden Gruppen homogen \\(s^2_{cat} = s^2_{dog}\\), können wir einen Student t-Test rechnen. Wir nutzen dazu die folgendeFormel des Student t-Tests.\n\n\n\n\n\n\nEigentlich wäre hier folgende Formel richtig…\n\\[\ns_{p} = \\sqrt{\\frac{1}{2} (s^2_{dog} + s^2_{cat})}\n\\] …aber auch hier erwischen wir einen Statistikengel um es etwas einfacher zu machen.\n\\[\nT_{calc} = \\cfrac{\\bar{y}_{dog}-\\bar{y}_{cat}}{s_{p} \\cdot \\sqrt{\\cfrac{2}{n_{group}}}}\n\\]\nmit der vereinfachten Formel für die gepoolte Standardabweichung \\(s_p\\).\n\\[\ns_{p} = \\cfrac{s_{dog} + s_{cat}}{2}\n\\]\nWir wollen nun die Werte für \\(\\bar{y}_{dog}\\), \\(\\bar{y}_{cat}\\) und \\(s_{p}\\) berechnen. Wir nutzen hierfür R auf die etwas komplizierte Art und Weise. Es gibt in R auch die Funktion t.test(), die für uns alles auf einmal macht, aber hier nochaml zu Fuß.\n\nsum_tbl <- data_tbl %>% \n  group_by(animal) %>% \n  summarise(mean = round(mean(jump_length), 2), \n            sd = round(sd(jump_length), 2)) \n\nsum_tbl\n\n# A tibble: 2 × 3\n  animal  mean    sd\n  <fct>  <dbl> <dbl>\n1 dog     8.13  2.14\n2 cat     4.74  1.9 \n\n\n\n\n\nWir erhalten durch die Funktion group_by() den Mittelwert und die Standardabweichung für die Sprungweite getrennt für die Hunde- und Katzenflöhe. Wir können damit die beiden obigen Formeln füllen.\nWir berechnen \\(s_p\\) wie folgt.\n\\[\ns_{pooled} = \\cfrac{2.14 + 1.9}{2} = 2.02\n\\]\nAnschließend können wir jetzt \\(s_p\\) und die Mittelwerte sowie die Gruppengröße \\(n_g = 7\\) in die Formel für den Student t-Test einsetzen und die Teststatistik \\(T_{calc}\\) berechnen.\n\\[\nT_{calc} = \\cfrac{8.13- 4.74}{2.02 \\cdot \\sqrt{\\cfrac{2}{7}}} = 3.14\n\\]\nWir erhalten eine Teststatistik \\(T_{calc} = 3.14\\) die wir mit dem kritischen Wert \\(T_{\\alpha = 5\\%} = 2.17\\) vergleichen können. Da \\(T_{calc} > T_{\\alpha = 5\\%}\\) ist, können wir die Nullhypothese ablehnen. Wir haben ein signifikanten Unterschied zwischen den mittleren Sprungweiten von Hunde- und Katzenflöhen nachgewiesen.\nSoweit für den Weg zu Fuß. Wir rechnen in der Anwendung keinen Student t-Test per Hand. Wir nutzen die Formel t.test(). Da wir den Student t-Test unter der Annahme der Varainzhomogenität nutzen wollen, müssen wir noch die Option var.equal = TRUE wählen.\nDie Funktion t.test() benötigt erst die das \\(y\\) und \\(x\\) in Modellschreibweise mit den Namen, wie die beiden Variablen auch im Datensatz data_tbl stehen. In unserem Fall ist die Modellschreibweise dann jump_length ~ animal. Im Weiteren müssen wir noch den Datensatz angeben den wir verwenden wollen durch die Option data = data_tbl. Dann können wir die Funktion t.test() ausführen.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  jump_length by animal\nt = 3.12528, df = 12, p-value = 0.0087684\nalternative hypothesis: true difference in means between group dog and group cat is not equal to 0\n95 percent confidence interval:\n 1.0253394 5.7460892\nsample estimates:\nmean in group dog mean in group cat \n        8.1285714         4.7428571 \n\n\nWir erhalten eine sehr lange Ausgabe, die aucb etwas verwirrend aussieht. Gehen wir die Ausgabe einmal durch. Ich gehe nicht auf alle Punkte ein, sondern konzentriere mich hier auf die wichtigsten Aspekte.\n\n\nt = 3.12528 ist die berechnete Teststatistik \\(T_{calc}\\). Der Wert unterscheidet sich leicht von unserem berechneten Wert. Der Unterschied war zu erwarten, wir haben ja auch die t-Test Formel vereinfacht.\n\np-value = 0.0087684 ist der berechnete p-Wert \\(Pr(T_{calc}|H_0)\\) aus der obigen Teststatistik. Daher die Fläche rechts von der Teststatistik.\n\n95 percent confidence interval: 1.0253394 5.7460892 ist das 95% Konfidenzintervall. Die erste Zahl ist die untere Grenze, die zweite Zahl ist die obere Grenze.\n\nWir erhalten hier dreimal die Möglichkeit eine Aussage über die \\(H_0\\) zu treffen. In dem obigen Output von R fehlt der kritische Wert \\(T_{\\alpha = 5\\%}\\). Daher ist die berechnete Teststatistik für die Testentscheidung nicht verwendbar. Wir nutzen daher den p-Wert und vergleichen den p-Wert mit dem \\(\\alpha\\)-Niveau von 5%. Da der p-Wert kleiner ist als das \\(\\alpha\\)-Niveau können wir wie Nullhypothese ablehnen. Wir haben einen signifikanten Unterschied. Die Entscheidung mit dem Konfidentintervall benötigt die Signifikanzschwelle. Da wir hier einen Mittelwertsvergleich vorliegen haben ist die Signifikanzschwelle gleich 0. Wenn die 0 im Konfidenzintervall liegt können wir die Nullhypothese nicht ablehnen. In unserem Fall ist das nicht der Fall. Das Konfidenzintervall läfut von 1.025 bis 5.75. Damit ist die 0 nicht im Konfidenzuntervall enthalten und wir können die Nullhypothese ablehnen. Wir haben ein signifikantes Konfidenznintervall vorliegen.\nWie wir sehen fehlt der Mittelwertsuntschied als Effekt \\(\\Delta\\) in der Standardausgabe des t-Tests in R. Wir können den Mittelwertsunterschied selber berechnen oder aber die Funktion tidy() aus dem R Paket broom nutzen. Da der Funktion tidy() kriegen wir die Informationen besser sortiert und einheitich wiedergegeben. Da tidy eine Funktion ist, die mit vielen statistischen Tests funktioniert müssen wir wissen was die einzelnen estimate sind. Es hilft in diesme Fall sich die Visualisierung der Daten anzuschauen und die Abbildung mit den berechneten Werten abzugleichen.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = TRUE) %>% \n  tidy() \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n1     3.39      8.13      4.74      3.13 0.00877        12     1.03      5.75\n# … with 2 more variables: method <chr>, alternative <chr>\n\n\nWir erkennen als erstes den Mittelwertsunterschied zwischen den beiden Gruppen von 3.39 cm. Danach folgen die einzelnen Mittelwerte der Sprungweiten der Hunde und Katzenflöhe mit jeweils 8.13 cm und 4.74 cm. Darauf folgt noch der p-Wert als p.value mit 0.00891 und die beiden Grenzen des Konfidenzintervalls [1.03; 5.75]."
  },
  {
    "objectID": "stat-tests-ttest.html#der-welch-t-test",
    "href": "stat-tests-ttest.html#der-welch-t-test",
    "title": "21  Der t-Test",
    "section": "\n21.6 Der Welch t-Test",
    "text": "21.6 Der Welch t-Test\nDer t-Test ist auch in der Lage mit Varianzhetrogenität umzugehen. Das heißt, wenn die Varianzen der beiden Gruppen nicht gleich sind. Dadurch ändert sich die Formel für den t-Test wie folgt. Dann nennen wir den statistsichen Test Welch t-Test.\n\\[\nT_{calc} = \\cfrac{\\bar{y_1} - \\bar{y_2}}{\\sqrt{\\cfrac{s^2_{y_1}}{n_1} + \\cfrac{s^2_{y_2}}{n_2}}}\n\\]\nWir sehen, dass sich die Formel etwas andert. Da wir nicht mehr annhemen, dass die Varianzen homogen und daher gleich sind, können wir auch keinen gepoolten Varianzschätzer \\(s_p\\) berechnen. Die Varianzen gehen einzeln in die Formel des Welch t-Tests ein. Ebenso müssen die beiden Gruppen nicht mehr gleich groß sein. Statt einen Wert \\(n_g\\) für die Gruppengröße können wir auch die beiden Gruppengrößen separat angeben.\n\n\nHier muss man noch bedenken, dass die Freiheitsgrade anders berechnte werden Die Freiheitsgrade werden wie folgt berechnet.\n\\[\ndf = \\cfrac{\\left(\\cfrac{s^2_{y_1}}{n} +\n    \\cfrac{s^2_{y_2}}{m}\\right)^2}{\\cfrac{\\left(\\cfrac{s^2_{y_1}}{n}\\right)^2}{n-1} + \\cfrac{\\left(\\cfrac{s^2_{y_2}}{m}\\right)^2}{m-1}}\n\\]\nEs ergibt keinen tieferen Sinn die obige Formel nochmal händisch auszurechnen. Die Zahlen ändern sich leicht, aber konzeptionell erhalten wir hier keinen Mehrwert. Deshalb schauen wir uns gleich die Umsetzung in R an. Wir nutzen erneut die Funtktion t.test() und zwar diesmal mit der Option var.equal = FALSE. Damit geben wir an, dass die Varianzen heterogen zwischen den beiden Gruppen sind. Wir nutzen in unserem Beispiel die gleichen Zahlen und Daten wie schon im obigen Student t-Test Beispiel.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  jump_length by animal\nt = 3.12528, df = 11.8307, p-value = 0.008906\nalternative hypothesis: true difference in means between group dog and group cat is not equal to 0\n95 percent confidence interval:\n 1.0215869 5.7498416\nsample estimates:\nmean in group dog mean in group cat \n        8.1285714         4.7428571 \n\n\nWir sehen das viele Zahlen nahezu gleich sind. Das liegt auch daran, dass wir in unserem Daten keine große Abweichung von der Varianzhomogenität haben. Wirerhalten die gleichen Aussagen wie auch schon im Student t-Test.\nSchauen wir uns nochmal die Ausgabe der Funkton tidy() an.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = FALSE) %>% \n  tidy() \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n1     3.39      8.13      4.74      3.13 0.00891      11.8     1.02      5.75\n# … with 2 more variables: method <chr>, alternative <chr>\n\n\nFür das Erkennen von Normalverteilung und Varianzhterogenität werden häufig sogenannte Vortest empfohlen. Aber auch hier gilt, bei kleiner Fallzahl liefern die Vortests keine verlässlichen Ergebnisse. In diesem Fall ist weiterhin die Beurteilung über einen Boxplot sinnvoller.\nWir sehen hier etwas besser, dass es kaum Abweichungen gibt. Alles egal? Nicht unbedingt. Das Problem ist eher das Erkennen von Varianzheterogenität in sehr kleinen Datensätzen. Kleine Datensätze meint Datensätze unter 30 Beobachtungen je Gruppe. Erst aber dieser Anzahl lassen sich unverzerrte Histogramme zeichnen und so aussagekräftige Abschätzungen der Varianzhomogenität oder Varianzheterogenität treffen."
  },
  {
    "objectID": "stat-tests-ttest.html#der-verbundene-t-test-paired-t-test",
    "href": "stat-tests-ttest.html#der-verbundene-t-test-paired-t-test",
    "title": "21  Der t-Test",
    "section": "\n21.7 Der verbundene t-Test (Paired t-Test)",
    "text": "21.7 Der verbundene t-Test (Paired t-Test)\nIm folgenden Datenbespiel in Tabelle 21.2 haben wir eine verbundene Stichprobe. Das heißt wir haben nicht zehn Flöhe gemessen sondern fünf Flöhe. Einmal im ungefütterten Zustand unfed und einmal im gefütterten Zustand fed. Wir wollen nun wissen, ob der Fütterungszustand Auswirkungen auf die Sprungweite in [cm] hat.\n\n\n\n\nTabelle 21.2— Tabelle der Sprunglängen [cm] von fünf Flöhen zu zwei Zeitpunkten. Einmal wurde die Sprungweite ungefüttert und einmal gefüttert bestimmt. Die Daten liegen im Wide Format vor.\n\nunfed\nfed\ndiff\n\n\n\n5.2\n6.1\n0.9\n\n\n4.1\n5.2\n1.1\n\n\n3.5\n3.9\n0.4\n\n\n3.2\n4.1\n0.9\n\n\n4.6\n5.3\n0.7\n\n\n\n\n\n\nWir nutzen folgende Formel für den paired t-Test für verbundene Stichproben.\n\\[\nT_{calc} = \\sqrt{n}\\cfrac{\\bar{d}}{s_d}\n\\]\nWir können \\(\\bar{d}\\) als Mittelwert der Differenzen der Variablen diff berechnen. Ebenso verfahren wir mit der Standardabweichung der Differenzen \\(s_d\\).\n\\[\nT_{calc} = \\sqrt{10}\\cfrac{0.8}{0.26} = 6.88\n\\]\nUm den die Funktion t.test()in R mit der Option paired = TRUE für den paired t-Test zu nutzen, müssen wir die Daten nochmal über die Funktion gather() in das Long Format umwandeln. Wir wollen nun wissen, ob der Fütterungszustand food_status Auswirkungen auf die Sprungweite in [cm] hat.\n\nt.test(jump_length ~ food_status, \n       data = paired_tbl, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  jump_length by food_status\nt = 6.76123, df = 4, p-value = 0.0024959\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.47148658 1.12851342\nsample estimates:\nmean difference \n            0.8 \n\n\nDie Ausgabe des paired t-Test ähnelt stark der Ausage des Student t-Test. Wir erhalten ebenfalls den wichtigen p-Wert mit 0.0025 sowie das 95% Konfidenzintervall mit [0.47; 1.13]. Zum einen ist \\(0.0025 < \\alpha\\) und somit können wir die Nullhypothese ablehnen, zum anderen ist auch die 0 nicht mit in dem Konfidentintervall, womit wir auch hier die Nullhypothese ablehnen können.\n\nt.test(jump_length ~ food_status, \n       data = paired_tbl, paired = TRUE) %>% \n  tidy() \n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl> <chr>      <chr>      \n1      0.8      6.76 0.00250         4    0.471      1.13 Paired t-… two.sided  \n\n\nDie Funktion tidy() gibt uns in diesem Fall keine neuen zusätzlichen Informationen."
  },
  {
    "objectID": "stat-tests-ttest.html#freiheitsgrade-im-t-test",
    "href": "stat-tests-ttest.html#freiheitsgrade-im-t-test",
    "title": "21  Der t-Test",
    "section": "\n21.8 Freiheitsgrade im t-Test",
    "text": "21.8 Freiheitsgrade im t-Test\nDer t-Verteilung der Teststatistiken des t-Tests verhält sich nicht wie eine klassische Normalverteilung, die durch den Mittelwert und die Standardabweichung definiert ist. Die t-Verteilung ist nur durch die Freiheistgrade definiert. Der Freiheitsgrade in einem t-Test mit zwei Stichproben ist gegeben durch \\(df = n_1 + n_2 -2\\). Damit beschreiben die Freiheitsgrade grob die Fallzahl. Je mehr Fallzahl desto großer der Freiheitsgrad eines t-Tests.\nAbbildung 21.5 visualisert diesen Zusammenhang von Freiheitsgraden und der Form der t-Verteilung. Je kleiner die Freiheitgarde und damit die Fallzahl, desto weiter sind die Verteilungsschwänze. Daher benötigen wir auch größere \\(T_{calc}\\) Werte um ein signifikantes Ergebnis zu erhalten. Die Fläche unter der t-Verteilung ist immer gleich.\n\n\nAbbildung 21.5— Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und dammit die Fallzahl, desto näher komtm die t-Verteilung einer Normalverteilung nahe. Da eine geringe Fallzahl weiter nach Außen geht, müssen größere \\(T_{calc}\\) Werte erreicht werden um eine signifikantes Ergebnis zu erhalten."
  },
  {
    "objectID": "stat-tests-anova.html",
    "href": "stat-tests-anova.html",
    "title": "22  Die ANOVA",
    "section": "",
    "text": "Version vom Oktober 12, 2022 um 14:13:17\nDie ANOVA (eng. analysis of variance) ist wichtig. Was für ein schöner Satz um anzufangen. Wir brauchen die ANOVA aus mehreren Gründen. Die Hochzeiten der ANOVA sind eigentlich vorbei, wir haben in der Statistik für viele Fälle mittlerweile besser Werkzeuge, aber als Allrounder ist die ANOVA immer noch nutzbar.\nWofür brauchen wir die ANOVA?\nWir sehen also, dass die ANOVA zum einen alt ist, aber auch heute noch viel verwendet wird. Daher werden wir in diesem langem Kapitel uns einmal mit der ANOVA ausgiebig beschäftigen. Fangen wir also an, dieses großartige Schwerzeitaschenmesser der Statistik besser zu verstehen."
  },
  {
    "objectID": "stat-tests-anova.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-anova.html#genutzte-r-pakete-für-das-kapitel",
    "title": "22  Die ANOVA",
    "section": "\n22.1 Genutzte R Pakete für das Kapitel",
    "text": "22.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, effectsize, ggpubr)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac1",
    "href": "stat-tests-anova.html#sec-fac1",
    "title": "22  Die ANOVA",
    "section": "\n22.2 Einfaktorielle ANOVA",
    "text": "22.2 Einfaktorielle ANOVA\nDie einfaktorielle ANOVA ist die simpelste Form der ANOVA. Wir nutzen einen Faktor mit mehr als zwei Leveln. Im Rahmen der einfaktoriellen ANOVA wollen wir usn auch die ANOVA theoretisch einmal anschauen. Danach wie die einfaktorielle ANOVA in R genutzt wird. Ebenso wie wir die einfaktorielle ANOVA visualsieren. Abschließend müssen wir uns noch überlegen, ob es einen Effektschätzer für die einfaktorielle ANOVA gibt.\n\n\n\n\n\n\nDie einfaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität über den Behandlungsfaktor \\(x\\). Daher alle Level von \\(x\\) sollen die gleiche Varianz haben.\nUnsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x\\) homogen in den Varianzen sind. Später mehr dazu, wenn wir beides nicht vorliegen haben…\n\n22.2.1 Daten für die einfaktorielle ANOVA\nWir wollen uns nun erstmal den einfachsten Fall anschauen mit einem simplen Datensatz. Wir nehmen ein normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Hätten wir nur zwei Level, dann können wir auch einen t-Test rechnen können.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac1_tbl <- read_csv2(\"data/flea_dog_cat_fox.csv\") %>%\n  select(animal, jump_length) %>% \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt fac1_tbl mit dem Datensatz in Tabelle 22.1 nochmal dargestellt.\n\n\n\n\nTabelle 22.1— Selektierter Datensatz für die einfaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\n\n\n\ndog\n5.7\n\n\ndog\n8.9\n\n\ndog\n11.8\n\n\ndog\n8.2\n\n\ndog\n5.6\n\n\ndog\n9.1\n\n\ndog\n7.6\n\n\ncat\n3.2\n\n\ncat\n2.2\n\n\ncat\n5.4\n\n\ncat\n4.1\n\n\ncat\n4.3\n\n\ncat\n7.9\n\n\ncat\n6.1\n\n\nfox\n7.7\n\n\nfox\n8.1\n\n\nfox\n9.1\n\n\nfox\n9.7\n\n\nfox\n10.6\n\n\nfox\n8.6\n\n\nfox\n10.3\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt fac1_tbl folgendes Modell für später:\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der einfaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n22.2.2 Hypothesen für die einfaktorielle ANOVA\nDie ANOVA betrachtet die Mittelwerte und nutzt die Varianzen um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mittelwerte jedes Levels des Faktors animal gleich sind.\n\\[\nH_0: \\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Mittelwerten unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mittelwerte unterschieden oder eben nur ein Paar. Wenn eine ANOVA die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mittelwerte sich unterscheiden.\n\\[\n\\begin{aligned}\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nWir schauen uns jetzt einmal die ANOVA theoretisch an bevor wir uns mit der Anwendung der ANOVA in R beschäftigen.\n\n22.2.3 Einfaktoriellen ANOVA theoretisch\nKommen wir zurück zu den Daten in Tabelle 22.1. Wenn wir die ANOVA per Hand rechnen wollen, dann ist nicht das Long Format die beste Wahl sondern das Wide Format. Wir haben ein balanciertes Design vorliegen, dass heißt in jeder Level sind die gleiche Anzahl Beobachtungen. Wir schauen uns jeweils sieben Flöhe von jeder Tierart an. Für eine ANOVA ist aber ein balanciertes Design nicht notwendig, wir können auch mit ungleichen Gruppengrößen eine ANOVA rechnen.\nStatt einer einfaktoriellen ANOVA könnten wir auch gleich einen pairwise.t.test()rechnen. Historisch ebtrachtet ist die einfaktorielle ANOVA die Visualsierung des paarweisen t-Tests.\nEien einfaktorielle ANOVA macht eigentlich keinen großen Sinn, wenn wir anschließend sowieso paarweise Vergleich, wie in Kapitel 29 beschrieben, rechnen. Aus der Hisotrie stellte sich die Frage, ob es sich lohnt die ganze Arbeit für die paarweisen t-Tests per Hand zu rechnen. Daher wurde die ANOVA davorgeschaltet. War die ANOVA nicht signifikant, dann konnte man sich dann auch die Rechnerei für die paaweisen t-Tests sparen.\nIn Tabelle 22.2 sehen wir die Daten einmal als Wide Format dargestellt.\n\n\nTabelle 22.2— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal.\n\nj\ndog\ncat\nfox\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\n\nWir können jetzt für jedes de Level den Mittelwert über all \\(j=7\\) Beobachtungen berechnen.\n\\[\n\\begin{aligned}\n\\bar{y}_{dog} &= 8.13 \\\\\n\\bar{y}_{cat} &= 4.74 \\\\\n\\bar{y}_{fox} &= 9.16 \\\\\n\\end{aligned}\n\\]\nWir tuen jetzt für einen Moment so, als gebe es den Faktor animal nicht in den Daten und schauen uns die Verteilung der einzelnen Beobachtungen in Abbildung 22.1 (a) einmal an. Wir sehen das sich die Beobachtungen von ca. 2.2cm bis 11 cm streuen. Woher kommt nun diese Streuung bzw. Varianz? Was ist die Quelle der Varianz? In Abbildung 22.1 (b) haben wir die Punkte einmal nach dem Faktor animal eingefärbt. Wir sehen, dass die blauen Beobachtungen eher weitere Sprunglängen haben als die grünen Beobachtungen. Wir gruppieren die Beobachtungen in Abbildung 22.1 (c) nach dem Faktor animal und sehen, dass ein Teil der Varianz der Daten von dem Faktor animal ausgelöst wird.\n\n\n\n\n\n(a) Die Sprungweite in [cm] ohne den Faktor animal betrachtet.\n\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal eingefärbt.\n\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal eingefärbt und gruppiert.\n\n\n\n\nAbbildung 22.1— Die Spungweite in [cm] in Abhängigkeit von dem Faktor animal dargestellt.\n\n\nGehen wir einen Schritt weiter und zeichnen einmal das globale Mittel in die Abbildung 22.2 (a) von \\(\\bar{y}_{..} = 7.34\\) und lassen die Beobachtungen gruppiert nach dem Faktor animal. Wir sehen, dass die Level des Faktors animal um das globale Mittel streuen. Was ja auch bei einem Mittelwert zu erwarten ist. Wir können jetzt in Abbildung 22.2 (b) die lokalen Mittel für die einzelnen Level dog, catund fox ergänzen. Und abschließend in Abbildung 22.2 (c) die Abweichungen \\(\\\\beta_i\\) zwischen dem globalen Mittel \\(\\bar{y}_{..} = 7.34\\) und den einzelnen lokalen Mittel berechnen. Die Summe der Abweichungen \\(\\\\beta_i\\) ist \\(0.79 + (-2.6) + 1.81 \\approx 0\\). Das ist auch zu erwarten, den das globale Mittel muss ja per Definition als Mittelwert gleich großen Abstand “nach oben” wie “nach unten” haben.\n\n\n\n\n\n(a) Die Sprungweite in [cm] mit den Faktor animal gruppiert und das globale Mittel \\(\\bar{y}_{..} = 7.34\\) ergänzt.\n\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die lokalen Mittel \\(\\bar{y}_{i.}\\) für jedes Level ergänzt.\n\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die Abweichungen \\(\\beta_i\\) ergänzt.\n\n\n\n\nAbbildung 22.2— Dotplot der Spungweite in [cm] in Abhängigkeit von dem Faktor animal.\n\n\nWir tragen die Werte der lokalen Mittlwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\) noch in die Tabelle 22.3 ein. Wir sehen in diesem Beispiel warum das Wide Format besser ist, wenn wir die lokalen Mittelwerte und die Abweichungen per Hand berechnen. Da wir in der Anwendung aber nie die ANOVA per Hand rechnen, liegen unsere Daten immer in R als Long Format vor.\n\n\nTabelle 22.3— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal. Wir ergänzen die lokalen Mittlwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\).\n\nj\ndog\ncat\nfox\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\\(\\bar{y}_{i.}\\)\n\\(8.13\\)\n\\(4.74\\)\n\\(9.16\\)\n\n\n\\(\\beta_i\\)\n\\(-2.6\\)\n\\(0.79\\)\n\\(1.81\\)\n\n\n\n\nWie kriegen wir nun die ANOVA rechnerisch auf die Straße? Schauen wir uns dazu einmal die Abbildung 22.3 an. Auf der linken Seiten sehen wir vier Gruppen, die keinen Effekt haben. Die Gruppen liegen alle auf der gleichen Höhe. Es ist mit keinem Unterschied zwischen den Gruppen zu rechnen. Alle Gruppenmittel liegen auf dem globalen Mittel. Die Abweichungen der einzelnen Gruppenmittel zum globalen Mittel ist damit gleich null. Auf der rechten Seite sehen wir vier Gruppen mit einem Effekt. Die Gruppen unterscheiden sich in ihren Gruppenmitteln. Dadurch unterscheide sich aber auch die Gruppenmittel von dem globalen Mittel.\n\n\n\n\n\n(a) Kein Effekt\n\n\n\n\n\n\n(b) Leichter bis mittlerer Effekt\n\n\n\n\nAbbildung 22.3— Darstellung von keinem Effekt und leichtem bis mittleren Effekt in einer einfaktoriellen ANOVA mit einem Faktor mit vier Leveln A - D.\n\n\nWir können daher wie in Tabelle 22.4 geschrieben die Funktionsweise der ANOVA zusammenfassen.\n\n\nTabelle 22.4— Zusammenfassung der ANOVA Funktionsweise.\n\n\n\n\n\n\nAll level means are equal.\n=\nThe differences between level means and the total mean are small.\n\n\n\nNun kommen wir zum eigentlichen Schwenk und warum eigentlich die ANOVA meist etwas verwirrt. Wir wollen eine Aussage über die Mittelwerte machen. Die Nullhypothese lautet, dass alle Mittelwerte gleich sind. Wie wir in Tabelle 22.4 sagen, heißt alle Mittelwerte gleich auch, dass die Abweichungen von den Gruppenmitteln zum globalen Mittel klein ist.\nWie weit die Gruppenmittel von dem globalen Mittel weg sind, dazu nutzt die ANOVA die Varianz. Die ANOVA vergleicht somit\n\ndie Varianz der einzelnen Mittelwerte der (Gruppen)Level zum globalen Mittel (eng. variability between levels)\nund die Varianz der Beobachtungen zu den einzelnen Mittelwerten der Level (eng. variability within one level)\n\nDie sum of squares sind nichts anderes als die Varianz. Wir nennen das hier nur einmal anders…\nWir berechnen also wie die Beobachtungen jeweils um das globale Mittel streuen (\\(SS_{total}\\)), die einzelnen Beobachtungen um die einzelnen Gruppenmittel \\(SS_{error}\\) und die Streuung der Gruppenmittel um das globale Mittel (\\(SS_{animal}\\)). Wir nennen die Streuung Abstandquadrate (eng. sum of squares) und damit sind die Sum of Square \\((SS)\\) nichts anderes als die Varianz. Die Tabelle 22.5 zeigt die Berechnung des Anteils jeder einzlenen Beobachtung an den jeweiligen Sum of Squares.\n\n\n\nTabelle 22.5— Berechnung der \\(SS_{animal}\\), \\(SS_{error}\\) und \\(SS_{total}\\) anhand der einzelnen gemessenen Werte \\(y\\) für durch die jeweiligen Gruppenmittel \\(\\bar{y}_{i.}\\) und dem globalen Mittel \\(\\bar{y}_{..}\\) über alle Beobachtungen\n\n\n\n\n\n\n\n\n\nanimal (x)\njump_length (y)\n\\(\\boldsymbol{\\bar{y}_{i.}}\\)\nSS\\(_{\\boldsymbol{animal}}\\)\n\nSS\\(_{\\boldsymbol{error}}\\)\n\nSS\\(_{\\boldsymbol{total}}\\)\n\n\n\n\ndog\n\\(5.7\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.7 - 8.13)^2 = 5.90\\)\n\\((5.7 - 7.34)^2 = 2.69\\)\n\n\ndog\n\\(8.9\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.9 - 8.13)^2 = 0.59\\)\n\\((8.9 - 7.34)^2 = 2.43\\)\n\n\ndog\n\\(11.8\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((11.8 - 8.13)^2 = 13.47\\)\n\\((11.8 - 7.34)^2 = 19.89\\)\n\n\ndog\n\\(8.2\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.2 - 8.13)^2 = 0.00\\)\n\\((8.2 - 7.34)^2 = 0.74\\)\n\n\ndog\n\\(5.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.6 - 8.13)^2 = 6.40\\)\n\\((5.6 - 7.34)^2 = 3.03\\)\n\n\ndog\n\\(9.1\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((9.1 - 8.13)^2 = 0.94\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\ndog\n\\(7.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((7.6 - 8.13)^2 = 0.28\\)\n\\((7.6 - 7.34)^2 = 0.07\\)\n\n\ncat\n\\(3.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((3.2 - 4.74)^2 = 2.37\\)\n\\((3.2 - 7.34)^2 = 17.14\\)\n\n\ncat\n\\(2.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((2.2 - 4.74)^2 = 6.45\\)\n\\((2.2 - 7.34)^2 = 26.42\\)\n\n\ncat\n\\(5.4\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((5.4 - 4.74)^2 = 0.44\\)\n\\((5.4 - 7.34)^2 = 3.76\\)\n\n\ncat\n\\(4.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.1 - 4.74)^2 = 0.41\\)\n\\((4.1 - 7.34)^2 = 10.50\\)\n\n\ncat\n\\(4.3\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.3 - 4.74)^2 = 0.19\\)\n\\((4.3 - 7.34)^2 = 9.24\\)\n\n\ncat\n\\(7.9\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((7.9 - 4.74)^2 = 9.99\\)\n\\((7.9 - 7.34)^2 = 0.31\\)\n\n\ncat\n\\(6.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((6.1 - 4.74)^2 = 1.85\\)\n\\((6.1 - 7.34)^2 = 1.54\\)\n\n\nfox\n\\(7.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((7.7 - 9.16)^2 = 2.13\\)\n\\((7.7 - 7.34)^2 = 0.13\\)\n\n\nfox\n\\(8.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.1 - 9.16)^2 = 1.12\\)\n\\((8.1 - 7.34)^2 = 0.58\\)\n\n\nfox\n\\(9.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.1 - 9.16)^2 = 0.00\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\nfox\n\\(9.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.7 - 9.16)^2 = 0.29\\)\n\\((9.7 - 7.34)^2 = 5.57\\)\n\n\nfox\n\\(10.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.6 - 9.16)^2 = 2.07\\)\n\\((10.6 - 7.34)^2 = 10.63\\)\n\n\nfox\n\\(8.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.6 - 9.16)^2 = 0.31\\)\n\\((8.6 - 7.34)^2 = 1.59\\)\n\n\nfox\n\\(10.3\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.3 - 9.16)^2 = 1.30\\)\n\\((10.3 - 7.34)^2 = 8.76\\)\n\n\n\n\n\n\\(74.68\\)\n\\(56.53\\)\n\\(131.21\\)\n\n\n\n\n\nDie ANOVA wird deshalb auch Varianzzerlegung genannt, da die ANOVA versucht den Abstand der Beoabchtungen auf die Variablen im Modell zu zerlegen. Also wieviel der Streuung von den Beobachtungen kann von dem Faktor animal erklärt werden? Genau der Abstand von den Gruppenmitteln zu dem globalen Mittlelwert.\nDu kannst dir das ungefähr als eine Reise von globalen Mittelwert zu der einzelnen Beobachtung vorstellen. Nehmen wir als Beispiel die kleinste Sprungweite eines Katzenflöhes von 2.2 cm und visualisieren wir uns die Reise wie in Abbildung 22.4 zu sehen. Wie kommen wir jetzt numerisch vom globalen Mittel mit \\(7.34\\) zu der Beobachtung? Wir können zum einen den direkten Abstand mit \\(2.2 - 7.34\\) gleich \\(-5.14\\) cm berechnen. Das wäre der total Abstand. Wie sieht es nun aus, wenn wir das Gruppenmittel mit beachten? In dem Fall gehen wir vom globalen Mittel zum Gruppenmittel cat mit \\(\\bar{y}_{cat} - \\bar{y}_{..} = 4.74 -7.34\\) gleich \\(\\beta_{cat} = -2.6\\) cm. Jetzt sind wir aber noch nicht bei der Beobachtung. Wir haben noch einen Rest von \\(y_{cat,2} - \\bar{y}_{cat} = 2.2 - 4.74\\) gleich \\(\\epsilon_{cat, 2} = -2.54\\) cm, die wir noch zurücklegen müssen. Das heißt, wir können einen Teil der Strecke mit dem Gruppenmittelwert erklären. Oder anders herum, wir können die Strecke vom globalen Mittelwert zu der Beobachtung in einen Teil für das Gruppenmittel und einen unerklärten Rest zerlegen.\n\n\n\nAbbildung 22.4— Visualisierung der Varianzzerlegung des Weges vom globalen Mittel zu der einzelnen Beoabchtung. Um zu einer einzelnen Beobachtung zu kommen legen wir den Weg vom globalen Mittelwert über den Abstand vom globalen Mittel zum Gruppenmittel \\(\\beta\\) zurück. Dann fehlt noch der Rest oder Fehler oder Residuum \\(\\epsilon\\).\n\n\n\nWir rechnen also eine ganze Menge an Abständen und quadrieren dann diese Absatände zu den Sum of Squares. Oder eben der Varianz. Dann fragen wir uns, ob der Faktor in unserem Modell einen Teil der Abstände erklären kann. Wir bauen uns dafür eine ANOVA Tabelle. Tabelle 22.6 zeigt eine theoretische, einfaktorielle ANOVA Tabelle. Wir berechnen zuerst die Abstände als \\(SS\\). Nun ist es aber so, dass wenn wir in einer Gruppe viele Level und/oder Beoabchtungen haben, wir auch größere Sum of Squares bekommen. Wir müssen also die Sum of Squares in mittlere Abweichungsqudrate (eng. mean squares) mitteln. Abschließend können wir die F Statistik berechnen, indem wir die \\(MS\\) des Faktors durch die \\(MS\\) des Fehlers teilen. Das Verhältnis von erklärter Varianz vom Faktor zu dem unerklärten Rest.\n\n\n\nTabelle 22.6— Einfaktorielle ANOVA in der theoretischen Darstellung. Die sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(k-1\\)\n\\(SS_{animal} = \\sum_{i=1}^{k}n_i(\\bar{y}_{i.} - \\bar{y}_{..})^2\\)\n\\(MS_{animal} = \\cfrac{SS_{animal}}{k-1}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nerror\n\\(n-k\\)\n\\(SS_{error} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{i.})^2\\)\n\\(MS_{error} = \\cfrac{SS_{error}}{N-k}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{..})^2\\)\n\n\n\n\n\n\n\nWir füllen jetzt die Tabelle 22.7 einmal mit den Werten aus. Nachdem wir das getan haben oder aber die Tabelle in R ausgegeben bekommen haben, können wir die Zahlen interpretieren.\n\n\n\nTabelle 22.7— Einfaktorielle ANOVA mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dan direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 74.68\\)\n\\(MS_{animal} = \\cfrac{74.68}{3-1} = 37.34\\)\n\\(F_{calc} = \\cfrac{37.34}{3.14} = 11.89\\)\n\n\nerror\n\\(21-3\\)\n\\(SS_{error} = 56.53\\)\n\\(MS_{error} = \\cfrac{56.53}{18} = 3.14\\)\n\n\n\ntotal\n\\(21-1\\)\n\\(SS_{total} = 131.21\\)\n\n\n\n\n\n\n\nZu erst ist die berechnete F Statistik \\(F_{calc}\\) von Interesse. Wir haben hier eine \\(F_{calc}\\) von 11.89. Wir vergleichen wieder die berechnete F Statistik mit einem kritsichen Wert. Der kritische F Wert \\(F_{\\alpha = 5\\%}\\) lautet für die einfaktorielle ANOVA in diesem konkreten Beispiel mit \\(F_{\\alpha = 5\\%} = 3.55\\). Die Enstscheidungsregel nach der F Testatitik lautet, die \\(H_0\\) abzulehnen, wenn \\(F_{calc} > F_{\\alpha = 5\\%}\\).\nWir können also die Nullhypothese \\(H_0\\) in unserem Beispiel ablehnen. Es liegt ein signifikanter Unterschied zwischen den Tiergruppen vor. Mindestens ein Mittelwertsunterschied in den Sprungweiten liegt vor.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(F_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(F_{calc}\\) gilt, wenn \\(F_{calc} \\geq F_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.\n\n\n\n22.2.4 Einfaktoriellen ANOVA in R\nUm eine ANOVA zu rechnen nutzen wir zuerst die Funktion lm(), warum das so ist kannst du im ?sec-modeling-simple-stat nachlesen. Du brauchst das Wissen aber hier nicht unbedingt.\nWir rechnen keine ANOVA per Hand sondern nutzen R. Dazu müssen wir als erstes das Modell definieren. Das ist im Falle der infaktoriellen ANOVA relativ einfach. Wir haben unseren Datensatz fac1_tbl mit einer kontinuierlichen Variable jump_lemgth als \\(y\\) vorliegen sowie einen Faktor animal mit mehr als zwei Leveln als \\(x\\). Wir definieren das Modell in R in der Form jump_length ~ animal. Um das Modell zu rechnen nutzen wir die Funktion lm() - die Abkürzung für linear model. Danach pipen wir die Ausgabe vom lm() direkt in die Funktion anova(). Die Funktion anova berechnet uns dann die eigentliche einfaktorielle ANOVA. Wir speichern die Ausgabe der ANOVA in fit_1. Schauen wir uns die ANOVA Ausgabe einmal an.\n\nfit_1 <-  lm(jump_length ~ animal, data = fac1_tbl) %>% \n  anova\n\nfit_1\n\nAnalysis of Variance Table\n\nResponse: jump_length\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nanimal     2 74.683  37.341   11.89 0.0005113 ***\nResiduals 18 56.529   3.140                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten die Information was wir gerechnet haben, eine Varianzanalyse. Darunter steht, was das \\(y\\) war nämlich die jump_length. Wir erhalten eine Zeile für den Faktor animal und damit die \\(SS_{animal}\\) und eine Zeile für den Fehler und damit den \\(SS_{error}\\). In R heißen die \\(SS_{error}\\) dann Residuals. Die Zeile für die \\(SS_{total}\\) fehlt.\nNeben der berechneten F Statistik \\(F_{calc}\\) von \\(11.89\\) erhalten wir auch den p-Wert mit \\(0.005\\). Wir ignorieren die F Statistik, da wir in der Anwendung nur den p-Wert berücksichtigen. Die Entscheidung gegen die Nulhypothese lautet, dass wenn der p-Wert kleiner ist als das Signifkanzniveau \\(\\alpha\\) von 5% wir die Nullhypothese ablehnen.\nWir haben hier ein signifikantes Ergebnis vorliegen. Mindestens ein Gruppenmittelerstunterschied ist signifikant. Abbildung 22.5 zeigt nochmal die Daten fac1_tbl als Boxplot. Wir überprüfen visuell, ob das Ergebnis der ANOVA stimmen kann. Ja, die Boxplots und das Ergebnis der ANOVA stimmen überein. Die Boxplots liegen nicht alle auf einer Ebene, so dass hier auch ein signifikanter Unterschied zu erwarten war.\n\n\n\n\nAbbildung 22.5— Boxplot der Sprungweiten [cm] von Hunden-, Katzen- und Fuchsflöhen.\n\n\n\n\nAbschließend können wir noch die Funktion eta_squared() aus dem R Paket effectsize nutzen um einen Effektschätzer für die einfaktorielle ANOVA zu berechnen. Wir können mit \\(\\eta^2\\) abschätzen, welchen Anteil der Faktor animal an der gesamten Varianz erklärt.\n\nfit_1 %>% eta_squared\n\nFor one-way between subjects designs, partial eta squared is equivalent to eta squared.\nReturning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nanimal    | 0.57 | [0.27, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDas \\(\\eta^2\\) können wir auch einfach händisch berechnen.\n\\[\n\\eta^2 = \\cfrac{SS_{animal}}{SS_{total}} = \\cfrac{74.68}{131.21} = 0.57 = 57\\%\n\\]\nWir haben nun die Information, das 57% der Varianz der Beobachtungen durch den Faktor animal rklärt wird. Je nach Anwendungsgebiet kann die Relevanz sehr stark variieren. Im Bereich der Züchtung mögen erklärte Varianzen von unter 10% noch sehr relevant sein. Im Bereich des Feldexperiments erwarten wir schon höhere Werte für \\(\\eta^2\\). Immerhin sollte ja unsere Behandlung maßgeblich für die z.B. größeren oder kleineren Pflanzen gesorgt haben."
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac2",
    "href": "stat-tests-anova.html#sec-fac2",
    "title": "22  Die ANOVA",
    "section": "\n22.3 Zweifaktorielle ANOVA",
    "text": "22.3 Zweifaktorielle ANOVA\n\n\n\n\n\n\nDie zweifaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität jeweils separat über beide Behandlungsfaktor \\(x_1\\) und \\(x_2\\). Daher alle Level von \\(x_1\\) sollen die gleiche Varianz haben. Ebenso sollen alle Level von \\(x_2\\) die gleiche Varianz haben.\nUnsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x_1\\) und \\(x_2\\) jewiels für sich homogen in den Varianzen sind. Später mehr dazu, wenn wir beides nicht vorliegen haben…\nDie zweifaktorielle ANOVA ist eine wunderbare Methode um herauszufinden, ob zwei Faktoren einen Einfluss auf ein normalverteiltes \\(y\\) haben. Die Stärke der zweifaktoriellen ANOVA ist hierbei, dass die ANOVA beide Effekte der Faktoren auf das \\(y\\) simultan modelliert. Darüber hinaus können wir auch noch einen Interaktionsterm mit in das Modell aufnehmen um zu schauen, ob die beiden Faktoren untereinander auch interagieren. Somit haben wir mit der zweifaktoriellen ANOVA die Auswertungsmehode für ein randomiziertes Blockdesign vorliegen.\n\n22.3.1 Daten für die zweifaktorielle ANOVA\nWir wollen uns nun einen etwas komplexes Modell anschauen mit einem etwas komplizierteren Datensatz flea_dog_cat_fox_site.csv. Wir brauchen hierfür ein normalverteiltes \\(y\\) und sowie zwei Faktoren. Das macht auch soweit Sinn, denn wir wollen ja auch eine zweifaktorielle ANOVA rechnen.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal sowie die Spalte site als \\(x\\). Danach müssen wir noch die Variable animal sowie die Variable site in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac2_tbl <- read_csv2(\"data/flea_dog_cat_fox_site.csv\") %>% \n  select(animal, site, jump_length) %>% \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 22.8 nochmal dargestellt.\n\n\n\n\nTabelle 22.8— Selektierter Datensatz für die zweifaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\nanimal\nsite\njump_length\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.10\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\ncat\ncity\n14.91\n\n\ncat\ncity\n11.17\n\n\ncat\ncity\n12.38\n\n\ncat\ncity\n15.06\n\n\ncat\nsmalltown\n15.24\n\n\ncat\nsmalltown\n13.36\n\n\ncat\nsmalltown\n15.08\n\n\ncat\nsmalltown\n12.83\n\n\ncat\nsmalltown\n14.68\n\n\ncat\nsmalltown\n10.73\n\n\ncat\nsmalltown\n13.35\n\n\ncat\nsmalltown\n14.54\n\n\ncat\nsmalltown\n12.99\n\n\ncat\nsmalltown\n14.51\n\n\ncat\nvillage\n17.59\n\n\ncat\nvillage\n11.24\n\n\ncat\nvillage\n12.44\n\n\ncat\nvillage\n13.63\n\n\ncat\nvillage\n14.92\n\n\ncat\nvillage\n17.43\n\n\ncat\nvillage\n18.30\n\n\ncat\nvillage\n16.35\n\n\ncat\nvillage\n16.34\n\n\ncat\nvillage\n14.23\n\n\ncat\nfield\n13.70\n\n\ncat\nfield\n15.13\n\n\ncat\nfield\n17.99\n\n\ncat\nfield\n14.60\n\n\ncat\nfield\n16.16\n\n\ncat\nfield\n14.26\n\n\ncat\nfield\n15.39\n\n\ncat\nfield\n16.85\n\n\ncat\nfield\n19.02\n\n\ncat\nfield\n18.76\n\n\ndog\ncity\n19.35\n\n\ndog\ncity\n17.10\n\n\ndog\ncity\n19.85\n\n\ndog\ncity\n15.33\n\n\ndog\ncity\n15.15\n\n\ndog\ncity\n19.57\n\n\ndog\ncity\n15.44\n\n\ndog\ncity\n16.09\n\n\ndog\ncity\n15.91\n\n\ndog\ncity\n13.01\n\n\ndog\nsmalltown\n17.72\n\n\ndog\nsmalltown\n17.11\n\n\ndog\nsmalltown\n17.57\n\n\ndog\nsmalltown\n17.12\n\n\ndog\nsmalltown\n16.02\n\n\ndog\nsmalltown\n22.61\n\n\ndog\nsmalltown\n16.49\n\n\ndog\nsmalltown\n18.64\n\n\ndog\nsmalltown\n17.21\n\n\ndog\nsmalltown\n19.90\n\n\ndog\nvillage\n16.60\n\n\ndog\nvillage\n15.28\n\n\ndog\nvillage\n16.91\n\n\ndog\nvillage\n15.08\n\n\ndog\nvillage\n18.56\n\n\ndog\nvillage\n16.34\n\n\ndog\nvillage\n17.61\n\n\ndog\nvillage\n14.80\n\n\ndog\nvillage\n17.52\n\n\ndog\nvillage\n16.93\n\n\ndog\nfield\n15.78\n\n\ndog\nfield\n17.02\n\n\ndog\nfield\n15.41\n\n\ndog\nfield\n15.61\n\n\ndog\nfield\n19.87\n\n\ndog\nfield\n19.24\n\n\ndog\nfield\n17.65\n\n\ndog\nfield\n18.83\n\n\ndog\nfield\n17.60\n\n\ndog\nfield\n14.67\n\n\nfox\ncity\n19.50\n\n\nfox\ncity\n18.49\n\n\nfox\ncity\n19.78\n\n\nfox\ncity\n19.45\n\n\nfox\ncity\n21.56\n\n\nfox\ncity\n21.37\n\n\nfox\ncity\n18.64\n\n\nfox\ncity\n20.08\n\n\nfox\ncity\n21.62\n\n\nfox\ncity\n20.68\n\n\nfox\nsmalltown\n19.81\n\n\nfox\nsmalltown\n17.78\n\n\nfox\nsmalltown\n19.65\n\n\nfox\nsmalltown\n16.38\n\n\nfox\nsmalltown\n17.46\n\n\nfox\nsmalltown\n17.02\n\n\nfox\nsmalltown\n19.38\n\n\nfox\nsmalltown\n15.89\n\n\nfox\nsmalltown\n17.15\n\n\nfox\nsmalltown\n17.43\n\n\nfox\nvillage\n15.32\n\n\nfox\nvillage\n17.59\n\n\nfox\nvillage\n15.70\n\n\nfox\nvillage\n18.58\n\n\nfox\nvillage\n16.85\n\n\nfox\nvillage\n18.25\n\n\nfox\nvillage\n18.75\n\n\nfox\nvillage\n16.96\n\n\nfox\nvillage\n13.38\n\n\nfox\nvillage\n18.38\n\n\nfox\nfield\n16.85\n\n\nfox\nfield\n13.55\n\n\nfox\nfield\n13.89\n\n\nfox\nfield\n15.67\n\n\nfox\nfield\n16.38\n\n\nfox\nfield\n14.59\n\n\nfox\nfield\n14.03\n\n\nfox\nfield\n13.63\n\n\nfox\nfield\n14.09\n\n\nfox\nfield\n15.52\n\n\n\n\n\n\nDie Beispieldaten sind in Abbildung 22.6 abgebildet. Wir sehen auf der x-Achse den Faktor animal mit den drei Leveln dog, cat und fox. Jeder dieser Faktorlevel hat nochmal einen Faktor in sich. Dieser Faktor lautet site und stellt dar, wo die Flöhe gesammelt wurden. Die vier Level des Faktors site sind city, smalltown, village und field.\n\n\n\n\nAbbildung 22.6— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nWir bauen dann mit den beiden Variablen bzw. Faktoren animal und site aus dem Objekt fac2_tbl folgendes Modell für die zweifaktorielle ANOVA:\n\\[\njump\\_length \\sim animal + site\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der zweifaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n22.3.2 Hypothesen für die zweifaktorielle ANOVA\nWir haben für jeden Faktor der zweifaktoriellen ANOVA ein Hypothesenpaar. Im Folgenden sehen wir die jeweiligen Hypothesenpaare.\nEinmal für animal, als Haupteffekt. Wir nennen einen Faktor den Hauptfaktor, weil wir an diesem Faktor am meisten interessiert sind. Wenn wir später einen Posthoc Test durchführen würden, dann würden wir diesen Faktor nehmen. Wir sind primär an dem Unterschied der Sprungweiten in [cm] in Gruppen Hund, Katze und Fuchs interessiert.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für site, als Nebeneffekt oder Blockeffekt oder Clustereffekt. Meist eine Variable, die wir auch erhoben haben und vermutlich auch einen Effekt auf das \\(y\\) haben wird. Oder aber wir haben durch das exprimentelle Design noch eine Aufteilungsvariable wie Block vorliegen. In unserem Beispiel ist es site oder der Ort, wo wir die Hunde-, Katzen, und Fuchsflöhe gefunden haben.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{city} = \\bar{y}_{smalltown} = \\bar{y}_{village} = \\bar{y}_{field}\\\\\nH_A: &\\; \\bar{y}_{city} \\ne \\bar{y}_{smalltown}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{village} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für die Interaktion animal:site - die eigentliche Stärke der zweifaktoriellen ANOVA. Wir können uns anschauen, ob die beiden Faktoren miteinander interagieren. Das heißt, ob eine Interaktion zwischen dem Faktor animal und dem Faktor site vorliegt.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{keine Interaktion}\\\\\nH_A: &\\; \\mbox{eine Interaktion zwischen animal und site}\n\\end{aligned}\n\\]\nWir haben also jetzt die verschiedenen Hypothesenpaare definiert und schauen uns jetzt die ANOVA in R einmal in der Anwendung an.\n\n22.3.3 Zweifaktoriellen ANOVA in R\nBei der einfaktoriellen ANOVA haben wir die Berechnungen der Sum of squares nochmal nachvollzogen. Im Falle der zweifaktoriellen ANOVA verzichten wir darauf. Das Prinzip ist das gleiche. Wir haben nur mehr Mitelwerte und mehr Abweichungen von diesen Mittelwerten, da wir ja nicht nur einen Faktor animal vorliegen haben sondern auch noch den Faktor site. Da wir aber die ANOVA nur Anwenden und dazu R nutzen, müssen wir jetzt nicht per Hand die zweifaktorielle ANOVA rechnen. Du musst aber die R Ausgabe der ANOVA verstehen. Und diese Ausgabe schauen wir uns jetzt einmal ohne und dann mit Interaktionsterm an.\n\n22.3.3.1 Ohne Interaktionsterm\nWir wollen nun einmal die zweifaktorielle ANOVA ohne Interaktionsterm rechnen die in Tabelle 22.9 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen die Funktion anova() in R.\n\n\n\nTabelle 22.9— Zweifaktorielle ANOVA ohne Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nerror\n\\(n-(a-1)(b-1)\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im ?sec-modeling-simple-stat erfahren.\nIm Folgenden sehen wir nochmal das Modell ohne Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel.\n\\[\njump\\_length \\sim animal + site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm().\n\nfit_2 <-  lm(jump_length ~ animal + site, data = fac2_tbl) %>% \n  anova\n\nfit_2\n\nAnalysis of Variance Table\n\nResponse: jump_length\n           Df Sum Sq Mean Sq F value   Pr(>F)    \nanimal      2 180.03  90.017 19.8808 3.92e-08 ***\nsite        3   9.13   3.042  0.6718    0.571    \nResiduals 114 516.17   4.528                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten wiederrum die ANOVA Ergebnistabelle. Ansatt nur die Zeile animal für den Effekt des Faktors animal sehen wir jetzt auch noch die Zeile site für den Effekt des Faktors site. Zuerst ist weiterhin der Faktor animal signifikant, da der \\(p\\)-Wert mit \\(0.000000039196\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können von mindestens einem Gurppenunterschied im Faktor animal ausgehen. Im Weiteren ist der Faktor site nicht signifikant. Es scheint keinen Untrschied zwischend den einzelnen Orten und der Sprunglänge von den Hunde-, Katzen- und Fuchsflöhen zu geben.\nNeben der Standausgabe von R können wir auch die tidy Variante uns ausgeben lassen. In dem Fall sieht die Ausgabe etwas mehr aufgeräumt aus.\n\nfit_2 %>% tidy\n\n# A tibble: 3 x 6\n  term         df  sumsq meansq statistic       p.value\n  <chr>     <int>  <dbl>  <dbl>     <dbl>         <dbl>\n1 animal        2 180.    90.0     19.9    0.0000000392\n2 site          3   9.13   3.04     0.672  0.571       \n3 Residuals   114 516.     4.53    NA     NA           \n\n\nAbschließend können wir uns übr \\(\\eta^2\\) auch die erklärten Anteile der Varainz wiedergeben lassen.\n\nfit_2 %>% eta_squared\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\nanimal    |           0.26 | [0.15, 1.00]\nsite      |           0.02 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 26%. Für den Faktor site haben wir nur einen Anteil von 2% der erklärten Varianz. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 22.10 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\nTabelle 22.10— Zweifaktorielle Anova ohne Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dan direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{4.53} = 19.88\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{4.53} = 0.67\\)\n\n\nerror\n\\(120-(3-1)(4-1)\\)\n\\(SS_{error} = 516.17\\)\n\\(MS_{error} = 4.53\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.33\\)\n\n\n\n\n\n\n\n\n22.3.3.2 Mit Interaktionssterm\nWir wollen nun noch einmal die zweifaktorielle ANOVA mit Interaktionsterm rechnen, die in Tabelle 22.11 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen wie immer die Funktion anova() in R.\n\n\n\nTabelle 22.11— Zweifaktorielle ANOVA mit Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nanimal \\(\\times\\) site\n\\((a-1)(b-1)\\)\n\\(SS_{animal \\times site}\\)\n\\(MS_{animal \\times site}\\)\n\\(F_{calc} = \\cfrac{MS_{animal \\times site}}{MS_{error}}\\)\n\n\nerror\n\\(n-ab\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\nIm Folgenden sehen wir nochmal das Modell mit Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel. Einen Interaktionsterm bilden wir durch das : in R ab. Wir können theoretisch auch noch weitere Interaktionsterme bilden, also auch x:y:z. Ich würde aber davon abraten, da diese Interaktionsterme schwer zu interpretieren sind.\n\\[\njump\\_length \\sim animal + site + animal:site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Es wieder das gleich wie schon zuvor. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm(). Auch die Interaktion müssen wir nicht extra in der ANOVA Funktion angeben. Alles wird im Modell des lm() abgebildet.\nDie visuelle Regel zur Überprüfung der Interaktion lautet nun wie folgt. Abbildung 22.7 zeigt die entsprechende Vislualisierung. Wir haben keine Interaktion vorliegen, wenn die Geraden parallel zueinander laufen und die Abstände bei bei jedem Faktorlevel gleich sind. Wir schauen uns im Prinzip die erste Faktorstufe auf der x-Achse an. Wir sehen den Abstand von der roten zu blauen Linie sowie das die blaue Gerade über der roten Gerade liegt. Dieses Muster erwarten wir jetzt auch an dem Faktorlevel B und C. Eine leichte bis mittlere Interaktion liegt vor, wenn sich die Abstaände von dem zweiten Fakotr über die Faktorstufen des ersten Faktors ändern. Eine starke Interaktion liegt vor, wenn sich die Geraden schneiden.\n\n\n\n\n\n(a) Keine Interaktion\n\n\n\n\n\n\n(b) Leichte bis mittlere Intraktion\n\n\n\n\n\n\n(c) Starke Interaktion\n\n\n\n\nAbbildung 22.7— Darstellung von keiner Interaktion, leichter bis mittler Interaktion und starker Interaktion in einer zweifaktoriellen ANOVA mit einem Faktor mit drei Leveln A, B und C sowie einem Faktor mit zwei Leveln (rot und blau).\n\n\nIn der Abbildung 22.8 sehen wir den Interaktionsplot für unser Beispiel. Auf der y-Achse ist die Sprunglänge abgebildet und auf der x-Achse der Faktor animal. Die einzelnen Farben stellen die Level des Faktor site dar.\n\nggplot(fac2_tbl, aes(x = animal, y = jump_length,\n                     color = site, group = site)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_bw()\n\n\n\nAbbildung 22.8— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nWenn sich die Geraden in einem Interaktionsplot schneiden, haben wir eine Interaktion zwischen den beiden Faktoren vorliegen\nWir schauen zur visuellen Überprüfung auf den Faktor animal und das erste level cat. Wir sehen die Ordnung des zweiten Faktors site mit field, village, smalltown und city. Diese Ordnung und die Abstände sind bei zweiten Faktorlevel dog schon nicht mehr gegeben. Die Geraden schneiden sich. Auch liegt bei dem Level fox eine andere ordnung vor. Daher sehen wir hier eine starke Interaktion zwischen den beiden Faktoren animal und site.\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im ?sec-modeling-simple-stat erfahren.\nWir nehmen jetzt auf jeden Fall den Interaktionsterm animal:site mit in unser Modell und schauen uns einmal das Ergebnis der ANOVA an. Das lineare Modell der ANOVA wird erneut über die Funktion lm() berechnet und anschließend in die Funktion anova() gepipt.\n\nfit_3 <-  lm(jump_length ~ animal + site + animal:site, data = fac2_tbl) %>% \n  anova\n\nfit_3\n\nAnalysis of Variance Table\n\nResponse: jump_length\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nanimal        2 180.03  90.017 30.2807 3.63e-11 ***\nsite          3   9.13   3.042  1.0233   0.3854    \nanimal:site   6 195.11  32.519 10.9391 1.71e-09 ***\nResiduals   108 321.05   2.973                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn wir eine signifikante Interaktion vorliegen haben, dann müssen wir den Faktor A getrennt für jedes Levels des Faktors B auswerten.\nDie Ergebnistabelle der ANOVA wiederholt sich. Wir sehen, dass der Faktor animal signifkant ist, da der p-Wert mit \\(0.000000000036\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese ablehnen. Mindestens ein Mittelwertsvergleich unterschiedet sich zwischen den Levels des Faktors animal. Im Weiteren sehen wir, dass der Faktor site nicht signifkant ist, da der p-Wert mit \\(0.39\\) größer ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese nicht ablehnen. Abschließend finden wir die Interaktion zwischen dem Faktor animalund site las signifkant vor. Wenn wir eine signifikante Interaktion vorliegen haben, dann müssen wir den Faktor animal getrennt für jedes Levels des Faktors site auswerten. Wir können keine Aussage über die Sprungweite von Hunde-, Katzen- und Fuchsflöhen unabhängig von der Herkunft site der Flöhe machen.\nIn Kapitel XX findest du ein Beispiel für eine signifikante Interaktion und die folgende Auswertung\nWir können wie immer die etwas aufgeräumte Variante der ANOVA Ausgabe mit der Funktion tidy() uns ausgeben lassen.\n\nfit_3 %>% tidy()\n\n# A tibble: 4 x 6\n  term           df  sumsq meansq statistic   p.value\n  <chr>       <int>  <dbl>  <dbl>     <dbl>     <dbl>\n1 animal          2 180.    90.0      30.3   3.63e-11\n2 site            3   9.13   3.04      1.02  3.85e- 1\n3 animal:site     6 195.    32.5      10.9   1.71e- 9\n4 Residuals     108 321.     2.97     NA    NA       \n\n\nIm Folgenden können wir noch die \\(\\eta^2\\) für die ANOVA als Effektschätzer berechnen lassen.\n\nfit_3 %>% eta_squared\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Eta2 (partial) |       95% CI\n-------------------------------------------\nanimal      |           0.36 | [0.24, 1.00]\nsite        |           0.03 | [0.00, 1.00]\nanimal:site |           0.38 | [0.24, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 36%. Für den Faktor site haben wir nur einen Anteil von 3% der erklärten Varianz. Die Interaktion zwischen animal und site erklärt 38% der beobachteten Varianz udn ist somit auch vom Effekt her nicht zu ignorieren. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 22.12 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\nTabelle 22.12— Zweifaktorielle Anova mit Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dan direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{2.97} = 30.28\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{2.97} = 1.02\\)\n\n\nanimal \\(\\times\\) site\n\\((3-1)(4-1)\\)\n\\(SS_{animal \\times site} = 195.12\\)\n\\(MS_{animal \\times site} = 32.52\\)\n\\(F_{calc} = \\cfrac{32.52}{2.97} = 10.94\\)\n\n\nerror\n\\(120 - (3 \\cdot 4)\\)\n\\(SS_{error} = 321.06\\)\n\\(MS_{error} = 2.97\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.34\\)"
  },
  {
    "objectID": "stat-tests-anova.html#und-weiter",
    "href": "stat-tests-anova.html#und-weiter",
    "title": "22  Die ANOVA",
    "section": "\n22.4 Und weiter?",
    "text": "22.4 Und weiter?\nNach einer berechnten ANOVA können wir zwei Fälle vorliegen haben.\nWenn du in deinem Experiment keine signifikanten Ergebnisse findest, ist das nicht schlimm. Du kannst deine Daten immer noch mit der explorativen Datenanalyse auswerten wie in ?sec-eda-ggplot beschrieben.\n\nWir habe eine nicht signifkante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) nicht ablehnen und die Mittelwerte über den Faktor sind vermutlich alle gleich. Wir enden hier mit unserer statistischen Analyse.\nWir haben eine signifikante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) ablehnen und mindestens ein Gruppenvergleich über mindestens einen Faktor ist vermutlich unterschiedlich. Wir können dann in Kapitel 29 eine Posthoc Analyse rechnen."
  },
  {
    "objectID": "stat-tests-ancova.html",
    "href": "stat-tests-ancova.html",
    "title": "23  Die ANCOVA",
    "section": "",
    "text": "Version vom Oktober 12, 2022 um 14:13:29\nEigentlich hat sich die Analysis of Covariance (ANCOVA) etwas überlebt. Wir können mit dem statistischen Modellieren eigentlich alles was die ANCOVA kann plus wir erhalten auch noch Effektschätzer für die Kovariaten und die Faktoren. Dennoch hat die ANCOVA ihren Platz in der Auswertung von Daten. Wenn du ein oder zwei Faktoren hast plus eine numerische Variable, wie das Startgewicht, für die du die Analyse adjustieren möchtest, dann ist die ANCOVA für dich gemacht.\nAlso kurz gesprochen adjustiert die Analysis of Covariance (ANCOVA) die Faktoren einer ANOVA um eine kontinuierliche Covariate. Adjustiert bedeutet in dem Fall, dass die Effekte des unterschiedlichen Startgewichts von Pflanzen durch das Einbringen der Kovariate mit in der statistischen Analyse berücksichtigt werden. Wir werden hier auch nur über die Nutzung in R sprechen und auf die theoretische Herleitung verzichten.\nWir können die einfaktorielle ANCOVA in folgender Form schreiben. Wir haben haben einen Faktor \\(x_1\\) und eine Kovariate oder aber ein numerisches \\(x_2\\). Damit sähe die ANCOVA wie folgt aus.\n\\[\ny \\sim x_1 + x_2\n\\]\nDamit ist die ANCOVA aber sehr abstrakt beschrieben. Der eine Faktor kommt damit gar nicht zur Geltung. Deshalb schreiben wir die ANCOVA wie folgt mit einem \\(f_1\\) für den Faktor und einem \\(c_1\\) für eine numerische Kovariate. Damit haben wir einen bessere Übersicht.\n\\[\ny \\sim f_1 + c_1\n\\]\nSomit erklärt sich die zweifaktorielle ANCOVA schon fast von alleine. Wir erweitern einfach das Modell um einen zweiten Faktor \\(f_2\\) und haben somit eine zweifaktorielle ANCOVA.\n\\[\ny \\sim f_1 + f_2 + c_1\n\\]\nIm Folgenden schauen wir uns einmal die Daten und die Hypothesen zu einer möglichen Fragestellung an."
  },
  {
    "objectID": "stat-tests-ancova.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-ancova.html#genutzte-r-pakete-für-das-kapitel",
    "title": "23  Die ANCOVA",
    "section": "\n23.1 Genutzte R Pakete für das Kapitel",
    "text": "23.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance)\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'conflicted' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\jokruppa\\AppData\\Local\\Temp\\RtmpkpmF0c\\downloaded_packages\n\n\nWarning: Paket 'conflicted' wurde unter R Version 4.2.1 erstellt\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'performance' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'bayestestR' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'datawizard' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'effectsize' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'insight' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'parameters' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'see' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\jokruppa\\AppData\\Local\\Temp\\RtmpkpmF0c\\downloaded_packages\n\n\nWarning: Paket 'see' wurde unter R Version 4.2.1 erstellt\n\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-ancova.html#daten",
    "href": "stat-tests-ancova.html#daten",
    "title": "23  Die ANCOVA",
    "section": "\n23.2 Daten",
    "text": "23.2 Daten\nFür unser Beispiel nutzen wir die Daten der Sprungweite in [cm] von Flöhen auf Hunde-, Katzen- und Füchsen. Damit haben wir den ersten Faktor animal mit drei Leveln. Als Kovariate schauen wir uns das Gewicht als numerische Variable an. Schlussendlich brauchen wir noch das Outcome jump_length als \\(y\\). Für die zweifaktorielle ANCOVA nehmen wir noch den Faktor sex mit zwei Leveln hinzu.\n\nancova_tbl <- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %>%\n  select(animal, sex, jump_length, weight) %>% \n  mutate(animal = as_factor(animal))\n\nIn der Tabelle 23.1 ist der Datensatz ancova_tbl nochmal dargestellt.\n\n\n\n\nTabelle 23.1— Datensatz zu der Sprunglänge in [cm] von Flöhen auf Hunde-, Katzen- und Füchsen.\n\nanimal\nsex\njump_length\nweight\n\n\n\ncat\nmale\n15.79\n6.02\n\n\ncat\nmale\n18.33\n5.99\n\n\ncat\nmale\n17.58\n8.05\n\n\ncat\nmale\n14.09\n6.71\n\n\ncat\nmale\n18.22\n6.19\n\n\ncat\nmale\n13.49\n8.18\n\n\n…\n…\n…\n…\n\n\nfox\nfemale\n27.81\n8.04\n\n\nfox\nfemale\n24.02\n9.03\n\n\nfox\nfemale\n24.53\n7.42\n\n\nfox\nfemale\n24.35\n9.26\n\n\nfox\nfemale\n24.36\n8.85\n\n\nfox\nfemale\n22.13\n7.89"
  },
  {
    "objectID": "stat-tests-ancova.html#hypothesen-für-die-ancova",
    "href": "stat-tests-ancova.html#hypothesen-für-die-ancova",
    "title": "23  Die ANCOVA",
    "section": "\n23.3 Hypothesen für die ANCOVA",
    "text": "23.3 Hypothesen für die ANCOVA\nWir haben für jeden Faktor der ANCOVA ein Hypothesenpaar sowie ein Hypothesenpaar für die Kovariate. Im Folgenden sehen wir die jeweiligen Hypothesenpaare.\nEinmal für animal, als Haupteffekt. Wir nennen einen Faktor den Hauptfaktor, weil wir an diesem Faktor am meisten interessiert sind. Wenn wir später einen Posthoc Test durchführen würden, dann würden wir diesen Faktor nehmen. Wir sind primär an dem Unterschied der Sprungweiten in [cm] in Gruppen Hund, Katze und Fuchs interessiert.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nDu kannst mehr über Geraden sowei lineare Modelle und deren Eigenschaften im ?sec-modeling-simple-stat erfahren.\nFür die Kovariate testen wir anders. Die Kovariate ist ja eine numerische Variable. Daher ist die Frage, wann gibt es keinen Effekt von weight auf die Sprunglänge? Wenn wir eine parallele Linie hätten. Das heißt, wenn sich der Wert von weight ändert, ändert sich der Wert von jump_length nicht. Wir schreiben, dass sich die Steigung der Geraden nicht ändert. Wir bezeichnen die Steigung einer Graden mit \\(\\beta\\). Wenn kein Effekt vorliegt und die Nullhpyothese gilt, dann ist die Steigung der Geraden \\(\\beta_{weight} = 0\\).\n\\[\n\\begin{aligned}\nH_0: &\\; \\beta_{weight} = 0\\\\\nH_A: &\\; \\beta_{weight} \\neq 0\n\\end{aligned}\n\\]\nDu kannst dir überlegen, ob due die Interaktion zwischen dem Faktor und der Kovariate mit ins Modell nehmen willst. Eigentlich schauen wir uns immer nur die Interaktion zwischen den Faktoren an. Generell schreiben wir eine Interaktionshypothese immer in Prosa.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{keine Interaktion}\\\\\nH_A: &\\; \\mbox{eine Interaktion zwischen animal und site}\n\\end{aligned}\n\\]\nWir haben also jetzt die verschiedenen Hypothesenpaare definiert und schauen uns jetzt die ANCOVA in R einmal in der Anwendung an."
  },
  {
    "objectID": "stat-tests-ancova.html#die-einfaktorielle-ancova-in-r",
    "href": "stat-tests-ancova.html#die-einfaktorielle-ancova-in-r",
    "title": "23  Die ANCOVA",
    "section": "\n23.4 Die einfaktorielle ANCOVA in R",
    "text": "23.4 Die einfaktorielle ANCOVA in R\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im ?sec-modeling-simple-stat erfahren.\nWir können die ANCOVA ganz klassisch mit dem linaren Modell fitten. Wir nutzen die Funktion lm() um die Koeffizienten des linearen Modellls zu erhalten. Wir erinnern uns, wir haben haben einen Faktor \\(f_1\\) und eine Kovariate bezwiehungsweise ein numerisches \\(c_1\\). In unserem Beispiel sieht dann der Fit des Modells wie folgt aus.\n\nfit_1 <- lm(jump_length ~ animal + weight + animal:weight, data = ancova_tbl)\n\nNachdem wir das Modell in dem Objekt fit_1 gespeichert haben können wir dann das Modell in die Funktion anova() pipen. Die Funktion erkennt, das wir eine ANCOVA rechnen wollen, da wir in unserem Modell einen Faktor und eine Kovariate mit enthalten haben.\n\nfit_1 %>% anova \n\nAnalysis of Variance Table\n\nResponse: jump_length\n               Df Sum Sq Mean Sq  F value Pr(>F)    \nanimal          2 2693.8 1346.88 204.2764 <2e-16 ***\nweight          1 1918.0 1917.99 290.8961 <2e-16 ***\nanimal:weight   2    0.3    0.14   0.0214 0.9788    \nResiduals     594 3916.5    6.59                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn der ANCOVA erkennne wir nun, dass der Faktor animal signifikant ist. Der \\(p\\)-Wert ist mit \\(<0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Ebenso ist die Kovariate weight signifikant. Der \\(p\\)-Wert ist ebenfalls mit \\(<0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können also schlussfolgern, dass sich mindestens eine Gruppenvergleich der Level des Faktors animal voneinander unterscheidet. Wir wissen auch, dass mit der Zunahme des Gewichts, die Sprunglänge sich ändert.\nDie ANCOVA liefert keine Informationen zu der Größe oder der Richtung des Effekts der Kovariate.\nWas wir nicht wissen, ist die Richtung. Wir wissen nicht, ob mit ansteigenden Gewicht sich die Sprunglänge erhöht oder vermindert. Ebenso wenig wissen wir etwas über den Betrag des Effekts. Wieviel weiter springen denn nun Flöhe mit 1 mg Gewicht mehr? Wir haben aber die Möglichkeit, den Sachverhalt uns einmal in einer Abbildung zu visualisieren. In Abbildung 23.1 sehen wir die Daten einmal als Scatterplot dargestellt.\n\nggplot(ancova_tbl, aes(weight, jump_length, color = animal)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_okabeito() +\n  theme_bw() +\n  geom_point() +\n  labs(color  = \"Tierart\", shape = \"Geschlecht\")  \n\n\n\nAbbildung 23.1— Scatterplot der Daten zur einfaktoriellen ANCOVA.\n\n\n\n\nDer Abbildung 23.1 können wir jetzt die positive Steigung entnehmen sowie die Reihenfolge der Tierarten nach Sprungweiten. Die ANCOVA sollte immer visualisiert werden, da sich hier die Stärke der Methode mit der Visualiserung verbindet."
  },
  {
    "objectID": "stat-tests-ancova.html#die-zweifaktorielle-ancova-in-r",
    "href": "stat-tests-ancova.html#die-zweifaktorielle-ancova-in-r",
    "title": "23  Die ANCOVA",
    "section": "\n23.5 Die zweifaktorielle ANCOVA in R",
    "text": "23.5 Die zweifaktorielle ANCOVA in R\nDie zweifaktorielle ANCOVA erweitert die einfaktorielle ANCOVA um einen weiteren Faktor. Das ist manchmal etwas verwirrend, da wir auf einmal drei oder mehr Terme in einem Modell haben. Klassischerweise haben wir nun zwei Faktoren \\(f_1\\) und \\(f_2\\) in dem Modell. Weiterhin haben wir nur eine Kovariate \\(c_1\\). Damit sehe das Modell wie folgt aus.\n\\[\ny \\sim f_1 + f_2 + c_1\n\\]\nWir können das Modell dann in R übertragen und ergänzen noch den Interaktionsterm für die Faktoren animal und sex in dem Modell. Das Modell wird klassisch in der Funktion lm() gefittet.\n\nfit_2 <- lm(jump_length ~ animal + sex + weight + animal:sex, data = ancova_tbl)\n\nNach dem Fit können wir das Modell in dem Obkjekt fit_2 in die Funktion anova() pipen. Die Funktion erkennt die Struktur des Modells und gibt uns eine ANCOVA Ausgabe wieder.\n\nfit_2 %>% anova \n\nAnalysis of Variance Table\n\nResponse: jump_length\n            Df Sum Sq Mean Sq  F value Pr(>F)    \nanimal       2 2693.8  1346.9 359.0568 <2e-16 ***\nsex          1 3608.1  3608.1 961.8568 <2e-16 ***\nweight       1    0.0     0.0   0.0053 0.9422    \nanimal:sex   2    2.2     1.1   0.2981 0.7424    \nResiduals  593 2224.4     3.8                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn der ANCOVA erkennne wir nun, dass der Faktor animal signifikant ist. Der \\(p\\)-Wert ist mit \\(<0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Ebenso ist der Faktor sex signifikant. Der \\(p\\)-Wert ist mit \\(<0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Die Kovariate weight ist nicht mehr signifikant. Der \\(p\\)-Wert ist mit \\(0.94\\) größer das das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können also schlussfolgern, dass sich mindestens eine Gruppenvergleich der Level des Faktors animal voneinander unterscheidet. Ebenso wie können wir schlussfolgern, dass sich mindestens eine Gruppenvergleich der Level des Faktors site voneinander unterscheidet. Da wir nur zwei Level in dem Faktor sex haben, wissenwir nun, dass sich die beiden Geschlechter der Flöhe in der Sprungweite unterscheiden. Wir wissen auch, dass mit der Zunahme des Gewichts, sich die Sprunglänge nicht ändert.\nIn Abbildung 23.2 sehen wir nochmal den Zusammenhang dargestellt. Wenn wir die Daten getrennt für den Faktor sex anschauen, dann sehen wir, dass das Gewicht keinen Einfluss mehr auf die Sprungweite hat.\n\nggplot(ancova_tbl, aes(weight, jump_length, color = animal)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_okabeito() +\n  theme_bw() +\n  geom_point() +\n  labs(color  = \"Tierart\", shape = \"Geschlecht\") +\n  facet_wrap(~ sex, scales = \"free_x\")\n\n\n\nAbbildung 23.2— Scatterplot der Daten zur einfaktoriellen ANCOVA aufgetelt nach dem Geschlecht der Flöhe."
  },
  {
    "objectID": "stat-tests-ancova.html#und-weiter",
    "href": "stat-tests-ancova.html#und-weiter",
    "title": "23  Die ANCOVA",
    "section": "\n23.6 Und weiter?",
    "text": "23.6 Und weiter?\nNach einer berechnten ANCOVA können wir zwei Fälle vorliegen haben.\nWenn du in deinem Experiment keine signifikanten Ergebnisse findest, ist das nicht schlimm. Du kannst deine Daten immer noch mit der explorativen Datenanalyse auswerten wie in ?sec-eda-ggplot beschrieben.\n\nWir habe eine nicht signifkante ANCOVA berechnet. Wir können die Nullhypothese \\(H_0\\) nicht ablehnen und die Mittelwerte über den Faktor sind vermutlich alle gleich. Wir enden hier mit unserer statistischen Analyse.\nWir haben eine signifikante ANCOVA berechnet. Wir können die Nullhypothese \\(H_0\\) ablehnen und mindestens ein Gruppenvergleich über mindestens einen Faktor ist vermutlich unterschiedlich. Wir können dann in Kapitel 29 eine Posthoc Analyse rechnen."
  },
  {
    "objectID": "stat-tests-utest.html",
    "href": "stat-tests-utest.html",
    "title": "24  Der Wilcoxon-Mann-Whitney-Test",
    "section": "",
    "text": "Version vom Oktober 12, 2022 um 14:13:46\nWann nutzen wir den Wilcoxon-Mann-Whitney-Test? Wir nutzen den Wilcoxon-Mann-Whitney-Test wenn wir zwei Verteilungen miteinander vergleichen wollen. Das ist jetzt sehr abstrakt. Konrekt, wenn wir zwei Gruppen haben und ein nicht normalverteiltes \\(y\\). Haben wir ein normalverteiltes \\(y\\) rechnen wir meist einen t-Test. Wir könnten aber auch einen Wilcoxon-Mann-Whitney-Test rechnen.\nWas ist jetzt der Unterschied zwischen einem Wilcoxon-Mann-Whitney-Test und einem t-Test? Der t-Test vergleicht die Mittelwerte zweier Normalverteilungen, also zum Beispiel die Verteilung der Sprungweiten der Hundeflöhe gegen die Verteilung der Sprungweiten der Katzenflöhe. Dazu nutzt der t-Test die Mittelwerte und die Standardabweichung. Beides sind Parameter einer Verteilung und somit ist der t-Test ein parametrischer Test.\nDer Wilcoxon-Mann-Whitney-Test ist die nicht-parametrische Variante in dem wir die Zahlen in Ränge umwandeln, also sortieren, und mit den Rängen der Zahlen rechnen. Die deskriptiven Maßzahlen wären dann Median, Quantile und Quartile. Das heißt wir vergleichen mit dem Wilcoxon-Mann-Whitney-Test die Mediane. Wir wollen also wissen, ob sich die Mediane zwischen den Sprungweiten von Hunde- und Katzenflöhen unterscheiden."
  },
  {
    "objectID": "stat-tests-utest.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-utest.html#genutzte-r-pakete-für-das-kapitel",
    "title": "24  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n24.1 Genutzte R Pakete für das Kapitel",
    "text": "24.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, coin)\n\ninstalliere auch Abhängigkeiten 'libcoin', 'matrixStats', 'modeltools'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'libcoin' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'matrixStats' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'modeltools' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'coin' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\jokruppa\\AppData\\Local\\Temp\\RtmpUBx2C8\\downloaded_packages\n\n\n\ncoin installed\n\n\nWarning: Paket 'coin' wurde unter R Version 4.2.1 erstellt\n\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-utest.html#daten-für-den-wilcoxon-mann-whitney-test",
    "href": "stat-tests-utest.html#daten-für-den-wilcoxon-mann-whitney-test",
    "title": "24  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n24.2 Daten für den Wilcoxon-Mann-Whitney-Test",
    "text": "24.2 Daten für den Wilcoxon-Mann-Whitney-Test\nBindungen (eng. ties) in den Daten sind ein Problem und müssen beachtet werden. Das heißt, wenn es gleiche Zahlen in den Gruppen gibt.\nFür die Veranschaulichung des Wilcoxon-Mann-Whitney-Test nehmen wir ein simples Beispiel. Wir nehmen ein nicht normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Wir nehmen hierbei an, dass die Sprunglänge jetzt mal nicht normalverteilt ist. Später sind es Boniturnoten, die definitiv nicht normalverteilt sind. Aber mit der Sprunglänge ist das Beispiel einfacher nachzuvollziehen. Darüber hinaus haben wir so keine Bindungen in den Daten. Bindungen (eng. ties) heißt, dass wir die numerisch gleichen Zahlen in beiden Gruppen haben.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln. Wir nehmen in diesem Beispiel an, dass die Variable jump_length nicht normalverteilt ist.\n\ndata_tbl <- read_excel(\"data/flea_dog_cat.xlsx\") %>% \n  select(animal, jump_length, grade) %>% \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt data_tbl mit dem Datensatz in Tabelle 24.1 nochmal dargestellt.\n\n\n\n\nTabelle 24.1— Selektierter Datensatz für den Wilcoxon-Mann-Whitney-Test mit einer nicht-normalverteilten Variable jump_length und einem Faktor animal mit zwei Leveln.\n\nanimal\njump_length\ngrade\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n8.2\n8\n\n\ndog\n5.6\n7\n\n\ndog\n9.1\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt data_tbl folgendes Modell für später:\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wie immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in dem Wilcoxon-Mann-Whitney-Test aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese."
  },
  {
    "objectID": "stat-tests-utest.html#hypothesen-für-den-wilcoxon-mann-whitney-test",
    "href": "stat-tests-utest.html#hypothesen-für-den-wilcoxon-mann-whitney-test",
    "title": "24  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n24.3 Hypothesen für den Wilcoxon-Mann-Whitney-Test",
    "text": "24.3 Hypothesen für den Wilcoxon-Mann-Whitney-Test\nDer Wilcoxon-Mann-Whitney-Test betrachtet die Mediane und Ränge um einen Unterschied nachzuweisen. Daher haben wir die Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mediane der beiden Levels des Faktors animal gleich sind. Wir vergleichen im Wilcoxon-Mann-Whitney-Test nur zwei Gruppen.\n\\[\nH_0: \\; \\widetilde{y}_{cat} = \\widetilde{y}_{dog}\n\\]\nDie Alternative lautet, dass sich die beiden Gruppen im Median unterscheiden. Wir können uns über die Boxplots oder aber die berechneten Mediane dann den Unterschied bewerten.\n\\[\nH_A: \\; \\widetilde{y}_{cat} \\ne \\widetilde{y}_{dog}\n\\]\nWir schauen uns jetzt einmal den Wilcoxon-Mann-Whitney-Test theoretisch an bevor wir uns mit der Anwendung des Wilcoxon-Mann-Whitney-Test in R beschäftigen."
  },
  {
    "objectID": "stat-tests-utest.html#wilcoxon-mann-whitney-test-theoretisch",
    "href": "stat-tests-utest.html#wilcoxon-mann-whitney-test-theoretisch",
    "title": "24  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n24.4 Wilcoxon-Mann-Whitney-Test theoretisch",
    "text": "24.4 Wilcoxon-Mann-Whitney-Test theoretisch\nDer Wilcoxon-Mann-Whitney-Test berechnet die U Teststatistik auf den Rängend der Daten. Es gibt genau soviele Ränge wie es Beobachtungen im Datensatz gibt. Wir haben \\(n = 14\\) Beobachtungen in unseren Daten zu der Sprungweite in [cm] von den Hunde- und Katzenflöhen. Somit müssen wir auch vierzehn Ränge vergeben.\nDie Tabelle 24.2 zeigt das Vorgehen der Rangvergabe. Wir sortieren als erstes das \\(y\\) aufsteigend. In unserem Fall ist das \\(y\\) die Sprunglänge. Dann vergeben wir die Ränge jweiles zugehörig zu der Position der Sprunglänge und der Tierart. Abschließend addieren wir die Rangsummmen für cat und dog zu den Rangsummen \\(R_{cat}\\) und \\(R_{dog}\\).\n\n\nTabelle 24.2— Datentablle absteigend sortiert nach der Sprunglänge in [cm]. Die Level cat und dog haben jeweils die entsprechenden Ränge zugeordnet bekommen und die Rangsummen wurden berechnet\n\nRank\nanimal\njump_length\nRänge “cat”\nRänge “dog”\n\n\n\n1\ncat\n2.2\n1\n\n\n\n2\ncat\n3.2\n2\n\n\n\n3\ncat\n4.1\n3\n\n\n\n4\ncat\n4.3\n4\n\n\n\n5\ncat\n5.4\n5\n\n\n\n6\ndog\n5.6\n\n6\n\n\n7\ndog\n5.7\n\n7\n\n\n8\ncat\n6.1\n8\n\n\n\n9\ndog\n7.6\n\n9\n\n\n10\ncat\n7.9\n10\n\n\n\n11\ndog\n8.2\n\n11\n\n\n12\ndog\n8.9\n\n12\n\n\n13\ndog\n9.1\n\n13\n\n\n14\ndog\n11.8\n\n14\n\n\n\n\nRangsummen\n\\(R_{cat} = 33\\)\n\\(R_{dog} = 72\\)\n\n\n\n\nGruppengröße\n7\n7\n\n\n\n\nDie Formel für die U Statistik sieht ein wenig wild aus, aber wir können eigentlich relativ einfach alle Zahlen einsetzen. Dann musst du dich etwas konzentrieren bei der Rechnung.\n\\[\nU_{calc} = n_1n_2 + \\cfrac{n_1(n_1+1)}{2}-R_1\n\\]\nmit\n\n\n\\(R_1\\) der größeren der beiden Rangsummen,\n\n\\(n_1\\) die Fallzahl der größeren der beiden Rangsummen\n\n\\(n_2\\) die Fallzahl der kleineren der beiden Rangsummen\n\nWir setzen nun die Zahlen ein. Da wir ein balanciertes Design vorliegen haben sind die Fallzahlen \\(n_1 = n_2 = 7\\) gleich. Wir müssen nur schauen, dass wir mit \\(R_1\\) die passende Rangsumme wählen. In unserem Fall ist \\(R_1 = R_{dog} = 72\\).\n\\[\nU_{calc} = 7 \\cdot 7 + \\cfrac{7(7+1)}{2}-72 = 5\n\\]\nDer kritische Wert für die U Statistik ist \\(U_{\\alpha = 5\\%} = 8\\) für \\(n_1 = 7\\) und \\(n_2 = 7\\). Bei der Entscheidung mit der berechneten Teststatistik \\(U_{calc}\\) gilt, wenn \\(U_{calc} \\leq U_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. Da in unserem Fall das \\(U_{calc}\\) mit \\(5\\) kleiner ist als das \\(U_{\\alpha = 5\\%} = 8\\) können wir die Nullhypothese ablehnen. Wir haben ein signifkianten Unterschied in den Medianen zwischen den beiden Tierarten im Bezug auf die Sprungweite in [cm] von Flöhen.\nBei grosser Stichprobe, wenn \\(n_1 + n_2 > 30\\) ist, können wir die U Statistik auch standariseren und damit in den z-Wert transformieren.\n\\[\nz_{calc} = \\cfrac{U_{calc} - \\bar{U}}{s_U} = \\cfrac{U_{calc} - \\cfrac{n_1 \\cdot n_2}{2}}{\\sqrt{\\cfrac{n_1 \\cdot n_2 (n_1 + n_2 +1)}{12}}}\n\\]\nmit\n\n\n\\(\\bar{U}\\) dem Mittelwert der U-Verteilung ohne Unterschied zwischen den Gruppen\n\n\\(s_U\\) Standardfehler des U-Wertes\n\n\\(n_1\\) Stichprobengrösse der Gruppe mit der grösseren Rangsumme\n\n\\(n_2\\) Stichprobengrösse der Gruppe mit der kleineren Rangsumme\n\nWir setzen dafür ebenfalls die berechnete U Statistik ein und müssen dann wieder konzentriert rechnen.\n\\[\nz_{calc} = \\cfrac{5 - \\cfrac{7 \\cdot 7}{2}}{\\sqrt{\\cfrac{7 \\cdot 7 (7 + 7 +1)}{12}}} = \\cfrac{-19.5}{7.83} = |-2.46|\n\\]\nDer kritische Wert für die z-Statistik ist \\(z_{\\alpha = 5\\%} = 1.96\\). Bei der Entscheidung mit der berechneten Teststatistik \\(z_{calc}\\) gilt, wenn \\(z_{calc} \\geq z_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. Wir haben eine berechnete z Statistik von \\(z_{calc} = 2.46\\). Damit ist \\(z_{calc}\\) größer als \\(z_{\\alpha = 5\\%} = 1.96\\) und wir können die Nullhypothese ablehnen. Wir haben einen signifkanten Unterschied zwischen den Medianen der beiden Floharten im Bezug auf die Sprunglänge in [cm].\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(U_{\\boldsymbol{calc}}\\) oder der Teststatistik \\(z_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(U_{calc}\\) gilt, wenn \\(U_{calc} \\leq U_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nBei der Entscheidung mit der berechneten Teststatistik \\(z_{calc}\\) gilt, wenn \\(z_{calc} \\geq z_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr."
  },
  {
    "objectID": "stat-tests-utest.html#wilcoxon-mann-whitney-test-in-r",
    "href": "stat-tests-utest.html#wilcoxon-mann-whitney-test-in-r",
    "title": "24  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n24.5 Wilcoxon-Mann-Whitney-Test in R",
    "text": "24.5 Wilcoxon-Mann-Whitney-Test in R\nDie Nutzung des Wilcoxon-Mann-Whitney-Test in R ist relativ einfach mit der Funktion wilxoc.test(). Wir müssen zum einen entscheiden, ob Bindungen in den Daten vorliegen. Sollte Bindungen vorliegen, warnt uns R und wir nutzen dann die Funktion wilcox_test() aus dem R Paket coin.\n\n24.5.1 Ohne Bindungen\nOhne Bindungen können wir die Funktion wilxoc.test() nutzen. Die Funktion benötigt das Modell in formula Syntax in der Form jump_length ~ animal. Wir geben noch an, dass wir die 95% Konfidenzintervalle wiedergegeben haben wollen.\n\nwilcox.test(jump_length ~ animal, data = data_tbl, \n            conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  jump_length by animal\nW = 44, p-value = 0.01107\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 1.0 5.9\nsample estimates:\ndifference in location \n                   3.5 \n\n\nWir sehen das der Wilcoxon-Mann-Whitney-Test ein signifikantes Ergebnis liefert, da der \\(p\\)-Wert mit 0.011 kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Die Nullhypothese kann daher abgelehnt werden. Wir haben einen medianen Unterschied in den Sprungweiten von 3.5 cm [1.0; 5.9] zwischen Hunde- und Katzenflöhen.\n\n24.5.2 Mit Bindungen\nMit Bindungen können wir die Funktion wilxoc_test() aus dem R Paket coin nutzen. Wir nutzen hier als \\(y\\) die Boniturnoten grade der Hunde und Katzen. Die Funktion benötigt das Modell in formula Syntax in der Form grade ~ animal. Wir geben noch an, dass wir die 95% Konfidenzintervalle wiedergegeben haben wollen. Wenn du die Funktion wilcox.test() nutzen würdest, würde dir R eine Warnung ausgeben: Warning: cannot compute exact p-value with ties. Du wüsstest dann, dass du die Funktion wechseln musst.\n\nwilcox_test(grade ~ animal, data = data_tbl, \n            conf.int = TRUE) \n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  grade by animal (dog, cat)\nZ = 2.4973, p-value = 0.01251\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n 1 3\nsample estimates:\ndifference in location \n                     2 \n\n\nWir sehen das der Wilcoxon-Mann-Whitney-Test ein signifikantes Ergebnis liefert, da der \\(p\\)-Wert mit 0.015 kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Die Nullhypothese kann daher abgelehnt werden. Wir haben einen medianen Unterschied in den Boniturnoten von 2 [0; 3] zwischen Hunde und Katzen."
  },
  {
    "objectID": "stat-tests-utest.html#minimale-fallzahl-je-gruppe",
    "href": "stat-tests-utest.html#minimale-fallzahl-je-gruppe",
    "title": "24  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n24.6 Minimale Fallzahl je Gruppe",
    "text": "24.6 Minimale Fallzahl je Gruppe\nHäufig wird auch der Wilcoxon-Mann-Whitney-Test eingesetzt, wenn wenig Beobachtungen vorliegen. Es gibt aber eine untere Grenze der Signifikanz. Das heißt unter einer Fallzahl von \\(n_1 = 3\\) und \\(n_2 = 3\\) wird ein Wilcoxon-Mann-Whitney-Test nicht mehr signifikant. Egal wie groß der Unterschied ist, ein Wilcoxon-Mann-Whitney-Test wird dann die Nulhypothese nicht ablehnen können. Schauen wir das Datenbeispiel in Tabelle 24.3 einmal an.\n\n\n\n\nTabelle 24.3— Kleiner Datensatz mit jeweils nur drei Beobachtungen pro Gruppe.\n\nanimal\njump_length\n\n\n\ndog\n1.2\n\n\ndog\n5.6\n\n\ndog\n3.2\n\n\ncat\n100.3\n\n\ncat\n111.2\n\n\ncat\n98.5\n\n\n\n\n\n\nWir sehen jeweils drei Beobachtunge für Hunde- und Katzensprungweiten. Der Unterschied ist numerisch riesig. Wir können uns den Unterschied nochmal in Abbildung 24.1 visualisieren.\n\n\n\n\nAbbildung 24.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\n\nWir sehen, der Unterschied ist riesig. Der Wilcoxon-Mann-Whitney-Test findet jedoch nur einen p-Wert von 0.1 und kann damit die Nullhypothese nicht ablehnen. Wir haben keinen signifkanten Unterschied.\n\nwilcox.test(jump_length ~ animal, data = small_tbl)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  jump_length by animal\nW = 9, p-value = 0.1\nalternative hypothesis: true location shift is not equal to 0\n\n\nWir sehen hier ein schönes Beispiel für die Begrenztheit von Algorithmen und mathematischen Formeln. Es gibt einen Unterschied, aber der Wilcoxon-Mann-Whitney-Test ist technisch nicht in der Lage einen Unterschied nochzuweisen. Daher solltest du immer versuchen die Ergebnisse eines Testes mit einer Abbildung zu überprüfen."
  },
  {
    "objectID": "stat-tests-kruskal.html",
    "href": "stat-tests-kruskal.html",
    "title": "25  Der Kruskal-Wallis-Test",
    "section": "",
    "text": "Version vom Oktober 12, 2022 um 14:13:57\nWann nutzen wir den Kruskal-Wallis-Test? Wir nutzen den Kruskal-Wallis-Test wenn wir mehrere Verteilungen miteinander vergleichen wollen. Das ist jetzt sehr abstrakt. Konrekt, wenn wir mehrere Gruppen haben und ein nicht normalverteiltes \\(y\\). Haben wir ein normalverteiltes \\(y\\) rechnen wir meist eine einfaktorielle ANOVA. Das heißt, der Kruskal-Wallis-Test ist im Prinzip die einfaktorielle ANOVA für nicht-normalverteilte Daten.\nWas ist jetzt der Unterschied zwischen einem Kruskal-Wallis-Test und einer einfaktoriellen ANOVA? Die ANOVA vergleicht die Mittelwerte mehrerer Normalverteilungen, also zum Beispiel die Verteilung der Sprungweiten der Hundeflöhe gegen die Verteilung der Sprungweiten der Katzenflöhe sowie gegen die Verteilung der Sprungweiten von Fuchsflöhen. Dazu nutzt die ANOVA die Abweichungsquadrate von den Mittelwerten. Damit nutze die ANOVAB Parameter einer Verteilung und somit ist der ANOVA ein parametrischer Test.\nDer Kruskal-Wallis-Test ist die nicht-parametrische Variante in dem wir die Zahlen in Ränge umwandeln, also sortieren, und mit den Rängen der Zahlen rechnen. Die deskriptiven Maßzahlen wären dann Median, Quantile und Quartile. Das heißt wir vergleichen mit dem Kruskal-Wallis-Test die Mediane mehrer Gruppen miteinander. Wir wollen also wissen, ob sich die Mediane zwischen den Sprungweiten von Hunde-, Katzen- und Fuchsflöhen unterscheiden."
  },
  {
    "objectID": "stat-tests-kruskal.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-kruskal.html#genutzte-r-pakete-für-das-kapitel",
    "title": "25  Der Kruskal-Wallis-Test",
    "section": "\n25.1 Genutzte R Pakete für das Kapitel",
    "text": "25.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, rstatix)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-kruskal.html#daten-für-den-kruskal-wallis-test",
    "href": "stat-tests-kruskal.html#daten-für-den-kruskal-wallis-test",
    "title": "25  Der Kruskal-Wallis-Test",
    "section": "\n25.2 Daten für den Kruskal-Wallis-Test",
    "text": "25.2 Daten für den Kruskal-Wallis-Test\nBindungen (eng. ties) in den Daten sind ein Problem und müssen beachtet werden. Das heißt, wenn es gleiche Zahlen in den Gruppen gibt.\nWir wollen uns nun erstmal den einfachsten Fall anschauen mit einem simplen Datensatz. Wir nehmen ein nicht-normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Hätten wir nur zwei Level, dann können wir auch einen Wilcoxon-Mann-Whitney-Test rechnen können.\nWir nehmen in diesem Abschnitt an, dass die Sprunglänge jetzt mal nicht normalverteilt ist. Später sind es Boniturnoten, die definitiv nicht normalverteilt sind. Aber mit der Sprunglänge ist das Beispiel einfacher nachzuvollziehen. Darüber hinaus haben wir so keine Bindungen in den Daten. Bindungen (eng. ties) heißt, dass wir die numerisch gleichen Zahlen in beiden Gruppen haben.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln. Wir nehmen in diesem Beispiel an, dass die Variable jump_length nicht normalverteilt ist.\n\nfac1_tbl <- read_csv2(\"data/flea_dog_cat_fox.csv\") %>%\n  select(animal, jump_length, grade) %>% \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt fac1_tbl mit dem Datensatz in Tabelle 25.1 nochmal dargestellt.\n\n\n\n\nTabelle 25.1— Selektierter Datensatz für den Kruskal-Wallis-Test mit einer nicht-normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\ngrade\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n8.2\n8\n\n\ndog\n5.6\n7\n\n\ndog\n9.1\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\nfox\n7.7\n5\n\n\nfox\n8.1\n4\n\n\nfox\n9.1\n4\n\n\nfox\n9.7\n5\n\n\nfox\n10.6\n4\n\n\nfox\n8.6\n4\n\n\nfox\n10.3\n3\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt fac1_tbl folgendes Modell für später:\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wie immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in dem Kruskal-Wallis-Test aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese."
  },
  {
    "objectID": "stat-tests-kruskal.html#hypothesen-für-den-kruskal-wallis-test",
    "href": "stat-tests-kruskal.html#hypothesen-für-den-kruskal-wallis-test",
    "title": "25  Der Kruskal-Wallis-Test",
    "section": "\n25.3 Hypothesen für den Kruskal-Wallis-Test",
    "text": "25.3 Hypothesen für den Kruskal-Wallis-Test\nDer Kruskal-Wallis-Test betrachtet die Mediane und Ränge um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mediane jedes Levels des Faktors animal gleich sind.\n\\[\nH_0: \\; \\widetilde{y}_{cat} = \\widetilde{y}_{dog} = \\widetilde{y}_{fox}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Medianen unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mediane unterschieden oder eben nur ein Paar. Wenn ein Kruskal-Wallis-Test die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mediane sich unterscheiden.\n\\[\n\\begin{aligned}\nH_A: &\\; \\widetilde{y}_{cat} \\ne \\widetilde{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{cat} \\ne \\widetilde{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{dog} \\ne \\widetilde{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nWir schauen uns jetzt einmal den Kruskal-Wallis-Test theoretisch an bevor wir uns mit der Anwendung des Kruskal-Wallis-Test in R beschäftigen."
  },
  {
    "objectID": "stat-tests-kruskal.html#kruskal-wallis-test-theoretisch",
    "href": "stat-tests-kruskal.html#kruskal-wallis-test-theoretisch",
    "title": "25  Der Kruskal-Wallis-Test",
    "section": "\n25.4 Kruskal-Wallis-Test theoretisch",
    "text": "25.4 Kruskal-Wallis-Test theoretisch\nDer Kruskal-Wallis-Test berechnet die H Teststatistik auf den Rängend der Daten. Es gibt genau soviele Ränge wie es Beobachtungen im Datensatz gibt. Wir haben \\(n = 21\\) Beobachtungen in unseren Daten zu der Sprungweite in [cm] von den Hunde-, Katzen- und Fuchsflöhen. Somit müssen wir auch einundzwanzig Ränge vergeben.\nDie Tabelle 25.2 zeigt das Vorgehen der Rangvergabe. Wir sortieren als erstes das \\(y\\) aufsteigend. In unserem Fall ist das \\(y\\) die Sprunglänge. Dann vergeben wir die Ränge jweiles zugehörig zu der Position der Sprunglänge und der Tierart. Abschließend addieren wir die Rangsummmen für cat, dog und fox zu den Rangsummen \\(R_{cat}\\), \\(R_{dog}\\) und \\(R_{fox}\\).\n\n\n\nTabelle 25.2— Datentablle absteigend sortiert nach der Sprunglänge in [cm]. Die Level cat, dog und fox haben jeweils die entsprechenden Ränge zugeordnet bekommen und die Rangsummen wurden berechnet\n\n\n\n\n\n\n\n\n\nRank\nanimal\njump_length\nRänge “cat”\nRänge “dog”\nRänge “fox”\n\n\n\n1\ncat\n2.2\n1\n\n\n\n\n2\ncat\n3.2\n2\n\n\n\n\n3\ncat\n4.1\n3\n\n\n\n\n4\ncat\n4.3\n4\n\n\n\n\n5\ncat\n5.4\n5\n\n\n\n\n6\ndog\n5.6\n\n6\n\n\n\n7\ndog\n5.7\n\n7\n\n\n\n8\ncat\n6.1\n8\n\n\n\n\n9\ndog\n7.6\n\n9\n\n\n\n10\nfox\n7.7\n\n\n10\n\n\n11\ncat\n7.9\n11\n\n\n\n\n12\nfox\n8.1\n\n\n12\n\n\n13\ndog\n8.2\n\n13\n\n\n\n14\nfox\n8.6\n\n\n14\n\n\n15\ndog\n8.9\n\n15\n\n\n\n16\ndog\n9.1\n\n16\n\n\n\n17\nfox\n9.1\n\n\n17\n\n\n18\nfox\n9.7\n\n\n18\n\n\n19\nfox\n10.3\n\n\n19\n\n\n20\nfox\n10.6\n\n\n20\n\n\n21\ndog\n11.8\n\n21\n\n\n\n\n\nRangsummen\n\\(R_{cat} = 34\\)\n\\(R_{dog} = 87\\)\n\\(R_{fox} = 110\\)\n\n\n\n\nGruppengröße\n7\n7\n7\n\n\n\n\n\nDie Summe aller Ränge ist \\(1+2+3+...+21 = 231\\). Wir überprüfen nochmal die Summe der Rangsummen als Gegenprobe \\(R_{cat} + R_{dog} + R_{fox} = 231\\). Das ist identisch, wir haben keinen Fehler bei der Rangaufteilung und der Summierung gemacht.\nDie Formel für die H Statistik sieht wie die U Statistik ein wenig wild aus, aber wir können eigentlich relativ einfach alle Zahlen einsetzen. Dann musst du dich etwas konzentrieren bei der Rechnung.\n\\[\nH = \\cfrac{12}{N(N+1)}\\sum_{i=1}^k\\cfrac{R_i^2}{n_i}-3(N+1)\n\\]\nmit\n\n\n\\(R_i\\) der Rangsummen für jede Gruppe mit insgesamt \\(k\\) Gruppen\n\n\\(n_i\\) der Fallzahl in jeder Gruppe\n\n\\(N\\) der Gesamtzahl an Beobachtungen also die gesamte Fallzahl\n\nWir setzen nun die Zahlen ein. Da wir ein balanciertes Design vorliegen haben sind die Fallzahlen \\(n_1 = n_2 = n_3 = 7\\) gleich.\n\\[\nH_{calc} = \\cfrac{12}{21(21+1)}\\left(\\cfrac{34^2}{7}+\\cfrac{87^2}{7}+\\cfrac{110^2}{7}\\right)-3(21+1) = 11.27\n\\]\nDer kritische Wert für die H Statistik ist \\(H_{\\alpha = 5\\%} = 5.99\\). Bei der Entscheidung mit der berechneten Teststatistik \\(H_{calc}\\) gilt, wenn \\(H_{calc} \\geq U_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. Da in unserem Fall das \\(H_{calc}\\) mit \\(11.27\\) größer ist als das \\(H_{\\alpha = 5\\%} = 5.99\\) können wir die Nullhypothese ablehnen. Wir haben ein signifkianten Unterschied in den Medianen zwischen den beiden Tierarten im Bezug auf die Sprungweite in [cm] von Flöhen.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(F_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(H_{calc}\\) gilt, wenn \\(H_{calc} \\geq H_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.\n\n\n\n25.4.1 Kruskal-Wallis-Test in R\nDie Nutzung des Kruskal-Wallis-Test in R ist relativ einfach mit der Funktion kruskal.test(). Wir nutzen die formual Syntax um das Modell zu definieren und können dann schon die Funktion nutzen.\n\nkruskal.test(jump_length ~ animal, data = fac1_tbl) \n\n\n    Kruskal-Wallis rank sum test\n\ndata:  jump_length by animal\nKruskal-Wallis chi-squared = 11.197, df = 2, p-value = 0.003704\n\n\nMit einem p-Wert von \\(0.0037\\) können wir die Nullhypothese ablehnen, da der p-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir haben mindestens einen medianen Unterschied zwischen den Sprungweiten der Hunde-, Katzen- und Fuchsflöhen.\nFür die Betrachtung der Effektgröße in einem Kruskal-Wallis-Test nutzen wir das R Paket rstatix und die darin enthaltende Funktion kruskal_effsize(). Wir berechnene hierbei analog zu einfaktoriellen ANOVA den \\(\\eta^2\\) Wert.\n\nfac1_tbl %>% kruskal_effsize(jump_length ~ animal)\n\n# A tibble: 1 x 5\n  .y.             n effsize method  magnitude\n* <chr>       <int>   <dbl> <chr>   <ord>    \n1 jump_length    21   0.511 eta2[H] large    \n\n\nDas \\(\\eta^2\\) nimmt Werte von 0 bis 1 an und gibt, multipliziert mit 100, den Prozentsatz der Varianz der durch die \\(x\\) Variable erklärt wird. In unserem Beispiel wird 51.1% der Varianz in de Daten durch den Faktor animal erklärt."
  },
  {
    "objectID": "stat-tests-chi-test.html",
    "href": "stat-tests-chi-test.html",
    "title": "26  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "",
    "text": "Version vom Oktober 12, 2022 um 14:14:04\nDer \\(\\mathcal{X}^2\\)-Test wird häufig verwendet, wenn wir zwei Faktoren mit jeweils zwei Leveln miteinander vergleichen wollen. Das heißt wir haben zum Beispiel unseren Faktor animal mit den beiden Leveln cat und dog. Wir schauen uns jetzt den Infektionsstatus mit Flöhen auf den Tieren an. Wir erhalten wiederum einen Faktor infected mit zwei Leveln yes und no. Wir sind bei dem \\(\\mathcal{X}^2\\)-Test nicht auf nur Faktoren mit zwei Leveln eingeschränkt. Traditionell wird aber versucht ein 2x2 Setting zu erreichen.\nIm Bereich der Agrarwissenschaften kommt der \\(\\mathcal{X}^2\\)-Test eher selten vor. Im Bereich der Humanwissenschaften und vor allem der Epidemiologie ist der \\(\\mathcal{X}^2\\)-Test weit verbreitet.\nDas eigentlich besondere an dem \\(\\mathcal{X}^2\\)-Test ist gra nicht mal der Test selber sondenr die Datenstruktur die der \\(\\mathcal{X}^2\\)-Test zugrunde liegt: der Vierfeldertafel oder 2x2 Kreuztabelle. Wir werden diese Form von Tabelle noch später im maschinellen Lernen und in der Testdiagnostik wiederfinden."
  },
  {
    "objectID": "stat-tests-chi-test.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-chi-test.html#genutzte-r-pakete-für-das-kapitel",
    "title": "26  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n26.1 Genutzte R Pakete für das Kapitel",
    "text": "26.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, effectsize)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-chi-test.html#daten-für-den-mathcalx2-test",
    "href": "stat-tests-chi-test.html#daten-für-den-mathcalx2-test",
    "title": "26  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n26.2 Daten für den \\(\\mathcal{X}^2\\)-Test",
    "text": "26.2 Daten für den \\(\\mathcal{X}^2\\)-Test\nWie eben schon benannt schauen wir uns für den \\(\\mathcal{X}^2\\)-Test eine Vierfeldertafel oder aber 2x2 Kreuztabelle an. In Tabelle 26.1 sehen wir eine solche 2x2 Kreuztabelle. Da wir eine Mindestanzahl an Zellbelegung brauchen um überhaupt mit dem \\(\\mathcal{X}^2\\)-Test rechnen zu können, mutzen wir hier gleich aggrigierte Beispieldaten. Wir brauchen mindestens fünf Beobachtungen je Zelle, dass heißt mindestens 20 Tiere. Da wir dann aber immer noch sehr wenig haben, ist die Daumenregel, dass wir etwa 30 bis 40 Beobachtungen brauchen. In unserem Beispiel schauen wir uns 65 Tiere an.\n\n\nTabelle 26.1— Eine 2x2 Tabelle als Beispiel für unterschiedliche Flohinfektionen bei Hunden und Katzen. Dargestellt sind die beobachteten Werte.\n\n\n\n\n\n\n\n\n\n\n\nInfected\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\nAnimal\nDog\n\\(23_{\\;\\Large a}\\)\n\\(10_{\\;\\Large b}\\)\n\\(\\mathbf{a+b = 33}\\)\n\n\n\nCat\n\\(18_{\\;\\Large c}\\)\n\\(14_{\\;\\Large d}\\)\n\\(\\mathbf{c+d = 32}\\)\n\n\n\n\n\\(\\mathbf{a+c = 41}\\)\n\\(\\mathbf{b+d = 24}\\)\n\\(n = 65\\)\n\n\n\n\nIn der Tabelle sehen wir, dass in den zeieln die Level des Faktors animal angegeben sind und in den Spalten die Level des Faktors infected. Wir haben somit \\(23\\) Hunde, die mit Flöhen infiziert sind, dann \\(10\\) Hunde, die nicht mit Flöhen infirziert sind. Auf der Seite der Katzen haben wir \\(18\\) Katzen, die infiziert sind und \\(14\\) Katzen, die keine Flöhe haben. An den Rändern stehen die Randsummen. Wir haben \\(33\\) Hunde und \\(32\\) Katzen sowie \\(41\\) infizierte Tiere und \\(24\\) nicht infizierte Tiere. Somit haben wir dann in Summe \\(n = 65\\) Tiere. Diese Form der Tabelle wird uns immer wieder begegnen.\nBevor wir jetzt diese 2x2 Kreuztabelle verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wie immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in dem \\(\\mathcal{X}^2\\)-Test aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese."
  },
  {
    "objectID": "stat-tests-chi-test.html#hypothesen-für-den-mathcalx2-test",
    "href": "stat-tests-chi-test.html#hypothesen-für-den-mathcalx2-test",
    "title": "26  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n26.3 Hypothesen für den \\(\\mathcal{X}^2\\)-Test",
    "text": "26.3 Hypothesen für den \\(\\mathcal{X}^2\\)-Test\nDer \\(\\mathcal{X}^2\\)-Test betrachtet die Zellbelegung gegeben den Randsummen um einen Unterschied nachzuweisen. Daher haben wir die Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Zahlen in den Zellen gegeben der Randsummen gleich sind. Wir betrachten hier nur die Hypothesen in Prosa und die mathematischen Hypothesen. Es ist vollkommen ausreichend, wenn du die Nullhypothese des \\(\\mathcal{X}^2\\)-Test nur in Prosa kennst.\n\\[\nH_0: \\; \\mbox{Zellbelegung sind gleichverteilt gegeben der Randsummen}\n\\]\nDie Alternative lautet, dass sich die Zahlen in den Zellen gegeben der Randsummen unterscheiden.\n\\[\nH_A: \\; \\mbox{Zellbelegung sind nicht gleichverteilt gegeben der Randsummen}\n\\]\nWir schauen uns jetzt einmal den \\(\\mathcal{X}^2\\)-Test theoretisch an bevor wir uns mit der Anwendung des \\(\\mathcal{X}^2\\)-Test in R beschäftigen."
  },
  {
    "objectID": "stat-tests-chi-test.html#mathcalx2-test-theoretisch",
    "href": "stat-tests-chi-test.html#mathcalx2-test-theoretisch",
    "title": "26  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n26.4 \\(\\mathcal{X}^2\\)-Test theoretisch",
    "text": "26.4 \\(\\mathcal{X}^2\\)-Test theoretisch\nIn Tabelle 26.1 von oben hatten wir die beobachteten Werte. Das sind die Zahlen, die wir in unserem Experiment erhoben und gemessen haben. Der \\(\\mathcal{X}^2\\)-Test vergleicht nun die beobachteten Werte mit den anhand der Randsummen zu erwartenden Werte. Daher ist die Formel für den \\(\\mathcal{X}^2\\)-Test wie folgt.\n\\[\n\\chi^2_{calc} = \\cfrac{(O - E)^2}{E}\n\\]\nmit\n\n\n\\(O\\) für die beobachteten Werte\n\n\\(E\\) für die nach den Randsummen zu erwartenden Werte\n\nIn Tabelle 26.2 kannst du sehen wie wir anhand der Randsummen die erwartenden Zellbelegungen berechnen. Hierbei können auch krumme Zahlen rauskommen. Wir würden keinen Unterschied zwischen Hunde und Katzen gegeben deren Infektionsstatus erwarten, wenn die Abweichungen zwischen den beobachteten Werten und den zu erwartenden Werten klein wären. Wir berechnen nun die zu erwartenden Werte indem wir die Randsummen der entsprechenden Zelle multiplizieren und durch die Gesamtanzahl teilen.\n\n\nTabelle 26.2— Eine 2x2 Tabelle als Beispiel für unterschiedliche Flohinfektionen bei Hunden und Katzen. Dargestellt sind die zu erwartenden Werte.\n\n\n\n\n\n\n\n\n\n\n\nInfected\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\nAnimal\nDog\n\\(\\cfrac{41 \\cdot 33}{65} = 20.82\\)\n\\(\\cfrac{24 \\cdot 33}{65} = 12.18\\)\n\\(\\mathbf{33}\\)\n\n\n\nCat\n\\(\\cfrac{41 \\cdot 32}{65} = 20.18\\)\n\\(\\cfrac{24 \\cdot 32}{65} = 11.82\\)\n\\(\\mathbf{32}\\)\n\n\n\n\n\\(\\mathbf{41}\\)\n\\(\\mathbf{24}\\)\n\\(n = 65\\)\n\n\n\n\nWir können dann die Formel für den \\(\\mathcal{X}^2\\)-Test entsprechend ausfüllen. Dabei ist wichtig, dass die Abstände quadriert werden. Das ist ein Kernkonzept der Statistik, Abstände bzw. Abweichungen werden immer quadriert.\n\\[\\begin{aligned}\n\\chi^2_{calc} &= \\cfrac{(23 - 20.82)^2}{20.82} + \\cfrac{(10 - 12.18)^2}{12.18} + \\\\\n&\\phantom{=}\\;\\; \\cfrac{(18 - 20.18)^2}{20.18} + \\cfrac{(14 - 11.82)^2}{11.82} = 1.25\n\\end{aligned}\\]\nEs ergibt sich ein \\(\\chi^2_{calc}\\) von \\(1.25\\) mit der Regel, dass wenn \\(\\chi^2_{calc} \\geq \\chi^2_{\\alpha=5\\%}\\) die Nullhypothese abgelehnt werden kann. Mit einem \\(\\chi^2_{\\alpha=5\\%} = 3.84\\) können wir die Nullhypothese nicht ablehnen. Es besteht kein Zusammenhang zwischen den Befall mit Flöhen und der Tierart. Oder anders herum, Hunde und Katzen werden gleich stark mit Flöhen infiziert."
  },
  {
    "objectID": "stat-tests-chi-test.html#mathcalx2-test-in-r",
    "href": "stat-tests-chi-test.html#mathcalx2-test-in-r",
    "title": "26  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n26.5 \\(\\mathcal{X}^2\\)-Test in R",
    "text": "26.5 \\(\\mathcal{X}^2\\)-Test in R\nDer \\(\\mathcal{X}^2\\)-Test wird meist in anderen Funktionen in R verwendet und nicht direkt. Wenn du Fragen dazu hast, schreib mir einfach eine Mail.\nWenn wir den \\(\\mathcal{X}^2\\)-Test in R rechnen wollen nutzen wir die Funktion chisq.test(), die eine Matrix von Zahlen verlangt. Dies ist etwas umständlich.\n\nmat <- matrix(c(23, 10, 18, 14), \n              byrow = TRUE, nrow = 2)\nchisq.test(mat, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  mat\nX-squared = 1.2613, df = 1, p-value = 0.2614\n\n\nDa der \\(\\mathcal{X}^2\\)-Test nicht so häufig im Bereich der Agrawissenschaften auftritt belasse ich es für das Erste hierbei. Wenn du noch Fragen hast, komme gerne in R Tutorium oder schreibe mir eine Mail. Dann können wir das Problem nochmal diskutieren und sehen, ob wirklich ein \\(\\mathcal{X}^2\\)-Test notwendig ist.\nAls ein mögliches Effektmaß können wir Cramers \\(V\\) berechnen. Wir nutzen hierzu die Funktion cramers_v(). Auf einer reinen 2x2 Kreuztabelle wäre aber Pearsons \\(\\phi\\) durch die Funktion phi() vorzuziehen. Siehe dazu auch \\(\\phi\\) and Other Contingency Tables Correlations auf der Hilfeseite des R Paketes effectsize.\n\ncramers_v(mat) \n\nCramer's V (adj.) |       95% CI\n--------------------------------\n0.06              | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\nphi(mat)\n\nPhi (adj.) |       95% CI\n-------------------------\n0.06       | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass der Effekt mit einem \\(V = 0.14\\) oder einem \\(\\phi = 0.14\\) schwach ist. Ein Wert von 0 bedeutet keine Assoziation und ein Wert von 1 einen maximalen Zusamenhang. Wir können die Werte von \\(V\\) und \\(\\phi\\) wie eine Korrelation interpretieren."
  },
  {
    "objectID": "stat-tests-diagnostic.html",
    "href": "stat-tests-diagnostic.html",
    "title": "27  Der diagnostische Test",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:38:45\nIn diesem Kapitel wollen wir uns mit dem diagnostischen Test beschäftigen. Eigentlich würde man meinen, dass Diagnostik nun eher in die Medizin gehört. Hauptsächlich wird das diagnostische Testen auch in der Labordiagnostik oder bei der Herstellung eines neuen medizinischen Testes verwendet. Deshalb gibt es hier auch erstmal (Stand Ende 2022) nur einen Ausschnitt aus dem diagnostischen Testen. Wir brauchen aber die Fachbegriffe, wenn wir uns später einmal mit dem maschinellen Lernen oder dem Klasifizieren beschäftigen wollen. Dann brauchen wir die hier verwendeten Fachbegriffe, wie Spezifität, Sensifität, AUC und auch die ROC Abildung."
  },
  {
    "objectID": "stat-tests-diagnostic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-diagnostic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "27  Der diagnostische Test",
    "section": "\n27.1 Genutzte R Pakete für das Kapitel",
    "text": "27.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, \n               pROC, readxl)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-diagnostic.html#die-daten-für-das-diagnostische-testen",
    "href": "stat-tests-diagnostic.html#die-daten-für-das-diagnostische-testen",
    "title": "27  Der diagnostische Test",
    "section": "\n27.2 Die Daten für das diagnostische Testen",
    "text": "27.2 Die Daten für das diagnostische Testen\nEs ist unabdingbar, dass oben links in der 2x2 Kreuztabelle immer die \\(T^+\\) und \\(K^+\\) Werte stehen. Sonst funktionieren alle Formeln in diesem Kapitel nicht.\nDie Datentabelle für das diagnostische Testen basiert wie der \\(\\chi^2\\)-Test auf der 2x2 Kreuztabelle oder Verfeldertafel. Es ist dabei unabdingbar, dass oben links in der 2x2 Kreuztabelle immer die \\(T^+\\) und \\(K^+\\) Werte stehen. Sonst funktionieren alle Formeln in diesem Kapitel nicht. Wir schauen usn auch immer das Schlechte an. Daher wollen wir immer wissen, ist der Hund krank? Ist der Hund tot? Ist der Hund weggelaufen? Beide Voraussetzung sind wichtig, damit wir mit der 2x2 Kreuztabelle wie in Tabelle 27.1 gezeigt rechnen können.\n\n\nTabelle 27.1— Eine 2x2 Tabelle oder Vierfeldertafel\n\n\n\n\n\n\n\n\n\n\n\nKrank\n\n\n\n\n\n\n\n\\(K^+\\) (1)\n\n\\(K^-\\) (0)\n\n\n\nTest\n\n\\(T^+\\) (1)\n\\(TP_{\\;\\Large a}\\)\n\\(FN_{\\;\\Large b}\\)\n\\(\\mathbf{a+b}\\)\n\n\n\n\n\\(T^-\\) (0)\n\\(FP_{\\;\\Large c}\\)\n\\(TN_{\\;\\Large d}\\)\n\\(\\mathbf{c+d}\\)\n\n\n\n\n\\(\\mathbf{a+c}\\)\n\\(\\mathbf{b+d}\\)\n\\(\\mathbf{n}\\)\n\n\n\n\nWir wollen die Tabelle 27.1 mit einem Beispiel von Tollwut an Hauskatzen in ländlicher Umgebung. Die Katzen haben also Auslauf und können sich auch mit Tollwut infizieren. Wir wollen einen neuen, nicht invasiven Labortesten Tollda darauf überprüfen, wie gut der diagnostische Test Tollwut bei Katzen im Frühstadium erkennt.\nWir haben jetzt folgende Informationen erhalten:\n\nDer diagnostische Test TollDa ist positiv \\(T^+\\), wenn Tollwut vorliegt \\(K^+\\) , in 80% der Fälle.\nDer diagnostische Test TollDa ist positiv \\(T^+\\), wenn keine Tollwut vorliegt \\(K^-\\), in 9.5% der Fälle.\nAbschließend haben noch 2% der Katzen in ländlicher Umgebung Tollwut. Wir haben eine Prävalenz der Erkrankung in der betrachteten Population von 2%.\n\nDie Halterin einer Katze möchte nun wissen, wie groß ist dei Wahrscheinlichkeit bei einem positiven Testergebnis, dass meine Katze Tollwut hat. Also die bedingte Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests.\n\n\n\nAbbildung 27.1— Visualisierung der Informationen zur Tollwutdiagnostik in einem Doppelbaum. Gefragt ist nach der Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests.\n\n\n\nAbbildung 27.1 visualisert unsere Frage in einem Doppelbaum. Wir haben 10000 Katzen vorliegen. Ja wir waren fleißig und wir können damit besser rechnen. Du siehst aber auch, für die Diagnostik brauchen wir eine Menge Beobachtungen.\nVon den 10000 Katzen sind 2% also 200 wirklich mit Tollwut infiziert, also haben den Status \\(K^+\\). Damit sind 9800 Katzen gesund oder nicht krank und haben den Status \\(K^-\\). Wir wissen jetzt, dass 80% der \\(K^+\\) Katzen als postitiv vom Test erkannt werden. Damit werden \\(200 \\cdot 0.8 = 160\\) Katzen \\(T^+\\). Im Umkehrschluss sind die anderen 40 Katzen dann \\(T^-\\). Von den 9800 gesunden Katzen werden 9.5% fälsch als krank erkannt, also \\(9800 \\cdot 0.095 = 931\\) Katzen. Wiederum im Umkehrschluss sind dann 8869 richtig als gesunde Tiere erkannt.\nWir können nun den Doppelbaum nach unten ergänzen. Wir haben damit \\(1091 = 160 + 931\\) positiv getestete Katzen \\(T^+\\) sowie \\(819 = 40 + 8869\\) negativ getestete Katzen \\(T^-\\).\nWie groß ist nun die Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests? Wir können die Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) direkt im Baum ablesen. Wir haben 160 kranke und positive Tier. Insgesamt sind 1091 Tiere positiv getestet. Daher Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests \\(\\tfrac{160}{1091} = 0.147\\) oder nur 14%.\nSammeln wir unsere Informationen um die Tabelle 27.1 zu füllen. Wir haben insgesamt \\(n = 10000\\) Katzen untersucht. In Tabelle 27.2 sehen wir die Ergebnisse unseres Tests auf Tollwut zusammengefasst. Wir haben \\(160\\) Katzen die Tollwut haben und vom Test als krank erkannt wurden. Dann haben wir \\(931\\) Katzen, die keine Tollwut hatten und vom Test als positiv erkannt wurden. Darüber hinaus haben wir \\(40\\) Katzen, die Tollwut haben, aber ein negatives Testergebnis. Sowie \\(8869\\) gesunde Katzen mit einem negativen Testergebnis.\n\n\nTabelle 27.2— Eine 2x2 Tabelle oder Vierfeldertafel gefüllt mit dem Beispiel aus dem Doppelbaum.\n\n\n\n\n\n\n\n\n\n\n\nKrank\n\n\n\n\n\n\n\n\\(K^+\\) (1)\n\n\\(K^-\\) (0)\n\n\n\nTest\n\n\\(T^+\\) (1)\n160\n931\n\\(\\mathbf{a+b} = 1091\\)\n\n\n\n\n\\(T^-\\) (0)\n40\n8869\n\\(\\mathbf{c+d} = 8109\\)\n\n\n\n\n\\(\\mathbf{a+c} = 200\\)\n\\(\\mathbf{b+d} = 9800\\)\n\\(\\mathbf{n} = 10000\\)\n\n\n\n\nWie du sehen kannst ist die mittlere Reihe des Doppelbaums nichts anderes als die ausgefüllte 2x2 Kreuztabelle. In Abbildung 27.2 sehen wir die letzte Visualisierung des Zusammenhangs von Testscore auf der x-Achse und der Verteilung der tollwütigen Katzen \\(K^+\\) und der gesunden Katzen \\(K^-\\). Links und rechts von der Testenstscheidung werden Katzen falsch klassifiziert. Das heißt, die Katzen werden als krank oder gesund von dem Test erkannt, obwohl die Katzen diesen Status nicht haben. Ein idelr Test würde zwei Verteilungen der kranken und gesunden Tiere hrvorbringen, die sich perfekt separieren lassen. Es gibt anhand des Testscores keine Überlappung der beiden Verteilungen\n\n\n\nAbbildung 27.2— Visualisierung des Zusammenhangs zwischen Testscore und den Verteilungen der kranken und gesunden Katzen sowie der Testenstscheidung. Links und rechts von der Testenstscheidung werden Katzen falsch klassifiziert."
  },
  {
    "objectID": "stat-tests-diagnostic.html#confusion-matrix-deu.-fehlermatrix",
    "href": "stat-tests-diagnostic.html#confusion-matrix-deu.-fehlermatrix",
    "title": "27  Der diagnostische Test",
    "section": "\n27.3 Confusion matrix (deu. Fehlermatrix)",
    "text": "27.3 Confusion matrix (deu. Fehlermatrix)\n\n\nDie 2x2 Kreuztabelle wird als Confusion matrix viel auch in der Klassifikation im machinellen Lernen genutzt. Im maschinellen Lernen heißt es dann meist nur anders…\nIn der Statistik beziehungsweise der Epidemiologie gibt es eine Vielzahl an statistischen Maßzahlen für die 2x2 Kreuztabelle. In der Fachsprache wird die 2x2 Kreuztabelle auch Confusion matrix genannt. Auf der Confusion Matrix können viele Maßzahlen berechnet werden wir konzentrieren uns hier erstmal auf die zwei wichtigsten Maßzahlen, nämlich der Spezifität und der Sensitivität.\nIn der wissenschaftlichen Fachsprache hat ein diagnostischer Test oder eine Methode, die erkrankte Personen sehr zuverlässig als krank (\\(K^+\\)) erkennt, eine hohe Sensitivität. Das heißt, sie übersieht kaum erkrankte Personen. Ein Test, der gesunde Personen zuverlässig als gesund (\\(K^-\\)) einstuft, hat eine hohe Spezifität. Wir können die Spezifität und die Sensitivität auf der 2x2 Kreuztabelle wir folgt berechnen.\n\\[\n\\mbox{Sensitivität} = \\mbox{sens} = \\cfrac{TP}{TP + FN} = \\cfrac{160}{160 + 931} = 0.147\n\\]\n\\[\n\\mbox{Spezifität} = \\mbox{spec} = \\cfrac{TN}{TN + FP} = \\cfrac{8869}{8869 + 40} = 0.995\n\\]\nBeide statistischen Maßzahlen benötigen wir im Besonderen bei der Erstellung der Reciever Operator Curve (ROC) im folgenden Abschnitt."
  },
  {
    "objectID": "stat-tests-diagnostic.html#receiver-operating-characteristic-roc",
    "href": "stat-tests-diagnostic.html#receiver-operating-characteristic-roc",
    "title": "27  Der diagnostische Test",
    "section": "\n27.4 Receiver Operating Characteristic (ROC)",
    "text": "27.4 Receiver Operating Characteristic (ROC)\nDie Reciever Operator Curve (ROC) ist die Abbildung wenn es darum geht darzustellen wie gut eine Klassifikation funktioniert hat. Wir werden die Abbildung später im maschinellen Lernen und der Klssifikation wiedertreffen. Da die ROC Kurve aber ursprünglich zum diagnostischen Testen gehört, kommt die ROC Kurve hier auch rein.\n\n27.4.1 Daten für die Receiver Operating Characteristic (ROC)\nSchauen wir uns erstmal ein Beispiel für die ROC Kurve an. In Tabelle 27.3 sehen wir Beispieldaten von vierzehn Hunden. Von den vierzehn Hunden haben sieben eine verdeckte Flohinfektion im Anfangsstadium und sieben Hunde sind flohfrei und gesund. Daher haben wir sieben kranke Hunde (\\(K^+\\) oder Infektionsstatus ist \\(1\\)) und sieben gesunde Hunde (\\(K^-\\) oder Infektionsstatus ist \\(0\\)). Wir testen jeden der Hunde mit dem Flohindikatormittel FleaDa. Das Flohindikatormittel gibt einen Farbwert als test_score zwischen \\(0\\) und \\(1\\) wieder. Wir wollen nun herausfinden, ob wir mit dem test_score die vierzehn Hunde korrekt nach ihrem Krankheitszustand klassifizieren können.\n\n\n\n\nTabelle 27.3— Datensatz für vierzehn Hunde mit Flohinfektion und keiner Flohinfektion sowie dem Testscore des Flohindikatormittels fleaDa.\n\ninfected\ntest_score\n\n\n\n1\n0.100\n\n\n1\n0.150\n\n\n1\n0.250\n\n\n1\n0.300\n\n\n1\n0.400\n\n\n1\n0.500\n\n\n1\n0.600\n\n\n0\n0.350\n\n\n0\n0.425\n\n\n0\n0.470\n\n\n0\n0.550\n\n\n0\n0.700\n\n\n0\n0.800\n\n\n0\n0.820\n\n\n\n\n\n\nWir sehen die Daten visualisiert in Abbildung 27.3. Auf der \\(y\\)-Achse ist der binäre Endpunkt \\(infected\\) und auf der \\(x\\)-Achse der Testscore des Flohindikators FleaDa.\n\n\n\n\nAbbildung 27.3— Aufteilung der kranken und gesunden Hunde nach Infektionsstatus und dem Testscore.\n\n\n\n\n\n\n\nDie Idee der ROC Kurve ist nun für jeden möglichen Wert des Testscores die Spezifität und Sensitivität zu berechnen. Wir müssen das aber nicht für alle Werte des Testscores machen, sondern nur für die Wert bei denen sich der Status einer Beobachtung anhand des Testscores ändern würde. Klingt ein wenig schräg, schauen wir es uns einmal an. Zuerst brauchen wir jeweils die Grenzen an denen wir für den Testscore eine Klassifikation machen. In Abbildung 27.4 sehen wir die Grenzen als gelbe Linie.\n\n\n\n\nAbbildung 27.4— Aufteilung der kranken und gesunden Hunde nach Infektionsstatus und dem Testscore.\n\n\n\n\nWir können jetzt für jede gelbe Linie als Threshold des Testscore die Spezifität und die Sensitivität berechen. berechnen Tabelle 27.4 sind die Werte von Spezifität und Sensitivität für jede gelbe Linie ausgegeben. Wenn wir als Entscheidungsgrenze einen Testscore von 0.12 nehmen würden, dann würden wir 1 Hund als krank und 13 als gesund klassifizieren. Wir hätten 7 TN, 1 TP, 6 FN und 0 FP. Aus diesen Zahlen, die eine 2x2 Kreuztabelle entsprechen, können wir dann die Spezifität und die Sensitivität berechnen. Wir berechnen dann die Spezifität und die Sensitivität für jeden Threshold.\n\n\n\n\nTabelle 27.4— Die Spezifität und die Sensitivität berechnet für jeden Threshold.\n\n\nthreshold\nspecificity\nsensitivity\ntn\ntp\nfn\nfp\n\n\n\n2\n0.12\n1.00\n0.14\n7\n1\n6\n0\n\n\n3\n0.20\n1.00\n0.29\n7\n2\n5\n0\n\n\n4\n0.28\n1.00\n0.43\n7\n3\n4\n0\n\n\n5\n0.32\n1.00\n0.57\n7\n4\n3\n0\n\n\n6\n0.38\n0.86\n0.57\n6\n4\n3\n1\n\n\n7\n0.41\n0.86\n0.71\n6\n5\n2\n1\n\n\n8\n0.45\n0.71\n0.71\n5\n5\n2\n2\n\n\n9\n0.48\n0.57\n0.71\n4\n5\n2\n3\n\n\n10\n0.52\n0.57\n0.86\n4\n6\n1\n3\n\n\n11\n0.58\n0.43\n0.86\n3\n6\n1\n4\n\n\n12\n0.65\n0.43\n1.00\n3\n7\n0\n4\n\n\n13\n0.75\n0.29\n1.00\n2\n7\n0\n5\n\n\n14\n0.81\n0.14\n1.00\n1\n7\n0\n6\n\n\n\n\n\n\nWir können jetzt für jeden Threshold und damit jedes verbundene Spezifität und Sensitivität Paar einen Punkt in einen Plot einzeichnen. Wir lassen damit Stück für Stück die ROC Kurve wachsen.\nIn der Abbildung 27.5 sehen wir die ROC Kurve mit dem Threshold von 0.32 vorliegen. Wir haben damit 7 TN, 4 TP, 3 FN und 0 FP klassifizierte Beobchtungen. Darauf ergibt sich eine Spezifität von 1 und eine Sensitivität von 0.57. In der ROC Kurve tragen wir auf die \\(x\\)-Achse die 1 - Spezifitätswerte auf. Damit haben wir eine schön ansteigende Kurve.\n\n\n\nAbbildung 27.5— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.32 und damit 7 TN, 4 TP, 3 FN und 0 FP.\n\n\n\nIn der Abbildung 27.6 istr die ROC Kurve schon nach rechts gewachsen, da wir die ersten falsch positiv (FP) klassifizierten Beobachtungen bei einem Threshold von 0.48 erhalten.\n\n\n\nAbbildung 27.6— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.48 und damit 4 TN, 5 TP, 2 FN und 3 FP.\n\n\n\nIn Abbildung 27.7 sind wir mit einem Threshold von 0.65 schon fast am rechten Rand der Verteilung des Testscores angekommen. Wir haben nur noch 3 richtig negative (TN) Beobachtungen, die jetzt mit steigenden Threshold alle zu falsch positiven (FP) klassifiziert werden.\n\n\n\nAbbildung 27.7— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.65 und damit 3 TN, 7 TP, 0 FN und 4 FP.\n\n\n\nEin AUC von 1 bedeutet eine perfekte Trennung der beiden Gruppen durch den Testscore. Ein AUC von 0.5 bedeutet keine Trennung der beiden Gruppen durch den Testscore.\nDie ROC Kurve endet dann oben rechts in der Abbidlung und startet immer unten links. Wir können noch die Fläche unter der ROC Kurve berechnen. Wir nennen diese Fläche unter der Kurve AUC (eng. area under the curce) und wir brauchen diese Maßzahl, wenn wir verschiedene Testscores und die entsprechenden ROC Kurven miteinander vergleichen wollen. Eine AUC von 0.5 bedeutet, dass unser Testscore nicht in der Lage war die kranken von den gesunden Hunden zu trennen."
  },
  {
    "objectID": "stat-tests-diagnostic.html#reciever-operator-curve-roc-in-r",
    "href": "stat-tests-diagnostic.html#reciever-operator-curve-roc-in-r",
    "title": "27  Der diagnostische Test",
    "section": "\n27.5 Reciever Operator Curve (ROC) in R",
    "text": "27.5 Reciever Operator Curve (ROC) in R\nWir berechnen die ROC Kurve nicht per Hand. Das wäre eine sehr große Fleißarbeit für jeden Threshold zwischen den Beobachtungen die jeweilige Spezifität und Sensitivität Paare zu berechnen. Wir nutzen daher das R Paket pROC. Das Pakt erlaubt es uns später auch recht einfach ROC Kurven miteinander zu vergleichen und einen statistischen Test zu rechnen.\nDie Daten von den obigen Beispiel sind in der Datei roc_data.xlsx gespeichert und können dann einfach verwendet werden. Die Funktion roc() akzeptiert ein binäres \\(y\\) wo bei die \\(1\\) das Schlechte bedeutet und die \\(0\\) die Abwesenheit von dem Schlechten. Also eben krank oder gesund wie oben beschrieben. Wir wollen dann noch die AUC und die 95% Konfidenzintervalle für das AUC wiedergegeben haben.\n\nroc_tbl <- read_excel(\"data/roc_data.xlsx\") \n\nroc_obj <- roc(infected ~ test_score, roc_tbl, auc = TRUE, ci = TRUE)\n\nroc_obj\n\n\nCall:\nroc.formula(formula = infected ~ test_score, data = roc_tbl,     auc = TRUE, ci = TRUE)\n\nData: test_score in 7 controls (infected 0) > 7 cases (infected 1).\nArea under the curve: 0.83673\n95% CI: 0.61765-1 (DeLong)\n\n\nWir sehen, dass wir eine AUC von 0.837 haben, was auf eine gute Trennschärfe des Tests hindeutet. Wäre die AUC näher an 0.5 dann würden wir sagen, dass der Test die kranken Hunde nicht von den gesunden Hunden unterscheiden kann.\nIm nächsten Schritt extrahieren wir noch alle wichtigen Informationen aus dem Objekt roc_obj damit wir die ROC Kurve in ggplot zeichnen können. Das Paket pROC hat auch eine eigne Plotfunktion. Es gibt eine Reihe on Screenshots vom pROC Paket, wo du noch andere Möglichkeiten siehst. Einfach mal ausprobieren.\n\nroc_res_tbl <- roc_obj %>% \n  coords(ret = \"all\") %>% \n  select(specificity, sensitivity)\n\nDas Objekt roc_obj nutzen wir jetzt um die ROC Kurve einmal darzustellen. Achte drauf, dass auf der \\(x\\)-Achse die 1-Spezifität Werte stehen. Wir erhalten die ROC Kurve wie in Abbildung 27.8 dargestellt. Wie zeichnen noch die Diagonale ein. Wenn die ROC nahe an der Diagonalen verläuft, dann ist der Testscore nicht in der Lage die Kranken von den Gesunden zu trennen. Eine perfekte ROC Kurve läuft senkrecht nach oben und dann waagerecht nach rechts und es ergebit sich eine AUC von 1.\n\nggplot(roc_res_tbl, aes(1-specificity, sensitivity)) +\n  geom_path() +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.75, color = \"red\") +\n  theme_bw() +\n  labs(x = \"1 - Spezifität\", y = \"Sensitivität\")\n\n\n\nAbbildung 27.8— Abbildung der ROC Kurve.\n\n\n\n\nIn späteren Kapiteln zur Klassifikation werden wir auch verschiedene ROC Kurven zusammen darstellen und miteinander Vergleichen. Dazu nutzen wir dann auch die ROC Kurve und die Möglichkeit auch einen roc.test() zu rechnen. Mehr dazu gibt es soweit erstmal auf der Hilfeseite des R Paketes pROC mit vielen Screenshots vom pROC Paket. Für dieses Kapitel soll die Darstellung einer ROC Kurve genügen."
  },
  {
    "objectID": "stat-tests-pretest.html",
    "href": "stat-tests-pretest.html",
    "title": "28  Pre-Tests oder Vortest",
    "section": "",
    "text": "Version vom October 13, 2022 um 17:14:31\nDer Pre-Test, ein Kapiel was ich nicht mag. Also eher weniger das Kapitel als den Pre-Test. Auf der einen Seite sind die Pre-Tests hoffnungslos veraltet. Pre-Tests stammen aus einer Zeit in der man sich nicht einfach die Daten angucken konnte. Mit angucken meine ich dann in ggplot visualisieren. Die Idee hinter Pre-Test ist eigentlich die Angst selber die Entscheidung zu treffen, ob die Daten varianzhomogen oder normalverteilt sind. Eine bessere Lösung ist immer noch das Outcome \\(y\\) zu transformieren (siehe Kapitel 16) und dann das untransformierte Modell mit transformierten Modell zu vergleichen (siehe Kapitel 33.6). Auf der anderen Seite ist der Pre-Test eine Art übermächtiger Entscheider. Dabei sind die Formel sehr trivial und das Konzept eher simpel.\nNeben dieser Angst eine Entscheidung zu treffen, hilft einem der Pre-Test zur Varianzhomogenität und der Pre-Test zur Normalverteilung bei kleiner Fallzahl auch nicht wirklich weiter, wie wir gleich sehen werden. Beide Pre-Tests funktionieren erst bei wirklich hohen Fallzahlen gut. Mit hohen Fallzahlen meine ich, Fallzahlen von über 20 Beobachtungen je Gruppe bzw. Level des Faktors. Bei kleiner Fallzahl, also der üblichen Anzahl von weniger als zehn Wiederholungen, können wir auch nur die Boxplots oder Dotplots anschauen. Darüber hinaus können wir uns auch schnell ins Abseits testen, so dass wir gar keinen Test mehr übrig haben um unsere Daten auszuwerten.\nEs ist grundsätzlich besser verschiedene Modelle zu fitten und dann sich in Kapitel 31 die Güte oder Qualität der Modelle anzuschauen. Jedenfalls ist das meiner Meinung nach die bessere Lösung. Da aber immer wieder nach den Pre-Tests gefragt wird, habe ich auch dieses Kapitel erschaffen."
  },
  {
    "objectID": "stat-tests-pretest.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-pretest.html#genutzte-r-pakete-für-das-kapitel",
    "title": "28  Pre-Tests oder Vortest",
    "section": "\n28.1 Genutzte R Pakete für das Kapitel",
    "text": "28.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted,\n               broom, car, performance, see)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")"
  },
  {
    "objectID": "stat-tests-pretest.html#pre-test-auf-varianzhomogenität",
    "href": "stat-tests-pretest.html#pre-test-auf-varianzhomogenität",
    "title": "28  Pre-Tests oder Vortest",
    "section": "\n28.2 Pre-Test auf Varianzhomogenität",
    "text": "28.2 Pre-Test auf Varianzhomogenität\nWas will also der Pre-Test auf Varianzhomogenität? Eigentlich ist der Test vollkommen verquer. Zum einen testet der Test auf Varianzhomogenität gar nicht die Anwesenheit von Homogenität. Wir können dank dem Falisifikationsprinzip nur Ablehnen. Deshalb steht in der Nullhypothese die Gleichheit der Varianzen, also Varianzhomogenität und in der Alternativen dann die Varianzheterogenität, als der Unterschied.\nAb wann sollten wir denn die Varianzhomogenität ablehnen? Wenn wir standardmäßig auf 5% testen, dann werden wir zu selten die Varianzhomogenität ablehnen. Daher ist es ratsam in diesem Fall auf ein Signifikanzniveau von \\(\\alpha\\) gleich 20% zu testen. Aber auch in diesem Fall können wir natürlich eine Varianzhomogenität übersehen oder aber eine Varianzhterogenität fälschlicherweise annehmen.\nEs ergeben sich folgende Hypothesen für den Pre-Test auf Varianzhomogenität.\n\\[\n\\begin{aligned}\nH_0: &\\; s^2_A = s^2_B\\\\\nH_A: &\\; s^2_A \\ne s^2_B\\\\\n\\end{aligned}\n\\]\nWir sehen, dass in der Nullhypothese die Gleichheit der Varianzen steht und in der Alternativehypothese der Unterschied, also die Varianzhterogenität.\n\n\n\n\n\n\nEntscheidung zur Varianzhomogenität\n\n\n\nBei der Entscheidung zur Varianzhomogenität gilt folgende Regel. Ist der \\(p\\)-Wert des Pre-Tests auf Varianzhomogenität kleiner als das Signifikanzniveau \\(\\alpha\\) von 20% lehnen wir die Nullhypothese ab. Wir nehmen Varianzheterogenität an.\n\nIst \\(p \\leq \\alpha = 20\\%\\) so nehmen wir Varianzheterogenität an.\nIst \\(p > \\alpha = 20\\%\\) so nehmen wir Varianzhomogenität an.\n\nAuf jeden Fall sollten wir das Ergebnis unseres Pre-Tests auf Varianzhomogenität nochmal visuell bestätigen.\n\n\nWir nutzen zum statistischen Testen den Levene-Test über die Funktion leveneTest() oder den Bartlett-Test über die Funktion bartlett.test(). Beide Tests sind in R implementiert und können über das Paket car genutzte werden. Wir werden uns jetzt nicht die Formel anschauen, wir nutzen wenn die beiden Tests nur in R und rechnen nicht selber die Werte nach.\nEinfach ausgedrückt, überprüft der Bartlett-Test die Homogenität der Varianzen auf der Grundlage des Mittelwerts. Der Leven Test überprüft die Homogenität der Varianzen auf der Grundlage des Medians ist also robuster gegenüber Ausreißern.\n\n\n\n\n\n\nEinigermaßen zuverlässig meint, dass wir dann in 1 von 20 Fällen eine Varianzhomogenität ablehnen, obwohl eine Varianzhomogenität vorliegt. Ebenso können wir in 1 von 5 Fällen die Nullhypothese nicht ablehnen, obwohl die Varianzen heterogen sind (siehe auch Kapitel 19.1).\nWir wollen uns nun zwei Fälle einmal näher anschauen. Zum einen den Fall, dass wir eine niedrige Fallzahl vorliegen haben und Varianzhomogenität sowie den Fall, dass wir eine niedriege Fallzahl und Varianzheterogenität vorliegen haben. Den Fall, dass wir hohe Fallzahl vorliegen haben, betrachten wir jetzt nicht weiter. In dem Fall funktionieren die Tests einigigermaßen zuverlässig."
  },
  {
    "objectID": "stat-tests-pretest.html#varianzen-sind-homogen-fallzahl-niedrig",
    "href": "stat-tests-pretest.html#varianzen-sind-homogen-fallzahl-niedrig",
    "title": "28  Pre-Tests oder Vortest",
    "section": "\n28.3 Varianzen sind homogen, Fallzahl niedrig",
    "text": "28.3 Varianzen sind homogen, Fallzahl niedrig\nWir bauen uns nun einen Datensatz mit zwei Gruppen \\(A\\) und \\(B\\) zu je zehn Beobachtungen. Beide Gruppen kommen aus einer Normalverteilung mit einem Mittelwert von \\(\\bar{y}_A = \\bar{y}_A = 10\\). Darüber hinaus haben wir Varianzhomogenität mit \\(s_A = s_B = 5\\) vorliegen. Ja, wir spezifizieren hier in der Funktion rnorm() die Standardabweichung, aber eine homogene Standardabweichung bedingt eine homogene Varianz und umgekehrt. Abschließend verwandeln wir das Wide-Format noch in das Long-Format um.\n\nset.seed(202209013)\nsmall_homogen_tbl <- tibble(A = rnorm(n = 10, mean = 10, sd = 5),\n                            B = rnorm(n = 10, mean = 10, sd = 5)) %>% \n  gather(trt, rsp) %>% \n  mutate(trt = as_factor(trt))\n\nIn der Abbildung 28.1 sehen wir die Daten aus dem small_homogen_tbl einmal als Boxplot visualisiert.\n\n\n\n\nAbbildung 28.1— Boxplot der beiden Treatment Level A und B. Beide Gruppen haben die gleichen Varianzen. Es liegt Varianzhomogenität vor.\n\n\n\n\nWir wollen nun die Varianz auf Homogenität testen. Wir nutzen dafür den levenTest() sowie den bartlett.test(). Beide Tests bieten sich an. Die Daumenregel ist, dass der Bartlett-Test etwas bessere statistische Eigenschaften hat. Dennoch ist der Levene-Test bekannter und wird häufiger angefragt und genutzt. Wir nutzen die Funktion tidy() aus dem Paket broom um die Ausgabe aufzuräumen und selektieren nur den \\(p\\)-Wert.\n\nleveneTest(rsp ~ trt, data = small_homogen_tbl) %>% tidy %>% select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    <dbl>\n1   0.345\n\nbartlett.test(rsp ~ trt, data = small_homogen_tbl) %>% tidy %>% select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    <dbl>\n1   0.325\n\n\nWir sehen, dass der \\(p\\)-Wert größer ist als das Signifikanzniveau \\(\\alpha\\) von 20%. Damit können wir die Nullhypothese nicht ablehnen. Wir nehmen Varianzhomogenität an. Überdies sehen wir auch, dass sich die \\(p\\)-Werte nicht groß voneinander unterscheiden.\nWir können auch die Funktion check_homogeneity() aus dem Paket performance nutzen. Wir erhalten hier auch gleich eine Entscheidung in englischer Sprache ausgegeben. Die Funktion check_homogeneity() nutzt den Bartlett-Test. Wir können in Funktion auch andere Methoden mit method = c(\"bartlett\", \"fligner\", \"levene\", \"auto\") wählen.\n\nlm(rsp ~ trt, data = small_homogen_tbl) %>% check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.325).\n\n\nWir nutzen das Paket performance für die Modellgüte im Kapitel 31."
  },
  {
    "objectID": "stat-tests-pretest.html#varianzen-sind-heterogen-fallzahl-niedrig",
    "href": "stat-tests-pretest.html#varianzen-sind-heterogen-fallzahl-niedrig",
    "title": "28  Pre-Tests oder Vortest",
    "section": "\n28.4 Varianzen sind heterogen, Fallzahl niedrig",
    "text": "28.4 Varianzen sind heterogen, Fallzahl niedrig\nNun stellt sich die Frage, wie sieht es aus, wenn wir ungleiche Varianzen vorliegen haben. Wir bauen uns nun einen Datensatz mit zwei Gruppen \\(A\\) und \\(B\\) zu je zehn Beobachtungen. Beide Gruppen kommen aus einer Normalverteilung mit einem Mittelwert von \\(\\bar{y}_A = \\bar{y}_A = 12\\). Darüber hinaus haben wir Varianzheterogenität mit \\(s_A = 10 \\ne s_B = 5\\) vorliegen.\n\nset.seed(202209013)\nsmall_heterogen_tbl <- tibble(A = rnorm(10, 10, 12),\n                              B = rnorm(10, 10, 5)) %>% \n  gather(trt, rsp) %>% \n  mutate(trt = as_factor(trt))\n\nIn der Abbildung 28.2 sehen wir die Daten aus dem small_heterogen_tbl einmal als Boxplot visualisiert.\n\n\n\n\nAbbildung 28.2— Boxplot der beiden Treatment Level A und B. Beide Gruppen haben ungleiche Varianzen. Es liegt Varianzheterogenität vor.\n\n\n\n\nWir wollen nun die Varianz auf Homogenität testen. Wir nutzen dafür den levenTest() sowie den bartlett.test(). Wir können nur die Varianzhomogenität testen, da jeder statistischer Test nur eine Aussage über die Nullhypothese erlaubt. Damit können wir hier nur die Varianzhomogenität testen.\n\nleveneTest(rsp ~ trt, data = small_heterogen_tbl) %>% tidy %>% select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    <dbl>\n1  0.0661\n\nbartlett.test(rsp ~ trt, data = small_heterogen_tbl) %>% tidy %>% select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    <dbl>\n1   0.127\n\n\nWir sehen, dass der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 20%. Damit können wir die Nullhypothese ablehnen. Wir nehmen Varianzheterogenität an. Überdies sehen wir auch, dass sich die \\(p\\)-Werte nicht groß voneinander unterscheiden. Was wir sehen ist, dass wir zu einem Signifikanzniveau von 5% die klare Varianzheterogenität nicht erkannt hätten und immer noch Varianzhomogenität angenommen hätten.\nWir können auch die Funktion check_homogeneity() aus dem Paket performance nutzen. Wir erhalten hier auch gleich eine Entscheidung in englischer Sprache ausgegeben. Die Funktion check_homogeneity() nutzt den Bartlett-Test. Wir können in Funktion auch andere Methoden mit method = c(\"bartlett\", \"fligner\", \"levene\", \"auto\") wählen.\n\nlm(rsp ~ trt, data = small_heterogen_tbl) %>% check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.127).\n\n\nWir sehen, dass sich die Implementierung des Bartlett-Tests in check_homogeneity() nicht von der Funktion bartlett.test() unterscheidet, aber die Entscheidung gegen die Varianzhomogenität zu einem Signifikanzniveau von 5% gefällt wird. Nicht immer hilft einem der Entscheidungtext einer Funktion."
  },
  {
    "objectID": "stat-tests-pretest.html#pre-test-auf-normalverteilung",
    "href": "stat-tests-pretest.html#pre-test-auf-normalverteilung",
    "title": "28  Pre-Tests oder Vortest",
    "section": "\n28.5 Pre-Test auf Normalverteilung",
    "text": "28.5 Pre-Test auf Normalverteilung\nWir treffen bei dem Test auf die Normalverteilung auch auf das gleiche Problem wie bei dem Pre-Test zur Varianzhomogenität. Wir haben wieder die Gleichheit, also das unser beobachtetes Outcome gleich einer Normalverteilung ist, in der Nullhypothese stehen. Den Unterschied, also das unser beobachtetes Outcome nicht aus einer Normalverteilung kommmt, in der Alternative.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{y ist gleich normalverteilt}\\\\\nH_A: &\\; \\mbox{y ist nicht gleich normalverteilt}\\\\\n\\end{aligned}\n\\]\nNun ist es aber so, dass es nicht nur zwei Verteilungen gibt. Es gibt mehr als die Normalverteilung und die Nicht-normalverteilung. Wir haben eine große Auswahl an möglichen Verteilungen und seit den 90zigern des letzten Jahrhunderts auch die Möglichkeiten andere Verteilungen des Outcomes \\(y\\) zu modellieren. Leider fällt dieser Fortschritt häufig unter den Tisch und wir bleiben gefangen zwischen der Normalverteilung oder eben keiner Normalverteilung.\n\n\nDer zentrale Grenzwertsatz besagt, dass wenn ein \\(y\\) von vielen Einflussfaktoren \\(x\\) bestimmt wird, man von einem normalverteilten \\(y\\) ausgehen.\nDas Gewicht wird von vielen Einflussfaktoren wie Sport, Kalorienaufnahme oder aber Veranlagung sowie vielem mehr bestimmt. Wir können davon ausgehen, dass das Gewicht normalverteilt ist.\nAbschließend sei noch gesagt, dass es fast unmöglich ist, eine Verteilung mit weniger als zwanzig Beobachtungen überhaupt abzuschätzen. Selbst dann können einzelne Beobachtunge an den Rändern der Verteilung zu einer Ablehnung der Normalverteilung führen, obwohl eine Normalverteilung vorliegt.\nAm Ende sei noch auf den QQ-plot verwiesen, mit dem wir auch visuell überprüfen können, ob eine Normalverteilung vorliegt.\n\n\n\n\n\n\nEntscheidung zur Normalverteilung\n\n\n\nBei der Entscheidung zur Normalverteilung gilt folgende Regel. Ist der \\(p\\)-Wert des Pre-Tests auf Normalverteilung kleiner als das Signifikanzniveau \\(\\alpha\\) von 5% lehnen wir die Nullhypothese ab. Wir nehmen eine Nicht-Normalverteilung an.\n\nIst \\(p \\leq \\alpha = 5\\%\\) so nehmen wir Nicht-Normalverteilung von \\(y\\) an.\nIst \\(p > \\alpha = 5\\%\\) so nehmen wir Normalverteilung von \\(y\\) an.\n\nAuf jeden Fall sollten wir das Ergebnis unseres Pre-Tests auf Normalverteilung nochmal visuell bestätigen."
  },
  {
    "objectID": "stat-tests-pretest.html#approximativ-normalverteilt-niedrige-fallzahl",
    "href": "stat-tests-pretest.html#approximativ-normalverteilt-niedrige-fallzahl",
    "title": "28  Pre-Tests oder Vortest",
    "section": "\n28.6 Approximativ normalverteilt, niedrige Fallzahl",
    "text": "28.6 Approximativ normalverteilt, niedrige Fallzahl\nAuch hier schauen wir uns den Fall mit einer niedrigen Fallzahl an. Dafür bauen wir usn erstmal Daten mit der Funktion rt(). Wir ziehen uns zufällig Beobachtungen aus einer t-Verteilung, die approximativ normalverteilt ist. Je höher die Freiheitsgrade df desto näher kommt die t-Verteilung einer Normalverteilung. Mit einem Freiheitsgrad von df = 30 sind wir sehr nah an einer Normalverteilung dran.\n\nset.seed(202209013)\nlow_normal_tbl <- tibble(A = rt(10, df = 30),\n                         B = rt(10, df = 30)) %>% \n  gather(trt, rsp) %>% \n  mutate(trt = as_factor(trt))\n\nIn Abbildung 28.3 sehen wir auf der linken Seite den Dotplot der zehn Beobachtungen aus den beiden Gruppen \\(A\\) und \\(B\\). Wir sehen, dass die Verteilung für das Outcome rsp in etwa normalverteilt ist.\n\n\n\n\n\n(a) Dotplot des Outcomes rsp.\n\n\n\n\n\n\n(b) Densityplot des Outcomes rsp.\n\n\n\n\nAbbildung 28.3— Verteilung des Outcomes rsp der zehn Beobachtungen aus den Gruppen \\(A\\) und \\(B\\). Beiden Gruppen kommen aus einer t-Verteilung.\n\n\nWir können den Shapiro-Wilk-Test nutzen um statistisch zu testen, ob eine Abweichung von der Normalverteilung vorliegt. Wir erfahren aber nicht, welche andere Verteilung vorliegt. Wir testen natürlich für die beiden Gruppen getrennt. Die Funktion shapiro.test()kann nur mit einem Vektor von Zahlen arbeiten, daher übergeben wir mit pull die entsprechend gefilterten Werte des Outcomes rsp.\n\nlow_normal_tbl %>% \n  filter(trt == \"A\") %>% \n  pull(rsp) %>% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.945699, p-value = 0.61797\n\nlow_normal_tbl %>% \n  filter(trt == \"B\") %>% \n  pull(rsp) %>% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.89291, p-value = 0.18282\n\n\nWir sehen, dass der \\(p\\)-Wert größer ist als das Signifikanzniveau \\(\\alpha\\) von 5% für beide Gruppen. Damit können wir die Nullhypothese nicht ablehnen. Wir nehmen eine Normalverteilung an.\nIn dem folgendem Beispiel sehen wir dann aber, was ich mit in die Ecke testen meine bzw. so lange statistisch zu Testen bis nichts mehr geht."
  },
  {
    "objectID": "stat-tests-pretest.html#nicht-normalverteilt-niedrige-fallzahl",
    "href": "stat-tests-pretest.html#nicht-normalverteilt-niedrige-fallzahl",
    "title": "28  Pre-Tests oder Vortest",
    "section": "\n28.7 Nicht normalverteilt, niedrige Fallzahl",
    "text": "28.7 Nicht normalverteilt, niedrige Fallzahl\nSchauen wir uns jetzt den anderen Fall an. Wir haben jetzt wieder eine niedrige Fallzahl mit je 10 Beobachtungen je Gruppe \\(A\\) und \\(B\\). In diesem Fall kommen die Beobachtungen aber aus einer exponentiellen Verteilung. Wir haben also definitiv keine Normalverteilung vorliegen. Wir generieren uns die Daten mit der Funktion rexp().\n\nset.seed(202209013)\nlow_nonnormal_tbl <- tibble(A = rexp(10, 1/1500),\n                            B = rexp(10, 1/1500)) %>% \n  gather(trt, rsp) %>% \n  mutate(trt = as_factor(trt))\n\nIn Abbildung 28.4 sehen wir auf der linken Seite den Dotplot der zehn Beobachtungen aus den beiden Gruppen \\(A\\) und \\(B\\). Wir sehen, dass die Verteilung für das Outcome für die Behandlung \\(B\\) in etwa normalverteilt ist sowie das das Outcome für die Behandlung \\(A\\) keiner Normalverteilung folgt oder zwei Ausreißer hat. Die Entscheidung was jetzt stimmt ohne zu wissen wie die Daten generiert wurden, ist in der Anwendung meist nicht möglich.\n\n\n\n\n\n(a) Dotplot des Outcomes rsp.\n\n\n\n\n\n\n(b) Densityplot des Outcomes rsp.\n\n\n\n\nAbbildung 28.4— Verteilung des Outcomes rsp der zehn Beobachtungen aus den Gruppen \\(A\\) und \\(B\\). Beiden Gruppen kommen aus einer Exponentialverteilung.\n\n\nWir können wieder den Shapiro-Wilk-Test nutzen um statistisch zu testen, ob eine Abweichung von der Normalverteilung vorliegt. Wir erfahren aber nicht, welche andere Verteilung vorliegt. Wir testen natürlich für die beiden Gruppen getrennt.\n\nlow_nonnormal_tbl %>% \n  filter(trt == \"A\") %>% \n  pull(rsp) %>% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.771138, p-value = 0.0064566\n\nlow_nonnormal_tbl %>% \n  filter(trt == \"B\") %>% \n  pull(rsp) %>% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.933164, p-value = 0.47973\n\n\nWir sehen, dass der \\(p\\)-Wert für die Behandlung \\(A\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Damit können wir die Nullhypothese ablehnen. Wir nehmen keine Normalverteilung für Gruppe \\(A\\) an. Auf der anderen Seite sehen wir, dass der \\(p\\)-Wert für die Behandlung \\(B\\) größer ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Damit können wir die Nullhypothese nicht ablehnen. Wir nehmen eine Normalverteilung für Gruppe \\(A\\) an.\nSuper, jetzt haben wir für die eine Gruppe eine Normalverteilung und für die andere nicht. Wir haben uns in die Ecke getestet. Wir können jetzt verschiedene Szenarien vorliegen haben.\n\nWir könnten in der Gruppe \\(A\\) zwei Ausreißer vorliegen haben.\nWir könnten in der Gruppe \\(B\\) zufällig eine Normalverteilung beobachtet haben.\n\nUnd nochmal zum Schluß, einem statistischen Test mit 4 bis 5 Wiederholungen in einer Gruppe zu glauben, ob eine Normalverteilung vorliegt, kannst du auch würfeln…\nLeider wissen wir im echten Leben nicht, aus welcher Verteilung unsere Daten stammen, wir können aber annehmen, dass die Daten einer Normalverteilung folgen oder aber die Daten so transformieren, dass die Daten einer approximativen Normalverteilung folgen. Siehe dazu auch das Kapitel 16 zur Transformation von Daten.\nWenn deine Daten keiner Normalverteilung folgen, dann kann es sein, dass du mit den Effektschätzern ein Problem bekommst. Du erfährst vielleicht, dass du die Nullhypothese ablehnen kannst, aber nicht wie stark der Effekt in der Einheit des gemessenen Outcomes ist."
  },
  {
    "objectID": "stat-tests-posthoc.html",
    "href": "stat-tests-posthoc.html",
    "title": "29  Multiple Vergleiche oder Post-hoc Tests",
    "section": "",
    "text": "Version vom Oktober 12, 2022 um 14:14:18\nIn diesem Kapitel wollen wir uns mit den multipen Vergleichen beschäftigen. Das heißt, wir wollen statistisch Testen, ob sich die Level eines Faktors voneinander unterscheiden. Eventuell hast du schon eine einfaktorielle ANOVA gerechnet, wie in Kapitel 22.2 beschrieben. Oder aber du hast eine mehrfaktorielle ANOVA gerechnet wie in Kapitel 22.3 gezeigt. In beiden Fällen hast du jetzt einen signifikanten Faktor, der mehr als zwei Level hat. Du willst nun wissen, welche der Gruppenmittelwerte der Level sich signifikant unterscheiden. Hierfür können wir verschiedene Ansätze wählen.\nWenn wir multiple Mittelwertsvergleiche rechnen, dann tritt das Problem des multipen Testens auf. Im Kapitel 19.3 kannst du mehr über die Problematik erfahren und wie wir mit der \\(\\alpha\\) Inflation umgehen. Hier in diesem Kapitel gehe ich jetzt davon aus, dass dir die \\(\\alpha\\) Adjustierung ein Begriff ist."
  },
  {
    "objectID": "stat-tests-posthoc.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-posthoc.html#genutzte-r-pakete-für-das-kapitel",
    "title": "29  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n29.1 Genutzte R Pakete für das Kapitel",
    "text": "29.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom,\n               multcomp, emmeans, ggpubr, multcompView,\n               rstatix, conflicted, see)\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'estimability' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'emmeans' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\jokruppa\\AppData\\Local\\Temp\\Rtmp8ChFoj\\downloaded_packages\n\n\nWarning: Paket 'emmeans' wurde unter R Version 4.2.1 erstellt\n\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-posthoc.html#daten",
    "href": "stat-tests-posthoc.html#daten",
    "title": "29  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n29.2 Daten",
    "text": "29.2 Daten\nWir nutzen in diesem Kapitel den Datensatz aus dem Beispiel in ?sec-example-3. Wir haben als Outcome die Sprunglänge in [cm] von Flöhen. Die Sprunglänge haben wir an Flöhen von Hunde, Katzen und Füchsen gemessen. Der Datensatz ist also recht übeerschaubar. Wir haben ein normalverteiltes \\(y\\) mit jump_length sowie einen multinomialverteiltes \\(y\\) mit grade und einen Faktor animal mit drei Leveln.\nDu kannst dir komplexere Auswertungen im ?sec-beispiel-auswertung anschauen. Dort sammelt sich mit der Zeit Auswertungen vom Fachbereich an. Daher finden sich dort auch Beispiele für multiple Vergleiche.\nIm Folgenden laden wir den Datensatz flea_dog_cat_fox.csv und selektieren mit der Funktion select() die benötigten Spalten. Abschließend müssen wir die Spalte animalnoch in einen Faktor umwandeln. Damit ist unsere Vorbereitung des Datensatzes abgeschlossen.\n\nfac1_tbl <- read_csv2(\"data/flea_dog_cat_fox.csv\") %>%\n  select(animal, jump_length, grade) %>% \n  mutate(animal = as_factor(animal))\n\nIn der Tabelle 29.1 ist der Datensatz fac1_tbl nochmal dargestellt.\n\n\n\n\nTabelle 29.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length sowie der multinominalverteilten Variable grade und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\ngrade\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n8.2\n8\n\n\ndog\n5.6\n7\n\n\ndog\n9.1\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\nfox\n7.7\n5\n\n\nfox\n8.1\n4\n\n\nfox\n9.1\n4\n\n\nfox\n9.7\n5\n\n\nfox\n10.6\n4\n\n\nfox\n8.6\n4\n\n\nfox\n10.3\n3\n\n\n\n\n\n\nWir werden nun den Datensatz fac1_tbl in den folgenden Abschnitten immer wieder nutzen.\n\n29.2.1 Hypothesen für multiple Vergleiche\nAls wir eine ANOVA gerechnet hatten, hatten wir nur eine Nullhypothese und eine Alternativehypothese. Wenn wir Nullhypothese abgelehnt hatten, wussten wir nur, dass sich mindestens ein paarweiser Vergleich unterschiedet. Multiple Vergleich lösen nun dieses Problem und führen ein Hypothesenpaar für jeden paarweisen Vergleich ein. Zum einen rechnen wir damit \\(k\\) Tests und haben damit auch \\(k\\) Hypothesenpaare (siehe auch Kapitel 19.3 zur Problematik des wiederholten Testens).\nWenn wir zum Beispiel alle Level des Faktors animal miteinander Vergleichen wollen, dann rechnen wir \\(k=3\\) paarweise Vergleiche. Im Folgenden sind alle drei Hypothesenpaare dargestellt.\n\\[\n\\begin{aligned}\nH_{01}: &\\; \\bar{y}_{cat} = \\bar{y}_{dog}\\\\\nH_{A1}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{02}: &\\; \\bar{y}_{cat} = \\bar{y}_{fox}\\\\\nH_{A2}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{03}: &\\; \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_{A3}: &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\nWenn wir drei Vergleiche rechnen, dann haben wir eine \\(\\alpha\\) Inflation vorliegen. Wir sagen, dass wir für das multiple Testen adjustieren müssen. In R gibt es eine Reihe von Adjustierungsverfahren. Wir nehmen meist Bonferroni oder das Verfahren, was in der jeweiligen Funktion als Standard (eng. default) gesetzt ist.\nWir adjustieren grundsätzlich die \\(p\\)-Werte und erhalten adjustierte \\(p\\)-Werte aus den jeweiligen Funktionen in R. Die adjustierten p-Werte können wir dann mit dem Signifikanzniveau von \\(\\alpha\\) gleich 5% vergleichen."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "href": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "title": "29  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n29.3 Gruppenvergleiche mit pairwise.*.test()\n",
    "text": "29.3 Gruppenvergleiche mit pairwise.*.test()\n\nDie Funktion pairwise.*.test() ist veraltet, wir nutzen das R Paket emmeansoder das R Paket multcomp.\nWenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, dann können wir die Funktion pairwise.*.test() nutzen. Der Stern * steht entweder als Platzhalter für t für den t-Test oder aber für wilcox für den Wilcoxon Test. Die Funktion ist relativ einfach zu nutzen und liefert auch sofort die entsprechenden p-Werte.\nDie Funktion pairwise.*.test() ist in dem Sinne veraltet, da wir keine 95% Konfidenzintervalle generieren können. Da die Funktion aber immer mal wieder angefragt wird, ist die Funktion hier nochmal aufgeführt.\n\n29.3.1 Paarweiser t Test\nWir nutzen den paarweisen t-Test,\n\nwenn wir ein normalverteiltes \\(y\\) vorliegen haben, wie jump_length.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\n\nDie Funktion pairwise.t.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und dhaer auch fehleranfällig.\n\n\nMehr zu mutate_if() erfährst du auf der Hilfeseite von mutate()\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein saubers tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.t.test(fac1_tbl$jump_length, fac1_tbl$animal,\n                p.adjust.method = \"bonferroni\") %>% \n  tidy %>% \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 x 3\n  group1 group2 p.value\n  <chr>  <chr>    <dbl>\n1 cat    dog      0.007\n2 fox    dog      0.876\n3 fox    cat      0.001\n\n\nWir erhalten in einem Tibble die adujstierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wir sehen, dass der Gruppenvergleich cat - dog signifikant ist, der Gruppenvergleich fox - dog nicht signifkant ist und der Gruppenvergleich fox - cat wiederum signifkant ist.\nLeider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket ggpubr anschauen. Die Hilfeseite des R Pakets ggpubr liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\).\nUm die Abbildung 29.1 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen t-Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert einer einfaktoriellen ANOVA als globalen Test ergänzen.\n\nstat_test_obj <- compare_means(\n jump_length ~ animal, data = fac1_tbl,\n method = \"t.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np <- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"jump_length\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(13, 16, 19)) +\n  stat_compare_means(label.y = 20, method = \"anova\")    \n\n\n\nAbbildung 29.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem t-Test und den Bonferroni adjustierten p-Werten.\n\n\n\n\n\n29.3.2 Paarweiser Wilcoxon Test\nWir nutzen den paarweisen Wilxocon Test,\n\nwenn wir ein nicht-normalverteiltes \\(y\\) vorliegen haben, wie grade.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\n\nDie Funktion pairwise.wilcox.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und dhaer auch fehleranfällig.\n\n\nMehr zu mutate_if() erfährst du auf der Hilfeseite von mutate()\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein saubers tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.wilcox.test(fac1_tbl$grade, fac1_tbl$animal,\n                     p.adjust.method = \"bonferroni\") %>% \n  tidy %>% \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 x 3\n  group1 group2 p.value\n  <chr>  <chr>    <dbl>\n1 cat    dog      0.045\n2 fox    dog      0.005\n3 fox    cat      0.011\n\n\nWir erhalten in einem Tibble die adujstierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wir sehen, dass der Gruppenvergleich cat - dog knapp signifikant ist, der Gruppenvergleich fox - dog ebenfalls signifkant ist und der Gruppenvergleich fox - cat auch signifkant ist.\nLeider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket ggpubr anschauen. Die Hilfeseite des R Pakets ggpubr liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\).\nUm die Abbildung 29.2 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen Wilcoxon Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert eines Kruskal Wallis als globalen Test ergänzen.\n\nstat_test_obj <- compare_means(\n grade ~ animal, data = fac1_tbl,\n method = \"wilcox.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np <- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"grade\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(10, 13, 16)) +\n  stat_compare_means(label.y = 20, method = \"kruskal.test\")    \n\n\n\nAbbildung 29.2— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem Wilcoxon Test und den Bonferroni adjustierten p-Werten."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "href": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "title": "29  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n29.4 Gruppenvergleich mit dem multcomp Paket",
    "text": "29.4 Gruppenvergleich mit dem multcomp Paket\nWir drehen hier einmal die Erklärung um. Wir machen erst die Anwendung in R und sollte dich dann noch mehr über die statistischen Hintergründe der Funktionen interessieren, folgt ein Abschnitt noch zur Theorie. Du wirst die Funktionen aus multcomp vermutlich in deiner Abschlussarbeit brauchen. Häufig werden multiple Gruppenvergleiche in Abschlussarbeiten gerechnet.\n\n29.4.1 Gruppenvergleiche mit multcomp in R\n\n\nDie Ausgabe von multcomp können über die Funktion tidy() aufgeräumt werden. Mehr dazu unter der Hilfeseite von tidy() zu multcomp.\nAls erstes brauchen wir ein lineares Modell für die Verwendung von multcomp. Normalerweise verenden wir das gleiche Modell, was wir schon in der ANOVA verwendet haben. Wir nutzen hier ein simples lineares Modell mit nur einem Faktor. Im Prinzip kann das Modell auch größer sein. Du findest immer Beispiel im ?sec-beispiel-auswertung, die dir eventuell dann nochmal zeigen, wie du deine Daten nutzen musst.\n\nfit_1 <- lm(jump_length ~ animal, data = fac1_tbl)\n\nWir haben das Objeckt fit_1 mit der Funktion lm() erstellt. Im Modell sind jetzt alle Mittelwerte und die entsprechenden Varianzen geschätzt worden. Mit summary(fit_1) kannst du dir gerne das Modell auch nochmal anschauen.\n\n\nWenn wir keinen all-pair Vergleich rechnen wollen, dann können wir auch einen many-to-one Vergleich mit dem Dunnett Kontrast rechnen.\nIm Anschluß nutzen wir die Funktion glht() um den multiplen vergleich zu rechnen. Als erstes musst du wissen, dass wenn wir alle Vergleiche rechnen wollen, wir einen all-pair Vergleich rechnen. In der Statistik heißt dieser Typ von Vergleich Tukey. Wir wollen jetzt als für den Faktor animal einen multiplen Tukey-Vergleich rechnen. Nichts anders sagt mcp(animal = \"Tukey\") aus, dabei steht mcp für multiple comparison procedure. Mit dem hinteren Teil der Funktion weiß jetzt die Funktion glht() was gerechnet werden soll. Wir müssen jetzt der Funktion nur noch mitgeben auf was der multiple vergleich gerehcnet werden soll, mit dem Objekt fit_1. Wir speichern die Ausgabe der Funktion in comp_1_obj.\n\ncomp_1_obj <- glht(fit_1, linfct = mcp(animal = \"Tukey\")) \n\nMit dem Objekt comp_1_fit können wir noch nicht soviel anfangen. Der Inhalt ist etwas durcheinander und wir wollen noch die Konfidenzintervalle haben. Daher pipen wir comp_1_fit erstmal in die Funktion tidy() und alssen mit der Option conf.int = TRUE die simultanen 95% Konfidenzintervalle berechnen. Dann nutzen wir die Funktion select() um die wichtigen Spalten zu selektieren. Abschließend mutieren wir noch alle numerischen Spalten in dem wir auf die dritte Kommastelle runden. Wir speichern alles in das Objekt res_1_obj.\n\nres_1_obj <- comp_1_obj %>% \n  tidy(conf.int = TRUE) %>% \n  select(contrast, estimate, adj.p.value, \n         conf.low, conf.high) %>% \n  mutate_if(is.numeric, round, 3)\n\nWir lassen uns dann den Inhalt von dem Objekt res_1_obj ausgeben.\n\nres_1_obj\n\n# A tibble: 3 x 5\n  contrast  estimate adj.p.value conf.low conf.high\n  <chr>        <dbl>       <dbl>    <dbl>     <dbl>\n1 cat - dog    -3.39       0.006    -5.80     -0.97\n2 fox - dog     1.03       0.535    -1.39      3.44\n3 fox - cat     4.41       0.001     2.00      6.83\n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir cat - dog rechnen. Also wirklich der Mittelwert der Sprungweite der Katzenflöhe minus den Mittelwert der Sprungweite der Hundeflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleichecat - dog mit einem p-Wert von \\(0.006\\) sowie für den Vergleich \\(fox - cat\\) mit einem p-Wert von \\(0.001\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nIn Abbildung 29.3 sind die simultanen 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab. Mit der Funktion factor() kannst du die Sortierung der Level einem Faktor ändern und somit auch Position auf den Achsen.\n\n  ggplot(res_1_obj, aes(contrast, y=estimate, \n                        ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 29.3— Simultane 95% Konfidenzintervalle für den paarweisen Vergleich der Sprungweiten in [cm] der Hunde-, Katzen- und Fuchsflöhe.\n\n\n\n\nDie Entscheidung gegen die Nullhypothese anhand der simultanen 95% Konfidenzintervalle ist inhaltlich gleich, wie die Entscheidung anhand der p-Werte. Wir entscheiden gegen die Nullhypothese, wenn die 0 nicht mit im Konfindenzintervall enthalten ist. Wir wählen hier die 0 zur Entscheidung gegen die Nullhypothese, weil wir einen Mittelwertsvergleich rechnen.\nFür den Vergleich fox -dog ist die 0 im 95% Konfidenzintervall, wir können daher die Nullhypothese nicht ablehnen. Das 95% Konfidenzintervall ist nicht signifikant. Bei dem Vergleich fox - cat sowie dem Vergleich cat - dog ist jeweils die 0 nicht im 95% Konfidenzintervall enthalten. Beide 95% Konfidenzintervalle sind signifikant, wir können die Nullhypothese ablehnen."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "href": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "title": "29  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n29.5 Gruppenvergleich mit dem emmeans Paket",
    "text": "29.5 Gruppenvergleich mit dem emmeans Paket\n\n\nWir können hier nicht alles erklären und im Detail durchgehen. Hier gibt es noch ein aufwendiges Tutorium zu emmeans: Getting started with emmeans.\nDaneben gibt es auch noch die Einführung mit Theorie auf der Seite des R Paktes\nIm Folgenden wollen wir uns mit einem anderen R Paket beschäftigen was auch multiple Vergleiche rechnen kann. In diesem Kapitel nutzen wir das R Paket emmeans. Im Prinzip kann emmeans das Gleiche wir das R Paket multcomp. Beide Pakete rechnen dir einen multipen Vergleich. Das Paket emmeans kann noch mit nested comparisons umgehen. Deshlb hier nochmal die Vorstellung von emmeans. Du kannst aber für eine simple Auswertung mit nur einem Faktor beide Pakete verwenden.\n\n29.5.1 Gruppenvergleiche mit emmeans in R\n\n\nDie Ausgabe von emmeans können über die Funktion tidy() aufgeräumt werden. Mehr dazu unter der Hilfeseite von tidy() zu emmeans.\nUm den multiplen Vergleich in emmeans durchführen zu können brauchen wir zuerst ein lineares Modell, was uns die notwenidgen Parameter wie Mittelwerte und Standardabweichungen liefert. Wir nutzen in unserem simplen Beispiel ein lineares Modell mit einer Einflussvariable \\(x\\) und nehmen an, dass unser Outcome \\(y\\) normalverteilt ist. Achtung, hier muss natürlich das \\(x\\) ein Faktor sein. Dann können wir ganz einfach die Funktion lm() nutzen. Im Folgenden fitten wir das Modell fit_2 was wir dann auch weiter nutzen werden.\n\nfit_2 <- lm(jump_length ~ animal, data = fac1_tbl)\n\nDer multiple Vergleich in emmeans ist mehrschrittig. Wir pipen unser Modell aus fit_2 in die Funktion emmeans(). Wir geben mit ~ animal an, dass wir über die Level des Faktors animal einen Vergleich rechnen wollen. Wir adjustieren die \\(p\\)-Werte nach Bonferroni. Danach pipen wir weiter in die Funktion contrast() wo der eigentliche Vergleich festgelegt wird. In unserem Fall wollen wir einen many-to-one Vergleich rechnen. Alle Gruppen zu der Gruppe fox. Du kannst mit ref = auch ein anderes Level deines Faktors wählen.\n\ncomp_2_obj <- fit_2 %>% \n  emmeans(~ animal, adjust = \"bonferroni\") %>% \n  contrast(method = \"trt.vs.ctrl\", ref = \"fox\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - fox    -1.03 0.947 18  -1.086  0.4682\n cat - fox    -4.41 0.947 18  -4.660  0.0004\n\nP value adjustment: dunnettx method for 2 tests \n\n\nWir können auch einen anderen Kontrast wählen. Wir überschreiben jetzt das Objekt comp_2_obj mit dem Kontrast all-pair, der alle möglichen Vergleiche rechnet. In emmeans heißt der all-pair Kontrast pairwise.\n\ncomp_2_obj <- fit_2 %>% \n  emmeans(~ animal, adjust = \"bonferroni\") %>% \n  contrast(method = \"pairwise\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - cat     3.39 0.947 18   3.574  0.0058\n dog - fox    -1.03 0.947 18  -1.086  0.5347\n cat - fox    -4.41 0.947 18  -4.660  0.0005\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nWir können das Ergebnis auch noch mit der Funktion tidy() weiter aufräumen und dann die Spalten selektieren, die wir brauchen. Häufig benötigen wir nicht alle Spalten, die eine Funktion wiedergibt.\n\nres_2_obj <- comp_2_obj %>% \n  tidy(conf.int = TRUE) %>% \n  select(contrast, estimate, adj.p.value, conf.low, conf.high) %>% \n  mutate(across(where(is.numeric), round, 4))\n\nres_2_obj\n\n# A tibble: 3 x 5\n  contrast  estimate adj.p.value conf.low conf.high\n  <chr>        <dbl>       <dbl>    <dbl>     <dbl>\n1 dog - cat     3.39      0.0058    0.968      5.80\n2 dog - fox    -1.03      0.535    -3.45       1.39\n3 cat - fox    -4.41      0.0005   -6.83      -2.00\n\n\nAbschließend wollen wir noch die 95% Konfidenzintervalle in Abbildung 29.4 abbilden. Hier ist es bei emmeans genauso wie bei multcomp. Wir können das Objekt res_2_obj direkt in ggplot() weiterverwenden und uns die 95% Konfidenzintervalle einmal plotten.\n\n  ggplot(res_2_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 29.4— Die 95% Konfidenzintervalle für den allpair-Vergleich des simplen Datensatzes.\n\n\n\n\nWir wollen uns noch einen etwas komplizierteren Fall anschauen, indem sich emmeans von multcomp in der Anwendung unterscheidet. Wir laden den Datensatz flea_dog_cat_fox_site.csv in dem wir zwei Faktoren haben. Damit können wir dann ein Modell mit einem Interaktionsterm bauen. Wir erinnern uns, dass wir in der zweifaktoriellen ANOAV eine signifikante Interaktion zwischen den beiden Faktoren animal und site festgestelt hatten.\n\nfac2_tbl <- read_csv2(\"data/flea_dog_cat_fox_site.csv\") %>% \n  select(animal, site, jump_length) %>% \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 29.2 nochmal dargestellt.\n\n\n\n\nTabelle 29.2— Selektierter Datensatz mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\nanimal\nsite\njump_length\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.1\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\n…\n…\n…\n\n\nfox\nfield\n16.38\n\n\nfox\nfield\n14.59\n\n\nfox\nfield\n14.03\n\n\nfox\nfield\n13.63\n\n\nfox\nfield\n14.09\n\n\nfox\nfield\n15.52\n\n\n\n\n\n\nIn Abbildung 29.5 sehen wir nochmal die Daten visualisiert. Wichtig ist hier, dass wir zwei Faktoren vorliegen haben. Den Faktor animal und den Faktor site. Dabei ist der Faktor animal in dem Faktor site genested. Wir messen jedes Level des Faktors animal jeweils in jedem Level des Faktors site.\n\n\n\n\nAbbildung 29.5— Boxplot der Sprungweiten [cm] von Hunden und Katzen gemessen an verschiedenen Orten.\n\n\n\n\nWir rechnen ein multiples lineares Modell mit einem Interaktionsterm. Daher packen wir beide Faktoren in das Modell sowie die Intraktion zwischen den beiden Faktoren. Wir erhalten nach dem fitten des Modells das Objekt fit_3.\n\nfit_3 <- lm(jump_length ~ animal + site + animal:site, data = fac2_tbl)\n\nDer Unterschied zu unserem vorherigen multiplen Vergleich ist nun, dass wir auch einen multiplen Vergleich für animal nested in site rechnen können. Dafür müssen wir den Vergleich in der Form animal | site schreiben. Wir erhalten dann die Vergleiche der Level des faktors animal getrennt für die Level es Faktors site.\n\ncomp_3_obj <- fit_3 %>% \n  emmeans(~ animal | site, adjust = \"bonferroni\") %>% \n  contrast(method = \"pairwise\") \n\ncomp_3_obj\n\nsite = city:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -3.101 0.771 108  -4.022  0.0003\n cat - fox   -6.538 0.771 108  -8.479  <.0001\n dog - fox   -3.437 0.771 108  -4.457  0.0001\n\nsite = smalltown:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -4.308 0.771 108  -5.587  <.0001\n cat - fox   -4.064 0.771 108  -5.271  <.0001\n dog - fox    0.244 0.771 108   0.316  0.9463\n\nsite = village:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -1.316 0.771 108  -1.707  0.2073\n cat - fox   -1.729 0.771 108  -2.242  0.0687\n dog - fox   -0.413 0.771 108  -0.536  0.8540\n\nsite = field:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -0.982 0.771 108  -1.274  0.4131\n cat - fox    1.366 0.771 108   1.772  0.1840\n dog - fox    2.348 0.771 108   3.045  0.0082\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nWir können uns das Ergebnis auch etwas schöner ausgeben lassen. Wir nutzen hier noch die Funktion format.pval() um die \\(p\\)-Werte besser zu formatieren. Die \\(p\\)-Wert, die kleiner sind als 0.001 werden als <0.001 ausgegeben und die anderen \\(p\\)-Werte auf zwei Nachstellen nach dem Komma gerundet.\n\ncomp_3_obj %>% \n  summary %>% \n  as_tibble %>% \n  select(contrast, site, p.value) %>% \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2))\n\n# A tibble: 12 x 3\n   contrast  site      p.value\n   <fct>     <fct>     <chr>  \n 1 cat - dog city      <0.001 \n 2 cat - fox city      <0.001 \n 3 dog - fox city      <0.001 \n 4 cat - dog smalltown <0.001 \n 5 cat - fox smalltown <0.001 \n 6 dog - fox smalltown 0.95   \n 7 cat - dog village   0.21   \n 8 cat - fox village   0.07   \n 9 dog - fox village   0.85   \n10 cat - dog field     0.41   \n11 cat - fox field     0.18   \n12 dog - fox field     0.01   \n\n\nIn der Ausgabe können wir erkennen, dass die Vergleich in der Stadt alle signifkant sind. Jedoch erkennen wir keine signifikanten Ergebnisse mehr in dem Dorf und im Feld ist nur der Vergleich dog - fox signifkant. Hier solltest du nochmal beachten, warum wir die Analyse getrennt machen. In der zweifaktoriellen ANOVA haben wir gesehen, dass ein signifkanter Interaktionsterm zwischen den beiden Faktoren animal und site vorliegt.\nWir wollen uns noch über die Funktion confint() die 95% Konfidenzintervalle wiedergeben lassen.\n\nres_3_obj <- comp_3_obj %>% \n  confint() %>% \n  as_tibble() %>% \n  select(contrast, site, estimate, conf.low = lower.CL, conf.high = upper.CL) \n\nres_3_obj\n\n# A tibble: 12 x 5\n   contrast  site      estimate conf.low conf.high\n   <fct>     <fct>        <dbl>    <dbl>     <dbl>\n 1 cat - dog city        -3.10    -4.93     -1.27 \n 2 cat - fox city        -6.54    -8.37     -4.71 \n 3 dog - fox city        -3.44    -5.27     -1.60 \n 4 cat - dog smalltown   -4.31    -6.14     -2.48 \n 5 cat - fox smalltown   -4.06    -5.90     -2.23 \n 6 dog - fox smalltown    0.244   -1.59      2.08 \n 7 cat - dog village     -1.32    -3.15      0.516\n 8 cat - fox village     -1.73    -3.56      0.103\n 9 dog - fox village     -0.413   -2.25      1.42 \n10 cat - dog field       -0.982   -2.81      0.850\n11 cat - fox field        1.37    -0.466     3.20 \n12 dog - fox field        2.35     0.516     4.18 \n\n\nBesonders mit den 95% Konfiendezintervallen sehen wir nochmal den Interaktionseffekt zwischen den beiden Faktoren animal und site. So dreht sich der Effekt von zum Beispiel dog - fox von \\(-3.44\\) in dem Level city zu \\(+2.35\\) in dem Level field. Wir haben eine Interaktion vorliegen und deshalb die Analyse getrennt für jeden Level des Faktors site durchgeführt.\nAbbildung 29.6 zeigt die entsprechenden 95% Konfidenzintervalle. Wir müssen hier etwas mit der position spielen, so dass die Punkte und der geom_errorbar richtig liegen.\n\n  ggplot(res_3_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high,\n                        color = site, group = site)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    scale_color_okabeito() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 29.6— Die 95% Konfidenzintervalle für den allpair-Vergleich des Models mit Interaktionseffekt."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-ght",
    "href": "stat-tests-posthoc.html#sec-posthoc-ght",
    "title": "29  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n29.6 Gruppenvergleich mit dem Games-Howell-Test",
    "text": "29.6 Gruppenvergleich mit dem Games-Howell-Test\nDer Games-Howell-Test ist eine Alternative zu dem Paket multcomp und dem Paket emmeans. Wir nutzen den Games-Howell-Test, wenn die Annahme der Homogenität der Varianzen, der zum Vergleich aller möglichen Kombinationen von Gruppenunterschieden verwendet wird, verletzt ist. Dieser Post-Hoc-Test liefert Konfidenzintervalle für die Unterschiede zwischen den Gruppenmitteln und zeigt, ob die Unterschiede statistisch signifikant sind. Der Test basiert auf der Welch’schen Freiheitsgradkorrektur und adjustiert die \\(p\\)-Werte. Der Test vergleicht also die Differenz zwischen den einzelnen Mittelwertpaaren mit einer Adjustierung für den Mehrfachtest. Es besteht also keine Notwendigkeit, zusätzliche p-Wert-Korrekturen vorzunehmen. Mit dem Games-Howell-Test ist nur ein all-pair Vergleich möglich.\nFür den Games-Howell-Test aus dem Paket rstatix müssen wir kein lineares Modell fitten. Wir schreiben einfach die wie in einem t-Test das Outcome und den Faktor mit den Gruppenleveln in die Funktion games_howell_test(). Wir erhalten dann direkt das Ergebnis des Games-Howell-Test. Wir nutzen in diesem Beispiel die Daten aus dem Objekt fac1_tbl zu sehen in Tabelle 29.1.\n\nfit_4 <- games_howell_test(jump_length ~ animal, data = fac1_tbl) \n\nWir wollen aber nicht mit der Ausgabe arbeiten sondern machen uns noch ein wenig Arbeit und passen die Ausgabe an. Zum einen brauchen wir noch die Kontraste und wir wollen die \\(p\\)-Werte auch ansprechend formatieren. Wir erhalten das Objekt res_4_obj und geben uns die Ausgabe wieder.\n\nres_4_obj <- fit_4 %>% \n  as_tibble %>% \n  mutate(contrast = str_c(group1, \"-\", group2)) %>% \n  select(contrast, estimate, p.adj, conf.low, conf.high) %>% \n  mutate(p.adj = format.pval(p.adj, eps = 0.001, digits = 2))\n\nres_4_obj\n\n# A tibble: 3 x 5\n  contrast estimate p.adj conf.low conf.high\n  <chr>       <dbl> <chr>    <dbl>     <dbl>\n1 dog-cat     -3.39 0.02     -6.28    -0.490\n2 dog-fox      1.03 0.52     -1.52     3.57 \n3 cat-fox      4.41 0.00      2.12     6.71 \n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt, den haben wir uns selber mit der Funktion mutate() und str_c() aus den Spalten group1 und group2 gebaut. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir dog-cat rechnen. Also wirklich den Mittelwert der Sprungweite der Hundeflöhe minus den Mittelwert der Sprungweite der Katzenflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleiche dog - cat mit einem p-Wert von \\(0.02\\) sowie für den Vergleich \\(cat - fox\\) mit einem p-Wert von \\(0.00\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nIn Abbildung 29.7 sind die simultanen 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab.\n\n  ggplot(res_4_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 29.7— Die 95% Konfidenzintervalle für den allpair-Vergleich des Games-Howell-Test.\n\n\n\n\nDie Entscheidungen nach den 95% Konfidenzintervallen sind die gleichen wie nach dem \\(p\\)-Wert. Da wir hier es mit einem Mittelwertsvergleich zu tun haben, ist die Entscheidung gegen die Nullhypothese zu treffen wenn die 0 im Konfidenzintervall ist."
  },
  {
    "objectID": "stat-tests-posthoc.html#compact-letter-display",
    "href": "stat-tests-posthoc.html#compact-letter-display",
    "title": "29  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n29.7 Compact letter display",
    "text": "29.7 Compact letter display\n\n\nCompact Letter Display (CLD) - What is it?\nDas compact letter display zeigt an, bei welchen Vergleichen der Behandlungen die Nullhypothese gilt. Daher werden die nicht signifikanten Ergebnisse visualisiert.\nIn der Pflanzenernährung ist es nicht unüblich sehr viele Substrate miteinander zu vergleichen. Oder andersherum, wenn wir sehr viele Gruppen haben, dann kann die Darstellung in einem all-pair Vergleich sehr schnell sehr unübersichltich werden. Deshalb wure das compact letter display entwickelt.\nSchauen wir uns aber zurerst einmal ein größeres Beispiel mit neun Behandlungen mit jeweils zwanzig Beobachtungen an. Wir erstellen uns den Datensatz in der Form, dass sich die Mittelwerte für die Behandlungen teilweise unterscheiden.\n\nset.seed(20220914)\ndata_tbl <- tibble(trt = gl(n = 9, k = 20, \n                            labels = c(\"pos_crtl\", \"neg_ctrl\", \"treat_A\", \"treat_B\", \n                                       \"treat_C\", \"treat_D\", \"treat_E\", \"treat_F\", \n                                       \"treat_G\")),\n                   rsp = c(rnorm(20, 10, 5), rnorm(20, 20, 5), rnorm(20, 22, 5), rnorm(20, 24, 5),\n                           rnorm(20, 35, 5), rnorm(20, 37, 5), rnorm(20, 40, 5), rnorm(20, 43, 5),\n                           rnorm(20, 50, 5)))\n\nIn der Abbildung 29.8 ist der Datensatz data_tbl nochmal als Boxplot dargestellt.\n\n\n\n\nAbbildung 29.8— Boxplot der Beispieldaten.\n\n\n\n\nWir sehen, dass sich die positive Kontrolle von dem Rest der Behandlungen unterscheidet. Danach haben wir ein Plateau mit der negativen Kontrolle und der Behanldung A und der Behandlung B. Nach diesem Plateau haben wir einen Sprung und sehen einen leicht linearen Anstieg der Mittelwerte der Behandlungen.\nSchauen wir uns zuerst einmal an, wie ein compact letter display aussehen würde, wenn kein Effekt vorliegen würde. Daher die Nullhypothese ist wahr und die Mittelwerte der Gruppen unterscheiden sich nicht. Wir nutzen hier einmal ein kleineres Beispiel mit den Behandlungslevels ctrl, treat_A und treat_B. Alle drei Behandlungslevel haben einen Mittelwert von 10. Es gilt die Nullhypothese und wir erhalten folgendes compact letter display in Tabelle 29.3.\n\n\nTabelle 29.3— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt, es gibt keinen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\\(\\phantom{a}\\)\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n10\na\n\n\n\n\ntreat_B\n10\na\n\n\n\n\n\n\nDas Gegenteil sehen wir in der Tabelle 29.4. Hier haben wir ein compact letter display wo sich alle drei Mittelwerte mit 10, 15 und 20 voneinander klar unterscheiden. Die Nullhypothese gilt für keinen der möglichen paarweisen Vergleiche.\n\n\nTabelle 29.4— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt nicht, es gibt einen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n15\n\nb\n\n\n\ntreat_B\n20\n\\(\\phantom{a}\\)\n\nc\n\n\n\n\nSchauen wir uns nun die Implementierung des compact letter display für die verschiedenen Möglichkeiten der Multiplen Vergleiche einmal an.\n\n29.7.1 … für das Paket multcomp\n\nWir schauen uns zuerst einmal die Implementierung des compact letter display in dem Paket multcomp an. Wir nutzen die Funktion multcompLetters() aus dem Paket multcompView um uns das compact letter display wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Das heist wir müssen aus der Funktion glht() die adjustierten \\(p\\)-Werte extrahieren und dann einen Vektor der Vergleiche bzw. Kontraste in der Form A-B bauen. Also ohne Leerzeichen und in der Beschreibung der Level der Behandlung trt. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nmultcomp_cld <- lm(rsp  ~ trt, data = data_tbl) %>%\n  glht(linfct = mcp(trt = \"Tukey\")) %>% \n  tidy %>% \n  mutate(contrast = str_replace_all(contrast, \"\\\\s\", \"\")) %>% \n  pull(adj.p.value, contrast) %>% \n  multcompLetters() \n\nWir erhalten dann folgendes compact letter display für die paarweisen Vergleiche aus multcomp.\n\nmultcomp_cld \n\nneg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F  treat_G \n     \"a\"      \"a\"      \"a\"      \"b\"     \"bc\"     \"cd\"      \"d\"      \"e\" \npos_crtl \n     \"f\" \n\n\nLeider sind diese Buchstaben in dieser Form schwer zu verstehen. Deshalb gibt es noch die Funktion plot() in dem Paket multcompView um uns die Buchstaben mit den Leveln der Behandlung einmal ausgeben zu lassen. Wir erhalten dann folgende Abbildung.\n\nmultcomp_cld %>% plot\n\n\n\n\nIn dem compact letter display bedeuten gleiche Buchstaben, dass die Behandlungen gleich sind. Es gilt die Nullhypothese für diesen Vergleich.\nWas sehen wir hier? Kombinieren wir einmal das compact letter display mit den Leveln der Behandlung und den Mittelwerten der Behandlungen in einer Tabelle 29.5. Wenn die Mittelwerte gleich sind, dann erhalten die Behandlungslevel den gleichen Buchstaben. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein a. Ebenso sind die MIttelwerte von treat_C und treat_D nahezu gleich, dehalb erhalten beide ein b. Das machen wir immer so weiter und konzentrieren uns also auf die nicht signifikanten Ergebnisse. Denn gleiche Buchstaben bedeuten, dass die Behandlungen gleich sind. Wir sehen hier also, bei welchen Vergleichen die Nullhypothese gilt.\n\n\nTabelle 29.5— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus multcomp. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\nneg_ctrl\n20\na\n\n\n\n\n\n\n\ntreat_A\n22\na\n\n\n\n\n\n\n\ntreat_B\n24\na\n\n\n\n\n\n\n\ntreat_C\n35\n\nb\n\n\n\n\n\n\ntreat_D\n37\n\nb\nc\n\n\n\n\n\ntreat_E\n40\n\n\nc\nd\n\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\npos_crtl\n10\n\n\n\n\n\nf\n\n\n\n\nWir können dann die Buchstaben auch in den Boxplot ergaänzen. Die y-Position kann je nach Belieben dann noch angepasst werden. zum Beispiel könnten hier auch die Mittelwerte aus einer summarise() Funktion ergänzt werden und so die y-Position angepasst werden.\n\nletters_tbl <- multcomp_cld$Letters %>% \n  enframe(\"trt\", \"label\") %>% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 29.9— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n\n29.7.2 … für das Paket emmeans\n\nIn dem Paket emmeans ist das compact letter display ebenfalls implementiert und wir müssen nicht die Funktion multcompLetters() nutzen. Durch die direkte Implementierung ist es etwas einfacher sich das compact letter display anzeigen zu lassen. Das Problem ist dann später sich die Buchstaben zu extrahieren um die Abbildung 29.10 zu ergänzen. Wir nutzen in emmeans die Funktion cld() um das compact letter display zu erstellen.\n\nemmeans_cld <- lm(rsp  ~ trt, data = data_tbl) %>%\n  emmeans(~ trt) %>%\n  cld(Letters = letters, adjust = \"bonferroni\")\n\nWir erhalten dann die etwas besser sortierte Ausgabe für die Behandlungen wieder.\n\nemmeans_cld \n\n trt      emmean   SE  df lower.CL upper.CL .group \n pos_crtl   9.67 1.12 171     6.51     12.8  a     \n neg_ctrl  20.02 1.12 171    16.86     23.2   b    \n treat_A   20.97 1.12 171    17.81     24.1   b    \n treat_B   23.35 1.12 171    20.19     26.5   b    \n treat_C   34.96 1.12 171    31.80     38.1    c   \n treat_D   37.46 1.12 171    34.30     40.6    cd  \n treat_E   40.17 1.12 171    37.01     43.3     de \n treat_F   43.23 1.12 171    40.07     46.4      e \n treat_G   50.51 1.12 171    47.35     53.7       f\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 9 estimates \nP value adjustment: bonferroni method for 36 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping letter,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWie die Ausgabe von cld() richtig anmerkt, können compact letter display irreführend sein weil sie eben Nicht-Unterschiede anstatt von signifikanten Unterschieden anzeigen. Zum Anderen sehen wir aber auch, dass wir 36 statistische Tests gerechnet haben und somit zu einem Signifikanzniveau von \\(\\cfrac{\\alpha}{k} = \\cfrac{0.05}{36} \\approx 0.0014\\) testen. Wir brauchen also schon sehr große Unterschiede oder aber eine sehr kleine Streuung um hier signifikante Effekte nachweisen zu können.\nIn Tabelle 29.6 sehen wir das Ergebnis des compact letter display nochmal mit den Mittelwerten der Behandlungslevel zusammen dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. Wir gehen so alle Vergleiche einmal durch.\n\n\nTabelle 29.6— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus emmeans. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\n\ntreat_E\n40\n\n\n\nd\ne\n\n\n\ntreat_F\n43\n\n\n\n\ne\n\n\n\ntreat_G\n45\n\n\n\n\n\nf\n\n\n\n\nAbschließend können wir die Buchstaben aus dem compact letter display noch in die Abbildung 29.10 ergänzen. Hier müssen wir etwas mehr machen um die Buchstaben aus dem Objekt emmeans_cld zu bekommen. Du kannst dann noch die y-Position anpassen wenn du möchtest.\n\nletters_tbl <- emmeans_cld %>% \n  tidy %>% \n  select(trt, label = .group) %>% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 29.10— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n\n29.7.3 … für den Games-Howell-Test\nAbschließend wollen wir uns die Implementierung des compact letter display für den Games-Howell-Test einmal anschauen. Es gilt vieles von dem in diesem Abschnitt schon gesagtes. Wir nutzen die Funktion multcompLetters() aus dem Paket multcompView um uns das compact letter display aus dem Games-Howell-Test wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nght_cld <- games_howell_test(rsp ~ trt, data = data_tbl) %>% \n  mutate(contrast = str_c(group1, \"-\", group2)) %>% \n  pull(p.adj, contrast) %>% \n  multcompLetters() \n\nDas compact letter display kennen wir schon aus der obigen Beschreibung.\n\nght_cld\n\npos_crtl neg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F \n     \"a\"      \"b\"      \"b\"      \"b\"      \"c\"     \"cd\"      \"d\"      \"d\" \n treat_G \n     \"e\" \n\n\nWir können uns dann auch das compact letter display als übersichtlicheren Plot wiedergeben lassen.\n\nght_cld %>% plot\n\n\n\n\nUm die Zusammenhänge besser zu verstehen ist in Tabelle 29.7 nochmal die Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. In der Form können wir alle Vergleiche einmal durchgehen.\n\n\nTabelle 29.7— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus dem Games-Howell-Test. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\ntreat_E\n40\n\n\n\nd\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\n\nWir können dann auch in Abbildung 29.11 sehen, wie das compact letter display mit den Boxplots verbunden wird.\n\nletters_tbl <- ght_cld$Letters %>% \n  enframe(\"trt\", \"label\") %>% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 29.11— Boxplot der Beispieldaten zusammen mit den compact letter display."
  },
  {
    "objectID": "stat-linear-reg-preface.html",
    "href": "stat-linear-reg-preface.html",
    "title": "Grundlagen der linearen Regression",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:39:09\nIn diesem Kapitel wollen wir uns mit den Grundlagen der simplen linearen Regression beschäftigen. Das heißt, wir haben ein Outcome \\(y\\), was normalverteilt ist, sowie eine Einflussvariable \\(x_1\\), die eine kontinuierliche Variable ist. Wir wollen jetzt herausfinden, welchen Einfluss oder Effekt das \\(x_1\\) auf das \\(y\\) hat. Sehr simple Gesprochen legen wir eine Gerade durch eine Punktewolke.\nViele der Konzepte hier brauchen wir in dem Kapitel zum statistischen Modellieren. Wir lernen hier also eher die Aufwärmübungen und Konzepte um dann multiple lineare Regressionen rechnen zu können. In den seltensten Fällen reichen simple lineare Modell aus, um die Realität abzubilden.\nFangen wir also mit den Grundlagen an und bauen dann systematisch die Konzepte der simplen linearen Regression auf."
  },
  {
    "objectID": "stat-linear-reg-preface.html#simple-lineare-regression",
    "href": "stat-linear-reg-preface.html#simple-lineare-regression",
    "title": "Grundlagen der linearen Regression",
    "section": "Simple lineare Regression",
    "text": "Simple lineare Regression\nIm Kapitel 30 wollen wir einmal die Grundlagen der linearen Regression an der simplen linearen Regression erarbeiten. Wir benötigen die Formeln und die Worte für das weitere Modellieren multipler lineare Modelle.\nKausales Modell\nSchon gleich hier vorweg, es gibt einen Unterschied zwischen einem kausalen und einem prädiktiven Modell. In diesem kurzen Kapitel 30.4.1 schauen wir uns das kausale Modell einmal an. Wenn wir eine klassische Regression rechnen, rechnen wir meist ein kausales Modell.\nPrädiktives Modell\nDie Vorhersage oder Prädiktion ist ein eigenes Kapitel wert. Im Bereich des maschinellen Lernens wird die Prädiktion auch Klassifikation genannt. Wir schauen uns in diesem Kapitel 30.4.2 nur die Grundlage einmal an. Später nutzen wir auch maschinelle Lernverfahren für die Klassifikation."
  },
  {
    "objectID": "stat-linear-reg-preface.html#maßzahlen-der-modelgüte",
    "href": "stat-linear-reg-preface.html#maßzahlen-der-modelgüte",
    "title": "Grundlagen der linearen Regression",
    "section": "Maßzahlen der Modelgüte",
    "text": "Maßzahlen der Modelgüte\nNachdem wir ein Modell mathematisch gerechnet haben, müssen wir herausfinden, ob das Modell auch gut funktioniert hat. In Kapitel 31 schauen wir uns verschiedene Maßzahlen an um zu sehen, ob die Wahl des mathematischen Algorithmus auch zu unseren Daten passt. Rechnen können wir alles, aber ob die Rechnung Sinn ergibt müssen wir erst herausfinden."
  },
  {
    "objectID": "stat-linear-reg-preface.html#korrelation",
    "href": "stat-linear-reg-preface.html#korrelation",
    "title": "Grundlagen der linearen Regression",
    "section": "Korrelation",
    "text": "Korrelation\nDie simple lineare Regression und Korrelation sind eng miteinander verwandt. In Kapitel 32 schauen wir uns einmal die beiden Korrelationskoeffizienten nach Pearson und Spearman einmal an."
  },
  {
    "objectID": "stat-linear-reg-basic.html",
    "href": "stat-linear-reg-basic.html",
    "title": "30  Simple lineare Regression",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:39:11\nin diesem Kapitel wollen wir uns mit den Grundlagen der linearen Regression beschäftigen. Damit meine ich erstmal die Idee eine Gerade durch eine Punktewolke zu zeichnen. Das ist erstmal die simpleste Anwendung. Wir lernen hier einmal die Grundbegriffe und erweitern diese dann auf komplexere Modelle.\nDu kanst duch aber davon gedanklich lösen, dass die lineare Regression nur eine Methode ist um eine Gerade durch eine Punktewolke zu legen. Die lineare Regression und damit auch das statistische Modellieren kann viel mehr."
  },
  {
    "objectID": "stat-linear-reg-basic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-linear-reg-basic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "30  Simple lineare Regression",
    "section": "\n30.1 Genutzte R Pakete für das Kapitel",
    "text": "30.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               readxl)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-linear-reg-basic.html#daten",
    "href": "stat-linear-reg-basic.html#daten",
    "title": "30  Simple lineare Regression",
    "section": "\n30.2 Daten",
    "text": "30.2 Daten\nWir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die lineare Regression auf sehr großen Datensätzen anwenden, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit \\(n=7\\) Beobachtungen. In der Tabelle 30.1 ist der Datensatz simplel_tbl dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in [cm] und dem Gewicht in [mg] für sieben Beobachtungen modellieren.\n\n\n\n\nTabelle 30.1— Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight.\n\njump_length\nweight\n\n\n\n1.2\n0.8\n\n\n1.8\n1.0\n\n\n1.3\n1.2\n\n\n1.7\n1.9\n\n\n2.6\n2.0\n\n\n1.8\n2.7\n\n\n2.7\n2.8\n\n\n\n\n\n\nIn Abbildung 30.1 sehen wir die Visualisierung der Daten simple_tbl in einem Scatterplot mit einer geschätzen Gerade.\n\n\n\n\nAbbildung 30.1— Scatterplot der Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Die Gerade verläuft mittig durch die Punkte.\n\n\n\n\nWir schauen uns in diesem Kapitel nur eine simple lineare Regression mit einem \\(x_1\\) an. In unserem Fall ist das \\(x_1\\) gleich dem weight. Später schauen wir dann multiple lineare Regressionen mit mehreren \\(x_1,..., x_p\\) an.\nBevor wir mit dem Modellieren anfangen können, müssen wir verstehen, wie ein simples Modell theoretisch aufgebaut ist. Danach können wir uns das lineare Modell in R anschauen."
  },
  {
    "objectID": "stat-linear-reg-basic.html#simple-lineare-regression-theoretisch",
    "href": "stat-linear-reg-basic.html#simple-lineare-regression-theoretisch",
    "title": "30  Simple lineare Regression",
    "section": "\n30.3 Simple lineare Regression theoretisch",
    "text": "30.3 Simple lineare Regression theoretisch\nWir haben nun die ersten sieben Beobachtungen in dem Objekt simple_tbl vorliegen. Wie sieht nun theoretisch eine lineare Regression aus? Wir wollen eine Grade durch Punkte legen, wie wie wir es in Abbildung 30.1 sehen. Die blaue Gerade wir durch eine Geradengleichung beschreiben. Du kenst vermutlich noch die Form \\(y = mx + b\\). In der Statistik beschreiben wir eine solche Gerade aber wie folgt.\n\\[\ny \\sim \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nmit\n\n\n\\(\\beta_0\\) als den y-Achsenabschnitt.\n\n\\(\\beta_1\\) als der Steigung der Geraden.\n\n\\(\\epsilon\\) als Residuen oder die Abweichungen von den \\(y\\)-Werten auf Geraden zu den einzelnen \\(y\\)-Werten der Beobachtungen.\n\nIn Tabelle 30.2 siehst du nochmal in einer Tabelle den Vergleich von der Schreibweise der linearen Regression in der Schule und in der Statistik. Darüber hinaus sind die deutschen Begriffe den englischen Begriffen gegenüber gestellt. Warum schreiben wir die Gleichung in der Form? Damit wir später noch weitere \\(\\beta_px_p\\)-Paare ergänzen könen und so multiple Modelle bauen können.\n\n\nTabelle 30.2— Vergleich und Übersicht der schulischen vs. statistischen Begriffe in den linearen Regression sowie die deutschen und englischen Begriffe.\n\n\n\n\n\n\n\n\\(\\boldsymbol{y = mx +b}\\)\n\\(\\boldsymbol{y \\sim \\beta_0 + \\beta_1 x_1 + \\epsilon}\\)\nDeutsch\nEnglisch\n\n\n\n\\(m\\)\n\\(\\beta_1\\)\nSteigung\nSlope\n\n\n\\(x\\)\n\\(x_1\\)\nEinflussvariable\nRisk factor\n\n\n\\(b\\)\n\\(\\beta_0\\)\ny-Achsenabschnitt\nIntercept\n\n\n\n\\(\\epsilon\\)\nResiduen\nResidual\n\n\n\n\nIn Abbildung 30.3 sehen wir die Visualisierung der Gleichung in einer Abbildung. Die Gerade läuft durch die Punktewolke und wird durch die statistischen Maßzahlen bzw. Parameter \\(\\beta_0\\), \\(\\beta_1\\) sowie den \\(\\epsilon\\) beschrieben. Wir sehen, dass das \\(\\beta_0\\) den Intercept darstellt und das \\(\\beta_1\\) die Steigung der Geraden. Wenn wir \\(x\\) um 1 Einheit erhöhen \\(x+1\\), dann steigt der \\(y\\) Wert um den Wert von \\(\\beta_1\\). Die einzelnen Abweichungen der beobachteten \\(y\\)-Wert zu den \\(y\\)-Werten auf der Gerade (\\(\\hat{y}\\)) werden als Residuen oder auch \\(\\epsilon\\) bezeichnet.\n\n\nAbbildung 30.2— Visualisierung der linearen Regression. Wir legen eine Gerade durch eine Punktewolke. Die Gerade wird durch die statistischen Maßzahlen bzw. Parameter \\(\\beta_0\\), \\(\\beta_1\\) sowie den \\(\\epsilon\\) beschrieben.\n\n\nIn R werden die \\(\\hat{y}\\) auch fitted values genannt. Die \\(\\epsilon\\) Werte werden dann residuals bezeichnet.\nSchauen wir uns einmal den Zusammenhang von \\(y\\), den beobachteten Werten, und \\(\\hat{y}\\), den geschätzen Werten auf der Gerade in unserem Beispiel an. In Tabelle 30.3 sehen wir die Berechnung der einzelnen Residuen für die Gerade aus der Abbildung 30.1. Wir nehmen jedes beobachtete \\(y\\) und ziehen den Wert von \\(y\\) auf der Gerade, bezeichnet als \\(\\hat{y}\\), ab. Diesen Schritt machen wir für jedes Wertepaar \\((y_i; \\hat{y}_i)\\).\n\n\n\nTabelle 30.3— Zusammenhang zwischen den \\(y\\), den beobachteten Werten, und \\(\\hat{y}\\), den geschätzen Werten auf der Gerade. Wir nennen den Abstand \\(y_i - \\hat{y}_i\\) auch Residuum oder Epsilon \\(\\epsilon\\).\n\n\n\n\n\n\n\n\nx\ny\n\\(\\boldsymbol{\\hat{y}}\\)\nResiduen (\\(\\boldsymbol{\\epsilon}\\))\nWert\n\n\n\n0.8\n1.2\n1.38\n\\(\\epsilon_1 = y_1 - \\hat{y}_1\\)\n\\(\\epsilon_1 = 1.2 - 1.38 = -0.18\\)\n\n\n1.0\n1.8\n1.48\n\\(\\epsilon_2 = y_2 - \\hat{y}_2\\)\n\\(\\epsilon_2 = 1.8 - 1.48 = +0.32\\)\n\n\n1.2\n1.3\n1.58\n\\(\\epsilon_3 = y_3 - \\hat{y}_3\\)\n\\(\\epsilon_3 = 1.3 - 1.58 = -0.28\\)\n\n\n1.9\n1.7\n1.94\n\\(\\epsilon_4 = y_4 - \\hat{y}_4\\)\n\\(\\epsilon_4 = 1.7 - 1.94 = -0.24\\)\n\n\n2.0\n2.6\n1.99\n\\(\\epsilon_5 = y_5 - \\hat{y}_5\\)\n\\(\\epsilon_5 = 2.6 - 1.99 = +0.61\\)\n\n\n2.7\n1.8\n2.34\n\\(\\epsilon_6 = y_6 - \\hat{y}_6\\)\n\\(\\epsilon_6 = 1.8 - 2.34 = -0.54\\)\n\n\n2.8\n2.7\n2.40\n\\(\\epsilon_7 = y_7 - \\hat{y}_7\\)\n\\(\\epsilon_7 = 2.7 - 2.40 = +0.30\\)\n\n\n\n\n\n\n\n\nIn R wird in Modellausgaben die Standardabweichung der Residuen \\(s_{\\epsilon}\\) als sigma bezeichnet.\nDie Abweichungen \\(\\epsilon\\) oder auch Residuen genannt haben einen Mittelwert von \\(\\bar{\\epsilon} = 0\\) und eine Varianz von \\(s^2_{\\epsilon} = 0.17\\). Wir schreiben, dass die Residuen normalverteilt sind mit \\(\\epsilon \\sim \\mathcal{N}(0, s^2_{\\epsilon})\\). Wir zeichnen die Gerade also so durch die Punktewolke, dass die Abstände zu den Punkten, die Residuen, im Mittel null sind. Die Optimierung erreichen wir in dem wir die Varianz der Residuuen minimieren. Folglich modellieren wir die Varianz."
  },
  {
    "objectID": "stat-linear-reg-basic.html#simples-lineare-regression-in-r",
    "href": "stat-linear-reg-basic.html#simples-lineare-regression-in-r",
    "title": "30  Simple lineare Regression",
    "section": "\n30.4 Simples lineare Regression in R",
    "text": "30.4 Simples lineare Regression in R\nIm Allgemeinen können wir ein Modell in R wie folgt schreiben. Wir brauchen ein y auf der linken Seite und in der simplen linearen Regressione ein \\(x\\) auf der rechten Seite der Gleichung. Wir brauchen also zwei Variablen \\(y\\) und \\(x\\), die natürlich nicht im Datensatz in R so heißen müssen.\n\n\nAbbildung 30.3— Modellschreibweise \\(y\\) hängt ab von \\(x\\). Das \\(y\\) repräsentiert eine Spalte im Datensatz und das \\(x\\) repräsentiert ebenso eine Spalte im Datensatz.\n\n\nKonkret würden wir in unserem Beispiel das Modell wie folgt benennen. Das \\(y\\) wird zu jump_length und das \\(x\\) wird zu weight. Wir haben dann das Modell in der simplesten Form definiert.\n\n\nAbbildung 30.4— Modellschreibweise bzw. formula Schreibweise in R. Die Variable \\(y\\) hängt ab von \\(x\\) am Beispiel des Datensatzes simple_tbl mit den beiden Variablen jump_length als \\(y\\) und weight als \\(x\\).\n\n\nNachdem wir das Modell definiert haben, setzen wir dieses Modell jump_length ~ weight in die Funktion lm() ein um das lineare Modell zu rechnen. Wie immer müssen wir auch festlegen aus welcher Datei die Spalten genommen werden sollen. Das machen wir mit der Option data = simple_tbl. Wir speichern dann die Ausgabe der Funktion lm() in dem Objekt fit_1 damit wir die Ausgabe noch in andere Funktionen pipen können.\n\nfit_1 <- lm(jump_length ~ weight, data = simple_tbl)\n\nAn dieser Stelle kannst du schnell in das Problem der Antwort auf alles kommen: “42”\nWir können jetzt mir dem Modell drei Dinge tun. Abhängig von der Fragestellung liefert uns natürlich jedes der drei Möglichkeiten eine andere Antwort.\n\nWir rechnen mit dem Fit des Modells eine ANOVA (siehe Kapitel 22)\nWir rechnen ein kausales Modell, uns interessieren die Effekte (siehe Kapitel 30.4.1)\nWir rechnen ein prädiktives Modell, uns interessiert der Wert neuer Werte (siehe Kapitel 30.4.2)\n\n\n30.4.1 Kausales Modell\nIm Folgenden rechnen wir ein kausales Modell, da wir an dem Effekt des \\(x\\) interessiert sind. Wenn also das \\(x_1\\) um eine Einheit ansteigt, um wie viel verändert sich dann das \\(y\\)? Der Schätzer \\(\\beta_1\\) gibt uns also den Einfluss oder den kausalen Zusammenhang zwischen \\(y\\) und \\(x_1\\) wieder.\nDie Funktion summary() gibt dir das Ergebnis eines kausalen Modells wieder\nIm ersten Schritt schauen wir uns die Ausgabe der Funktion lm() in der Funktion summary() an. Daher pipen wir das Objekt fit_1 in die Funktion summary().\n\nfit_1 %>% summary\n\nWir erhalten folgende Ausgabe dargestellt in Abbildung 30.5.\n\n\nAbbildung 30.5— Die summary() Ausgabe des Modells fit_1.\n\n\nWas sehen wir in der Ausgabe der summary() Funktion? Als erstes werden uns die Residuen wiedergegeben. Wenn wir nur wenige Beobachtungen haben, dann werden uns die Residuen direkt wiedergegeben, sonst die Verteilung der Residuen. Mit der Funktion augment() aus dem R Paket broom können wir uns die Residuen wiedergeben lassen. Die Residuen schauen wir uns aber nochmal im Kapitel 31 genauer an.\n\nfit_1 %>% augment\n\n# A tibble: 7 × 8\n  jump_length weight .fitted .resid  .hat .sigma .cooksd .std.resid\n        <dbl>  <dbl>   <dbl>  <dbl> <dbl>  <dbl>   <dbl>      <dbl>\n1         1.2    0.8    1.38 -0.176 0.388  0.496  0.0778     -0.496\n2         1.8    1      1.48  0.322 0.297  0.471  0.151       0.844\n3         1.3    1.2    1.58 -0.280 0.228  0.483  0.0725     -0.701\n4         1.7    1.9    1.94 -0.237 0.147  0.492  0.0275     -0.564\n5         2.6    2      1.99  0.612 0.156  0.384  0.199       1.47 \n6         1.8    2.7    2.34 -0.545 0.367  0.376  0.656      -1.51 \n7         2.7    2.8    2.40  0.304 0.417  0.467  0.276       0.877\n\n\nIm zweiten Block erhalten wir die Koeffizienten (eng. coefficients) der linearen Regression. Das heißt, wir kriegen dort \\(\\beta_0\\) als y-Achsenabschnitt sowie die Steigung \\(\\beta_1\\) für das Gewicht. Dabei ist wichtig zu wissen, dass immer als erstes der y-Achsenabschnitt (Intercept) auftaucht. Dann die Steigungen der einzelnen \\(x\\) in dem Modell. Wir haben nur ein kontinuierliches \\(x\\), daher ist die Interpretation der Ausgabe einfach. Wir können die Gradengleichung wie folgt formulieren.\n\\[\njump\\_length \\sim 0.97 + 0.51 \\cdot weight\n\\]\nWas heißt die Gleichung nun? Wenn wir das \\(x\\) um eine Einheit erhöhen dann verändert sich das \\(y\\) um den Wert von \\(\\beta_1\\). Wir haben hier eine Steigung von \\(0.51\\) vorliegen. Ohne Einheit keine Interpretation! Wir wissen, dass das Gewicht in [mg] gemessen wurde und die Sprungweite in [cm]. Damit können wir aussagen, dass wenn ein Floh 1 mg mehr wiegt der Floh 0.51 cm weiter springen würde.\nSchauen wir nochmal in die saubere Ausgabe der tidy() Funktion. Wir sehen nämlich noch einen \\(p\\)-Wert für den Intercept und die Steigung von weight.\n\nfit_1 %>% tidy\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    0.969     0.445      2.18  0.0813\n2 weight         0.510     0.232      2.20  0.0790\n\n\nWenn wir einen \\(p\\)-Wert sehen, dann brauchen wir eine Nullhypothese, die wir dann eventuell mit der Entscheidung am Signifikanzniveau \\(\\alpha\\) von 5% ablehnen können. Die Nullhypothese ist die Gleichheitshypothese. Wenn es also keinen Effekt von dem Gewicht auf die Sprungweite gebe, wie groß wäre dann \\(\\beta_1\\)? Wir hätten dann keine Steigung und die Grade würde parallel zur x-Achse laufen. Das \\(\\beta_1\\) wäre dann gleich null.\n\\[\n\\begin{align*}\nH_0: \\beta_i &= 0\\\\  \nH_A: \\beta_i &\\neq 0 \\\\   \n\\end{align*}\n\\]\nWir haben für jedes \\(\\beta_i\\) ein eigenes Hypothesenpaar. Meistens interessiert uns der Intercept nicht. Ob der Intercept nun durch die Null geht oder nicht ist eher von geringem Interessen.\nSpannder ist aber wie sich der \\(p\\)-Wert berechnet. Der \\(p\\)-Wert basiert auf einer t-Statistik, also auf dem t-Test. Wir rechnen für jeden Koeffizienten \\(\\beta_i\\) einen t-Test. Das machen wir in dem wir den Koeffizienten estimate durch den Fehler des Koeffizienten std.error teilen.\n\\[\n\\begin{align*}\nT_{(Intercept)} &= \\cfrac{\\mbox{estimate}}{\\mbox{std.error}}  = \\cfrac{0.969}{0.445} = 2.18\\\\  \nT_{weight} &= \\cfrac{\\mbox{estimate}}{\\mbox{std.error}}  = \\cfrac{0.510}{0.232} = 2.20\\\\   \n\\end{align*}\n\\]\nWir sehen in diesem Fall, dass weder der Intercept noch die Steigung von weight signifikant ist, da die \\(p\\)-Werte mit \\(0.081\\) und \\(0.079\\) leicht über dem Signifikanzniveau von \\(\\alpha\\) gleich 5% liegen. Wir haben aber einen starkes Indiz gegen die Nullhypothese, da die Wahrscheinlichkeit die Daten zu beobachten sehr gering ist unter der Annahme das die Nullhypothese gilt.\nZun Abschluß noch die Funktion glance() ebenfalls aus dem R Paket broom, die uns erlaubt noch die Qualitätsmaße der linearen Regression zu erhalten. Wir müssen nämlich noch schauen, ob die Regression auch funktioniert hat. Die Überprüfung geht mit einem \\(x\\) sehr einfach. Wir können uns die Grade ja anschauen. Das geht dann mit einem Model mit mehreren \\(x\\) nicht mehr und wir brauchen andere statistsiche Maßzahlen.\n\nfit_1 %>% glance \n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.492         0.391 0.455      4.84  0.0790     1  -3.24  12.5  12.3\n# … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\n30.4.2 Prädiktives Modell\nNeben dem kausalen Modell gibt es auch die Möglichkeit ein prädiktives Modell zu rechnen. Im Prinzip ist die Sprache hier etwas ungenau. Wir verwenden das gefittete Modell nur anders. Anstatt das Modell fit_1 in die Funktion summary() zu pipen, pipen wir die das Modell in die Funktion predict(). Die Funktion predict() kann dann für neue Daten über die Option newdata = das \\(y\\) vorhersagen.\nIn unserem Fall müssen wir uns deshalb ein tibble mit einer Spalte bauen. Wir haben ja oben im Modell auch nur ein \\(x_1\\) mit aufgenommen. Später können wir natürlich auch für multiple Modelle die Vorhersage machen. Wichtig ist, dass die Namen gleich sind. Das heißt in dem neuen Datensatz müssen die Spalten exakt so heißen wir in dem alten Datensatz in dem das Modell gefittet wurde.\n\nsimple_new_tbl <- tibble(weight = c(1.7, 1.4, 2.1, 3.0)) \n\npredict(fit_1, newdata = simple_new_tbl) %>% round(2)\n\n   1    2    3    4 \n1.84 1.68 2.04 2.50 \n\n\nWie wir sehen ist die Anwendung recht einfach. Wir haben die vier jump_length Werte vorhergesagt bekommen, die sich mit dem Fit des Modells mit den neuen weight Werten ergeben.\nIn Abbildung 30.6 sehen wir die Visualisierung der vier vorhergesagten Werte. Die Werte müssen auf der Geraden liegen.\n\n\n\n\nAbbildung 30.6— Scatterplot der alten Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Sowie der neuen vorhergesagten Beobachtungen auf der Geraden.\n\n\n\n\nWir werden später in der Klassiifkation, der Vorhersage von \\(0/1\\)-Werten, sowie in der multiplen Regression noch andere Prädktionen und deren Maßzahlen kennenlernen. Im Rahmen der simplen Regression soll dies aber erstmal hier genügen."
  },
  {
    "objectID": "stat-linear-reg-quality.html",
    "href": "stat-linear-reg-quality.html",
    "title": "31  Maßzahlen der Modelgüte",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:39:16\nWir interpretieren keine der Gütekriterien und statistischen Maßzahlen alleine sondern in der Gesamtheit. Es gibt eine Reihe von Maßzahlen für die Güte eines Modells, wir schauen uns hier einige an. Später werden wir uns noch andere Maßzahlen anschauen, wenn wir eine multiple lineare Regression rechnen. Das R Paket performance werden wir später auch nutzen um die notwendigen Gütekriterien zu erhalten.\nWir wollen eine Gerade durch Punkte legen. Deshalb müssen wir folgende Fragen klären:"
  },
  {
    "objectID": "stat-linear-reg-quality.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-linear-reg-quality.html#genutzte-r-pakete-für-das-kapitel",
    "title": "31  Maßzahlen der Modelgüte",
    "section": "\n31.1 Genutzte R Pakete für das Kapitel",
    "text": "31.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-linear-reg-quality.html#daten",
    "href": "stat-linear-reg-quality.html#daten",
    "title": "31  Maßzahlen der Modelgüte",
    "section": "\n31.2 Daten",
    "text": "31.2 Daten\nNachdem wir uns im vorherigen Kapitel mit einem sehr kleinen Satensatz beschäftigt haben, nehmen wir einen großen Datensatz. Bleiben aber bei einem simplen Modell. Wir brauchen dafür den Datensatz flea_dog_cat_length_weight.xlsx. In einer simplen linearen Regression schauen wir uns den Zusammenhang zwischen einem \\(y\\) und einem \\(x_1\\) an. Daher wählen wir aus dem Datensatz die beiden Spalten jump_length und weight. Wir wollen nun feststellen, ob es einen Zusammenhang zwischen der Sprungweite in [cm] und dem Flohgewicht in [mg] gibt. In dem Datensatz finden wir 400 Flöhe von Hunden und Katzen.\n\nmodel_tbl <- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %>%\n  select(animal, jump_length, weight)\n\nIn der Tabelle 37.1 ist der Datensatz model_tbl nochmal dargestellt.\n\n\n\n\nTabelle 31.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\nanimal\njump_length\nweight\n\n\n\ncat\n15.79\n6.02\n\n\ncat\n18.33\n5.99\n\n\ncat\n17.58\n8.05\n\n\ncat\n14.09\n6.71\n\n\ncat\n18.22\n6.19\n\n\ncat\n13.49\n8.18\n\n\ncat\n16.28\n7.46\n\n\n\n\n\n\nIm Folgenden ignorieren wir, dass die Sprungweiten und die Gewichte der Flöhe auch noch von den Hunden oder Katzen sowie dem unterschiedlichen Geschlecht der Flöhe abhängen könnten. Wir schmeißen alles in einen Pott und schauen nur auf den Zusammenhang von Sprungweite und Gewicht."
  },
  {
    "objectID": "stat-linear-reg-quality.html#das-simple-lineare-modell",
    "href": "stat-linear-reg-quality.html#das-simple-lineare-modell",
    "title": "31  Maßzahlen der Modelgüte",
    "section": "\n31.3 Das simple lineare Modell",
    "text": "31.3 Das simple lineare Modell\nWir fitten ein simples lineares Modell mit nur einem Einflussfaktor weight auf die Sprunglänge jump_length. Wir erhalten dann das Objekt fit_1 was wir dann im Weiteren nutzen werden.\n\nfit_1 <- lm(jump_length ~ weight, data = model_tbl)\n\nWir nutzen jetzt dieses simple lineare Modell für die weiteren Gütekritierien."
  },
  {
    "objectID": "stat-linear-reg-quality.html#sec-linreg-residual",
    "href": "stat-linear-reg-quality.html#sec-linreg-residual",
    "title": "31  Maßzahlen der Modelgüte",
    "section": "\n31.4 Residualplot",
    "text": "31.4 Residualplot\nIn R wird in Modellausgaben die Standardabweichung der Residuen \\(s_{\\epsilon}\\) als sigma bezeichnet.\nWir wollen mit dem Residualplot die Frage beantworten, ob die Gerade mittig durch die Punktewolke läuft. Die Residuen \\(\\epsilon\\) sollen normalverteilt sein mit einem Mittelwert von Null \\(\\epsilon \\sim \\mathcal{N}(0, s^2_{\\epsilon})\\).\nWir erhalten die Residuen resid und die angepassten Werte .fitted auf der Geraden über die Funktion augment(). Die Funktion augment() gibt noch mehr Informationen wieder, aber wir wollen uns jetzt erstmal auf die Residuen konzentrieren.\n\nresid_plot_tbl <- fit_1 %>% \n  augment() %>% \n  select(.fitted, .resid)\n\nresid_plot_tbl %>% \n  head(5)\n\n# A tibble: 5 × 2\n  .fitted .resid\n    <dbl>  <dbl>\n1    17.9 -2.09 \n2    17.8  0.489\n3    20.6 -3.03 \n4    18.8 -4.72 \n5    18.1  0.110\n\n\nDie Daten selber interssieren uns nicht einer Tabelle. Stattdessen zeichen wir einmal den Residualplot. Bei dem Residualplot tragen wir die Werte der Residuen .resid auf die y-Achse auf und die angepassten y-Werte auf der Geraden .fitted auf die x-Achse. Wir kippen im Prinzip die gefittete Gerade so, dass die Gerade parallel zu x-Achse läuft.\n\nggplot(resid_plot_tbl, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  theme_bw()\n\n\n\nAbbildung 31.1— Residualplot der Residuen des Models fit_1. Die rote Linie stellt die geschätzte Gerade da. Die Punkte sollen gleichmäßig und ohne eine Struktur um die Gerade verteilt sein.\n\n\n\n\nThe residual plot should look like the sky at night, with no pattern of any sort.\nIn Abbildung 31.1 sehen wir den Residualplot von unseren Beispieldaten. Wir sehen, dass wir keine Struktur in der Punktewolke erkennen. Auch sind die Punkte gleichmäßig um die Gerade verteilt. Wir haben zwar einen Punkt, der sehr weit von der Gerade weg ist, das können wir aber ignorieren. Später können wir uns noch überlegen, ob wir einen Ausreißer (eng. outlier) vorliegen haben."
  },
  {
    "objectID": "stat-linear-reg-quality.html#sec-linreg-bestimmt",
    "href": "stat-linear-reg-quality.html#sec-linreg-bestimmt",
    "title": "31  Maßzahlen der Modelgüte",
    "section": "\n31.5 Bestimmtheitsmaß \\(R^2\\)\n",
    "text": "31.5 Bestimmtheitsmaß \\(R^2\\)\n\nNachdem wir nun wissen wie gut die Gerade durch die Punkte läuft, wollen wir noch bestimmen wie genau die Punkte auf der Geraden liegen. Das heißt wir wollen mit dem Bestimmtheitsmaß \\(R^2\\) ausdrücken wie stark die Punkte um die Gerade variieren. Wir können folgende Aussage über das Bestimmtheitsmaß \\(R^2\\) treffen. Die Abbildung 31.2 visualisiert nochmal den Zusammenhang.\n\nwenn alle Punkte auf der Geraden liegen, dann ist das Bestimmtheitsmaß \\(R^2\\) gleich 1.\nwenn alle Punkte sehr stark um die Gerade streuuen, dann läuft das Bestimmtheitsmaß \\(R^2\\) gegen 0.\n\n\n\nAbbildung 31.2— Visualisierung des Bestimmtheitsmaßes \\(R^2\\). Auf der linken Seite sehen wir eine perfekte Übereinstimmung der Punkte und der geschätzten Gerade. Wir haben ein \\(R^2\\) von 1 vorliegen. Sind die Punkte und die geschätzte Gerade nicht deckungsgleich, so läuft das \\(R^2\\) gegen 0.\n\n\nDa die Streuung um die Gerade auch gleichzeitig die Varianz wiederspiegelt, können wir auch sagen, dass wenn alle Punkte auf der Geraden liegen, die Varianz gleich Null ist. Die Einflussvariable \\(x_1\\) erklärt die gesamte Varianz, die durch die Beobachtungen verursacht wurde.\nDamit beschreibt das Bestimmtheitsmaß \\(R^2\\) auch den Anteil der Varianz, der durch die lineare Regression, daher der Graden, erklärt wird. Wenn wir ein Bestimmtheitsmaß \\(R^2\\) von Eins haben, wird die gesamte Varianz von unserem Modell erklärt. Haben wir ein Bestimmtheitsmaß \\(R^2\\) von Null, wird gar keine Varianz von unserem Modell erklärt. Damit ist ein niedriges Bestimmtheitsmaß \\(R^2\\) schlecht.\nWir können die Funktion glance() nutzen um uns das r.squared und das adj.r.squared wiedergeben zu lassen.\n\nfit_1 %>% \n  glance() %>% \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      <dbl>         <dbl>\n1     0.251         0.250\n\n\nWir nutzen grundsätzlich das adjustierte \\(R^2\\)adj.r.squared in der Anwendung.\nWir haben wir ein \\(R^2\\) von \\(0.31\\) vorliegen. Damit erklärt unser Modell bzw. die Gerade 31% der Varianz. Das ist jetzt nicht viel, aber wundert uns auch erstmal nicht. Wir haben ja die Faktoren animal und sex ignoriert. Beide Faktoren könnten ja auch einen Teil der Varianz erklären. Dafür müssten wir aber eine multiple lineare Regression mit mehren \\(x\\) rechnen.\nWenn wir eine multiple Regression rechnen, dann nutzen wir das adjustierte \\(R^2\\)adj.r.squared in der Anwendung. Das hat den Grund, dass das \\(R^2\\) automatisch ansteigt je mehr Variablen wir in das Modell nehmen. Jede neue Variable wird immer etwas erklären. Um dieses Überanpassen (eng. overfitting) zu vermeiden nutzen wir das adjustierte \\(R^2\\). Im Falle des adjustierte \\(R^2\\) wird ein Strafterm eingeführt, der das adjustierte \\(R^2\\) kleiner macht je mehr Einflussvariablen in das Modell aufgenommen werdenn."
  },
  {
    "objectID": "stat-linear-reg-quality.html#sec-linreg-qq",
    "href": "stat-linear-reg-quality.html#sec-linreg-qq",
    "title": "31  Maßzahlen der Modelgüte",
    "section": "\n31.6 QQ-Plot",
    "text": "31.6 QQ-Plot\n\n\n\n\n\n\nDas klingt hier alles etwas wage… Ja, das stimmt. Aber wir brauchen den QQ-Plot nur ganz kurz und wir müssten sehr viel Energei investieren um den QQ-Plot zu durchdringen. Deshalb hier die wage und grobe Darstellung.\nMit dem Quantile-Quantile Plot oder kurz QQ-Plot können wir überprüfen, ob unser \\(y\\) aus einer Normalverteilung stammt. Oder andersherum, ob unser \\(y\\) approximativ normalverteilt ist. Der QQ-Plot ist ein visuelles Tool. Daher musst du immer schauen, ob dir das Ergebnis passt oder die Abweichungen zu groß sind. Es hilft dann manchmal die Daten zum Beispiel einmal zu \\(log\\)-Transformieren und dann die beiden QQ-Plots miteinander zu vergleichen.\nWir brauchen für einen QQ-Plot viele Beobachtungen. Das heißt, wir brauchen auf jeden Fall mehr als 20 Beobachtungen. Dann ist es auch häufig schwierig den QQ-Plot zu bewerten, wenn es viele Behandlungsgruppen oder Blöcke gibt. Am Ende haben wir dann zwar mehr als 20 Beoabchtungen aber pro Kombination Behandlung und Block nur vier Wiederholungen. Und vier Wiederholungen sind zu wenig für eine sinnvolle Interpretation eines QQ-Plots.\nGrob gesprochen vergleicht der QQ Plot die Quantile der vorliegenden Beobachtungen, in unserem Fall der Variablen jump_length, mir den Quantilen einer theoretischen Normalverteilung, die sich aus den Daten mit dem Mittelwert und der Standardabweichung von jump_length ergeben würden.\nWir können die Annahme der Normalverteilung recht einfach in ggplot überprüfen. Wir sehen in Abbildung 31.3 den QQ-Plot für die Variable jump_length. Die Punkte sollten alle auf einer Diagonalen liegen. Hier dargestellt durch die rote Linie. Häufig weichen die Punkte am Anfang und Ende der Spannweite der Beobachtungen etwas ab.\n\nggplot(model_tbl, aes(sample = jump_length)) + \n  stat_qq() + \n  stat_qq_line(color = \"red\") +\n  theme_bw()\n\n\n\nAbbildung 31.3— QQ-Plot der Sprungweite in [cm]. Die Gerade geht einmal durch die Mitte der Punkte und die Punkte liegen nicht exakt auf der Geraden. Eine leichte Abweichung von der Normalverteilung könnte vorliegen.\n\n\n\n\nWir werden uns später auch noch häufig die Residuen aus den Modellen anschauen. Die Residuen müssen nach dem Fit des Modells einer Normalverteilung folgen. Wir können diese Annahme an die Residuen mit einem QQ-Plot überprüfen. In Abbildung 31.4 sehen wir die Residuen aus dem Modell fit_1 in einem QQ-Plot. Wir würden sagen, dass die Residuen approximativ normalvertelt sind. Die Punkte liegen fast alle auf der roten Diagonalen.\n\nggplot(resid_plot_tbl, aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line(color = \"red\") +\n  theme_bw() \n\n\n\nAbbildung 31.4— QQ-Plot der Residuen aus dem Modell fit_1. Die Residuen müssen einer approximativen Normalverteilung folgen, sonst hat der Fit des Modelles nicht funktioniert."
  },
  {
    "objectID": "stat-linear-reg-quality.html#modellgüte-mit-dem-r-paket-performance",
    "href": "stat-linear-reg-quality.html#modellgüte-mit-dem-r-paket-performance",
    "title": "31  Maßzahlen der Modelgüte",
    "section": "\n31.7 Modellgüte mit dem R Paket performance\n",
    "text": "31.7 Modellgüte mit dem R Paket performance\n\nAbschließend möchte ich hier nochmal das R Paket performance vorstellen. Wir können mit dem Paket auch die Normalverteilungsannahme der Residuen überprüfen. Das geht ganz einfach mit der Funktion check_normality() in die wir einfach das Objekt mit dem Fit des Modells übergeben.\n\ncheck_normality(fit_1)\n\nOK: residuals appear as normally distributed (p = 0.555).\n\n\nWir haben auch die Möglichkeit uns einen Plot der Modellgüte anzeigen zu lassen. In Abbildung 31.5 sehen wir die Übersicht von bis zu sechs Abbildungen, die uns Informationen zu der Modellgüte liefern. Wir müssen nur den Fit unseres Modells an die Funktion check_model() übergeben.\nDas Schöne an der Funktion ist, dass jeder Subplot eine Beschreibung in Englisch hat, wie der Plot auszusehen hat, wenn alles gut mit dem Modellieren funktioniert hat.\nWir kommen dann in der multiplen linearen Regression nochmal auf das Paket performance zurück. Für dieses Kapitel reicht dieser kurze Abriss.\n\ncheck_model(fit_1, colors = cbbPalette[6:8])\n\n\n\nAbbildung 31.5— Übersicht der Plots zu der Modellgüte aus der Funktion check_model()."
  },
  {
    "objectID": "stat-linear-reg-corr.html",
    "href": "stat-linear-reg-corr.html",
    "title": "32  Korrelation",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:39:24\nDie Korrelation gibt uns die Information welche Steigung die Gerade in einer simplen linearen Regression hat. Dabei erlaubt es die Korrelation uns verschiedene Geraden miteinander zu vergleichen. Die Korrelation ist nämlich einheitslos. Wir standardisieren durch die Anwendung der Korrelation die Steigung der Geraden dafür auf -1 bis +1. Damit ist die Korrelation ein bedeutendes Effektmaß für die Abschätzung eines Zusammenhangs zwischen zwei Variablen.\nEs gibt aber ein Problem. Nämlich nur weil etwas miteinander korreliert muss es keinen kausalen Zusammenhang geben. So ist die Ursache und Wirkung manchmal nicht klar zu benennen. Nehmen wir als plaktives Beispiel dicke Kinder, die viel Fernsehen. Wir würden annehmen, dass zwischen dem Fernsehkonsum und dem Gewicht von Kindern eine hohe Korrelation vorliegt. Sind jetzt aber die Kinder dick, weil die Kinder so viel Fernsehen oder schauen einfach dicke Kinder mehr Fernsehen, da die Kinder dick sind und sich nicht mehr so viel bewegen wollen?\nIm Weiteren ist das Wort korrelieren zum Gattungsbegriff in der Statistik geworden, wenn es um den Vergleich oder den Zusammenhang von zwei oder mehreren Variablen geht. Das heißt, in der Anwendung wird gesagt, dass wir A mit B korrelieren lassen wollen. Das Wort korrelieren steht jetzt aber nicht für das Konzept statistische Korrelation sondern ist Platzhalter für eine noch vom Anwender zu definierende oder zu findene statistische Methode.\nIn diesem Kapitel wollen wir uns mit der statisischen Korrelation beschäftigen. Die statistische Korrelation ist weniger aufregender, denn am Ende ist die Korrelation nur eine Zahl zwischen -1 und +1."
  },
  {
    "objectID": "stat-linear-reg-corr.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-linear-reg-corr.html#genutzte-r-pakete-für-das-kapitel",
    "title": "32  Korrelation",
    "section": "\n32.1 Genutzte R Pakete für das Kapitel",
    "text": "32.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, readxl,\n               corrplot)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-linear-reg-corr.html#daten",
    "href": "stat-linear-reg-corr.html#daten",
    "title": "32  Korrelation",
    "section": "\n32.2 Daten",
    "text": "32.2 Daten\nWir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die Korrelation auf sehr großen Datensätzen berechnen, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit \\(n=7\\) Beobachtungen. In der Tabelle 32.1 ist der Datensatz simplel_tbl dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in [cm] und dem Gewicht in [mg] für sieben Beobachtungen modellieren.\n\n\n\n\nTabelle 32.1— Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight.\n\njump_length\nweight\n\n\n\n1.2\n0.8\n\n\n1.8\n1.0\n\n\n1.3\n1.2\n\n\n1.7\n1.9\n\n\n2.6\n2.0\n\n\n1.8\n2.7\n\n\n2.7\n2.8\n\n\n\n\n\n\nIn Abbildung 32.1 sehen wir die Visualisierung der Daten simple_tbl in einem Scatterplot mit einer geschätzen Gerade. Wir wollen jetzt mit der Korrelation die Steigung der Geraden unabhängig von der Einheit beschreiben. Oder wir wollen die Steigung der Geraden standardisieren auf -1 bis 1.\n\n\n\n\nAbbildung 32.1— Scatterplot der Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Die Gerade verläuft mittig durch die Punkte."
  },
  {
    "objectID": "stat-linear-reg-corr.html#korrelation-theoretisch",
    "href": "stat-linear-reg-corr.html#korrelation-theoretisch",
    "title": "32  Korrelation",
    "section": "\n32.3 Korrelation theoretisch",
    "text": "32.3 Korrelation theoretisch\nWenn wir zwei verteilte Variablen miteinander korrelieren möchten, dann nutzen wir die Korrelation nach Pearson. Wenn wir nicht-normalverteilte Variablen miteinander korrelieren möchten, dann nutzen wir die Korrelation nach Spearman\nWir schauen uns hier die Korrelation nach Pearson an. Die Korrelation nach Pearson nimmt an, dass beide zu korrelierende Variablen einer Normalverteilung entstammen. Wenn wir keine Normalverteilung vorliegen haben, dann nutzen wir die Korrelation nach Spearman. Die Korrelation nach Spearman basiert auf den Rängen der Daten und ist ein nicht-parametrisches Verfahren. Die Korrelation nach Pearson ist die parametrische Variante. Wir bezeichnen die Korrelation entweder mit \\(r\\) oder dem griechischen Buchstaben \\(\\rho\\) als rho gesprochen.\nWas macht nun die Korrelation? Die Korrelation gibt die Richtung der Geraden an. Oder noch konkreter die Steigung der Geraden normiert auf -1 bis 1. Die Abbildung 32.2 zeigt die Visualisierung der Korrelation für drei Ausprägungen. Eine Korrelation von \\(r = -1\\) bedeutet eine maximale negative Korrelation. Die Gerade fällt in einem 45° Winkel. Eine Korrelation von \\(r = +1\\) bedeutet eine maximale positive Korrelation. Die gerade steigt in einem 45° Winkel. Eine Korrelation von \\(r = 0\\) bedeutet, dass keine Korrelation vorliegt. Die Grade verläuft parallel zur x-Achse.\n\n\n\nAbbildung 32.2— Visualisierung der Korrelation für drei Ausprägungen des Korrelationskoeffizient.\n\n\n\nIm Folgenden sehen wir die Formel für den Korrelationskoeffizient nach Pearson.\n\\[\n\\rho = r_{x,y} = \\cfrac{s_{x,y}}{s_x \\cdot s_y}\n\\]\nWir berechnen die Korrelation immer zwischen zwei Variablen \\(x\\) und \\(y\\). Es gibt keine multiple Korrelation über mehr als zwei Variablen. Im Zähler der Formel zur Korrelation steht die Kovarianz von \\(x\\) und \\(y\\).\nWir können mit folgender Formel die Kovarianzen zwischen den beiden Variablen \\(x\\) und \\(y\\) berechnen.\n\\[\ns_{x,y} = \\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\]\nDie folgende Formel berechnet die quadrierten Abweichung der Beobachtungen von \\(x\\) zum Mittelwert \\(\\bar{x}\\).\n\\[\ns_x = \\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nDie folgende Formel berechnet die quadrierten Abweichung der Beobachtungen von \\(y\\) zum Mittelwert \\(\\bar{y}\\).\n\\[\ns_y = \\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n\\]\nIn Tabelle 32.2 ist der Zusammenhang nochmal Schritt für Schrit aufgeschlüsselt. Wir berechnen erst die Abweichungsquadrate von \\(x\\) und die Abweichungsquadrate von \\(y\\). Dann noch die Quadrate der Abstände von \\(x\\) zu \\(y\\). Abschließend summieren wir alles und zeihen noch die Wurzel für die Abweichungsquadrate von \\(x\\) und \\(y\\).\n\n\n\nTabelle 32.2— Tabelle zur Berechnung des Korrelationskoeffizient\n\n\n\n\n\n\n\n\njump_length \\(\\boldsymbol{y}\\)\n\nweight \\(\\boldsymbol{x}\\)\n\n\\(\\boldsymbol{(y_i-\\bar{y})^2}\\)\n\\(\\boldsymbol{(x_i-\\bar{x})^2}\\)\n\\(\\boldsymbol{(x_i-\\bar{x})(y_i-\\bar{y})}\\)\n\n\n\n1.2\n0.8\n0.45\n0.94\n0.65\n\n\n1.8\n1.0\n0.01\n0.60\n0.06\n\n\n1.3\n1.2\n0.33\n0.33\n0.33\n\n\n1.7\n1.9\n0.03\n0.02\n-0.02\n\n\n2.6\n2.0\n0.53\n0.05\n0.17\n\n\n1.8\n2.7\n0.03\n0.86\n-0.07\n\n\n2.7\n2.8\n0.69\n1.06\n0.85\n\n\n\n\\(\\sum\\)\n2.05\n3.86\n1.97\n\n\n\n\\(\\sqrt{\\sum}\\)\n1.43\n1.96\n\n\n\n\n\n\nWir können die Zahlen dann aus der Tabelle in die Formel der Korrelation nach Pearson einsetzen. Wir erhalten eine Korrelation von 0.70 und haben damit eine recht starke positve Korrelation vorliegen.\n\\[\n\\rho = r_{x,y} = \\cfrac{1.97}{1.96 \\cdot 1.43} = 0.70\n\\] Wir können mit der Funktion cor() in R die Korrelation zwischen zwei Spalten in einem Datensatz berechnen. Wir überprüfen kurz unsere Berechnung und stellen fest, dass wir richtig gerechnet haben.\n\ncor(simple_tbl$jump_length, simple_tbl$weight)\n\n[1] 0.70149847\n\n\nIn Abbildung 32.3 sehen wir nochmal die Zusammenhänge der Abstände farbig hervorgehoben. Dader Nenner nur positive Zahlen annehmen kann, wird das Vorzeichen der Korrelation durch den Zähler bestimmt.\n\n\nAbbildung 32.3— Visualisierung der Korrelation nochmal anders. Um die Korrelation zu berechnen teilen wir die Quadrate mit den roten und grünen Rändern durch die Wurzel der Quadrate der roten und grünen Linien. Wenn die Quadrate gleich groß sind, dann haben wir eine Korrelation von \\(r = 1.\\)"
  },
  {
    "objectID": "stat-linear-reg-corr.html#korrelation-in-r",
    "href": "stat-linear-reg-corr.html#korrelation-in-r",
    "title": "32  Korrelation",
    "section": "\n32.4 Korrelation in R",
    "text": "32.4 Korrelation in R\nWir nutzen die Korrelation in R selten nrur für zwei Variablen. Meistens schauen wir uns alle numerischen Variablen gemeinsam in einer Abbildung an. Wir nennen diese Abildung auch Korrelationsplot. Faktoren sind keine numerischen Variablen. Daher kann es sein, dass für dein Experiment kein Korrelationsplot in Frage kommt.\nWir schauen uns jetzt nochmal einen die Berechnung für den Datensatz simple_tbl an. Wir müssen für die Korrelation zwischen zwei Variablen diese Variablen mit dem $-Zeichen aus dem Datensatz extrahieren. Die Funktion cor() kann nur mit Vektoren oder ganzen numerischen Datensätzen arbeiten.\nWir können den Korrelationskoeffizienten nach Pearson mit der Option method = \"pearson\" auswählen.\n\ncor(simple_tbl$jump_length, simple_tbl$weight, method = \"pearson\")\n\n[1] 0.70149847\n\n\nWenn wir die nicht-parametrische Variante des Korrelationskoeffizienten nach Spearman berechnen wollen nutzen wir die Option method = \"spearman\".\n\ncor(simple_tbl$jump_length, simple_tbl$weight, method = \"spearman\")\n\n[1] 0.79282497\n\n\nWir können auch einen statistischen Test für die Korrelation rechnen. Die Nullhypothese \\(H_0\\) wäre hierbei, dass die Korrelation \\(r = 0\\) ist. Die Funktion cor.test() liefert den entsprechenden \\(p\\)-Wert für die Entscheidung gegen die Nullhypothese.\n\ncor.test(simple_tbl$jump_length, simple_tbl$weight, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  simple_tbl$jump_length and simple_tbl$weight\nt = 2.20101, df = 5, p-value = 0.078993\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.10929883  0.95176731\nsample estimates:\n       cor \n0.70149847 \n\n\nAus dem Test erhalten wir den \\(p\\)-Wert von \\(0.079\\). Damit liegt der \\(p\\)-Wert über den Signifikanzniveau von \\(\\alpha\\) gleich 5%. Wir können somit die Nullhypothese nicht ablehnen. Wir sehen hier, die Probelematik der kleinen Falzahl. Obwohl unsere Korrelation mit \\(0.7\\) groß ist erhalten wir einen \\(p\\)-Wert, der nicht die Grenze von 5% unterschreitet. Wir sehen, dass die starre Grenze von \\(\\alpha\\) auch Probleme bereitet.\nAbschließend wollen wir uns noch die Funktion corrplot() aus dem gleichnamigen R Paket corrplot anschauen. Die Hilfeseite zum Paket ist sehr ausführlich und bietet noch eine Reihe an anderen Optionen. Wir benötigen dafür einen etwas größeren Datensatz mit mehreren numerischen Variablen. Wir nutzen daher den Gummibärchendatensatz und selektieren die Spalten count_bears bis semester aus.\n\ncorr_gummi_tbl <- read_excel(\"data/gummibears.xlsx\") %>% \n  select(count_bears:semester)\n\nWir brauchen für die Funktion corrplot() eine Matrix mit den paarweisen Korrelationen. Wir können diese Matrix wiederum mit der Funktion cor() erstellen. Wir müssen dazu aber erstmal alle numerischen Variablen mit select_if() selektieren und dann alle fehlenden Werte über na.omit() entfernen.\n\ncor_mat <- corr_gummi_tbl %>% \n  select_if(is.numeric) %>% \n  na.omit %>% \n  cor()\n\ncor_mat %>% round(3)\n\n            count_bears count_color    age height semester\ncount_bears       1.000       0.336  0.106 -0.129    0.107\ncount_color       0.336       1.000 -0.021 -0.131    0.001\nage               0.106      -0.021  1.000 -0.106    0.031\nheight           -0.129      -0.131 -0.106  1.000   -0.002\nsemester          0.107       0.001  0.031 -0.002    1.000\n\n\nWir sehen das in der Korrelationsmatrix jeweils über und unterhalb der Diagonalen die gespiegelten Zahlen stehen. Wir können jetzt die Matrix cor_mat in die Funktion corrplot() stecken und uns den Korrelationsplot in Abbildung 32.4 einmal anschauen.\n\ncorrplot(cor_mat)\n\n\n\nAbbildung 32.4— Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke.\n\n\n\n\nWir sehen in Abbildung 32.4, dass wir eine schwache positive Korrelation zwischen count_color und count_bears haben, angezeigt durch den schwach blauen Kreis. Der Rest der Korrelation ist nahe Null, tendiert aber eher ins negative.\nNun ist in dem Plot natürlich eine der beiden Seiten überflüssig. Wir können daher die Funktion corrplot.mixed() nutzen um in das untere Feld die Zahlenwerte der Korrelation darzustellen.\n\ncorrplot.mixed(cor_mat)\n\n\n\nAbbildung 32.5— Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke. In das untere Feld werden die Werte der Korrelation angegeben.\n\n\n\n\nEs gibt noch eine Vielzahl an weiteren Möglichkeiten in den Optionen von der Funktion corr.mixed(). Hier hilft dann die Hilfeseite der Funktion oder aber die Hilfeseite zum Paket."
  },
  {
    "objectID": "stat-modeling-preface.html",
    "href": "stat-modeling-preface.html",
    "title": "Statistisches Modellieren",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:39:31\nIn diesem und den nun folgenden Kapitel wollen wir uns mit den Grundlagen der multiplen linearen Regression beschäftigen. Das heißt, wir haben ein Outcome \\(y\\), was einer Verteilung folgt, sowie mehrere Einflussvariable \\(x_1\\) bis \\(x_p\\). Wir wollen jetzt herausfinden, welchen Einfluss oder Effekt die \\(x_1, ..., x_p\\) auf das \\(y\\) hat. Sehr simple Gesprochen legen wir eine Gerade durch eine mehrdimensionale Punktewolke.\nWir erinnern und nochmal, das ein simples lineares Modell nur ein \\(x_1\\) hat:\n\\[\ny \\sim x_1\n\\]\nEin multiples lineares Modell hat von \\(x_1\\) bis \\(x_p\\) Einflussvariablen:\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir brauchen viele der Konzepte aus den vorherigen Kapiteln zur Modellgüte sowie Konfidenzintervallen. Vieles fällt hier zusammen. Schau dir ruhig nochmal die vorherigen Kapitel an, wenn dir etwas unklar ist.\nHäufig wollen wir auch etwas spezifischer sein. Wir schreiben daher einen Faktor mit einem \\(f\\) wie folgt.\n\\[\ny \\sim f_1\n\\]\nWenn wir dann noch einen Block mit hinzunehmen, dann erweitern wir das Modell um einen Blockfaktor mit einem \\(b\\) wie folgt.\n\\[\ny \\sim f_1 + b_1\n\\] Es kann auch sein, dass wir noch einen zufälligen Effekt (eng. random effect) mit in das Modell nehmen wollen. Wir bezeichnen einen zufälligen Effekt mit einem \\(z\\) wie folgt.\n\\[\ny \\sim f_1 + z_1\n\\]"
  },
  {
    "objectID": "stat-modeling-preface.html#genutzte-r-pakete-für-die-folgenden-kapitel",
    "href": "stat-modeling-preface.html#genutzte-r-pakete-für-die-folgenden-kapitel",
    "title": "Statistisches Modellieren",
    "section": "Genutzte R Pakete für die folgenden Kapitel",
    "text": "Genutzte R Pakete für die folgenden Kapitel\nNeben den R Paketen, die wir in den jeweiligen Kapiteln brauchen, kommen noch folgende R Pakete immer wieder dran. Deshalb sind die R Pakete hier schonmal mit den jeweiligen Internetseiten aufgeführt.\n\nDas Buch Tidy Modeling with R gibt nochmal einen tieferen Einblick in das Modellieren in R. Wir immer, es ist ein Vorschlag aber kein Muss.\nDas R Paket parameters nutzen wir um die Parameter eines Modells aus den Fits der Modelle zu extrahieren. Teilweise sind die Standardausgaben der Funktionen sehr unübersichtich. Hier hilft das R Paket.\nDas R Paket performance hilft uns zu verstehen, ob die Modelle, die wir gefittet haben, auch funktioniert haben. In einen mathematischen Algorithmus können wir alles reinstecken, fast immer kommt eine Zahl wieder raus.\nDas R Paket tidymodels nutzen wir als das R Paket um mit Modellen umgehen zu können und eine Vorhersage neuer Daten zu berechnen. Das Paket tidymodels ist wie das Paket tidyverse eine Sammlung an anderen R Paketen, die wir brauchen werden."
  },
  {
    "objectID": "stat-modeling-preface.html#generalisierung-von-lm-zu-glm-und-glmer",
    "href": "stat-modeling-preface.html#generalisierung-von-lm-zu-glm-und-glmer",
    "title": "Statistisches Modellieren",
    "section": "Generalisierung von lm() zu glm() und [g]lmer()\n",
    "text": "Generalisierung von lm() zu glm() und [g]lmer()\n\n\nDie Funktion lm() nutzen wir, wenn das Outcome \\(y\\) einer Normalverteilung folgt.\nDie Funktion glm() nutzen wir, wenn das Outcome \\(y\\) einer andere Verteilung folgt.\nDie Funktion lmer() nutzen wir, wenn das Outcome \\(y\\) einer Normalverteilung folgt und wir noch einen Block- oder Clusterfaktor vorliegen haben.\nDie Funktion glmer() nutzen wir, wenn das Outcome \\(y\\) einer andere Verteilung folgt und wir noch einen Block- oder Clusterfaktor vorliegen haben.\n\n\n\n\nAbbildung 1— Übersicht der Namen der Funktionen in R für das lm(), glm() und glmer().\n\n\n\nIn Abbildung 2 sehen wir wie wir den Namen einer Regression bilden. Zuerst entscheiden wir, ob wir nur ein \\(x\\) haben oder mehrere. Mit einem \\(x\\) sprechen wir von einem simplen Modell, wenn wir mehrere \\(x\\) haben wir ein multiples Modell. Im nächsten Schritt benennen wir die Verteilung für das Outcome \\(y\\). Dann müssen wir noch entscheiden, ob wir ein gemischtes Modell vorliegen haben, dann schreiben wir das hin. Sonst lassen wir den Punkt leer. Anschließend kommt noch lineares Modell hinten ran.\n\n\n\nAbbildung 2— Wie bilde ich den Namen einer Regression? Erst beschreiben wir das \\(x\\), dann das \\(y\\). Am Ende müssen wir noch sagen, ob wir ein gemischtes Modell vorliegen haben oder nicht."
  },
  {
    "objectID": "stat-modeling-preface.html#das-regressionskreuz",
    "href": "stat-modeling-preface.html#das-regressionskreuz",
    "title": "Statistisches Modellieren",
    "section": "Das Regressionskreuz",
    "text": "Das Regressionskreuz\nIn diesem Kapitel wollen wir uns mit der Grundlage der multiplen linearen Regression beschäftigen. Das heist wir schauen uns Modelle mit mehreren \\(x\\) an. Wir haben also nicht mehr nur eine Einflussvariable auf der rechten Seite der Gleichung sondern meist mehrere. Je nachdem wie diese \\(x\\) beschffen sind, müssen wir die \\(x\\) auch interpretieren. Wir unterscheiden in vier Arten von \\(x\\).\n\nDas zu betrachtende \\(x\\) ist eine Variable mit kontinuierlichen Zahlen (siehe Kapitel 33.3.1)\nDas zu betrachtende \\(x\\) ist eine Variablen mit einem Faktor mit zwei Leveln (siehe Kapitel 33.3.2).\nDas zu betrachtende \\(x\\) ist eine Variablen mit einem Faktor mit mehr als zwei Leveln (siehe Kapitel 33.3.3).\nDas zu betrachtende \\(x\\) ist eine Variable, die einen Block oder Cluster beschreibt (siehe Kapitel 38).\n\nWir finden die Fälle 1) bis 3) in der Abbildung 3 in den Spalten wieder.\nNeben dem \\(x\\) müssen wir auch das \\(y\\) in diesem Kapitel betrachten. Das \\(y\\) kann aus verschiedenen Verteilungen kommen. Häufig nehmen wir an, dass das \\(y\\) normalverteilt ist, das muss das \\(y\\) aber nicht sein. Je nachdem wir das \\(y\\) verteilt ist, rechen wir eine andere multiple Regression.\n\nDas zu betrachtende \\(y\\) folgt einer Normalverteilung bzw. entstammt einer Gaussian Vertreilungsfamilie. Wir wollen dann eine multiple Gaussian Regression rechen.\nDas zu betrachtende \\(y\\) folgt einer Poissonverteilung bzw. entstammt einer Poisson Vertreilungsfamilie. Wir wollen dann eine multiple Poisson Regression rechen.\nDas zu betrachtende \\(y\\) folgt einer Ordinalen- oder Multinominalenverteilung bzw. entstammt einer Ordinalen- oder Multinominalen Vertreilungsfamilie. Wir wollen dann eine multiple ordinale oder multinominale Regression rechen.\nDas zu betrachtende \\(y\\) folgt einer Binomialverteilung bzw. entstammt einer Binomialen Vertreilungsfamilie. Wir wollen dann eine multiple logistische Regression rechen.\n\nWir finden die Fälle 1) bis 4) in der Abbildung 3 in den Zeilen wieder.\n\n\n\nAbbildung 3— Das Regressionskreuz als allgemeine Übersicht der Möglichkeiten einer multiplen linearen Regression."
  },
  {
    "objectID": "stat-modeling-basic.html",
    "href": "stat-modeling-basic.html",
    "title": "33  Multiple lineare Regression",
    "section": "",
    "text": "Version vom October 17, 2022 um 09:16:07\nWir wollen uns nun folgende Aspekte einmal genauer anschauen.\nZuerst nochmal eine Wiederholung oder Einführung. Ein multiples lineare Modell hat mehrere \\(x\\). Daher haben wir auf der linken Seite ein \\(y\\) und auf der rechten Seite \\(p\\)-mal ein \\(x\\). Wir können daher ein multiples lineares Modell daher wie folgt in R schreiben.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nOder konkreter können wir sagen, dass jump_length von animal, sex und weight abhängt und wir diesen Zusammenhang modellieren wollen.\n\\[\njump\\_length \\sim animal + sex + weight\n\\]\nDas hilft uns jetzt nur bedingt, denn wir wollen ja aus einer Modellierung in R die Koeffizienten der Regression wiederbekommen. Dafür müssen wir uns nochmal klar werden, was die Koeffizienten einer Regression sind. Das sind zum einen der y-Achsenabschnitt \\(\\beta_0\\) und die Steigung der einzelnen Variablen mit \\(\\beta_1\\) bis \\(\\beta_p\\). Wir erhalten auch bei einem multiplen Regressionsmodell nur einen Vektor mit den Residuen wieder.\n\\[\ny \\sim \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon\n\\]\nDennoch sind die Residuen \\(\\epsilon\\) normalverteilt. Wir haben im Mittel einen Abstand der \\(\\epsilon\\)’s zur geraden von 0 und eine Streuung von \\(s^2_{\\epsilon}\\).\n\\[\n\\epsilon \\sim \\mathcal{N}(0, s^2_{\\epsilon})\n\\]\nWir zeichen im Prinzip eine Gerade durch den \\(p\\)-dimensionellen Raum."
  },
  {
    "objectID": "stat-modeling-basic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-basic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.1 Genutzte R Pakete für das Kapitel",
    "text": "33.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance, car, parameters)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-modell-matrix",
    "href": "stat-modeling-basic.html#sec-modell-matrix",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.2 Die Modellmatrix in R",
    "text": "33.2 Die Modellmatrix in R\nWozu brauche ich das? Eine gute Frage. Du brauchst das Verständnis der Modellmatrix, wenn du verstehen willst wie R in einer linearen Regression zu den Koeffizienten kommt…\nAls erstes wollen wir verstehen, wie ein Modell in R aussieht. Dann können wir auch besser verstehen, wie die eigentlichen Koeffizienten aus dem Modell entstehen. Wir bauen uns dafür ein sehr simplen Datensatz. Wir bauen uns einen Datensatz mit Schlangen.\n\nsnake_tbl <- tibble(\n  svl = c(40, 45, 39, 51, 52, 57, 58, 49),\n  mass = c(6, 8, 5, 7, 9, 11, 12, 10),\n  region = as_factor(c(\"west\",\"west\", \"west\", \"nord\",\"nord\",\"nord\",\"nord\",\"nord\")),\n  color = as_factor(c(\"schwarz\", \"schwarz\", \"rot\", \"rot\", \"rot\", \"blau\", \"blau\", \"blau\"))\n) \n\nIn der Tabelle 33.1 ist der Datensatz snake_tbl nochmal dargestellt. Wir haben die Schlangenlänge svl als Outcome \\(y\\) sowie das Gewicht der Schlangen mass, die Sammelregion region und die Farbe der Schlangen color. Dabei ist mass eine kontinuierliche Variable, region eine kategorielle Variable als Faktor mit zwei Leveln und color eine kategorielle Variable als Faktor mit drei Leveln.\n\n\n\n\nTabelle 33.1— Datensatz zu Schlangen ist entlehnt und modifiiert nach Kéry (2010, p. 77)\n\n\nsvl\nmass\nregion\ncolor\n\n\n\n40\n6\nwest\nschwarz\n\n\n45\n8\nwest\nschwarz\n\n\n39\n5\nwest\nrot\n\n\n51\n7\nnord\nrot\n\n\n52\n9\nnord\nrot\n\n\n57\n11\nnord\nblau\n\n\n58\n12\nnord\nblau\n\n\n49\n10\nnord\nblau\n\n\n\n\n\n\nWir wollen uns nun einmal anschauen, wie ein Modell in R sich zusammensetzt. Je nachdem welche Spalte \\(x\\) wir verwenden um den Zusammenhang zum \\(y\\) aufzuzeigen.\n\n33.2.1 Kontinuierliches \\(x\\)\n\nIm ersten Schritt wollen wir uns einmal das Modell mit einem kontinuierlichen \\(x\\) anschauen. Daher bauen wir uns ein lineares Modell mit der Variable mass. Wir erinnern uns, dass mass eine kontinuierliche Variable ist, da wir hier nur Zahlen in der Spalte finden. Die Funktion model.matrix() gibt uns die Modellmatrix wieder.\n\nmodel.matrix(svl ~ mass, data = snake_tbl) %>% as_tibble\n\n# A tibble: 8 × 2\n  `(Intercept)`  mass\n          <dbl> <dbl>\n1             1     6\n2             1     8\n3             1     5\n4             1     7\n5             1     9\n6             1    11\n7             1    12\n8             1    10\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte mass als kontinuierliche Variable.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 6 \\\\\n  1 & 8 \\\\\n  1 & 5 \\\\\n  1 & 7 \\\\\n  1 & 9 \\\\\n  1 & 11\\\\\n  1 & 12\\\\\n  1 & 10\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_{mass}\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt mit der Funktion lm() fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten aus dem Objekt fit_1 wiedergeben zu lassen.\n\nfit_1 <- lm(svl ~ mass, data = snake_tbl) \nfit_1 %>% coef %>% round(2)\n\n(Intercept)        mass \n      26.71        2.61 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_1 wieder.\n\nfit_1 %>% residuals() %>% round(2)\n\n    1     2     3     4     5     6     7     8 \n-2.36 -2.57 -0.75  6.04  1.82  1.61  0.00 -3.79 \n\n\nWir können jetzt die Koeffizienten in die Modellmatrix ergänzen. Wir haben den Intercept mit \\(\\beta_0 = 26.71\\) geschätzt. Weiter ergänzen wir die Koeffizienten aus dem linearen Modell für mass mit \\(\\beta_{mass}=2.61\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein. Wir erhalten dann folgende ausgefüllte Gleichung mit den Matrixen.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  26.71 & \\phantom{0}6 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}8 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}5 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}7 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}9 \\cdot 2.61\\\\\n  26.71 & 11\\cdot 2.61\\\\\n  26.71 & 12\\cdot 2.61\\\\\n  26.71 & 10\\cdot 2.61\\\\\n\\end{pmatrix}\n  +\n  \\begin{pmatrix}\n  -2.36\\\\\n  -2.57\\\\\n  -0.75 \\\\\n  +6.04\\\\\n  +1.82\\\\\n  +1.61\\\\\n  \\phantom{+}0.00\\\\\n  -3.79\\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis. Wie du siehst ergänzen wir hier noch eine Reihe von \\(+\\) um den Intercept mit der Steigung zu verbinden. Steht ja auch so in der Gleichung des linearen Modells drin, alles wird mit einem \\(+\\) miteinander verbunden.\n\nc(26.71 +  6*2.61 - 2.36,\n  26.71 +  8*2.61 - 2.57,\n  26.71 +  5*2.61 - 0.75,\n  26.71 +  7*2.61 + 6.04,\n  26.71 +  9*2.61 + 1.82,\n  26.71 + 11*2.61 + 1.61,\n  26.71 + 12*2.61 + 0.00,\n  26.71 + 10*2.61 - 3.79) %>% round() \n\n[1] 40 45 39 51 52 57 58 49\n\n\nOh ha! Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat. Das heißt, die ganze Sache hat funktioniert.\n\n33.2.2 Kategorielles \\(x\\) mit 2 Leveln\nIm diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen \\(x\\) mit 2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable region``. Die Funktionmodel.matrix()` gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ region, data = snake_tbl) %>% as_tibble\n\n# A tibble: 8 × 2\n  `(Intercept)` regionnord\n          <dbl>      <dbl>\n1             1          0\n2             1          0\n3             1          0\n4             1          1\n5             1          1\n6             1          1\n7             1          1\n8             1          1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte regionnord. In dieser Spalte steht die Dummykodierung für die Variable region. Die ersten drei Schlangen kommen nicht aus der Region nord und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region nord und erhalten daher eine 1 in der Spalte.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 &  0  \\\\\n  1 &  0 \\\\\n  1 &  0\\\\\n  1 &  1\\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta^{region}_{nord} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen.\n\nfit_2 <- lm(svl ~ region, data = snake_tbl) \nfit_2 %>% coef %>% round(2)\n\n(Intercept)  regionnord \n      41.33       12.07 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_2 wieder.\n\nfit_2 %>% residuals() %>% round(2)\n\n    1     2     3     4     5     6     7     8 \n-1.33  3.67 -2.33 -2.40 -1.40  3.60  4.60 -4.40 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 41.33\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für die Region und das Level nord mit \\(\\beta^{region}_{nord} = 12.07\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  -1.33\\\\\n  +3.67 \\\\\n  -2.33 \\\\\n  -2.40 \\\\\n  -1.40 \\\\\n  +3.60 \\\\\n  +4.60 \\\\\n  -4.40 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(41.33 + 0*12.07 - 1.33,\n  41.33 + 0*12.07 + 3.67,\n  41.33 + 0*12.07 - 2.33,\n  41.33 + 1*12.07 - 2.40,\n  41.33 + 1*12.07 - 1.40,\n  41.33 + 1*12.07 + 3.60,\n  41.33 + 1*12.07 + 4.60,\n  41.33 + 1*12.07 - 4.40) %>% round() \n\n[1] 40 45 39 51 52 57 58 49\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat.\n\n33.2.3 Kategorielles \\(x\\) mit >2 Leveln\nIm diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen \\(x\\) mit >2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable color. Die Funktion model.matrix() gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ color, data = snake_tbl) %>% as_tibble\n\n# A tibble: 8 × 3\n  `(Intercept)` colorrot colorblau\n          <dbl>    <dbl>     <dbl>\n1             1        0         0\n2             1        0         0\n3             1        1         0\n4             1        1         0\n5             1        1         0\n6             1        0         1\n7             1        0         1\n8             1        0         1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalten für color. Die Spalten colorrot und colorblau geben jeweils an, ob die Schlange das Level rot hat oder blau oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 0 & 0 \\\\\n  1 & 0 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 0 & 1\\\\\n  1 & 0 & 1\\\\\n  1 & 0 & 1\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta^{color}_{rot} \\\\\n  \\beta^{color}_{blau} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen.\n\nfit_3 <- lm(svl ~ color, data = snake_tbl) \nfit_3 %>% coef %>% round(2)\n\n(Intercept)    colorrot   colorblau \n      42.50        4.83       12.17 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_3 wieder.\n\nfit_3 %>% residuals() %>% round(2)\n\n    1     2     3     4     5     6     7     8 \n-2.50  2.50 -8.33  3.67  4.67  2.33  3.33 -5.67 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 25\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für die Farbe und das Level rot mit \\(\\beta^{color}_{rot} = 4.83\\) und für die Farbe und das Level blau mit \\(\\beta^{color}_{blau} = 12.17\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  42.50 & 0 \\cdot 4.83& 0 \\cdot 12.17 \\\\\n  42.50 & 0 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  -2.50 \\\\\n  +2.50 \\\\\n  -8.33 \\\\\n  +3.67 \\\\\n  +4.67 \\\\\n  +2.33 \\\\\n  +3.33 \\\\\n  -5.67 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(42.50 + 0*4.83 + 0*-12.17 - 2.50,\n  42.50 + 0*4.83 + 0*-12.17 + 2.50,\n  42.50 + 1*4.83 + 0*-12.17 - 8.33,\n  42.50 + 1*4.83 + 0*-12.17 + 3.67,\n  42.50 + 1*4.83 + 0*-12.17 + 4.67,\n  42.50 + 0*4.83 + 1*-12.17 + 2.33,\n  42.50 + 0*4.83 + 1*-12.17 + 3.33,\n  42.50 + 0*4.83 + 1*-12.17 - 5.67) %>% round()\n\n[1] 40 45 39 51 52 33 34 25\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat.\n\n33.2.4 Das volle Modell\nIm letzten Schritt wollen wir uns einmal das volle Modell anschauen. Wir bauen uns ein Modell mit allen Variablen in dem Datensatz snake_tbl. Die Funktion model.matrix() gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ mass + region + color, data = snake_tbl) %>% as_tibble() \n\n# A tibble: 8 × 5\n  `(Intercept)`  mass regionnord colorrot colorblau\n          <dbl> <dbl>      <dbl>    <dbl>     <dbl>\n1             1     6          0        0         0\n2             1     8          0        0         0\n3             1     5          0        1         0\n4             1     7          1        1         0\n5             1     9          1        1         0\n6             1    11          1        0         1\n7             1    12          1        0         1\n8             1    10          1        0         1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte mass als kontenuierliche Variable. In der Spalte regionnord steht die Dummykodierung für die Variable region. Die ersten drei Schlangen kommen nicht aus der Region nord und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region nord und erhalten daher eine 1 in der Spalte. Die nächsten beiden Spalten sind etwas komplizierter. Die Spalten colorrot und colorblau geben jeweils an, ob die Schlange das Level rot hat oder blau oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 6 & 0 & 0 & 0 \\\\\n  1 & 8 & 0 & 0 & 0\\\\\n  1 & 5 & 0 & 1 & 0\\\\\n  1 & 7 & 1 & 1 & 0\\\\\n  1 & 9 & 1 & 1 & 0\\\\\n  1 & 11& 1 & 0 & 1\\\\\n  1 & 12& 1 & 0 & 1\\\\\n  1 & 10& 1 & 0 & 1\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_{mass} \\\\\n  \\beta^{region}_{nord} \\\\\n  \\beta^{color}_{rot} \\\\\n  \\beta^{color}_{blau} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen.\n\nfit_4 <- lm(svl ~ mass + region + color, data = snake_tbl) \nfit_4 %>% coef %>% round(2)\n\n(Intercept)        mass  regionnord    colorrot   colorblau \n      25.00        2.50        5.00        1.50       -2.83 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_4 wieder.\n\nfit_4 %>% residuals() %>% round(2)\n\n    1     2     3     4     5     6     7     8 \n 0.00  0.00  0.00  2.00 -2.00  2.33  0.83 -3.17 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 25\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für mass mit \\(\\beta_{mass}=2.5\\), für Region und das Level nord mit \\(\\beta^{region}_{nord} = 5\\), für die Farbe und das Level rot mit \\(\\beta^{color}_{rot} = 1.5\\) und für die Farbe und das Level blau mit \\(\\beta^{color}_{blau} = -2.83\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  25 & \\phantom{0}6 \\cdot 2.5 & 0 \\cdot 5 & 0 \\cdot 1.5& 0 \\cdot -2.83 \\\\\n  25 & \\phantom{0}8 \\cdot 2.5 & 0 \\cdot 5 & 0 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}5 \\cdot 2.5 & 0 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}7 \\cdot 2.5 & 1 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}9 \\cdot 2.5 & 1 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & 11\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n  25 & 12\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n  25 & 10\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\phantom{+}0.00 \\\\\n  \\phantom{+}0.00 \\\\\n  \\phantom{+}0.00 \\\\\n  +2.00 \\\\\n  -2.00 \\\\\n  +2.33 \\\\\n  +0.83 \\\\\n  -3.17 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(25 +  6*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,\n  25 +  8*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,\n  25 +  5*2.5 + 0*5 + 1*1.5 + 0*-2.83 + 0.00,\n  25 +  7*2.5 + 1*5 + 1*1.5 + 0*-2.83 + 2.00,\n  25 +  9*2.5 + 1*5 + 1*1.5 + 0*-2.83 - 2.00,\n  25 + 11*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 2.33,\n  25 + 12*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 0.83,\n  25 + 10*2.5 + 1*5 + 0*1.5 + 1*-2.83 - 3.17) \n\n[1] 40 45 39 51 52 57 58 49\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat. Was haben wir gelernt?\n\nIn einem Modell gibt es immer ein Faktorlevel weniger als ein Faktor Level hat. Die Information des alphanumerisch ersten Levels steckt dann mit in dem Intercept.\nIn einem Modell geht eine kontinuierliche Variable als eine Spalte mit ein.\nIn einem Modell gibt es immer nur eine Spalte für die Residuen und damit nur eine Residue für jede Beobachtung, egal wie viele Variablen ein Modell hat."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-interpret-x",
    "href": "stat-modeling-basic.html#sec-interpret-x",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.3 Interpretation von \\(x\\)\n",
    "text": "33.3 Interpretation von \\(x\\)\n\nWi interpretiee wir nun das Ergebnis einer linearen Regression? Zum einen nutzen wir häufig das Modell nur um das Modell dann weiter in einem multiplen Vergleich zu nutzen. Hier wollen wir uns jetzt aber wirklich die Koeffizienten aus einer multiplen linearen Regression anschauen udn diese Zahlen einmal interpretieren. Wichtig ist, dass wir ein normalverteiltes \\(y\\) vorliegen haben und uns verschiedene Formen des \\(x\\) anschauen. Wir betrachten ein kontinuierliches \\(x\\), ein kategorielles \\(x\\) mit zwei Leveln und ein kategorielles \\(x\\) mit drei oder mehr Leveln.\n\n33.3.1 Kontinuierliches \\(x\\)\n\nBauen wir uns also einmal einen Datensatz mit einem kontinuierlichen \\(x\\) und einem normalverteilten \\(y\\). Unser \\(x\\) soll von 1 bis 7 laufen. Wir erschaffen uns das \\(y\\) indem wir das \\(x\\) mit 1.5 multiplizieren, den \\(y\\)-Achsenabschnitt von 5 addieren und einen zufälligen Fehler aus einer Normalverteilung mit \\(\\mathcal{N}(0, 1)\\) aufaddieren. Wir haben also eine klassische Regressionsgleichung mit \\(y = 5 + 1.5 \\cdot x\\).\n\nset.seed(20137937)\ncont_tbl <- tibble(x = seq(from = 1, to = 7, by = 1),\n                   y = 5 + 1.5 * x + rnorm(length(x), 0, 1))\ncont_tbl\n\n# A tibble: 7 × 2\n      x     y\n  <dbl> <dbl>\n1     1  5.93\n2     2  8.06\n3     3  8.95\n4     4 12.1 \n5     5 11.9 \n6     6 14.9 \n7     7 16.3 \n\n\nWenn wir keinen Fehler addieren würden, dann hätten wir auch eine Linie von Punkten wie an einer Perlschnur aufgereiht. Dann wäre das \\(R^2 = 1\\) und wir hätten keine Varianz in den Daten. Das ist aber in einem biologischen Setting nicht realistisch, dass unser \\(y\\) vollständig von \\(x\\) erklärt wird.\nSchauen wir uns nochmal die Modellmatrix an. Hier erwartet uns aber keine Überraschung. Wir schätzen den Intercept und dann kommt der Wert für jedes \\(x\\) in der zweiten Spalte.\n\nmodel.matrix(y ~ x, data = cont_tbl)\n\n  (Intercept) x\n1           1 1\n2           1 2\n3           1 3\n4           1 4\n5           1 5\n6           1 6\n7           1 7\nattr(,\"assign\")\n[1] 0 1\n\n\nIm Folgenden schätzen wir jetzt das lineare Modell um die Koeffizienten der Geraden zu erhalten. Wir nutzen die Funktion model_parameter() aus dem R Paket parameters für die Ausgabe der Koeffizienten.\n\nlm(y ~ x, data = cont_tbl) %>% \n  model_parameters() %>% \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |        4.32\nx           |        1.71\n\n\nSteigt das \\(x\\) um 1 Einheit an, so erhöht sich das \\(y\\) um den Wert der Steigung von \\(x\\).\nWir erhalten einen Intercept von \\(4.32\\) und eine Steigung von \\(x\\) mit \\(1.71\\). Wichtig nochmal, wir haben uns hier zufällige Zahlen erstellen lassen. Wenn du oben den Fehler rausnimmst, dann erhälst du auch die exakten Zahlen für den Intercept und die Steigung von \\(x\\) wieder. Wir sehen also, wenn wir ein kontinuierliches \\(x\\) haben, dann können wir das \\(x\\) in dem Sinne einr Steigung interpretieren. Steigt das \\(x\\) um 1 Einheit an, so erhöht sich das \\(y\\) um den Wert der Steigung von \\(x\\).\nIn Abbildung 33.1 sehen wir den Zusammenhang nochmal graphisch dargestellt. Wir sehen, dass wir die voreingestellten Parameter von \\(\\beta_0 = 5\\) und \\(\\beta_1 = 1.5\\) fast treffen.\n\n\n\n\nAbbildung 33.1— Graphische Darstellung der Interpretation von einem kontinuierlichen \\(x\\).\n\n\n\n\n\n33.3.2 Kategorielles \\(x\\) mit 2 Leveln\nEtwas anders wird der Fall wenn wir ein kategorielles \\(x\\) mit 2 Leveln vorliegen haben. Wir bauen faktisch zwei Punktetürme an zwei \\(x\\) Positionen auf. Dennoch können wir durch diese Punkte eine Gerade zeichnen. Bauen wir uns erst die Daten mit der Funktion rnorm(). Wir haben zwei Gruppen vorliegen, die Gruppe A hat sieben Beobachtungen und einen Mittelwert von 10. Die Gruppe B hat ebenfalls sieben Beobchatungen und einen Mittelwert von 15. Der Effekt zwischen den beiden Gruppen A und B ist die Mittelwertsdifferenz \\(\\Delta_{A-B}\\) ist somit 5. Wir erhlalten dann folgenden Datensatz wobei die Werte der Gruppe A um die 10 streuen und die Werte der Gruppe B um die 15 streuen.\n\nset.seed(20339537)\ncat_two_tbl <- tibble(A = rnorm(n = 7, mean = 10, sd = 1),\n                      B = rnorm(n = 7, mean = 15, sd = 1)) %>% \n  gather(key = x, value = y) %>% \n  mutate(x = as_factor(x))\ncat_two_tbl\n\n# A tibble: 14 × 2\n   x         y\n   <fct> <dbl>\n 1 A     10.0 \n 2 A     10.8 \n 3 A     10.7 \n 4 A     11.0 \n 5 A      9.25\n 6 A      8.98\n 7 A      9.71\n 8 B     15.1 \n 9 B     16.0 \n10 B     14.5 \n11 B     15.1 \n12 B     14.2 \n13 B     16.8 \n14 B     15.6 \n\n\nWir wollen uns wieder die Modellmatrix einmal anschauen. Wir sehen hier schon einen Unterschied. Zum einen sehen wir, dass der Intercept für alle Beobachtungen geschätzt wird und in der zweiten Spalte nur xB steht. Somit werden in der zweiten Spalte nur die Beobachtungen in der Gruppe B berücksichtigt.\n\nmodel.matrix(y ~ x, data = cat_two_tbl)\n\n   (Intercept) xB\n1            1  0\n2            1  0\n3            1  0\n4            1  0\n5            1  0\n6            1  0\n7            1  0\n8            1  1\n9            1  1\n10           1  1\n11           1  1\n12           1  1\n13           1  1\n14           1  1\nattr(,\"assign\")\n[1] 0 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$x\n[1] \"contr.treatment\"\n\n\nWir rechnen einmal das lineare Modell und lassen uns überraschen was als Ergebnis herauskomt. Wir nutzen die Funktion model_parameter() aus dem R Paket parameters für die Ausgabe der Koeffizienten.\n\nlm(y ~ x, data = cat_two_tbl) %>% \n  model_parameters() %>% \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |       10.07\nxB          |        5.27\n\n\nNun erhalten wir als den Intercept 10.07 und die Steigung xB mit 5.27 zurück. Wir sehen, der Intercept ist der Mittelwert der Gruppe A und die das xB ist die Änderung von dem Mittelwert A zu dem Mittelwert B. Wir erhalten die Mittelwertsdifferenz \\(\\Delta_{A-B}\\) von 5.27 zurück. In Tabelle 33.2 siehst du den Zusammenhang von den Faktorleveln A und B, den jeweiligen Mittelwerte der Level sowie die Differenz zum Mittel von dem ersten Level A.\n\n\n\n\nTabelle 33.2— Zusammenhang von den Mittelwerten der Level des Faktores \\(x\\) und deren Differenz zu Level A.\n\nFactor x\nMean of level\nDifference to level A\n\n\n\nA\n10.07\n0.00\n\n\nB\n15.33\n5.27\n\n\n\n\n\n\nSteigt das \\(x\\) um 1 Einheit an, also springt von Gruppe A zu Gruppe B, so erhöht sich das \\(y\\) um den Mittelwertsunterschied \\(\\Delta_{A-B}\\).\nIn Abbildung 33.2 kannst du nochmal den visuellen Zusammenhang zwischen den einzelnen Beobachtungen und der sich ergebenen Geraden sehen. Die Gerade geht durch die beiden Mittelwerte der Gruppe A und B. Daher ist die Steigung der Mittlwertsunterschied zwischen der Gruppe A und der Gruppe B.\n\n\n\n\nAbbildung 33.2— Graphische Darstellung der Interpretation von einem kategoriellen \\(x\\) mit 2 Leveln.\n\n\n\n\n\n33.3.3 Kategorielles \\(x\\) mit >2 Leveln\nNachdem wir das Problem der Interpretation von einem kategoriellen \\(x\\) mit zwei Leveln verstanden haben, werden wir uns jetzt den Fall für ein kategoriellen \\(x\\) mit drei oder mehr Leveln anschauen. Wir nutzen hier nur drei Level, da es für vier oder fünf Level gleich abläuft. Du kannst das Datenbeispiel gerne auch noch um eine vierte Gruppe erweitern und schauen was sich da ändert.\nUnser Datensatz besteht aus drei Gruppen A, B und C mit den Mittelwerten von 10, 15 und 3. Hierbei ist wichtig, dass wir in der Gruppe C nur einen Mittelwert von 3 haben. Also wir haben keinen linearen Anstieg über die drei Gruppen.\n\nset.seed(20339537)\ncat_three_tbl <- tibble(A = rnorm(n = 7, mean = 10, sd = 1),\n                        B = rnorm(n = 7, mean = 15, sd = 1),\n                        C = rnorm(n = 7, mean = 3, sd = 1)) %>% \n  gather(key = x, value = y) %>% \n  mutate(x = as_factor(x))\ncat_three_tbl\n\n# A tibble: 21 × 2\n   x         y\n   <fct> <dbl>\n 1 A     10.0 \n 2 A     10.8 \n 3 A     10.7 \n 4 A     11.0 \n 5 A      9.25\n 6 A      8.98\n 7 A      9.71\n 8 B     15.1 \n 9 B     16.0 \n10 B     14.5 \n# … with 11 more rows\n\n\nSchauen wir uns einmal die Modellmatrix an. Wir sehen wieder, dass der Intercept über alle Gruppen geschätzt wird und wir Koeffizienten für die Gruppen B und C erhalten. Mit der Modellmatrix wird dann auch das lineare Modell geschätzt.\n\nmodel.matrix(y ~ x, data = cat_three_tbl)\n\n   (Intercept) xB xC\n1            1  0  0\n2            1  0  0\n3            1  0  0\n4            1  0  0\n5            1  0  0\n6            1  0  0\n7            1  0  0\n8            1  1  0\n9            1  1  0\n10           1  1  0\n11           1  1  0\n12           1  1  0\n13           1  1  0\n14           1  1  0\n15           1  0  1\n16           1  0  1\n17           1  0  1\n18           1  0  1\n19           1  0  1\n20           1  0  1\n21           1  0  1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$x\n[1] \"contr.treatment\"\n\n\nWir rechnen wieder das Modell mit der Funktion lm(). Wir nutzen die Funktion model_parameter() aus dem R Paket parameters für die Ausgabe der Koeffizienten zu erhalten.\n\nlm(y ~ x, data = cat_three_tbl) %>% \n  model_parameters() %>% \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |       10.07\nxB          |        5.27\nxC          |       -7.41\n\n\nWir sehen wieder, dass der Intercept der Mittelwert der Gruppe A ist. Wir hatten einen Mittelwert von 10 für die Gruppe A eingestellt und wir erhalten diesen Wert wieder. Die anderen Koeffizienten sind die Änderung zum Mittelwert von der Gruppe A. In Tabelle 33.3 ist der Zusammenhang nochmal für alle Level des Faktors \\(x\\) dargestellt. Wir haben für die Gruppe B einen Mittelwert von 15 und für die Gruppe C einen Mittelwert von 3 eingestellt. Wir erhalten diese Mittelwerte wieder, wenn wir die Differenzen zu dem Intercept bilden.\n\n\n\n\nTabelle 33.3— Zusammenhang von den Mittelwerten der Level des Faktores \\(x\\) und deren Differenz zu Level A.\n\nFactor x\nMean of level\nDifference to level A\n\n\n\nA\n10.07\n0.00\n\n\nB\n15.33\n5.27\n\n\nC\n2.66\n-7.41\n\n\n\n\n\n\nWir sehen den Zusammenhang nochmal in der Abbildung 33.3 visualisiert. Wir sehen die Änderung on der Gruppe A zu der Gruppe B sowie die Änderung von der Gruppe A zu der Gruppe C. Die Abbildung ist etwas verwirrend da wir nicht das \\(x\\) um 2 Einheiten erhöhen um auf den Mittelwert von C zu kommen. Wir rechnen sozusagen ausgehend von A die Änderung zu B und C aus. Dabei nehmen wir jedesmal an, dass die Gruppe B und die Gruppe C nur eine Einheit von \\(x\\) von A entfernt ist.\n\n\n\n\nAbbildung 33.3— Graphische Darstellung der Interpretation von einem kategoriellen \\(x\\) mit 3 Leveln."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-confounder",
    "href": "stat-modeling-basic.html#sec-confounder",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.4 Adjustierung für Confounder",
    "text": "33.4 Adjustierung für Confounder\nIm folgenden Abschnitt wollen wir einmal auf Confounder eingehen. Was sind Confounder? Zum einen gibt es kein gutes deutsches Wort für Confounder. Du könntest Confounder in etwa mit Verzerrer oder Störfakor übersetzen. Zum Anderen können Confounder nur zuammen mit anderen Vriabln in einer multiplen Regression auftreten. Das heißt, wir brauchen mindestens zwei Variablen in einer Regression. Ein Confounder verursacht einen Effekt, den wir eigentlich nicht so erwartet hätten. Wir wollen eigentlich einen Effekt schätzen, aber der Effekt ist viel größer oder kleiner, da der Effekt eigentlich von einder anderen Variable verursacht oder aber verdeckt wird. Wir können einen Confounder in beide Richtungen haben. Wichtig ist hierbei, das wir eigentlich nicht an dem Effekt des Confounders interessiert sind. Wir wollen uns zum Beispiel den Effekt einer Düngung auf das Trockengewicht anschauen, aber der Effekt den wir beobachten wird durch die unterschiedlichen Pflanzorte verursacht.\nSchauen wir uns das ganze mal für das Beispiel des Zusammenhangs von dem Flohgewicht mit der Sprungweite an. Dafür benötigen wir den Datensatz flea_dog_cat_length_weight.csv.\n\nmodel_tbl <- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %>%\n  select(animal, sex, weight, jump_length) %>% \n  mutate(animal = as_factor(animal),\n         sex = as_factor(sex))\n\nIn der Tabelle 33.4 ist der Datensatz model_tbl nochmal dargestellt.\n\n\n\n\nTabelle 33.4— Datensatz für die Confounder Adjustierung. Die Variable jump_length ist das \\(y\\) und die Variable weight das \\(x\\) von Interesse.\n\nanimal\nsex\nweight\njump_length\n\n\n\ncat\nmale\n6.02\n15.79\n\n\ncat\nmale\n5.99\n18.33\n\n\ncat\nmale\n8.05\n17.58\n\n\ncat\nmale\n6.71\n14.09\n\n\ncat\nmale\n6.19\n18.22\n\n\ncat\nmale\n8.18\n13.49\n\n\n…\n…\n…\n…\n\n\nfox\nfemale\n8.04\n27.81\n\n\nfox\nfemale\n9.03\n24.02\n\n\nfox\nfemale\n7.42\n24.53\n\n\nfox\nfemale\n9.26\n24.35\n\n\nfox\nfemale\n8.85\n24.36\n\n\nfox\nfemale\n7.89\n22.13\n\n\n\n\n\n\nWir können uns jetzt drei verschiedene Modelle anschauen.\n\nDas Modell jump_length ~ weight. Wir modellieren die Abhängigkeit von der Sprungweite von dem Gewicht der Flöhe über alle anderen Variablen hinweg.\nDas Modell jump_length ~ weight + animal. Wir modellieren die Abhängigkeit von der Sprungweite von dem Gewicht der Flöhe und berücksichtigen die Tierart des Flohes. Ignorieren aber das Geschlecht des Flohes.\nDas Modell jump_length ~ weight + animal + sex. Wir modellieren die Abhängigkeit von der Sprungweite von dem Gewicht der Flöhe und berücksichtigen alle Variablen, die wir gemessen haben.\n\nHierbei ist wichtig, dass es natürlich auch Confounder geben kann, die wir gar nicht in den Daten erhoben haben. Also, dass sich Gruppen finden lassen, die wir gar nicht als Variable mit in den Daten erfasst haben. Hier könnte eine Hauptkomponentenanalyse helfen.\n\n\n\n\n\n(a) jump_length ~ weight\n\n\n\n\n\n\n(b) jump_length ~ weight + animal\n\n\n\n\n\n\n(c) jump_length ~ weight + animal + sex\n\n\n\n\nAbbildung 33.4— Darstellung des counfounder Effekts anhand des Zusammenhangs der Sprungweite in [cm] und dem Gewicht von Flöhen [mg].\n\n\nIn Abbildung 33.4 (a) sehen wir die blaue Linie als die Ausgabe von dem Modell jump_length ~ weight. Wir sehen, dass mit dem Anstieg des Gewichtes der Flöhe auch die Sprungweite sich erhöht. Wir würden annehmen, dass wir hier einen signifikanten Unterschied vorliegen haben. Schauen wir uns die Koeffizienten des Modells aus dem lm() einmal an.\n\nlm(jump_length ~ weight, data = model_tbl) %>% \n  model_parameters()\n\nParameter   | Coefficient |   SE |        95% CI | t(598) |      p\n------------------------------------------------------------------\n(Intercept) |        9.79 | 0.77 | [8.28, 11.30] |  12.73 | < .001\nweight      |        1.34 | 0.09 | [1.16,  1.53] |  14.16 | < .001\n\n\nWir sehen, dass wir einen Effekt des Gewichts auf die Sprungweite von \\(1.34\\) vorliegen haben. Auch ist der Effekt und damit die Steigung signifikant.\nErweitern wir nun das Modell um die Tierart und erhalten die Abbildung 33.4 (b). Zum einen sehen wir, dass der globale Effekt nicht so ganz stimmen kann. Die Tierarten haben alle einen unterschiedlichen starken Effekt von dem Gewicht auf die Sprungweite. Auch hier fitten wir einmal das lineare Modell und schauen uns die Koeffizienten an.\n\nlm(jump_length ~ weight + animal, data = model_tbl) %>% \n  model_parameters()\n\nParameter    | Coefficient |   SE |       95% CI | t(596) |      p\n------------------------------------------------------------------\n(Intercept)  |        7.82 | 0.62 | [6.60, 9.03] |  12.64 | < .001\nweight       |        1.27 | 0.07 | [1.13, 1.42] |  17.08 | < .001\nanimal [dog] |        2.62 | 0.26 | [2.12, 3.13] |  10.23 | < .001\nanimal [fox] |        4.97 | 0.26 | [4.47, 5.48] |  19.38 | < .001\n\n\nDer Effekt des Gewichtes ist hier in etwa gleich geblieben. Wir sehen aber auch, dass die Sprungweite anscheinend bei Hunden und Füchsen höher ist. Daher haben wir hier einen Effekt on der Tierart. Wir adjustieren für den Confoundereffekt durch die Tierart und erhalten einen besseren Effektschätzer für das Gewicht.\nIn der letzten Abbildung 33.4 (c) wollen wir nochmal das Geschlecht der Flöhe mit in das Modell nehmen. Wir sehen, dass in diesem Fall, das Gewicht gar keinen Einfluss mehr auf die Sprungweite hat. Den Effekt des Gewichtes war nur der Effekt des unterschiedlichen Gewichtes der weiblichen und männlichen Flöhe. Schauen wir auch hier nochmal in des Modell.\n\nlm(jump_length ~ weight + animal + sex, data = model_tbl) %>% \n  model_parameters() %>% \n  mutate(Coefficient = round(Coefficient, 2))\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | t(595) |      p\n-------------------------------------------------------------------\n(Intercept) |       15.42 | 0.59 | [14.27, 16.58] |  26.23 | < .001\nweight      |        0.01 | 0.08 | [-0.16,  0.17] |   0.07 | 0.942 \nanimaldog   |        2.61 | 0.19 | [ 2.23,  2.99] |  13.50 | < .001\nanimalfox   |        5.19 | 0.19 | [ 4.81,  5.57] |  26.75 | < .001\nsexfemale   |        4.89 | 0.23 | [ 4.44,  5.34] |  21.25 | < .001\n\n\nNun können wir sehen, dass von unserem ursprünglichen Effekt von dem Gewicht auf die Sprungweite nichts mehr übrigbleibt. Wir haben gar keinen Effekt von dem Gewicht auf die Sprungweite vorliegen. Was wir gesehen haben, war der Effekt des Gewichtes der unterschiedlichen Geschlechter. Wenn wir unser Modell für die Confounder animal und sex adjustieren, haben wir einen unverzerrten Schätzer für weight. Wichtig ist nochmal, dafür müssen wir natürlich auch alle variablen erhoben haben. Hätten wir das Geschlecht der Flöhe nicht bestimt, hätten wir hier eventuell einen falschen Schluß getroffen."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-vif",
    "href": "stat-modeling-basic.html#sec-vif",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.5 Variance inflation factor (VIF)",
    "text": "33.5 Variance inflation factor (VIF)\n\n\n\n\n\n\nWir kürzen hier stark ab. Wenn du mehr über Variablen in einem Modell wissen willst, gibt es dann in den Kapitel zur Variablen Selektion mehr Informationen.\nWenn du nur Outcomes und Blöcke vorliegen hast, dann ist das VIF für dich uninteressant.\nWenn wir sehr viele Daten erheben, dann kann es sein, dass die Variablen stark miteinander korrelieren. Wir können aber stark mieinander korrelierte Variablen nicht zusammen in ein Modell nehmen. Die Effekte der beiden Variablen würden sich gegenseitig aufheben. Wir hätten eigentlich zwei getrennt signifikante Variablen. Nehmen wir aber beide Variablen mit ins Modell, sind beide Variablen nicht mehr signifikant.\nUm Variablen zu finden, die sich sehr ähnlich verhalten, können wir den Variance inflation factor (VIF) nutzen. Wir können den VIF jedoch nicht für kategoriale Daten verwenden. Statistisch gesehen würde es keinen Sinn machen. Die Anwendung ist recht einfach. Wir fitten als erstes unser Modell und dann können wir die Funktion vif() aus dem R Paket car verwenden.\n\ngummi_tbl <- read_excel(\"data/gummibears.xlsx\")\nmodel <- lm(height ~ semester + age + count_color + count_bears, data = gummi_tbl)\n\nWir sehen im Folgenden das Eregbnis der Funktion vif(). Wenn der berechnete Wert für den VIF größer als 5 ist, dann liegt mit der Variable ein Problem vor. Wir vermuten dann, dass eine andere variable sehr stark mit dieser Variable korreliert. Wir sehen an den Werten keine Aufälligkeiten.\n\nvif(model)\n\n   semester         age count_color count_bears \n  1.0132537   1.0154998   1.1326705   1.1577185 \n\n\nWir können uns mit der Funcktion check_model() aus dem R Paket performance auch die Unsicherheit mit angeben lassen. In unserem Beispiel hieft dies gerade nicht sehr viel weiter. Wir bleiben bei den geschätzen Werten und ignorieren das Intervall.\n\n\n\n\nAbbildung 33.5— Graphische Darstellung des VIF mit der Funktion check_model().\n\n\n\n\nIn Abbildung 33.5 sehen wir nochmal die Visualisierung. Wie immer, manchmal helfen solche Abbildungen, manchmal verwirren die Abbildungen mehr. Wir konzentrieren uns hier auf die Werte des VIF’s und ignorieren die Streuung."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-model-basic-compare",
    "href": "stat-modeling-basic.html#sec-model-basic-compare",
    "title": "33  Multiple lineare Regression",
    "section": "\n33.6 Vergleich von Modellen",
    "text": "33.6 Vergleich von Modellen\n\n\n\n\n\n\nWir kürzen hier stark ab bzw. gehen nicht im Detail auf jedes Gütekriterium ein. Wichtig ist, dass du Modelle statistisch vergleichen kannst. Bedenke immmer, dass die statistische Bewertung nicht immer ausreicht! Auch was die Biologie über das Modell sagt ist wichtig.\nIm Folgenden wollen wir einmal verschiedene Modelle miteinander Vergleichen und uns statistisch wiedergeben lassen, was das beste Modell ist. Und hier holen wir auch einmal kurz Luft, denn wir entschieden nur was das statistisch beste Modell ist. Es kann sein, dass ein Modell biologisch mehr Sinn macht und nicht auf Platz 1 der statistischen Maßzahlen steht. Das ist vollkommen in Ordnung. Du musst abweägen, was für sich das beste Modell ist. Im Zweifel komme ruhig nochmal in meine statistische Beratung oder schreibe mir eine Mail.\nWir bauchen uns jetzt fünf Modelle von fit_1 bis fit_5. Jedes dieser Modelle hat andere \\(x\\) aber immer das gleiche Outcome y. Das ist wichtig, wir haben im Modellvergleich immer das gleiche Outcome \\(y\\) und ändern die \\(x\\)-Terme ab. Im Weiteren sortieren wir die Modelle von einfach nach komplex. Ich versuche immmer das einfachste Modell fit_1 zu nennen bzw. eher die niedrige Nummer zu geben. Im Idealfall benennst du die Modellobjekte nach den Modellen, die in en Objekten gespeichert sind. Oft sind die Modelle aber sehr groß und die Objekte der Fits haben dann sehr lange Namen.\n\nfit_1 <- lm(jump_length ~ animal, data = model_tbl)\nfit_2 <- lm(jump_length ~ animal + sex, data = model_tbl)\nfit_3 <- lm(jump_length ~ animal + sex + weight, data = model_tbl)\nfit_4 <- lm(jump_length ~ animal + sex + sex:weight, data = model_tbl)\nfit_5 <- lm(log(jump_length) ~ animal + sex, data = model_tbl)\n\nDu kannst auch das \\(R^2\\) bzw. das \\(R^2_{adj}\\) für die Modellauswahl nehmen. Das \\(AIC\\) ist neuer und auch für komplexere Modelle geeignet.\nAls Ergänzung zum Bestimmtheitsmaß \\(R^2\\) wollen wir uns noch dasAkaike information criterion (\\(AIC\\)) anschauen. Es gilt hierbei, je kleiner das \\(AIC\\) ist, desto besser ist das \\(AIC\\). Wir wollen also Modelle haben, die ein kleines \\(AIC\\) haben. Wir gehen jetzt nicht auf die Berechnung der \\(AIC\\)’s für jedes Modell ein. Wir erhalten nur ein \\(AIC\\) für jedes Modell. Die einzelnen Werte des \\(AIC\\)’s sagen nichts aus. Ein \\(AIC\\) ist ein mathematisches Konstrukt. Wir können aber verwandete Modelle mit dem \\(AIC\\) untereinander vergleichen. Daher berechnen wir ein \\(\\Delta\\) über die \\(AIC\\). Dafür nehmen wir das Modell mit dem niedrigsten \\(AIC\\) und berechnen die jeweiligen Differenzen zu den anderen \\(i\\) Modellen. In unserem Beispiel ist \\(i\\) dann gleich fünf, da wir fünf Modelle haben.\n\\[\n\\Delta_i = AIC_i - AIC_{min}\n\\]\n\nwenn \\(\\Delta_i < 2\\), gibt es keinen Unterschied zwischen den Modellen. Das \\(i\\)-te Modell ist genauso gut wie das Modell mit dem \\(AIC_{min}\\).\nwenn \\(2 < \\Delta_i < 4\\), dann gibt es eine starke Unterstützung für das \\(i\\)-te Modell. Das \\(i\\)-te Modell ist immer noch ähnlich gut wie das \\(AIC_{min}\\).\nwenn \\(4 < \\Delta_i < 7\\), dann gibt es deutlich weniger Unterstützung für das \\(i\\)-te Modell;\nModelle mit \\(\\Delta_i > 10\\) sind im Vergleich zu dem besten \\(AIC\\) Modell nicht zu verwenden.\n\nNehmen wir ein \\(AIC_1 = AIC_{min} = 100\\) und \\(AIC_2\\) ist \\(100,7\\) an. Dann ist \\(\\Delta_2=0,7<2\\), so dass es keinen wesentlichen Unterschied zwischen den Modellen gibt. Wir können uns entscheiden, welches der beiden Modelle wir nehmen. Hier ist dann wichtig, was auch die Biologie sagt oder eben andere Kritieren, wie Kosten und Nutzen. Wenn wir ein \\(AIC_1 = AIC_{min} = 100000\\) und \\(AIC_2\\) ist \\(100700\\) vorliegen haben, dann ist \\(\\Delta_2 = 700 \\gg 10\\), also gibt es keinen Grund für das \\(2\\)-te Modell. Das \\(2\\)-te Modell ist substantiell schlechter als das erste Modell.\nNeben dem \\(AIC\\) gibt es auch das Bayesian information criterion (\\(BIC\\)). Auch beim \\(BIC\\) gilt, je kleiner das BIC ist, desto besser ist das BIC.\n\\[\np_i = \\exp\\left(\\cfrac{-\\Delta_i}{2}\\right)\n\\]\nDas \\(p_i\\) ist die relative (im Vergleich zu \\(AIC_{min}\\)) Wahrscheinlichkeit, dass das \\(i\\)-te Modell den AIC minimiert. Zum Beispiel entspricht \\(\\Delta_i = 1.5\\) einem \\(p_i\\) von \\(0.47\\) (ziemlich hoch) und ein \\(\\Delta_ = 15\\) entspricht einem \\(p_i =0.0005\\) (ziemlich niedrig). Im ersten Fall besteht eine Wahrscheinlichkeit von 47%, dass das \\(i\\)-te Modell tatsächlich eine bessere Beschreibung ist als das Modell, das \\(AIC_{min}\\) ergibt, und im zweiten Fall beträgt diese Wahrscheinlichkeit nur 0,05%.\n\nmodel_performance(fit_1) %>% \n  as_tibble() %>% \n  select(AIC, BIC) %>% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 1 × 2\n    AIC   BIC\n  <dbl> <dbl>\n1 3076. 3093.\n\n\n\ncomp_res <- compare_performance(fit_1, fit_2, fit_3, fit_4, fit_5, rank = TRUE)\n\ncomp_res\n\n# Comparison of Model Performance Indices\n\nName  | Model |    R2 | R2 (adj.) |  RMSE | Sigma | AIC weights | BIC weights | Performance-Score\n-------------------------------------------------------------------------------------------------\nfit_2 |    lm | 0.739 |     0.738 | 1.926 | 1.933 |       0.644 |       0.959 |            79.81%\nfit_5 |    lm | 0.731 |     0.729 | 0.099 | 0.099 |    2.58e-09 |    3.84e-09 |            66.01%\nfit_3 |    lm | 0.739 |     0.737 | 1.926 | 1.935 |       0.237 |       0.039 |            53.28%\nfit_4 |    lm | 0.739 |     0.737 | 1.925 | 1.935 |       0.119 |       0.002 |            49.57%\nfit_1 |    lm | 0.316 |     0.314 | 3.118 | 3.126 |   5.42e-126 |   7.27e-125 |             0.00%\n\n\n\nplot(comp_res)\n\n\n\n\n\ntest_vuong(fit_1, fit_2, fit_3, fit_4, fit_5)\n\nName  | Model | Omega2 | p (Omega2) |      LR | p (LR)\n------------------------------------------------------\nfit_1 |    lm |        |            |         |       \nfit_2 |    lm |   0.41 |     < .001 |  -18.46 | < .001\nfit_3 |    lm |   0.41 |     < .001 |  -18.45 | < .001\nfit_4 |    lm |   0.41 |     < .001 |  -18.48 | < .001\nfit_5 |    lm |   0.47 |     < .001 | -123.94 | < .001\nEach model is compared to fit_1.\n\n\nWar die Transformation sinnvoll?\n\nmodel_tbl <- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %>%\n  mutate(log_hatch_time = round(log(hatch_time), 2))\n\n\nfit_1 <- lm(hatch_time ~ animal + sex, data = model_tbl)\nfit_2 <- lm(log_hatch_time ~ animal + sex, data = model_tbl)\n\n\ncomp_res <- compare_performance(fit_1, fit_2, rank = TRUE)\n\nWarning: When comparing models, please note that probably not all models were fit\n  from same data.\n\ncomp_res\n\n# Comparison of Model Performance Indices\n\nName  | Model |    R2 | R2 (adj.) |    RMSE |   Sigma | AIC weights | BIC weights | Performance-Score\n-----------------------------------------------------------------------------------------------------\nfit_2 |    lm | 0.006 |     0.001 |   0.992 |   0.995 |        1.00 |        1.00 |            66.67%\nfit_1 |    lm | 0.008 |     0.003 | 789.441 | 792.086 |    0.00e+00 |    0.00e+00 |            33.33%"
  },
  {
    "objectID": "stat-modeling-basic.html#referenzen",
    "href": "stat-modeling-basic.html#referenzen",
    "title": "33  Multiple lineare Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nKéry, Marc. 2010. Introduction to WinBUGS for ecologists: Bayesian approach to regression, ANOVA, mixed models and related analyses. Academic Press."
  },
  {
    "objectID": "stat-modeling-outlier.html",
    "href": "stat-modeling-outlier.html",
    "title": "34  Ausreißer",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:39:43"
  },
  {
    "objectID": "stat-modeling-outlier.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-outlier.html#genutzte-r-pakete-für-das-kapitel",
    "title": "34  Ausreißer",
    "section": "\n34.1 Genutzte R Pakete für das Kapitel",
    "text": "34.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-outlier.html#daten",
    "href": "stat-modeling-outlier.html#daten",
    "title": "34  Ausreißer",
    "section": "\n34.2 Daten",
    "text": "34.2 Daten\nNachdem wir uns im vorherigen Kapitel mit einem sehr kleinen Satensatz beschäftigt haben, nehmen wir einen großen Datensatz. Bleiben aber bei einem simplen Modell. Wir brauchen dafür den Datensatz flea_dog_cat_length_weight.xlsx. In einer simplen linearen Regression schauen wir uns den Zusammenhang zwischen einem \\(y\\) und einem \\(x_1\\) an. Daher wählen wir aus dem Datensatz die beiden Spalten jump_length und weight. Wir wollen nun feststellen, ob es einen Zusammenhang zwischen der Sprungweite in [cm] und dem Flohgewicht in [mg] gibt. In dem Datensatz finden wir 400 Flöhe von Hunden und Katzen.\n\nmodel_tbl <- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %>%\n  select(animal, jump_length, weight)\n\nIn der Tabelle 37.1 ist der Datensatz model_tbl nochmal dargestellt.\n\n\n\n\nTabelle 34.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\nanimal\njump_length\nweight\n\n\n\ncat\n15.79\n6.02\n\n\ncat\n18.33\n5.99\n\n\ncat\n17.58\n8.05\n\n\ncat\n14.09\n6.71\n\n\ncat\n18.22\n6.19\n\n\ncat\n13.49\n8.18\n\n\ncat\n16.28\n7.46\n\n\n\n\n\n\nIm Folgenden ignorieren wir, dass die Sprungweiten und die Gewichte der Flöhe auch noch von den Hunden oder Katzen sowie dem unterschiedlichen Geschlecht der Flöhe abhängen könnten. Wir schmeißen alles in einen Pott und schauen nur auf den Zusammenhang von Sprungweite und Gewicht."
  },
  {
    "objectID": "stat-modeling-outlier.html#das-simple-lineare-modell",
    "href": "stat-modeling-outlier.html#das-simple-lineare-modell",
    "title": "34  Ausreißer",
    "section": "\n34.3 Das simple lineare Modell",
    "text": "34.3 Das simple lineare Modell\nWir fitten ein simples lineares Modell mit nur einem Einflussfaktor weight auf die Sprunglänge jump_length. Wir erhalten dann das Objekt fit_1 was wir dann im Weiteren nutzen werden.\n\nfit_1 <- lm(jump_length ~ weight, data = model_tbl)\n\nWir nutzen jetzt dieses simple lineare Modell für die weiteren Gütekritierien."
  },
  {
    "objectID": "stat-modeling-outlier.html#cooks-abstand",
    "href": "stat-modeling-outlier.html#cooks-abstand",
    "title": "34  Ausreißer",
    "section": "\n34.4 Cook`s Abstand",
    "text": "34.4 Cook`s Abstand\nDie Cook’sche Distanz hat im Wesentlichen eine Aufgabe. Die Cook’sche Distanz misst, wie stark sich alle angepassten Werte im Modell ändern, wenn der i-te Datenpunkt gelöscht wird.\n\n\nTabelle 34.2— Tables\n\n\n\n\n(a) Cars\n\nweight\njump_length\n\n\n\n1\n22\n\n\n2\n23\n\n\n2\n24\n\n\n3\n23\n\n\n4\n19\n\n\n5\n34\n\n\n7\n35\n\n\n3\n36\n\n\n4\n23\n\n\n5\n22\n\n\n\n\n\n\n(b) Pressure\n\nweight\njump_length\n\n\n\n1\n190\n\n\n2\n23\n\n\n2\n24\n\n\n3\n23\n\n\n4\n19\n\n\n5\n24\n\n\n7\n25\n\n\n4\n26\n\n\n5\n28\n\n\n8\n180\n\n\n\n\n\n\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n\n(b) \\(log\\)-transformierte Daten.\n\n\n\n\nAbbildung 34.1— Histogramm der nicht transfomierten und transformierten Daten.\n\n\n\nfit_2 <- lm(jump_length ~ weight, data = out_tbl)\n\n\nplot_tbl <- fit_2 %>% \n  augment %>% \n  select(weight, .cooksd)\n\n\ncooks_border <- 4/nrow(plot_tbl)\ncooks_border\n\n[1] 0.4\n\n\n\nggplot(plot_tbl, aes(weight, .cooksd)) +\n  geom_point() +\n  geom_hline(yintercept = cooks_border, color = \"red\") +\n  theme_bw()\n\n\n\nAbbildung 34.2— .\n\n\n\n\n\nremove_weight_id <- which(plot_tbl$.cooksd > cooks_border)\nout_tbl <- out_tbl[-remove_weight_id,]\n\nout_tbl\n\n# A tibble: 8 × 2\n  weight jump_length\n   <dbl>       <dbl>\n1      2          23\n2      2          24\n3      3          23\n4      4          19\n5      5          24\n6      7          25\n7      4          26\n8      5          28\n\n\nDas R Paket performance\n\ncheck_normality(fit_1)\n\nOK: residuals appear as normally distributed (p = 0.555)."
  },
  {
    "objectID": "stat-modeling-missing.html",
    "href": "stat-modeling-missing.html",
    "title": "35  Imputation fehlender Werte",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:39:49"
  },
  {
    "objectID": "stat-modeling-missing.html#was-sind-fehlende-werte",
    "href": "stat-modeling-missing.html#was-sind-fehlende-werte",
    "title": "35  Imputation fehlender Werte",
    "section": "\n35.1 Was sind fehlende Werte?",
    "text": "35.1 Was sind fehlende Werte?"
  },
  {
    "objectID": "stat-modeling-missing.html#imputation-von-fehlenden-werten",
    "href": "stat-modeling-missing.html#imputation-von-fehlenden-werten",
    "title": "35  Imputation fehlender Werte",
    "section": "\n35.2 Imputation von fehlenden Werten",
    "text": "35.2 Imputation von fehlenden Werten"
  },
  {
    "objectID": "stat-modeling-variable-selection.html",
    "href": "stat-modeling-variable-selection.html",
    "title": "36  Variablenselektion",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:39:52"
  },
  {
    "objectID": "stat-modeling-gaussian.html",
    "href": "stat-modeling-gaussian.html",
    "title": "37  Gausian lineare Regression",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:39:55"
  },
  {
    "objectID": "stat-modeling-gaussian.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-gaussian.html#genutzte-r-pakete-für-das-kapitel",
    "title": "37  Gausian lineare Regression",
    "section": "\n37.1 Genutzte R Pakete für das Kapitel",
    "text": "37.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance, emmeans, multcomp, scales)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-gaussian.html#daten",
    "href": "stat-modeling-gaussian.html#daten",
    "title": "37  Gausian lineare Regression",
    "section": "\n37.2 Daten",
    "text": "37.2 Daten\nWir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir brauchen dafür den Datensatz flea_dog_cat_length_weight.xlsx. In einer simplen linearen Regression schauen wir uns den Zusammenhang zwischen einem \\(y\\) und einem \\(x_1\\) an. Daher wählen wir aus dem Datensatz flea_dog_cat_length_weight.xlsx die beiden Spalten jump_length und weight. Wir wollen nun feststellen, ob es einen Zusammenhang zwischen der Sprungweite in [cm] und dem Flohgewicht in [mg] gibt. In dem Datensatz finden wir 400 Flöhe, wir wollen uns aber nur die ersten sieben Zeilen des Datensatzes zuerst anschauen.\n\ngaussian_tbl <- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %>%\n  mutate(animal = as_factor(animal),\n         sex = as_factor(sex))\n\nIn der Tabelle 37.1 ist der Datensatz gaussian_tbl nochmal dargestellt.\n\n\n\n\nTabelle 37.1— Datensatz mit mehreren Outcomes zu Flöhen auf verschiedenen Tierarten.\n\nanimal\nsex\nweight\njump_length\nflea_count\nhatch_time\n\n\n\ncat\nmale\n6.02\n15.79\n5\n483.6\n\n\ncat\nmale\n5.99\n18.33\n1\n82.56\n\n\ncat\nmale\n8.05\n17.58\n1\n296.73\n\n\ncat\nmale\n6.71\n14.09\n3\n140.9\n\n\ncat\nmale\n6.19\n18.22\n1\n162.2\n\n\ncat\nmale\n8.18\n13.49\n1\n167.47\n\n\n…\n…\n…\n…\n…\n…\n\n\nfox\nfemale\n8.04\n27.81\n4\n424.46\n\n\nfox\nfemale\n9.03\n24.02\n1\n349.48\n\n\nfox\nfemale\n7.42\n24.53\n3\n151.43\n\n\nfox\nfemale\n9.26\n24.35\n1\n182.68\n\n\nfox\nfemale\n8.85\n24.36\n3\n104.89\n\n\nfox\nfemale\n7.89\n22.13\n2\n62.99"
  },
  {
    "objectID": "stat-modeling-gaussian.html#fit-des-modells",
    "href": "stat-modeling-gaussian.html#fit-des-modells",
    "title": "37  Gausian lineare Regression",
    "section": "\n37.3 Fit des Modells",
    "text": "37.3 Fit des Modells\n\ngaussian_fit <- lm(jump_length ~ animal + sex + animal:sex +\n                                 weight + flea_count + hatch_time, \n                   data = gaussian_tbl)"
  },
  {
    "objectID": "stat-modeling-gaussian.html#performance-des-modells",
    "href": "stat-modeling-gaussian.html#performance-des-modells",
    "title": "37  Gausian lineare Regression",
    "section": "\n37.4 Performance des Modells",
    "text": "37.4 Performance des Modells\n\nr2(gaussian_fit)\n\n# R2 for Linear Regression\n       R2: 0.740\n  adj. R2: 0.736\n\n\n\ncheck_heteroscedasticity(gaussian_fit)\n\nOK: Error variance appears to be homoscedastic (p = 0.181).\n\n\n\ncheck_model(gaussian_fit, colors = cbbPalette[6:8], check = c(\"qq\", \"outliers\", \"pp_check\", \"homogeneity\")) \n\n\n\nAbbildung 37.1— Ausgabe ausgewählter Modelgüteplots der Funktion check_model()."
  },
  {
    "objectID": "stat-modeling-gaussian.html#interpretation-des-modells",
    "href": "stat-modeling-gaussian.html#interpretation-des-modells",
    "title": "37  Gausian lineare Regression",
    "section": "\n37.5 Interpretation des Modells",
    "text": "37.5 Interpretation des Modells\n\ngaussian_fit %>% \n  tidy %>% \n  mutate(p.value = pvalue(p.value))\n\n# A tibble: 9 × 5\n  term                  estimate std.error statistic p.value\n  <chr>                    <dbl>     <dbl>     <dbl> <chr>  \n1 (Intercept)         15.3        0.613      24.9    <0.001 \n2 animaldog            2.73       0.275       9.93   <0.001 \n3 animalfox            5.33       0.276      19.3    <0.001 \n4 sexfemale            5.07       0.331      15.3    <0.001 \n5 weight               0.00536    0.0827      0.0648 0.948  \n6 flea_count           0.0513     0.0590      0.869  0.385  \n7 hatch_time          -0.0000334  0.000101   -0.332  0.740  \n8 animaldog:sexfemale -0.235      0.389      -0.606  0.545  \n9 animalfox:sexfemale -0.276      0.389      -0.709  0.479"
  },
  {
    "objectID": "stat-modeling-gaussian.html#wie-weiter",
    "href": "stat-modeling-gaussian.html#wie-weiter",
    "title": "37  Gausian lineare Regression",
    "section": "\n37.6 Wie weiter?",
    "text": "37.6 Wie weiter?\nKapitel 29\n\nemmean_res <- gaussian_fit %>% \n  emmeans(~ animal, adjust = \"bonferroni\") \n\nNOTE: Results may be misleading due to involvement in interactions\n\nemmean_res %>% \n  contrast(method = \"pairwise\")  %>% \n  tidy(conf.int = TRUE) %>% \n  select(contrast, estimate, adj.p.value, conf.low, conf.high) %>% \n  mutate(across(where(is.numeric), round, 4)) %>% \n  mutate(adj.p.value = pvalue(adj.p.value))\n\n# A tibble: 3 × 5\n  contrast  estimate adj.p.value conf.low conf.high\n  <chr>        <dbl> <chr>          <dbl>     <dbl>\n1 cat - dog    -2.61 <0.001         -3.07     -2.16\n2 cat - fox    -5.19 <0.001         -5.65     -4.73\n3 dog - fox    -2.58 <0.001         -3.03     -2.12\n\nemmean_res %>% \n  cld(adjust = \"tukey\", Letters=letters, details = TRUE)\n\nNote: adjust = \"tukey\" was changed to \"sidak\"\nbecause \"tukey\" is only appropriate for one set of pairwise comparisons\n\n\n$emmeans\n animal emmean    SE  df lower.CL upper.CL .group\n cat      17.9 0.137 591     17.6     18.2  a    \n dog      20.5 0.137 591     20.2     20.9   b   \n fox      23.1 0.138 591     22.8     23.4    c  \n\nResults are averaged over the levels of: sex \nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \nsignificance level used: alpha = 0.05 \nNOTE: Compact letter displays can be misleading\n      because they show NON-findings rather than findings.\n      Consider using 'pairs()', 'pwpp()', or 'pwpm()' instead. \n\n$comparisons\n contrast  estimate    SE  df t.ratio p.value\n dog - cat     2.61 0.194 591  13.458  <.0001\n fox - cat     5.19 0.194 591  26.700  <.0001\n fox - dog     2.58 0.195 591  13.207  <.0001\n\nResults are averaged over the levels of: sex \nP value adjustment: tukey method for comparing a family of 3 estimates"
  },
  {
    "objectID": "stat-modeling-mixed.html",
    "href": "stat-modeling-mixed.html",
    "title": "38  Lineare gemischte Modelle",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:40:03"
  },
  {
    "objectID": "stat-modeling-mixed.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-mixed.html#genutzte-r-pakete-für-das-kapitel",
    "title": "38  Lineare gemischte Modelle",
    "section": "\n38.1 Genutzte R Pakete für das Kapitel",
    "text": "38.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom, see,\n               multcomp, emmeans, lme4, broom.mixed,\n               parameters, ggridges, scales, performance)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-mixed.html#daten",
    "href": "stat-modeling-mixed.html#daten",
    "title": "38  Lineare gemischte Modelle",
    "section": "\n38.2 Daten",
    "text": "38.2 Daten\nIn diesem fiktiven Datenbeispiel wollen wir uns die Testscores eines Intelligentest bei \\(N = 480\\) Drachen anschauen. Wir sind dafür an acht Berge gefahren und haben die dortigen Drachen an den drei Flanken des Berges getestet. Daher hat der Faktor mountain_range acht Level mit Bavarian, Ligurian, Emmental, Central, Maritime, Southern, Julian, und Sarntal. Die drei Flanken des Berges bilden wir im Faktor site mit den Leveln north, east und south ab. In Abbildung 38.1 sehen wir eine Skizze für drei Berge mit den jeweiligen Flanken, wo gemessen wurde.\n\n\nAbbildung 38.1— Beispiel für drei der acht Berge mit Bavarian, Central und Julian. Auf jeden der acht Berge wurden an drei Seiten north, east und south die Testscores der dort lebenden Drachen erhoben.\n\n\nDie Daten liegen in dem Datensatz dragons.csv ab. Wir müssen aber noch einen Faktor body_length_cat bilden in dem wir die body_length der einzelnen Drachen in Kategorien umwandeln. Wir wollen später noch Gruppenvergleiche rechnen und brauchen daher einen Faktor mit Leveln. Daher nutzen wir die Funktion case_when() um einen Faktor mit fünf Größenkategorien zu bilden. Danach müssen wir wie immer noch die character Spalten in die entsprechenden Faktoren umwandeln.\n\ndragons_tbl <- read.csv2(\"data/dragons.csv\") %>% \n  mutate(body_length_cat = \n           case_when(body_length < 170 ~ \"tiny\",\n                     body_length >= 170 & body_length < 180 ~ \"small\",\n                     body_length >= 180 & body_length < 200 ~ \"medium\",\n                     body_length >= 200 & body_length < 220 ~ \"large\",\n                     body_length >= 220 ~ \"gigantic\"),\n         body_length_cat = as_factor(body_length_cat),\n         mountain_range = as_factor(mountain_range),\n         site = factor(site, labels = c(\"north\", \"east\", \"south\"))) %>% \n  select(test_score, body_length, body_length_cat, everything())\n\nEs ergibt sich dann der Datensatz wie in Tabelle 38.1 gezeigt. Wir belassen die Körperlänge der Drachen in der kontinuierlichen Form nochmal mit in den Daten.\n\n\n\n\nTabelle 38.1— Datensatz der Testscores für die Drachen auf verschiedenen Bergen und dort an verschiedenen Flanken der Berge.\n\n\n\n\n\n\n\n\n\n\ntest_score\nbody_length\nbody_length_cat\nmountain_range\nsite\n\n\n\n1\n16.15\n165.55\ntiny\nBavarian\nnorth\n\n\n2\n33.89\n167.56\ntiny\nBavarian\nnorth\n\n\n3\n6.04\n165.88\ntiny\nBavarian\nnorth\n\n\n4\n18.84\n167.69\ntiny\nBavarian\nnorth\n\n\n5\n…\n…\n…\n…\n…\n\n\n477\n59.37\n213.58\nlarge\nSarntal\nsouth\n\n\n478\n68.75\n207.63\nlarge\nSarntal\nsouth\n\n\n479\n74.89\n198.25\nmedium\nSarntal\nsouth\n\n\n480\n65.95\n208.06\nlarge\nSarntal\nsouth\n\n\n\n\n\n\nBevor wir mit dem Modellieren beginnen, wollen wir erstmal visuell überprüfen, ob unser Outcome \\(y\\) mit dem Testscore auch normalverteilt ist. Wir benötigen für das klaissche lineare gemischte Modell ein normalverteiltes Outcome \\(y\\). In Abbildung 38.2 sehen wir das Histogramm der Verteilung des Testscores für alle \\(N = 480\\) Drachen.\n\nggplot(dragons_tbl, aes(test_score)) +\n  geom_histogram() +\n  theme_bw() \n\n\n\nAbbildung 38.2— Histogramm des Testscores über alle Datenpunkte zur Überprüfung der Annahme der Normalverteilung. Wir sehen eine approximative Normalverteilung des Testscores.\n\n\n\n\nWir können in der Abbildung 38.3 auch nochmal schauen, ob die Annahme der annährenden Normalverteilung für unseren Testscore auch für jedes Level unseres Faktors der Körperlängen gegeben ist. Wir sehen auch hier, dass der Testscore einer Normalverteilung über alle Kategorien der Körperlänge folgt.\n\nggplot(dragons_tbl, aes(y = body_length_cat, x = test_score, fill = body_length_cat)) +\n  theme_bw() +\n  stat_density_ridges() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung 38.3— Histogramm des Testscores aufgeteilt nach Kategorie der Körpergröße zur Überprüfung der Annahme der Normalverteilung. Wir sehen eine approximative Normalverteilung des Testscores für alle Kategorien der Körpergröße.\n\n\n\n\nNatürlich können wir uns hier noch weitere Abbildungen erstellen, aber hier soll es erstmal reichen. Wir sehen, dass der Testscore einer Normalverteilung folgt und dass die Varianzen vermutlich homogen sind, da die Histogramme ungefähr gleich breit sind. Ja, ein wenig unterscheiden sich die Verteilungen, aber so gravierend ist es erstmal nicht."
  },
  {
    "objectID": "stat-modeling-mixed.html#modellierung",
    "href": "stat-modeling-mixed.html#modellierung",
    "title": "38  Lineare gemischte Modelle",
    "section": "\n38.3 Modellierung",
    "text": "38.3 Modellierung\nIm Folgenden wollen wir uns verschiedene statistische Modelle anschauen um uns dem linearen gemischten Modell einmal anzunähern. Dabei beginnen wir mit einem simplen Gaussian lineare Modell mit einem Faktor \\(f_1\\):\n\\[\ny \\sim f_1\n\\]\nWir haben also nur einen Faktor in unserem Modell vorliegen und ignorieren die restlichen in den Daten vorhandenen Variablen.\nAls zweites Modell betrachten wir eine multiples Gaussian lineares Modell mit einem Faktor \\(f_1\\) und einem Blockfaktor \\(b_1\\):\n\\[\ny \\sim f_1 + b_1\n\\]\nJetzt erweitern wir das Modell nochmal um einen Block oder auch Clustereffekt. Das heißt, wir haben alle Beobachtungen nicht auf einem Feld oder in einem Stall durchgeführt, sondern an mehreren Orten.\nDer eigentliche Bruch kommt jetzt. Wie wollen wir den Effekt des Blocks betrachten? Hier entscheidet sich, ob wir den Block als festen Effekt (eng. fixed effect) oder als zufälligen Effekt (eng. random effect) ausweisen wollen. Zuerst ist dies eine Modellierungsentscheidung. Wir müssen uns also zwischen zwei Modellen entscheiden. Daher können wir auch beide Arten bauen und dann Modelle vergleichen. Machen wir dann auch am Ende des Kapitels.\n\n\nDie Idee hinter dem Modell mit festen Effekten ist, dass die beobachteten Effektgrößen von Block zu Block variieren können, was aber nur auf den Stichprobenfehler \\(\\epsilon\\) zurückzuführen ist. In Wirklichkeit sind die wahren Effektgrößen alle gleich: Sie sind fix. (siehe auch The Fixed-Effect Model)\n\nDas Modell der zufälligen Effekte geht davon aus, dass es nicht nur eine wahre Effektgröße gibt, sondern eine Verteilung der wahren Effektgrößen. Das Ziel des Modells mit zufälligen Effekten ist es daher nicht, die eine wahre Effektgröße aller Studien zu schätzen, sondern den Mittelwert der Verteilung der wahren Effekte. (siehe auch The Random-Effect Model)\n\nDaher kommt jetzt als drittes Model ein multiples Gaussian lineares gemischtes Modell mit einem festen Faktor \\(f_1\\) und einem zufälligen Blockfaktor \\(z_1\\):\n\\[\ny \\sim f_1 + 1|z_1\n\\]\nWir schreiben in R den Term für da zufällige Modell in der Form \\(z_0|z_1\\). Meist setzen wir den Intercept \\(z_0\\) für den zufälligen Effekt auf 1.\nAbschießend schauen wir uns noch ein multiples Gaussian lineares gemischtes Modell mit einem festen Faktor \\(f_1\\) und einem zufälligen Blockfaktor \\(z_2\\) genested in einem einem zufälligen Blockfaktor \\(z_1\\):\n\\[\ny \\sim f_1 + 1|z_1/z_2\n\\]\nWir sagen nested, wenn wir meinen, dass ein Faktor in einen anderen Faktor verschränkt ist. Die Klassen einer Schule sind in der Schule genested.\nDas heißt, dass der zufällige Blockfaktor \\(z_2\\) in den zufälligen Blockfaktor \\(z_1\\) genested ist. Das heist, die Faktorlevel des Blockfaktors \\(z_2\\) finden sich jeweils nur in jeweils einem der Faktorlevel des Blocks \\(z_1\\). Das klingt jetzt etwas schräg, also einmal ein Beispiel. Wir haben eine Schule, dann sind die Schulklassen dieser Schule in der Schule genested. Es gibt diese spezifischen Klassen mit den Schülern schlichtweg nicht in anderen Schulen.\nBevor wir jetzt mit dem Modellieren beginnen, müssen wir noch kurz in einem QQ-Plot schauen, ob unser Ourcome testscore auch ungefähr normalverteilt ist. Abbildung 38.4 zeigt den QQ-Plot des Testscores. Wir sehen, dass der Hauptteil der Beobachtungen auf der Geraden liegt und wir nehmen daher an, dass der Testscore zumindest approximativ normalverteilt ist. Wir können also mit einem gaussian linearen gemischten Modell weitermachen.\n\nggplot(dragons_tbl, aes(sample = test_score)) +\n  stat_qq() + stat_qq_line(color = \"red\") +\n  theme_bw() +\n  scale_color_okabeito()\n\n\n\nAbbildung 38.4— QQ-Plot zu Überprüfung, ob der Testscore einer Normalverteilung folgt. Die Punkte liegen ungefähr auf der Geraden als Winkelhalbierende, so dass wi eine Normalverteilung des Testscores annehmen können.\n\n\n\n\nSchauen wir uns nun als erstes das Modell lm_simple_fit einmal an. Wir bauen das Modell nur mit der Faktorvariable body_length_cat. Wir erhalten dann gleich die Ausgabe des Modells über die Funktion model_parameters() in einer aufgearbeiteten Form.\n\nlm_simple_fit <- lm(test_score ~ body_length_cat, data = dragons_tbl)\n\nlm_simple_fit %>% model_parameters()\n\nParameter                  | Coefficient |   SE |         95% CI | t(475) |      p\n----------------------------------------------------------------------------------\n(Intercept)                |       24.61 | 4.33 | [16.10, 33.12] |   5.68 | < .001\nbody length cat [small]    |        2.63 | 5.39 | [-7.96, 13.21] |   0.49 | 0.626 \nbody length cat [medium]   |       24.89 | 4.68 | [15.70, 34.07] |   5.32 | < .001\nbody length cat [large]    |       32.50 | 4.55 | [23.56, 41.43] |   7.15 | < .001\nbody length cat [gigantic] |       29.32 | 5.20 | [19.10, 39.54] |   5.64 | < .001\n\n\nDer Intercept beinhaltet den Mittelwert für die Drachen des Levels [tiny]. Die jeweiligen Koeffizienten dann die Abweichung von den Drachen des Levels [tiny]. Daher sind Drachen des Levels [small] ungefähr um \\(2.63\\) Einheiten intelligenter. Wir sehen dann an dem \\(p\\)-Wert, ob sich die Koeffizienten signifikant von 0 unterscheiden. In Abbildung 38.5 sehen wir nochmal die Boxplots der einzelnen Testscores aufgeteilt nach der Körpergröße. Wir erkennen, dass die kleineren Drachen tendenziell dümmer sind als die großen Drachen. Wir sehen zwei Plateaus.\n\nggplot(dragons_tbl, aes(x = body_length_cat, y = test_score, fill = body_length_cat)) +\n  theme_bw() +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung 38.5— Boxplots der einzelnen Testscores aufgeteilt nach der Körpergröße.\n\n\n\n\nNun haben wir aber nicht nur die Körpergrößen gemessen sondern auch auf welchem Berg wir die jeweiligen Drachen gefunden haben. Nun könnte es sein, dass der Berg einen viel größeren Einfluss auf die Inteliegenz hat als die Drachenkörpergröße. Wir könnten einen Confoundereffekt durch die Berge vorliegen haben. Ergänzen wir also das Modell um den Faktor mountain_range und erhalten das Modell lm_mountain_fit.\n\nlm_mountain_fit <- lm(test_score ~ body_length_cat + mountain_range, data = dragons_tbl)\nlm_mountain_fit %>% model_parameters()\n\nParameter                  | Coefficient |   SE |         95% CI | t(468) |      p\n----------------------------------------------------------------------------------\n(Intercept)                |       20.93 | 3.34 | [14.36, 27.49] |   6.27 | < .001\nbody length cat [small]    |        1.67 | 3.89 | [-5.98,  9.32] |   0.43 | 0.668 \nbody length cat [medium]   |        3.55 | 3.72 | [-3.76, 10.85] |   0.95 | 0.341 \nbody length cat [large]    |        3.59 | 4.30 | [-4.86, 12.03] |   0.83 | 0.405 \nbody length cat [gigantic] |        0.08 | 4.85 | [-9.45,  9.60] |   0.02 | 0.988 \nmountain range [Ligurian]  |       17.33 | 3.58 | [10.28, 24.37] |   4.83 | < .001\nmountain range [Emmental]  |       15.91 | 3.63 | [ 8.79, 23.04] |   4.39 | < .001\nmountain range [Central]   |       35.62 | 3.69 | [28.36, 42.88] |   9.64 | < .001\nmountain range [Maritime]  |       48.75 | 3.24 | [42.39, 55.11] |  15.06 | < .001\nmountain range [Southern]  |        8.47 | 2.74 | [ 3.08, 13.85] |   3.09 | 0.002 \nmountain range [Julian]    |       45.74 | 3.86 | [38.15, 53.33] |  11.85 | < .001\nmountain range [Sarntal]   |       41.03 | 3.30 | [34.54, 47.53] |  12.42 | < .001\n\n\nWie wir sehen, werden nun die Körpergrößen der Drachen nicht mehr als signifikant ausgegeben. Die Effekte der Körpergröße auf den Testscore sind auch viel kleiner geworden, wenn wir die mountain_range mit in das Modell nehmen. Anscheinend hat der Berg auf dem wir den Drachen getroffen haben einen viel größeren Einfluss auf die Intelligenz als die Körpergröße. Wir können uns den Zusammenhang zwischen dem Testscore und dem Berg auch in der Abbildung 38.6 einmal anschauen.\nEigentlich würden wir erwarten, dass es keinen Effekt der Berge auf den Testscore der Drachen gibt. Es müsste eigentlich egal sein, wo wir einen Drachen befragen, wenn wir nur an der Körpergröße und dem Testscore interessiert sind. Wir sehen jedoch in der Abbildung 38.6 einen klaren Unterschied zwischen den Bergen im Bezug auf den Testscore.\n\nggplot(dragons_tbl, aes(mountain_range, test_score, fill = mountain_range)) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito()\n\n\n\nAbbildung 38.6— Boxplots der einzelnen Testscores aufgeteilt nach dem Berg der Befragung.\n\n\n\n\nIn der Abbildung 38.7 sehen wir den Zusammenhang von Testscore und der Körpergröße sowie den Bergen auf denen das Interview stattgefunden hat. so langsam dämmert uns warum wir hier einen Effekt der Körperlänge zu dem Testscore sehen. Die kleineren Drache sind alle nur auf bestimmten Bergen zu finden! Betrachten wir die Berge mit in dem Modell, dann hat die Körpergröße keinen Einfluß mehr.\n\nggplot(dragons_tbl, aes(x = body_length_cat, y = test_score, fill = mountain_range)) +\n  geom_boxplot(position = position_dodge(preserve = \"single\")) +\n  theme_bw() +\n  scale_fill_okabeito() +\n  labs(fill = \"Mountain\")\n\n\n\nAbbildung 38.7— Boxplots der einzelnen Testscores aufgeteilt nach dem Berg der Befragung und der Körpergröße.\n\n\n\n\nDer Zusammenhang wird vielleicht in Abbildung 38.8 nochmal klarer. Hier schauen wir uns den Zusamenhang wieder für die Körperlänge getrennt für die Berge an. Nur zeichnen wir jetzt jeden einzelnen Berg in ein Subplot. Wir sehen, dass es hier fast keinen Unterschied macht, wie lang die Drachen sind. Der Testscore ist immer gleich. Was einen Unterschied macht, sind die Berge.\n\nggplot(dragons_tbl, aes(x = body_length_cat, y = test_score, fill = mountain_range)) +\n  geom_boxplot(position = position_dodge(preserve = \"single\")) +\n  theme_bw() +\n  scale_fill_okabeito() +\n  labs(fill = \"Mountain\") +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ mountain_range) \n\n\n\nAbbildung 38.8— Boxplots der einzelnen Testscores aufgeteilt nach dem Berg der Befragung und der Körpergröße in getrennten Subplots.\n\n\n\n\nSchauen wir uns nun einmal ein lineares gemischtes Modell an. Wir nutzen daszu das R Paket lme4. Wir haben auch noch andere Pakete zur Aswahl, aber wir nutzen hier erstmal das gängiste Paket. Um ein lineares gemischtes Modell in R zu schätzen nutzen wir die Funktion lmer(). Die Funktion lmer() nimmt an, dass das Outcome test_score normalverteilt ist. Wir haben diese Annahme ja weiter oben in dem QQ-Plot überprüft.\nIn einem lineare gemischten Modell müssen wir die festen Effekte sowie die zufälligen Effekte definieren. Die festen Effekte werden ganz normal wie wir es gewohnt sind in das Modell eingegeben. Die zufälligen Effkete schreiben wir in eine Klammer in der Form (1|).\nWir schreiben (1|moutain_range) und definieren damit die Variable mountain_range als zufälligen Effekt im Modell. Wir schreiben 1| vor mountain_range, da wir für jeden Berg die gleiche Steigung von Körperlänge und Testscore annehmen. Wir können dann später noch das Model komplizierter aufbauen und jedem Berg eine eigene Steigung erlauben. Bauen wir uns jetzt erstmal ein lineares gemischtes Modell mit einem festen Effekt body_length_cat und einem zufälligen Effekt (1|mountain_range).\n\nlmer_1_fit <- lmer(test_score ~ body_length_cat + (1 | mountain_range), data = dragons_tbl)\nlmer_1_fit %>% model_parameters()\n\n# Fixed Effects\n\nParameter                  | Coefficient |   SE |         95% CI | t(473) |      p\n----------------------------------------------------------------------------------\n(Intercept)                |       46.85 | 7.43 | [32.25, 61.45] |   6.30 | < .001\nbody length cat [small]    |        1.68 | 3.89 | [-5.97,  9.33] |   0.43 | 0.666 \nbody length cat [medium]   |        4.08 | 3.71 | [-3.20, 11.37] |   1.10 | 0.271 \nbody length cat [large]    |        4.49 | 4.27 | [-3.90, 12.88] |   1.05 | 0.293 \nbody length cat [gigantic] |        1.03 | 4.81 | [-8.43, 10.49] |   0.21 | 0.831 \n\n# Random Effects\n\nParameter                      | Coefficient |   SE |         95% CI\n--------------------------------------------------------------------\nSD (Intercept: mountain_range) |       18.21 | 4.94 | [10.70, 30.98]\nSD (Residual)                  |       14.96 | 0.49 | [14.03, 15.95]\n\n\nUnser Model sieht etwas aufgeräumter aus. Als feste Effekte haben wir nur noch die Körperlänge body_length_cat und die dazugehörigen Koeffizienten des Modells. Unsere Variable mountain_range verschwindet dann in den zufälligen Effekten. Die Funktion summary liefert uns den gesamten Ausdruck, der etwas überwältigend ist. Vieles brauchen wir auch nicht davon.\n\nlmer_1_fit %>% summary()\n\n\n\n\n\nWas wir extrahieren wollen ist die Information von den zufälligen Effekten. Wir wollen wissen, wieviel Varianz durch die zufälligen Effekte erklärt wird. Wir nutzen dazu die Funktion VarCorr(), die uns erlaubt die Information zu en zufälligen Effekten zu extrahieren und auszugeben.\n\nprint(VarCorr(lmer_1_fit), comp = \"Variance\")\n\n Groups         Name        Variance\n mountain_range (Intercept) 331.422 \n Residual                   223.828 \n\n\nWieviel Varianz erklären nun die Berge? Wir können die erklärte Varianz der zufälligen Effekte einfach berechnen. Wir vergleichen die erklärte Varianz von mountain_range mit der gesamten Varianz. Die gesamte Varianz ist die Varianz aller zufälligen Effekte plus der residualen Vamrianz. Wir erhalten dann \\(R^2_{random} = 339.7/(339.7 + 223.8) \\approx 0.60\\). Wir sehen, dass ca. 60% der Varianz in unseren Daten von der Variable mountain_range verursacht wird.\nWir können die Funktion model_performance() nutzen um mehr über den Fit des Modells zu erfahren. Das R2 (cond.) ist faktisch das gleiche wie wir gerade oben berechnet haben. Wir benötigen also nicht immer den Ausdruck der zufälligen Effekte. Wir können auch die Informationen aus der Funktion model_performance() nehmen.\n\nlmer_1_fit %>% model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n----------------------------------------------------------------------------------\n3983.403 | 3983.640 | 4012.620 |      0.598 |      0.004 | 0.597 | 14.774 | 14.961\n\n\nIn der Abbildung Abbildung 38.9 schauen wir uns nochmal an, ob wir das Modell auch gut gefittet haben. Der Residualplot sieht gut aus, wir erkennen kein Muster. Ebenso sieht der QQ-Plot gut aus, die Beobachtungen liegen alle auf der Geraden. Wir sind mit dem Modell soweit erstmal ganz zufrieden.\n\n\n\n\n\n(a) Residualplot\n\n\n\n\n\n\n(b) QQ-Plot\n\n\n\n\nAbbildung 38.9— Visuelle Überprüfung des Modells mit dem Residual und QQ-Plot.\n\n\nWir haben noch eine Variable in unseren Daten ignoriert. Wir haben uns bis jetzt nicht die Variabl site angeschaut. Auf jedem Berg haben wir die Drachen noch auf verschiedenen Flanken des Berges site befragt. Das heißt, wir haben die Variable site, die in der Variable mountain_site genestet ist. Wir schreiben daher ein neues Modell und nutzen die Schreibweise (1|mountain_range/site) um zu beschreiben, dass site immer zusamen in einem Berg vorkommt. Schaue dir dazu nochmal die Abbidlung ganz zu Beginn dieses Kapitels an um die Zusammenhänge nochmal visualisiert zu bekommen.\n\nlmer_2_fit <- lmer(test_score ~ body_length_cat + (1|mountain_range/site), data = dragons_tbl) \nlmer_2_fit %>% model_parameters()\n\n# Fixed Effects\n\nParameter                  | Coefficient |   SE |          95% CI | t(472) |      p\n-----------------------------------------------------------------------------------\n(Intercept)                |       46.89 | 7.80 | [ 31.56, 62.22] |   6.01 | < .001\nbody length cat [small]    |        1.32 | 3.99 | [ -6.51,  9.16] |   0.33 | 0.740 \nbody length cat [medium]   |        3.34 | 4.61 | [ -5.73, 12.41] |   0.72 | 0.470 \nbody length cat [large]    |        4.85 | 5.15 | [ -5.26, 14.97] |   0.94 | 0.346 \nbody length cat [gigantic] |        1.37 | 5.83 | [-10.08, 12.83] |   0.24 | 0.814 \n\n# Random Effects\n\nParameter                           | Coefficient |   SE |         95% CI\n-------------------------------------------------------------------------\nSD (Intercept: site:mountain_range) |        4.79 | 1.29 | [ 2.83,  8.12]\nSD (Intercept: mountain_range)      |       17.88 | 4.97 | [10.36, 30.84]\nSD (Residual)                       |       14.46 | 0.48 | [13.55, 15.43]\n\n\nDas Modell hat nun einen weiteren zufälligen Effekt. Es werden jetzt auch nochmal für jeden Berg die Flankeneffekte mit berücksichtigt. Hat das überhaupt einen Einfluss auf das Modell? Schauen wir uns einmal die Modellgüte mit der Funktion model_performance() an.\n\nlmer_2_fit %>% model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n----------------------------------------------------------------------------------\n3970.693 | 3970.999 | 4004.084 |      0.623 |      0.004 | 0.621 | 14.120 | 14.460\n\n\nWir sehen, dass sich die erklärte varianz leicht erhöht hat. Die \\(R^2_{random}\\) liegt jetzt bei \\(0.623\\) also fast 62%. Etwas besser als vorher, aber auch nicht unbedingt sehr viel mehr.\nWie können wir nun unsere vier Modelle miteinander vergleichen? Wir haben ja folgende Modelle vorliegen:\n\nDas simple lineare Modell lm_simple_fit mit test_score ~ body_length_cat.\nDas multiple lineare Modell lm_mountain_fit mit test_score ~ body_length_cat + mountain_range.\nDas gemischte lineare Modell lmer_1_fit mit test_score ~ body_length_cat + (1|mountin_range).\nDas genestete gemischte lineare Modell lmer_2_fit mit test_score ~ body_length_cat + (1|mountain_range/site).\n\nUm die Modelle miteinander zu vergleichen können wir die Funktion compare_performance() nutzen. Wir erhalten mit der Option rank = TRUE auch eine Sortierung der Modelle wieder. Das beste Modell steht dann ganz oben.\n\ncompare_performance(lm_simple_fit, lm_mountain_fit, lmer_1_fit, lmer_2_fit, rank = TRUE)\n\n# Comparison of Model Performance Indices\n\nName            |   Model |   RMSE |  Sigma | AIC weights | BIC weights | Performance-Score\n-------------------------------------------------------------------------------------------\nlm_mountain_fit |      lm | 14.772 | 14.960 |       1.000 |       0.330 |            83.11%\nlmer_2_fit      | lmerMod | 14.120 | 14.460 |    5.84e-05 |       0.655 |            75.00%\nlmer_1_fit      | lmerMod | 14.774 | 14.961 |    1.72e-07 |       0.016 |            46.11%\nlm_simple_fit   |      lm | 20.661 | 20.770 |    1.24e-67 |    9.06e-62 |             0.00%\n\n\nIn diesem Beispiel wäre sogar eine multiple lineare Regression das beste Modell. Wir würden also auch mit zwei festen Effekten die Variabilität der Berge richtig mdellieren. Der Effekt der Flanken auf den Testscore scheint ziemlich klein zu sein, so dass wir auch auf die Variable site verzichten können.\nWas machen wir jetzt noch zum Schluß? Wir machen noch einen paarweisen Vergleich über alle Level der Vaeiable body_length_cat. Ich will hier nochmal zeigen, wie du einen multiplen Vergleich mit einem gemischten Modell in R rechnen kannst. Wir nutzen hier dann das R Paket emmeans um das compact letter display nutzen zu können.\nAls erstes nutzen wir die Funktion emmeans um die multiplen Vergleich über alle Level des Faktors body_length_cat zurechnen.\n\nres_lmer <- lmer_2_fit %>% \n  emmeans(~ body_length_cat) \n\nIm Weiteren nutzen wir jetzt das Objekt res_lmer um die Vergleiche zu rechnen und zu asjustieren. Wir nutzen die Bonferroni Methode für die Adjustierung der \\(p\\)-Werte.\n\nres_lmer %>% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\n contrast          estimate   SE  df t.ratio p.value\n tiny - small       -1.3218 4.02 453  -0.329  1.0000\n tiny - medium      -3.3379 4.76 118  -0.701  1.0000\n tiny - large       -4.8531 5.31 148  -0.914  1.0000\n tiny - gigantic    -1.3744 6.02 152  -0.228  1.0000\n small - medium     -2.0161 3.83 134  -0.526  1.0000\n small - large      -3.5313 4.50 179  -0.784  1.0000\n small - gigantic   -0.0526 5.32 175  -0.010  1.0000\n medium - large     -1.5152 2.39 362  -0.634  1.0000\n medium - gigantic   1.9635 3.71 229   0.529  1.0000\n large - gigantic    3.4787 2.95 241   1.178  1.0000\n\nDegrees-of-freedom method: kenward-roger \nP value adjustment: bonferroni method for 10 tests \n\n\nWenn wir an dem compact letter display interessiert sind, dann müsen wir die Funktion cld() nutzen. Was wir brauchen, hängt dann immer davon ab, was wir zeigen wollen und was die Fragestellung ist.\n\nres_lmer_cld <- res_lmer %>% \n  cld(adjust = \"bonferroni\", Letters = letters) %>% \n  tidy() %>% \n  select(body_length_cat, estimate, conf.low, conf.high, .group) %>% \n  mutate(across(where(is.numeric), round, 2))\n\nres_lmer_cld \n\n# A tibble: 5 × 5\n  body_length_cat estimate conf.low conf.high .group\n  <chr>              <dbl>    <dbl>     <dbl> <chr> \n1 tiny                46.9     23.6      70.2 \" a\"  \n2 small               48.2     25.5      71.0 \" a\"  \n3 gigantic            48.3     25.8      70.7 \" a\"  \n4 medium              50.2     27.8      72.7 \" a\"  \n5 large               51.7     29.2      74.2 \" a\"  \n\n\nAn dem compact letter display sehen wir schon, dass es keinen Unterschied zwischen den Gruppen bzw. Leveln des Faktors body_length_cat gibt. Wir sehen bei allen Leveln ein a. Wir haben keine signifikante Unterschiede.\nIn Abbildung 38.10 siehst du nochmal die Daten zusammen mit dem compact letter display dargestellt.\n\nggplot() +\n  theme_bw() +\n  geom_point(data = dragons_tbl, aes(x = body_length_cat, y = test_score)) +\n  geom_text(data = res_lmer_cld, \n            aes(x = body_length_cat , y = estimate, label = .group),\n            position = position_nudge(x = 0.2), color = \"red\") +\n  geom_errorbar(data = res_lmer_cld,\n                aes(ymin = conf.low, ymax = conf.high, x = body_length_cat),\n                color = \"red\", width = 0.1,\n                position = position_nudge(x = 0.1)) +\n  geom_point(data = res_lmer_cld, \n             aes(x = body_length_cat , y = estimate),\n             position = position_nudge(x = 0.1), color = \"red\") +\n  scale_color_okabeito() +\n  labs(x = \"Körperlänge in Kategorien\", y = \"Testscore\", \n       caption = \"Schwarze Punkte stellen Rohdaten dar.\n       Rote Punkte und Fehlerbalken stellen bereinigte Mittelwerte mit 95% Konfidenzgrenzen pro Behandlung dar.\n       Mittelwerte, mit einem gemeinsamen Buchstaben, sind nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 38.10— Scatterplot der Körperlängen zusammen mit den 95% Konfidenzintervall und dem compact letter display.\n\n\n\n\nManchmal wollen wir auch die 95% Konfidenzintervalle anzeigen, dann müssen wir wiederum die Funktion contrast() nutzen. Wir lassen uns auch hier die adjustoerten \\(p\\)-Werte wiedergeben. Wir nutzen dann das Objekt res_lmer_tbl um die 95% Konfidenzintervalle zu plotten.\n\nres_lmer_tbl <- res_lmer %>% \n  contrast(method = \"pairwise\") %>% \n  tidy(conf.int = TRUE) %>% \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) %>% \n  select(contrast, estimate, p.value,\n         conf.low, conf.high) \n\nres_lmer_tbl\n\n# A tibble: 10 × 5\n   contrast          estimate p.value conf.low conf.high\n   <chr>                <dbl> <chr>      <dbl>     <dbl>\n 1 tiny - small         -1.32 0.997     -12.3       9.68\n 2 tiny - medium        -3.34 0.956     -16.5       9.85\n 3 tiny - large         -4.85 0.891     -19.5       9.81\n 4 tiny - gigantic      -1.37 >0.999    -18.0      15.2 \n 5 small - medium       -2.02 0.985     -12.6       8.58\n 6 small - large        -3.53 0.935     -15.9       8.87\n 7 small - gigantic     -0.05 >0.999    -14.7      14.6 \n 8 medium - large       -1.52 0.969      -8.06      5.03\n 9 medium - gigantic     1.96 0.984      -8.23     12.2 \n10 large - gigantic      3.48 0.764      -4.64     11.6 \n\n\nIn Abbildung 38.11 sehen wir die 95% Konfidenzintervalle für alle paarweisen Vergleiche der Körperlängen.\n\nggplot(res_lmer_tbl, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1) + \n  geom_point() +\n  coord_flip() +\n  theme_bw()  +\n  labs(x = \"Vergleich\", y = \"Mittelwertsunterschied des Gewichtes [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die bereinigten Mittelwertsunterschiede mit 95% Konfidenzgrenzen dar.\n       Enthält ein 95% Konfidenzintervalle die 0 ist es nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 38.11— Abbildung der 95% Konfidenzintervallefür alle paarweisen Vergleiche der Körperlängen."
  },
  {
    "objectID": "stat-modeling-mixed.html#nested",
    "href": "stat-modeling-mixed.html#nested",
    "title": "38  Lineare gemischte Modelle",
    "section": "\n38.4 Nested",
    "text": "38.4 Nested\nhttps://stats.stackexchange.com/questions/228800/crossed-vs-nested-random-effects-how-do-they-differ-and-how-are-they-specified\nhttps://www.statology.org/nested-anova-in-r/"
  },
  {
    "objectID": "experimentel-design-preface.html",
    "href": "experimentel-design-preface.html",
    "title": "Experimentelles Design",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:40:15\nWas ist ein experimentelles Design? Wir unterscheiden zuerst nach dem Organismus, den wir uns anschauen. Wenn wir über Tiere und Pflanzen sprechen, dann haben wir meist ein feststehendes experimentelles Design. Wir haben ein Feldexperiment vorliegen, dass natürlich mit Tieren auch in einem Stall stattfinden kann. Wenn wir uns Menschen betrachten, dann haben wir eine klinische Studie vorliegen. Wir werden uns in den folgenden Kapiteln erstmal nur mit dem experimentellen Design für Pflanzen und Tiere beschäftigen. Der große Unterschied its zum einen ethischer Natur und rein praktisch, dass sich Menschen frei bewegen können und Pflanzen sowie Tiere in einem Experiement nicht. Ja, es gibt natürlich noch weitreichende moralische und ethische Unterschiede. Deshalb jetzt erstmal das klassische Feldexperiement."
  },
  {
    "objectID": "experimentel-design-preface.html#übersicht-der-experimentellen-designs",
    "href": "experimentel-design-preface.html#übersicht-der-experimentellen-designs",
    "title": "Experimentelles Design",
    "section": "Übersicht der experimentellen Designs",
    "text": "Übersicht der experimentellen Designs\nWir schauen uns in den folgenden Kapiteln einmal eine Auswahl an experimentellen Designs an. Im Laufe derZeit werden sicherlich noch andere Designs ergänzt werden. Soweit erstmal diese Auswahl hier.\n\nDas Complete randomized design findest du in Kapitel 39.2. Das Complete randomized design ist der Klassiker unter den experimentellen Designs und wird häufig verwendet.\nDas Randomized vomplete block design findest du in Kapitel 39.3. Das Randomized vomplete block design ist entweder eine Erweiterung des Complete randomized design oder aber bringt noch eine neuen Faktor für die Wiederholung mit in das Experiment mit ein.\nDas Latin square design findest du in Kapitel 39.4. Das Latin square design liefert eine gleichmäßige Aufteilung der experimentellen Einheiten über ein Feld oder ein Stall.\nDas Alpha design findest du in Kapitel 39.5 und ist eine etwas komplexere Einteilung für die Randomisierung.\nDas Augmented design findest du in Kapitel 39.8. Das Augmented design wird seltener genutzt.\nDas Splot plot design findest du in Kapitel 39.7. Das Splot plot design gibt es in vielen Varianten. Wir schauen uns hier eine der häufigsten Varianten einmal an. Je nachdem wie man die Plots anordnet ergibt sich dann auch teilweise ein anderes Splot plot design.\n\nDormann (2013) und Hurlbert (1984)"
  },
  {
    "objectID": "experimentel-design-preface.html#referenzen",
    "href": "experimentel-design-preface.html#referenzen",
    "title": "Experimentelles Design",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer.\n\n\nHurlbert, Stuart H. 1984. „Pseudoreplication and the design of ecological field experiments“. Ecological monographs 54 (2): 187–211."
  },
  {
    "objectID": "experimentel-design-basic.html",
    "href": "experimentel-design-basic.html",
    "title": "\n39  Grundlagen der Versuchsplanung\n",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:40:19\nIn diesem Kapitel wollen wir uns mit der Auswertung von verschiedenen experiemnetellen Designs beschäftigen. Wir schauen uns dafür jeweils eine mögliche Visualisierung an und bauen uns dann die Daten künstlich nach. Warum eigentlich künstliche Daten? Das heist wir erschaffen uns Daten wo wir genau wissen, wie der Mittelwert und die Standardabweichungen in den einzelnen Gruppen sind. Warum ist das hilfreich? Dadurch das wir wissen, dass der Mittelwertsunterschied zwischen Gruppe \\(A\\) und Gruppe \\(B\\) mit dem Effekt von \\(\\Delta_{A-B} = 5\\) erschaffen wurde, können wir dann auch die Ausgaben der Funktionen besser bewerten."
  },
  {
    "objectID": "experimentel-design-basic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "experimentel-design-basic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "\n39  Grundlagen der Versuchsplanung\n",
    "section": "\n39.1 Genutzte R Pakete für das Kapitel",
    "text": "39.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, emmeans, multcomp, scales, performance,\n               effectsize, parameters)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette <- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "experimentel-design-basic.html#sec-crd",
    "href": "experimentel-design-basic.html#sec-crd",
    "title": "\n39  Grundlagen der Versuchsplanung\n",
    "section": "\n39.2 Complete randomized design (CRD)",
    "text": "39.2 Complete randomized design (CRD)\nDas komplette randomizierte Design (eng. complete randomized design) ist das simpleste Felddesign was wir anzubieten haben. Wir haben einen Stall oder ein Feld oder einen Tisch und unterteilen diesen Raum zufällig in Untereinheiten. Auf oder in jeder Untereinheit bringen wir dann eine Behandlung aus.\n\n\nAbbildung 39.1— Visualisierung des complete randomized design mit einer Behandlung und vier Behandlungsleveln.\n\n\nWir haben einen Tisch und stellen Töpfe mit Pflanzen auf den Tisch. Jeder Topf erhält zufällig eine Behandlung. Wir haben gleich viele Töpfe mit Pflanzen für jede Behandlung.\nWir haben einen Stall mit Buchten für Schweine. Jede Bucht erhält eine zufällige Behandlung. Wir haben gleich viele Buchten für jede Behandlung.\nWir haben ein Feld und erschaffen Parzellen auf dem Feld. Auf jeder Parzelle wird zufällig eine Variante ausgebracht. Wir haben geich viele Parzellen für jede Variante.\nSchauen wir uns das Complete randomized design einmal an einem konkreten Beispiel an. Wir nutzen dafür einen Faktor mit der Behandlung. Die Behandlung hat vier Level mit den einzelnen Leveln \\(A\\), \\(B\\), \\(C\\) und \\(D\\).\n\n39.2.1 Visualisierung\nIn Abbildung 39.2 sehen wir die Visualisierung unseres Versuches. Wir haben einen großen Raun in dem sich zufällig die Level der Behandlung drauf verteilen. Hierbei ist es wichtig zu verstehen, dass die Anordnung rein zufällig ist. Wir sehen, dass jedes Level der Behandlung mit \\(n = 5\\) auf das Feld aufgebracht wurde. Wir haben also ein balanciertes Design mit \\(N = 20\\) Beobachtungen. Wir könnten hier auch einen Tisch mit \\(n=20\\) Pflanzentöpfen vorliegen haben oder einen Stall mit \\(n = 20\\) Buchten.\n\n\nAbbildung 39.2— Visualisierung des complete randomized design mit einer Behandlung und vier Behandlungsleveln.\n\n\n\n39.2.2 Daten\nIm Folgenden bauen wir uns die Daten für das Complete randomized design. Dafür nuten wir die Funktion rnorm(). Die Funktion rnorm() erlaubt es aus einer Normalverteilung n Beobachtungen mit einem Mittelwert mean und einer Standardabweichung sd zu ziehen. Wir erschaffen uns so vier Behandlungsgruppen \\(A\\) bis \\(D\\) mit jeweils unterschiedlichen Mittelwerten von \\(\\bar{y}_A = 10\\), \\(\\bar{y}_B = 12\\), \\(\\bar{y}_C = 16\\) und \\(\\bar{y}_D = 20\\) sowie homogenen Varianzen mit \\(s_A = s_B = s_C = s_D = 2\\). Jede Behandlung hat \\(n = 5\\) Beobachtungen. Wir haben also ein balanziertes Design vorliegen.\n\nset.seed(20220916)\ncrd_tbl <- tibble(A = rnorm(n = 5, mean = 10, sd = 2),\n                  B = rnorm(n = 5, mean = 12, sd = 2),\n                  C = rnorm(n = 5, mean = 16, sd = 2),\n                  D = rnorm(n = 5, mean = 20, sd = 2)) %>% \n  gather(key = trt, value = rsp) %>% \n  mutate(trt = as_factor(trt))\n\nSchauen wir uns einmal die Daten an, die wir in R erhalten. Das Objekt crd_tbl ist ein tibble in Long-Format nach der Anwendung der Funktion gather(). Wir haben auch die Spalte trt für die Behanldung als Faktor umgewandelt.\n\ncrd_tbl\n\n# A tibble: 20 × 2\n   trt     rsp\n   <fct> <dbl>\n 1 A     12.3 \n 2 A     11.0 \n 3 A     13.4 \n 4 A     13.2 \n 5 A      4.27\n 6 B     12.2 \n 7 B     11.4 \n 8 B     15.4 \n 9 B      9.51\n10 B     11.9 \n11 C     13.1 \n12 C     15.4 \n13 C     16.3 \n14 C     19.0 \n15 C     19.4 \n16 D     20.9 \n17 D     17.8 \n18 D     20.6 \n19 D     24.6 \n20 D     17.5 \n\n\nWir haben also \\(N = 20\\) Beobachtungen vorliegen. Wir immer ist es schwer eine Datentabelle zu erfasen. Daher schauen wir uns die Daten einmal in Abbildung 39.3 als Boxplots an. Wir wolllen uns noch die Punkte zusätzlich anzeigen lassen. bei der geringen Anzahl an Beobachtungen wäre ein Dotplot oder ein Scatterplot auch eine Möglichkeit.\n\nggplot(crd_tbl, aes(trt, rsp, fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, shape = 4, size = 3) +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung 39.3— Boxplots der Behandlungsgruppen zufällig aus einer Normalverteilung mit Varianzhomogenität generierten Daten.\n\n\n\n\nWir erinnern uns, dass die Daten alle varianzhomogen und normalverteilt sind. Wir haben die Daten so erschaffen. Dennoch wirken die Boxplots so, als würde teilweise eine schiefe Verteilung vorliegen. Bei so wenigen Beobachtungen ist es immer schwer, für oder gegen eine Verteilung zu argumentieren. Wir bleiben bei einer Normalverteilung, wenn wir glauben, dass das \\(y\\) approimativ normalverteilt ist. Wir schreiben dann, dass wir ein normalverteiltes \\(y\\) annehmen.\n\n39.2.3 Modellierung\nIm Folgenden wollen wir die Daten modellieren. Das heist wir wollen eine Linie durch eine multidimensionale Punktewolke zeichnen. Daher auch lineares Modell oder eben durch die Funktion lm() in R für linear model. Wir nutzen das Paket parameters und die Funktion model_parameters() um uns die Parameter des Modells auszugeben. Wir könnten auch die Funktion tidy() nutzen, aber wir erhalten durch die Funktion model_parameters() etwas mehr Informationen und bessere Spaltenüberschriften.\nWir bauen das Modell in folgender Form. Wir haben ein numerisches Outcome \\(y\\) sowie einen Faktor \\(f_1\\).\n\\[\ny \\sim f_1\n\\]\nNun können wir das abstrakte Modell in die Daten übersetzen und erhalten folgendes Modell.\n\\[\nrsp \\sim trt\n\\]\nDas heist, unsere numerische Variable rsp hängt ab von unserer faktoriellen Variable trt. Wir müssen immer wissen, wie die Spaltennamen in unserem Datensatz crd_tbl lauten sonst kann R die Spalten nicht finden.\n\nfit_crd <- lm(rsp ~ trt, crd_tbl)\n\nfit_crd %>%  model_parameters()\n\nParameter   | Coefficient |   SE |         95% CI | t(16) |      p\n------------------------------------------------------------------\n(Intercept) |       10.83 | 1.30 | [ 8.07, 13.59] |  8.33 | < .001\ntrt [B]     |        1.25 | 1.84 | [-2.65,  5.15] |  0.68 | 0.506 \ntrt [C]     |        5.80 | 1.84 | [ 1.90,  9.70] |  3.15 | 0.006 \ntrt [D]     |        9.47 | 1.84 | [ 5.57, 13.37] |  5.15 | < .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nÜberlege mal, was die Spalte Coefficient aussagen möchte. Wir erhalten den (Intercept) mit \\(10.38\\) und damit den MIttelwert der Gruppe \\(A\\). In den folgenden Zeilen sind die Änderungen zu dem (Intercept) und damit zu der Gruppe \\(A\\) dargestellt. Da wir nur eine sehr kleine Anzhl an Beoabchtungen haben, haben wir hier auch Abweichungen zu den voreingestellten Mittelwerten und Standardabweichungen. Wir schauen uns ja auch nur eine Realisierung von möglichen Daten \\(D\\) an. Wir sehen, dass alle Koeffizienten signifikant und damit unterschiedlich von der Null sind. Der \\(p\\)-Wert ist kleiner als das Signiifkanzniveau von \\(\\alpha\\) gleich 5%.\nWir können jetzt nochmal überprüfen, ob die Residuen die Annahme der Varianzhomogenität erfüllen.\n\nfit_crd %>% check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.737).\n\n\nSowie ob die Residuen normalverteilt sind.\n\nfit_crd %>% check_normality()\n\nOK: residuals appear as normally distributed (p = 0.620).\n\n\nDa wir ja hiermit nur eine Zeile Text produziert haben und darübr hinaus wir gerne uns Dinge anschauen, können wir auch die Residuen einmal visualisieren. In Abbildung 39.4 sehen wir den QQ-Plot der Residuen sowie die Verteilung unserer Residuen in einem Desnityplot. Wir sehen, dass die Residuen einer Normalverteilung folgen.\n\ncheck_model(fit_crd, check = c(\"qq\", \"normality\"))\n\n\n\nAbbildung 39.4— QQ-Plot und Densityplot der Residuen aus dem lineare Modell.\n\n\n\n\nWunderbar. Wir können jetzt eine Varianzanalyse und dann eine Mittelwertsvergleich durchführen. Achtung, wir können uns hier auch etwas in die Ecke testen. Wenn wir nur lange genug neue Daten generieren, werden wir irgendwann auch einen Datensatz finden, der die Varianzhomogenität und die Normalverteilung ablehnt. Das liegt in der Theorie des statistischen Testens sowie der kleinen Fallzahl verborgen. Deshalb können wir im Zweifel gerne einmal deine Vortests in dem R Tutorium oder in einer statistischen Beratung diskutieren.\n\n39.2.4 Varianzanalyse und Mittelwertsvergleich\nDie einfaktorielle Varianzanalyse ist ziemlich einfach und ergibt sich fast von alleine. Wir nehmen das Objekt des Modells und pipen das Modell in die Funktion anova(). Wir lassen uns dann wieder die Modellparameter der ANOVA widergeben.\n\nres_anova <- fit_crd %>% \n  anova() \n\nres_anova %>% model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |     F |      p\n-----------------------------------------------------------\ntrt       |      283.24 |  3 |       94.41 | 11.16 | < .001\nResiduals |      135.35 | 16 |        8.46 |       |       \n\nAnova Table (Type 1 tests)\n\n\nWir sehen, dass der Faktor Behandlung signifkant ist, da der \\(p\\)-Wert kleiner ist als das Signifkanzniveau \\(\\alpha\\) gleich 5%. Wir können damit die Nullhypothese ablehnen, wir haben zumindestens einen paarweisen Gruppenunterschied in der Behandlung. Welchen wissen wir nicht, dafür machen wir dann die paarweisen Vergleiche. Eigentlich können wir uns in diesem simplen Fall die ANOVA schhenken und gleich den Mittelwertsvergleich rechnen. Aber das es Usus ist und auch in vielen Abschlussarbeiten verlangtt wird, machen wir hier es einfach mal gleich mit.\nJetzt brauchen wir nur noch die Effektstärke der ANOVA, also wieviel Varianz eigentlich der Faktor Behandlung erklärt. Dfür nutzen wir die Funktion eta_squared() aus dem Paket effectsize.\n\nres_anova %>% eta_squared(partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 |       95% CI\n-------------------------------\ntrt       | 0.68 | [0.38, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nMit einem \\(\\eta^2\\) von \\(0.86\\) wissen wir, dass 86% der Varianz von dem Faktor Behandlung erklärt wird. Das wundert uns nicht, denn wir haben ja nur den Faktor Behandlung in unseren Daten aus denen sich unser Outcome ergibt.\nNachdem wir kurz die ANOVA gerechnet haben, wollen wir noch den Mittelwertsvergleich rechnen. Wir nutzen dazu das Paket emmeans. Wir müssen der Funktion emmeans() ein Objekt aus einem Modell übergeben und der Funktion mitteilen, was der Faktor ist mit dem der Vergleich gerechnet werden soll. Wir haben hier den Faktor trt vorliegen und wollen einen parweisen Vergleich über alle Level des Faktors rechnen.\n\nres_crd <- fit_crd %>% \n  emmeans(~ trt) \n\nWir haben die Ausgabe der Funktion emmeans() in dem Objekt res_crd gespeichert und nutzen das Objekt zuerst um einmal die Ausgabe für das comapct letter display zu erhalten. Als Adjustierung des \\(\\alpha\\) Fehlers nutzen wir die Adjustierung nach Bonferroni. Es sind auch andere Adjustierungen möglich, aber aus Gründen der Einfachheit nehmen wir hier mal den Klassiker der Adjustierung. Je nach Fragestellung gibt es sicherlich auch eine bessere Alternative für Bonferroni.\n\nres_crd_cld <- res_crd %>% \n  cld(adjust = \"bonferroni\", Letters = letters) %>% \n  tidy() %>% \n  select(trt, estimate, conf.low, conf.high, .group) %>% \n  mutate(across(where(is.numeric), round, 2))\n\nNachdem wir noch ein wenig gerundet haben und die Spalten passend gewählt, erhalten wir dann folgende Ausgabe.\n\nres_crd_cld \n\n# A tibble: 4 × 5\n  trt   estimate conf.low conf.high .group\n  <chr>    <dbl>    <dbl>     <dbl> <chr> \n1 A         10.8     7.17      14.5 \" a  \"\n2 B         12.1     8.42      15.7 \" ab \"\n3 C         16.6    13.0       20.3 \"  bc\"\n4 D         20.3    16.6       24.0 \"   c\"\n\n\nWir nutzen die Ausgabe res_crd_cld direkt in der Abbildung 39.5 um uns das compact letter display zusammen mit den Daten und den entsprechenden 95% konfidenzintervallen anzeigen zu lassen. Der Code ist etwas länger, da wir hier verschiedene Schichten von einem geom übereinander legen müssen.\n\nggplot() +\n  theme_bw() +\n  geom_point(data = crd_tbl, aes(x = trt, y = rsp, fill = trt)) +\n  geom_text(data = res_crd_cld, \n            aes(x = trt , y = estimate, label = .group),\n            position = position_nudge(x = 0.2), color = \"red\") +\n  geom_errorbar(data = res_crd_cld,\n                aes(ymin = conf.low, ymax = conf.high, x = trt),\n                color = \"red\", width = 0.1,\n                position = position_nudge(x = 0.1)) +\n  geom_point(data = res_crd_cld, \n             aes(x = trt , y = estimate),\n             position = position_nudge(x = 0.1), color = \"red\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"Behandlung\", y = \"Gewicht [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die Rohdaten dar.\n       Rote Punkte und Fehlerbalken stellen bereinigte Mittelwerte mit 95% Konfidenzgrenzen pro Behandlung dar.\n       Mittelwerte, mit einem gemeinsamen Buchstaben, sind nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 39.5— Scatterplot der Behandlungsgruppen zusammen mit den 95% Konfidenzintervall und dem compact letter display.\n\n\n\n\nWi sehen an dem compact letter display, dass sich die Behandlung \\(A\\) von der Behandlung \\(B\\), \\(C\\) und \\(D\\) unterscheidet. Die Behandlung \\(B\\) und \\(C\\) sind gleich. Die Behandlung \\(C\\) unterschdeit sich von all den anderen Behandlungen. Wir erinnern uns, wenn die Buchstaben in dem compact letter display gleich sind, dann können wie die Nullhypothese für diese Vergleiche nicht ablehnen. Wir haben keinen signifikanten Unterschied vorliegen.\nNun ist es so, dass wir meistens noch die \\(p\\)-Werte für die paarweisen Vergleich sowie die 95% Konfidenzintervalle darstellen wollen. Wir nutzen dafür die Funktion contrast() aus dem Paket emmeans. Danach müssen wir noch Spalten auswählen und die \\(p\\)-Werte über die Funktion pvalue() aus dem Paket scales schöner formatieren. Wir erhalten dann das Objekt res_crd_tbl.\n\nres_crd_tbl <- res_crd %>% \n  contrast(method = \"pairwise\") %>% \n  tidy(conf.int = TRUE) %>% \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) %>% \n  select(contrast, estimate, p.value,\n         conf.low, conf.high) \n\nIn dem Objekt res_crd_tbl finden wir dann die \\(p\\)-Werte für alle paarweisen Vergleiche sowie die 95% Konfidenzintevalle.\n\nres_crd_tbl\n\n# A tibble: 6 × 5\n  contrast estimate p.value conf.low conf.high\n  <chr>       <dbl> <chr>      <dbl>     <dbl>\n1 A - B       -1.25 0.903      -6.51      4.01\n2 A - C       -5.8  0.028     -11.1      -0.54\n3 A - D       -9.47 <0.001    -14.7      -4.21\n4 B - C       -4.55 0.103      -9.81      0.71\n5 B - D       -8.22 0.002     -13.5      -2.96\n6 C - D       -3.67 0.231      -8.93      1.59\n\n\nHier sehen wir dann die \\(p\\)-Werte für alle paarweisen Vergleiche und können dann die Entscheidung gegen die Nullhypothese für jeden der Kontraste einmal durchführen. Wir sehen, dass wir für alle Vergleiche die Nullhypothese ablehnen können, bis auf den Vergleich zwischen der Behandlung \\(B\\) und der Behandlung \\(C\\).\nIn der Abbildung 39.6 sehen wir die 95% Konfidenzintervalle für alle Vergleiche einmal dargestellt. Da wir es hier mit einem Mittelwertsvergleich zu tun haben, ist die Entscheidungsregel gegen die Nullhyppthese, dass wir ein signifikantes Konfidenzintervall vorliegen haben, wenn die Null nicht im Konfidenzintervall enthalten ist.\n\nggplot(res_crd_tbl, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1) + \n  geom_point() +\n  coord_flip() +\n  theme_bw()  +\n  labs(x = \"Vergleich\", y = \"Mittelwertsunterschied des Gewichtes [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die bereinigten Mittelwertsunterschiede mit 95% Konfidenzgrenzen dar.\n       Enthält ein 95% Konfidenzintervalle die 0 ist es nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 39.6— Abbildung der 95% Konfidenzintervallefür alle paarweisen Vergleiche der Behandlungsgruppen."
  },
  {
    "objectID": "experimentel-design-basic.html#sec-rcbd",
    "href": "experimentel-design-basic.html#sec-rcbd",
    "title": "\n39  Grundlagen der Versuchsplanung\n",
    "section": "\n39.3 Randomized complete block design (RCBD)",
    "text": "39.3 Randomized complete block design (RCBD)\nDas randomisierte, vollständige Blockdesign (eng. randomized complete block design) ist das Design, wenn es darum geht für verschiedene Räume die Varianz zu adjustieren bzw. zu modellieren. Was meinen wir mit Räumen? Wir meinen damit verschiedene Ställe, verschiedene Felder oder aber verschiedene Tische. Wir nennen diese zusätzlichen Beobachtungsräume auch Block.\n\n\n\n\n\n(a) Visualisierung des Randomized complete block design mit einer Behandlung und vier Behandlungsleveln. In jedem Block findet sich nur ein Behandlungslevel randomisiert wieder.\n\n\n\n\n\n\n(b) Visualisierung des Randomized complete block design mit einer Behandlung und vier Behandlungsleveln. In jedem Block finden wir mehrfach die Level der Behandlung. Im Prinzip ein Complete randomized design in mehreren Wiederholungen.\n\n\n\n\nAbbildung 39.7— Visualisierung der zwei Möglichkeiten ein Randomized complete block design zu konstruieren.\n\n\nWichtig ist zu unterschieden, wir pro Block nur einmal ein Level der Behandlung vorliegen haben. Dann hätten wir nämlich nur einen Topf mit Behandlung pro Block wie in Abbildung 39.7 (a) dargestellt. Damit haben wir den Block als Wiederholung. Oder wir haben ein Complete randomized design in Blöcken wiederholen vorliegen. Dann haben wir nämlich pro Block mehrere Wiederholungen der Behandlung wie in Abbildung 39.7 (b) veranschaulicht. Wir schauen uns erstmal den ersten Fall an. Das heist im Prinzip, dass unser Block die Wiederholung ist.\nHier ein paar Beispiele in Prosa, wie so ein Randomized complete block design konstruiert sein könnte.\nWir haben drei Tische und auf jeden der Tische steht zufällig vier ein Töpfe mit je einer Behandlung\nWir haben drei Ställe und in jedem Stall werden vier Buchten mit jeweils einer Behandlung genutzt.\nWir haben drei Felder mit jeweils vier Parzellen die zufällig mit jeweils einer der Behandlungen versehen werden.\nWir können natürlich auch auf den Tischen mehrere Wiederholungen einer Behandlung haben. Dann wird der Datensatz nur größer, aber die Auswertung unterschiedet sich nicht. Wir haben dann mehr Beobachtungen pro Block und Behandlung.\n\n39.3.1 Visualisierung\nIn der Abbildung 39.8 sehen wir eine Realisierung des Randomized complete block design. Wir haben insgesamt drei Blöcke vorliegen mit Block I, Block II und Block III. In jedem Block haben wir die Behandlungen \\(A\\), \\(B\\), \\(C\\) und \\(D\\) zufällig randomisiert. In jedem Block haben wir genau einmal ein Level der Behandlung vorliegen.\n\n\nAbbildung 39.8— Visualisierung des complete randomized design mit einer Behandlung und vier Behandlungsleveln.\n\n\n\n39.3.2 Daten\nIm Folgenden generieren wir uns die Daten für das Randomized complete block design. Wir wissen, dass in jedem Block die Behandlung genau einmal vorkommt. Um diese Datenstruktur mit zwei Faktoren nachzubauen, können wir die Funktion expand_grid() nutzen. Wir definieren zuerst, dass wir vier Behandlungslevel wollen und für jedes Behandlungslevel dann die drei Level des Blocks. Hier muss ich auch immer wieder rumspielen und probieren, bis ich die Daten dann zu dem Design passend habe. Wir erstellen uns so das Objekt factor_tbl.\n\nset.seed(20221001)\nfactor_tbl <- expand_grid(trt = 1:4, block = 1:3) %>% \n  mutate(trt = factor(trt, labels = c(\"A\", \"B\", \"C\", \"D\")),\n         block = factor(block, labels = as.roman(1:3))) \n\nfactor_tbl\n\n# A tibble: 12 × 2\n   trt   block\n   <fct> <fct>\n 1 A     I    \n 2 A     II   \n 3 A     III  \n 4 B     I    \n 5 B     II   \n 6 B     III  \n 7 C     I    \n 8 C     II   \n 9 C     III  \n10 D     I    \n11 D     II   \n12 D     III  \n\n\nWir sehen, dass jede Behandlung in allen drei Level des Blocks hat. Das entspricht unser Abbildung 39.8 und somit können wir uns darum kümmern, den Leveln der Behandlung und des Blocks einen Effekt zuzuweisen. Dafür brauchen wir die Modellmatrix, die beschreibt, wie sich für jede Beobachtung die Effekte zum Outcome rsp aufsummieren. Nicht jede Beobachtung ist in jedem Block in jeder Behandlung vertreten. Genau genommen hat jede Beobachtung nur eine einzige Behandlung/Block-Kombintation. Wir sehen diese Kombination dann in der Modellmatrix.\n\nmodel_mat <- factor_tbl %>% \n  model_matrix(~ trt + block) %>% \n  as.matrix()\n\nmodel_mat\n\n      (Intercept) trtB trtC trtD blockII blockIII\n [1,]           1    0    0    0       0        0\n [2,]           1    0    0    0       1        0\n [3,]           1    0    0    0       0        1\n [4,]           1    1    0    0       0        0\n [5,]           1    1    0    0       1        0\n [6,]           1    1    0    0       0        1\n [7,]           1    0    1    0       0        0\n [8,]           1    0    1    0       1        0\n [9,]           1    0    1    0       0        1\n[10,]           1    0    0    1       0        0\n[11,]           1    0    0    1       1        0\n[12,]           1    0    0    1       0        1\n\n\nWir sehen in der Modellmarix in jeder Zeile eine zukünftige Beobachtung. In den Spalten wird angegeben zu welchen Faktorleveln die Beobachtung gehört. Dabei bedeutet eine 1 ein Ja und eine 0 ein Nein. Die Beobachtung in der Zeile 5 wird zu Behandlungslevel \\(B\\) und Block \\(II\\) gehören.\nWir legen jetzt folgende Effekte für die einzelnen Behandlungslevel fest. Für den Intercept und damit auch für die Behandlung \\(A\\) auf \\(\\beta_{0} = \\beta_{A} = 20\\). Das Behandlunsglevel \\(B\\) wird auf \\(\\beta_{B} = 15\\), die Behandlung \\(C\\) auf \\(\\beta_{C} = 10\\) sowie die Behandlung \\(D\\) auf \\(\\beta_{D} = 5\\) gesetzt. Um die Sachlage zu vereinfachen setzen wir die Effekte der Blöcke auf \\(\\beta_{0} = \\beta_{I} = 0\\) sowie \\(\\beta_{II} = 0\\) und \\(\\beta_{III} = 0\\). Wir haben also faktisch keinen Effekt der Blöcke. Es ist egal welchen Tisch wir benutzen, die Effekte der Behandlung sind immer die Gleichen. Wenn wir die Daten so bauen würden, dann erhalten wir die Spalte rsp_eff in dem Datensatz rcbd_tbl. Wir haben keine Varianz. Deshalb müssen wir noch die Residuen mit \\(\\epsilon \\sim \\mathcal{N}(0, 2)\\) auf die Werte in der Spalte rsp_eff addieren. Wir erhalten die Spalte rsp für die Auswertung.\n\nrcbd_tbl <- factor_tbl %>% \n  mutate(rsp_eff = as.numeric(model_mat %*% c(20, 15, 10, 5, 0, 0)),\n         rsp = rsp_eff + rnorm(n(), 0, 2))\n\nrcbd_tbl\n\n# A tibble: 12 × 4\n   trt   block rsp_eff   rsp\n   <fct> <fct>   <dbl> <dbl>\n 1 A     I          20  19.8\n 2 A     II         20  17.6\n 3 A     III        20  18.2\n 4 B     I          35  34.4\n 5 B     II         35  33.9\n 6 B     III        35  32.3\n 7 C     I          30  28.4\n 8 C     II         30  29.5\n 9 C     III        30  29.5\n10 D     I          25  23.6\n11 D     II         25  24.9\n12 D     III        25  23.0\n\n\nIn Tabelle 39.1 sehen wir nochmal den Zusammenhang zwischen den generierten Daten und den entsprechenden berechneten Mittelwerten je Behandlungsgruppe. Wir berechnen den Mittelwert auf der Spalte rsp_eff. Wir sehen, dass wir die voreingestellten Mittelwerte in den Daten widerfinden.\n\n\n\n\nTabelle 39.1— Vergleich der Mittlwerte aus den Daten und den voreingestellten Effekten für die Generierung der Daten.\n\nFactor trt\nMean of level\nDifference to level A\nBeta\n\n\n\nA\n20\n0\n20\n\n\nB\n35\n15\n15\n\n\nC\n30\n10\n10\n\n\nD\n25\n5\n5\n\n\n\n\n\n\nAbschließend wollen wir uns die generierten Daten nochmal als einen Dotplot anschauen. Wir wollen dafür einen Dotplot nutzen, da wir mit drei Beobachtungen pro Level der Behandlung keinen sinnvollen Boxplot zeichnen können.\n\nggplot(rcbd_tbl, aes(trt, rsp, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", \n               position = position_dodge(width = 0.4)) +\n  ylim(15, 40) +\n  scale_fill_okabeito() +\n  labs(fill = \"Block\", x = \"Behandlung\", y = \"Outcome\")\n\n\n\nAbbildung 39.9— Dotplot der Level der Behandlungen aufgeteilt für die Level des Blocks.\n\n\n\n\nWir können die Daten aus dem Datensatz rcbd_tbl jetzt für die Varianzanalyse und Mittelwertsvergleich nutzen.\n\n39.3.3 Modellierung\nIm Folgenden wollen wir die Daten modellieren. Das heist wir wollen eine Linie durch eine multidimensionale Punktewolke zeichnen. Daher auch lineares Modell oder eben durch die Funktion lm() in R für linear model. Wir nutzen das Paket parameters und die Funktion model_parameters() um uns die Parameter des Modells auszugeben. Wir könnten auch die Funktion tidy() nutzen, aber wir erhalten durch die Funktion model_parameters() etwas mehr Informationen und bessere Spaltenüberschriften.\nWir bauen das Modell in folgender Form. Wir haben ein numerisches Outcome \\(y\\) sowie einen Faktor \\(f_1\\) sowie einem Faktor für den Block \\(b_1\\).\n\\[\ny \\sim f_1 + b_1\n\\]\nNun können wir das abstrakte Modell in die Daten übersetzen und erhalten folgendes Modell.\n\\[\nrsp \\sim trt + block\n\\]\nDas heist, unsere numerische Variable rsp hängt ab von unserer faktoriellen Variable trt und der faktoriellen Blockvariable block. Wir müssen immer wissen, wie die Spaltennamen in unserem Datensatz crd_tbl lauten sonst kann R die Spalten nicht finden.\n\nfit_rcbd <- lm(rsp ~ trt + block, rcbd_tbl)\n\nfit_rcbd %>%  model_parameters()\n\nParameter   | Coefficient |   SE |         95% CI |  t(6) |      p\n------------------------------------------------------------------\n(Intercept) |       18.79 | 0.70 | [17.08, 20.51] | 26.75 | < .001\ntrt [B]     |       15.03 | 0.81 | [13.04, 17.01] | 18.53 | < .001\ntrt [C]     |       10.57 | 0.81 | [ 8.59, 12.56] | 13.03 | < .001\ntrt [D]     |        5.29 | 0.81 | [ 3.30,  7.27] |  6.52 | < .001\nblock [II]  |       -0.05 | 0.70 | [-1.77,  1.67] | -0.08 | 0.942 \nblock [III] |       -0.77 | 0.70 | [-2.49,  0.94] | -1.10 | 0.313 \n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nWir sehen, dass wir die Koeffizienten, die wir vorher eingestellt haben, auch hier wiederfinden. Alle Steigungen der Behandlungslevel sind signifikant. Das hilft uns aber noch nicht so richtig weiter. Wir werden gleich das Modell in einer zweifaktoriellen ANOVA und einem Mittelwertsvergleich anschauen. Vorher wollen wir einmal statistisch Testen, ob die Varianzen homogens sind. Wir können die Varianzen aber nicht über das volle Modell testen, da wir nur eine Beobachtung per Behandlung/Block-Kombintation vorliegen haben.\n\nfit_rcbd %>% check_homogeneity()\n\nError in bartlett.test.default(x = mf[[1L]], g = mf[[2L]]) :  there must be at least 2 observations in each group\nDaher schauen wir uns nur die Varianzen für die Behandlung an und nehmen an, dass die Varanzen über die Blöcke homogen sind. Wir können nur einen Faktor testen und deshalb nehmen wir den für uns wichtigeren Faktor die Behandlung.\n\nlm(rsp ~ trt, rcbd_tbl) %>% check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.907).\n\n\nAbschließend schauen wir nochmal auf die Normalverteilung der Residuen.\n\nfit_rcbd %>% check_normality()\n\nOK: residuals appear as normally distributed (p = 0.391).\n\n\nIn der Abbildung 39.10 sehen wir den QQ-Plot und die Verteilung der Residuen im Densityplot. Auch die Visualisierung zeigt keine Aufälligkeiten. Wir sehen, dass die Residuen einer Normalverteilung folgen.\n\ncheck_model(fit_rcbd, check = c(\"qq\", \"normality\"))\n\n\n\nAbbildung 39.10— QQ-Plot und Densityplot der Residuen aus dem lineare Modell.\n\n\n\n\nWir können jetzt eine Varianzanalyse und dann eine Mittelwertsvergleich durchführen. Achtung, wir können uns hier auch etwas in die Ecke testen. Wenn wir nur lange genug neue Daten generieren, werden wir irgendwann auch einen Datensatz finden, der die Varianzhomogenität und die Normalverteilung ablehnt. Besonders in dem Fall, dass wir wenige Blöcke haben. Das liegt in der Theorie des statistischen Testens sowie der kleinen Fallzahl verborgen. Deshalb können wir im Zweifel gerne einmal deine Vortests in dem R Tutorium oder in einer statistischen Beratung diskutieren.\n\n39.3.4 Varianzanalyse und Mittelwertsvergleich\nAls erstes Rechnen wir eine zweifaktroielle ANOVA, da unser Modell zwei Faktoren hat. In R müssen wir dazu nur das Modell fit_rcbd in die Funktion anova() pipen. Wir erhalten dann die Ergebnisse aus der ANOVA mit der Funktion model_parameters() aus dem Paket parameters besser aufgearbeitet wieder. Die Mittelwertsunterschiede der Level der Behandlung haben wir bewusst sehr hoch angesetzt, so dass wir auf jeden Fall eine signifikante ANOVA erhalten sollen.\n\nres_anova <- fit_rcbd %>% \n  anova() \n\nres_anova %>% model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |      F |      p\n------------------------------------------------------------\ntrt       |      381.24 |  3 |      127.08 | 128.72 | < .001\nblock     |        1.50 |  2 |        0.75 |   0.76 | 0.509 \nResiduals |        5.92 |  6 |        0.99 |        |       \n\nAnova Table (Type 1 tests)\n\n\nAls Ergebnis haben wir einen signifikanten Faktor Behandlung trt sowie einen nicht signifikanten Faktor Block block. Wir können die Signifkanz an dem \\(p\\)-Wert bestimmen. Liegt der \\(p\\)-Wert unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5% so können wir die Nullhypothese ablehnen. Wir haben dann mindestens einen signifikanten paarweisen Mittelwertsunterschied vorliegen.\nSchauen wir uns nun noch den Anteil der erklärten Varianz an. Wir nutzen dafür den Effektschätzer \\(\\eta^2\\).\n\nres_anova %>% eta_squared(partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter |     Eta2 |       95% CI\n-----------------------------------\ntrt       |     0.98 | [0.93, 1.00]\nblock     | 3.85e-03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass durch den Faktor trt mit 92% der Varianz erklärt werden. Der Faktor Block erklärt nur ca. 2% der Varianz. Beides war so zu erwarten, denn wir haben ja auch den Datensatz in dieser Form gebaut. Die Behandlung hat einen starken Effekt und der Block hat gar keinen Effekt.\nSchauen wir nun auf den Mittelwertsvergleich. Wir nutzen dafür die Funktion emmeans() aus dem R Paket emmeans. Wichtig ist hier, dass wir uns jetzt die Vergleiche der Gruppen bzw. Level der Behandlung anschauen wollen.\n\nres_rcbd <- fit_rcbd %>% \n  emmeans(~ trt) \n\nAls erstes nutzen wir die Ausagbe der Funktion emmeans um uns das compact letter display wiedergeben zu lassen. Wir wollen wieder die Ausgaben runden und nutzen die Adjustierung der \\(p\\)-Werte für multiple Vergleiche nach Bonferroni. Nochmal als Erinnerung, das compact letter display gibt uns keine \\(p\\)-Werte wieder sondern wir Entscheiden anhand der vergebenen Buchstaben und deren Gleichheit über ein signifikantes Ergebnis oder ein nicht signifikantes Ergebnis.\n\nres_rcbd_cld <- res_rcbd %>% \n  cld(adjust = \"bonferroni\", Letters = letters) %>% \n  tidy() %>% \n  select(trt, estimate, conf.low, conf.high, .group) %>% \n  mutate(across(where(is.numeric), round, 2))\n\nres_rcbd_cld \n\n# A tibble: 4 × 5\n  trt   estimate conf.low conf.high .group \n  <chr>    <dbl>    <dbl>     <dbl> <chr>  \n1 A         18.5     16.5      20.5 \" a   \"\n2 D         23.8     21.8      25.8 \"  b  \"\n3 C         29.1     27.1      31.1 \"   c \"\n4 B         33.6     31.5      35.6 \"    d\"\n\n\nAn dem compact letter display sehen wir, dass sich alle Mittelwerte der Level der Behandlungen signifikant unterscheiden. In Abbildung 39.11 sehen wir die Daten zusammen mit dem compact letter display in einer Abbildung. Wir ändern hier das geom_point() zu geom_jitter() um ein Overplotting zu vermeiden. So können wir alle Beobachtungen als Punkte erkennen.\n\nggplot() +\n  theme_bw() +\n  geom_jitter(data = rcbd_tbl, aes(x = trt, y = rsp, fill = trt),\n              width = 0.05) +\n  geom_text(data = res_rcbd_cld, \n            aes(x = trt , y = estimate, label = .group),\n            position = position_nudge(x = 0.2), color = \"red\") +\n  geom_errorbar(data = res_rcbd_cld,\n                aes(ymin = conf.low, ymax = conf.high, x = trt),\n                color = \"red\", width = 0.1,\n                position = position_nudge(x = 0.1)) +\n  geom_point(data = res_rcbd_cld, \n             aes(x = trt , y = estimate),\n             position = position_nudge(x = 0.1), color = \"red\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"Behandlung\", y = \"Gewicht [kg/ha]\",\n       caption = \"Schwarze Punkte stellen Rohdaten dar.\n       Rote Punkte und Fehlerbalken stellen bereinigte Mittelwerte mit 95% Konfidenzgrenzen pro Behandlung dar.\n       Mittelwerte, mit einem gemeinsamen Buchstaben, sind nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 39.11— Scatterplot der Behandlungsgruppen zusammen mit den 95% Konfidenzintervall und dem compact letter display.\n\n\n\n\nHäufig wollen wir nicht nur das compact letter display sehen sondern auch die dazugehörigen \\(p\\)-Werte und die entsprechenden 95% Konfidenzintervalle. Wir berechnen im Folgenden alle paarweisen Vergleiche bzw. Kontraste und lassen uns die adjustierten sowie formatierten \\(p\\)-Werte ausgeben. Wir runden wieder die Ausgabe.\n\nres_rcbd_tbl <- res_rcbd %>% \n  contrast(method = \"pairwise\") %>% \n  tidy(conf.int = TRUE) %>% \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) %>% \n  select(contrast, estimate, p.value,\n         conf.low, conf.high) \n\nres_rcbd_tbl\n\n# A tibble: 6 × 5\n  contrast estimate p.value conf.low conf.high\n  <chr>       <dbl> <chr>      <dbl>     <dbl>\n1 A - B      -15.0  <0.001    -17.8     -12.2 \n2 A - C      -10.6  <0.001    -13.4      -7.76\n3 A - D       -5.29 0.003      -8.1      -2.48\n4 B - C        4.46 0.006       1.65      7.26\n5 B - D        9.74 <0.001      6.93     12.6 \n6 C - D        5.29 0.003       2.48      8.09\n\n\nAuch hier passen die \\(p\\)-Werte zu dem compact letter display. Alle Vergleiche sind signifikant. Das haben wir noch dem compact letter display auch so erwartet. Auch sehen wir das gleiche Ergebnis in Abbildung 39.12 für die 95% Konfidenzintervalle. Wir betrachten Mittelwertsunterschiede und kein Konfidenzintervall beinhaltet die Null somit sind alle Konfidenzintervalle signifikant.\n\nggplot(res_rcbd_tbl, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1) + \n  geom_point() +\n  coord_flip() +\n  theme_bw()  +\n  labs(x = \"Vergleich\", y = \"Mittelwertsunterschied des Gewichtes [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die bereinigten Mittelwertsunterschiede mit 95% Konfidenzgrenzen dar.\n       Enthält ein 95% Konfidenzintervalle die 0 ist es nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 39.12— Abbildung der 95% Konfidenzintervallefür alle paarweisen Vergleiche der Behandlungsgruppen."
  },
  {
    "objectID": "experimentel-design-basic.html#sec-lsd",
    "href": "experimentel-design-basic.html#sec-lsd",
    "title": "\n39  Grundlagen der Versuchsplanung\n",
    "section": "\n39.4 Latin square design (LSD)",
    "text": "39.4 Latin square design (LSD)\n\n39.4.1 Visualisierung\n\n\nAbbildung 39.13— Visualisierung des latin square design mit einer Behandlung und vier Behandlungsleveln.\n\n\n\n39.4.2 Daten\n\nexpand_grid(trt = 1:4, block = 1:4)\n\n# A tibble: 16 × 2\n     trt block\n   <int> <int>\n 1     1     1\n 2     1     2\n 3     1     3\n 4     1     4\n 5     2     1\n 6     2     2\n 7     2     3\n 8     2     4\n 9     3     1\n10     3     2\n11     3     3\n12     3     4\n13     4     1\n14     4     2\n15     4     3\n16     4     4\n\n\n\n39.4.3 Modellierung\n\n39.4.4 Varianzanalyse und Mittelwertsvergleich"
  },
  {
    "objectID": "experimentel-design-basic.html#sec-alpha",
    "href": "experimentel-design-basic.html#sec-alpha",
    "title": "\n39  Grundlagen der Versuchsplanung\n",
    "section": "\n39.5 Alpha design",
    "text": "39.5 Alpha design\n\n39.5.1 Visualisierung\n\n\n\nAbbildung 39.14— Visualisierung des alpha design mit einer Behandlung und vier Behandlungsleveln und zwölf unvollständigen Blöcken sowie vier Wiederholungen.\n\n\n\n\n39.5.2 Daten\n\n39.5.3 Modellierung\n\n39.5.4 Varianzanalyse und Mittelwertsvergleich"
  },
  {
    "objectID": "experimentel-design-basic.html#sec-augment",
    "href": "experimentel-design-basic.html#sec-augment",
    "title": "\n39  Grundlagen der Versuchsplanung\n",
    "section": "\n39.8 Augmented design",
    "text": "39.8 Augmented design"
  },
  {
    "objectID": "experimentel-design-basic.html#sec-split",
    "href": "experimentel-design-basic.html#sec-split",
    "title": "\n39  Grundlagen der Versuchsplanung\n",
    "section": "\n39.7 Split plot design",
    "text": "39.7 Split plot design\n\n39.7.1 Visualisierung\n\n\n\nAbbildung 39.15— Visualisierung des split plot design mit einer Behandlung und vier Behandlungsleveln sowie einer zweiten Behandlung mit fünf Behandlungsleveln. Die erste Behandlung ist über die drei Blöcke randomisiert.\n\n\n\n\n39.7.2 Daten\n\ndata_tbl <- expand_grid(trt = 1:4, block = 1:4, rep = 1:5) %>% \n    mutate(rsp = 20 + 2.5 * trt + 1.5 * block + rnorm(n(), 0, 1),\n           trt = factor(trt, labels = c(\"ctrl\", \"A\", \"B\", \"C\")),\n           block = factor(block, labels = as.roman(1:4)),\n           rep = as_factor(rep))\n\n\n39.7.3 Modellierung\n\n39.7.4 Varianzanalyse und Mittelwertsvergleich"
  },
  {
    "objectID": "app-example-analysis.html",
    "href": "app-example-analysis.html",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:40:27"
  },
  {
    "objectID": "app-example-analysis.html#genutzte-r-pakete-für-das-kapitel",
    "href": "app-example-analysis.html#genutzte-r-pakete-für-das-kapitel",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.1 Genutzte R Pakete für das Kapitel",
    "text": "A.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, \n               broom, multcomp, emmeans, \n               conflicted, effectsize, report,\n               see)\n\n## resolve some conflicts with same function naming\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "app-example-analysis.html#analyse-von-anzahlen",
    "href": "app-example-analysis.html#analyse-von-anzahlen",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.2 Analyse von Anzahlen",
    "text": "A.2 Analyse von Anzahlen\nIn dieser sehr simplen Analyse haben wir zwei Gruppen vorliegen. Die Gruppe 1 ist hat zwei Level oder Behandlungen abgekürzt mit I und II. Die Gruppe 2 hat insgesamt vier Level oder eben Behandlungen, die wir mit A, B, C und D bezeichnen. Wir haben jetzt für die jeweiligen Kombinationen auf dem Feld etwas gezählt. Wir haben also für jede dieser Kombinationen nur eine Zahl. Es ergbit sich somit die folgende Matrix an Zahlen.\n\nrel_mat <- matrix(c(45, 14, 4, 0,\n                    25, 32, 5, 1), nrow = 2, byrow = TRUE,\n                  dimnames = list(c(\"I\", \"II\"), c(\"A\", \"B\", \"C\", \"D\")))\nrel_mat\n\n    A  B C D\nI  45 14 4 0\nII 25 32 5 1\n\n\nNun können wir den \\(\\mathcal{X}^2\\)-Test nutzen, um zu testen, ob die Zahlen in der Matrix bzw. auf unseren Feld gelcihverteilt sind. Die Nullhypothese lautet, dass es keinen Zusammenhang zwischen der Gruppe 1 und der Gruppe 2 auf dem Feld gibt. Die Zahlen sind also rein zufällig in dieser Anordnung.\n\nchisq.test(rel_mat)\n\n\n    Pearson's Chi-squared test\n\ndata:  rel_mat\nX-squared = 13.8689, df = 3, p-value = 0.0030892\n\n\nWir erhalten einen sehr kleinen \\(p\\)-Wert mit \\(0.003\\). Wir können daher die Nullhypothese ablehnen, da der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) mit 5%. Wir haben ein signifikantes Ergebnis. Wir können von einen Zusamenhang zwischen den beiden Gruppen ausgehen.\nMit Cramers V können wir auch noch die Effektstärke für einen \\(\\mathcal{X}^2\\)-Test berechnen.\n\ncramers_v(rel_mat) \n\nCramer's V |       95% CI\n-------------------------\n0.33       | [0.15, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDer Effekt ist mit \\(0.33\\) nicht besonders stark. Du kannst Cramers V wie die Korrelation interpretieren. Ein V von 0 bedeutet keinen Zusammenhang und ein V von 1 einen maximalen Zusammenhang. Wir wollen uns die Daten dann nochmal in einer Abbidlung anschauen. Dafür müssen wir die Matrix erstmal in einen Datensatz umwandeln und die Gruppen zu Faktoren machen.\n\nplot_tbl <- rel_mat %>% \n  as_tibble(rownames = \"group1\") %>% \n  gather(A:D, key = \"group2\", value = \"value\") %>% \n  mutate(group1 = as_factor(group1),\n         group2 = as_factor(group2))\n\nIn Abbildung A.1 sehen wir die Matrix der Zähldaten für die beiden Gruppen nochmal visualisiert. Beim betrachten fällt auf, dass die beiden Level C und D kaum Zähldaten enthalten. Hier wäre zu überlegen die beiden Level aus der Analyse herauszunehmen und einen klassischen \\(\\mathcal{X}^2\\)-Test auf einer 2x2 Kreuztabelle zu rechnen.\n\nggplot(plot_tbl, aes(x = group2, y = value, fill = group1)) +\n  theme_bw() +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  labs(x = \"Gruppe 2\", y = \"Anzahl\", fill = \"Gruppe 1\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung A.1— Barplot der Zähldaten aus der Matrix."
  },
  {
    "objectID": "app-example-analysis.html#auswertung-von-gewichten",
    "href": "app-example-analysis.html#auswertung-von-gewichten",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.3 Auswertung von Gewichten",
    "text": "A.3 Auswertung von Gewichten\n\n\n\n\n\n\n\ntrt\nblock\nrep\nrsp\n\n\n\nlow\nI\n1\n7.80\n\n\nlow\nI\n2\n16.26\n\n\nlow\nI\n3\n11.39\n\n\nlow\nI\n4\n17.09\n\n\nlow\nII\n1\n10.59\n\n\nlow\nII\n2\n12.09\n\n\nlow\nII\n3\n12.56\n\n\nlow\nII\n4\n10.27\n\n\nlow\nIII\n1\n16.54\n\n\nlow\nIII\n2\n15.50\n\n\nlow\nIII\n3\n13.63\n\n\nlow\nIII\n4\n15.17\n\n\nmid\nI\n1\n18.48\n\n\nmid\nI\n2\n17.00\n\n\nmid\nI\n3\n16.84\n\n\nmid\nI\n4\n14.21\n\n\nmid\nII\n1\n13.14\n\n\nmid\nII\n2\n12.35\n\n\nmid\nII\n3\n19.27\n\n\nmid\nII\n4\n13.59\n\n\nmid\nIII\n1\n14.00\n\n\nmid\nIII\n2\n14.75\n\n\nmid\nIII\n3\n18.02\n\n\nmid\nIII\n4\n15.23\n\n\nhigh\nI\n1\n13.51\n\n\nhigh\nI\n2\n17.37\n\n\nhigh\nI\n3\n20.80\n\n\nhigh\nI\n4\n18.35\n\n\nhigh\nII\n1\n17.51\n\n\nhigh\nII\n2\n18.72\n\n\nhigh\nII\n3\n20.63\n\n\nhigh\nII\n4\n17.19\n\n\nhigh\nIII\n1\n18.01\n\n\nhigh\nIII\n2\n17.34\n\n\nhigh\nIII\n3\n18.51\n\n\nhigh\nIII\n4\n15.48\n\n\n\n\n\n\nA.3.1 Explorative Datenanalyse (EDA)\n\nggplot(data_tbl, aes(trt, rsp, color = block)) +\n  geom_boxplot()\n\n\n\n\n\nstat_tbl <- data_tbl %>% \n  group_by(trt, block) %>% \n  summarise(mean = mean(rsp),\n            sd = sd(rsp),\n            se = sd/sqrt(n()))\n\nggplot(stat_tbl, aes(x = trt, y = mean, fill = block)) + \n    geom_bar(position = position_dodge(), stat = \"identity\") +\n    geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                  width = 0.2,\n                  position = position_dodge(.9))\n\n\n\n\n\nA.3.2 Lineares Modell\n\nfit_1 <- lm(rsp ~ trt + block, data = data_tbl)\n\n\nA.3.3 ANOVA\n\nfit_1 %>% anova\n\nAnalysis of Variance Table\n\nResponse: rsp\n          Df   Sum Sq Mean Sq  F value     Pr(>F)    \ntrt        2 123.9702 61.9851 10.35749 0.00035896 ***\nblock      2   9.4272  4.7136  0.78762 0.46381669    \nResiduals 31 185.5215  5.9846                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nA.3.4 Gruppenvergleich mit dem multcomp Paket\nhttps://broom.tidymodels.org/reference/tidy.glht.html\n\nfit_1 %>% \n  glht(linfct = mcp(trt = \"Tukey\")) %>% \n  tidy %>% \n  select(contrast, estimate, adj.p.value) %>% \n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 3 × 3\n  contrast   estimate adj.p.value\n  <chr>         <dbl>       <dbl>\n1 mid - low      2.33      0.0655\n2 high - low     4.54      0.0002\n3 high - mid     2.21      0.0844\n\n\n\nA.3.5 Gruppenvergleich mit der emmeans Paket\nhttps://broom.tidymodels.org/reference/tidy.emmGrid.html\n\nfit_1 %>% \n  emmeans(\"trt\") %>% \n  contrast(method = \"pairwise\") %>% \n  tidy %>% \n  select(contrast, estimate, adj.p.value) %>% \n  mutate(across(where(is.numeric), round, 4))\n\n# A tibble: 3 × 3\n  contrast   estimate adj.p.value\n  <chr>         <dbl>       <dbl>\n1 low - mid     -2.33      0.0655\n2 low - high    -4.54      0.0002\n3 mid - high    -2.21      0.0845"
  },
  {
    "objectID": "app-example-analysis.html#auswertung-von-boniturnoten",
    "href": "app-example-analysis.html#auswertung-von-boniturnoten",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.4 Auswertung von Boniturnoten",
    "text": "A.4 Auswertung von Boniturnoten\n\n\n\n\n\n\n\nvariety\nblock\nrating\n\n\n\nA\nI\n2\n\n\nA\nI\n3\n\n\nA\nI\n3\n\n\nA\nI\n4\n\n\nA\nI\n1\n\n\nA\nII\n3\n\n\nA\nII\n2\n\n\nA\nII\n2\n\n\nA\nII\n4\n\n\nA\nII\n4\n\n\nA\nIII\n2\n\n\nA\nIII\n2\n\n\nA\nIII\n3\n\n\nA\nIII\n1\n\n\nA\nIII\n2\n\n\nB\nI\n8\n\n\nB\nI\n9\n\n\nB\nI\n8\n\n\nB\nI\n9\n\n\nB\nI\n7\n\n\nB\nII\n7\n\n\nB\nII\n7\n\n\nB\nII\n8\n\n\nB\nII\n8\n\n\nB\nII\n7\n\n\nB\nIII\n8\n\n\nB\nIII\n9\n\n\nB\nIII\n7\n\n\nB\nIII\n9\n\n\nB\nIII\n8\n\n\nC\nI\n6\n\n\nC\nI\n5\n\n\nC\nI\n5\n\n\nC\nI\n6\n\n\nC\nI\n4\n\n\nC\nII\n4\n\n\nC\nII\n5\n\n\nC\nII\n3\n\n\nC\nII\n6\n\n\nC\nII\n4\n\n\nC\nIII\n7\n\n\nC\nIII\n6\n\n\nC\nIII\n4\n\n\nC\nIII\n6\n\n\nC\nIII\n4\n\n\nD\nI\n2\n\n\nD\nI\n4\n\n\nD\nI\n1\n\n\nD\nI\n2\n\n\nD\nI\n2\n\n\nD\nII\n2\n\n\nD\nII\n4\n\n\nD\nII\n4\n\n\nD\nII\n1\n\n\nD\nII\n3\n\n\nD\nIII\n3\n\n\nD\nIII\n4\n\n\nD\nIII\n2\n\n\nD\nIII\n1\n\n\nD\nIII\n3\n\n\nE\nI\n4\n\n\nE\nI\n4\n\n\nE\nI\n2\n\n\nE\nI\n7\n\n\nE\nI\n5\n\n\nE\nII\n4\n\n\nE\nII\n3\n\n\nE\nII\n4\n\n\nE\nII\n7\n\n\nE\nII\n7\n\n\nE\nIII\n5\n\n\nE\nIII\n5\n\n\nE\nIII\n4\n\n\nE\nIII\n6\n\n\nE\nIII\n6\n\n\n\n\n\n\nA.4.1 Explorative Datenanalyse (EDA)\n\nggplot(data_tbl, aes(variety, rating, color = block)) +\n  geom_boxplot() +\n  geom_dotplot(aes(fill = block), binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.8))  \n\n\n\n\n\nggplot(data_tbl, aes(variety, rating, fill = block)) +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.8)) +\n  stat_summary(fun = median, fun.min = median, fun.max = median,\n               geom = \"crossbar\", width = 0.5, \n               position=position_dodge(0.8)) \n\n\n\n\n\nA.4.2 Friedman Test\n\n#friedman.test(rating ~ variety | block, data = data_tbl)\n\ndata_tbl <- tibble(Block = 1:4,\n                   Sorte_1 = c(2,3,4,3),\n                   Sorte_2 = c(7,9,8,9),\n                   Sorte_3 = c(6,5,4,7),\n                   Sorte_4 = c(2,4,1,2),\n                   Sorte_5 = c(4,5,3,7)) %>%\n  gather(key, value, Sorte_1:Sorte_5)\n\nfriedman.test(value ~ key | Block, data = data_tbl)\n\n\n    Friedman rank sum test\n\ndata:  value and key and Block\nFriedman chi-squared = 13.5263, df = 4, p-value = 0.0089709"
  },
  {
    "objectID": "app-example-analysis.html#auswertung-von-infektionsstatus",
    "href": "app-example-analysis.html#auswertung-von-infektionsstatus",
    "title": "Appendix A — Beispielhafte Auswertungen",
    "section": "\nA.5 Auswertung von Infektionsstatus",
    "text": "A.5 Auswertung von Infektionsstatus"
  },
  {
    "objectID": "app-r-tutorial.html",
    "href": "app-r-tutorial.html",
    "title": "Appendix B — Tutorium in R",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:40:34\nIn diesem Kapitel gibt es eine etwas wilde Sammlung an Fragen und Antworten, die im Rahmen des R Tutoriums aufkamen. Vielleicht findest du ja was, was dich inspiriert."
  },
  {
    "objectID": "app-r-tutorial.html#genutzte-r-pakete-für-das-kapitel",
    "href": "app-r-tutorial.html#genutzte-r-pakete-für-das-kapitel",
    "title": "Appendix B — Tutorium in R",
    "section": "\nB.1 Genutzte R Pakete für das Kapitel",
    "text": "B.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, broom, broom.mixed, \n               multcomp, emmeans, performance, lme4, effectsize)\n\nSchaue dir bitte erst den R Code zu deiner Frage an und dann kannst du die Pakete noch nachinstallieren. Es werden sich hier sicherlich eine Menge ansammeln."
  },
  {
    "objectID": "app-r-tutorial.html#sec-black-circle-point",
    "href": "app-r-tutorial.html#sec-black-circle-point",
    "title": "Appendix B — Tutorium in R",
    "section": "\nB.2 Wie setze ich um einen farbigen Punkt einen schwarzen Rand?",
    "text": "B.2 Wie setze ich um einen farbigen Punkt einen schwarzen Rand?\nIn ggplot können wir verschiedene Typen von Punkte auswählen - auch shape genannt. Der shape mit der Nummer 21 hat die Möglichkeit die Füllung fill anders zu wählen, als die Randfarbe über color. Daher können wir die Punkte nach dem Faktor trt einfärben und setzen dann in dem geom_point() die Randfarbe auf black. Wir könnten da auch jede andere Farbe nehmen.\n\nplot_tbl <- tibble(x = 1:10,\n                   y = rnorm(10, 5, 1),\n                   trt = rep(c(\"A\", \"B\"), each = 5)) %>% \n  mutate(trt =  as_factor(trt))\n\nggplot(plot_tbl, aes(x, y, fill = trt)) +\n  geom_point(color = \"black\", shape = 21, size = 4) +\n  theme_bw() \n\n\n\nAbbildung B.1— Farbige Punkte mit einem schwarzen Rand."
  },
  {
    "objectID": "app-r-tutorial.html#sec-order-x-names",
    "href": "app-r-tutorial.html#sec-order-x-names",
    "title": "Appendix B — Tutorium in R",
    "section": "\nB.3 Wie kann ich die Anordnung der Nutzungen bzw. Behandlungen in einer Grafik definieren?",
    "text": "B.3 Wie kann ich die Anordnung der Nutzungen bzw. Behandlungen in einer Grafik definieren?\nHäufig ist es so, dass unser Behandlungs oder Nutzenspalte eine bestimmte Ordnung hat. Wenn wir die Ordnung beibehalten wollen, wie die Ordnung auch im Datensatz ist, dann können wir nach dem Einlesen der Daten die Funktion as_factor() nutzen. Dann bleibt die ursprüngliche Ordnung erhalten.\nWenn wir eine andere Ordnung haben wollen, dann können wir mit der Funktion factor() und der Option level = eine neue Ordnung der existierenden Level vorgeben.\nFür die Umbenennung in R empfehle ich die Funktion recode().\n\nplot_tbl <- tibble(y = rnorm(20, 5, 1),\n                   trt = rep(c(\"Gemüse\", \"Obst\", \"Strauch\", \"Brache\"), \n                             each = 5)) %>% \n  mutate(trt =  factor(trt, level = c(\"Obst\", \"Strauch\", \"Brache\", \"Gemüse\")))\n\nggplot(plot_tbl, aes(trt, y, fill = trt)) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung B.2— Neuordnung des Faktors trt für die Boxplots."
  },
  {
    "objectID": "app-r-tutorial.html#sec-order-error-bars",
    "href": "app-r-tutorial.html#sec-order-error-bars",
    "title": "Appendix B — Tutorium in R",
    "section": "\nB.4 Wie kann ich die Anordnung der Fehlerbalken in einem Barplot verändern?",
    "text": "B.4 Wie kann ich die Anordnung der Fehlerbalken in einem Barplot verändern?\nIn dem Kapitel 15 haben wir uns ja nur mit Balkendiagrammen mit einem Faktor beschäftigt. Das heist, wir haben den Faktor auf die x-Achse gelegt und schon hatten wir den Plot. Wenn wir zwei Faktoren haben, dann müssen wir über die Option position = position_dodge() etwas spielen. Wir können auch die Position und des Abstand etwas ändern. Ich habe hier 0.9 für die Fehlerbalken in position = position_dodge() probiert und es sieht ziemlich gut aus.\n\nplot_tbl <- expand_grid(site = 1:4, \n                        trt = 1:2, \n                        rep = 1:5) %>% \n  mutate(site = factor(site, labels = c(\"A\", \"B\", \"C\", \"D\")),\n         trt = factor(trt, labels = c(\"new\", \"old\")),\n         rsp = rnorm(n(), 10, 2))\n\nstat_tbl <- plot_tbl %>% \n  group_by(site, trt) %>% \n  summarise(mean = mean(rsp),\n            sd = sd(rsp))\n\nggplot(stat_tbl, aes(x = site, y = mean, group = trt, fill = trt)) + \n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                width = 0.2, position = position_dodge(0.9)) +\n  theme_bw() +\n  labs(fill = \"Behandlung\")\n\n\n\nAbbildung B.3— Fehlerbalken für einen Barplot mit zwei Faktoren."
  },
  {
    "objectID": "app-r-tutorial.html#keimung",
    "href": "app-r-tutorial.html#keimung",
    "title": "Appendix B — Tutorium in R",
    "section": "\nB.5 Keimung",
    "text": "B.5 Keimung\n\ngerm_tbl <- read_excel(\"data/germination_data.xlsx\")"
  },
  {
    "objectID": "app-r-tutorial.html#schweine",
    "href": "app-r-tutorial.html#schweine",
    "title": "Appendix B — Tutorium in R",
    "section": "\nB.6 Schweine",
    "text": "B.6 Schweine\n\npig_tbl <- read_excel(\"data/pig_feed_data.xlsx\")"
  },
  {
    "objectID": "app-r-tutorial.html#kohlenstoffnitrat",
    "href": "app-r-tutorial.html#kohlenstoffnitrat",
    "title": "Appendix B — Tutorium in R",
    "section": "\nB.7 Kohlenstoff/Nitrat",
    "text": "B.7 Kohlenstoff/Nitrat\n\ncarbon_tbl <- read_excel(\"data/carbon_data.xlsx\") %>% \n  mutate(c2n = c_o/n,\n         c_m2c_o = c_m/c_o)"
  },
  {
    "objectID": "app-r-tutorial.html#lichtintensität",
    "href": "app-r-tutorial.html#lichtintensität",
    "title": "Appendix B — Tutorium in R",
    "section": "\nB.8 Lichtintensität",
    "text": "B.8 Lichtintensität\n\nintensity_tbl <- read_excel(\"data/light_intensity_data.xlsx\") %>% \n  mutate(rack = as_factor(rack),\n         layer = as_factor(layer),\n         light_intensity = factor(light_intensity, \n                                  labels = c(\"low\", \"mid\", \"high\")))\n\n\n\n\n\nfit_1 <- lm(growth ~ light_intensity + rack + layer, data = intensity_tbl)\n\nfit_1 %>% anova %>% tidy\n\n# A tibble: 4 × 6\n  term               df  sumsq meansq statistic  p.value\n  <chr>           <int>  <dbl>  <dbl>     <dbl>    <dbl>\n1 light_intensity     2  433.   217.      3.87   0.0278 \n2 rack                2  628.   314.      5.61   0.00652\n3 layer               2   70.9   35.5     0.633  0.535  \n4 Residuals          47 2631.    56.0    NA     NA      \n\n\n\nfit_1 %>% glht(linfct = mcp(light_intensity = \"Tukey\")) %>% tidy\n\n# A tibble: 3 × 7\n  term            contrast   null.value estimate std.error statistic adj.p.value\n  <chr>           <chr>           <dbl>    <dbl>     <dbl>     <dbl>       <dbl>\n1 light_intensity mid - low           0    -2.88      2.49     -1.15      0.486 \n2 light_intensity high - low          0    -6.91      2.49     -2.77      0.0214\n3 light_intensity high - mid          0    -4.03      2.49     -1.61      0.249 \n\n\n\nmarginal <- emmeans(fit_1, \"light_intensity\")\ntidy(marginal) %>% \n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 3 × 6\n  light_intensity estimate std.error    df statistic p.value\n  <chr>              <dbl>     <dbl> <dbl>     <dbl>   <dbl>\n1 low                 18.2      1.76    47     10.3        0\n2 mid                 15.3      1.76    47      8.69       0\n3 high                11.3      1.76    47      6.4        0\n\n\n\nmarginal %>% contrast(method = \"pairwise\") %>% tidy %>% \n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 3 × 8\n  term        contrast null.value estimate std.error    df statistic adj.p.value\n  <chr>       <chr>         <dbl>    <dbl>     <dbl> <dbl>     <dbl>       <dbl>\n1 light_inte… low - m…          0     2.88      2.49    47      1.15        0.49\n2 light_inte… low - h…          0     6.91      2.49    47      2.77        0.02\n3 light_inte… mid - h…          0     4.03      2.49    47      1.61        0.25"
  },
  {
    "objectID": "app-r-tutorial.html#komplexes-weizenbeispiel",
    "href": "app-r-tutorial.html#komplexes-weizenbeispiel",
    "title": "Appendix B — Tutorium in R",
    "section": "\nB.9 Komplexes Weizenbeispiel",
    "text": "B.9 Komplexes Weizenbeispiel\nWir wollen uns nun ein kpmplexeres Datenbeispiel anschauen. In diesem Beispiel liegen zum einen die Daten in einem ungünstigen Wide-Format vor und müssen über gather() erst in das Long-Format gebracht werden. Zum anderen entstehen dadurch ungünstige Einträge in der key-Spalte, so dass wir hier nochmal einen regulären Ausdruck benötigen um den character Vektor umwandeln zu können.\nAls wäre dies nicht schon kompliziert genug, schauen wir uns nicht nur ein Outcome an, sondern in der Summe die Outcomes Weizenhöhe, Chlorophyllgehalt sowie Frisch- und Trockengewichte. Der Weizen wurde in vier Blöcken angezogen und zu verschiedenen Zeitpunkten gemessen. Hierdurch entsteht ein komplexer Versuchsaufbau.\n\nB.9.1 Weizenhöhe\nDie Höhe der Weizenpflanzen [cm] wurde in vier Blöcken an insgesamt neun Tagen gemessen. Die Datei corn_plant_height.csv beinhaltet die Daten des Versuchs. Für die folgende Auswertung nehmen wir an, das die Weizenhöhe normalverteilt ist. Wie beginnen mit einer exploratven Datenanalyse udn schauen uns die Daten einmal an.\n\nB.9.1.1 Exlorative Datenanalyse\n\nplant_tbl <- read_csv2(\"data/corn_plant_height.csv\") %>% \n  gather(key = \"day\", value = \"height\", \"1...3\":\"9...47\") %>% \n  mutate(day = str_replace(day, \"...\\\\d+\", \"\"),\n         day = as_factor(day),\n         treatment = as_factor(treatment),\n         block = factor(block, labels = c(\"I\", \"II\", \"III\", \"IV\")))\n\nIn der csv-Datei sind die die Tage jeweils fünfmal mit einer 1 bis 9 in den Spalten abgebildet. Wir nutzen die Funktion read_csv2 um mit dem deutschen Format der csv-Datei umgehen zu können. Die Funktion read_csv2 erkennt das ; als Separator. Da R nicht mit gleichen Benennungen in den Spalten umgehen kann, setzt R hinter jeden Spaltennamen, der gleich ist drei Punkte und eine fortlaufende Zahl. Mit der Funktion gather() können wir die Spalten 1...3 bis 9...47 untereinanderkleben. Abschließend müssen wir noch den ...[Zahl]-Teil loswerden. Das machen wir über den regulären Ausdruck in der Funktion str_replace(). Reguläre Ausdrücke musst du nicht verstehen, sind aber sehr mächtige Werkzeuge im Umgang mit großen Datensätzen.\nSchauen wir uns nun einmal die Daten an. Unser Outcome (Y) ist height und auf X wollen wir das treatment. Das wollen wir die Boxplots noch nach dem Tag einfärben und jeweils ein Subplot für die vier Blöcke bauen.\n\nggplot(plant_tbl, aes(x = treatment, y = height, fill = day)) +\n  geom_boxplot() +\n  facet_wrap(~ block) +\n  labs(x = \"Behandlung\", y = \"Weizenhöhe [cm]\", fill = \"Messtag\") +\n  theme_bw()\n\n\n\nAbbildung B.4— An 39 Hunden wurde die Anzahl an Flöhen gezählt.\n\n\n\n\nAbbildung B.4 zeigt den entsprechenden Boxplot. Du siehst, dass du auf den ersten Blick nichts siehst. Bei einer so großen Datenmenge ist es selbst mit einem guten ggplot() schwer etwas zu erkennen. Hier müssen wir uns mehrere Fragen stellen…\n\n… wollen wir wirklich alle Blöcke getrennt auswerten?\n… wollen wir uns wirklich alle Tage anschauen? Oder geht es nicht eher um die Pflanzenhöhe am Ende des Versuches?\n… wollen wir wirklich alle treatment Stufen vergleichen?\n\n\nplant_tbl %>% \n  filter(block == \"I\") %>% \n  filter(day %in% c(6, 7, 8, 9)) %>% \n  ggplot(aes(x = treatment, y = height, fill = day)) +\n  geom_boxplot() +\n  facet_wrap(~ block) +\n  labs(x = \"Behandlung\", y = \"Weizenhöhe [cm]\", fill = \"Messtag\") +\n  theme_bw()\n\n\n\nAbbildung B.5— An 39 Hunden wurde die Anzahl an Flöhen gezählt.\n\n\n\n\nAbbildung B.5 zeigt einen Auschnitt in dem wir nur nach Block I und den Tagen 6 bis 9 gefiltert haben. In diesem Fall könnten wir auf den vollen Datensatz weitermachen oder vorab über filter() einen kleinern Datensatz bauen, der unsere Fragestellung bgut beantworten kann. Wir gehen jetzt den steinigeren Weg und analysieren den ganzen Datensatz - das muss nicht der bessere Weg sein!\n\nB.9.1.2 Lineares Modell mit lm()\n\nWir beginnen mit einer ANOVA und müssen dafür ein lineare Modell schätzen. Dafür nutzen wir erst die Funktion lm() und anschließend mit dem Ergebnis des linearen Modells die Funktion anova() um eine Varianzanalyse durchzuführen.\n\nfit_height <- lm(height ~ treatment + day + block + \n                   treatment:day + treatment:block, \n                 data = plant_tbl)\n\nfit_height %>% anova\n\nAnalysis of Variance Table\n\nResponse: height\n                  Df  Sum Sq Mean Sq    F value                 Pr(>F)    \ntreatment          7  2402.5  343.22  211.20218 < 0.000000000000000222 ***\nday                8 41280.2 5160.03 3175.28495 < 0.000000000000000222 ***\nblock              3    63.4   21.14   13.01124     0.0000000222375065 ***\ntreatment:day     56  1076.3   19.22   11.82650 < 0.000000000000000222 ***\ntreatment:block   21   162.2    7.72    4.75297     0.0000000000099287 ***\nResiduals       1344  2184.1    1.63                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir konzentrieren uns auf die Spalte Pr(>F) welche den p-Wert beinhaltet. Wir schauen welcher p-Wert kleiner ist als \\(\\alpha = 5\\% = 0.05\\). Alle p-Werte sind signifikant. Mindestens zwei treatment Level unterscheiden sich, mindestens zwei day Level unterschieden sich und mindestens zwei block Level unterscheiden sich. Abschließend ist auch der Interaktionsterm zwischen den Behandlungen und den Tagen sowie den Behandlungen und den Blöcken signifikant.\n\nfit_height %>% anova %>% eta_squared(partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter       |     Eta2 |       95% CI\n-----------------------------------------\ntreatment       |     0.05 | [0.03, 1.00]\nday             |     0.88 | [0.87, 1.00]\nblock           | 1.34e-03 | [0.00, 1.00]\ntreatment:day   |     0.02 | [0.00, 1.00]\ntreatment:block | 3.44e-03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\n\nfit_height_lme <- lmer(height ~ treatment + block + (1|day), \n                       data = plant_tbl)\n\nfit_height_lme %>% summary\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: height ~ treatment + block + (1 | day)\n   Data: plant_tbl\n\nREML criterion at convergence: 5430.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.09029 -0.62046 -0.00151  0.57885  4.37546 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n day      (Intercept) 32.2351  5.6776  \n Residual              2.4085  1.5519  \nNumber of obs: 1440, groups:  day, 9\n\nFixed effects:\n                  Estimate Std. Error  t value\n(Intercept)       13.51875    1.89739   7.1249\ntreatmentlow_Fe   -0.15778    0.16359  -0.9645\ntreatmentmid_P     1.30278    0.16359   7.9637\ntreatmentmid_Fe    1.38111    0.16359   8.4425\ntreatmenthigh_P    1.16167    0.16359   7.1011\ntreatmenthigh_Fe   1.63722    0.16359  10.0081\ntreatmentlow_N    -1.87333    0.16359 -11.4514\ntreatmentlow_P_Fe -1.61278    0.16359  -9.8587\nblockII            0.58861    0.11568   5.0885\nblockIII           0.24611    0.11568   2.1276\nblockIV            0.23472    0.11568   2.0291\n\nCorrelation of Fixed Effects:\n            (Intr) trtmntl_F trtmntm_P trtmntm_F trtmnth_P trtmnth_F trtm_N\ntretmntlw_F -0.043                                                         \ntretmntmd_P -0.043  0.500                                                  \ntretmntmd_F -0.043  0.500     0.500                                        \ntrtmnthgh_P -0.043  0.500     0.500     0.500                              \ntrtmnthgh_F -0.043  0.500     0.500     0.500     0.500                    \ntretmntlw_N -0.043  0.500     0.500     0.500     0.500     0.500          \ntrtmntl_P_F -0.043  0.500     0.500     0.500     0.500     0.500     0.500\nblockII     -0.030  0.000     0.000     0.000     0.000     0.000     0.000\nblockIII    -0.030  0.000     0.000     0.000     0.000     0.000     0.000\nblockIV     -0.030  0.000     0.000     0.000     0.000     0.000     0.000\n            tr_P_F blckII blcIII\ntretmntlw_F                     \ntretmntmd_P                     \ntretmntmd_F                     \ntrtmnthgh_P                     \ntrtmnthgh_F                     \ntretmntlw_N                     \ntrtmntl_P_F                     \nblockII      0.000              \nblockIII     0.000  0.500       \nblockIV      0.000  0.500  0.500\n\nfit_height_lme %>% \n  tidy(conf.int = TRUE, effects = \"fixed\")\n\n# A tibble: 11 × 7\n   effect term              estimate std.error statistic conf.low conf.high\n   <chr>  <chr>                <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n 1 fixed  (Intercept)         13.5       1.90      7.12   9.80       17.2  \n 2 fixed  treatmentlow_Fe     -0.158     0.164    -0.964 -0.478       0.163\n 3 fixed  treatmentmid_P       1.30      0.164     7.96   0.982       1.62 \n 4 fixed  treatmentmid_Fe      1.38      0.164     8.44   1.06        1.70 \n 5 fixed  treatmenthigh_P      1.16      0.164     7.10   0.841       1.48 \n 6 fixed  treatmenthigh_Fe     1.64      0.164    10.0    1.32        1.96 \n 7 fixed  treatmentlow_N      -1.87      0.164   -11.5   -2.19       -1.55 \n 8 fixed  treatmentlow_P_Fe   -1.61      0.164    -9.86  -1.93       -1.29 \n 9 fixed  blockII              0.589     0.116     5.09   0.362       0.815\n10 fixed  blockIII             0.246     0.116     2.13   0.0194      0.473\n11 fixed  blockIV              0.235     0.116     2.03   0.00800     0.461\n\nmodel_performance(fit_height_lme) \n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n--------------------------------------------------------------------------------\n5456.619 | 5456.875 | 5525.161 |      0.934 |      0.047 | 0.930 | 1.542 | 1.552\n\nr2(fit_height_lme)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.934\n     Marginal R2: 0.047\n\nconf_tbl <- glht(fit_height_lme, linfct = mcp(treatment = \"Tukey\")) %>% \n  tidy(conf.int = TRUE) %>% \n  arrange(estimate) %>% \n  mutate(contrast = as_factor(contrast))\n\nJoining, by = c(\"term\", \"contrast\", \"estimate\")\n\nggplot(conf_tbl, aes(x = contrast, y = estimate, \n                     ymin = conf.low, ymax = conf.high)) +\n  geom_pointrange() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(x = \"\", y = \"Mittelwertsdifferenz der Weizenhöhe [cm]\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\nB.9.2 Chlorophyllgehalt\n\nchlorophyl_tbl <- read_csv2(\"data/corn_chlorophyl.csv\") %>% \n  gather(key = \"day\", value = \"chlorophyl\", \"1...3\":\"3...62\") %>% \n  mutate(day = str_replace(day, \"...\\\\d+\", \"\"),\n         day = as_factor(day),\n         treatment = as_factor(treatment),\n         block = factor(block, labels = c(\"I\", \"II\", \"III\", \"IV\"))) %>% \n  filter(chlorophyl >= 20 & chlorophyl <= 100)\n\n\nggplot(chlorophyl_tbl, aes(x = treatment, y = chlorophyl, fill = day)) +\n  geom_boxplot() +\n  facet_wrap(~ block) +\n  labs(x = \"Behandlung\", y = \"Chlorophyllgehalt\", fill = \"Messtag\") +\n  theme_bw()\n\n\n\n\n\nB.9.3 Frisch- und Trockenmasse\n\nburn_tbl <- read_csv2(\"data/corn_burning.csv\") %>% \n  gather(key = \"day_outcome\", value = \"drymatter\", \"1_FM\":\"3_TMperc\") %>%\n  separate(day_outcome, c(\"day\", \"outcome\")) %>% \n  mutate(day = as_factor(day),\n         treatment = as_factor(treatment),\n         block = factor(block, labels = c(\"I\", \"II\", \"III\", \"IV\")),\n         outcome = as_factor(outcome)) \n\n\nburn_tbl %>% \n  filter(outcome == \"FM\") %>% \n  ggplot(aes(x = treatment, y = drymatter, color = day)) +\n  geom_point() +\n  ##facet_wrap(~ block, scales = \"free_y\") +\n  labs(x = \"Behandlung\", y = \"Gewicht\", fill = \"Messtag\") +\n  theme_bw()"
  },
  {
    "objectID": "app-how-to-write.html",
    "href": "app-how-to-write.html",
    "title": "Appendix C — Writing principles",
    "section": "",
    "text": "Version vom October 11, 2022 um 21:40:48"
  },
  {
    "objectID": "app-how-to-write.html#wie-schreibe-ich-eine-gute-abschlussarbeit",
    "href": "app-how-to-write.html#wie-schreibe-ich-eine-gute-abschlussarbeit",
    "title": "Appendix C — Writing principles",
    "section": "\nC.1 Wie schreibe ich eine gute Abschlussarbeit",
    "text": "C.1 Wie schreibe ich eine gute Abschlussarbeit\nSchreiben ist nicht einfach. Aber es folgt einem Schema. Schreiben ist anstrengend und dauert seine Zeit. Ein guter Text wird mehrfach revidiert, umgeschrieben und gelöscht, bis er zu einem guten Text geworden ist. Häufig vergehen mehrere Tage bis man eine Idee so auf das Blatt Papier gebracht hat, dass auch ein Dritter den Text lesen und verstehen kann.\n\n\n\n\n\n\nZentraler Gedanke\n\n\n\nDer erste Entwurf ist für einen selber, ab dem zweiten geht es um den Leser.\n\n\nEs geht also am Anfang erstmal darum, Text auf das weiße Blatt zu kriegen.\n\n\n\n\n\n\nOtto Kruse – Keine Angst vor dem leeren Blatt\n\n\n\nViele wissenschaftliche Themen kann man erst dann lösen, wenn man alle Aspekte explizit formuliert hat. Das Denken ist dafür insofern nicht genügend vorbereitet, als es immer nur kleine Ausschnitte fokussieren kann. Systematisch denken kann man nur, wenn man schreibt, also die Ergebnisse seines Denkens festhält und mit weiteren Aspekten in Beziehung setzt.\n\n\nDas ist die Idee vom Schreiben. Andere merken, wie weit du mit deinen Gedanken gekommen bist. Wir bringen komplexe Gedankengänge in die lineare Form des Textes."
  },
  {
    "objectID": "app-how-to-write.html#der-schreibprozess",
    "href": "app-how-to-write.html#der-schreibprozess",
    "title": "Appendix C — Writing principles",
    "section": "\nC.2 Der Schreibprozess",
    "text": "C.2 Der Schreibprozess\nDer Schreibprozess läuft nach Beginn in den nächsten Wochen und Monaten in mehreren Phasen ab. In jeder Phase ist es wichtig, sich ein gutes Umfeld für konzentriertes Arbeiten zu schaffen, gleichzeitig aber die Kommunikation mit den Betreuenden und anderen Mitschaffenden nicht abreißen zu lassen."
  },
  {
    "objectID": "app-how-to-write.html#zeitplan",
    "href": "app-how-to-write.html#zeitplan",
    "title": "Appendix C — Writing principles",
    "section": "\nC.3 Zeitplan",
    "text": "C.3 Zeitplan\nHier ist es wichtig was geschrieben werden soll. Eine Bachelor- und Masterarbeit hat einen klaren Zeitplan, der vorab feststehen muss. Bis wann müssen wie viele Wörter geschrieben sein, damit die Arbeit fertig werden kann. Beide Abschlussformen haben ja unterschiedliche Zeitrahmen. Und das ist wirklich wichtig! Wann sollte man fertig sein mit Programmieren, wie lange soll man sich Zeit für Vortragsvorbereitung nehmen, etc. Auch eine wissenschaftliche Veröffentlichung hat einen Zeitplan! Leider – das ist der Primat der Forschung – kann sich der Zeitplan immer wieder ändern, wenn Methoden nicht klappen oder neue Erkenntnisse gewonnen werden. Dennoch muss klar sein, dass in endlicher Zeit – meist ein Jahr – ein Paper eingereicht werden kann. Auf dieses Ziel sollten sich alle Einschwören. Sonst ist eine Promotion in endlicher Zeit nicht machbar."
  },
  {
    "objectID": "app-how-to-write.html#ideen-entwickeln",
    "href": "app-how-to-write.html#ideen-entwickeln",
    "title": "Appendix C — Writing principles",
    "section": "\nC.4 Ideen entwickeln",
    "text": "C.4 Ideen entwickeln\nMan sammelt Ideen zunächst in der Breite und fokussiert dann, was davon man aufschreiben will. Man liest andere wissenschaftliche Paper, lernt vielleicht noch Grundlagen und Methoden, und macht sich Notizen, was interessant sein könnte, und wo es steht. Natürlich kann man sich bei den Betreuern und Mitschaffenden Hilfe und Anregungen holen, wo man Input herbekommt. Holen heißt aber nicht, dass man gebracht bekommt.\n\n\n\n\n\n\nOCAR Prinzip\n\n\n\nFrage dich, was ist das Opening (der Hintergrund der Arbeit), die Challenge (was ist das Problem, was gelöst werden soll?), die Action (was wirst du tun um dieses Problem zu lösen?) und die Results (Was kam dabei raus oder soll rauskommen?)"
  },
  {
    "objectID": "app-how-to-write.html#strukturieren",
    "href": "app-how-to-write.html#strukturieren",
    "title": "Appendix C — Writing principles",
    "section": "\nC.5 Strukturieren",
    "text": "C.5 Strukturieren\nDas grobe Gerüst ist ja vorgegeben. Eine wissenschaftliche Arbeit folgt dem IMRaD Schema. Erst die Einleitung (Introduction), dann die Methoden (Methods), gefolgt von den Ergebnissen (Results) und der Diskussion (Discussion). Am Ende der Einleitung wird nochmal die Fragestellung benannt. Welche Frage soll in der Arbeit beantwortet werden? Dann fehlt noch die Zusammenfassung am Anfang (abstract) und der Schluss bzw. das Fazit (conclusion). So ist das vorgegebene Schema für die Arbeit, das soll mit Inhalt gefüllt werden. Klingt erstmal einfach und ist es auch. Mit Zwischenüberschriften in den einzelnen Abschnitten kann man sich eine grobe Ordnung vorgeben, was in welcher Reihenfolge aufgeschrieben werden soll. Die Struktur innerhalb von Methodenteilen ist zum Beispiel oft gleich, Beschreibung der Studie, Beschreibung der interessierenden Variablen und ihrer Erhebung, Beschreibung der Auswertungsmethodik. Das kommt aber aufs Thema an. Und unterhalb dieser kann man sich wieder Unter-zwischen-unterüberschriften machen. Es wird sowieso noch alles überarbeitet.\n\n\n\n\n\n\nGrobe Strukturierung nach IMRaD\n\n\n\n\nZusammenfassung\n\n\nEinleitung\n\nForschungsfrage\n\n\nMaterial und Methoden\nErgebnisse\nDiskussion\n\n\nLiteratur\n\n\n\nWenn es einen Flowchart gibt, so gibt dieser auch die Struktur vor. Ein Flowchart ist nicht final und ändert sich mit der Zeit! Mach den Flowchart am besten auf einem Blatt Papier. Da kannst du schneller was ergänzen.\n\n\n\n\n\n\nHinweis\n\n\n\nZeichne einen Flowchart, der aufzeigt was in der Arbeit passiert!"
  },
  {
    "objectID": "app-how-to-write.html#rohtexten",
    "href": "app-how-to-write.html#rohtexten",
    "title": "Appendix C — Writing principles",
    "section": "\nC.6 Rohtexten",
    "text": "C.6 Rohtexten\nDas kann wirklich sloppy sein, in Stichpunkten oder hingerotzt, aber hier soll man sich auch nicht an Details aufhalten, sondern dem Arbeits- und Denkfluss folgen. Gerne auch Denglisch. Lieber erst Text schreiben und dann korrigieren. Wenn das Englische Wort nicht einfällt, das deutsche Hinschreiben. Den Schreibprozess nicht durch im Internet suchen und dann mal was Anderes gucken unterbrechen. Gerade wenn die Arbeit selbst noch im Entstehen ist, schreibt man erst einmal auf, was man tut, was man gemacht, gelernt, oder gelesen hat. Diese Frage stellt sich meist nach den ersten paar Sätzen in einer wissenschaftlichen Arbeit. Worum geht es hier eigentlich? Es könnte alles so einfach sein. Das ist normal. Durch das Aufschreiben werden einem meist die Dinge klarer und uns wird bewusst, wo wir nochmal genauer einhaken müssen."
  },
  {
    "objectID": "app-how-to-write.html#reflektieren",
    "href": "app-how-to-write.html#reflektieren",
    "title": "Appendix C — Writing principles",
    "section": "\nC.7 Reflektieren",
    "text": "C.7 Reflektieren\nDie Grundideen sind jetzt schon mal auf dem Papier, jetzt muss man sich überlegen, wie daraus ein Text wird. Gut ist es, sich schon an dieser Stelle Feedback von Betreuern oder Kommilitonen zu holen. Das hilft auch, die eigene Perspektive auf den Text zu ändern und ihn aus mehreren Richtungen zu betrachten – was ist wichtig, was soll viel Platz einnehmen, was fehlt vielleicht noch?\n\n\n\n\n\n\nHinweis\n\n\n\nWas könnte die zentrale Abbildung in den Ergebnissen sein?\n\n\nEin guter Ansatz um einen Fokus zu haben!"
  },
  {
    "objectID": "app-how-to-write.html#jetzt-schreiben-wir-wie-geht-es-am-besten",
    "href": "app-how-to-write.html#jetzt-schreiben-wir-wie-geht-es-am-besten",
    "title": "Appendix C — Writing principles",
    "section": "\nC.8 Jetzt schreiben wir! Wie geht es am besten?",
    "text": "C.8 Jetzt schreiben wir! Wie geht es am besten?\nDafür geht gut die Hälfte der Schreibzeit drauf. Am wichtigsten sind Inhalt und Struktur, Korrektheit und Verständlichkeit dürfen aber auch nicht unterschätzt werden: Wir nutzen einfache englische Sprache. Das geschriebene Wort muss nicht schlau klingen, sondern der Inhalt muss schlau sein. Keine umständlichen, gekünstelten Verben verwenden, wenn es ein einfaches Verb auch tut. Wir machen es dem Leser einfach. Später wirst du in einem wissenschaftlichen Paper „we” schreiben, da man selten ein Paper alleine schreibt. Um das jetzt hier gleich am Anfang zu üben, schreibst du keine verschachtelten Passivkonstruktionen, sondern „I do/did something”. Das fühlt sich erst seltsam an, aber wir als Leser danken dir!\n\n\n\n\n\n\nSchlecht\n\n\n\nAfter the raw methylation data has been preprocessed, a student t test was used for the differential analysis.\n\n\n\n\n\n\n\n\nGut\n\n\n\nI used the student t test for the differential analysis after the preprocessing of the raw methylation data.\n\n\nWir nutzen auch keine Pronomen, wo man nicht weiß, was diese Pronomen aussagen sollen. Was ist „it” oder „they”?\n\n\n\n\n\n\nSchlecht\n\n\n\nThe dog and the cat walk into a house. It eats all the cookies.\n\n\n\n\n\n\n\n\nGut\n\n\n\nThe dog and the cat walk into a house. The cat eats all the cookies.\n\n\nWir führen alle Begriffe vorher ein, daher erklären wir diese Begriffe, bevor wir die Begriffe verwenden. Ja, mache Begriffe sind klar, aber welche das sind, ergibt sich manchmal erst auf Nachfrage bei uns und ist für einen Neuling in einem Fachbereich gar nicht zu wissen.\n\n\n\n\n\n\nSchlecht\n\n\n\nI analyzed the NGS data with an ANOVA after checking the residuals with a QQ-plot.\n\n\n\n\n\n\n\n\nGut\n\n\n\nI analyzed the next generation sequencing data (NGS) with an analysis of variance (ANOVA) after plotting the residuals of the model in a quantile-quantile plot (QQ-plot).\n\n\nIst dir der Begriff nicht klar, erkläre ihn. Später können wir immer noch kürzen. Erkenntnisgewinn durch schreiben ist das Ziel."
  },
  {
    "objectID": "app-how-to-write.html#wer-macht-was-die-frage-des-lesers-an-jeden-einzelnen-satz",
    "href": "app-how-to-write.html#wer-macht-was-die-frage-des-lesers-an-jeden-einzelnen-satz",
    "title": "Appendix C — Writing principles",
    "section": "\nC.9 Wer macht was? Die Frage des Lesers an jeden einzelnen Satz!",
    "text": "C.9 Wer macht was? Die Frage des Lesers an jeden einzelnen Satz!\nKomme in den ersten sieben Wörtern zum Punkt, wer was macht. Subjekt und Prädikat sollen nah beieinander sein und möglichst früh im Satz kommen. Wir schreiben kurze Sätze und vermeiden komplizierte Schachtelsätze.\n\n\n\n\n\n\nSchlecht\n\n\n\nThe differential analysis of whole genome genetic data - like methylation pattern or expression analysis - has a long history of different invented methods and I used different analysis methods to find the best method for the analysis of methylation data with repeated measurements.\n\n\n\n\n\n\n\n\nGut\n\n\n\n[1] A long history of different analysis methods like methylation pattern or expression analysis exists. [2] Hence, many scientist have invented different analysis methods of whole genome."
  },
  {
    "objectID": "app-how-to-write.html#löschen-von-text",
    "href": "app-how-to-write.html#löschen-von-text",
    "title": "Appendix C — Writing principles",
    "section": "\nC.10 Löschen von Text",
    "text": "C.10 Löschen von Text\nText löschen macht keine Freude. Es ist immer nervig Teile des Textes, den man so mühsam in die Maschine getippt hat, zu löschen. Lege dir eine neue Datei an, in der du alles was du löschen willst reinkopierst. Bei mir heißt die Datei dump.docx oder dump.R oder auch anders. Auf jeden Fall löschst du so keinen Text, sondern bewahrst ihn erstmal auf. Denk immer daran, es geht nicht darum nur viel Text zu produzieren!"
  },
  {
    "objectID": "app-how-to-write.html#zum-schluss-kommen",
    "href": "app-how-to-write.html#zum-schluss-kommen",
    "title": "Appendix C — Writing principles",
    "section": "\nC.11 Zum Schluss kommen",
    "text": "C.11 Zum Schluss kommen\nWie Anne und Jochen1 jetzt in diesem Augenblick musst du zum Schluss kommen. Kein Text ist perfekt, kein Gedankengang so klar niedergeschrieben, wie er sein könnte. Aber irgendwann muss gut sein. Lass das Perfekte nicht der Feind des Guten sein. Dieser Leitfaden ist good enough und somit muss es reichen. Viel Erfolg!1 Die beide zusammen die erste Version von diesem Text mal 2019 geschrieben haben. Tja, wie so die Zeit vergeht.\n\n\n\n\n\n\nErfahrungsbericht von ehemaligen Bachelorstudierenden\n\n\n\n\nkeine neuen (wichtigen) Begriffe verwenden, wenn sie nicht vorher eingeführt wurden\nwenn man einen Begriff nicht richtig versteht oder nicht richtig erklären kann, sollte man ihn nicht verwenden. Daher ist es sehr hilfreich sich gründlich in den Hintergrund des Themas einzulesen.\nman sollte mehr wissen über das Thema, als man eigentlich im Text erklärt (um auf Fragen vorbereitet zu sein)\nAchtgeben auf die Zeitformen\n“it”, “they” und weitere Pronomen vermeiden, d.h. immer klar machen worauf man sich bezieht\nwenn etwas komplizierter scheint, sollte man Beispiele benutzen oder es eventuell graphisch darstellen. Aber: ein Bild nicht verwenden, wenn es keinen inhaltlichen Wert hat! Bringt dieses Bild etwas zum Verständnis des Lesers bei?\nin der Diskussion kein neues “Fass aufmachen”\ndie Limitationen in der Diskussion im Fließtext “verstecken”\ndie eigene Arbeit nicht schlecht reden; “schlechte” Resultate sind auch Resultate\nkeine zu langen Sätze\nes sollte ein roter Faden in jedem Kapitel zu erkennen sein: Was ist das Ziel? Wie erreiche ich das? Was bringt mir diese Methode? Was sagen mir die Resultate (in Bezug auf mein Ziel)?\nund für die eigene Motivation: Es werden oft Schreibblockaden kommen, es wird oft frustrierend sein, weil vielleicht etwas umgeschmissen wird und man von vorne anfangen muss, etc., aber davon soll man sich nicht entmutigen lassen\nDer Text muss nicht gleich beim ersten Hinschreiben perfekt sein, sondern es hilft erstmal etwas drauflos zu schreiben um die Gedanken besser ordnen zu können\nähnlich beim Programmieren: hier hat es mir auch geholfen einfach erstmal anzufangen und Ideen aufzuschreiben, aus welchen sich kann das Programm aufbauen kann\nMan sollte nicht zögern so etwas wie deepl (deepl.com) zu benutzen, wenn man mal mit einer englischen Formulierung nicht weiterkommt\nMan kann vor Beginn eines Tages ein realistisches Tagesziel (oder alternativ ein Wochenziel) festlegen, dann hat man ein Zwischenziel vor Augen und ist zufrieden mit sich, wenn dieses erreicht ist"
  },
  {
    "objectID": "app-lectures-semester.html",
    "href": "app-lectures-semester.html",
    "title": "Appendix D — Logbuch",
    "section": "",
    "text": "Version vom Oktober 12, 2022 um 14:16:30\nIm Folgenden findest du die Ablaufpläne der jeweiligen Vorlesungen solltest du später einmal Fragen zu den Inhalten haben, da du dich auf eine Klausur vorbereiten möchtest.\nDie Ablaufpläne sind eventuell etwas schwer zu verstehen, deshalb einfach eine Mail an mich schreiben, sollte noch etwas unklar sein. Häufig kann es auch helfen die Suchfunktion zu nutzen um Begriffe im Skript hier wiederzufinden."
  },
  {
    "objectID": "app-lectures-semester.html#spezielle-statistik-und-versuchswesen",
    "href": "app-lectures-semester.html#spezielle-statistik-und-versuchswesen",
    "title": "Appendix D — Logbuch",
    "section": "\nD.1 Spezielle Statistik und Versuchswesen",
    "text": "D.1 Spezielle Statistik und Versuchswesen\n\n\nMehr Informationen zum Modul Spezielle Statistik und Versuchswesen\nTermin am Montag, den 26.09.2022\n\nFragen an die Leitung\nGummibärchendatensatz erstellen\n\nWie befüllt Haribo anhand der erhobenen Daten vermutlich die Tüten?\nZeichnen Sie die Verteilung der Körpergröße!\n\n\nWer hat Hunde? Wer hat Katzen? Fragen zu den Flöhen auf den Hunde und Katzen.\nWie lange pinkelt ein Hund? Wie lange pinkelt eine Katze?\n\n\nTermin am Dienstag, den 27.09.2022\n\nStatistische Testtheorie am Beispiel von “Bin ich im Urlaub?”\n\nAussagen über Wahrscheinlichkeiten\nFalsifikationsprinzip\nAussagen über Populationen\n\n\nUnterschied zwischen kausalen und prädiktiven Modellen\nEinführung in R und Daten in R\n\n\nTermin am Montag, den 03.10.2022\n\nTag der deutschen Einheit\n\n\nTermin am Dienstag, den 04.10.2022\n\nEinführung in R und Daten\n\nWas sind Daten?\nVon dem Feld über den Block zu Excel\nEinlesen von Excel in R\n\n\n\n\nTermin am Montag, den 10.10.2022\n\nWie sieht eine Abschlussarbeit aus?\n\nStruktur einer Abschlussarbeit\nWie finden wir die passende Literatur?\nWas an statistischen Kennzahlen muss wo mit rein?\n\n\nWiederholung von deskriptiven Maßzahlen\nWas ist ggplot() und wie funktioniert die Visualisierung?\n\n\nTermin am Dienstag, den 04.10.2022\n\nÜbung für die Klausur\n\n\n\n\n\n\n\n\n\nPlatzhalter Archiv\n\n\n\n\n\nHier kommt dann das Archiv rein…"
  },
  {
    "objectID": "app-lectures-semester.html#mathematik-und-statistik",
    "href": "app-lectures-semester.html#mathematik-und-statistik",
    "title": "Appendix D — Logbuch",
    "section": "\nD.2 Mathematik und Statistik",
    "text": "D.2 Mathematik und Statistik\n\n\nMehr Informationen zum Modul Mathematik und Statistik\nTermin am Mittwoch, den 28.09.2022\n\nFragen an die Leitung\nGummibärchendatensatz erstellen\n\nWie befüllt Haribo anhand der erhobenen Daten vermutlich die Tüten?\nZeichnen Sie die Verteilung der Körpergröße!\n\n\nWer hat Hunde? Wer hat Katzen? Fragen zu den Flöhen auf den Hunde und Katzen.\nEinführung in R.\n\nWas sind Daten und ein Datensatz?\nVektor und Funktionen\n\n\nMittelwert und Standardabweichung an den Hundeflöhen erklärt.\nVisualisierung des Mittelwerts und der Varianz.\n\n\nTermin am Mittwoch, den 05.10.2022\n\nEinführung in die deskritpive Statistik\n\nWiederholung Mittelwert und Standardabweichung\nMedian und Quartile\nSpannweite, Standardfehler und weitere Maßzahlen\n\n\nEinführung in Vektoren in R\n\nBerechnung des Mittelwertes und der Standardabweichung\nErweiterung des Beispiels um die Katzenflohsprungweite\n\n\n\n\nTermin am Mittwoch, den 05.10.2022\n\nEinführung in die Testentscheidung\n\nWas ist das Falsifikationsprinzip?\nWas sind Wahrscheinlichkeiten?\nWas ist ein Modell?\n\n\nMittelwertsvergleich von Hunde und Katzenflöhen\n\nBerechnung der Teststatistik\nErörterung des \\(p\\)-Wertes\n\n\n\n\n\n\n\n\n\n\nPlatzhalter Archiv\n\n\n\n\n\nHier kommt dann das Archiv rein…"
  },
  {
    "objectID": "app-lectures-semester.html#statistik",
    "href": "app-lectures-semester.html#statistik",
    "title": "Appendix D — Logbuch",
    "section": "\nD.3 Statistik",
    "text": "D.3 Statistik\n\n\nMehr Informationen zum Modul Statistik\nTermin am Donnerstag, den 29.09.2022\n\nFragen an die Leitung\nGummibärchendatensatz erstellen\n\nWie befüllt Haribo anhand der erhobenen Daten vermutlich die Tüten?\nZeichnen Sie die Verteilung der Körpergröße!\n\n\nWer hat Hunde? Wer hat Katzen? Fragen zu den Flöhen auf den Hunde und Katzen.\nEinführung in R.\n\nWas sind Daten und ein Datensatz?\nVektor und Funktionen\n\n\nMittelwert und Standardabweichung an den Hundeflöhen erklärt.\nVisualisierung des Mittelwerts und der Varianz.\nMedian und Quartile erklärt\n\n\nTermin am Donnerstag, den 06.10.2022\n\nRückfragen an die Leitung\nBarplot erklärt\nBoxplot erklärt\nHistogramm für Zähldaten und kontenuierliche Daten erklärt\nDotplot erklärt\nGrundlagen in R\n\nPipen\nDaten einlesen\n\n\nHundebeispiel erweitert auf die Katzen\n\nDaten in R vorgestellt\n\n\nWas ist ggplot?\n\n\nTermin am Donnerstag, den 13.10.2022\n\nGrundlagen in R\n\nPipen\nDaten einlesen\n\n\nHundebeispiel erweitert auf die Katzen\n\nDaten in R vorgestellt\n\n\nWas ist ggplot?\n\n\n\n\n\n\n\n\nPlatzhalter Archiv\n\n\n\n\n\nHier kommt dann das Archiv rein…"
  },
  {
    "objectID": "app-lectures-semester.html#angewandte-statistik-für-bioverfahrenstechnik",
    "href": "app-lectures-semester.html#angewandte-statistik-für-bioverfahrenstechnik",
    "title": "Appendix D — Logbuch",
    "section": "\nD.4 Angewandte Statistik für Bioverfahrenstechnik",
    "text": "D.4 Angewandte Statistik für Bioverfahrenstechnik\n\n\nMehr Informationen zum Modul Angewandte Statistik für Bioverfahrenstechnik\nTermin am Montag, den xx.03.2023\n\nTermin am Dienstag, den xx.03.2023\n\n\n\n\n\n\n\nPlatzhalter Archiv\n\n\n\n\n\nHier kommt dann das Archiv rein…"
  },
  {
    "objectID": "app-lectures-semester.html#biostatistik",
    "href": "app-lectures-semester.html#biostatistik",
    "title": "Appendix D — Logbuch",
    "section": "\nD.5 Biostatistik",
    "text": "D.5 Biostatistik\n\n\nMehr Informationen zum Modul Biostatistik\nTermin am Dienstag, den xx.03.2023\n\nTermin am Donnerstag, den xx.03.2023\n\n\n\n\n\n\n\nPlatzhalter Archiv\n\n\n\n\n\nHier kommt dann das Archiv rein…"
  },
  {
    "objectID": "app-lectures-semester.html#angewandte-statistik-und-versuchswesen",
    "href": "app-lectures-semester.html#angewandte-statistik-und-versuchswesen",
    "title": "Appendix D — Logbuch",
    "section": "\nD.6 Angewandte Statistik und Versuchswesen",
    "text": "D.6 Angewandte Statistik und Versuchswesen\n\n\nMehr Informationen zum Modul Angewandte Statistik und Versuchswesen\nTermin am Mittwoch, den xx.03.2023\n\n\n\n\n\n\n\nPlatzhalter Archiv\n\n\n\n\n\nHier kommt dann das Archiv rein…"
  }
]