```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra)
```

# Maßzahlen der Modelgüte {#sec-lin-reg-quality}

*Letzte Änderung am `r format(fs::file_info("stat-linear-reg-quality.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

> *"Uncontrolled variation is the enemy of quality." --- W. Edwards Deming*

Wir interpretieren keine der Gütekriterien und statistischen Maßzahlen alleine sondern in der Gesamtheit. Wir meinen mit Gütekriterien wie gut das statistische Modellieren funktioniert hat. Wir haben eine große Auswahl an Methoden und wir müssen das Ergebnis des Modellierens überprüfen. Es gibt daher eine Reihe von Maßzahlen für die Güte eines Modells, wir schauen uns hier einige an. Später werden wir uns noch andere Maßzahlen anschauen, wenn wir eine multiple lineare Regression rechnen. Das [R Paket performance](https://easystats.github.io/performance/) werden wir später auch nutzen um die notwendigen Gütekriterien zu erhalten. Wir wollen eine Gerade durch Punkte legen. Deshalb müssen wir folgende Fragen klären um zu wissen, ob das Ziehen der Gerade auch gut funktioniert hat:

-   Läuft die Gerade durch die Mitte der Punkte? Hier hilft ein Residualplot für die Bewertung (siehe @sec-linreg-residual).
-   Liegen die Punkte alle auf der Geraden? Hier hilft das Bestimmtheitsmaß $R^2$ weiter (siehe @sec-linreg-bestimmt)
-   Folgt unser Outcome $y$ einer Normalverteilung? Hier kann der QQ-Plot helfen (siehe @sec-linreg-qq)

Wir gehen jetzt mal alle Punkte durch und schauen, ob wir eine gute Gerade angepasst (eng. *fit*) haben. Wenn wir keine gute Gerade angepasst haben, dann müssen wir überlegen, ob wir nicht unser Modell der Regression ändern. Es ist also vollkommen okay, eine Regression zu rechnen und dann festzustellen, dass die Regression so nicht geklappt hat. Dann rechnen wir eine andere Regression. Wie du in den folgenden Kapiteln feststellen wirst, gibt es nämlich eine Menge Typen von Regression. Auch ist es eine Überlegung wert einmal das Outcome $y$ zu transformieren. Mehr dazu kannst du dann im [Kapitel zur Transformation](#sec-eda-transform) von Variablen nachlesen.

{{< video https://youtu.be/dLlgWQI4M8w?si=IacLwPIwurRGbLHU >}}

::: callout-tip
## Weitere Tutorien für das Finden von Ausreißern

Wir immer geht natürlich mehr als ich hier Vorstellen kann. Du findest im Folgenden Tutorien, die mich hier in dem Kapitel inspiriert haben.

-   Das [R Paket `{olsrr}` und die Webseite](https://olsrr.rsquaredacademy.com/) liefert nochmal sehr viel mehr Möglichkeiten sich die Qualität einer Gaussian linearen Regression anzuschauen.
-   Den QQ-Plot selber per Hand bauen und zu SPSS vergleichen kannst du dann auch nochmal in [QQ-plots in R vs SPSS](https://rpubs.com/markheckmann/45771).
:::

## Genutzte R Pakete

Wir wollen folgende R Pakete in diesem Kapitel nutzen.

```{r echo = TRUE}
#| message: false
pacman::p_load(tidyverse, magrittr, conflicted, broom,
               see, performance)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("mutate", "dplyr")
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

Am Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren.

## Daten

Nachdem wir uns im vorherigen Kapitel mit einem sehr kleinen Datensatz beschäftigt haben, nehmen wir einen großen Datensatz. Bleiben aber bei einem simplen Modell. Wir brauchen dafür den Datensatz `flea_dog_cat_length_weight.xlsx`. In einer simplen linearen Regression schauen wir uns den Zusammenhang zwischen einem $y$ und einem $x_1$ an. Daher wählen wir aus dem Datensatz die beiden Spalten `jump_length` und `weight`. Wir wollen nun feststellen, ob es einen Zusammenhang zwischen der Sprungweite in \[cm\] und dem Flohgewicht in \[mg\] gibt. In dem Datensatz finden wir 400 Flöhe von Hunden und Katzen.

```{r}
#| message: false

model_tbl <- read_csv2("data/flea_dog_cat_length_weight.csv") |>
  select(animal, jump_length, weight)
```

In der @tbl-model-1 ist der Datensatz `model_tbl` nochmal dargestellt.

```{r}
#| message: false
#| echo: false
#| tbl-cap: Selektierter Datensatz mit einer normalverteilten Variable `jump_length` und der normalverteilten Variable `weight`. Wir betrachten die ersten sieben Zeilen des Datensatzes.
#| label: tbl-model-1

model_tbl |> head(7) |> kable(align = "c", "pipe")
```

Im Folgenden *ignorieren* wir, dass die Sprungweiten und die Gewichte der Flöhe auch noch von den Hunden oder Katzen sowie dem unterschiedlichen Geschlecht der Flöhe abhängen könnten. Wir schmeißen alles in einen Pott und schauen nur auf den Zusammenhang von Sprungweite und Gewicht.

## Das simple lineare Modell

Wir fitten ein simples lineares Modell mit nur einem Einflussfaktor `weight` auf die Sprunglänge `jump_length`. Wir erhalten dann das Objekt `fit_1` was wir dann im Weiteren nutzen werden.

```{r}
fit_1 <- lm(jump_length ~ weight, data = model_tbl)
```

Wir nutzen jetzt dieses simple lineare Modell für die weiteren Gütekritierien.

## Residualplot {#sec-linreg-residual}

[In R wird in Modellausgaben die Standardabweichung der Residuen $s_{\epsilon}$ als `sigma` bezeichnet.]{.aside}

Wir wollen mit dem Residualplot die Frage beantworten, ob die Gerade *mittig* durch die Punktewolke läuft. Die Residuen $\epsilon$ sollen normalverteilt sein mit einem Mittelwert von Null $\epsilon \sim \mathcal{N}(0, s^2_{\epsilon})$.

Wir erhalten die Residuen `resid` und die angepassten Werte `.fitted` auf der Geraden über die Funktion `augment()`. Die Funktion `augment()` gibt noch mehr Informationen wieder, aber wir wollen uns jetzt erstmal auf die Residuen konzentrieren.

```{r}
resid_plot_tbl <- fit_1 |> 
  augment() |> 
  select(.fitted, .resid)

resid_plot_tbl |> 
  head(5)
```

Die Daten selber interessieren uns nicht einer Tabelle. Stattdessen zeichnen wir einmal den Residualplot. Bei dem Residualplot tragen wir die Werte der Residuen `.resid` auf die y-Achse auf und die angepassten y-Werte auf der Geraden `.fitted` auf die x-Achse. Wir kippen im Prinzip die gefittete Gerade so, dass die Gerade parallel zu x-Achse läuft.

```{r}
#| echo: true
#| message: false
#| label: fig-scatter-qual-01
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "Residualplot der Residuen des Models `fit_1`. Die rote Linie stellt die geschätzte Gerade da. Die Punkte sollen gleichmäßig und ohne eine Struktur um die Gerade verteilt sein."

ggplot(resid_plot_tbl, aes(.fitted, .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "#CC79A7") +
  theme_minimal()
```

[The residual plot should look like the sky at night, with no pattern of any sort.]{.aside}

In @fig-scatter-qual-01 sehen wir den Residualplot von unseren Beispieldaten. Wir sehen, dass wir keine Struktur in der Punktewolke erkennen. Auch sind die Punkte gleichmäßig um die Gerade verteilt. Wir haben zwar einen Punkt, der sehr weit von der Gerade weg ist, das können wir aber ignorieren. Später können wir uns noch überlegen, ob wir einen Ausreißer (eng. *outlier*) vorliegen haben.

Kommen wir nochmal auf die Funktion `augment()` zurück und schauen uns einmal an, was die ganzen Spalten hier zu bedeuten haben. Dafür nutzen wir nochmal einen simpleren Datensatz in der die vierte Beobachtung mit $(4.1, 5.2)$ sehr extreme Werte im Vergleich zu den anderen drei Beobachtungen annimmt. Danach fitten wir dann wieder unser lineares Modell.

```{r}
simple_tbl <- tibble(jump_length = c(1.2, 1.8, 1.3, 5.2),
                     weight = c(0.8, 1, 1.2, 4.1))

fit_2 <- lm(jump_length ~ weight, data = simple_tbl)
```

Im Folgenden sehen wir dann die Ausgabe der Funktion `augment()`. Dabei sind die ersten beiden Spalten noch selbsterklärend. Wir haben hier mit `jump_length` und `weight` die Werte für das Outcome $y$ und die Einflussvariablen $x$ dargestellt.

```{r}
#| message: false
#| warning: false

fit_2 |> 
  augment() |> 
  mutate(across(where(is.numeric), round, 2))
```

Die anderen Spalten sind dann wie folgt zu lesen.

-   `.fitted` sind die vorhergesagten Werte auf der Geraden. Wir bezeichnen diese Werte auch die $\hat{y}$ Werte.
-   `.resid` sind die Residuen oder auch $\epsilon$. Daher der Abstand zwischen den beobachteten $y$-Werten und den $\hat{y}$ Werten auf der Geraden.
-   `.hat` gibt den Einfluss jeder einzelnen Beobachtung auf die endgültige Gerade wieder. Also den Hebel (eng. *leverage*) jeder einzelnen Beobachtung und damit wie stark eine Beobachtung an der Geraden zieht.
-   `.sigma` beschreibt die geschätzte $s^2_{\epsilon}$ , wenn die entsprechende Beobachtung aus dem Modell herausgenommen wird.
-   `.cooksd` definiert, ob eine Beobachtung ein tendenzieller Outlier im Bezug zu den anderen Beobachtungen ist. Im Prinzip eine Standardisierung der `hat` Spalte.
-   `std.resid` sind die standardisierten Residuen. Dabei werden die Residuen durch die Standardabweichung der Residuen $s_{\epsilon}$ geteilt. Die standardisierten Residuen folgen dann einer Standardnormalverteilung.

Wir können dann der Ausgabe von `augment()` entnehmen, dass unsere vierte Beobachtung vermutlich ein Outlier ist.

## Bestimmtheitsmaß $R^2$ {#sec-linreg-bestimmt}

Nachdem wir nun wissen wie gut die Gerade durch die Punkte läuft, wollen wir noch bestimmen wie genau die Punkte auf der Geraden liegen. Das heißt wir wollen mit dem Bestimmtheitsmaß $R^2$ ausdrücken wie stark die Punkte um die Gerade variieren. Wir können folgende Aussage über das Bestimmtheitsmaß $R^2$ treffen. Die @fig-rsquare visualisiert nochmal den Zusammenhang.

-   wenn alle Punkte auf der Geraden liegen, dann ist das Bestimmtheitsmaß $R^2$ gleich 1.
-   wenn alle Punkte sehr stark um die Gerade streuen, dann läuft das Bestimmtheitsmaß $R^2$ gegen 0.

![Visualisierung des Bestimmtheitsmaßes $R^2$. Auf der linken Seite sehen wir eine perfekte Übereinstimmung der Punkte und der geschätzten Gerade. Wir haben ein $R^2$ von 1 vorliegen. Sind die Punkte und die geschätzte Gerade nicht deckungsgleich, so läuft das $R^2$ gegen 0.](images/statistical_modeling_rsquare){#fig-rsquare fig-align="center" width="100%"}

Da die Streuung um die Gerade auch gleichzeitig die Varianz widerspiegelt, können wir auch sagen, dass wenn alle Punkte auf der Geraden liegen, die Varianz gleich Null ist. Die Einflussvariable $x_1$ erklärt die gesamte Varianz, die durch die Beobachtungen verursacht wurde. Damit beschreibt das Bestimmtheitsmaß $R^2$ auch den Anteil der Varianz, der durch die lineare Regression, daher der Graden, erklärt wird. Wenn wir ein Bestimmtheitsmaß $R^2$ von Eins haben, wird die gesamte Varianz von unserem Modell erklärt. Haben wir ein Bestimmtheitsmaß $R^2$ von Null, wird gar keine Varianz von unserem Modell erklärt. Damit ist ein niedriges Bestimmtheitsmaß $R^2$ schlecht.

Im Folgenden können wir uns noch einmal die Formel des Bestimmtheitsmaß $R^2$ anschauen um etwas besser zu verstehen, wie die Zusammenhänge mathematisch sind.

$$
\mathit{R}^2 = 
\cfrac{\sum_{i=1}^N \left(\hat{y}_i- \bar{y}\right)^2}{\sum_{i=1}^N \left(y_i - \bar{y}\right)^2}
$$

In der @fig-bestimmtheit-01 sehen wir den Zusammenhang nochmal visualisiert. Wenn die Abstände von dem Mittelwert zu den einzelnen Punkten mit $y_i - \bar{y}$ gleich dem Abstand der Mittelwerte zu den Punkten *auf* der Geraden mit $\hat{y}_i- \bar{y}$ ist, dann haben wir einen perfekten Zusammenhang.

![Auf der linken Seite sehen wir eine Gerade die nicht perfekt durch die Punkte läuft. Wir nehmen ein Bestimmtheitsmaß $R^2$ von ca. 0.7 an. Die Abstände der einzelnen Beobachtungen $y_i$ zu dem Mittelwert der y-Werte $\bar{y}$ ist nicht gleich den Werten auf der Geraden $\hat{y}_i$ zu dem Mittelwert der y-Werte $\bar{y}$. Dieser Zusammenhang wird in der rechten Abbildung mit einem Bestimmtheitsmaß $R^2$ von 1 nochmal deutlich.](images/statistical_modeling_bestimmtheit.png){#fig-bestimmtheit-01 fig-align="center"}

Wir können die Funktion `glance()` nutzen um uns das `r.squared` und das `adj.r.squared` wiedergeben zu lassen.

```{r}
#| message: false

fit_1 |> 
  glance() |> 
  select(r.squared, adj.r.squared)
```

[Wir nutzen grundsätzlich das adjustierte $R^2$`adj.r.squared` in der Anwendung.]{.aside}

Wir haben wir ein $R^2$ von $0.31$ vorliegen. Damit erklärt unser Modell bzw. die Gerade 31% der Varianz. Das ist jetzt nicht viel, aber wundert uns auch erstmal nicht. Wir haben ja die Faktoren `animal` und `sex` ignoriert. Beide Faktoren könnten ja auch einen Teil der Varianz erklären. Dafür müssten wir aber eine multiple lineare Regression mit mehren $x$ rechnen.

Wenn wir eine multiple Regression rechnen, dann nutzen wir das adjustierte $R^2$ in der Anwendung. Das hat den Grund, dass das $R^2$ automatisch ansteigt je mehr Variablen wir in das Modell nehmen. Jede neue Variable wird immer *etwas* erklären. Um dieses Überanpassen (eng. *overfitting*) zu vermeiden nutzen wir das adjustierte $R^2$. Im Falle des adjustierte $R^2$ wird ein Strafterm eingeführt, der das adjustierte $R^2$ kleiner macht je mehr Einflussvariablen in das Modell aufgenommen werdenn.

## QQ-Plot {#sec-linreg-qq}

Mit dem Quantile-Quantile Plot oder kurz QQ-Plot können wir überprüfen, ob unser $y$ aus einer Normalverteilung stammt. Oder andersherum, ob unser $y$ approximativ normalverteilt ist. Der QQ-Plot ist ein visuelles Tool. Daher musst du immer schauen, ob dir das Ergebnis passt oder die Abweichungen zu groß sind. Es hilft dann manchmal die Daten zum Beispiel einmal zu $log$-Transformieren und dann die beiden QQ-Plots miteinander zu vergleichen.

Wir brauchen für einen QQ-Plot viele Beobachtungen. Das heißt, wir brauchen auf jeden Fall mehr als 20 Beobachtungen. Dann ist es auch häufig schwierig den QQ-Plot zu bewerten, wenn es viele Behandlungsgruppen oder Blöcke gibt. Am Ende haben wir dann zwar mehr als 20 Beobachtungen aber pro Kombination Behandlung und Block nur vier Wiederholungen. Und vier Wiederholungen sind zu wenig für eine sinnvolle Interpretation eines QQ-Plots.

::: {layout="[15,85]" layout-valign="top"}
![](images/angel_01.png){fig-align="center" width="100%"}

> Das klingt hier alles etwas wage... Ja, das stimmt. Aber wir wenden hier den QQ-Plot erstmal an und schauen uns dann im Anschluss nochmal genauer an, wie der QQ-Plot entsteht.
:::

Grob gesprochen vergleicht der QQ Plot die Quantile der vorliegenden Beobachtungen, in unserem Fall der Variablen `jump_length`, mir den Quantilen einer theoretischen Normalverteilung, die sich aus den Daten mit dem Mittelwert und der Standardabweichung von `jump_length` ergeben würden.

Wir können die Annahme der Normalverteilung recht einfach in `ggplot` überprüfen. Wir sehen in @fig-scatter-qual-02 den QQ-Plot für die Variable `jump_length`. Die Punkte sollten alle auf einer Diagonalen liegen. Hier dargestellt durch die rote Linie. Häufig weichen die Punkte am Anfang und Ende der Spannweite der Beobachtungen etwas ab.

```{r}
#| echo: true
#| message: false
#| label: fig-scatter-qual-02
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "QQ-Plot der Sprungweite in \\[cm\\]. Die Gerade geht einmal durch die Mitte der Punkte und die Punkte liegen nicht exakt auf der Geraden. Eine leichte Abweichung von der Normalverteilung könnte vorliegen."

ggplot(model_tbl, aes(sample = jump_length)) + 
  stat_qq() + 
  stat_qq_line(color = "#CC79A7") +
  labs(x = "Theoretischen Quantile der Standardnormalverteilung",
       y = "Quantile der beobachteten Stichprobe") + 
  theme_minimal()
```

Wir werden uns später auch noch häufig die Residuen aus den Modellen anschauen. Die Residuen müssen nach dem Fit des Modells einer Normalverteilung folgen. Wir können diese Annahme an die Residuen mit einem QQ-Plot überprüfen. In @fig-scatter-qual-resid sehen wir die Residuen aus dem Modell `fit_1` in einem QQ-Plot. Wir würden sagen, dass die Residuen approximativ normalvertelt sind. Die Punkte liegen fast alle auf der roten Diagonalen.

```{r}
#| echo: true
#| message: false
#| label: fig-scatter-qual-resid
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "QQ-Plot der Residuen aus dem Modell `fit_1`. Die Residuen müssen einer approximativen Normalverteilung folgen, sonst hat der Fit des Modelles nicht funktioniert."

ggplot(resid_plot_tbl, aes(sample = .resid)) + 
  stat_qq() + 
  stat_qq_line(color = "#CC79A7") +
  theme_minimal() 

```

::: callout-note
## Den QQ-Plot per Hand erstellen und verstehen

![](images/caution.png){fig-align="center" width="50%"}

Gut das war jetzt die Anwendung und wie bauen wir uns jetzt einen QQ-Plot per Hand selber?

```{r}
x <- c(7.19, 6.31, 5.89, 4.5, 3.77, 4.25, 5.19, 5.79, 6.79)
```

$p=(r − 0.5)/n$

```{r}
n <- length(x)          
r <- rank(x)    
p <- (r - 0.5) / n      
q <- qnorm(p)           
```

```{r}
#| echo: false
#| message: false
#| label: fig-qq-by-hand-3
#| fig-align: center
#| fig-height: 4
#| fig-width: 7
#| fig-cap: "QQ-Plot der Residuen aus dem Modell `fit_1`. Die Residuen müssen einer approximativen Normalverteilung folgen, sonst hat der Fit des Modelles nicht funktioniert."

q_tbl <- tibble(q)

tibble(x = seq(-3, 3, by = 0.1),
       y = dnorm(x)) |> 
  ggplot(aes(x, y)) +
  theme_minimal() +
  geom_line() +
  geom_segment(data = q_tbl, aes(x = q, xend = q, y = 0, yend = dnorm(q)),
               color = "#0072B2") +
  annotate("text", x = q, y = -0.01, label = round(q, 2), size = 3) 


```

```{r}
#| echo: true
#| message: false
#| label: fig-qq-by-hand-0
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "QQ-Plot der Residuen aus dem Modell `fit_1`. Die Residuen müssen einer approximativen Normalverteilung folgen, sonst hat der Fit des Modelles nicht funktioniert."

ggplot(tibble(x, q), aes(x, q)) +
  theme_minimal() +
  geom_point() # plot empirical against theoretical values 
```

```{r}
ps <- c(.25, .75)               # reference probabilities
a <- quantile(x, ps)            # empirical quantiles
b <- qnorm(ps)                  # theoretical quantiles
#lines(a, b, lwd=4, col="red")   # our QQ line
#qqline(x, datax=T)              # R QQ line
```

```{r}
#| echo: true
#| message: false
#| label: fig-qq-by-hand-1
#| fig-align: center
#| fig-height: 5
#| fig-width: 6
#| fig-cap: "QQ-Plot der Residuen aus dem Modell `fit_1`. Die Residuen müssen einer approximativen Normalverteilung folgen, sonst hat der Fit des Modelles nicht funktioniert."

ggplot(tibble(x, q), aes(x, q)) +
  theme_minimal() +
  geom_point() +
  geom_segment(aes(a[1], b[1], xend = a[2], yend = b[2]), color = "#CC79A7") +
  annotate("text", x = c(4.5, 6.31), y = c(-0.45, 0.45), color = "#CC79A7",
           label = c(expression(1^{st}), expression(3^{rd})))
```
:::

## Modellgüte mit dem R Paket `performance`

Abschließend möchte ich hier nochmal das [R Paket performance](https://easystats.github.io/performance/) vorstellen. Wir können mit dem Paket auch die Normalverteilungsannahme der Residuen überprüfen. Das geht ganz einfach mit der Funktion `check_normality()` in die wir einfach das Objekt mit dem Fit des Modells übergeben.

```{r}
check_normality(fit_1)
```

Wir haben auch die Möglichkeit uns einen Plot der Modellgüte anzeigen zu lassen. In @fig-scatter-qual-04 sehen wir die Übersicht von bis zu sechs Abbildungen, die uns Informationen zu der Modellgüte liefern. Wir müssen nur den Fit unseres Modells an die Funktion `check_model()` übergeben.

Das Schöne an der Funktion ist, dass jeder Subplot eine Beschreibung in Englisch hat, wie der Plot auszusehen hat, wenn alles gut mit dem Modellieren funktioniert hat.

Wir kommen dann in der multiplen linearen Regression nochmal auf das Paket `performance` zurück. Für dieses Kapitel reicht dieser kurze Abriss.

```{r}
#| echo: true
#| message: false
#| label: fig-scatter-qual-04
#| fig-align: center
#| fig-height: 12
#| fig-width: 8
#| fig-cap: "Übersicht der Plots zu der Modellgüte aus der Funktion `check_model()`."
#| column: page

check_model(fit_1, colors = cbbPalette[6:8])

```
