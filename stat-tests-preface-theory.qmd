# Testen von Hypothesen {#sec-statistisches-testen}

*Letzte Änderung am `r format(fs::file_info("stat-tests-preface-theory.qmd")$modification_time, '%d. %B %Y um %H:%M:%S')`*

```{r echo = FALSE}
pacman::p_load(tidyverse, readxl, knitr, kableExtra, broom)
```

> *"Das ist die Logik der Forschung, die nie verifizieren, sondern immer nur jene Erklärungen beibehalten kann, die beim derzeitigen Erkenntnisstand am wenigsten falsifiziert sind." --- Wößmann, L.*

::: callout-tip
## Grundlagen der Wissenschaft und Falsifikationsprinzip

Du findest auf YouTube [Grundlagen der Wissenschaft und Falsifikationsprinzip](https://youtu.be/h45ftLNsspM) als Video.
:::

Das statistische Hypothesentesten - eine Geschichte voller Missverständnisse. Wir wollen uns in den folgenden Kapiteln mit den Grundlagen des frequentistischen Hypothesentestens beschäftigen. Wenn ich hier einen Unterschied mache, dann muss es ja auch noch ein anderes Hypothesentesten geben. Ja, das nennt man dann bayesianische Statistik und kommt eventuell mal später. Wir konzentrieren uns aber zuerst auf frequentistische Hypothesentesten was seit gut hundert Jahren genutzt wird. Ich werde hier textlich nur einen kurzen Einstieg liefern. Vielleicht wird es in den folgenden Jahren länger aber aktuell (Ende 2022) bleiben wir hier bei einem kurzen Einstieg.

[Forschung basiert auf dem Falsifikationsprinzip. Wir können **nur ablehnen** und behalten das weniger schlechte Modell bei.]{.aside}

Beginnen wir mit der Logik der Forschung oder allgemeiner formuliert, als die Grundlage der Wissenschaft. Wir basieren all unsere Entscheidungen in der Wissenschaft auf dem Falsifikationsprinzip. Also bitte merken, wir können nur ablehnen (eng. *reject*). Wir ersetzen schlechte Modelle (der Wirklichkeit) durch weniger schlechte Modelle (der Wirklichkeit) -- das ist die Logik der Forschung.

Wir wollen hier auf keinen Fall die Leistungen von Altvorderen schmälern. Dennoch hatten [Ronald Fischer (1890 - 1962)](https://en.wikipedia.org/wiki/Ronald_Fisher), als der Begründer der Statistik, andere Vorraussetzungen als wir heutzutage. Als wichtigster Unterschied sei natürlich das Gerät genannt, an dem du gerade diese Zeilen liest: dem Computer. Selbst die Erstellung einfachster Abbildungen war sehr, sehr zeitaufwendig. Die Berechnung von Zahlen lohnte sich mehr, als die Zahlen zu visualisieren. Insbesondere wenn wir die Explorative Datenanalyse nach [John Tukey (1915 - 2000)](https://en.wikipedia.org/wiki/John_Tukey) durchführen. Undenkbar zu den Zeiten von Ronald Fischer mehrere Abbildungen unterschiedlich nach Faktoren einzufärben und sich die Daten *anzugucken*.

[Über die Nullhypothese erfährst du mehr in dem folgenden @sec-hypothesen]{.aside}

Neben dieser Begrenzung von moderner Rechenkapazität um 1900 gab es noch eine andere ungünstige Entwicklung. Stark vereinfacht formuliert entwickelte Ronald Fischer statistische Werkzeuge um abzuschätzen wir wahrscheinlich die Nullhypothese unter dem Auftreten der beobachteten Daten ist. Nun ist es aber so, dass wir ja auch eine Entscheidung treffen wollen. Nach der Logik der Forschung wollen wir ja eine Hypothese falsifizieren, in unserem Fall die Nullhypothese. Die Entscheidungsregeln, also die statistische Testtheorie, kommen nun von [Jerzy Neyman (1894 - 1981)](https://en.wikipedia.org/wiki/Jerzy_Neyman) und [Egon Pearson (1895 - 1980)](https://en.wikipedia.org/wiki/Egon_Pearson), beide als die Begründer der frequentistischen Hypothesentests.

Schlussendlich gibt es noch eine andere Strömung in der Statistik, die auf den mathematischen Formeln von [Thomas Bayes (1701 - 1761)](https://en.wikipedia.org/wiki/Thomas_Bayes) basieren. In sich eine geschlossene Theorie, die auf der *inversen* Wahrscheinlichkeit basiert. Das klingt jetzt etwas schräg, aber eigentlich ist die bayesianische Statistik die Statistik, die die Fragen um die Alternativehypothese beantwortet. Der Grund warum die bayesianische Statistik nicht angewendet wurde, war der Bedarf an Rechenleistung. Die bayesiansiche Statistik lässt sich nicht händisch in endlicher Zeit lösen. Dieses *technische* Problem haben wir aber nicht mehr. Eigentlich könnten wir also die bayesiansiche Statistik verwenden. Wir wollen hier aber (noch) nicht auf die bayesianische Statistik eingehen, das werden wir später tun. Wenn du allgemein Interesse hast an der Geschichte der Statistik dann sei auf @salsburg2001lady verwiesen. Ein sehr schönes Buch, was die *geschichtlichen* Zusammenhänge nochmal aufzeigt.

------------------------------------------------------------------------

Die *frequentistische Statistik* basiert - wie der Name andeutet - auf Wiederholungen in einem Versuch. Daher der Name frequentistisch. Also eine Frequenz von Beobachtungen. Ist ein wenig gewollt, aber daran gewöhnen wir uns schon mal. Konkret, ein Experiment welches wir frequentistisch Auswerten wollen besteht immer aus biologischen Wiederholungen. Wir müssen also ein Experiment planen in dem wir wiederholt ein Outcome an vielen Tieren, Pflanzen oder Menschen messen. Auf das Outcome gehen wir noch später ein. Im Weiteren konzentrieren wir uns hier auf die *parametrische* Statistik. Die parametrische Statistik beschäftigt sich mit Parametern von Verteilungen.

Wenn wir nun ein Experiment durchführen dann erheben wir einmalig Daten $D_1$. Wir könnten das Experiment wiederholen und erneut Daten $D_2$ erheben. Wir können das Experiment $j$-mal wiederholen und haben dann Daten von $D_1,..., D_j$. Dennoch werden wir nie *alle* Daten erheben können, die mit einem Experiment verbunden sind.

[**Strukturgleichkeit** erreichen wir durch **Randomisierung**.]{.aside}

Nehmen wir das Beispiel, dass wir die Sprungweite von Hunde- und Katzenflöhen vergleichen wollen. Wir können nicht *alle* Hunde- und Katzenflöhe messen. Wir können nur eine Stichprobe an Daten $D_1$ erheben. Über diese Daten $D_1$ können wir dann später durch statistische Algorithmen eine Aussage treffen. Wichtig ist hier sich zu merken, dass wir eine Grundgesamtheit haben aus der wir eine Stichprobe ziehen. Wir müssen darauf achten, dass die Stichprobe *repräsentativ* ist und damit *strukturgleich* zur Grundgesamtheit ist. Die Strukturgleichkeit erreichen wir durch Randomisierung. Wir veranschaulichen diesen Zusammenhang in @fig-grundgesamtheit-schema. Ein Rückschluß von der Stichprobe ist nur möglich, wenn die Stichprobe die Grundgesamtheit repräsentiert. Auch eine Randomisierung mag dieses Ziel nicht immer erreichen. Im Beispiel der Hundeflöhe könnte wir eine Art an Flöhen übersehen und diese Flohart nicht mit in die Stichprobe aufnehmen. Ein Rückschluß auf diese Flohart wäre dann mit unserem Experiment nicht möglich.

![Abbildung über die Grundgesamtheit und die Stichprobe(n) $D_1$ bis $D_j$. Durch Randomisierung wird Sturkturgleichheit erreicht, die dann einen Rückschluß von der Stichprobe auf die Grundgesamtheit erlaubt. Jede Stichprobe ist anders und nicht jede Randomisierung ist erfolgreich was die Strukturgleicheit betrifft.](images/test_theory/preface-grundgesamtheit.png){#fig-grundgesamtheit-schema fig-align="center" width="80%"}

------------------------------------------------------------------------

Die folgenden Kapitel ist sehr umfangreich und enthalten viele Informationen, die wir teilweise später nochmal brauchen. Darüber hinaus müssen wir noch das *Lernen und Verstehen* von der *Anwendung* unterscheiden. Wir teilen dabei die Test*entscheidung* und die Test*theorie* in zwei Kapitel auf.

Du erfährst im @sec-stat-entscheidung mehr zur Testentscheidung und welche Konzepte wir dort nutzen:

-   Wir verstehen die statistischen Testentscheidung nutzen wir das Konzept der Teststatistik $T$ (siehe @sec-teststatistik)
-   Wir können die statistischen Testentscheidung anwenden, da wir das Konzept des p-Wertes $Pr(T|H_0)$ (siehe @sec-pwert) und das Konzept der 95% Konfidenzintervalle verstanden haben (siehe @sec-ki)

Du erfährst im @sec-stat-theorie mehr zur Testtheorie und welche Konzepte wir dort nutzen:

-   Wir verstehen den Unterschied zwischen dem $\alpha$-Fehler und der $\beta$-Fehler (siehe @sec-alpha-beta)
-   Wir wissen um den Unterschied des einseitigen und zweiseitigen statistischen Testens (siehe @sec-einseitig-zweiseitig)
-   Wir verstehen die Adjustierung für multiple Vergleiche (siehe @sec-statistisches-testen-alpha-adjust)

::: callout-note
## Beispiel: Bin ich im Urlaub?

Hier nochmal zusammengefasst die Idee der frequentistischen Testtheorie anhand der Frage, ob ich im Urlaub bin. Du versteht die Idee vielleicht besser, wenn du dich einmal durch die folgenden Kapitel gearbeitet hast. Ja gearbeitet, nur lesen ist dann einfach zu wenig.

-   Bin ich im Urlaub?
-   Wie wahrscheinlich bin ich im Urlaub? $Pr(U)$
-   Wie wahrscheinlich bin ich nicht im Urlaub? $Pr(\bar{U})$
-   Wie wahrscheinlich sind wir nicht im Urlaub? $Pr(\bar{U})$
-   Wie wahrscheinlich ist es, die Daten $D_1$ zu beobachten, wenn wir nicht im Urlaub sind? $Pr(D_1|\bar{U})$

Wir würden meinen, dass wir die Frage "Bin ich im Urlaub?" beantworten können. Das stimmt aber nicht. Durch das Falsifikationsprinzip können wir nur eine Aussage über "nicht-im-Urlaub-sein" treffen. Darüber hinaus können wir keine Entscheidungen per se treffen sondern erhalten eine Wahrscheinlichkeitsaussage. Um die Sachlage noch komplizierter zu machen, treffen wir Aussagen über eine Population. Also sind *wir* im nicht im Urlaub. Abschließend treffen wir eine Aussage über die beobachteten Daten und können dann eine Wahrscheinlichkeit berechnen diese Dateen beobachtet zu haben, wenn wir nicht im Urlaub sind.

1)  Wir machen Aussagen über Wahrscheinlichkeiten!
2)  Wir machen Aussagen über Populationen!
3)  Wir machen Aussagen über den Nicht-Zustand/Keinen Effekt!
:::

## Referenzen {.unnumbered}
