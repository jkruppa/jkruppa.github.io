[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skript Bio Data Science",
    "section": "",
    "text": "Willkommen\nAuf den folgenden Seiten wirst du umfangreiche Kenntnisse über Statistik und Data Science erwerben können, ohne dass du an einer meiner Veranstaltungen teilnehmen musst. Du bist herzlich eingeladen, hier und dort vorbei zuschauen, um zu sehen, ob etwas für dich von Interesse ist. Das Skript wird kontinuierlich von mir aktualisiert. Zusätzlich zu den Skripten stehen dir auch erläuternde YouTube-Videos zur Verfügung. Es freut mich, dass du daran interessiert bist, hier etwas Neues zu lernen, sei es aus eigenem Antrieb oder weil du dich auf eine anstehende Klausur vorbereiten möchtest. In jedem Fall empfehle ich dir, dich einfach um zuschauen."
  },
  {
    "objectID": "index.html#lernen-auf-youtube",
    "href": "index.html#lernen-auf-youtube",
    "title": "Skript Bio Data Science",
    "section": "Lernen auf YouTube",
    "text": "Lernen auf YouTube\nDu liest gerade das Skript für meine Vorlesungen an der Hochschule Osnabrück in der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL). Um den Stoff, den ich vermitteln möchte, zu erlernen, stehen dir verschiedene Möglichkeiten offen. Neben meinen Vorlesungen kannst du auch auf YouTube lernen, indem du meine Lernvideos anschaust. In den Videos wiederhole ich Inhalte der Vorlesung sowie der R Programmierung. Der große Vorteil ist aber, dass du auf Pause drücken und dir Inhalte wiederholt anschauen kannst. Gerne kannst du dir einmal das Einführungsvideo auf  anschauen."
  },
  {
    "objectID": "index.html#lernen-für-die-klausur",
    "href": "index.html#lernen-für-die-klausur",
    "title": "Skript Bio Data Science",
    "section": "Lernen für die Klausur",
    "text": "Lernen für die Klausur\nIm Weiteren findest du meine gesammelten Klausurfragen für alle Module auf GitHub unter folgendem Link: gesammelten Klausurfragen auf GitHub oder auf ILIAS im entsprechenden Modul.\nDie Klausurfragen zu den einzelnen Vorlesungen innerhalb eines Moduls werden in den entsprechenden Übungen behandelt. Zusätzlich gibt es ein Archiv, das alle bisherigen Klausuren über alle Studiengänge hinweg enthält. Dieses Archiv findest du hier: Archive aller bisherigen Klausuren.\nIn der  Playlist der Fragen & Antworten findest du nochmal alle Antworten zu den Klausurfragen kurz besprochen. Gerne kannst du dir einmal das Einführungsvideo für die Playlist der Fragen & Antworten auf  anschauen."
  },
  {
    "objectID": "index.html#lernen-für-ein-projekt",
    "href": "index.html#lernen-für-ein-projekt",
    "title": "Skript Bio Data Science",
    "section": "Lernen für ein Projekt",
    "text": "Lernen für ein Projekt\nDir ist das alles irgendwie zu stückig und gekünstelt? Dann habe ich noch die Spielwiese in R für dich. Dort zeige ich an Beispielen wie die Statistik, das Programmieren in R und die Data Science zusammenkommt.\nDa es hier dann doch recht eng wurde, habe ich das Kapitel Beispielhafte Auswertungen dann einmal auf eine andere Internetseite ausgelagert. Jetzt findest du alle Beispiele zur der Anwendung auf der neuen Internetseite. In dem neuen Skript gibt es dann aber keine weiteren Informationen mehr zu dem R Code oder der Statistik.\nGerne kannst du dir einmal das Einführungsvideo für die Playlist der Spielwiese auf  anschauen."
  },
  {
    "objectID": "index.html#kontakt",
    "href": "index.html#kontakt",
    "title": "Skript Bio Data Science",
    "section": "Kontakt",
    "text": "Kontakt\nNoch Fragen? Wie erreichst du mich? Am einfachsten über die gute, alte E-Mail. Bitte beachte, dass gerade kurz vor den Prüfungen ich mehr E-Mails kriege. Leider kann es dann einen Tick dauern.\nEinfach an j.kruppa@hs-osnabrueck.de schreiben. Du findest hier auch eine kurze Formulierungshilfe.\nBitte gib immer in deiner E-Mail dein Modul - was du belegst - mit an. Pro Semester unterrichte ich immer drei sehr ähnlich klingende Module. Daher schau nochmal hier in der Liste, wenn du unsicher bist.\n\n\n\n\n\n\nE-Mailvorlage mit beispielhafter Anrede\n\n\n\nHallo Herr Kruppa,\n… ich belege gerade Ihr Modul Modulname und hätte eine Bitte/Frage/Anregung…\n… ich benötige Hilfe bei der Planung/Auswertung meiner Bachelorarbeit…\nMit freundlichen Grüßen\nM. Muster"
  },
  {
    "objectID": "organisation.html#statistische-beratung",
    "href": "organisation.html#statistische-beratung",
    "title": "\n1  Organisation\n",
    "section": "\n1.1 Statistische Beratung",
    "text": "1.1 Statistische Beratung\nNeben der klassischen Vorlesung biete ich auch Termine für die statistische Beratung von Abschlussarbeiten sowie Projekten an. Dieses Angebot gilt es für alle Mitglieder der Hochschule Osnabrück. Primär für Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL), aber ntürlich auch für alle anderen Fakultäten. Dafür musst du mir einfach nur eine E-Mail schreiben und dann erhälst du einen Termin innerhalb der nächsten zwei Wochen.\nDie Beratung ist grundsätzlich anonym und vertraulich. Wenn du willst kannst du gerne noch dein:e Betreuer:in mitbringen. Das ist aber keine Voraussetzung oder Notwendigkeit. Meistens finden mehrere Besprechungen statt, wir versuchen aber natürlich zusammen zügig dein Problem zu lösen. Ziel ist der Beratung ist es dich in die Lage zu versetzen selbstständig deine Analyse zu rechnen."
  },
  {
    "objectID": "organisation.html#sec-vorlesungen-hs",
    "href": "organisation.html#sec-vorlesungen-hs",
    "title": "\n1  Organisation\n",
    "section": "\n1.2 Vorlesungen an der Hochschule Osnabrück",
    "text": "1.2 Vorlesungen an der Hochschule Osnabrück\nVon mir angebotene Vorlesungen werden an der Hochschule Osnabrück an der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) in ILIAS verwaltet. Alle notwendigen Informationen und Materialien sind auf ILIAS unter https://lms.hs-osnabrueck.de/ zu finden. Wenn du in dem Kurs nicht angemeldet bist, dann kontaktiere mich bitte per Mail. Auch die Kommunikation erfolgt von meiner Seite aus über ILIAS.\nAuf ILIAS findest du alle aktuellen Kursinformationen und erhälst auch die Mails, wenn Änderungen im Kursablauf stattfinden.\nWenn du nicht in der Fakultät Agrarwissenschaften und Landschaftsarchitektur (AuL) studierst oder aber in einem Studiengang, der meine Module nicht anbietet, steht es dir natürlich frei, sich in meine Vorlesungen zu setzten. Du findest in Anhang D eine Übersicht der angebotenen Module und auch die inhaltliche Ordnung nach Lernstufe. Dort findest du auch die aktuellen Entwürfe der Modulbeschreibungen. Bitte informiere dich in deinem Studierendensekretariat über die Modalitäten zur Prüfungsteilnahme."
  },
  {
    "objectID": "organisation.html#sec-bachelorarbeit",
    "href": "organisation.html#sec-bachelorarbeit",
    "title": "\n1  Organisation\n",
    "section": "\n1.3 Bachelorarbeit",
    "text": "1.3 Bachelorarbeit\nHier findest sich eine aktuelle Struktursammlung für die Bachelorarbeit. Hier findest du keine Themen. Dafür musst du mich bitte ansprechen oder eine E-Mail schreiben. Die Themen finden sich dann etwa mit Kooperationspartern oder aber eher methodisch ohne echte Daten. Das müssen wir dann aber Absprechen.\nBitte halte Rücksprache, wenn dir Teile der Regeln für die Bachelorarbeit unklar sind.\n\nIch empfehle die Arbeit in engischer Sprache zu verfassen.\nDie Bachelorarbeit umfasst einen Zeitraum von 12 Wochen. Bei Unsicherheit über das Thema kann einmalig eine 4-wöchige Einarbeitungsphase vereinbart werden. Danach wird das Thema der Bachelorarbeit konkretisiert.\nDer Umfang sollte die 30 Seiten nicht überschreiten.\nDie Arbeit umfasst ca. 30 Referenzen, davon sind die meisten aktuelleren Datums. Internetseiten zählen ausdrücklich nicht als Referenz.\nEin Bewertungsbogen für die Bachelorarbeit steht zu Beginn der Arbeit zu Verfügung und kann jederzeit eingesehen werden.\nIm Rahmen der Betreuung finden mindestens jede Wochen ein kurzes Zoom-Treffen statt in dem der aktuelle Fortschritt der Arbeit besprochen wird.\nIn der 4-ten Woche wird eine kurze Präsentation der bisherigen erarbeiteten Inhalte gegeben. Diese Präsentation kann in der 8-ten Woche erneut erfolgen.\nMit einem methodischen Thema wird die Arbeit in Quarto in R oder in LaTeX in Overleaf geschreiben.\n\nIm Folgenden siehst du nochmal den groben zeitlichen Ablauf in Abbildung 1.1. Der Ablauf dient der groben Orientierung, damit du auch weißt, wo du etwa stehst. Nach vier Wochen solltest du gut 3500 Wörter geschrieben haben und nach 8 Wochen ca. 7000 Worte. Damit solltest du dann am Ende auf die 30 Seiten mit ca. 10500 Worten kommen.\n\n\nAbbildung 1.1— Grober zeitlicher Ablauf einer Bachelorarbeit. Die Präsentation umfasst ca. 10 Slides und folgt ebenfalls dem IMRAD Schema. Ich rechne mit 350 Worten pro deutscher Standardseite.\n\nBitte bachte auch die Hilfestellungen und die Erfahrungsberichte in dem Anhang B, wo ich nochmal über Writing principles etwas aufgeschrieben habe. Vielleicht hilft dir das dann auch.\n\n\n\n\n\n\nKorrekte Schreibweise von einer Formel\n\n\n\n\n\n\\[\ny \\sim x_1 + x_2\n\\]\nmit\n\n\n\\(y\\) gleich dem gemessenen Frischgewicht in [kg/ha],\n\n\\(x_1\\) als der kontinuierliche Einflussvariable 1,\n\n\\(x_2\\) als der Einflussvariable 2 als Faktor mit den Leveln \\(a\\), \\(b\\) und \\(c\\).\n\n\n\n\n\n\n\n\n\n\nKorrekte Beschriftung und Referenzierung einer Abbildung\n\n\n\n\n\n\n\n\n\nAbbildung 1.2— Boxplots der Sprungweiten in [cm] getrennt für Hunde- und Katzenflöhe.\n\n\n\nIn Abbildung 1.2 sind die Sprungweiten in [cm] von Hunde- und Katzenflöhen als Boxplots dargestellt.\n\n\n\n\n\n\n\n\n\nKorrekte Beschriftung und Referenzierung einer Tabelle\n\n\n\n\n\n\n\n\n\nTabelle 1.1— Tabelle der Sprunglängen in [cm] von Hunde- und Katzenflöhen. Es wurden sieben Hundeflöhe und sieben Katzenflöhe gemessen (\\(n= 14\\)).\n\nanimal\njump_length\n\n\n\ndog\n5.7\n\n\ndog\n8.9\n\n\ndog\n11.8\n\n\ndog\n8.2\n\n\ndog\n5.6\n\n\ndog\n9.1\n\n\ndog\n7.6\n\n\ncat\n3.2\n\n\ncat\n2.2\n\n\ncat\n5.4\n\n\ncat\n4.1\n\n\ncat\n4.3\n\n\ncat\n7.9\n\n\ncat\n6.1\n\n\n\n\n\n\nIn Tabelle 1.1 sind die Sprunglängen in [cm] in der Spalte jump_length von Hunde- und Katzenflöhen in der Spalte animal dargestellt. Insgesamt wurden \\(n = 14\\) Flöhe gemessen davon sieben Hundeflöhe und sieben Katzenflöhe. Wir haben ein balanciertes Design vorliegen."
  },
  {
    "objectID": "literature.html#parametrische-statistik",
    "href": "literature.html#parametrische-statistik",
    "title": "2  Literatur",
    "section": "\n2.1 Parametrische Statistik",
    "text": "2.1 Parametrische Statistik\n\n\n\n\nDormann (2013) liefert ein tolles deutsches Buch für die Vertiefung in die Statistik. Insbesondere wenn du wissenschaftlich Arbeiten willst weit über die Bachelorarbeit hinaus. Dormann baut in seinem Buch eine hervorragende Grundlage auf. Das Buch ist an der Hochschule Osnabrück kostenlos über den Link zu erhalten."
  },
  {
    "objectID": "literature.html#experimental-methods-in-agriculture",
    "href": "literature.html#experimental-methods-in-agriculture",
    "title": "2  Literatur",
    "section": "\n2.2 Experimental methods in agriculture",
    "text": "2.2 Experimental methods in agriculture\n\n\n\n\nOnofri und Sacco (2021) haben das Buch Experimental methods in agriculture geschrieben. Wir werden auf dieses englische Buch ab und zu mal verweisen. Insbesondere der Einleitungstext zur Wissenschaft und dem Design von Experiementen ist immer wieder lesenswert. Spätere Teile des Buches sind etwas mathematischer und nicht für den Einstieg unbedingt geeignet. Aber schaue es dir selber an."
  },
  {
    "objectID": "literature.html#r-for-data-science",
    "href": "literature.html#r-for-data-science",
    "title": "2  Literatur",
    "section": "\n2.3 R for Data Science",
    "text": "2.3 R for Data Science\n\n\n\n\nWickham (2016) ist die Grundlage für die R Programmierung. Das Material von Wickahm findet sich kostenlos online unter https://r4ds.had.co.nz/ und https://www.tidyverse.org/. Wir werden uns hauptsächlich mit R wie es Wickham lehrt beschäftigen. Somit ist Wickham unsere Grundlage für R."
  },
  {
    "objectID": "literature.html#practical-statistics-for-data-scientists",
    "href": "literature.html#practical-statistics-for-data-scientists",
    "title": "2  Literatur",
    "section": "\n2.4 Practical Statistics for Data Scientists",
    "text": "2.4 Practical Statistics for Data Scientists\n\n\n\n\nBruce (2020) schreibt ein Buch für den Anwender. Ohne Vorkenntnisse ist das Buch vermutlich etwas schwer zu lesen. Dafür bietet das Buch aber nach einem Statistikkurs sehr gute Anknüpfungspunkte Richtung maschinelles Lernen und somit der Klassifikation. Das Buch ist auch hier in der englischen Version und hier in der deutschen Version zu erhalten. Beide Links benötigen den Zugang über die Hochschule Osnabrück."
  },
  {
    "objectID": "literature.html#data-science-for-agriculture-in-r",
    "href": "literature.html#data-science-for-agriculture-in-r",
    "title": "2  Literatur",
    "section": "\n2.5 Data Science for Agriculture in R",
    "text": "2.5 Data Science for Agriculture in R\n\n\n\n\nSchmidt liefert auf der Webseite https://schmidtpaul.github.io/DSFAIR/index.html eine tolle Sammlung an experimentellen Designs bzw. Versuchsanlagen samt der Auswertung in R. Ohne Vorkenntnisse schwer zu verstehen. Sollte aber nach einem Kurs Statistik dann möglich sein. Gerne hier auch mich fragen, dann können wir gemeinsam das passende Design raussuchen und besprechen."
  },
  {
    "objectID": "literature.html#odds-ends",
    "href": "literature.html#odds-ends",
    "title": "2  Literatur",
    "section": "\n2.6 Odds & Ends",
    "text": "2.6 Odds & Ends\n\n\n\n\nAm Ende dann noch eine Mathebuch von Weisberg zu finden unter https://jonathanweisberg.org/vip/. Eigentlich eher ein Buch über Wahrscheinlichkeiten und wenn ein Buch am Ende stehen muss, dann ist es dieses Buch. Ich finde es sehr spannend zu lesen, aber das ist dann vermutlich special intrest."
  },
  {
    "objectID": "literature.html#referenzen",
    "href": "literature.html#referenzen",
    "title": "2  Literatur",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nBruce, Peter, Andrew Bruce, und Peter Gedeck. 2020. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media.\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer.\n\n\nWickham, Hadley, und Garrett Grolemund. 2016. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc."
  },
  {
    "objectID": "preface.html#ein-wort-der-warnung",
    "href": "preface.html#ein-wort-der-warnung",
    "title": "3  Einführung",
    "section": "Ein Wort der Warnung…",
    "text": "Ein Wort der Warnung…\nWenn du dieses Bild eines niedergeschlagenen Engels der Statistik siehst…\n\n\n\n\n… dann bedeutet der niedergeschlagene Engel der Statistik:\n\nWir opfern Genauigkeit für Anwendbarkeit. Ja, manchmal ist es eben statstisch nicht richtig was hier steht, aber aus Gründen der Anwendung fahren wir mal über den Engel drüber. Schade.\nWir sind hier Anfänger und Anwender. Später kannst du noch tiefer ins Detail gehen. Hier wollen wir die Grundlagen lernen. Das hat dann einen Preis an Richtigkeit.\nWir wollen fertig werden. Durch geschicktes Manövrieren können wir an einen Punkt kommen, wo kein statistischer Test mehr passt. Das wollen wir nicht. Deshalb zahlen wir hier auch einen Preis. Passt aber.\n\nDeshalb konzentrieren wir uns auf einige wichtige Lernziele, die wir jetzt einmal nacheinander durchgehen."
  },
  {
    "objectID": "preface.html#lernziel-1-eine-explorative-datananalyse-durchführen",
    "href": "preface.html#lernziel-1-eine-explorative-datananalyse-durchführen",
    "title": "3  Einführung",
    "section": "\n3.1 Lernziel 1: Eine explorative Datananalyse durchführen",
    "text": "3.1 Lernziel 1: Eine explorative Datananalyse durchführen\nGleich zu Beginn R Code zu zeigen und eine entsprechende Abbildung ist vielleicht ungewöhnlich, aber wir wollen zu dieser Abbildung 3.1 hin. In Abbildung 3.1 siehst du einen Boxplot. Und wie wir aus den Daten flea_dog_cat.xlsx einen Boxplot erstellen, das soll uns in den nächsten Kapitel beschäftigen. Dafür müssen wir nämlich eine Menge in dem Codeblock verstehen und dann auch Anwenden können. Und natürlich lernen was eigentlich ein Boxplot ist und was in einem Boxplot eigentlich dargestellt ist.\n\n\n\n\nAbbildung 3.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\nHier ist der Codeblock der in R die Abbildung 3.1 erstellt.\n\n## Einlesen von Daten aus Excel\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n\n## Umformen der &lt;chr&gt; Spalte in einen Factor &lt;fct&gt;\ndata_tbl &lt;- data_tbl %&gt;% \n  mutate(animal = as_factor(animal))\n\n## Auswählen der wichtigen Spalten für den Boxplot\ndata_tbl &lt;- data_tbl %&gt;% \n  select(animal, jump_length) \n\n## Generieren des Boxplots in ggplot()\nggplot(data_tbl, aes(x = animal, y = jump_length, \n                     fill = animal)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, shape = 4, size = 4) +\n  labs(x = \"Tierart\", y = \"Sprungweite in [cm]\", \n       fill = \"Tierart\") +\n  scale_x_discrete(labels = c(\"Hund\", \"Katze\")) +\n  theme_bw()\n\nWir müssen nun folgende Dinge lernen um den Codeblock zu verstehen:\n\nWir müssen das Datenbeispiel verstehen. Was sind das eigentlich für Daten, die wir da abbilden? Was sind überhaupt Daten im Sinne der Statistik bzw. für R.\nWir müssen den R Code verstehen. Von einzelnen wichtigen Operatoren wie -&gt; und %\\&gt;% zu dem den Unterschieden von Worten und Objekten.\nWie kriegen wir Daten aus Excel in R hinein? Wir können die Daten ja nicht einfach in R eintragen sondern haben die Daten ja meist in einer (Excel) Datei wie flea_dog_cat.xlsx.\nWas ist eigentlich ein Boxplot und welche statistischen Maßzahlen werden hier eigentlich abgebildet?\nWie funktioniert eigentlich die Funktionalen ggplot() mit der wir den Boxplot erstellt haben?\n\nAll diese Fragen und weitere Fragen, die sich diesen Fragen anschließen, wollen wir uns in den nächsten Kapitel anschauen. Leider kann ich hier nur linear schreiben. Deshalb musst du eventuell mal ein Kapitel wiederholen oder etwas quer lesen. Du kannst dir ja auch nicht immer alles auf einmal merken."
  },
  {
    "objectID": "preface.html#lernziel-2-rstudio-und-r",
    "href": "preface.html#lernziel-2-rstudio-und-r",
    "title": "3  Einführung",
    "section": "\n3.2 Lernziel 2: RStudio und R",
    "text": "3.2 Lernziel 2: RStudio und R\n\n\n\n\n\n\nWas ist eigentlich RStudio und woher kriege ich das?\n\n\n\nDu findest auf YouTube Einführung in R - Teil 01 - Installation von RStudio und R als Video. Ich gehe in dem Video einmal alle wichtigen Schritte durch und so kannst du dir Rstudio und R installieren.\n\n\nUm Data Science durchführen zu können musst du etwas Programmieren können. Wir programmieren in R und nutzen die Software um Abbildungen zu erstellen und Analysen zu rechnen.\nWir arbeiten in R und nutzen dafür das RStudio. Führe einfach folgende Schritte aus um erst R zu installieren und dann das RStudio.\n\nR installieren unter https://cran.rstudio.com/\n\nRStudio installieren unter https://www.rstudio.com/products/rstudio/download/#download\n\n\nBitte die Reihenfolge beachten. Beide Schritte kannst du dir auch nochmals im Video anschauen oder aber du kommst in das R Tutorium was regelmäßig an der Hochschule Osnabrück von mir angeboten wird. Die Termine findest du im ?sec-r-tutorium."
  },
  {
    "objectID": "preface.html#lernziel-3-falsifikationsprinzip",
    "href": "preface.html#lernziel-3-falsifikationsprinzip",
    "title": "3  Einführung",
    "section": "\n3.3 Lernziel 3: Falsifikationsprinzip",
    "text": "3.3 Lernziel 3: Falsifikationsprinzip\n\n\n\n\n\n\nGrundlagen der Wissenschaft und Falsifikationsprinzip\n\n\n\nDu findest auf YouTube Grundlagen der Wissenschaft und Falsifikationsprinzip als Video Reihe.\n\n\nWie funktioniert ein statistischer Versuch? Ich könnte auch wissenschaftliches Experiment schreiben, aber ein wissenschaftliches Experiment ist sehr abstrakt. Wir wollen ja einen Versuch durchführen und danach - ja was eigentlich? Was wollen wir nach dem Versuch haben? Meistens eine neue Erkenntnis. Um diese Erkenntnis zu validieren oder aber abzusichern nutzen wir Statistik. Dazu musst du noch wissen, dass wir eine spezielle Form der Statistik nutzen: die frequentistische Statistik.\nEine biologische Wiederholung beinhaltet ein neues Tier, Pflanze oder Mensch. Eine technische Wiederholung ist die gleiche Messung an dem gleichen Tier, Pflanze oder Mensch.\nWir nennen das Outcome auch Endpunkt, Response oder kurz \\(y\\).\nDie frequentistische Statistik basiert - wie der Name andeutet - auf Wiederholungen in einem Versuch. Daher der Name frequentistisch. Also eine Frequenz von Beobachtungen. Ist ein wenig gewollt, aber daran gewöhnen wir uns schon mal. Konkret, ein Experiment welches wir frequentistisch Auswerten wollen besteht immer aus biologischen Wiederholungen. Wir müssen also ein Experiment planen in dem wir wiederholt ein Outcome an vielen Tieren, Pflanzen oder Menschen messen. Auf das Outcome gehen wir noch später ein. Im Weiteren konzentrieren wir uns hier auf die parametrische Statistik. Die parametrische Statistik beschäftigt sich mit Parametern von Verteilungen.\n\n\n\n\n\n\nWie gehen wir nun vor, wenn wir ein Experiment durchführen wollen?\n\n\n\n\nWir müssen auf jeden Fall wiederholt ein Outcome an verschiedenen Tieren, Pflanzen oder Menschen messen.\nWir überlegen uns aus welcher Verteilungsfamilie unser Outcome stammt, damit wir dann die entsprechende Verfahren zur Analyse nehmen können.\n\n\n\nWenn wir nun ein Experiment durchführen dann erheben wir einmalig Daten \\(D_1\\). Wir könnten das Experiment wiederholen und erneut Daten \\(D_2\\) erheben. Wir können das Experiment \\(j\\)-mal wiederholen und haben dann Daten von \\(D_1,..., D_j\\). Dennoch werden wir nie alle Daten erheben können, die mit einem Experiment verbunden sind.\nStrukturgleichkeit erreichen wir durch Randomisierung.\nNehmen wir das Beispiel, dass wir die Sprungweite von Hunde- und Katzenflöhen vergleichen wollen. Wir können nicht alle Hunde- und Katzenflöhe messen. Wir können nur eine Stichprobe an Daten \\(D_1\\) erheben. Über diese Daten \\(D_1\\) können wir dann später durch statistische Algorithmen eine Aussage treffen. Wichtig ist hier sich zu merken, dass wir eine Grundgesamtheit haben aus der wir eine Stichprobe ziehen. Wir müssen darauf achten, dass die Stichprobe repräsentativ ist und damit strukturgleich zur Grundgesamtheit ist. Die Strukturgleichkeit erreichen wir durch Randomisierung. Wir veranschaulichen diesen Zusammenhang in Abbildung 3.2. Ein Rückschluß von der Stichprobe ist nur möglich, wenn die Stichprobe die Grundgesamtheit repräsentiert. Auch eine Randomisierung mag dieses Ziel nicht immer erreichen. Im Beispiel der Hundeflöhe könnte wir eine Art an Flöhen übersehen und diese Flohart nicht mit in die Stichprobe aufnehmen. Ein Rückschluß auf diese Flohart wäre dann mit unserem Experiment nicht möglich.\n\n\nAbbildung 3.2— Abbildung über die Grundgesamtheit und die Stichprobe(n) \\(D_1\\) bis \\(D_j\\). Durch Randomisierung wird Sturkturgleichheit erreicht, die dann einen Rückschluß von der Stichprobe auf die Grundgesamtheit erlaubt. Jede Stichprobe ist anders und nicht jede Randomisierung ist erfolgreich was die Strukturgleicheit betrifft.\n\nTabelle 3.1 zeigt nochmal die Zusammenfassung von der Grundgesamtheit un der Stichprobe im Vergleich. Wichtig ist zu merken, dass wir mit unserem kleinen Experiment Daten \\(D\\) generieren mit denen wir einen Rückschluß und somit eine Verallgemeinerung erreichen wollen.\n\n\nTabelle 3.1— Vergleich von Grundgesamtheit und Stichprobe.\n\n\n\n\n\nGrundgesamtheit\nStichprobe\n\n\n\n… \\(n\\) ist riesig bis unfassbar.\n… \\(n_1\\) von \\(D_1\\) ist klein.\n\n\n… der Mittelwert wird mit \\(\\mu_y\\) beschrieben.\n… der Mittelwert wird mit \\(\\bar{y}\\) beschrieben.\n\n\n… die Varianz wird mit \\(\\sigma^2\\) beschrieben.\n… die Varianz wird mit \\(s^2\\) beschrieben.\n\n\n… die Standardabweichung wird mit \\(\\sigma\\) beschrieben.\n… die Standardabweichung wird mit \\(s\\) beschrieben."
  },
  {
    "objectID": "preface.html#referenzen",
    "href": "preface.html#referenzen",
    "title": "3  Einführung",
    "section": "Referenzen",
    "text": "Referenzen"
  },
  {
    "objectID": "example-preface.html#von-flöhen-und-hunden",
    "href": "example-preface.html#von-flöhen-und-hunden",
    "title": "Datenbeispiele",
    "section": "Von Flöhen und Hunden",
    "text": "Von Flöhen und Hunden\nIn unserem ersten Beispiel in Kapitel 4.1 geht es darum einmal ein Gefühl für Daten zu kriegen. Also was sind diese Zahlen und Buchstaben eigentlich? Wie sind Daten aufgebaut und wie musst du Daten bauen, so dass wir auch mit den Daten arbeiten können? Wir schauen uns dafür einmal Flöhe auf Hunden an und fragen uns welche Typen von Zahlen können wir erheben?"
  },
  {
    "objectID": "example-preface.html#von-flöhen-hunden-und-katzen",
    "href": "example-preface.html#von-flöhen-hunden-und-katzen",
    "title": "Datenbeispiele",
    "section": "Von Flöhen, Hunden und Katzen",
    "text": "Von Flöhen, Hunden und Katzen\nIn unserem zweiten Beispiel in Kapitel 4.2 erweitern wir unserer erstes Beispiel um die Katzen. Das heist, dass eigentlich alles gleich bleibt. Wir schauen usn zusätlich noch als zweite Gruppe die Katzen an. Nun können wir die Frage stellen, unterscheiden sich Flöhe auf Hunden und Katzen gegeben von gemessenen Eigenschaften?"
  },
  {
    "objectID": "example-preface.html#von-flöhen-auf-tieren",
    "href": "example-preface.html#von-flöhen-auf-tieren",
    "title": "Datenbeispiele",
    "section": "Von Flöhen auf Tieren",
    "text": "Von Flöhen auf Tieren\nIn unserem dritten Beispiel in Kapitel 5 erweitern wir das Beispiel um den Fuchs mit einem weiteren Tier. Dadurch haben wir nicht mehr einen Faktor mit zwei Leveln vorliegen sondern einen mit drei Leveln. Die Fragestrellung erweitert sich jetzt auf einen multiplen Gruppenvergleich. Wir vergleichen nicht mehr nur noch zwei Gruppen miteinander sondern drei."
  },
  {
    "objectID": "example-preface.html#von-flöhen-auf-tieren-in-habitaten",
    "href": "example-preface.html#von-flöhen-auf-tieren-in-habitaten",
    "title": "Datenbeispiele",
    "section": "Von Flöhen auf Tieren in Habitaten",
    "text": "Von Flöhen auf Tieren in Habitaten\nIn unserem vierten Beispiel in Kapitel 5.2 schauen wir uns zusätzlich zu dem dritten Beispiel noch verschiedene Habitate (eng. site) an. Wir haben nämlich die Hunde-, Katzen-, und Fuchsflöhe nicht nur an einem Ort sondern an verschiedenen Orten gesammelt und gemessen. Wir haben einen zweiten Faktor vorliegen."
  },
  {
    "objectID": "example-preface.html#von-vielen-flöhen-auf-hunden-und-katzen",
    "href": "example-preface.html#von-vielen-flöhen-auf-hunden-und-katzen",
    "title": "Datenbeispiele",
    "section": "Von vielen Flöhen auf Hunden und Katzen",
    "text": "Von vielen Flöhen auf Hunden und Katzen\nIm fünften Beispiel in Kapitel 6 schauen wir uns wiederum nur noch zwei Tierarten an: Hunde und Katzen. Dafür aber eine große Anzahl an Tieren. Wir schauen uns hier die Daten von 400 Tiere an. Auf diesen Tierarten messen wir mehrere Variablen unn wollen uns diese Daten später in der Regression anschauen."
  },
  {
    "objectID": "example-preface.html#gummibärchen",
    "href": "example-preface.html#gummibärchen",
    "title": "Datenbeispiele",
    "section": "Gummibärchen",
    "text": "Gummibärchen\nIm Beispiel mit den Gummibärchen in Kapitel 7 geht es um die Darstellung verschiedener Verteilungen. Wir brauchen den Datensatz um zu verstehen, wie Daten verteilt sind. Sonst können wir den Datensatz auch gut nutzen um einmal in R zu filtern und zu selektieren. Auch für die Erstellung von Abbilungen eignet sich der Datensatz sehr gut."
  },
  {
    "objectID": "example-fleas-dogs-cats.html#sec-example-1",
    "href": "example-fleas-dogs-cats.html#sec-example-1",
    "title": "\n4  Von Flöhen auf Hunden und Katzen\n",
    "section": "\n4.1 Von Flöhen und Hunden",
    "text": "4.1 Von Flöhen und Hunden\nIn unserem ersten Beispiel wollen wir uns verschiedene Daten \\(D\\) von Hunden und Hundeflöhen anschauen. Unter anderem sind dies die Sprungweite, die Anzahl an Flöhen, die Boniturnoten auf einer Hundemesse sowie der Infektionsstatus. Hier nochmal detailiert, was wir uns im Folgenden immer wieder anschauen wollen.\n\nSprungweite in [cm] von verschiedenen Flöhen \\[\nY_{jump} = \\{5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6\\}.\n\\]\nAnzahl an Flöhen auf verschiedenen Hunden \\[\n  Y_{count} = \\{18, 22, 17, 12, 23, 18, 21\\}.\n  \\]\nBoniturnoten [1 = schlechteste bis 9 = beste Note] von verschiedenen Hunden \\[\n  Y_{grade} = \\{8, 8, 6, 8, 7, 7, 9\\}.\n  \\]\nInfektionstatus [0 = gesund, 1 = infiziert] mit Flöhen von verschiedenen Hunden \\[\n  Y_{infected} = \\{0, 1, 1, 0, 1, 0, 0\\}.\n  \\]\n\nJe nachdem was wir messen, nimmt \\(Y\\) andere Zahlenräume an. Wir sagen, \\(Y\\) folgt einer Verteilung. Die Sprungweite ist normalverteilt, die Anzahl an Flöhen folgt einer Poisson Verteilung, die Boniturnoten sind multinominal/ordinal bzw. kategorial verteilt. Der Infektionsstatus ist binomial verteilt. Wir werden uns später die Verteilungen anschauen und visualisieren. Das können wir hier aber noch nicht. Wichtig ist, dass du schon mal gehört hast, dass \\(Y\\) unterschiedlich verteilt ist, je nachdem welche Dinge wir messen.\nTabelle 4.1 zeigt dir die Darstellung der Daten von oben in einer einzigen Tabelle. Bitte beachte, dass genau eine Zeile für eine Beobachutng, in diesem Fall einem Hund, vorgesehen ist.\n\n\n\n\nTabelle 4.1— Sprunglängen [cm] für Hundeflöhe. Die Tabelle ist im Long-Format dargestellt.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\n\n\n\n\n\n\n\n\n\n\nDatei für von Flöhen und Hunden\n\n\n\nDu findest die Datei flea_dog.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-fleas-dogs-cats.html#sec-example-2",
    "href": "example-fleas-dogs-cats.html#sec-example-2",
    "title": "\n4  Von Flöhen auf Hunden und Katzen\n",
    "section": "\n4.2 Von Flöhen, Hunden und Katzen",
    "text": "4.2 Von Flöhen, Hunden und Katzen\nWir wollen jetzt das Beispiel von den Hunden und Flöhen um eine Spezies erweitern. Wir nehmen noch die Katzen mit dazu und fragen uns, wie sieht es mit der Sprungfähigkeit von Katzen und Hundeflöhen aus? Konzentrieren wir uns hier einmal auf die Sprungweite. Wir können wie in dem vorherigen Beispiel mit den Hundeflöhen die Sprungweiten [cm] der Katzenflöhe wieder in der gleichen Weise aufschreiben:\n\\[\nY_{jump} = \\{3.2, 2.2, 5.4, 4.1, 4.3, 7.9, 6.1\\}.\n\\]\nWenn wir jetzt die Sprungweiten der Hundeflöhe mit den Katzenflöhen vergleichen wollen haben wir ein Problem. Beide Zahlenvektoren heißen gleich, nämlich \\(Y_{jump}\\). Wir könnten jeweils in die Indizes noch \\(dog\\) und \\(cat\\) schreiben als \\(Y_{jump,\\, dog}\\) und \\(Y_{jump,\\, cat}\\) und erhalten folgende Vektoren.\n\\[\nY_{jump,\\, dog} = \\{5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6\\}\n\\]\n\\[\nY_{jump,\\, cat} = \\{3.2, 2.2, 5.4, 4.1, 4.3, 7.9, 6.1\\}\n\\]\nDadurch werden die Indizes immer länger und unübersichtlicher. Auch das \\(Y\\) einfach \\(Y_{dog}\\) oder \\(Y_{cat}\\) zu nennen ist keine Lösung - wir wollen uns vielleicht später nicht nur die Sprungweite vergleichen, sondern vielleicht auch die Anzahl an Flöhen oder den Infektionsstatus. Dann ständen wir wieder vor dem Problem die \\(Y\\) für die verschiedenen Outcomes zu unterscheiden. Daher erstellen wir uns die Tabelle 4.2. Wir haben jetzte eine Datentabelle.\n\n\n\n\nTabelle 4.2— Sprunglängen [cm] für Hunde- und Katzenflöhe. Die Tabelle ist im Wide-Format dargestellt.\n\ndog\ncat\n\n\n\n5.7\n3.2\n\n\n8.9\n2.2\n\n\n11.8\n5.4\n\n\n8.2\n4.1\n\n\n5.6\n4.3\n\n\n9.1\n7.9\n\n\n7.6\n6.1\n\n\n\n\n\n\nIntuitiv ist die Tabelle 4.2 übersichtlich und beinhaltet die Informationen die wir wollten. Dennoch haben wir das Probem, das wir in dieser Tabelle 4.2 nicht noch weitere Outcomes angeben können. Wir können die Anzahl an Flöhen auf den Hunde und Katzen nicht darstellen. Als Lösung ändern wir die Tabelle 4.2 in das Long-Format. Dargestellt in Tabelle 4.3. Jede Beobachtung belegt nun eine Zeile. Dies ist sehr wichtig im Kopf zu behalten, wenn du eigene Daten in z.B. Excel einstellst.\n\n\n\n\nTabelle 4.3— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Boniturnote sowie der Infektionsstatus von Hunde- und Katzenflöhe. Die Tabelle ist im Long-Format dargestellt.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\ncat\n3.2\n12\n7\n1\n\n\ncat\n2.2\n13\n5\n0\n\n\ncat\n5.4\n11\n7\n0\n\n\ncat\n4.1\n12\n6\n0\n\n\ncat\n4.3\n16\n6\n1\n\n\ncat\n7.9\n9\n6\n0\n\n\ncat\n6.1\n7\n5\n0\n\n\n\n\n\n\n\n\n\n\n\n\nDatei für von Flöhen, Hunden und Katzen\n\n\n\nDu findest die Datei flea_dog_cat.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-fleas-dogs-cats-foxes.html#von-flöhen-auf-hunde-katzen-und-füchsen",
    "href": "example-fleas-dogs-cats-foxes.html#von-flöhen-auf-hunde-katzen-und-füchsen",
    "title": "5  Von Flöhen auf Tieren",
    "section": "\n5.1 Von Flöhen auf Hunde, Katzen und Füchsen",
    "text": "5.1 Von Flöhen auf Hunde, Katzen und Füchsen\nWir wollen jetzt das Beispiel von den Hunde- und Katzenflöhen um eine weitere Spezies erweitern. Warum machen wir das? Später wollen wir uns anschauen, wie sich verschiedene Gruppen oder Behandlungen voneinander unterscheiden. Wir brauchen alos mehr Spezies. Wir nehmen noch die Füchse mit dazu und fragen uns, wie sieht es mit der Sprungfähigkeit von Hunde-, Katzen- und Fuchsflöhen aus?\n\n\n\n\nTabelle 5.1— Sprunglängen [cm] für Hunde-, Katzen- und Fuchsflöhe.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\ncat\n3.2\n12\n7\n1\n\n\ncat\n2.2\n13\n5\n0\n\n\ncat\n5.4\n11\n7\n0\n\n\ncat\n4.1\n12\n6\n0\n\n\ncat\n4.3\n16\n6\n1\n\n\ncat\n7.9\n9\n6\n0\n\n\ncat\n6.1\n7\n5\n0\n\n\nfox\n7.7\n21\n5\n1\n\n\nfox\n8.1\n25\n4\n1\n\n\nfox\n9.1\n31\n4\n1\n\n\nfox\n9.7\n12\n5\n1\n\n\nfox\n10.6\n28\n4\n0\n\n\nfox\n8.6\n18\n4\n1\n\n\nfox\n10.3\n19\n3\n0\n\n\n\n\n\n\nDer Datensatz in Tabelle 5.1 beginnt schon recht groß zu werden. Deshalb brauchen wir auch R als Werkzeug um große Datensätze auswerten zu können.\n\n\n\n\n\n\nDatei für von Flöhen auf Tieren\n\n\n\nDu findest die Datei flea_dog_cat_fox.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-fleas-dogs-cats-foxes.html#sec-example-4",
    "href": "example-fleas-dogs-cats-foxes.html#sec-example-4",
    "title": "5  Von Flöhen auf Tieren",
    "section": "\n5.2 Von Flöhen auf Tieren in Habitaten",
    "text": "5.2 Von Flöhen auf Tieren in Habitaten\nWir schauen uns in diesem Beispiel wiederum drei Tierarten an: Hunde, Katzen und Füchse. Auf diesen Tierarten messen wir die Sprunglänge von jeweils zehn Tieren. Im Vergleich zu dem vorherigen Beispiel erweitern wir die Daten um eine Spalte site in der wir vier verschiedene Messorte protokollieren. Es ergibt sich folgende Tabelle 5.2 und die dazugehörige Abbildung 5.1.\n\n\n\n\nTabelle 5.2— Sprunglängen [cm] für Hunde-, Katzen- und Fuchsflöhe in verschiedenen Habitaten.\n\nanimal\nsite\nrep\njump_length\n\n\n\ncat\ncity\n1\n12.04\n\n\ncat\ncity\n2\n11.98\n\n\ncat\ncity\n3\n16.1\n\n\ncat\ncity\n4\n13.42\n\n\ncat\ncity\n5\n12.37\n\n\ncat\ncity\n6\n16.36\n\n\n…\n…\n…\n…\n\n\nfox\nfield\n5\n16.38\n\n\nfox\nfield\n6\n14.59\n\n\nfox\nfield\n7\n14.03\n\n\nfox\nfield\n8\n13.63\n\n\nfox\nfield\n9\n14.09\n\n\nfox\nfield\n10\n15.52\n\n\n\n\n\n\nÜber die explorative Datenanalyse erfährst du mehr im Kapitel 16\nDie Datentabelle ist in dieser Form schon fast nicht mehr überschaubar. Daher hilft hier die explorative Datenanalyse weiter. Wir schauen uns daher die Daten einmal als einen Boxplot in Abbildung 5.1 an. Wir sehen hier, dass wir drei Tierarten an vier Orten die Sprungweite in [cm] gemessen haben.\n\n\n\n\nAbbildung 5.1— Boxplot der Sprungweiten [cm] für Hunde-, Katzen- und Fuchsflöhe in verschiedenen Habitaten.\n\n\n\n\n\n\n\n\n\nDatei für von Flöhen auf Tieren in Habitaten\n\n\n\nDu findest die Datei flea_dog_cat_fox_site.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-fleas-dogs-cats-length-weight.html",
    "href": "example-fleas-dogs-cats-length-weight.html",
    "title": "6  Von vielen Flöhen auf Hunden und Katzen",
    "section": "",
    "text": "Version vom June 14, 2023 um 09:24:06\nWir schauen uns in diesem Beispiel wiederum nur zwei Tierarten an: Hunde und Katzen. Auf diesen Tierarten messen wir wieder die Sprunglänge in [cm] von jeweils 400 Tieren. Im Vergleich zu dem vorherigen Beispiel erweitern wir die Daten um eine Spalte jump_weight in [mg] sowie sex [male, female]. Bei Versuch wurde noch in der Variable hatch_time gemessen, wie lange die Flöhe in Stunden zum Schlümpfen brauchen. Es ergibt sich folgende Tabelle 6.1 mit den ersten zehn Beobachtungen und die dazugehörige Abbildung 6.1.\n\n\n\n\nTabelle 6.1— Sprunglängen [cm], Gewichte [mg], Geschecht [sex] und Schlüpfzeit [h] für Hunde- und Katzenflöhe.\n\nanimal\nsex\nweight\njump_length\nflea_count\nhatch_time\n\n\n\ncat\nmale\n6.02\n15.79\n5\n483.60\n\n\ncat\nmale\n5.99\n18.33\n1\n82.56\n\n\ncat\nmale\n8.05\n17.58\n1\n296.73\n\n\ncat\nmale\n6.71\n14.09\n3\n140.90\n\n\ncat\nmale\n6.19\n18.22\n1\n162.20\n\n\ncat\nmale\n8.18\n13.49\n1\n167.47\n\n\ncat\nmale\n7.46\n16.28\n1\n291.20\n\n\ncat\nmale\n5.58\n14.54\n0\n112.58\n\n\ncat\nmale\n6.19\n16.36\n1\n143.97\n\n\ncat\nmale\n7.53\n15.08\n1\n766.31\n\n\n\n\n\n\nÜber die explorative Datenanalyse erfährst du mehr im Kapitel 16\nDie Datentabelle ist in dieser Form schon fast nicht mehr überschaubar. Daher hilft hier die explorative Datenanalyse weiter. Wir schauen uns daher die Daten einmal als einen Scatterplot in Abbildung 6.1 an. Wir sehen hier, dass wir das mit dem Gewicht [mg] der Flöhe auch die Sprungweite in [cm] steigt.\n\n\n\n\nAbbildung 6.1— Scatterplot der Sprunglängen [cm] und Gewichte [mg] für Hunde- und Katzenflöhe.\n\n\n\n\n\n\n\n\n\nDatei für von vielen Flöhen auf Hunden und Katzen\n\n\n\nDu findest die Datei flea_dog_cat_length_weight.xlsx auf GitHub jkruppa.github.io/data/ als Excel oder auch als CSV."
  },
  {
    "objectID": "example-gummi-bears.html#referenzen",
    "href": "example-gummi-bears.html#referenzen",
    "title": "7  Von Gummibärchen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nKruppa, Jochen, und Björn Kiehne. 2019. „Statistik lebendig lehren durch Storytelling und forschungsbasiertes Lernen“. Beiträge zu Praxis, Praxisforschung und Forschung, 501.\n\n\nKruppa, Jochen, und Miriam Sieg. 2021. „Spielerisch Daten reinigen“. In Zeig mir Health Data Science!, 93–103. Springer."
  },
  {
    "objectID": "example-complex.html#sec-example-pigs",
    "href": "example-complex.html#sec-example-pigs",
    "title": "8  Von komplexeren Daten",
    "section": "\n8.1 Von infizierten Ferkeln",
    "text": "8.1 Von infizierten Ferkeln\nIm Folgenden schauen wir uns den anonymisierten Datensatz zu einer Ferkelinfektion an. Wir haben verschiedene Gesundheitsparameter an den Ferkeln gemessen und wollen an diesen Rückschließen, ob diese Gesundheitsparameter etwas mit der Infektion zu tun haben. Insgesamt haben wir \\(412\\) Ferkel an vier verschiedenen Orten in Niedersachsen gemessen.\n\n\n\n\nTabelle 8.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\ninfected\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\n1\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\n1\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\n0\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.15\n19.05\n4.35\n1\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n55.38\n18.44\n5.27\n1\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n60.29\n18.42\n4.78\n1\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\npre-frail\n55.32\n19.75\n3.92\n1\n\n\n56\nmale\nwest\n13.91\n20.8\nfrail\n58.37\n17.28\n7.44\n0\n\n\n57\nmale\nnorthwest\n12.49\n21.95\npre-frail\n56.66\n16.86\n2.44\n1\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\n1\n\n\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\n0\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\n1\n\n\n\n\n\n\nAuch hier haben wir nur eingeschränkte Informationen zu den erhobenen Variablen. Daher müssen wir schauen, dass die Variablen in etwa Sinn ergeben.\n\n\nage, das Alter in Lebenstagen der untersuchten Ferkel.\n\nsex, das bestimmte Geschlecht der Ferkel.\n\nlocation, anonymisierter Ort der Untersuchung. Wir unterscheiden zwischen Norden, Nordosten, West und Nordwest in Niedersachsen.\n\nactivity, Minuten an Aktivität pro Stunde. Die Aktivität wurde über eine automatische Bilderkennung bestimmt. Dabei musste die Bewegung ein gewisses Limit übersteigen. Einfach rumgehen hat nicht gereicht um gezählt zu werden.\n\ncrp, der CRP-Wert in mg/l aus der Blutprobe. Das Ausmaß des CRP-Anstiegs gibt einen Hinweis auf die Schwere der zugrundeliegenden Krankheit.\n\nfrailty, die visuelle Einordnung des Gesundheitszustandes anhand der Beweglichkeit des Ferkels. Nach einem Punkteschema wurden die Ferkel in die drei Gruppen robust, pre-frail und frail eingeteilt.\n\nbloodpressure, gemessener Blutdruck der Ferkel.\n\nweight, das gemessene Gewicht der Ferkel in kg.\n\ncreatinin, der Creatinin-Wert aus der Blutprobe. Zu hohe Kreatinin-Werte können auf eine Nierenschwäche, Verletzungen der Muskulatur oder eine Entzündung der Haut und Muskulatur hindeuten.\n\ninfected, der Infektionsstatus zum Zeitpunkt der Untersuchung.\n\nWir nutzen den Datensatz unter anderem in der logistischen Regression in Kapitel 43.\n\n\n\n\n\n\nDatei von den infizierten Ferkeln\n\n\n\nDu findest die Datei infected_pigs.xlsx auf GitHub jkruppa.github.io/data/ als Excel Datei."
  },
  {
    "objectID": "example-complex.html#sec-example-longnose",
    "href": "example-complex.html#sec-example-longnose",
    "title": "8  Von komplexeren Daten",
    "section": "\n8.2 Von langnasigen Hechten",
    "text": "8.2 Von langnasigen Hechten\nIn der folgenden Datentabelle wollen wir uns die Anzahl an Hechten in verschiedenen nordamerikanischen Flüßen anschauen. Jede Zeile des Datensatzes steht für einen Fluss. Wir haben dann in jedem Fluss die Anzahl an Hechten gezählt und weitere Flussparameter erhoben. Wir fragen uns, ob wir anhand der Flussparameter eine Aussage über die Anzahl an Hechten in einem Fluss machen können.\n\n\nDie Daten zu den langnasigen Hechten stammt von Salvatore S. Mangiafico - An R Companion for the Handbook of Biological Statistics.\n\n\n\n\nTabelle 8.2— Auszug aus dem Daten zu den langnasigen Hechten.\n\n\n\n\n\n\n\n\n\n\n\nstream\nlongnose\narea\ndo2\nmaxdepth\nno3\nso4\ntemp\n\n\n\nbasin_run\n13\n2528\n9.6\n80\n2.28\n16.75\n15.3\n\n\nbear_br\n12\n3333\n8.5\n83\n5.34\n7.74\n19.4\n\n\nbear_cr\n54\n19611\n8.3\n96\n0.99\n10.92\n19.5\n\n\nbeaver_dam_cr\n19\n3570\n9.2\n56\n5.44\n16.53\n17\n\n\nbeaver_run\n37\n1722\n8.1\n43\n5.66\n5.91\n19.3\n\n\nbennett_cr\n2\n583\n9.2\n51\n2.26\n8.81\n12.9\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nseneca_cr\n23\n18422\n9.9\n45\n1.58\n8.37\n20.1\n\n\nsouth_br_casselman_r\n2\n6311\n7.6\n46\n0.64\n21.16\n18.5\n\n\nsouth_br_patapsco\n26\n1450\n7.9\n60\n2.96\n8.84\n18.6\n\n\nsouth_fork_linganore_cr\n20\n4106\n10\n96\n2.62\n5.45\n15.4\n\n\ntuscarora_cr\n38\n10274\n9.3\n90\n5.45\n24.76\n15\n\n\nwatts_br\n19\n510\n6.7\n82\n5.25\n14.19\n26.5\n\n\n\n\n\n\nWie immer haben wir nicht so viele Informationen über die Daten vorliegen. Einiges können wir aber aus den Namen der Spalten in dem Datensatz ableiten. Wir haben in verschiedenen Flüssen die Anzahl an Hechten gezählt und noch weitere Flussparameter gemessen. Ein wenig müssen wir hier auch unsere eigene Geschichte spinnen.\n\n\nstream, beschreibt den Fluss, wo die Messung der Anzahl an langnasigen Hechten stattgefunden hat.\n\nlongnose, die Anzahl der Hechte, die in einem Flussarm in einer definierten Zeit gezählet wurden.\n\narea, erfasste Oberfläche des Flusses in dem gemessenen Gebiet. Die Fläche wurde über Satelietenbilder bestimmt.\n\ndo2, gemessener Partialdruck von Sauerstoiff \\(O_2\\) im Wasser und damit auch der verfügbarer Sauerstoff (engl. Oxygen-Delivery, DO2) im Wasser.\n\nmaxdepth, die maximale Tiefe des Flusses über mindestens einen Kilometer. Kürze Tiefen wurden nicht berücksichtigt.\n\nno3, die gemessene Nitratkonzentration im Wasser.\n\nso4, die gemessene Schwefelkonzentration im Wasser.\n\ntemp, gemessene Temperatur in dem Flussarm zur Zeit der Zählung.\n\nWir nutzen den Datensatz unter anderem in der Poisson Regression in Kapitel 41.\n\n\n\n\n\n\nDatei von den langnasigen Hechten\n\n\n\nDu findest die Datei longnose.csv auf GitHub jkruppa.github.io/data/ als Csv Datei."
  },
  {
    "objectID": "example-complex.html#sec-example-chickpea",
    "href": "example-complex.html#sec-example-chickpea",
    "title": "8  Von komplexeren Daten",
    "section": "\n8.3 Von den Kichererbsen in Brandenburg",
    "text": "8.3 Von den Kichererbsen in Brandenburg\nIm Folgenden schauen wir uns die Daten eines Pilotprojektes zum Anbau von Kichererbsen in Brandenburg an. Wir haben an verschiedenen anonymisierten Bauernhöfen Kichererbsen angebaut und das Trockengewicht als Endpunkt bestimmt. Darüber hinaus haben wir noch andere Umweltparameter erhoben und wollen schauen, welche dieser Parameter einen Einfluss auf das Trockengewicht hat.\n\n\n\n\nTabelle 8.3— Auszug aus dem Daten zu den Kichererbsen in Brandenburg.\n\n\n\n\n\n\n\n\n\n\n\ntemp\nrained\nlocation\nno3\nfe\nsand\nforest\ndryweight\n\n\n\n25.26\nhigh\nnorth\n5.56\n4.43\n63\n&gt;1000m\n253.42\n\n\n21.4\nhigh\nnortheast\n9.15\n2.58\n51.17\n&lt;1000m\n213.88\n\n\n27.84\nhigh\nnortheast\n5.57\n2.19\n55.57\n&gt;1000m\n230.71\n\n\n24.59\nlow\nnorth\n7.97\n1.47\n62.49\n&gt;1000m\n257.74\n\n\n26.51\nlow\nnorth\n6.29\n4.3\n59.09\n&gt;1000m\n242.03\n\n\n22.3\nlow\nnortheast\n6.69\n4.78\n58.72\n&gt;1000m\n236.98\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n25.04\nlow\nnortheast\n5.64\n2.22\n59.47\n&gt;1000m\n240.28\n\n\n28.77\nlow\nwest\n6.55\n2.26\n61.11\n&gt;1000m\n268.39\n\n\n25.47\nlow\nnorth\n6.92\n3.18\n64.55\n&lt;1000m\n268.58\n\n\n29.04\nlow\nnorth\n5.64\n2.87\n53.27\n&gt;1000m\n236.07\n\n\n24.11\nhigh\nnortheast\n4.31\n3.66\n63\n&lt;1000m\n259.82\n\n\n28.88\nlow\nnortheast\n7.92\n2\n65.75\n&gt;1000m\n274.75\n\n\n\n\n\n\nEs ist ja schon fast Mode, aber auch hier haben wir wenig bis gar keine Informationen zu den erhobenen Variablen. Daher machen wir das Beste aus der Sachlage und überlegen uns was hier passen könnte.\n\n\ntemp, die mittlere Temperatur über die Wachstumsperiode.\n\nrained, erfasste Regenmenge im Vergleich zum 10jähigen Mittel.\n\nlocation, anonymisierter Ort der Untersuchung.\n\nno3, die gemessene Nitratkonzentration im Boden.\n\nso4, die gemessene Eisenkonzentration im Boden.\n\nsand, der Anteil an Sand im Boden.\n\nforest, der Abstand zum nächsten geschlossenen Waldstück.\n\ndryweight, das Trockengewicht der Kichererbsen gemittelt über eine Hektar.\n\nWir nutzen den Datensatz unter anderem in der Gaussian Regression in Kapitel 40.\n\n\n\n\n\n\nDatei von den Kichererbsen in Brandenburg\n\n\n\nDu findest die Datei chickpeas.xlsx auf GitHub jkruppa.github.io/data/ als Excel Datei."
  },
  {
    "objectID": "programing-preface.html",
    "href": "programing-preface.html",
    "title": "Programmieren in R",
    "section": "",
    "text": "Version vom June 20, 2023 um 08:14:46\n\nActually, I see it as part of my job to inflict R on people who are perfectly happy to have never heard of it. Happiness doesn’t equal proficient and efficient. In some cases the proficiency of a person serves a greater good than their momentary happiness. — Patrick Burns, R-help April 2005\n\n\n\n\n\n\n\nEinführung in R per Video\n\n\n\nDu findest auf YouTube Grundlagen in R als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\n\nUm eine Sprache zu lernen brauchst du Vokabeln und Grammatik. Beides lernt sich alleine immer recht sinnlos. Aber zusammen bringen kann man beides erst, wenn man beides gelernt hat.\nWas solltest du nun zuerst Lesen um Programmieren in R zu verstehen und zu lernen? Es ist sehr schwierig die Programmierung exakt so zu schreiben, dass das Programmieren linear verständlich ist. Du brauchst im Prinzip das Wissen aus Kapitel 10 Operatoren, Funktionen und Pakete um die Grundlagen von Operatoren und Funktionen in R zu verstehen.\nAuf der anderen Seite fehlt dir vielleicht noch das Verständnis von Buchstaben und Zahlen in R. Diesen Zusammenhang zwischen Buchstaben und Zahlen erkläre ich als erstes im folgenden Kapitel 9 Von Buchstaben und Zahlen. Vielleicht musst du beide Kapitel jeweils nochmal lesen. Oder aber in der Anwendung sehen. Stell dir vor es ist wie eine Sprache zu lernen. Ohne Vokabeln keine Sätze aber ohne Grammatik kein Sinn.\nBevor wir uns weiter mit statistischen Kennzahlen beschäftigen, wollen wir uns einmal die Realisierung einer Datentabelle mit den Hunde- und Katzenflöhen in R anschauen. Dabei wollen wir auch Eigenschaften von Zahlen und Buchstaben lernen, die notwendig sind um mit einem Programm wie R kommunizieren zu können. Wir wollen später R nutzen um die explorative Datenanalyse anzuwenden. Fangen wir also an die Grammatik und die Vokabeln in R zu verstehen um dann mit dem Rechner kommunizieren zu können.\nViele Fragen mich, warum ich so gut programmieren kann… um es in einem treffenden Cartoon von Sarah Anderson so zusammenzufassen…\n\n\nAbbildung 1— © 2023 Sarah C Andersen, https://sarahcandersenshop.com/"
  },
  {
    "objectID": "programing-letters-numbers.html#daten-in-r-sind-tibble",
    "href": "programing-letters-numbers.html#daten-in-r-sind-tibble",
    "title": "9  Buchstaben und Zahlen",
    "section": "\n9.1 Daten in R sind tibble()\n",
    "text": "9.1 Daten in R sind tibble()\n\nIm Folgenden sehen wir die Daten aus der Tabelle 9.1 in R als tibble dargestellt. Was ist nun ein tibble? Ein tibble ist zu aller erst ein Speicher für Daten in R. Das heist wir haben Spalten und Zeilen. Jede Spalte repräsentiert eine Messung oder Variable und die Zeilen jeweils eine Beobachtung.\n\n\n# A tibble: 14 × 5\n   animal jump_length flea_count grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt;   \n 1 dog            5.7         18     8 FALSE   \n 2 dog            8.9         22     8 TRUE    \n 3 dog           11.8         17     6 TRUE    \n 4 dog            8.2         12     8 FALSE   \n 5 dog            5.6         23     7 TRUE    \n 6 dog            9.1         18     7 FALSE   \n 7 dog            7.6         21     9 FALSE   \n 8 cat            3.2         12     7 TRUE    \n 9 cat            2.2         13     5 FALSE   \n10 cat            5.4         11     7 FALSE   \n11 cat            4.1         12     6 FALSE   \n12 cat            4.3         16     6 TRUE    \n13 cat            7.9          9     6 FALSE   \n14 cat            6.1          7     5 FALSE   \n\n\nAls erstes erfahren wir, dass wir einen A tibble: 14 x 5 vorliegen haben. Das heist, wir haben 14 Zeile und 5 Spalten. In einem tibble wird immer in der ersten Zeile angezeigt wieviele Beobachtungen wir in dem Datensatz haben. Wenn das tibble zu groß wird, werden wir nicht mehr das ganze tibble sehen sondern nur noch einen Ausschnitt. Im Weiteren hat jede Spalte noch eine Eigenschaft unter dem Spaltennamen:\n\n\n&lt;chr&gt; bedeutet character. Wir haben also hier Worte vorliegen.\n\n&lt;dbl&gt; bedeutet double. Ein double ist eine Zahl mit Kommastellen.\n\n&lt;int&gt; bedeutet integer. Ein integer ist eine ganze Zahl ohne Kommastellen.\n\n&lt;lgl&gt; bedeutet logical oder boolean. Hier gibt es nur die Ausprägung wahr oder falsch. Somit TRUE oder FALSE. Statt den Worten TRUE oder FALSE kann hier auch 0 oder 1 stehen.\n\n&lt;str&gt; bedeutet string der aus verschiedenen character besteht kann, getrennt durch Leerzeichen."
  },
  {
    "objectID": "programing-letters-numbers.html#faktoren-als-wörter-zu-zahlen",
    "href": "programing-letters-numbers.html#faktoren-als-wörter-zu-zahlen",
    "title": "9  Buchstaben und Zahlen",
    "section": "\n9.2 Faktoren als Wörter zu Zahlen",
    "text": "9.2 Faktoren als Wörter zu Zahlen\nEin Faktor ist eine Variable mit mehrern Faktorstufen oder Leveln. Für uns sieht der Faktor wie ein Wort aus, hinter jedem Wort steht aber eine Zahl mit der gerechnet werden kann.\nEin Computer und somit auch eine Programmsprache wie R kann keine Buchstaben verrechnen. Ein Programm kann nur mit Zahlen rechnen. Wir haben aber in der Tabelle 9.1 in der Spalte animal Buchstaben stehen. Da wir hier einen Kompromiss eingehen müssen führen wir Faktoren ein. Ein Faktor kombiniert Buchstaben mit Zahlen. Wir als Anwender sehen die Buchstaben, die Wörter bilden. Intern steht aber jedes Wort für eine Zahl, so dass R mit den Zahlen rechnen kann. Klingt ein wenig kryptisch, aber wir schauen uns einen factor einmal an.\n\ndata_tbl$animal[1:8]\n\n[1] \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"dog\" \"cat\"\n\n\nÜber das $ Symbol kannst du im Kapitel 10.7 mehr erfahren.\nWas haben wir gemacht? Als erstes haben wir die Spalte animal aus dem Datensatz data_tbl mit dem Dollarzeichen $ herausgezogen. Mit dem $ Zeichen können wir uns eine einzelne Spalte aus dem Datensatz data_tbl rausziehen. Du kannst dir das $ wie einen Kleiderbügel und das data_tbl als einen Schrank für Kleiderbügel verstellen. An dem Kleiderbügel hängen dann die einzelnen Zahlen und Worte. Wir nehmen aber nicht den ganzen Vektor sondern nur die Zahlen 1 bis 8, dargestellt durch [1:8]. Die Gänsefüße \" um dog zeigen uns, dass wir hier Wörter oder charactervorliegen haben. Schauen wir auf das Ergebnis, so erhalten wir sieben Mal dog und einmal cat. Insgesamt die ersten acht Einträge der Datentabelle. Wir wollen diesen Vektor uns nun einmal als Faktor anschauen. Wir nutzen die Funktion as_factor().\nÜber Funktionen kannst du im Kapitel 10.4 mehr erfahren.\n\nas.factor(data_tbl$animal[1:8])\n\n[1] dog dog dog dog dog dog dog cat\nLevels: cat dog\n\n\nIm direkten vergleich verschwinden die Gänsefüße \" um dog und zeigen uns, dass wir hier keine character mehr vorliegen haben. Darüber hinaus sehen wir auch, dass die der Faktor jetzt Levels hat. Exakt zwei Stück. Jeweils einen für dog und einen für cat. Wir werden später Faktoren benötigen, wenn wir zum Beispiel eine einfaktorielle ANOVA rechnen. Hier siehst du schon den Begriff Faktor wieder."
  },
  {
    "objectID": "programing-letters-numbers.html#von-wörtern-und-objekten",
    "href": "programing-letters-numbers.html#von-wörtern-und-objekten",
    "title": "9  Buchstaben und Zahlen",
    "section": "\n9.3 Von Wörtern und Objekten",
    "text": "9.3 Von Wörtern und Objekten\nDas mag etwas verwirrend sein, denn es gibt in R Wörter string &lt;str&gt; oder character &lt;chr&gt;. Wörter sind was anderes als Objekte. Streng genommen sind beides Wörter, aber in Objekten werden Dinge gespeichert wohin gegen das Wort einfach ein Wort ist. Deshalb kennezeichnen wir Wörter auch mit Gänsefüßchen als win \"wort\" und zeigen damit, dass es sich hier um einen String handelt.\nWir tippen \"animal\" in R und erhalten \"animal\" als Wort zurück. Das sehen wir auch an dem Ausdruck mit den Gänsefüßchen.\n\n\"animal\"\n\n[1] \"animal\"\n\n\nÜber den Zuweisungspfeil &lt;- kannst du im Kapitel 10.5 mehr erfahren.\nWir tippen animal ohne die Anführungszeichen in R und erhalten den Inhalt von animal ausgegeben. Dafür müssen wir aber das Objekt animal erst einmal über den Zuweisungspfeil &lt;-erschaffen.\n\nanimal &lt;- c(\"dog\", \"cat\", \"fox\")\nanimal\n\n[1] \"dog\" \"cat\" \"fox\"\n\n\nSollte es das Objekt animal nicht geben, also nicht über den Zuweisungspfeil &lt;- erschaffen worden, dann wird eine Fehlermeldung von R ausgegeben:\nFehler in eval(expr, envir, enclos) : Objekt 'animal' nicht gefunden"
  },
  {
    "objectID": "programing-letters-numbers.html#zusammenfassung",
    "href": "programing-letters-numbers.html#zusammenfassung",
    "title": "9  Buchstaben und Zahlen",
    "section": "\n9.4 Zusammenfassung",
    "text": "9.4 Zusammenfassung\nVariablennamen meint hier immer den Namen der Spalte im Datensatz bzw. tibble\nTabelle 9.2 zeigt eine Übersicht wie einzelne Variablennamen und deren zugehörigen Beispielen sowie den Namen in R, der Informatik allgemein, als Skalenniveau und welcher Verteilungsfamilie die Variable angehören würde. Leider ist es so, dass wieder gleiche Dinge unterschiedliche benannt werden. Aber an dieses doppelte Benennen können wir uns in der Statistik schonmal gewöhnen.\n\n\n\nTabelle 9.2— Zusammenfassung und Übersicht von Variablennamen und deren Bennung in R, in der Informatik allgemein, als Skalenniveau und die dazugehörige Verteilungsfamilie.\n\n\n\n\n\n\n\n\n\nVariablenname\nBeispiel\nR\nInfomatik\nSkalenniveau\nVerteilungsfamilie\n\n\n\nweight\n12.3, 12.4, 5.4, 21.3, 13.4\nnumeric\ndouble\ncontinuous\nGaussian\n\n\ncount\n5, 0, 12, 23, 1, 4, 21\ninteger\ninteger\ndiscrete\nPoisson\n\n\ndosis\nlow, mid, high\nordered\n\ncategorical / ordinal\nOrdinal\n\n\nfield\nmainz, berlin, kiel\nfactor\n\ncategorical\nMultinomial\n\n\ncancer\n0, 1\nfactor\n\ndichotomous / binary / nominal\nBinomial\n\n\ntreatment\n“placebo”, “aspirin”\ncharacter\ncharacter/string\ndichotomous / binary / nominal\nBinomial\n\n\nbirth\n2001-12-02, 2005-05-23\ndate"
  },
  {
    "objectID": "programing-basics.html#sec-R-packages",
    "href": "programing-basics.html#sec-R-packages",
    "title": "10  Operatoren, Funktionen und Pakete",
    "section": "\n10.1 Pakete und library()\n",
    "text": "10.1 Pakete und library()\n\n\n\n\n\n\n\nUnterschied von Packages und Libraries in R\n\n\n\nDu findest auf YouTube Einführung in R - Teil 03 - Unterschied Packages und Libraries in R als Video. Hier erkläre ich nochmal den Ablauf zwischen Installieren eines Paketes und dem Laden eines Paketes.\n\n\nAls Vanilla beschreibt man in der Informatikerwelt ein Programm, was keine zusätzlichen Pakete geladen hat. Also die reinst Form ohne zusätzlichen Geschmack.\nIn der Vanilla-Variante hat R sehr wenige Funktionen. Ohne zusätzliche Pakete ist R mehr ein sehr potenter Taschenrechner. Leider mit der Funktionalität aus den 90’zigern, was die Programmierumgebung und die Funktionen angeht. Das wollen wir aber nicht. Wir wollen auf den aktuellen Stand der Technik und auch Sprache programmieren. Daher nutzen wir zusätzliche R Pakete.\n\n\nAbbildung 10.1— Auf den Reiter Packages klicken und dann Install. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\nIn Abbildung 10.1 wird gezeigt wie du ein zusätzliches Paket installieren kannst. Hierbei ist nochmal wichtig den semantischen Unterschied zu wissen. Es gibt das Paket tidyverse was wir viel nutzen. Wir installieren einmalig Pakete der Funktion install.packages() oder eben wie in Abbildung 10.1 gezeigt. Wir nutzen die Funktion library() um ein Paket in R zu laden. Ja, es müsste anders heisen, tut es aber nicht.\n\n## Das Paket tidyverse installieren - einmalig\ninstall.packages(tidyverse)\n\n## Das Paket tidyverse laden - jedes Mal\nlibrary(tidyverse)\n\nNun muss man sich immer merken, ob das Paket schon installiert ist oder man schreibt relativ viele library() untereinander. Das passiert schnell, wenn du viele Pakete laden willst. Dafür erlaubt dir das Paket pacman eine Vereinfachung. Die Funktion p_load() installiert Pakete, wenn die Pakete nicht installiert sind. Sollten die Pakete installiert sein, so werden die Pakete geladen. Du musst nur einmal install.packages(pacman) ausführen um das Paket pacman zu installieren.\n\npacman::p_load(tidyverse, magrittr, readxl)"
  },
  {
    "objectID": "programing-basics.html#anordnung-der-fenster-im-rstudio",
    "href": "programing-basics.html#anordnung-der-fenster-im-rstudio",
    "title": "10  Operatoren, Funktionen und Pakete",
    "section": "\n10.2 Anordnung der Fenster im RStudio",
    "text": "10.2 Anordnung der Fenster im RStudio\nWie dir sicherlich aufgefallen ist, sind in meinen Videos die einzelnen Kacheln im RStudio anders angeordnet. Der Grund ist einfach. Wir sind die meiste Zeit in dem Skript auf der linken Seite und schicken dann den R Code auf die rechte Seite. Normalerweise sind das Skript und die R Console links untereinander angeordnet. Das finde ich aber disfunktional. In Abbildung 10.2 und Abbildung 10.3 kannst du nachvollziehen, wie du die Anordnung der Kacheln im R Studio ändern kannst.\n\n\nAbbildung 10.2— Auf den Reiter Tools klicken und dann Global Options…. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\nAbbildung 10.3— Auf den Reiter Pane Layout klicken und dann die Kacheln so anordnen wie du sie hier siehst. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\nDu kannst vieles in den Global Options… anpassen - unter anderem auch das Aussehen (eng. Appearance)."
  },
  {
    "objectID": "programing-basics.html#sec-R-vector",
    "href": "programing-basics.html#sec-R-vector",
    "title": "10  Operatoren, Funktionen und Pakete",
    "section": "\n10.3 Einen Vektor bauen c()\n",
    "text": "10.3 Einen Vektor bauen c()\n\nWir können mit der Funktion c() Zahlen und Wörter zu einem Vektor kombinieren.\n\nc(\"dog\", \"dog\", \"cat\", \"cat\", \"fox\", \"fox\")\n\n[1] \"dog\" \"dog\" \"cat\" \"cat\" \"fox\" \"fox\"\n\n\nHier werden die Wörter “dog”, “cat” und “fox” miteinader in einen Vektor kombiniert. Wir erinnern uns an das $ Zeichen, was uns erlaubt eine Variable als Vektor aus einem tibble()herauszuziehen.\nWir können auch Zahlen zusammenbauen oder aber ganze Bereiche mit dem : definieren. Wir lesen den : als “von bis”.\n\nc(1, 8, 4, 5)\n\n[1] 1 8 4 5\n\n\nDie Zahlen von 1 bis 5 werden durch den : ausgegeben.\n\nc(1:5)\n\n[1] 1 2 3 4 5"
  },
  {
    "objectID": "programing-basics.html#sec-R-function",
    "href": "programing-basics.html#sec-R-function",
    "title": "10  Operatoren, Funktionen und Pakete",
    "section": "\n10.4 Funktionen",
    "text": "10.4 Funktionen\nWir haben schon einige Funktion nebenbei in R kennengelernt. Zum einen as.factor() um einen Faktor zu erstellen oder aus dem Kapitel 10.1, wo wir die Funktion install.packages() nutzen um ein Paket zu installieren oder aber die Funktion library() um ein Paket in R zu laden.\nFunktionen sehen aus wie Wörter. Haben aber keine Gänsefüßchen und beinhalten auch keine Daten oder Vektoren. Funktionen können mit Daten und Vektoren rechnen und geben das Berechnete dann wieder. Nehmen wir als Beispiel die Funktion mean(), die den Mittelwert von einer Reihe Zahlen berechnet.\n\ny &lt;- c(1.2, 3.4, 2.1, 6, 4.3)\nmean(y)\n\n[1] 3.4\n\n\nEigentlich müssen in der Programmierung Objekte erst deklariert und somit erschaffen werden. Erst dann können Objekte initalisiert und somit befüllt bzw. etwas zugewiesen werden.\nWir sehen, dass wir mit der Funktion c() die Zahlen \\(1.2, 3.4, 2.1, 6, 4.3\\) zusammenkleben. Danach speichern wir die Zahlen in den Objekt y als einen Vektor ab. Wir müssen y nicht erst erschaffen, das Erschaffen und Speichern passiert in R in einem Schritt. Wir stecken nun den Vektor y in die Funktion mean() und erhalten den Mittelwert von \\(3.4\\) der Zahlen wiedergegeben.\nWir können auch eigene Funktionen mit dem Befehl function(){} erstellen. Du siehst schon den Unterschied, es sind hier zwei unterschiedlich Klammern. Schauen wir uns die Anwendung einmal im Beispiel an. Im ersten Fall bauen wir uns eine Funktion, die eigentlich nur die Aufgabe hat mehre Zeilen Code zusammenzufassen. Wir müssen die drei ... Punkte in die Funktion schreiben, da wir der Funktion nichts an Werten übergeben. Wenn wir my_mean() ausführen, erhalten wir immer den gleichen Mittelwert wieder.\n\n\nDie Funktion divide_by() ist ein Alias für /. Siehe dazu auch Aliases in magrittr.\n\nmy_mean &lt;- function(...){\n   res &lt;- c(1.2, 3.4, 2.1, 6, 4.3) %&gt;% sum() %&gt;% divide_by(5) \n   return(res)\n}\n\nmy_mean()\n\n[1] 3.4\n\n\nIn dem vorherigen Fall können wir die Funktion my_mean() zwar wiederholt nutzen, aber es wäre schon besser, wenn wir einen beliebigen Zahlenvektor y in die Funktion stecken könnten und dann den Mittelwert berechnet bekommen. Wir bauen uns also eigentlich die in R schon existierende Funktion mean() nach.\n\nmy_mean &lt;- function(y){\n   res &lt;- sum(y) / length(y)\n   return(res)\n}\n\nmy_mean(y = c(1.2, 3.4, 2.1, 6, 4.3))\n\n[1] 3.4\n\n\nWichtig ist, dass jeder Funktionsblock {} mit einem return() endet in dem das Objekt steht, was von der Finktion zurückgegeben werden soll."
  },
  {
    "objectID": "programing-basics.html#sec-R-pfeil",
    "href": "programing-basics.html#sec-R-pfeil",
    "title": "10  Operatoren, Funktionen und Pakete",
    "section": "\n10.5 Zuweisungspfeil <-\n",
    "text": "10.5 Zuweisungspfeil &lt;-\n\nMit dem Zuweisungspfeil speichern wir Dinge in Objekte. Das heißt wir speichern damit intern in R Datensätze und viele andere Sachen, die wir dan später wieder verwenden wollen. Schauen wir uns das einmal im Beispiel an. Schrieben wir nur den Vektor c() mit Hunden und Katzen darin, so erscheint eine Ausgabe in R.\n\nc(\"dog\", \"dog\", \"cat\", \"cat\", \"fox\", \"fox\")\n\n[1] \"dog\" \"dog\" \"cat\" \"cat\" \"fox\" \"fox\"\n\n\nSchreiben wir den gleichen Vektor und nutzen den Zuweisungspfeil, dann wird der Vektor in dem Objekt animal gespeichert. Wenn du Strg Enter drückst, dann erstellt das RStudio automatisch den Zuweisungspfeil &lt;-.\n\nanimal &lt;- c(\"dog\", \"dog\", \"cat\", \"cat\", \"fox\", \"fox\")\n\nWie kommen wir jetzt an die Sachen, die in animal drin sind? Wir können einfach animal in R schreiben und dann wird uns der Inhalt von animal ausgegeben.\n\nanimal\n\n[1] \"dog\" \"dog\" \"cat\" \"cat\" \"fox\" \"fox\"\n\n\nDer Zuweisungspfeil &lt;- ist zentral für die Nutzung von R. Wenn du Strg Enter drückst, dann erstellt das RStudio automatisch den Zuweisungspfeil &lt;-.\nWir nutzen den Zuweisungspfeil &lt;- ist zentral für die Nutzung von R. Wir brauchen den Zuweisungspfeil &lt;- um Objekte in R zu erschaffen und Ergebnisse intern abzuspeichern. Zusammen mit Funktionen nutzen wir nur noch die Pipe %&gt;% öfter."
  },
  {
    "objectID": "programing-basics.html#sec-R-pipe",
    "href": "programing-basics.html#sec-R-pipe",
    "title": "10  Operatoren, Funktionen und Pakete",
    "section": "\n10.6 Pipe %>%\n",
    "text": "10.6 Pipe %&gt;%\n\n\n\n\n\n\n\nPipes in R\n\n\n\nDu findest auf YouTube Einführung in R - Teil 11 - Pipes in R als Video. Hier erkläre ich den Zusammenhang nochmal in einem Video.\n\n\nIm Weiteren nutzen wir den Pipe Operator dargestellt als %&gt;%. Du kannst dir den Pipe Operator als eine Art Röhre vorstellen in dem die Daten verändert werden und dann an die nächste Funktion weitergeleitet werden. Im folgenden siehst du viele Funktionen, die aneinander über Objekte miteinander verbunden werden. Im Kapitel 12 erfährst du mehr über die Funktionen select()und filter().\n\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\nanimal_1_tbl &lt;- select(data_tbl, animal, jump_length)\nanimal_2_tbl &lt;- filter(animal_1_tbl, jump_length &gt;= 4)\nsort(animal_2_tbl$jump_length)\n\n [1]  4.1  4.3  5.4  5.6  5.7  6.1  7.6  7.9  8.2  8.9  9.1 11.8\n\ndata_tbl %&gt;% \n  select(animal, jump_length) %&gt;% \n  filter(jump_length &gt;= 4) %&gt;% \n  pull(jump_length) %&gt;% \n  sort\n\n [1]  4.1  4.3  5.4  5.6  5.7  6.1  7.6  7.9  8.2  8.9  9.1 11.8\n\n\nIm unteren Beispiel siehst du die Nutzung des Pipe Operators %&gt;%. Das Ergebnis ist das gleiche, aber der Code ist einfacher zu lesen. Wir nehmen den Datensatz data_tbl leiten den Datensatz in den Funktion select() und wählen die Spalten animal sowie jump_length. Dann filtern wir noch nach jump_lengthgrößer als 4 cm. Dann ziehen wir uns mit der Funktion pull() die Spalte jump_length aus dem Datensatz. Den Vektor leiten wir dann weiter in die Funktion sort() und erhalten die sortierten Sprunglängen zurück.\nIn Abbildung 10.4 und Abbildung 10.5 sehen wir, wie wir den Shortcut für das Erstellen des Pipe Operators umdefinieren. Danach können wir einfach den Shortcut nutzen und müssen nicht immer händisch den Pipe Operator eingeben.\n\n\nAbbildung 10.4— Auf den Reiter Modify Keyboard Shortcuts klicken. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\nAbbildung 10.5— Im Suchfeld pipe eingeben und dann in das Feld mit dem Shortcut klicken. Danach Alt und . klicken. Danach wird der Pipe Operator mit dem Shortcut Alt . gesetzt. In der deutschen Version vom RStudio mögen die Begriffe leicht anders sein.\n\nWas gibt es noch an Pipes? Wir haben noch den %$%-Pipe oder auch Dollar-Pipe genannt. Hier können wir dann die Namen der Spalten weiterleiten.\n\ndata_tbl %$%\n  cor(jump_length, flea_count)\n\n[1] 0.2952872\n\n\nOder wir nutzen die %&lt;&gt;%-Pipe oder Backward-Pipe. Hier können wir uns dann auch den Zuweisungspfeil sparen. Ja, das ist schon eine Freude.\n\ndata_tbl %&lt;&gt;%\n  mutate(animal = as.factor(animal))\n\ndata_tbl %&gt;% head(n = 2)\n\n# A tibble: 2 × 5\n  animal jump_length flea_count grade infected\n  &lt;fct&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dog            5.7         18     8        0\n2 dog            8.9         22     8        1"
  },
  {
    "objectID": "programing-basics.html#sec-dollar",
    "href": "programing-basics.html#sec-dollar",
    "title": "10  Operatoren, Funktionen und Pakete",
    "section": "\n10.7 Spalte extrahieren $\n",
    "text": "10.7 Spalte extrahieren $\n\nWir nutzen eigentlich die Funktion pull() um eine Spalte bzw. Vektor aus einem Datensatz zu extrahieren.\n\ndata_tbl %&gt;% \n  pull(animal)\n\n [1] dog dog dog dog dog dog dog cat cat cat cat cat cat cat\nLevels: cat dog\n\n\nManche Funktionen in R, besonders die älteren Funktionen, benötigen keinen Datensatz sondern meist zwei bis drei Vektoren. Das heißt, wir können nicht einfach einen Datensatz in eine Funktion über data = data_tbl stecken sondern müssen der Funktion Vektoren übergeben. Dafür nutzen wir den $ Operator.\n\ndata_tbl$animal\n\n [1] dog dog dog dog dog dog dog cat cat cat cat cat cat cat\nLevels: cat dog\n\ndata_tbl$jump_length\n\n [1]  5.7  8.9 11.8  8.2  5.6  9.1  7.6  3.2  2.2  5.4  4.1  4.3  7.9  6.1\n\n\nWir werden versuchen diese Schreibweise zu vermeiden, aber manchmal ist es sehr nützlich die Möglichkeit zu haben auf diese Weise eine Spalte zu extrahieren."
  },
  {
    "objectID": "programing-basics.html#sec-formula",
    "href": "programing-basics.html#sec-formula",
    "title": "10  Operatoren, Funktionen und Pakete",
    "section": "\n10.8 Modelle definieren mit formula\n",
    "text": "10.8 Modelle definieren mit formula\n\nWir müssen später Modelle in R definieren um zum Beispiel den t Test oder aber eine lineare Regression rechnen zu können. Wir nutzen dazu in R die formula Syntax. Das heißt links von der Tilde ~ steht das \\(y\\), also der Spaltenname aus dem Datensatz data = den wir nutzen, der das Outcome repräsentiert. Rechts von der Tilde ~ stehen alle \\(x_1, ..., x_p\\), also alle Spalten aus dem Datensatz data = den wir nutzen, der die Einflussfaktoren repräsentiert.\nIn unserem Beispiel mit den Hunde- und Katzenflöhen aus Kapitel 4.2 wäre das \\(y\\) die Spalte jump_length und das \\(x\\) der Faktor animal. Wir erstellen mit der Funktion formula() das Modell in R. Wir brauchen später die Funktion formula nur implizit, aber hier ist es gut, das du einmal siehst, wie so eine Formula in R aussieht.\n\nformula(jump_length ~ animal)\n\njump_length ~ animal\n\n\nWenn die Formel sehr lang wird bzw. wir die Namen der Spalten aus anderen Funktionen haben, können wir auch die Funktion reformulate() nutzen. Wir brauchen die Funktion aber eher im Bereich des maschinellen Lernens. Hier ist die Funktion reformulate() aufgeführt, da es inhaltlich passt.\n\nreformulate(termlabels = c(\"animal\", \"sex\", \"site\"),\n            response = \"jump_length\",\n            intercept = TRUE)\n\njump_length ~ animal + sex + site"
  },
  {
    "objectID": "programing-basics.html#sec-R-help",
    "href": "programing-basics.html#sec-R-help",
    "title": "10  Operatoren, Funktionen und Pakete",
    "section": "\n10.9 Hilfe mit ?\n",
    "text": "10.9 Hilfe mit ?\n\nDas Fragezeichen ? vor einem Funktionsnamen erlaubt die Hilfeseite zu öffnen. Die Hilfsseiten findest du auch in einem der Reiter im RStudio.\n\n\nAbbildung 10.6— Neben den Paketen in R findet sich auch der Reiter Help, wo du Hilfe für die einzelnen Funktionen findets.."
  },
  {
    "objectID": "programing-import.html#genutzte-r-pakete-für-das-kapitel",
    "href": "programing-import.html#genutzte-r-pakete-für-das-kapitel",
    "title": "11  Daten einlesen",
    "section": "\n11.1 Genutzte R Pakete für das Kapitel",
    "text": "11.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, janitor)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "programing-import.html#sec-format",
    "href": "programing-import.html#sec-format",
    "title": "11  Daten einlesen",
    "section": "\n11.2 Dateiformat",
    "text": "11.2 Dateiformat\n\n\nDas Buch Cookbook for R stellt auch Beispiele für die Funktion gather() zu Verfügung für die Umwandlung von Wide zu Long Format: Converting data between wide and long format\nWir unterschieden bei Datenformaten zwischen den Wide Format und dem Long Format. Meistens gibst du die Daten intuitv im Wide Format in Excel ein. Das ist in Excel auch übersichtlicher. R und später die Funktion ggplot() zur Visualisierung der Daten kann aber nur mit dem Long Format arbeiten. Wir können aber mit der Funktion gather() das Wide Format in das Long Format umwandeln.\n\n11.2.1 Wide Format\nIn Tabelle 11.1 sehen wir eine typische Datentabelle in einem Wide Format. Die Spalten egeben jeweils die Tierart wieder und die Einträge in den Spalten sind die Sprungweiten in [cm].\n\n\nTabelle 11.1— Eine Datentabelle mit Sprungweiten in [cm] von Hunde- und Katzenflöhen im Wide Format.\n\ndog\ncat\n\n\n\n5.2\n10.1\n\n\n4.9\n9.4\n\n\n12.1\n11.8\n\n\n8.2\n6.7\n\n\n5.6\n8.2\n\n\n9.1\n9.1\n\n\n7.4\n7.1\n\n\n\n\nWir können diese Datentablle auch in R erstellen und uns als tibble() wiedergeben lassen.\n\njump_wide_tbl &lt;- tibble(dog = c(5.2, 4.9, 12.1, 8.2, 5.6, 9.1, 7.4),\n                        cat = c(10.1, 9.4, 11.8, 6.7, 8.2, 9.1, 7.1))\njump_wide_tbl\n\n# A tibble: 7 × 2\n    dog   cat\n  &lt;dbl&gt; &lt;dbl&gt;\n1   5.2  10.1\n2   4.9   9.4\n3  12.1  11.8\n4   8.2   6.7\n5   5.6   8.2\n6   9.1   9.1\n7   7.4   7.1\n\n\nWenn du schon Daten hast, dann macht es eventuell mehr Sinn eine neue Exceldatei anzulegen in der du dann die Daten in das Long Format kopierst.\nWir können aber mit einem Wide-Format nicht mit ggplot() die Daten aus der Tabelle 11.1 visualisieren. Deshalb müssen wir entweder das Wide Format in das Long Format umwandeln oder die Daten gleich in Excel im Long Format erstellen.\n\n11.2.2 Long Format\nWenn du Daten erstellst ist es wichtig, dass du die Daten in Excel im Long-Format erstellst. Dabei muss eine Beobachtung eine Zeile sein. Du siehst in Abbildung 11.1 ein Beispiel für eine Tabelle in Excel, die dem Long Format folgt.\n\n\nAbbildung 11.1— Beispiel für eine Exceldatentabelle in Long Format.\n\nIm Folgenden sehen wir einmal wie die Funktion gather() das tibble() in Wide Format in ein tibble() in Long Format umwandelt. Wir müssen dafür noch die Spalte benennen mit der Option key = in die die Namen der Spalten aus dem Wide Format geschrieben werden sowie den Spaltennamen für die eigentlichen Messwerte mit der Option value =.\n\njump_tbl &lt;- tibble(dog = c(5.2, 4.9, 12.1, 8.2, 5.6, 9.1, 7.4),\n                   cat = c(10.1, 9.4, 11.8, 6.7, 8.2, 9.1, 7.1)) %&gt;%\n  gather(key = \"animal\", value = \"jump_length\")\njump_tbl\n\n# A tibble: 14 × 2\n   animal jump_length\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 dog            5.2\n 2 dog            4.9\n 3 dog           12.1\n 4 dog            8.2\n 5 dog            5.6\n 6 dog            9.1\n 7 dog            7.4\n 8 cat           10.1\n 9 cat            9.4\n10 cat           11.8\n11 cat            6.7\n12 cat            8.2\n13 cat            9.1\n14 cat            7.1\n\n\nWir sehen, dass ein Long Format viel mehr Paltz benötigt. Das ist aber in R kein Problem. Wir sehen die Daten kaum sondern nutzen Funktionen wie ggplot() um die Daten zu visualisieren. Wichtig ist, dass du die Daten in Excel sauber abgelegt hast."
  },
  {
    "objectID": "programing-import.html#beispiel-in-excel",
    "href": "programing-import.html#beispiel-in-excel",
    "title": "11  Daten einlesen",
    "section": "\n11.3 Beispiel in Excel…",
    "text": "11.3 Beispiel in Excel…\nSchauen wir uns den Fall nochmal als Beispiel in einer Exceldatei an. Du findest die Beispieldatei germination_data.xlsx auf GitHub zum Herunterladen. Eventuell muss du bei dir den Pfad ändern oder aber die Importfunktion des RStudios nutzen. Dafür siehe einfach den nächsten Abschnitt.\nWir haben in der Beispieldatei germination_data.xlsx zum einen Messwiederholungen, gekenntzeichnet durch die Spalten t1 bis t4 sowie einmal gemessene Spalten wie freshmatter, drymatter, count_small_leaf und count_large_leaf.\n\n11.3.1 …ohne Messwiederholung\nWir schauen uns erstmal die Spalten ohne Messwiederholung an. Wenn du also keine Messwiederholungen hast, also die hast das Frischegewicht nur einmal an einer Pflanze gemessen, dann sieht deine Datei so aus wie in Abbildung 11.2. Ich zeige hier nur die Spalten A und F bis I aus der Datei germination_data.xlsx.\n\n\nAbbildung 11.2— Beispiel für eine Exceldatentabelle ohne Messwiederholungen.\n\nWir können dann die Datei auch über die Funktion read_excel() einlesen. Ich nutze noch die Funktion select() um die Spalten auszuwählen, die wir auch oben in der Abbildung sehen. Durch den Doppelpunkt : kann ich zusammenhängende Spalten auswählen und muss die Namen nicht einzeln eingeben.\n\nread_excel(\"data/germination_data.xlsx\") %&gt;% \n  select(treatment, freshmatter:count_large_leaf)\n\n# A tibble: 20 × 5\n   treatment freshmatter drymatter count_small_leaf count_large_leaf\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 control          13.7      0.98                2               12\n 2 control          18.1      1.31                4               12\n 3 control          14.4      1.01                4               12\n 4 control          10.7      0.74                2               12\n 5 control          15.9      1.11                2               12\n 6 low              26.4      1.9                10               18\n 7 low              24.3      1.68                8               14\n 8 low              27.1      1.87                6               18\n 9 low              27.2      1.8                 4               18\n10 low              18.2      1.22                6               18\n11 mid              20.9      1.31                6                8\n12 mid              19.4      1.41                4                8\n13 mid              21.5      1.44                4               14\n14 mid              24        1.56                6               11\n15 mid              25.8      1.77                6               11\n16 high             30.9      2.22                6               24\n17 high             36.3      2.52                2               19\n18 high             25.6      1.82                4               22\n19 high             33.3      2.27                6               22\n20 high             30.4      2.14                6               22\n\n\nWir könnten jetzt die Ausgabe auch in ein Objekt schreiben und dann mit der eingelesenen Datei weiterarbeiten.\n\n11.3.2 … mit Messwiederholung\nIn Abbildung 11.3 siehst du ein Datenbeispiel für eine Behandlung mit Messwiederholungen. Das heist wir haben immer noch eine Pflanze pro Zeile, aber die Pflanze wurde zu den Zeitpunkten t1 bis t4 gemessen. In dieser Form ist es viel einfacher aufzuschreiben, aber wir brauchen einen Faktor time_point mit vier Leveln t1 bis t4.\n\n\nAbbildung 11.3— Beispiel für eine Exceldatentabelle mit Messwiederholungen.\n\nWir nutzen die Funktion pivot_longer() um die Spalten t1 bis t4 zusammenzufassen und untereinander zu kleben. Die Spalte treatment wird dann einfach viermal wiederholt. Wir müssen dann noch die Spalte für den Faktor benennen und die Spalte für die eigentlichen Messwerte. Beides machen wir einmal über die Option names_to = und values_to =. Wir haben dann im Anschluss einen Datensatz im Long-Format mit dem wir dann weiterarbeiten können.\n\nread_excel(\"data/germination_data.xlsx\") %&gt;% \n  select(treatment, t1:t4) %&gt;% \n  pivot_longer(cols = t1:t4, \n               names_to = \"time_point\", \n               values_to = \"weight\")\n\n# A tibble: 80 × 3\n   treatment time_point weight\n   &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n 1 control   t1             16\n 2 control   t2             21\n 3 control   t3             23\n 4 control   t4             23\n 5 control   t1             17\n 6 control   t2             19\n 7 control   t3             18\n 8 control   t4             24\n 9 control   t1             16\n10 control   t2             22\n# ℹ 70 more rows\n\n\nWir könnten jetzt die Ausgabe auch in ein Objekt schreiben und dann mit der eingelesenen Datei weiterarbeiten.\n\n11.3.3 … mit mehreren Tabellenblättern\nWenn du eine Datei mit mehreren Tabellenblättern hast, dann geht das Einlesen der Datei auch, aber dann müssen die Tabellenblätter wirklich alle für R einlesbar sein. Das heist keine Leerzeilen oder andere Dinge, die stören könnten. Als erstes musst du den Pfad zu deiner Datei angeben. Das kann ganz einfach sein, wenn die Datei in dem gleichen Ordner wie dein R Skript liegt. Dann ist der Pfad wirklich nur ein Punkt in Anführungszeichen path &lt;- \".\". Sicher ist natürlich du gibst den Pfad absolut ein. Hier einmal wie der Pfad in meinem Fall aussehen würde.\n\npath &lt;- file.path(\"data/multiple_sheets.xlsx\")\n\npath\n\n[1] \"data/multiple_sheets.xlsx\"\n\n\nWir können dann den Pfad zu der Ecxeldatei an die Funktion excel_sheets() pipen, die alle Tabellenblätter in der Datei findet. Dann müssen wir noch die Funktion set_names() verwenden um die Namen der Tabellenblätter zu recyclen. Abschließend können wir alle Exceltabellenblätter in eine Liste laden.\n\ndata_lst &lt;- path %&gt;% \n  excel_sheets() %&gt;% \n  rlang::set_names() %&gt;% \n  map(read_excel, path = path)\n\ndata_lst\n\n$day_1\n# A tibble: 9 × 3\n  animal jump_length infected\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 dog            8.9        1\n2 dog           11.8        1\n3 dog            8.2        0\n4 cat            4.3        1\n5 cat            7.9        0\n6 cat            6.1        0\n7 fox            7.7        1\n8 fox            8.1        1\n9 fox            9.1        1\n\n$day_3\n# A tibble: 8 × 5\n  animal jump_length flea_count grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 cat            3.2         12     7        1\n2 cat            2.2         13     5        0\n3 cat            5.4         11     7        0\n4 cat            4.1         12     6        0\n5 fox            9.7         12     5        1\n6 fox           10.6         28     4        0\n7 fox            8.6         18     4        1\n8 fox           10.3         19     3        0\n\n$day12\n# A tibble: 4 × 3\n  animal jump_length flea_count\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 dog            5.7         18\n2 dog            8.9         22\n3 dog           11.8         17\n4 dog            8.2         12\n\n\nDas ist ja schonmal gut, aber wie kommen wir jetzt an die einzelnen Tabellenblätter ran? Dafür gibt es die Funktion pluck(), die es erlaubt aus einer Liste nach Namen oder Position das Tabellenblatt herauszuziehen. Wir können dann das Tabellenblatt wieder in einem Objekt speichern und dann weiter mit den Daten arbeiten.\n\npluck(data_lst, \"day_3\")\n\n# A tibble: 8 × 5\n  animal jump_length flea_count grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 cat            3.2         12     7        1\n2 cat            2.2         13     5        0\n3 cat            5.4         11     7        0\n4 cat            4.1         12     6        0\n5 fox            9.7         12     5        1\n6 fox           10.6         28     4        0\n7 fox            8.6         18     4        1\n8 fox           10.3         19     3        0\n\n\n\npluck(data_lst, 2)\n\n# A tibble: 8 × 5\n  animal jump_length flea_count grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 cat            3.2         12     7        1\n2 cat            2.2         13     5        0\n3 cat            5.4         11     7        0\n4 cat            4.1         12     6        0\n5 fox            9.7         12     5        1\n6 fox           10.6         28     4        0\n7 fox            8.6         18     4        1\n8 fox           10.3         19     3        0"
  },
  {
    "objectID": "programing-import.html#importieren-mit-rstudio",
    "href": "programing-import.html#importieren-mit-rstudio",
    "title": "11  Daten einlesen",
    "section": "\n11.4 Importieren mit RStudio",
    "text": "11.4 Importieren mit RStudio\nWir können das RStudio nutzen um Daten mit Point-and-Klick rein zuladen und dann den Code wieder in den Editor kopieren. Im Prinzip ist dieser Weg der einfachste um einmal zu sehen, wie ein pfad funktioniert und der Code lautet. Später benötigt man diese ‘Krücke’ nicht mehr. Wir nutzen dann direkt den Pfad zu der Datei. Abbildung 11.4 zeigt einen Ausschnitt, wo wir im RStudio die Import Dataset Funktionalität finden.\n\n\nAbbildung 11.4— Auf den Reiter Einviroment klicken und dann Import Dataset. In der deutschen version vom RStudio mögen die Begriffe leicht anders sein.\n\n\n\n\n\n\n\nImportieren mit RStudio als Video\n\n\n\nDu findest auf YouTube Einführung in R - Teil 21.0 - Daten importieren mit RStudio - Point and Klick als Video. Point and Klick ist als Video einfacher nachzuvollziehen als Screenshots in einem Fließtext."
  },
  {
    "objectID": "programing-import.html#sec-pfad",
    "href": "programing-import.html#sec-pfad",
    "title": "11  Daten einlesen",
    "section": "\n11.5 Importieren per Pfad",
    "text": "11.5 Importieren per Pfad\nIn Abbildung 11.5 können wir sehen wie wir den Pfad zu unserer Excel Datei flea_dog_cat.xlsx finden. Natürlich kannst du den Pfad auch anders herausfinden bzw. aus dem Explorer oder Finder kopieren.\n\n\nAbbildung 11.5— Durch den Rechts-Klick auf die Eigenschaften einer Datei kann man sich den Pfad zur Datei anzeigen lassen. Achtung! Unter Windows muss der Slash \\ noch in den Backslash / gedreht werden.\n\nNachdem wir den Pfad gefunden haben, können wir den Pfad in die Funktion read_excel() kopieren und die Datei in das Objekt data_tbl einlesen. Ja, es wird nichts in der R Console ausgegeben, da sich die Daten jetzt in dem Object data_tbl befinden.\n\n## Ganzer Pfad zur Datei flea_dog_cat.xlsx\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n\n\n\n\n\n\n\nUnterschied zwischen \\ in Windows und / in R\n\n\n\nAchte einmal auf den Slash im Pfad in R und einem im Pfsd in Windows. Einmal ist es der Slash \\ im Dateipfad und einmal der Backslash /. Das ist sehr ärgerlich, aber dieses Problem geht zurück in die 80’ziger. Bill hat entschieden für sein Windows / zu nutzen und Steve (und Unix) eben /. Und mit dieser Entscheidung müssen wir jetzt leben…"
  },
  {
    "objectID": "programing-import.html#sec-umlaute",
    "href": "programing-import.html#sec-umlaute",
    "title": "11  Daten einlesen",
    "section": "\n11.6 Auf ein englisches Wort in Dateien",
    "text": "11.6 Auf ein englisches Wort in Dateien\nEin großes Problem in Datein sind Umlaute (ä,ö,ü) oder aber andere (Sonder)zeichen (ß, ?, oder #). Als dies sollte vermieden werden. Eine gute Datei für R beinhaltet nur ganze Wörter, Zahlen oder aber leere Felder. Ein leeres Feld ist ein fehlender Wert. Abbildung 11.6 zeigt eine gute Exceldatentablle. Wir schreiben jump_length mit Unterstrich um den Namen besser zu lesen zu können. Sonst ist auch alles in Englisch geschrieben. Wir vermeiden durch die neglische Schreibweise aus versehen einen Umlaut oder anderweitig problematische Zeichen zu verwenden. Später können wir alles noch für Abbildungen anpassen.\n\n\nAbbildung 11.6— Beispiel für eine gute (Excel)Datentabelle. Keine Umlaute sind vorhanden und die Spaltennamen haben keine Leerzeichen oder Sonderzeichen."
  },
  {
    "objectID": "programing-import.html#sec-spalten",
    "href": "programing-import.html#sec-spalten",
    "title": "11  Daten einlesen",
    "section": "\n11.7 Spaltennamen in der (Excel)-Datei",
    "text": "11.7 Spaltennamen in der (Excel)-Datei\nDie Funktion clean_names() aus dem R Paket janitor erlaubt es die Spaltennamen einer eingelesenen Datei in eine für R gute Form zu bringen.\n\nKeine Leerzeichen in den Spaltennamen.\nAlle Spaltennamen sind klein geschrieben.\n\n\ndata_tbl %&gt;% \n  clean_names()\n\n# A tibble: 14 × 5\n   animal jump_length flea_count grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            5.7         18     8        0\n 2 dog            8.9         22     8        1\n 3 dog           11.8         17     6        1\n 4 dog            8.2         12     8        0\n 5 dog            5.6         23     7        1\n 6 dog            9.1         18     7        0\n 7 dog            7.6         21     9        0\n 8 cat            3.2         12     7        1\n 9 cat            2.2         13     5        0\n10 cat            5.4         11     7        0\n11 cat            4.1         12     6        0\n12 cat            4.3         16     6        1\n13 cat            7.9          9     6        0\n14 cat            6.1          7     5        0\n\n\n\n\n\n\n\n\nR Code des Kapitels\n\n\n\n\n\n\npacman::p_load(tidyverse, readxl, magrittr, janitor,\n               conflicted)\nconflict_prefer(\"set_names\", \"rlang\")\n\njump_wide_tbl &lt;- tibble(dog = c(5.2, 4.9, 12.1, 8.2, 5.6, 9.1, 7.4),\n                        cat = c(10.1, 9.4, 11.8, 6.7, 8.2, 9.1, 7.1))\n\nread_excel(\"data/germination_data.xlsx\") %&gt;% \n  select(treatment, freshmatter:count_large_leaf)\n\nread_excel(\"data/germination_data.xlsx\") %&gt;% \n  select(treatment, t1:t4) %&gt;% \n  pivot_longer(cols = t1:t4, \n               names_to = \"time_point\", \n               values_to = \"weight\")\n\npath &lt;- file.path(\"data/multiple_sheets.xlsx\")\n\ndata_lst &lt;- path %&gt;% \n  excel_sheets() %&gt;% \n  rlang::set_names() %&gt;% \n  map(read_excel, path = path)\n\npluck(data_lst, \"day_3\")\n\npluck(data_lst, 2)\n\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")"
  },
  {
    "objectID": "programing-dplyr.html#genutzte-r-pakete",
    "href": "programing-dplyr.html#genutzte-r-pakete",
    "title": "12  Daten bearbeiten",
    "section": "\n12.1 Genutzte R Pakete",
    "text": "12.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, readxl, magrittr, janitor)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "programing-dplyr.html#spalten-wählen-mit-select",
    "href": "programing-dplyr.html#spalten-wählen-mit-select",
    "title": "12  Daten bearbeiten",
    "section": "\n12.2 Spalten wählen mit select()\n",
    "text": "12.2 Spalten wählen mit select()\n\n\n\n\n\n\n\nYouTube - Spalten auswählen mit select()\n\n\n\nDu findest auf YouTube Einführung in R - Teil 12 - Spalten auswählen mit select() als Video zum nochmal anschauen.\n\n\nWir nutzen die Funktion select()um Spalten zu wählen.\nDer Datensatz, den wir im Experiment erschaffen, ist meist riesig. Jetzt könnten wir natürlich eine Exceltabelle mit unterschiedlichen Sheets bzw. Reitern erstellen oder aber die Spalten die wir brauchen in R selektieren. Wir nutzen die Funktion select()um Spalten zu wählen. Im folgenden Codeblock wählen wir die Spalten animal, jump_length und flea_count.\n\ndata_tbl %&gt;% \n  select(animal, jump_length, flea_count)\n\n# A tibble: 21 × 3\n   animal jump_length flea_count\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 dog            5.7         18\n 2 dog            8.9         22\n 3 dog           11.8         17\n 4 dog            8.2         12\n 5 dog            5.6         23\n 6 dog            9.1         18\n 7 dog            7.6         21\n 8 cat            3.2         12\n 9 cat            2.2         13\n10 cat            5.4         11\n# ℹ 11 more rows\n\n\nWir können die Spalten beim selektieren auch umbenennen und in eine andere Reihenfolge bringen.\n\ndata_tbl %&gt;% \n  select(Sprungweite = jump_length, flea_count, animal)\n\n# A tibble: 21 × 3\n   Sprungweite flea_count animal\n         &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; \n 1         5.7         18 dog   \n 2         8.9         22 dog   \n 3        11.8         17 dog   \n 4         8.2         12 dog   \n 5         5.6         23 dog   \n 6         9.1         18 dog   \n 7         7.6         21 dog   \n 8         3.2         12 cat   \n 9         2.2         13 cat   \n10         5.4         11 cat   \n# ℹ 11 more rows\n\n\nDu findest auf der englischen Hilfeseite für select() noch weitere Beispiele für die Nutzung."
  },
  {
    "objectID": "programing-dplyr.html#zeilen-wählen-mit-filter",
    "href": "programing-dplyr.html#zeilen-wählen-mit-filter",
    "title": "12  Daten bearbeiten",
    "section": "\n12.3 Zeilen wählen mit filter()\n",
    "text": "12.3 Zeilen wählen mit filter()\n\n\n\n\n\n\n\nYouTube - Zeilen auswählen mit filter()\n\n\n\nDu findest auf YouTube Einführung in R - Teil 13 - Zeilen auswählen mit filter() als Video zum nochmal anschauen.\n\n\nWir nutzen die Funktion filter() um Zeilen nach Kriterien zu wählen.\nWährend wir die Auswahl an Spalten gut und gerne auch in Excel durchführen können, so ist dies bei der Auswahl der Zeilen nicht so einfach. Wir können in R hier auf die Funktion filter() zurückgreifen. Wir nutzen die Funktion filter() um Zeilen nach Kriterien zu wählen.\nIm folgenden Codeblock wählen wir die Zeilen aus in denen die Worte dog und fox stehen. Wir nutzen dazu den Operator %in% um auszudrücken, dass wir alle Einträge in der Spalte animal wollen die in dem Vektor c(\"dog\", \"fox\") beschrieben sind.\n\ndata_tbl %&gt;% \n  filter(animal %in% c(\"dog\", \"fox\"))\n\n# A tibble: 14 × 5\n   animal jump_length flea_count grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            5.7         18     8        0\n 2 dog            8.9         22     8        1\n 3 dog           11.8         17     6        1\n 4 dog            8.2         12     8        0\n 5 dog            5.6         23     7        1\n 6 dog            9.1         18     7        0\n 7 dog            7.6         21     9        0\n 8 fox            7.7         21     5        1\n 9 fox            8.1         25     4        1\n10 fox            9.1         31     4        1\n11 fox            9.7         12     5        1\n12 fox           10.6         28     4        0\n13 fox            8.6         18     4        1\n14 fox           10.3         19     3        0\n\n\nEs stehen dir Folgende logische Operatoren zu Verfügung wie in Tabelle 12.2 gezeigt. Am Anfang ist es immer etwas schwer sich in den logischen Operatoren zurechtzufinden. Daher kann ich dir nur den Tipp geben einmal die Operatoren selber auszuprobieren und zu schauen, was du da so raus filterst.\n\n\nTabelle 12.2— Logische Operatoren und R und deren Beschreibung\n\n\n\n\n\nLogischer Operator\nBeschreibung\n\n\n\n&lt;\nkleiner als (eng. less than)\n\n\n&lt;=\nkleiner als oder gleich (eng. less than or equal to)\n\n\n&gt;\ngrößer als (eng. greater than)\n\n\n&gt;=\ngrößer als oder gleich (eng. greater than or equal to)\n\n\n==\nexact gleich (eng. exactly equal to)\n\n\n!=\nnicht gleich (eng. not equal to)\n\n\n!x\nnicht (eng. not x)\n\n\nx | y\noder (eng. x or y)\n\n\nx & y\nund (eng. x and y)\n\n\n\n\nHier ein paar Beispiele. Probiere gerne auch mal Operatoren selber aus. Im folgenden Codeblock wollen wir nur die Zeilen haben, die eine Anzahl an Flöhen größer von 15 haben.\n\ndata_tbl %&gt;% \n  filter(flea_count &gt; 15)\n\n# A tibble: 13 × 5\n   animal jump_length flea_count grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            5.7         18     8        0\n 2 dog            8.9         22     8        1\n 3 dog           11.8         17     6        1\n 4 dog            5.6         23     7        1\n 5 dog            9.1         18     7        0\n 6 dog            7.6         21     9        0\n 7 cat            4.3         16     6        1\n 8 fox            7.7         21     5        1\n 9 fox            8.1         25     4        1\n10 fox            9.1         31     4        1\n11 fox           10.6         28     4        0\n12 fox            8.6         18     4        1\n13 fox           10.3         19     3        0\n\n\nWir wollen nur die infizierten Tiere haben.\n\ndata_tbl %&gt;% \n  filter(infected == TRUE)\n\n# A tibble: 10 × 5\n   animal jump_length flea_count grade infected\n   &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            8.9         22     8        1\n 2 dog           11.8         17     6        1\n 3 dog            5.6         23     7        1\n 4 cat            3.2         12     7        1\n 5 cat            4.3         16     6        1\n 6 fox            7.7         21     5        1\n 7 fox            8.1         25     4        1\n 8 fox            9.1         31     4        1\n 9 fox            9.7         12     5        1\n10 fox            8.6         18     4        1\n\n\nWir wollen nur die infizierten Tiere haben UND die Tiere mit einer Flohanzahl größer als 20.\n\ndata_tbl %&gt;% \n  filter(infected == TRUE & flea_count &gt; 20)\n\n# A tibble: 5 × 5\n  animal jump_length flea_count grade infected\n  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dog            8.9         22     8        1\n2 dog            5.6         23     7        1\n3 fox            7.7         21     5        1\n4 fox            8.1         25     4        1\n5 fox            9.1         31     4        1\n\n\nDu findest auf der englischen Hilfeseite für filter() noch weitere Beispiele für die Nutzung."
  },
  {
    "objectID": "programing-dplyr.html#spalten-ändern-mit-mutate",
    "href": "programing-dplyr.html#spalten-ändern-mit-mutate",
    "title": "12  Daten bearbeiten",
    "section": "\n12.4 Spalten ändern mit mutate()\n",
    "text": "12.4 Spalten ändern mit mutate()\n\n\n\n\n\n\n\nYouTube - Eigenschaften von Variablen ändern mit mutate()\n\n\n\nDu findest auf YouTube Einführung in R - Teil 14 - Eigenschaften von Variablen ändern mit mutate() als Video zum nochmal anschauen.\n\n\n\n\nWir nutzen die Funktion mutate() um die Eigenschaften von Spalten daher Variablen zu ändern.\nDie Reihenfolge der Funktionen ist wichtig um unliebsame Effekte zu vermeiden.\n\nErst wählen wir die Spalten mit select()\n\nDann filtern wir die Zeilen mit filter()\n\nAbschließend ändern wir die Eigenschaften der Spalten mit mutate()\n\n\nNachdem wir die Spalten mit select() udn eventuell die Zeieln mit filter() gewählt haben. müssen wir jetzt noch die Eigenschaften der Spalten ändern. Das Ändern müssen wir nicht immer tun, aber häufig müssen wir noch einen Faktor erschaffen. Wir nutzen noch die Funktion pull() um uns die Spalte animal aus dem Datensatz zu ziehen. Nur so sehen wir die vollen Eigenschaften des Faktors. Später nutzen wir pull seltener und nur um zu kontrollieren, was wir gemacht haben.\nIm folgenden Codeblock verwandeln wir die Variable animal in einen Faktor durch die Funktion as_factor. Wir sehen, dass die Level des Faktoes so sortiert sind, wie das Auftreten in der Spalte animal.\n\ndata_tbl %&gt;% \n  mutate(animal = as_factor(animal)) %&gt;% \n  pull(animal)\n\n [1] dog dog dog dog dog dog dog cat cat cat cat cat cat cat fox fox fox fox fox\n[20] fox fox\nLevels: dog cat fox\n\n\nWollen wir die Sortierung der Level ändern, können wir die Funktion factor() nutzen. Wir ändern die Sortierung des Faktors zu fox, dog und cat.\n\ndata_tbl %&gt;% \n  mutate(animal = factor(animal, levels = c(\"fox\", \"dog\", \"cat\"))) %&gt;% \n  pull(animal)\n\n [1] dog dog dog dog dog dog dog cat cat cat cat cat cat cat fox fox fox fox fox\n[20] fox fox\nLevels: fox dog cat\n\n\nWir können auch die Namen (eng. labels) der Level ändern. Hier musst du nur aufpassen wie du die alten Labels überschreibst. Wenn ich gleichzeitig die Level und die Labels ändere komme ich häufig durcheinander. Da muss du eventuell nochmal schauen, ob auch alles so geklappt hat wie du wolltest.\n\ndata_tbl %&gt;% \n  mutate(animal = factor(animal, labels = c(\"Hund\", \"Katze\", \"Fuchs\"))) %&gt;% \n  pull(animal)\n\n [1] Katze Katze Katze Katze Katze Katze Katze Hund  Hund  Hund  Hund  Hund \n[13] Hund  Hund  Fuchs Fuchs Fuchs Fuchs Fuchs Fuchs Fuchs\nLevels: Hund Katze Fuchs\n\n\nDu findest auf der englischen Hilfeseite für mutate() noch weitere Beispiele für die Nutzung. Insbesondere die Nutzung von mutate() über mehrere Spalten gleichzeitig erlaubt sehr effiezientes Programmieren. Aber das ist für den Anfang etwas viel.\n\n\n\n\n\n\nDie Funktionen select(), filter() und mutate() in R\n\n\n\nBitte schaue dir auch die Hilfeseiten der Funktionen an. In diesem Skript kann ich nicht alle Funktionalitäten der Funktionen zeigen. Oder du kommst in das R Tutorium welches ich anbiete und fragst dort nach den Möglichkeiten Daten in R zu verändern."
  },
  {
    "objectID": "programing-dplyr.html#gruppieren-mit-group_by",
    "href": "programing-dplyr.html#gruppieren-mit-group_by",
    "title": "12  Daten bearbeiten",
    "section": "\n12.5 Gruppieren mit group_by()\n",
    "text": "12.5 Gruppieren mit group_by()\n\nSobald wir einen Faktor erschaffen haben, können wir die Daten in R auch nach dem Faktor gruppieren. Das heißt wir nutzen die Funktion group_by() um R mitzuteilen, dass nun folgende Funktionen getrennt für die einzelen Gruppen erfolgen sollen. Im folgenden Codeblock siehst du die Anwendung.\n\ndata_tbl %&gt;% \n  mutate(animal = as_factor(animal)) %&gt;% \n  group_by(animal)\n\n# A tibble: 21 × 5\n# Groups:   animal [3]\n   animal jump_length flea_count grade infected\n   &lt;fct&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 dog            5.7         18     8        0\n 2 dog            8.9         22     8        1\n 3 dog           11.8         17     6        1\n 4 dog            8.2         12     8        0\n 5 dog            5.6         23     7        1\n 6 dog            9.1         18     7        0\n 7 dog            7.6         21     9        0\n 8 cat            3.2         12     7        1\n 9 cat            2.2         13     5        0\n10 cat            5.4         11     7        0\n# ℹ 11 more rows\n\n\nAuf den ersten Blick ändert sich nicht viel. Es entsteht aber die Zeile # Groups: animal [3]. Wir wissen nun, dass wir nach der Variable animal mit drei Gruppen die Datentabelle gruppiert haben. Die Anwendung siehst du in Kapitel 15.10 bei der Berechung von deskriptiven Maßzahlen."
  },
  {
    "objectID": "programing-dplyr.html#mehr-informationen-durch-glimpse-und-str",
    "href": "programing-dplyr.html#mehr-informationen-durch-glimpse-und-str",
    "title": "12  Daten bearbeiten",
    "section": "\n12.6 Mehr Informationen durch glimpse() und str()\n",
    "text": "12.6 Mehr Informationen durch glimpse() und str()\n\nAm Ende noch zwei Funktionen zur Kontrolle, was wir hier eigentlich gerade tun. Mit der Funktion glimpse() können wir uns einen Einblick in die Daten geben lassen. Wir sehen dann nochmal kurz und knapp wieviel Zeieln und Spalten wir haben und welche Inhalte in den Spalten stehen. Die gleichen Informationen erhalten wir auch durch die Funktion str(). Die Funktion str()geht aber noch einen Schritt weiter und nennt uns auch Informationen zu dem Objekt. Daher wir wissen jetzt, dass es sich beim dem Objekt data_tbl um ein tibble() handelt.\n\nglimpse(data_tbl)\n\nRows: 21\nColumns: 5\n$ animal      &lt;chr&gt; \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"cat\", \"c…\n$ jump_length &lt;dbl&gt; 5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6, 3.2, 2.2, 5.4, 4.1, 4.…\n$ flea_count  &lt;dbl&gt; 18, 22, 17, 12, 23, 18, 21, 12, 13, 11, 12, 16, 9, 7, 21, …\n$ grade       &lt;dbl&gt; 8, 8, 6, 8, 7, 7, 9, 7, 5, 7, 6, 6, 6, 5, 5, 4, 4, 5, 4, 4…\n$ infected    &lt;dbl&gt; 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1…\n\nstr(data_tbl)\n\ntibble [21 × 5] (S3: tbl_df/tbl/data.frame)\n $ animal     : chr [1:21] \"dog\" \"dog\" \"dog\" \"dog\" ...\n $ jump_length: num [1:21] 5.7 8.9 11.8 8.2 5.6 9.1 7.6 3.2 2.2 5.4 ...\n $ flea_count : num [1:21] 18 22 17 12 23 18 21 12 13 11 ...\n $ grade      : num [1:21] 8 8 6 8 7 7 9 7 5 7 ...\n $ infected   : num [1:21] 0 1 1 0 1 0 0 1 0 0 ...\n\n\n\n\n\n\n\n\nR Code des Kapitels\n\n\n\n\n\n\npacman::p_load(tidyverse, readxl, magrittr, janitor)\n\ndata_tbl %&gt;% \n  select(animal, jump_length, flea_count)\n\ndata_tbl %&gt;% \n  select(Sprungweite = jump_length, flea_count, animal)\n\n\ndata_tbl %&gt;% \n  filter(animal %in% c(\"dog\", \"fox\"))\n\ndata_tbl %&gt;% \n  filter(flea_count &gt; 15)\n\ndata_tbl %&gt;% \n  filter(infected == TRUE)\n\ndata_tbl %&gt;% \n  filter(infected == TRUE & flea_count &gt; 20)\n\ndata_tbl %&gt;% \n  mutate(animal = as_factor(animal)) %&gt;% \n  pull(animal)\n\ndata_tbl %&gt;% \n  mutate(animal = factor(animal, levels = c(\"fox\", \"dog\", \"cat\"))) %&gt;% \n  pull(animal)\n\ndata_tbl %&gt;% \n  mutate(animal = factor(animal, labels = c(\"Hund\", \"Katze\", \"Fuchs\"))) %&gt;% \n  pull(animal)\n\ndata_tbl %&gt;% \n  mutate(animal = as_factor(animal)) %&gt;% \n  group_by(animal)\n\nglimpse(data_tbl)\nstr(data_tbl)"
  },
  {
    "objectID": "programing-strings.html#sec-stringr",
    "href": "programing-strings.html#sec-stringr",
    "title": "13  Reguläre Ausdrücke",
    "section": "\n13.1 Das R Paket stringr\n",
    "text": "13.1 Das R Paket stringr\n\nDas R Paket stringr und das Cheat Sheet zu stringr geben eine große Übersicht über die Möglichkeiten ein character zu bearbeiten. Im Folgenden schauen wir uns einmal einen simplen Datensatz an. Wir wollen auf dem Datensatz ein paar Funktionen aus dem R Paket stringr anwenden.\n\nregex_tbl &lt;- tibble(animal = c(\"cat\", \"cat\", \"dog\", \"bird\"),\n                    site = c(\"village\", \"village\", \"town\", \"cities\"),\n                    time = c(\"t1_1\", \"t2_2\", \"t3_3\", \"t3_5\"))\n\nDie einfachste und am meisten genutzte Funktion ist str_c(). Die Funktion str_c() klebt verschiedene Vektoren zusammen. Wir können auch Zeichen wählen, wie das -, um die Vektoren zu verbinden. Wir bauen uns also eine neue Spalte animal_site in dem wir die Spalten animal und site mit einem - verbinden. Wir können statt dem - auch ein beliebiges anderes Zeichen oder auch Wort nehmen.\n\nregex_tbl %&gt;% \n  mutate(animal_site = str_c(animal, \"-\", site))\n\n# A tibble: 4 × 4\n  animal site    time  animal_site\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      \n1 cat    village t1_1  cat-village\n2 cat    village t2_2  cat-village\n3 dog    town    t3_3  dog-town   \n4 bird   cities  t3_5  bird-cities\n\n\nHäufig brauchen wir auch eine ID Variable, die exakt \\(n\\) Zeichen lang ist. Hier können wir die Funktion str_pad() nutzen um Worte auf die Zeichenlänge width = zu verlängern. Wir können auch das Zeichen wählen, was angeklebt wird und die Seite des Wortes wählen an die wir kleben wollen. Wir verlängern also links die Spalte site auf ein Wort mit acht Zeichen und als Füllzeichen nehmen wir die Null.\n\nregex_tbl %&gt;% \n  mutate(village_pad = str_pad(site, pad = \"0\", \n                               width = 8, side = \"left\"))\n\n# A tibble: 4 × 4\n  animal site    time  village_pad\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      \n1 cat    village t1_1  0village   \n2 cat    village t2_2  0village   \n3 dog    town    t3_3  0000town   \n4 bird   cities  t3_5  00cities   \n\n\nAbschließend können wir auch eine Spalte in zwei Spalten aufteilen. Dafür müssen wir den Separator wählen nachdem die Spalte aufgetrennt werden soll. Wir können eine Spalte auch in mehrere Spalten aufteilen, wenn der Separator eben an zwei oder mehr Stellen steht. Wir haben die Spalte time und trennen die Spalte time an der Stelle _ in zwei Spalten auf.\n\nregex_tbl %&gt;% \n  separate(time, into = c(\"time\", \"rep\"), \n           sep = \"_\", convert = TRUE)\n\n# A tibble: 4 × 4\n  animal site    time    rep\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;\n1 cat    village t1        1\n2 cat    village t2        2\n3 dog    town    t3        3\n4 bird   cities  t3        5\n\n\nEs gibt noch sehr viel mehr Möglichkeiten einen character Vektor zu bearbeiten. Teilweise nutze ich stringr bei der Auswertung von den Beispielen im Anhang. Schau dir da mal um, dort wirst du immer mal wieder die Funktionen aus dem Paket finden."
  },
  {
    "objectID": "programing-strings.html#sec-regex",
    "href": "programing-strings.html#sec-regex",
    "title": "13  Reguläre Ausdrücke",
    "section": "\n13.2 Regular expressions",
    "text": "13.2 Regular expressions\nEin regulärer Ausdruck (eng. regular expression, abk. RegExp oder Regex) ist eine verworrene Zeichenkette, die einer Maschine ein Muster übersetzt. Dieses Muster soll dann auch alle Wörter angewendet werden. Das klingt kryptisch und ist es auch. Reguläre Ausdrücke sind das Nerdthema schlechthin in der Programmierung. Also tauchen wir mal ab in die Welt der seht nützlichen und mächtigen regulären Ausdrücke.\n\n\nWir immer gibt es schöne Tutorials. Einmal Regular expressions in stingr und dann von Hadley Wickham ein Kapitel zu Regular expressions.\nFangen wir mit der grundsätzlichen Idee an. Reguläre Ausdrücke finden bestimmte Zeichen in Wörtern. Ich habe dir hier einmal eine winzige Auswahl an regulären Ausdrücken mitgebracht. Du kannst auch reguläre Ausdrück miteinander verknüpfen. Daher kommt die eigentlich Macht eines regulären Ausdruck. Aber später mehr dazu.\n\n\n\\d: entspricht einer beliebigen Ziffer. Wir finden also eine Ziffer oder Zahl in einem Wort.\n\n\\s: entspricht einem beliebigen Leerzeichen (z. B. Leerzeichen, Tabulator, Zeilenumbruch). Wir finden also ein Leerzeichen in einem String.\n\n[abc]: passt auf a, b oder c. Das ist jetzt wörtlich gemeint. Wir finden die Buchstaben a, b oder c.\n\n[^abc]: passt auf alles außer a, b oder c. Das ist jetzt ebenfalls wörtlich gemeint.\n\nDenk daran, dass du, um einen regulären Ausdruck zu erstellen, der \\d oder \\s enthält, das \\ für die Zeichenkette ausschließen musst, also gib \\\\d oder \\\\s ein. Das ist eien Besonderheit in R. Wir müssen in R immer ein doppeltes \\\\ schreiben.\nEs gibt eine große Auswahl an möglichen regulären Ausdrücken. Ich nutze meist dann noch ein Cheat Sheet um den Überblick zu bewahren. Aber wie schon oben geschrieben, reguläre Ausdrücke braucht man meist erst, wenn die Daten so große werden, dass wir die Daten nicht mehr händisch bearbeiten können.\n\n\nIch selber nutze immer das Regular Expressions Cheat Sheet by DaveChild um den Überblick zu bewahren. Es ist dann auch einfach zu viel zu merken.\nWir können wieder das R Paket stringr nutzen um die regulären Ausdrück in R anzuwenden. Beginnen wir erstmal mit den einfachen Funktionen und arbeiten uns dann vor. Bitte beachte auch, dass du in der Funktion select() auch Helferfunktionen nutzen kannst, die dir das Leben wirklich einfacher machen. Auf diese Helferfunktionen gehen wir später nochmal ein.\n\n\nstr_detect() gibt TRUE/FALSE wieder, wenn das Wort eine Zeichenkette enthält.\n\nstr_subset() gibt die Werte wieder in denen die Zeichenkette erkannt wird.\n\nstr_replace() ersetzt das erste Auftreten der Zeichenkette durch eine andere Zeichenkette. Die Funktion str_replace_all() ersetzt dann jedes Auftreten der Zeichenkette.\n\nFangen wir simple an, wir wollen Wörter finden, die eine Zeichenkette enthalten. Dafür nutzen wir die Funktion str_detect().\n\nc(\"dog\", \"small-dog\", \"doggy\", \"cat\", \"kitty\") %&gt;% \n  str_detect(\"dog\")\n\n[1]  TRUE  TRUE  TRUE FALSE FALSE\n\n\nOkay, das hat ja schon mal gut funktioniert. Wenn wir die Worte wiederhaben wollen, dann können wir auch die Funktion str_subset() nutzen. Wir wollen jetzt aber nur die Einträge, die mit der Zeichenkette dog anfangen. Deshalb schreiben wir ein ^ vor die Zeichenkette. Wenn wir nur die Einträge gewollt hätten, die mit dog enden, dann hätten wir dog$ geschrieben. Das $ schaut dann von hinten in die Wörter.\n\nc(\"dog\", \"small-dog\", \"doggy\", \"cat\", \"kitty\") %&gt;% \n  str_subset(\"^dog\")\n\n[1] \"dog\"   \"doggy\"\n\n\nNun haben wir uns verschrieben und wollen das small entfernen und mit large ersetzen. Statt large hätten wir auch nichts \"\" hinschreiben können. Dafür können wir die Funktion str_replace() nutzen. Die Funktion entfernt das erste Auftreten einer Zeichenkette. Wenn du alle Zeichenketten entfernen willst, dann musst du die Funktion str_replace_all() verwenden.\n\nc(\"dog\", \"small-dog\", \"doggy\", \"cat\", \"kitty\") %&gt;% \n  str_replace(\"small\", \"large\")\n\n[1] \"dog\"       \"large-dog\" \"doggy\"     \"cat\"       \"kitty\"    \n\n\nDamit haben wir die wichtigsten drei Funktionen einmal erklärt. Wir werden dann diese Funktionen immer mal wieder anwenden und dann kannst du sehen, wie die regulären Ausdrücke in den Funktionen funktionieren. Auf der Hilfeseite von stringr gibt es nochmal ein Tutorium zu Regular expressions, wenn du hier mehr erfahren möchtest.\nViel häufiger nutzen wir die Helferfunktionen in select(). Wir haben hier eine große Auswahl, die uns das selektieren von Spalten sehr erleichtert. Im Folgenden einmal die Liste alle möglichen Funktionen. Beachte auch dabei folgende weitere Funktionen im Overview of selection features.\n\n\nmatches(), wählt alle Spalten die eine Zeichenkette enthalten. Hier können wir reguläre Ausdrücke verwenden.\n\nall_of(), ist für die strenge Auswahl. Wenn eine der Variablen im Zeichenvektor fehlt, wird ein Fehler ausgegeben.\n\nany_of(), prüft nicht auf fehlende Variablen. Dies ist besonders nützlich bei der negativen Selektionen, wenn du sicherstellen möchtest, dass eine Variable entfernt wird.\n\ncontains(), wählt alle Spalten die eine Zeichenkette enthalten. Funktioniert nicht mit regulären Ausdrücken.\n\nstarts_with(), wählt alle Spalten die mit einer Zeichenkette beginnt. Funktioniert nicht mit regulären Ausdrücken.\n\nends_with(), wählt alle Spalten die mit einer Zeichenkette endet. Funktioniert nicht mit regulären Ausdrücken.\n\neverything(), sortiert die restlichen Spalten hinten an. Wir können uns also die wichtigen Spalten namentlich nach vorne holen und dann den Rest mit everything() hinten an kleben.\n\nlast_col(), wählt die letzte Spalte aus. Besonders wichtig, wenn wir von einer Spalte bis zur letzten Spalte auswählen wollen.\n\nnum_range(), können wir nutzen, wenn wir über eine Zahl eine Spalte wählen wollen. Das heißt, unsere Spalten haben Zahlen in den Namen und wir könne darüber dann die Spalten wählen.\n\nHier einmal das Beispiel mit matches() wo wir alle Spalten nehmen in denen ein _ als Unterstrich vorkommt.\n\ndata_tbl %&gt;% \n  select(matches(\"_\"))\n\n# A tibble: 3 × 2\n  jump_length flea_count\n        &lt;dbl&gt;      &lt;dbl&gt;\n1         5.7         18\n2         8.9         22\n3        11.8         17\n\n\nWir können auch über einen Vektor die Spalten auswählen. Das ist meistens nötig, wenn wir die Namen der Spalten zum Beispiel aus einer anderen Funktion erhalten. Dafür können wir dann die Funktion one_of() nutzen.\n\ndata_tbl %&gt;% \n  select(one_of(\"animal\", \"grade\"))\n\n# A tibble: 3 × 2\n  animal grade\n  &lt;chr&gt;  &lt;dbl&gt;\n1 dog        8\n2 dog        8\n3 dog        6\n\n\nEs gibt noch eine Menge anderer Tools zum Nutzen von Regulären Ausdrücken. Aber hier soll es erstmal reichen. Wir gehen dann später in der Anwendung immer mal wieder auf die Funktionen hier ein."
  },
  {
    "objectID": "programing-strings.html#sec-time-date",
    "href": "programing-strings.html#sec-time-date",
    "title": "13  Reguläre Ausdrücke",
    "section": "\n13.3 Zeit und Datum",
    "text": "13.3 Zeit und Datum\nDie Arbeit mit Datumsdaten in R kann frustrierend sein. Die R Befehle für Datumszeiten sind im Allgemeinen nicht intuitiv und ändern sich je nach Art des verwendeten Datumsobjekts. Hier gibt es aber eine Lösung mit dem R Paket lubridate, die uns die Arbeit etwas erleichtert.\n\n\nQuelle: https://xkcd.com/\n\nDer beste Tipp ist eigentlich immer, das Datum in ein Format zu bringen und dann dieses Format weiterzuverarbeiten.\n\nWandle immer deine Datumsspalte in eine character Spalte mit as.character() um. Ein Datum besteht immer aus dem Tag, dem Monat und dem Jahr. Dabei ist wichtig, dass der Monat und der Tag immer zweistellig sind. Manchmal muss man dann über str_split() und str_pad() erstmal echt sich einen Wolf splitten und kleben, bis dann das Format so passt. Dann geht es meistens wie folgt weiter.\nVersuche dann mit der Funktion as_date() eine Datumspalte zu erschaffen. Häufig erkennt die Funktion die Spalte richtig und schneidet das Datum korrekt in Jahr/Monat/Tag. Manchmal klappt das aber auch nicht. Dann müssen wir uns weiter strecken.\nWenn die Umwandlung mit as_date() nicht klappt, musst du nochmal über parse_date_time() gehen und angeben, wie dein Datum formatiert ist.\n\n\n\nAuf der Hilfeseite der Funktion parse_date_time() erfährst du dann mehr über User friendly date-time parsing functions\nSchauen wir uns einmal ein Beispiel für Daten an. Ich habe hier die Daten von Deutschen Wetterdienst runtergeladen und möchte die Spalte jjjjmmdd in ein Datum in R umwandeln.\n\ntime_tbl &lt;- read_table(\"data/day_values_osnabrueck.txt\") %&gt;% \n  clean_names() %&gt;% \n  select(jjjjmmdd) %&gt;% \n  print(n = 3)\n\n# A tibble: 501 × 1\n  jjjjmmdd\n     &lt;dbl&gt;\n1 20221030\n2 20221029\n3 20221028\n# ℹ 498 more rows\n\n\nWir nehmen die Datumsspalte, die eine Zahl ist und transformieren die Spalte in einen character. Danach können wir dann die Funktion as_date() nutzen um uns ein Datum wiedergeben zu lassen.\n\ntime_tbl %&gt;% \n  mutate(jjjjmmdd = as.character(jjjjmmdd),\n         jjjjmmdd = as_date(jjjjmmdd)) %&gt;% \n  print(n = 3)\n\n# A tibble: 501 × 1\n  jjjjmmdd  \n  &lt;date&gt;    \n1 2022-10-30\n2 2022-10-29\n3 2022-10-28\n# ℹ 498 more rows\n\n\nWie wir sehen passt die Umwandlung in diesem Fall hervorragend. Die Funktion as_date() erkennt das Jahr, den Monat und den Tag und baut uns dann die Datumsspalte zusammen. Meistens passt es auch, dann können wir hier enden.\nAls eine Alternative haben wir auch die Möglichkeit die Funktion as.Date() zu nutzen. Hier können wir das Datumformat in einer etwas kryptischen Form angeben. Schauen wir uns erst die Funktion in Arbeit an und dann was wir hier gemacht haben.\n\ntime_tbl %&gt;% \n  mutate(jjjjmmdd = as.character(jjjjmmdd),\n         jjjjmmdd = as.Date(jjjjmmdd, \"%Y%m%d\")) %&gt;% \n  print(n = 3)\n\n# A tibble: 501 × 1\n  jjjjmmdd  \n  &lt;date&gt;    \n1 2022-10-30\n2 2022-10-29\n3 2022-10-28\n# ℹ 498 more rows\n\n\nWir können der Funktion das Datumsformat mitgeben. Im Folgenden einmal eine Auswahl an Möglichkeiten. Die jeweiligen Prozent/Buchstaben-Kombinationen stehen dann immer für ein Jahr oder eben ein Monat.\n\n\n%Y: 4-Zeichen Jahr (1982)\n\n%y: 2-Zeichen Jahr (82)\n\n%m: 2-Zeichen Monat (01)\n\n%d: 2-Zeichen Tag des Monats (13)\n\n%A: Wochentag (Wednesday)\n\n%a: Abgekürzter Wochentag (Wed)\n\n%B: Monat (January)\n\n%b: Abgekürzter Monat (Jan)\n\nNehmen wir einmal an, wir haben das Datum in folgender Form 2012-11-02 vorliegen. Dann können wir dafür als Format %Y-%m-%d schreiben und das Datum wird erkennt. Hier ist es besonders hilfreich, dass wir die Trennzeichen mit angeben können. Sonst müssen wir die Trennzeichen dann über str_replace_all() entfernen und könten dann schauen, ob es über die Funktion as_date() geht.\nAls ein weiteres Beispiel nochmal das Einlesen von einer Datei mit dem Datum und der Uhrzeit in zwei Spalten. Wir wollen die beiden Spalten zusammenführen, so dass wir nur noch eine Spalte mit datetime haben.\n\ndate_time_tbl &lt;- read_excel(\"data/date_time_data.xlsx\") %&gt;% \n  clean_names()\ndate_time_tbl\n\n# A tibble: 7 × 5\n  datum               uhrzeit             messw   min   max\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2023-04-11 00:00:00 1899-12-31 13:30:00  22    22    22  \n2 2023-04-11 00:00:00 1899-12-31 14:00:00  19.5  19.5  22.6\n3 2023-04-11 00:00:00 1899-12-31 14:30:00  23.4  19.5  25.8\n4 2023-04-11 00:00:00 1899-12-31 15:00:00  19.2  18.3  28.1\n5 2023-04-11 00:00:00 1899-12-31 15:30:00  17.1  17.1  23.3\n6 2023-04-11 00:00:00 1899-12-31 16:00:00  19.2  15.3  21.9\n7 2023-04-11 00:00:00 1899-12-31 16:30:00  29.2  18.3  31  \n\n\nDazu nutzen wir die Funktion format() die es uns erlaubt die Spalten einmal als Datum ohne Uhrzeit zu formatieren und einmal erlaubt die Uhrzeit ohne das Datum zu bauen. Dann nehmen wir beide Spalten und packen das Datum und die Uhrzeit wieder zusammen.\n\ndate_time_tbl %&gt;% \n  mutate(uhrzeit = format(uhrzeit, format = \"%H:%M:%S\"),\n         datum = format(datum, format = \"%Y-%m-%d\"),\n         datum = ymd(datum) + hms(uhrzeit))\n\n# A tibble: 7 × 5\n  datum               uhrzeit  messw   min   max\n  &lt;dttm&gt;              &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2023-04-11 13:30:00 13:30:00  22    22    22  \n2 2023-04-11 14:00:00 14:00:00  19.5  19.5  22.6\n3 2023-04-11 14:30:00 14:30:00  23.4  19.5  25.8\n4 2023-04-11 15:00:00 15:00:00  19.2  18.3  28.1\n5 2023-04-11 15:30:00 15:30:00  17.1  17.1  23.3\n6 2023-04-11 16:00:00 16:00:00  19.2  15.3  21.9\n7 2023-04-11 16:30:00 16:30:00  29.2  18.3  31  \n\n\n\n\n\n\nIm Weiteren hilft das Tutorial zum R Paket lubridate - Make Dealing with Dates a Little Easier und natürlich das weitere Tutorial Dates and times."
  },
  {
    "objectID": "programing-purrr-furrr.html#genutzte-r-pakete",
    "href": "programing-purrr-furrr.html#genutzte-r-pakete",
    "title": "14  Mit purrr und furrr",
    "section": "\n14.1 Genutzte R Pakete",
    "text": "14.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, rstatix, \n               janitor, purrr, furrr, see,\n               readxl, tictoc, multcompView, \n               parameters, scales,\n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "programing-purrr-furrr.html#die-daten",
    "href": "programing-purrr-furrr.html#die-daten",
    "title": "14  Mit purrr und furrr",
    "section": "\n14.2 Die Daten",
    "text": "14.2 Die Daten\nAls Datenbeispiel schauen wir uns einmal ein einfaktoriellen Datensatz an und suchen uns auch nur acht Zeilen und drei Outcomes raus. Wir könnten die Analyse auch über den vollen Datensatz rechnen, aber dann wird hier alles sehr voll. Es geht ja hier mehr um die Demonstration.\n\nsoil_tbl &lt;- read_excel(\"data/soil_1fac_data.xlsx\") %&gt;% \n  mutate(variante = str_c(variante, \"_\", amount),\n         variante = as_factor(variante),\n         across(where(is.numeric), round, 2)) %&gt;% \n  select(-amount) %&gt;%\n  extract(1:8, 1:4) %&gt;% \n  pivot_longer(cols = fe:no3, \n               names_to = \"outcome\",\n               values_to = \"rsp\") \n\nAls zweiten Datensatz nehmen wir noch eine zweifaktorilles Design mit einem Behandlungs- und einem Blockeffekt. Darüberhinaus haben wir dann noch verschiedene Outcomes und diese Outcomes dann auch an zwei Orten, einmal im Blatt und einmal im Stiel, gemessen. Das heißt, wir haben immer eine Outcome/Sample-Kombination vorliegen. Bei drei Outcomes und zwei Messorten macht das dann sechs Kombinationen auf denen wir dann immer unsere zweifaktoriellen Analysen rechnen wollen.\n\nspinach_tbl &lt;- read_excel(\"data/spinach_metal_data.xlsx\") %&gt;% \n  mutate(trt = as_factor(trt),\n         sample = as_factor(sample),\n         block = as_factor(block)) %&gt;% \n  pivot_longer(cols = fe:zn,\n               names_to = \"outcome\",\n               values_to = \"rsp\") %&gt;% \n  mutate(outcome = as_factor(outcome))"
  },
  {
    "objectID": "programing-purrr-furrr.html#daten-aufteilen",
    "href": "programing-purrr-furrr.html#daten-aufteilen",
    "title": "14  Mit purrr und furrr",
    "section": "\n14.3 Daten aufteilen…",
    "text": "14.3 Daten aufteilen…\nIn R haben wir zwei Möglichkeiten für map() die Daten aufzuteilen. Klar, wir können die Aufteilung sicherlich auch mit anderen Funktionen machen, aber diese beiden Funktionen sind sehr nützlich.\n\n14.3.1 … mit split()\n\nMit der Funktion split() können wir einen Datensatz nach einer Faktorspalte in eine Liste aufspalten. Die Liste ist dann auch gleich so benannt wie das Level es Faktors für das wir die Aufteilung gemacht haben. Die Benamung der Liste ist dann praktisch, wenn wir später wieder einen Datensatz aus den Ergebnissen bauen.\n\nsoil_lst &lt;- soil_tbl %&gt;%\n  split(.$outcome) \n\nsoil_lst\n\n$fe\n# A tibble: 8 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 fe       0.26\n2 Holzfasern_0 fe       0.33\n3 Holzfasern_0 fe       0.27\n4 Holzfasern_0 fe       0.31\n5 Torf_30      fe       0.46\n6 Torf_30      fe       0.37\n7 Torf_30      fe       0.28\n8 Torf_30      fe       0.5 \n\n$k\n# A tibble: 8 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 k        3.7 \n2 Holzfasern_0 k        3.66\n3 Holzfasern_0 k        3.83\n4 Holzfasern_0 k        3.66\n5 Torf_30      k        2.89\n6 Torf_30      k        3.41\n7 Torf_30      k        2.94\n8 Torf_30      k        2.89\n\n$no3\n# A tibble: 8 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 no3      3.28\n2 Holzfasern_0 no3      3.24\n3 Holzfasern_0 no3      3.64\n4 Holzfasern_0 no3      3.24\n5 Torf_30      no3      5.02\n6 Torf_30      no3      9.44\n7 Torf_30      no3      8.71\n8 Torf_30      no3      5.02\n\n\n\n14.3.2 … mit nest()\n\nWenn wir mehr als eine Gruppierungsspalte haben, dann können wir die Funktion nest() nutzen. In unserem Beispiel haben wir die Spalte outcome und sample. Für jede Kombination der beiden Spalte wollen wir dann jeweils ein Modell rechnen. Hier meine ich mit Modell eine lineare Regression und dann eine ANOVA. Als erstes müssen wir unsere Daten gruppieren und dann können wir die Daten nesten. Mit unnest() lässt sich dann die genestete Struktur wieder in einen normalen Datensatz zurückführen.\n\nspinach_nest_tbl &lt;- spinach_tbl %&gt;% \n  group_by(sample, outcome) %&gt;% \n  nest() \n\nspinach_nest_tbl\n\n# A tibble: 6 × 3\n# Groups:   sample, outcome [6]\n  sample outcome data             \n  &lt;fct&gt;  &lt;fct&gt;   &lt;list&gt;           \n1 leaf   fe      &lt;tibble [28 × 3]&gt;\n2 leaf   cd      &lt;tibble [28 × 3]&gt;\n3 leaf   zn      &lt;tibble [28 × 3]&gt;\n4 stem   fe      &lt;tibble [28 × 3]&gt;\n5 stem   cd      &lt;tibble [28 × 3]&gt;\n6 stem   zn      &lt;tibble [28 × 3]&gt;"
  },
  {
    "objectID": "programing-purrr-furrr.html#sec-purrr",
    "href": "programing-purrr-furrr.html#sec-purrr",
    "title": "14  Mit purrr und furrr",
    "section": "\n14.4 Mit purrr über Daten",
    "text": "14.4 Mit purrr über Daten\nDas R Paket purrr erlaubt es sehr effizient immer das Gleiche auf Listeneinträgen oder genereller auf Daten anzuwenden. Wir können uns dabei selber eine Funktion schreiben oder aber schon implementierte Funktionen anwenden. Gehen wir eimal alle Funktionen durch. Wir werden hier nicht alle zeigen, aber es ist gut einmal zu wissen, welche Funktionen es gibt.\n\n\nSchaue auch mal in das Cheat Sheet des R Paketes purrr rein: Apply functions with purrr::cheat sheet\n\n\nmap() erlaubt über eine Liste von Datensätzen ein Funktion anzuwenden. Dabei können wir dann die einzelnen Listeneinträge über .x an die Funktionen weitergeben. Siehe hierzu auch Basic map functions.\n\nmap2() erlaubt es über zwei gleichlange Vektoren zu laufen. Wir können hier zwei Optionen in der Form .x, .y an die Funktion weitergeben. Siehe hierzu auch Map with multiple inputs.\n\npmap() kann nun über eine Liste an Vektoren laufen und somit mehrere Inputoptionen verarbeiten. Damit ist pmap() die Generalisierung der map() Funktion. Siehe hierzu auch Map with multiple inputs.\n\nwalk() ist ein silent map(). Damit können wir Daten in eine Datei schreiben, ohne ein Output wieder zubekommen.\n\nimap() können wir nutzen, wenn wir den Index \\(i\\) wieder haben wollen. Das heißt, wir wollen über einen Vektor laufen und brauchen dafür den Index. Hier hilft die imap() Familie.\n\nmodify()können wir anwenden, wenn wir nur Spalten modifizieren oder mutieren wollen. Wir haben einen Datensatz und wollen alle character Spalten in einen Faktor umwandeln. Siehe hierzu auch Modify elements selectively.\n\nSchauen wir uns die Anwendung von der Funktion map() auf eine Liste an. In folgenden Code entfernen wir einmal in jedem Listeneintrag die Spalte outcome. Dann lassen wir uns von jedem Listeneintrag die erste Zeile wiedergeben.\n\nsoil_lst %&gt;%\n  map(select, -outcome) %&gt;% \n  map(head, 1)\n\n$fe\n# A tibble: 1 × 2\n  variante       rsp\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Holzfasern_0  0.26\n\n$k\n# A tibble: 1 × 2\n  variante       rsp\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Holzfasern_0   3.7\n\n$no3\n# A tibble: 1 × 2\n  variante       rsp\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Holzfasern_0  3.28\n\n\nWir können zum einen map(head, 1) schreiben oder aber den Listeneintrag als .x direkt in die Funktion weiterleiten. Dann schreiben wir map(~head(.x, 1)) und müssen noch die Tilde ~ vor die Funktion setzen.\n\nsoil_lst %&gt;%\n  map(~head(.x, 1))\n\n$fe\n# A tibble: 1 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 fe       0.26\n\n$k\n# A tibble: 1 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 k         3.7\n\n$no3\n# A tibble: 1 × 3\n  variante     outcome   rsp\n  &lt;fct&gt;        &lt;chr&gt;   &lt;dbl&gt;\n1 Holzfasern_0 no3      3.28\n\n\nDie richtige Stärke entwickelt dann map(), wenn wir mehrere Funktionen hineinander schalten. In unserem Fall rechnen wir einen Games-Howell Test und wollen uns dann das compact letter display wiedergeben lassen. Da wir am Ende dann einen Datensatz haben wollen, nutzen wir die Funktion bind_rows() um die Listeneinträge in einen Datensatz zusammen zukleben.\n\nsoil_lst %&gt;% \n  map(~games_howell_test(rsp ~ variante, data = .x)) %&gt;% \n  map(~mutate(.x, contrast = str_c(.x$group1, \"-\", .x$group2))) %&gt;% \n  map(pull, p.adj, contrast) %&gt;% \n  map(~multcompLetters(.x)$Letters) %&gt;% \n  bind_rows(.id = \"outcome\") \n\n# A tibble: 3 × 3\n  outcome Holzfasern_0 Torf_30\n  &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;  \n1 fe      a            a      \n2 k       a            b      \n3 no3     a            a      \n\n\nWir können auch auch auf genesteten Daten die Funktion map() anwenden. In diesem Fall belieben wir die ganze Zeit in einem tibble. Wir lagern unsere Ergebnisse sozusagen immer in einer Zelle und können auf diese Einträge dann immer wieder zugreifen. Einmal zu Demonstration rechnen wir sechs Mal ein lineares Modell mit der Funktion lm() und speichern das Ergebnis in der Spalte model. Wir haben jetzt dort jeweils &lt;lm&gt; stehen. Damit wissen wir auch, dass wir dort unser Modell drin haben. Wir können jetzt auf der Spalte model weiterechnen und uns neue Spalten mit Ergebnissen erschaffen.\n\nspinach_nest_tbl %&gt;%\n  mutate(model = map(data, ~lm(rsp ~ trt + block, data = .x)))\n\n# A tibble: 6 × 4\n# Groups:   sample, outcome [6]\n  sample outcome data              model \n  &lt;fct&gt;  &lt;fct&gt;   &lt;list&gt;            &lt;list&gt;\n1 leaf   fe      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n2 leaf   cd      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n3 leaf   zn      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n4 stem   fe      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n5 stem   cd      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n6 stem   zn      &lt;tibble [28 × 3]&gt; &lt;lm&gt;  \n\n\nIm Folgenden rechnen wir ein lineares Modell, dann eine ANOVA und lassen uns das Ergebnis der ANOVA mit der Funktion model_parameters() aufhübschen. Wie du siehst, geben wie immer den Spaltennamen eine Funktion weiter. Dann wählen wir noch die Spalten, die wir dann unnesten wollen.\n\nspinach_nest_tbl %&lt;&gt;%\n  mutate(model = map(data, ~lm(rsp ~ trt + block, data = .x))) %&gt;% \n  mutate(anova = map(model, anova)) %&gt;% \n  mutate(parameter = map(anova, model_parameters)) %&gt;% \n  select(sample, outcome, parameter) \n\nZum Abschluss nutzen wir die Funktion unnest() um uns die aufgehübschten Ergebnisse der ANOVA wiedergeben zu lassen. Dann will ich noch, dass die Namen alle klein geschrieben sind und auch sonst sauber sind. Dafür nutze ich dann die Funktion clean_names(). Abschließend filtere ich und runde ich noch die Ergebnisse. Am Ende will ich dann nur die Kombinationen aus sample und outcome haben sowie den \\(p\\)-Wert aus der ANOVA.\n\nspinach_nest_tbl %&gt;%\n  unnest(parameter) %&gt;% \n  clean_names() %&gt;% \n  mutate(across(where(is.numeric), round, 2)) %&gt;% \n  filter(parameter != \"Residuals\") %&gt;% \n  select(sample, outcome, parameter, p)\n\n# A tibble: 12 × 4\n# Groups:   sample, outcome [6]\n   sample outcome parameter     p\n   &lt;fct&gt;  &lt;fct&gt;   &lt;chr&gt;     &lt;dbl&gt;\n 1 leaf   fe      trt        0   \n 2 leaf   fe      block      0.05\n 3 leaf   cd      trt        0.05\n 4 leaf   cd      block      0   \n 5 leaf   zn      trt        0.42\n 6 leaf   zn      block      0.29\n 7 stem   fe      trt        0.02\n 8 stem   fe      block      0.02\n 9 stem   cd      trt        0.33\n10 stem   cd      block      0   \n11 stem   zn      trt        0.51\n12 stem   zn      block      0   \n\n\nHier noch ein weiteres Beispiel für split(), nest() und nest_by() zum Ausprobieren und rumspielen. Wir wollen für hier einmal auf ganze vielen Behandlungen den Shapiro-Wilk-Tests für die Abweichung von der Normalverteilung rechnen. Dazu laden wir uns einmal die Daten clove_germ_rate.xlsx.\n\nclove_tbl &lt;- read_excel(\"data/clove_germ_rate.xlsx\") %&gt;% \n  mutate(clove_strain = as_factor(clove_strain),\n         germ_rate = as.numeric(germ_rate))\n\nWir rechnen einmal die Shapiro-Wilk-Tests über die Funktion split() und dann einer Liste.\n\nclove_tbl %&gt;% \n  split(.$clove_strain) %&gt;% \n  map(~shapiro.test(.x$germ_rate)) %&gt;% \n  map(tidy) %&gt;% \n  bind_rows(.id = \"test\") %&gt;%\n  select(test, p.value) %&gt;% \n  mutate(decision = ifelse(p.value &lt;= 0.05, \"reject normal\", \"normal\"),\n         p.value = pvalue(p.value, accuracy = 0.001))\n\n# A tibble: 20 × 3\n   test          p.value decision     \n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;        \n 1 standard      0.272   normal       \n 2 west_rck_1    0.272   normal       \n 3 south_III_V   0.855   normal       \n 4 west_rck_2_II 0.653   normal       \n 5 comb_001      0.103   normal       \n 6 western_4     0.849   normal       \n 7 north_549     0.855   normal       \n 8 subtype_09    0.983   normal       \n 9 subtype_III_4 0.051   normal       \n10 ctrl_pos      0.992   normal       \n11 ctrl_7        0.683   normal       \n12 trans_09_I    0.001   reject normal\n13 new_xray_9    0.406   normal       \n14 old_09        0.001   reject normal\n15 recon_1       0.100   normal       \n16 recon_3456    0.001   reject normal\n17 east_new      0.907   normal       \n18 east_old      0.161   normal       \n19 south_II_U    0.048   reject normal\n20 west_3_cvl    0.272   normal       \n\n\nDann das selbe nochmal mit der Funktion nest_by(), die jetzt Vektoren generiert.\n\nclove_tbl %&gt;% \n  nest_by(clove_strain) %&gt;% \n  mutate(shapiro = map(data, ~shapiro.test(.x)),\n         clean = tidy(shapiro)) %&gt;% \n  reframe(clean)\n\n# A tibble: 20 × 4\n   clove_strain  statistic p.value method                     \n   &lt;fct&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                      \n 1 standard          0.863 0.272   Shapiro-Wilk normality test\n 2 west_rck_1        0.863 0.272   Shapiro-Wilk normality test\n 3 south_III_V       0.972 0.855   Shapiro-Wilk normality test\n 4 west_rck_2_II     0.940 0.653   Shapiro-Wilk normality test\n 5 comb_001          0.801 0.103   Shapiro-Wilk normality test\n 6 western_4         0.971 0.849   Shapiro-Wilk normality test\n 7 north_549         0.972 0.855   Shapiro-Wilk normality test\n 8 subtype_09        0.995 0.983   Shapiro-Wilk normality test\n 9 subtype_III_4     0.763 0.0511  Shapiro-Wilk normality test\n10 ctrl_pos          0.998 0.992   Shapiro-Wilk normality test\n11 ctrl_7            0.945 0.683   Shapiro-Wilk normality test\n12 trans_09_I        0.630 0.00124 Shapiro-Wilk normality test\n13 new_xray_9        0.895 0.406   Shapiro-Wilk normality test\n14 old_09            0.630 0.00124 Shapiro-Wilk normality test\n15 recon_1           0.799 0.0996  Shapiro-Wilk normality test\n16 recon_3456        0.630 0.00124 Shapiro-Wilk normality test\n17 east_new          0.981 0.907   Shapiro-Wilk normality test\n18 east_old          0.827 0.161   Shapiro-Wilk normality test\n19 south_II_U        0.760 0.0476  Shapiro-Wilk normality test\n20 west_3_cvl        0.863 0.272   Shapiro-Wilk normality test\n\n\nUnd nochmal mit der Pipe von group_by() zu nest(). Die Funktion nest() hate auch eine .by =-Option, so dass wir auch den Schritt mit group_by() weglassen könnten.\n\nclove_tbl %&gt;%\n  group_by(clove_strain) %&gt;% \n  nest() %&gt;% \n  mutate(shapiro = map(data, ~shapiro.test(.x$germ_rate)),\n         clean = map(shapiro, tidy)) %&gt;% \n  unnest(clean)\n\n# A tibble: 20 × 6\n# Groups:   clove_strain [20]\n   clove_strain  data             shapiro statistic p.value method              \n   &lt;fct&gt;         &lt;list&gt;           &lt;list&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               \n 1 standard      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.863 0.272   Shapiro-Wilk normal…\n 2 west_rck_1    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.863 0.272   Shapiro-Wilk normal…\n 3 south_III_V   &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.972 0.855   Shapiro-Wilk normal…\n 4 west_rck_2_II &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.940 0.653   Shapiro-Wilk normal…\n 5 comb_001      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.801 0.103   Shapiro-Wilk normal…\n 6 western_4     &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.971 0.849   Shapiro-Wilk normal…\n 7 north_549     &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.972 0.855   Shapiro-Wilk normal…\n 8 subtype_09    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.995 0.983   Shapiro-Wilk normal…\n 9 subtype_III_4 &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.763 0.0511  Shapiro-Wilk normal…\n10 ctrl_pos      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.998 0.992   Shapiro-Wilk normal…\n11 ctrl_7        &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.945 0.683   Shapiro-Wilk normal…\n12 trans_09_I    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.630 0.00124 Shapiro-Wilk normal…\n13 new_xray_9    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.895 0.406   Shapiro-Wilk normal…\n14 old_09        &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.630 0.00124 Shapiro-Wilk normal…\n15 recon_1       &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.799 0.0996  Shapiro-Wilk normal…\n16 recon_3456    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.630 0.00124 Shapiro-Wilk normal…\n17 east_new      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.981 0.907   Shapiro-Wilk normal…\n18 east_old      &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.827 0.161   Shapiro-Wilk normal…\n19 south_II_U    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.760 0.0476  Shapiro-Wilk normal…\n20 west_3_cvl    &lt;tibble [4 × 1]&gt; &lt;htest&gt;     0.863 0.272   Shapiro-Wilk normal…"
  },
  {
    "objectID": "programing-purrr-furrr.html#sec-furrr",
    "href": "programing-purrr-furrr.html#sec-furrr",
    "title": "14  Mit purrr und furrr",
    "section": "\n14.5 Mit furrr parallel über Daten",
    "text": "14.5 Mit furrr parallel über Daten\nWarum geht es den jetzt hier? Wenn du purrr und die Funktionen map() verstanden hast, dann geht natürlich alles auch in paralleler Berechnung. Die parallele Berechnung ist in dem R Paket furrr implementiert. Das heißt wir müssen nur die Funktionsnamen ändern und schon rechnet sich alles in Parallel. Wir nutzen also nicht nur einen Kern von unseren Rechnern sondern eben alles was wir haben.\n\nno_cores &lt;- availableCores() - 1\nno_cores\n\nsystem \n     7 \n\n\nEinmal das ganze in sequenzieller Programmierung. Also alles nacheinander gerechnet.\n\nplan(sequential)\n\ntic()\nnothingness &lt;- future_map(c(2, 2, 2), ~Sys.sleep(.x))\ntoc()\n\n6.03 sec elapsed\n\n\nDer folgende Code sollte ca. 2 Sekunden dauern, wenn der Code parallel läuft. Wir haben einen kleinen Overhead in future_map() durch das Senden von Daten an die einzelnen Kerne. Es gibt auch einmalige Zeitkosten für plan(multisession), um die Kerne einzurichten.\n\nplan(multisession, workers = 3)\n\ntic()\nnothingness &lt;- future_map(c(2, 2, 2), ~Sys.sleep(.x))\ntoc()\n\n2.297 sec elapsed\n\n\nWie du siehst, must du nur future_ vor die map() Funktion ergänzen und schon kannst du parallel rechnen."
  },
  {
    "objectID": "programing-purrr-furrr.html#progressr-an-introduction",
    "href": "programing-purrr-furrr.html#progressr-an-introduction",
    "title": "14  Mit purrr und furrr",
    "section": "\n14.6 progressr: An Introduction",
    "text": "14.6 progressr: An Introduction\nDie Funktion map() hat die Option .progress = TRUE mit der du dir auch einen Fortschritt anzeigen lassen kannst. Also wie lange noch die Funktion braucht um über alle Listeneinträge zu rechnen. Wenn du es noch schöner haben willst, dann schaue dir einmal das R Paket progressr: An Introduction an."
  },
  {
    "objectID": "eda-preface.html",
    "href": "eda-preface.html",
    "title": "Explorative Datenanalyse",
    "section": "",
    "text": "Version vom June 20, 2023 um 08:14:33\n\n“Richtiges Auffassen einer Sache und Missverstehen der gleichen Sache schließen einander nicht vollständig aus.” — Franz Kafka, Vor dem Gesetz\n\nWir kürzen die explorative Datenanalyse als EDA ab.\nWir haben die Daten jetzt in R Eingelesen und im Zweifel noch angepasst. Nun wollen wir uns die Daten einmal angucken. Nicht in dem Sinne, dass wir auf die Datentabelle schauen. Sondern wir wollen die Daten visualisieren. Wir erstellen Abbildungen von den Daten und versuchen so mehr über die Daten zu erfahren. Sehen wir Zusammenhänge zwischen verschiedenen Variablen bzw. Spalten? Wir führen eine explorative Datenanalyse durch. Über die explorative Datenanalyse wollen wir uns in diesem Kapitel einmal Gedanken machen."
  },
  {
    "objectID": "eda-descriptive.html#mittelwert",
    "href": "eda-descriptive.html#mittelwert",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.1 Mittelwert",
    "text": "15.1 Mittelwert\nDer Mittelwert einer Zahlenreihe beschreibt den Schwerpunkt der Zahlen. Der Mittelwert wird auch als Lageparameter benannt.  Wir schreiben den Mittelwert mit einem Strich über den Vektor, der die Zahlen enthält. Im folgenden ist die Formel für den Mittelwert der Sprungweite in [cm] der Hunde gezeigt. Der Mittelwert ist in dem Sinne eine künstliche Zahl, da der Mittelwert häufig nicht in den beobachteten Zahlen vorkommt.Der Mittelwert und der Median sind zwei Lageparameter einer Verteilung. Beide beschreiben die Stelle, wo die Verteilungskurve am höchsten ist.\n\n\n\n\n\n\nWir werden immer mal wieder Formeln vereinfachen. Zum Beispiel nur \\(\\sum\\) schreiben anstatt \\(\\sum_i^n\\), wenn wir einen Vektor aufsummieren und uns die Indizes sparen…\n\\[\n\\bar{y} = \\sum_{i=1}^{n}\\cfrac{x_i}{n} =\n\\cfrac{5.7 + 8.9 + 11.8 + 8.2 + 5.6 + 9.1 + 7.6}{7} =\n8.13\n\\]\nIm Durchschnitt oder im Mittel springen Hundeflöhe 8.13 cm weit. In der Abbildung 15.1 wollen wir die Formel nochmal visualisieren. Vielleicht fällt dir dann der Zusammenhang von dem Index \\(i\\) und der gesamten Fallzahl \\(n\\) leichter.\n\n\nAbbildung 15.1— Zusammenhang zwischen \\(y\\) sowie dem Index \\(i\\) in der Formel für den Mittelwert.\n\nIn R können wir den Mittelwert einfach mit der Funktion mean() berechnen. Wir wollen dann den Mittelwert noch auf die zweite Kommastelle runden. Das machen wir dann mit der Funktion round().\nDu findest in Kapitel 10 den Einstieg für die Programmierung in R. Da findest du auch die Erklärung für den Pipe Operator %&gt;%.\n\n## ohne pipe-Operator\nmean(y)\n\n[1] 8.128571\n\n## mit pipe-Operator\ny %&gt;% mean %&gt;% round(2)\n\n[1] 8.13\n\n\nWir erhalten das gleiche Ergebnis wie oben in unserer händischen Rechnung. Die Hundeflöhe springen im Mittel 8.13 cm weit.\nDer Mittelwert ist eine bedeutende Maßzahl der Normalverteilung. Daher merken wir uns hier schon mal, dass wir den Mittelwert brauchen werden. Auch wenn wir darüber nachdenken ob sich zwei Gruppen unterscheiden, so nutzen wir hierzu den Mittelwert. Unterscheiden sich die mittleren Sprungweiten in [cm] von Hunde- und Katzenflöhen?"
  },
  {
    "objectID": "eda-descriptive.html#spannweite",
    "href": "eda-descriptive.html#spannweite",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.3 Spannweite",
    "text": "15.3 Spannweite\nDie Spannweite erlaubt uns zu überprüfen was die kleinste Zahl und die größte Zahl ist. Also uns das Minimum und das Maximum einer Zahlenreihe anzuschauen. Auf den ersten Blick mag das nicht so sinnig sein, aber wenn wir uns hunderte von Beobachtungen anschauen, wollen wir wissen, ob wir nicht einen Fehler bei Eintragen der Daten gemacht haben. Wir wissen eigentlich, dass z.B keine negativen Zuwachsraten auftreten können.\nDie Spannweite dient dazu in einem Datensatz zu überprüfen ob die Spalte, oder auch Variable genannt, den richtigen Zahlenraum aufweist. Das machen wir durch die Funktion range().\n\\[\ny_{range} = y_{max} - y_{min} = 12.1 - 4.9 = 7.2\n\\]\nDie Hundeflöhe springen in einer Spannweite von 7.2 cm. Das kommt einem normal vor. Die Spannweite ist nicht übertrieben groß. Der minimale Wert ist 4.9 und der maximale Wert ist 12.1 und somit sind beide Zahlen in Ordnung. Keine der beiden Zahlen ist übertrieben groß oder gar negativ.\nIn R können wir die Spannweite mit range() wie folgt berechnen. Wir erhalten den minimalen und maximalen Wert.\n\n## ohne pipe-Operator\nrange(y) \n\n[1]  5.6 11.8\n\n## mit pipe-Operator\ny %&gt;% range()\n\n[1]  5.6 11.8\n\n\nWir merken uns, dass die Spannweite eine Maßzahl für die Validität der Daten ist. Hat das Experiment geklappt oder kamen da nur komische Zahlen bei raus, die wir so in der Realität nicht erwarten würden. Zum Beispiel negative Sprungweiten, weil wir einmal auf das Minuszeichen auf der Tastatur beim eingeben der Zahlen gekommen sind."
  },
  {
    "objectID": "eda-descriptive.html#varianz",
    "href": "eda-descriptive.html#varianz",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.4 Varianz",
    "text": "15.4 Varianz\nBis jetzt können wir mit dem Mittelwert \\(\\bar{y}\\) die Lage oder den Mittelpunkt unserer Zahlenreihe beschreiben. Uns fehlt damit aber die Information über die Streuung der Zahlen. Sind die Zahlen alle eher gleich oder sehr verschieden? Liegen die Zahlen daher alle bei dem Mittelwert oder sind die Zahlen weit um den Mittelwert gestreut.\nDie Streuung der Zahlen um den Mittelwert beschreibt die Varianz oder auch \\(s^2\\). Wir berechnen die Varianz indem wir von jeder Zahl den Mittelwert aller Zahlen abziehen und dann das Ergebnis quadrieren. Das machen wir für alle Zahlen und addieren dann die Summe auf. Wir erhalten die Quadratsumme von \\(y\\).\nAbweichungsquadrate sind ein wichtiges Konzept in der Statistik. Wenn wir wissen wollen, wie groß eine Abweichung von einer Zahl zu einer anderen ist, dann nutzen wir immer das Quadrat der Abweichung und bilden die Quadratsumme.\n\\[\ns^2 = \\sum_{i=1}^n\\cfrac{(y_i - \\bar{y})^2}{n-1} = \\cfrac{(5.7 -\n8.13)^2 + ... + (7.6 - 8.13)^2}{7-1} = 4.6\n\\]\nDie Varianz beschreibt also die Streuung der Zahlen im Quadrat um den Mittelwert. Das heißt in unserem Beispiel, dass die Sprungweite eine Varianz von 4.6 cm\\(^2\\) hat. Wir können Quadratzentimeter schlecht interpretieren. Deshalb führen wir gleich die Wurzel der Varianz ein: die Standardabweichung.\nIn R lässt sich die Varianz einfach durch die Funktion var() berechnen.\n\n## ohne pipe-Operator\nvar(y)\n\n[1] 4.599048\n\n## mit pipe-Operator\ny %&gt;% var %&gt;% round(2) \n\n[1] 4.6\n\n\nWir benötigen die Varianz häufig nur als Zwischenschritt um die Standardabweichung zu berechnen. Das Konzept der Abweichungsquadrate benötigen wir aber in der Varianzanalyse (ANOVA) und für die Beschreibung einer Normalverteilung."
  },
  {
    "objectID": "eda-descriptive.html#standardabweichung",
    "href": "eda-descriptive.html#standardabweichung",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.5 Standardabweichung",
    "text": "15.5 Standardabweichung\nDie Standardabweichung ist die Wurzel der Varianz. Wo die Varianz die Abweichung der Sprungweite in [cm\\(^2\\)] beschreibt, beschreibt die Standardabweichung die Streuung der Sprungweite in [cm].\n\\[\ns = \\sqrt{s^2} = \\sqrt{4.6} = 2.14\n\\] Wir schreiben immer den Mittelwert plusminus die Standardabweichung. Also immer \\(\\bar{y} \\pm s\\).\nWir können also schreiben, dass die Flöhe im Mittel 8.13 \\(\\pm\\) 2.14cm weit springen. Somit haben wir die Lage und die Streuung der Zahlenreihe \\(y\\) der Sprungweite in [cm] mit zwei Zahlen beschrieben.\nIn R können wir die Standardabweichung einfach mit der Funktion sd() berechnen.\n\n## ohne pipe-Operator\nsd(y)\n\n[1] 2.144539\n\n## mit pipe-Operator\ny %&gt;% sd %&gt;% round(2) \n\n[1] 2.14"
  },
  {
    "objectID": "eda-descriptive.html#mittelwert-und-varianz---eine-herleitung",
    "href": "eda-descriptive.html#mittelwert-und-varianz---eine-herleitung",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.6 Mittelwert und Varianz - eine Herleitung",
    "text": "15.6 Mittelwert und Varianz - eine Herleitung\nWas ist der Mittelwert und die Varianz genau? Schauen wir uns das einmal in Abbildung 15.2 an. Die graue Linie oder Grade beschreibt den Mittelwert der fünf Beobachtungen. Die fünf Beobachtungen sind als blaue Punkt dargestellt. Auf der x-Achse ist nur der Index des Punktes. Das heißt \\(y_1\\) ist der erste Punkte, das der Index \\(i\\) gleich 1 ist.\n\n\nAbbildung 15.2— Die graue Linie beschreibt den Mittelwert der genau so durch die blauen Punkte geht, dass die Abstände der Punkte oberhalb und unterhalb zu Null aufaddieren. Die Linie liegt in der Mitte der Punkte. Die quadrierten Abstände sind die Varianz der blauen Punkte. Auf der x-Achse ist der Index des Punktes eingetragen.\n\nWenn wir die Summe der Abweichungen von \\(y_1\\) bis \\(y_5\\) zu dem Mittelwert bilden, so wird diese Summe 0 sein. Der Mittelwert liegt genau in der Mitte der Punkte. In unserem Beispiel ist der Mittelwert \\(\\bar{y} = 5.8\\). Wir können jetzt die Abstände wie in der folgenden Tabelle 15.1 berechnen.\n\n\nTabelle 15.1— Zusammenhang von den Werten von \\(y\\), dem Mittelwert sowie die Abweichung vom Mittelwert \\(\\epsilon\\)\n\n\n\n\n\n\n\n\n\nIndex \\(i\\)\n\ny\n\\(\\boldsymbol{y_i - \\bar{y}}\\)\nWert\n\\(\\boldsymbol{\\epsilon}\\)\n\n\n\n1\n5.7\n\\(y_1 - \\bar{y}\\)\n\\(5.7 - 8.13 = -2.43\\)\n\\(\\epsilon_1\\)\n\n\n2\n8.9\n\\(y_2 - \\bar{y}\\)\n\\(8.9 - 8.13 = 0.77\\)\n\\(\\epsilon_2\\)\n\n\n3\n11.8\n\\(y_3 - \\bar{y}\\)\n\\(11.8 - 8.13 = 3.67\\)\n\\(\\epsilon_3\\)\n\n\n4\n8.2\n\\(y_4 - \\bar{y}\\)\n\\(8.2 - 8.13 = 0.07\\)\n\\(\\epsilon_4\\)\n\n\n5\n5.6\n\\(y_5 - \\bar{y}\\)\n\\(5.6 - 8.13 = -2.53\\)\n\\(\\epsilon_5\\)\n\n\n6\n9.1\n\\(y_4 - \\bar{y}\\)\n\\(9.1 - 8.13 = 0.97\\)\n\\(\\epsilon_4\\)\n\n\n7\n7.6\n\\(y_5 - \\bar{y}\\)\n\\(7.6 - 8.13 = -0.53\\)\n\\(\\epsilon_5\\)\n\n\n\n\nWir nennen die Abstände \\(y_i - \\bar{y}\\) nach dem griechischen Buchstaben Epsilon \\(\\epsilon\\). Das \\(\\epsilon\\) soll an das \\(e\\) von Error erinnern. So meint dann Error eben auch Abweichung. Ja, es gibt hier viele Worte für das gleiche Konzept.\nWir berechnen einen Mittelwert von den Epsilons mit \\(\\bar{\\epsilon} = 3.4892724\\times 10^{-16}\\). Ein Mittelwert nahe Null bzw. von Null wundert uns nicht. Wir haben die Gerade ja so gebaut, das nach oben und unten die gleichen Abstände sind. Die Varianz \\(s^2\\) der \\(y\\) ist \\(s_y^2 = 4.599\\) und die Varianz von \\(\\epsilon\\) ist \\(s_{\\epsilon}^2 = 4.599\\). In beiden Fällen ist die Zahl gleich."
  },
  {
    "objectID": "eda-descriptive.html#standardfehler-oder-standard-error-se",
    "href": "eda-descriptive.html#standardfehler-oder-standard-error-se",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.7 Standardfehler oder Standard Error (SE)",
    "text": "15.7 Standardfehler oder Standard Error (SE)\nWenn wir den Mittelwert der Sprungweiten berichten dann gehört die Standardabweichung der Sprungweiten mit als beschreibendes Maß dazu. Wir berichten keinen Mittelwert ohne Standardabweichung.\nNun ist es aber so, dass der Mittelwert und die Standardabweichung von der Fallzahl abhängen. Je mehr Fallzahl bzw. Beobachtungen wir haben, desto genauer wird der Mittelwert sein. Oder anders ausgedrückt \\(\\bar{y}\\) wird sich \\(\\mu_y\\) annähern. Das gleiche gilt auch für die Standardabweichung \\(s_y\\), die sich \\(\\sigma_y\\) mit steigender Fallzahl annähert.\nAus diesem Grund brauchen wir noch einen Fehler bzw. eine Maßzahl für die Streuung, die unabhängig von der Fallzahl ist. Wir skalieren also die Standardabweichung mit der Fallzahl indem wir die Standardabweichung durch die Wurzel der Fallzahl teilen.\n\\[\nSE = \\cfrac{s}{\\sqrt{n}} = \\cfrac{2.14}{2.65} = 0.81\n\\]\nWir müssten ein Paket in R laden um den Standardfehler zu berechnen. Das Laden von zusätzlichen Paketen wollen wir hier aber vermeiden; wir können den Standardfehler auch einfach selber berechnen.\n\nse &lt;- sd(y)/sqrt(length(y))\nse %&gt;% round(2)\n\n[1] 0.81\n\n\nWir erhalten einen Standardfehler von 0.81. Diese Zahl ist in dem Sinne nicht zu interpretieren, da wir hier nur Experimente losgelöst von deren Fallzahl miteinander vergleichen können. Auf der anderen Seite können wir ohne die berichtete Fallzahl nicht vom Standardfehler auf die Standardabweichung schließen.\nWir berichten den Standardfehler immer zusammen mit der Fallzahl, so dass die Standardabweichung berechnet werden kann.\nWir benötigen den Standardfehler eigentlich nicht zum Berichten von Ergebnissen. Der Standardfehler ist nicht als Zahl interpretierbar und somit eine reine statistische Größe. Tabelle 15.2 zeigt die Zusammenfassung und den Vergleich von Standardabweichung und Standardfehler.\n\n\nTabelle 15.2— Zusammenfassung und Vergleich von Standardabweichung und Standardfehler\n\n\n\n\n\nStandardabweichung\nStandardfehler\n\n\n\n… ist eine Aussage über die Streuung der erhobenen Werte einer Stichprobe.\n… ist eine Aussage über die Genauigkeit des Mittelwertes einer Stichprobe.\n\n\n… hängt von der biologischen Variabilität ab.\n… abhängig von der Messgenauigkeit\n\n\n… ist ein beschreibendes Maß.\n… ist ein statistisches Maß.\n\n\n… ist nur wenig durch die Größe der Stichprobe beineinflussbar.\n… steht im direkten Verhältnis zur Größe der Stichprobe.\n\n\n\n\n\n\nDer Standardfehler oder Standard Error (SE) oder Standard Error of the Mean (SEM) wird uns wieder beim statistischen Testen und dem t-Test begegnen.\n\\[\nT_{calc} = \\cfrac{\\bar{y_1} - \\bar{y_2}}{s_p \\cdot \\sqrt{\\tfrac{2}{n}}} \\approx \\cfrac{\\bar{y_1} - \\bar{y_2}}{SEM}\n\\]\nDer Nenner beim t-Test kann als Standardfehler gesehen werden. Wir benötigen den Standardfehler also im Kontext des statistischen Testen als eine statistische Maßzahl.\n\n\n\n\n\n\nStandardfehler wird in der Metaanalyse genutzt\n\n\n\nDer Standardfehler ist bedeutend in der Metaanalyse. Also dem gemeinsamen Auswerten von mehreren klinischen Studien. Du kannst im Buch Doing Meta-Analysis with R: A Hands-On Guide mehr darüber erfahren. Wir nutzen keine Metaanalysen in den Grundlagenveranstaltungen."
  },
  {
    "objectID": "eda-descriptive.html#median",
    "href": "eda-descriptive.html#median",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.8 Median",
    "text": "15.8 Median\nWir wollen uns jetzt noch eine andere Art der Zusammenfassung von Zahlen anschauen. Anstatt mit den Zahlen zu rechnen, sortieren wir jetzt die Zahlen aus dem Vektor \\(y = \\{5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6\\}\\) nach dem Rang. Wir rechnen dann mit den Rängen. Die kleinste Zahl kriegt den kleinsten Rang. Wir können R nutzen über due Funktion sort() um den Vektor \\(y\\) zu sortieren.\n\ny %&gt;% sort()\n\n[1]  5.6  5.7  7.6  8.2  8.9  9.1 11.8\n\n\nDer Median \\(\\tilde{y}\\) ist die mittlere Zahl eines Zahlenvektors. Wir haben hier sieben Zahlen, also ist der Median die vierte Zahl. Wir müssen hier aber zwischen einr ungeraden Anzahl und einer geraden Anzahl unterscheiden.\n\n\nUngerade Anzahl von Zahlen, der Median ist die mittlere Zahl des Vektors \\(y\\): \\[\n5.6,  5.7,  7.6,  \\underbrace{8.2,}_{Median}  8.9,  9.1, 11.8\n\\]\n\n\nIn R können wir den Median einfach mit der Funktion median()berechnen.\n\n## ohne pipe-Operator\nmedian(y) \n\n[1] 8.2\n\n## mit pipe-Operator\ny %&gt;% median()\n\n[1] 8.2\n\n\n\n\nGerade Anzahl von Zahlen, der Median ist der Mittelwert der beiden mittleren Zahlen des Vektors \\(y\\). Ich habe hier einfach die Zahl 13.1 aus dem Hut gezaubert. Es könnte auch eine beliebige andere Zahl sein, die größer als 11.8 ist. Nur damit wir hier eine gerade Anzahl an Zahlen haben: \\[\n5.6,  5.7,  7.6,  \\underbrace{8.2, 8.9,}_{Median = \\tfrac{8.2+8.9}{2}=8.55} 9.1, 11.8, \\color{blue}{13.1}\n\\].\n\nIn R können wir den Median wieder einfach mit der Funktion median()berechnen. Wir müssen nur die Zahl 13.1 zu dem Vektor y mit der Funktion c() hinzufügen.\n\nc(y, 13.1) %&gt;% median() \n\n[1] 8.55\n\n\nWenn der Mittelwert stark von dem Median abweicht, deutet dies auf eine schiefe Verteilung oder aber Ausreißer in den Daten hin. Wir müssen dann in der explorativen Datenanalyse der Sachlage nachgehen\nDer Median ist eine Alternative zu dem Mittelwert. Insbesondere in Fällen, wo es sehr große Zahlen gibt, die den Mittelwert in der Aussage verzerren, kann der Median sinnvoll sein.\n\n\n\n\n\n\nMedian versus Mittelwert\n\n\n\nZur Veranschaulichung des Unterschiedes zwischen Median und Mittelwert nehmen wir die Mietpreise in New York. Der mittlere Mietpreis für eine 2-Zimmerwohnung in Manhattan liegt bei 5000$ pro Monat. In den mittleren Mietpreis gehen aber auch die Mieten der Billionaires’ Row mit ein. Der mediane Mietpreis liegt bei 4000$. Die hohen Mieten ziehen den Mittelwert nach rechts."
  },
  {
    "objectID": "eda-descriptive.html#quantile-und-quartile",
    "href": "eda-descriptive.html#quantile-und-quartile",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.9 Quantile und Quartile",
    "text": "15.9 Quantile und Quartile\nBei dem Mittelwert beschreibt die Standardabweichung die Streuung der Daten um den Mitelwert. Bei dem Median sind dies die Quartile. Die Quartile beschreiben die Streuung der Daten um den Median. Um die Quartile bestimmen zu können, teilen wir die Daten in 100 Quantile. Du kannst dir Quantile wie Prozente vorstellen. Wir schneiden die Daten also in 100 Scheiben. Das geht natürlich erst wirklich, wenn wir hundert Zahlen haben. Deshalb hilft man sich mit Quartilen - von Quarta, ein Viertel - aus. Tabelle 15.3 zeigt den Zusammenhang.\n\n\nTabelle 15.3— Zusammenfassung und Vergleich von Quantilen, Quartilen und Median\n\nQuantile\nQuartile\nMedian\n\n\n\n25% Quantile\n1\\(^{st}\\) Quartile\n\n\n\n50% Quantile\n2\\(^{nd}\\) Quartile\nMedian\n\n\n75% Quantile\n3\\(^{rd}\\) Quartile\n\n\n\n\n\nWir bestimmen die Quartile wie den Median. Wir müssen unterscheiden, ob wir eine ungerade Anzahl an Zahlen oder eine gerade Anzahl an Zahlen vorliegen haben.\n\nUngerade Anzahl von Zahlen, das 1\\(^{st}\\) Quartile ist die mittlere Zahl des unteren Mittels und das 3\\(^{rd}\\) Quartile ist die mittlere Zahl des oberen Mittels des Vektors \\(y\\): \\[\n5.6,  \\underbrace{5.7,}_{1st\\ Quartile}  7.6,  8.2,  8.9,  \\underbrace{9.1,}_{3rd\\ Quartile} 11.8\n\\]\nGerade Anzahl von Zahlen, das 1\\(^{st}\\) Quartile ist der Mittelwert der beiden mittleren Zahl des unteren Mittels und das 3\\(^{rd}\\) Quartile ist der Mittelwert der beiden mittleren Zahlen des oberen Mittels des Vektors \\(y\\). Ich habe hier einfach die Zahl 13.1 aus dem Hut gezaubert. Es könnte auch eine beliebige andere Zahl sein, die größer als 11.8 ist. Nur damit wir hier eine gerade Anzahl an Zahlen haben: \\[\n5.6,  \\underbrace{5.7, 7.6,}_{1st\\ Quartile = \\tfrac{5.7+7.6}{2}=6.65}    8.2,  8.9,  \\underbrace{9.1, 11.8}_{3rd\\ Quartile = \\tfrac{9.1+11.8}{2}=10.45} \\color{blue}{13.1}\n\\]\n\nDas 95% Quantile und das 97.25% Quantile werden wir später nochmal im statistischen Testen brauchen. Auch hier ist die Idee, dass wir die Daten in hundert Teile schneiden und uns dann die extremen Zahlen anschauen.\nIn R können wir den Median einfach mit der Funktion quantile() berechnen. Wir berechnen hier das 25% Quantile also das 1\\(^{st}\\) Quartile sowie das 50% Quantile also den Median und das 75% Quantile also das 3\\(^{rd}\\) Quartile.\n\ny %&gt;% quantile(probs = c(0.25, 0.5, 0.75)) %&gt;% round(2)\n\n 25%  50%  75% \n6.65 8.20 9.00 \n\nc(y, 13.1) %&gt;% quantile(probs = c(0.25, 0.5, 0.75)) %&gt;% round(2) \n\n 25%  50%  75% \n7.12 8.55 9.77 \n\n\nWarum unterscheiden sich die händisch berechneten Quartile von den Quartilen aus R? Es gibt verschiedene Arten der Berechnung. In der Klausur nutzen wir die Art und Weise wie die händische Berechnung hier beschrieben ist. Später in der Anwendung nehmen wir die Werte, die R ausgibt. Die Abweichungen sind so marginal, dass wir diese Abweichungen in der praktischen Anwendung ignorieren wollen."
  },
  {
    "objectID": "eda-descriptive.html#interquartilesabstand-iqr",
    "href": "eda-descriptive.html#interquartilesabstand-iqr",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.10 Interquartilesabstand (IQR)",
    "text": "15.10 Interquartilesabstand (IQR)\nDer Interquartilesabstand (IQR) beschreibt den Abstand zwischen dem 1\\(^{st}\\) Quartile und dem 3\\(^{rd}\\) Quartile. Daher ist der Interquartilesabstand (IQR) ähnlich der Spannweite zwischen dem maximalen und minimalen Wert. Wir benötigen das Interquartilesabstand (IQR) in der explorativen Datenanalyse wenn wir einen Boxplot erstellen wollen.\n\\[\nIQR = 3^{rd}\\,\\mbox{Quartile} - 1^{st}\\,\\mbox{Quartile} = 9.1 - 5.7 = 3.4\n\\]\nWir verwenden das IQR als Zahl eher selten.\n\n\n\n\n\n\nParametrik versus Nicht-Parametrik\n\n\n\nWenn wir einen Zahlenvektor wie durch \\(y = \\{5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6\\}\\) beschrieben zusammenfassen wollen, haben wir zwei Möglichkeiten.\n\nDie parametrische Variante indem wir mit den Zahlen rechnen und deskriptive Maßzahlen wie Mittelwert, Varianz und Standardabweichung berechnen. Diese Maßzahlen kommen aber in den Zahlen nicht vor.\nDie nicht-parametrische Variante indem wir die Zahlen in Ränge umwandeln, also sortieren, und mit den Rängen der Zahlen rechnen. Die deskriptiven Maßzahlen wären dann Median, Quantile und Quartile."
  },
  {
    "objectID": "eda-descriptive.html#sec-desc-group-by",
    "href": "eda-descriptive.html#sec-desc-group-by",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.12 Zusammenfassen von Daten per Faktor",
    "text": "15.12 Zusammenfassen von Daten per Faktor\nGut und soll ich jetzt für jeden Faktorlevel überall den Mittelwert mit mean() berechnen? Geht das nicht einfacher? Ja, geht es. Im folgenden siehst du, wie du den verschiedene deskriptive Maßzahlen in einem Rutsch berechnen kannst.\n\n## Einlesen der Daten aus der Datei flea_dog_cat.xlsx\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n\n## Berechnen der deskriptiven Statistiken \n## getrennt für beide Tierarten\ndata_tbl %&gt;%\n  mutate(animal = as_factor(animal)) %&gt;%\n  group_by(animal) %&gt;%\n  reframe(mean = mean(jump_length),\n            sd = sd(jump_length),\n            median = median(jump_length),\n            quantiles = quantile(jump_length, \n                                 probs = c(0.25, 0.5, 0.75))) %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 6 × 5\n  animal  mean    sd median quantiles\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 dog     8.13  2.14    8.2      6.65\n2 dog     8.13  2.14    8.2      8.2 \n3 dog     8.13  2.14    8.2      9   \n4 cat     4.74  1.9     4.3      3.65\n5 cat     4.74  1.9     4.3      4.3 \n6 cat     4.74  1.9     4.3      5.75"
  },
  {
    "objectID": "eda-ggplot.html#genutzte-r-pakete",
    "href": "eda-ggplot.html#genutzte-r-pakete",
    "title": "16  Visualisierung von Daten",
    "section": "\n16.1 Genutzte R Pakete",
    "text": "16.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, readxl, ggmosaic, \n               janitor, see, patchwork)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "eda-ggplot.html#grundlagen-in-ggplot",
    "href": "eda-ggplot.html#grundlagen-in-ggplot",
    "title": "16  Visualisierung von Daten",
    "section": "\n16.2 Grundlagen in ggplot()",
    "text": "16.2 Grundlagen in ggplot()\nIm Gegensatz zu dem Pipe-Operator %&gt;% nutzt ggplot den Operator + um die verschiedenen ggplot Funktionen (geom_) miteinander zu verbinden.\nWir nutzen in R das R Paket ggplot2 um unsere Daten zu visualisieren. Die zentrale Idee von ggplot2 ist, dass wir uns eine Abbildung wie ein Sandwich bauen. Zuerst legen wir eine Scheibe Brot hin und legen uns dann Scheibe für Scheibe weitere Schichten übereinander. Oder die Idee eines Bildes, wo wir erst die Leinwand definieren und dann Farbschicht über Farbschicht auftragen. Das Konzept von ggplot2ist schlecht zu beschreiben deshalb habe ich auch noch zwei Videos hierfür gemacht. Um den Prozess von ggplot2 zu visualisieren…\n\n\n\n\n\n\nGrundlagen von ggplot() im Video\n\n\n\nDu findest auf YouTube Einführung in R - Teil 16.0 - Trockenübung ggplot2 simpel und einfach erklärt als Video.\nSowie auch auf YouTube Einführung in R - Teil 16.1 - Abbildungen mit ggplot in R erstellen. Idee und Konzept von ggplot als Video. Also alles nochmal als Video - vielleicht einfacher nachzuvollziehen als in einem Fließtext.\n\n\nDie Funktion ggplot() ist die zentrale Funktion, die die Leinwand erschafft auf der wir dann verschiedene Schichten aufbringen werden. Diese Schichten heißen geom. Es gibt nicht nur ein geom sondern mehrere. Zum Beispiel das geom_boxplot für die Erstellung von Boxplots, das geom_histogram für die Erstellung von Histogrammen. Die Auswahl ist riesig. Die einzelnen Schichten werden dann über den Operator + miteinander verbunden. Soviel erstmal zur Trockenübung. Schauen wir uns das ganze einmal an einem Beispiel an.\n\n16.2.1 Datenbeispiel\nWir importieren den Datensatz flea_cat_dog.xlsx und wollen einzelne Variablen visualisieren. Wir kennen den Datensatz schon aus den vorherigen Beispielen. Dennoch nochmal hier der Datensatz in Tabelle 16.1 einmal dargestellt.\n\nflea_dog_cat_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\") %&gt;% \n  mutate(animal = as_factor(animal))\n\nSpaltennamen sind in Englisch und haben keine Leerzeichen. Die Funktion clean_names() aus dem R Paket janitor ist hier eine Hilfe.\nIm Folgenden ist es wichtig, dass du dir die Spaltennamen merkst. Wir können nur die exakten, wortwörtlichen Spaltennamen verwenden. Sonst erhalten wir einen Fehler. Deshalb haben wir auch keine Leerzeichen in den Spaltennamen.\n\n\n\n\nTabelle 16.1— Beispieldatensatz für Eigenschaften von Flöhen von zwei Tierarten.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\ncat\n3.2\n12\n7\n1\n\n\ncat\n2.2\n13\n5\n0\n\n\ncat\n5.4\n11\n7\n0\n\n\ncat\n4.1\n12\n6\n0\n\n\ncat\n4.3\n16\n6\n1\n\n\ncat\n7.9\n9\n6\n0\n\n\ncat\n6.1\n7\n5\n0\n\n\n\n\n\n\n\n16.2.2 Erste Abbildung in ggplot()\nDer folgende R Code erstellt die Leinwand in der Abbildung 16.1 für die folgende, zusätzliches Schichten (geom).\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal , y = jump_length))\n\nWir schauen uns einmal den Code im Detail an.\n\n\nggplot ruft die Funktion auf. Die Funktion ist dafür da den Plot zu zeichnen.\n\ndata = flea_dog_cat_tbl bennent den Datensatz aus dem der Plot gebaut werden soll.\n\naes()ist die Abkürzung für aesthetics und beschreibt, was auf die x-Achse soll, was auf die y-Achse soll sowie ob es noch andere Faktoren in den Daten gibt.\n\n\nx braucht den Spaltennamen für die Variable auf der x-Achse.\n\ny braucht den Spaltennamen für die Variable auf der y-Achse.\n\n\n\nFaktoren meint hier andere Gruppenvariablen. Variablen sind ein anderes Wort für Spalten. Also Variablen die wir mit as_factorerschaffen haben.\n\n\n\n\nAbbildung 16.1— Leere ggplot() Leinwand mit den Spalten animal und jump_length aus dem Datensatz flea_dog_cat_tbl.\n\n\n\nWir sehen, dass wir nichts sehen in Abbildung 16.1. Der Grund ist, dass wir noch kein geom hinzugefügt haben. Das geom beschreibt nun wie die Zahlen in der Datentabelle flea_dog_cat_tbl visualisiert werden sollen.\n\n\n\n\n\n\nHistogramm, Boxplot, Scatterplot und Mosaicplot im Video\n\n\n\nDu findest auf YouTube Einführung in R - Teil 16.2 - Histogramm, Boxplot, Scatterplot und Mosaicplot mit ggplot in R als Video. Weitere Videos werden dann noch folgen und ergänzt.\n\n\n\n16.2.3 Histogramm\nWir nutzen für die Erstellung eines Histogramms den Datensatz dog_fleas_hist.csv. Wir brauchen für ein anständiges Histogramm, wo du auch was erkennen kannst, mindestens 20 Beobachtung. Am besten mehr noch mhr Beobachtungen. Deshalb schauen wir uns jetzt einmal 39 Hunde an und zählen wieviele Flöhe die Hunde jeweils haben, dargestellt in der Spalteflea_count. Darüber hinaus bestimmen wir auch noch das mittlere Gewicht der Flöhe auf dem jeweiligen Hund, dargestellt in der Spalte flea_weight.\n\ndog_fleas_hist_tbl &lt;- read_csv(\"data/dog_fleas_hist.csv\")\n\n\n\n\n\nTabelle 16.2— Beispieldatensatz für die Anzahl an Flöhen auf 39 Hunden. Gezählt wurde die Anzahl an Flöhen flea_count und das gemittelte Gewicht der Flöhe flea_weight.\n\nflea_count\nflea_weight\n\n\n\n0\n0.00\n\n\n1\n7.43\n\n\n4\n21.04\n\n\n2\n20.07\n\n\n1\n21.90\n\n\n0\n0.00\n\n\n2\n24.96\n\n\n1\n27.08\n\n\n5\n16.58\n\n\n1\n19.92\n\n\n0\n0.00\n\n\n0\n0.00\n\n\n2\n24.63\n\n\n4\n21.64\n\n\n3\n20.97\n\n\n1\n23.15\n\n\n0\n0.00\n\n\n3\n14.91\n\n\n1\n19.39\n\n\n2\n17.66\n\n\n1\n19.15\n\n\n1\n25.10\n\n\n2\n26.38\n\n\n2\n19.33\n\n\n2\n13.29\n\n\n1\n17.81\n\n\n0\n0.00\n\n\n2\n23.56\n\n\n1\n18.64\n\n\n1\n15.64\n\n\n3\n19.88\n\n\n1\n18.40\n\n\n1\n25.17\n\n\n0\n0.00\n\n\n0\n0.00\n\n\n\n\n\n\nTabelle 16.2 zeigt den Datensatz dog_fleas_hist.csv. Wir wollen jetzt die Variable flea_count und flea_weight jeweils abbilden. Wir beginnen mit der diskreten Variable flea_count. Im Gegensatz zu der Variable flea_weight haben wir bei der Anzahl gleiche Zahlen vorliegen, die wir dann zusammen darstellen können. Abbildung 16.2 zeigt die Darstellung der Tabelle. Auf der x-Achse ist die Anzahl an Flöhen dargestellt. Auf der y-Achse die Anzahl der jeweiligen Anzahl an Flöhen. Das klingt jetzt etwas schief, aber schauen wir uns die Abbilung näher an.\n\n\n\n\nAbbildung 16.2— Die Anzahl von Flöhen auf 39 Hunden. Jeder Punkt entspricht einem Hund und der entsprechenden Anzahl an Flöhen auf dem Hund.\n\n\n\nWir sehen in Abbildung 16.2 das acht Hunde keine Flöhe hatten - also eine Anzahl an Flöhen von 0. Auf der anderen Seite hatten zwei Hunde vier Flöhe und ein Hund hatte sogar fünf Flöhe. Wir sehen also die Verteilung der Anzahl an Flöhen über alle unsere 39 Hundebeobachtungen.\nWir schauen uns aber die Verteilung der Anzahl an Flöhen meist nicht in der Form von gestapelten Punkten an, sondern in der Form eines Histogramms also einem Balkendiagramm. Abbildung 16.3 zeigt das Histogramm für die Anzahl der Flöhe.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_count)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Anzahl Flöhe\", y = \"Anzahl\") \n\n\n\nAbbildung 16.3— Histogramm der Anzahl von Flöhen auf 39 Hunden.\n\n\n\nWas sehen wir in der Abbildung 16.3? Anstatt von gestapelten Punkten sehen wir jetzt Balken, die die jeweilige Anzahl an Flöhen zusammenfassen. Der Unterschied ist bei einer diskreten Variable wie der Anzahl (eng. count) relativ gering.\nAnders sieht es für kontenuierliche Variablen mit Kommazahlen aus. Schauen wir uns das Gewicht der Flöhe an, so sehen wir, dass es sehr viele Zahlen gibt, die nur einmal vorkomen. Abbildung 16.4 zeigt das Histogramm für das Geicht der Flöhe.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_weight)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Gewicht [mg]\", y = \"Anzahl\") \n\n\n\nAbbildung 16.4— Histogramm des Gewichts von Flöhen auf 39 Hunden.\n\n\n\nWie entsteht nun ein Hisotgramm für konetnierliche Zahlen? Schauen wir uns dafür einmal ein kleineres Datenbeispiel an, in dem wir nur Flöhe mit einem Gewicht größer als 11 und kleiner als 19 wäheln. Wir nutzen dazu die Funktion filter(flea_weight &gt; 11 & flea_weight &lt; 19). Wir erhalten folgende Zahlen und das entsprechende Histogramm.\n\n\n[1] 13.29 14.91 15.64 16.58 17.66 17.81 18.40 18.64\n\n\n\n\nAbbildung 16.5— Zusammenhang zwischen den einzelnen Beobachtungen und der Höhe der einzelnen Balken am Beispiel von acht Hunden.\n\n\n\nAbbildung 16.5 zeigt das Histogramm der reduzierten Daten. Die roten vertikalen Linien zeigen die Position der einzelnen Flohgewichte auf der x-Achse. Die blauen Hilfslinien machen nochmal klarer, wie hoch die einzelnen Balken sind sowie welche Beobachtungen auf der x-Achse in den jeweiligen Balken mit eingehen. Wir sehen, dass wir einen Hund mit Flöhen haben, die zwischen 12.5 und 13.5 wiegen - der entsprechende Balken erhält die Anzahl von eins. Auf der anderen Seite sehen wir, dass es drei Hunde mit Flöhen, die zwischen 17.5 und 18.5 wiegen. Daher wächst der Balken auf eine Anzahl von drei.\nWir können mit der Option binwidth in dem geom_histogram() einstellen, wie breit auf der x-Achse die jeweiligen Balken sein sollen. Hier empfiehlt es sich verschiedene Zahlen für binwidthauszuprobieren.\n\n16.2.4 Density Plot\nEine weitere Möglichkeit sich eine Verteilung anzuschauen, ist die Daten nicht als Balkendiagramm sondern als Densityplot - also Dichteverteilung - anzusehen. Im Prinzip verwandeln wir die Balken in eine Kurve. Damit würden wir im Prinzip unterschiedliche Balkenhöhen ausgleichen udn eine “glattere” Darstellung erreichen. Wir wir aber gleich sehen werden, benötigen wir dazu eine Menge an Beoabchtungen und auch dann ist das Ergebnis eventuell nicht gut zu interpretieren.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_count)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Anzahl Flöhe\", y = \"Anzahl\")\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_count)) +\n  geom_density(fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Anzahl Flöhe\", y = \"Häufigkeit\") \n\n\n\n\n\n(a) Histogramm\n\n\n\n\n\n(b) Densityplot\n\n\n\nAbbildung 16.6— Zusammenhang von Histogramm und Densityplot an der Anzahl der Flöhe auf 39 Hunden.\n\n\n\nAbbildung 16.6 zeigt auf der linken Seite erneut die Abbildung des Histogramms als Balkendiagramm für die Anzahl der Flöhe auf den 39 Hunden. Auf der rechten Seite die entsprechenden gleichen Daten als Denistyplot. Klar ist die Wellenbewegung des Densityplots zu erkennen. Hier leigen zu wenige Beobachtungen und Kategorien auf der x-Achse vor, so dass der Densityplot nicht zu empfehlen ist.\n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_weight)) +\n  geom_histogram(binwidth = 1, fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Gewicht [mg]\", y = \"Anzahl\") \n\nggplot(data = dog_fleas_hist_tbl, aes(x = flea_weight)) +\n  geom_density(fill = \"gray\", color = \"black\") +\n  theme_bw() +\n  labs(x = \"Gewicht [mg]\", y = \"Häufigkeit\") \n\n\n\n\n\n(a) Histogramm\n\n\n\n\n\n(b) Densityplot\n\n\n\nAbbildung 16.7— Zusammenhang von Histogramm und Densityplot am Gewicht der Flöhe auf 39 Hunden.\n\n\n\nAbbildung 16.7 zeigt auf der linken Seite erneut die Abbildung des Histogramms als Balkendiagramm für das Gewicht der Flöhe auf den 39 Hunden. Insbesondere bei dieser Abbildung erkennst du die Nachteile des Densityplot. Dadurch das es einen Peak von acht Hunden mit einem Flohgewicht von 0 gibt, zeigt der Densityplot eine seltsame Wellenform. Es emppfielt sich daher die Daten zuerst als Histogramm zu betrachten.\n\n16.2.5 Boxplot\nMit dem Boxplot können wir den Median und die Quartile visualisieren. In Abbildung 16.8 sehen wir einen Boxplot, der den Median und die Quartile visualisiert. Die Box wird aus dem IQR gebildet. Der Median wird als Strich in der Box gezeigt. Die Schnurrhaare (eng. Whiskers) sind das 1.5 fache des IQR. Punkte die außerhalb der Schnurrhaare liegen werden als einzelne Punkte dargestellt. Diese einzelnen Punkte werden auch als Ausreißer (eng. Outlier) bezeichnet.\n\n\n\nAbbildung 16.8— Ein Boxplot der die statistischen Maßzahlen Median und Quartile visualisiert. Die Box wird aus dem IQR gebildet. Der Median wird als Strich in der Box gezeigt. Die Schnurrhaare sind das 1.5 fache des IQR. Punkte die außerhalb der Schnurrhaare liegen werden als einzele Punkte dargestellt.\n\n\nIn Abbildung 16.9 sehen wir den Zusammenhang zwischen einem Histogramm, Densityplot und dem Boxplot. Der Median \\(\\tilde{y}\\) im Boxplot zeigt die höchste Stelle des Densityplots an. Durch einen Boxplot kann die Verteilung der entsprechenden Zahlen abgeschätzt werden.\n\n\nAbbildung 16.9— Der Zusammenhang von Histogram, Densityplot und Boxplot.\n\nDie “liegende” Darstellung des Boxplots dient nur der Veranschaulichung und dem Verständnis des Zusammenhangs von Histogramm und Boxplot. In der Abbildung 16.10 sehen wir drei Boxplots für einen Faktor mit drei Leveln. Jedes Level wird duch einen Boxplot dargestellt. Zum Beispiel eine Düngerbehandlung mit drei Konzentrationen. Auf der x-Achse würden wir die Behandelung finden und auf der y-Achse das Trockengewicht in [kg/ha].\n\n\nAbbildung 16.10— Typische Darstellung von drei Gruppen jeweils dargestellt durch einen Boxplot. Boxplots werden in der Anwendung stehtend dargestellt. Insbesondere wenn die Boxplots mehrere Gruppen repräsentieren.\n\nWie erstellen wir nun einen Boxplot in R? Zuerst laden wir die Daten mit der Funktion read_excel() in R, wenn du die Daten als .xlsx Datei vorliegen hast. Im XX kannst du nochmal das Importieren von Daten wiederholen.\n\nflea_dog_cat_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n\n\n\n\n\nAbbildung 16.11— An 39 Hunden wurde die Anzahl an Flöhen gezählt.\n\n\n\nIn Abbildung 16.11 ist der Boxplot für die Daten aus der Datei flea_dog_cat.xlsx dargestellt. Auf der x-Achse finden wir die Tierart als cat und dog. Auf der y-Achse ist die Sprungweite in [cm] dargestellt.\nWir erkennen auf einen Blick, dass die Sprungweite von den Hundeflöhen weiter ist als die Sprungweite der Katzenflöhe. Im Weiteren können wir abschätzen, dass die Streuung etwa gleich groß ist. Die Boxen sind in etwa gleich groß und die Whiskers in etwa gleich lang.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = jump_length,\n                                    fill = animal)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.25, shape = 1) +\n  theme_bw() +\n  labs(x = \"Tierart\", y = \"Sprungweite [cm]\") \n\n\n\nAbbildung 16.12— An 39 Hunden wurde die Anzahl an Flöhen gezählt.\n\n\n\nWir neigen dazu die Boxplots über zu interpretieren, wenn die Anzahl der Beobachtungen klein ist. Deshalb können wir mit dem geom_jitter() noch die Beobachtungen zu den Boxplot ergänzen, dargestellt in Abbildung 16.12. Die Funktion geom_jitter() streut die Punkte zufällig, so dass keine Punkte übereinander liegen. Wir haben hier die Streuweite durch die Option width = 0.25 etwas eingeschränkt. Darüber hinaus habe wir das Aussehen der Punkte mit shape = 1 geändert, so dass wir die Jitter-Punkte von den potenziellen Ausreißer-Punkten unterscheiden können. Du kannst auch andere Zahlen hinter shape eintragen um verschiedene Punktesymbole durch zuprobieren. Eine Übersicht an shapes findest du auch hier unter Cookbook for R &gt; Graphs &gt; Shapes and line types.\n\n16.2.6 Barplot oder Balkendiagramm\nDer Barplot oder das Balkendiagramm auch Säulendiagramm ist eigentlich veraltet. Wir haben mit dem Boxplot eine viel bessere Methode um eine Verteilung und gleichzeitig auch die Gruppenunterschiede zu visualisieren. Warum nutzen wir jetzt so viel den Barplot? Das hat damit zu tun, dass früher - oder besser bis vor kurzem - in Excel kein Boxplot möglich war. Daher nutzte jeder der mit Excel seine Daten auswertet den Barplot. Und was der Bauer nicht kennt… deshalb ist hier auch der Barplot dargestellt. Ich persönlich mag den Barplot eher weniger. Der Barplot ist einfach schlechter als der Boxplot. Wir haben nur die Standardabweichung als Maßzahl für die Streuung. Beim Boxplot haben wir das IQR, was uns mehr über die Streuung aussagt. Aber gut, häufig musst du den Barplot in deiner Abschlussarbeit machen. Also dann hier der Barplot. Wie erstellen wir nun einen Barplot in R? Zuerst laden wir die Daten mit der Funktion read_excel() in R, wenn du die Daten als .xlsx Datei vorliegen hast.\n\nflea_dog_cat_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n\nWir müssen jetzt für ggplot() noch den Mittelwert und die Streuung für die Gruppen berechnen. Du kannst als Streuung die Standardabweichung oder den Standardfehler nehmen. Ich würde die Standardabweichung bei kleinen Fallzahlen kleiner als 20 Beobachtungen nehmen.\n\nstat_tbl &lt;- flea_dog_cat_tbl %&gt;% \n  group_by(animal) %&gt;% \n  summarise(mean = mean(jump_length),\n            sd = sd(jump_length),\n            se = sd/sqrt(n()))\n\nWir nutzen nun das Objekt stat_tbl um den Barplot mit der Funktion ggplot() zu erstellen. Dabei müssen wir zum einen schauen, dass die Balken nicht übereinander angeordnet sind. Nebeneinander angeordnete Balken kriegen wir mit der Option stat = \"identity\" in dem geom_bar(). Dann müssen wir noch die Fehlerbalken ergänzen mit dem geom_errorbar. Hier kann nochmal mit der Option width = an der Länge der Fehlerenden gedreht werden.\n\nggplot(stat_tbl, aes(x = animal, y = mean, fill = animal)) + \n    geom_bar(stat = \"identity\") +\n    geom_errorbar(aes(ymin = mean-sd, ymax = mean+sd),\n                  width = 0.2)\n\n\n\n\nIm Zweifel muss du nochmal googlen und schauen welche Form dir am besten zusagt. Es gibt sehr viele Möglichkeiten einen Barplot zu erstellen.\n\n16.2.7 Dotplot\nWenn wir weniger als fünf Beobachtungen haben, dann ist meist ein Boxplot verzerrend. Wir sehen eine Box und glauben, dass wir viele Datenpunkte vorliegen haben. Bei 3 bis 7 Beobachtungen je Gruppe bietet sich der Dotplot als eine Lösung an. Wir stellen hier alle Beobachtungen als einzelne Punkte dar. Wie erstellen wir nun einen Dotplot in R? Zuerst laden wir die Daten mit der Funktion read_excel() in R, wenn du die Daten als .xlsx Datei vorliegen hast.\n\nflea_dog_cat_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\")\n\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = grade,\n                                    fill = animal)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  theme_bw() +\n  labs(x = \"Tierart\", y = \"Boniturnote [1-9]\") \n\n\n\nAbbildung 16.13— Der Dotplot für die Anzahl der Flöhe für die beiden Tierarten Hund und Katze.\n\n\n\nIn Abbildung 16.13 sehen wir den Dotplot aus der Datei flea_dog_cat.xlsx. Auf der x-Achse sind die Level des Faktors animal dargestellt und auf der y-Achse die Notenbewertung grade der einzelnen Hunde und Katzen. Die Funktion geom_dotplot() erschafft das Layer für die Dots bzw. Punkte. Wir müssen in der Funktion noch zwei Dinge angeben, damit der Plot so aussieht, dass wir den Dotplot gut interpretieren können. Zum einen müssen wir die Option binaxis = y wählen, damit die Punkte horizontal geordnet werden. Zum anderen wollen wir auch, dass die Punkte zentriert sind und nutzen dafür die Option stackdir = center.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = grade,\n                            fill = animal)) +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  stat_summary(fun = median, fun.min = median, fun.max = median,\n               geom = \"crossbar\", width = 0.5) +\n  theme_bw() +\n  labs(x = \"Tierart\", y = \"Boniturnote [1-9]\") \n\n\n\nAbbildung 16.14— Der Dotplot für die Anzahl der Flöhe für die beiden Tierarten Hund und Katze. Die schwarze Linie stelt den Median für die beiden Tierarten dar.\n\n\n\nNun macht es wenig Sinn bei sehr wenigen Beobachtungen noch statistische Maßzahlen mit in den Plot zu zeichnen. Sonst hätten wir auch gleich einen Boxplot als Visualisierung der Daten wählen können. In Abbildung 16.14 sehen wir die Ergänzung des Medians. Hier müssen wir etwas mehr angeben, aber immerhin haben wir so eine Idee, wo die “meisten” Beobachtungen wären. Aber auch hier ist Vorsicht geboten. Wir haben sehr wenige Beobachtungen, so dass eine Beobachtung mehr oder weniger große Auswirkungen auf den Median und die Interpretation hat.\n\n16.2.8 Scatterplot\nDer Scatterplot wird auch xy-Plot genannt. Wir stellen in einem Scatterplot zwei kontenuierliche Variablen dar. Dann wollen wir eine Linie durch die Punkte legen. Im Prinzip fragen wir uns, wie hänge die Werte auf der y-Achse von den Werten auf der x-Achse ab? Wenn sich also die Werte auf der x-Achse erhöhen, wie verhalten sich dann die Werte auf der y-Achse?\n\nggplot(data = flea_dog_cat_tbl, aes(x = flea_count, y = jump_length)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  labs(x = \"Anzahl der Flöhe\", y = \"Sprungweite in [cm]\") \n\n\n\nAbbildung 16.15— Zusammenhang zwischen der Sprungweite in [cm] und der Anzahl an Flöhen auf den 39 Hunden. Jeder Punkt stellt einen Hund dar.\n\n\n\nAbbildung 16.15 zeigt den Scatterplot für die Spalte flea_count auf der x-Achse und jump_length auf der y-Achse. Mit der Funktion geom_point() können wir die Punktepaare für jede Beobachtung zeichnen. In unserem Fall zeichnen wir mit der Funktion stat_smooth() noch die entsprechende Grade durch die Punkte. Es handelt sich hierbei um eine Regression, da wir eine Gerade durch die Punktewolke zeichnen.\n\n16.2.9 Mosaic Plot\nWenn wir zwei Spalten visualisieren wollen, die aus zwei Faktoren bestehen mit jeweils zwei Leveln, dann nutzen wir den Mosaic Plot. Wir nutzen den Datensatz flea_dog_cat.xlsx mit vierzehn Beobachtungen. Zuerst drehen wir nochmal die Ordnung der Level in dem Faktor animal.\n\nflea_dog_cat_mosaic_tbl &lt;- flea_dog_cat_tbl %&gt;%\n  mutate(animal = factor(animal, levels = c(\"dog\", \"cat\"))) \n\nSchauen wir uns jetzt einmal die 2x2 Kreuztabelle der beiden Spalten animal and infected an. Um die 2x2 Tabelle in R in der richtigen Orientierung vorliegen zu haben, müssen wir nochmal einen kleinen Klimmzug über mutate() nehmen. Wir wandeln die Variable infected in einen Faktor um und sortieren die Level entsprechend, so dass wir die richtige Ordnung wie später im Mosaic Plot haben. Dieser Umweg hat nur didaktische Gründe, später plotten wir den Mosaic Plot direkt und schauen uns vorher nicht die 2x2 Tabelle in R an. Hier also die 2x2 Kreuztablle aus R.\n\nflea_dog_cat_mosaic_tbl %&gt;% \n  mutate(infected = factor(infected, levels = c(1, 0))) %&gt;% \n  tabyl(infected, animal) \n\n infected dog cat\n        1   3   2\n        0   4   5\n\n\nWir sehen in der Tabelle, dass wir mehr nicht infizierte Tiere (n = 9) als infizierte Tiere haben (n = 5). Die Aufteilung zwischen den beiden Tierarten ist nahezu gleich. Im folgenden wollen wir diese Tabelle durch einen Mosaic Plot einmal visualisieren.\nUm jetzt einen Mosaic Plot zeichnen zu können müssen wir die relativen Anteile pro Spalte bzw. für jedes Level von \\(x\\) berechnen. In unserem Fall ist \\(x\\) die Variable animal und die Level sind dog und cat. In der folgenden 2x2 Kreuztablle werden die relativen Anteile für die Hunde- und Katzenflöhe für den Infektionsstatus berechnet.\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal\n\n\n\n\n\n\nDog\nCat\n\n\n\nInfected\nYes (1)\n\\(\\cfrac{3}{7} = 0.43\\)\n\\(\\cfrac{2}{7} = 0.29\\)\n\\(\\mathbf{5}\\)\n\n\n\nNo (0)\n\\(\\cfrac{4}{7} = 0.57\\)\n\\(\\cfrac{5}{7} = 0.71\\)\n\\(\\mathbf{9}\\)\n\n\n\n\n\\(\\mathbf{7}\\)\n\\(\\mathbf{7}\\)\n\\(n = 14\\)\n\n\n\nWir können jetzt die relativen Anteile in den Mosaic Plot übertragen und erhalten die Abbildung 16.16. Wir müssen also zuerst die absoluten Anteile bestimmen um dann die relativen Anteile für die Spalten berechnen zu können. Abschließend zeichnen wir dann den Mosaic Plot. Wir nutzen dafür das R Paket ggmosaic mit der Funktion geom_mosaic().\n\nggplot(data = flea_dog_cat_mosaic_tbl) +\n  geom_mosaic(aes(x = product(infected, animal), fill = animal)) +\n  annotate(\"text\", x = c(0.25, 0.25, 0.75, 0.75), \n                   y = c(0.25, 0.75, 0.25, 0.85), \n           label = c(\"0.57\", \"0.43\", \"0.71\", \"0.29\"), size = 7) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 16.16— Visualisierung einer 2x2 Tabelle als Mosaic Plot. Die unterschiedlich großen Flächen geben die Verhältnisse per Spalte wieder.\n\n\n\nAbbildung 16.16 zeigt den Mosaic Plot für die Variable animal and infected. Die untrschiedlich großen Flächen bilden die Verhältnisse der 2x2 Tabelle ab. So sehen wir, dass es mehr uninfizierte Tiere als infizierte Tiere gibt. Am meisten gibt es uninfizierte Katzen. Am wenigstens treten infizierte Katzen auf."
  },
  {
    "objectID": "eda-ggplot.html#überschriften-achsen-und-legenden",
    "href": "eda-ggplot.html#überschriften-achsen-und-legenden",
    "title": "16  Visualisierung von Daten",
    "section": "\n16.3 Überschriften, Achsen und Legenden",
    "text": "16.3 Überschriften, Achsen und Legenden\nWenn du mehr machen willst, also die Überschriften anpassen oder aber die Achsenbeschriftung ändern, dann gibt es hier global Hilfe im ggplot Manual. Die Webseite R Cookbook hat auch spezielle Hilfe für ggplot().\n\nÜberschriften von Abbildungen\nAchsenbeschriftung\nLegende\nFarben\n\nIn Abbildung 16.17 siehst du eine Abbildung mit Titel und veränderten Beschriftungen. Die Möglichkeiten sind nahezu unbegrenzt und sprengen auch hier den Rahmen. Im Zweifel im R Tutorium vorbeischauen oder aber in der Vorlesung fragen.\n\nggplot(data = flea_dog_cat_tbl, aes(x = animal, y = jump_length,\n                                    fill = animal)) +\n  geom_boxplot() +\n  labs(title = \"Frischgewicht in Abhängigkeit von der Behandlung\",\n       x = \"Behandlung\", y = \"Frischgewicht in kg/ha\") +\n  scale_x_discrete(labels = c(\"Katze\", \"Hund\")) +\n  scale_fill_discrete(name = \"Behandlung\", labels = c(\"Katze\", \"Hund\")) +\n  theme_bw() \n\n\n\nAbbildung 16.17— Beispielhafte Abbildung mit Titel und geänderter Achsenbeschrittung"
  },
  {
    "objectID": "eda-ggplot.html#die-okabe-ito-farbpalette",
    "href": "eda-ggplot.html#die-okabe-ito-farbpalette",
    "title": "16  Visualisierung von Daten",
    "section": "\n16.4 Die Okabe-Ito Farbpalette",
    "text": "16.4 Die Okabe-Ito Farbpalette\n\n\nMehr zum R Paket see auf der Hilfeseite des Paketes\nNeben den klassischen Farben im R Paket ggplotgibt es noch weit, weit mehr Farbpaletten. Wir nutzen in der Folge immer wieder die Okabe-Ito Farbpalette aus dem R Paket see. Die Okabe-Ito Farbpalette ist speziell so gebaut, dass die Farben sich gut für farbenblinde Personen unterscheiden. Der Kontrast zwischen den Farben ist sehr gut. Wenn du eine andere Farbpalette nutzen willst, findest du hier noch andere Color Scales.\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal, y = jump_length,\n           fill = animal)) +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  theme_bw()\n\n\n\nAbbildung 16.18— Beispielhafte Abbildung der Okabe-Ito Farbpalette für Boxplots.\n\n\n\n\nggplot(data = flea_dog_cat_tbl, \n       aes(x = animal, y = jump_length,\n           color = animal)) +\n  geom_point() +\n  scale_color_okabeito() +\n  theme_bw()\n\n\n\nAbbildung 16.19— Beispielhafte Abbildung der Okabe-Ito Farbpalette für Punkte."
  },
  {
    "objectID": "eda-ggplot.html#abbildungen-nebeneinander",
    "href": "eda-ggplot.html#abbildungen-nebeneinander",
    "title": "16  Visualisierung von Daten",
    "section": "\n16.5 Abbildungen nebeneinander",
    "text": "16.5 Abbildungen nebeneinander\nDas R Paket patchwork erlaubt es mehrere ggplot Abbildungen nebeneinander oder in einem beliebigen Layout miteinander zu verbinden. Das tolle ist, dass die Idee sehr intutiv ist. Wir nutzen wieder das + um verschiedene Plots miteinander zu verbinden.\nIm Folgenden erschaffen wir uns zwei ggplots und speichern die Plots in den Objekten p1 und p2. Das ist wie wir es bisher kennen, nur das jetzt keine Abbildung erscheint sondern beide Plots in zwei Objekten gespeichert sind.\n\np1 &lt;- ggplot(data = flea_dog_cat_tbl, \n             aes(x = flea_count, y = jump_length,\n                 color = animal)) +\n  geom_point() +\n  scale_color_okabeito() +\n  theme_bw()\n\np2 &lt;- ggplot(data = flea_dog_cat_tbl, \n                aes(x = animal, y = jump_length,\n                    color = animal)) +\n  geom_point() +\n  scale_color_okabeito() +\n  theme_bw()\n\nWie können wir nun die beiden Abbildungen nebeneinander zeichnen? Wir nutzen einfach das + Symbol.\n\np1 + p2\n\n\n\nAbbildung 16.20— Beispielhafte Abbildung der zweier Plots nebeneinander.\n\n\n\nAuf der Seite des R Paket patchwork findest du viel mehr Möglichkeiten das Layout anzupassen und auch die einzelnen Subplots zu beschriften."
  },
  {
    "objectID": "eda-transform.html#genutzte-r-pakete-für-das-kapitel",
    "href": "eda-transform.html#genutzte-r-pakete-für-das-kapitel",
    "title": "17  Transformieren von Daten",
    "section": "\n17.1 Genutzte R Pakete für das Kapitel",
    "text": "17.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "eda-transform.html#daten",
    "href": "eda-transform.html#daten",
    "title": "17  Transformieren von Daten",
    "section": "\n17.2 Daten",
    "text": "17.2 Daten\nWir wollen uns in diesem Kapitel mit der normalverteilten Variable jump_length gemessen in [cm] und der nicht-normalverteilten Variable hatch_time gemessen in [h] aus dem Datensatz flea_dog_cat_length_weight.csv\" beschäftigen. Wir wählen über die Funktion select() nur die beiden Spalten aus dem Datensatz, die wir benötigen.\n\ndata_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %&gt;%\n  select(jump_length, hatch_time)\n\nIn der Tabelle 17.1 ist der Datensatz data_tbl nochmal dargestellt. Wir zeigen hier nur die ersten sieben zeilen des Datensatzes.\n\n\n\n\nTabelle 17.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length und der nicht-normalverteilten Variable hatch_time. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\njump_length\nhatch_time\n\n\n\n15.79\n483.60\n\n\n18.33\n82.56\n\n\n17.58\n296.73\n\n\n14.09\n140.90\n\n\n18.22\n162.20\n\n\n13.49\n167.47\n\n\n16.28\n291.20\n\n\n\n\n\n\nIm Folgenden nutzen wir oft die Funktion mutate(). Schau dir im Zweifel nochmal im Kapitel zu Programmierung die Funktion mutate() an."
  },
  {
    "objectID": "eda-transform.html#log-transformation",
    "href": "eda-transform.html#log-transformation",
    "title": "17  Transformieren von Daten",
    "section": "\n17.3 \\(log\\)-Transformation",
    "text": "17.3 \\(log\\)-Transformation\nWir nutzen die \\(log\\)-Transformation, wenn wir aus einem nicht-normalverteiltem Outcome \\(y\\) ein approxomativ normalverteiltes Outcome \\(y\\) machen wollen. Dabei ist wichtig, dass wir natürlich auch die Einheit mit \\(log\\)-transformieren.\nIm Folgenden sehen wir die \\(log\\)-Transformation der Variable hatch_time mit der Funktion log(). Wir erschaffen eine neue Spalte im tibble damit wir die beiden Variable vor und nach der \\(log\\)-Transformation miteinander vergleichen können.\n\n\nDas R Paket dlookr hat eine große Auswahl an implementierten Funktionen für \\(y\\)-Transformationen.\n\nlog_tbl &lt;- data_tbl %&gt;% \n  mutate(log_hatch_time = log(hatch_time))\n\nWir können dann über ein Histogramm die beiden Verteilungen anschauen. In Abbildung 17.1 (a) sehen wir die nicht transformierte, rohe Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Abbildung 17.1 (b) zeigt die \\(log\\)-transmutierten Daten. In diesem Fall sehen wir normalverteilte Daten. Wir haben also ein \\(log\\) normalverteiltes Outcome \\(y\\) mit dem wir jetzt weiterechnen können.\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n(b) \\(log\\)-transformierte Daten.\n\n\n\nAbbildung 17.1— Histogramm der nicht transfomierten und transformierten Daten."
  },
  {
    "objectID": "eda-transform.html#quadratwurzel-transformationen",
    "href": "eda-transform.html#quadratwurzel-transformationen",
    "title": "17  Transformieren von Daten",
    "section": "\n17.4 Quadratwurzel-Transformationen",
    "text": "17.4 Quadratwurzel-Transformationen\nDie Quadratwurzel-Transformationen ist eine etwas seltenere Transformation. Meist wird die Quadratwurzel-Transformationen als die schwächere \\(log\\)-Transformation bezeichnet. Wir sehen in Abbildung 17.2 (b) den Grund dafür. Aber zuerst müssen wir aber über die Funktion sqrt() unsere Daten transformieren. Wir können auch die Funktion transform() aus dem R Paket dlookr verwenden und haben eine große Auswahl an möglichen Transformationen. Einfach mal die Hilfeseite von transform() aufrufen und nachschauen.\n\nsqrt_tbl &lt;- data_tbl %&gt;% \n  mutate(sqrt_hatch_time = sqrt(hatch_time),\n         sqrt_hatch_time_2 = transform(hatch_time, \"sqrt\"))\n\nIn Abbildung 17.2 (a) sehen wir die nicht transformierte, rohe Daten. Es gibt einen klaren Peak Schlüpfzeiten am Anfang. Dann läuft die Verteilung langsam nach rechts aus. Wir können nicht annehmen, dass die Schlüpfzeiten normalverteilt sind. Abbildung 17.2 (b) zeigt die Wurzel-transmutierten Daten. Unser Ziel besser normalverteilte Daten vorliegen zu haben, haben wir aber mit der Quadratwurzel-Transformationen nicht erreicht. Die Daten sind immer noch rechtsschief. Wir würden also die \\(log\\)-Transformation bevorzugen.\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n(b) Wurzel-transformierte Daten.\n\n\n\nAbbildung 17.2— Histogramm der nicht transfomierten und transformierten Daten."
  },
  {
    "objectID": "eda-transform.html#standardisierung",
    "href": "eda-transform.html#standardisierung",
    "title": "17  Transformieren von Daten",
    "section": "\n17.5 Standardisierung",
    "text": "17.5 Standardisierung\nDie Standardisierung wird auch \\(z\\)-Transformation genannt. In dem Fall der Standardisierung schieben wir die Daten auf den Ursprung, in dem wir von jedem Datenpunkt \\(y_i\\) den Mittelwert \\(\\bar{y}\\) abziehen. Dann setzen wir noch die Standardabweichung auf Eins in dem wir durch die Standardabweichung \\(y_s\\) teilen. Unser standardisiertes \\(y\\) ist nun standard normalverteilt mit \\(\\mathcal{N(0,1)}\\). Wir nutzen für die Standardisierung folgende Formel.\n\\[\ny_z = \\cfrac{y_i - \\bar{y}}{s_y}\n\\]\nIn R können wir für die Standardisierung die Funktion scale() verwenden. Wir müssen auch nichts weiter in den Optionen von scale() angeben. Die Standardwerte der Funktion sind so eingestellt, dass eine Standardnormalverteilung berechnet wird. Wir können auch die Funktion transform() aus dem R Paket dlookr verwenden.\n\nscale_tbl &lt;- data_tbl %&gt;% \n  mutate(scale_jump_length = scale(jump_length),\n         scale_jump_length_2 = transform(jump_length, \"zscore\"))\n\nIn Abbildung 17.3 (a) sehen wir nochmal die nicht transformierten, rohen Daten. Wir haben in diesem Beispiel die normalvertielte Variable jump_length gewählt. Der Mittelwert von jump_length ist 20.51 und die Standardabweichung ist 3.77. Ziehen wir nun von jedem Wert von jump_length den Mittelwert mit 19.3 ab, so haben wir einen neuen Schwerpunkt bei Null. Teilen wir dann jede Zahl durch 3.36 so haben wir eine reduzierte Spannweite der Verteilung. Es ergibt sich die Abbildung 17.3 (b) als Standardnormalverteilung. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die Sprungweite auf der \\(z\\)-Skala nicht mehr biologisch interpretieren.\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n(b) \\(z\\)-transformierte Daten.\n\n\n\nAbbildung 17.3— Histogramm der nicht transfomierten und transformierten Daten."
  },
  {
    "objectID": "eda-transform.html#normalisierung",
    "href": "eda-transform.html#normalisierung",
    "title": "17  Transformieren von Daten",
    "section": "\n17.6 Normalisierung",
    "text": "17.6 Normalisierung\nAbschließend wollen wir uns nochmal die Normalisierung anschauen. In diesem Fall wollen wir die Daten so transformieren, dass die Daten nur noch in der Spannweite 0 bis 1 vorkommen. Egal wie die Einheiten vorher waren, alle Variablen haben jetzt nur noch eine Ausprägung von 0 bis 1. Das ist besonders wichtig wenn wir viele Variablen haben und anhand der Variablen eine Vorhersage machen wollen. Uns interessieren die Werte in den Variablen an sich nicht, sondern wir wollen ein Outcome vorhersagen. Wir brauchen die Normalisierung später für das maschinelle Lernen und die Klassifikation. Die Formel für die Normalisierung lautet wie folgt.\n\\[\ny_n = \\cfrac{y_i - \\min(y)}{\\max(y) - \\min(y)}\n\\]\nIn R gibt es die Normalisierungsfunktion nicht direkt. Wir könnten hier ein extra Paket laden, aber bei so einer simplen Formel können wir auch gleich die Berechnung in der Funktion mutate() machen. Wir müssen nur etwas mit den Klammern aufpassen. Wir können auch die Funktion transform() aus dem R Paket dlookr verwenden.\n\n\nnorm_tbl &lt;- data_tbl %&gt;% \n  mutate(norm_jump_length = (jump_length - min(jump_length))/(max(jump_length) - min(jump_length)),\n         norm_jump_length_2 = transform(jump_length, \"minmax\"))\n\n\nIn Abbildung 17.4 (a) sehen wir nochmal die nicht transformierten, rohen Daten. In Abbildung 17.4 (b) sehen wir die normalisierten Daten. Hier fällt dann auf, dass die normalisierten Sprungweiten nur noch Werte zwischen Null und Eins annehmen. Die Zahlen der auf der x-Achse haben jetzt aber keine Bedeutung mehr. Wie können die normalisierten Sprungweiten nicht mehr biologisch interpretieren.\n\n\n\n\n\n(a) Nicht transformierte, rohe Daten\n\n\n\n\n\n(b) Normalisierte Daten\n\n\n\nAbbildung 17.4— Histogramm der nicht transfomierten und transformierten Daten."
  },
  {
    "objectID": "eda-distribution.html#genutzte-r-pakete",
    "href": "eda-distribution.html#genutzte-r-pakete",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.1 Genutzte R Pakete",
    "text": "18.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, see, readxl)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "eda-distribution.html#daten-für-verteilungen",
    "href": "eda-distribution.html#daten-für-verteilungen",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.2 Daten für Verteilungen",
    "text": "18.2 Daten für Verteilungen\nDamit wir uns auch eine Verteilung anschauen können bruachen wir viele Beobachtungen. Wir haben das ja schon bei den Histogrammen gesehen, wenn wir ein aussagekräftiges Histogramm erstellen wollen, dann brauchen wir mehr als zwanzig Beobachtungen. Daher nehmen wir für dieses Kapitel einmal den Gummibärchendatensatz und schauen uns dort die Variablen gender, height, count_bears und count_color einmal genauer an. Wie immer nutzen wir die Funktion select() um die Spalten zu selektieren. Abschließend verwandeln wir das Geschlecht gender und das module noch in einen Faktor.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  %&gt;%\n  select(year, module, gender, height, count_bears, count_color,\n         most_liked) %&gt;% \n  mutate(gender = as_factor(gender),\n         module = as_factor(module))\n\nWir erhalten das Objekt gummi_tbl mit dem Datensatz in Tabelle 18.1 nochmal dargestellt. Wir brauchen nicht alle Spalten aus dem ursprünglichen Datensatz und somit ist die Tabelle etwas übersichtlicher.\n\n\n\n\nTabelle 18.1— Auszug aus den selektierten Daten zu den Gummibärchendaten.\n\n\n\n\n\n\n\n\n\n\nyear\nmodule\ngender\nheight\ncount_bears\ncount_color\nmost_liked\n\n\n\n2018\nFU Berlin\nm\n193\n9\n3\nlightred\n\n\n2018\nFU Berlin\nw\n159\n10\n5\nyellow\n\n\n2018\nFU Berlin\nw\n159\n9\n6\nwhite\n\n\n2018\nFU Berlin\nw\n180\n10\n5\nwhite\n\n\n2018\nFU Berlin\nm\n180\n10\n6\nwhite\n\n\n2018\nFU Berlin\nm\nNA\n10\n5\nwhite\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n2023\nGirls and Boys Day\nNA\nNA\n9\n4\nNA\n\n\n2023\nGirls and Boys Day\nNA\nNA\n9\n4\nNA\n\n\n2023\nGirls and Boys Day\nNA\nNA\n8\n4\nNA\n\n\n2023\nGirls and Boys Day\nNA\nNA\n8\n3\nNA\n\n\n2023\nGirls and Boys Day\nNA\nNA\n8\n4\nNA\n\n\n2023\nGirls and Boys Day\nNA\nNA\n8\n4\nNA"
  },
  {
    "objectID": "eda-distribution.html#sec-normal",
    "href": "eda-distribution.html#sec-normal",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.3 Die Normalverteilung",
    "text": "18.3 Die Normalverteilung\nWir sprechen in der Statistik auch von Verteilungsfamilien. Daher schreiben wir in R auch family = gaussian, wenn wir sagen wollen, dass unsere Daten einer Normalverteilung entstammen.\nWenn wir von de Normalverteilung sprechen, dann schreiben wir ein \\(\\mathcal{N}\\) Symbol - also ein großes N mit Serifen. Die Normalverteilung sieht aus wie eine Glocke, deshalb wird die Normalverteilung auch Glockenkurve genannt. Im englischen Sprachgebrauch und auch in R nutzen wir dagegen die Bezeichnung nach dem “Entdecker” der Normalverteilung, Carl Friedrich Gauß (1777 - 1985). Wir nennen daher die Normalverteilung auch Gaussian-Verteilung.\nParameter sind Zahlen, die eine Verteilungskurve beschreiben.\nEine Normalverteilung wird ruch zwei Verteilungsparameter definiert. Eine Verteilung hat Parameter. Parameter sind die Eigenschaften einer Verteilung, die notwendig sind um eine Verteilung vollständig zu beschreiben. Im Falle der Normalverteilung brauchen wir zum einen den Mittelwert \\(\\bar{y}\\), der den höchsten Punkt unserer Glockenkurve beschreibt. Zum anderen brauchen wir auch die Standardabweichung \\(s^2_y\\), die die Ausbreitung oder Breite der Glockenkurve bestimmt. Wir beschreiben eine Normalverteilung für eine Stichprobe mit \\(\\bar{y}\\) und \\(s^2_y\\) wie folgt.\n\\[\n\\mathcal{N}(\\bar{y}, s^2_y)\n\\]\nOder mit mehr Details in folgender Form. Wir können hier Verallgemeinern und schreiben in der Grundgesamtheit mit \\(\\mu = \\bar{y}\\) und \\(\\sigma^2 = s^2_y\\). Das heißt, wenn wir unendlich viele Beobachtungen vorliegen hätten, dann wüssetn wir auch den wahren Mittelwert \\(\\mu\\) und die wahre Varianz \\(\\sigma^2\\) der Daten.\n\\[\nf(y \\mid\\mu,\\sigma^2)=\\cfrac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\cfrac{(y-\\mu)^2}{2\\sigma^2}}\\quad -\\infty&lt;y&lt;\\infty\n\\]\nIm Falle der Normalverteilung brauchen wir einen Paramter für den höchsten Punkt der Kurve, sowie einen Parameter für die Ausbreitung, also wie weit geht die Kurve nach links und nach rechts. Je nach \\(\\bar{y}\\) und \\(s^2_y\\) können wir verschiedenste Normalverteilungen vorliegen haben. Eine Sammlung von Verteilungen nennen wir auch Familie (eng. family).\nWir haben Varianzhomogenität vorliegen, wenn \\(s^2_{1} = s^2_{2} = s^2_{3}\\) sind. Wir haben Varianzheterogenität vorliegen, wenn \\(s^2_{1} \\neq s^2_{2} \\neq s^2_{3}\\) sind.\nIn Abbildung 18.1 sehen wir verschiedene Normalverteilungen mit unterschiedlichen Mittelwerten. In Abbildung 18.1 (a) sehen wir eine Varianzhomogenität vorliegen, da die Varianzen in allen drei Normalverteilungen gleich sind. Wir können auch schreiben, dass \\(s^2_{1} = s^2_{2} = s^2_{3} = 2\\). In Abbildung 18.1 (b) haben wir Varianzheterogenität vorliegen, da die Varianzen der Normalverteilungen ungleich sind. Wir können hier dann schreiben, dass \\(s^2_{1} = 6 \\neq s^2_{2} = 1 \\neq s^2_{3} = 3\\) sind. Häufig gehen statistische Verfahren davon aus, dass wir Varianzhomogenität über die Gruppen und daher auch die Normalverteilungen vorliegen haben. Konkret, wenn wir die Sprungweiten in[cm] von Hunde- und Katzenflöhen mit einander vergleichen wollen, dann gehen wir erstmal davon aus, dass die Mittelwerte verschieden sind, aber die Varianzen gleich sind.\n\n\n\n\n\n(a) Drei Normalverteilungen mit Varianzhomogenität.\n\n\n\n\n\n(b) Drei Normalverteilungen unter Varianzheterogenität.\n\n\n\nAbbildung 18.1— Histogramm verschiedener Normalverteilungen mit unterschiedlichen Mittelwerten.\n\n\nIn einer Normalverteilung liegen 68% der Werte innerhalb \\(\\bar{y}\\pm 1 \\cdot s_y\\) und 95% der Werte innerhalb \\(\\bar{y}\\pm 2 \\cdot s_y\\)\nWenn wir eine Normalverteilung vorliegen haben, dann liegen 68% der Werte plus/minus einer Standardabweichung vom Mittelwert. Ebenso liegen 95% der Werte plus/minus zwei Standabweichungen vom Mittelwert. Über 99% der Werte befinden sich innerhalb von drei Standardabweichungen vom Mittelwert. Diese Eigenschaft einer Normalverteilung können wir später noch nutzen um abzuschätzen, ob wir einen relevanten Gruppenunterschied vorliegen haben oder aber ob unsere Daten unnatürlich breit streuen.\nWir nutzen das Wort approximativ wenn wir sagen wollen, dass ein Outcome näherungsweise normalverteilt ist.\nSchauen wir uns die Normalverteilung einmal am Beispiel unserer Gummibärchendaten und der Körpergröße der Studierenden an. Wir färben das Histogramm nach dem Geschlecht ein. In Abbildung 18.2 sehen wir das Ergebnis einmal als Histogramm und einmal als Densityplot dargestellt. Wir können annehmen, dass die Größe approximativ normalverteilt ist.\n\n\n\n\n\n(a) Histogramm.\n\n\n\n\n\n(b) Densityplot.\n\n\n\nAbbildung 18.2— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt.\n\n\nWir können die Funktion rnorm() nutzen um uns zufällige Zahlen aus der Normalverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert mean = und die gewünschte Standardabweichung mit sd = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rnorm() mit zehn Werten.\n\nrnorm(n = 10, mean = 5, sd = 2) %&gt;% round(2)\n\n [1] 6.97 6.24 5.59 5.27 6.30 4.91 4.13 4.92 3.67 4.42\n\n\nDu kannst ja mal den Mittelwert und die Standardabweichung der zehn Zahlen ausrechnen. Da wir es hier mit einer Stichprobe mit zehn Beobachtungen zu tun haben, wird der Mittelwert \\(\\bar{y}\\) und die Standardabweichung \\(s_y\\) sich von den vorher definierten Mittelwert \\(\\mu_y = 5\\) und Standardabweichung \\(\\sigma_y = 2\\) der Grundgesamtheit unterscheiden.\nWir können auch aus unseren Gummibärchendaten für die Körpergröße in [cm] jeweils den Mittelwert und die Standardabweichung getrennt für die Geschlechter berechnen und dann die theoretische Normalverteilung zeichenen. In Abbildung 18.3 (b) und Abbildung 18.3 (d) sehen wir die Verteilung der theoretischen Werte, wenn wir die Mittelwerte und die Standardabweichung aus den Verteilungen in Abbildung 18.3 (a) schätzen. Spannderweise bildet sich den zufällig gezogenen Daten auch eine leichte Schulter bei der Verteilung der Körpergrößen. Auch \\(n = 514\\) vollständige Beobachtungen bedeuten nicht, dass wir eine perfekte Normalverteilung erhalten.\n\n\n\n\n\n(a) Verteilung der beobachteten Werte.\n\n\n\n\n\n(b) Verteilung der theoretischen Werte.\n\n\n\n\n\n\n\n(c) Verteilung der beobachteten Werte.\n\n\n\n\n\n(d) Verteilung der theoretischen Werte.\n\n\n\nAbbildung 18.3— Darstellung der Körpergröße in [cm] für die Geschlechter getrennt. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte. Einmal dargestellt als Histogramm und einmal als Densityplot."
  },
  {
    "objectID": "eda-distribution.html#die-standardnormalverteilung",
    "href": "eda-distribution.html#die-standardnormalverteilung",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.4 Die Standardnormalverteilung",
    "text": "18.4 Die Standardnormalverteilung\nEs gibt viele Normalverteilungen. Eiegntlich gibt es unednlich viele Normalverteilunge, da wir für die Parameter Mittelwert \\(\\bar{y}\\) und die Standardabweichung \\(s_y\\) beliebige Zahlen einsetzen können. Aber es gibt eine besondere Normalverteilung, so dass diese Verteilung einen eigenen Namen hat. Wir sprechen von der Standardnormalverteilung, wenn der Mittelwert gleich Null ist und die Standardabweichung gleich Eins. Du siehst hier nochmal die Standardnormalverteilung ausgeschrieben.\n\\[\n\\mathcal{N}(0, 1)\n\\]\nFolgende Eigenschaften sind der Standardnormalverteilung gegeben. Die Standardnormalverteilung hat eine Fläche von \\(A = 1\\) unter der Kurve. Darüber hinaus liegen 95% der Werte zwischen \\(\\approx -2\\) und \\(\\approx 2\\). Die einzelnen Werte einer Standardnormalverteilung nennen wir \\(z\\)-Werte. Wenn wir eine beliebige Normalverteilung in eine Standardnormalverteilung überführen wollen so machen wir die Umwandlung mit der \\(z\\)-Transformation. Und jetzt fahren wir wieder in die Doppeldeutigkeit in R.\n\n\nAbbildung 18.4— Darstellung von dem Zusammenhang von pnorm(q = 1.96) und qnorm(p = 0.025). Mit der Option lower.tail bestimmen wir auf welche Seite der Verteilung wir sein wollen.\n\nIn Abbildung 18.4 haben wir eine Standardnormalverteilung gegeben. Können jetzt verschiedene Werte auf der \\(x\\)-Achse und die Flächen links und rechts von diesen Werten berechnen. Wir nutzen die Funktion pnorm() wenn wir die Fläche rechts oder links von einem Wert \\(q\\) berechnen wollen.\n\npnorm(q = 1.96, mean = 0, sd = 1, lower.tail = FALSE) %&gt;% \n  round(3)\n\n[1] 0.025\n\n\nWir berechen die Fläche links von \\(q\\) und damit auch die Wahrscheinlichkeit \\(Pr(X \\leq q)\\) mit lower.tail = TRUE. Warum ist die Fläche jetzt eine Wahrscheinlichkeit? Wir haben unter der Kurve der Standardnormalverteilung eine Fläche von \\(A = 1\\). Damit ist jede Fläche auch gleich einer Wahrscheinlichkeit. Wenn wir an der Fläche rechts von \\(q\\) interessiert sind und damit auch an der Wahrscheinlichkeit \\(Pr(X &gt; q)\\) nutzen wir die Option lower.tail = FALSE. Das ist erstmal immer etwas verwirrend, aber schau dir den Zusammenhang nochmal in der Abbildung 18.4 an. Wir brauchen diese Idee von der Fläche ist auch gleich Wahrscheinlichkeit im Kapitel 20 zum statistischen Testen.\nWir können die Berechnung von \\(q\\) zu \\(p\\) auch umdrehen. Wir geben eine Fläche vor und wollen wissen wie der Wert auf der x-Achse zu der entsprechenden Fläche ist. In diesem Fall will ich die Werte zu den Flächen von \\(p = 0.025\\) und \\(p = 0.05\\). Da wir lower.tail = FALSE ausgewählt haben, sind wir auf der rechten Seite der Verteilung.\n\nqnorm(p = c(0.025, 0.05), mean = 0, sd = 1, lower.tail = FALSE) %&gt;% \n  round(3)\n\n[1] 1.960 1.645\n\n\nUnd hier einmal als Gegenprobe mit der Option lower.tail = TRUE. Wir springen dann damit auf die linke Seite der Verteilung und wie zu erwarten erhlaten wir dann auch den negativen Wert für die Fläche von \\(p = 0.05\\).\n\nqnorm(p = 0.05, mean = 0, sd = 1, lower.tail = TRUE) %&gt;% \n  round(3)\n\n[1] -1.645\n\n\nDie ganzen Berechnungen funktionieren natürlich auch, wenn wir nicht die Fläche \\(A=1\\) unterhalb der Standardnormalverteilung hätten. Aber wir nutzen hier eben den Zusammenhang von Fläche zu Wahrscheinlichkeit um mit der Verteilung zu rechnen und Wahrscheinlichkeiten abzuschätzen."
  },
  {
    "objectID": "eda-distribution.html#sec-t-dist",
    "href": "eda-distribution.html#sec-t-dist",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.5 Die t-Verteilung",
    "text": "18.5 Die t-Verteilung\nDie t-Verteilung ist eine Abwandlung der Standardnormalverteilung. Wir haben wieder eine Fläche \\(A = 1\\) unter der Verteilungskurve. Wir benötigen die t-Verteilung, als eine künstliche Verteilung, im Kapitel 22 zum statistischen Testen mit dem t-Test. Wir bezeichnen die t-Verteilung als eine künstliche Verteilung, da wir in der Biologie nichts beobachten können, was t-verteilt ist. Wir nutzen die t-Verteilung nur im statistischen Kontext und in diesem Kontekt nur um uns klar zu machen wie statistisches Testen konzeptionell funktioniert. Anwenden werden wir die Verteilung nicht.\nDer Unterschied ist die Form der t-Verteilung. Wir geben mit der Option df = die Freiheitsgrade der Verteilung an. Hier soll es reichen, dass mit \\(\\lim_{df \\to \\infty}\\) sich die t-Verteilung der Standardnormalverteilung fast gleicht. Bei niedrigeren Freiheitsgraden ist die t-Verteilung nicht mehr so hoch und daher sind die Verteilungsenden weiter nach außen geschoben. Die t-Verteilung ist gestaucht wie wir in Abbildung 18.5 etwas überspitzt gezeichnet sehen. Die Freiheitsgrade hängen direkt an der beobachteten Fallzahl mit \\(df = n_1 + n_2 - 2\\).\n\n\nAbbildung 18.5— Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und damit die Fallzahl, desto näher kommt die t-Verteilung einer Normalverteilung nahe.\n\nWie auch bei der Standardnormalverteilung gilt folgender Zusammenhang, wenn wir die Flächen anhand eines gegebenen t-Wertes berechnen wollen. Wenn wir die Fläche links von dem t-Wert berechnen wollen, also die Wahrscheinlichkeit \\(Pr(X \\leq t)\\), dann nutzen wir die Option lower.tail = TRUE. Wenn wir die Fläche auf der rechten Seite von unserem t-Wert berechnen wollen, dann nutzen wir mit \\(Pr(X &gt; t)\\) die Option lower.tail = FALSE. In der Funktion pt() ist das q= als t= zu lesen. Das macht das Verständnis vielleicht leichter.\n\npt(q = 2.571, df = 5, lower.tail = FALSE) %&gt;% \n  round(3)\n\n[1] 0.025\n\n\nNeben der Berechnung der Wahrscheinlichkeit rechts und links eines gegebenen Wertes \\(t\\) können wir auch \\(t\\) berechnen, wenn wir eine Fläche vorgeben. Das kann uns dann die Funktion qt() liefern. Wir sehen, dass mit steigender Fallzahl und damit steigenden Freiheitsgrad sich der berechnete Wert sich dem Wert der Standardnormalverteilung von \\(1.96\\) für \\(p = 0.05\\) annähert.\n\nqt(p = c(0.025), df = c(5, 10, 20, 100, 1000), lower.tail = FALSE) %&gt;% \n  round(3)\n\n[1] 2.571 2.228 2.086 1.984 1.962\n\n\nWir haben gelernt, dass der Zusammenhang zwischen der Standardnormalverteilung und der t-Verteilung ziemlich stark ist. Nutzen werden wir die t-Verteilung aber nur im Rahmen des statistischen Testens."
  },
  {
    "objectID": "eda-distribution.html#sec-poisson",
    "href": "eda-distribution.html#sec-poisson",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.6 Die Poissonverteilung",
    "text": "18.6 Die Poissonverteilung\nEine weitere wichtige Verteilung ist die Poissonverteilung. Die Poissonverteilung ist eine diskrete Verteilung. Daher kommen nur ganze Zahlen vor. Damit bildet die Poissonverteilung die Zähldaten ab. Wenn wir also etwas Zählen, dann ist diese Variable mit den gezählten Ergebnissen poissonverteilt. Im Folgenden sehen wir die Poissonverteilung einmal dargestellt.\n\\[\n\\mathcal{Pois}(\\lambda)\n\\]\nOder mit mehr Details in folgender Form.\n\\[\nP_\\lambda (k) = \\frac{\\lambda^k}{k!}\\, \\mathrm{e}^{-\\lambda}\n\\]\nDie Poisson-Verteilung gibt dann die Wahrscheinlichkeit einer bestimmten Ereignisanzahl \\(k\\) im Einzelfall an, wenn die mittlere Ereignisrate \\(\\lambda\\) bekannt ist. Im Gegensatz zur Normalverteilung hat die Poissonverteilung nur einen Parameter. Den Lageparameter \\(\\lambda\\) ausgedrückt durch den griechischen Buchstaben Lambda. Eine Poissonverteilung mit \\(\\mathcal{Pois}(4)\\) hat den höchsten Punkt bei vier. Nun hat die Poissonverteilung hat mehrere Besonderheiten. Da die Poissonverteilung keinen Streuungsparameter hat, steigt mit dem \\(\\lambda\\) auch die Streuung. Daher haben Poissonverteilungen mit einem großen \\(\\lambda\\) auch eine große Streuung. ie Ausbreitung der Kurve ist eine Funktion von \\(\\lambda\\) und steigt mit \\(\\lambda\\) an. Du kannst diesen Zusammenhang in Abbildung 18.6 beobachten.\nDarüber hinaus kann eine Poissonverteilung nicht negativ werden. Es kann keine kleinere Zahl als die Null geben. Durch die diskreten Zahlen haben wir auch immer mal Lücken zwischen den Balken der Poissonverteilung. Das passiert besonders, wenn wir eine kleine Anzahl an Beobachtungen haben. Abschließend konvergiert die Poissonverteilung bei großen \\(\\lambda\\) hin zu einer Normalverteilung.\n\n\n\n\nAbbildung 18.6— Histogramm verschiedener Poissonverteilungen.\n\n\n\nSchauen wir uns nun einmal die Poissonverteilung im Beispiel an. In Abbildung 18.7 sehen wir die Histogramme der Anzahl an Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte. Da wir es hier mit Zähldaten zu tun haben, könnte es sich um eine Poissonverteilung handeln. Wie müssen uns nun die Frage stellen, ob die Gummibärchen in einer Tüte und die Anzahl an Farben in einer Tüte wirklich eine zufällige Realistierung sind. Daher eine zufällige Stichprobe der Grundgesamtheit. Wir können diese Annahme überprüfen in dem wir die theoretischen Werte für die beiden Poissonverteilung mit \\(\\mathcal{Pois}(10)\\) und \\(\\mathcal{Pois}(5)\\) genieren.\n\n\n\n\n\n(a) Anzahl an Bärchen\n\n\n\n\n\n(b) Anzahl an Farben\n\n\n\nAbbildung 18.7— Histogramme der Anzahl an Gummibärchen und die Anzahl an Farben in einer Tüte. Es gibt nicht mehr als sechs Farben.\n\n\nWir können die Funktion rpois() nutzen um uns zufällige Zahlen aus der Poissonverteilung ziehen zu lassen. Dazu müssen wir mit n = spezifizieren wie viele Beobachtungen wir wollen und den Mittelwert lambda = angeben. Im Folgenden einmal ein Beispiel für die Nutzung der Funktion rpois() mit zehn Werten.\n\nrpois(n = 10, lambda = 5)\n\n [1] 3 5 4 5 3 3 9 3 5 5\n\n\nEs gibt neben der Poissonverteilung auch die negative Binomialverteilung sowie die Quasi-Poissonverteilung, die es erlauben einen Streuungsparameter für die Poissonverteilung zu schätzen.\nWir können nun auch aus unseren Gummibärchendaten für die Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte die theoretische Poissonverteilung berechnen. In Abbildung 18.8 sehen wir die Verteilung der beobachteten Werte für Anzahl an Bärchen in einer Tüte sowie die Anzahl an Farben in einer Tüte und deren theoretischen Verteilung nach dem geschätzen \\(\\lambda = 10\\) und \\(\\lambda = 5\\). Wir sehen ganz klar, dass die beide Variablen keine Zufallsrealisierung sind. Zum einen haben wir das auch nicht erwartet, es gibt nicht mehr als sechs Farben und zum anderen ist zu vermuten, dass Haribo technisch in den Auswahlprozess eingreift. Wir haben auf jeden Fall eine sehr viel kleinere Streuung als bei einer klassischen Poissonverteilung anzunehmen wäre.\n\n\n\n\n\n(a) Verteilung der beobachteten Anzahl an Bärchen.\n\n\n\n\n\n(b) Verteilung der theoretischen Anzahl an Bärchen.\n\n\n\n\n\n\n\n(c) Verteilung der beobachteten Anzahl an Farben.\n\n\n\n\n\n(d) Verteilung der theoretischen Anzahl an Farben.\n\n\n\nAbbildung 18.8— Darstellung Anzahl an Bärchen und Anzahl an Farben. Es gibt nicht mehr als sechs Farben. Auf der linken Seite die beobachteten Werte und auf der rechten Seite die theoretischen Werte.\n\n\nIn Abbildung 18.9 schauen wir uns nochmal an in wie weit sich die Füllung der Tütchen im Laufe der Jahre entwickelt hat. Die Daten werden ja schon seit 2018 erhoben. Wir schauen uns daher die Densityplot einmal aufgetrennt für die Jahre 2018 bis heute an. Das Jahr 2020 fehlt, da bedingt durch die Coronapandemie keine Präsenslehre stattfand. Wir sehen, dass sich die Verteilung anscheinend in dem Jahr 2022 langsam nach links zu weniger Bärchen in einer Tüte bewegt. Wir bleiben gespannt auf den weiteren Trend.\n\n\n\n\nAbbildung 18.9— Densityplot der Anzahl an Bärchen in einer Tüte aufgetrennt nach den Jahren der Erhebung. Das Jahr 2020 fehlt bedingt durch die Coronapandemie.\n\n\n\nIn Abbildung 18.10 betrachten wir die Verteilung der am meisten gemochten Gummibärchen aufgeteilt nach dem angegebenen Geschlecht im Vergeich zu den Gummibärchen in den Tütchen. Wir sehen, dass Haribo die Tütchen sehr gleichmäßig verteilt und auf die Geschmäcker keinerlei Rücksicht nimmt. Entweder weiß Haribo nichts von den Vorlieben seiner Käufer:innen oder aber es ist dann doch zu viel Aufwand die Produktion anzupassen.\n\n\n\n\n\n(a) Anzahl am liebsten gemochten Gummibärchen aufgeteilt nach Geschlecht.\n\n\n\n\n\n(b) Anzahl der Gummibärchen pro Tüte nach Farbe.\n\n\n\nAbbildung 18.10— Histogramme der am liebsten gemochten Gummibärchchen im Vergleich zum Inhalt der Tütchen."
  },
  {
    "objectID": "eda-distribution.html#sec-binom",
    "href": "eda-distribution.html#sec-binom",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.7 Die Binominalverteilung",
    "text": "18.7 Die Binominalverteilung\nDie Binomialverteilung wird uns vor allem später in den logistischen Regression wieder begegnen. An dieser Stelle ist es wichtig zu wissen, dass wir es bei der Binomialverteilung mit binären Ereignissen zu tun haben. Wir haben nur Erfolg oder nicht. Daher haben wir nur das Ergebnis \\(0/1\\) daher Null oder Eins. Dieses Ergebnis ist im Prinzip auch die Beschreibung eines Patienten, ob dieser krank oder nicht krank ist. Deshalb finden wir die Binomialverteilung auch häufig in einem medizinischen Kontext.\n\n\nEs gibt auch ein schönes Tutorial zur Binomial Distribution von David Arnold.\nSchauen wir uns die Formel für die Binomialverteilung einmal genauer an. Wichtig ist, dass wir etwas \\(n\\)-mal wiederholen und uns dann fragen, wie exakt \\(k\\)-oft haben wir Erfolg.\n\\[\nB(k\\mid p,n)=\n\\begin{cases}\n  \\binom nk p^k (1-p)^{n-k} &\\text{falls} \\quad k\\in\\left\\{0,1,\\dots,n\\right\\}\\\\\n  0            & \\text{sonst.}\n  \\end{cases}\n\\]\nmit\n\n\n\\(n\\) gleich der Anzahl an Versuchen (eng. trails)\n\n\\(k\\) gleich der Anzahl an Erfolgen\n\n\\(p\\) gleich der Wahrscheinlichkeit für einen Erfolg.\n\nBevor wir mit dem Beispiel beginnen können brauchen wir noch etwas mehr für die Berechnung der Formel. Wir brauchen noch für die Berechnung der Binomalverteilung den Binomialkoeffizienten \\(\\tbinom {n}{k}\\), den wir wie folgt bestimmen können. Dabei bedeutet das \\(!\\), dass wir eine Zahl aufmultiplizieren. Daher müssen wir für \\(4!\\) dann wie folgt rechnen \\(4! = 1 \\cdot 2 \\cdot 3 \\cdot 4 = 24\\).\n\\[\n\\binom nk = \\cfrac{n!}{k! \\cdot (n-k)!}\n\\]\nNehmen wir dafür einmal ein Beispiel mit 5 über 3 und schauen uns die Rechnung einmal an. Wir erhalten den Binomialkoeffizienten \\(\\tbinom {5}{3}\\) wie folgt.\n\\[\n\\binom 5 3 = \\frac{5!}{3! \\cdot (5-3)!} = \\frac{5!}{3! \\cdot 2!} = \\frac{1\\cdot 2\\cdot 3\\cdot 4\\cdot 5}{(1\\cdot 2\\cdot 3) \\cdot (1\\cdot 2)} = \\frac{4\\cdot 5}{1\\cdot 2} = 10\n\\]\nViele Taschenrechner können den Binomialkoeffizienten flott ausrechnen. Wenn wir keinen Taschenrechner haben, dann können wir auch das Pascalsche (oder Pascal’sche) Dreieck nutzen. Das Pascalsche Dreieck ist eine Form der grafischen Darstellung der Binomialkoeffizienten \\(\\tbinom {n}{k}\\). Wir sehen einmal in Abbildung 18.11 den Zusammenhang mit dem Binomialkoeffizienten dargestellt. Mit dem Pascalsche Dreieck können wir auch ohne Taschenrechner den Binomialkoeffizienten bestimmen.\n\n\nAbbildung 18.11— Darstellung des Pascalsche (oder Pascal’sche) Dreieckes im Zusammenhang zum Binomialkoeffizienten.\n\nSomit können wir auch einmal ein erweitertes Beispiel der Binomialverteilung rechnen. Was ist die Wahrscheinlichkeit bei \\(n = 5\\) Münzwürfen genau dann \\(k = 2\\) Erfolge zu erzielen, wenn die Münze fair ist und damit gilt \\(p = 0.5\\)?\n\\[\n\\begin{aligned}\nPr(Y = 3) &= \\binom {5}{3} 0.5^{3} (1-0.5)^{5-3} \\\\  \n&= 10 \\cdot 0.5^3 \\cdot 0.5^2 \\\\\n&= 0.31\n\\end{aligned}\n\\]\nWir immer können wir die ganze Rechnung dann auch in R durchführen. Dank der Funktion choose() können wir schnell den Binomialkoeffizienten berechnen. Der Rest ist dann nur noch das Einsetzen.\n\nchoose(5,3) * 0.5^3 * 0.5^2\n\n[1] 0.3125\n\n\nAuch hier geht es natürlich auch in R noch einen Schritt schneller. Leider heißt dann wieder alles anders. Wir wollen x = k = 3 Erfolge aus size = n = 5 Versuchen mit einer Erfolgswahrscheinlichkeit von prob = 0.5. Daran muss man sich dann gewöhnen, dass sich die Begrifflichkeiten dann doch immer mal wieder ändern.\n\ndbinom(x = 3, size = 5, prob = 0.5)\n\n[1] 0.3125\n\n\nWas wäre wenn wir jetzt die Wahrscheinlichkeit \\(Pr(Y \\leq 3)\\) berechnen wollen? Also nicht exakt die Wahrscheinlichkeit für \\(k=3\\) Erfolge sondern eben \\(k\\) Erfolge oder weniger \\(k \\leq 3\\). Dann müssen wir die Wahrscheinlichkeiten für \\(Pr(Y = 0)\\), \\(Pr(Y = 1)\\), \\(Pr(Y = 2 )\\) und \\(Pr(Y = 3)\\) berechnen und diese Wahrscheinlichkeiten aufaddieren.\n\ndbinom(0, 5, 0.5) + dbinom(1, 5, 0.5) + dbinom(2, 5, 0.5) + dbinom(3, 5, 0.5)\n\n[1] 0.8125\n\n\nOder wir rechen einfach die Fläche und damit die Wahrscheinlichkeit links von \\(k = 3\\) aus. Dafür haben wir dann die Funktion pbinom(). Es geht dann eben doch etwas flotter. Wie immer können wir dann über die Option lower.tail = entscheiden, auf welche Seite der Verteilung wir schauen wollen.\n\npbinom(3, 5, 0.5, lower.tail = TRUE)\n\n[1] 0.8125\n\n\nAngenommen, eine Münze wird so gewichtet, dass sie in 60 % der Fälle Kopf ergibt. Wie hoch ist die Wahrscheinlichkeit, dass Sie nach 50 Würfen 25 oder mehr Köpfe erhalten? Dafür können wir dann auch die Funktion pbinom() einmal nutzen. Da wir mehr wollen, also “größer als”, müssen wir rechts von dem berechneten Wert schauen, also auswählen, dass lower.tail = FALSE ist.\n\npbinom(25, 50, 0.6, lower.tail = FALSE)\n\n[1] 0.9021926\n\n\nZum Abschluss schauen wir nochmal in unseren Gummibärchendaten, wie dort ein Histogramm einer binären Variable mit nur zwei Ausprägungen aussehen würde. In Abbildung 18.12 sehen wir einmal das Geschlecht als Balkendiagramm dargestellt. Mehr gibt es zu diesem Diagramm erstmal nicht zu berichten. Bei einer Variable bei einem unbekannten \\(p\\) für eine der Kategorien, ist schwer etwas zu bewerten. Wir sehen aber, dass wir eine sehr schöne Gleichverteilung von den Geschlechtern in den Daten haben.\n\n\n\n\nAbbildung 18.12— Beispiel für eine Binomialverteilung anhand des Geschlechts. Die fehlenden Angaben wurden entfernt."
  },
  {
    "objectID": "eda-distribution.html#sec-uniform",
    "href": "eda-distribution.html#sec-uniform",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.8 Die Uniformverteilung",
    "text": "18.8 Die Uniformverteilung\nDie Gleichverteilung oder Uniformverteilung brauchen wir in der Statistik eher selten. Da wir aber hin und wieder mal auf die Gleichverteilung in technischen Prozessen stoßen, wollen wir uns die Gleichverteilung nochmal anschauen. Wenn wir eine Gleichverteilung vorliegen haben, dann sind alle Kategorien gleich häufig vertreten. Es ergibt sich dann folgende Verteilung als Plateau. Das Eintreten jedes Ereignisses ist gleich wahrscheinlich.\n\\[\nf(y)=\n\\begin{cases}\n  \\cfrac 1{b-a} & a \\le y \\le b\\\\\n  0            & \\text{sonst.}\n\\end{cases}\n\\]\n\n\nAbbildung 18.13— Darstellung der Uniformverteilung zwischen den beiden Punkten \\(a\\) und \\(b\\).\n\nDa gibt es auch sonst wenig mehr zu berichten. Nehmen wir daher nochmal ein technisches Beispiel aus unseren Gummibärchendaten. Wir würden je Farbe 924 Gummibärchen erwarten. Warum ist das so? Wir haben insgesamt 5545 ausgezählt. Wenn jede der sechs Kategorien mit der gleichen Wahrscheinlichkeit auftritt, dann erwarten wir jeweils \\(1/6\\) von der Gesamtzahl. Wir erkennen, dass wir etwas zu wenig grüne Bärchen haben. Ebenso sind die hellroten Bärchen unterrepräsentiert. Dafür haben wir dann zwangsweise etwas mehr an gelben und orangen Gummibärchen. Dennoch würde ich hier von einer Gleichverteilung ausgehen.\n\n\n\n\n\n(a) Verteilung der beobachteten Anzahl der Gummibärchen pro Tüte nach Farbe.\n\n\n\n\n\n(b) Verteilung der theoretischen Anzahl der Gummibärchen pro Tüte nach Farbe.\n\n\n\nAbbildung 18.14— Beispiel für eine uniforme Verteilung anhand der Anzahl der Gummibärchen pro Tüte nach Farbe"
  },
  {
    "objectID": "eda-distribution.html#weitere-verteilungen",
    "href": "eda-distribution.html#weitere-verteilungen",
    "title": "\n18  Verteilung von Daten\n",
    "section": "\n18.9 Weitere Verteilungen",
    "text": "18.9 Weitere Verteilungen\n\n\nWir besuchen gerne die R Shiny App The distribution zoo um mehr über die verschiedenen Verteilungen und deren Parameter zu erfahren.\nWeitere Beispiele finden sich unter Basic Probability Distributions in R. Im Weiteren liefert Dormann (2013) eine gute Übersicht über verschiedene Verteilungen und deren Repräsentation in R. Das ist nur eine Auswahl an möglichen Verteilungen. Bitte hier nicht ins rabbit hole der Verteilungen gehen. Wir benötigen in unserer täglichen Arbeit nur einen kleinen Teil der Verteilungen. Es reicht, wenn du eine Vorstellungen der Verteilungen in diesem Kapitel hat."
  },
  {
    "objectID": "eda-distribution.html#referenzen",
    "href": "eda-distribution.html#referenzen",
    "title": "\n18  Verteilung von Daten\n",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "stat-tests-preface.html#referenzen",
    "href": "stat-tests-preface.html#referenzen",
    "title": "Frequentistische Hypothesentests",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nSalsburg, David. 2001. The lady tasting tea: How statistics revolutionized science in the twentieth century. Macmillan."
  },
  {
    "objectID": "stat-tests-basic.html#sec-hypothesen",
    "href": "stat-tests-basic.html#sec-hypothesen",
    "title": "19  Die Testentscheidung",
    "section": "\n19.1 Die Hypothesen",
    "text": "19.1 Die Hypothesen\nWir können auf allen Daten einen statistischen Test rechnen und erhalten statistische Maßzahlen wie eine Teststatistik oder einen p-Wert. Nur leider können wir mit diesen statistischen Maßzahlen nicht viel anfangen ohne die Hypothesen zu kennen. Jeder statistische Test testet eine Nullhypothese. Ob diese Hypothese dem Anwender nun bekannt ist oder nicht, ein statistischer Test testet eine Nullhypothese. Daher müssen wir uns immer klar sein, was die entsprechende Nullhypothese zu unserer Fragestellung ist. Wenn du hier stockst, ist das ganz normal. Eine Fragestellung mit einer statistischen Hypothese zu verbinden ist nicht immer so einfach gemacht.\n\n\n\n\n\n\nDie Nullhypothese \\(H_0\\) und die Alternativhypothese \\(H_A\\)\n\n\n\nDie Nullhypothese \\(H_0\\) nennen wir auch die Null oder Gleichheitshypothese. Die Nullhypothese sagt aus, dass zwei Gruppen gleich sind oder aber kein Effekt zu beobachten ist.\n\\[\nH_0: \\bar{y}_{1} = \\bar{y}_{2}\n\\]\nDie Alternativhypothese \\(H_A\\) oder \\(H_1\\) auch Alternative genannt nennen wir auch Unterschiedshypothese. Die Alternativhypothese besagt, dass ein Unterschied vorliegt oder aber ein Effekt vorhanden ist.\n\\[\nH_A: \\bar{y}_{1} \\neq \\bar{y}_{2}\n\\]\n\n\nAls Veranschaulichung nehmen wir das Beispiel aus der unterschiedlichen Sprungweiten in [cm] für Hunde- und Katzenflöhe. Wir formulieren als erstes die Fragestellung. Eine Fragestellung endet mit einem Fragezeichen.\nLiegt ein Unterschied zwischen den Sprungweiten von Hunde- und Katzenflöhen vor?\nWir können die Frage auch anders formulieren.\nSpringen Hunde- und Katzenflöhe unterschiedlich weit?\nWichtig ist, dass wir eine Fragestellung formulieren. Wir können auch mehrere Fragen an einen Datensatz haben. Das ist auch vollkommen normal. Nur hat jede Fragestellung ein eigenes Hypothesenpaar. Wir bleiben aber bei dem simplen Beispiel mit den Sprungweiten von Hunde- und Katzenflöhen.\nEine statistische Hypothese ist eine Aussage über einen Parameter einer Population.\nWie sieht nun die statistische Hypothese in diesem Beispiel aus? Wir wollen uns die Sprungweite in [cm] anschauen und entscheiden, ob die Sprungweite für Hunde- und Katzenflöhen sich unterscheidet. Eine statistische Hypothese ist eine Aussage über einen Parameter einer Population. Wir entscheiden jetzt, dass wir die mittlere Sprungweite der Hundeflöhe \\(\\bar{y}_{dog}\\) mit der mittleren Sprungweite der Katzenflöhe \\(\\bar{y}_{cat}\\) vergleichen wollen. Es ergibt sich daher folgendes Hypothesenpaar.\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{dog} &= \\bar{y}_{cat} \\\\  \nH_A: \\bar{y}_{dog} &\\neq \\bar{y}_{cat} \\\\   \n\\end{aligned}\n\\]\nDas Falisifkationsprinzip - wir können nur Ablehnen - kommt hier zusammen mit der frequentistischen Statistik in der wir nur eine Wahrscheinlichkeitsaussage über das Auftreten der Daten \\(D\\) - unter der Annahme \\(H_0\\) gilt - treffen können.\nEs ist wichtig sich in Erinnerung zu rufen, dass wir nur und ausschließlich Aussagen über die Nullhypothese treffen werden. Das frequentistische Hypothesentesten kann nichts anders. Wir kriegen keine Aussage über die Alternativhypothese sondern nur eine Abschätzung der Wahrscheinlichkeit des Auftretens der Daten im durchgeführten Experiment, wenn die Nullhypothese wahr wäre. Wenn die Nullhypothese war ist, dann liegt kein Effekt oder Unterschied vor."
  },
  {
    "objectID": "stat-tests-basic.html#die-testentscheidung",
    "href": "stat-tests-basic.html#die-testentscheidung",
    "title": "19  Die Testentscheidung",
    "section": "\n19.2 Die Testentscheidung…",
    "text": "19.2 Die Testentscheidung…\nIn den folgenden Kapiteln werden wir verschiedene statistische Tests kennenlernen. Alle statistischen Tests haben gemein, dass ein Test eine Teststatistik \\(T_{calc}\\) berechnet. Darüber hinaus liefert jeder Test auch einen p-Wert (eng. p-value). Manche statistischen Test geben auch ein 95% Konfidenzintervall wieder. Eine Testentscheidung gegen die Nullhypothese \\(H_0\\) kann mit jedem der drei statistischen Maßzahlen - Teststatistik, \\(p\\)-Wert und Konfidenzintervall - durchgeführt werden. Die Regel für die Entscheidung, ob die Nullhypothese \\(H_0\\) abgelehnt werden kann, ist nur jeweils anders. In Tabelle 19.1 sind die Entscheidungsregeln einmal zusammengefasst.\n\n\n\nTabelle 19.1— Zusammenfassung der statistischen Testentscheidung unter der Nutzung der Teststatistik, dem p-Wert und dem 95% Konfidenzintervall. Die Entscheidung nach der Teststatistik ist veraltet und dient nur dem konzeptionellen Verständnisses. In der Forschung angewandt wird der \\(p\\)-Wert und das 95% Konfidenzintervall. Im Fall des 95% Konfidenzintervalls müssen wir noch unterschieden, ob wir einen Mittelwertsunterschied \\(\\Delta_{A-B}\\) oder aber einen Anteilsunterschied \\(\\Delta_{A/B}\\) betrachten.\n\n\n\n\n\n\n\n\nTeststatistik\np-Wert\n95% Konfidenzintervall\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(\\boldsymbol{KI_{1-\\alpha}}\\)\n\n\nH\\(_0\\) ablehnen\n\\(T_{calc} \\geq T_{\\alpha = 5\\%}\\)\n\\(Pr(\\geq T_{calc}| H_0) \\leq \\alpha\\)\n\n\\(\\Delta_{A-B}\\): enthält nicht 0\n\n\n\nH\\(_0\\) ablehnen\n\n\n\n\\(\\Delta_{A/B}\\): enthält nicht 1\n\n\n\n\n\n\nWir wollen in den folgenden Abschnitten die jeweiligen Entscheidungsregeln eines statistisches Tests einmal durchgehen.\n\nDie Testentscheidung gegen die Nullhypothese anhand der Teststatistik in Kapitel 19.2.1\n\nDie Testentscheidung gegen die Nullhypothese anhand dem p-Wert in Kapitel 19.2.2\n\nDie Testentscheidung gegen die Nullhypothese anhand des 95% Konfidenzintervall in Kapitel 19.2.3\n\n\n\n\n\n\n\n\nStreng genommen gilt die Regel \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) nur für eine Auswahl an statistischen Tests siehe dazu auch Kapitel 19.2.1. Bei manchen statistischen Tests ist die Entscheidung gedreht. Hier lassen wir das aber mal so stehen…\n\n19.2.1 … anhand der Teststatistik\n\n\n\n\n\n\nPrinzip des statistischen Testens I - Die Teststatistik\n\n\n\nDu findest auf YouTube Prinzip des statistischen Testens I - Die Teststatistik als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nWir wollen uns dem frequentistischen Hypothesentesten über die Idee der Teststatistik annähern. Im folgenden sehen wir die Formel für den t-Test. Den t-Test werden wir im Kapitel 22 uns nochmal detaillierter anschauen. Hier nutzen wir die vereinfachte Formel um das Konzept der Teststatistik \\(T\\) zu verstehen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_2}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nmit\n\n\n\\(\\bar{y}_1\\) dem Mittelwert für die erste Gruppe.\n\n\\(\\bar{y}_2\\) dem Mittelwert für die zweite Gruppe.\n\n\\(s_{p}\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{s_A + s_B}{2}\\).\n\n\\(n_g\\) der Gruppengröße der gruppen. Wir nehmen an beide Gruppen sind gleich groß.\n\nWir benötigen also zwei Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) und deren gepoolte Standardabweichung \\(s_p\\) sowie die Anzahl der Beobachtungen je Gruppe \\(n_g\\). Wenden wir die Formel des t-Tests einmal auf den folgenden Beispieldatensatz zu den Sprunglängen in [cm] von jeweils vier Hunde- und Kätzenflohen an. In Tabelle 19.2 ist das Datenbeispiel gegeben.\n\n\n\n\nTabelle 19.2— Beispiel für die Berechnung von einem Mittelwertseffekt an der Sprunglänge [cm] von Hunde und Katzenflöhen.\n\nanimal\njump_length\n\n\n\ncat\n8.5\n\n\ncat\n9.9\n\n\ncat\n8.9\n\n\ncat\n9.4\n\n\ndog\n8.0\n\n\ndog\n7.2\n\n\ndog\n8.4\n\n\ndog\n7.5\n\n\n\n\n\n\nNun berechnen wir die Mittelwerte und die Standardabweichungen aus der obigen Datentabelle für die Sprungweiten getrennt für die Hunde- und Katzenflöhe. Die Werte setzen wir dann in die Formel ein und berechnen die Teststatistik \\(T_{calc}\\).\n\\[\nT_{calc}=\\cfrac{9.18 - 7.78}{\\cfrac{(0.61 + 0.53)}{2} \\cdot \\sqrt{2/4}} = 3.47\n\\]\nmit\n\n\n\\(\\bar{y}_{cat} = 9.18\\) dem Mittelwert für die Gruppe cat.\n\n\\(\\bar{y}_{dog} = 7.78\\) dem Mittelwert für die Gruppe dog.\n\n\\(s_p = 0.57\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.61 + 0.53}{2}\\).\n\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nWir haben nun die Teststatistik \\(T_{calc} = 3.47\\) berechnet. In der ganzen Rechnererei verliert man manchmal den Überblick. Erinnern wir uns, was wir eigentlich wollten. Die Frage war, ob sich die mittleren Sprungweiten der Hunde- und Katzenflöhe unterschieden. Wenn die \\(H_0\\) wahr wäre, dann wäre der Unterschied \\(\\Delta\\) der beiden Mittelwerte der Hunde- und Katzenflöhe gleich null. Oder nochmal in der Analogie der t-Test Formel, dann wäre im Zähler \\(\\Delta = \\bar{y}_{cat} - \\bar{y}_{dog} = 0\\). Wenn die Mittelwerte der Sprungweite [cm] der Hunde- und Katzenflöhe gleich wäre, dann wäre die berechnete Teststatistik \\(T_{calc} = 0\\), da im Zähler Null stehen würde. Die Differenz von zwei gleichen Zahlen ist Null.\nJe größer die berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die beiden Mittelwerte per Zufall gleich sind. Wie groß muss nun die berechnete Teststatistik \\(T_{calc}\\) werden damit wir die Nullhypothese ablehnen können?\n\n\n\nAbbildung 19.1— Die t-Verteilung aller möglichen \\(T_{calc}\\) wenn die Nullhypothese wahr ist. Der Mittelwert der t-Verteilung ist \\(T=0\\). Wenn wir keinen Effekt erwarten würden dann wären die beiden Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) gleich groß. Die Differenz wäre 0. Je größer der \\(T_{calc}\\) wird desto weniger können wir davon ausgehen, dass die beiden Mittelwerte gleich sind. Liegt der \\(T_{calc}\\) über dem kritischen Wert von \\(T_{\\alpha = 5\\%}\\) dann wir die Nullhypothese abgelehnt.\n\n\nIn Abbildung 19.1 ist die Verteilung aller möglichen \\(T_{calc}\\) Werte unter der Annahme, dass die Nullhypothese wahr ist, dargestellt. Wir sehen, dass die t-Verteilung den Gipfel bei \\(T_{calc} = 0\\) hat und niedrigere Werte mit steigenden Werten der Teststatistik annimmt. Wenn \\(T = 0\\) ist, dann sind auch die Mittelwerte gleich. Je größer unsere berechnete Teststatistik \\(T_{calc}\\) wird, desto unwahrscheinlicher ist es, dass die Nullhypothese gilt.\nDie t-Verteilung ist so gebaut, dass die Fläche \\(A\\) unter der Kurve gleich \\(A=1\\) ist. Wir können nun den kritischen Wert \\(T_{\\alpha = 5\\%}\\) berechnen an dem rechts von dem Wert eine Fläche von 0.05 oder 5% liegt. Somit liegt dann links von dem kritischen Wert die Fläche von 0.95 oder 95%. Den kritischen Wert \\(T_{\\alpha = 5\\%}\\) können wir statistischen Tabellen entnehmen. Oder wir berechnen den kritischen Wert direkt in R mit \\(T_{\\alpha = 5\\%} = 2.78\\).\nKommen wir zurück zu unserem Beispiel. Wir haben in unserem Datenbeispiel für den Vergleich von der Sprungweite in [cm] von Hunde- und Katzenflöhen eine Teststatistik von \\(T_{calc} = 3.47\\) berechnet. Der kritische Wert um die Nullhypothese abzulehnen liegt bei \\(T_{\\alpha = 5\\%} = 2.78\\). Wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. In unserem Fall ist \\(3.47 \\geq 2.78\\). Wir können die Nullhypothese ablehnen. Es gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen.\n\n\n\n\n\n\nWoher kommt die Testverteilung von \\(T\\), wenn \\(H_0\\) gilt?\n\n\n\n\n\nIn diesem Exkurs wollen wir einmal überlegen, woher die Testverteilung \\(T\\) herkommt, wenn die \\(H_0\\) gilt. Wir wollen die Verteilung der Teststatistik einmal in R herleiten. Zuerst gehen wir davon aus, dass die Mittelwerte der Sprungweite der Hunde- und Katzenflöhe gleich sind \\(\\bar{y}_{cat} = \\bar{y}_{dog} = (9.18 + 7.78)/2 = 8.48\\). Daher nehmen wir an, dass die Mittelwerte aus der gleichen Normalverteilung kommen. Wir ziehen also vier Sprungweiten jeweils für die Hunde- und Katzenflöhe aus einer Normalverteilung mit \\(\\mathcal{N}(8.48, 0.57)\\). Wir nutzen dafür die Funktion rnorm(). Anschließend berechnen wir die Teststatistik. Diesen Schritt wiederholen wir eintausend Mal.\n\nset.seed(20201021)\nT_vec &lt;- map_dbl(1:1000, function(...){\n  dog_vec &lt;- rnorm(n = 4, mean = 8.48, sd = 0.57)\n  cat_vec &lt;- rnorm(n = 4, mean = 8.48, sd = 0.57)\n  s_p &lt;- (sd(cat_vec) + sd(dog_vec))/2 \n  T_calc &lt;- (mean(cat_vec) - mean(dog_vec))/(s_p * sqrt(2/4)) \n  return(T_calc)  \n}) %&gt;% round(2)\n\nNachdem wir eintausend Mal die Teststatistik unter der \\(H_0\\) berechnet haben, schauen wir uns die sortierten ersten 100 Werte der Teststatistik einmal an. Wir sehen, dass extrem kleine Teststatistiken bis sehr große Teststatistiken zufällig auftreten können, auch wenn die Mittelwerte für das Ziehen der Zahlen gleich waren.\n\nT_vec %&gt;% magrittr::extract(1:100) %&gt;% sort()  \n\n  [1] -5.19 -3.48 -3.29 -2.65 -2.40 -2.10 -1.48 -1.35 -1.30 -1.29 -1.29 -1.27\n [13] -1.24 -1.22 -1.10 -1.03 -1.02 -1.02 -0.91 -0.87 -0.84 -0.79 -0.79 -0.76\n [25] -0.76 -0.76 -0.73 -0.66 -0.63 -0.63 -0.62 -0.61 -0.57 -0.56 -0.55 -0.52\n [37] -0.52 -0.50 -0.48 -0.48 -0.43 -0.35 -0.33 -0.32 -0.26 -0.26 -0.22 -0.21\n [49] -0.20 -0.18 -0.17 -0.17 -0.14 -0.14 -0.12 -0.12 -0.10 -0.06  0.04  0.10\n [61]  0.14  0.16  0.17  0.31  0.34  0.41  0.45  0.50  0.50  0.51  0.55  0.63\n [73]  0.63  0.68  0.73  0.73  0.77  0.89  0.92  0.95  0.99  1.07  1.07  1.09\n [85]  1.12  1.16  1.22  1.33  1.33  1.76  2.11  2.16  2.51  2.79  2.87  3.24\n [97]  3.48  3.56  3.60  6.56\n\n\nUnsere berechnete Teststatistik war \\(T_{calc} = 3.47\\). Wenn wir diese Zahl mit den ersten einhundert, sortierten Teststatistiken vergleichen, dann sehen wir, dass nur 4 von 100 Zahlen größer sind als unsere berechnete Teststatistik. Wir beobachten also sehr seltene Daten wie in Tabelle 19.2, wenn wir davon ausgehen, dass kein Unterschied zwischen der Sprungweite der Hunde- und Katzenflöhe vorliegt.\nIn Abbildung 19.2 sehen wir die Verteilung der berechneten eintausend Verteilungen nochmal als ein Histogramm dargestellt. Wiederum sehen wir, dass unsere berechnete Teststatistik - dargestellt als rote Linie - sehr weit rechts am Rand der Verteilung liegt.\n\nggplot(as_tibble(T_vec), aes(x = value)) +\n  theme_bw() +\n  labs(x = \"Teststatistik\", y = \"Anzahl\") +\n  geom_histogram() +\n  geom_vline(xintercept = 3.47, color = \"red\")\n\n\n\nAbbildung 19.2— Histogramm der 1000 gerechneten Teststaistiken \\(T_{calc}\\), wenn die \\(H_0\\) war wäre und somit kein Unterschied zwischen den Mittelwerten der Sprungweiten der Hunde- und Katzenflöhe vorliegen würde.\n\n\n\n\n\n\n\n\n\n\n\n\nEs gibt einen Unterschied zwischen der mittleren Sprungweite von Hunde- und Katzenflöhen. Die Aussage ist statistisch falsch. Wir können im frequentistischen Hypothesentesten keine Aussage über die \\(H_A\\) treffen. Im Sinne der Anwendbarkeit soll es hier so stehen bleiben.\nNun ist es leider so, dass jeder statistische Test seine eigene Teststatistik \\(T\\) hat. Daher ist es etwas mühselig sich immer neue und andere kritische Werte für jeden Test zu merken. Es hat sich daher eingebürgert, sich nicht die Teststatistik für die Testentscheidung gegen die Nullhypothese zu nutzen sondern den \\(p\\)-Wert. Den \\(p\\)-Wert wollen wir uns in dem folgenden Abschnitt anschauen.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik\n\n\n\nBei der Entscheidung mit der Teststatistik müssen wir zwei Fälle unterschieden.\n\nBei einem t-Test und einem \\(\\mathcal{X}^2\\)-Test gilt, wenn \\(T_{calc} \\geq T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nBei einem Wilcoxon-Mann-Whitney-Test gilt, wenn \\(T_{calc} &lt; T_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\n\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr.\n\n\n\n19.2.2 … anhand dem p-Wert\n\n\n\n\n\n\nPrinzip des statistischen Testens II - Der p-Wert\n\n\n\nDu findest auf YouTube Prinzip des statistischen Testens II - Der p-Wert als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nIn dem vorherigen Abschnitt haben wir gelernt, wie wir zu einer Entscheidung gegen die Nullhypothese anhand der Teststatistik kommen. Wir haben einen kritischen Wert \\(T_{\\alpha = 5\\%}\\) definiert bei dem rechts von dem Wert 5% der Werte liegen. Anstatt nun den berechneten Wert \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) zu vergleichen, vergleichen wir jetzt die Flächen rechts von den jeweiligen Werten. Wir machen es uns an dieser Stelle etwas einfacher, denn wir nutzen immer den absoluten Wert der Teststatistik.\nWir schreiben \\(\\boldsymbol{Pr}\\) und meinen damit eine Wahrscheinlichkeit (eng. probability). Häufig wird auch nur das \\(P\\) verwendet, aber dann kommen wir wieder mit anderen Konzepten in die Quere.\nIn Abbildung 19.1 sind die Flächen auch eingetragen. Da die gesamte Fläche unter der t-Verteilung mit \\(A = 1\\) ist, können wir die Flächen auch als Wahrscheinlichkeiten lesen. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert genannt. Die gesamte Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%. Wir können also die Teststatistiken oder den p-Wert mit dem \\(\\alpha\\)-Niveau von 5% vergleichen.\n\n\nTabelle 19.3— Zusammenhang zwischen der Teststatistik \\(T\\) und der Fläche \\(A\\) rechts von der Teststatistik. Die Fläche rechts von der berechneten Teststatistik \\(T_{calc}\\) wird \\(Pr(T|H_0)\\) oder \\(p\\)-Wert genannt. Die Fläche rechts von dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) wird \\(\\alpha\\) genannt und liegt bei 5%.\n\nTeststatistik \\(T\\)\n\nFläche \\(A\\)\n\n\n\n\n\\(T_{calc}\\)\n\n\\(Pr(T_{calc}|H_0)\\) oder \\(p\\)-Wert\n\n\n\\(T_{\\alpha = 5\\%}\\)\n\\(\\alpha\\)\n\n\n\n\nIn der folgenden Abbildung 19.3 ist dann nochmal der Zusammenhang aus der Tabelle als eine Abbildung visualisiert. Mit dem \\(p\\)-Wert entscheiden wir anhand von Flächen. Wir schauen uns in diesem Fall die beiden Seiten der Testverteilung mit jeweils \\(T_{\\alpha = 2.5\\%}\\) für \\(-T_K\\) und \\(T_K\\) an und vergleichen die Flächen rechts neben der berechneten Teststatistik \\(T_{calc}\\).\n\n\n\nAbbildung 19.3— Die Flächen links und rechts von \\(T_{\\alpha = 2.5\\%}\\) nochmal separat dargestellt. Wir vergleichen bei der Entscheidung mit dem \\(p\\)-Wert nicht die berechnete Teststatistik \\(T_{calc}\\) mit dem kritischen Wert \\(T_{\\alpha = 5\\%}\\) sondern die Flächen rechts von den jeweiligen Teststatistiken mit \\(A_K = 5\\%\\) und \\(A_{calc}\\) als den \\(p\\)-Wert. An dem Flächenvergleich machen wir dann die Testentscheidung fest.\n\n\nDer p-Wert oder \\(Pr(T|H_0)\\) ist eine Wahrscheinlichkeit. Eine Wahrscheinlichkeit kann die Zahlen von 0 bis 1 annehmen. Dabei sind die Grenzen einfach zu definieren. Eine Wahrscheinlichkeit von \\(Pr(A) = 0\\) bedeutet, dass das Ereignis A nicht auftritt; eine Wahrscheinlichkeit von \\(Pr(A) = 1\\) bedeutet, dass das Ereignis A eintritt. Der Zahlenraum dazwischen stellt jeden von uns schon vor große Herausforderungen. Der Unterschied zwischen 40% und 60% für den Eintritt des Ereignisses A sind nicht so klar zu definieren, wie du auf den ersten Blick meinen magst.\nEin frequentistischer Hypothesentest beantwortet die Frage, mit welcher Wahrscheinlichkeit \\(Pr\\) die Teststatistik \\(T\\) aus dem Experiment mit den Daten \\(D\\) zu beobachten wären, wenn es keinen Effekt gäbe (\\(H_0\\) ist wahr).\nLikelihood heißt Plausibilität und Probability heißt Wahrscheinlichkeit.\nIn anderen Büchern liest man an dieser Stelle auch gerne etwas über die Likelihood, nicht so sehr in deutschen Büchern, schon aber in englischen Veröffentlichungen. Im Englischen gibt es die Begrifflichkeiten einer Likelihood und einer Probability. Meist wird beides ins Deutsche ungenau mit Wahrscheinlichkeit übersetzt oder wir nutzen einfach Likelihood. Was aber auch nicht so recht weiterhilft, wenn wir ein Wort mit dem gleichen Wort übersetzen. Es handelt sich hierbei aber um zwei unterschiedliche Konzepte. Deshalb Übersetzen wir Likelihood mit Plausibilität und Probability mit Wahrscheinlichkeit.\nIm Folgenden berechnen wir den \\(p\\)-Wert in R mit der Funktion t.test(). Mehr dazu im Kapitel 22, wo wir den t-Test und deren Anwendung im Detail besprechen. Hier fällt der \\(p\\)-Wert etwas aus den Himmel. Wir wollen aber nicht per Hand Flächen unter einer Kurve berechnen sondern nutzen für die Berechnung von \\(p\\)-Werten statistische Tests in R.\n\n\n# A tibble: 1 × 2\n  statistic p.value\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      3.47  0.0133\n\n\nWir sagen, dass wir ein signifikantes Ergebnis haben, wenn der \\(p\\)-Wert kleiner ist als die Signifikanzschwelle \\(\\alpha\\) von 5%.\nWir erhalten einen \\(p\\)-Wert von 0.013 und vergleichen diesen Wert zu einem \\(\\alpha\\) von 5%. Ist der \\(p\\)-Wert kleiner als der \\(\\alpha\\)-Wert von 5%, dann können wir die Nullhypothese ablehnen. Da 0.013 kleiner ist als 0.05 können wir die Nullhypothese und damit die Gleichheit der mittleren Sprungweiten in [cm] ablehnen. Wir sagen, dass wir ein signifikantes Ergebnis vorliegen haben.\n\n\n\n\n\n\nEntscheidung mit dem p-Wert\n\n\n\nWenn der p-Wert \\(\\leq \\alpha\\) dann wird die Nullhypothese (H\\(_0\\)) abgelehnt. Das Signifikanzniveau \\(\\alpha\\) wird als Kulturkonstante auf 5% oder 0.05 gesetzt. Die Nullhypothese (H\\(_0\\)) kann auch Gleichheitshypothese gesehen werden. Wenn die H\\(_0\\) gilt, liegt kein Unterschied zwischen z.B. den Behandlungen vor.\n\n\n\n19.2.3 … anhand des 95% Konfidenzintervall\n\n\n\n\n\n\nPrinzip des statistischen Testens III - Das 95% Konfidenzintervall\n\n\n\nDu findest auf YouTube Statistik und Data Science - Teil 12.0 - Konfidenzintervalle erklärt als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nEin statistischer Test der eine Teststatistik \\(T\\) berechnet liefert auch immer einen \\(p\\)-Wert. Nicht alle statistischen Tests ermöglichen es ein 95% Konfidenzintervall zu berechnen. Abbildung 19.4 zeigt ein 95% Konfidenzintervall.\n\n\nAbbildung 19.4— Ein 95% Konfidenzintervall. Der Punkt in der Mitte entspricht dem Unterschied oder Effekt \\(\\Delta\\).\n\nMit p-Werten haben wir Wahrscheinlichkeitsaussagen und damit über die Signifikanz. Damit haben wir noch keine Aussage über die Relevanz des beobachtenten Effekts.\nMit der Teststatistik \\(T\\) und dem damit verbundenen \\(p\\)-Wert haben wir uns Wahrscheinlichkeiten angeschaut und erhalten eine Wahrscheinlichkeitsaussage. Eine Wahrscheinlichkeitsaussage sagt aber nichts über den Effekt \\(\\Delta\\) aus. Also wie groß ist der mittlere Sprungunterschied zwischen Hunde- und Katzenflöhen.\nDie Idee von 95% Kondifenzintervallen ist es jetzt den Effekt mit der Wahrscheinlichkeitsaussage zusammenzubringen und beides in einer Visualisierung zu kombinieren. Im Folgenden sehen wir die vereinfachte Formel für das 95% Konfidenzintervall eines t-Tests.\n\\[\n\\left[\n(\\bar{y}_1-\\bar{y}_2) -\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}}; \\;\n(\\bar{y}_1-\\bar{y}_2) +\nT_{\\alpha = 5\\%} \\cdot \\frac {s_p}{\\sqrt{n}}\n\\right]\n\\]\nDie Formel ist ein wenig komplex, aber im Prinzip einfach, wenn du ein wenig die Formel auf dich wirken lässt. Der linke und der rechte Teil neben dem Semikolon sind fast gleich, bis auf das Plus- und Minuszeichen. Abbildung 19.5 visualisert die Formel einmal. Wir sehen Folgendes in der Formel und dann in der entsprechenden Abbildung:\n\n\n\\((\\bar{y}_{1}-\\bar{y}_{2})\\) ist der Effekt \\(\\Delta\\). In diesem Fall der Mittelwertsunterschied. Wir finden den Effekt als Punkt in der Mitte des Intervals.\n\n\\(T_{\\alpha = 5\\%} \\cdot \\frac {s}{\\sqrt{n}}\\) ist der Wert, der die Arme des Intervalls bildet. Wir vereinfachen die Formel mit \\(s_p\\) für die gepoolte Standardabweichung und \\(n_g\\) für die Fallzahl der beiden Gruppen. Wir nehmen an das beide Gruppen die gleiche Fallzahl \\(n_1 = n_2\\) haben.\n\n\n\nAbbildung 19.5— Zusammenhang zwischen der vereinfachten Formel für das 95% Konfidenzintervall und der Visualisierung des 95% Konfidenzintervalls. Der Effektschätzer wird als Punkt in der Mitte des Intervalls dargestellt. Der Effektschäter \\(\\Delta\\) kann entweder ein Mittelwertsunterschied sein oder ein Anteilsunterschied. Bei einem Mittelwertsunterschied kann die Nullhypothese abgelehnt werden, wenn die 0 nicht im Konfidenzintervall ist; bei einem Anteilsunterschied wenn die 1 nicht im Konfidenzintervall ist. Die Arme werden länger oder kürzer je nachdem wie sich die statistischen Maßzahlen \\(s\\) und \\(n\\) verändern.\n\nDie Funktion factor() in R erlaubt es dir die Level eines Faktors zu sortieren und so festzulegen ob Level cat minus Level dog oder umgekehrt von R gerechnet wird.\nWir können eine biologische Relevanz definieren, dadurch das ein 95% Konfidenzintervall die Wahrscheinlichkeitsaussage über die Signifkanz, daher ob die Nullhypothese abgelehnt werden kann, mit dem Effekt zusammenbringt. Wo die Signifikanzschwelle klar definiert ist, hängt die Relevanzschwelle von der wissenschaftlichen Fragestellung und weiteren externen Faktoren ab. Die Signifikanzschwelle liegt bei 0, wenn wir Mittelwerte miteinander vergleichen und bei 1, wenn wir Anteile vergleichen. Abbildung 19.6 zeigt fünf 95% Konfidenzintervalle (a-e), die sich anhand der Signifikanz und Relevanz unterscheiden. Bei der Relevanz ist es wichtig zu wissen in welche Richtung der Effekt gehen soll. Erwarten wir einen positiven Effekt wenn wir die Differenz der beiden Gruppen bilden oder einen negativen Effekt?\n\n\nAbbildung 19.6— Verschiedene signifikante und relevante Konfidenzintervalle: (a) signifikant und nicht relevant; (b) nicht signifikant und nicht relevant; (c) signifikant und relevant; (d) signifikant und nicht relevant, der Effekt ist zu klein; (e) signifikant und potenziell relevant, Effekt zeigt in eine unerwartete Richtung gegeben der Relevanzschwelle.\n\nWir wollen uns nun einmal anschauen, wie sich ein 95% Konfidenzintervall berechnet. Wir nehmen dafür die vereinfachte Formel und setzen die berechneten statistischen Maßzahlen ein. In der Anwendung werden wir die Konfidenzintervalle nicht selber berechnen. Wenn ein statistisches Verfahren konfidenzintervalle berechnen kann, dann liefert die entsprechende Funktion in R das Konfidenzintervall.\nEs ergibt sich Folgende ausgefüllte, vereinfachte Formel für das 95% Konfidenzintervalls eines t-Tests für das Beispiel des Sprungweitenunterschieds [cm] zwischen Hunde- und Katzenflöhen.\n\n\n\n\n\n\nWir nutzen hier eine vereinfachte Formel für das Konfidenzintervall um das Konzept zu verstehen. Später berechnen wir das Konfidenzintervall in R.\n\\[\n\\left[\n(9.18-7.78) -\n2.78 \\cdot \\frac {0.57}{\\sqrt{4}}; \\;\n(9.18-7.78) +\n2.78 \\cdot \\frac {0.57}{\\sqrt{4}};\n\\right]\n\\]\nmit\n\n\n\\(\\bar{y}_{cat} = 9.18\\) dem Mittelwert für die Gruppe cat.\n\n\\(\\bar{y}_{dog} = 7.78\\) dem Mittelwert für die Gruppe dog.\n\n\\(T_{\\alpha = 5\\%} = 2.78\\) dem kritischen Wert.\n\n\\(s_p = 0.57\\) der gepoolten Standardabweichung mit \\(s_p = \\tfrac{0.61 + 0.53}{2}\\).\n\n\\(n_g = 4\\) der Gruppengröße der Gruppe A und B. Wir nehmen an beide Gruppen sind gleich groß.\n\nLösen wir die Formel auf, so ergibt sich folgendes 95% Konfidenzintervall des Mittelwertsunterschiedes der Hunde- und Katzenflöhe.\n\\[[0.64; 2.16]\\]\nWir können sagen, dass mit 95% Wahrscheinlichkeit das Konfidenzintervall den wahren Effektunterschied \\(\\Delta\\) überdeckt. Oder etwas mehr in Prosa, dass wir eine Sprungweitenunterschied von 0.64 cm bis 2.16 cm zwischen Hunde- und Katzenflöhen erwarten würden.\nDie Entscheidung gegen die Nullhypothese bei einem Mittelwertsunterschied erfolgt bei einem 95% Konfidenzintervall danach ob die Null mit im Konfidenzintervall liegt oder nicht. In dem Interval \\([0.64; 2.16]\\) ist die Null nicht enthalten, also können wir die Nullhypothese ablehnen. Es ist mit einem Unterschied zwischen den mittleren Sprungweiten von Hunde- und Katzenflöhen auszugehen.\nIn unserem Beispiel, könnten wir die Relevanzschwelle für den mittleren Sprungweitenunterschied zwischen Hund- und Katzenflöhen auf 2 cm setzen. In dem Fall würden wir entscheiden, dass der mittlere Sprungweitenunterschied nicht relevant ist, da die 2 cm im Konfidenzintervall enthalten sind. Was wäre wenn wir die Relevanzschwelle auf 4 cm setzen? Dann wäre zwar die Relevanzschwelle nicht mehr im Konfidenzintervall, aber wir hätten Fall (d) in der Abbildung 19.6 vorliegen. Der Effekt ist einfach zu klein, dass der Effekt relevant sein könnte.\nWir können dann die 95% Konfidenzintervall des Mittelwertsunterschiedes der Hunde- und Katzenflöhe auch nochmal richtig in R berechnen. Wir haben ja oben eine einfachere Formel für die gepoolte Standardabweichung genutzt. Wenn wir also ganz genau rechnen wollen, dann sind die 95% Konfidenzintervall wie folgt. Wir nutzen auch hier die Funktion t.test(). Mehr dazu im Kapitel 22, wo wir den t-Test und deren Anwendung im Detail besprechen.\n\n\n# A tibble: 1 × 2\n  conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.412      2.39\n\n\n\n\n\n\n\n\nEntscheidung mit dem 95% Konfidenzintervall\n\n\n\nBei der Entscheidung mit dem 95% Konfidenzinterval müssen wir zwei Fälle unterscheiden.\n\nEntweder schauen wir uns einen Mittelwertsunterschied (\\(\\Delta_{y_1-y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 0 im 95% Konfidenzinterval ist.\nOder wir schauen uns einen Anteilsunterschied (\\(\\Delta_{y_1/y_2}\\)) an, dann können wir die Nullhypothese (H\\(_0\\)) nicht ablehnen, wenn die 1 im 95% Konfidenzinterval ist."
  },
  {
    "objectID": "stat-tests-basic.html#sec-delta-n-s",
    "href": "stat-tests-basic.html#sec-delta-n-s",
    "title": "19  Die Testentscheidung",
    "section": "\n19.3 Auswirkung des Effektes, der Streuung und der Fallzahl",
    "text": "19.3 Auswirkung des Effektes, der Streuung und der Fallzahl\nWir wollen einmal den Zusammenhang zwischen dem Effekt \\(\\Delta\\), der Streuung als Standardabweichung \\(s\\) und Fallzahl \\(n\\) uns näher anschauen. Wir können die Formel des t-Tests wie folgt vereinfachen.\n\\[\nT_{calc}=\\cfrac{\\bar{y}_1-\\bar{y}_1}{s_{p} \\cdot \\sqrt{2/n_g}}\n\\]\nFür die Betrachtung der Zusammenhänge wandeln wir \\(\\sqrt{2/n_g}\\) in \\(1/n\\) um. Dadurch wandert die Fallzahl \\(n\\) in den Zähler. Die Standardabweichung verallgemeinern wir zu \\(s\\) und damit allgemein zur Streuung. Abschließend betrachten wir \\(\\bar{y}_A-\\bar{y}_B\\) als den Effekt \\(\\Delta\\). Es ergibt sich folgende vereinfachte Formel.\n\\[\nT_{calc} = \\cfrac{\\Delta \\cdot n}{s}\n\\]\nWir können uns nun die Frage stellen, wie ändert sich die Teststatistik \\(T_{calc}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\) in den Daten. Die Tabelle 19.4 zeigt die Zusammenhänge auf. Die Aussagen in der Tabelle lassen sich generalisieren. So bedeutet eine steigende Fallzahl meist mehr signifikante Ergebnisse. Eine steigende Streuung reduziert die Signifikanz eines Vergleichs. Ein Ansteigen des Effektes führt zu mehr signifikanten Ergebnissen. Ebenso verschiebt eine Veränderung des Effekt das 95% Konfidenzintervall, eine Erhöhung der Streuung macht das 95% Konfidenzintervall breiter, eine sinkende Streuung macht das 95% Konfidenzintervall schmaler. Bei der Fallzahl verhält es sich umgekehrt. Eine Erhöhung der Fallzahl macht das 95% Konfidenzintervall schmaler und eine sinkende Fallzahl das Konfidenzintervall breiter.\n\n\n\nTabelle 19.4— Zusammenhang von der Teststatistik \\(T_{calc}\\) und dem p-Wert \\(Pr(\\geq T_{calc}|H_0)\\) sowie dem \\(KI_{1-\\alpha}\\) in Abhängigkeit vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\\(\\boldsymbol{T_{calc}}\\)\n\\(\\boldsymbol{Pr(\\geq T_{calc}|H_0)}\\)\n\\(KI_{1-\\alpha}\\)\n\n\n\n\\(\\Delta \\uparrow\\)\nsteigt\nsinkt\nverschoben\n\\(\\Delta \\downarrow\\)\nsinkt\nsteigt\nverschoben\n\n\n\\(s \\uparrow\\)\nsinkt\nsteigt\nbreiter\n\\(s \\downarrow\\)\nsteigt\nsinkt\nschmaler\n\n\n\\(n \\uparrow\\)\nsteigt\nsinkt\nschmaler\n\\(n \\downarrow\\)\nsinkt\nsteigt\nbreiter"
  },
  {
    "objectID": "stat-tests-theorie.html#sec-alpha-beta",
    "href": "stat-tests-theorie.html#sec-alpha-beta",
    "title": "20  Die Testtheorie",
    "section": "\n20.1 Der \\(\\alpha\\)-Fehler und der \\(\\beta\\)-Fehler",
    "text": "20.1 Der \\(\\alpha\\)-Fehler und der \\(\\beta\\)-Fehler\nVielleicht ist die Idee der Testtheorie und der Testentscheidung besser mit der Analogie des Rauchmelders zu verstehen. Wir nehmen an, dass der Rauchmelder der statistische Test ist. Der Rauchmelder hängt an der Decke und soll entscheiden, ob es brennt oder nicht. Daher muss der Rauchmelder entscheiden, die Nullhypothese “kein Feuer” abzulehnen oder die Hypothese “kein Feuer” beizubehalten.\n\\[\n\\begin{aligned}\nH_0&: \\mbox{kein Feuer im Haus}  \\\\  \nH_A&: \\mbox{Feuer im Haus}  \\\\   \n\\end{aligned}\n\\]\nWir können jetzt den Rauchmelder so genau einstellen, dass der Rauchmelder bei einer Kerze losgeht. Oder aber wir stellen den Rauchmelder so ein, dass er erst bei einem Stubenbrand ein Piepen von sich gibt. Wie sensibel auf Rauch wollen wir den Rauchmelder einstellen? Soll der Rauchmelder sofort die Nullhypothese ablehnen? Wenn also nur eine Kerze brennt. Soll also der \\(\\alpha\\)-Fehler groß sein? Erinnere dich, mit einem großen \\(\\alpha\\)-Fehler würden wir mehr Nullhypothesen ablehnen oder anders gesprochen leichter die Nullhypothese ablehnen. Wir würden ja den \\(\\alpha\\)-Fehler zum Beispiel von 5% auf 20% setzen können. Das wäre nicht sehr sinnvoll. Die Feuerwehr würde schon bei einer Kerze kommen oder wenn wir mal was anbrennen. Wir dürfen also den \\(\\alpha\\)-Fehler nicht zu groß einstellen.\nIntuitiv würde man meinen, ein sehr kleiner \\(\\alpha\\)-Fehler nun sinnvoll sei. Wenn wir aber den Rauchmelder sehr unsensibel einstellen, also der Rauchmelder erst bei sehr viel Rauch die Nullhypothese ablehnt, könnte das Haus schon unrettbar in Flammen stehen. Dieser Fehler, Haus steht in Flammen und der Rauchmelder geht nicht, wird als \\(\\beta\\)-Fehler bezeichnet. Wie du siehst hängen die beiden Fehler miteinander zusammen. Wichtig hierbei ist immer, dass wir uns einen Zustand vorstellen, das Haus brennt nicht (\\(H_0\\) ist wahr) oder das Haus brennt nicht (\\(H_A\\) ist wahr). An diesem Zustand entscheiden wir dann, wie hoch der Fehler jeweils sein soll diesen Zustand zu übersehen.\n\n\n\n\n\n\nDer \\(\\alpha\\)-Fehler und \\(\\beta\\)-Fehler als Rauchmelderanalogie\n\n\n\nHäufig verwirrt die etwas theoretische Herangehensweise an den \\(\\alpha\\)-Fehler und \\(\\beta\\)-Fehler. Wir versuchen hier nochmal die Analogie eines Rauchmelders und dem Feuer im Haus.\n\n\nAbbildung 20.1— Andere Art der Darstellung des \\(\\alpha\\)-Fehlers als Alarm without fire und dem \\(\\beta\\)-Fehler als Fire without alarm. Je nachdem wie empfindlich wir den Alarm des Rauchmelders (den statistischen Test) über das \\(\\alpha\\) einstellen, desto mehr Alarm bekommen wir ohne das ein Effekt vorhanden wäre. Drehen wir den Alarm zu niedrig, dann kriegen wir kein Feuer mehr angezeigt, den \\(\\beta\\)-Fehler.\n\n\n\n\\(\\boldsymbol{\\alpha}\\)-Fehler: Alarm without fire. Der statistische Test schlägt Alarm und wir sollen die \\(H_0\\) ablehnen, obwohl die \\(H_0\\) in Wahrheit gilt und kein Effekt vorhanden ist.\n\n\\(\\boldsymbol{\\beta}\\)-Fehler: Fire without alarm. Der statistische Test schlägt nicht an und wir sollen die \\(H_0\\) beibehalten, obwohl die \\(H_0\\) in Wahrheit nicht gilt und ein Effekt vorhanden ist.\n\n\n\nWie sieht nun die Lösung, erstmal für unseren Rauchmelder, aus? Wir müssen Grenzen für den \\(\\alpha\\) und \\(\\beta\\)-Fehler festlegen bei denen der Rauchmelder angeht und wir die Feuerwehr rufen.\nWir setzen den \\(\\alpha\\)-Fehler auf 5%.\n\nWir setzen den \\(\\alpha\\)-Fehler auf 5%. Somit haben wir in 1 von 20 Fällen das Problem, dass uns der Rauchmelder angeht obwohl gar kein Feuer da ist. Wir lehnen die Nullhypothese ab, obwohl die Nullhypothese gilt.\n\nWir setzen den \\(\\beta\\)-Fehler auf 20%.\n\nAuf der anderen Seite setzen wir den \\(\\beta\\)-Fehler auf 20%. Damit brennt uns die Bude in 1 von 5 Fällen ab ohne das der Rauchmelder einen Pieps von sich gibt. Wir behalten die Nullhypothese bei, obwohl die Nullhypothese nicht gilt.\n\nNachdem wir uns die Testentscheidung mit der Analogie des Rauchmelders angesehen haben, wollen wir uns wieder der Statistik zuwenden. Betrachten wir das Problem nochmal von der theoretischen Seite mit den statistischen Fachbegriffen.\nSoweit haben wir es als gegeben angesehen, dass wir eine Testentscheidung durchführen. Entweder mit der Teststatistik, dem \\(p\\)-Wert oder dem 95% Konfidenzintervall. Immer wenn wir eine Entscheidung treffen, können wir auch immer eine falsche Entscheidung treffen. Wie wir wissen hängt die berechnete Teststatistik \\(T_{calc}\\) nicht nur vom Effekt \\(\\Delta\\) ab sondern auch von der Streuung \\(s\\) und der Fallzahl \\(n\\). Auch können wir den falschen Test wählen oder Fehler im Design des Experiments gemacht haben. Schlussendlich gibt es viele Dinge, die unsere simple mathematischen Formeln beeinflussen können, die wir nicht kennen. Ein frequentistischer Hypothesentest gibt immer nur eine Aussage über die Nullhypothese wieder. Also ob wir die Nullhypothese ablehnen können oder nicht.\nAbbildung 20.2 zeigt die theoretische Verteilung der Nullyhypothese und der Alternativehypothese. Wenn die beiden Verteilungen sehr nahe beieinander sind, wird es schwer für den statistischen Test die Hypothesen klar voneinander zu trennen. Die Verteilungen überlappen. Es gibt einen sehr kleinen Unterschied in den Sprungweiten zwischen Hunde- und Katzenflöhen. In dem Beispiel wurde der gesamte \\(\\alpha\\)-Fehler auf die rechte Seite gelegt. Das macht die Darstellung etwas einfacher.\n\n\n\nAbbildung 20.2— Darstellung der Null- und Alternativehypothese. Mit steigendem \\(T_{calc}\\) wird die Wahrscheinlichkeit für die \\(H_0\\) immer kleiner. Leider ist uns nichts über \\(H_A\\) und deren Lage bekannt. Sollte die \\(H_A\\) Verteilung zu weit nach links ragen, könnten wir die \\(H_0\\) beibehalten, obwohl die \\(H_A\\) gilt.\n\n\nAchtung In der Regression wird uns auch wieder das \\(\\beta\\) als Symbol begegnen. In der statistischen Testtheorie ist das \\(\\beta\\) ein Fehler; in der Regression ist das \\(\\beta\\) ein Koeffizient der Regression. Hier ist der Kontext wichtig.\nWir können daher bei statistischen Testen zwei Arten von Fehlern machen. Zum einen den \\(\\alpha\\) Fehler oder auch Type I Fehler genannt. Zum anderen den \\(\\beta\\) Fehler oder auch Type II Fehler genannt. Die Grundidee basiert darauf, dass wir eine Testentscheidung gegen die Nullhypothese machen. Diese Entscheidung kann richtig sein, da in Wirklichkeit die Nullhypothese gilt oder aber falsch sein, da in Wirklichkeit die Nullhypothese nicht gilt. In Abbildung 20.3 wird der Zusammenhang in einer 2x2 Tafel veranschaulicht.\n\n\n\nAbbildung 20.3— Zusammenhang zwischen der Testentscheidung gegen die \\(H_0\\) Hypothese sowie dem Beibehalten der \\(H_0\\) Hypothese und der unbekannten Wahrheit in der die \\(H_0\\) falsch sein kann oder die \\(H_0\\) wahr sein kann. Wir können mit unserer Testenstscheidung richtig liegen oder falsch. Mit welcher Wahrscheinlichkeit geben der \\(\\alpha\\) Fehler und \\(\\beta\\) Fehler wieder. Unten rechts ist der Zusammenhang zu der Abbildung 20.2 gezeigt.\n\n\n\n\nDie Diskussion über den \\(p\\)-Wert und dem Vergleich mit dem \\(\\alpha\\)-Fehler wird in der Statistik seit 2019 verstärkt diskutiert (Wasserstein, Schirm, und Lazar 2019). Das Nullritual wird schon lamge kritisiert (Gigerenzer, Krauss, und Vitouch 2004). Siehe dazu auch The American Statistician, Volume 73, Issue sup1 (2019).\nBeide Fehler sind Kulturkonstanten. Das heißt, dass sich diese Zahlen von 5% und 20% so ergeben haben. Es gibt keinen rationalen Grund diese Zahlen so zu nehmen. Prinzipiell schon, aber die Gründe leigen eher in der Anwendbarkeit von feststehenden Tabellen vor der Entwicklung des Computers. Man kann eigentlich sagen, dass die 5% und die 20% eher einem Zufall entsprungen sind, als einer tieferen Rationalen. Wir behalten diese beiden Zahlen bei aus den beiden schlechtesten Gründe überhaupt: i) es wurde schon immer so gemacht und ii) viele machen es so.\nEine weitere wichtige statistische Maßzahl im Kontext der Testtheorie ist die \\(Power\\) oder auch \\(1-\\beta\\). Die \\(Power\\) ist die Gegenwahrscheinlichkeit von dem \\(\\beta\\)-Fehler. In der Analogie des Rauchmelders wäre die \\(Power\\) daher Alarm with fire. Das heißt, wie wahrscheinlich ist es einen wahren Effekt - also einen Unterschied - mit dem statistischen Test auch zu finden. Oder anders herum, wenn wir wüssten, dass die Hunde- und Katzenflöhe unterschiedliche weit springen, mit welcher Wahrscheinlichkeit würde diesen Unterschied ein statistsicher Test auch finden? Mit eben der \\(Power\\), also gut 80%. Tabelle 20.1 zeigt die Abhängigkeit der \\(Power\\) vom Effekt \\(\\Delta\\), der Streuung \\(s\\) und der Fallzahl \\(n\\).\nDie \\(Power\\) ist eine Wahrscheinlichkeit und sagt nichts über die Relevanz des Effektes aus.\n\n\nTabelle 20.1— Abhängigkeit der \\(Power (1-\\beta)\\) vom Effekt \\(\\Delta\\), der Fallzahl \\(n\\) und der Streuung \\(s\\). Die \\(Power\\) ist eine Wahrscheinlichkeit und sagt nichts über die Relevanz des Effektes aus.\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{Power (1-\\beta)}\\)\n\n\\(\\boldsymbol{Power (1-\\beta)}\\)\n\n\n\n\\(\\Delta \\uparrow\\)\nsteigt\n\\(\\Delta \\downarrow\\)\nsinkt\n\n\n\\(s \\uparrow\\)\nsinkt\n\\(s \\downarrow\\)\nsteigt\n\n\n\\(n \\uparrow\\)\nsteigt\n\\(n \\downarrow\\)\nsinkt"
  },
  {
    "objectID": "stat-tests-theorie.html#sec-einseitig-zweiseitig",
    "href": "stat-tests-theorie.html#sec-einseitig-zweiseitig",
    "title": "20  Die Testtheorie",
    "section": "\n20.2 Einseitig oder zweiseitig?",
    "text": "20.2 Einseitig oder zweiseitig?\nManchmal kommt die Frage auf, ob wir einseitig oder zweiseitig einen statistischen Test durchführen wollen. Beim Fall des zweiseitigen Testens verteilen wir den \\(\\alpha\\)-Fehler auf beide Seiten der Testverteilung mit jeweils \\(\\cfrac{\\alpha}{2}\\). In dem Fall des einseitigen Tests liegt der gesamte \\(\\alpha\\)-Fehler auf der rechten oder linken Seite der Testverteilung. In Abbildung 20.4 wird der Zusammenhang beispielhaft an der t-Verteilung gezeigt.\n\n\n\nAbbildung 20.4— Zusammenhang zwischen dem einseitigen und zweiseitigen Testen. Im Falle des zweiseitigen Testens teilen wir den \\(\\alpha\\)-Fehler auf beide Seiten der beispielhaften t-Verteilung auf. Im Falle des einseitigen Testen leigt der gesamte \\(\\alpha\\)-Fehler auf der rechten oder der linken Seite der t-Verteilung.\n\n\nIn der Anwendung testen wir immer zweiseitig.\nIn der Anwendung testen wir immer zweiseitig. Der Grund ist, dass das Vorzeichen von der Teststatik davon abhängt, welche der beiden Gruppen den größeren Mittelwert hat. Da wir die Mittelwerte vor der Auswertung nicht kennen, können wir auch nicht sagen in welche Richtung der Effekt und damit die Teststatistik laufen wird.\nEs gibt theoretisch Gründe, die für ein einseitiges Testen unter bestimmten Bedingungen sprechen, aber wir nutzen in der Anwendung nur das zweiseite Testen. Wir müssen dazu in R auch nichts weiter angeben. Ein durchgeführter statistischer Test in R testet automatisch immer zweiseitig.\n\n\n\n\n\n\nEinseitig oder zweiseitig im Spiegel der Regulierungsbehörden\n\n\n\nIn den allgemeinen Methoden des IQWiG, einer Regulierungsbehörde für klinische Studien, wird grundsätzlich das zweiseitige Testen empfohlen. Wenn einseitig getestet werden sollte, so soll das \\(\\alpha\\)-Niveau halbiert werden. Was wiederum das gleiche wäre wie zweiseitiges Testen - nur mit mehr Arbeit.\nZur besseren Vergleichbarkeit mit 2-seitigen statistischen Verfahren wird in einigen Guidelines für klinische Studien eine Halbierung des üblichen Signifikanzniveaus von 5 % auf 2,5 % gefordert. – Allgemeine Methoden Version 6.1 vom 24.01.2022, p. 180"
  },
  {
    "objectID": "stat-tests-theorie.html#sec-statistisches-testen-alpha-adjust",
    "href": "stat-tests-theorie.html#sec-statistisches-testen-alpha-adjust",
    "title": "20  Die Testtheorie",
    "section": "\n20.3 Adjustierung für multiple Vergleiche",
    "text": "20.3 Adjustierung für multiple Vergleiche\nDas simultane Testen von mehreren Hypothesen führt zu einer \\(\\alpha\\)-Fehler Inflation\nIm Kapitel 31 werden wir mehrere multiple Gruppenvergleiche durchführen. Das heißt, wir wollen nicht nur die Sprungweite von Hunde- und Katzenflöhen miteinander vergleichen, sondern auch die Sprungweite von Hunde- und Fuchsflöhen sowie Katzen- und Fuchsflöhen. Wir würden also \\(k = 3\\) t-Tests für die Mittelwertsvergleiche rechnen.\nDieses mehrfache Testen führt aber zu einer Inflation des \\(\\alpha\\)-Fehlers oder auch Alphafehler-Kumulierung genannt. Daher ist die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird, nicht mehr durch das Signifikanzniveau \\(\\alpha\\) kontrolliert, sondern kann sehr groß werden.\nGehen wir von einer Situation mit \\(k\\) Null- und Alternativhypothesen aus. Wir rechnen also \\(k\\) statistische Tests und alle Nullhypothesen werden zum lokalen Niveau \\(\\alpha_{local} = 0.05\\) getestet. Im Weiteren nehmen wir an, dass tatsächlich alle Nullhypothesen gültig sind. Wir rechnen also \\(k\\) mal einen t-Test und machen jedes mal einen 5% Fehler Alarm zu geben, obwohl kein Effekt vorhanden ist.\nDie Wahrscheinlichkeit für einen einzelnen Test korrekterweise \\(H_0\\) abzulehnen ist \\((1 − \\alpha)\\). Da die \\(k\\) Tests unabhängig sind, ist die Wahrscheinlichkeit alle \\(k\\) Tests korrekterweise abzulehnen \\((1 − \\alpha)^k\\). Somit ist die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird \\(1-(1-\\alpha)^k\\). In der Tabelle 20.2 wird dieser Zusammenhang nochmal mit Zahlen für verschiedene \\(k\\) deutlich.\n\n\nTabelle 20.2— Inflation des \\(\\alpha\\)-Fehlers. Wenn 50 Hypothesen getestet werden, ist die Wahrscheinlichkeit mindestens eine falsche Testentscheidung zu treffen fast sicher.\n\nAnzahl Test \\(\\boldsymbol{k}\\)\n\n\\(\\boldsymbol{1-(1-\\alpha)^k}\\)\n\n\n\n1\n0.05\n\n\n2\n0.10\n\n\n10\n0.40\n\n\n50\n0.92\n\n\n\n\nAus Tabelle 20.3 können wir entnehmen, dass wenn 100 Hypothesen getestet werden, werden 5 Hypothesen im Schnitt fälschlicherweise abgelehnt. Die Tabelle 20.3 ist nochmal die Umkehrung der vorherigen Tabelle 20.2.\n\n\nTabelle 20.3— Inflation des \\(\\alpha\\)-Fehlers. Erwartete Anzahl fälschlich abgelehnter Nullhypothesen abhängig von der Anzahl der durchgeführten Tests\n\nAnzahl Test \\(\\boldsymbol{k}\\)\n\n\\(\\boldsymbol{\\alpha \\cdot k}\\)\n\n\n\n1\n0.05\n\n\n20\n1\n\n\n100\n5\n\n\n200\n10\n\n\n\n\nNachdem wir verstanden haben, dass wiederholtes statistisches Testen irgendwann immer ein signifikantes Ergebnis produziert, müssen wir für diese \\(\\alpha\\) Inflation unsere Ergebnisse adjustieren. Ich folgenden stelle ich verschiedene Adjustierungsverfahren vor.\nWie können wir nun die p-Werte in R adjustieren? Zum einen passiert dies teilweise automatisch zum anderen müssen wir aber wissen, wo wir Informationen zu den Adjustierungsmethoden finden. Die Funktion p.adjust() ist hier die zentrale Anlaufstelle. Hier finden sich alle implementierten Adjustierungsmethoden in R.\nIm folgenden Code erschaffen wir uns 50 \\(z\\)-Werte von denen 25 aus einer Normalverteilung \\(\\mathcal{N}(0, 1)\\) und 25 aus einer Normalverteilung mit \\(\\mathcal{N}(3, 1)\\) kommen. Die Fläche unter allen Normalverteilungen ist Eins, da die Standatdabweichung Eins ist. Wir berechnen die \\(p-Wert\\) anhand der Fläche rechts von dem \\(z\\)-Wert. Wir testen zweiseitig, deshalb multiplizieren wir die \\(p\\)-Werte mit Zwei. Diese \\(p\\)-Werte können wir nun im Folgenden für die Adjustierung nutzen.\n\nz &lt;- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))\np &lt;- 2*pnorm(sort(-abs(z)))\n\nÜber die eckigen Klammern [] und das : können wir uns die ersten zehn p-Werte wiedergeben lassen.\n\np[1:10] %&gt;% round(5)\n\n [1] 0.00000 0.00001 0.00007 0.00009 0.00027 0.00050 0.00124 0.00251 0.00324\n[10] 0.00442\n\n\nWir sehen, dass die ersten fünf p-Werte hoch signifikant sind. Das würden wir auch erwarten, immerhin haben wir ja auch 25 \\(z\\)-Werte mit einem Mittelwert von Drei. Du kannst dir den \\(z\\)-Wert wie den \\(t\\)-Wert der Teststatistik vorstellen.\n\n20.3.1 Bonferroni Korrektur\nDie Bonferroni Korrektur ist die am weitesten verbreitete Methode zur \\(\\alpha\\) Adjustierung, da die Bonferroni Korrektur einfach durchzuführen ist. Damit die Wahrscheinlichkeit, dass mindestens eine Nullhypothese fälschlicherweise abgelehnt wird beim simultanen Testen von \\(k\\) Hypothesen durch das globale (und multiple) Signifikanzniveau \\(\\alpha = 5\\%\\) kontrolliert ist, werden die Einzelhypothesen zum lokalen Signifikanzniveau \\(\\alpha_{local} = \\tfrac{\\alpha_{5\\%}}{k}\\) getestet.\nDabei ist das Problem der Bonferroni Korrektur, dass die Korrektur sehr konservativ ist. Wir meinen damit, dass das tatsächliche globale (und multiple) \\(\\sum\\alpha_{local}\\) Niveau liegt deutlich unter \\(\\alpha_{5\\%}\\) und somit werden die Nullhypothesen zu oft beibehalten.\n\n\n\n\n\n\nAdjustierung des \\(\\boldsymbol{\\alpha}\\)-Fehlers\n\n\n\n\nDas globale \\(\\alpha\\)-Level wird durch die Anzahl \\(k\\) an durchgeführten statistischen Tests geteilt.\n\n\\(\\alpha_{local} = \\tfrac{\\alpha}{k}\\) für die Entscheidung \\(p &lt; \\alpha_{local}\\)\n\n\n\n\n\n\n\n\n\n\nAdjustierung des \\(\\boldsymbol{p}\\)-Wertes\n\n\n\n\nDie p-Werte werden mit der Anzahl an durchgeführten statistischen Tests \\(k\\) multipliziert.\n\n\\(p_{adjust} = p_{raw} \\cdot k\\) mit \\(k\\) gleich Anzahl der Vergleiche.\nwenn \\(p_{adjust} &gt; 1\\), wird \\(p_{adjust}\\) gleich 1 gesetzt, da \\(p_{adjust}\\) eine Wahrscheinlichkeit ist.\n\n\n\nWir schauen uns die ersten zehn nach Bonferroni adjustierten p-Wert nach der Anwendung der Funktion p.adjust() einmal an.\n\np.adjust(p, \"bonferroni\")[1:10] %&gt;% round(3)\n\n [1] 0.000 0.000 0.004 0.005 0.014 0.025 0.062 0.126 0.162 0.221\n\n\nNach der Adjustierung erhalten wir weniger signifikante \\(p\\)-Werte als vor der Adjustierung. Wir sehen aber, dass wir weit weniger signifikante Ergebnisse haben, als wir eventuell erwarten würden. Wir haben immerhin 25 \\(z\\)-Werte mit einem Mittelwert von Drei. Nach der Bonferroni-Adjustierung hgaben wir nur noch sechs signifikante \\(p\\)-Werte.\n\n20.3.2 Benjamini-Hochberg\nDie Benjamini-Hochberg Adjustierung für den \\(\\alpha\\)-Fehler wird auch Adjustierung nach der false discovery rate (abk. FDR) bezeichnet. Meistens werden beide Namen synoym verwendet, der Trend geht jedoch hin zur Benennung mit der Abkürzung FDR. In der Tabelle 20.4 sehen wir einmal ein Beispiel für die FDR Adjustierung. Die Idee ist, dass wir uns für jeden der \\(m\\) Vergleiche eine eigene lokale Signifikanzschwelle \\(\\alpha_{local}\\) berechnen. Dafür rangieren wir zuerst unsere Vergleiche nach dem \\(p\\)-Wert. Der kleinste \\(p\\)-Wert kommt zuerst und dann der Rest der anderen \\(p\\)-Werte. Wir berechnen jedes lokale Signifkanzniveau mit \\(\\alpha_{local} =(i/m)\\cdot Q\\). Dabei steht das \\(i\\) für den jeweiligen Rang und das \\(m\\) für unsere Anzahl an Vergleichen. In unserem Beispiel haben wir \\(m = 25\\) Vergleiche. Jetzt kommt der eugentlich spannende Teil. Wir können jetzt \\(Q\\) als unsere false discovery rate selber wählen! In unserem Beispiel setzen wir die FDR auf 25%. Es geht aber auch 20% oder 10%. Wie du möchtest.\nIn der letzten Spalte der Tavbelle siehst du die Entscheidung, ob eine Variable noch signifkant ist oder nicht. Wir entscheiden nach folgender Regel. Der fettgedruckte p-Wert für den fertilizer ist der höchste p-Wert, der auch kleiner mit \\(0.042 &lt; 0.050\\) als der kritische Wert ist. Alle darüber liegenden Werte und damit diejenigen mit niedrigeren p-Werten werden hervorgehoben und als signifikant betrachtet, auch wenn diese p-Werte unter den kritischen Werten liegen. Beispielsweise sind N und sun einzeln nicht signifikant, wenn Sie das Ergebnis mit der letzten Spalte vergleichen. Mit der FDR-Korrektur werden sie jedoch als signifikant angesehen.\n\n\nTabelle 20.4— Beispiel für die Benjamini-Hochberg-Prozedur der \\(\\alpha\\)-Fehleradjustierung.\n\n\n\n\n\n\n\n\nVariable\n\\(\\boldsymbol{Pr(D|H_0)}\\)\nRang (\\(\\boldsymbol{i}\\))\n\\(\\boldsymbol{(i/m)\\cdot Q}\\)\n\n\n\n\nfe\n0.001\n1\n\\(1/25 \\cdot 0.25 = 0.01\\)\ns.\n\n\nwater\n0.008\n2\n\\(2/25 \\cdot 0.25 = 0.02\\)\ns.\n\n\nN\n0.039\n3\n\\(3/25 \\cdot 0.25 = 0.03\\)\ns.\n\n\nsun\n0.041\n4\n\\(4/25 \\cdot 0.25 = 0.04\\)\ns.\n\n\nfertilizer\n0.042\n5\n\\(5/25 \\cdot 0.25 = 0.05\\)\ns.\n\n\ninfected\n0.060\n6\n\\(6/25 \\cdot 0.25 = 0.06\\)\nn.s.\n\n\nwind\n0.074\n7\n\\(7/25 \\cdot 0.25 = 0.07\\)\nn.s.\n\n\nS\n0.205\n8\n\\(8/25 \\cdot 0.25 = 0.08\\)\nn.s.\n\n\n…\n…\n…\n…\n…\n\n\nblock\n0.915\n25\n\\(25/25 \\cdot 0.25 = 0.25\\)\nn.s.\n\n\n\n\nDie adjustierten \\(p\\)-Werte nach der Benjamini-Hochberg-Prozedur ist etwas umständlicher, deshalb benutzen wir hier die Funktion p.adjust() in R und erhalten damit die adjustierten \\(p\\)-Werte wieder.\n\np.adjust(p, \"BH\")[1:10] %&gt;% round(3)\n\n [1] 0.000 0.000 0.001 0.001 0.003 0.004 0.009 0.016 0.018 0.022\n\n\nMit der Bonferroni Adjustierung und der FDR Adjustierung haben wir zwei sehr gute Möglichkeiten für das multiple Testen zu korrigieren. Die FDR Korrektur wird häufiger in der Genetik bzw. Bioinformatik eingesetzt. In dem Kontext kommt auch die recht ähnliche Benjamini & Yekutieli (abk. BY) Adjustierung vor. Die Benjamini & Yekutieli unterscheidet sich aber nur in Nuancen unter bestimmten Rahmenbedingungen von der FDR Adjustierung. Wir lassen den statistischen Engel hier mal am Straßenrand stehen und wenden uns der letzten Adjustierung zu.\n\n20.3.3 Dunn-Sidak\nDie Sidak Korrektur berechnet das lokale Signifikanzniveau \\(\\alpha_{local}\\) auf folgende Art und Weise. Wir haben ein globales \\(\\alpha\\) von 5%. Das \\(\\alpha_{local}\\) berechnen wir indem wir die Anzahl der Vergleiche \\(m\\) wie folgt miteinander verbinden.\n\\[\n\\alpha_{local} = 1 - (1 - \\alpha)^{\\tfrac{1}{m}}\n\\]\nDie Sidak Korrektur ist weniger streng als die Bonferroni-Korrektur, aber das auch nur sehr geringfügig. Zum Beispiel beträgt für \\(\\alpha = 0.05\\) und \\(m = 10\\) das Bonferroni-bereinigte lokale Signifikanzniveau \\(\\alpha_{local} = 0.005\\) und das nach Sidak Korrektur ungefähr \\(0.005116\\). Das ist jetzt auch kein so großer Unterschied. Wir finden die Sidak Korrektur dann im R Paket emmeans für die Adjustierung bei den multiplen Vergleichen wieder."
  },
  {
    "objectID": "stat-tests-theorie.html#referenzen",
    "href": "stat-tests-theorie.html#referenzen",
    "title": "20  Die Testtheorie",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nGigerenzer, Gerd, Stefan Krauss, und Oliver Vitouch. 2004. „The null ritual“. The Sage handbook of quantitative methodology for the social sciences, 391–408.\n\n\nWasserstein, Ronald L, Allen L Schirm, und Nicole A Lazar. 2019. „Moving to a world beyond ‚p&lt; 0.05‘“. The American Statistician. Taylor & Francis."
  },
  {
    "objectID": "stat-tests-effect.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-effect.html#genutzte-r-pakete-für-das-kapitel",
    "title": "21  Der Effektschätzer",
    "section": "\n21.1 Genutzte R Pakete für das Kapitel",
    "text": "21.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, see, effectsize)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-effect.html#unterschied-zweier-mittelwerte",
    "href": "stat-tests-effect.html#unterschied-zweier-mittelwerte",
    "title": "21  Der Effektschätzer",
    "section": "\n21.2 Unterschied zweier Mittelwerte",
    "text": "21.2 Unterschied zweier Mittelwerte\nWir berechnen zwei Mittelwerte \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\). Wenn wir wissen wollen wie groß der Effekt zwischen den beiden Mittelwerten ist, dann bilden wir die Differenz. Wir berechnen das \\(\\Delta_{y_1-y_2}\\) für \\(y_1\\) und \\(y_2\\) indem wir die beiden Mittelwerte voneinander abziehen.\n\\[\n\\Delta_{y_1-y_2} = \\bar{y}_1 - \\bar{y}_2\n\\]\nWarum schreiben wir hier vermutlich? Ein statistischer Test ist eine Funktion von \\(\\Delta\\), \\(s\\) und \\(n\\). Wir können auch mit kleinem \\(\\Delta\\) die Nullhypothese ablehnen, wenn \\(s\\) und \\(n\\) eine passende Teststatistik generieren. Siehe dazu auch das Kapitel 19.3.\nWenn es keinen Unterschied zwischen den beiden Mittelwerten \\(\\bar{y}_1\\) und \\(\\bar{y}_2\\) gibt, dann ist die Differenz \\(\\Delta_{y_1-y_2} = \\bar{y}_1 - \\bar{y}_2\\) gleich 0. Wir sagen, die Nullhypothese vermutlich gilt, wenn die Differenz klein ist. Was wir besser annehmen können ist, dass die Relevanz klein ist. Effekt mit einem geringen Mittelwertsunterschied sind meistens nicht relevant. Aber diese Einschätzung hängt stark von der Fragestellung ab.\n\\[\nH_0: \\Delta_{y_1-y_2} = \\bar{y}_1 - \\bar{y}_2 = 0\n\\]\nIn Tabelle 21.1 ist nochmal ein sehr simples Datenbeispiel gegeben an dem wir den Zusammenhang nochmal nachvollziehen wollen.\n\n\n\n\nTabelle 21.1— Beispiel für die Berechnung von einem Mittelwertseffekt an der Sprunglänge [cm] von Hunde und Katzenflöhen.\n\nanimal\njump_length\n\n\n\ncat\n8.0\n\n\ncat\n7.9\n\n\ncat\n8.3\n\n\ncat\n9.1\n\n\ndog\n8.0\n\n\ndog\n7.8\n\n\ndog\n9.2\n\n\ndog\n7.7\n\n\n\n\n\n\nNehmen wir an, wir berechnen für die Sprungweite [cm] der Hundeflöhe einen Mittelwert von \\(\\bar{y}_{dog} = 8.2\\) und für die Sprungweite [cm] der Katzenflöhe einen Mittelwert von \\(\\bar{y}_{cat} =8.3\\). Wie große ist nun der Effekt? Oder anders gesprochen, welchen Unterschied in der Sprungweite macht es aus ein Hund oder eine Katze zu sein? Was ist also der Effekt von animal? Wir rechnen \\(\\bar{y}_{dog} - \\bar{y}_{cat} = 8.2 - 8.3 = -0.1\\). Zum einen wissen wir jetzt “die Richtung”. Da wir ein Minus vor dem Mittelwertsunterschied haben, müssen die Katzenflöhe weiter springen als die Hundeflöhe, nämlich 0.1 cm. Dennoch ist der Effekt sehr klein.\n\n21.2.1 Cohen’s d\nDa der Mittlwertsunterschied alleine nnur eine eingeschränkte Aussage über den Effekt erlaubt, gibt es noch Effektschätzer, die den Mittelwertsunterschied \\(\\Delta_{y_1-y_2}\\) mit der Streuung \\(s^2\\) sowie der Fallzahl zusammenbringt. Der bekannteste Effektschätzer für einen Mittelwertsunterschied bei großer Fallzahl mit mehr als 20 Beobachtungen ist Cohen’s d. Wir können Cohen’s d wie folgt berechnen.\n\\[\n|d| = \\cfrac{\\bar{y}_1-\\bar{y}_2}{\\sqrt{\\cfrac{s_1^2+s_2^2}{2}}}\n\\]\nWenn wir die berechneten Mittelwerte und die Varianz der beiden Gruppen in die Formel einsetzten ergibt sich ein absolutes Cohen’s d von 0.24 für den Gruppenvergleich.\n\\[\n|d| = \\cfrac{8.2 - 8.3}{\\sqrt{(0.5^2+0.3^2) /2}} = \\cfrac{-0.1}{0.41} = \\lvert-0.24\\rvert\n\\]\n\n\nMehr Informationen zu Cohen’s d gibt es auf der Hilfeseite von effectsize: Interpret standardized differences\nWas denn nun Cohen’s d exakt aussagt, kann niemand sagen. Aber wir haben einen Wust an möglichen Grenzen. Hier soll die Grenzen von Cohen (1988) einmal angegeben werden. Cohen (1988) hat in seiner Arbeit folgende Grenzen in Tabelle 21.2 für die Interpretation von \\(d\\) vorgeschlagen.\n\n\nTabelle 21.2— Interpretation der Effektstärke nach Cohen (1988).\n\nCohen’s d\nInterpretation des Effekts\n\n\n\n\\(d &lt; 0.2\\)\nSehr klein\n\n\n\\(0.2 \\leq d &lt; 0.5\\)\nKlein\n\n\n\\(0.5 \\leq d &lt; 0.8\\)\nMittel\n\n\n\\(d \\geq 0.8\\)\nStark\n\n\n\n\nWir können auch über die Funktion cohens_d() Cohen’s d einfach in R berechnen. Die Funktion cohens_d() akzeptiert die Formelschreibweise. Die 95% Konfidenzintervalle sind mit Vorsicht zu interpretieren. Denn die Nullhypothese ist hier nicht so klar formuliert. Wir lassen also die 95% Konfidenzintervalle erstmal hier so stehen.\n\ncohens_d(jump_length ~ animal, data = data_tbl, pooled_sd = TRUE)\n\nCohen's d |        95% CI\n-------------------------\n0.24      | [-1.16, 1.62]\n\n- Estimated using pooled SD.\n\n\nDankenswerterweise gibt es noch die Funktion interpret_cohens_d, die es uns erlaubt auszusuchen nach welche Literturquelle wir den Wert von Cohen’s d interpretieren wollen. Ob dieser Effekt relevant zur Fragestellung ist musst du selber entscheiden.\n\ninterpret_cohens_d(0.24, rules = \"cohen1988\")\n\n[1] \"small\"\n(Rules: cohen1988)\n\n\n\n21.2.2 Hedges’ g\nSoweit haben wir uns mit sehr großen Fallzahlen beschäftigt. Cohen’s d ist dafür auch sehr gut geeigent und wenn wir mehr als 20 Beobachtungen haben, können wir Cohen’s d auch gut anwenden. Wenn wir weniger Fallzahl vorliegen haben, dann können wir Hedges’ g nutzen. Hedges’ g bietet eine Verzerrungskorrektur für kleine Stichprobengrößen (\\(N &lt; 20\\)) sowie die Möglichkeit auch für unbalanzierte Gruppengrößen einen Effektschätzeer zu berechnen. Die Formel sieht mit dem Korrekturterm recht mächtig aus.\n\\[\ng = \\cfrac{\\bar{y}_1 - \\bar{y}_2}{s^*} \\cdot \\left(\\cfrac{N-3}{N-2.25}\\right) \\cdot \\sqrt{\\cfrac{N-2}{N}}\n\\]\nmit\n\\[\ns^* = \\sqrt{\\cfrac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\n\\]\nWir können aber einfach die Mittelwerte und die Varianzen aus unserem beispiel einsetzen. Da unsere beiden Gruppen gleich groß sind \\(n_1 = n_2\\) und damit ein balanziertes Design vorliegt, sind Cohen’s d und Hedges’ g numerisch gleich. Wir können dann noch für die geringe Fallzahl korrigieren und erhalten ein händisches \\(g = 0.18\\).\n\\[\ng = \\cfrac{8.2 - 8.3}{0.41} \\cdot \\left(\\cfrac{8-3}{8-2.25}\\right) \\cdot \\sqrt{\\cfrac{8-2}{8}} = \\lvert-0.24\\rvert \\cdot 0.87 \\cdot 0.87 \\approx 0.18\n\\]\nmit\n\\[\ns^* = \\sqrt{\\cfrac{(4-1)\\cdot0.5^2 + (4-1)\\cdot0.3^2}{4+4-2}} = \\sqrt{\\cfrac{0.75 + 0.27}{6}} = 0.41\n\\]\nIn R gibt es die Funktion hedges_g() die uns erlaubt in der Formelschreibweise direkt Hedges’ g zu berechnen. Wir sehen hier eine Abweichung von unserer händischen Rechnung. Das ist aber in soweit nicht ungewöhnlich, da es noch eine Menge Varianten der Anpassung für die geringe Fallzahl gibt. In der Anwendung nutzen wir die Funktion aus dem Paket effectsize wie hier durchgeführt.\nWir ignorieren wie auch bei Cohen’s d das 95% Konfidenzintervall, da die Interpretation ohne die Nullhypothese nicht möglich ist. Die Nullhypothese ist in diesem Fall komplexer. Wir lassen daher das 95% Konfidenzintervall erstmal einfach hier so stehen.\n\nhedges_g(jump_length ~ animal, data = data_tbl, pooled_sd = TRUE)\n\nHedges' g |        95% CI\n-------------------------\n0.21      | [-1.01, 1.41]\n\n- Estimated using pooled SD.\n\n\nAuch für Hedges’ g gibt es die Möglichkeit sich über die Funktion interpret_hedges_g() den Wert von \\(g=0.21\\) interpretieren zu lassen. Nach Sawilowsky (2009) haben wir hier einen kleinen Effekt vorliegen. Ob dieser Effekt relevant zur Fragestellung ist musst du selber entscheiden.\n\ninterpret_hedges_g(0.21, rules = \"sawilowsky2009\")\n\n[1] \"small\"\n(Rules: sawilowsky2009)\n\n\nDie Hilfeseite zu dem Paket effectsize bietet eine Liste an möglichen Referenzen für die Wahl der Interpretation der Effektstärke. Du musst dann im Zweifel schauen, welche der Quellen und damit Grenzen du nutzen willst."
  },
  {
    "objectID": "stat-tests-effect.html#unterschied-zweier-anteile",
    "href": "stat-tests-effect.html#unterschied-zweier-anteile",
    "title": "21  Der Effektschätzer",
    "section": "\n21.3 Unterschied zweier Anteile",
    "text": "21.3 Unterschied zweier Anteile\nEine Wahrscheinlichkeit und eine Chance sind nicht das Gleiche. Mehr in diesem Abschnitt.\nNeben den Unterschied zweier Mittelwerte ist auch häufig das Interesse an dem Unterschied zwischen zwei Anteilen. Nun unterscheiden wir zwischen Wahrscheinlichkeiten und Chancen. Beide Maßzahlen, die Wahrscheinlichkeit wie auch die Chance, beschreiben einen Anteil. Hier tritt häufig Verwirrung auf, daher hier zuerst ein Beispiel.\nWir behandelt \\(n = 65\\) Hunde mit dem Antiflohmittel FleaEx. Um die Wirkung von FleaEx auch bestimmen zu können haben wir uns zwei Gruppen von Hunden ausgesucht. Wir haben Hunde, die mit Flöhe infiziert sind und Hunde, die nicht mit Flöhen infiziert sind. Wir schauen nun in wie weit FleaEx gegen Flöhe hilft im Vergleich zu einer Kontrolle.\n\n\nTabelle 21.3— Eine 2x2 Tabelle als Beispiel für unterschiedliche Flohinfektionen bei nach einer Behandlung mit FleaEx für die Berechnung von Effektschätzern eines Anteils.\n\n\n\n\nGroup\n\n\n\n\n\nFleaEx\nControl\n\n\nInfected\nYes (1)\n\\(18_{\\;\\Large a}\\)\n\\(23_{\\;\\Large b}\\)\n\n\n\nNo (0)\n\\(14_{\\;\\Large c}\\)\n\\(10_{\\;\\Large d}\\)\n\n\n\n\nAus der Tabelle 21.3 können wir entnehmen, dass 18 behandelte Hunde mit Flöhen infiziert sind und 14 Hunde keine Infektion aufweisen. Bei den Hunden aus der Kontrolle haben wir 23 infizierte und 10 gesunde Tiere beobachtet.\nEs gibt verschiedene Typen von klinischen Studien, also Untersuchungen an Menschen. Einige Studien liefern nur \\(OR\\) wieder andere Studientypen liefern \\(RR\\).\nWir können nun zwei Arten von Anteilen berechnen um zu beschreiben, wie sich der Anteil an infizierten Hunden verhält. Das bekanntere ist die Frequenz oder Wahrscheinlichkeit oder Risk Ratio (\\(RR\\)). Das andere ist das Chancenverhältnis oder Odds Ratio (\\(OR\\)). Beide kommen in der Statistik vor und sind unterschiedlich zu interpretieren.\nUm die die Odds Ratio und die Risk Ratios auch in R berechnen zu können müssen wir einmal die 2x2 Kreuzabelle in R nachbauen. Wir nutzen dafür die Funktion matrix() und müssen schauen, dass die Zahlen in der 2x2 Kreuztabelle in R dann auch so sind, wie in der Datentabelle. Das ist jetzt ein schöner Codeblock, ist aber dafür da um sicherzustellen, dass wir die Zahlen richtig eintragen.\n\ncross_mat &lt;- matrix(c(18, 23, 14, 10),\n  nrow = 2, byrow = TRUE,\n  dimnames = list(\n    Infected = c(\"Yes\", \"No\"),\n    Group = c(\"FleaEx\", \"Control\")\n  )\n)\n\ncross_mat\n\n        Group\nInfected FleaEx Control\n     Yes     18      23\n     No      14      10\n\n\n\n\nGeorge, Stead, und Ganti (2020) liefert eine gute Übersicht über What’s the risk: differentiating risk ratios, odds ratios, and hazard ratios?\nSpäter werden wir das \\(OR\\) und \\(RR\\) wieder treffen. Das \\(OR\\) kommt in der logistsichen Regression als Effektschätzer vor. Wir nutzen das \\(RR\\) als Effektschätzer in der Poissonregression.\n\n21.3.1 Wahrscheinlichkeitsverhältnis oder Risk Ratio (RR)\nWir berechnen wir nun das Wahrscheinlichkeitsverhältnis oder Risk Ratio (RR)? Das Risk Ratio ist das Verhältnis von den infizierten Hunden in der Behandlung (\\(a\\)) zu allen infizierten Hunden (\\(a+c\\)) zu dem Verhältnis der gesunden Hunde in der Behandlung (\\(b\\)) zu allen gesunden Hunden (\\(b+d\\)). Das klingt jetzt etwas wirr, deshlab helfen manchaml wirklich Formeln, den Zusammenhang besser zu verstehen.\n\\(Pr(\\mbox{FleaEx}|\\mbox{infected})\\) ist die Wahrscheinlichkeit infiziert zu sein, wenn der Hund mit FleaEx behandelt wurde.\n\\[\nPr(\\mbox{FleaEx}|\\mbox{infected}) = \\cfrac{a}{a+c} = \\cfrac{18}{18+14} \\approx 0.56\n\\]\n\\(Pr(\\mbox{Control}|\\mbox{infected})\\) ist die Wahrscheinlichkeit infiziert zu sein, wenn der Hund mit in der Kontrolle war.\n\\[\nPr(\\mbox{Control}|\\mbox{infected}) = \\cfrac{b}{b+d} = \\cfrac{23}{23 + 10} \\approx 0.70\n\\]\nDas Risk Ratio ist mehr oder minder das Verhältnis von der beiden Spalten der Tabelle 21.3 für die Behandlung. Wir erhalten also ein \\(RR\\) von \\(0.76\\). Damit mindert die Gabe von FleaEx die Wahrscheinlichkeit sich mit Flöhen zu infizieren.\n\\[\n\\Delta_{y_1/y_2} = RR = \\cfrac{Pr(\\mbox{FleaEx}|\\mbox{infected})}{Pr(\\mbox{Control}|\\mbox{infected})} =  \\cfrac{0.56}{0.70} \\approx 0.80\n\\]\nWir überprüfen kurz mit der Funktion riskratio() ob wir richtig gerechnet haben. Das 95% Konfidenzintervall können wir interpretieren, dafür brauchen wir aber noch einmal eine Idee was “kein Effekt” bei einem Risk Ratio heist.\n\nriskratio(cross_mat)\n\nRisk ratio |       95% CI\n-------------------------\n0.81       | [0.32, 2.01]\n\n\nWann liegt nun kein Effekt bei einem Anteil wie dem RR vor? Wenn der Anteil in der einen Gruppe genauso groß ist wie der Anteil der anderen Gruppe.\n\\[\nH_0: RR = \\cfrac{Pr(\\mbox{dog}|\\mbox{infected})}{Pr(\\mbox{cat}|\\mbox{infected})} = 1\n\\]\nWir interpretieren das \\(RR\\) nun wie folgt. Unter der Annahme, dass ein kausaler Effekt zwischen der Behandlung und dem Outcome besteht, können die Werte des relativen Risikos auf folgende Art und Weise interpretiert werden:\n\n\n\\(RR = 1\\) bedeutet, dass die Behandlung keinen Einfluss auf das Outcome hat\n\n\\(RR &lt; 1\\) bedeutet, dass das Risiko für das Outcome durch die Behandlung verringert wird, was ein “Schutzfaktor” ist\n\n\\(RR &gt; 1\\) bedeutet, dass das Risiko für das Outcome durch die Behandlung erhöht wird, was ein “Risikofaktor” ist.\n\nDas heist in unserem Fall, dass wir mit einem RR von \\(0.80\\) eine protektive Behandlung vorliegen haben. Die Gabe von FleaEx reduziert das Risiko mit Flöhen infiziert zu werden. Durch das 95% Konfidenzintervall wissen wir auch, dass das \\(RR\\) nicht signifikant ist, da die 1 im 95% Konfidenzintervall enthalten ist.\n\n21.3.2 Chancenverhältnis oder Odds Ratio (OR)\nNeben dem Risk Ratio gibt es noch das Odds Ratio. Das Odds Ratio ist ein Chancenverhältnis. Wenn der Mensch an sich schon Probleme hat für sich Wahrscheinlichkeiten richtig einzuordnen, scheitert man allgemein an der Chance vollkommen. Dennoch ist das Odds Ratio eine gute Maßzahl um abzuschätzen wie die Chancen stehen, einen infizierten Hund vorzufinden, wenn der Hund behandelt wurde.\nScaheun wir uns einmal die Formeln an. Im Gegensatz zum Risk Ratio, welches die Spalten miteinander vergleicht, vergleicht das Odds Ratio die Zeilen. Als erstes berechnen wir die Chance unter der Gabe von FleaEx infiziert zu sein wie folgt.\n\\[\nOdds(\\mbox{FleaEx}|\\mbox{infected}) = a:b = 18:23 = \\cfrac{18}{23} = 0.78\n\\] Dann berechnen wir die Chance in der Kontrollgruppe infiziert zu sein wie folgt.\n\\[\nOdds(\\mbox{Control}|\\mbox{infected}) = c:d = 14:10 = \\cfrac{14}{10} \\approx 1.40\n\\] Abschließend bilden wir das Chancenverhältnis der Chance unter der Gabe von FleaEx infiziert zu sein zu der Chance in der Kontrollgruppe infiziert zu sein. Es ergbit sich das Odds Ratio wie folgt.\n\\[\n\\Delta_{y_1/y_2} = OR =  \\cfrac{Odds(\\mbox{Flea}|\\mbox{infected})}{Odds(\\mbox{Control}|\\mbox{infected})} = \\cfrac{a \\cdot d}{b \\cdot c} = \\cfrac{0.78}{1.40} \\approx 0.56\n\\]\nWir überprüfen kurz mit der Funktion oddsratio() ob wir richtig gerechnet haben. Das 95% Konfidenzintervall können wir interpretieren, dafür brauchen wir aber noch einmal eine Idee was “kein Effekt” bei einem Odds Ratio heist.\n\noddsratio(cross_mat)\n\nOdds ratio |       95% CI\n-------------------------\n0.56       | [0.20, 1.55]\n\n\nWann liegt nun kein Effekt bei einem Anteil wie dem OR vor? Wenn der Anteil in der einen Gruppe genauso groß ist wie der Anteil der anderen Gruppe.\n\\[\nH_0: OR =  \\cfrac{Odds(\\mbox{dog}|\\mbox{infected})}{Odds(\\mbox{cat}|\\mbox{infected})} = 1\n\\]\nWir interpretieren das \\(OR\\) nun wie folgt. Unter der Annahme, dass ein kausaler Effekt zwischen der Behandlung und dem Outcome besteht, können die Werte des Odds Ratio auf folgende Art und Weise interpretiert werden:\n\n\n\\(OR = 1\\) bedeutet, dass die Behandlung keinen Einfluss auf das Outcome hat\n\n\\(OR &lt; 1\\) bedeutet, dass sich die Chance das Outcome zu bekommen durch die Behandlung verringert wird, was ein “Schutzfaktor” ist\n\n\\(OR &gt; 1\\) bedeutet, dass sich die Chance das Outcome zu bekommen durch die Behandlung erhöht wird, was ein “Risikofaktor” ist.\n\nDas heist in unserem Fall, dass wir mit einem OR von \\(0.56\\) eine protektive Behandlung vorliegen haben. Die Gabe von FleaEx reduziert die Chance mit Flöhen infiziert zu werden. Durch das 95% Konfidenzintervall wissen wir auch, dass das \\(OR\\) nicht signifikant ist, da die 1 im 95% Konfidenzintervall enthalten ist.\n\n21.3.3 Odds Ratio (OR) zu Risk Ratio (RR)\n\n\nGrant (2014) gibt nochmal eine wissenschaftliche Diskussion des Themas zur Konvertierung von OR zu RR.\nWenn wir das OR berechnet haben, wollen wir eventuell das \\(OR\\) in dem Sinne eines Riskoverhältnisses berichten. Leider ist es nun so, dass wir das nicht einfach mit einem \\(OR\\) machen können. Ein \\(OR\\) von 3.5 ist ein großes Chancenverhältnis. Aber ist es auch 3.5-mal so wahrscheinlich? Nein so einfach können wir das OR nicht interpretieren. Wir können aber das \\(OR\\) in das \\(RR\\) umrechnen. Dafür brauchen wir aber das \\(p_0\\). Dabei ist das \\(p_0\\) das Basisrisiko also die Wahrscheinlichkeit des Ereignisses ohne die Intervention. Wenn wir nichts tun würden, wie wahrscheinlich wäre dann das Auftreten des Ereignisses? Es ergibt sich dann die folgende Formel für die Umrechnung des \\(OR\\) in das \\(RR\\).\n\\[\nRR = \\cfrac{OR}{(1 - p_0 + (p_0 \\cdot OR))}\n\\]\nSchauen wir uns das einmal in einem Beispiel an. Wir nutzen für die Umrechnung die Funktion oddsratio_to_riskratio() aus dem R Paket effectsize. Wenn wir ein \\(OR\\) von 3.5 haben, so hängt das \\(RR\\) von dem Basisriskio ab. Wenn das Basisirisko für die Erkrankung ohne die Behandlung sehr hoch ist mit \\(p_0 = 0.85\\), dann ist das \\(RR\\) sehr klein.\n\nOR &lt;- 3.5\nbaserate &lt;- 0.85\n\noddsratio_to_riskratio(OR, baserate) %&gt;% round(2)\n\n[1] 1.12\n\n\nAuf der anderen Seite nähert sich das \\(OR\\) dem \\(RR\\) an, wenn das Basisriskio für die Erkrankung mit \\(p_0 = 0.04\\) sehr klein ist.\n\nOR &lt;- 3.5\nbaserate &lt;- 0.04\n\noddsratio_to_riskratio(OR, baserate) %&gt;% round(2)\n\n[1] 3.18\n\n\nWeil wir natürlich das Basisrisiko nur abschätzen können, verbleibt hier eine gewisse Unsicherheit, wie das \\(RR\\) zu einem gegebenen \\(OR\\) aussieht."
  },
  {
    "objectID": "stat-tests-effect.html#referenzen",
    "href": "stat-tests-effect.html#referenzen",
    "title": "21  Der Effektschätzer",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nAbbott, Walter S u. a. 1925. „A method of computing the effectiveness of an insecticide“. J. econ. Entomol 18 (2): 265–67.\n\n\nFinner, H, J Kunert, und E Sonnemann. 1989. „Über die Berechnung des Wirkungsgrades von Pflanzenschutzmitteln“. Nachrichtenblatt des Deutschen Pflanzenschutzdienstes (Braunschweig) 41 (8-9): 145–49.\n\n\nGeorge, Andrew, Thor S Stead, und Latha Ganti. 2020. „What’s the risk: differentiating risk ratios, odds ratios, and hazard ratios?“ Cureus 12 (8).\n\n\nGrant, Robert L. 2014. „Converting an odds ratio to a range of plausible relative risks for better communication of research findings“. Bmj 348."
  },
  {
    "objectID": "stat-tests-ttest.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-ttest.html#genutzte-r-pakete-für-das-kapitel",
    "title": "22  Der t-Test",
    "section": "\n22.1 Genutzte R Pakete für das Kapitel",
    "text": "22.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, readxl)\n##\nset.seed(20221206)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-ttest.html#daten-und-modell-für-den-t-test",
    "href": "stat-tests-ttest.html#daten-und-modell-für-den-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.2 Daten und Modell für den t-Test",
    "text": "22.2 Daten und Modell für den t-Test\nWichtig ist, dass wir schon jetzt die Modellschreibweise lernen um die Daten später richtig in R nutzen zu können. Wir werden die Modellschreibweise immer wieder sehen. Die Modellschreibweise ist die Art und Weise wie wir in R eine Abhängigkeit beschreiben. Wir brauchen dieses Konzept in den folgenden Kapiteln. In R heißt y ~ x auch formula, also eine Formelschreibweise.\n\n\nAbbildung 22.1— Modellschreibweise \\(y\\) hängt ab von \\(x\\). Das \\(y\\) repräsentiert eine Spalte im Datensatz und das \\(x\\) repräsentiert ebenso eine Spalte im Datensatz. Wir brauchen also zwei Variablen \\(y\\) und \\(x\\), die natürlich nicht so heißen müssen.\n\nEtwas unbefriedigend, dass der t-Test nur zwei Gruppen miteinander Vergleichen kann. Mehr Gruppen gehen in der ANOVA im Kapitel 23\nWas brauchen wir damit wir den t-Test in R rechnen können? Später in der Anwendung nutzt du ja nur die Implementierung des t-Tests in R. Wir rechnen ja unsere Auswertung nicht per Hand. In R brauchen wir für den t-Test eine Spalte \\(y\\) mit kontinuierlichen Zahlen und einer Spalte \\(x\\) in dem wir einen Faktor mit zwei Leveln finden. Jedes Level steht dann für eine der beiden Gruppen. Das war es schon. Schauen wir uns nochmal den Datensatz flea_dog_cat.xlsx in Tabelle 22.1 an und überlegen, wie wir das realisieren können.\n\n\n\n\nTabelle 22.1— Tabelle der Sprunglängen [cm], Anzahl an Flöhen, Boniturnote sowie der Infektionsstatus von Hunden und Katzen.\n\nanimal\njump_length\nflea_count\ngrade\ninfected\n\n\n\ndog\n5.7\n18\n8\n0\n\n\ndog\n8.9\n22\n8\n1\n\n\ndog\n11.8\n17\n6\n1\n\n\ndog\n8.2\n12\n8\n0\n\n\ndog\n5.6\n23\n7\n1\n\n\ndog\n9.1\n18\n7\n0\n\n\ndog\n7.6\n21\n9\n0\n\n\ncat\n3.2\n12\n7\n1\n\n\ncat\n2.2\n13\n5\n0\n\n\ncat\n5.4\n11\n7\n0\n\n\ncat\n4.1\n12\n6\n0\n\n\ncat\n4.3\n16\n6\n1\n\n\ncat\n7.9\n9\n6\n0\n\n\ncat\n6.1\n7\n5\n0\n\n\n\n\n\n\nIn Abbildung 22.2 sehen wir einmal den Zusammenhang zwischen den Schreibweise \\(y \\sim x\\) und den beiden Variablen jump_length als \\(y\\) und animal als \\(x\\) aus dem Datensatz flea_dog_cat.xlsx. Wir haben also die formula Schreibweise in R als jump_length ~ animal.\n\n\nAbbildung 22.2— Modellschreibweise bzw. formula-Schreibweise in R. Die Variable \\(y\\) hängt ab von \\(x\\) am Beispiel des Datensatzes flea_dog_cat.xlsx mit den beiden Variablen jump_length als \\(y\\) und animal als \\(x\\).\n\nWir benötigen für den t-Test ein normalverteiltes \\(y\\) und einen Faktor mit zwei Leveln als \\(x\\). Wir nehmen daher mit select()die Spalte jump_length und animal aus dem Datensatz flea_dog_cat.xlsx. Wichtig ist, dass wir die Spalte animal mit der Funktion as_factor() in einen Faktor umwandeln. Anschließend speichern wir die Auswahl in dem Objekt data_tbl.\n\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\") %&gt;% \n  mutate(animal = as_factor(animal)) %&gt;% \n  select(animal, jump_length)\n\ndata_tbl\n\n# A tibble: 14 × 2\n   animal jump_length\n   &lt;fct&gt;        &lt;dbl&gt;\n 1 dog            5.7\n 2 dog            8.9\n 3 dog           11.8\n 4 dog            8.2\n 5 dog            5.6\n 6 dog            9.1\n 7 dog            7.6\n 8 cat            3.2\n 9 cat            2.2\n10 cat            5.4\n11 cat            4.1\n12 cat            4.3\n13 cat            7.9\n14 cat            6.1\n\n\nWir haben jetzt die Daten richtig vorbereiten und können uns nun mit dem t-Test beschäftigen. Bevor wir den t-Test jedoch rechnen können, müssen wir uns nochmal überlegen, was der t-Test eigentlich testet und uns die Daten einmal visualisieren."
  },
  {
    "objectID": "stat-tests-ttest.html#visualiserung-der-daten",
    "href": "stat-tests-ttest.html#visualiserung-der-daten",
    "title": "22  Der t-Test",
    "section": "\n22.3 Visualiserung der Daten",
    "text": "22.3 Visualiserung der Daten\nBevor wir einen statistischen Test rechnen, wollen wir uns erstmal die Daten, die dem Test zugrunde liegen, visualisieren. Wir schauen uns in Abbildung 22.3 einmal den Boxplot für die Sprungweiten getrennt nach Hund und Katze an.\nWir sehen, dass sich die Boxen nicht überschneiden, ein Indiz für einen signifikanten Unterschied zwischen den beiden Gruppen. Im Weiteren liegt der Median in etwa in der Mitte der beiden Boxen. Die Whisker sind ungefähr gleich bei Hunden und Katzen. Ebenso sehen wir bei beiden Gruppen keine Ausreißer.\nWir schließen daher nach der Betrachtung der Boxplots auf Folgendes:\n\nDie Sprungweite ist für beide Gruppen ist annähernd bzw. approximativ normalverteilt.\nDie Standardabweichungen und damit die Varianzen \\(s^2_{dog} = s^2_{cat}\\) der beiden Gruppen sind gleich. Es liegt somit Varianzhomogenität vor.\n\n\n\n\n\nAbbildung 22.3— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\nManchmal ist es etwas verwirrend, dass wir uns in einem Boxplot mit Median und IQR die Daten für einen t-Test anschauen. Immerhin rechnet ja ein t-Test mit den Mittelwerten und der Standardabweichung. Hier vergleichen wir etwas Äpfel mit Birnen. Deshalb in der Abbildung 22.4 der Dotplot mit dem Mittelwert und den entsprechender Standardabweichung als Fehlerbalken.\n\n\n\n\nAbbildung 22.4— Dotplot der Sprungweiten [cm] von Hunden und Katzen zusammen mit dem Mittelwert und der Stanardabweichung als Fehlerbalken.\n\n\n\nWir nutzen aber später häufig den Boxplot zur Visualisierung der einzelnen Gruppen. Über den Boxplot können wir auch gut abschätzen, ob wir eine annährende bzw. approximative Normalverteilung vorliegen haben."
  },
  {
    "objectID": "stat-tests-ttest.html#hypothesen-für-den-t-test",
    "href": "stat-tests-ttest.html#hypothesen-für-den-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.4 Hypothesen für den t-Test",
    "text": "22.4 Hypothesen für den t-Test\nOhne eine Hypothese ist das Ergebnis eines statistischen Tests wie auch der t-Test nicht zu interpretieren. Wir berechenen eine Teststatistik und einen p-Wert. Beide statistischen Maßzahlen machen eine Aussage über die beobachteten Daten \\(D\\) unter der Annahme, das die Nullhypothese \\(H_0\\) gilt.\nWie lautet nun das Hypothesenpaar des t-Tests? Der t-Test vergleicht die Mittelwerte von zwei Gruppen. Die Nullhypothese ist auch die Gleichheitshypothese. Die Alternativehypothese haben wir auch als Unterschiedshypothese bezeichnet.\nDaher ergibt sich für unser Beispiel mit den Sprungweiten für Hunde- und Katzenflöhen folgende Hypothesen. Die Nullhypothese sagt, dass die mittleren Sprungweite für die Hundeflöhe gleich der mittleren Sprungweite der Katzenflöhe ist. Die Alternativehypothese sagt aus, dass sich die mittlere Sprungweite von Hunde- und Katzenflöhen unterscheidet.\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{dog} &= \\bar{y}_{cat} \\\\  \nH_A: \\bar{y}_{dog} &\\neq \\bar{y}_{cat} \\\\   \n\\end{aligned}\n\\]\nWir testen grundsätzlich auf ein zweiseitiges \\(\\alpha\\)-Niveau von 5%."
  },
  {
    "objectID": "stat-tests-ttest.html#der-student-t-test",
    "href": "stat-tests-ttest.html#der-student-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.5 Der Student t-Test",
    "text": "22.5 Der Student t-Test\nLiegt ein normalverteiltes \\(y\\) vor und sind die Varianzen für die beiden zu vergleichenden Gruppen homogen \\(s^2_{cat} = s^2_{dog}\\), können wir einen Student t-Test rechnen. Wir nutzen dazu die folgendeFormel des Student t-Tests.\n\n\n\n\n\n\nEigentlich wäre hier folgende Formel richtig…\n\\[\ns_{p} = \\sqrt{\\frac{1}{2} (s^2_{dog} + s^2_{cat})}\n\\] …aber auch hier erwischen wir einen Statistikengel um es etwas einfacher zu machen.\n\\[\nT_{calc} = \\cfrac{\\bar{y}_{dog}-\\bar{y}_{cat}}{s_{p} \\cdot \\sqrt{\\cfrac{2}{n_{group}}}}\n\\]\nmit der vereinfachten Formel für die gepoolte Standardabweichung \\(s_p\\).\n\\[\ns_{p} = \\cfrac{s_{dog} + s_{cat}}{2}\n\\]\nWir wollen nun die Werte für \\(\\bar{y}_{dog}\\), \\(\\bar{y}_{cat}\\) und \\(s_{p}\\) berechnen. Wir nutzen hierfür R auf die etwas komplizierte Art und Weise. Es gibt in R auch die Funktion t.test(), die für uns alles auf einmal macht, aber hier nochaml zu Fuß.\n\nsum_tbl &lt;- data_tbl %&gt;% \n  group_by(animal) %&gt;% \n  summarise(mean = round(mean(jump_length), 2), \n            sd = round(sd(jump_length), 2)) \n\nsum_tbl\n\n# A tibble: 2 × 3\n  animal  mean    sd\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 dog     8.13  2.14\n2 cat     4.74  1.9 \n\n\nWir erhalten durch die Funktion group_by() den Mittelwert und die Standardabweichung für die Sprungweite getrennt für die Hunde- und Katzenflöhe. Wir können damit die beiden obigen Formeln füllen.\nWir berechnen \\(s_p\\) wie folgt.\n\\[\ns_{pooled} = \\cfrac{2.14 + 1.9}{2} = 2.02\n\\]\nAnschließend können wir jetzt \\(s_p\\) und die Mittelwerte sowie die Gruppengröße \\(n_g = 7\\) in die Formel für den Student t-Test einsetzen und die Teststatistik \\(T_{calc}\\) berechnen.\n\\[\nT_{calc} = \\cfrac{8.13- 4.74}{2.02 \\cdot \\sqrt{\\cfrac{2}{7}}} = 3.14\n\\]\nWir erhalten eine Teststatistik \\(T_{calc} = 3.14\\) die wir mit dem kritischen Wert \\(T_{\\alpha = 5\\%} = 2.17\\) vergleichen können. Da \\(T_{calc} &gt; T_{\\alpha = 5\\%}\\) ist, können wir die Nullhypothese ablehnen. Wir haben ein signifikanten Unterschied zwischen den mittleren Sprungweiten von Hunde- und Katzenflöhen nachgewiesen.\nSoweit für den Weg zu Fuß. Wir rechnen in der Anwendung keinen Student t-Test per Hand. Wir nutzen die Formel t.test(). Da wir den Student t-Test unter der Annahme der Varianzhomogenität nutzen wollen, müssen wir noch die Option var.equal = TRUE wählen.\nDie Funktion t.test() benötigt erst die das \\(y\\) und \\(x\\) in Modellschreibweise mit den Namen, wie die beiden Variablen auch im Datensatz data_tbl stehen. In unserem Fall ist die Modellschreibweise dann jump_length ~ animal. Im Weiteren müssen wir noch den Datensatz angeben den wir verwenden wollen durch die Option data = data_tbl. Dann können wir die Funktion t.test() ausführen.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  jump_length by animal\nt = 3.1253, df = 12, p-value = 0.008768\nalternative hypothesis: true difference in means between group dog and group cat is not equal to 0\n95 percent confidence interval:\n 1.025339 5.746089\nsample estimates:\nmean in group dog mean in group cat \n         8.128571          4.742857 \n\n\nWir erhalten eine sehr lange Ausgabe, die aucb etwas verwirrend aussieht. Gehen wir die Ausgabe einmal durch. Ich gehe nicht auf alle Punkte ein, sondern konzentriere mich hier auf die wichtigsten Aspekte.\n\n\nt = 3.12528 ist die berechnete Teststatistik \\(T_{calc}\\). Der Wert unterscheidet sich leicht von unserem berechneten Wert. Der Unterschied war zu erwarten, wir haben ja auch die t-Test Formel vereinfacht.\n\np-value = 0.0087684 ist der berechnete p-Wert \\(Pr(T_{calc}|H_0)\\) aus der obigen Teststatistik. Daher die Fläche rechts von der Teststatistik.\n\n95 percent confidence interval: 1.0253394 5.7460892 ist das 95% Konfidenzintervall. Die erste Zahl ist die untere Grenze, die zweite Zahl ist die obere Grenze.\n\nWir erhalten hier dreimal die Möglichkeit eine Aussage über die \\(H_0\\) zu treffen. In dem obigen Output von R fehlt der kritische Wert \\(T_{\\alpha = 5\\%}\\). Daher ist die berechnete Teststatistik für die Testentscheidung nicht verwendbar. Wir nutzen daher den p-Wert und vergleichen den p-Wert mit dem \\(\\alpha\\)-Niveau von 5%. Da der p-Wert kleiner ist als das \\(\\alpha\\)-Niveau können wir wie Nullhypothese ablehnen. Wir haben einen signifikanten Unterschied. Die Entscheidung mit dem Konfidenzintervall benötigt die Signifikanzschwelle. Da wir hier einen Mittelwertsvergleich vorliegen haben ist die Signifikanzschwelle gleich 0. Wenn die 0 im Konfidenzintervall liegt können wir die Nullhypothese nicht ablehnen. In unserem Fall ist das nicht der Fall. Das Konfidenzintervall läfut von 1.025 bis 5.75. Damit ist die 0 nicht im Konfidenzintervall enthalten und wir können die Nullhypothese ablehnen. Wir haben ein signifikantes Konfidenzintervall vorliegen.\nWie wir sehen fehlt der Mittelwertsuntschied als Effekt \\(\\Delta\\) in der Standardausgabe des t-Tests in R. Wir können den Mittelwertsunterschied selber berechnen oder aber die Funktion tidy() aus dem R Paket broom nutzen. Da der Funktion tidy() kriegen wir die Informationen besser sortiert und einheitlich wiedergegeben. Da tidy eine Funktion ist, die mit vielen statistischen Tests funktioniert müssen wir wissen was die einzelnen estimate sind. Es hilft in diesme Fall sich die Visualisierung der Daten anzuschauen und die Abbildung mit den berechneten Werten abzugleichen.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = TRUE) %&gt;% \n  tidy() \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     3.39      8.13      4.74      3.13 0.00877        12     1.03      5.75\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nWir erkennen als erstes den Mittelwertsunterschied zwischen den beiden Gruppen von 3.39 cm. Danach folgen die einzelnen Mittelwerte der Sprungweiten der Hunde und Katzenflöhe mit jeweils 8.13 cm und 4.74 cm. Darauf folgt noch der p-Wert als p.value mit 0.00891 und die beiden Grenzen des Konfidenzintervalls [1.03; 5.75]."
  },
  {
    "objectID": "stat-tests-ttest.html#der-welch-t-test",
    "href": "stat-tests-ttest.html#der-welch-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.6 Der Welch t-Test",
    "text": "22.6 Der Welch t-Test\nDer t-Test ist auch in der Lage mit Varianzhetrogenität umzugehen. Das heißt, wenn die Varianzen der beiden Gruppen nicht gleich sind. Dadurch ändert sich die Formel für den t-Test wie folgt. Dann nennen wir den statistischen Test Welch t-Test.\n\\[\nT_{calc} = \\cfrac{\\bar{y_1} - \\bar{y_2}}{\\sqrt{\\cfrac{s^2_{y_1}}{n_1} + \\cfrac{s^2_{y_2}}{n_2}}}\n\\]\nWir sehen, dass sich die Formel etwas andert. Da wir nicht mehr annehmen, dass die Varianzen homogen und daher gleich sind, können wir auch keinen gepoolten Varianzschätzer \\(s_p\\) berechnen. Die Varianzen gehen einzeln in die Formel des Welch t-Tests ein. Ebenso müssen die beiden Gruppen nicht mehr gleich groß sein. Statt einen Wert \\(n_g\\) für die Gruppengröße können wir auch die beiden Gruppengrößen separat angeben.\n\n\nHier muss man noch bedenken, dass die Freiheitsgrade anders berechnet werden Die Freiheitsgrade werden wie folgt berechnet.\n\\[\ndf = \\cfrac{\\left(\\cfrac{s^2_{y_1}}{n} +\n    \\cfrac{s^2_{y_2}}{m}\\right)^2}{\\cfrac{\\left(\\cfrac{s^2_{y_1}}{n}\\right)^2}{n-1} + \\cfrac{\\left(\\cfrac{s^2_{y_2}}{m}\\right)^2}{m-1}}\n\\]\nEs ergibt keinen tieferen Sinn die obige Formel nochmal händisch auszurechnen. Die Zahlen ändern sich leicht, aber konzeptionell erhalten wir hier keinen Mehrwert. Deshalb schauen wir uns gleich die Umsetzung in R an. Wir nutzen erneut die Funktion t.test() und zwar diesmal mit der Option var.equal = FALSE. Damit geben wir an, dass die Varianzen heterogen zwischen den beiden Gruppen sind. Wir nutzen in unserem Beispiel die gleichen Zahlen und Daten wie schon im obigen Student t-Test Beispiel.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  jump_length by animal\nt = 3.1253, df = 11.831, p-value = 0.008906\nalternative hypothesis: true difference in means between group dog and group cat is not equal to 0\n95 percent confidence interval:\n 1.021587 5.749842\nsample estimates:\nmean in group dog mean in group cat \n         8.128571          4.742857 \n\n\nWir sehen das viele Zahlen nahezu gleich sind. Das liegt auch daran, dass wir in unserem Daten keine große Abweichung von der Varianzhomogenität haben. Wir erhalten die gleichen Aussagen wie auch schon im Student t-Test.\nSchauen wir uns nochmal die Ausgabe der Funkton tidy() an.\n\nt.test(jump_length ~ animal, \n       data = data_tbl, var.equal = FALSE) %&gt;% \n  tidy() \n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     3.39      8.13      4.74      3.13 0.00891      11.8     1.02      5.75\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\nFür das Erkennen von Normalverteilung und Varianzheterogenität werden häufig sogenannte Vortest empfohlen. Aber auch hier gilt, bei kleiner Fallzahl liefern die Vortests keine verlässlichen Ergebnisse. In diesem Fall ist weiterhin die Beurteilung über einen Boxplot sinnvoller.\nWir sehen hier etwas besser, dass es kaum Abweichungen gibt. Alles egal? Nicht unbedingt. Das Problem ist eher das Erkennen von Varianzheterogenität in sehr kleinen Datensätzen. Kleine Datensätze meint Datensätze unter 30 Beobachtungen je Gruppe. Erst aber dieser Anzahl lassen sich unverzerrte Histogramme zeichnen und so aussagekräftige Abschätzungen der Varianzhomogenität oder Varianzheterogenität treffen."
  },
  {
    "objectID": "stat-tests-ttest.html#der-verbundene-t-test-paired-t-test",
    "href": "stat-tests-ttest.html#der-verbundene-t-test-paired-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.7 Der verbundene t-Test (Paired t-Test)",
    "text": "22.7 Der verbundene t-Test (Paired t-Test)\nIm folgenden Datenbeispiel in Tabelle 22.2 haben wir eine verbundene Stichprobe. Das heißt wir haben nicht zehn Flöhe gemessen sondern fünf Flöhe. Einmal im ungefütterten Zustand unfed und einmal im gefütterten Zustand fed. Wir wollen nun wissen, ob der Fütterungszustand Auswirkungen auf die Sprungweite in [cm] hat.\n\n\n\n\nTabelle 22.2— Tabelle der Sprunglängen [cm] von fünf Flöhen zu zwei Zeitpunkten. Einmal wurde die Sprungweite ungefüttert und einmal gefüttert bestimmt. Die Daten liegen im Wide Format vor.\n\nunfed\nfed\ndiff\n\n\n\n5.2\n6.1\n0.9\n\n\n4.1\n5.2\n1.1\n\n\n3.5\n3.9\n0.4\n\n\n3.2\n4.1\n0.9\n\n\n4.6\n5.3\n0.7\n\n\n\n\n\n\nWir nutzen folgende Formel für den paired t-Test für verbundene Stichproben.\n\\[\nT_{calc} = \\sqrt{n}\\cfrac{\\bar{d}}{s_d}\n\\]\nWir können \\(\\bar{d}\\) als Mittelwert der Differenzen der Variablen diff berechnen. Ebenso verfahren wir mit der Standardabweichung der Differenzen \\(s_d\\).\n\\[\nT_{calc} = \\sqrt{10}\\cfrac{0.8}{0.26} = 6.88\n\\]\nUm den die Funktion t.test()in R mit der Option paired = TRUE für den paired t-Test zu nutzen, müssen wir die Daten nochmal über die Funktion gather() in das Long Format umwandeln. Wir wollen nun wissen, ob der Fütterungszustand food_status Auswirkungen auf die Sprungweite in [cm] hat.\n\nt.test(jump_length ~ food_status, \n       data = paired_tbl, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  jump_length by food_status\nt = 6.7612, df = 4, p-value = 0.002496\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.4714866 1.1285134\nsample estimates:\nmean difference \n            0.8 \n\n\nDie Ausgabe des paired t-Test ähnelt stark der Ausage des Student t-Test. Wir erhalten ebenfalls den wichtigen p-Wert mit 0.0025 sowie das 95% Konfidenzintervall mit [0.47; 1.13]. Zum einen ist \\(0.0025 &lt; \\alpha\\) und somit können wir die Nullhypothese ablehnen, zum anderen ist auch die 0 nicht mit in dem Konfidenzintervall, womit wir auch hier die Nullhypothese ablehnen können.\n\nt.test(jump_length ~ food_status, \n       data = paired_tbl, paired = TRUE) %&gt;% \n  tidy() \n\n# A tibble: 1 × 8\n  estimate statistic p.value parameter conf.low conf.high method     alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      \n1      0.8      6.76 0.00250         4    0.471      1.13 Paired t-… two.sided  \n\n\nDie Funktion tidy() gibt uns in diesem Fall keine neuen zusätzlichen Informationen."
  },
  {
    "objectID": "stat-tests-ttest.html#freiheitsgrade-im-t-test",
    "href": "stat-tests-ttest.html#freiheitsgrade-im-t-test",
    "title": "22  Der t-Test",
    "section": "\n22.8 Freiheitsgrade im t-Test",
    "text": "22.8 Freiheitsgrade im t-Test\nDer t-Verteilung der Teststatistiken des t-Tests verhält sich nicht wie eine klassische Normalverteilung, die durch den Mittelwert und die Standardabweichung definiert ist. Die t-Verteilung ist nur durch die Freiheistgrade definiert. Der Freiheitsgrade in einem t-Test mit zwei Stichproben ist gegeben durch \\(df = n_1 + n_2 -2\\). Damit beschreiben die Freiheitsgrade grob die Fallzahl. Je mehr Fallzahl desto großer der Freiheitsgrad eines t-Tests.\nAbbildung 22.5 visualisiert diesen Zusammenhang von Freiheitsgraden und der Form der t-Verteilung. Je kleiner die Freiheitgrade und damit die Fallzahl, desto weiter sind die Verteilungsschwänze. Daher benötigen wir auch größere \\(T_{calc}\\) Werte um ein signifikantes Ergebnis zu erhalten. Die Fläche unter der t-Verteilung ist immer gleich.\n\n\nAbbildung 22.5— Die t-Verteilung für drei beispielhafte Freiheitsgrade. Je größer die Freiheitsgrade und damit die Fallzahl, desto näher kommt die t-Verteilung einer Normalverteilung nahe. Bei einer geringeren Fallzahl, müssen damit größere \\(T_{calc}\\) Werte erreicht werden um eine signifikantes Ergebnis zu erhalten, da mehr Fläche nach rechts wandert.\n\nIn diesem Exkurs wollen wir einmal überlegen, warum wir verschiedene Freiheitsgrade für die Testverteilungen brauchen. Wir wollen die Verteilung der Teststatistik unter der Nullhypothese für den Freiheitsgrad \\(df_1 = n_1 + n_2 -2 = 6\\) und somit \\(n_1 = n_2 = 4\\) sowie den Freiheitsgrad \\(df_2 = n_1 + n_2 -2 = 18\\) und somit \\(n_1 = n_2 = 10\\) einmal herleiten.\nZuerst gehen wir davon aus, dass die Mittelwerte der Sprungweite der Hunde- und Katzenflöhe gleich sind \\(\\bar{y}_{cat} = \\bar{y}_{dog} = (8.13 + 4.74)/2 = 6.43\\). Daher nehmen wir an, dass die Mittelwerte aus der gleichen Normalverteilung kommen. Wir ziehen also vier Sprungweiten jeweils für die Hunde- und Katzenflöhe aus einer Normalverteilung mit \\(\\mathcal{N}(6.43, 2.0)\\). Das ganze wiederholen wir dann für jeweils zehn Hunde- und Katzenflöhe. Wir nutzen dafür die Funktion rnorm(). Anschließend berechnen wir die Teststatistik. Diesen Schritt wiederholen wir eintausend Mal.\nIm Folgenden berechnen wir eintausend Mal die Teststatistik, wenn die Nullhypothese gilt, für jeweils vier Hunde- und Katzenflöhe. Wir haben damit einen Freiheitsgrad von \\(df_1 = 6\\).\n\nT_n4_vec &lt;- map_dbl(1:1000, function(...){\n  dog_vec &lt;- rnorm(n = 4, mean = 6.43, sd = 2.0)\n  cat_vec &lt;- rnorm(n = 4, mean = 6.43, sd = 2.0)\n  s_p &lt;- (sd(cat_vec) + sd(dog_vec))/2 \n  T_calc &lt;- (mean(cat_vec) - mean(dog_vec))/(s_p * sqrt(2/4)) \n  return(T_calc)  \n}) %&gt;% round(2)\n\nUnd das ganze nochmal für jeweils zehn Hunde- und Katzenflöhe. Wir haben damit einen Freiheitsgrad von \\(df_2 = 18\\).\n\nT_n10_vec &lt;- map_dbl(1:1000, function(...){\n  dog_vec &lt;- rnorm(n = 10, mean = 6.43, sd = 2.0)\n  cat_vec &lt;- rnorm(n = 10, mean = 6.43, sd = 2.0)\n  s_p &lt;- (sd(cat_vec) + sd(dog_vec))/2 \n  T_calc &lt;- (mean(cat_vec) - mean(dog_vec))/(s_p * sqrt(2/10)) \n  return(T_calc)  \n}) %&gt;% round(2)\n\nSchauen wir uns einmal die Spannweite der Teststatistiken mit einer niedrigen Fallzahl an. Darüber hinaus wollen wir wissen, wie viele Teststatistiken gibt es, die unsere Teststatistik \\(T_{calc} = 3.14\\) vom obigen Beispiel übersteigen.\n\nT_n4_vec %&gt;% range()\n\n[1] -7.59  6.72\n\nsum(T_n4_vec &gt; 3.14)\n\n[1] 8\n\n\nIm Gegensatz dazu dann die Spannweite der Teststatistiken mit einer höheren Fallzahl an. Auch hier wollen wir wissen, wie viele Teststatistiken es gibt, die unsere Teststatistik \\(T_{calc} = 3.14\\) vom obigen Beispiel übersteigen.\n\nT_n10_vec %&gt;% range()\n\n[1] -3.93  3.39\n\nsum(T_n10_vec &gt; 3.14)\n\n[1] 1\n\n\nIn Abbildung 22.6 sehen wir die Verteilung der berechneten eintausend Verteilungen nochmal als ein Histogramm dargestellt. Wiederum sehen wir, dass unsere berechnete Teststatistik \\(T_{calc} = 3.14\\),dargestellt als gestrichelte Linie, sehr weit rechts am Rand der Verteilung liegt.\n\n\n\n\nAbbildung 22.6— Histogramm der 1000 gerechneten Teststaistiken \\(T_{calc} = 3.14\\), wenn die \\(H_0\\) war wäre und somit kein Unterschied zwischen den Mittelwerten der Sprungweiten der Hunde- und Katzenflöhe vorliegen würde."
  },
  {
    "objectID": "stat-tests-anova.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-anova.html#genutzte-r-pakete-für-das-kapitel",
    "title": "23  Die ANOVA",
    "section": "\n23.1 Genutzte R Pakete für das Kapitel",
    "text": "23.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, effectsize, ggpubr, \n               see)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac1",
    "href": "stat-tests-anova.html#sec-fac1",
    "title": "23  Die ANOVA",
    "section": "\n23.2 Einfaktorielle ANOVA",
    "text": "23.2 Einfaktorielle ANOVA\n\n\n\n\n\n\nEinführung in die einfaktorielle ANOVA per Video\n\n\n\nDu findest auf YouTube die einfaktorielle ANOVA als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\nEin weiteres Video mit mehr Fokus auf die Visualisierung findest du ebenfalls auf YouTube.\n\n\nDie einfaktorielle ANOVA ist die simpelste Form der ANOVA. Wir nutzen einen Faktor \\(x\\) mit mehr als zwei Leveln. Im Rahmen der einfaktoriellen ANOVA wollen wir uns auch die ANOVA theoretisch einmal anschauen. Danach wie die einfaktorielle ANOVA in R genutzt wird. Ebenso werden wir die einfaktorielle ANOVA visualisieren. Abschließend müssen wir uns noch überlegen, ob es einen Effektschätzer für die einfaktorielle ANOVA gibt.\n\n\n\n\n\n\nDie einfaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität über den Behandlungsfaktor \\(x\\). Daher alle Level von \\(x\\) sollen die gleiche Varianz haben.\nUnsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x\\) homogen in den Varianzen sind. Später mehr dazu, wenn wir beides nicht vorliegen haben…\n\n23.2.1 Daten für die einfaktorielle ANOVA\nWir wollen uns nun erstmal den einfachsten Fall anschauen mit einem simplen Datensatz. Wir nehmen ein normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Hätten wir nur zwei Level, dann hätten wir auch einen t-Test rechnen können.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac1_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox.csv\") %&gt;%\n  select(animal, jump_length) %&gt;% \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt fac1_tbl mit dem Datensatz in Tabelle 23.1 nochmal dargestellt.\n\n\n\n\nTabelle 23.1— Selektierter Datensatz für die einfaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\n\n\n\ndog\n5.7\n\n\ndog\n8.9\n\n\ndog\n11.8\n\n\ndog\n8.2\n\n\ndog\n5.6\n\n\ndog\n9.1\n\n\ndog\n7.6\n\n\ncat\n3.2\n\n\ncat\n2.2\n\n\ncat\n5.4\n\n\ncat\n4.1\n\n\ncat\n4.3\n\n\ncat\n7.9\n\n\ncat\n6.1\n\n\nfox\n7.7\n\n\nfox\n8.1\n\n\nfox\n9.1\n\n\nfox\n9.7\n\n\nfox\n10.6\n\n\nfox\n8.6\n\n\nfox\n10.3\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt fac1_tbl folgendes Modell für die spätere Analyse in R.\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der einfaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n23.2.2 Hypothesen für die einfaktorielle ANOVA\nDie ANOVA betrachtet die Mittelwerte und nutzt die Varianzen um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mittelwerte jedes Levels des Faktors animal gleich sind.\n\\[\nH_0: \\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Mittelwerten unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mittelwerte unterschieden oder eben nur ein Paar. Wenn eine ANOVA die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mittelwerte sich unterscheiden.\n\\[\n\\begin{aligned}\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nWir schauen uns jetzt einmal die ANOVA theoretisch an bevor wir uns mit der Anwendung der ANOVA in R beschäftigen.\n\n23.2.3 Einfaktoriellen ANOVA theoretisch\nKommen wir zurück zu den Daten in Tabelle 23.1. Wenn wir die ANOVA per Hand rechnen wollen, dann ist nicht das Long Format die beste Wahl sondern das Wide Format. Wir haben ein balanciertes Design vorliegen, dass heißt in jeder Level sind die gleiche Anzahl Beobachtungen. Wir schauen uns jeweils sieben Flöhe von jeder Tierart an. Für eine ANOVA ist aber ein balanciertes Design nicht notwendig, wir können auch mit ungleichen Gruppengrößen eine ANOVA rechnen.\nStatt einer einfaktoriellen ANOVA könnten wir auch gleich einen pairwise.t.test()rechnen. Historisch betrachtet ist die einfaktorielle ANOVA die Visualisierung des paarweisen t-Tests.\nEine einfaktorielle ANOVA macht eigentlich keinen großen Sinn, wenn wir anschließend sowieso paarweise Vergleich, wie in Kapitel 31 beschrieben, rechnen. Aus der Historie stellte sich die Frage, ob es sich lohnt die ganze Arbeit für die paarweisen t-Tests per Hand zu rechnen. Daher wurde die ANOVA davor geschaltet. War die ANOVA nicht signifikant, dann konnte man sich dann auch die Rechnerei für die paarweisen t-Tests sparen.\nIn Tabelle 23.2 sehen wir die Daten einmal als Wide-Format dargestellt.\n\n\nTabelle 23.2— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal.\n\nj\ndog\ncat\nfox\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\n\nWir können jetzt für jedes der Level den Mittelwert über all \\(j=7\\) Beobachtungen berechnen.\n\\[\n\\begin{aligned}\n\\bar{y}_{dog} &= 8.13 \\\\\n\\bar{y}_{cat} &= 4.74 \\\\\n\\bar{y}_{fox} &= 9.16 \\\\\n\\end{aligned}\n\\]\nWir tun jetzt für einen Moment so, als gebe es den Faktor animal nicht in den Daten und schauen uns die Verteilung der einzelnen Beobachtungen in Abbildung 23.1 (a) einmal an. Wir sehen das sich die Beobachtungen von ca. 2.2cm bis 11 cm streuen. Woher kommt nun diese Streuung bzw. Varianz? Was ist die Quelle der Varianz? In Abbildung 23.1 (b) haben wir die Punkte einmal nach dem Faktor animal eingefärbt. Wir sehen, dass die blauen Beobachtungen eher weitere Sprunglängen haben als die grünen Beobachtungen. Wir gruppieren die Beobachtungen in Abbildung 23.1 (c) nach dem Faktor animal und sehen, dass ein Teil der Varianz der Daten von dem Faktor animal ausgelöst wird.\n\n\n\n\n\n(a) Die Sprungweite in [cm] ohne den Faktor animal betrachtet.\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal eingefärbt.\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal eingefärbt und gruppiert.\n\n\n\nAbbildung 23.1— Die Spungweite in [cm] in Abhängigkeit von dem Faktor animal dargestellt.\n\n\nGehen wir einen Schritt weiter und zeichnen einmal das globale Mittel in die Abbildung 23.2 (a) von \\(\\bar{y}_{..} = 7.34\\) und lassen die Beobachtungen gruppiert nach dem Faktor animal. Wir sehen, dass die Level des Faktors animal um das globale Mittel streuen. Was ja auch bei einem Mittelwert zu erwarten ist. Wir können jetzt in Abbildung 23.2 (b) die lokalen Mittel für die einzelnen Level dog, catund fox ergänzen. Und abschließend in Abbildung 23.2 (c) die Abweichungen \\(\\\\beta_i\\) zwischen dem globalen Mittel \\(\\bar{y}_{..} = 7.34\\) und den einzelnen lokalen Mittel berechnen. Die Summe der Abweichungen \\(\\\\beta_i\\) ist \\(0.79 + (-2.6) + 1.81 \\approx 0\\). Das ist auch zu erwarten, den das globale Mittel muss ja per Definition als Mittelwert gleich großen Abstand “nach oben” wie “nach unten” haben.\n\n\n\n\n\n(a) Die Sprungweite in [cm] mit den Faktor animal gruppiert und das globale Mittel \\(\\bar{y}_{..} = 7.34\\) ergänzt.\n\n\n\n\n\n(b) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die lokalen Mittel \\(\\bar{y}_{i.}\\) für jedes Level ergänzt.\n\n\n\n\n\n(c) Die Sprungweite in [cm] mit den Faktor animal gruppiert und die Abweichungen \\(\\beta_i\\) ergänzt.\n\n\n\nAbbildung 23.2— Dotplot der Spungweite in [cm] in Abhängigkeit von dem Faktor animal.\n\n\nWir tragen die Werte der lokalen Mittelwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\) noch in die Tabelle 23.3 ein. Wir sehen in diesem Beispiel warum das Wide Format besser zum Verstehen der ANOVA ist, weil wir ja die lokalen Mittelwerte und die Abweichungen per Hand berechnen. Da wir in der Anwendung aber nie die ANOVA per Hand rechnen, liegen unsere Daten immer in R als Long-Format vor. Es handelt sich hier nur um die Veranschaulichung des Konzepts der ANOVA.\n\n\nTabelle 23.3— Wide Format der Beispieldaten fac1_tbl für die jeweils \\(j=7\\) Beobachtungen für den Faktor animal. Wir ergänzen die lokalen Mittlwerte \\(\\bar{y}_{i.}\\) und deren Abweichungen \\(\\beta_i\\) vom globalen Mittelwert \\(\\bar{y}_{..} = 7.34\\).\n\nj\ndog\ncat\nfox\n\n\n\n1\n5.7\n3.2\n7.7\n\n\n2\n8.9\n2.2\n8.1\n\n\n3\n11.8\n5.4\n9.1\n\n\n4\n8.2\n4.1\n9.7\n\n\n5\n5.6\n4.3\n10.6\n\n\n6\n9.1\n7.9\n8.6\n\n\n7\n7.6\n6.1\n10.3\n\n\n\\(\\bar{y}_{i.}\\)\n\\(8.13\\)\n\\(4.74\\)\n\\(9.16\\)\n\n\n\\(\\beta_i\\)\n\\(-2.6\\)\n\\(0.79\\)\n\\(1.81\\)\n\n\n\n\nWie kriegen wir nun die ANOVA rechnerisch auf die Straße? Schauen wir uns dazu einmal die Abbildung 23.3 an. Auf der linken Seiten sehen wir vier Gruppen, die keinen Effekt haben. Die Gruppen liegen alle auf der gleichen Höhe. Es ist mit keinem Unterschied zwischen den Gruppen zu rechnen. Alle Gruppenmittel liegen auf dem globalen Mittel. Die Abweichungen der einzelnen Gruppenmittel zum globalen Mittel ist damit gleich null. Auf der rechten Seite sehen wir vier Gruppen mit einem Effekt. Die Gruppen unterscheiden sich in ihren Gruppenmitteln. Dadurch unterscheide sich aber auch die Gruppenmittel von dem globalen Mittel.\n\n\n\n\n\n(a) Kein Effekt\n\n\n\n\n\n(b) Leichter bis mittlerer Effekt\n\n\n\nAbbildung 23.3— Darstellung von keinem Effekt und leichtem bis mittleren Effekt in einer einfaktoriellen ANOVA mit einem Faktor mit vier Leveln A - D.\n\n\nWir können daher wie in Tabelle 23.4 geschrieben die Funktionsweise der ANOVA zusammenfassen. Wir vergleichen die Mittelwerte indem wir die Varianzen nutzen.\n\n\nTabelle 23.4— Zusammenfassung der ANOVA Funktionsweise.\n\n\n\n\n\n\nAll level means are equal.\n=\nThe differences between level means and the total mean are small.\n\n\n\nNun kommen wir zum eigentlichen Schwenk und warum eigentlich die ANOVA meist etwas verwirrt. Wir wollen eine Aussage über die Mittelwerte machen. Die Nullhypothese lautet, dass alle Mittelwerte gleich sind. Wie wir in Tabelle 23.4 sagen, heißt alle Mittelwerte gleich auch, dass die Abweichungen von den Gruppenmitteln zum globalen Mittel klein ist.\nWie weit die Gruppenmittel von dem globalen Mittel weg sind, dazu nutzt die ANOVA die Varianz. Die ANOVA vergleicht somit\n\ndie Varianz der einzelnen Mittelwerte der (Gruppen)Level zum globalen Mittel (eng. variability between levels)\nund die Varianz der Beobachtungen zu den einzelnen Mittelwerten der Level (eng. variability within one level)\n\nDie sum of squares sind nichts anderes als die Varianz. Wir nennen das hier nur einmal anders…\nWir berechnen also wie die Beobachtungen jeweils um das globale Mittel streuen (\\(SS_{total}\\)), die einzelnen Beobachtungen um die einzelnen Gruppenmittel \\(SS_{error}\\) und die Streuung der Gruppenmittel um das globale Mittel (\\(SS_{animal}\\)). Wir nennen die Streuung Abstandquadrate (eng. sum of squares) und damit sind die Sum of Square \\((SS)\\) nichts anderes als die Varianz. Die Tabelle 23.5 zeigt die Berechnung des Anteils jeder einzelnen Beobachtung an den jeweiligen Sum of Squares.\n\n\n\nTabelle 23.5— Berechnung der \\(SS_{animal}\\), \\(SS_{error}\\) und \\(SS_{total}\\) anhand der einzelnen gemessenen Werte \\(y\\) für durch die jeweiligen Gruppenmittel \\(\\bar{y}_{i.}\\) und dem globalen Mittel \\(\\bar{y}_{..}\\) über alle Beobachtungen\n\n\n\n\n\n\n\n\n\nanimal (x)\njump_length (y)\n\\(\\boldsymbol{\\bar{y}_{i.}}\\)\nSS\\(_{\\boldsymbol{animal}}\\)\n\nSS\\(_{\\boldsymbol{error}}\\)\n\nSS\\(_{\\boldsymbol{total}}\\)\n\n\n\n\ndog\n\\(5.7\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.7 - 8.13)^2 = 5.90\\)\n\\((5.7 - 7.34)^2 = 2.69\\)\n\n\ndog\n\\(8.9\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.9 - 8.13)^2 = 0.59\\)\n\\((8.9 - 7.34)^2 = 2.43\\)\n\n\ndog\n\\(11.8\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((11.8 - 8.13)^2 = 13.47\\)\n\\((11.8 - 7.34)^2 = 19.89\\)\n\n\ndog\n\\(8.2\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((8.2 - 8.13)^2 = 0.00\\)\n\\((8.2 - 7.34)^2 = 0.74\\)\n\n\ndog\n\\(5.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((5.6 - 8.13)^2 = 6.40\\)\n\\((5.6 - 7.34)^2 = 3.03\\)\n\n\ndog\n\\(9.1\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((9.1 - 8.13)^2 = 0.94\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\ndog\n\\(7.6\\)\n\\(8.13\\)\n\\((8.13 - 7.34)^2 = 0.62\\)\n\\((7.6 - 8.13)^2 = 0.28\\)\n\\((7.6 - 7.34)^2 = 0.07\\)\n\n\ncat\n\\(3.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((3.2 - 4.74)^2 = 2.37\\)\n\\((3.2 - 7.34)^2 = 17.14\\)\n\n\ncat\n\\(2.2\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((2.2 - 4.74)^2 = 6.45\\)\n\\((2.2 - 7.34)^2 = 26.42\\)\n\n\ncat\n\\(5.4\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((5.4 - 4.74)^2 = 0.44\\)\n\\((5.4 - 7.34)^2 = 3.76\\)\n\n\ncat\n\\(4.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.1 - 4.74)^2 = 0.41\\)\n\\((4.1 - 7.34)^2 = 10.50\\)\n\n\ncat\n\\(4.3\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((4.3 - 4.74)^2 = 0.19\\)\n\\((4.3 - 7.34)^2 = 9.24\\)\n\n\ncat\n\\(7.9\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((7.9 - 4.74)^2 = 9.99\\)\n\\((7.9 - 7.34)^2 = 0.31\\)\n\n\ncat\n\\(6.1\\)\n\\(4.74\\)\n\\((4.74 - 7.34)^2 = 6.76\\)\n\\((6.1 - 4.74)^2 = 1.85\\)\n\\((6.1 - 7.34)^2 = 1.54\\)\n\n\nfox\n\\(7.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((7.7 - 9.16)^2 = 2.13\\)\n\\((7.7 - 7.34)^2 = 0.13\\)\n\n\nfox\n\\(8.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.1 - 9.16)^2 = 1.12\\)\n\\((8.1 - 7.34)^2 = 0.58\\)\n\n\nfox\n\\(9.1\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.1 - 9.16)^2 = 0.00\\)\n\\((9.1 - 7.34)^2 = 3.10\\)\n\n\nfox\n\\(9.7\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((9.7 - 9.16)^2 = 0.29\\)\n\\((9.7 - 7.34)^2 = 5.57\\)\n\n\nfox\n\\(10.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.6 - 9.16)^2 = 2.07\\)\n\\((10.6 - 7.34)^2 = 10.63\\)\n\n\nfox\n\\(8.6\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((8.6 - 9.16)^2 = 0.31\\)\n\\((8.6 - 7.34)^2 = 1.59\\)\n\n\nfox\n\\(10.3\\)\n\\(9.16\\)\n\\((9.16 - 7.34)^2 = 3.31\\)\n\\((10.3 - 9.16)^2 = 1.30\\)\n\\((10.3 - 7.34)^2 = 8.76\\)\n\n\n\n\n\n\\(74.68\\)\n\\(56.53\\)\n\\(131.21\\)\n\n\n\n\n\nDie ANOVA wird deshalb auch Varianzzerlegung genannt, da die ANOVA versucht den Abstand der Beobachtungen auf die Variablen im Modell zu zerlegen. Also wie viel der Streuung von den Beobachtungen kann von dem Faktor animal erklärt werden? Genau der Abstand von den Gruppenmitteln zu dem globalen Mittelwert.\nDu kannst dir das ungefähr als eine Reise von globalen Mittelwert zu der einzelnen Beobachtung vorstellen. Nehmen wir als Beispiel die kleinste Sprungweite eines Katzenflohs von 2.2 cm und visualisieren wir uns die Reise wie in Abbildung 23.4 zu sehen. Wie kommen wir jetzt numerisch vom globalen Mittel mit \\(7.34\\) zu der Beobachtung? Wir können zum einen den direkten Abstand mit \\(2.2 - 7.34\\) gleich \\(-5.14\\) cm berechnen. Das wäre der total Abstand. Wie sieht es nun aus, wenn wir das Gruppenmittel mit beachten? In dem Fall gehen wir vom globalen Mittel zum Gruppenmittel cat mit \\(\\bar{y}_{cat} - \\bar{y}_{..} = 4.74 -7.34\\) gleich \\(\\beta_{cat} = -2.6\\) cm. Jetzt sind wir aber noch nicht bei der Beobachtung. Wir haben noch einen Rest von \\(y_{cat,2} - \\bar{y}_{cat} = 2.2 - 4.74\\) gleich \\(\\epsilon_{cat, 2} = -2.54\\) cm, die wir noch zurücklegen müssen. Das heißt, wir können einen Teil der Strecke mit dem Gruppenmittelwert erklären. Oder anders herum, wir können die Strecke vom globalen Mittelwert zu der Beobachtung in einen Teil für das Gruppenmittel und einen unerklärten Rest zerlegen.\n\n\n\nAbbildung 23.4— Visualisierung der Varianzzerlegung des Weges vom globalen Mittel zu der einzelnen Beoabchtung. Um zu einer einzelnen Beobachtung zu kommen legen wir den Weg vom globalen Mittelwert über den Abstand vom globalen Mittel zum Gruppenmittel \\(\\beta\\) zurück. Dann fehlt noch der Rest oder Fehler oder Residuum \\(\\epsilon\\).\n\n\nWir rechnen also eine ganze Menge an Abständen und quadrieren dann diese Abstände zu den Sum of Squares. Oder eben der Varianz. Dann fragen wir uns, ob der Faktor in unserem Modell einen Teil der Abstände erklären kann. Wir bauen uns dafür eine ANOVA Tabelle. Tabelle 23.6 zeigt eine theoretische, einfaktorielle ANOVA Tabelle. Wir berechnen zuerst die Abstände als \\(SS\\). Nun ist es aber so, dass wenn wir in einer Gruppe viele Level und/oder Beobachtungen haben, wir auch größere Sum of Squares bekommen. Wir müssen also die Sum of Squares in mittlere Abweichungsquadrate (eng. mean squares) mitteln. Abschließend können wir die F Statistik berechnen, indem wir die \\(MS\\) des Faktors durch die \\(MS\\) des Fehlers teilen. Das Verhältnis von erklärter Varianz vom Faktor zu dem unerklärten Rest.\n\n\n\nTabelle 23.6— Einfaktorielle ANOVA in der theoretischen Darstellung. Die sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(k-1\\)\n\\(SS_{animal} = \\sum_{i=1}^{k}n_i(\\bar{y}_{i.} - \\bar{y}_{..})^2\\)\n\\(MS_{animal} = \\cfrac{SS_{animal}}{k-1}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nerror\n\\(n-k\\)\n\\(SS_{error} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{i.})^2\\)\n\\(MS_{error} = \\cfrac{SS_{error}}{N-k}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total} = \\sum_{i=1}^{k}\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{..})^2\\)\n\n\n\n\n\n\n\nWir füllen jetzt die Tabelle 23.7 einmal mit den Werten aus. Nachdem wir das getan haben oder aber die Tabelle in R ausgegeben bekommen haben, können wir die Zahlen interpretieren.\n\n\n\nTabelle 23.7— Einfaktorielle ANOVA mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dann direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 74.68\\)\n\\(MS_{animal} = \\cfrac{74.68}{3-1} = 37.34\\)\n\\(F_{calc} = \\cfrac{37.34}{3.14} = 11.89\\)\n\n\nerror\n\\(21-3\\)\n\\(SS_{error} = 56.53\\)\n\\(MS_{error} = \\cfrac{56.53}{18} = 3.14\\)\n\n\n\ntotal\n\\(21-1\\)\n\\(SS_{total} = 131.21\\)\n\n\n\n\n\n\n\nZu erst ist die berechnete F Statistik \\(F_{calc}\\) von Interesse. Wir haben hier eine \\(F_{calc}\\) von 11.89. Wir vergleichen wieder die berechnete F Statistik mit einem kritischen Wert. Der kritische F Wert \\(F_{\\alpha = 5\\%}\\) lautet für die einfaktorielle ANOVA in diesem konkreten Beispiel mit \\(F_{\\alpha = 5\\%} = 3.55\\). Die Entscheidungsregel nach der F Teststatistik lautet, die \\(H_0\\) abzulehnen, wenn \\(F_{calc} &gt; F_{\\alpha = 5\\%}\\).\nWir können also die Nullhypothese \\(H_0\\) in unserem Beispiel ablehnen. Es liegt ein signifikanter Unterschied zwischen den Tiergruppen vor. Mindestens ein Mittelwertsunterschied in den Sprungweiten liegt vor.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(F_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(F_{calc}\\) gilt, wenn \\(F_{calc} \\geq F_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr. Wir nutzen in der praktischen Anwendung den \\(p\\)-Wert.\n\n\nAbschließend noch ein Wort zu der Annahme an eine ANOVA. Wir wollen ja für eine ANOVA ein normalverteiltes Outcome \\(y\\) und die gleichen Varianzen über alle Gruppen. Wir wollen Varianzhomogenität vorliegen haben. In Abbildung 23.5 siehst du vier Behandlungsgruppen sowie deren lokale Mittel. Die Gruppe D hat eine sehr viel größere Varianz. Auch ist fraglich, ob die Gruppe D einer Normalverteilung folgt. Dadurch passieren zwei Dinge. Erstens ist der lokale Mittelwert von der Gruppe D viel näher am globalen Mittel als er eigentlich wäre, dadurch wird die Summe aus \\(MS_{treatment}\\) kleiner. Zweitens sind die Beobachtungen sehr weit um das lokale Mittel gestreut, dadurch wird die Summe aus \\(MS_{error}\\) viel größer. Wir erhalten dadurch eine sehr viel kleinere F Statistik. Im Prinzip vergleichst du mit einer ANOVA ein Set an gleich aussehenden Normalverteilungen. Wenn alles schief und krumm ist, dann ist es vermutlich sogar besser, als wenn wir eine total verkorkste Gruppenverteilung haben, wie hier in Beispiel D.\nIn Kapitel 17 kannst du nochmal über die \\(\\log\\)-Transformation vom Outcome \\(y\\) nachlesen. Nach der \\(\\log\\)-Transformation können Daten dann approximativ normalverteilt sein.\n\n\nAbbildung 23.5— Visualisierung der Annahmen an eine ANOVA. Wir berechnen den \\(MS_{treatment}\\) in dem wir alle Abweichungen vom gloablen Mittel \\(\\bar{y}_{..}\\) aufaddieren. Ebenso berechnen wir den \\(MS_{error}\\) indem wir die einzelen Abweichungen von den Bobachtungen \\(\\epsilon\\) zu den lokalen Mitteln berechnen. Hat jetzt eine Gruppe, wie die Gruppe D, eine abweichende Streuung, wird das \\(MS_{error}\\) überproportional groß. Damit wird dann die F Statistik klein.\n\nWir können das einmal numerisch mit einem kritischen Wert für \\(F_{\\alpha = 5\\%} = 3.55\\) durchspielen. Nehmen wir folgende Summe für \\(MS_{treatment}\\) aus der Abbildung 23.5 mit den einzelnen \\(MS\\) einmal wie folgt an. Wir nehmen an das \\(MS_A\\) gleich 10, \\(MS_B\\) gleich 20, \\(MS_C\\) gleich 8 und \\(MS_D\\) gleich 7 ist. Dann können wir durch das Summieren das \\(MS_{treatment}\\) berechnen.\n\\[\nMS_{treatment} = 10 + 20 + 8 + 7 = 45\n\\]\nWenn wir jetzt die Summe für \\(MS_{error}\\) berechnen, wird diese Summe sehr große, da die Gruppe D eine sehr große Varianz mit einbringt.\n\\[\nMS_{error} = 5 + 6 + 4 + 20 = 35\n\\]\nDas führt am Ende dazu, dass wir eine sehr kleine F Statistik erhalten. Auch wenn sich vielleicht Gruppe B von Gruppe C unterscheidet, können wir den Unterschied wegen der großen Varianz aus Gruppe D nicht nachweisen.\n\\[\nF = \\cfrac{MS_{treatment}}{MS_{error}} = \\cfrac{45}{35} = 1.28 \\leq 3.55\n\\]\nDu siehst hier wie wichtig es ist, ich die Daten einmal zu visualisieren um zu sehen vorher mögliche Probleme herrühren können. Auf der anderen Seite können wir die ANOVA auf ein Recht breites Spektrum an Daten anwenden, solange wir wissen, das die Verteilungen in etwa für jede Gruppe gleich aussehen. Wir müssen nur mit dem \\(\\eta^2\\) aufpassen, wenn wir zu schiefe Verteilungen haben. Dann gibt uns \\(\\eta^2\\) keine valide Aussage mehr über den Anteil der erklärten Varianz.\n\n23.2.4 Einfaktoriellen ANOVA in R\nUm eine ANOVA zu rechnen nutzen wir zuerst die Funktion lm(), warum das so ist kannst du im Kapitel 32 nachlesen. Du brauchst das Wissen aber hier nicht unbedingt.\nWir rechnen keine ANOVA per Hand sondern nutzen R. Dazu müssen wir als erstes das Modell definieren. Das ist im Falle der einfaktoriellen ANOVA relativ einfach. Wir haben unseren Datensatz fac1_tbl mit einer kontinuierlichen Variable jump_lemgth als \\(y\\) vorliegen sowie einen Faktor animal mit mehr als zwei Leveln als \\(x\\). Wir definieren das Modell in R in der Form jump_length ~ animal. Um das Modell zu rechnen nutzen wir die Funktion lm() - die Abkürzung für linear model. Danach pipen wir die Ausgabe vom lm() direkt in die Funktion anova(). Die Funktion anova() berechnet uns dann die eigentliche einfaktorielle ANOVA. Wir speichern die Ausgabe der ANOVA in fit_1. Schauen wir uns die ANOVA Ausgabe einmal an.\n\nfit_1 &lt;-  lm(jump_length ~ animal, data = fac1_tbl) %&gt;% \n  anova\n\nfit_1\n\nAnalysis of Variance Table\n\nResponse: jump_length\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nanimal     2 74.683  37.341   11.89 0.0005113 ***\nResiduals 18 56.529   3.140                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten die Information was wir gerechnet haben, eine Varianzanalyse. Darunter steht, was das \\(y\\) war nämlich die jump_length. Wir erhalten eine Zeile für den Faktor animal und damit die \\(SS_{animal}\\) und eine Zeile für den Fehler und damit den \\(SS_{error}\\). In R heißen die \\(SS_{error}\\) dann Residuals. Die Zeile für die \\(SS_{total}\\) fehlt.\nNeben der berechneten F Statistik \\(F_{calc}\\) von \\(11.89\\) erhalten wir auch den p-Wert mit \\(0.005\\). Wir ignorieren die F Statistik, da wir in der Anwendung nur den p-Wert berücksichtigen. Die Entscheidung gegen die Nulhypothese lautet, dass wenn der p-Wert kleiner ist als das Signifkanzniveau \\(\\alpha\\) von 5% wir die Nullhypothese ablehnen.\nWir haben hier ein signifikantes Ergebnis vorliegen. Mindestens ein Gruppenmittelerstunterschied ist signifikant. Abbildung 23.6 zeigt nochmal die Daten fac1_tbl als Boxplot. Wir überprüfen visuell, ob das Ergebnis der ANOVA stimmen kann. Ja, die Boxplots und das Ergebnis der ANOVA stimmen überein. Die Boxplots liegen nicht alle auf einer Ebene, so dass hier auch ein signifikanter Unterschied zu erwarten war.\n\n\n\n\nAbbildung 23.6— Boxplot der Sprungweiten [cm] von Hunden-, Katzen- und Fuchsflöhen.\n\n\n\nAbschließend können wir noch die Funktion eta_squared() aus dem R Paket effectsize nutzen um einen Effektschätzer für die einfaktorielle ANOVA zu berechnen. Wir können mit \\(\\eta^2\\) abschätzen, welchen Anteil der Faktor animal an der gesamten Varianz erklärt.\n\nfit_1 %&gt;% eta_squared\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nanimal    | 0.57 | [0.27, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nDas \\(\\eta^2\\) können wir auch einfach händisch berechnen.\n\\[\n\\eta^2 = \\cfrac{SS_{animal}}{SS_{total}} = \\cfrac{74.68}{131.21} = 0.57 = 57\\%\n\\]\nWir haben nun die Information, das 57% der Varianz der Beobachtungen durch den Faktor animal rklärt wird. Je nach Anwendungsgebiet kann die Relevanz sehr stark variieren. Im Bereich der Züchtung mögen erklärte Varianzen von unter 10% noch sehr relevant sein. Im Bereich des Feldexperiments erwarten wir schon höhere Werte für \\(\\eta^2\\). Immerhin sollte ja unsere Behandlung maßgeblich für die z.B. größeren oder kleineren Pflanzen gesorgt haben."
  },
  {
    "objectID": "stat-tests-anova.html#sec-fac2",
    "href": "stat-tests-anova.html#sec-fac2",
    "title": "23  Die ANOVA",
    "section": "\n23.3 Zweifaktorielle ANOVA",
    "text": "23.3 Zweifaktorielle ANOVA\n\n\n\n\n\n\nEinführung in die zweifaktorielle ANOVA per Video\n\n\n\nDu findest auf YouTube die zweifaktorielle ANOVA als Video. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\n\n\n\n\n\n\nDie zweifaktorielle ANOVA verlangt ein normalverteiltes \\(y\\) sowie Varianzhomogenität jeweils separat über beide Behandlungsfaktor \\(x_1\\) und \\(x_2\\). Daher alle Level von \\(x_1\\) sollen die gleiche Varianz haben. Ebenso sollen alle Level von \\(x_2\\) die gleiche Varianz haben.\nUnsere Annahme an die Daten \\(D\\) ist, dass das dein \\(y\\) normalverteilt ist und das die Level vom \\(x_1\\) und \\(x_2\\) jeweils für sich homogen in den Varianzen sind. Später mehr dazu, wenn wir beides nicht vorliegen haben…\nDie zweifaktorielle ANOVA ist eine wunderbare Methode um herauszufinden, ob zwei Faktoren einen Einfluss auf ein normalverteiltes \\(y\\) haben. Die Stärke der zweifaktoriellen ANOVA ist hierbei, dass die ANOVA beide Effekte der Faktoren auf das \\(y\\) simultan modelliert. Darüber hinaus können wir auch noch einen Interaktionsterm mit in das Modell aufnehmen um zu schauen, ob die beiden Faktoren untereinander auch interagieren. Somit haben wir mit der zweifaktoriellen ANOVA die Auswertungsmehode für ein randomiziertes Blockdesign vorliegen.\n\n23.3.1 Daten für die zweifaktorielle ANOVA\nWir wollen uns nun einen etwas komplexes Modell anschauen mit einem etwas komplizierteren Datensatz flea_dog_cat_fox_site.csv. Wir brauchen hierfür ein normalverteiltes \\(y\\) und sowie zwei Faktoren. Das macht auch soweit Sinn, denn wir wollen ja auch eine zweifaktorielle ANOVA rechnen.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal sowie die Spalte site als \\(x\\). Danach müssen wir noch die Variable animal sowie die Variable site in einen Faktor mit der Funktion as_factor() umwandeln.\n\nfac2_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox_site.csv\") %&gt;% \n  select(animal, site, jump_length) %&gt;% \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 23.8 nochmal dargestellt.\n\n\n\n\nTabelle 23.8— Selektierter Datensatz für die zweifaktorielle ANOVA mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\nanimal\nsite\njump_length\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.10\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\ncat\ncity\n14.91\n\n\n\n\n\n\nDie Beispieldaten sind in Abbildung 23.7 abgebildet. Wir sehen auf der x-Achse den Faktor animal mit den drei Leveln dog, cat und fox. Jeder dieser Faktorlevel hat nochmal einen Faktor in sich. Dieser Faktor lautet site und stellt dar, wo die Flöhe gesammelt wurden. Die vier Level des Faktors site sind city, smalltown, village und field.\n\n\n\n\nAbbildung 23.7— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\nWir bauen dann mit den beiden Variablen bzw. Faktoren animal und site aus dem Objekt fac2_tbl folgendes Modell für die zweifaktorielle ANOVA:\n\\[\njump\\_length \\sim animal + site\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wir immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in der zweifaktoriellen ANOVA aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese.\n\n23.3.2 Hypothesen für die zweifaktorielle ANOVA\nWir haben für jeden Faktor der zweifaktoriellen ANOVA ein Hypothesenpaar. Im Folgenden sehen wir die jeweiligen Hypothesenpaare. Einmal für animal, als Haupteffekt. Wir nennen einen Faktor den Hauptfaktor, weil wir an diesem Faktor am meisten interessiert sind. Wenn wir später einen Posthoc Test durchführen würden, dann würden wir diesen Faktor nehmen. Wir sind primär an dem Unterschied der Sprungweiten in [cm] in Gruppen Hund, Katze und Fuchs interessiert.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für site, als Nebeneffekt oder Blockeffekt oder Clustereffekt. Meist eine Variable, die wir auch erhoben haben und vermutlich auch einen Effekt auf das \\(y\\) haben wird. Oder aber wir haben durch das exprimentelle Design noch eine Aufteilungsvariable wie Block vorliegen. In unserem Beispiel ist es site oder der Ort, wo wir die Hunde-, Katzen, und Fuchsflöhe gefunden haben.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{city} = \\bar{y}_{smalltown} = \\bar{y}_{village} = \\bar{y}_{field}\\\\\nH_A: &\\; \\bar{y}_{city} \\ne \\bar{y}_{smalltown}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{city} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{village}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{smalltown} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{village} \\ne \\bar{y}_{field}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nEinmal für die Interaktion animal:site - die eigentliche Stärke der zweifaktoriellen ANOVA. Wir können uns anschauen, ob die beiden Faktoren miteinander interagieren. Das heißt, ob eine Interaktion zwischen dem Faktor animal und dem Faktor site vorliegt.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{keine Interaktion}\\\\\nH_A: &\\; \\mbox{eine Interaktion zwischen animal und site}\n\\end{aligned}\n\\]\nWir haben also jetzt die verschiedenen Hypothesenpaare definiert und schauen uns jetzt die ANOVA in R einmal in der Anwendung an.\n\n23.3.3 Zweifaktoriellen ANOVA in R\nBei der einfaktoriellen ANOVA haben wir die Berechnungen der Sum of squares nochmal nachvollzogen. Im Falle der zweifaktoriellen ANOVA verzichten wir darauf. Das Prinzip ist das gleiche. Wir haben nur mehr Mitelwerte und mehr Abweichungen von diesen Mittelwerten, da wir ja nicht nur einen Faktor animal vorliegen haben sondern auch noch den Faktor site. Da wir aber die ANOVA nur Anwenden und dazu R nutzen, müssen wir jetzt nicht per Hand die zweifaktorielle ANOVA rechnen. Du musst aber die R Ausgabe der ANOVA verstehen. Und diese Ausgabe schauen wir uns jetzt einmal ohne und dann mit Interaktionsterm an.\n\n23.3.3.1 Ohne Interaktionsterm\nWir wollen nun einmal die zweifaktorielle ANOVA ohne Interaktionsterm rechnen die in Tabelle 23.9 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen die Funktion anova() in R.\n\n\n\nTabelle 23.9— Zweifaktorielle ANOVA ohne Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nerror\n\\(n-(a-1)(b-1)\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 32 erfahren.\nIm Folgenden sehen wir nochmal das Modell ohne Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel.\n\\[\njump\\_length \\sim animal + site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm().\n\nfit_2 &lt;-  lm(jump_length ~ animal + site, data = fac2_tbl) %&gt;% \n  anova\n\nfit_2\n\nAnalysis of Variance Table\n\nResponse: jump_length\n           Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nanimal      2 180.03  90.017 19.8808 3.92e-08 ***\nsite        3   9.13   3.042  0.6718    0.571    \nResiduals 114 516.17   4.528                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir erhalten wiederum die ANOVA Ergebnistabelle. Anstatt nur die Zeile animal für den Effekt des Faktors animal sehen wir jetzt auch noch die Zeile site für den Effekt des Faktors site. Zuerst ist weiterhin der Faktor animal signifikant, da der \\(p\\)-Wert mit \\(0.000000039196\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können von mindestens einem Gurppenunterschied im Faktor animal ausgehen. Im Weiteren ist der Faktor site nicht signifikant. Es scheint keinen Unterschied zwischend den einzelnen Orten und der Sprunglänge von den Hunde-, Katzen- und Fuchsflöhen zu geben.\nNeben der Standausgabe von R können wir auch die tidy Variante uns ausgeben lassen. In dem Fall sieht die Ausgabe etwas mehr aufgeräumt aus.\n\nfit_2 %&gt;% tidy\n\n# A tibble: 3 × 6\n  term         df  sumsq meansq statistic       p.value\n  &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 animal        2 180.    90.0     19.9    0.0000000392\n2 site          3   9.13   3.04     0.672  0.571       \n3 Residuals   114 516.     4.53    NA     NA           \n\n\nAbschließend können wir uns übr \\(\\eta^2\\) auch die erklärten Anteile der Varianz wiedergeben lassen.\n\nfit_2 %&gt;% eta_squared\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\nanimal    |           0.26 | [0.15, 1.00]\nsite      |           0.02 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 26%. Für den Faktor site haben wir nur einen Anteil von 2% der erklärten Varianz. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 23.10 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\nTabelle 23.10— Zweifaktorielle Anova ohne Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dann direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{4.53} = 19.88\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{4.53} = 0.67\\)\n\n\nerror\n\\(120-(3-1)(4-1)\\)\n\\(SS_{error} = 516.17\\)\n\\(MS_{error} = 4.53\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.33\\)\n\n\n\n\n\n\n\n\n23.3.3.2 Mit Interaktionssterm\nWir wollen nun noch einmal die zweifaktorielle ANOVA mit Interaktionsterm rechnen, die in Tabelle 23.11 dargestellt ist. Die \\(SS\\) und \\(MS\\) für die zweifaktorielle ANOVA berechnen wir nicht selber sondern nutzen wie immer die Funktion anova() in R.\n\n\n\nTabelle 23.11— Zweifaktorielle ANOVA mit Interaktionseffekt in der theoretischen Darstellung. Die Sum of squares müssen noch zu den Mean squares gemittelt werden. Abschließend wird die F Statistik als Prüfgröße berechnet.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(a-1\\)\n\\(SS_{animal}\\)\n\\(MS_{animal}\\)\n\\(F_{calc} = \\cfrac{MS_{animal}}{MS_{error}}\\)\n\n\nsite\n\\(b-1\\)\n\\(SS_{site}\\)\n\\(MS_{site}\\)\n\\(F_{calc} = \\cfrac{MS_{site}}{MS_{error}}\\)\n\n\nanimal \\(\\times\\) site\n\\((a-1)(b-1)\\)\n\\(SS_{animal \\times site}\\)\n\\(MS_{animal \\times site}\\)\n\\(F_{calc} = \\cfrac{MS_{animal \\times site}}{MS_{error}}\\)\n\n\nerror\n\\(n-ab\\)\n\\(SS_{error}\\)\n\\(MS_{error}\\)\n\n\n\ntotal\n\\(n-1\\)\n\\(SS_{total}\\)\n\n\n\n\n\n\n\nIm Folgenden sehen wir nochmal das Modell mit Interaktionsterm. Wir nutzen die Schreibweise in R für eine Modellformel. Einen Interaktionsterm bilden wir durch das : in R ab. Wir können theoretisch auch noch weitere Interaktionsterme bilden, also auch x:y:z. Ich würde aber davon abraten, da diese Interaktionsterme schwer zu interpretieren sind.\n\\[\njump\\_length \\sim animal + site + animal:site\n\\]\nWir bauen nun mit der obigen Formel ein lineares Modell mit der Funktion lm() in R. Es wieder das gleich wie schon zuvor. Danach pipen wir das Modell in die Funktion anova() wie auch in der einfaktoriellen Variante der ANOVA. Die Funktion bleibt die Gleiche, was sich ändert ist das Modell in der Funktion lm(). Auch die Interaktion müssen wir nicht extra in der ANOVA Funktion angeben. Alles wird im Modell des lm() abgebildet.\nDie visuelle Regel zur Überprüfung der Interaktion lautet nun wie folgt. Abbildung 23.8 zeigt die entsprechende Vislualisierung. Wir haben keine Interaktion vorliegen, wenn die Geraden parallel zueinander laufen und die Abstände bei bei jedem Faktorlevel gleich sind. Wir schauen uns im Prinzip die erste Faktorstufe auf der x-Achse an. Wir sehen den Abstand von der roten zu blauen Linie sowie das die blaue Gerade über der roten Gerade liegt. Dieses Muster erwarten wir jetzt auch an dem Faktorlevel B und C. Eine leichte bis mittlere Interaktion liegt vor, wenn sich die Abstände von dem zweiten Faktor über die Faktorstufen des ersten Faktors ändern. Eine starke Interaktion liegt vor, wenn sich die Geraden schneiden.\n\n\n\n\n\n(a) Keine Interaktion\n\n\n\n\n\n(b) Leichte bis mittlere Intraktion\n\n\n\n\n\n(c) Starke Interaktion\n\n\n\nAbbildung 23.8— Darstellung von keiner Interaktion, leichter bis mittler Interaktion und starker Interaktion in einer zweifaktoriellen ANOVA mit einem Faktor mit drei Leveln A, B und C sowie einem Faktor mit zwei Leveln (rot und blau).\n\n\nIn der Abbildung 23.9 sehen wir den Interaktionsplot für unser Beispiel. Auf der y-Achse ist die Sprunglänge abgebildet und auf der x-Achse der Faktor animal. Die einzelnen Farben stellen die Level des Faktor site dar.\n\nggplot(fac2_tbl, aes(x = animal, y = jump_length,\n                     color = site, group = site)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_bw() +\n  scale_color_okabeito()\n\n\n\nAbbildung 23.9— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\nWenn sich die Geraden in einem Interaktionsplot schneiden, haben wir eine Interaktion zwischen den beiden Faktoren vorliegen\nWir schauen zur visuellen Überprüfung auf den Faktor animal und das erste level cat. Wir sehen die Ordnung des zweiten Faktors site mit field, village, smalltown und city. Diese Ordnung und die Abstände sind bei zweiten Faktorlevel dog schon nicht mehr gegeben. Die Geraden schneiden sich. Auch liegt bei dem Level fox eine andere ordnung vor. Daher sehen wir hier eine starke Interaktion zwischen den beiden Faktoren animal und site.\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 32 erfahren.\nWir nehmen jetzt auf jeden Fall den Interaktionsterm animal:site mit in unser Modell und schauen uns einmal das Ergebnis der ANOVA an. Das lineare Modell der ANOVA wird erneut über die Funktion lm() berechnet und anschließend in die Funktion anova() gepipt.\n\nfit_3 &lt;-  lm(jump_length ~ animal + site + animal:site, data = fac2_tbl) %&gt;% \n  anova\n\nfit_3\n\nAnalysis of Variance Table\n\nResponse: jump_length\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nanimal        2 180.03  90.017 30.2807 3.63e-11 ***\nsite          3   9.13   3.042  1.0233   0.3854    \nanimal:site   6 195.11  32.519 10.9391 1.71e-09 ***\nResiduals   108 321.05   2.973                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn wir eine signifikante Interaktion vorliegen haben, dann müssen wir den Faktor A getrennt für jedes Levels des Faktors B auswerten.\nDie Ergebnistabelle der ANOVA wiederholt sich. Wir sehen, dass der Faktor animal signifikant ist, da der p-Wert mit \\(0.000000000036\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese ablehnen. Mindestens ein Mittelwertsvergleich unterschiedet sich zwischen den Levels des Faktors animal. Im Weiteren sehen wir, dass der Faktor site nicht signifkant ist, da der p-Wert mit \\(0.39\\) größer ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können daher die Nullhypothese nicht ablehnen. Abschließend finden wir die Interaktion zwischen dem Faktor animalund site las signifikant vor. Wenn wir eine signifikante Interaktion vorliegen haben, dann müssen wir den Faktor animal getrennt für jedes Levels des Faktors site auswerten. Wir können keine Aussage über die Sprungweite von Hunde-, Katzen- und Fuchsflöhen unabhängig von der Herkunft site der Flöhe machen.\nIn ?sec-app-example-anova-inter findest du ein Beispiel für eine signifikante Interaktion und die folgende Auswertung\nWir können wie immer die etwas aufgeräumte Variante der ANOVA Ausgabe mit der Funktion tidy() uns ausgeben lassen.\n\nfit_3 %&gt;% tidy()\n\n# A tibble: 4 × 6\n  term           df  sumsq meansq statistic   p.value\n  &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 animal          2 180.    90.0      30.3   3.63e-11\n2 site            3   9.13   3.04      1.02  3.85e- 1\n3 animal:site     6 195.    32.5      10.9   1.71e- 9\n4 Residuals     108 321.     2.97     NA    NA       \n\n\nIm Folgenden können wir noch die \\(\\eta^2\\) für die ANOVA als Effektschätzer berechnen lassen.\n\nfit_3 %&gt;% eta_squared\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Eta2 (partial) |       95% CI\n-------------------------------------------\nanimal      |           0.36 | [0.24, 1.00]\nsite        |           0.03 | [0.00, 1.00]\nanimal:site |           0.38 | [0.24, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass nur ein kleiner Teil der Varianz von dem Faktor animal erklärt wird, nämlich 36%. Für den Faktor site haben wir nur einen Anteil von 3% der erklärten Varianz. Die Interaktion zwischen animal und site erklärt 38% der beobachteten Varianz udn ist somit auch vom Effekt her nicht zu ignorieren. Somit hat die site weder einen signifikanten Einflluss auf die Sprungweite von Flöhen noch ist dieser Einfluss als relevant zu betrachten.\nAbschließend können wir die Werte in der Tabelle 23.12 ergänzen. Die Frage ist inwieweit diese Tabelle in der Form von Interesse ist. Meist wird geschaut, ob die Faktoren signifikant sind oder nicht. Abschließend eventuell noch die \\(\\eta^2\\) Werte berichtet. Hier musst du schauen, was in deinem Kontext der Forschung oder Abschlussarbeit erwartet wird.\n\n\n\nTabelle 23.12— Zweifaktorielle Anova mit Interaktionseffekt mit den ausgefüllten Werten. Die \\(SS_{total}\\) sind die Summe der \\(SS_{animal}\\) und \\(SS_{error}\\). Die \\(MS\\) berechnen sich dan direkt aus den \\(SS\\) und den Freiheitsgraden (\\(df\\)). Abschließend ergibt sich dann die F Statistik.\n\n\n\n\n\n\n\n\nVarianzquelle\ndf\nSum of squares\nMean squares\nF\\(_{\\boldsymbol{calc}}\\)\n\n\n\n\nanimal\n\\(3-1\\)\n\\(SS_{animal} = 180.03\\)\n\\(MS_{animal} = 90.02\\)\n\\(F_{calc} = \\cfrac{90.02}{2.97} = 30.28\\)\n\n\nsite\n\\(4-1\\)\n\\(SS_{site} = 9.13\\)\n\\(MS_{site} = 3.04\\)\n\\(F_{calc} = \\cfrac{3.04}{2.97} = 1.02\\)\n\n\nanimal \\(\\times\\) site\n\\((3-1)(4-1)\\)\n\\(SS_{animal \\times site} = 195.12\\)\n\\(MS_{animal \\times site} = 32.52\\)\n\\(F_{calc} = \\cfrac{32.52}{2.97} = 10.94\\)\n\n\nerror\n\\(120 - (3 \\cdot 4)\\)\n\\(SS_{error} = 321.06\\)\n\\(MS_{error} = 2.97\\)\n\n\n\ntotal\n\\(120-1\\)\n\\(SS_{total} = 705.34\\)"
  },
  {
    "objectID": "stat-tests-anova.html#und-weiter",
    "href": "stat-tests-anova.html#und-weiter",
    "title": "23  Die ANOVA",
    "section": "\n23.4 Und weiter?",
    "text": "23.4 Und weiter?\nNach einer berechneten ANOVA können wir zwei Fälle vorliegen haben.\nWenn du in deinem Experiment keine signifikanten Ergebnisse findest, ist das nicht schlimm. Du kannst deine Daten immer noch mit der explorativen Datenanalyse auswerten wie in Kapitel 16 beschrieben.\n\nWir habe eine nicht signifikante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) nicht ablehnen und die Mittelwerte über den Faktor sind vermutlich alle gleich. Wir enden hier mit unserer statistischen Analyse.\nWir haben eine signifikante ANOVA berechnet. Wir können die Nullhypothese \\(H_0\\) ablehnen und mindestens ein Gruppenvergleich über mindestens einen Faktor ist vermutlich unterschiedlich. Wir können dann in Kapitel 31 eine Posthoc Analyse rechnen."
  },
  {
    "objectID": "stat-tests-ancova.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-ancova.html#genutzte-r-pakete-für-das-kapitel",
    "title": "24  Die ANCOVA",
    "section": "\n24.1 Genutzte R Pakete für das Kapitel",
    "text": "24.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-ancova.html#daten",
    "href": "stat-tests-ancova.html#daten",
    "title": "24  Die ANCOVA",
    "section": "\n24.2 Daten",
    "text": "24.2 Daten\nFür unser Beispiel nutzen wir die Daten der Sprungweite in [cm] von Flöhen auf Hunde-, Katzen- und Füchsen. Damit haben wir den ersten Faktor animal mit drei Leveln. Als Kovariate schauen wir uns das Gewicht als numerische Variable an. Schlussendlich brauchen wir noch das Outcome jump_length als \\(y\\). Für die zweifaktorielle ANCOVA nehmen wir noch den Faktor sex mit zwei Leveln hinzu.\n\nancova_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %&gt;%\n  select(animal, sex, jump_length, weight) %&gt;% \n  mutate(animal = as_factor(animal))\n\nIn der Tabelle 33.1 ist der Datensatz ancova_tbl nochmal dargestellt.\n\n\n\n\nTabelle 24.1— Datensatz zu der Sprunglänge in [cm] von Flöhen auf Hunde-, Katzen- und Füchsen.\n\nanimal\nsex\njump_length\nweight\n\n\n\ncat\nmale\n15.79\n6.02\n\n\ncat\nmale\n18.33\n5.99\n\n\ncat\nmale\n17.58\n8.05\n\n\ncat\nmale\n14.09\n6.71\n\n\ncat\nmale\n18.22\n6.19\n\n\ncat\nmale\n13.49\n8.18\n\n\n…\n…\n…\n…\n\n\nfox\nfemale\n27.81\n8.04\n\n\nfox\nfemale\n24.02\n9.03\n\n\nfox\nfemale\n24.53\n7.42\n\n\nfox\nfemale\n24.35\n9.26\n\n\nfox\nfemale\n24.36\n8.85\n\n\nfox\nfemale\n22.13\n7.89"
  },
  {
    "objectID": "stat-tests-ancova.html#hypothesen-für-die-ancova",
    "href": "stat-tests-ancova.html#hypothesen-für-die-ancova",
    "title": "24  Die ANCOVA",
    "section": "\n24.3 Hypothesen für die ANCOVA",
    "text": "24.3 Hypothesen für die ANCOVA\nWir haben für jeden Faktor der ANCOVA ein Hypothesenpaar sowie ein Hypothesenpaar für die Kovariate. Im Folgenden sehen wir die jeweiligen Hypothesenpaare.\nEinmal für animal, als Haupteffekt. Wir nennen einen Faktor den Hauptfaktor, weil wir an diesem Faktor am meisten interessiert sind. Wenn wir später einen Posthoc Test durchführen würden, dann würden wir diesen Faktor nehmen. Wir sind primär an dem Unterschied der Sprungweiten in [cm] in Gruppen Hund, Katze und Fuchs interessiert.\n\\[\n\\begin{aligned}\nH_0: &\\; \\bar{y}_{cat} = \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_A: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nDu kannst mehr über Geraden sowei lineare Modelle und deren Eigenschaften im Kapitel 32 erfahren.\nFür die Kovariate testen wir anders. Die Kovariate ist ja eine numerische Variable. Daher ist die Frage, wann gibt es keinen Effekt von weight auf die Sprunglänge? Wenn wir eine parallele Linie hätten. Das heißt, wenn sich der Wert von weight ändert, ändert sich der Wert von jump_length nicht. Wir schreiben, dass sich die Steigung der Geraden nicht ändert. Wir bezeichnen die Steigung einer Graden mit \\(\\beta\\). Wenn kein Effekt vorliegt und die Nullhpyothese gilt, dann ist die Steigung der Geraden \\(\\beta_{weight} = 0\\).\n\\[\n\\begin{aligned}\nH_0: &\\; \\beta_{weight} = 0\\\\\nH_A: &\\; \\beta_{weight} \\neq 0\n\\end{aligned}\n\\]\nDu kannst dir überlegen, ob due die Interaktion zwischen dem Faktor und der Kovariate mit ins Modell nehmen willst. Eigentlich schauen wir uns immer nur die Interaktion zwischen den Faktoren an. Generell schreiben wir eine Interaktionshypothese immer in Prosa.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{keine Interaktion}\\\\\nH_A: &\\; \\mbox{eine Interaktion zwischen animal und site}\n\\end{aligned}\n\\]\nWir haben also jetzt die verschiedenen Hypothesenpaare definiert und schauen uns jetzt die ANCOVA in R einmal in der Anwendung an."
  },
  {
    "objectID": "stat-tests-ancova.html#die-einfaktorielle-ancova-in-r",
    "href": "stat-tests-ancova.html#die-einfaktorielle-ancova-in-r",
    "title": "24  Die ANCOVA",
    "section": "\n24.4 Die einfaktorielle ANCOVA in R",
    "text": "24.4 Die einfaktorielle ANCOVA in R\nDu kannst mehr über Geraden sowie lineare Modelle und deren Eigenschaften im Kapitel 32 erfahren.\nWir können die ANCOVA ganz klassisch mit dem linaren Modell fitten. Wir nutzen die Funktion lm() um die Koeffizienten des linearen Modellls zu erhalten. Wir erinnern uns, wir haben haben einen Faktor \\(f_1\\) und eine Kovariate bezwiehungsweise ein numerisches \\(c_1\\). In unserem Beispiel sieht dann der Fit des Modells wie folgt aus.\n\nfit_1 &lt;- lm(jump_length ~ animal + weight + animal:weight, data = ancova_tbl)\n\nNachdem wir das Modell in dem Objekt fit_1 gespeichert haben können wir dann das Modell in die Funktion anova() pipen. Die Funktion erkennt, das wir eine ANCOVA rechnen wollen, da wir in unserem Modell einen Faktor und eine Kovariate mit enthalten haben.\n\nfit_1 %&gt;% anova \n\nAnalysis of Variance Table\n\nResponse: jump_length\n               Df Sum Sq Mean Sq  F value Pr(&gt;F)    \nanimal          2 2693.8 1346.88 204.2764 &lt;2e-16 ***\nweight          1 1918.0 1917.99 290.8961 &lt;2e-16 ***\nanimal:weight   2    0.3    0.14   0.0214 0.9788    \nResiduals     594 3916.5    6.59                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn der ANCOVA erkennne wir nun, dass der Faktor animal signifikant ist. Der \\(p\\)-Wert ist mit \\(&lt;0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Ebenso ist die Kovariate weight signifikant. Der \\(p\\)-Wert ist ebenfalls mit \\(&lt;0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können also schlussfolgern, dass sich mindestens eine Gruppenvergleich der Level des Faktors animal voneinander unterscheidet. Wir wissen auch, dass mit der Zunahme des Gewichts, die Sprunglänge sich ändert.\nDie ANCOVA liefert keine Informationen zu der Größe oder der Richtung des Effekts der Kovariate.\nWas wir nicht wissen, ist die Richtung. Wir wissen nicht, ob mit ansteigenden Gewicht sich die Sprunglänge erhöht oder vermindert. Ebenso wenig wissen wir etwas über den Betrag des Effekts. Wieviel weiter springen denn nun Flöhe mit 1 mg Gewicht mehr? Wir haben aber die Möglichkeit, den Sachverhalt uns einmal in einer Abbildung zu visualisieren. In Abbildung 24.1 sehen wir die Daten einmal als Scatterplot dargestellt.\n\nggplot(ancova_tbl, aes(weight, jump_length, color = animal)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_okabeito() +\n  theme_bw() +\n  geom_point() +\n  labs(color  = \"Tierart\", shape = \"Geschlecht\")  \n\n\n\nAbbildung 24.1— Scatterplot der Daten zur einfaktoriellen ANCOVA.\n\n\n\nDer Abbildung 24.1 können wir jetzt die positive Steigung entnehmen sowie die Reihenfolge der Tierarten nach Sprungweiten. Die ANCOVA sollte immer visualisiert werden, da sich hier die Stärke der Methode mit der Visualiserung verbindet."
  },
  {
    "objectID": "stat-tests-ancova.html#die-zweifaktorielle-ancova-in-r",
    "href": "stat-tests-ancova.html#die-zweifaktorielle-ancova-in-r",
    "title": "24  Die ANCOVA",
    "section": "\n24.5 Die zweifaktorielle ANCOVA in R",
    "text": "24.5 Die zweifaktorielle ANCOVA in R\nDie zweifaktorielle ANCOVA erweitert die einfaktorielle ANCOVA um einen weiteren Faktor. Das ist manchmal etwas verwirrend, da wir auf einmal drei oder mehr Terme in einem Modell haben. Klassischerweise haben wir nun zwei Faktoren \\(f_1\\) und \\(f_2\\) in dem Modell. Weiterhin haben wir nur eine Kovariate \\(c_1\\). Damit sehe das Modell wie folgt aus.\n\\[\ny \\sim f_1 + f_2 + c_1\n\\]\nWir können das Modell dann in R übertragen und ergänzen noch den Interaktionsterm für die Faktoren animal und sex in dem Modell. Das Modell wird klassisch in der Funktion lm() gefittet.\n\nfit_2 &lt;- lm(jump_length ~ animal + sex + weight + animal:sex, data = ancova_tbl)\n\nNach dem Fit können wir das Modell in dem Obkjekt fit_2 in die Funktion anova() pipen. Die Funktion erkennt die Struktur des Modells und gibt uns eine ANCOVA Ausgabe wieder.\n\nfit_2 %&gt;% anova \n\nAnalysis of Variance Table\n\nResponse: jump_length\n            Df Sum Sq Mean Sq  F value Pr(&gt;F)    \nanimal       2 2693.8  1346.9 359.0568 &lt;2e-16 ***\nsex          1 3608.1  3608.1 961.8568 &lt;2e-16 ***\nweight       1    0.0     0.0   0.0053 0.9422    \nanimal:sex   2    2.2     1.1   0.2981 0.7424    \nResiduals  593 2224.4     3.8                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn der ANCOVA erkennne wir nun, dass der Faktor animal signifikant ist. Der \\(p\\)-Wert ist mit \\(&lt;0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Ebenso ist der Faktor sex signifikant. Der \\(p\\)-Wert ist mit \\(&lt;0.001\\) kleiner das das Signifikanzniveau \\(\\alpha\\) von 5%. Die Kovariate weight ist nicht mehr signifikant. Der \\(p\\)-Wert ist mit \\(0.94\\) größer das das Signifikanzniveau \\(\\alpha\\) von 5%. Wir können also schlussfolgern, dass sich mindestens eine Gruppenvergleich der Level des Faktors animal voneinander unterscheidet. Ebenso wie können wir schlussfolgern, dass sich mindestens eine Gruppenvergleich der Level des Faktors site voneinander unterscheidet. Da wir nur zwei Level in dem Faktor sex haben, wissenwir nun, dass sich die beiden Geschlechter der Flöhe in der Sprungweite unterscheiden. Wir wissen auch, dass mit der Zunahme des Gewichts, sich die Sprunglänge nicht ändert.\nIn Abbildung 24.2 sehen wir nochmal den Zusammenhang dargestellt. Wenn wir die Daten getrennt für den Faktor sex anschauen, dann sehen wir, dass das Gewicht keinen Einfluss mehr auf die Sprungweite hat.\n\nggplot(ancova_tbl, aes(weight, jump_length, color = animal)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_okabeito() +\n  theme_bw() +\n  geom_point() +\n  labs(color  = \"Tierart\", shape = \"Geschlecht\") +\n  facet_wrap(~ sex, scales = \"free_x\")\n\n\n\nAbbildung 24.2— Scatterplot der Daten zur einfaktoriellen ANCOVA aufgetelt nach dem Geschlecht der Flöhe."
  },
  {
    "objectID": "stat-tests-ancova.html#und-weiter",
    "href": "stat-tests-ancova.html#und-weiter",
    "title": "24  Die ANCOVA",
    "section": "\n24.6 Und weiter?",
    "text": "24.6 Und weiter?\nNach einer berechnten ANCOVA können wir zwei Fälle vorliegen haben.\nWenn du in deinem Experiment keine signifikanten Ergebnisse findest, ist das nicht schlimm. Du kannst deine Daten immer noch mit der explorativen Datenanalyse auswerten wie in Kapitel 16 beschrieben.\n\nWir habe eine nicht signifkante ANCOVA berechnet. Wir können die Nullhypothese \\(H_0\\) nicht ablehnen und die Mittelwerte über den Faktor sind vermutlich alle gleich. Wir enden hier mit unserer statistischen Analyse.\nWir haben eine signifikante ANCOVA berechnet. Wir können die Nullhypothese \\(H_0\\) ablehnen und mindestens ein Gruppenvergleich über mindestens einen Faktor ist vermutlich unterschiedlich. Wir können dann in Kapitel 31 eine Posthoc Analyse rechnen."
  },
  {
    "objectID": "stat-tests-utest.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-utest.html#genutzte-r-pakete-für-das-kapitel",
    "title": "25  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n25.1 Genutzte R Pakete für das Kapitel",
    "text": "25.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, coin)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-utest.html#daten-für-den-wilcoxon-mann-whitney-test",
    "href": "stat-tests-utest.html#daten-für-den-wilcoxon-mann-whitney-test",
    "title": "25  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n25.2 Daten für den Wilcoxon-Mann-Whitney-Test",
    "text": "25.2 Daten für den Wilcoxon-Mann-Whitney-Test\nBindungen (eng. ties) in den Daten sind ein Problem und müssen beachtet werden. Das heißt, wenn es gleiche Zahlen in den Gruppen gibt.\nFür die Veranschaulichung des Wilcoxon-Mann-Whitney-Test nehmen wir ein simples Beispiel. Wir nehmen ein nicht normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Wir nehmen hierbei an, dass die Sprunglänge jetzt mal nicht normalverteilt ist. Später sind es Boniturnoten, die definitiv nicht normalverteilt sind. Aber mit der Sprunglänge ist das Beispiel einfacher nachzuvollziehen. Darüber hinaus haben wir so keine Bindungen in den Daten. Bindungen (eng. ties) heißt, dass wir die numerisch gleichen Zahlen in beiden Gruppen haben.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln. Wir nehmen in diesem Beispiel an, dass die Variable jump_length nicht normalverteilt ist.\n\ndata_tbl &lt;- read_excel(\"data/flea_dog_cat.xlsx\") %&gt;% \n  select(animal, jump_length, grade) %&gt;% \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt data_tbl mit dem Datensatz in Tabelle 25.1 nochmal dargestellt.\n\n\n\n\nTabelle 25.1— Selektierter Datensatz für den Wilcoxon-Mann-Whitney-Test mit einer nicht-normalverteilten Variable jump_length und einem Faktor animal mit zwei Leveln.\n\nanimal\njump_length\ngrade\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n8.2\n8\n\n\ndog\n5.6\n7\n\n\ndog\n9.1\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt data_tbl folgendes Modell für später:\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wie immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in dem Wilcoxon-Mann-Whitney-Test aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese."
  },
  {
    "objectID": "stat-tests-utest.html#hypothesen-für-den-wilcoxon-mann-whitney-test",
    "href": "stat-tests-utest.html#hypothesen-für-den-wilcoxon-mann-whitney-test",
    "title": "25  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n25.3 Hypothesen für den Wilcoxon-Mann-Whitney-Test",
    "text": "25.3 Hypothesen für den Wilcoxon-Mann-Whitney-Test\nDer Wilcoxon-Mann-Whitney-Test betrachtet die Mediane und Ränge um einen Unterschied nachzuweisen. Daher haben wir die Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mediane der beiden Levels des Faktors animal gleich sind. Wir vergleichen im Wilcoxon-Mann-Whitney-Test nur zwei Gruppen.\n\\[\nH_0: \\; \\widetilde{y}_{cat} = \\widetilde{y}_{dog}\n\\]\nDie Alternative lautet, dass sich die beiden Gruppen im Median unterscheiden. Wir können uns über die Boxplots oder aber die berechneten Mediane dann den Unterschied bewerten.\n\\[\nH_A: \\; \\widetilde{y}_{cat} \\ne \\widetilde{y}_{dog}\n\\]\nWir schauen uns jetzt einmal den Wilcoxon-Mann-Whitney-Test theoretisch an bevor wir uns mit der Anwendung des Wilcoxon-Mann-Whitney-Test in R beschäftigen."
  },
  {
    "objectID": "stat-tests-utest.html#wilcoxon-mann-whitney-test-theoretisch",
    "href": "stat-tests-utest.html#wilcoxon-mann-whitney-test-theoretisch",
    "title": "25  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n25.4 Wilcoxon-Mann-Whitney-Test theoretisch",
    "text": "25.4 Wilcoxon-Mann-Whitney-Test theoretisch\nDer Wilcoxon-Mann-Whitney-Test berechnet die U Teststatistik auf den Rängend der Daten. Es gibt genau soviele Ränge wie es Beobachtungen im Datensatz gibt. Wir haben \\(n = 14\\) Beobachtungen in unseren Daten zu der Sprungweite in [cm] von den Hunde- und Katzenflöhen. Somit müssen wir auch vierzehn Ränge vergeben.\nDie Tabelle 25.2 zeigt das Vorgehen der Rangvergabe. Wir sortieren als erstes das \\(y\\) aufsteigend. In unserem Fall ist das \\(y\\) die Sprunglänge. Dann vergeben wir die Ränge jweiles zugehörig zu der Position der Sprunglänge und der Tierart. Abschließend addieren wir die Rangsummmen für cat und dog zu den Rangsummen \\(R_{cat}\\) und \\(R_{dog}\\).\n\n\nTabelle 25.2— Datentablle absteigend sortiert nach der Sprunglänge in [cm]. Die Level cat und dog haben jeweils die entsprechenden Ränge zugeordnet bekommen und die Rangsummen wurden berechnet\n\nRank\nanimal\njump_length\nRänge “cat”\nRänge “dog”\n\n\n\n1\ncat\n2.2\n1\n\n\n\n2\ncat\n3.2\n2\n\n\n\n3\ncat\n4.1\n3\n\n\n\n4\ncat\n4.3\n4\n\n\n\n5\ncat\n5.4\n5\n\n\n\n6\ndog\n5.6\n\n6\n\n\n7\ndog\n5.7\n\n7\n\n\n8\ncat\n6.1\n8\n\n\n\n9\ndog\n7.6\n\n9\n\n\n10\ncat\n7.9\n10\n\n\n\n11\ndog\n8.2\n\n11\n\n\n12\ndog\n8.9\n\n12\n\n\n13\ndog\n9.1\n\n13\n\n\n14\ndog\n11.8\n\n14\n\n\n\n\nRangsummen\n\\(R_{cat} = 33\\)\n\\(R_{dog} = 72\\)\n\n\n\n\nGruppengröße\n7\n7\n\n\n\n\nDie Formel für die U Statistik sieht ein wenig wild aus, aber wir können eigentlich relativ einfach alle Zahlen einsetzen. Dann musst du dich etwas konzentrieren bei der Rechnung.\n\\[\nU_{calc} = n_1n_2 + \\cfrac{n_1(n_1+1)}{2}-R_1\n\\]\nmit\n\n\n\\(R_1\\) der größeren der beiden Rangsummen,\n\n\\(n_1\\) die Fallzahl der größeren der beiden Rangsummen\n\n\\(n_2\\) die Fallzahl der kleineren der beiden Rangsummen\n\nWir setzen nun die Zahlen ein. Da wir ein balanciertes Design vorliegen haben sind die Fallzahlen \\(n_1 = n_2 = 7\\) gleich. Wir müssen nur schauen, dass wir mit \\(R_1\\) die passende Rangsumme wählen. In unserem Fall ist \\(R_1 = R_{dog} = 72\\).\n\\[\nU_{calc} = 7 \\cdot 7 + \\cfrac{7(7+1)}{2}-72 = 5\n\\]\nDer kritische Wert für die U Statistik ist \\(U_{\\alpha = 5\\%} = 8\\) für \\(n_1 = 7\\) und \\(n_2 = 7\\). Bei der Entscheidung mit der berechneten Teststatistik \\(U_{calc}\\) gilt, wenn \\(U_{calc} \\leq U_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. Da in unserem Fall das \\(U_{calc}\\) mit \\(5\\) kleiner ist als das \\(U_{\\alpha = 5\\%} = 8\\) können wir die Nullhypothese ablehnen. Wir haben ein signifkianten Unterschied in den Medianen zwischen den beiden Tierarten im Bezug auf die Sprungweite in [cm] von Flöhen.\nBei grosser Stichprobe, wenn \\(n_1 + n_2 &gt; 30\\) ist, können wir die U Statistik auch standariseren und damit in den z-Wert transformieren.\n\\[\nz_{calc} = \\cfrac{U_{calc} - \\bar{U}}{s_U} = \\cfrac{U_{calc} - \\cfrac{n_1 \\cdot n_2}{2}}{\\sqrt{\\cfrac{n_1 \\cdot n_2 (n_1 + n_2 +1)}{12}}}\n\\]\nmit\n\n\n\\(\\bar{U}\\) dem Mittelwert der U-Verteilung ohne Unterschied zwischen den Gruppen\n\n\\(s_U\\) Standardfehler des U-Wertes\n\n\\(n_1\\) Stichprobengrösse der Gruppe mit der grösseren Rangsumme\n\n\\(n_2\\) Stichprobengrösse der Gruppe mit der kleineren Rangsumme\n\nWir setzen dafür ebenfalls die berechnete U Statistik ein und müssen dann wieder konzentriert rechnen.\n\\[\nz_{calc} = \\cfrac{5 - \\cfrac{7 \\cdot 7}{2}}{\\sqrt{\\cfrac{7 \\cdot 7 (7 + 7 +1)}{12}}} = \\cfrac{-19.5}{7.83} = |-2.46|\n\\]\nDer kritische Wert für die z-Statistik ist \\(z_{\\alpha = 5\\%} = 1.96\\). Bei der Entscheidung mit der berechneten Teststatistik \\(z_{calc}\\) gilt, wenn \\(z_{calc} \\geq z_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. Wir haben eine berechnete z Statistik von \\(z_{calc} = 2.46\\). Damit ist \\(z_{calc}\\) größer als \\(z_{\\alpha = 5\\%} = 1.96\\) und wir können die Nullhypothese ablehnen. Wir haben einen signifkanten Unterschied zwischen den Medianen der beiden Floharten im Bezug auf die Sprunglänge in [cm].\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(U_{\\boldsymbol{calc}}\\) oder der Teststatistik \\(z_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(U_{calc}\\) gilt, wenn \\(U_{calc} \\leq U_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nBei der Entscheidung mit der berechneten Teststatistik \\(z_{calc}\\) gilt, wenn \\(z_{calc} \\geq z_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr."
  },
  {
    "objectID": "stat-tests-utest.html#wilcoxon-mann-whitney-test-in-r",
    "href": "stat-tests-utest.html#wilcoxon-mann-whitney-test-in-r",
    "title": "25  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n25.5 Wilcoxon-Mann-Whitney-Test in R",
    "text": "25.5 Wilcoxon-Mann-Whitney-Test in R\nDie Nutzung des Wilcoxon-Mann-Whitney-Test in R ist relativ einfach mit der Funktion wilxoc.test(). Wir müssen zum einen entscheiden, ob Bindungen in den Daten vorliegen. Sollte Bindungen vorliegen, warnt uns R und wir nutzen dann die Funktion wilcox_test() aus dem R Paket coin.\n\n25.5.1 Ohne Bindungen\nOhne Bindungen können wir die Funktion wilxoc.test() nutzen. Die Funktion benötigt das Modell in formula Syntax in der Form jump_length ~ animal. Wir geben noch an, dass wir die 95% Konfidenzintervalle wiedergegeben haben wollen.\n\nwilcox.test(jump_length ~ animal, data = data_tbl, \n            conf.int = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  jump_length by animal\nW = 44, p-value = 0.01107\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 1.0 5.9\nsample estimates:\ndifference in location \n                   3.5 \n\n\nWir sehen das der Wilcoxon-Mann-Whitney-Test ein signifikantes Ergebnis liefert, da der \\(p\\)-Wert mit 0.011 kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Die Nullhypothese kann daher abgelehnt werden. Wir haben einen medianen Unterschied in den Sprungweiten von 3.5 cm [1.0; 5.9] zwischen Hunde- und Katzenflöhen.\n\n25.5.2 Mit Bindungen\nMit Bindungen können wir die Funktion wilxoc_test() aus dem R Paket coin nutzen. Wir nutzen hier als \\(y\\) die Boniturnoten grade der Hunde und Katzen. Die Funktion benötigt das Modell in formula Syntax in der Form grade ~ animal. Wir geben noch an, dass wir die 95% Konfidenzintervalle wiedergegeben haben wollen. Wenn du die Funktion wilcox.test() nutzen würdest, würde dir R eine Warnung ausgeben: Warning: cannot compute exact p-value with ties. Du wüsstest dann, dass du die Funktion wechseln musst.\n\nwilcox_test(grade ~ animal, data = data_tbl, \n            conf.int = TRUE) \n\n\n    Asymptotic Wilcoxon-Mann-Whitney Test\n\ndata:  grade by animal (dog, cat)\nZ = 2.4973, p-value = 0.01251\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n 1 3\nsample estimates:\ndifference in location \n                     2 \n\n\nWir sehen das der Wilcoxon-Mann-Whitney-Test ein signifikantes Ergebnis liefert, da der \\(p\\)-Wert mit 0.015 kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Die Nullhypothese kann daher abgelehnt werden. Wir haben einen medianen Unterschied in den Boniturnoten von 2 [0; 3] zwischen Hunde und Katzen."
  },
  {
    "objectID": "stat-tests-utest.html#minimale-fallzahl-je-gruppe",
    "href": "stat-tests-utest.html#minimale-fallzahl-je-gruppe",
    "title": "25  Der Wilcoxon-Mann-Whitney-Test",
    "section": "\n25.6 Minimale Fallzahl je Gruppe",
    "text": "25.6 Minimale Fallzahl je Gruppe\nHäufig wird auch der Wilcoxon-Mann-Whitney-Test eingesetzt, wenn wenig Beobachtungen vorliegen. Es gibt aber eine untere Grenze der Signifikanz. Das heißt unter einer Fallzahl von \\(n_1 = 3\\) und \\(n_2 = 3\\) wird ein Wilcoxon-Mann-Whitney-Test nicht mehr signifikant. Egal wie groß der Unterschied ist, ein Wilcoxon-Mann-Whitney-Test wird dann die Nulhypothese nicht ablehnen können. Schauen wir das Datenbeispiel in Tabelle 25.3 einmal an.\n\n\n\n\nTabelle 25.3— Kleiner Datensatz mit jeweils nur drei Beobachtungen pro Gruppe.\n\nanimal\njump_length\n\n\n\ndog\n1.2\n\n\ndog\n5.6\n\n\ndog\n3.2\n\n\ncat\n100.3\n\n\ncat\n111.2\n\n\ncat\n98.5\n\n\n\n\n\n\nWir sehen jeweils drei Beobachtunge für Hunde- und Katzensprungweiten. Der Unterschied ist numerisch riesig. Wir können uns den Unterschied nochmal in Abbildung 25.1 visualisieren.\n\n\n\n\nAbbildung 25.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen.\n\n\n\nWir sehen, der Unterschied ist riesig. Der Wilcoxon-Mann-Whitney-Test findet jedoch nur einen p-Wert von 0.1 und kann damit die Nullhypothese nicht ablehnen. Wir haben keinen signifkanten Unterschied.\n\nwilcox.test(jump_length ~ animal, data = small_tbl)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  jump_length by animal\nW = 9, p-value = 0.1\nalternative hypothesis: true location shift is not equal to 0\n\n\nWir sehen hier ein schönes Beispiel für die Begrenztheit von Algorithmen und mathematischen Formeln. Es gibt einen Unterschied, aber der Wilcoxon-Mann-Whitney-Test ist technisch nicht in der Lage einen Unterschied nochzuweisen. Daher solltest du immer versuchen die Ergebnisse eines Testes mit einer Abbildung zu überprüfen."
  },
  {
    "objectID": "stat-tests-kruskal.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-kruskal.html#genutzte-r-pakete-für-das-kapitel",
    "title": "26  Der Kruskal-Wallis-Test",
    "section": "\n26.1 Genutzte R Pakete für das Kapitel",
    "text": "26.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, rstatix)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-kruskal.html#daten-für-den-kruskal-wallis-test",
    "href": "stat-tests-kruskal.html#daten-für-den-kruskal-wallis-test",
    "title": "26  Der Kruskal-Wallis-Test",
    "section": "\n26.2 Daten für den Kruskal-Wallis-Test",
    "text": "26.2 Daten für den Kruskal-Wallis-Test\nBindungen (eng. ties) in den Daten sind ein Problem und müssen beachtet werden. Das heißt, wenn es gleiche Zahlen in den Gruppen gibt.\nWir wollen uns nun erstmal den einfachsten Fall anschauen mit einem simplen Datensatz. Wir nehmen ein nicht-normalverteiltes \\(y\\) aus den Datensatz flea_dog_cat_fox.csv und einen Faktor mit mehr als zwei Leveln. Hätten wir nur zwei Level, dann können wir auch einen Wilcoxon-Mann-Whitney-Test rechnen können.\nWir nehmen in diesem Abschnitt an, dass die Sprunglänge jetzt mal nicht normalverteilt ist. Später sind es Boniturnoten, die definitiv nicht normalverteilt sind. Aber mit der Sprunglänge ist das Beispiel einfacher nachzuvollziehen. Darüber hinaus haben wir so keine Bindungen in den Daten. Bindungen (eng. ties) heißt, dass wir die numerisch gleichen Zahlen in beiden Gruppen haben.\nIm Folgenden selektieren mit der Funktion select() die beiden Spalten jump_length als \\(y\\) und die Spalte animal als \\(x\\). Danach müssen wir noch die Variable animal in einen Faktor mit der Funktion as_factor() umwandeln. Wir nehmen in diesem Beispiel an, dass die Variable jump_length nicht normalverteilt ist.\n\nfac1_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox.csv\") %&gt;%\n  select(animal, jump_length, grade) %&gt;% \n  mutate(animal = as_factor(animal))\n\nWir erhalten das Objekt fac1_tbl mit dem Datensatz in Tabelle 26.1 nochmal dargestellt.\n\n\n\n\nTabelle 26.1— Selektierter Datensatz für den Kruskal-Wallis-Test mit einer nicht-normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\ngrade\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n8.2\n8\n\n\ndog\n5.6\n7\n\n\ndog\n9.1\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\nfox\n7.7\n5\n\n\nfox\n8.1\n4\n\n\nfox\n9.1\n4\n\n\nfox\n9.7\n5\n\n\nfox\n10.6\n4\n\n\nfox\n8.6\n4\n\n\nfox\n10.3\n3\n\n\n\n\n\n\nWir bauen daher mit den beiden Variablen mit dem Objekt fac1_tbl folgendes Modell für später:\n\\[\njump\\_length \\sim animal\n\\]\nBevor wir jetzt das Modell verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wie immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in dem Kruskal-Wallis-Test aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese."
  },
  {
    "objectID": "stat-tests-kruskal.html#hypothesen-für-den-kruskal-wallis-test",
    "href": "stat-tests-kruskal.html#hypothesen-für-den-kruskal-wallis-test",
    "title": "26  Der Kruskal-Wallis-Test",
    "section": "\n26.3 Hypothesen für den Kruskal-Wallis-Test",
    "text": "26.3 Hypothesen für den Kruskal-Wallis-Test\nDer Kruskal-Wallis-Test betrachtet die Mediane und Ränge um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mediane jedes Levels des Faktors animal gleich sind.\n\\[\nH_0: \\; \\widetilde{y}_{cat} = \\widetilde{y}_{dog} = \\widetilde{y}_{fox}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Medianen unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mediane unterschieden oder eben nur ein Paar. Wenn ein Kruskal-Wallis-Test die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mediane sich unterscheiden.\n\\[\n\\begin{aligned}\nH_A: &\\; \\widetilde{y}_{cat} \\ne \\widetilde{y}_{dog}\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{cat} \\ne \\widetilde{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{dog} \\ne \\widetilde{y}_{fox}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\]\nWir schauen uns jetzt einmal den Kruskal-Wallis-Test theoretisch an bevor wir uns mit der Anwendung des Kruskal-Wallis-Test in R beschäftigen."
  },
  {
    "objectID": "stat-tests-kruskal.html#kruskal-wallis-test-theoretisch",
    "href": "stat-tests-kruskal.html#kruskal-wallis-test-theoretisch",
    "title": "26  Der Kruskal-Wallis-Test",
    "section": "\n26.4 Kruskal-Wallis-Test theoretisch",
    "text": "26.4 Kruskal-Wallis-Test theoretisch\nDer Kruskal-Wallis-Test berechnet die H Teststatistik auf den Rängend der Daten. Es gibt genau soviele Ränge wie es Beobachtungen im Datensatz gibt. Wir haben \\(n = 21\\) Beobachtungen in unseren Daten zu der Sprungweite in [cm] von den Hunde-, Katzen- und Fuchsflöhen. Somit müssen wir auch einundzwanzig Ränge vergeben.\nDie Tabelle 26.2 zeigt das Vorgehen der Rangvergabe. Wir sortieren als erstes das \\(y\\) aufsteigend. In unserem Fall ist das \\(y\\) die Sprunglänge. Dann vergeben wir die Ränge jweiles zugehörig zu der Position der Sprunglänge und der Tierart. Abschließend addieren wir die Rangsummmen für cat, dog und fox zu den Rangsummen \\(R_{cat}\\), \\(R_{dog}\\) und \\(R_{fox}\\).\n\n\n\nTabelle 26.2— Datentablle absteigend sortiert nach der Sprunglänge in [cm]. Die Level cat, dog und fox haben jeweils die entsprechenden Ränge zugeordnet bekommen und die Rangsummen wurden berechnet\n\n\n\n\n\n\n\n\n\nRank\nanimal\njump_length\nRänge “cat”\nRänge “dog”\nRänge “fox”\n\n\n\n1\ncat\n2.2\n1\n\n\n\n\n2\ncat\n3.2\n2\n\n\n\n\n3\ncat\n4.1\n3\n\n\n\n\n4\ncat\n4.3\n4\n\n\n\n\n5\ncat\n5.4\n5\n\n\n\n\n6\ndog\n5.6\n\n6\n\n\n\n7\ndog\n5.7\n\n7\n\n\n\n8\ncat\n6.1\n8\n\n\n\n\n9\ndog\n7.6\n\n9\n\n\n\n10\nfox\n7.7\n\n\n10\n\n\n11\ncat\n7.9\n11\n\n\n\n\n12\nfox\n8.1\n\n\n12\n\n\n13\ndog\n8.2\n\n13\n\n\n\n14\nfox\n8.6\n\n\n14\n\n\n15\ndog\n8.9\n\n15\n\n\n\n16\ndog\n9.1\n\n16\n\n\n\n17\nfox\n9.1\n\n\n17\n\n\n18\nfox\n9.7\n\n\n18\n\n\n19\nfox\n10.3\n\n\n19\n\n\n20\nfox\n10.6\n\n\n20\n\n\n21\ndog\n11.8\n\n21\n\n\n\n\n\nRangsummen\n\\(R_{cat} = 34\\)\n\\(R_{dog} = 87\\)\n\\(R_{fox} = 110\\)\n\n\n\n\nGruppengröße\n7\n7\n7\n\n\n\n\n\nDie Summe aller Ränge ist \\(1+2+3+...+21 = 231\\). Wir überprüfen nochmal die Summe der Rangsummen als Gegenprobe \\(R_{cat} + R_{dog} + R_{fox} = 231\\). Das ist identisch, wir haben keinen Fehler bei der Rangaufteilung und der Summierung gemacht.\nDie Formel für die H Statistik sieht wie die U Statistik ein wenig wild aus, aber wir können eigentlich relativ einfach alle Zahlen einsetzen. Dann musst du dich etwas konzentrieren bei der Rechnung.\n\\[\nH = \\cfrac{12}{N(N+1)}\\sum_{i=1}^k\\cfrac{R_i^2}{n_i}-3(N+1)\n\\]\nmit\n\n\n\\(R_i\\) der Rangsummen für jede Gruppe mit insgesamt \\(k\\) Gruppen\n\n\\(n_i\\) der Fallzahl in jeder Gruppe\n\n\\(N\\) der Gesamtzahl an Beobachtungen also die gesamte Fallzahl\n\nWir setzen nun die Zahlen ein. Da wir ein balanciertes Design vorliegen haben sind die Fallzahlen \\(n_1 = n_2 = n_3 = 7\\) gleich.\n\\[\nH_{calc} = \\cfrac{12}{21(21+1)}\\left(\\cfrac{34^2}{7}+\\cfrac{87^2}{7}+\\cfrac{110^2}{7}\\right)-3(21+1) = 11.27\n\\]\nDer kritische Wert für die H Statistik ist \\(H_{\\alpha = 5\\%} = 5.99\\). Bei der Entscheidung mit der berechneten Teststatistik \\(H_{calc}\\) gilt, wenn \\(H_{calc} \\geq U_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt. Da in unserem Fall das \\(H_{calc}\\) mit \\(11.27\\) größer ist als das \\(H_{\\alpha = 5\\%} = 5.99\\) können wir die Nullhypothese ablehnen. Wir haben ein signifkianten Unterschied in den Medianen zwischen den beiden Tierarten im Bezug auf die Sprungweite in [cm] von Flöhen.\n\n\n\n\n\n\nEntscheidung mit der berechneten Teststatistik \\(F_{\\boldsymbol{calc}}\\)\n\n\n\nBei der Entscheidung mit der berechneten Teststatistik \\(H_{calc}\\) gilt, wenn \\(H_{calc} \\geq H_{\\alpha = 5\\%}\\) wird die Nullhypothese (H\\(_0\\)) abgelehnt.\nAchtung – Wir nutzen die Entscheidung mit der Teststatistik nur und ausschließlich in der Klausur. In der praktischen Anwendung hat die Betrachtung der berechneten Teststatistik keine Verwendung mehr."
  },
  {
    "objectID": "stat-tests-kruskal.html#kruskal-wallis-test-in-r",
    "href": "stat-tests-kruskal.html#kruskal-wallis-test-in-r",
    "title": "26  Der Kruskal-Wallis-Test",
    "section": "\n26.5 Kruskal-Wallis-Test in R",
    "text": "26.5 Kruskal-Wallis-Test in R\nDie Nutzung des Kruskal-Wallis-Test in R ist relativ einfach mit der Funktion kruskal.test(). Wir nutzen die formual Syntax um das Modell zu definieren und können dann schon die Funktion nutzen.\n\nkruskal.test(jump_length ~ animal, data = fac1_tbl) \n\n\n    Kruskal-Wallis rank sum test\n\ndata:  jump_length by animal\nKruskal-Wallis chi-squared = 11.197, df = 2, p-value = 0.003704\n\n\nMit einem p-Wert von \\(0.0037\\) können wir die Nullhypothese ablehnen, da der p-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Wir haben mindestens einen medianen Unterschied zwischen den Sprungweiten der Hunde-, Katzen- und Fuchsflöhen.\nFür die Betrachtung der Effektgröße in einem Kruskal-Wallis-Test nutzen wir das R Paket rstatix und die darin enthaltende Funktion kruskal_effsize(). Wir berechnene hierbei analog zu einfaktoriellen ANOVA den \\(\\eta^2\\) Wert.\n\nfac1_tbl %&gt;% kruskal_effsize(jump_length ~ animal)\n\n# A tibble: 1 × 5\n  .y.             n effsize method  magnitude\n* &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;ord&gt;    \n1 jump_length    21   0.511 eta2[H] large    \n\n\nDas \\(\\eta^2\\) nimmt Werte von 0 bis 1 an und gibt, multipliziert mit 100, den Prozentsatz der Varianz der durch die \\(x\\) Variable erklärt wird. In unserem Beispiel wird 51.1% der Varianz in de Daten durch den Faktor animal erklärt."
  },
  {
    "objectID": "stat-tests-friedman.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-friedman.html#genutzte-r-pakete-für-das-kapitel",
    "title": "\n27  Friedman Test\n",
    "section": "\n27.1 Genutzte R Pakete für das Kapitel",
    "text": "27.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, \n               readxl, rstatix, coin,\n               effectsize, PMCMRplus, rcompanion)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-friedman.html#daten-für-den-friedman-test",
    "href": "stat-tests-friedman.html#daten-für-den-friedman-test",
    "title": "\n27  Friedman Test\n",
    "section": "\n27.2 Daten für den Friedman Test",
    "text": "27.2 Daten für den Friedman Test\nFür das Datenbeispiel bauen wir uns den Datensatz schnell selber zusammen. Wir wollen uns fünf Weizensorten \\(A\\) bis \\(E\\) anschauen und bonitieren, wie die Qualität der Pflanzen nach einer Dürreperiode aussieht. Daher vergeben wir einen Score auf der Likertskala von \\(1\\) bis \\(9\\), wobei \\(9\\) bedeutet, dass die Weizenpflanze vollkommen intakt ist. Wir haben unseren Versuch in vier Blöcken angelegt.\n\ngrade_tbl &lt;- tibble(block = 1:4,\n                    A = c(2,3,4,3),\n                    B = c(7,9,8,9),\n                    C = c(6,5,4,7),\n                    D = c(2,3,1,2),\n                    E = c(4,5,7,6)) %&gt;%\n  gather(key = variety, value = grade, A:E) %&gt;% \n  mutate(block =  as_factor(block))\n\nSchauen wir uns nochmal unseren Datensatz an. Wir haben den Datensatz jetzt im Long-Format vorliegen und können dann gleich mit dem Datensatz weiterarbeiten.\n\ngrade_tbl\n\n# A tibble: 20 × 3\n   block variety grade\n   &lt;fct&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 1     A           2\n 2 2     A           3\n 3 3     A           4\n 4 4     A           3\n 5 1     B           7\n 6 2     B           9\n 7 3     B           8\n 8 4     B           9\n 9 1     C           6\n10 2     C           5\n11 3     C           4\n12 4     C           7\n13 1     D           2\n14 2     D           3\n15 3     D           1\n16 4     D           2\n17 1     E           4\n18 2     E           5\n19 3     E           7\n20 4     E           6\n\n\nIn Abbildung 27.1 sehen wir nochmal die Visualisierung der Boniturnoten über die fünf Weizensorten und den vier Blöcken. Bei einer so kleinen Fallzahl von nur vier Blöcken pro Sorte entscheiden wir uns dann für einen Dotplot um uns die Daten einmal anzuschauen.\n\nggplot(grade_tbl, aes(variety, grade, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.3)) +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9))\n\n\n\nAbbildung 27.1— Dotplot des Datenbeispiels für die Bonitur von fünf Weizensorten.\n\n\n\nIm Folgenden schauen wir usn nun einmal die Hypothesen des Friedman Tests an und rechen dann den Friedman Test in R einmal durch."
  },
  {
    "objectID": "stat-tests-friedman.html#hypothesen-für-den-friedman-test",
    "href": "stat-tests-friedman.html#hypothesen-für-den-friedman-test",
    "title": "\n27  Friedman Test\n",
    "section": "\n27.3 Hypothesen für den Friedman Test",
    "text": "27.3 Hypothesen für den Friedman Test\nDer Friedman Test betrachtet wie auch der Kruskal-Wallis-Test die Mediane \\(\\widetilde{y}\\) und Ränge um einen Unterschied nachzuweisen. Daher haben wir in der Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Mediane jedes Levels des Faktors variety gleich sind.\n\\[\nH_0: \\; \\widetilde{y}_{A} = \\widetilde{y}_{B} = \\widetilde{y}_{C} = \\widetilde{y}_{D} = \\widetilde{y}_{E}\n\\]\nDie Alternative lautet, dass sich mindestens ein paarweiser Vergleich in den Medianen unterschiedet. Hierbei ist das mindestens ein Vergleich wichtig. Es können sich alle Mediane unterschieden oder eben nur ein Paar. Wenn ein Friedman Test die \\(H_0\\) ablehnt, also ein signifikantes Ergebnis liefert, dann wissen wir nicht, welche Mediane sich unterscheiden. Bein unseren fünf Weizensorten kommt da eine ganze Menge an Vergleichen zusammen. In der Folge nur ein Ausschnitt aller Alternativehypothesen.\n\\[\n\\begin{aligned}\nH_A: &\\; \\widetilde{y}_{A} \\ne \\widetilde{y}_{B}\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{A} \\ne \\widetilde{y}_{C}\\\\\n\\phantom{H_A:} &\\; ...\\\\\n\\phantom{H_A:} &\\; \\widetilde{y}_{D} \\ne \\widetilde{y}_{E}\\\\\n\\phantom{H_A:} &\\; \\mbox{für mindestens ein Paar}\n\\end{aligned}\n\\] Damit kommen uns die Hypothesen nicht so unbekannt vor. Die Hypothesenpaare sind die gleichen wie in einer ANOVA bzw. dem Kruskal-Wallis-Test."
  },
  {
    "objectID": "stat-tests-friedman.html#friedman-test-in-r",
    "href": "stat-tests-friedman.html#friedman-test-in-r",
    "title": "\n27  Friedman Test\n",
    "section": "\n27.4 Friedman Test in R",
    "text": "27.4 Friedman Test in R\nDer Friedman Test ist in R in verschiedenen Paketen implementiert. Wir nehmen die Standardfunktion friedman.test(). Wichtig ist wie wir in der Funktion das Modell definieren. Wir nutzen das Symbol | um die Behandlung von dem Block zu trennen. Vor dem | symbol steht die Behandlung, hinter dem | steht der Block. Damit sieht das Modell etwas anders aus, aber im Prinzip ist die Definition des Modells einfach.\n\n\nDas Tutorial von Salvatore S. Mangiafico zum Friedman Test liefert eine sehr ausführliche Anwendung über mehrere R Pakete hinweg.\n\nfriedman.test(grade ~ variety | block, data = grade_tbl)\n\n\n    Friedman rank sum test\n\ndata:  grade and variety and block\nFriedman chi-squared = 14.684, df = 4, p-value = 0.005403\n\n\nNachdem wir die Funktion aufgerufen haben, erhalten wir auch gleich den \\(p\\)-Wert von \\(0.005\\) wieder. Da der \\(p\\)-Wert kleienr ist als das Signifikanzniveau \\(\\alpha\\) von 5% können wir die Nullhypothese ablehnen. Es gibt mindestens einen paarweisen Unterschied. Das war auch anhand des Doplots zu erwarten. Nun stellt sich noch die Frage, wie groß der Effekt ist. Das können wir mit Kendalls \\(W\\) bestimmen. Auch die Funktionalität ist in der R Paket effectsize implementiert. Wir müssen nur wieder das ganze Modell in die Funktion kendalls_w() stecken.\n\nkendalls_w(grade ~ variety | block, data = grade_tbl)\n\nKendall's W |       95% CI\n--------------------------\n0.92        | [0.89, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nNun kriegen wir ein Kendalls \\(W\\) von 0.92 raus. Wir immer ist die Frage nach der Interpretation. Dankenswerterweise gibt es auch die Funktion interpret_kendalls_w(), die uns mit Quelle ausgibt, wie stark der Effekt ist. Einfach nochmal für die Referenz in die Hilfeseite von der Funktion schauen.\n\ninterpret_kendalls_w(0.92)\n\n[1] \"almost perfect agreement\"\n(Rules: landis1977)\n\n\nWir haben also herausgefunden, dass wir einen starken Effek haben und sich die Mediane der Gruppen unterscheiden."
  },
  {
    "objectID": "stat-tests-friedman.html#posthoc-test",
    "href": "stat-tests-friedman.html#posthoc-test",
    "title": "\n27  Friedman Test\n",
    "section": "\n27.5 Posthoc Test",
    "text": "27.5 Posthoc Test\nNachdem wir jetzt festgestellt haben, dass sich mindestens ein Gruppenunterschied zwischen den Weizensorten finden lassen muss, wollen wir noch feststellen wo dieser Unterschied liegt. Wir nutzen dafür den Siegel- und Castellan-Vergleichstests für alle Paare durch. Der Test ist mit der Funktion frdAllPairsSiegelTest() in dem R Paket PMCMRplus implementiert und einfach nutzbar. In dem Paket sind noch eine weitere Reihe an statistischen Test für paarweise nicht-parametrische Test enthalten. Leider sind das Paket und die Funktion schon älter, so dass wir nicht einmal die Formelschreibweise zu Verfügung haben. Wir müssen alle Spalten aus unserem Datensatz mit dem $ einzeln selektieren und den Optionen zuordnen.\n\nsiegel_test_res &lt;-  frdAllPairsSiegelTest(y = grade_tbl$grade,\n                                          groups = grade_tbl$variety,\n                                          blocks = grade_tbl$block,\n                                          p.adjust.method = \"none\")\nsiegel_test_res\n\n  A      B      C      D     \nB 0.0052 -      -      -     \nC 0.1461 0.1797 -      -     \nD 0.5762 0.0008 0.0442 -     \nE 0.1797 0.1461 0.9110 0.0573\n\n\nIch empfehle ja immer eine Adjustierung für multiple Vergleiche, aber du kannst das selber entscheiden. Wenn du für die multiplen Vergleiche adjustieren willst, dann nutze gerne die Option p.adjust.method = \"bonferroni\" oder eben statt bonferroni die Adjustierungsmethode fdr.\nDamit wir auch das compact letter display aus unseren \\(p\\)-Werte berechnen können, müssen wir unser Ergebnisobjekt nochmal in eine Tabelle umwandeln. Dafür gibt es dann auch eine passende Funktion mit PMCMRTable. Wir sehen, es ist alles etwas altbacken.\n\nsiegel_test_tab &lt;- PMCMRTable(siegel_test_res )\n\nsiegel_test_tab\n\n   Comparison  p.value\n1   B - A = 0  0.00519\n2   C - A = 0    0.146\n3   D - A = 0    0.576\n4   E - A = 0     0.18\n5   C - B = 0     0.18\n6   D - B = 0 0.000796\n7   E - B = 0    0.146\n8   D - C = 0   0.0442\n9   E - C = 0    0.911\n10  E - D = 0   0.0573\n\n\nNachdem wir die Tabellenschreibweise der \\(p\\)-Werte vorliegen haben, können wir dann das compact letter display uns über die Funktion cldList() anzeigen lassen. Leider gibt es keine Möglichkeit die 95% Konfidenzintervalle zu erhalten. Wir auch beim Kruskal-Wallis-Test erhalten wir bei dem Friedman Test keine 95% Konfidenzintervalle und müssen im Zweifel mit dieser Einschränkung leben.\n\ncldList(p.value ~ Comparison, data = siegel_test_tab)\n\n  Group Letter MonoLetter\n1     B      a        a  \n2     C     ab        ab \n3     D      c          c\n4     E    abc        abc\n5     A     bc         bc\n\n\nAm compact letter display sehen wir im Prinzip das gleiche Muster wie in den \\(p\\)-Werten. Nur nochmal etwas anders dargestellt und mit dem Fokus auf die nicht Unterschiede. Wenn du mehr über das compact letter display wissen willst dann lese gerne nochmal im Kapitel 31.7 nach."
  },
  {
    "objectID": "stat-tests-chi-test.html#genutzte-r-pakete",
    "href": "stat-tests-chi-test.html#genutzte-r-pakete",
    "title": "28  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n28.1 Genutzte R Pakete",
    "text": "28.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, effectsize, rstatix,\n               scales, parameters, conflicted)\nconflicts_prefer(stats::chisq.test)\nconflicts_prefer(stats::fisher.test)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-chi-test.html#daten-für-den-mathcalx2-test",
    "href": "stat-tests-chi-test.html#daten-für-den-mathcalx2-test",
    "title": "28  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n28.2 Daten für den \\(\\mathcal{X}^2\\)-Test",
    "text": "28.2 Daten für den \\(\\mathcal{X}^2\\)-Test\nWie eben schon benannt schauen wir uns für den \\(\\mathcal{X}^2\\)-Test eine Vierfeldertafel oder aber 2x2 Kreuztabelle an. In Tabelle 28.1 sehen wir eine solche 2x2 Kreuztabelle. Da wir eine Mindestanzahl an Zellbelegung brauchen um überhaupt mit dem \\(\\mathcal{X}^2\\)-Test rechnen zu können, mutzen wir hier gleich aggrigierte Beispieldaten. Wir brauchen mindestens fünf Beobachtungen je Zelle, dass heißt mindestens 20 Tiere. Da wir dann aber immer noch sehr wenig haben, ist die Daumenregel, dass wir etwa 30 bis 40 Beobachtungen brauchen. In unserem Beispiel schauen wir uns 65 Tiere an.\n\n\nTabelle 28.1— Eine 2x2 Tabelle als Beispiel für unterschiedliche Flohinfektionen bei Hunden und Katzen. Dargestellt sind die beobachteten Werte.\n\n\n\n\n\n\n\n\n\n\n\nInfected\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\nAnimal\nDog\n\\(23_{\\;\\Large a}\\)\n\\(10_{\\;\\Large b}\\)\n\\(\\mathbf{a+b = 33}\\)\n\n\n\nCat\n\\(18_{\\;\\Large c}\\)\n\\(14_{\\;\\Large d}\\)\n\\(\\mathbf{c+d = 32}\\)\n\n\n\n\n\\(\\mathbf{a+c = 41}\\)\n\\(\\mathbf{b+d = 24}\\)\n\\(n = 65\\)\n\n\n\n\nIn der Tabelle sehen wir, dass in den zeieln die Level des Faktors animal angegeben sind und in den Spalten die Level des Faktors infected. Wir haben somit \\(23\\) Hunde, die mit Flöhen infiziert sind, dann \\(10\\) Hunde, die nicht mit Flöhen infirziert sind. Auf der Seite der Katzen haben wir \\(18\\) Katzen, die infiziert sind und \\(14\\) Katzen, die keine Flöhe haben. An den Rändern stehen die Randsummen. Wir haben \\(33\\) Hunde und \\(32\\) Katzen sowie \\(41\\) infizierte Tiere und \\(24\\) nicht infizierte Tiere. Somit haben wir dann in Summe \\(n = 65\\) Tiere. Diese Form der Tabelle wird uns immer wieder begegnen.\nBevor wir jetzt diese 2x2 Kreuztabelle verwenden, müssen wir uns nochmal überlegen, welchen Schluß wir eigentlich über die Nullhypothese machen. Wie immer können wir nur die Nullhypothese ablehnen. Daher überlegen wir uns im Folgenden wie die Nullhypothese in dem \\(\\mathcal{X}^2\\)-Test aussieht. Dann bilden wir anhand der Nullhypothese noch die Alternativehypothese."
  },
  {
    "objectID": "stat-tests-chi-test.html#hypothesen-für-den-mathcalx2-test",
    "href": "stat-tests-chi-test.html#hypothesen-für-den-mathcalx2-test",
    "title": "28  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n28.3 Hypothesen für den \\(\\mathcal{X}^2\\)-Test",
    "text": "28.3 Hypothesen für den \\(\\mathcal{X}^2\\)-Test\nDer \\(\\mathcal{X}^2\\)-Test betrachtet die Zellbelegung gegeben den Randsummen um einen Unterschied nachzuweisen. Daher haben wir die Nullhypothese als Gleichheitshypothese. In unserem Beispiel lautet die Nullhypothese, dass die Zahlen in den Zellen gegeben der Randsummen gleich sind. Wir betrachten hier nur die Hypothesen in Prosa und die mathematischen Hypothesen. Es ist vollkommen ausreichend, wenn du die Nullhypothese des \\(\\mathcal{X}^2\\)-Test nur in Prosa kennst.\n\\[\nH_0: \\; \\mbox{Zellbelegung sind gleichverteilt gegeben der Randsummen}\n\\]\nDie Alternative lautet, dass sich die Zahlen in den Zellen gegeben der Randsummen unterscheiden.\n\\[\nH_A: \\; \\mbox{Zellbelegung sind nicht gleichverteilt gegeben der Randsummen}\n\\]\nWir schauen uns jetzt einmal den \\(\\mathcal{X}^2\\)-Test theoretisch an bevor wir uns mit der Anwendung des \\(\\mathcal{X}^2\\)-Test in R beschäftigen."
  },
  {
    "objectID": "stat-tests-chi-test.html#mathcalx2-test-theoretisch",
    "href": "stat-tests-chi-test.html#mathcalx2-test-theoretisch",
    "title": "28  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n28.4 \\(\\mathcal{X}^2\\)-Test theoretisch",
    "text": "28.4 \\(\\mathcal{X}^2\\)-Test theoretisch\nIn Tabelle 28.1 von oben hatten wir die beobachteten Werte. Das sind die Zahlen, die wir in unserem Experiment erhoben und gemessen haben. Der \\(\\mathcal{X}^2\\)-Test vergleicht nun die beobachteten Werte mit den anhand der Randsummen zu erwartenden Werte. Daher ist die Formel für den \\(\\mathcal{X}^2\\)-Test wie folgt.\n\\[\n\\chi^2_{calc} = \\cfrac{(O - E)^2}{E}\n\\]\nmit\n\n\n\\(O\\) für die beobachteten Werte\n\n\\(E\\) für die nach den Randsummen zu erwartenden Werte\n\nIn Tabelle 28.2 kannst du sehen wie wir anhand der Randsummen die erwartenden Zellbelegungen berechnen. Hierbei können auch krumme Zahlen rauskommen. Wir würden keinen Unterschied zwischen Hunde und Katzen gegeben deren Infektionsstatus erwarten, wenn die Abweichungen zwischen den beobachteten Werten und den zu erwartenden Werten klein wären. Wir berechnen nun die zu erwartenden Werte indem wir die Randsummen der entsprechenden Zelle multiplizieren und durch die Gesamtanzahl teilen.\n\n\nTabelle 28.2— Eine 2x2 Tabelle als Beispiel für unterschiedliche Flohinfektionen bei Hunden und Katzen. Dargestellt sind die zu erwartenden Werte.\n\n\n\n\n\n\n\n\n\n\n\nInfected\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\nAnimal\nDog\n\\(\\cfrac{41 \\cdot 33}{65} = 20.82\\)\n\\(\\cfrac{24 \\cdot 33}{65} = 12.18\\)\n\\(\\mathbf{33}\\)\n\n\n\nCat\n\\(\\cfrac{41 \\cdot 32}{65} = 20.18\\)\n\\(\\cfrac{24 \\cdot 32}{65} = 11.82\\)\n\\(\\mathbf{32}\\)\n\n\n\n\n\\(\\mathbf{41}\\)\n\\(\\mathbf{24}\\)\n\\(n = 65\\)\n\n\n\n\nWir können dann die Formel für den \\(\\mathcal{X}^2\\)-Test entsprechend ausfüllen. Dabei ist wichtig, dass die Abstände quadriert werden. Das ist ein Kernkonzept der Statistik, Abstände bzw. Abweichungen werden immer quadriert.\n\\[\\begin{aligned}\n\\chi^2_{calc} &= \\cfrac{(23 - 20.82)^2}{20.82} + \\cfrac{(10 - 12.18)^2}{12.18} + \\\\\n&\\phantom{=}\\;\\; \\cfrac{(18 - 20.18)^2}{20.18} + \\cfrac{(14 - 11.82)^2}{11.82} = 1.25\n\\end{aligned}\\]\nEs ergibt sich ein \\(\\chi^2_{calc}\\) von \\(1.25\\) mit der Regel, dass wenn \\(\\chi^2_{calc} \\geq \\chi^2_{\\alpha=5\\%}\\) die Nullhypothese abgelehnt werden kann. Mit einem \\(\\chi^2_{\\alpha=5\\%} = 3.84\\) können wir die Nullhypothese nicht ablehnen. Es besteht kein Zusammenhang zwischen den Befall mit Flöhen und der Tierart. Oder anders herum, Hunde und Katzen werden gleich stark mit Flöhen infiziert.\nWie stark ist nun der beobachtete Effekt? Wir konnten zwar die Nullhypothese nicht ablehnen, aber es wäre auch von Interesse für die zukünftige Versuchsplanung, wie stark sich die Hunde und Katzen im Befall mit Flöhen unterscheiden. Wir haben nun die Wahl zwischen zwei statistischen Maßzahlen für die Beschreibung eines Effektes bei einem \\(\\mathcal{X}^2\\)-Test.\nZum einen Cramers \\(V\\), das wir in etwa interpretieren wie eine Korrelation und somit auch einheitslos ist. Wenn Cramers \\(V\\) gleich 0 ist, dann haben wir keinen Unterschied zwischen den Hunden und Katzen gegeben dem Flohbefall. Bei einem Cramers \\(V\\) von 0.5 haben wir einen sehr starken Unterschied zwischen dem Flohbefall zwischen den beiden Tierarten. Der Vorteil von Cramers V ist, dass wir Cramers \\(V\\) auch auf einer beliebig großen Kreuztabelle berechnen können. Die Formel ist nicht sehr komplex.\n\\[\nV = \\sqrt{\\cfrac{\\mathcal{X}^2/n}{\\min(c-1, r-1)}}\n\\]\nmit\n\n\n\\(\\mathcal{X}^2\\) gleich der Chi-Quadrat-Statistik\n\n\\(n\\) gleich der Gesamtstichprobengröße\n\n\\(r\\): Anzahl der Reihen\n\n\\(c\\): Anzahl der Spalten\n\n\\[\nV = \\sqrt{\\cfrac{1.26/65}{1}} = 0.14\n\\]\nWir setzen also den Wert \\(\\chi^2_{calc}\\) direkt in die Formel ein. Wir wissen ja auch, dass wir \\(n = 65\\) Tiere untersucht haben. Da wir eine 2x2 Kreuztabelle vorliegen haben, haben wir \\(r = 2\\) und \\(c = 2\\) und somit ist das Minimum von \\(r-1\\) und \\(c-1\\) gleich \\(1\\). Wir erhalten ein Cramers \\(V\\) von \\(0.14\\) was für einen schwachen Effekt spricht. Wir können uns auch grob an der folgenden Tabelle der Effektstärken für Carmers \\(V\\) orientieren.\n\n\n\nschwach\nmittel\nstark\n\n\nCramers \\(V\\)\n\n0.1\n0.3\n0.5\n\n\nZum anderen haben wir noch die Möglichkeit die Odds Ratios oder das Chancenverhältnis zu berechnen. Die Odds Ratios lassen sich direkter als Effekt interpretieren als Cramers \\(V\\) haben aber den Nachteil, dass wir die Odds Ratios nur auf einer 2x2 Kreuztabelle berechnen können. Wichtig bei der Berechnung der Odds Ratios und der anschließenden Interpretation ist die obere Zeile der 2x2 Kreuztabelle. Die obere Zeile ist der Bezug für die Interpretation. Wir nutzen folgende Formel für die Berechnung der Odds Ratios.\n\\[\n\\mbox{Odds Ratio} = OR = \\cfrac{a\\cdot d}{b \\cdot c} = \\cfrac{23\\cdot 14}{10 \\cdot 18} = 1.79\n\\]\nEs ist zwingend notwendig für die folgenden Interpretation der Odds Ratios, dass in den Spalten links die \\(ja\\) Spalte steht und rechts die \\(nein\\) Spalte. Ebenso interpretieren wie die Odds Ratios im Bezug zur oberen Zeile. In unserem Fall ist also die Chance sich mit Flöhen zu infizieren bei Hunden 1.79 mal größer als bei Katzen. Diese Interpretation ist nur korrekt, wenn die 2x2 Kreuztabelle wie beschrieben erstellt ist!"
  },
  {
    "objectID": "stat-tests-chi-test.html#mathcalx2-test-in-r",
    "href": "stat-tests-chi-test.html#mathcalx2-test-in-r",
    "title": "28  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n28.5 \\(\\mathcal{X}^2\\)-Test in R",
    "text": "28.5 \\(\\mathcal{X}^2\\)-Test in R\nDer \\(\\mathcal{X}^2\\)-Test wird meist in anderen Funktionen in R verwendet und nicht direkt. Wenn du Fragen dazu hast, schreib mir einfach eine Mail.\nWenn wir den \\(\\mathcal{X}^2\\)-Test in R rechnen wollen nutzen wir die Funktion chisq.test(), die eine Matrix von Zahlen verlangt. Dies ist etwas umständlich. Wir müssen nur beachten, dass wir die Matrix so bauen, wie wir die Matrix auch brauchen. Deshalb immer mal doppelt schauen, ob deine Matrix auch deinen beobachteten Werten entspricht.\n\nmat &lt;- matrix(c(23, 10, 18, 14), \n              byrow = TRUE, nrow = 2,\n              dimnames = list(animal = c(\"dog\", \"cat\"),\n                              infected = c(\"yes\", \"no\")))\nchisq.test(mat, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  mat\nX-squared = 1.2613, df = 1, p-value = 0.2614\n\n\nAls ein mögliches Effektmaß können wir Cramers \\(V\\) berechnen. Wir nutzen hierzu die Funktion cramers_v(). Auf einer reinen 2x2 Kreuztabelle wäre aber Pearsons \\(\\phi\\) durch die Funktion phi() vorzuziehen. Siehe dazu auch \\(\\phi\\) and Other Contingency Tables Correlations auf der Hilfeseite des R Paketes effectsize. Wir bleiben hier dann aber bei Cramers \\(V\\).\n\ncramers_v(mat) \n\nCramer's V (adj.) |       95% CI\n--------------------------------\n0.06              | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass der Effekt mit einem \\(V = 0.06\\) schwach ist. Ein Wert von 0 bedeutet keine Assoziation und ein Wert von 1 einen maximalen Zusammenhang. Wir können die Werte von \\(V\\) wie eine Korrelation interpretieren. Der in R berechnete Wert unterscheidet sich von unseren händsichen berechneten Wert, da wir hier mit dem adjustierten Cramers \\(V\\) rechnen. Wir würden auch in der Anwendung den adjustierten Wert verwenden, aber für das Verständnis reicht der händisch berechnete Wert.\nHaben wir eine geringe Zellbelegung von unter 5 in einer der Zellen der 2x2 Kreuztabelle, dann verwenden wir den Fisher Exakt Test. Der Fisher Exakt Test hat auch den Vorteil, dass wir direkt die Odds Ratios wiedergegeben bekommen. Wir können auch den Fisher Exakt Test rechnen, wenn wir viele Beobachtungen pro Zelle haben und einfach an die Odds Ratios rankommen wollen. Der Unterschied zwischen dem klassischen \\(\\mathcal{X}^2\\)-Test und dem Fisher Exakt Test ist in der praktischen Anwendung nicht so groß.\n\nfisher.test(mat)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  mat\np-value = 0.3102\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.5756546 5.6378438\nsample estimates:\nodds ratio \n   1.77279 \n\n\nWir sehen auch hier den nicht signifikanten \\(p\\)-Wert sowie eine Odds Ratio von 1.77. Hunde haben aso eine um 1.77 höhere Chance sich mit Flöhen zu infizieren."
  },
  {
    "objectID": "stat-tests-chi-test.html#test-auf-gleiche-oder-gegebene-anteile",
    "href": "stat-tests-chi-test.html#test-auf-gleiche-oder-gegebene-anteile",
    "title": "28  Der \\(\\mathcal{X}^2\\)-Test",
    "section": "\n28.6 Test auf gleiche oder gegebene Anteile",
    "text": "28.6 Test auf gleiche oder gegebene Anteile\nNeben dem Test auf absolute Anteile, wie der \\(\\mathcal{X}^2\\)-Test es rechnet, wollen wir manchmal auch relative Anteile testen. Also haben wir nicht 8 kranke Erdbeeren gezählt sondern 8 kranke von 12 Erdbeeren. Damit sind dann 66% bzw. 0.66 kranke Erdbeeren vorhanden. Wir rechnen also mit Wahrscheinlichkeiten, also eng. proportions. Deshalb können wir hier auch die R Funktion prop.test() nutzen. Wichtig ist zu wissen, dass wir trotz allem erst die Anzahl an beschädigten Erdbeeren \\(x\\) sowie die absolute Anzahl an Erdbeeren \\(n\\) brauchen. Das wird jetzt aber gleich in dem Beispiel etwas klarer. Im Prinzip ist der prop.test() also eine andere Art den \\(\\mathcal{X}^2\\)-Test zu rechnen.\n\n28.6.1 Vergleich eines Anteils \\(p_1\\) gegen einen gegebenen Anteil \\(p_0\\)\n\nNehmen wir ein etwas konstruiertes Beispiel zur Erdbeerernte. Wir haben einen neuen Roboter entwickelt, der Erdbeeren erntet. Nun stellen wir fest, dass von 100 Erdbeeren 76 heile sind. Jetzt lesen wir im Handbuch, dass der Ernteroboter eigentlich 84% der Erdbeeren heile ernten sollte. Sind jetzt 76 von 100, also 76%, signifikant unterschiedlich von 84%? Oder können wir die Nullhypothese der Gleichheit zwischen den beiden Wahrscheinlichkeiten nicht ablehnen? Damit können wir den Test auf Anteile nutzen, wenn wir eine beobachtete Wahrscheinlichkeit oder Anteil \\(p_1\\) gegen eine gegebene Wahrscheinlichkeit oder Anteil \\(p_0\\) vergleichen wollen.\nIn unserem Fall haben wir mit \\(p_1\\) die Wahrscheinlichkeit vorliegen, dass die Erdbeeren in unserem Experiment heile sind. Wir wissen also, dass \\(p_1 = \\cfrac{x}{n} = \\cfrac{76}{100} = 0.76 = 76\\%\\) ist. Damit ist die Wahrscheinlichkeit \\(p_1\\) auch die beobachte Wahrscheinlichkeit. Wir erwarten auf der anderen Seite die Wahrscheinlichkeit \\(p_0 = 0.84 = 84\\%\\). Die Roboterfirma hat uns aj zugesagt, dass 84% der Erdbeeren heile bleiben sollen.\nWir berechnen jetzt einmal die beobachten Werte. Zum einen haben wir \\(x=76\\) heile Erdbeeren von \\(n=100\\) Erdbeeren gezählt. Damit ergeben sich dann \\(x = 76\\) heile Erdbeeren und \\(24\\) beschädigte Erdbeeren. Die Summe muss ja am Ende wieder 100 Erdbeeren ergeben.\n\\[\n\\begin{aligned}\nO &=\n\\begin{pmatrix}\nx &|& n - x\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n76 &|& 100 - 76\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\n76 &|& 24\n\\end{pmatrix}\n\\end{aligned}\n\\]\nDann müssen wir uns noch die Frage stellen, welche Anzahl an heilen Erdbeeren hätten wir erwartet? In diesem Fall ja 84% heile Erdbeeren. Das macht dann bei 100 Erdbeeren \\(0.84 \\cdot 100 = 84\\) heile Erdbeeren und \\((1 - 0.84) \\cdot 100 = 16\\) beschädigte Erdbeeren.\n\\[\n\\begin{aligned}\nE &=\n\\begin{pmatrix}\nn \\cdot p &|& n \\cdot (1 - p)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n100 \\cdot 0.84 &|& 100 \\cdot (1 - 0.84)\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}\n84 &|& 16\n\\end{pmatrix}\n\\end{aligned}\n\\]\nJetzt müssen wir nur noch die beobachteten Anteile mit den zu erwarteten Anteilen durch die \\(\\mathcal{X}^2\\)-Formel in Kontext setzten. Wir subtrahieren von jedem beobachten Anteil den zu erwartenden Anteil, quadrieren und addieren auf. Dann erhalten wir die \\(\\mathcal{X}^2\\)-Statistik.\n\\[\n\\chi^2_{calc} = \\sum\\cfrac{(O-E)^2}{E} = \\cfrac{(76 - 84)^2}{84} + \\cfrac{(24 - 16)^2}{16} = 4.76\n\\]\nWir können diese einfache Berechnung dann auch schnell nochmal in R durchführen. Wir setzten dafür einfach x, n und p fest und berechnen dann die beobachten Anteile \\(O\\) sowwie die zu erwartenden Anteile \\(E\\). Wir berechnen dann hier auch den gleichen Wert für \\(\\mathcal{X}^2\\)-Statistik.\n\nx &lt;- 76\nn &lt;- 100\np &lt;- 0.84\nO &lt;- cbind(x, n - x)\nE &lt;- cbind(n * p, n * (1 - p))\nsum((abs(O - E))^2/E)\n\n[1] 4.761905\n\n\nUnd wie immer gibt es auch eine Funktion prop.test(), die uns ermöglicht einen beobachteten Anteil x/n zu einem erwarteten Anteil p zu vergleichen. Auch hier sehen wir, dass sich die \\(\\mathcal{X}^2\\)-Statistik aus der R Funktion nicht von unser berechneten \\(\\mathcal{X}^2\\)-Statistik unterscheidet.\n\nprop.test(x = 76, n = 100, p = 0.84, correct = FALSE) \n\n\n    1-sample proportions test without continuity correction\n\ndata:  76 out of 100, null probability 0.84\nX-squared = 4.7619, df = 1, p-value = 0.0291\nalternative hypothesis: true p is not equal to 0.84\n95 percent confidence interval:\n 0.6676766 0.8330867\nsample estimates:\n   p \n0.76 \n\n\n\n\nWas ist die Yates Korrektur, die wir mit correct = TRUE auswählen? Die Korrektur von Yates subtrahiert von jedem Summanden des Chi-Quadrat-Tests 0.5, so dass die Chi-Quadrat-Statistik geringer ausfällt.\n\n28.6.2 Vergleich zweier Anteile \\(p_1\\) und \\(p_2\\)\n\nNun haben wir nicht einen Ernteroboter sondern zwei brandneue Robotertypen. Einmal die Marke Picky und einmal den Roboter Colly. Wir wollen jetzt wieder bestimmen, wie viel Erdbeeren bei der Ernte beschädigt werden. Erdbeeren sind ja auch ein sehr weiches Obst. Wir vergleichen hier also wieder zwei Anteile miteinander. Der Roboter Picky beschädigt 76 von 100 Erdbeeren und der Roboter Colly detscht 91 von 100 Erdbeeren an. Das sind ganz schön meise Werte, aber was will man machen, wir haben jetzt nur die beiden Roboter vorliegen. Die Frage ist nun, ob sich die beiden Roboter in der Häufigkeit der beschädigten Erdbeeren unterscheiden. Wir können hier eine 2x2 Kreuztabelle in der Tabelle 28.5 aufmachen und die jeweiligen Anteile berechnen. Picky beschädigt 76% der Erdbeeren und Colly ganze 91%.\n\n\nTabelle 28.3— Eine 2x2 Tabelle für die zwei Ernteroboter und der beschädigten Erdbeeren. Dargestellt sind die beobachteten Werte.\n\n\n\n\n\n\n\n\n\n\n\nDamaged\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\nRobot\nPicky\n\\(76\\)\n\\(24\\)\n\\(\\mathbf{100}\\)\n\n\n\nColly\n\\(91\\)\n\\(9\\)\n\\(\\mathbf{100}\\)\n\n\n\n\n\\(\\mathbf{167}\\)\n\\(\\mathbf{33}\\)\n\\(n = 200\\)\n\n\n\n\nWir können die erwarteten Anteile jetzt wie schon bekannt berechnen oder aber wir nutzen folgende Formel um \\(\\hat{p}\\), die Wahrscheinlichkeit für ein Ereignis, zu berechnen. Wir nutzen dann \\(\\hat{p}\\) um die erwarteten Werte \\(E\\) aus zurechnen. Dafür addieren wir alle beobachteten \\(x\\) zusammen und teilen diese Summe durch die gesammte Anzahl an Beobachtungen.\n\\[\n\\hat{p} = \\cfrac{\\sum x}{\\sum n} = \\cfrac{79 + 91}{100 + 100} = \\cfrac{170}{200} = 0.85\n\\]\nIm Folgenden die Rechenschritte nochmal in R aufgedröselt zum besseren nachvollziehen. Wie auch schon im obigen Beispiel berechnen wir erst die beobachten Anteil \\(O\\) sowie die erwartenden Anteile \\(E\\). Dann nutzen wir die Formel des \\(\\mathcal{X}^2\\)-Test um die \\(\\mathcal{X}^2\\)-Statistik zu berechnen.\n\nx &lt;- c(76, 91)\nn &lt;- c(100, 100)\np &lt;- sum(x)/sum(n)\nO &lt;- cbind(x, n - x)\nE &lt;- cbind(n * p, n * (1 - p))\nsum((abs(O - E))^2/E)\n\n[1] 8.165487\n\n\nAuch hier vergleichen wir nochmal unser händisches Ergebnis mit dem Ergebnis der R Funktion prop.test(). Der Funktion übergeben wir dann einmal die beobachteten Anteile \\(x\\) sowie dann die jeweils Gesamtanzahlen \\(n\\). Wichtig ist hier, dass wir als 95% Konfidenzintervall die Differenz der beiden Wahrscheinlichkeiten erhalten.\n\nprop.test(x = c(76, 91), n = c(100, 100), correct = FALSE) \n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(76, 91) out of c(100, 100)\nX-squared = 8.1655, df = 1, p-value = 0.00427\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.25076198 -0.04923802\nsample estimates:\nprop 1 prop 2 \n  0.76   0.91 \n\n\nWir wir sehen unterscheiden sich die beiden Anteil von \\(76/100\\) gleich 76% von \\(91/100\\) gleich 91%. Damit sollten wir den Roboter Picky nehmen, denn da werden prozentual weniger Erdbeeren zermanscht. Ob das jetzt gut oder schlecht ist 76% Erdbeeren zu zerstören ist aber wieder eine Frage, die die Statistik an dieser Stelle nicht beantworten kann.\n\n28.6.3 Vergleich mehrerer Anteile \\(p_1\\), \\(p_2\\) bis \\(p_k\\)\n\nIm Folgenden haben wir jetzt nicht mehr zwei Gruppen, die wir miteinander vergleichen wollen, sondern mehrere Behandlungen als Gruppen. Wir haben uns in einem Experiment die beschädigten Erdbeeren nach vier neue Erntearten, A bis D, angeschaut. Beide Vektoren können wir dann in die Funktion prop.test() stecken.\n\ndamaged  &lt;- c(A = 105, B = 100, C = 139, D = 96)\nberries &lt;- c(106, 113, 156, 102)\nprop.test(damaged, berries)\n\n\n    4-sample test for equality of proportions without continuity correction\n\ndata:  damaged out of berries\nX-squared = 11.747, df = 3, p-value = 0.008303\nalternative hypothesis: two.sided\nsample estimates:\n   prop 1    prop 2    prop 3    prop 4 \n0.9905660 0.8849558 0.8910256 0.9411765 \n\n\nWir erhalten dann einen \\(p\\)-Wert für die Signifikanz von 0.0056 wieder. Was testen wir hier eigentlich? Unsere Nullhypothese ist, dass alle paarweisen Wahrscheinlichkeiten zwischen den Gruppen gleich sind.\n\\[\nH_0: \\; \\mbox{Der Anteil der beschädigten Erdbeeren ist in den vier Gruppen ähnlich hoch}\n\\]\n\\[\nH_A: \\; \\mbox{Der Anteil der beschädigten Erdbeeren in mindestens einer der Behandlungen unterschiedlich ist.}\n\\]\nWie wir sehen ist das nicht der Fall. Wir haben hier vier Wahrscheinlichkeiten vorliegen und mindestens zwei unterscheiden sich. Welche das sind, ist wieder die Frage. Hierzu nutzen wir dann gleich die Funktion pairwise.prop.test(). Wir immer geht die Ausgabe auch schöner und aufgeräumter.\n\nprop.test(damaged, berries) %&gt;% \n  model_parameters()\n\n4-sample test for equality of proportions without continuity correction\n\nProportion                        | Chi2(3) |     p\n---------------------------------------------------\n99.06% / 88.50% / 89.10% / 94.12% |   11.75 | 0.008\n\nAlternative hypothesis: two.sided\n\n\nWarum ist ein Test auf Anteile ein \\(\\mathcal{X}^2\\)-Test? Hierfür brauchen wir noch die Informationen zu den nicht beschädigten Erdbeeren.\n\nnon_damaged &lt;- berries - damaged \nnon_damaged\n\n A  B  C  D \n 1 13 17  6 \n\n\nNun können wir uns erstmal eine Tabelle bauen auf der wir dann den \\(\\mathcal{X}^2\\)-Test und den prop.test() rechnen können. Der \\(\\mathcal{X}^2\\)-Test ist nicht nur auf eine 2x2 Kreuztabelle beschränkt. Wir können in einem \\(\\mathcal{X}^2\\)-Test auch andere \\(n \\times m\\) Tabellen testen. Auf der anderen Seite ist der prop.test() auf eine \\(n \\times 2\\) Tabelle beschränkt. Es müssen also immer zwei Spalten sein.\n\ndamaged_tab &lt;- cbind(damaged, non_damaged) %&gt;% \n  as.table()\ndamaged_tab\n\n  damaged non_damaged\nA     105           1\nB     100          13\nC     139          17\nD      96           6\n\n\nWir erhalten die Tabelle 28.4 mit den beobachteten Werten sowie die Tabelle 28.5 mit den erwarteten Werten. Die Berechnung der erwarteten Werte kennen wir schon aus dem klassischen \\(\\mathcal{X}^2\\)-Test. Hier machen wir die Berechnungen nur auf einer größeren Tabelle.\n\n\nTabelle 28.4— Eine 4x2 Tabelle für die vier Erntearten und der beschädigten Erdbeeren. Dargestellt sind die beobachteten Werte.\n\n\n\n\nDamaged\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\n\n\ndamaged\nnon_damaged\nberries\n\n\nGroup\nA\n\\(105\\)\n\\(1\\)\n\\(\\mathbf{106}\\)\n\n\n\nB\n\\(100\\)\n\\(13\\)\n\\(\\mathbf{113}\\)\n\n\n\nC\n\\(139\\)\n\\(17\\)\n\\(\\mathbf{156}\\)\n\n\n\nD\n\\(96\\)\n\\(6\\)\n\\(\\mathbf{102}\\)\n\n\n\n\n\\(\\mathbf{440}\\)\n\\(\\mathbf{37}\\)\n\\(n = 477\\)\n\n\n\n\nJetzt können wir auch die Anteile der beschädigten Erdbeeren von allen Erdbeeren berechnen pro Ernteart berechnen.\n\\[\n\\begin{aligned}\nPr(A) &= \\cfrac{105}{106} = 0.9906 = 99.06\\%\\\\\nPr(B) &= \\cfrac{100}{113} = 0.8850 = 88.50\\%\\\\\nPr(C) &= \\cfrac{139}{156} = 0.8910 = 89.10\\%\\\\\nPr(D) &= \\cfrac{96}{102} = 0.9411 = 94.12\\%\n\\end{aligned}\n\\]\n\n\nTabelle 28.5— Eine 4x2 Tabelle für die vier Erntearten und der beschädigten Erdbeeren. Dargestellt sind die zu erwartenden Werte, die sich aus den Randsummen ergeben würden.\n\n\n\n\n\n\n\n\n\n\n\nDamaged\n\n\n\n\n\n\nYes (1)\nNo (0)\n\n\n\n\n\ndamaged\nnon_damaged\nberries\n\n\nGroup\nA\n\\(\\cfrac{440 \\cdot 106}{477} = 97.78\\)\n\\(\\cfrac{37 \\cdot 106}{477} = 8.22\\)\n\\(\\mathbf{106}\\)\n\n\n\nB\n\\(\\cfrac{440 \\cdot 113}{477} = 104.23\\)\n\\(\\cfrac{37 \\cdot 113}{477} = 8.77\\)\n\\(\\mathbf{113}\\)\n\n\n\nC\n\\(\\cfrac{440 \\cdot 156}{477} = 143.90\\)\n\\(\\cfrac{37 \\cdot 156}{477} = 12.10\\)\n\\(\\mathbf{156}\\)\n\n\n\nD\n\\(\\cfrac{440 \\cdot 102}{477} = 94.09\\)\n\\(\\cfrac{37 \\cdot 102}{477} = 7.91\\)\n\\(\\mathbf{102}\\)\n\n\n\n\n\\(\\mathbf{440}\\)\n\\(\\mathbf{37}\\)\n\\(n = 477\\)\n\n\n\n\nSchauen wir uns nun an, ob es einen Unterschied zwischen den vier Erntearten A bis D für die Erdbeeren gibt. Einmal nutzen wir hierfür die Funktion chisq.test() und einmal die Funktion prop.test().\n\nchisq.test(damaged_tab) %&gt;% \n  model_parameters()\n\nPearson's Chi-squared test\n\nChi2(3) |     p\n---------------\n11.75   | 0.008\n\nprop.test(damaged_tab) %&gt;% \n  model_parameters()\n\n4-sample test for equality of proportions without continuity correction\n\nProportion                        | Chi2(3) |     p\n---------------------------------------------------\n99.06% / 88.50% / 89.10% / 94.12% |   11.75 | 0.008\n\nAlternative hypothesis: two.sided\n\n\nNachdem wir beide Funktionen gerechnet haben, sehen wir, dass beide Tests auf der \\(\\mathcal{X}^2\\) Statistik basieren. Das macht ja auch Sinn, denn wir rechnen ja die Proportions indem wir die beobachteten Werte durch die Gesamtzahl berechnen. Hier haben wir im Prinzip die gleiche Idee wie schon in den beiden obigen Beispielen umgesetzt. Wir können daher den \\(\\mathcal{X}^2\\)-Test auch einmal per Hand rechnen und kommen auf fast die gleiche \\(\\mathcal{X}^2\\)-Statistik. Wir haben eine leichte andere Statistik, da wir hier mehr runden.\n\\[\n\\begin{aligned}\n\\chi^2_{calc} &= \\cfrac{(105 - 97.78)^2}{97.78} + \\cfrac{(1 - 8.22)^2}{8.22} + \\\\\n&\\phantom{=}\\;\\; \\cfrac{(100 - 104.23)^2}{104.23} + \\cfrac{(13 - 8.77)^2}{8.77} + \\\\\n&\\phantom{=}\\;\\; \\cfrac{(139 - 143.90)^2}{143.90} + \\cfrac{(17 - 12.10)^2}{12.10} + \\\\\n&\\phantom{=}\\;\\; \\cfrac{(96 - 94.09)^2}{94.09} + \\cfrac{(6 - 7.91)^2}{7.91} = 11.74 \\approx 10.04\n\\end{aligned}\n\\]\nWir wissen nun, dass es mindestens einen paarweisen Unterschied zwischen den Wahrscheinlichkeiten für eine Beschädigung der Erdbeeren der vier Behandlungen gibt.\nFühren wir den Test nochmal detaillierter mit der Funktion prop_test() aus dem R Paket rstatix durch. Es handelt sich hier um eine Alternative zu der Standardfunktion prop.test(). Das Ergebnis ist das Gleiche, aber die Aufarbeitung der Ausgabe ist anders und manchmal etwas besser weiterverarbeiteten.\nDer Zugang ist etwas anders, deshalb bauen wir uns erstmal eine Tabelle mit den beschädigten und nicht beschädigten Erdbeeren. Dann benennen wir uns noch die Tabelle etwas um, damit haben wir dann einen besseren Überblick. Eigentlich unnötig, aber wir wollen uns hier ja auch mal mit der Programmierung beschäftigen.\n\ndamaged_tab &lt;- rbind(damaged, non_damaged) %&gt;% \n  as.table()\ndimnames(damaged_tab) &lt;- list(\n  Infected = c(\"yes\", \"no\"),\n  Groups = c(\"A\", \"B\", \"C\", \"D\")\n)\ndamaged_tab \n\n        Groups\nInfected   A   B   C   D\n     yes 105 100 139  96\n     no    1  13  17   6\n\n\nDann können wir die Funktion prop_test() nutzen um den Test zu rechnen. Wir erhalten hier viele Informationen und müssen dann schauen, was wir dann brauchen. Dafür nutzen wir dann die Funktion select() und mutieren dann die Variablen in der Form, wie wir die Variablen haben wollen.\n\nprop_test(damaged_tab, detailed = TRUE) %&gt;% \n  select(matches(\"estimate\"), p) %&gt;% \n  mutate(p = pvalue(p)) %&gt;% \n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 1 × 5\n  estimate1 estimate2 estimate3 estimate4 p    \n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;\n1      0.99      0.88      0.89      0.94 0.008\n\n\nWir erhalten auch hier das gleiche Ergebnis. War auch zu erwarten, denn im Kern sind die beiden Funktionen prop.test() und prop_test() gleich. Nun können wir uns einmal den paarweisen Vergleich anschauen. Wir wollen dann im Anschluß noch das Compact letter display für die Darstellung der paarweisen Vergleiche berechnen. Dafür brauchen wir dann noch folgende R Pakete.\n\n\nDu kannst mehr über das Compact letter display in dem Kapitel Multiple Vergleiche oder Post-hoc Tests erfahren.\n\npacman::p_load(rcompanion, multcompView, multcomp)\n\nDie Standardfunktion für die paarweisen Vergleiche von Anteilen in R ist die Funktion pairwise.prop.test(). Wir wollen hier einmal die \\(p\\)-Werte nicht adjustieren, deshalb schreiben wir auch p.adjust.method = \"none\".\n\npairwise.prop.test(damaged, berries, \n                   p.adjust.method = \"none\") \n\n\n    Pairwise comparisons using Pairwise comparison of proportions \n\ndata:  damaged out of berries \n\n  A      B      C     \nB 0.0035 -      -     \nC 0.0040 1.0000 -     \nD 0.1118 0.2264 0.2466\n\nP value adjustment method: none \n\n\nWir können auch die Funktion pairwise_prop_test() aus dem R Paket rstatix nutzen. Hier haben wir dann eine andere Ausgabe der Vergleiche. Manchmal ist diese Art der Ausgabe der Ergebnisse etwas übersichlicher. Wir nutzen dann noch die Funktion pvalue() um die \\(p\\)-Werte einmal besser zu formatieren.\n\npairwise_prop_test(damaged_tab, \n                   p.adjust.method = \"none\") %&gt;% \n  mutate(p = pvalue(p.adj))\n\n# A tibble: 6 × 5\n  group1 group2 p        p.adj p.adj.signif\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 A      B      0.004  0.00354 **          \n2 A      C      0.004  0.00401 **          \n3 B      C      &gt;0.999 1       ns          \n4 A      D      0.112  0.112   ns          \n5 B      D      0.226  0.226   ns          \n6 C      D      0.247  0.247   ns          \n\n\nDann benutzen wir noch die Funktion multcompLetters() um uns das Compact letter display wiedergeben zu lassen.\n\npairwise.prop.test(damaged, berries, \n                   p.adjust.method = \"none\") %&gt;% \n  pluck(\"p.value\") %&gt;% \n  fullPTable() %&gt;% \n  multcompLetters() %&gt;% \n  pluck(\"Letters\")\n\n   A    B    C    D \n \"a\"  \"b\"  \"b\" \"ab\" \n\n\nHier sehen wir dann auch den Unterschied zwischen den beiden Funktionen. Wir können für die Funktion pairwise_prop_test() etwas einfacher das Compact letter display berechnen lassen. Wir müssen uns nur eine neue Spalte contrast mit den Vergleichen bauen.\n\npairwise_prop_test(damaged_tab, \n                   p.adjust.method = \"none\") %&gt;% \n  mutate(contrast = str_c(group1, \"-\", group2)) %&gt;% \n  pull(p, contrast) %&gt;% \n  multcompLetters() \n\n   A    B    C    D \n \"a\"  \"b\"  \"b\" \"ab\" \n\n\nAm Ende sehen wir, dass sich die Behandlung \\(A\\) von den Behandlungen \\(B\\) und \\(C\\) unterscheidet, da sich die Behandlungen nicht den gleichen Buchstaben teilen. Die Behandlung \\(A\\) unterscheidet sich aber nicht von der Behandlung \\(D\\). In dieser Art und Weise können wir dann alle Behandlungen durchgehen. Du kannst mehr über das Compact letter display in dem Kapitel Multiple Vergleiche oder Post-hoc Tests erfahren."
  },
  {
    "objectID": "stat-tests-diagnostic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-diagnostic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "29  Der diagnostische Test",
    "section": "\n29.1 Genutzte R Pakete für das Kapitel",
    "text": "29.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, \n               pROC, readxl)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-diagnostic.html#die-daten-für-das-diagnostische-testen",
    "href": "stat-tests-diagnostic.html#die-daten-für-das-diagnostische-testen",
    "title": "29  Der diagnostische Test",
    "section": "\n29.2 Die Daten für das diagnostische Testen",
    "text": "29.2 Die Daten für das diagnostische Testen\nEs ist unabdingbar, dass oben links in der 2x2 Kreuztabelle immer die \\(T^+\\) und \\(K^+\\) Werte stehen. Sonst funktionieren alle Formeln in diesem Kapitel nicht.\nDie Datentabelle für das diagnostische Testen basiert wie der \\(\\chi^2\\)-Test auf der 2x2 Kreuztabelle oder Verfeldertafel. Es ist dabei unabdingbar, dass oben links in der 2x2 Kreuztabelle immer die \\(T^+\\) und \\(K^+\\) Werte stehen. Sonst funktionieren alle Formeln in diesem Kapitel nicht. Wir schauen usn auch immer das Schlechte an. Daher wollen wir immer wissen, ist der Hund krank? Ist der Hund tot? Ist der Hund weggelaufen? Beide Voraussetzung sind wichtig, damit wir mit der 2x2 Kreuztabelle wie in Tabelle 29.1 gezeigt rechnen können.\n\n\nTabelle 29.1— Eine 2x2 Tabelle oder Vierfeldertafel\n\n\n\n\n\n\n\n\n\n\n\nKrank\n\n\n\n\n\n\n\n\\(K^+\\) (1)\n\n\\(K^-\\) (0)\n\n\n\nTest\n\n\\(T^+\\) (1)\n\\(TP_{\\;\\Large a}\\)\n\\(FN_{\\;\\Large b}\\)\n\\(\\mathbf{a+b}\\)\n\n\n\n\n\\(T^-\\) (0)\n\\(FP_{\\;\\Large c}\\)\n\\(TN_{\\;\\Large d}\\)\n\\(\\mathbf{c+d}\\)\n\n\n\n\n\\(\\mathbf{a+c}\\)\n\\(\\mathbf{b+d}\\)\n\\(\\mathbf{n}\\)\n\n\n\n\nWir wollen die Tabelle 29.1 mit einem Beispiel von Tollwut an Hauskatzen in ländlicher Umgebung. Die Katzen haben also Auslauf und können sich auch mit Tollwut infizieren. Wir wollen einen neuen, nicht invasiven Labortesten Tollda darauf überprüfen, wie gut der diagnostische Test Tollwut bei Katzen im Frühstadium erkennt.\nWir haben jetzt folgende Informationen erhalten:\n\nDer diagnostische Test TollDa ist positiv \\(T^+\\), wenn Tollwut vorliegt \\(K^+\\) , in 80% der Fälle.\nDer diagnostische Test TollDa ist positiv \\(T^+\\), wenn keine Tollwut vorliegt \\(K^-\\), in 9.5% der Fälle.\nAbschließend haben noch 2% der Katzen in ländlicher Umgebung Tollwut. Wir haben eine Prävalenz der Erkrankung in der betrachteten Population von 2%.\n\nDie Halterin einer Katze möchte nun wissen, wie groß ist dei Wahrscheinlichkeit bei einem positiven Testergebnis, dass meine Katze Tollwut hat. Also die bedingte Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests.\n\n\n\nAbbildung 29.1— Visualisierung der Informationen zur Tollwutdiagnostik in einem Doppelbaum. Gefragt ist nach der Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests.\n\n\nAbbildung 29.1 visualisert unsere Frage in einem Doppelbaum. Wir haben \\(10000\\) Katzen vorliegen. Ja wir waren fleißig und wir können damit besser rechnen. Du siehst aber auch, für die Diagnostik brauchen wir eine Menge Beobachtungen.\nVon den \\(10000\\) Katzen sind 2% also \\(200\\) wirklich mit Tollwut infiziert, also haben den Status \\(K^+\\). Damit sind \\(9800\\) Katzen gesund oder nicht krank und haben den Status \\(K^-\\). Wir wissen jetzt, dass 80% der \\(K^+\\) Katzen als postitiv vom Test erkannt werden. Damit werden \\(200 \\cdot 0.8 = 160\\) Katzen \\(T^+\\). Im Umkehrschluss sind die anderen \\(40\\) Katzen dann \\(T^-\\). Von den \\(9800\\) gesunden Katzen werden 9.5% fälsch als krank erkannt, also \\(9800 \\cdot 0.095 = 931\\) Katzen. Wiederum im Umkehrschluss sind dann \\(9069\\) richtig als gesunde Tiere erkannt.\nWir können nun den Doppelbaum nach unten ergänzen. Wir haben damit \\(1091 = 160 + 931\\) positiv getestete Katzen \\(T^+\\) sowie \\(9109 = 40 + 9069\\) negativ getestete Katzen \\(T^-\\).\nWie groß ist nun die Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) oder die Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests? Wir können die Wahrscheinlichkeit \\(Pr(K^+|T^+)\\) direkt im Baum ablesen. Wir haben \\(160\\) kranke und positive Tier. Insgesamt sind \\(1091\\) Tiere positiv getestet. Daher Wahrscheinlichkeit krank zu sein gegeben eines positiven Tests \\(\\tfrac{160}{1091} = 0.147\\) oder nur 14%.\nSammeln wir unsere Informationen um die Tabelle 29.1 zu füllen. Wir haben insgesamt \\(n = 10000\\) Katzen untersucht. In Tabelle 29.2 sehen wir die Ergebnisse unseres Tests auf Tollwut zusammengefasst. Wir haben \\(160\\) Katzen die Tollwut haben und vom Test als krank erkannt wurden. Dann haben wir \\(931\\) Katzen, die keine Tollwut hatten und vom Test als positiv erkannt wurden. Darüber hinaus haben wir \\(40\\) Katzen, die Tollwut haben, aber ein negatives Testergebnis. Sowie \\(8109\\) gesunde Katzen mit einem negativen Testergebnis.\n\n\nTabelle 29.2— Eine 2x2 Tabelle oder Vierfeldertafel gefüllt mit dem Beispiel aus dem Doppelbaum.\n\n\n\n\n\n\n\n\n\n\n\nKrank\n\n\n\n\n\n\n\n\\(K^+\\) (1)\n\n\\(K^-\\) (0)\n\n\n\nTest\n\n\\(T^+\\) (1)\n160\n931\n\\(\\mathbf{a+b} = 1091\\)\n\n\n\n\n\\(T^-\\) (0)\n40\n9069\n\\(\\mathbf{c+d} = 9109\\)\n\n\n\n\n\\(\\mathbf{a+c} = 200\\)\n\\(\\mathbf{b+d} = 9800\\)\n\\(\\mathbf{n} = 10000\\)\n\n\n\n\nWie du sehen kannst ist die mittlere Reihe des Doppelbaums nichts anderes als die ausgefüllte 2x2 Kreuztabelle. In Abbildung 29.2 sehen wir die letzte Visualisierung des Zusammenhangs von Testscore auf der x-Achse und der Verteilung der tollwütigen Katzen \\(K^+\\) und der gesunden Katzen \\(K^-\\). Links und rechts von der Testenstscheidung werden Katzen falsch klassifiziert. Das heißt, die Katzen werden als krank oder gesund von dem Test erkannt, obwohl die Katzen diesen Status nicht haben. Ein idelr Test würde zwei Verteilungen der kranken und gesunden Tiere hrvorbringen, die sich perfekt separieren lassen. Es gibt anhand des Testscores keine Überlappung der beiden Verteilungen\n\n\n\nAbbildung 29.2— Visualisierung des Zusammenhangs zwischen Testscore und den Verteilungen der kranken und gesunden Katzen sowie der Testenstscheidung. Links und rechts von der Testenstscheidung werden Katzen falsch klassifiziert."
  },
  {
    "objectID": "stat-tests-diagnostic.html#confusion-matrix-deu.-fehlermatrix",
    "href": "stat-tests-diagnostic.html#confusion-matrix-deu.-fehlermatrix",
    "title": "29  Der diagnostische Test",
    "section": "\n29.3 Confusion matrix (deu. Fehlermatrix)",
    "text": "29.3 Confusion matrix (deu. Fehlermatrix)\n\n\nDie 2x2 Kreuztabelle wird als Confusion matrix viel auch in der Klassifikation im machinellen Lernen genutzt. Im maschinellen Lernen heißt es dann meist nur anders…\nIn der Statistik beziehungsweise der Epidemiologie gibt es eine Vielzahl an statistischen Maßzahlen für die 2x2 Kreuztabelle. In der Fachsprache wird die 2x2 Kreuztabelle auch Confusion matrix genannt. Auf der Confusion Matrix können viele Maßzahlen berechnet werden wir konzentrieren uns hier erstmal auf die zwei wichtigsten Maßzahlen, nämlich der Spezifität und der Sensitivität.\nIn der wissenschaftlichen Fachsprache hat ein diagnostischer Test oder eine Methode, die erkrankte Personen sehr zuverlässig als krank (\\(K^+\\)) erkennt, eine hohe Sensitivität. Das heißt, sie übersieht kaum erkrankte Personen. Ein Test, der gesunde Personen zuverlässig als gesund (\\(K^-\\)) einstuft, hat eine hohe Spezifität. Wir können die Spezifität und die Sensitivität auf der 2x2 Kreuztabelle wir folgt berechnen.\n\\[\n\\mbox{Sensitivität} = \\mbox{sens} = \\cfrac{TP}{TP + FN} = \\cfrac{160}{160 + 931} = 0.147\n\\]\n\\[\n\\mbox{Spezifität} = \\mbox{spec} = \\cfrac{TN}{TN + FP} = \\cfrac{8869}{8869 + 40} = 0.995\n\\]\nBeide statistischen Maßzahlen benötigen wir im Besonderen bei der Erstellung der Reciever Operator Curve (ROC) im folgenden Abschnitt."
  },
  {
    "objectID": "stat-tests-diagnostic.html#receiver-operating-characteristic-roc",
    "href": "stat-tests-diagnostic.html#receiver-operating-characteristic-roc",
    "title": "29  Der diagnostische Test",
    "section": "\n29.4 Receiver Operating Characteristic (ROC)",
    "text": "29.4 Receiver Operating Characteristic (ROC)\nDie Reciever Operator Curve (ROC) ist die Abbildung wenn es darum geht darzustellen wie gut eine Klassifikation funktioniert hat. Wir werden die Abbildung später im maschinellen Lernen und der Klssifikation wiedertreffen. Da die ROC Kurve aber ursprünglich zum diagnostischen Testen gehört, kommt die ROC Kurve hier auch rein.\n\n29.4.1 Daten für die Receiver Operating Characteristic (ROC)\nSchauen wir uns erstmal ein Beispiel für die ROC Kurve an. In Tabelle 29.3 sehen wir Beispieldaten von vierzehn Hunden. Von den vierzehn Hunden haben sieben eine verdeckte Flohinfektion im Anfangsstadium und sieben Hunde sind flohfrei und gesund. Daher haben wir sieben kranke Hunde (\\(K^+\\) oder Infektionsstatus ist \\(1\\)) und sieben gesunde Hunde (\\(K^-\\) oder Infektionsstatus ist \\(0\\)). Wir testen jeden der Hunde mit dem Flohindikatormittel FleaDa. Das Flohindikatormittel gibt einen Farbwert als test_score zwischen \\(0\\) und \\(1\\) wieder. Wir wollen nun herausfinden, ob wir mit dem test_score die vierzehn Hunde korrekt nach ihrem Krankheitszustand klassifizieren können.\n\n\n\n\nTabelle 29.3— Datensatz für vierzehn Hunde mit Flohinfektion und keiner Flohinfektion sowie dem Testscore des Flohindikatormittels fleaDa.\n\ninfected\ntest_score\n\n\n\n1\n0.100\n\n\n1\n0.150\n\n\n1\n0.250\n\n\n1\n0.300\n\n\n1\n0.400\n\n\n1\n0.500\n\n\n1\n0.600\n\n\n0\n0.350\n\n\n0\n0.425\n\n\n0\n0.470\n\n\n0\n0.550\n\n\n0\n0.700\n\n\n0\n0.800\n\n\n0\n0.820\n\n\n\n\n\n\nWir sehen die Daten visualisiert in Abbildung 29.3. Auf der \\(y\\)-Achse ist der binäre Endpunkt \\(infected\\) und auf der \\(x\\)-Achse der Testscore des Flohindikators FleaDa.\n\n\n\n\nAbbildung 29.3— Aufteilung der kranken und gesunden Hunde nach Infektionsstatus und dem Testscore.\n\n\n\nDie Idee der ROC Kurve ist nun für jeden möglichen Wert des Testscores die Spezifität und Sensitivität zu berechnen. Wir müssen das aber nicht für alle Werte des Testscores machen, sondern nur für die Wert bei denen sich der Status einer Beobachtung anhand des Testscores ändern würde. Klingt ein wenig schräg, schauen wir es uns einmal an. Zuerst brauchen wir jeweils die Grenzen an denen wir für den Testscore eine Klassifikation machen. In Abbildung 29.4 sehen wir die Grenzen als gelbe Linie.\n\n\n\n\nAbbildung 29.4— Aufteilung der kranken und gesunden Hunde nach Infektionsstatus und dem Testscore.\n\n\n\nWir können jetzt für jede gelbe Linie als Threshold des Testscore die Spezifität und die Sensitivität berechen. berechnen Tabelle 29.4 sind die Werte von Spezifität und Sensitivität für jede gelbe Linie ausgegeben. Wenn wir als Entscheidungsgrenze einen Testscore von 0.12 nehmen würden, dann würden wir 1 Hund als krank und 13 als gesund klassifizieren. Wir hätten 7 TN, 1 TP, 6 FN und 0 FP. Aus diesen Zahlen, die eine 2x2 Kreuztabelle entsprechen, können wir dann die Spezifität und die Sensitivität berechnen. Wir berechnen dann die Spezifität und die Sensitivität für jeden Threshold.\n\n\n\n\nTabelle 29.4— Die Spezifität und die Sensitivität berechnet für jeden Threshold.\n\n\nthreshold\nspecificity\nsensitivity\ntn\ntp\nfn\nfp\n\n\n\n2\n0.12\n1.00\n0.14\n7\n1\n6\n0\n\n\n3\n0.20\n1.00\n0.29\n7\n2\n5\n0\n\n\n4\n0.28\n1.00\n0.43\n7\n3\n4\n0\n\n\n5\n0.32\n1.00\n0.57\n7\n4\n3\n0\n\n\n6\n0.38\n0.86\n0.57\n6\n4\n3\n1\n\n\n7\n0.41\n0.86\n0.71\n6\n5\n2\n1\n\n\n8\n0.45\n0.71\n0.71\n5\n5\n2\n2\n\n\n9\n0.48\n0.57\n0.71\n4\n5\n2\n3\n\n\n10\n0.52\n0.57\n0.86\n4\n6\n1\n3\n\n\n11\n0.58\n0.43\n0.86\n3\n6\n1\n4\n\n\n12\n0.65\n0.43\n1.00\n3\n7\n0\n4\n\n\n13\n0.75\n0.29\n1.00\n2\n7\n0\n5\n\n\n14\n0.81\n0.14\n1.00\n1\n7\n0\n6\n\n\n\n\n\n\nWir können jetzt für jeden Threshold und damit jedes verbundene Spezifität und Sensitivität Paar einen Punkt in einen Plot einzeichnen. Wir lassen damit Stück für Stück die ROC Kurve wachsen.\nIn der Abbildung 29.5 sehen wir die ROC Kurve mit dem Threshold von 0.32 vorliegen. Wir haben damit 7 TN, 4 TP, 3 FN und 0 FP klassifizierte Beobchtungen. Darauf ergibt sich eine Spezifität von 1 und eine Sensitivität von 0.57. In der ROC Kurve tragen wir auf die \\(x\\)-Achse die 1 - Spezifitätswerte auf. Damit haben wir eine schön ansteigende Kurve.\n\n\n\nAbbildung 29.5— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.32 und damit 7 TN, 4 TP, 3 FN und 0 FP.\n\n\nIn der Abbildung 29.6 istr die ROC Kurve schon nach rechts gewachsen, da wir die ersten falsch positiv (FP) klassifizierten Beobachtungen bei einem Threshold von 0.48 erhalten.\n\n\n\nAbbildung 29.6— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.48 und damit 4 TN, 5 TP, 2 FN und 3 FP.\n\n\nIn Abbildung 29.7 sind wir mit einem Threshold von 0.65 schon fast am rechten Rand der Verteilung des Testscores angekommen. Wir haben nur noch 3 richtig negative (TN) Beobachtungen, die jetzt mit steigenden Threshold alle zu falsch positiven (FP) klassifiziert werden.\n\n\n\nAbbildung 29.7— Visualisierung des Anwachsen der ROC Kurve mit einem Threshold von 0.65 und damit 3 TN, 7 TP, 0 FN und 4 FP.\n\n\nEin AUC von 1 bedeutet eine perfekte Trennung der beiden Gruppen durch den Testscore. Ein AUC von 0.5 bedeutet keine Trennung der beiden Gruppen durch den Testscore.\nDie ROC Kurve endet dann oben rechts in der Abbidlung und startet immer unten links. Wir können noch die Fläche unter der ROC Kurve berechnen. Wir nennen diese Fläche unter der Kurve AUC (eng. area under the curce) und wir brauchen diese Maßzahl, wenn wir verschiedene Testscores und die entsprechenden ROC Kurven miteinander vergleichen wollen. Eine AUC von 0.5 bedeutet, dass unser Testscore nicht in der Lage war die kranken von den gesunden Hunden zu trennen."
  },
  {
    "objectID": "stat-tests-diagnostic.html#reciever-operator-curve-roc-in-r",
    "href": "stat-tests-diagnostic.html#reciever-operator-curve-roc-in-r",
    "title": "29  Der diagnostische Test",
    "section": "\n29.5 Reciever Operator Curve (ROC) in R",
    "text": "29.5 Reciever Operator Curve (ROC) in R\nWir berechnen die ROC Kurve nicht per Hand. Das wäre eine sehr große Fleißarbeit für jeden Threshold zwischen den Beobachtungen die jeweilige Spezifität und Sensitivität Paare zu berechnen. Wir nutzen daher das R Paket pROC. Das Pakt erlaubt es uns später auch recht einfach ROC Kurven miteinander zu vergleichen und einen statistischen Test zu rechnen.\nDie Daten von den obigen Beispiel sind in der Datei roc_data.xlsx gespeichert und können dann einfach verwendet werden. Die Funktion roc() akzeptiert ein binäres \\(y\\) wo bei die \\(1\\) das Schlechte bedeutet und die \\(0\\) die Abwesenheit von dem Schlechten. Also eben krank oder gesund wie oben beschrieben. Wir wollen dann noch die AUC und die 95% Konfidenzintervalle für das AUC wiedergegeben haben.\n\nroc_tbl &lt;- read_excel(\"data/roc_data.xlsx\") \n\nroc_obj &lt;- roc(infected ~ test_score, roc_tbl, auc = TRUE, ci = TRUE)\n\nroc_obj\n\n\nCall:\nroc.formula(formula = infected ~ test_score, data = roc_tbl,     auc = TRUE, ci = TRUE)\n\nData: test_score in 7 controls (infected 0) &gt; 7 cases (infected 1).\nArea under the curve: 0.8367\n95% CI: 0.6176-1 (DeLong)\n\n\nWir sehen, dass wir eine AUC von 0.837 haben, was auf eine gute Trennschärfe des Tests hindeutet. Wäre die AUC näher an 0.5 dann würden wir sagen, dass der Test die kranken Hunde nicht von den gesunden Hunden unterscheiden kann.\nIm nächsten Schritt extrahieren wir noch alle wichtigen Informationen aus dem Objekt roc_obj damit wir die ROC Kurve in ggplot zeichnen können. Das Paket pROC hat auch eine eigne Plotfunktion. Es gibt eine Reihe on Screenshots vom pROC Paket, wo du noch andere Möglichkeiten siehst. Einfach mal ausprobieren.\n\nroc_res_tbl &lt;- roc_obj %&gt;% \n  coords(ret = \"all\") %&gt;% \n  select(specificity, sensitivity)\n\nDas Objekt roc_obj nutzen wir jetzt um die ROC Kurve einmal darzustellen. Achte drauf, dass auf der \\(x\\)-Achse die 1-Spezifität Werte stehen. Wir erhalten die ROC Kurve wie in Abbildung 29.8 dargestellt. Wie zeichnen noch die Diagonale ein. Wenn die ROC nahe an der Diagonalen verläuft, dann ist der Testscore nicht in der Lage die Kranken von den Gesunden zu trennen. Eine perfekte ROC Kurve läuft senkrecht nach oben und dann waagerecht nach rechts und es ergebit sich eine AUC von 1.\n\nggplot(roc_res_tbl, aes(1-specificity, sensitivity)) +\n  geom_path() +\n  geom_abline(intercept = 0, slope = 1, alpha = 0.75, color = \"red\") +\n  theme_bw() +\n  labs(x = \"1 - Spezifität\", y = \"Sensitivität\")\n\n\n\nAbbildung 29.8— Abbildung der ROC Kurve.\n\n\n\nIn späteren Kapiteln zur Klassifikation werden wir auch verschiedene ROC Kurven zusammen darstellen und miteinander Vergleichen. Dazu nutzen wir dann auch die ROC Kurve und die Möglichkeit auch einen roc.test() zu rechnen. Mehr dazu gibt es soweit erstmal auf der Hilfeseite des R Paketes pROC mit vielen Screenshots vom pROC Paket. Für dieses Kapitel soll die Darstellung einer ROC Kurve genügen."
  },
  {
    "objectID": "stat-tests-pretest.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-pretest.html#genutzte-r-pakete-für-das-kapitel",
    "title": "30  Pre-Tests oder Vortest",
    "section": "\n30.1 Genutzte R Pakete für das Kapitel",
    "text": "30.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, \n               broom, car, performance, \n               see, scales,\n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")"
  },
  {
    "objectID": "stat-tests-pretest.html#pre-test-auf-varianzhomogenität",
    "href": "stat-tests-pretest.html#pre-test-auf-varianzhomogenität",
    "title": "30  Pre-Tests oder Vortest",
    "section": "\n30.2 Pre-Test auf Varianzhomogenität",
    "text": "30.2 Pre-Test auf Varianzhomogenität\nWas will also der Pre-Test auf Varianzhomogenität? Eigentlich ist der Test vollkommen verquer. Zum einen testet der Test auf Varianzhomogenität gar nicht die Anwesenheit von Homogenität. Wir können dank dem Falisifikationsprinzip nur Ablehnen. Deshalb steht in der Nullhypothese die Gleichheit der Varianzen, also Varianzhomogenität und in der Alternativen dann die Varianzheterogenität, als der Unterschied.\nAb wann sollten wir denn die Varianzhomogenität ablehnen? Wenn wir standardmäßig auf 5% testen, dann werden wir zu selten die Varianzhomogenität ablehnen. Daher ist es ratsam in diesem Fall auf ein Signifikanzniveau von \\(\\alpha\\) gleich 20% zu testen. Aber auch in diesem Fall können wir natürlich eine Varianzhomogenität übersehen oder aber eine Varianzhterogenität fälschlicherweise annehmen.\nEs ergeben sich folgende Hypothesen für den Pre-Test auf Varianzhomogenität.\n\\[\n\\begin{aligned}\nH_0: &\\; s^2_A = s^2_B\\\\\nH_A: &\\; s^2_A \\ne s^2_B\\\\\n\\end{aligned}\n\\]\nWir sehen, dass in der Nullhypothese die Gleichheit der Varianzen steht und in der Alternativehypothese der Unterschied, also die Varianzhterogenität.\n\n\n\n\n\n\nEntscheidung zur Varianzhomogenität\n\n\n\nBei der Entscheidung zur Varianzhomogenität gilt folgende Regel. Ist der \\(p\\)-Wert des Pre-Tests auf Varianzhomogenität kleiner als das Signifikanzniveau \\(\\alpha\\) von 20% lehnen wir die Nullhypothese ab. Wir nehmen Varianzheterogenität an.\n\nIst \\(p \\leq \\alpha = 20\\%\\) so nehmen wir Varianzheterogenität an.\nIst \\(p &gt; \\alpha = 20\\%\\) so nehmen wir Varianzhomogenität an.\n\nAuf jeden Fall sollten wir das Ergebnis unseres Pre-Tests auf Varianzhomogenität nochmal visuell bestätigen.\n\n\n\n\n\n\n\n\nEinigermaßen zuverlässig meint, dass wir dann in 1 von 20 Fällen eine Varianzhomogenität ablehnen, obwohl eine Varianzhomogenität vorliegt. Ebenso können wir in 1 von 5 Fällen die Nullhypothese nicht ablehnen, obwohl die Varianzen heterogen sind (siehe auch Kapitel 20.1).\nWir nutzen zum statistischen Testen den Levene-Test über die Funktion leveneTest() oder den Bartlett-Test über die Funktion bartlett.test(). Beide Tests sind in R implementiert und können über das Paket car genutzte werden. Wir werden uns jetzt nicht die Formel anschauen, wir nutzen wenn die beiden Tests nur in R und rechnen nicht selber die Werte nach.\nEinfach ausgedrückt, überprüft der Bartlett-Test die Homogenität der Varianzen auf der Grundlage des Mittelwerts. Dementsprechend ist der Bartlett-Test empfindlicher gegen eine Abweichung von der Normalverteilung der Daten, die er überprüfen soll. Der Levene-Test überprüft die Homogenität der Varianzen auch auf der Grundlage des Mittelwerts. Wir haben aber auch die Wahl, den Median zu nutzen dann ist der Levene-Test robuster gegenüber Ausreißern.\nIm Folgenden wollen wir uns einmal in der Theorie den Levene-Test anschauen. Der Levene-Test ist eigentlich nichts anderes als eine etwas versteckte einfaktorielle ANOVA, aber dazu dann am Ende mehr. Dafür nutzen wir als erstes die folgende Formel um die Teststatistik zu berechnen. Dabei ist \\(W\\) die Teststatistik, die wir zu einer \\(F\\)-Verteilung, die wir schon aus der ANOVA kennen, vergleichen können.\n\\[\nW = \\frac{(N-k)}{(k-1)} \\cdot \\frac{\\sum_{i=1}^k N_i (\\bar{Z}_{i\\cdot}-\\bar{Z}_{\\cdot\\cdot})^2} {\\sum_{i=1}^k \\sum_{j=1}^{N_i} (Z_{ij}-\\bar{Z}_{i\\cdot})^2}\n\\]\nZur Veranschaulichung bauen wir uns einen simplen Datensatz mit \\(N = 14\\) Beobachtungen für \\(k = 2\\) Tierarten mit Hunden und Katzen. Damit hat jede Tierart \\(7\\) Beobachtungen der Sprunglängen der jeweiligen Hunde- und Katzenflöhe.\n\nanimal_tbl &lt;- tibble(dog = c(5.7, 8.9, 11.8, 8.2, 5.6, 9.1, 7.6),\n                     cat = c(3.2, 2.2, 5.4, 4.1, 1.1, 7.9, 8.6))\nanimal_tbl\n\n# A tibble: 7 × 2\n    dog   cat\n  &lt;dbl&gt; &lt;dbl&gt;\n1   5.7   3.2\n2   8.9   2.2\n3  11.8   5.4\n4   8.2   4.1\n5   5.6   1.1\n6   9.1   7.9\n7   7.6   8.6\n\n\nDann berechnen wir uns auch gleich die absoluten Abstände \\(Z_{ij}\\) von jeder Beobachtung zu den jeweiligen Mittelwerten der Gruppe. Wir könnten auch die Abstände zu den jeweiligen Medianen der Gruppe berechnen. Beides ist möglich. Hier sehen wir auch den Unterschied zu der ANOVA, wir berechnen hier nicht die quadratischen Abstände sondern die absoluten Abstände.\n\nz_tbl &lt;- animal_tbl %&gt;% \n  mutate(dog_abs = abs(dog - mean(dog)),\n         cat_abs = abs(cat - mean(cat)))\nz_tbl\n\n# A tibble: 7 × 4\n    dog   cat dog_abs cat_abs\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1   5.7   3.2  2.43     1.44 \n2   8.9   2.2  0.771    2.44 \n3  11.8   5.4  3.67     0.757\n4   8.2   4.1  0.0714   0.543\n5   5.6   1.1  2.53     3.54 \n6   9.1   7.9  0.971    3.26 \n7   7.6   8.6  0.529    3.96 \n\n\nIm Folgenden nochmal in formelschreibweise der Unterschied zwischen den beiden Abstandsberechnungen \\(Z_{ij}\\) für jeden Wert. Wir haben die Wahl zwischen den Abständen von jeder Beobachtung zu dem Mittelwert oder dem Median.\n\\[\nZ_{ij} =\n\\begin{cases}\n|Y_{ij} - \\bar{Y}_{i\\cdot}|, & \\bar{Y}_{i\\cdot} \\text{ ist der Mittelwert der } i\\text{-ten Gruppe}, \\\\\n|Y_{ij} - \\tilde{Y}_{i\\cdot}|, & \\tilde{Y}_{i\\cdot} \\text{ ist der Median der } i\\text{-ten Gruppe}.\n\\end{cases}\n\\]\nBerechnen wir nun die Gruppenmittelwerte \\(\\bar{Z}_{i\\cdot}\\) für die Hunde und die Katzen jeweils einmal separat.\n\nmean(z_tbl$dog_abs)\n\n[1] 1.567347\n\n\n\nmean(z_tbl$cat_abs)\n\n[1] 2.277551\n\n\nDann brauchen wir noch den Mittelwert über alle Beobachtungen hinweg \\(\\bar{Z}_{\\cdot\\cdot}\\). Den können wir aus allen Beoachtungen berechnen oder aber einfach den Mittelwert der beiden Gruppenmittelwerte nehmen.\n\n(1.57 + 2.28)/2\n\n[1] 1.925\n\n\nAm Ende fehlt uns dann noch der Nenner mit der Summe der einzelnen quadratischen Abstände \\(Z_{ij}\\) zu den Abstandsmitteln der einzelnen Gruppen \\(\\bar{Z}_{i\\cdot}\\) mit \\(\\sum_{i=1}^k \\sum_{j=1}^{N_i} (Z_{ij}-\\bar{Z}_{i\\cdot})^2\\). Den Wert können wir dann in R direkt einmal berechnen. Wir nehmen also die Vektoren der Einzelwerte und ziehen dort immer den Mittelwert der Abstände der Gruppe ab. Abschließend summieren wir dann einmal auf.\n\nsum((z_tbl$dog_abs - 1.57)^2)\n\n[1] 10.3983\n\nsum((z_tbl$cat_abs - 2.28)^2)\n\n[1] 11.42651\n\n\nWir können dann alle Zahlen einmal zusammenbringen und in die Formel des Levene-Test einsetzen. Nun rechen wir dann wieder die quadratischen Abstände auf den absoluten Abständen. Ja, das ist etwas wirr, wenn man es zum ersten Mal liest.\n\\[\nW =\n\\cfrac{14-2}{2-1}\\cdot\n\\cfrac{7 \\cdot (1.57 - 1.93)^2 + 7 \\cdot (2.28 - 1.93)^2}\n{10.39 + 11.43} =\n\\cfrac{12}{1} \\cdot \\cfrac{1.76}{21.82} =\n\\cfrac{21.12}{21.82} \\approx 0.968\n\\]\nWir erhalten ein \\(W = 0.968\\), was wir direkt als eine F-Statistik interpretieren können. Schauen wir uns das Ergebnis einmal in der R Funktion leveneTest() aus dem R Paket car an. Wir brauchen dafür einmal die Werte für die Sprungweiten und müssen dann die Daten in das long-Format umbauen und dann rechnen wir den Levene-Test. Wir erhalten fast die numerisch gleichen Werte. Bei uns haben wir etwas gerundet und dann kommt die Abweichung zustande.\n\nz_tbl %&gt;% \n  select(dog, cat) %&gt;% \n  gather(key = animal, value = jump_length) %$% \n  leveneTest(jump_length ~ animal, center = \"mean\")\n\nLevene's Test for Homogeneity of Variance (center = \"mean\")\n      Df F value Pr(&gt;F)\ngroup  1  0.9707  0.344\n      12               \n\n\nDer Levene-Test ist eigentlich nichts anderes als eine einfaktorielle ANOVA auf den absoluten Abständen von den einzelnen Werten zu dem Mittelwert oder dem Median. Das können wir hier einmal nachvollziehen indem wir auf den absoluten Werten einmal eine einfaktorielle ANOVA in R rechnen. Wir erhalten den gleichen F-Wert in beiden Fällen. Eigentlich ist die ANOVA sogar etwas genauer, da wir hier auch die Sum of squares wie auch Mean squares erhalten.\n\nz_tbl %&gt;% \n  select(dog_abs, cat_abs) %&gt;% \n  gather(key = animal, value = jump_length) %$% \n  lm(jump_length ~ animal) %&gt;% \n  anova()\n\nAnalysis of Variance Table\n\nResponse: jump_length\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\nanimal     1  1.7654  1.7654  0.9707  0.344\nResiduals 12 21.8247  1.8187               \n\n\nWir wollen uns nun im Folgenden nun zwei Fälle einmal näher anschauen. Zum einen den Fall, dass wir eine niedrige Fallzahl vorliegen haben und Varianzhomogenität sowie den Fall, dass wir eine niedrige Fallzahl und Varianzheterogenität vorliegen haben. Den Fall, dass wir hohe Fallzahl vorliegen haben, betrachten wir jetzt nicht weiter. In dem Fall funktionieren die Tests einigermaßen zuverlässig."
  },
  {
    "objectID": "stat-tests-pretest.html#varianzen-sind-homogen-fallzahl-niedrig",
    "href": "stat-tests-pretest.html#varianzen-sind-homogen-fallzahl-niedrig",
    "title": "30  Pre-Tests oder Vortest",
    "section": "\n30.3 Varianzen sind homogen, Fallzahl niedrig",
    "text": "30.3 Varianzen sind homogen, Fallzahl niedrig\nWir bauen uns nun einen Datensatz mit zwei Gruppen \\(A\\) und \\(B\\) zu je zehn Beobachtungen. Beide Gruppen kommen aus einer Normalverteilung mit einem Mittelwert von \\(\\bar{y}_A = \\bar{y}_A = 10\\). Darüber hinaus haben wir Varianzhomogenität mit \\(s_A = s_B = 5\\) vorliegen. Ja, wir spezifizieren hier in der Funktion rnorm() die Standardabweichung, aber eine homogene Standardabweichung bedingt eine homogene Varianz und umgekehrt. Abschließend verwandeln wir das Wide-Format noch in das Long-Format um.\n\nset.seed(202209013)\nsmall_homogen_tbl &lt;- tibble(A = rnorm(n = 10, mean = 10, sd = 5),\n                            B = rnorm(n = 10, mean = 10, sd = 5)) %&gt;% \n  gather(trt, rsp) %&gt;% \n  mutate(trt = as_factor(trt))\n\nIn der Abbildung 30.1 sehen wir die Daten aus dem small_homogen_tbl einmal als Boxplot visualisiert.\n\n\n\n\nAbbildung 30.1— Boxplot der beiden Treatment Level A und B. Beide Gruppen haben die gleichen Varianzen. Es liegt Varianzhomogenität vor.\n\n\n\nWir wollen nun die Varianz auf Homogenität testen. Wir nutzen dafür den leveneTest() sowie den bartlett.test(). Beide Tests bieten sich an. Die Daumenregel ist, dass der Bartlett-Test etwas bessere statistische Eigenschaften hat. Dennoch ist der Levene-Test bekannter und wird häufiger angefragt und genutzt. Wir nutzen die Funktion tidy() aus dem Paket broom um die Ausgabe aufzuräumen und selektieren nur den \\(p\\)-Wert.\n\nleveneTest(rsp ~ trt, data = small_homogen_tbl) %&gt;% \n  tidy %&gt;% \n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.345\n\nbartlett.test(rsp ~ trt, data = small_homogen_tbl) %&gt;% \n  tidy %&gt;% \n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.325\n\n\nWir sehen, dass der \\(p\\)-Wert größer ist als das Signifikanzniveau \\(\\alpha\\) von 20%. Damit können wir die Nullhypothese nicht ablehnen. Wir nehmen Varianzhomogenität an. Überdies sehen wir auch, dass sich die \\(p\\)-Werte nicht groß voneinander unterscheiden.\nWir können auch die Funktion check_homogeneity() aus dem Paket performance nutzen. Wir erhalten hier auch gleich eine Entscheidung in englischer Sprache ausgegeben. Die Funktion check_homogeneity() nutzt den Bartlett-Test. Wir können in Funktion auch andere Methoden mit method = c(\"bartlett\", \"fligner\", \"levene\", \"auto\") wählen.\n\nlm(rsp ~ trt, data = small_homogen_tbl) %&gt;% \n  check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.325).\n\n\nWir nutzen das Paket performance für die Modellgüte im Kapitel 33."
  },
  {
    "objectID": "stat-tests-pretest.html#varianzen-sind-heterogen-fallzahl-niedrig",
    "href": "stat-tests-pretest.html#varianzen-sind-heterogen-fallzahl-niedrig",
    "title": "30  Pre-Tests oder Vortest",
    "section": "\n30.4 Varianzen sind heterogen, Fallzahl niedrig",
    "text": "30.4 Varianzen sind heterogen, Fallzahl niedrig\nNun stellt sich die Frage, wie sieht es aus, wenn wir ungleiche Varianzen vorliegen haben. Wir bauen uns nun einen Datensatz mit zwei Gruppen \\(A\\) und \\(B\\) zu je zehn Beobachtungen. Beide Gruppen kommen aus einer Normalverteilung mit einem Mittelwert von \\(\\bar{y}_A = \\bar{y}_A = 12\\). Darüber hinaus haben wir Varianzheterogenität mit \\(s_A = 10 \\ne s_B = 5\\) vorliegen.\n\nset.seed(202209013)\nsmall_heterogen_tbl &lt;- tibble(A = rnorm(10, 10, 12),\n                              B = rnorm(10, 10, 5)) %&gt;% \n  gather(trt, rsp) %&gt;% \n  mutate(trt = as_factor(trt))\n\nIn der Abbildung 30.2 sehen wir die Daten aus dem small_heterogen_tbl einmal als Boxplot visualisiert.\n\n\n\n\nAbbildung 30.2— Boxplot der beiden Treatment Level A und B. Beide Gruppen haben ungleiche Varianzen. Es liegt Varianzheterogenität vor.\n\n\n\nWir wollen nun die Varianz auf Homogenität testen. Wir nutzen dafür den levenTest() sowie den bartlett.test(). Wir können nur die Varianzhomogenität testen, da jeder statistischer Test nur eine Aussage über die Nullhypothese erlaubt. Damit können wir hier nur die Varianzhomogenität testen.\n\nleveneTest(rsp ~ trt, data = small_heterogen_tbl) %&gt;% \n  tidy %&gt;% \n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0661\n\nbartlett.test(rsp ~ trt, data = small_heterogen_tbl) %&gt;% \n  tidy %&gt;% \n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1   0.127\n\n\nWir sehen, dass der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 20%. Damit können wir die Nullhypothese ablehnen. Wir nehmen Varianzheterogenität an. Überdies sehen wir auch, dass sich die \\(p\\)-Werte nicht groß voneinander unterscheiden. Was wir sehen ist, dass wir zu einem Signifikanzniveau von 5% die klare Varianzheterogenität nicht erkannt hätten und immer noch Varianzhomogenität angenommen hätten.\nWir können auch die Funktion check_homogeneity() aus dem Paket performance nutzen. Wir erhalten hier auch gleich eine Entscheidung in englischer Sprache ausgegeben. Die Funktion check_homogeneity() nutzt den Bartlett-Test. Wir können in Funktion auch andere Methoden mit method = c(\"bartlett\", \"fligner\", \"levene\", \"auto\") wählen.\n\nlm(rsp ~ trt, data = small_heterogen_tbl) %&gt;% \n  check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.127).\n\n\nWir sehen, dass sich die Implementierung des Bartlett-Tests in check_homogeneity() nicht von der Funktion bartlett.test() unterscheidet, aber die Entscheidung gegen die Varianzhomogenität zu einem Signifikanzniveau von 5% gefällt wird. Nicht immer hilft einem der Entscheidungtext einer Funktion."
  },
  {
    "objectID": "stat-tests-pretest.html#pre-test-auf-normalverteilung",
    "href": "stat-tests-pretest.html#pre-test-auf-normalverteilung",
    "title": "30  Pre-Tests oder Vortest",
    "section": "\n30.5 Pre-Test auf Normalverteilung",
    "text": "30.5 Pre-Test auf Normalverteilung\nWir treffen bei dem Test auf die Normalverteilung auch auf das gleiche Problem wie bei dem Pre-Test zur Varianzhomogenität. Wir haben wieder die Gleichheit, also das unser beobachtetes Outcome gleich einer Normalverteilung ist, in der Nullhypothese stehen. Den Unterschied, also das unser beobachtetes Outcome nicht aus einer Normalverteilung kommmt, in der Alternative.\n\\[\n\\begin{aligned}\nH_0: &\\; \\mbox{y ist gleich normalverteilt}\\\\\nH_A: &\\; \\mbox{y ist nicht gleich normalverteilt}\\\\\n\\end{aligned}\n\\]\nNun ist es aber so, dass es nicht nur zwei Verteilungen gibt. Es gibt mehr als die Normalverteilung und die Nicht-normalverteilung. Wir haben eine große Auswahl an möglichen Verteilungen und seit den 90zigern des letzten Jahrhunderts auch die Möglichkeiten andere Verteilungen des Outcomes \\(y\\) zu modellieren. Leider fällt dieser Fortschritt häufig unter den Tisch und wir bleiben gefangen zwischen der Normalverteilung oder eben keiner Normalverteilung.\n\n\nDer zentrale Grenzwertsatz besagt, dass wenn ein \\(y\\) von vielen Einflussfaktoren \\(x\\) bestimmt wird, man von einem normalverteilten \\(y\\) ausgehen.\nDas Gewicht wird von vielen Einflussfaktoren wie Sport, Kalorienaufnahme oder aber Veranlagung sowie vielem mehr bestimmt. Wir können davon ausgehen, dass das Gewicht normalverteilt ist.\nAbschließend sei noch gesagt, dass es fast unmöglich ist, eine Verteilung mit weniger als zwanzig Beobachtungen überhaupt abzuschätzen. Selbst dann können einzelne Beobachtunge an den Rändern der Verteilung zu einer Ablehnung der Normalverteilung führen, obwohl eine Normalverteilung vorliegt.\nDas R Paket oslrr bietet hier noch eine Funktion ols_test_normality(), die es erlaubt mit allen bekannten statistischen Tests auf Normalverteilung zu testen. Wenn der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\), dann können wir die Nullhypothese, dass unsere Daten gleich einer Normalverteilung wären, ablehnen. Die Anwendung kannst du dir in Kapitel 40 anschauen. Um jetzt kurz einen statistischen Engel anzufahren, wir nutzen wenn überhaupt den Shapiro-Wilk-Test oder den Kolmogorov-Smirnov-Test. Für die anderen beiden steigen wir jetzt hier nicht in die Therorie ab.\nAm Ende sei noch auf den QQ-plot verwiesen, mit dem wir auch visuell überprüfen können, ob eine Normalverteilung vorliegt.\n\n\n\n\n\n\nEntscheidung zur Normalverteilung\n\n\n\nBei der Entscheidung zur Normalverteilung gilt folgende Regel. Ist der \\(p\\)-Wert des Pre-Tests auf Normalverteilung kleiner als das Signifikanzniveau \\(\\alpha\\) von 5% lehnen wir die Nullhypothese ab. Wir nehmen eine Nicht-Normalverteilung an.\n\nIst \\(p \\leq \\alpha = 5\\%\\) so nehmen wir Nicht-Normalverteilung von \\(y\\) an.\nIst \\(p &gt; \\alpha = 5\\%\\) so nehmen wir Normalverteilung von \\(y\\) an.\n\nAuf jeden Fall sollten wir das Ergebnis unseres Pre-Tests auf Normalverteilung nochmal visuell bestätigen."
  },
  {
    "objectID": "stat-tests-pretest.html#approximativ-normalverteilt-niedrige-fallzahl",
    "href": "stat-tests-pretest.html#approximativ-normalverteilt-niedrige-fallzahl",
    "title": "30  Pre-Tests oder Vortest",
    "section": "\n30.6 Approximativ normalverteilt, niedrige Fallzahl",
    "text": "30.6 Approximativ normalverteilt, niedrige Fallzahl\nAuch hier schauen wir uns den Fall mit einer niedrigen Fallzahl an. Dafür bauen wir usn erstmal Daten mit der Funktion rt(). Wir ziehen uns zufällig Beobachtungen aus einer t-Verteilung, die approximativ normalverteilt ist. Je höher die Freiheitsgrade df desto näher kommt die t-Verteilung einer Normalverteilung. Mit einem Freiheitsgrad von df = 30 sind wir sehr nah an einer Normalverteilung dran.\n\nset.seed(202209013)\nlow_normal_tbl &lt;- tibble(A = rt(10, df = 30),\n                         B = rt(10, df = 30)) %&gt;% \n  gather(trt, rsp) %&gt;% \n  mutate(trt = as_factor(trt))\n\nIn Abbildung 30.3 sehen wir auf der linken Seite den Dotplot der zehn Beobachtungen aus den beiden Gruppen \\(A\\) und \\(B\\). Wir sehen, dass die Verteilung für das Outcome rsp in etwa normalverteilt ist.\n\n\n\n\n\n(a) Dotplot des Outcomes rsp.\n\n\n\n\n\n(b) Densityplot des Outcomes rsp.\n\n\n\nAbbildung 30.3— Verteilung des Outcomes rsp der zehn Beobachtungen aus den Gruppen \\(A\\) und \\(B\\). Beiden Gruppen kommen aus einer t-Verteilung.\n\n\nWir können den Shapiro-Wilk-Test nutzen um statistisch zu testen, ob eine Abweichung von der Normalverteilung vorliegt. Wir erfahren aber nicht, welche andere Verteilung vorliegt. Wir testen natürlich für die beiden Gruppen getrennt. Die Funktion shapiro.test()kann nur mit einem Vektor von Zahlen arbeiten, daher übergeben wir mit pull die entsprechend gefilterten Werte des Outcomes rsp.\n\nlow_normal_tbl %&gt;% \n  filter(trt == \"A\") %&gt;% \n  pull(rsp) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.9457, p-value = 0.618\n\nlow_normal_tbl %&gt;% \n  filter(trt == \"B\") %&gt;% \n  pull(rsp) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.89291, p-value = 0.1828\n\n\nWir sehen, dass der \\(p\\)-Wert größer ist als das Signifikanzniveau \\(\\alpha\\) von 5% für beide Gruppen. Damit können wir die Nullhypothese nicht ablehnen. Wir nehmen eine Normalverteilung an.\nIn dem folgendem Beispiel sehen wir dann aber, was ich mit in die Ecke testen meine bzw. so lange statistisch zu Testen bis nichts mehr geht."
  },
  {
    "objectID": "stat-tests-pretest.html#nicht-normalverteilt-niedrige-fallzahl",
    "href": "stat-tests-pretest.html#nicht-normalverteilt-niedrige-fallzahl",
    "title": "30  Pre-Tests oder Vortest",
    "section": "\n30.7 Nicht normalverteilt, niedrige Fallzahl",
    "text": "30.7 Nicht normalverteilt, niedrige Fallzahl\nSchauen wir uns jetzt den anderen Fall an. Wir haben jetzt wieder eine niedrige Fallzahl mit je 10 Beobachtungen je Gruppe \\(A\\) und \\(B\\). In diesem Fall kommen die Beobachtungen aber aus einer exponentiellen Verteilung. Wir haben also definitiv keine Normalverteilung vorliegen. Wir generieren uns die Daten mit der Funktion rexp().\n\nset.seed(202209013)\nlow_nonnormal_tbl &lt;- tibble(A = rexp(10, 1/1500),\n                            B = rexp(10, 1/1500)) %&gt;% \n  gather(trt, rsp) %&gt;% \n  mutate(trt = as_factor(trt))\n\nIn Abbildung 30.4 sehen wir auf der linken Seite den Dotplot der zehn Beobachtungen aus den beiden Gruppen \\(A\\) und \\(B\\). Wir sehen, dass die Verteilung für das Outcome für die Behandlung \\(B\\) in etwa normalverteilt ist sowie das das Outcome für die Behandlung \\(A\\) keiner Normalverteilung folgt oder zwei Ausreißer hat. Die Entscheidung was jetzt stimmt ohne zu wissen wie die Daten generiert wurden, ist in der Anwendung meist nicht möglich.\n\n\n\n\n\n(a) Dotplot des Outcomes rsp.\n\n\n\n\n\n(b) Densityplot des Outcomes rsp.\n\n\n\nAbbildung 30.4— Verteilung des Outcomes rsp der zehn Beobachtungen aus den Gruppen \\(A\\) und \\(B\\). Beiden Gruppen kommen aus einer Exponentialverteilung.\n\n\nWir können wieder den Shapiro-Wilk-Test nutzen um statistisch zu testen, ob eine Abweichung von der Normalverteilung vorliegt. Wir erfahren aber nicht, welche andere Verteilung vorliegt. Wir testen natürlich für die beiden Gruppen getrennt.\n\nlow_nonnormal_tbl %&gt;% \n  filter(trt == \"A\") %&gt;% \n  pull(rsp) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.77114, p-value = 0.006457\n\nlow_nonnormal_tbl %&gt;% \n  filter(trt == \"B\") %&gt;% \n  pull(rsp) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.93316, p-value = 0.4797\n\n\nWir sehen, dass der \\(p\\)-Wert für die Behandlung \\(A\\) kleiner ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Damit können wir die Nullhypothese ablehnen. Wir nehmen keine Normalverteilung für Gruppe \\(A\\) an. Auf der anderen Seite sehen wir, dass der \\(p\\)-Wert für die Behandlung \\(B\\) größer ist als das Signifikanzniveau \\(\\alpha\\) von 5%. Damit können wir die Nullhypothese nicht ablehnen. Wir nehmen eine Normalverteilung für Gruppe \\(A\\) an.\nSuper, jetzt haben wir für die eine Gruppe eine Normalverteilung und für die andere nicht. Wir haben uns in die Ecke getestet. Wir können jetzt verschiedene Szenarien vorliegen haben.\n\nWir könnten in der Gruppe \\(A\\) zwei Ausreißer vorliegen haben.\nWir könnten in der Gruppe \\(B\\) zufällig eine Normalverteilung beobachtet haben.\n\nUnd nochmal zum Schluß, einem statistischen Test mit 4 bis 5 Wiederholungen in einer Gruppe zu glauben, ob eine Normalverteilung vorliegt, kannst du auch würfeln…\nLeider wissen wir im echten Leben nicht, aus welcher Verteilung unsere Daten stammen, wir können aber annehmen, dass die Daten einer Normalverteilung folgen oder aber die Daten so transformieren, dass die Daten einer approximativen Normalverteilung folgen. Siehe dazu auch das Kapitel 17 zur Transformation von Daten.\nWenn deine Daten keiner Normalverteilung folgen, dann kann es sein, dass du mit den Effektschätzern ein Problem bekommst. Du erfährst vielleicht, dass du die Nullhypothese ablehnen kannst, aber nicht wie stark der Effekt in der Einheit des gemessenen Outcomes ist."
  },
  {
    "objectID": "stat-tests-pretest.html#testen-der-normalverteilung-in-mehreren-gruppen",
    "href": "stat-tests-pretest.html#testen-der-normalverteilung-in-mehreren-gruppen",
    "title": "30  Pre-Tests oder Vortest",
    "section": "\n30.8 Testen der Normalverteilung in mehreren Gruppen",
    "text": "30.8 Testen der Normalverteilung in mehreren Gruppen\nIm folgenden Beispiel zu den Keimungsraten von verschiedenen Nelkensorten wollen wir einmal testen, ob jede Sorte einer Normalverteilung folgt. Da wir hier insgesamt 20 Sorten vorliegen haben, nutzen wir die Funktion map() aus dem R Paket purrr um hier schneller voranzukommen. Wie immer laden wir erst die Daten und mutieren die Spalten in dem Sinne wie wir die Spalten brauchen.\n\nclove_tbl &lt;- read_excel(\"data/clove_germ_rate.xlsx\") %&gt;% \n  mutate(clove_strain = as_factor(clove_strain),\n         germ_rate = as.numeric(germ_rate))\n\nJetzt können wir die Daten nach der Sorte der Nelken in eine Liste mit der Funktion split() aufspalten und dann auf jedem Listeneintrag einen Shapiro-Wilk-Test rechnen. Dann machen wir uns noch die Ausgabe schöner und erschaffen uns eine decision-Spalte in der wir gleich das Ergebnis für oder gegen die Normalverteilung ablesen können.\n\nclove_tbl %&gt;% \n  split(.$clove_strain) %&gt;% \n  map(~shapiro.test(.x$germ_rate)) %&gt;% \n  map(tidy) %&gt;% \n  bind_rows(.id = \"test\") %&gt;%\n  select(test, p.value) %&gt;% \n  mutate(decision = ifelse(p.value &lt;= 0.05, \"reject normal\", \"normal\"),\n         p.value = pvalue(p.value, accuracy = 0.001))\n\n# A tibble: 20 × 3\n   test          p.value decision     \n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;        \n 1 standard      0.272   normal       \n 2 west_rck_1    0.272   normal       \n 3 south_III_V   0.855   normal       \n 4 west_rck_2_II 0.653   normal       \n 5 comb_001      0.103   normal       \n 6 western_4     0.849   normal       \n 7 north_549     0.855   normal       \n 8 subtype_09    0.983   normal       \n 9 subtype_III_4 0.051   normal       \n10 ctrl_pos      0.992   normal       \n11 ctrl_7        0.683   normal       \n12 trans_09_I    0.001   reject normal\n13 new_xray_9    0.406   normal       \n14 old_09        0.001   reject normal\n15 recon_1       0.100   normal       \n16 recon_3456    0.001   reject normal\n17 east_new      0.907   normal       \n18 east_old      0.161   normal       \n19 south_II_U    0.048   reject normal\n20 west_3_cvl    0.272   normal"
  },
  {
    "objectID": "stat-tests-posthoc.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-tests-posthoc.html#genutzte-r-pakete-für-das-kapitel",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.1 Genutzte R Pakete für das Kapitel",
    "text": "31.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom,\n               multcomp, emmeans, ggpubr, multcompView,\n               rstatix, conflicted, see, rcompanion)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-posthoc.html#daten",
    "href": "stat-tests-posthoc.html#daten",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.2 Daten",
    "text": "31.2 Daten\nWir nutzen in diesem Kapitel den Datensatz aus dem Beispiel in Kapitel 5. Wir haben als Outcome die Sprunglänge in [cm] von Flöhen. Die Sprunglänge haben wir an Flöhen von Hunde, Katzen und Füchsen gemessen. Der Datensatz ist also recht übeerschaubar. Wir haben ein normalverteiltes \\(y\\) mit jump_length sowie einen multinomialverteiltes \\(y\\) mit grade und einen Faktor animal mit drei Leveln.\nDu kannst dir komplexere Auswertungen im Anhang A anschauen. Dort sammelt sich mit der Zeit Auswertungen vom Fachbereich an. Daher finden sich dort auch Beispiele für multiple Vergleiche.\nIm Folgenden laden wir den Datensatz flea_dog_cat_fox.csv und selektieren mit der Funktion select() die benötigten Spalten. Abschließend müssen wir die Spalte animalnoch in einen Faktor umwandeln. Damit ist unsere Vorbereitung des Datensatzes abgeschlossen.\n\nfac1_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox.csv\") %&gt;%\n  select(animal, jump_length, grade) %&gt;% \n  mutate(animal = as_factor(animal))\n\nIn der Tabelle 31.1 ist der Datensatz fac1_tbl nochmal dargestellt.\n\n\n\n\nTabelle 31.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length sowie der multinominalverteilten Variable grade und einem Faktor animal mit drei Leveln.\n\nanimal\njump_length\ngrade\n\n\n\ndog\n5.7\n8\n\n\ndog\n8.9\n8\n\n\ndog\n11.8\n6\n\n\ndog\n8.2\n8\n\n\ndog\n5.6\n7\n\n\ndog\n9.1\n7\n\n\ndog\n7.6\n9\n\n\ncat\n3.2\n7\n\n\ncat\n2.2\n5\n\n\ncat\n5.4\n7\n\n\ncat\n4.1\n6\n\n\ncat\n4.3\n6\n\n\ncat\n7.9\n6\n\n\ncat\n6.1\n5\n\n\nfox\n7.7\n5\n\n\nfox\n8.1\n4\n\n\nfox\n9.1\n4\n\n\nfox\n9.7\n5\n\n\nfox\n10.6\n4\n\n\nfox\n8.6\n4\n\n\nfox\n10.3\n3\n\n\n\n\n\n\nWir werden nun den Datensatz fac1_tbl in den folgenden Abschnitten immer wieder nutzen.\n\n31.2.1 Hypothesen für multiple Vergleiche\nAls wir eine ANOVA gerechnet hatten, hatten wir nur eine Nullhypothese und eine Alternativehypothese. Wenn wir Nullhypothese abgelehnt hatten, wussten wir nur, dass sich mindestens ein paarweiser Vergleich unterschiedet. Multiple Vergleich lösen nun dieses Problem und führen ein Hypothesenpaar für jeden paarweisen Vergleich ein. Zum einen rechnen wir damit \\(k\\) Tests und haben damit auch \\(k\\) Hypothesenpaare (siehe auch Kapitel 20.3 zur Problematik des wiederholten Testens).\nWenn wir zum Beispiel alle Level des Faktors animal miteinander Vergleichen wollen, dann rechnen wir \\(k=3\\) paarweise Vergleiche. Im Folgenden sind alle drei Hypothesenpaare dargestellt.\n\\[\n\\begin{aligned}\nH_{01}: &\\; \\bar{y}_{cat} = \\bar{y}_{dog}\\\\\nH_{A1}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{dog}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{02}: &\\; \\bar{y}_{cat} = \\bar{y}_{fox}\\\\\nH_{A2}: &\\; \\bar{y}_{cat} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nH_{03}: &\\; \\bar{y}_{dog} = \\bar{y}_{fox}\\\\\nH_{A3}: &\\; \\bar{y}_{dog} \\ne \\bar{y}_{fox}\\\\\n\\end{aligned}\n\\]\nWenn wir drei Vergleiche rechnen, dann haben wir eine \\(\\alpha\\) Inflation vorliegen. Wir sagen, dass wir für das multiple Testen adjustieren müssen. In R gibt es eine Reihe von Adjustierungsverfahren. Wir nehmen meist Bonferroni oder das Verfahren, was in der jeweiligen Funktion als Standard (eng. default) gesetzt ist.\nWir adjustieren grundsätzlich die \\(p\\)-Werte und erhalten adjustierte \\(p\\)-Werte aus den jeweiligen Funktionen in R. Die adjustierten p-Werte können wir dann mit dem Signifikanzniveau von \\(\\alpha\\) gleich 5% vergleichen."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "href": "stat-tests-posthoc.html#sec-posthoc-pairwise",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.3 Gruppenvergleiche mit pairwise.*.test()\n",
    "text": "31.3 Gruppenvergleiche mit pairwise.*.test()\n\nDie Funktion pairwise.*.test() ist veraltet, wir nutzen das R Paket emmeansoder das R Paket multcomp.\nWenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, dann können wir die Funktion pairwise.*.test() nutzen. Der Stern * steht entweder als Platzhalter für t für den t-Test oder aber für wilcox für den Wilcoxon Test. Die Funktion ist relativ einfach zu nutzen und liefert auch sofort die entsprechenden p-Werte.\nDie Funktion pairwise.*.test() ist in dem Sinne veraltet, da wir keine 95% Konfidenzintervalle generieren können. Da die Funktion aber immer mal wieder angefragt wird, ist die Funktion hier nochmal aufgeführt.\n\n31.3.1 Paarweiser t Test\nWir nutzen den paarweisen t-Test,\n\nwenn wir ein normalverteiltes \\(y\\) vorliegen haben, wie jump_length.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\nwenn wir Varianzhomogenität über alle Level vorliegen haben, dann nutzen wir die Option pool.sd = TRUE. Wenn wir Varianzheterogenität über alle Level des Faktors vorliegen haben, dann nutzen wir pool.sd = FALSE.\n\nDie Funktion pairwise.t.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und daher auch fehleranfällig. Wir können auch den %$%-Operator aus dem R Paket magrittr nutzen um die Problematik zu umgehen. Weiter unten siehst du dann einmal beide Anwendungen.\n\n\nMehr zu mutate_if() erfährst du auf der Hilfeseite von mutate()\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein sauberes tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.t.test(fac1_tbl$jump_length, fac1_tbl$animal,\n                p.adjust.method = \"bonferroni\",\n                pool.sd = TRUE) %&gt;% \n  tidy %&gt;% \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 × 3\n  group1 group2 p.value\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 cat    dog      0.007\n2 fox    dog      0.876\n3 fox    cat      0.001\n\n\nUnd das ganze nochmal mit dem %$%-Operator aus dem R Paket magrittr. In diesem Fall können wir uns das $ sparen und greifen direkt auf die Spalten des Datensatzes zu.\n\nfac1_tbl %$%\n  pairwise.t.test(jump_length, animal,\n                  p.adjust.method = \"bonferroni\",\n                  pool.sd = TRUE) %&gt;% \n  tidy %&gt;% \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 × 3\n  group1 group2 p.value\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 cat    dog      0.007\n2 fox    dog      0.876\n3 fox    cat      0.001\n\n\nWir erhalten in einem Tibble die adjustierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wenn du keine adjustierten p-Werte möchtest, dann nutze die Option p.adjust.method = \"none\". Wir sehen, dass der Gruppenvergleich cat - dog signifikant ist, der Gruppenvergleich fox - dog nicht signifikant ist und der Gruppenvergleich fox - cat wiederum signifikant ist.\nLeider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket ggpubr anschauen. Die Hilfeseite des R Pakets ggpubr liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\).\nUm die Abbildung 31.1 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen t-Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert einer einfaktoriellen ANOVA als globalen Test ergänzen.\n\nstat_test_obj &lt;- compare_means(\n jump_length ~ animal, data = fac1_tbl,\n method = \"t.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np &lt;- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"jump_length\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(13, 16, 19)) +\n  stat_compare_means(label.y = 20, method = \"anova\")    \n\n\n\nAbbildung 31.1— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem t-Test und den Bonferroni adjustierten p-Werten.\n\n\n\n\n31.3.2 Paarweiser Wilcoxon Test\nWir nutzen den paarweisen Wilxocon Test,\n\nwenn wir ein nicht-normalverteiltes \\(y\\) vorliegen haben, wie grade.\nwenn wir nur einen Faktor mit mehr als zwei Leveln vorliegen haben, wie animal.\n\nDie Funktion pairwise.wilcox.test kann nicht mit Datensätzen arbeiten sondern nur mit Vektoren. Daher können wir der Funktion auch keine formula übergeben sondern müssen die Vektoren aus dem Datensatz mit fac1_tbl$jump_length für das Outcome und mit fac1_tbl$animal für die Gruppierende Variable benennen. Das ist umständlich und dhaer auch fehleranfällig.\n\n\nMehr zu mutate_if() erfährst du auf der Hilfeseite von mutate()\nAls Adjustierungsmethode für den \\(\\alpha\\) Fehler wählen wir die Bonferroni-Methode mit p.adjust.method = \"bonferroni\" aus. Da wir eine etwas unübersichtliche Ausgabe in R erhalten nutzen wir die Funktion tidy()um die Ausgabe in ein saubers tibble zu verwandeln. Abschließend runden wir noch alle numerischen Spalten mit der Funktion round auf drei Stellen hinter dem Komma.\n\npairwise.wilcox.test(fac1_tbl$grade, fac1_tbl$animal,\n                     p.adjust.method = \"bonferroni\") %&gt;% \n  tidy %&gt;% \n  mutate_if(is.numeric, round, 3)\n\n# A tibble: 3 × 3\n  group1 group2 p.value\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 cat    dog      0.045\n2 fox    dog      0.005\n3 fox    cat      0.011\n\n\nWir erhalten in einem Tibble die adujstierten p-Werte nach Bonferroni. Wir können daher die adjustierten p-Werte ganz normal mit dem Signifikanzniveau \\(\\alpha\\) von 5% vergleichen. Wir sehen, dass der Gruppenvergleich cat - dog knapp signifikant ist, der Gruppenvergleich fox - dog ebenfalls signifkant ist und der Gruppenvergleich fox - cat auch signifkant ist.\nLeider können wir uns keine Konfidenzintervalle wiedergeben lassen, so dass die Funktion nicht dem Stand der Wissenschaft und deren Ansprüchen genügt.\nIm Folgenden wollen wir uns nochmal die Visualisierung mit dem R Paket ggpubr anschauen. Die Hilfeseite des R Pakets ggpubr liefert noch eine Menge weitere Beispiele für den simplen Fall eines Modells \\(y ~ x\\), also von einem \\(y\\) und einem Faktor \\(x\\).\nUm die Abbildung 31.2 zu erstellen müssen wir als erstes die Funktion compare_mean() nutzen um mit der formula Syntax einen Wilcoxon Test zu rechnen. wir adjustieren die p-Werte nach Bonferroni. Anschließend erstellen wir einen Boxplot mit der Funktion ggboxplot() und speichern die Ausgabe in dem Objekt p. Wie in ggplot üblich können wir jetzt auf das Layer p über das +-Zeichen noch weitere Layer ergänzen. Wir nutzen die Funktion stat_pvalue_manual() um die asjustierten p-Werte aus dem Objekt stat_test_obj zu ergänzen. Abschließend wollen wir noch den p-Wert eines Kruskal Wallis als globalen Test ergänzen.\n\nstat_test_obj &lt;- compare_means(\n grade ~ animal, data = fac1_tbl,\n method = \"wilcox.test\",\n p.adjust.method = \"bonferroni\"\n)\n\np &lt;- ggboxplot(data = fac1_tbl, x = \"animal\", y = \"grade\",\n               color = \"animal\", palette =c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n               add = \"jitter\", shape = \"animal\")\n\np + stat_pvalue_manual(stat_test_obj, label = \"p.adj\", y.position = c(10, 13, 16)) +\n  stat_compare_means(label.y = 20, method = \"kruskal.test\")    \n\n\n\nAbbildung 31.2— Boxplot der Sprungweiten [cm] von Hunden und Katzen ergänzt um den paarweisen Vergleich mit dem Wilcoxon Test und den Bonferroni adjustierten p-Werten."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "href": "stat-tests-posthoc.html#sec-posthoc-multcomp",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.4 Gruppenvergleich mit dem multcomp Paket",
    "text": "31.4 Gruppenvergleich mit dem multcomp Paket\nWir drehen hier einmal die Erklärung um. Wir machen erst die Anwendung in R und sollte dich dann noch mehr über die statistischen Hintergründe der Funktionen interessieren, folgt ein Abschnitt noch zur Theorie. Du wirst die Funktionen aus multcomp vermutlich in deiner Abschlussarbeit brauchen. Häufig werden multiple Gruppenvergleiche in Abschlussarbeiten gerechnet.\n\n31.4.1 Gruppenvergleiche mit multcomp in R\n\n\nDie Ausgabe von multcomp können über die Funktion tidy() aufgeräumt werden. Mehr dazu unter der Hilfeseite von tidy() zu multcomp.\nAls erstes brauchen wir ein lineares Modell für die Verwendung von multcomp. Normalerweise verenden wir das gleiche Modell, was wir schon in der ANOVA verwendet haben. Wir nutzen hier ein simples lineares Modell mit nur einem Faktor. Im Prinzip kann das Modell auch größer sein. Du findest immer Beispiel im Anhang A, die dir eventuell dann nochmal zeigen, wie du deine Daten nutzen musst.\n\nfit_1 &lt;- lm(jump_length ~ animal, data = fac1_tbl)\n\nWir haben das Objeckt fit_1 mit der Funktion lm() erstellt. Im Modell sind jetzt alle Mittelwerte und die entsprechenden Varianzen geschätzt worden. Mit summary(fit_1) kannst du dir gerne das Modell auch nochmal anschauen.\n\n\nWenn wir keinen all-pair Vergleich rechnen wollen, dann können wir auch einen many-to-one Vergleich mit dem Dunnett Kontrast rechnen.\nIm Anschluß nutzen wir die Funktion glht() um den multiplen vergleich zu rechnen. Als erstes musst du wissen, dass wenn wir alle Vergleiche rechnen wollen, wir einen all-pair Vergleich rechnen. In der Statistik heißt dieser Typ von Vergleich Tukey. Wir wollen jetzt als für den Faktor animal einen multiplen Tukey-Vergleich rechnen. Nichts anders sagt mcp(animal = \"Tukey\") aus, dabei steht mcp für multiple comparison procedure. Mit dem hinteren Teil der Funktion weiß jetzt die Funktion glht() was gerechnet werden soll. Wir müssen jetzt der Funktion nur noch mitgeben auf was der multiple vergleich gerehcnet werden soll, mit dem Objekt fit_1. Wir speichern die Ausgabe der Funktion in comp_1_obj.\n\ncomp_1_obj &lt;- glht(fit_1, linfct = mcp(animal = \"Tukey\")) \n\nMit dem Objekt comp_1_fit können wir noch nicht soviel anfangen. Der Inhalt ist etwas durcheinander und wir wollen noch die Konfidenzintervalle haben. Daher pipen wir comp_1_fit erstmal in die Funktion tidy() und alssen mit der Option conf.int = TRUE die simultanen 95% Konfidenzintervalle berechnen. Dann nutzen wir die Funktion select() um die wichtigen Spalten zu selektieren. Abschließend mutieren wir noch alle numerischen Spalten in dem wir auf die dritte Kommastelle runden. Wir speichern alles in das Objekt res_1_obj.\n\nres_1_obj &lt;- comp_1_obj %&gt;% \n  tidy(conf.int = TRUE) %&gt;% \n  select(contrast, estimate, adj.p.value, \n         conf.low, conf.high) %&gt;% \n  mutate_if(is.numeric, round, 3)\n\nWir lassen uns dann den Inhalt von dem Objekt res_1_obj ausgeben.\n\nres_1_obj\n\n# A tibble: 3 × 5\n  contrast  estimate adj.p.value conf.low conf.high\n  &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 cat - dog    -3.39       0.006    -5.80    -0.966\n2 fox - dog     1.03       0.535    -1.39     3.45 \n3 fox - cat     4.41       0.001     2.00     6.83 \n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir cat - dog rechnen. Also wirklich der Mittelwert der Sprungweite der Katzenflöhe minus den Mittelwert der Sprungweite der Hundeflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleichecat - dog mit einem p-Wert von \\(0.006\\) sowie für den Vergleich \\(fox - cat\\) mit einem p-Wert von \\(0.001\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nIn Abbildung 31.3 sind die simultanen 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab. Mit der Funktion factor() kannst du die Sortierung der Level einem Faktor ändern und somit auch Position auf den Achsen.\n\n  ggplot(res_1_obj, aes(contrast, y=estimate, \n                        ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.3— Simultane 95% Konfidenzintervalle für den paarweisen Vergleich der Sprungweiten in [cm] der Hunde-, Katzen- und Fuchsflöhe.\n\n\n\nDie Entscheidung gegen die Nullhypothese anhand der simultanen 95% Konfidenzintervalle ist inhaltlich gleich, wie die Entscheidung anhand der p-Werte. Wir entscheiden gegen die Nullhypothese, wenn die 0 nicht mit im Konfindenzintervall enthalten ist. Wir wählen hier die 0 zur Entscheidung gegen die Nullhypothese, weil wir einen Mittelwertsvergleich rechnen.\nFür den Vergleich fox -dog ist die 0 im 95% Konfidenzintervall, wir können daher die Nullhypothese nicht ablehnen. Das 95% Konfidenzintervall ist nicht signifikant. Bei dem Vergleich fox - cat sowie dem Vergleich cat - dog ist jeweils die 0 nicht im 95% Konfidenzintervall enthalten. Beide 95% Konfidenzintervalle sind signifikant, wir können die Nullhypothese ablehnen."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "href": "stat-tests-posthoc.html#sec-posthoc-emmeans",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.5 Gruppenvergleich mit dem emmeans Paket",
    "text": "31.5 Gruppenvergleich mit dem emmeans Paket\n\n\nWir können hier nicht alles erklären und im Detail durchgehen. Hier gibt es noch ein aufwendiges Tutorium zu emmeans: Getting started with emmeans.\nDaneben gibt es auch noch die Einführung mit Theorie auf der Seite des R Paktes\nIm Folgenden wollen wir uns mit einem anderen R Paket beschäftigen was auch multiple Vergleiche rechnen kann. In diesem Kapitel nutzen wir das R Paket emmeans. Im Prinzip kann emmeans das Gleiche wir das R Paket multcomp. Beide Pakete rechnen dir einen multipen Vergleich. Das Paket emmeans kann noch mit nested comparisons umgehen. Deshlb hier nochmal die Vorstellung von emmeans. Du kannst aber für eine simple Auswertung mit nur einem Faktor beide Pakete verwenden.\n\n31.5.1 Gruppenvergleiche mit emmeans in R\n\n\nDie Ausgabe von emmeans können über die Funktion tidy() aufgeräumt werden. Mehr dazu unter der Hilfeseite von tidy() zu emmeans.\nUm den multiplen Vergleich in emmeans durchführen zu können brauchen wir zuerst ein lineares Modell, was uns die notwenidgen Parameter wie Mittelwerte und Standardabweichungen liefert. Wir nutzen in unserem simplen Beispiel ein lineares Modell mit einer Einflussvariable \\(x\\) und nehmen an, dass unser Outcome \\(y\\) normalverteilt ist. Achtung, hier muss natürlich das \\(x\\) ein Faktor sein. Dann können wir ganz einfach die Funktion lm() nutzen. Im Folgenden fitten wir das Modell fit_2 was wir dann auch weiter nutzen werden.\n\nfit_2 &lt;- lm(jump_length ~ animal, data = fac1_tbl)\n\nDer multiple Vergleich in emmeans ist mehrschrittig. Wir pipen unser Modell aus fit_2 in die Funktion emmeans(). Wir geben mit ~ animal an, dass wir über die Level des Faktors animal einen Vergleich rechnen wollen. Wir adjustieren die \\(p\\)-Werte nach Bonferroni. Danach pipen wir weiter in die Funktion contrast() wo der eigentliche Vergleich festgelegt wird. In unserem Fall wollen wir einen many-to-one Vergleich rechnen. Alle Gruppen zu der Gruppe fox. Du kannst mit ref = auch ein anderes Level deines Faktors wählen.\n\ncomp_2_obj &lt;- fit_2 %&gt;% \n  emmeans(~ animal) %&gt;% \n  contrast(method = \"trt.vs.ctrl\", ref = \"fox\", adjust = \"bonferroni\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - fox    -1.03 0.947 18  -1.086  0.5837\n cat - fox    -4.41 0.947 18  -4.660  0.0004\n\nP value adjustment: bonferroni method for 2 tests \n\n\nWir können auch einen anderen Kontrast wählen. Wir überschreiben jetzt das Objekt comp_2_obj mit dem Kontrast all-pair, der alle möglichen Vergleiche rechnet. In emmeans heißt der all-pair Kontrast pairwise.\n\ncomp_2_obj &lt;- fit_2 %&gt;% \n  emmeans(~ animal) %&gt;% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\ncomp_2_obj\n\n contrast  estimate    SE df t.ratio p.value\n dog - cat     3.39 0.947 18   3.574  0.0065\n dog - fox    -1.03 0.947 18  -1.086  0.8756\n cat - fox    -4.41 0.947 18  -4.660  0.0006\n\nP value adjustment: bonferroni method for 3 tests \n\n\nWir können das Ergebnis auch noch mit der Funktion tidy() weiter aufräumen und dann die Spalten selektieren, die wir brauchen. Häufig benötigen wir nicht alle Spalten, die eine Funktion wiedergibt.\n\nres_2_obj &lt;- comp_2_obj %&gt;% \n  tidy(conf.int = TRUE) %&gt;% \n  select(contrast, estimate, adj.p.value, conf.low, conf.high) %&gt;% \n  mutate(across(where(is.numeric), round, 4))\n\nres_2_obj\n\n# A tibble: 3 × 5\n  contrast  estimate adj.p.value conf.low conf.high\n  &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 dog - cat     3.39      0.0065    0.886      5.89\n2 dog - fox    -1.03      0.876    -3.53       1.47\n3 cat - fox    -4.41      0.0006   -6.91      -1.91\n\n\nAbschließend wollen wir noch die 95% Konfidenzintervalle in Abbildung 31.4 abbilden. Hier ist es bei emmeans genauso wie bei multcomp. Wir können das Objekt res_2_obj direkt in ggplot() weiterverwenden und uns die 95% Konfidenzintervalle einmal plotten.\n\n  ggplot(res_2_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1) + \n    geom_point() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.4— Die 95% Konfidenzintervalle für den allpair-Vergleich des simplen Datensatzes.\n\n\n\nWir wollen uns noch einen etwas komplizierteren Fall anschauen, indem sich emmeans von multcomp in der Anwendung unterscheidet. Wir laden den Datensatz flea_dog_cat_fox_site.csv in dem wir zwei Faktoren haben. Damit können wir dann ein Modell mit einem Interaktionsterm bauen. Wir erinnern uns, dass wir in der zweifaktoriellen ANOAV eine signifikante Interaktion zwischen den beiden Faktoren animal und site festgestelt hatten.\n\nfac2_tbl &lt;- read_csv2(\"data/flea_dog_cat_fox_site.csv\") %&gt;% \n  select(animal, site, jump_length) %&gt;% \n  mutate(animal = as_factor(animal),\n         site = as_factor(site))\n\nWir erhalten das Objekt fac2_tbl mit dem Datensatz in Tabelle 31.2 nochmal dargestellt.\n\n\n\n\nTabelle 31.2— Selektierter Datensatz mit einer normalverteilten Variable jump_length und einem Faktor animal mit drei Leveln sowie dem Faktor site mit vier Leveln.\n\nanimal\nsite\njump_length\n\n\n\ncat\ncity\n12.04\n\n\ncat\ncity\n11.98\n\n\ncat\ncity\n16.1\n\n\ncat\ncity\n13.42\n\n\ncat\ncity\n12.37\n\n\ncat\ncity\n16.36\n\n\n…\n…\n…\n\n\nfox\nfield\n16.38\n\n\nfox\nfield\n14.59\n\n\nfox\nfield\n14.03\n\n\nfox\nfield\n13.63\n\n\nfox\nfield\n14.09\n\n\nfox\nfield\n15.52\n\n\n\n\n\n\nIn Abbildung 31.5 sehen wir die Ergebnisse des multiplen Vergleiches nochmal anders als Pairwise P-value plot dargestellt. Wir haben auf der y-Achse zu Abwechselung mal die Gruppen dargestellt und auf der x-Achse die \\(p\\)-Werte. In den Kästchen sind die Effekte der Gruppen nochmal gezeigt. In unserem Fall die Mittelwerte der Sprungweiten für die drei Gruppen. Wir sehen jetzt immer den \\(p\\)-Wert für den jeweiligen Vergleich durch eine farbige Linie miteinander verbunden. So können wir nochmal eine andere Übersicht über das Ergebnis des multiplen Vergleich kriegen.\n\nfit_2 %&gt;% \n  emmeans(~ animal) %&gt;% \n  pwpp(adjust = \"bonferroni\") +\n  theme_bw()\n\n\n\nAbbildung 31.5— Visualisierung der Ergebnisse im Pairwise P-value plot.\n\n\n\nAuch haben wir die Möglichkeit un die \\(p\\)-Werte mit der Funktion pwpm() als eine Matrix ausgeben zu lassen. Wir erhalten in dem oberen Triangel die \\(p\\)-Wert für den jeweiligen Vergleich. In dem unteren Triangel die geschätzten Mittelwertsunterschiede. Auf der Diagonalen dann die geschätzten Mittelwerte für die jeweilige Gruppe. So haben wir nochmal alles sehr kompakt zusammen dargestellt.\n\nfit_2 %&gt;% \n  emmeans(~ animal) %&gt;% \n  pwpm(adjust = \"bonferroni\")\n\n       dog    cat    fox\ndog [8.13] 0.0065 0.8756\ncat   3.39 [4.74] 0.0006\nfox  -1.03  -4.41 [9.16]\n\nRow and column labels: animal\nUpper triangle: P values   adjust = \"bonferroni\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later\n\n\nIn Abbildung 31.6 sehen wir nochmal die Daten visualisiert. Wichtig ist hier, dass wir zwei Faktoren vorliegen haben. Den Faktor animal und den Faktor site. Dabei ist der Faktor animal in dem Faktor site genested. Wir messen jedes Level des Faktors animal jeweils in jedem Level des Faktors site.\n\n\n\n\nAbbildung 31.6— Boxplot der Sprungweiten [cm] von Hunden und Katzen gemessen an verschiedenen Orten.\n\n\n\nWir rechnen ein multiples lineares Modell mit einem Interaktionsterm. Daher packen wir beide Faktoren in das Modell sowie die Intraktion zwischen den beiden Faktoren. Wir erhalten nach dem fitten des Modells das Objekt fit_3.\n\nfit_3 &lt;- lm(jump_length ~ animal + site + animal:site, data = fac2_tbl)\n\nDer Unterschied zu unserem vorherigen multiplen Vergleich ist nun, dass wir auch einen multiplen Vergleich für animal nested in site rechnen können. Dafür müssen wir den Vergleich in der Form animal | site schreiben. Wir erhalten dann die Vergleiche der Level des faktors animal getrennt für die Level es Faktors site.\n\ncomp_3_obj &lt;- fit_3 %&gt;% \n  emmeans(~ animal | site) %&gt;% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\ncomp_3_obj\n\nsite = city:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -3.101 0.771 108  -4.022  0.0003\n cat - fox   -6.538 0.771 108  -8.479  &lt;.0001\n dog - fox   -3.437 0.771 108  -4.457  0.0001\n\nsite = smalltown:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -4.308 0.771 108  -5.587  &lt;.0001\n cat - fox   -4.064 0.771 108  -5.271  &lt;.0001\n dog - fox    0.244 0.771 108   0.316  1.0000\n\nsite = village:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -1.316 0.771 108  -1.707  0.2722\n cat - fox   -1.729 0.771 108  -2.242  0.0809\n dog - fox   -0.413 0.771 108  -0.536  1.0000\n\nsite = field:\n contrast  estimate    SE  df t.ratio p.value\n cat - dog   -0.982 0.771 108  -1.274  0.6167\n cat - fox    1.366 0.771 108   1.772  0.2379\n dog - fox    2.348 0.771 108   3.045  0.0088\n\nP value adjustment: bonferroni method for 3 tests \n\n\nWir können uns das Ergebnis auch etwas schöner ausgeben lassen. Wir nutzen hier noch die Funktion format.pval() um die \\(p\\)-Werte besser zu formatieren. Die \\(p\\)-Wert, die kleiner sind als 0.001 werden als &lt;0.001 ausgegeben und die anderen \\(p\\)-Werte auf zwei Nachstellen nach dem Komma gerundet.\n\ncomp_3_obj %&gt;% \n  summary %&gt;% \n  as_tibble %&gt;% \n  select(contrast, site, p.value) %&gt;% \n  mutate(p.value = format.pval(p.value, eps = 0.001, digits = 2))\n\n# A tibble: 12 × 3\n   contrast  site      p.value\n   &lt;fct&gt;     &lt;fct&gt;     &lt;chr&gt;  \n 1 cat - dog city      &lt;0.001 \n 2 cat - fox city      &lt;0.001 \n 3 dog - fox city      &lt;0.001 \n 4 cat - dog smalltown &lt;0.001 \n 5 cat - fox smalltown &lt;0.001 \n 6 dog - fox smalltown 1.00   \n 7 cat - dog village   0.27   \n 8 cat - fox village   0.08   \n 9 dog - fox village   1.00   \n10 cat - dog field     0.62   \n11 cat - fox field     0.24   \n12 dog - fox field     0.01   \n\n\nIn der Ausgabe können wir erkennen, dass die Vergleich in der Stadt alle signifkant sind. Jedoch erkennen wir keine signifikanten Ergebnisse mehr in dem Dorf und im Feld ist nur der Vergleich dog - fox signifkant. Hier solltest du nochmal beachten, warum wir die Analyse getrennt machen. In der zweifaktoriellen ANOVA haben wir gesehen, dass ein signifkanter Interaktionsterm zwischen den beiden Faktoren animal und site vorliegt.\nWir wollen uns noch über die Funktion confint() die 95% Konfidenzintervalle wiedergeben lassen.\n\nres_3_obj &lt;- comp_3_obj %&gt;% \n  confint() %&gt;% \n  as_tibble() %&gt;% \n  select(contrast, site, estimate, conf.low = lower.CL, conf.high = upper.CL) \n\nres_3_obj\n\n# A tibble: 12 × 5\n   contrast  site      estimate conf.low conf.high\n   &lt;fct&gt;     &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 cat - dog city        -3.10    -4.98     -1.23 \n 2 cat - fox city        -6.54    -8.41     -4.66 \n 3 dog - fox city        -3.44    -5.31     -1.56 \n 4 cat - dog smalltown   -4.31    -6.18     -2.43 \n 5 cat - fox smalltown   -4.06    -5.94     -2.19 \n 6 dog - fox smalltown    0.244   -1.63      2.12 \n 7 cat - dog village     -1.32    -3.19      0.559\n 8 cat - fox village     -1.73    -3.60      0.146\n 9 dog - fox village     -0.413   -2.29      1.46 \n10 cat - dog field       -0.982   -2.86      0.893\n11 cat - fox field        1.37    -0.509     3.24 \n12 dog - fox field        2.35     0.473     4.22 \n\n\nBesonders mit den 95% Konfiendezintervallen sehen wir nochmal den Interaktionseffekt zwischen den beiden Faktoren animal und site. So dreht sich der Effekt von zum Beispiel dog - fox von \\(-3.44\\) in dem Level city zu \\(+2.35\\) in dem Level field. Wir haben eine Interaktion vorliegen und deshalb die Analyse getrennt für jeden Level des Faktors site durchgeführt.\nAbbildung 31.7 zeigt die entsprechenden 95% Konfidenzintervalle. Wir müssen hier etwas mit der position spielen, so dass die Punkte und der geom_errorbar richtig liegen.\n\n  ggplot(res_3_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high,\n                        color = site, group = site)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    scale_color_okabeito() +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.7— Die 95% Konfidenzintervalle für den allpair-Vergleich des Models mit Interaktionseffekt."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-posthoc-ght",
    "href": "stat-tests-posthoc.html#sec-posthoc-ght",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.6 Gruppenvergleich mit dem Games-Howell-Test",
    "text": "31.6 Gruppenvergleich mit dem Games-Howell-Test\nDer Games-Howell-Test ist eine Alternative zu dem Paket multcomp und dem Paket emmeans. Wir nutzen den Games-Howell-Test, wenn die Annahme der Homogenität der Varianzen, der zum Vergleich aller möglichen Kombinationen von Gruppenunterschieden verwendet wird, verletzt ist. Dieser Post-Hoc-Test liefert Konfidenzintervalle für die Unterschiede zwischen den Gruppenmitteln und zeigt, ob die Unterschiede statistisch signifikant sind. Der Test basiert auf der Welch’schen Freiheitsgradkorrektur und adjustiert die \\(p\\)-Werte. Der Test vergleicht also die Differenz zwischen den einzelnen Mittelwertpaaren mit einer Adjustierung für den Mehrfachtest. Es besteht also keine Notwendigkeit, zusätzliche p-Wert-Korrekturen vorzunehmen. Mit dem Games-Howell-Test ist nur ein all-pair Vergleich möglich.\nFür den Games-Howell-Test aus dem Paket rstatix müssen wir kein lineares Modell fitten. Wir schreiben einfach die wie in einem t-Test das Outcome und den Faktor mit den Gruppenleveln in die Funktion games_howell_test(). Wir erhalten dann direkt das Ergebnis des Games-Howell-Test. Wir nutzen in diesem Beispiel die Daten aus dem Objekt fac1_tbl zu sehen in Tabelle 31.1.\n\nfit_4 &lt;- games_howell_test(jump_length ~ animal, data = fac1_tbl) \n\nWir wollen aber nicht mit der Ausgabe arbeiten sondern machen uns noch ein wenig Arbeit und passen die Ausgabe an. Zum einen brauchen wir noch die Kontraste und wir wollen die \\(p\\)-Werte auch ansprechend formatieren. Wir erhalten das Objekt res_4_obj und geben uns die Ausgabe wieder.\n\nres_4_obj &lt;- fit_4 %&gt;% \n  as_tibble %&gt;% \n  mutate(contrast = str_c(group1, \"-\", group2)) %&gt;% \n  select(contrast, estimate, p.adj, conf.low, conf.high) %&gt;% \n  mutate(p.adj = format.pval(p.adj, eps = 0.001, digits = 2))\n\nres_4_obj\n\n# A tibble: 3 × 5\n  contrast estimate p.adj conf.low conf.high\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 dog-cat     -3.39 0.02     -6.28    -0.490\n2 dog-fox      1.03 0.52     -1.52     3.57 \n3 cat-fox      4.41 0.00      2.12     6.71 \n\n\nWir erhalten ein tibble() mit fünf Spalten. Zum einen den contrast, der den Vergleich widerspiegelt, den haben wir uns selber mit der Funktion mutate() und str_c() aus den Spalten group1 und group2 gebaut. Wir vergleichen im ersten Kontrast die Katzen- mit den Hundeflöhen, wobei wir dog-cat rechnen. Also wirklich den Mittelwert der Sprungweite der Hundeflöhe minus den Mittelwert der Sprungweite der Katzenflöhe rechnen. In der Spalte estimate sehen wir den Mittelwertsunterschied. Der Mittelwertsunterschied ist in der Richtung nicht ohne den Kontrast zu interpretieren. Danach erhalten wir die adjustierten \\(p\\)-Wert sowie die simultanen 95% Konfidenzintervalle.\nWir können die Nullhypothese ablehnen für den Vergleiche dog - cat mit einem p-Wert von \\(0.02\\) sowie für den Vergleich \\(cat - fox\\) mit einem p-Wert von \\(0.00\\). Beide p-Werte liegen unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5%.\nIn Abbildung 31.8 sind die simultanen 95% Konfidenzintervalle nochmal in einem ggplot visualisiert. Die Kontraste und die Position hängen von dem Faktorlevel ab.\n\n  ggplot(res_4_obj, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n    geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n    geom_errorbar(width=0.1, position = position_dodge(0.5)) + \n    geom_point(position = position_dodge(0.5)) +\n    coord_flip() +\n    theme_classic()\n\n\n\nAbbildung 31.8— Die 95% Konfidenzintervalle für den allpair-Vergleich des Games-Howell-Test.\n\n\n\nDie Entscheidungen nach den 95% Konfidenzintervallen sind die gleichen wie nach dem \\(p\\)-Wert. Da wir hier es mit einem Mittelwertsvergleich zu tun haben, ist die Entscheidung gegen die Nullhypothese zu treffen wenn die 0 im Konfidenzintervall ist."
  },
  {
    "objectID": "stat-tests-posthoc.html#sec-compact-letter",
    "href": "stat-tests-posthoc.html#sec-compact-letter",
    "title": "31  Multiple Vergleiche oder Post-hoc Tests",
    "section": "\n31.7 Compact letter display",
    "text": "31.7 Compact letter display\nIn der Pflanzenernährung ist es nicht unüblich sehr viele Substrate miteinander zu vergleichen. Oder andersherum, wenn wir sehr viele Gruppen haben, dann kann die Darstellung in einem all-pair Vergleich sehr schnell sehr unübersichltich werden. Deshalb wure das compact letter display entwickelt.\n\n\nCompact Letter Display (CLD) - What is it?. Das compact letter display zeigt an, bei welchen Vergleichen der Behandlungen die Nullhypothese gilt. Daher werden die nicht signifikanten Ergebnisse visualisiert.\nSchauen wir uns aber zurerst einmal ein größeres Beispiel mit neun Behandlungen mit jeweils zwanzig Beobachtungen an. Wir erstellen uns den Datensatz in der Form, dass sich die Mittelwerte für die Behandlungen teilweise unterscheiden.\n\nset.seed(20220914)\ndata_tbl &lt;- tibble(trt = gl(n = 9, k = 20, \n                            labels = c(\"pos_crtl\", \"neg_ctrl\", \"treat_A\", \"treat_B\", \n                                       \"treat_C\", \"treat_D\", \"treat_E\", \"treat_F\", \n                                       \"treat_G\")),\n                   rsp = c(rnorm(20, 10, 5), rnorm(20, 20, 5), rnorm(20, 22, 5), rnorm(20, 24, 5),\n                           rnorm(20, 35, 5), rnorm(20, 37, 5), rnorm(20, 40, 5), rnorm(20, 43, 5),\n                           rnorm(20, 50, 5)))\n\nIn der Abbildung 31.9 ist der Datensatz data_tbl nochmal als Boxplot dargestellt.\n\n\n\n\nAbbildung 31.9— Boxplot der Beispieldaten.\n\n\n\nWir sehen, dass sich die positive Kontrolle von dem Rest der Behandlungen unterscheidet. Danach haben wir ein Plateau mit der negativen Kontrolle und der Behanldung A und der Behandlung B. Nach diesem Plateau haben wir einen Sprung und sehen einen leicht linearen Anstieg der Mittelwerte der Behandlungen.\nSchauen wir uns zuerst einmal an, wie ein compact letter display aussehen würde, wenn kein Effekt vorliegen würde. Daher die Nullhypothese ist wahr und die Mittelwerte der Gruppen unterscheiden sich nicht. Wir nutzen hier einmal ein kleineres Beispiel mit den Behandlungslevels ctrl, treat_A und treat_B. Alle drei Behandlungslevel haben einen Mittelwert von 10. Es gilt die Nullhypothese und wir erhalten folgendes compact letter display in Tabelle 31.3.\n\n\nTabelle 31.3— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt, es gibt keinen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\\(\\phantom{a}\\)\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n10\na\n\n\n\n\ntreat_B\n10\na\n\n\n\n\n\n\nDas Gegenteil sehen wir in der Tabelle 31.4. Hier haben wir ein compact letter display wo sich alle drei Mittelwerte mit 10, 15 und 20 voneinander klar unterscheiden. Die Nullhypothese gilt für keinen der möglichen paarweisen Vergleiche.\n\n\nTabelle 31.4— Das compact letter display für drei Behandlungen nach einem paarweisen Vergleich. Die Nullhypothese gilt nicht, es gibt einen Mittelwertsunterschied.\n\n\n\n\n\n\n\n\nBehandlung\nMittelwert\n\n\n\n\n\n\nctrl\n10\na\n\\(\\phantom{a}\\)\n\\(\\phantom{a}\\)\n\n\ntreat_A\n15\n\nb\n\n\n\ntreat_B\n20\n\\(\\phantom{a}\\)\n\nc\n\n\n\n\nSchauen wir uns nun die Implementierung des compact letter display für die verschiedenen Möglichkeiten der Multiplen Vergleiche einmal an.\n\n31.7.1 … für pairwise.*.test()\n\nWenn wir für die Funktionen pairwise.*.test() das compact letter display berechnen wollen, dann müssen wir etwas ausholen. Denn wir müssen dafür die Funktion multcompLetters() nutzen. Diese Funktion braucht die \\(p\\)-Werte als Matrix und diese Matrix der \\(p\\)-Werte kriegen wir über die Funktion fullPTable(). Am Ende haben wir aber dann das was wir wollten. Ich habe hier nochmal das einfache Beispiel mit den Sprungweiten von oben genommen.\n\npairwise.t.test(fac1_tbl$jump_length, fac1_tbl$animal,\n                p.adjust.method = \"bonferroni\") %&gt;% \n  extract2(\"p.value\") %&gt;% \n  fullPTable() %&gt;% \n  multcompLetters()\n\ndog cat fox \n\"a\" \"b\" \"a\" \n\n\nAls Ergebnis erhalten wir, dass Hund- und Fuchsflöhe gleich weit springen, beide teilen sich den gleichen Buchstaben. Katzenflöhe springen unterschiedlich zu Hunden- und Fuchsflöhen. Das Vorgehen ändert sich dann nicht, wenn wir eine andere Funktion wie pairwise.wilcox.test() nehmen.\n\n31.7.2 … für das Paket multcomp\n\nWir schauen uns zuerst einmal die Implementierung des compact letter display in dem Paket multcomp an. Wir nutzen die Funktion multcompLetters() aus dem Paket multcompView um uns das compact letter display wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Das heist wir müssen aus der Funktion glht() die adjustierten \\(p\\)-Werte extrahieren und dann einen Vektor der Vergleiche bzw. Kontraste in der Form A-B bauen. Also ohne Leerzeichen und in der Beschreibung der Level der Behandlung trt. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nmultcomp_cld &lt;- lm(rsp  ~ trt, data = data_tbl) %&gt;%\n  glht(linfct = mcp(trt = \"Tukey\")) %&gt;% \n  tidy %&gt;% \n  mutate(contrast = str_replace_all(contrast, \"\\\\s\", \"\")) %&gt;% \n  pull(adj.p.value, contrast) %&gt;% \n  multcompLetters() \n\nWir erhalten dann folgendes compact letter display für die paarweisen Vergleiche aus multcomp.\n\nmultcomp_cld \n\nneg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F  treat_G \n     \"a\"      \"a\"      \"a\"      \"b\"     \"bc\"     \"cd\"      \"d\"      \"e\" \npos_crtl \n     \"f\" \n\n\nLeider sind diese Buchstaben in dieser Form schwer zu verstehen. Deshalb gibt es noch die Funktion plot() in dem Paket multcompView um uns die Buchstaben mit den Leveln der Behandlung einmal ausgeben zu lassen. Wir erhalten dann folgende Abbildung.\n\nmultcomp_cld %&gt;% plot\n\n\n\n\nIn dem compact letter display bedeuten gleiche Buchstaben, dass die Behandlungen gleich sind. Es gilt die Nullhypothese für diesen Vergleich.\nWas sehen wir hier? Kombinieren wir einmal das compact letter display mit den Leveln der Behandlung und den Mittelwerten der Behandlungen in einer Tabelle 31.5. Wenn die Mittelwerte gleich sind, dann erhalten die Behandlungslevel den gleichen Buchstaben. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein a. Ebenso sind die MIttelwerte von treat_C und treat_D nahezu gleich, dehalb erhalten beide ein b. Das machen wir immer so weiter und konzentrieren uns also auf die nicht signifikanten Ergebnisse. Denn gleiche Buchstaben bedeuten, dass die Behandlungen gleich sind. Wir sehen hier also, bei welchen Vergleichen die Nullhypothese gilt.\n\n\nTabelle 31.5— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus multcomp. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\nneg_ctrl\n20\na\n\n\n\n\n\n\n\ntreat_A\n22\na\n\n\n\n\n\n\n\ntreat_B\n24\na\n\n\n\n\n\n\n\ntreat_C\n35\n\nb\n\n\n\n\n\n\ntreat_D\n37\n\nb\nc\n\n\n\n\n\ntreat_E\n40\n\n\nc\nd\n\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\npos_crtl\n10\n\n\n\n\n\nf\n\n\n\n\nWir können dann die Buchstaben auch in den Boxplot ergaänzen. Die y-Position kann je nach Belieben dann noch angepasst werden. zum Beispiel könnten hier auch die Mittelwerte aus einer summarise() Funktion ergänzt werden und so die y-Position angepasst werden.\n\nletters_tbl &lt;- multcomp_cld$Letters %&gt;% \n  enframe(\"trt\", \"label\") %&gt;% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 31.10— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n31.7.3 … für das Paket emmeans\n\nIn dem Paket emmeans ist das compact letter display ebenfalls implementiert und wir müssen nicht die Funktion multcompLetters() nutzen. Durch die direkte Implementierung ist es etwas einfacher sich das compact letter display anzeigen zu lassen. Das Problem ist dann später sich die Buchstaben zu extrahieren um die Abbildung 31.11 zu ergänzen. Wir nutzen in emmeans die Funktion cld() um das compact letter display zu erstellen.\n\nemmeans_cld &lt;- lm(rsp  ~ trt, data = data_tbl) %&gt;%\n  emmeans(~ trt) %&gt;%\n  cld(Letters = letters, adjust = \"bonferroni\")\n\nWir erhalten dann die etwas besser sortierte Ausgabe für die Behandlungen wieder.\n\nemmeans_cld \n\n trt      emmean   SE  df lower.CL upper.CL .group \n pos_crtl   9.67 1.12 171     6.51     12.8  a     \n neg_ctrl  20.02 1.12 171    16.86     23.2   b    \n treat_A   20.97 1.12 171    17.81     24.1   b    \n treat_B   23.35 1.12 171    20.19     26.5   b    \n treat_C   34.96 1.12 171    31.80     38.1    c   \n treat_D   37.46 1.12 171    34.30     40.6    cd  \n treat_E   40.17 1.12 171    37.01     43.3     de \n treat_F   43.23 1.12 171    40.07     46.4      e \n treat_G   50.51 1.12 171    47.35     53.7       f\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 9 estimates \nP value adjustment: bonferroni method for 36 tests \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWie die Ausgabe von cld() richtig anmerkt, können compact letter display irreführend sein weil sie eben Nicht-Unterschiede anstatt von signifikanten Unterschieden anzeigen. Zum Anderen sehen wir aber auch, dass wir 36 statistische Tests gerechnet haben und somit zu einem Signifikanzniveau von \\(\\cfrac{\\alpha}{k} = \\cfrac{0.05}{36} \\approx 0.0014\\) testen. Wir brauchen also schon sehr große Unterschiede oder aber eine sehr kleine Streuung um hier signifikante Effekte nachweisen zu können.\nIn Tabelle 31.6 sehen wir das Ergebnis des compact letter display nochmal mit den Mittelwerten der Behandlungslevel zusammen dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. Wir gehen so alle Vergleiche einmal durch.\n\n\nTabelle 31.6— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus emmeans. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\n\ntreat_E\n40\n\n\n\nd\ne\n\n\n\ntreat_F\n43\n\n\n\n\ne\n\n\n\ntreat_G\n45\n\n\n\n\n\nf\n\n\n\n\nAbschließend können wir die Buchstaben aus dem compact letter display noch in die Abbildung 31.11 ergänzen. Hier müssen wir etwas mehr machen um die Buchstaben aus dem Objekt emmeans_cld zu bekommen. Du kannst dann noch die y-Position anpassen wenn du möchtest.\n\nletters_tbl &lt;- emmeans_cld %&gt;% \n  tidy %&gt;% \n  select(trt, label = .group) %&gt;% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 31.11— Boxplot der Beispieldaten zusammen mit den compact letter display.\n\n\n\n\n31.7.4 … für den Games-Howell-Test\nAbschließend wollen wir uns die Implementierung des compact letter display für den Games-Howell-Test einmal anschauen. Es gilt vieles von dem in diesem Abschnitt schon gesagtes. Wir nutzen die Funktion multcompLetters() aus dem Paket multcompView um uns das compact letter display aus dem Games-Howell-Test wiedergeben zu lassen. Davor müssen wir noch einige Schritte an Sortierung und Umbenennung durchführen. Das hat den Grund, dass die Funktion multcompLetters() nur einen benannten Vektor mit \\(p\\)-Werten akzeptiert. Die Funktion pull() erlaubt uns einen Spalte als Vektor aus einem tibble() zu ziehen und dann nach der Spalte contrast zu benennen.\n\nght_cld &lt;- games_howell_test(rsp ~ trt, data = data_tbl) %&gt;% \n  mutate(contrast = str_c(group1, \"-\", group2)) %&gt;% \n  pull(p.adj, contrast) %&gt;% \n  multcompLetters() \n\nDas compact letter display kennen wir schon aus der obigen Beschreibung.\n\nght_cld\n\npos_crtl neg_ctrl  treat_A  treat_B  treat_C  treat_D  treat_E  treat_F \n     \"a\"      \"b\"      \"b\"      \"b\"      \"c\"     \"cd\"      \"d\"      \"d\" \n treat_G \n     \"e\" \n\n\nWir können uns dann auch das compact letter display als übersichtlicheren Plot wiedergeben lassen.\n\nght_cld %&gt;% plot\n\n\n\n\nUm die Zusammenhänge besser zu verstehen ist in Tabelle 31.7 nochmal die Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display dargestellt. Wir sehen wieder, dass sich pos_crtl von allen anderen Behandlungen unterscheidet, deshalb hat nur die Behandlung pos_crtl den Buchstaben a. Die Mittelwerte vom neg_ctrl, treat_A und treat_B sind nahezu gleich, also damit nicht signifikant. Deshalb erhalten diese Behandlungslevel ein b. In der Form können wir alle Vergleiche einmal durchgehen.\n\n\nTabelle 31.7— Kombination der Level der Behandlungen und deren Mittelwerte zur Generieung sowie dem compact letter display generiert aus den adjustierten \\(p\\)-Werten aus dem Games-Howell-Test. Gleiche Buchstaben bedeuten kein signifikanter Unterschied.\n\nBehandlung\nMittelwert\n\n\n\n\n\n\n\n\npos_crtl\n10\na\n\n\n\n\n\n\nneg_ctrl\n20\n\nb\n\n\n\n\n\ntreat_A\n22\n\nb\n\n\n\n\n\ntreat_B\n24\n\nb\n\n\n\n\n\ntreat_C\n35\n\n\nc\n\n\n\n\ntreat_D\n37\n\n\nc\nd\n\n\n\ntreat_E\n40\n\n\n\nd\n\n\n\ntreat_F\n43\n\n\n\nd\n\n\n\ntreat_G\n45\n\n\n\n\ne\n\n\n\n\nWir können dann auch in Abbildung 31.12 sehen, wie das compact letter display mit den Boxplots verbunden wird.\n\nletters_tbl &lt;- ght_cld$Letters %&gt;% \n  enframe(\"trt\", \"label\") %&gt;% \n  mutate(rsp = 0)\n\nggplot(data_tbl, aes(x = trt, y = rsp, \n                     fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  scale_fill_okabeito() +\n  geom_jitter(width = 0.15, alpha = 0.5) + \n  geom_text(data = letters_tbl, \n            aes(x = trt , y = rsp, label = label)) +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 31.12— Boxplot der Beispieldaten zusammen mit den compact letter display."
  },
  {
    "objectID": "stat-linear-reg-preface.html#simple-lineare-regression",
    "href": "stat-linear-reg-preface.html#simple-lineare-regression",
    "title": "Grundlagen der linearen Regression",
    "section": "Simple lineare Regression",
    "text": "Simple lineare Regression\nIm Kapitel 32 wollen wir einmal die Grundlagen der linearen Regression an der simplen linearen Regression erarbeiten. Wir benötigen die Formeln und die Worte für das weitere Modellieren multipler lineare Modelle.\nKausales Modell\nSchon gleich hier vorweg, es gibt einen Unterschied zwischen einem kausalen und einem prädiktiven Modell. In diesem kurzen Kapitel 32.4.1 schauen wir uns das kausale Modell einmal an. Wenn wir eine klassische Regression rechnen, rechnen wir meist ein kausales Modell.\nPrädiktives Modell\nDie Vorhersage oder Prädiktion ist ein eigenes Kapitel wert. Im Bereich des maschinellen Lernens wird die Prädiktion auch Klassifikation genannt. Wir schauen uns in diesem Kapitel 32.4.2 nur die Grundlage einmal an. Später nutzen wir auch maschinelle Lernverfahren für die Klassifikation."
  },
  {
    "objectID": "stat-linear-reg-preface.html#maßzahlen-der-modelgüte",
    "href": "stat-linear-reg-preface.html#maßzahlen-der-modelgüte",
    "title": "Grundlagen der linearen Regression",
    "section": "Maßzahlen der Modelgüte",
    "text": "Maßzahlen der Modelgüte\nNachdem wir ein Modell mathematisch gerechnet haben, müssen wir herausfinden, ob das Modell auch gut funktioniert hat. In Kapitel 33 schauen wir uns verschiedene Maßzahlen an um zu sehen, ob die Wahl des mathematischen Algorithmus auch zu unseren Daten passt. Rechnen können wir alles, aber ob die Rechnung Sinn ergibt müssen wir erst herausfinden."
  },
  {
    "objectID": "stat-linear-reg-preface.html#korrelation",
    "href": "stat-linear-reg-preface.html#korrelation",
    "title": "Grundlagen der linearen Regression",
    "section": "Korrelation",
    "text": "Korrelation\nDie simple lineare Regression und Korrelation sind eng miteinander verwandt. In Kapitel 34 schauen wir uns einmal die beiden Korrelationskoeffizienten nach Pearson und Spearman einmal an."
  },
  {
    "objectID": "stat-linear-reg-basic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-linear-reg-basic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "32  Simple lineare Regression",
    "section": "\n32.1 Genutzte R Pakete für das Kapitel",
    "text": "32.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               readxl)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-linear-reg-basic.html#daten",
    "href": "stat-linear-reg-basic.html#daten",
    "title": "32  Simple lineare Regression",
    "section": "\n32.2 Daten",
    "text": "32.2 Daten\nWir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die lineare Regression auf sehr großen Datensätzen anwenden, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit \\(n=7\\) Beobachtungen. In der Tabelle 32.1 ist der Datensatz simplel_tbl dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in [cm] und dem Gewicht in [mg] für sieben Beobachtungen modellieren.\n\nsimple_tbl &lt;- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),\n                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))\n\n\n\n\n\nTabelle 32.1— Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight.\n\njump_length\nweight\n\n\n\n1.2\n0.8\n\n\n1.8\n1.0\n\n\n1.3\n1.2\n\n\n1.7\n1.9\n\n\n2.6\n2.0\n\n\n1.8\n2.7\n\n\n2.7\n2.8\n\n\n\n\n\n\nIn Abbildung 32.1 sehen wir die Visualisierung der Daten simple_tbl in einem Scatterplot mit einer geschätzen Gerade.\n\n\n\n\nAbbildung 32.1— Scatterplot der Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Die Gerade verläuft mittig durch die Punkte.\n\n\n\nWir schauen uns in diesem Kapitel nur eine simple lineare Regression mit einem \\(x_1\\) an. In unserem Fall ist das \\(x_1\\) gleich dem weight. Später schauen wir dann multiple lineare Regressionen mit mehreren \\(x_1,..., x_p\\) an.\nBevor wir mit dem Modellieren anfangen können, müssen wir verstehen, wie ein simples Modell theoretisch aufgebaut ist. Danach können wir uns das lineare Modell in R anschauen."
  },
  {
    "objectID": "stat-linear-reg-basic.html#simple-lineare-regression-theoretisch",
    "href": "stat-linear-reg-basic.html#simple-lineare-regression-theoretisch",
    "title": "32  Simple lineare Regression",
    "section": "\n32.3 Simple lineare Regression theoretisch",
    "text": "32.3 Simple lineare Regression theoretisch\nWir haben nun die ersten sieben Beobachtungen in dem Objekt simple_tbl vorliegen. Wie sieht nun theoretisch eine lineare Regression aus? Wir wollen eine Grade durch Punkte legen, wie wie wir es in Abbildung 32.1 sehen. Die blaue Gerade wir durch eine Geradengleichung beschreiben. Du kenst vermutlich noch die Form \\(y = mx + b\\). In der Statistik beschreiben wir eine solche Gerade aber wie folgt.\n\\[\ny \\sim \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nmit\n\n\n\\(\\beta_0\\) als den y-Achsenabschnitt.\n\n\\(\\beta_1\\) als der Steigung der Geraden.\n\n\\(\\epsilon\\) als Residuen oder die Abweichungen von den \\(y\\)-Werten auf Geraden zu den einzelnen \\(y\\)-Werten der Beobachtungen.\n\nIn Tabelle 32.2 siehst du nochmal in einer Tabelle den Vergleich von der Schreibweise der linearen Regression in der Schule und in der Statistik. Darüber hinaus sind die deutschen Begriffe den englischen Begriffen gegenüber gestellt. Warum schreiben wir die Gleichung in der Form? Damit wir später noch weitere \\(\\beta_px_p\\)-Paare ergänzen könen und so multiple Modelle bauen können.\n\n\nTabelle 32.2— Vergleich und Übersicht der schulischen vs. statistischen Begriffe in den linearen Regression sowie die deutschen und englischen Begriffe.\n\n\n\n\n\n\n\n\\(\\boldsymbol{y = mx +b}\\)\n\\(\\boldsymbol{y \\sim \\beta_0 + \\beta_1 x_1 + \\epsilon}\\)\nDeutsch\nEnglisch\n\n\n\n\\(m\\)\n\\(\\beta_1\\)\nSteigung\nSlope\n\n\n\\(x\\)\n\\(x_1\\)\nEinflussvariable\nRisk factor\n\n\n\\(b\\)\n\\(\\beta_0\\)\ny-Achsenabschnitt\nIntercept\n\n\n\n\\(\\epsilon\\)\nResiduen\nResidual\n\n\n\n\nIn Abbildung 32.3 sehen wir die Visualisierung der Gleichung in einer Abbildung. Die Gerade läuft durch die Punktewolke und wird durch die statistischen Maßzahlen bzw. Parameter \\(\\beta_0\\), \\(\\beta_1\\) sowie den \\(\\epsilon\\) beschrieben. Wir sehen, dass das \\(\\beta_0\\) den Intercept darstellt und das \\(\\beta_1\\) die Steigung der Geraden. Wenn wir \\(x\\) um 1 Einheit erhöhen \\(x+1\\), dann steigt der \\(y\\) Wert um den Wert von \\(\\beta_1\\). Die einzelnen Abweichungen der beobachteten \\(y\\)-Wert zu den \\(y\\)-Werten auf der Gerade (\\(\\hat{y}\\)) werden als Residuen oder auch \\(\\epsilon\\) bezeichnet.\n\n\nAbbildung 32.2— Visualisierung der linearen Regression. Wir legen eine Gerade durch eine Punktewolke. Die Gerade wird durch die statistischen Maßzahlen bzw. Parameter \\(\\beta_0\\), \\(\\beta_1\\) sowie den \\(\\epsilon\\) beschrieben.\n\nIn R werden die \\(\\hat{y}\\) auch fitted values genannt. Die \\(\\epsilon\\) Werte werden dann residuals bezeichnet.\nSchauen wir uns einmal den Zusammenhang von \\(y\\), den beobachteten Werten, und \\(\\hat{y}\\), den geschätzen Werten auf der Gerade in unserem Beispiel an. In Tabelle 32.3 sehen wir die Berechnung der einzelnen Residuen für die Gerade aus der Abbildung 32.1. Wir nehmen jedes beobachtete \\(y\\) und ziehen den Wert von \\(y\\) auf der Gerade, bezeichnet als \\(\\hat{y}\\), ab. Diesen Schritt machen wir für jedes Wertepaar \\((y_i; \\hat{y}_i)\\).\n\n\n\nTabelle 32.3— Zusammenhang zwischen den \\(y\\), den beobachteten Werten, und \\(\\hat{y}\\), den geschätzen Werten auf der Gerade. Wir nennen den Abstand \\(y_i - \\hat{y}_i\\) auch Residuum oder Epsilon \\(\\epsilon\\).\n\n\n\n\n\n\n\n\nx\ny\n\\(\\boldsymbol{\\hat{y}}\\)\nResiduen (\\(\\boldsymbol{\\epsilon}\\))\nWert\n\n\n\n0.8\n1.2\n1.38\n\\(\\epsilon_1 = y_1 - \\hat{y}_1\\)\n\\(\\epsilon_1 = 1.2 - 1.38 = -0.18\\)\n\n\n1.0\n1.8\n1.48\n\\(\\epsilon_2 = y_2 - \\hat{y}_2\\)\n\\(\\epsilon_2 = 1.8 - 1.48 = +0.32\\)\n\n\n1.2\n1.3\n1.58\n\\(\\epsilon_3 = y_3 - \\hat{y}_3\\)\n\\(\\epsilon_3 = 1.3 - 1.58 = -0.28\\)\n\n\n1.9\n1.7\n1.94\n\\(\\epsilon_4 = y_4 - \\hat{y}_4\\)\n\\(\\epsilon_4 = 1.7 - 1.94 = -0.24\\)\n\n\n2.0\n2.6\n1.99\n\\(\\epsilon_5 = y_5 - \\hat{y}_5\\)\n\\(\\epsilon_5 = 2.6 - 1.99 = +0.61\\)\n\n\n2.7\n1.8\n2.34\n\\(\\epsilon_6 = y_6 - \\hat{y}_6\\)\n\\(\\epsilon_6 = 1.8 - 2.34 = -0.54\\)\n\n\n2.8\n2.7\n2.40\n\\(\\epsilon_7 = y_7 - \\hat{y}_7\\)\n\\(\\epsilon_7 = 2.7 - 2.40 = +0.30\\)\n\n\n\n\n\nIn R wird in Modellausgaben die Standardabweichung der Residuen \\(s_{\\epsilon}\\) als sigma bezeichnet.\nDie Abweichungen \\(\\epsilon\\) oder auch Residuen genannt haben einen Mittelwert von \\(\\bar{\\epsilon} = 0\\) und eine Varianz von \\(s^2_{\\epsilon} = 0.17\\). Wir schreiben, dass die Residuen normalverteilt sind mit \\(\\epsilon \\sim \\mathcal{N}(0, s^2_{\\epsilon})\\). Wir zeichnen die Gerade also so durch die Punktewolke, dass die Abstände zu den Punkten, die Residuen, im Mittel null sind. Die Optimierung erreichen wir in dem wir die Varianz der Residuuen minimieren. Folglich modellieren wir die Varianz."
  },
  {
    "objectID": "stat-linear-reg-basic.html#simples-lineare-regression-in-r",
    "href": "stat-linear-reg-basic.html#simples-lineare-regression-in-r",
    "title": "32  Simple lineare Regression",
    "section": "\n32.4 Simples lineare Regression in R",
    "text": "32.4 Simples lineare Regression in R\nIm Allgemeinen können wir ein Modell in R wie folgt schreiben. Wir brauchen ein y auf der linken Seite und in der simplen linearen Regressione ein \\(x\\) auf der rechten Seite der Gleichung. Wir brauchen also zwei Variablen \\(y\\) und \\(x\\), die natürlich nicht im Datensatz in R so heißen müssen.\n\n\nAbbildung 32.3— Modellschreibweise \\(y\\) hängt ab von \\(x\\). Das \\(y\\) repräsentiert eine Spalte im Datensatz und das \\(x\\) repräsentiert ebenso eine Spalte im Datensatz.\n\nKonkret würden wir in unserem Beispiel das Modell wie folgt benennen. Das \\(y\\) wird zu jump_length und das \\(x\\) wird zu weight. Wir haben dann das Modell in der simplesten Form definiert.\n\n\nAbbildung 32.4— Modellschreibweise bzw. formula Schreibweise in R. Die Variable \\(y\\) hängt ab von \\(x\\) am Beispiel des Datensatzes simple_tbl mit den beiden Variablen jump_length als \\(y\\) und weight als \\(x\\).\n\nNachdem wir das Modell definiert haben, setzen wir dieses Modell jump_length ~ weight in die Funktion lm() ein um das lineare Modell zu rechnen. Wie immer müssen wir auch festlegen aus welcher Datei die Spalten genommen werden sollen. Das machen wir mit der Option data = simple_tbl. Wir speichern dann die Ausgabe der Funktion lm() in dem Objekt fit_1 damit wir die Ausgabe noch in andere Funktionen pipen können.\n\nfit_1 &lt;- lm(jump_length ~ weight, data = simple_tbl)\n\nAn dieser Stelle kannst du schnell in das Problem der Antwort auf alles kommen: “42”\nWir können jetzt mir dem Modell drei Dinge tun. Abhängig von der Fragestellung liefert uns natürlich jedes der drei Möglichkeiten eine andere Antwort.\n\nWir rechnen mit dem Fit des Modells eine ANOVA (siehe Kapitel 23)\nWir rechnen ein kausales Modell, uns interessieren die Effekte (siehe Kapitel 32.4.1)\nWir rechnen ein prädiktives Modell, uns interessiert der Wert neuer Werte (siehe Kapitel 32.4.2)\n\n\n32.4.1 Kausales Modell\nIm Folgenden rechnen wir ein kausales Modell, da wir an dem Effekt des \\(x\\) interessiert sind. Wenn also das \\(x_1\\) um eine Einheit ansteigt, um wie viel verändert sich dann das \\(y\\)? Der Schätzer \\(\\beta_1\\) gibt uns also den Einfluss oder den kausalen Zusammenhang zwischen \\(y\\) und \\(x_1\\) wieder.\nDie Funktion summary() gibt dir das Ergebnis eines kausalen Modells wieder\nIm ersten Schritt schauen wir uns die Ausgabe der Funktion lm() in der Funktion summary() an. Daher pipen wir das Objekt fit_1 in die Funktion summary().\n\nfit_1 %&gt;% summary\n\nWir erhalten folgende Ausgabe dargestellt in Abbildung 32.5.\n\n\nAbbildung 32.5— Die summary() Ausgabe des Modells fit_1.\n\nWas sehen wir in der Ausgabe der summary() Funktion? Als erstes werden uns die Residuen wiedergegeben. Wenn wir nur wenige Beobachtungen haben, dann werden uns die Residuen direkt wiedergegeben, sonst die Verteilung der Residuen. Mit der Funktion augment() aus dem R Paket broom können wir uns die Residuen wiedergeben lassen. Die Residuen schauen wir uns aber nochmal im Kapitel 33 genauer an.\n\nfit_1 %&gt;% augment\n\n# A tibble: 7 × 8\n  jump_length weight .fitted .resid  .hat .sigma .cooksd .std.resid\n        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1         1.2    0.8    1.38 -0.176 0.388  0.496  0.0778     -0.496\n2         1.8    1      1.48  0.322 0.297  0.471  0.151       0.844\n3         1.3    1.2    1.58 -0.280 0.228  0.483  0.0725     -0.701\n4         1.7    1.9    1.94 -0.237 0.147  0.492  0.0275     -0.564\n5         2.6    2      1.99  0.612 0.156  0.384  0.199       1.47 \n6         1.8    2.7    2.34 -0.545 0.367  0.376  0.656      -1.51 \n7         2.7    2.8    2.40  0.304 0.417  0.467  0.276       0.877\n\n\nIm zweiten Block erhalten wir die Koeffizienten (eng. coefficients) der linearen Regression. Das heißt, wir kriegen dort \\(\\beta_0\\) als y-Achsenabschnitt sowie die Steigung \\(\\beta_1\\) für das Gewicht. Dabei ist wichtig zu wissen, dass immer als erstes der y-Achsenabschnitt (Intercept) auftaucht. Dann die Steigungen der einzelnen \\(x\\) in dem Modell. Wir haben nur ein kontinuierliches \\(x\\), daher ist die Interpretation der Ausgabe einfach. Wir können die Gradengleichung wie folgt formulieren.\n\\[\njump\\_length \\sim 0.97 + 0.51 \\cdot weight\n\\]\nWas heißt die Gleichung nun? Wenn wir das \\(x\\) um eine Einheit erhöhen dann verändert sich das \\(y\\) um den Wert von \\(\\beta_1\\). Wir haben hier eine Steigung von \\(0.51\\) vorliegen. Ohne Einheit keine Interpretation! Wir wissen, dass das Gewicht in [mg] gemessen wurde und die Sprungweite in [cm]. Damit können wir aussagen, dass wenn ein Floh 1 mg mehr wiegt der Floh 0.51 cm weiter springen würde.\nSchauen wir nochmal in die saubere Ausgabe der tidy() Funktion. Wir sehen nämlich noch einen \\(p\\)-Wert für den Intercept und die Steigung von weight.\n\nfit_1 %&gt;% tidy\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    0.969     0.445      2.18  0.0813\n2 weight         0.510     0.232      2.20  0.0790\n\n\nWenn wir einen \\(p\\)-Wert sehen, dann brauchen wir eine Nullhypothese, die wir dann eventuell mit der Entscheidung am Signifikanzniveau \\(\\alpha\\) von 5% ablehnen können. Die Nullhypothese ist die Gleichheitshypothese. Wenn es also keinen Effekt von dem Gewicht auf die Sprungweite gebe, wie groß wäre dann \\(\\beta_1\\)? Wir hätten dann keine Steigung und die Grade würde parallel zur x-Achse laufen. Das \\(\\beta_1\\) wäre dann gleich null.\n\\[\n\\begin{align*}\nH_0: \\beta_i &= 0\\\\  \nH_A: \\beta_i &\\neq 0 \\\\   \n\\end{align*}\n\\]\nWir haben für jedes \\(\\beta_i\\) ein eigenes Hypothesenpaar. Meistens interessiert uns der Intercept nicht. Ob der Intercept nun durch die Null geht oder nicht ist eher von geringem Interessen.\nSpannder ist aber wie sich der \\(p\\)-Wert berechnet. Der \\(p\\)-Wert basiert auf einer t-Statistik, also auf dem t-Test. Wir rechnen für jeden Koeffizienten \\(\\beta_i\\) einen t-Test. Das machen wir in dem wir den Koeffizienten estimate durch den Fehler des Koeffizienten std.error teilen.\n\\[\n\\begin{align*}\nT_{(Intercept)} &= \\cfrac{\\mbox{estimate}}{\\mbox{std.error}}  = \\cfrac{0.969}{0.445} = 2.18\\\\  \nT_{weight} &= \\cfrac{\\mbox{estimate}}{\\mbox{std.error}}  = \\cfrac{0.510}{0.232} = 2.20\\\\   \n\\end{align*}\n\\]\nWir sehen in diesem Fall, dass weder der Intercept noch die Steigung von weight signifikant ist, da die \\(p\\)-Werte mit \\(0.081\\) und \\(0.079\\) leicht über dem Signifikanzniveau von \\(\\alpha\\) gleich 5% liegen. Wir haben aber einen starkes Indiz gegen die Nullhypothese, da die Wahrscheinlichkeit die Daten zu beobachten sehr gering ist unter der Annahme das die Nullhypothese gilt.\nZun Abschluß noch die Funktion glance() ebenfalls aus dem R Paket broom, die uns erlaubt noch die Qualitätsmaße der linearen Regression zu erhalten. Wir müssen nämlich noch schauen, ob die Regression auch funktioniert hat. Die Überprüfung geht mit einem \\(x\\) sehr einfach. Wir können uns die Grade ja anschauen. Das geht dann mit einem Model mit mehreren \\(x\\) nicht mehr und wir brauchen andere statistsiche Maßzahlen.\n\nfit_1 %&gt;% glance \n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.492         0.391 0.455      4.84  0.0790     1  -3.24  12.5  12.3\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n32.4.2 Prädiktives Modell\nNeben dem kausalen Modell gibt es auch die Möglichkeit ein prädiktives Modell zu rechnen. Im Prinzip ist die Sprache hier etwas ungenau. Wir verwenden das gefittete Modell nur anders. Anstatt das Modell fit_1 in die Funktion summary() zu pipen, pipen wir die das Modell in die Funktion predict(). Die Funktion predict() kann dann für neue Daten über die Option newdata = das \\(y\\) vorhersagen.\nIn unserem Fall müssen wir uns deshalb ein tibble mit einer Spalte bauen. Wir haben ja oben im Modell auch nur ein \\(x_1\\) mit aufgenommen. Später können wir natürlich auch für multiple Modelle die Vorhersage machen. Wichtig ist, dass die Namen gleich sind. Das heißt in dem neuen Datensatz müssen die Spalten exakt so heißen wir in dem alten Datensatz in dem das Modell gefittet wurde.\n\nsimple_new_tbl &lt;- tibble(weight = c(1.7, 1.4, 2.1, 3.0)) \n\npredict(fit_1, newdata = simple_new_tbl) %&gt;% round(2)\n\n   1    2    3    4 \n1.84 1.68 2.04 2.50 \n\n\nWie wir sehen ist die Anwendung recht einfach. Wir haben die vier jump_length Werte vorhergesagt bekommen, die sich mit dem Fit des Modells mit den neuen weight Werten ergeben.\nIn Abbildung 32.6 sehen wir die Visualisierung der vier vorhergesagten Werte. Die Werte müssen auf der Geraden liegen.\n\n\n\n\nAbbildung 32.6— Scatterplot der alten Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Sowie der neuen vorhergesagten Beobachtungen auf der Geraden.\n\n\n\nWir werden später in der Klassiifkation, der Vorhersage von \\(0/1\\)-Werten, sowie in der multiplen Regression noch andere Prädktionen und deren Maßzahlen kennenlernen. Im Rahmen der simplen Regression soll dies aber erstmal hier genügen."
  },
  {
    "objectID": "stat-linear-reg-quality.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-linear-reg-quality.html#genutzte-r-pakete-für-das-kapitel",
    "title": "33  Maßzahlen der Modelgüte",
    "section": "\n33.1 Genutzte R Pakete für das Kapitel",
    "text": "33.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-linear-reg-quality.html#daten",
    "href": "stat-linear-reg-quality.html#daten",
    "title": "33  Maßzahlen der Modelgüte",
    "section": "\n33.2 Daten",
    "text": "33.2 Daten\nNachdem wir uns im vorherigen Kapitel mit einem sehr kleinen Datensatz beschäftigt haben, nehmen wir einen großen Datensatz. Bleiben aber bei einem simplen Modell. Wir brauchen dafür den Datensatz flea_dog_cat_length_weight.xlsx. In einer simplen linearen Regression schauen wir uns den Zusammenhang zwischen einem \\(y\\) und einem \\(x_1\\) an. Daher wählen wir aus dem Datensatz die beiden Spalten jump_length und weight. Wir wollen nun feststellen, ob es einen Zusammenhang zwischen der Sprungweite in [cm] und dem Flohgewicht in [mg] gibt. In dem Datensatz finden wir 400 Flöhe von Hunden und Katzen.\n\nmodel_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %&gt;%\n  select(animal, jump_length, weight)\n\nIn der Tabelle 33.1 ist der Datensatz model_tbl nochmal dargestellt.\n\n\n\n\nTabelle 33.1— Selektierter Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\nanimal\njump_length\nweight\n\n\n\ncat\n15.79\n6.02\n\n\ncat\n18.33\n5.99\n\n\ncat\n17.58\n8.05\n\n\ncat\n14.09\n6.71\n\n\ncat\n18.22\n6.19\n\n\ncat\n13.49\n8.18\n\n\ncat\n16.28\n7.46\n\n\n\n\n\n\nIm Folgenden ignorieren wir, dass die Sprungweiten und die Gewichte der Flöhe auch noch von den Hunden oder Katzen sowie dem unterschiedlichen Geschlecht der Flöhe abhängen könnten. Wir schmeißen alles in einen Pott und schauen nur auf den Zusammenhang von Sprungweite und Gewicht."
  },
  {
    "objectID": "stat-linear-reg-quality.html#das-simple-lineare-modell",
    "href": "stat-linear-reg-quality.html#das-simple-lineare-modell",
    "title": "33  Maßzahlen der Modelgüte",
    "section": "\n33.3 Das simple lineare Modell",
    "text": "33.3 Das simple lineare Modell\nWir fitten ein simples lineares Modell mit nur einem Einflussfaktor weight auf die Sprunglänge jump_length. Wir erhalten dann das Objekt fit_1 was wir dann im Weiteren nutzen werden.\n\nfit_1 &lt;- lm(jump_length ~ weight, data = model_tbl)\n\nWir nutzen jetzt dieses simple lineare Modell für die weiteren Gütekritierien."
  },
  {
    "objectID": "stat-linear-reg-quality.html#sec-linreg-residual",
    "href": "stat-linear-reg-quality.html#sec-linreg-residual",
    "title": "33  Maßzahlen der Modelgüte",
    "section": "\n33.4 Residualplot",
    "text": "33.4 Residualplot\nIn R wird in Modellausgaben die Standardabweichung der Residuen \\(s_{\\epsilon}\\) als sigma bezeichnet.\nWir wollen mit dem Residualplot die Frage beantworten, ob die Gerade mittig durch die Punktewolke läuft. Die Residuen \\(\\epsilon\\) sollen normalverteilt sein mit einem Mittelwert von Null \\(\\epsilon \\sim \\mathcal{N}(0, s^2_{\\epsilon})\\).\nWir erhalten die Residuen resid und die angepassten Werte .fitted auf der Geraden über die Funktion augment(). Die Funktion augment() gibt noch mehr Informationen wieder, aber wir wollen uns jetzt erstmal auf die Residuen konzentrieren.\n\nresid_plot_tbl &lt;- fit_1 %&gt;% \n  augment() %&gt;% \n  select(.fitted, .resid)\n\nresid_plot_tbl %&gt;% \n  head(5)\n\n# A tibble: 5 × 2\n  .fitted .resid\n    &lt;dbl&gt;  &lt;dbl&gt;\n1    17.9 -2.09 \n2    17.8  0.489\n3    20.6 -3.03 \n4    18.8 -4.72 \n5    18.1  0.110\n\n\nDie Daten selber interessieren uns nicht einer Tabelle. Stattdessen zeichnen wir einmal den Residualplot. Bei dem Residualplot tragen wir die Werte der Residuen .resid auf die y-Achse auf und die angepassten y-Werte auf der Geraden .fitted auf die x-Achse. Wir kippen im Prinzip die gefittete Gerade so, dass die Gerade parallel zu x-Achse läuft.\n\nggplot(resid_plot_tbl, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  theme_bw()\n\n\n\nAbbildung 33.1— Residualplot der Residuen des Models fit_1. Die rote Linie stellt die geschätzte Gerade da. Die Punkte sollen gleichmäßig und ohne eine Struktur um die Gerade verteilt sein.\n\n\n\nThe residual plot should look like the sky at night, with no pattern of any sort.\nIn Abbildung 33.1 sehen wir den Residualplot von unseren Beispieldaten. Wir sehen, dass wir keine Struktur in der Punktewolke erkennen. Auch sind die Punkte gleichmäßig um die Gerade verteilt. Wir haben zwar einen Punkt, der sehr weit von der Gerade weg ist, das können wir aber ignorieren. Später können wir uns noch überlegen, ob wir einen Ausreißer (eng. outlier) vorliegen haben.\nKommen wir nochmal auf die Funktion augment() zurück und schauen uns einmal an, was die ganzen Spalten hier zu bedeuten haben. Dafür nutzen wir nochmal einen simpleren Datensatz in der die vierte Beobachtung mit \\((4.1, 5.2)\\) sehr extreme Werte im Vergleich zu den anderen drei Beobachtungen annimmt. Danach fitten wir dann wieder unser lineares Modell.\n\nsimple_tbl &lt;- tibble(jump_length = c(1.2, 1.8, 1.3, 5.2),\n                     weight = c(0.8, 1, 1.2, 4.1))\n\nfit_2 &lt;- lm(jump_length ~ weight, data = simple_tbl)\n\nIm Folgenden sehen wir dann die Ausgabe der Funktion augment(). Dabei sind die ersten beiden Spalten noch selbsterklärend. Wir haben hier mit jump_length und weight die Werte für das Outcome \\(y\\) und die Einflussvariablen \\(x\\) dargestellt.\n\nfit_2 %&gt;% \n  augment() %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 4 × 8\n  jump_length weight .fitted .resid  .hat .sigma .cooksd .std.resid\n        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1         1.2    0.8    1.2    0     0.38   0.52    0          0   \n2         1.8    1      1.44   0.36  0.33   0.29    0.35       1.18\n3         1.3    1.2    1.68  -0.38  0.3    0.26    0.32      -1.23\n4         5.2    4.1    5.18   0.02  0.99   0.45   32.2        0.73\n\n\nDie anderen Spalten sind dann wie folgt zu lesen.\n\n\n.fitted sind die vorhergesagten Werte auf der Geraden. Wir bezeichnen diese Werte auch die \\(\\hat{y}\\) Werte.\n\n.resid sind die Residuen oder auch \\(\\epsilon\\). Daher der Abstand zwischen den beobachteten \\(y\\)-Werten und den \\(\\hat{y}\\) Werten auf der Geraden.\n\n.hat gibt den Einfluss jeder einzelnen Beobachtung auf die endgültige Gerade wieder. Also den Hebel (eng. leverage) jeder einzelnen Beobachtung und damit wie stark eine Beobachtung an der Geraden zieht.\n\n.sigma beschreibt die geschätzte \\(s^2_{\\epsilon}\\) , wenn die entsprechende Beobachtung aus dem Modell herausgenommen wird.\n\n.cooksd definiert, ob eine Beobachtung ein tendenzieller Outlier im Bezug zu den anderen Beobachtungen ist. Im Prinzip eine Standardisierung der hat Spalte.\n\nstd.resid sind die standardisierten Residuen. Dabei werden die Residuen durch die Standardabweichung der Residuen \\(s_{\\epsilon}\\) geteilt. Die standardisierten Residuen folgen dann einer Standardnormalverteilung.\n\nWir können dann der Ausgabe von augment() entnehmen, dass unsere vierte Beobachtung vermutlich ein Outlier ist."
  },
  {
    "objectID": "stat-linear-reg-quality.html#sec-linreg-bestimmt",
    "href": "stat-linear-reg-quality.html#sec-linreg-bestimmt",
    "title": "33  Maßzahlen der Modelgüte",
    "section": "\n33.5 Bestimmtheitsmaß \\(R^2\\)\n",
    "text": "33.5 Bestimmtheitsmaß \\(R^2\\)\n\nNachdem wir nun wissen wie gut die Gerade durch die Punkte läuft, wollen wir noch bestimmen wie genau die Punkte auf der Geraden liegen. Das heißt wir wollen mit dem Bestimmtheitsmaß \\(R^2\\) ausdrücken wie stark die Punkte um die Gerade variieren. Wir können folgende Aussage über das Bestimmtheitsmaß \\(R^2\\) treffen. Die Abbildung 33.2 visualisiert nochmal den Zusammenhang.\n\nwenn alle Punkte auf der Geraden liegen, dann ist das Bestimmtheitsmaß \\(R^2\\) gleich 1.\nwenn alle Punkte sehr stark um die Gerade streuen, dann läuft das Bestimmtheitsmaß \\(R^2\\) gegen 0.\n\n\n\nAbbildung 33.2— Visualisierung des Bestimmtheitsmaßes \\(R^2\\). Auf der linken Seite sehen wir eine perfekte Übereinstimmung der Punkte und der geschätzten Gerade. Wir haben ein \\(R^2\\) von 1 vorliegen. Sind die Punkte und die geschätzte Gerade nicht deckungsgleich, so läuft das \\(R^2\\) gegen 0.\n\nDa die Streuung um die Gerade auch gleichzeitig die Varianz widerspiegelt, können wir auch sagen, dass wenn alle Punkte auf der Geraden liegen, die Varianz gleich Null ist. Die Einflussvariable \\(x_1\\) erklärt die gesamte Varianz, die durch die Beobachtungen verursacht wurde. Damit beschreibt das Bestimmtheitsmaß \\(R^2\\) auch den Anteil der Varianz, der durch die lineare Regression, daher der Graden, erklärt wird. Wenn wir ein Bestimmtheitsmaß \\(R^2\\) von Eins haben, wird die gesamte Varianz von unserem Modell erklärt. Haben wir ein Bestimmtheitsmaß \\(R^2\\) von Null, wird gar keine Varianz von unserem Modell erklärt. Damit ist ein niedriges Bestimmtheitsmaß \\(R^2\\) schlecht.\nIm Folgenden können wir uns noch einmal die Formel des Bestimmtheitsmaß \\(R^2\\) anschauen um etwas besser zu verstehen, wie die Zusammenhänge mathematisch sind.\n\\[\n\\mathit{R}^2 =\n\\cfrac{\\sum_{i=1}^N \\left(\\hat{y}_i- \\bar{y}\\right)^2}{\\sum_{i=1}^N \\left(y_i - \\bar{y}\\right)^2}\n\\]\nIn der Abbildung 33.3 sehen wir den Zusammenhang nochmal visualisiert. Wenn die Abstände von dem Mittelwert zu den einzelnen Punkten mit \\(y_i - \\bar{y}\\) gleich dem Abstand der Mittelwerte zu den Punkten auf der Geraden mit \\(\\hat{y}_i- \\bar{y}\\) ist, dann haben wir einen perfekten Zusammenhang.\n\n\nAbbildung 33.3— Auf der linken Seite sehen wir eine Gerade die nicht perfekt durch die Punkte läuft. Wir nehmen ein Bestimmtheitsmaß \\(R^2\\) von ca. 0.7 an. Die Abstände der einzelnen Beobachtungen \\(y_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\) ist nicht gleich den Werten auf der Geraden \\(\\hat{y}_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\). Dieser Zusammenhang wird in der rechten Abbildung mit einem Bestimmtheitsmaß \\(R^2\\) von 1 nochmal deutlich.\n\nWir können die Funktion glance() nutzen um uns das r.squared und das adj.r.squared wiedergeben zu lassen.\n\nfit_1 %&gt;% \n  glance() %&gt;% \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.251         0.250\n\n\nWir nutzen grundsätzlich das adjustierte \\(R^2\\)adj.r.squared in der Anwendung.\nWir haben wir ein \\(R^2\\) von \\(0.31\\) vorliegen. Damit erklärt unser Modell bzw. die Gerade 31% der Varianz. Das ist jetzt nicht viel, aber wundert uns auch erstmal nicht. Wir haben ja die Faktoren animal und sex ignoriert. Beide Faktoren könnten ja auch einen Teil der Varianz erklären. Dafür müssten wir aber eine multiple lineare Regression mit mehren \\(x\\) rechnen.\nWenn wir eine multiple Regression rechnen, dann nutzen wir das adjustierte \\(R^2\\) in der Anwendung. Das hat den Grund, dass das \\(R^2\\) automatisch ansteigt je mehr Variablen wir in das Modell nehmen. Jede neue Variable wird immer etwas erklären. Um dieses Überanpassen (eng. overfitting) zu vermeiden nutzen wir das adjustierte \\(R^2\\). Im Falle des adjustierte \\(R^2\\) wird ein Strafterm eingeführt, der das adjustierte \\(R^2\\) kleiner macht je mehr Einflussvariablen in das Modell aufgenommen werdenn."
  },
  {
    "objectID": "stat-linear-reg-quality.html#sec-linreg-qq",
    "href": "stat-linear-reg-quality.html#sec-linreg-qq",
    "title": "33  Maßzahlen der Modelgüte",
    "section": "\n33.6 QQ-Plot",
    "text": "33.6 QQ-Plot\n\n\n\n\n\n\nDas klingt hier alles etwas wage… Ja, das stimmt. Aber wir brauchen den QQ-Plot nur ganz kurz und wir müssten sehr viel Energei investieren um den QQ-Plot zu durchdringen. Deshalb hier die wage und grobe Darstellung.\nMit dem Quantile-Quantile Plot oder kurz QQ-Plot können wir überprüfen, ob unser \\(y\\) aus einer Normalverteilung stammt. Oder andersherum, ob unser \\(y\\) approximativ normalverteilt ist. Der QQ-Plot ist ein visuelles Tool. Daher musst du immer schauen, ob dir das Ergebnis passt oder die Abweichungen zu groß sind. Es hilft dann manchmal die Daten zum Beispiel einmal zu \\(log\\)-Transformieren und dann die beiden QQ-Plots miteinander zu vergleichen.\nWir brauchen für einen QQ-Plot viele Beobachtungen. Das heißt, wir brauchen auf jeden Fall mehr als 20 Beobachtungen. Dann ist es auch häufig schwierig den QQ-Plot zu bewerten, wenn es viele Behandlungsgruppen oder Blöcke gibt. Am Ende haben wir dann zwar mehr als 20 Beobachtungen aber pro Kombination Behandlung und Block nur vier Wiederholungen. Und vier Wiederholungen sind zu wenig für eine sinnvolle Interpretation eines QQ-Plots.\nGrob gesprochen vergleicht der QQ Plot die Quantile der vorliegenden Beobachtungen, in unserem Fall der Variablen jump_length, mir den Quantilen einer theoretischen Normalverteilung, die sich aus den Daten mit dem Mittelwert und der Standardabweichung von jump_length ergeben würden.\nWir können die Annahme der Normalverteilung recht einfach in ggplot überprüfen. Wir sehen in Abbildung 33.4 den QQ-Plot für die Variable jump_length. Die Punkte sollten alle auf einer Diagonalen liegen. Hier dargestellt durch die rote Linie. Häufig weichen die Punkte am Anfang und Ende der Spannweite der Beobachtungen etwas ab.\n\nggplot(model_tbl, aes(sample = jump_length)) + \n  stat_qq() + \n  stat_qq_line(color = \"red\") +\n  labs(x = \"Theoretischen Quantile der Standardnormalverteilung\",\n       y = \"Quantile der beobachteten Stichprobe\") + \n  theme_bw()\n\n\n\nAbbildung 33.4— QQ-Plot der Sprungweite in [cm]. Die Gerade geht einmal durch die Mitte der Punkte und die Punkte liegen nicht exakt auf der Geraden. Eine leichte Abweichung von der Normalverteilung könnte vorliegen.\n\n\n\nWir werden uns später auch noch häufig die Residuen aus den Modellen anschauen. Die Residuen müssen nach dem Fit des Modells einer Normalverteilung folgen. Wir können diese Annahme an die Residuen mit einem QQ-Plot überprüfen. In Abbildung 33.5 sehen wir die Residuen aus dem Modell fit_1 in einem QQ-Plot. Wir würden sagen, dass die Residuen approximativ normalvertelt sind. Die Punkte liegen fast alle auf der roten Diagonalen.\n\nggplot(resid_plot_tbl, aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line(color = \"red\") +\n  theme_bw() \n\n\n\nAbbildung 33.5— QQ-Plot der Residuen aus dem Modell fit_1. Die Residuen müssen einer approximativen Normalverteilung folgen, sonst hat der Fit des Modelles nicht funktioniert."
  },
  {
    "objectID": "stat-linear-reg-quality.html#modellgüte-mit-dem-r-paket-performance",
    "href": "stat-linear-reg-quality.html#modellgüte-mit-dem-r-paket-performance",
    "title": "33  Maßzahlen der Modelgüte",
    "section": "\n33.7 Modellgüte mit dem R Paket performance\n",
    "text": "33.7 Modellgüte mit dem R Paket performance\n\nAbschließend möchte ich hier nochmal das R Paket performance vorstellen. Wir können mit dem Paket auch die Normalverteilungsannahme der Residuen überprüfen. Das geht ganz einfach mit der Funktion check_normality() in die wir einfach das Objekt mit dem Fit des Modells übergeben.\n\ncheck_normality(fit_1)\n\nOK: residuals appear as normally distributed (p = 0.555).\n\n\nWir haben auch die Möglichkeit uns einen Plot der Modellgüte anzeigen zu lassen. In Abbildung 33.6 sehen wir die Übersicht von bis zu sechs Abbildungen, die uns Informationen zu der Modellgüte liefern. Wir müssen nur den Fit unseres Modells an die Funktion check_model() übergeben.\nDas Schöne an der Funktion ist, dass jeder Subplot eine Beschreibung in Englisch hat, wie der Plot auszusehen hat, wenn alles gut mit dem Modellieren funktioniert hat.\nWir kommen dann in der multiplen linearen Regression nochmal auf das Paket performance zurück. Für dieses Kapitel reicht dieser kurze Abriss.\n\ncheck_model(fit_1, colors = cbbPalette[6:8])\n\n\n\nAbbildung 33.6— Übersicht der Plots zu der Modellgüte aus der Funktion check_model()."
  },
  {
    "objectID": "stat-linear-reg-corr.html#genutzte-r-pakete",
    "href": "stat-linear-reg-corr.html#genutzte-r-pakete",
    "title": "34  Korrelation",
    "section": "\n34.1 Genutzte R Pakete",
    "text": "34.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, readxl,\n               corrplot)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-linear-reg-corr.html#daten",
    "href": "stat-linear-reg-corr.html#daten",
    "title": "34  Korrelation",
    "section": "\n34.2 Daten",
    "text": "34.2 Daten\nWir wollen uns erstmal mit einem einfachen Datenbeispiel beschäftigen. Wir können die Korrelation auf sehr großen Datensätzen berechnen, wie auch auf sehr kleinen Datensätzen. Prinzipiell ist das Vorgehen gleich. Wir nutzen jetzt aber erstmal einen kleinen Datensatz mit \\(n=7\\) Beobachtungen. In der Tabelle 34.1 ist der Datensatz simplel_tbl dargestellt. Wir wollen den Zusammenhang zwischen der Sprungweite in [cm] und dem Gewicht in [mg] für sieben Beobachtungen modellieren.\n\nsimple_tbl &lt;- tibble(jump_length = c(1.2, 1.8, 1.3, 1.7, 2.6, 1.8, 2.7),\n                     weight = c(0.8, 1, 1.2, 1.9, 2, 2.7, 2.8))\n\n\n\n\n\nTabelle 34.1— Datensatz mit einer normalverteilten Variable jump_length und der normalverteilten Variable weight.\n\njump_length\nweight\n\n\n\n1.2\n0.8\n\n\n1.8\n1.0\n\n\n1.3\n1.2\n\n\n1.7\n1.9\n\n\n2.6\n2.0\n\n\n1.8\n2.7\n\n\n2.7\n2.8\n\n\n\n\n\n\nIn Abbildung 34.1 sehen wir die Visualisierung der Daten simple_tbl in einem Scatterplot mit einer geschätzen Gerade. Wir wollen jetzt mit der Korrelation die Steigung der Geraden unabhängig von der Einheit beschreiben. Oder wir wollen die Steigung der Geraden standardisieren auf -1 bis 1.\n\n\n\n\nAbbildung 34.1— Scatterplot der Beobachtungen der Sprungweite in [cm] und dem Gewicht in [mg]. Die Gerade verläuft mittig durch die Punkte."
  },
  {
    "objectID": "stat-linear-reg-corr.html#korrelation-theoretisch",
    "href": "stat-linear-reg-corr.html#korrelation-theoretisch",
    "title": "34  Korrelation",
    "section": "\n34.3 Korrelation theoretisch",
    "text": "34.3 Korrelation theoretisch\nWenn wir zwei verteilte Variablen miteinander korrelieren möchten, dann nutzen wir die Korrelation nach Pearson. Wenn wir nicht-normalverteilte Variablen miteinander korrelieren möchten, dann nutzen wir die Korrelation nach Spearman\nWir schauen uns hier die Korrelation nach Pearson an. Die Korrelation nach Pearson nimmt an, dass beide zu korrelierende Variablen einer Normalverteilung entstammen. Wenn wir keine Normalverteilung vorliegen haben, dann nutzen wir die Korrelation nach Spearman. Die Korrelation nach Spearman basiert auf den Rängen der Daten und ist ein nicht-parametrisches Verfahren. Die Korrelation nach Pearson ist die parametrische Variante. Wir bezeichnen die Korrelation entweder mit \\(r\\) oder dem griechischen Buchstaben \\(\\rho\\) als rho gesprochen.\nWas macht nun die Korrelation? Die Korrelation gibt die Richtung der Geraden an. Oder noch konkreter die Steigung der Geraden normiert auf -1 bis 1. Die Abbildung 34.2 zeigt die Visualisierung der Korrelation für drei Ausprägungen. Eine Korrelation von \\(r = -1\\) bedeutet eine maximale negative Korrelation. Die Gerade fällt in einem 45° Winkel. Eine Korrelation von \\(r = +1\\) bedeutet eine maximale positive Korrelation. Die gerade steigt in einem 45° Winkel. Eine Korrelation von \\(r = 0\\) bedeutet, dass keine Korrelation vorliegt. Die Grade verläuft parallel zur x-Achse.\n\n\n\nAbbildung 34.2— Visualisierung der Korrelation für drei Ausprägungen des Korrelationskoeffizient.\n\n\nIm Folgenden sehen wir die Formel für den Korrelationskoeffizient nach Pearson.\n\\[\n\\rho = r_{x,y} = \\cfrac{s_{x,y}}{s_x \\cdot s_y}\n\\]\nWir berechnen die Korrelation immer zwischen zwei Variablen \\(x\\) und \\(y\\). Es gibt keine multiple Korrelation über mehr als zwei Variablen. Im Zähler der Formel zur Korrelation steht die Kovarianz von \\(x\\) und \\(y\\).\nWir können mit folgender Formel die Kovarianzen zwischen den beiden Variablen \\(x\\) und \\(y\\) berechnen.\n\\[\ns_{x,y} = \\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n\\]\nDie folgende Formel berechnet die quadrierten Abweichung der Beobachtungen von \\(x\\) zum Mittelwert \\(\\bar{x}\\).\n\\[\ns_x = \\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n\\]\nDie folgende Formel berechnet die quadrierten Abweichung der Beobachtungen von \\(y\\) zum Mittelwert \\(\\bar{y}\\).\n\\[\ns_y = \\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n\\]\nIn Tabelle 34.2 ist der Zusammenhang nochmal Schritt für Schrit aufgeschlüsselt. Wir berechnen erst die Abweichungsquadrate von \\(x\\) und die Abweichungsquadrate von \\(y\\). Dann noch die Quadrate der Abstände von \\(x\\) zu \\(y\\). Abschließend summieren wir alles und zeihen noch die Wurzel für die Abweichungsquadrate von \\(x\\) und \\(y\\).\n\n\n\nTabelle 34.2— Tabelle zur Berechnung des Korrelationskoeffizient\n\n\n\n\n\n\n\n\njump_length \\(\\boldsymbol{y}\\)\n\nweight \\(\\boldsymbol{x}\\)\n\n\\(\\boldsymbol{(y_i-\\bar{y})^2}\\)\n\\(\\boldsymbol{(x_i-\\bar{x})^2}\\)\n\\(\\boldsymbol{(x_i-\\bar{x})(y_i-\\bar{y})}\\)\n\n\n\n1.2\n0.8\n0.45\n0.94\n0.65\n\n\n1.8\n1.0\n0.01\n0.60\n0.06\n\n\n1.3\n1.2\n0.33\n0.33\n0.33\n\n\n1.7\n1.9\n0.03\n0.02\n-0.02\n\n\n2.6\n2.0\n0.53\n0.05\n0.17\n\n\n1.8\n2.7\n0.03\n0.86\n-0.07\n\n\n2.7\n2.8\n0.69\n1.06\n0.85\n\n\n\n\\(\\sum\\)\n2.05\n3.86\n1.97\n\n\n\n\\(\\sqrt{\\sum}\\)\n1.43\n1.96\n\n\n\n\n\n\nWir können die Zahlen dann aus der Tabelle in die Formel der Korrelation nach Pearson einsetzen. Wir erhalten eine Korrelation von 0.70 und haben damit eine recht starke positve Korrelation vorliegen.\n\\[\n\\rho = r_{x,y} = \\cfrac{1.97}{1.96 \\cdot 1.43} = 0.70\n\\]\nWir können mit der Funktion cor() in R die Korrelation zwischen zwei Spalten in einem Datensatz berechnen. Wir überprüfen kurz unsere Berechnung und stellen fest, dass wir richtig gerechnet haben.\n\ncor(simple_tbl$jump_length, simple_tbl$weight)\n\n[1] 0.7014985"
  },
  {
    "objectID": "stat-linear-reg-corr.html#korrelation-in-r",
    "href": "stat-linear-reg-corr.html#korrelation-in-r",
    "title": "34  Korrelation",
    "section": "\n34.4 Korrelation in R",
    "text": "34.4 Korrelation in R\nWir nutzen die Korrelation in R selten nrur für zwei Variablen. Meistens schauen wir uns alle numerischen Variablen gemeinsam in einer Abbildung an. Wir nennen diese Abildung auch Korrelationsplot. Faktoren sind keine numerischen Variablen. Daher kann es sein, dass für dein Experiment kein Korrelationsplot in Frage kommt.\nWir schauen uns jetzt nochmal einen die Berechnung für den Datensatz simple_tbl an. Wir müssen für die Korrelation zwischen zwei Variablen diese Variablen mit dem $-Zeichen aus dem Datensatz extrahieren. Die Funktion cor() kann nur mit Vektoren oder ganzen numerischen Datensätzen arbeiten.\nWir können den Korrelationskoeffizienten nach Pearson mit der Option method = \"pearson\" auswählen.\n\ncor(simple_tbl$jump_length, simple_tbl$weight, method = \"pearson\")\n\n[1] 0.7014985\n\n\nWenn wir die nicht-parametrische Variante des Korrelationskoeffizienten nach Spearman berechnen wollen nutzen wir die Option method = \"spearman\".\n\ncor(simple_tbl$jump_length, simple_tbl$weight, method = \"spearman\")\n\n[1] 0.792825\n\n\nWir können auch einen statistischen Test für die Korrelation rechnen. Die Nullhypothese \\(H_0\\) wäre hierbei, dass die Korrelation \\(r = 0\\) ist. Die Funktion cor.test() liefert den entsprechenden \\(p\\)-Wert für die Entscheidung gegen die Nullhypothese.\n\ncor.test(simple_tbl$jump_length, simple_tbl$weight, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  simple_tbl$jump_length and simple_tbl$weight\nt = 2.201, df = 5, p-value = 0.07899\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1092988  0.9517673\nsample estimates:\n      cor \n0.7014985 \n\n\nAus dem Test erhalten wir den \\(p\\)-Wert von \\(0.079\\). Damit liegt der \\(p\\)-Wert über den Signifikanzniveau von \\(\\alpha\\) gleich 5%. Wir können somit die Nullhypothese nicht ablehnen. Wir sehen hier, die Problematik der kleinen Fallzahl. Obwohl unsere Korrelation mit \\(0.7\\) groß ist erhalten wir einen \\(p\\)-Wert, der nicht die Grenze von 5% unterschreitet. Wir sehen, dass die starre Grenze von \\(\\alpha\\) auch Probleme bereitet.\n\n\n\n\n\n\nDie Korrelation als Vergleich zweier Steigungen\n\n\n\nIn der Abbildung 34.3 können wir uns noch einmal den Vorteil der Korrelation als ein einheitsloses Maß anschauen. Wenn wir uns nur die Steigung der beiden Gerade betrachten würden, dann wäre die Steigung \\(\\beta_{kopfgewicht} = 0.021\\) und die Steigung \\(\\beta_{strunkdurchmesser} = 5.15\\). Man könnte meinen, das es keinen Zusammenhang zwischen der Boniturnote und dem Gewicht gäbe wohl aber einen starken Zusammenhang zwischen der Boniturnote und dem Durchmesser. Die Steigung der Geraden wird aber stark von den unterschiedlich skalierten Einheiten von Kopfgewicht in [g] und dem Strunkdurchmesserdurchmesser in [cm] beeinflusst.\n\n\nAbbildung 34.3— Der Zusammenhang von Hohlstrunk Boniturnote und Kopfgewicht sowie Strunkdurchmesser. In dem Beispiel ist gut der Zusammenhang zwischen der Steigung \\(\\beta_1\\) von \\(x\\) und der Einheit von \\(x\\) zu erkennen.\n\nWir wollen den Zusammenhang nochmal mit der Korrelation überprüfen, da die Korrelation nicht durch die Einheiten von \\(y\\) und \\(x\\), in diesem Fall den Einheiten von Kopfgewicht in [g] und dem Durchmesser in [cm], beeinflusst wird. Wir bauen uns zuerst einen künstlichen Datensatz in dem wir die Informationen aus der Geradengleichung nutzen. Dann addieren wir mit der Funktion rnorm() noch einen kleinen Fehler auf jede Beobachtung drauf.\n\nstrunk_tbl &lt;- tibble(durchmesser = seq(3.5, 4.5, by = 0.05),\n                     bonitur = 5.15 * durchmesser - 16.65 + rnorm(length(durchmesser), 0, 1))\nkopf_tbl &lt;- tibble(gewicht = seq(410, 700, by = 2),\n                   bonitur = 0.021 * gewicht - 8.42 + rnorm(length(gewicht), 0, 1))\n\nWir können und jetzt einmal die Korrelation aus den Daten berechnen. Die Koeffizienten der Geraden sind die gleichen Koeffizienten wie in der Abbildung 34.3. Was wir aber sehen, ist das sich die Korrelation für beide Gerade sehr ähnelt oder fast gleich ist.\n\nstrunk_tbl %$% \n  cor(durchmesser, bonitur, method = \"spearman\")\n\n[1] 0.8688312\n\nkopf_tbl %$% \n  cor(gewicht, bonitur, method = \"spearman\")\n\n[1] 0.8807708\n\n\nWie wir sehen, können wir mit der Korrelation sehr gut verschiedene Zusammenhänge vergleichen. Insbesondere wenn die Gerade zwar das gleiche Outcome haben aber eben verschiedene Einheiten auf der \\(x\\)-Achse. Prinzipiell geht es natürlich auch für die Einheiten auf der \\(y\\)-Achse, aber meistens ist das Outcome der konstante Modellteil.\n\n\nAbschließend wollen wir uns noch die Funktion corrplot() aus dem gleichnamigen R Paket corrplot anschauen. Die Hilfeseite zum Paket ist sehr ausführlich und bietet noch eine Reihe an anderen Optionen. Wir benötigen dafür einen etwas größeren Datensatz mit mehreren numerischen Variablen. Wir nutzen daher den Gummibärchendatensatz und selektieren die Spalten count_bears bis semester aus.\n\ncorr_gummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") %&gt;% \n  select(count_bears:semester)\n\nWir brauchen für die Funktion corrplot() eine Matrix mit den paarweisen Korrelationen. Wir können diese Matrix wiederum mit der Funktion cor() erstellen. Wir müssen dazu aber erstmal alle numerischen Variablen mit select_if() selektieren und dann alle fehlenden Werte über na.omit() entfernen.\n\ncor_mat &lt;- corr_gummi_tbl %&gt;% \n  select_if(is.numeric) %&gt;% \n  na.omit %&gt;% \n  cor()\n\ncor_mat %&gt;% round(3)\n\n            count_bears count_color    age height semester\ncount_bears       1.000       0.328  0.191 -0.046    0.203\ncount_color       0.328       1.000 -0.016 -0.141    0.018\nage               0.191      -0.016  1.000 -0.018    0.108\nheight           -0.046      -0.141 -0.018  1.000    0.021\nsemester          0.203       0.018  0.108  0.021    1.000\n\n\nWir sehen das in der Korrelationsmatrix jeweils über und unterhalb der Diagonalen die gespiegelten Zahlen stehen. Wir können jetzt die Matrix cor_mat in die Funktion corrplot() stecken und uns den Korrelationsplot in Abbildung 34.4 einmal anschauen.\n\ncorrplot(cor_mat)\n\n\n\nAbbildung 34.4— Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke.\n\n\n\nWir sehen in Abbildung 34.4, dass wir eine schwache positive Korrelation zwischen count_color und count_bears haben, angezeigt durch den schwach blauen Kreis. Der Rest der Korrelation ist nahe Null, tendiert aber eher ins negative.\nNun ist in dem Plot natürlich eine der beiden Seiten überflüssig. Wir können daher die Funktion corrplot.mixed() nutzen um in das untere Feld die Zahlenwerte der Korrelation darzustellen.\n\ncorrplot.mixed(cor_mat)\n\n\n\nAbbildung 34.5— Farbiger paarweiser Korrelationsplot für die numerischen Variablen aus dem Datensatz zu den Gummibärchen. Die Farben spiegeln die Richtung der Korrelation wieder, die Größe der Kreise die Stärke. In das untere Feld werden die Werte der Korrelation angegeben.\n\n\n\nEs gibt noch eine Vielzahl an weiteren Möglichkeiten in den Optionen von der Funktion corr.mixed(). Hier hilft dann die Hilfeseite der Funktion oder aber die Hilfeseite zum Paket."
  },
  {
    "objectID": "stat-modeling-preface.html#genutzte-r-pakete-für-die-folgenden-kapitel",
    "href": "stat-modeling-preface.html#genutzte-r-pakete-für-die-folgenden-kapitel",
    "title": "Statistisches Modellieren",
    "section": "Genutzte R Pakete für die folgenden Kapitel",
    "text": "Genutzte R Pakete für die folgenden Kapitel\nNeben den R Paketen, die wir in den jeweiligen Kapiteln brauchen, kommen noch folgende R Pakete immer wieder dran. Deshalb sind die R Pakete hier schonmal mit den jeweiligen Internetseiten aufgeführt.\n\nDas Buch Tidy Modeling with R gibt nochmal einen tieferen Einblick in das Modellieren in R. Wir immer, es ist ein Vorschlag aber kein Muss.\nDas R Paket parameters nutzen wir um die Parameter eines Modells aus den Fits der Modelle zu extrahieren. Teilweise sind die Standardausgaben der Funktionen sehr unübersichtich. Hier hilft das R Paket.\nDas R Paket performance hilft uns zu verstehen, ob die Modelle, die wir gefittet haben, auch funktioniert haben. In einen mathematischen Algorithmus können wir alles reinstecken, fast immer kommt eine Zahl wieder raus.\nDas R Paket tidymodels nutzen wir als das R Paket um mit Modellen umgehen zu können und eine Vorhersage neuer Daten zu berechnen. Das Paket tidymodels ist wie das Paket tidyverse eine Sammlung an anderen R Paketen, die wir brauchen werden."
  },
  {
    "objectID": "stat-modeling-preface.html#generalisierung-von-lm-zu-glm-und-glmer",
    "href": "stat-modeling-preface.html#generalisierung-von-lm-zu-glm-und-glmer",
    "title": "Statistisches Modellieren",
    "section": "Generalisierung von lm() zu glm() und [g]lmer()\n",
    "text": "Generalisierung von lm() zu glm() und [g]lmer()\n\n\nDie Funktion lm() nutzen wir, wenn das Outcome \\(y\\) einer Normalverteilung folgt.\nDie Funktion glm() nutzen wir, wenn das Outcome \\(y\\) einer andere Verteilung folgt.\nDie Funktion lmer() nutzen wir, wenn das Outcome \\(y\\) einer Normalverteilung folgt und wir noch einen Block- oder Clusterfaktor vorliegen haben.\nDie Funktion glmer() nutzen wir, wenn das Outcome \\(y\\) einer andere Verteilung folgt und wir noch einen Block- oder Clusterfaktor vorliegen haben.\n\n\n\n\nAbbildung 1— Übersicht der Namen der Funktionen in R für das lm(), glm() und glmer().\n\n\nIn Abbildung 2 sehen wir wie wir den Namen einer Regression bilden. Zuerst entscheiden wir, ob wir nur ein \\(x\\) haben oder mehrere. Mit einem \\(x\\) sprechen wir von einem simplen Modell, wenn wir mehrere \\(x\\) haben wir ein multiples Modell. Im nächsten Schritt benennen wir die Verteilung für das Outcome \\(y\\). Dann müssen wir noch entscheiden, ob wir ein gemischtes Modell vorliegen haben, dann schreiben wir das hin. Sonst lassen wir den Punkt leer. Anschließend kommt noch lineares Modell hinten ran.\n\n\n\nAbbildung 2— Wie bilde ich den Namen einer Regression? Erst beschreiben wir das \\(x\\), dann das \\(y\\). Am Ende müssen wir noch sagen, ob wir ein gemischtes Modell vorliegen haben oder nicht."
  },
  {
    "objectID": "stat-modeling-preface.html#das-regressionskreuz",
    "href": "stat-modeling-preface.html#das-regressionskreuz",
    "title": "Statistisches Modellieren",
    "section": "Das Regressionskreuz",
    "text": "Das Regressionskreuz\nIn diesem Kapitel wollen wir uns mit der Grundlage der multiplen linearen Regression beschäftigen. Das heist wir schauen uns Modelle mit mehreren \\(x\\) an. Wir haben also nicht mehr nur eine Einflussvariable auf der rechten Seite der Gleichung sondern meist mehrere. Je nachdem wie diese \\(x\\) beschffen sind, müssen wir die \\(x\\) auch interpretieren. Wir unterscheiden in vier Arten von \\(x\\).\n\nDas zu betrachtende \\(x\\) ist eine Variable mit kontinuierlichen Zahlen (siehe Kapitel 35.3.1)\nDas zu betrachtende \\(x\\) ist eine Variablen mit einem Faktor mit zwei Leveln (siehe Kapitel 35.3.2).\nDas zu betrachtende \\(x\\) ist eine Variablen mit einem Faktor mit mehr als zwei Leveln (siehe Kapitel 35.3.3).\nDas zu betrachtende \\(x\\) ist eine Variable, die einen Block oder Cluster beschreibt (siehe Kapitel 44).\n\nWir finden die Fälle 1) bis 3) in der Abbildung 3 in den Spalten wieder.\nNeben dem \\(x\\) müssen wir auch das \\(y\\) in diesem Kapitel betrachten. Das \\(y\\) kann aus verschiedenen Verteilungen kommen. Häufig nehmen wir an, dass das \\(y\\) normalverteilt ist, das muss das \\(y\\) aber nicht sein. Je nachdem wir das \\(y\\) verteilt ist, rechen wir eine andere multiple Regression.\n\nDas zu betrachtende \\(y\\) folgt einer Normalverteilung bzw. entstammt einer Gaussian Vertreilungsfamilie. Wir wollen dann eine multiple Gaussian Regression rechen.\nDas zu betrachtende \\(y\\) folgt einer Poissonverteilung bzw. entstammt einer Poisson Vertreilungsfamilie. Wir wollen dann eine multiple Poisson Regression rechen.\nDas zu betrachtende \\(y\\) folgt einer Ordinalen- oder Multinominalenverteilung bzw. entstammt einer Ordinalen- oder Multinominalen Vertreilungsfamilie. Wir wollen dann eine multiple ordinale oder multinominale Regression rechen.\nDas zu betrachtende \\(y\\) folgt einer Binomialverteilung bzw. entstammt einer Binomialen Vertreilungsfamilie. Wir wollen dann eine multiple logistische Regression rechen.\n\nWir finden die Fälle 1) bis 4) in der Abbildung 3 in den Zeilen wieder.\n\n\n\nAbbildung 3— Das Regressionskreuz als allgemeine Übersicht der Möglichkeiten einer multiplen linearen Regression."
  },
  {
    "objectID": "stat-modeling-basic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-basic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "35  Multiple lineare Regression",
    "section": "\n35.1 Genutzte R Pakete für das Kapitel",
    "text": "35.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom,\n               see, performance, car, parameters,\n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"set_names\", \"magrittr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-modell-matrix",
    "href": "stat-modeling-basic.html#sec-modell-matrix",
    "title": "35  Multiple lineare Regression",
    "section": "\n35.2 Die Modellmatrix in R",
    "text": "35.2 Die Modellmatrix in R\nWozu brauche ich das? Eine gute Frage. Du brauchst das Verständnis der Modellmatrix, wenn du verstehen willst wie R in einer linearen Regression zu den Koeffizienten kommt…\nAls erstes wollen wir verstehen, wie ein Modell in R aussieht. Dann können wir auch besser verstehen, wie die eigentlichen Koeffizienten aus dem Modell entstehen. Wir bauen uns dafür ein sehr simplen Datensatz. Wir bauen uns einen Datensatz mit Schlangen.\n\nsnake_tbl &lt;- tibble(\n  svl = c(40, 45, 39, 51, 52, 57, 58, 49),\n  mass = c(6, 8, 5, 7, 9, 11, 12, 10),\n  region = as_factor(c(\"west\",\"west\", \"west\", \"nord\",\"nord\",\"nord\",\"nord\",\"nord\")),\n  color = as_factor(c(\"schwarz\", \"schwarz\", \"rot\", \"rot\", \"rot\", \"blau\", \"blau\", \"blau\"))\n) \n\nIn der Tabelle 35.1 ist der Datensatz snake_tbl nochmal dargestellt. Wir haben die Schlangenlänge svl als Outcome \\(y\\) sowie das Gewicht der Schlangen mass, die Sammelregion region und die Farbe der Schlangen color. Dabei ist mass eine kontinuierliche Variable, region eine kategorielle Variable als Faktor mit zwei Leveln und color eine kategorielle Variable als Faktor mit drei Leveln.\n\n\n\n\nTabelle 35.1— Datensatz zu Schlangen ist entlehnt und modifiiert nach Kéry (2010, p. 77)\n\n\nsvl\nmass\nregion\ncolor\n\n\n\n40\n6\nwest\nschwarz\n\n\n45\n8\nwest\nschwarz\n\n\n39\n5\nwest\nrot\n\n\n51\n7\nnord\nrot\n\n\n52\n9\nnord\nrot\n\n\n57\n11\nnord\nblau\n\n\n58\n12\nnord\nblau\n\n\n49\n10\nnord\nblau\n\n\n\n\n\n\nWir wollen uns nun einmal anschauen, wie ein Modell in R sich zusammensetzt. Je nachdem welche Spalte \\(x\\) wir verwenden um den Zusammenhang zum \\(y\\) aufzuzeigen.\n\n35.2.1 Kontinuierliches \\(x\\)\n\nIm ersten Schritt wollen wir uns einmal das Modell mit einem kontinuierlichen \\(x\\) anschauen. Daher bauen wir uns ein lineares Modell mit der Variable mass. Wir erinnern uns, dass mass eine kontinuierliche Variable ist, da wir hier nur Zahlen in der Spalte finden. Die Funktion model.matrix() gibt uns die Modellmatrix wieder.\n\nmodel.matrix(svl ~ mass, data = snake_tbl) %&gt;% as_tibble\n\n# A tibble: 8 × 2\n  `(Intercept)`  mass\n          &lt;dbl&gt; &lt;dbl&gt;\n1             1     6\n2             1     8\n3             1     5\n4             1     7\n5             1     9\n6             1    11\n7             1    12\n8             1    10\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte mass als kontinuierliche Variable.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 6 \\\\\n  1 & 8 \\\\\n  1 & 5 \\\\\n  1 & 7 \\\\\n  1 & 9 \\\\\n  1 & 11\\\\\n  1 & 12\\\\\n  1 & 10\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_{mass}\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt mit der Funktion lm() fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten aus dem Objekt fit_1 wiedergeben zu lassen.\n\nfit_1 &lt;- lm(svl ~ mass, data = snake_tbl) \nfit_1 %&gt;% coef %&gt;% round(2)\n\n(Intercept)        mass \n      26.71        2.61 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_1 wieder.\n\nfit_1 %&gt;% residuals() %&gt;% round(2)\n\n    1     2     3     4     5     6     7     8 \n-2.36 -2.57 -0.75  6.04  1.82  1.61  0.00 -3.79 \n\n\nWir können jetzt die Koeffizienten in die Modellmatrix ergänzen. Wir haben den Intercept mit \\(\\beta_0 = 26.71\\) geschätzt. Weiter ergänzen wir die Koeffizienten aus dem linearen Modell für mass mit \\(\\beta_{mass}=2.61\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein. Wir erhalten dann folgende ausgefüllte Gleichung mit den Matrixen.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  26.71 & \\phantom{0}6 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}8 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}5 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}7 \\cdot 2.61\\\\\n  26.71 & \\phantom{0}9 \\cdot 2.61\\\\\n  26.71 & 11\\cdot 2.61\\\\\n  26.71 & 12\\cdot 2.61\\\\\n  26.71 & 10\\cdot 2.61\\\\\n\\end{pmatrix}\n  +\n  \\begin{pmatrix}\n  -2.36\\\\\n  -2.57\\\\\n  -0.75 \\\\\n  +6.04\\\\\n  +1.82\\\\\n  +1.61\\\\\n  \\phantom{+}0.00\\\\\n  -3.79\\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis. Wie du siehst ergänzen wir hier noch eine Reihe von \\(+\\) um den Intercept mit der Steigung zu verbinden. Steht ja auch so in der Gleichung des linearen Modells drin, alles wird mit einem \\(+\\) miteinander verbunden.\n\nc(26.71 +  6*2.61 - 2.36,\n  26.71 +  8*2.61 - 2.57,\n  26.71 +  5*2.61 - 0.75,\n  26.71 +  7*2.61 + 6.04,\n  26.71 +  9*2.61 + 1.82,\n  26.71 + 11*2.61 + 1.61,\n  26.71 + 12*2.61 + 0.00,\n  26.71 + 10*2.61 - 3.79) %&gt;% round() \n\n[1] 40 45 39 51 52 57 58 49\n\n\nOh ha! Die Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat. Das heißt, die ganze Sache hat funktioniert.\n\n35.2.2 Kategorielles \\(x\\) mit 2 Leveln\nIm diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen \\(x\\) mit 2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable region``. Die Funktionmodel.matrix()` gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ region, data = snake_tbl) %&gt;% as_tibble\n\n# A tibble: 8 × 2\n  `(Intercept)` regionnord\n          &lt;dbl&gt;      &lt;dbl&gt;\n1             1          0\n2             1          0\n3             1          0\n4             1          1\n5             1          1\n6             1          1\n7             1          1\n8             1          1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte regionnord. In dieser Spalte steht die Dummykodierung für die Variable region. Die ersten drei Schlangen kommen nicht aus der Region nord und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region nord und erhalten daher eine 1 in der Spalte.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 &  0  \\\\\n  1 &  0 \\\\\n  1 &  0\\\\\n  1 &  1\\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n  1 &  1 \\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta^{region}_{nord} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen.\n\nfit_2 &lt;- lm(svl ~ region, data = snake_tbl) \nfit_2 %&gt;% coef %&gt;% round(2)\n\n(Intercept)  regionnord \n      41.33       12.07 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_2 wieder.\n\nfit_2 %&gt;% residuals() %&gt;% round(2)\n\n    1     2     3     4     5     6     7     8 \n-1.33  3.67 -2.33 -2.40 -1.40  3.60  4.60 -4.40 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 41.33\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für die Region und das Level nord mit \\(\\beta^{region}_{nord} = 12.07\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 0 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n  41.33 & 1 \\cdot 12.07  \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  -1.33\\\\\n  +3.67 \\\\\n  -2.33 \\\\\n  -2.40 \\\\\n  -1.40 \\\\\n  +3.60 \\\\\n  +4.60 \\\\\n  -4.40 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(41.33 + 0*12.07 - 1.33,\n  41.33 + 0*12.07 + 3.67,\n  41.33 + 0*12.07 - 2.33,\n  41.33 + 1*12.07 - 2.40,\n  41.33 + 1*12.07 - 1.40,\n  41.33 + 1*12.07 + 3.60,\n  41.33 + 1*12.07 + 4.60,\n  41.33 + 1*12.07 - 4.40) %&gt;% round() \n\n[1] 40 45 39 51 52 57 58 49\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat.\n\n35.2.3 Kategorielles \\(x\\) mit &gt;2 Leveln\nIm diesem Schritt wollen wir uns einmal das Modell mit einem kategoriellen \\(x\\) mit &gt;2 Leveln anschauen. Wir bauen uns ein Modell mit der Variable color. Die Funktion model.matrix() gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ color, data = snake_tbl) %&gt;% as_tibble\n\n# A tibble: 8 × 3\n  `(Intercept)` colorrot colorblau\n          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1             1        0         0\n2             1        0         0\n3             1        1         0\n4             1        1         0\n5             1        1         0\n6             1        0         1\n7             1        0         1\n8             1        0         1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalten für color. Die Spalten colorrot und colorblau geben jeweils an, ob die Schlange das Level rot hat oder blau oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 0 & 0 \\\\\n  1 & 0 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 1 & 0\\\\\n  1 & 0 & 1\\\\\n  1 & 0 & 1\\\\\n  1 & 0 & 1\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta^{color}_{rot} \\\\\n  \\beta^{color}_{blau} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen.\n\nfit_3 &lt;- lm(svl ~ color, data = snake_tbl) \nfit_3 %&gt;% coef %&gt;% round(2)\n\n(Intercept)    colorrot   colorblau \n      42.50        4.83       12.17 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_3 wieder.\n\nfit_3 %&gt;% residuals() %&gt;% round(2)\n\n    1     2     3     4     5     6     7     8 \n-2.50  2.50 -8.33  3.67  4.67  2.33  3.33 -5.67 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 25\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für die Farbe und das Level rot mit \\(\\beta^{color}_{rot} = 4.83\\) und für die Farbe und das Level blau mit \\(\\beta^{color}_{blau} = 12.17\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  42.50 & 0 \\cdot 4.83& 0 \\cdot 12.17 \\\\\n  42.50 & 0 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 1 \\cdot 4.83& 0 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n  42.50 & 0 \\cdot 4.83& 1 \\cdot 12.17\\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  -2.50 \\\\\n  +2.50 \\\\\n  -8.33 \\\\\n  +3.67 \\\\\n  +4.67 \\\\\n  +2.33 \\\\\n  +3.33 \\\\\n  -5.67 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(42.50 + 0*4.83 + 0*-12.17 - 2.50,\n  42.50 + 0*4.83 + 0*-12.17 + 2.50,\n  42.50 + 1*4.83 + 0*-12.17 - 8.33,\n  42.50 + 1*4.83 + 0*-12.17 + 3.67,\n  42.50 + 1*4.83 + 0*-12.17 + 4.67,\n  42.50 + 0*4.83 + 1*-12.17 + 2.33,\n  42.50 + 0*4.83 + 1*-12.17 + 3.33,\n  42.50 + 0*4.83 + 1*-12.17 - 5.67) %&gt;% round()\n\n[1] 40 45 39 51 52 33 34 25\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat.\n\n35.2.4 Das volle Modell\nIm letzten Schritt wollen wir uns einmal das volle Modell anschauen. Wir bauen uns ein Modell mit allen Variablen in dem Datensatz snake_tbl. Die Funktion model.matrix() gibt uns die Modelmatrix wieder.\n\nmodel.matrix(svl ~ mass + region + color, data = snake_tbl) %&gt;% as_tibble() \n\n# A tibble: 8 × 5\n  `(Intercept)`  mass regionnord colorrot colorblau\n          &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1             1     6          0        0         0\n2             1     8          0        0         0\n3             1     5          0        1         0\n4             1     7          1        1         0\n5             1     9          1        1         0\n6             1    11          1        0         1\n7             1    12          1        0         1\n8             1    10          1        0         1\n\n\nIn der ersten Spalte ist der Intercept angegeben, danach folgt dann die Spalte mass als kontenuierliche Variable. In der Spalte regionnord steht die Dummykodierung für die Variable region. Die ersten drei Schlangen kommen nicht aus der Region nord und werden deshalb mit einen Wert von 0 versehen. Die nächsten vier Schlangen kommen aus der Region nord und erhalten daher eine 1 in der Spalte. Die nächsten beiden Spalten sind etwas komplizierter. Die Spalten colorrot und colorblau geben jeweils an, ob die Schlange das Level rot hat oder blau oder keins von beiden. Wenn die Schlange weder rot noch blau ist, dann sind beide Spalten mit einer 0 versehen. Dann ist die Schlange schwarz.\nWir können die Modellmatrix auch mathematisch schreiben und die \\(y\\) Spalte für das Outcome svl ergänzen. Eben so ergänzen wir die \\(\\beta\\)-Werte als mögliche Koeefizienten aus der linearen Regression sowie die Residuen als Abweichung von der gefitteten Gerade.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  1 & 6 & 0 & 0 & 0 \\\\\n  1 & 8 & 0 & 0 & 0\\\\\n  1 & 5 & 0 & 1 & 0\\\\\n  1 & 7 & 1 & 1 & 0\\\\\n  1 & 9 & 1 & 1 & 0\\\\\n  1 & 11& 1 & 0 & 1\\\\\n  1 & 12& 1 & 0 & 1\\\\\n  1 & 10& 1 & 0 & 1\\\\\n\\end{pmatrix}\n\\times\n  \\begin{pmatrix}\n  \\beta_0 \\\\\n  \\beta_{mass} \\\\\n  \\beta^{region}_{nord} \\\\\n  \\beta^{color}_{rot} \\\\\n  \\beta^{color}_{blau} \\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\epsilon_1 \\\\\n  \\epsilon_2 \\\\\n  \\epsilon_3 \\\\\n  \\epsilon_4 \\\\\n  \\epsilon_5 \\\\\n  \\epsilon_6 \\\\\n  \\epsilon_7 \\\\\n  \\epsilon_8 \\\\\n\\end{pmatrix}\n\\]\nJetzt brauche wir die Koeffizienten aus dem linearen Modell, welches wir wie folgt fitten. Wir nutzen dann die Funktion coef() um uns die Koeffizienten wiedergeben zu lassen.\n\nfit_4 &lt;- lm(svl ~ mass + region + color, data = snake_tbl) \nfit_4 %&gt;% coef %&gt;% round(2)\n\n(Intercept)        mass  regionnord    colorrot   colorblau \n      25.00        2.50        5.00        1.50       -2.83 \n\n\nDie Funktion residuals() gibt uns die Residuen der Geraden aus dem Objekt fit_4 wieder.\n\nfit_4 %&gt;% residuals() %&gt;% round(2)\n\n    1     2     3     4     5     6     7     8 \n 0.00  0.00  0.00  2.00 -2.00  2.33  0.83 -3.17 \n\n\nWir können jetzt die Koeffizienten ergänzen mit \\(\\beta_0 = 25\\) für den Intercept. Weiter ergänzen wir die Koeffizienten für mass mit \\(\\beta_{mass}=2.5\\), für Region und das Level nord mit \\(\\beta^{region}_{nord} = 5\\), für die Farbe und das Level rot mit \\(\\beta^{color}_{rot} = 1.5\\) und für die Farbe und das Level blau mit \\(\\beta^{color}_{blau} = -2.83\\). Ebenfalls setzen wir Werte für die Residuen für jede der Beobachtungen in die Gleichung ein.\n\\[\n\\begin{pmatrix}\n  40 \\\\\n  45 \\\\\n  39 \\\\\n  50 \\\\\n  52 \\\\\n  57 \\\\\n  58 \\\\\n  59 \\\\\n\\end{pmatrix}\n=\n  \\begin{pmatrix}\n  25 & \\phantom{0}6 \\cdot 2.5 & 0 \\cdot 5 & 0 \\cdot 1.5& 0 \\cdot -2.83 \\\\\n  25 & \\phantom{0}8 \\cdot 2.5 & 0 \\cdot 5 & 0 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}5 \\cdot 2.5 & 0 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}7 \\cdot 2.5 & 1 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & \\phantom{0}9 \\cdot 2.5 & 1 \\cdot 5 & 1 \\cdot 1.5& 0 \\cdot -2.83\\\\\n  25 & 11\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n  25 & 12\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n  25 & 10\\cdot 2.5 & 1 \\cdot 5 & 0 \\cdot 1.5& 1 \\cdot -2.83\\\\\n\\end{pmatrix} +\n  \\begin{pmatrix}\n  \\phantom{+}0.00 \\\\\n  \\phantom{+}0.00 \\\\\n  \\phantom{+}0.00 \\\\\n  +2.00 \\\\\n  -2.00 \\\\\n  +2.33 \\\\\n  +0.83 \\\\\n  -3.17 \\\\\n\\end{pmatrix}\n\\]\nWir können jetzt diese gewaltige Sammlung an Matrixen einmal auflösen. Steht denn nun wirklich rechts das Gleiche wie links von der Gleichung? Wir bauen uns die Zahlen von der rechten Seite der Gleichung einmal in R nach und schauen auf das Ergebnis.\n\nc(25 +  6*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,\n  25 +  8*2.5 + 0*5 + 0*1.5 + 0*-2.83 + 0.00,\n  25 +  5*2.5 + 0*5 + 1*1.5 + 0*-2.83 + 0.00,\n  25 +  7*2.5 + 1*5 + 1*1.5 + 0*-2.83 + 2.00,\n  25 +  9*2.5 + 1*5 + 1*1.5 + 0*-2.83 - 2.00,\n  25 + 11*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 2.33,\n  25 + 12*2.5 + 1*5 + 0*1.5 + 1*-2.83 + 0.83,\n  25 + 10*2.5 + 1*5 + 0*1.5 + 1*-2.83 - 3.17) \n\n[1] 40 45 39 51 52 57 58 49\n\n\nDie Zahlen, die wir rauskriegen, sind die gleichen Werte die unser Outcome \\(y\\) hat. Was haben wir gelernt?\n\nIn einem Modell gibt es immer ein Faktorlevel weniger als ein Faktor Level hat. Die Information des alphanumerisch ersten Levels steckt dann mit in dem Intercept.\nIn einem Modell geht eine kontinuierliche Variable als eine Spalte mit ein.\nIn einem Modell gibt es immer nur eine Spalte für die Residuen und damit nur eine Residue für jede Beobachtung, egal wie viele Variablen ein Modell hat."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-interpret-x",
    "href": "stat-modeling-basic.html#sec-interpret-x",
    "title": "35  Multiple lineare Regression",
    "section": "\n35.3 Interpretation von \\(x\\)\n",
    "text": "35.3 Interpretation von \\(x\\)\n\nWi interpretiee wir nun das Ergebnis einer linearen Regression? Zum einen nutzen wir häufig das Modell nur um das Modell dann weiter in einem multiplen Vergleich zu nutzen. Hier wollen wir uns jetzt aber wirklich die Koeffizienten aus einer multiplen linearen Regression anschauen udn diese Zahlen einmal interpretieren. Wichtig ist, dass wir ein normalverteiltes \\(y\\) vorliegen haben und uns verschiedene Formen des \\(x\\) anschauen. Wir betrachten ein kontinuierliches \\(x\\), ein kategorielles \\(x\\) mit zwei Leveln und ein kategorielles \\(x\\) mit drei oder mehr Leveln.\n\n35.3.1 Kontinuierliches \\(x\\)\n\nBauen wir uns also einmal einen Datensatz mit einem kontinuierlichen \\(x\\) und einem normalverteilten \\(y\\). Unser \\(x\\) soll von 1 bis 7 laufen. Wir erschaffen uns das \\(y\\) indem wir das \\(x\\) mit 1.5 multiplizieren, den \\(y\\)-Achsenabschnitt von 5 addieren und einen zufälligen Fehler aus einer Normalverteilung mit \\(\\mathcal{N}(0, 1)\\) aufaddieren. Wir haben also eine klassische Regressionsgleichung mit \\(y = 5 + 1.5 \\cdot x\\).\n\nset.seed(20137937)\ncont_tbl &lt;- tibble(x = seq(from = 1, to = 7, by = 1),\n                   y = 5 + 1.5 * x + rnorm(length(x), 0, 1))\ncont_tbl\n\n# A tibble: 7 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1  5.93\n2     2  8.06\n3     3  8.95\n4     4 12.1 \n5     5 11.9 \n6     6 14.9 \n7     7 16.3 \n\n\nWenn wir keinen Fehler addieren würden, dann hätten wir auch eine Linie von Punkten wie an einer Perlschnur aufgereiht. Dann wäre das \\(R^2 = 1\\) und wir hätten keine Varianz in den Daten. Das ist aber in einem biologischen Setting nicht realistisch, dass unser \\(y\\) vollständig von \\(x\\) erklärt wird.\nSchauen wir uns nochmal die Modellmatrix an. Hier erwartet uns aber keine Überraschung. Wir schätzen den Intercept und dann kommt der Wert für jedes \\(x\\) in der zweiten Spalte.\n\nmodel.matrix(y ~ x, data = cont_tbl)\n\n  (Intercept) x\n1           1 1\n2           1 2\n3           1 3\n4           1 4\n5           1 5\n6           1 6\n7           1 7\nattr(,\"assign\")\n[1] 0 1\n\n\nIm Folgenden schätzen wir jetzt das lineare Modell um die Koeffizienten der Geraden zu erhalten. Wir nutzen die Funktion model_parameter() aus dem R Paket parameters für die Ausgabe der Koeffizienten.\n\nlm(y ~ x, data = cont_tbl) %&gt;% \n  model_parameters() %&gt;% \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |        4.32\nx           |        1.71\n\n\nSteigt das \\(x\\) um 1 Einheit an, so erhöht sich das \\(y\\) um den Wert der Steigung von \\(x\\).\nWir erhalten einen Intercept von \\(4.32\\) und eine Steigung von \\(x\\) mit \\(1.71\\). Wichtig nochmal, wir haben uns hier zufällige Zahlen erstellen lassen. Wenn du oben den Fehler rausnimmst, dann erhälst du auch die exakten Zahlen für den Intercept und die Steigung von \\(x\\) wieder. Wir sehen also, wenn wir ein kontinuierliches \\(x\\) haben, dann können wir das \\(x\\) in dem Sinne einr Steigung interpretieren. Steigt das \\(x\\) um 1 Einheit an, so erhöht sich das \\(y\\) um den Wert der Steigung von \\(x\\).\nIn Abbildung 35.1 sehen wir den Zusammenhang nochmal graphisch dargestellt. Wir sehen, dass wir die voreingestellten Parameter von \\(\\beta_0 = 5\\) und \\(\\beta_1 = 1.5\\) fast treffen.\n\n\n\n\nAbbildung 35.1— Graphische Darstellung der Interpretation von einem kontinuierlichen \\(x\\).\n\n\n\n\n35.3.2 Kategorielles \\(x\\) mit 2 Leveln\nEtwas anders wird der Fall wenn wir ein kategorielles \\(x\\) mit 2 Leveln vorliegen haben. Wir bauen faktisch zwei Punktetürme an zwei \\(x\\) Positionen auf. Dennoch können wir durch diese Punkte eine Gerade zeichnen. Bauen wir uns erst die Daten mit der Funktion rnorm(). Wir haben zwei Gruppen vorliegen, die Gruppe A hat sieben Beobachtungen und einen Mittelwert von 10. Die Gruppe B hat ebenfalls sieben Beobchatungen und einen Mittelwert von 15. Der Effekt zwischen den beiden Gruppen A und B ist die Mittelwertsdifferenz \\(\\Delta_{A-B}\\) ist somit 5. Wir erhlalten dann folgenden Datensatz wobei die Werte der Gruppe A um die 10 streuen und die Werte der Gruppe B um die 15 streuen.\n\nset.seed(20339537)\ncat_two_tbl &lt;- tibble(A = rnorm(n = 7, mean = 10, sd = 1),\n                      B = rnorm(n = 7, mean = 15, sd = 1)) %&gt;% \n  gather(key = x, value = y) %&gt;% \n  mutate(x = as_factor(x))\ncat_two_tbl\n\n# A tibble: 14 × 2\n   x         y\n   &lt;fct&gt; &lt;dbl&gt;\n 1 A     10.0 \n 2 A     10.8 \n 3 A     10.7 \n 4 A     11.0 \n 5 A      9.25\n 6 A      8.98\n 7 A      9.71\n 8 B     15.1 \n 9 B     16.0 \n10 B     14.5 \n11 B     15.1 \n12 B     14.2 \n13 B     16.8 \n14 B     15.6 \n\n\nWir wollen uns wieder die Modellmatrix einmal anschauen. Wir sehen hier schon einen Unterschied. Zum einen sehen wir, dass der Intercept für alle Beobachtungen geschätzt wird und in der zweiten Spalte nur xB steht. Somit werden in der zweiten Spalte nur die Beobachtungen in der Gruppe B berücksichtigt.\n\nmodel.matrix(y ~ x, data = cat_two_tbl)\n\n   (Intercept) xB\n1            1  0\n2            1  0\n3            1  0\n4            1  0\n5            1  0\n6            1  0\n7            1  0\n8            1  1\n9            1  1\n10           1  1\n11           1  1\n12           1  1\n13           1  1\n14           1  1\nattr(,\"assign\")\n[1] 0 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$x\n[1] \"contr.treatment\"\n\n\nWir rechnen einmal das lineare Modell und lassen uns überraschen was als Ergebnis herauskomt. Wir nutzen die Funktion model_parameter() aus dem R Paket parameters für die Ausgabe der Koeffizienten.\n\nlm(y ~ x, data = cat_two_tbl) %&gt;% \n  model_parameters() %&gt;% \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |       10.07\nxB          |        5.27\n\n\nNun erhalten wir als den Intercept 10.07 und die Steigung xB mit 5.27 zurück. Wir sehen, der Intercept ist der Mittelwert der Gruppe A und die das xB ist die Änderung von dem Mittelwert A zu dem Mittelwert B. Wir erhalten die Mittelwertsdifferenz \\(\\Delta_{A-B}\\) von 5.27 zurück. In Tabelle 35.2 siehst du den Zusammenhang von den Faktorleveln A und B, den jeweiligen Mittelwerte der Level sowie die Differenz zum Mittel von dem ersten Level A.\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\nTabelle 35.2— Zusammenhang von den Mittelwerten der Level des Faktores \\(x\\) und deren Differenz zu Level A.\n\nFactor x\nMean of level\nDifference to level A\n\n\n\nA\n10.07\n0.00\n\n\nB\n15.33\n5.27\n\n\n\n\n\n\nSteigt das \\(x\\) um 1 Einheit an, also springt von Gruppe A zu Gruppe B, so erhöht sich das \\(y\\) um den Mittelwertsunterschied \\(\\Delta_{A-B}\\).\nIn Abbildung 35.2 kannst du nochmal den visuellen Zusammenhang zwischen den einzelnen Beobachtungen und der sich ergebenen Geraden sehen. Die Gerade geht durch die beiden Mittelwerte der Gruppe A und B. Daher ist die Steigung der Mittlwertsunterschied zwischen der Gruppe A und der Gruppe B.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\nAbbildung 35.2— Graphische Darstellung der Interpretation von einem kategoriellen \\(x\\) mit 2 Leveln.\n\n\n\n\n35.3.3 Kategorielles \\(x\\) mit &gt;2 Leveln\nNachdem wir das Problem der Interpretation von einem kategoriellen \\(x\\) mit zwei Leveln verstanden haben, werden wir uns jetzt den Fall für ein kategoriellen \\(x\\) mit drei oder mehr Leveln anschauen. Wir nutzen hier nur drei Level, da es für vier oder fünf Level gleich abläuft. Du kannst das Datenbeispiel gerne auch noch um eine vierte Gruppe erweitern und schauen was sich da ändert.\nUnser Datensatz besteht aus drei Gruppen A, B und C mit den Mittelwerten von 10, 15 und 3. Hierbei ist wichtig, dass wir in der Gruppe C nur einen Mittelwert von 3 haben. Also wir haben keinen linearen Anstieg über die drei Gruppen.\n\nset.seed(20339537)\ncat_three_tbl &lt;- tibble(A = rnorm(n = 7, mean = 10, sd = 1),\n                        B = rnorm(n = 7, mean = 15, sd = 1),\n                        C = rnorm(n = 7, mean = 3, sd = 1)) %&gt;% \n  gather(key = x, value = y) %&gt;% \n  mutate(x = as_factor(x))\ncat_three_tbl\n\n# A tibble: 21 × 2\n   x         y\n   &lt;fct&gt; &lt;dbl&gt;\n 1 A     10.0 \n 2 A     10.8 \n 3 A     10.7 \n 4 A     11.0 \n 5 A      9.25\n 6 A      8.98\n 7 A      9.71\n 8 B     15.1 \n 9 B     16.0 \n10 B     14.5 \n# ℹ 11 more rows\n\n\nSchauen wir uns einmal die Modellmatrix an. Wir sehen wieder, dass der Intercept über alle Gruppen geschätzt wird und wir Koeffizienten für die Gruppen B und C erhalten. Mit der Modellmatrix wird dann auch das lineare Modell geschätzt.\n\nmodel.matrix(y ~ x, data = cat_three_tbl)\n\n   (Intercept) xB xC\n1            1  0  0\n2            1  0  0\n3            1  0  0\n4            1  0  0\n5            1  0  0\n6            1  0  0\n7            1  0  0\n8            1  1  0\n9            1  1  0\n10           1  1  0\n11           1  1  0\n12           1  1  0\n13           1  1  0\n14           1  1  0\n15           1  0  1\n16           1  0  1\n17           1  0  1\n18           1  0  1\n19           1  0  1\n20           1  0  1\n21           1  0  1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$x\n[1] \"contr.treatment\"\n\n\nWir rechnen wieder das Modell mit der Funktion lm(). Wir nutzen die Funktion model_parameter() aus dem R Paket parameters für die Ausgabe der Koeffizienten zu erhalten.\n\nlm(y ~ x, data = cat_three_tbl) %&gt;% \n  model_parameters() %&gt;% \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter   | Coefficient\n-------------------------\n(Intercept) |       10.07\nxB          |        5.27\nxC          |       -7.41\n\n\nWir sehen wieder, dass der Intercept der Mittelwert der Gruppe A ist. Wir hatten einen Mittelwert von 10 für die Gruppe A eingestellt und wir erhalten diesen Wert wieder. Die anderen Koeffizienten sind die Änderung zum Mittelwert von der Gruppe A. In Tabelle 35.3 ist der Zusammenhang nochmal für alle Level des Faktors \\(x\\) dargestellt. Wir haben für die Gruppe B einen Mittelwert von 15 und für die Gruppe C einen Mittelwert von 3 eingestellt. Wir erhalten diese Mittelwerte wieder, wenn wir die Differenzen zu dem Intercept bilden.\n\n\n\n\nTabelle 35.3— Zusammenhang von den Mittelwerten der Level des Faktores \\(x\\) und deren Differenz zu Level A.\n\nFactor x\nMean of level\nDifference to level A\n\n\n\nA\n10.07\n0.00\n\n\nB\n15.33\n5.27\n\n\nC\n2.66\n-7.41\n\n\n\n\n\n\nWir sehen den Zusammenhang nochmal in der Abbildung 35.3 visualisiert. Wir sehen die Änderung on der Gruppe A zu der Gruppe B sowie die Änderung von der Gruppe A zu der Gruppe C. Die Abbildung ist etwas verwirrend da wir nicht das \\(x\\) um 2 Einheiten erhöhen um auf den Mittelwert von C zu kommen. Wir rechnen sozusagen ausgehend von A die Änderung zu B und C aus. Dabei nehmen wir jedesmal an, dass die Gruppe B und die Gruppe C nur eine Einheit von \\(x\\) von A entfernt ist.\n\n\n\n\nAbbildung 35.3— Graphische Darstellung der Interpretation von einem kategoriellen \\(x\\) mit 3 Leveln."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-confounder",
    "href": "stat-modeling-basic.html#sec-confounder",
    "title": "35  Multiple lineare Regression",
    "section": "\n35.4 Adjustierung für Confounder",
    "text": "35.4 Adjustierung für Confounder\nIm folgenden Abschnitt wollen wir einmal auf Confounder eingehen. Was sind Confounder? Zum einen gibt es kein gutes deutsches Wort für Confounder. Du könntest Confounder in etwa mit Verzerrer oder Störfakor übersetzen. Zum Anderen können Confounder nur zuammen mit anderen Vriabln in einer multiplen Regression auftreten. Das heißt, wir brauchen mindestens zwei Variablen in einer Regression. Ein Confounder verursacht einen Effekt, den wir eigentlich nicht so erwartet hätten. Wir wollen eigentlich einen Effekt schätzen, aber der Effekt ist viel größer oder kleiner, da der Effekt eigentlich von einder anderen Variable verursacht oder aber verdeckt wird. Wir können einen Confounder in beide Richtungen haben. Wichtig ist hierbei, das wir eigentlich nicht an dem Effekt des Confounders interessiert sind. Wir wollen uns zum Beispiel den Effekt einer Düngung auf das Trockengewicht anschauen, aber der Effekt den wir beobachten wird durch die unterschiedlichen Pflanzorte verursacht.\nSchauen wir uns das ganze mal für das Beispiel des Zusammenhangs von dem Flohgewicht mit der Sprungweite an. Dafür benötigen wir den Datensatz flea_dog_cat_length_weight.csv.\n\nmodel_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %&gt;%\n  select(animal, sex, weight, jump_length) %&gt;% \n  mutate(animal = as_factor(animal),\n         sex = as_factor(sex))\n\nIn der Tabelle 35.4 ist der Datensatz model_tbl nochmal dargestellt.\n\n\n\n\nTabelle 35.4— Datensatz für die Confounder Adjustierung. Die Variable jump_length ist das \\(y\\) und die Variable weight das \\(x\\) von Interesse.\n\nanimal\nsex\nweight\njump_length\n\n\n\ncat\nmale\n6.02\n15.79\n\n\ncat\nmale\n5.99\n18.33\n\n\ncat\nmale\n8.05\n17.58\n\n\ncat\nmale\n6.71\n14.09\n\n\ncat\nmale\n6.19\n18.22\n\n\ncat\nmale\n8.18\n13.49\n\n\n…\n…\n…\n…\n\n\nfox\nfemale\n8.04\n27.81\n\n\nfox\nfemale\n9.03\n24.02\n\n\nfox\nfemale\n7.42\n24.53\n\n\nfox\nfemale\n9.26\n24.35\n\n\nfox\nfemale\n8.85\n24.36\n\n\nfox\nfemale\n7.89\n22.13\n\n\n\n\n\n\nWir können uns jetzt drei verschiedene Modelle anschauen.\n\nDas Modell jump_length ~ weight. Wir modellieren die Abhängigkeit von der Sprungweite von dem Gewicht der Flöhe über alle anderen Variablen hinweg.\nDas Modell jump_length ~ weight + animal. Wir modellieren die Abhängigkeit von der Sprungweite von dem Gewicht der Flöhe und berücksichtigen die Tierart des Flohes. Ignorieren aber das Geschlecht des Flohes.\nDas Modell jump_length ~ weight + animal + sex. Wir modellieren die Abhängigkeit von der Sprungweite von dem Gewicht der Flöhe und berücksichtigen alle Variablen, die wir gemessen haben.\n\nHierbei ist wichtig, dass es natürlich auch Confounder geben kann, die wir gar nicht in den Daten erhoben haben. Also, dass sich Gruppen finden lassen, die wir gar nicht als Variable mit in den Daten erfasst haben. Hier könnte eine Hauptkomponentenanalyse helfen.\n\n\n\n\n\n(a) jump_length ~ weight\n\n\n\n\n\n(b) jump_length ~ weight + animal\n\n\n\n\n\n(c) jump_length ~ weight + animal + sex\n\n\n\nAbbildung 35.4— Darstellung des counfounder Effekts anhand des Zusammenhangs der Sprungweite in [cm] und dem Gewicht von Flöhen [mg].\n\n\nIn Abbildung 35.4 (a) sehen wir die blaue Linie als die Ausgabe von dem Modell jump_length ~ weight. Wir sehen, dass mit dem Anstieg des Gewichtes der Flöhe auch die Sprungweite sich erhöht. Wir würden annehmen, dass wir hier einen signifikanten Unterschied vorliegen haben. Schauen wir uns die Koeffizienten des Modells aus dem lm() einmal an.\n\nlm(jump_length ~ weight, data = model_tbl) %&gt;% \n  model_parameters()\n\nParameter   | Coefficient |   SE |        95% CI | t(598) |      p\n------------------------------------------------------------------\n(Intercept) |        9.79 | 0.77 | [8.28, 11.30] |  12.73 | &lt; .001\nweight      |        1.34 | 0.09 | [1.16,  1.53] |  14.16 | &lt; .001\n\n\nWir sehen, dass wir einen Effekt des Gewichts auf die Sprungweite von \\(1.34\\) vorliegen haben. Auch ist der Effekt und damit die Steigung signifikant.\nErweitern wir nun das Modell um die Tierart und erhalten die Abbildung 35.4 (b). Zum einen sehen wir, dass der globale Effekt nicht so ganz stimmen kann. Die Tierarten haben alle einen unterschiedlichen starken Effekt von dem Gewicht auf die Sprungweite. Auch hier fitten wir einmal das lineare Modell und schauen uns die Koeffizienten an.\n\nlm(jump_length ~ weight + animal, data = model_tbl) %&gt;% \n  model_parameters()\n\nParameter    | Coefficient |   SE |       95% CI | t(596) |      p\n------------------------------------------------------------------\n(Intercept)  |        7.82 | 0.62 | [6.60, 9.03] |  12.64 | &lt; .001\nweight       |        1.27 | 0.07 | [1.13, 1.42] |  17.08 | &lt; .001\nanimal [dog] |        2.62 | 0.26 | [2.12, 3.13] |  10.23 | &lt; .001\nanimal [fox] |        4.97 | 0.26 | [4.47, 5.48] |  19.38 | &lt; .001\n\n\nDer Effekt des Gewichtes ist hier in etwa gleich geblieben. Wir sehen aber auch, dass die Sprungweite anscheinend bei Hunden und Füchsen höher ist. Daher haben wir hier einen Effekt on der Tierart. Wir adjustieren für den Confoundereffekt durch die Tierart und erhalten einen besseren Effektschätzer für das Gewicht.\nIn der letzten Abbildung 35.4 (c) wollen wir nochmal das Geschlecht der Flöhe mit in das Modell nehmen. Wir sehen, dass in diesem Fall, das Gewicht gar keinen Einfluss mehr auf die Sprungweite hat. Den Effekt des Gewichtes war nur der Effekt des unterschiedlichen Gewichtes der weiblichen und männlichen Flöhe. Schauen wir auch hier nochmal in des Modell.\n\nlm(jump_length ~ weight + animal + sex, data = model_tbl) %&gt;% \n  model_parameters() %&gt;% \n  mutate(Coefficient = round(Coefficient, 2))\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | t(595) |      p\n-------------------------------------------------------------------\n(Intercept) |       15.42 | 0.59 | [14.27, 16.58] |  26.23 | &lt; .001\nweight      |        0.01 | 0.08 | [-0.16,  0.17] |   0.07 | 0.942 \nanimaldog   |        2.61 | 0.19 | [ 2.23,  2.99] |  13.50 | &lt; .001\nanimalfox   |        5.19 | 0.19 | [ 4.81,  5.57] |  26.75 | &lt; .001\nsexfemale   |        4.89 | 0.23 | [ 4.44,  5.34] |  21.25 | &lt; .001\n\n\nNun können wir sehen, dass von unserem ursprünglichen Effekt von dem Gewicht auf die Sprungweite nichts mehr übrigbleibt. Wir haben gar keinen Effekt von dem Gewicht auf die Sprungweite vorliegen. Was wir gesehen haben, war der Effekt des Gewichtes der unterschiedlichen Geschlechter. Wenn wir unser Modell für die Confounder animal und sex adjustieren, haben wir einen unverzerrten Schätzer für weight. Wichtig ist nochmal, dafür müssen wir natürlich auch alle variablen erhoben haben. Hätten wir das Geschlecht der Flöhe nicht bestimt, hätten wir hier eventuell einen falschen Schluß getroffen."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-vif",
    "href": "stat-modeling-basic.html#sec-vif",
    "title": "35  Multiple lineare Regression",
    "section": "\n35.5 Variance inflation factor (VIF)",
    "text": "35.5 Variance inflation factor (VIF)\n\n\n\n\n\n\nWir kürzen hier stark ab. Wenn du mehr über Variablen in einem Modell wissen willst, gibt es dann in den Kapitel zur Variablen Selektion mehr Informationen.\nWenn du nur Outcomes und Blöcke vorliegen hast, dann ist das VIF für dich uninteressant.\nWenn wir sehr viele Daten erheben, dann kann es sein, dass die Variablen stark miteinander korrelieren. Wir können aber stark mieinander korrelierte Variablen nicht zusammen in ein Modell nehmen. Die Effekte der beiden Variablen würden sich gegenseitig aufheben. Wir hätten eigentlich zwei getrennt signifikante Variablen. Nehmen wir aber beide Variablen mit ins Modell, sind beide Variablen nicht mehr signifikant.\nUm Variablen zu finden, die sich sehr ähnlich verhalten, können wir den Variance inflation factor (VIF) nutzen. Wir können den VIF jedoch nicht für kategoriale Daten verwenden. Statistisch gesehen würde es keinen Sinn machen. Die Anwendung ist recht einfach. Wir fitten als erstes unser Modell und dann können wir die Funktion vif() aus dem R Paket car verwenden.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")\nmodel &lt;- lm(height ~ semester + age + count_color + count_bears, data = gummi_tbl)\n\nWir sehen im Folgenden das Eregbnis der Funktion vif(). Wenn der berechnete Wert für den VIF größer als 5 ist, dann liegt mit der Variable ein Problem vor. Wir vermuten dann, dass eine andere variable sehr stark mit dieser Variable korreliert. Wir sehen an den Werten keine Aufälligkeiten.\n\nvif(model)\n\n   semester         age count_color count_bears \n   1.050674    1.050264    1.131455    1.216024 \n\n\nWir können uns mit der Funcktion check_model() aus dem R Paket performance auch die Unsicherheit mit angeben lassen. In unserem Beispiel hieft dies gerade nicht sehr viel weiter. Wir bleiben bei den geschätzen Werten und ignorieren das Intervall.\n\n\n\n\nAbbildung 35.5— Graphische Darstellung des VIF mit der Funktion check_model().\n\n\n\nIn Abbildung 35.5 sehen wir nochmal die Visualisierung. Wie immer, manchmal helfen solche Abbildungen, manchmal verwirren die Abbildungen mehr. Wir konzentrieren uns hier auf die Werte des VIF’s und ignorieren die Streuung."
  },
  {
    "objectID": "stat-modeling-basic.html#sec-model-basic-compare",
    "href": "stat-modeling-basic.html#sec-model-basic-compare",
    "title": "35  Multiple lineare Regression",
    "section": "\n35.6 Vergleich von Modellen",
    "text": "35.6 Vergleich von Modellen\n\n\n\n\n\n\nWir kürzen hier stark ab bzw. gehen nicht im Detail auf jedes Gütekriterium ein. Wichtig ist, dass du Modelle statistisch vergleichen kannst. Bedenke immmer, dass die statistische Bewertung nicht immer ausreicht! Auch was die Biologie über das Modell sagt ist wichtig.\nIm Folgenden wollen wir einmal verschiedene Modelle miteinander Vergleichen und uns statistisch wiedergeben lassen, was das beste Modell ist. Und hier holen wir auch einmal kurz Luft, denn wir entschieden nur was das statistisch beste Modell ist. Es kann sein, dass ein Modell biologisch mehr Sinn macht und nicht auf Platz 1 der statistischen Maßzahlen steht. Das ist vollkommen in Ordnung. Du musst abweägen, was für sich das beste Modell ist. Im Zweifel komme ruhig nochmal in meine statistische Beratung oder schreibe mir eine Mail.\nWir bauchen uns jetzt fünf Modelle von fit_1 bis fit_5. Jedes dieser Modelle hat andere \\(x\\) aber häufig das gleiche Outcome y. In dem Beispiel am Ende des Kapitels nutzen wir auch verschiedene \\(y\\) dafür aber dann gleiche \\(x\\) in dem Modell. Im Weiteren sortieren wir die Modelle von einfach nach komplex. Ich versuche immmer das einfachste Modell fit_1 zu nennen bzw. eher die niedrige Nummer zu geben. Im Idealfall benennst du die Modellobjekte nach den Modellen, die in en Objekten gespeichert sind. Oft sind die Modelle aber sehr groß und die Objekte der Fits haben dann sehr lange Namen.\n\nfit_1 &lt;- lm(jump_length ~ animal, data = model_tbl)\nfit_2 &lt;- lm(jump_length ~ animal + sex, data = model_tbl)\nfit_3 &lt;- lm(jump_length ~ animal + sex + weight, data = model_tbl)\nfit_4 &lt;- lm(jump_length ~ animal + sex + sex:weight, data = model_tbl)\nfit_5 &lt;- lm(log(jump_length) ~ animal + sex, data = model_tbl)\n\nDu kannst auch das \\(R^2\\) bzw. das \\(R^2_{adj}\\) für die Modellauswahl nehmen. Das \\(AIC\\) ist neuer und auch für komplexere Modelle geeignet.\nAls Ergänzung zum Bestimmtheitsmaß \\(R^2\\) wollen wir uns noch dasAkaike information criterion (\\(AIC\\)) anschauen. Es gilt hierbei, je kleiner das \\(AIC\\) ist, desto besser ist das \\(AIC\\). Wir wollen also Modelle haben, die ein kleines \\(AIC\\) haben. Wir gehen jetzt nicht auf die Berechnung der \\(AIC\\)’s für jedes Modell ein. Wir erhalten nur ein \\(AIC\\) für jedes Modell. Die einzelnen Werte des \\(AIC\\)’s sagen nichts aus. Ein \\(AIC\\) ist ein mathematisches Konstrukt. Wir können aber verwandete Modelle mit dem \\(AIC\\) untereinander vergleichen. Daher berechnen wir ein \\(\\Delta\\) über die \\(AIC\\). Dafür nehmen wir das Modell mit dem niedrigsten \\(AIC\\) und berechnen die jeweiligen Differenzen zu den anderen \\(i\\) Modellen. In unserem Beispiel ist \\(i\\) dann gleich fünf, da wir fünf Modelle haben.\n\\[\n\\Delta_i = AIC_i - AIC_{min}\n\\]\n\nwenn \\(\\Delta_i &lt; 2\\), gibt es keinen Unterschied zwischen den Modellen. Das \\(i\\)-te Modell ist genauso gut wie das Modell mit dem \\(AIC_{min}\\).\nwenn \\(2 &lt; \\Delta_i &lt; 4\\), dann gibt es eine starke Unterstützung für das \\(i\\)-te Modell. Das \\(i\\)-te Modell ist immer noch ähnlich gut wie das \\(AIC_{min}\\).\nwenn \\(4 &lt; \\Delta_i &lt; 7\\), dann gibt es deutlich weniger Unterstützung für das \\(i\\)-te Modell;\nModelle mit \\(\\Delta_i &gt; 10\\) sind im Vergleich zu dem besten \\(AIC\\) Modell nicht zu verwenden.\n\nNehmen wir ein \\(AIC_1 = AIC_{min} = 100\\) und \\(AIC_2\\) ist \\(100,7\\) an. Dann ist \\(\\Delta_2=0,7&lt;2\\), so dass es keinen wesentlichen Unterschied zwischen den Modellen gibt. Wir können uns entscheiden, welches der beiden Modelle wir nehmen. Hier ist dann wichtig, was auch die Biologie sagt oder eben andere Kritieren, wie Kosten und Nutzen. Wenn wir ein \\(AIC_1 = AIC_{min} = 100000\\) und \\(AIC_2\\) ist \\(100700\\) vorliegen haben, dann ist \\(\\Delta_2 = 700 \\gg 10\\), also gibt es keinen Grund für das \\(2\\)-te Modell. Das \\(2\\)-te Modell ist substantiell schlechter als das erste Modell.\nWir können das \\(\\Delta_i\\) auch in eine Wahrscheinlichkeit umrechnen. Wir können \\(p_i\\) berechnen und damit die relative (im Vergleich zu \\(AIC_{min}\\)) Wahrscheinlichkeit, dass das \\(i\\)-te Modell den AIC minimiert.\n\\[\np_i = \\exp\\left(\\cfrac{-\\Delta_i}{2}\\right)\n\\]\nZum Beispiel entspricht \\(\\Delta_i = 1.5\\) einem \\(p_i\\) von \\(0.47\\) (ziemlich hoch) und ein \\(\\Delta_i = 15\\) entspricht einem \\(p_i =0.0005\\) (ziemlich niedrig). Im ersten Fall besteht eine Wahrscheinlichkeit von 47%, dass das \\(i\\)-te Modell tatsächlich eine bessere Beschreibung ist als das Modell, das \\(AIC_{min}\\) ergibt, und im zweiten Fall beträgt diese Wahrscheinlichkeit nur 0,05%. Wir können so einmal nachrechnen, ob sich eine Entscheidung für ein anderes Modell lohnen würde.\nNeben dem \\(AIC\\) gibt es auch das Bayesian information criterion (\\(BIC\\)). Auch beim \\(BIC\\) gilt, je kleiner das BIC ist, desto besser ist das BIC.\nDu siehst schon, es gibt eine Reihe von Möglichkeiten sich mit der Güte oder Qualität eines Modells zu beschäftigen. Wir nutzen die Funktion model_performance() um uns die Informationen über die Güte eines Modells wiedergeben zu lassen. Im folgenden Codeblock habe ich mich nur auf das \\(AIC\\) und das \\(BIC\\) konzentriert.\n\nmodel_performance(fit_1) %&gt;% \n  as_tibble() %&gt;% \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1 3076. 3093.\n\n\nGut soweit. Du kannst jetzt für jedes der Modelle das \\(AIC\\) berechnen und dann dir die Modelle entsprechend ordnen. Wir müssen das aber nicht tun. Wir können uns auch die Funktion compare_performance() zu nutze machen. Die Funktion gibt uns die \\(R^2\\)-Werte wieder wie auch die \\(AIC\\) sowie die \\(s^2_{\\epsilon}\\) als sigma wieder. Wir haben also alles zusammen was wir brauchen. Darüber hinaus kann die Funktion auch die Modelle rangieren. Das nutzen wir natürlich gerne.\n\ncomp_res &lt;- compare_performance(fit_1, fit_2, fit_3, fit_4, fit_5, rank = TRUE)\n\ncomp_res\n\n# Comparison of Model Performance Indices\n\nName  | Model |    R2 | R2 (adj.) |  RMSE | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n----------------------------------------------------------------------------------------------------------------\nfit_2 |    lm | 0.739 |     0.738 | 1.926 | 1.933 |       0.644 |        0.650 |       0.959 |            82.69%\nfit_5 |    lm | 0.731 |     0.729 | 0.099 | 0.099 |    2.58e-09 |     2.61e-09 |    3.84e-09 |            56.58%\nfit_3 |    lm | 0.739 |     0.737 | 1.926 | 1.935 |       0.237 |        0.235 |       0.039 |            50.83%\nfit_4 |    lm | 0.739 |     0.737 | 1.925 | 1.935 |       0.119 |        0.115 |       0.002 |            45.01%\nfit_1 |    lm | 0.316 |     0.314 | 3.118 | 3.126 |   5.42e-126 |    5.57e-126 |   7.27e-125 |             0.00%\n\n\nAnhand der Ausgabe der Funktion compare_performance() sehen wir, dass unser Modell fit_2 das beste Modell ist. Zwar ist die Streuung der Residuen nicht die Kleinste aller Modelle (Sigma = 1.933) aber wir haben ein hohes \\(R^2_{adj}\\) und auch ein kleines \\(AIC\\). Wir würden damit sagen, dass das Modell fit_2 mit den Variablen animal und sex für \\(x\\) das Outcome jump_length am besten statistisch beschreibt.\nIn Abbildung 35.6 sehen wir die Ausgabe der Funktion compare_performance() nochmal visualisiert. Wir können dann die einzelnen Modelle nochmal besser vergleichen. Auch siehst du hier, ob ein Modell in einem Bereich sehr schlecht ist oder aber nur in einem Bereich sehr gut.\n\n\n\n\nAbbildung 35.6— Graphische Darstellung der Funktion compare_performance() Wir sehen hier die einzelnen Gütekriterien in einer Übersicht dargestellt.\n\n\n\nZum Ende stellt sich die Frage nach dem statistischen Test. Können wir auch statistisch Testen, ob das Modell fit_1 signifikant unterschiedlich ist? Ja wir können die Funktion test_vuong() nutzen um ein Model zu den anderen Modellen zu vergleichen. Wenn du mehrere Modell miteinander vergleichen möchtest, dann muss du die Funktion mehrfach ausführen.\n\ntest_vuong(fit_1, fit_2, fit_3, fit_4, fit_5)\n\nName  | Model | Omega2 | p (Omega2) |      LR | p (LR)\n------------------------------------------------------\nfit_1 |    lm |        |            |         |       \nfit_2 |    lm |   0.41 |     &lt; .001 |  -18.46 | &lt; .001\nfit_3 |    lm |   0.41 |     &lt; .001 |  -18.45 | &lt; .001\nfit_4 |    lm |   0.41 |     &lt; .001 |  -18.48 | &lt; .001\nfit_5 |    lm |   0.47 |     &lt; .001 | -123.94 | &lt; .001\nEach model is compared to fit_1.\n\n\nNutzen wir die Modellselektion auch einmal an einem konkreten Beispiel. War die Transformation sinnvoll? Wir haben also ein Outcome \\(y\\) vorliegen und wollen wissen, ob wir nicht lieber das Modell mit einem \\(y\\) mit einer \\(log\\)-Transformation rechnen sollten. Wir nutzen dazu den Datensatz mit den Hunde- und Katzenflöhen und die Schlüpfdauer als Outcome. Also wie lange brauchen die Flöhe bis die Flöhe aus den Eiern geschlüpft sind.\n\nmodel_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %&gt;%\n  mutate(log_hatch_time = round(log(hatch_time), 2))\n\nWir bauen uns jetzt zwei Modelle. Zum einen das Modell fit_raw mit der hatch_time und den Variablen ainmal und sex. Das zweite Modell enthält die log(hatch_time) also das Outcome \\(y\\) als \\(log\\)-Transformation. Wiederum sind die \\(x\\) variablen die gleichen wie im untransformierten Modell. Wir fitten beide Modelle und speichern die Objekte entsprechend ab.\n\nfit_raw &lt;- lm(hatch_time ~ animal + sex, data = model_tbl)\nfit_log &lt;- lm(log_hatch_time ~ animal + sex, data = model_tbl)\n\nDie Funktion compare_performance() erlaubt uns wieder die beiden Modelle fit_raw und fit_log miteinander zu vergleichen. Wir erhalten die folgende Ausgabe der Funktion.\n\ncomp_res &lt;- compare_performance(fit_raw, fit_log, rank = TRUE)\n\ncomp_res\n\n# Comparison of Model Performance Indices\n\nName    | Model |    R2 | R2 (adj.) |    RMSE |   Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n----------------------------------------------------------------------------------------------------------------------\nfit_log |    lm | 0.006 |     0.001 |   0.992 |   0.995 |        1.00 |         1.00 |        1.00 |            71.43%\nfit_raw |    lm | 0.008 |     0.003 | 789.441 | 792.086 |    0.00e+00 |     0.00e+00 |    0.00e+00 |            28.57%\n\n\nIn diesem Beispiel wäre das \\(log\\)-transformierte Modell das bessere statistische Modell. Wir müssen jetzt überlegen, ob wir den Preis dafür bezahlen wollen. Wenn wir nämlich alles mit der \\(log\\)-Transformation rechnen, dann erhalten wir auch alle Koeffizienten auf der \\(log\\)-Skala (eng. log scale). Hier kannst du nur von Fall zu Fall entscheiden."
  },
  {
    "objectID": "stat-modeling-basic.html#referenzen",
    "href": "stat-modeling-basic.html#referenzen",
    "title": "35  Multiple lineare Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nKéry, Marc. 2010. Introduction to WinBUGS for ecologists: Bayesian approach to regression, ANOVA, mixed models and related analyses. Academic Press."
  },
  {
    "objectID": "stat-modeling-variable-selection.html#theoretischer-hintergrund",
    "href": "stat-modeling-variable-selection.html#theoretischer-hintergrund",
    "title": "36  Variablenselektion",
    "section": "\n36.1 Theoretischer Hintergrund",
    "text": "36.1 Theoretischer Hintergrund\nDie Selektion von Variablen in einem Modell. Ein schwieriges Thema. Entweder kenne ich mein Experiment und habe das Experiment so geplant, dass nur die bedeutenden Variablen mit in dem Experiment sind oder ich habe keine Ahnung. Gut, dass ist überspitzt und gemein formuliert. Wir wollen uns in diesem Kapitel den Fall anschauen, dass du sehr viele Variablen \\(x\\) erhoben hast und nun statistisch bestimmen willst, welche Variablen nun mit in das finale Modell sollen. Achtung, ich spreche hier nicht von einem Blockdesign oder aber einem Feldexperiment. Da hat die Variablenselektion nichts zu suchen. Daher tritt der Fall der Variablenselektion eher in dem Feld Verhaltensbiologie oder aber auch Ökologie auf. Ebenfalls kann die Anwendung in automatisch erfassten Daten einen Sinn machen. Wir nutzen dann die Variablenselektion (eng. feature selection) zu Dimensionsreduktion des Datensatzes. Der Datensatz soll damit einfacher sein… ob der Datensatz das damit auch wird, ist wieder eine andere Frage.\n\n\n\n\n\n\nIn diesem Kapitel prügeln wir aber einen statistischen Engel. Wir werden hier mal schauen müssen, was alles geht und was nicht. Variablen Selektion ist faktisch nicht ein Kapitel sondern ein Regal(kilo)meter voll mit Büchern.\nZu der Frage welches Verfahren denn nun das richtige Verfahren zur Selektion von Variablen ist, gibt es die Standardantwort in der Statistik: Es kommt auf die Fragestellung an…. Oder aber was ist wie gut implementiert, dass wir das Verfahren einigermaßen gut nutzen können. Wir gehen daher von einfach zu kompliziert und du musst dann schauen, was du nutzen willst und kannst. Wir müssen zum Beispiel unterscheiden, welcher Verteilung das Outcome \\(y\\) folgt. Wenn wir ein normalverteiltes \\(y\\) haben, dann haben wir andere Möglichkeiten, als wenn wir uns ein poissonverteiltes oder binominalverteiltes \\(y\\) anschauen.\n\n\nDas R Paket olsrr erlaubt eine weitreichende Variablen Selektion, wenn ein normalverteiltes Outcome \\(y\\) vorliegt.\nIm Folgenden will ich kurz die fünf Mythen der Variablenselektion von Heinze und Dunkler (2017) zusammenfassen. Wie immer ersetzt meine deutsche Zusammenfassung und Auswahl nicht das eigenständige Lesen der englischen Orgnialquelle, wenn du die Informationen in deiner Abschlussarbeit zitieren willst.\n\n\nDie Anzahl der Variablen in einem Modell sollte reduziert werden, bis es 10 Ereignisse pro Variable gibt. Simulationsstudien haben gezeigt, dass multivariable Modelle bei zu niedrigen Verhältnissen von Ereignissen bzw. Beobachtungen pro Variable (eng events per variable, abk. EPV) sehr instabil werden. Aktuelle Empfehlungen besagen, dass je nach Kontext mindestens 5-15 EPV verfügbar sein sollten. Wahrscheinlich sind sogar viel höhere Werte wie 50 EPV erforderlich, um annähernd stabile Ergebnisse zu erzielen.\n\nNur Variablen mit nachgewiesener Signifikanz des univariaten Modells sollten in ein Modell aufgenommen werden. Die univariable Vorfilterung trägt nicht zur Stabilität des Auswahlprozesses bei, da sie auf stochastischen Quanten beruht und dazu führen kann, dass wichtige Anpassungsvariablen übersehen werden.\n\nNicht signifikante Effekte sollten aus einem Modell entfernt werden. Regressionskoeffizienten hängen im Allgemeinen davon ab, welche anderen Variablen in einem Modell enthalten sind, und ändern folglich ihren Wert, wenn eine der anderen Variablen in einem Modell weggelassen wird.\n\nDer berichtete P-Wert quantifiziert den Typ-I-Fehler einer fälschlich ausgewählten Variablen. Ein P-Wert ist ein Ergebnis der Datenerhebung und -analyse und quantifiziert die Plausibilität der beobachteten Daten unter der Nullhypothese. Daher quantifiziert der P-Wert nicht den Fehler vom Typ I. Es besteht auch die Gefahr einer falschen Eliminierung von Variablen, deren Auswirkungen durch die bloße Angabe des endgültigen Modells eines Variablenauswahlverfahrens überhaupt nicht quantifiziert werden können.\n\nVariablenauswahl vereinfacht die Analyse. Für das jeweilige Problem muss eine geeignete Variablenauswahlmethode gewählt werden. Statistiker haben die Rückwärtselimination als die zuverlässigste unter den Methoden empfohlen, die sich mit Standardsoftware leicht durchführen lassen. Eine Auswahl ist eine “harte Entscheidung”, die aber oft auf vagen Größen beruht. Untersuchungen zur Modellstabilität sollten jede angewandte Variablenauswahl begleiten, um die Entscheidung für das schließlich berichtete Modell zu rechtfertigen oder zumindest die mit der Auswahl der Variablen verbundene Unsicherheit zu quantifizieren.\n\nIm Weiteren sei auch noch auf Heinze, Wallisch, und Dunkler (2018) und Talbot und Massamba (2019) verwiesen. Beide Veröffentlichungen liefern nochmal einen fundierten Block auf die Variablenselektion. Wiederum ist das natürlich nur ein winziger Ausschnitt aus der Literatur zur Variablenselektion. Im Zweifel einfach einmal bei Google Scholar nach Variablenselektion suchen und schauen was so in dem Feld der eigenen Forschung alles gemacht wird.\n\n\n\n\n\n\nSensitivitätsanalysen nach der Variablenselektion\n\n\n\n“Variable selection should always be accompanied by sensitivity analyses to avoid wrong conclusions.” (Heinze und Dunkler 2017, p. 9)\nNachdem wir Variablen aus unseren Daten entfernt haben, ist es üblich noch eine Sensitivitätsanalysen durchzuführen. Wir Vergleich dann das selektierte Modell mit anderen Modellen. Oder wir wollen die Frage beantworten, was hat eigentlich meine Variablenselektion am Ergebnis geändert? Habe ich eine wichtige Variable rausgeschmissen? Das machen wir dann gesammelt in dem Kapitel 39 zu den Sensitivitätsanalysen."
  },
  {
    "objectID": "stat-modeling-variable-selection.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-variable-selection.html#genutzte-r-pakete-für-das-kapitel",
    "title": "36  Variablenselektion",
    "section": "\n36.2 Genutzte R Pakete für das Kapitel",
    "text": "36.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, dlookr, \n               MASS, ranger, Boruta, broom, \n               scales, olsrr, gtsummary, parameters,\n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-variable-selection.html#daten",
    "href": "stat-modeling-variable-selection.html#daten",
    "title": "36  Variablenselektion",
    "section": "\n36.3 Daten",
    "text": "36.3 Daten\nUm die Variablenselektion einmal durchzuführen nurtzen wir zwei Datensätze. Zum einen den Datensatz zu den Kichererbsen in Brandenburg mit einem normalverteilten Outcome \\(y\\) mit dryweight. Wir laden wieder den Datensatz in R und schauen uns einmal die Daten in Tabelle 36.1 als Auszug aus dem Tabellenblatt an.\nWir du schon siehst, wir brauchen Fallzahl um hier überhaupt was zu machen. Bitte keine Variablenselektion im niedrigen zweistelligen Bereich an Beobachtungen.\n\nchickpea_tbl &lt;- read_excel(\"data/chickpeas.xlsx\") \n\nWir sehen, dass wir sehr viele Variablen vorleigen haben. Sind denn jetzt alle Variablen notwendig? Oder können auch ein paar Variablen raus aus dem Modell. So viele Beobachtungen haben wir mit \\(n = 95\\) ja nicht vorliegen. Daher wollen wir an diesem Datensatz die Variablenselektion unter der Annahme eines normalverteilten \\(y\\) durchgehen.\n\n\n\n\nTabelle 36.1— Auszug aus dem Daten zu den Kichererbsen in Brandenburg.\n\n\n\n\n\n\n\n\n\n\n\ntemp\nrained\nlocation\nno3\nfe\nsand\nforest\ndryweight\n\n\n\n25.26\nhigh\nnorth\n5.56\n4.43\n63\n&gt;1000m\n253.42\n\n\n21.4\nhigh\nnortheast\n9.15\n2.58\n51.17\n&lt;1000m\n213.88\n\n\n27.84\nhigh\nnortheast\n5.57\n2.19\n55.57\n&gt;1000m\n230.71\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n29.04\nlow\nnorth\n5.64\n2.87\n53.27\n&gt;1000m\n236.07\n\n\n24.11\nhigh\nnortheast\n4.31\n3.66\n63\n&lt;1000m\n259.82\n\n\n28.88\nlow\nnortheast\n7.92\n2\n65.75\n&gt;1000m\n274.75\n\n\n\n\n\n\nWas wir auch noch wissen, ist wie die Effekte in den Daten wirklich sind. Die Daten wurden ja künstlich erstellt, deshalb hier die Ordnung der Effektstärke für jede Variable. Im Prinzip müsste diese Reihenfolge auch bei der Variablenselektion rauskommen. Schauen wir mal, was wir erhalten.\n\\[\ny \\sim \\beta_0 + 3 * sand + 2 * temp + 1.5 * rained - 1.2 * forest + 1.1 * no3  \n\\]\nViele Beispiele laufen immer unter der Annahme der Normalverteilung. Deshalb als zweites Beispiel nochmal die Daten von den infizierten Ferkeln mit einem binomialverteilten Outcome \\(y\\) mit infected. Auch hier können wir uns den Auszug der Daten in Tabelle 36.2 anschauen.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") \n\nDas schöne an diesem Datensatz ist jetzt, dass wir mit \\(n = 412\\) Beobachtungen sehr viele Daten vorliegen haben. Daher können wir auch alle Methoden gut verwenden und haben nicht das Problem einer zu geringen Fallzahl.\n\n\n\n\nTabelle 36.2— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\ninfected\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\n1\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\n1\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\n1\n\n\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\n0\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\n1\n\n\n\n\n\n\nAuch in diesem Beispiel wurden die Daten von mir mit folgenden Effekten generiert. Schauen wir mal, was die Variablenselektion bei der hohen Fallzahl mit den Variablen macht bzw. welche Sortierung am Ende rauskommt.\n\\[\ny \\sim \\beta_0 + 2 * crp + 0.5 * sex + 0.5 * frailty + 0.2 * bloodpressure + 0.05 * creatinin +  0.01 * weight\n\\]\nDamit haben wir unsere beiden Beispiel und jetzt gehen wir mal eine Auswahl an Methoden zur Variablenselektion durch. Besonders hier, haltet den statistsichen Engel nah bei euch. Es wird leider etwas ruppig für den statistischen Engel."
  },
  {
    "objectID": "stat-modeling-variable-selection.html#methoden-der-variablenselektion",
    "href": "stat-modeling-variable-selection.html#methoden-der-variablenselektion",
    "title": "36  Variablenselektion",
    "section": "\n36.4 Methoden der Variablenselektion",
    "text": "36.4 Methoden der Variablenselektion\nIn den folgenden Abschnitten wollen wir uns eine Reihe an Methoden anschauen um eine Variablenselektion durchzuführen. Dabei gehen wir von einfach nach komplex. Wobei das komplex eher die Methode und nicht die Anwendung meint. Wir nutzen R Pakete und gehen nicht sehr ins Detail wie der Algorithmus jetzt die Auswahl trifft. Für den Hintergrund sind dann die Verweise auf die anderen Kapitel.\n\n36.4.1 Per Hand\nManchmal ist der Anfang auch das Ende. Wir müssen ja gar keinen Algorithmus auf unsere Daten loslassen um eine Variablenselektion durchzuführen. Deshalb möchte ich gleich den ersten Abschnitt mit einem Zitat von Heinze und Dunkler (2017) beginnen.\n“Oft gibt es keinen wissenschaftlichen Grund, eine (algorithmische) Variablenauswahl durchzuführen. Insbesondere erfordern Methoden der (algorithmische) Variablenselektion einen viel größeren Stichprobenumfang als die Schätzung eines multiplen Modells mit einem festen Satz von Prädiktoren auf der Grundlage (klinischer) Erfahrung.” (Übersetzt und ergänzt nach Heinze und Dunkler 2017, p. 9)\nFazit dieses kurzen Abschnitts. Wir können auf alles Folgende einfach verzichten und uns überlegen welche Variablen sinnvollerweise mit ins Modell sollen und das mit unserem Expertenwissen begründen. Gut, und was ist, wenn ich kein Experte bin? Oder wir aber wirklich Neuland betreten? Dann können wir eine Reihe anderer Verfahren nutzen um uns algortimisch einer Wahrheit anzunähern.\n\n36.4.2 Univariate Vorselektion\nUnd weiter geht es mit Zitaten aus Heinze und Dunkler (2017) zu der Variablenselektion. Dazu musst du wissen, dass die univariate Vorselektion sehr beliebt war und auch noch ist. Denn die univariate Vorselektion ist einfach durchzuführen und eben auch gut darzustellen.\n“Obwohl die univariable Vorfilterung nachvollziehbar und mit Standardsoftware leicht durchführbar ist, sollte man sie besser ganz vergessen, da sie für die Erstellung multivariabler Modelle weder Voraussetzung noch von Nutzen ist.” (Übersetzt nach Heinze und Dunkler 2017, p. 8)\nIch sage immer, auch mit einem Hammer kann man Scheiben putzen. Halt nur einmal… Deshalb auch hier die univariate Variante der Vorselektion.\nWir sehen also, eigentlich ist die univariate Variablensleketion nicht so das gelbe vom Ei, aber vielleicht musst die Variablenselektion durchführen, so dass her die Lösung in R einmal dargestellt ist. Wir nutzen einmal die gaussian lineare Regression für den Kichererbsendatensatz. Es ist eine ganze Reihe an Code, das hat aber eher damit zu tun, dass wir die Modellausgabe noch filtern und anpassen wollen. Die eigentliche Idee ist simple. Wir nehmen unseren Datensatz und pipen den Datensatz in select und entfernen unser Outcome drymatter. Nun iterieren wir für jede Variable .x im Datensatz mit der Funktion map() und rechnen in jeder Iteration eine gaussian lineare Regression. Dann entferne wir noch den Intercept und sortieren nach den \\(p\\)-Werten.\n\nchickpea_tbl %&gt;%\n  select(-dryweight) %&gt;%                   \n  map(~glm(dryweight ~ .x, data = chickpea_tbl, family = gaussian)) %&gt;%    \n  map(tidy) %&gt;%                          \n  map(filter, term != \"(Intercept)\") %&gt;%       \n  map(select, -term, -std.error, -statistic) %&gt;%                        \n  bind_rows(.id=\"term\") %&gt;% \n  arrange(p.value) %&gt;% \n  mutate(p.value = pvalue(p.value),\n         estimate = round(estimate, 2))\n\n# A tibble: 8 × 3\n  term     estimate p.value\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  \n1 sand         2.9  &lt;0.001 \n2 temp         1.65 0.011  \n3 no3          2.51 0.037  \n4 fe           2.61 0.091  \n5 location    -7.27 0.130  \n6 rained       4.95 0.165  \n7 forest      -4.39 0.197  \n8 location    -2.87 0.434  \n\n\nWürden wir nur nach dem Signifikanzniveau von 5% gehen, dann hätten wir die Variablen sand und location selektiert. Bei der selektion mit dem \\(p\\)-Wert wird aber eher eine Schwelle von 15.7% vorgeschlagen (Heinze und Dunkler 2017, p. 9). Daher würden wir auch noch no3 und temp mit Selektieren und in unser Modell nehmen.\nEs gibt ja immer zwei Wege nach Rom. Deshalb hier auch nochmal die Funktion tbl_uvregression() aus dem R Paket gtsummary, die es erlaubt die univariaten Regressionen über alle Variablen laufen zu lassen. Wir kriegen dann auch eine schöne Tabelle 36.3 wieder.\n\n\nDas R Paket gtsummary erlaubt es Ergebnisse der Regression in dem Tutorial: tbl_regression gut darzustellen.\n\nchickpea_tbl %&gt;%\n  tbl_uvregression(\n    method = glm,\n    y = dryweight,\n    method.args = list(family = gaussian),\n    pvalue_fun = ~style_pvalue(.x, digits = 2)\n  ) %&gt;%\n  add_global_p() %&gt;%  # add global p-value \n  add_q() %&gt;%         # adjusts global p-values for multiple testing\n  bold_p() %&gt;%        # bold p-values under a given threshold (default 0.05)\n  bold_p(t = 0.10, q = TRUE) %&gt;% # now bold q-values under the threshold of 0.10\n  bold_labels()\n\n\n\n\n\n\nTabelle 36.3— Univariate Regression mit der Funktion tbl_uvregression().\n\nCharacteristic\nN\nBeta\n\n95% CI1\n\np-value\n\nq-value2\n\n\n\n\ntemp\n95\n1.6\n0.41, 2.9\n0.009\n0.032\n\n\nrained\n95\n\n\n0.16\n0.23\n\n\n    high\n\n—\n—\n\n\n\n\n    low\n\n4.9\n-2.0, 12\n\n\n\n\nlocation\n95\n\n\n0.31\n0.31\n\n\n    north\n\n—\n—\n\n\n\n\n    northeast\n\n-2.9\n-10, 4.3\n\n\n\n\n    west\n\n-7.3\n-17, 2.1\n\n\n\n\nno3\n95\n2.5\n0.18, 4.8\n0.035\n0.081\n\n\nfe\n95\n2.6\n-0.38, 5.6\n0.087\n0.15\n\n\nsand\n95\n2.9\n2.5, 3.3\n&lt;0.001\n&lt;0.001\n\n\nforest\n95\n\n\n0.19\n0.23\n\n\n    &lt;1000m\n\n—\n—\n\n\n\n\n    &gt;1000m\n\n-4.4\n-11, 2.2\n\n\n\n\n\n\n\n1 CI = Confidence Interval\n\n\n\n2 False discovery rate correction for multiple testing\n\n\n\n\n\n\n\n\nNun führen wir die univariate Regression erneut auf den Ferkeldaten aus. Hier ändern wir nur die family = binomial, da wir hier jetzt eine logistische lineare Regression rechnen müssen. Unser Outcome infected ist ja \\(0/1\\) codiert. Sonst ändert sich der Code nicht.\n\npig_tbl %&gt;%\n  select(-infected) %&gt;%                   \n  map(~glm(infected ~ .x, data = pig_tbl, family = binomial)) %&gt;%    \n  map(tidy) %&gt;%                          \n  map(filter, term != \"(Intercept)\") %&gt;%       \n  map(select, -term, -std.error, -statistic) %&gt;%                        \n  bind_rows(.id=\"term\") %&gt;% \n  arrange(p.value) %&gt;% \n  mutate(p.value = pvalue(p.value),\n         estimate = round(estimate, 2))\n\n# A tibble: 12 × 3\n   term          estimate p.value\n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  \n 1 crp               0.96 &lt;0.001 \n 2 bloodpressure     0.08 0.013  \n 3 creatinin         0.11 0.133  \n 4 location         -0.4  0.141  \n 5 sex              -0.29 0.188  \n 6 location         -0.24 0.435  \n 7 frailty           0.16 0.645  \n 8 activity          0.02 0.698  \n 9 frailty          -0.12 0.699  \n10 location          0.12 0.712  \n11 weight           -0.02 0.765  \n12 age               0    0.877  \n\n\nIn diesem Fall reicht die Schwelle von 15.7% nur für zwei Variablen (Heinze und Dunkler 2017, p. 9). Wir erhalten die Variablen crp und bloodpressure für das Modell selektiert.\nIn der Tabelle 36.4 sehen wir dann nochmal die Anwendung der Funktion tbl_uvregression() auf den Ferkeldatensatz. Ich musste hier die Option pvalue_fun = ~style_pvalue(.x, digits = 2) entfernen, da sonst die Variable crp keinen \\(p\\)-Wert erhält. Leider sehe ich den \\(p\\)-Wert mit \\(&lt;0.001\\) in meiner Ausgabe in R aber wie du siehst, wird die Tabelle auf der Webseite nicht korrekt angezeigt. Das Problem von automatischen Tabellen. Ein Fluch und Segen zugleich. Du musst immer wieder überprüfen, ob die Optionen dann auch für sich und deine Analyse passen.\n\npig_tbl %&gt;%\n  tbl_uvregression(\n    method = glm,\n    y = infected,\n    method.args = list(family = binomial),\n    exponentiate = TRUE\n  ) %&gt;%\n  add_global_p() %&gt;%  # add global p-value \n  add_nevent() %&gt;%    # add number of events of the outcome\n  add_q() %&gt;%         # adjusts global p-values for multiple testing\n  bold_p() %&gt;%        # bold p-values under a given threshold (default 0.05)\n  bold_p(t = 0.10, q = TRUE) %&gt;% # now bold q-values under the threshold of 0.10\n  bold_labels()\n\n\n\n\n\n\nTabelle 36.4— Univariate Regression mit der Funktion tbl_uvregression().\n\nCharacteristic\nN\nEvent N\n\nOR1\n\n\n95% CI1\n\np-value\n\nq-value2\n\n\n\n\nage\n412\n276\n1.00\n0.95, 1.04\n0.9\n0.9\n\n\nsex\n412\n276\n\n\n0.2\n0.4\n\n\n    female\n\n\n—\n—\n\n\n\n\n    male\n\n\n0.75\n0.49, 1.15\n\n\n\n\nlocation\n412\n276\n\n\n0.3\n0.5\n\n\n    north\n\n\n—\n—\n\n\n\n\n    northeast\n\n\n1.12\n0.61, 2.10\n\n\n\n\n    northwest\n\n\n0.67\n0.39, 1.14\n\n\n\n\n    west\n\n\n0.79\n0.43, 1.44\n\n\n\n\nactivity\n412\n276\n1.02\n0.90, 1.16\n0.7\n0.9\n\n\ncrp\n412\n276\n2.62\n2.14, 3.27\n&lt;0.001\n&lt;0.001\n\n\nfrailty\n412\n276\n\n\n0.5\n0.7\n\n\n    frail\n\n\n—\n—\n\n\n\n\n    pre-frail\n\n\n1.17\n0.59, 2.27\n\n\n\n\n    robust\n\n\n0.88\n0.46, 1.64\n\n\n\n\nbloodpressure\n412\n276\n1.08\n1.02, 1.15\n0.012\n0.053\n\n\nweight\n412\n276\n0.98\n0.86, 1.12\n0.8\n0.9\n\n\ncreatinin\n412\n276\n1.12\n0.97, 1.30\n0.13\n0.4\n\n\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n2 False discovery rate correction for multiple testing\n\n\n\n\n\n\n\n\nNeben der Berechnung von univariaten logistischen Regressionen ist auch die Darstellung der Daten in einer Tabelle 36.5 bei Medizinern sehr beliebt. Deshalb an dieser Stelle auch die Tabelle 1 (eng. table 1) für die Zusammenfasung der Daten getrennt nach dem Infektionsstatus zusammen mit dem \\(p\\)-Wert. Ich nutze hier die Funktion tbl_summary() aus dem R Paket gtsummary.\n\npig_tbl %&gt;% tbl_summary(by = infected) %&gt;% add_p()\n\n\n\n\n\n\nTabelle 36.5— Zusammenfasung der Daten getrennt nach dem Infektionsstatus zusammen mit dem \\(p\\)-Wert.\n\nCharacteristic\n\n0, N = 1361\n\n\n1, N = 2761\n\n\np-value2\n\n\n\n\nage\n59.5 (57.0, 63.0)\n60.0 (57.0, 63.0)\n0.9\n\n\nsex\n\n\n0.2\n\n\n    female\n47 (35%)\n114 (41%)\n\n\n\n    male\n89 (65%)\n162 (59%)\n\n\n\nlocation\n\n\n0.3\n\n\n    north\n36 (26%)\n85 (31%)\n\n\n\n    northeast\n23 (17%)\n61 (22%)\n\n\n\n    northwest\n48 (35%)\n76 (28%)\n\n\n\n    west\n29 (21%)\n54 (20%)\n\n\n\nactivity\n13.39 (12.25, 14.34)\n13.24 (12.28, 14.54)\n0.8\n\n\ncrp\n19.12 (18.12, 19.83)\n20.56 (19.77, 21.46)\n&lt;0.001\n\n\nfrailty\n\n\n0.5\n\n\n    frail\n18 (13%)\n37 (13%)\n\n\n\n    pre-frail\n42 (31%)\n101 (37%)\n\n\n\n    robust\n76 (56%)\n138 (50%)\n\n\n\nbloodpressure\n56.2 (54.3, 58.5)\n57.2 (55.1, 59.6)\n0.021\n\n\nweight\n18.61 (17.34, 19.41)\n18.32 (17.19, 19.60)\n0.8\n\n\ncreatinin\n4.85 (3.67, 5.93)\n4.86 (4.06, 5.84)\n0.3\n\n\n\n\n\n1 Median (IQR); n (%)\n\n\n\n2 Wilcoxon rank sum test; Pearson's Chi-squared test\n\n\n\n\n\n\n\n\nTja, auch hier ist dann die Frage, wie sortiere ich Variablen. Da es sich bei dem table 1-Stil um eine Übersichtstabelle handelt, ist die Tabelle nach den Variablen sortiert. Auch hier finden wir dann die Variablen crp und bloodpressure wieder. Das Problem hierbei ist natürlich, dass sich die \\(p\\)-Werte unterscheiden. Das muss ja auch so sein, denn eine logitische Regression ist nun mal kein Wilcoxon rank sum test oder ein Pearson’s Chi-squared test.\nFassen wir als Fazit dieses Abschnitts zusammen wie unsere Modelle nach der Variablenslektion aussehen würde. In unserem Beispiel für die Kichererbsen im sandigen Brandenburg würden wir dann folgendes Modell nehmen.\n\\[\ny \\sim \\beta_0 + sand + location + no3 + temp\n\\]\nUnsere infizierten Ferkel würden dann folgendes selektiertes Modell erhalten.\n\\[\ny \\sim \\beta_0 + crp + bloodpressure\n\\]\nSchauen wir mal, was die anderen Algorithmen noch so finden.\n\n36.4.3 Sonderfall Gaussian linear Regression\nFür die gaussian lineare Regression gibt es mit dem R Paket oslrr eine große Auswahl an Variable Selection Methods. Einfach mal die Möglichkeiten anschauen, die dort angeboten werden. Wir nutzen jetzt nicht alles was din oslrr möglich ist, sondern nur eien Auswahl. Zuerst müssen wir wieder unser Modell fitten. Wir nehmen alle Variablen mit rein und nutzen die Funktion lm() für ein lineares Modell mit einem normalverteilten Outcome \\(y\\) mit dryweight.\n\nchickenpea_fit &lt;- lm(dryweight ~ temp + rained + location + no3 + fe + sand + forest, \n                     data = chickpea_tbl)\n\nNun gibt es wirklich viele Lösungen in dem R Paket oslrr. Ich möchte einmal die Variante mit ols_step_all_possible präsnetieren. In dem Fall rechnen wir alle Modelle die gehen. Und damit meine ich wirklich alle Modelle. Deshalb filtern wir noch nach dem \\(R^2_{adj}\\) um nicht von dem Angebot erschlagen zu werden. Darüber hinaus möchte ich nur Modelle sehen, die maximal vier Variablen mit in dem Modell haben. Das ist zufällig von mir gewählt… ich will ja ein kleineres Modell haben.\n\nols_step_all_possible(chickenpea_fit) %&gt;%\n  as_tibble %&gt;%\n  arrange(desc(adjr)) %&gt;%\n  filter(n &lt;= 4) %&gt;% \n  select(predictors, adjr, aic) \n\n# A tibble: 98 × 3\n   predictors                 adjr   aic\n   &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt;\n 1 temp no3 sand forest      0.893  587.\n 2 temp rained no3 sand      0.886  592.\n 3 temp no3 fe sand          0.886  593.\n 4 temp no3 sand             0.884  593.\n 5 temp location no3 sand    0.882  596.\n 6 temp rained sand forest   0.881  597.\n 7 temp sand forest          0.877  599.\n 8 temp fe sand forest       0.876  600.\n 9 temp location sand forest 0.875  602.\n10 temp rained fe sand       0.872  603.\n# ℹ 88 more rows\n\n\nEine Alternative ist die Funktion ols_step_backward_aic(), die es erlaubt die Selektion anhand dem \\(AIC\\)-Wert zu machen. Der \\(AIC\\)-Wert beschreibt die Güte eines Modells und je kleiner der \\(AIC\\)-Wert ist, desto besser ist das Modell im Vergleich zu anderen Modellen gleicher Art. Da der \\(AIC\\)-Wert von den Daten abhängt in denen der \\(AIC\\)-Wert geschätzt wurde, können verschiedene \\(AIC\\)-Werte nicht übergreifend vergleichen werden.\n\nchick_step_aic &lt;- ols_step_backward_aic(chickenpea_fit)\n\nIn Abbildung 36.1 sehen wir einmal den Verlauf der \\(AIC\\)-Wert durch die Entfernung der jeweiligen Variable. Wenn du auf die y-Achse schaust, ist der Effekt numerisch nicht sehr groß. Davon darf man sich aber nicht beeindrucken lassen., Wir erhalten ein besseres Modell, wenn wir Variablen entfernen. Darüber hinaus sehen wir auch eine Sättigung.\n\nplot(chick_step_aic) \n\n\n\nAbbildung 36.1— Visualisierung der ols_step_backward_aic mit der Reduktion des AIC-Wertes.\n\n\n\nGut, und wie sieht nun unser finales Modell aus? Dafür müssen wir usn aus dem Objekt chick_step_aic das model raus ziehen. Dafür nutzen wir die Funktion pluck(). Dann noch die Ausgabe in die Funktion model_parameters() gepipt und schon haben wir das finale Modell nach \\(AIC\\)-Werten in einer gaussian linearen Regression.\n\npluck(chick_step_aic, \"model\") %&gt;% \n  model_parameters()\n\nParameter       | Coefficient |   SE |          95% CI | t(89) |      p\n-----------------------------------------------------------------------\n(Intercept)     |       -2.09 | 9.44 | [-20.85, 16.67] | -0.22 | 0.825 \ntemp            |        2.37 | 0.22 | [  1.94,  2.81] | 10.88 | &lt; .001\nrained [low]    |        1.79 | 1.18 | [ -0.56,  4.15] |  1.51 | 0.133 \nno3             |        1.42 | 0.40 | [  0.62,  2.22] |  3.53 | &lt; .001\nsand            |        3.03 | 0.12 | [  2.80,  3.26] | 26.12 | &lt; .001\nforest [&gt;1000m] |       -3.17 | 1.11 | [ -5.38, -0.95] | -2.84 | 0.006 \n\n\nSpannenderweise ist location nicht mehr im finalen Modell plus die Variable location flog auch sehr früh raus. Das passt auch besser zu den Daten. Ich hatte die Daten so gebaut, dass der Ort eigentlich keinen Effekt haben sollte. Wir sehen, dass je nach Verfahren was anderes herauskommt. Aber Achtung, das schrittweise Verfahren ist der Auswahl nach \\(p\\)-Werten auf jeden Fall vorzuziehen!\n\n36.4.4 Schrittweise mit stepAIC\n\nWas das R Paket oslrr für die gaussian linear Regression kann, kann das R Paket MASS mit der Funktion stepAIC für den Rest der möglichen Verteilungen. Da wir mit dem Fekerldatensatz ein binominales Outcome \\(y\\) mit infected vorliegen haben nutzen wir un die Funktion stepAIC(). Wir hätten auch den Kichererbsendatensatz mit der Funktion bearbeiten können, aber im Falle der Normalverteilung stehen uns dann eben noch andere Algorithmen zu Verfügung. Wie immer müssen wir zuerst das volle Modell mit der Funktion glm() fitten. Wir geben noch die Verteilungsfamilie mit family = binomial noch mit an und definieren so eine logistische lineare Regression.\n\nfit &lt;- glm(infected ~ age + sex + location + activity + crp + frailty + bloodpressure + weight + creatinin, \n           data = pig_tbl, family = binomial)\n\nNachdem wir das Modell gefittet haben, können wir das Modell direkt in die Funktion stepAIC stecken. Wir nutzen noch die Option direction = \"backward\" um eine Rückwärtsselektion durchzuführen.\n\nfit_step &lt;- stepAIC(fit, direction = \"backward\")\n\nStart:  AIC=421.22\ninfected ~ age + sex + location + activity + crp + frailty + \n    bloodpressure + weight + creatinin\n\n                Df Deviance    AIC\n- location       3   398.49 418.49\n- frailty        2   396.61 418.61\n- weight         1   395.22 419.22\n- age            1   395.24 419.24\n- activity       1   395.47 419.47\n- sex            1   395.93 419.93\n- creatinin      1   396.74 420.74\n&lt;none&gt;               395.22 421.22\n- bloodpressure  1   399.67 423.67\n- crp            1   504.90 528.90\n\nStep:  AIC=418.49\ninfected ~ age + sex + activity + crp + frailty + bloodpressure + \n    weight + creatinin\n\n                Df Deviance    AIC\n- frailty        2   399.68 415.68\n- weight         1   398.50 416.50\n- age            1   398.50 416.50\n- activity       1   398.83 416.83\n- sex            1   399.01 417.01\n- creatinin      1   400.10 418.10\n&lt;none&gt;               398.49 418.49\n- bloodpressure  1   403.24 421.24\n- crp            1   509.50 527.50\n\nStep:  AIC=415.68\ninfected ~ age + sex + activity + crp + bloodpressure + weight + \n    creatinin\n\n                Df Deviance    AIC\n- weight         1   399.69 413.69\n- age            1   399.73 413.73\n- activity       1   400.06 414.06\n- sex            1   400.30 414.30\n- creatinin      1   401.21 415.21\n&lt;none&gt;               399.68 415.68\n- bloodpressure  1   404.17 418.17\n- crp            1   511.29 525.29\n\nStep:  AIC=413.69\ninfected ~ age + sex + activity + crp + bloodpressure + creatinin\n\n                Df Deviance    AIC\n- age            1   399.74 411.74\n- activity       1   400.09 412.09\n- sex            1   400.40 412.40\n- creatinin      1   401.22 413.22\n&lt;none&gt;               399.69 413.69\n- bloodpressure  1   404.18 416.18\n- crp            1   512.26 524.26\n\nStep:  AIC=411.74\ninfected ~ sex + activity + crp + bloodpressure + creatinin\n\n                Df Deviance    AIC\n- activity       1   400.11 410.11\n- sex            1   400.45 410.45\n- creatinin      1   401.40 411.40\n&lt;none&gt;               399.74 411.74\n- bloodpressure  1   404.20 414.20\n- crp            1   512.28 522.28\n\nStep:  AIC=410.11\ninfected ~ sex + crp + bloodpressure + creatinin\n\n                Df Deviance    AIC\n- sex            1   400.47 408.47\n- creatinin      1   401.86 409.86\n&lt;none&gt;               400.11 410.11\n- bloodpressure  1   404.72 412.72\n- crp            1   513.77 521.77\n\nStep:  AIC=408.47\ninfected ~ crp + bloodpressure + creatinin\n\n                Df Deviance    AIC\n- creatinin      1   402.21 408.21\n&lt;none&gt;               400.47 408.47\n- bloodpressure  1   406.37 412.37\n- crp            1   514.11 520.11\n\nStep:  AIC=408.21\ninfected ~ crp + bloodpressure\n\n                Df Deviance    AIC\n&lt;none&gt;               402.21 408.21\n- bloodpressure  1   408.12 412.12\n- crp            1   516.27 520.27\n\n\nJetzt ist die Selektion durchgelaufen und wir sehen in jeden Schritt welche Variable jeweils entfernt wurde und wie sich dann der \\(AIC\\)-Wert ändert. Wir starten mit einem \\(AIC = 425.65\\) und enden bei einem \\(AIC=415.01\\). Schauen wir uns nochmal das finale Modell an.\n\nfit_step %&gt;% \n  model_parameters()\n\nParameter     | Log-Odds |   SE |           95% CI |     z |      p\n-------------------------------------------------------------------\n(Intercept)   |   -23.54 | 3.14 | [-29.96, -17.61] | -7.49 | &lt; .001\ncrp           |     0.97 | 0.11 | [  0.76,   1.20] |  8.83 | &lt; .001\nbloodpressure |     0.09 | 0.04 | [  0.02,   0.16] |  2.39 | 0.017 \n\n\nHier erscheint jetzt noch die Variable sex mit in der Auswahl. Das hat natürlich auch weitreichende Auswirkungen! Es macht schon einen gewaltigen Unterschied, ob wir annehmen das, dass Geschelcht der Ferkel keinen Einfluss auf die Infektion hat oder eben doch. Wir sehen auch hier, dass wir Aufpassen müssen wenn wir eine Variablenselektion durchführen. Aber Achtung, das schrittweise Verfahren ist der Auswahl nach \\(p\\)-Werten auf jeden Fall vorzuziehen!\n\n36.4.5 Feature Selektion mit ranger\n\nIn diesem Abschnitt wollen wir die Variablenselektion mit einem maschinellen Lernverfahren durchführen. Im Bereich des maschinellen Lernens heist die Variablenselektion dann aber Feature Selektion. Wir versuchen jetzt die Selektion auf den Orginaldaten durchzuführen. Eigentlich wird empfohlen die Daten vorher zu normalisieren und dann mit den maschinellen Lernverfahren zu nutzen.\n\n\n\n\n\n\nStandardisieren oder Normalisieren von Daten\n\n\n\nEine Herausforderung für maschinelle Lernverfahren sind nicht normalisierte Daten. Das heist, dass wir Variablen haben, die kategorial oder kontinuierlich sein können oder aber sehr unterschiedlich von den Einheiten sind. Deshalb wird empfohlen die Daten vorher zu Standardisieren oder zu Normalisieren. In dem Kapitel 17 kannst du mehr über das Transformieren von Daten nachlesen.\n\n\nWir nutzen als erstes einen Random Forest Algorithmus wie er in Kapitel 59 beschrieben ist. Es bietet sich hier die Implementation im R Paket ranger an. Bevor wir aber einen Random Forest auf unsere Daten laufen lassen, Standardisieren wir unsere Daten nochmal. Überall wo wir einen numerischen Wert als Variableneintrag haben rechnen wir eine \\(z\\)-Transformation. Wir erhalten dann die standardisierten Daten zurück.\n\npig_norm_tbl &lt;- pig_tbl %&gt;% \n  mutate(across(where(is.character), as_factor),\n         across(where(is.numeric), dlookr::transform, \"zscore\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), dlookr::transform, \"zscore\")`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\npig_norm_tbl\n\n# A tibble: 412 × 10\n   age        sex    location  activity    crp        frailty   bloodpressure\n   &lt;transfrm&gt; &lt;fct&gt;  &lt;fct&gt;     &lt;transfrm&gt;  &lt;transfrm&gt; &lt;fct&gt;     &lt;transfrm&gt;   \n 1  0.2156014 male   northeast  1.23906607  1.6154076 robust     1.6152124   \n 2 -1.5521155 male   northwest -0.15495130 -0.9942285 robust    -0.7895759   \n 3  1.3204244 female northeast -1.18531196 -0.9104969 robust     0.3274677   \n 4 -0.2263278 female north      0.03899895 -0.4848611 robust    -0.2085934   \n 5  0.6575306 male   northwest  0.87540937  1.0502190 robust    -0.4391895   \n 6 -1.1101862 male   northwest  1.54211333  0.9664873 robust     1.0312351   \n 7 -2.4359739 male   west       0.32386336 -0.7569889 pre-frail -0.6488224   \n 8 -1.5521155 male   northwest  0.13597407 -0.7569889 robust    -0.4122367   \n 9 -0.4472924 female west      -0.87014282  1.2735034 robust     0.6838436   \n10 -0.6682570 male   northwest  0.46326510  0.6245832 robust    -0.3343731   \n# ℹ 402 more rows\n# ℹ 3 more variables: weight &lt;transfrm&gt;, creatinin &lt;transfrm&gt;,\n#   infected &lt;transfrm&gt;\n\n\nDen Random Forest rechnen wir mit der Funktion ranger(). Dafür müssen wir wieder unser vollständiges Modell definieren und können dann die Funktion starten. Damit wir eine Variablenwichtigkeit (eng. variable importance) wiederbekommen, müssen wir noch die Option importance = \"permutation\" verwenden.\n\nfit_raw &lt;- ranger(infected ~ age + sex + location + activity + crp + frailty + bloodpressure + weight + creatinin, \n                  data = pig_tbl, ntree = 1000, importance = \"permutation\")\n\npluck(fit_raw, \"variable.importance\") %&gt;% \n  sort(decreasing = TRUE) %&gt;% \n  round(3)\n\n          crp bloodpressure     creatinin      location           sex \n        0.097         0.006         0.004         0.001         0.000 \n      frailty        weight           age      activity \n       -0.001        -0.002        -0.002        -0.003 \n\n\nWir sehen, dass wir als wichtigste Variable wiederum crp zurückbekommen. Danach wird es schon etwas schwieriger, da die Werte sehr schnell kleiner werden und auch ein Art Plateau bilden. Daher würde man hier nur annehmen, dass crp bedeutend für das Modell ist. Es kann aber auch sein, dass hier eine kontinuierliche Variable sehr vom Algorithmus bevorzugt wurde. Daher schauen wir uns die Sachlage einmal mit den standardisierten Daten an.\n\nfit_norm &lt;- ranger(infected ~ age + sex + location + activity + crp + frailty + bloodpressure + weight + creatinin, \n                  data = pig_norm_tbl, ntree = 1000, importance = \"permutation\")\n\npluck(fit_norm , \"variable.importance\") %&gt;% \n  sort(decreasing = TRUE) %&gt;% \n  round(3)\n\n          crp     creatinin bloodpressure      location       frailty \n        0.420         0.025         0.013         0.001        -0.002 \n       weight           age           sex      activity \n       -0.004        -0.006        -0.007        -0.020 \n\n\nAuch hier erhalten wir ein ähnliches Bild. Audf jeden Fall ist crp bedeutend für den Infektionsstatus. Danach werden die Werte etwas zufällig. Wir können Werte für die variable importance nicht unter Datensätzen vergleichen. Jeder Datensatz hat seine eigene variable importance, die von den Werten in dem Datensatz abhängt.\nWir ziehen als Fazit, dass wir nur crp als bedeutenden Wert für die Klassifikation des Infektionsstatus ansehen würden. Hier stehen wir wirklich etwas wie das Schwein vor dem Uhrwerk, denn was nun richtiger ist, stepAIC oder ranger lässt sich so einfach nicht bewerten. Zum einen wollen wir ja eigentlich mit Random Forest eine Klassifikation durchführen und mit linearen Regressionsmodellen eher kausale Modelle schätzen. Am Ende musst du selber abschätzen, was in das finale Modell soll. Ich kann ja auch den Threshold für den Variablenausschluss selber wählen. Wähle ich einen Threshold von \\(0.008\\), dann hätte ich crp, weight, bloodpressure und sex mit in dem finalen Modell.\n\n36.4.6 Feature Selektion mit boruta\n\nMit dem Boruta Algorithmus steht uns noch eine andere Implemnetierung des Random Forest Algorithmus zu verfügung um Feature Selektion zu betreiben. Wir nutzen wieder den Boruta Algorithmus in seine einfachen Form und gehen nicht tiefer auf alle Optionen ein. Wir nehmen wieder als Beispiel den Datensatz zu den infizierten Ferkeln und nutzen in diesem Fall auch nur die rohen Daten. Über eine Standardisierung könnte man wiederum nachdenken.\n\n\nIch empfehle noch das Tutorium Feature Selection in R with the Boruta R Package. Wir gehen hier nicht tiefer auf die Funktionalität von Boruta ein.\n\n\n\n\n\n\nStandardisieren oder Normalisieren von Daten\n\n\n\nEine Herausforderung für maschinelle Lernverfahren sind nicht normalisierte Daten. Das heist, dass wir Variablen haben, die kategorial oder kontinuierlich sein können oder aber sehr unterschiedlich von den Einheiten sind. Deshalb wird empfohlen die Daten vorher zu Standardisieren oder zu Normalisieren. In dem Kapitel 17 kannst du mehr über das Transformieren von Daten nachlesen.\n\n\nUm die Funktion Boruta() zu nutzen brauchen wir wieder das Modell und den Datensatz. Sonst ist erstmal nichts weiter anzugeben. Die Funktion läuft dann durch und gibt auch gleich den gewollten Informationshappen. Wichtig ist hierbei, dass wir natürlich noch andere Optionen mit angeben können. Wir können die Anzahl an Iterationen erhöhen und andere Tuning Parameter eingeben. Hier muss man immer schauen was am besten passt. Da würde ich auch immer rumprobieren und auf der Hilfeseite der Funktion ?Boruta einmal nachlesen.\n\nset.seed(20221031)\nboruta_output &lt;- Boruta(infected ~ age + sex + location + activity + crp + frailty + bloodpressure + weight + creatinin,  \n                        data = pig_tbl)  \n\nboruta_output\n\nBoruta performed 99 iterations in 2.884944 secs.\n 1 attributes confirmed important: crp;\n 6 attributes confirmed unimportant: activity, age, frailty, location,\nsex and 1 more;\n 2 tentative attributes left: bloodpressure, creatinin;\n\n\nManchmal ist es super praktisch, wenn eine Funktion einem die Antwort auf die Frage welche Variable bedeutend ist, gleich liefert. Wir erhalten die Information, dass die Variable crp als bedeutsam angesehen wird. Wir können uns den Zusammenhang auch in der Abbildung 36.2 auch einmal anschauen. Die grünen Variablen sind die bedeutenden Variablen.\n\nplot(boruta_output, cex.axis=.7, las=2, xlab=\"\", main=\"Variable Importance\")  \n\n\n\nAbbildung 36.2— Visualisierung der Boruta Ausgabe.\n\n\n\nAm Ende finden wir auch hier die Variable crp als einziges als bedeutend wieder. Wenn wir noch Variablen haben die verdächtig oder vorläufig bedeutend sind, angezeigt durch 2 tentative attributes left: bloodpressure, sex, dann können wir noch die Funktion TentativeRoughFix() nutzen. Die Funktion TentativeRoughFix() rechnet die Variablen nochmal nach und versucht alle Variablen in bedeutend oder nicht bedeutend zu klassifizieren. Wir haben ja zwei tentative Variablen in unseren Fall vorliegen, also nutzen wir noch kurz die Funktion um uns auch hier Klarheit zu schaffen.\n\nTentativeRoughFix(boruta_output)\n\nBoruta performed 99 iterations in 2.884944 secs.\nTentatives roughfixed over the last 99 iterations.\n 2 attributes confirmed important: creatinin, crp;\n 7 attributes confirmed unimportant: activity, age, bloodpressure,\nfrailty, location and 2 more;\n\n\nAm Ende ist die klare Aussage einer Funktion auch immer ein zweischneidiges Schwert. Wir verlieren jetzt noch die beiden tentative Variablen. Wo wir bei ranger die Qual der Wahl haben, werden wir bei Boruta eher vor vollendete Tatsachen gestellt. Meistens neigt man nach einer Boruta-Analyse nicht dazu noch eine zusätzliche Variable mit ins Modell zu nehmen. Dafür ist dann die Ausgabe zu bestimmt, obwohl die Entscheidung am Ende auch genau so unsicher ist wie von ranger und den anderen Modellen."
  },
  {
    "objectID": "stat-modeling-variable-selection.html#referenzen",
    "href": "stat-modeling-variable-selection.html#referenzen",
    "title": "36  Variablenselektion",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nHeinze, Georg, und Daniela Dunkler. 2017. „Five myths about variable selection“. Transplant International 30 (1): 6–10.\n\n\nHeinze, Georg, Christine Wallisch, und Daniela Dunkler. 2018. „Variable selection–a review and recommendations for the practicing statistician“. Biometrical journal 60 (3): 431–49.\n\n\nTalbot, Denis, und Victoria Kubuta Massamba. 2019. „A descriptive review of variable selection methods in four epidemiologic journals: there is still room for improvement“. European journal of epidemiology 34 (8): 725–30."
  },
  {
    "objectID": "stat-modeling-outlier.html#theoretischer-hintergrund",
    "href": "stat-modeling-outlier.html#theoretischer-hintergrund",
    "title": "37  Ausreißer",
    "section": "\n37.1 Theoretischer Hintergrund",
    "text": "37.1 Theoretischer Hintergrund\nWas sind Ausreißer (eng. Outlier) in einem Datensatz? An scih schon eine schwierige Frage. Einige Wissenschaftler behaupten es gebe keine Ausreißer. Die Daten müssetn so ausgewertet werden wie die Daten erhoben wurden. Was es gäbe wären technische Artefakte, diese müssten entdeckt und entfernt werden. Andere Wissenschaftler meinen, dass Ausreißer schon existieren und entferent werden müssen, wenn diese Ausreißer nicht zu der Fragestellung oder den restlichen Daten passen. Es ist eine unbekannte Subpopulation, die sich mit einem oder zwei Vertretern in unsere Daten geschmugelt hat. Diese Subpopulation verzerrt nur das Ergebnis, da wir mit diesen wenigen anderen Beobachtungen sowieso keine Aussage treffen können. Am Ende geht es aber draum Ausreißer zu finden und diese aus den Daten zu entfernen. Wir setzen dann diese Werte der Ausreißer auf NA für fehlender Wert (eng. not available). Oder aber wir erstetzen die Ausreißer durch pasendere Werte aus unseren Daten. Im Prinzip ein wenig wie finde den Ausreißer und imputiere den Ausreißer mit einer anderen Zahl. Mehr zur Imputation von fehlenden Werten findest du in Kapitel 38. Vermeide bitte eine Ausreißer/Imputationsschleife in der du immer wieder Ausreißer findest und diese dann wieder imputierst! Gerade dieses Thema Ausreißer kann sehr gut von biologischen Fachexperten diskutiert werden.\nIn den folgenden Abschnitten wollen wir uns verschiedene Möglichkeiten der Detektion von Ausreißern annähern. Es geht wie immer von algorithmisch einfach zu komplexer.\n\n\n\n\n\n\nBitte beiße dich nicht an der statistischen Auslegung eines Ausreißers fest. Du bist der Herr oder die Frau über deine Daten. Kein Algorithmus weis mehr als du. Das macht statistischen Engel natürlich traurig…\nBitte beachte, dass wenn du weist, dass ein Wert nicht richtig ist, diesen dann auch entfernt. Wenn du während der Beprobung feststellst, dass du leider auf dem Feld zu wenig Erde mitgenommen hast, dann trage ein NA in die Tabelle ein. Unsinnige Werte einzutragen nur weil die ja so entstanden sind, macht keinen Sinn. Auch kann es sein, dass du dich mal vertippst. Das heist, du hast in die Exceltabelle eine 0 oder ein Komma falsch gesetzt. Das findest du jetzt in der explorativen Datenanalyse raus. Dann bitte korrigiere diese Werte und mache bitte nicht hier mit der Detektion von Ausreißern weiter. Wenn du selber weist, warum da so ein komischer Wert in der Tabelle steht, dann korrigiere den Wert und schreibe in deinen Bericht, was du getan hast.\n\n\n\n\n\n\nSensitivitätsanalysen nach der Entfernung von Ausreißer\n\n\n\nNachdem wir Beobachtungen aus unseren Daten entfernt haben, ist es üblich noch eine Sensitivitätsanalysen durchzuführen. Wir Vergleich dann das gereinigte Modell mit anderen Modellen. Oder wir wollen die Frage beantworten, was hat eigentlich mein Entfernen von Ausreßern am Ergebnis geändert? Habe ich eine wichtige Beobachtung rausgeschmissen? Das machen wir dann gesammelt in dem Kapitel 39 zu den Sensitivitätsanalysen."
  },
  {
    "objectID": "stat-modeling-outlier.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-outlier.html#genutzte-r-pakete-für-das-kapitel",
    "title": "37  Ausreißer",
    "section": "\n37.2 Genutzte R Pakete für das Kapitel",
    "text": "37.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, dlookr, broom,\n               see, performance, ggpubr, factoextra, FactoMineR,\n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\nconflict_prefer(\"set_names\", \"magrittr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-outlier.html#daten",
    "href": "stat-modeling-outlier.html#daten",
    "title": "37  Ausreißer",
    "section": "\n37.3 Daten",
    "text": "37.3 Daten\nUm die Detektion von Ausreißern besser zu verstehen, schauen wir uns zwei Beispieldaten an. Zum einen den Datensatz zu den langnasigen Hechten. Der Datensatz der langnasigen Hechte ist ein echter Datensatz, so dass wir hier eventuell Ausreißer finden werden.\n\nlongnose_tbl &lt;- read_csv2(\"data/longnose.csv\") \n\nIn der Tabelle 37.1 ist der Datensatz longnose_tbl nochmal für die ersten sieben Zeilen dargestellt. Wir haben hier keine fehlenden Werte vorliegen.\n\n\n\n\nTabelle 37.1— Auszug aus dem Datensatz longnose_tbl. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\n\n\n\n\n\n\n\n\n\n\nstream\nlongnose\narea\ndo2\nmaxdepth\nno3\nso4\ntemp\n\n\n\nbasin_run\n13\n2528\n9.6\n80\n2.28\n16.75\n15.3\n\n\nbear_br\n12\n3333\n8.5\n83\n5.34\n7.74\n19.4\n\n\nbear_cr\n54\n19611\n8.3\n96\n0.99\n10.92\n19.5\n\n\nbeaver_dam_cr\n19\n3570\n9.2\n56\n5.44\n16.53\n17.0\n\n\nbeaver_run\n37\n1722\n8.1\n43\n5.66\n5.91\n19.3\n\n\nbennett_cr\n2\n583\n9.2\n51\n2.26\n8.81\n12.9\n\n\nbig_br\n72\n4790\n9.4\n91\n4.10\n5.65\n16.7\n\n\n\n\n\n\nIm Weiteren betrachten wir noch das Beispiel der Gummibärchendaten. Auch hier haben wir echte Daten vorliegen, so dass wir eventuell Ausreißer entdecken könnten. Da wir hier fehlende Werte in den Daten haben, entfernen wir alle fehlenden Werte mit der Funktion na.omit(). Damit löschen wir jede Zeile in den Daten, wo mindestens ein fehlender Wert auftritt.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  %&gt;%\n  select(gender, age, height, semester, most_liked) %&gt;% \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) %&gt;% \n  na.omit()\n\nIn der Tabelle 37.2 ist der Datensatz gummi_tbl nochmal für die ersten sieben Zeilen dargestellt. Nun haben wir hier in dem Datensatz zu den Gummibärchen auch keine fehlenden Werte mehr.\n\n\n\n\nTabelle 37.2— Auszug aus dem Datensatz gummi_tbl. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\ngender\nage\nheight\nsemester\nmost_liked\n\n\n\nm\n35\n193\n10\nlightred\n\n\nw\n21\n159\n6\nyellow\n\n\nw\n21\n159\n6\nwhite\n\n\nw\n36\n180\n10\nwhite\n\n\nm\n22\n180\n3\nwhite\n\n\nm\n22\n180\n3\ngreen\n\n\nw\n21\n163\n3\ngreen\n\n\n\n\n\n\nNun wollen wir uns aber erstmal den simpelsten Fall von Ausreißern und die Problematik dahinter visualisieren."
  },
  {
    "objectID": "stat-modeling-outlier.html#ausreißer-mit-cooks-abstand",
    "href": "stat-modeling-outlier.html#ausreißer-mit-cooks-abstand",
    "title": "37  Ausreißer",
    "section": "\n37.4 Ausreißer mit Cook`s Abstand",
    "text": "37.4 Ausreißer mit Cook`s Abstand\nMit der Cook’schen Distanz können wir herausfinden, ob eine einzelne Beobachtung ein Ausreißer im Zusammenhang zu den anderen Beobachtungen ist. Die Cook’sche Distanz misst, wie stark sich alle geschätzten Werte im Modell ändern, wenn der \\(i\\)-te Datenpunkt gelöscht wird. Veranschaulichen wir uns einmal den Zusammenhang an zwei Beispieldaten.\n\n\nDas R Paket olsrr erlaubt eine weitreichende Diagnostik auf Ausreißer für einem normalverteilten Outcome \\(y\\).\n\n\nTabelle 37.3— Zwei Datentabellen zum Vergleich der Detektion von Ausreißern nach Cook’s Abstand.\n\n\n\n\n(a) Keine Ausreißer\n\nweight\njump_length\n\n\n\n1.2\n22\n\n\n2.1\n23\n\n\n2.5\n24\n\n\n3.1\n23\n\n\n4.6\n19\n\n\n5.1\n34\n\n\n7.9\n35\n\n\n3.1\n36\n\n\n4.3\n23\n\n\n5.5\n22\n\n\n\n\n\n\n(b) Zwei Ausreißer\n\nweight\njump_length\n\n\n\n1.1\n190\n\n\n2.3\n23\n\n\n2.1\n24\n\n\n3.7\n23\n\n\n4.1\n19\n\n\n5.4\n24\n\n\n7.6\n25\n\n\n4.3\n26\n\n\n5.8\n28\n\n\n8.1\n180\n\n\n\n\n\n\nWir können uns die Daten der Tabelle 37.3 auch als Visualisierung in Abbildung 37.1 anschauen. Wir sehen die starken Ausreißer in der Visualisierung. Das ist auch so gewollt, wir haben die Ausreißer extra sehr extrem gewählt.\n\n\n\n\n\n(a) Scatterplot ohne Ausreißer\n\n\n\n\n\n(b) Scatterplot mit Ausreißer\n\n\n\nAbbildung 37.1— Scatterplots der Datentablle zum Vergleich der Detektion von Ausreißern nach Cook’s Abstand.\n\n\nIm Folgenden rechnen wir eine simple Gaussian lineare Regression auf den Daten und schauen einmal, was wir dann über die einzelnen Beobachtungen erfahren und ob wir die eingestellten Ausreißer wiederfinden.\n\nfit_cook &lt;- lm(jump_length ~ weight, data = out_tbl)\n\nWir können die Funktion augment() nutzen um die Cook’sche Distanz als .cooksd aus dem linearen Modellfit zu berechnen. Wir lassen uns noch die Variable weight wiedergeben um uns später dann die Visualisierung zu erleichtern.\n\ncook_tbl &lt;- fit_cook %&gt;% \n  augment %&gt;% \n  select(weight, .cooksd)\ncook_tbl\n\n# A tibble: 10 × 2\n   weight .cooksd\n    &lt;dbl&gt;   &lt;dbl&gt;\n 1    1.1  1.33  \n 2    2.3  0.0300\n 3    2.1  0.0321\n 4    3.7  0.0147\n 5    4.1  0.0168\n 6    5.4  0.0158\n 7    7.6  0.0665\n 8    4.3  0.0108\n 9    5.8  0.0150\n10    8.1  1.39  \n\n\nZuerst sehen wir, dass die \\(1\\)-ste und die \\(10\\)-te Beobachtung sehr hohe Werte der Cook’schen Distanz haben. Das heist hier ist irgendwas nicht in Ordnung. Das haben wir ja auch so erwartet. Die beiden Beobachtungen sind ja auch unsere erschaffene Ausreißer. Nun brauchen wir noch einen Threshold um zu entscheiden ab wann wir eine Beobachtung als Ausreißer definieren. Es hat sich als “Kulturkonstante” der Wert von \\(4/n\\) als Threshold etabliert. Berechnen wir also einmal den Threshold für unseren Datensatz indem wir \\(4\\) durch \\(n = 10\\) teilen.\n\ncooks_border &lt;- 4/nrow(cook_tbl)\ncooks_border\n\n[1] 0.4\n\n\nWir erhalten einen Threshold von \\(0.4\\) gespeichert in cooks_border. In Abbildung 37.2 haben wir den Threshold einmal eingezeichnet. Auf der x-Achse ist das weight, damit sich die Punkte etwas verteilen. Wir könnten auch den Index auf die x-Achse legen.\n\nggplot(cook_tbl, aes(weight, .cooksd)) +\n  geom_point() +\n  geom_hline(yintercept = cooks_border, color = \"red\") +\n  theme_bw()\n\n\n\nAbbildung 37.2— Visualisierung der Ausreißer nach der Cook’schen Distanz. Die Grenze als rote Linie ist mit \\(4/n = 0.4\\) berechnet worden.\n\n\n\nWir können jetzt mit der Funktion which() bestimmen welche Beobachtungen wir als Ausreißer identifizert haben. Was wir jetzt mit den Ausreißern machen, müssen wir uns überlegen. Im Prinzip haben wir zwei Möglichkeiten. Entweder entfernen wir die Beobachtungen aus unserem Datensatz oder aber wir setzen die Werte der Ausreißer auf NA oder eine andere pasendere Zahl.\n\nremove_weight_id &lt;- which(cook_tbl$.cooksd &gt; cooks_border)\n\nWeil es hier am Anfang noch relativ einfach sein soll, entfernen wir einfach die beiden Ausreißer aus unseren Daten. Wir erhalten dann einen kleineren Datensatz mit \\(n = 8\\) Beobachtungen.\n\nout_tbl &lt;- out_tbl[-remove_weight_id,]\n\nout_tbl\n\n# A tibble: 8 × 2\n  weight jump_length\n   &lt;dbl&gt;       &lt;dbl&gt;\n1    2.3          23\n2    2.1          24\n3    3.7          23\n4    4.1          19\n5    5.4          24\n6    7.6          25\n7    4.3          26\n8    5.8          28\n\n\nDu siehst, dieser Zugang an die Detektion von Ausreißern ist sehr simple. Wir schauen einfach auf die Cook’sche Distanz und haben so einen schnellen Überblick. Ich empfehle auch gerne dieses Vorgehen um einmal einen Überblick über die Daten zu erhalten. Leider leifern nicht alle Modelle eine Cook’sche Distanz, daher müssen wir uns jetzt etwas strecken und noch andere Verfahren einmal ausprobieren."
  },
  {
    "objectID": "stat-modeling-outlier.html#ausreißer-mit-performance",
    "href": "stat-modeling-outlier.html#ausreißer-mit-performance",
    "title": "37  Ausreißer",
    "section": "\n37.5 Ausreißer mit performance\n",
    "text": "37.5 Ausreißer mit performance\n\nNun wollen wir uns den echten Daten zuwenden und dort einmal schauen, ob wir Ausreißer finden können. Wir nutzen hierzu einmal die Funktion check_outliers() aus dem R Paket performance. Die Funktion check_outliers() rechnet nicht eine staistische Maßzahl für die Bestimmung eines Ausreißers sondern eine ganze Reihe an Maßzahlen und gewichtet diese Maßzahlen. Am Ende trifft die Funktion check_outliers() dann eine Entscheidung welche Beobachtungen Aureißer sind. Dabei werden alle Variablen betrachtet. Es gibt keinen Unterschied zwischen \\(y\\) oder \\(x\\). Wir nutzen den ganzen Datensatz.\n\n\nDas R Paket performance hat die Möglichkeit zur Outliers detection (check for influential observations).\n\ncheck_outliers(longnose_tbl)\n\n2 outliers detected: cases 18, 43.\n- Based on the following method and threshold: mahalanobis (24.32).\n- For variables: longnose, area, do2, maxdepth, no3, so4, temp.\n\n\nWir finden also sieben Aureißer in unseren Daten. Wir können diese Beobachtungen einmal mit der Funktion extract() rausziehen und uns anschauen. Wie immer mit so Datensätzen erkennen wir hier noch nicht so ein richtiges Muster. Da müssen wir dann nochmal andere Funktionen nutzen.\n\nlongnose_tbl %&gt;% \n  extract(c(8, 17, 18, 21, 28, 43, 44), )\n\n# A tibble: 7 × 8\n  stream              longnose  area   do2 maxdepth   no3   so4  temp\n  &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 big_elk_cr               164 35971  10.2       81  3.2  17.5   13.8\n2 conowingo_cr             112 27350   8.5       65  6.95 14.9   24.1\n3 dead_run                  25  4145   8.7       51  0.34 44.9   23  \n4 dorsey_run                 8  7814   6.8      160  0.44 20.2   22.6\n5 haines_br                 98  1967   8.6       50  7.71 26.4   16.8\n6 mainstem_patuxent_r      239  8636   8.4      150  3.31  5.95  18.1\n7 meadow_br                234  4803   8.5       93  5.01 11.0   24.3\n\n\nEbenso schauen wir einmal bei dem Gummibärchendaten nach, welche der Beobachtungen ein Ausreißer sein könnten.\n\ncheck_outliers(gummi_tbl)\n\n11 outliers detected: cases 95, 115, 119, 122, 138, 179, 226, 272, 273,\n  274, 275.\n- Based on the following method and threshold: mahalanobis (16.27).\n- For variables: age, height, semester.\n\n\nWir wissen ja, dass wir recht viele Beobachtungen haben und wie wir sehen, scheint sich so ein Cluster in den hohen Zweihundertsiebzigern gebildet zu haben. Manchmal echt spanned, was man so finden kann.\n\ngummi_tbl %&gt;% \n  extract(c(1, 4, 66, 81, 95, 115, 119, 122, 138, 155, 179, 226, 248, 263, 272, 273, 274, 275), )\n\n# A tibble: 18 × 5\n   gender   age height semester most_liked\n   &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;     \n 1 m         35    193       10 lightred  \n 2 w         36    180       10 white     \n 3 w         43    168        1 darkred   \n 4 w         42    175        1 green     \n 5 w         27    168       15 white     \n 6 w         47    178        1 darkred   \n 7 w         54    163        1 darkred   \n 8 w         53    167        1 darkred   \n 9 m         22    191       13 none      \n10 w         23    159       11 darkred   \n11 m         26    176       15 none      \n12 w         26    150       12 darkred   \n13 m         31    185       11 green     \n14 m         26    185       12 darkred   \n15 w         54    171        0 orange    \n16 w         46    165        0 darkred   \n17 w         55    177        0 darkred   \n18 w         46    173        0 green     \n\n\nAm Ende muss man dann bei der Funktion check_outliers() daran glauben, dass es sich um Ausreißer handelt. Wir müssen uns dann die Daten genau anschauen und entscheiden, ob wir wirklich so viele Beobachtungen entfernen wollen. Darüber hinaus wisen wir jetzt gar nicht, warum wir eine Beobachtung als Ausreißer definiert haben. Dafür müsstest du dann tiefer in die Funktion mit der Hilfeseite ?check_outliers() einsteigen."
  },
  {
    "objectID": "stat-modeling-outlier.html#ausreißer-mit-dlookr",
    "href": "stat-modeling-outlier.html#ausreißer-mit-dlookr",
    "title": "37  Ausreißer",
    "section": "\n37.6 Ausreißer mit dlookr\n",
    "text": "37.6 Ausreißer mit dlookr\n\nEine weitere Möglichkeit Ausreißer zu finden bietet das R Paket dlookr mit der Funktion diagnose_outlier(). Neben dieser Funkion hat das R Paket dlookr noch eine Vielzahl an weiteren Funktionen, die im Bereich der explorativen Datenanalyse von nutzen sein mögen. Schau dir einfach einmal das R Paket und die Webseite näher an.\n\n\nDas R Paket dlookr hat eine große Auswahl an simplen Algorithmen für die Anpassung von Ausreißern.\nWir wollen hier jetzt nur die Funktion diagnose_outlier() und plot_outlier() nutzen. Also wir werden zuerst die Ausreißer finden und dann die Ausreißer einmal visualisieren. Im Gegensatz zu dem R Paket performance nutzt das R Paket dlookr nicht eine große Auswahl an Algorithmen und pooled die Ergebnisse. Wir schauen einmal was wir so finden und was die Vorteile und Nachteile sind.\n\ndiagnose_outlier(longnose_tbl) %&gt;% \n  filter(outliers_cnt &gt; 0) %&gt;% \n  arrange(desc(outliers_cnt))\n\n# A tibble: 6 × 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean without_mean\n  &lt;chr&gt;            &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 longnose             7          10.3         150.       38.8         26   \n2 area                 5           7.35      28204.     7565.        5927.  \n3 so4                  3           4.41         32.0      11.7         10.7 \n4 do2                  2           2.94          9.15      8.76         8.75\n5 maxdepth             2           2.94        155        72.6         70.1 \n6 no3                  1           1.47          7.71      2.67         2.60\n\n\nNachdem wir wieder den ganzen Datensatz unser langnasigen Hechte in dei Funktion diagnose_outlier() gesteckt haben, kriegen wir einen Datensatz wieder in dem jede Variable mit ihrer anzahl an Ausreißern outliers_cnt angezeigt wird. Wir haben also \\(7\\) Ausreißer in dem Outcome longnose sowie in anderen Variablen für \\(x\\). Schauen wir uns einmal die Ausreißer und deren Effekt in der Abbildung 37.3 an.\n\nlongnose_tbl %&gt;% plot_outlier(longnose)\nlongnose_tbl %&gt;% plot_outlier(area)\n\n\n\n\n\n(a) Outcome longnose.\n\n\n\n\n\n(b) Variable area.\n\n\n\nAbbildung 37.3— Boxplot und Histogramm der Variablen longnose und area mit und ohne Ausreißer.\n\n\n\nIn Abbildung 37.3 (a) sehen wir den Boxplot und das Histogramm der des Outcomes longnose einmal mit den Ausreißern und einmal ohne die Ausreißer. Wie du sehen kannst ändert sih die Verteilung des Outcomes dramatisch, wenn wir die Ausreißer entfernen. Deshalb müssen wir besonders bei einem Outcome gut überlegen, ob es eine gute Idee ist, die Ausreißer zu entfernen. Auch in Abbildung 37.3 (b) sehen wir für die Variable area die Boxplots und das Histogramm einmal mit den Ausreißern und einmal ohne die Ausreißer. Auch hier sehen wir eine Veränderung in der Verteilung, wenn wir die Ausreißer entfernen. Du siehst, Ausreißer sind immer Teil der Daten und ändern deren Verteilung.\nWir machen jetzt einfach weiter und stören uns nicht an möglichen biologischen Implikationen. Wir wollen jetzt die Ausreißer nicht nur entfernen, sondern durch neue Datenpunkte ersetzen. Faktisch imputieren wir die Ausreißer. Wenn unsere zu imputierende Variable kontinuierlich ist, dann können wir folgende Verfahren nutzen um die Ausreißer in der Variablen zu ersetzen.\n\n\nmean: Wir ersetzen die gefundenen Ausreißer mit dem Mittelwert der Variable.\n\nmedian : Wir ersetzen die gefundenen Ausreißer mit dem Median der Variable.\n\nmode : Wir ersetzen die gefundenen Ausreißer mit dem Modus der Variable. Also dem am meisten vorkommenden Wert in der Variable.\n\ncapping : Wir imputieren die “oberen” Ausreißer mit dem 95% Quantile und die “unteren” Ausreißer mit dem 5% Quantile. Wir schneiden sozusagen die Verteilungsenden ab.\n\nIm folgenden nutzen wir einmal das capping oder die Quantileimputation sowie die median Imputation. Schauen wir mal was dann pasiert.\n\nlongnose_imp_tbl &lt;- longnose_tbl %&gt;% \n  mutate(longnose_capping = imputate_outlier(., longnose, method = \"capping\"),\n         longnose_median = imputate_outlier(., longnose, method = \"median\"))\n\nIn Abbildung 37.4 sehen wir die beiden Imputationsmethoden capping und median für die Ausreißer in den Daten einmal dargestellt. Wir sehen das in der Abbildung 37.4 (a) durch die Quantilimputation die Ausreißer alle nach links geschoben werden. Es entsteht ein neues lokales Maximum. Ob das so sinnvoll ist, sei mal dahingestellt. Der Vorteil ist, dass sich dadurch die anderen Teile der Verteilung nicht ändern. Wenn wir die Medianimputation durchführen, dann erhöhen wir das Maximum der ursprünglichen Verteilung noch weiter. Dafür verlieren wir dann eben alle Werte über Einhundert. Hier müssen wir vermutlich wirklich mal an die Sensitivitätsanalyse ran.\n\npluck(longnose_imp_tbl, \"longnose_capping\") %&gt;% \n  plot\n\npluck(longnose_imp_tbl, \"longnose_median\") %&gt;% \n  plot\n\n\n\n\n\n(a) Imputation mit dem Mittelwert.\n\n\n\n\n\n(b) Imputation mit dem Median.\n\n\n\nAbbildung 37.4— Densityplot der Verteilungen vor und nach der Imputation.\n\n\n\nWie du siehst ist eine Detektion von Ausreißern nicht so einfach. Zum einen brauchen wir dazu Daten, damit wir auch Ausrißer finden können. Zu irgendwas müssen wir ja die einzelnen Beobachtungen vergleichen. Zum anderen können wir durch die Imputation die Verteilung der Daten stark ändern. Es bleibt ein Drahtseilakt."
  },
  {
    "objectID": "stat-modeling-outlier.html#hauptkomponentenanalyse",
    "href": "stat-modeling-outlier.html#hauptkomponentenanalyse",
    "title": "37  Ausreißer",
    "section": "\n37.7 Hauptkomponentenanalyse",
    "text": "37.7 Hauptkomponentenanalyse\nWir kürzen die Hauptkomponentenanalyse mit PCA ab.\nDie Hauptkomponentenanalyse (eng. Principle Component Analysis, abk. PCA) als Abschnitt im Ausreißerkapitel ist der Hauptkomponentenanalyse nicht würdig. Wir könnten hier ein eigenes Kapitel nur über die PCA, wie ich die Hauptkomponentenanalyse ab jetzt immer abkürzen werde, schreiben und das würde nicht reichen. Das hat vor allem damit zu tun, dass die PCA in den Sozialwissenschaften sehr weitreichend genutzt wird. Ebenso ist die Auswertung von Fragebögen allgemein ein Schwerpunkt der PCA. Wir nutzen die PCA hier jetzt um zu sehen, ob wir Ausreißer in unseren Beobachtungen haben. Das ist also eine sehr spezifische Anwendung. Vielleicht wird dieses Kapitel nochmal größer, aber jetzt bleiben wir einmal bei der Anwendung.\nWas ist grob die Idee der PCA? Wir wollen unseren Daten, also die ganze Datenmatrix einmal so transformieren, dass wir neue Komponenten aus den Daten extrahieren, die die Daten auf einer anderen Dimension beschreiben. Klingt etwas kryptisch, aber im Prinzip handelt es sich bei der PCA um eine Transformation der Daten. Wir nutzen dabei die Varianzstruktur und die Varianz/Covarianzmatrix. Im Prinzip also die Korrelation zwischen den einzelnen Variablen in dem Datensatz.\nNeben der PCA existiert noch das Multidimensional Scaling (abk. MDS). Das MDS ist im Prinzip eine Spezialform der PCA. Im Unterschied zur PCA wird die MDS auf einer Distanzmatrix gerechnet. In einer MDS können wir nicht einfach so unsere Daten reinstecken sondern müssen zuerst die Daten in eine Distanzmatrix umrechnen. Dafür gibt es die Funktion dist() oder as.dist(), wenn wir schon Distanzen vorliegen haben. Daher ist die Anwendung einer MDS nicht besonders komplizierter.\n\n\nWie immer gibt es eine Vielzahl an tollen Tutorien, die die PCA gut erklären. Ich habe hier einmal eine Auswahl zusammengestellt und du kannst dich da ja mal vertiefend mit beschäftigen, wenn du willst. Teile der Tutorien findest du vermutlich hier im Haupttext wieder.\n\nPrincipal Component Analysis (PCA) For Dummies\nPrincipal Component Analysis 4 Dummies: Eigenvectors, Eigenvalues and Dimension Reduction\nThe most gentle introduction to Principal Component Analysis\n\nEs gibt eine natürlich große Anzahl an Quellen wie du in R eine PCA oder ein MDS durchführst. In der folgenden Box findest du eine Sammlung an Tutorien und R Code, der dir als Inspiration dienen mag. Ich werde teile von den Tutorien in der Folge verwenden, kann aber natürlich nichts alles nochmal machen.\n\n\n\n\n\n\nWeitere R Quellen für die Principal Component Analysis\n\n\n\n\n\n\nPrincipal Component Analysis in R Tutorial\nFactoextra R Package: Easy Multivariate Data Analyses and Elegant Visualization\nPrincipal Component Analysis in R: prcomp vs princomp\nMultidimensional Scaling Essentials: Algorithms and R Code\n\n\n\n\n\n37.7.1 Principal Component Analysis (PCA)\nWenn wir eine PCA in R rechnen wollen, dann haben wir zuerst die Wahl zwischen den Funktionen prcomp() und princomp(). Laut der R-Hilfe hat die Funktion prcomp() eine etwas bessere numerische Genauigkeit. Daher ist die Funktion prcomp() gegenüber princomp() vorzuziehen. Daher nutzen wir jetzt auch die Funktion prcomp(). Es gibt aber noch eine neuere Implementierung der Funktionalität in dem R Paket FactoMineR und der Funktion PCA(). Langer Satz kurzes Fazit, wir nutzen die Funktion PCA() im folgenden Abschnitt.\nWir müssen uns jetzt leider etwas von dem tibble verabschieden. Für die PCA brauchen wir einen Datensatz, in dem nur Zahlen oder Faktoren stehen. Daher schieben wir die Namen der Beobachtungen oder die ID in die Zeilennamen. Eigentlich keine gute Idee für die Arbeit mit Daten, aber für die PCA passt es. Wir haben dann also den data.frame() als longnose_pca_df vorliegen. Mit diesem Datensatzobjekt können wir dann in die PCA starten.\n\nlongnose_pca_df &lt;- longnose_tbl %&gt;%\n  select(-stream) %&gt;% \n  as.data.frame() %&gt;% \n  set_rownames(longnose_tbl$stream)\n\nWenn wir die Daten von jeglichen character Spalten gereinigt haben, dann können wir die Funktion PCA() nutzen. Wir wollen uns die Visualisierung gleich selber nochmal nachbauen, deshalb hier die Option mit graph = FALSE. Im folgenden schauen wir uns nur eine Auswahl an möglichen Abbildungen an. Davon natürlich die wichtigsten Abbildungen, aber das Factoextra R Package: Easy Multivariate Data Analyses and Elegant Visualization kann natürlich noch viel mehr.\n\npca_res &lt;- PCA(longnose_pca_df,  graph = FALSE)\n\nNachdem wir die PCA durchgeführt haben, schauen wir uns einmal an, ob es überhaupt irgendwas gebracht hat, dass wir die PCA durchgeführt haben. Wir haben ja unsere Daten transformiert und erhalten pro Variable eine neue Dimension wieder. In einem Scree Plot wie in Abbildung 37.5 wird die erklärte Varianz pro Hauptkomponente gezeigt.\n\nfviz_screeplot(pca_res, addlabels = TRUE, ylim = c(0, 50))\n\n\n\nAbbildung 37.5— Scree Plot der PCA mit der erklärten Varianz pro Hauptkomponente.\n\n\n\nWir erkennen, dass unsere Hauptkomponenten teilweise viel Varianz erklären, aber wir keine Hauptkomponente gefunden haben, die sehr viel Varianz erklärt. Es gibt also in unseren erhobenen Daten keine Variable, die “alles” erklärt. Mit “alles” ist dann natürlich die Varianz gemeint. In Abbildung 37.6 (a) sehen wir das Diagramm der Variablen, also der Spalten in der Datenmatrix. Positiv korrelierte Variablen zeigen auf dieselbe Seite des Diagramms. Negativ korrelierte Variablen zeigen auf die gegenüberliegenden Seiten des Diagramms. In Abbildung 37.6 (b) ist das Diagramm der Beobachtungen, also den Zeilen in der Datenmatrix, dargestellt. Beobachtungen mit einem ähnlichen Muster über die Zeilen werden in Gruppen zusammengefasst. In der Abbildung 37.6 (c) sehen wir nochmal die beiden Abbilungen zusammen und übereinander.\n\nfviz_pca_var(pca_res, col.var = \"black\")\n\nfviz_pca_ind(pca_res,\n             col.ind = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE)\n\nfviz_pca_biplot(pca_res)\n\n\n\n\n\n(a) Richtung der Variablen (Spalten)\n\n\n\n\n\n(b) Verteilung der Beobachtungen (Zeilen).\n\n\n\n\n\n(c) Komibination Variablen und Individuen.\n\n\n\nAbbildung 37.6— Diagramm der PCA für die Variablen (Saplten) und den Beobachtungen (Zeilen) der Datenmatrix.\n\n\n\nGut, und was sollte das ganze jetzt? Wenn wir uns die Abbildungen ansehen, dann erkennen wir zuerst, dass es zwar Variablen gibt, die sich sehr ähnlich sind. Die Variable area und maxdepth sind stark miteinander korreliert. Die Pfeile zeigen beide in die gleiche Richtung. Ebenso scheint es einen negativen Zusammenhang zwischen so4 und do2 zu geben. Die Richtung ist egal, wir können die Dimensionen nicht direkt interpretieren, aber die Zusammenhänge zwischen den Variablen. Abschließend sehen wir, dass wir eine zu erwartende Aufteilung der Variablen haben. Immerhin messen wir Werte eines Flusses, da sollten die Variablen was miteinander zu tun haben. Auch ist die Kausalität gegeben. Wir erwarten bei einer hohen Temperatur weniger Sauerstoff und umgekehrt.\nBei der Aufteilung der Beobachtungen sehen wir auch keine Auffälligkeiten. Wir sehen eine große Wolke mit keinen separaten Gruppen. Das heißt, obwohl wir weiter oben Ausreißer gefunden haben, würde ich hier alle Beoachtungen in den Daten lassen, wenn ich die PCA sehe. Es ist dann immer eine Abschätzung. Wir sehen aber keine sonderlich auffälligen Beobachtungen. Wir könnten noch überlegen, ob wir das Outcome longnose nicht doch lieber aus der PCA nehmen und festhalten, dass es im Outcome keine Ausreißer gibt. Kontroverse Entscheidung, dir wir uns überlegen müssten. Beobachtungen aus einem Datensatz zu entfernen, den man nicht selber erschaffen hat, ist immer eine sehr schwere Sache.\n\n37.7.2 Multi Dimensional Scaling (MDS)\nEine besondere Form der Hauptkomponentenanalyse ist das Multidimensional Scaling (abk. MDS). Im Prinzip sind die Mechanismen sehr ähnlich. Der Hauptunterschied ist aber, das wir für die MDS eine Distanzmatrix benötigen. Wir können dafür die Funktion dist() oder as.dist() nehmen, wenn wir schon Distanzen vorliegen haben. Nehmen wir als plakatives Beispiel einmal die Distanzen von europäischen Städten zueinander. Wir haben die Daten in der Exceldatei distance.xlsx vorliegen. Wir lesen die Daten einmal ein und schauen uns die ersten fünf Spalten und die ersten fünf Zeilen des Datensatzes einmal an.\n\ndistance_tbl &lt;- read_excel(\"data/distance.xlsx\")\n\ndistance_tbl[1:5, 1:5]\n\n# A tibble: 5 × 5\n  city      Amsterdam Antwerp Athens Barcelona\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 Amsterdam         0     160   3082      1639\n2 Antwerp         160       0   2766      1465\n3 Athens         3082    2766      0      3312\n4 Barcelona      1639    1465   3312         0\n5 Berlin          649     723   2552      1899\n\n\nWenn wir jetzt auf diesem Datensatz jetzt ein MDS rechnen wollen, dann müssen wir zum einen alle Spalten mit einem character entfernen. Wir haben dann nur noch einen Datensatz bzw. Datenmatrix mit den Distanzen vorliegen. Dann kann wir das tibble in einen dist-Objekt mit der Funktion as.dist() umwandeln. Die eigentliche Berechnung für das Multidimensional Scaling findet in der Funktion cmdscale() statt. Mit der Option k = 2 legen wir fest, dass wir nur zwei Hauptkomponenten bzw. Dimensionen bestimmen wollen. Wir machen also aus unserem 37x37 großen Datenmatrix durch Multidimensional Scaling eine Reduktion auf zwei Dimensionen bzw. Spalten.\n\nmds &lt;- distance_tbl %&gt;%\n  select(-city) %&gt;% \n  as.dist() %&gt;%          \n  cmdscale(k = 2) %&gt;%\n  as_tibble() %&gt;% \n  mutate(V1 = -V1,\n         V2 = -V2)\ncolnames(mds) &lt;- c(\"Dim.1\", \"Dim.2\")\n\nIn Abbildung 37.7 sehen wir das Ergebnis der Dimensionsreduktion auf zwei Dimensionen. Wir erhalten die Zusammenhänge bzw. Distanzen aus der Datenmatrix in einem Scatterplot. Ein Scatterplot ist ja nichts anders als die Darstellung von zwei Dimensionen. Wie wir sehen können nimmt die Anordnung der Orte in etwa die Positionen von den Orten auf der Landkarte in Europa ein. Natürlich stimmen die Relationen nicht perfekt, aber das Abbild ist schon recht nahe dran. Wir können also auf diese Art und Weise auch Ausreißer bestimmen.\n\nggscatter(mds, x = \"Dim.1\", y = \"Dim.2\", \n          label = distance_tbl$city,\n          size = 1,\n          repel = TRUE)\n\n\n\nAbbildung 37.7— Scatterplot der zwei Dimensionen nach dem Multidimensional Scaling für den Abstand europäischer Städte.\n\n\n\nWenn wir keine Distanzmatrix wie im obigen Beispiel zu den Entfernungen der europäischen Städte vorliegen haben, dann können wir uns die Distanzen auch mit der Funktion dist() berechnen lassen. Wir nutzen jetzt mal als Echtdaten die Daten der Gummibärchen. Mal sehen, ob wir hier irgendwelche Gruppen erkennen. Die Hilfeseite der Funktion ?dist zeigt welche mathematischen Distanzmaße wir auf die Daten anwenden können. In unseren Fall berechnen wir die euklidische Distanz zwischen den Beobachtungen. Dann rufen wir über die Funkion cmdsscale das Multidimensional Scaling auf.\n\nmds &lt;- gummi_tbl %&gt;%\n  dist(method = \"euclidean\") %&gt;%          \n  cmdscale(k = 2) %&gt;%\n  as_tibble() %&gt;% \n  set_names(c(\"Dim.1\", \"Dim.2\"))\n\nDas Ergebnis des Multidimensional Scaling hat keine Bedeutung für uns. Wir können die Zahlen nicht interpretieren. Was wir können ist das Ergebnis in einem Scatterplot wie in Abbildung 37.8 zu visualisieren.\n\nggscatter(mds, x = \"Dim.1\", y = \"Dim.2\", \n          label = rownames(gummi_tbl),\n          size = 1,\n          repel = TRUE)\n\n\n\nAbbildung 37.8— Scatterplot der zwei Dimensionen nach dem Multidimensional Scaling für den Gummibärchendatensatz.\n\n\n\nNa endlich, wir sehen mal eine Gruppe von Beobachtungen oder Ausreißern, die nicht in der Wolke aller Beobachtungen liegen. Beobachtungen mit einem Wert kleiner \\(-20\\) in der 2. Dimension könnten wir dann als Ausreißer entfernen. Der Rest bildet dann eine recht homogene Gruppe. Wir können uns aber auch das \\(k\\)-NN Verfahren aus dem Kapitel 58 nutzen um Cluster in den Daten zu finden. Das heißt wir nutzen das maschinelle Lernverfahren \\(k-NN\\) um uns \\(k\\) Cluster bestimmen zu lassen. Dafür nutzen wir die Funktion kmeans() und ziehen uns über die Funktion pluck() die Cluster raus. Daher erhalten wir einen Vektor mit Zahlen, die beschreiben in welchem Cluster die jeweilige \\(i\\)-te Beobachtung ist.\n\n# K-means clustering\nclust &lt;- kmeans(mds, centers = 5) %&gt;%\n  pluck(\"cluster\") %&gt;% \n  as.factor()\n\nWir wollen jetzt unser MDS Ergebnis von den Gummibärchen um eine Spalte für die Clusterergebnisse von \\(k\\)-NN ergänzen.\n\nmds &lt;- mds %&gt;%\n  mutate(groups = clust)\n\nNun sehen in Abbildung 37.9 die gleiche Abbildung wie oben nur ergänzt um die farbliche Hinterlegung der \\(k=5\\) Clustern aus dem \\(k\\)-NN Algorithmus. Wir können jetzt die Ausreißer numerisch feststellen und dann aus den Daten entfernen, wenn wir dies wollen würden. Entweder machen wir das über die Clusterzuordnung vom gelben Cluster 2 über die Funktion filter() oder aber wir suchen uns die Beobachtungen und damit Zeilen raus, die wir nicht mehr in den Daten wollen. Die Nummern stehen ja dabei.\n\nggscatter(mds, x = \"Dim.1\", y = \"Dim.2\", \n          label = rownames(gummi_tbl),\n          color = \"groups\",\n          palette = \"jco\",\n          size = 1, \n          ellipse = TRUE,\n          ellipse.type = \"convex\",\n          repel = TRUE)\n\n\n\nAbbildung 37.9— Scatterplot der zwei Dimensionen nach dem Multidimensional Scaling für den Gummibärchendatensatz mit den \\(k=5\\) Clustern aus dem \\(k\\)-NN Algorithmus."
  },
  {
    "objectID": "stat-modeling-missing.html#was-sind-fehlende-werte",
    "href": "stat-modeling-missing.html#was-sind-fehlende-werte",
    "title": "38  Imputation fehlender Werte",
    "section": "\n38.1 Was sind fehlende Werte?",
    "text": "38.1 Was sind fehlende Werte?\nWir beschränken uns hier auf drei Arten von fehlenden Daten. Es gibt noch mehr Abstufungen, aber für den Einstieg reicht es, wenn wir nach drei Typen von fehlenden Daten unterscheiden. Die anderen Typen sind Mischtypen bzw. nicht so von Belang für die Anwendung.\n\n\nMCAR (eng. missing completely at random): völlig zufällig fehlende Daten. Dies ist das wünschenswerte Szenario im Falle fehlender Daten. Fehlende Werte werden als missing completely at random bezeichnet, wenn die Wahrscheinlichkeit für das Fehlen eines Wertes weder von erfassten noch unerfassten Merkmalen abhängt. Daher kann man sagen, dass MCAR-Werte die Fallzahl reduzieren, aber das Studienergebnis nicht verzerren.\n\nMAR (eng. missing at random): Fehlende Werte werden als missing at random bezeichnet, wenn die Wahrscheinlichkeit für das Fehlen eines Wertes von einem anderen Merkmal abhängt aber nicht von der Ausprägung des fehlenden Merkmals selbst. MAR-Werte reduzieren die Fallzahl und verzerren möglicherweise das Studienergebnis.\n\nMNAR (eng. missing not at random): nicht zufällig fehlende Daten. Fehlende, nicht zufällige Daten sind ein schwerwiegenderes Problem, und in diesem Fall kann es ratsam sein, den Datenerhebungsprozess weiter zu überprüfen und zu versuchen zu verstehen, warum die Informationen fehlen. Wenn zum Beispiel die meisten Teilnehmer einer Umfrage eine bestimmte Frage nicht beantwortet haben, warum haben sie das getan? War die Frage unklar? Daher werden fehlende Werte als missing not at random bezeichnet, wenn die Wahrscheinlichkeit für das Fehlen eines Wertes von der Ausprägung des fehlenden Merkmals selbst abhängt. MNAR-Werte reduzieren die Fallzahl und verzerren das Studienergebnis. MNAR sind Non-ignorable missings und müssen auch berichtet werden.\n\nWie schon angemerkt. Die Struktur der fehlenden Werte lässt sich meist schwer vorhersagen bzw. bestimmen. Wir müssen eine Annahme treffen und diese dann auch in unseren statistischen Berichte oder Abschlussarbeit niederschreiben. Es gibt dann häufig auch Mischformen: MCAR, MAR, MNAR können ineinander verwoben sein. Häufig glauben wir daran, dass unsere Daten der MCAR genügen. Unter der Annahme, dass es sich bei den Daten um MCAR handelt, können auch zu viele fehlende Daten ein Problem darstellen. In der Regel liegt die sichere Obergrenze bei großen Datensätzen bei 5% der Gesamtmenge. Wenn die fehlenden Daten für ein bestimmtes Merkmal oder eine Stichprobe mehr als 5% betragen, sollten Sie dieses Merkmal oder diese Stichprobe wahrscheinlich weglassen. Wir prüfen daher im folgenden Abschnitten, ob in den Merkmalen (Spalten) und Stichproben (Zeilen) mehr als 5% der Daten fehlen. Auch hier gibt es dann Möglichkeiten erstmal die Daten zu visualiseren und dann zu schauen, welches Verfahren zur Imputation geeignet ist.\n\n\n\n\n\n\nSensitivitätsanalysen nach der Imputation von fehlenden Werten\n\n\n\nNachdem wir neue Daten bzw. Beobachtungen in unseren Daten erschaffen haben, ist es üblich noch eine Sensitivitätsanalysen durchzuführen. Wir Vergleich dann die Imputation mit der complete-case Analyse. Oder wir wollen die Frage beantworten, was hat eigentlich meine Imputation am Ergebnis geändert? Das machen wir dann gesammelt in dem Kapitel 39 zu den Sensitivitätsanalysen."
  },
  {
    "objectID": "stat-modeling-missing.html#univariat-vs.-multivariate-imputation",
    "href": "stat-modeling-missing.html#univariat-vs.-multivariate-imputation",
    "title": "38  Imputation fehlender Werte",
    "section": "\n38.2 Univariat vs. multivariate Imputation",
    "text": "38.2 Univariat vs. multivariate Imputation\nWas soll jetzt an dieser Stelle univariat und multivariat bedeuten? Wir haben uns die beiden Begriffe aufgehoben und nutzen diese Begriffe hier in dem Kontext der Imputation. Wir sprechen von einer univariaten Imputation, wenn wir nur eine Variable \\(x\\) imputieren. Das heist, wir ignorieren die Zusammehänge der Variable \\(x\\) zu irgendwelchen anderen Variablen in dem Datensatz. Das macht zum Beispiel für die Körpergröße in unserem Gummibärchendatensatz nicht so viel Sinn, denn wir haben ja Frauen und Männer befragt. Wir müssen die Körpergröße getrennt für die Variable Geschlecht imputieren. Wenn wir also Variablen mit Bezug zu anderen Variablen imputieren, dann nennen wir diese Verfahren multivariate Imputationsverfahren. In den folgenden Abschnitten werde ich einnmal die gängisten univariaten Verfahren vorstellen und zwei sehr gut funktionierende multivariate Verfahren."
  },
  {
    "objectID": "stat-modeling-missing.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-missing.html#genutzte-r-pakete-für-das-kapitel",
    "title": "38  Imputation fehlender Werte",
    "section": "\n38.3 Genutzte R Pakete für das Kapitel",
    "text": "38.3 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, see, readxl, \n               mice, naniar, missForest, missRanger,\n               dlookr, parameters, recipes)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-missing.html#daten",
    "href": "stat-modeling-missing.html#daten",
    "title": "38  Imputation fehlender Werte",
    "section": "\n38.4 Daten",
    "text": "38.4 Daten\nIn diesem Kapitel nutzen wir zwei Datensätze. Zum einen den echten Datensatz mit den Gummibärchen aus dem Kapitel 7 und dem Datensatz zu dem Infketionsstatus von Ferkeln aus dem Kapitel 8.1. Der Ferkeldatzensatz hat keine fehlenden Werte und deshlab müssen wir da noch einmal nachhelfen und künstlich fehlende Werte erschaffen. Schauen wir uns nochmal den Gummibärchendatensatz an und wählen nur die Spalten gender, age, height, semester, count_bears, count_color und most_liked. Die anderen Spalten haben keine fehlenden Werte bzw. wenn eine Farbe der Bärchen nicht in der Tüte vorkam, dann war keine drin. Das sind dann keine fehlenden Werte.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  %&gt;%\n  select(gender, age, height, semester, count_bears, count_color,\n         most_liked) %&gt;% \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked),\n         count_color = as_factor(count_color))\n\nZum anderen laden wir nochmal den Ferkeldatensatz mit unseren \\(n = 412\\) Ferkeln.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") \n\nWir haben haben aber in dem Ferkeldatensatz keine fehlenden Werte vorliegen. Deshalb nutzen wir die Funktion generateNA() aus dem R Paket missRanger. Wir können in der Funktion missRanger() angeben wieviel fehlende Werte global in dem Datensatz erschaffen werden sollen oder aber per Spalte. Wir erschaffen die fehlenden Werte bei der Spalte, denn die zehnte Spalte ist unser Outcome infected und da wollen wir ja keine fehlenden Werte imputieren. Fehlende Werte in der Outcomespalte bedeutet dann ja, dass die Beobachtung aus den Daten entfernt wird. Das brauchen wir aber hier nicht. Wie du sehen kannst erschaffen wir in jeder Spalte ein unterschiedliches Verhältnis von fehlenden Daten.\n\npig_miss_tbl &lt;- pig_tbl %&gt;% \n  generateNA(c(0.1, 0.15, 0.05, 0.3, 0.1, 0.05, 0.15, 0.01, 0.05, 0))\n\nNun haben wir zwei Datensätze vorliegen an denen wir einmal schauen können, wie wir fehlende Daten imputieren können. Ein fehlender Wert wird in R als NA (eng. not availabe) bezeichnet."
  },
  {
    "objectID": "stat-modeling-missing.html#visualisierung-von-fehlenden-werten",
    "href": "stat-modeling-missing.html#visualisierung-von-fehlenden-werten",
    "title": "38  Imputation fehlender Werte",
    "section": "\n38.5 Visualisierung von fehlenden Werten",
    "text": "38.5 Visualisierung von fehlenden Werten\nWie immer ist es sehr wichtig, sich die Struktur der fehlenden Daten einmal zu veranschaulichen. Wir nutzen dazu zwei drei Funktion aus zwei R Paketen. Zum einen die Funktion vis_miss() und gg_miss_upset() aus dem R Paket naniar sowie die Funktion plot_na_pareto() aus dem R Paket dlookr. Wir schauen uns mit den Funktionen einmal die Daten an und entscheiden, ob wir wirklich MCAR als Struktur der fehlenden Daten vorliegen haben.\n\n\nWie immer haben beide R Pakete noch eine Reihe von weiteren Funktionen zu bieten. Bei naniar sind es weitreichende Visualisierungen zu fehlenden Werten. Bei dlookr sind es eine ergiebige Auswahl an Funktionen zur Diagnose, Report und Explorative Datenanalyse.\nBetrachten wir zunächst die Gummibärchendaten in der Abbildung 38.1. Zuerst sehen wir in Abbildung 38.1 (a), dass sich einige Blöcke in der Variable Semester gebildet haben. Das kann auf einen nicht zufälliges fehlenden der Daten deuten. Auch scheint die Angabe von dem Lieblingsgeschmack in den älteren Beobachtungen besser eingetragen worden zu sein. Trotzdem können wir hier auf ein reltiv zuflliges Fehlen der Daten tippen. Wir nehmen mit etwas Bauchschmerzen hier MCAR an und machen dann mit der Imputation weiter. Du könntest auch überlegen, alle fehlenden Wert zu entfernen. Es scheint, dass Beobachtungen häufig über Geschlecht, Alter, Körpergröße und Semester fehlen. Dieser Zusammenhang wird dann in Abbildung 38.1 (b) nochmal deutlich. Wir haben viele NA’s im Semester. Häufig sind dann diese fehlenden Werte aber auch gleichseitig mit fehlenden Werten in der Variable Körpergröße, Alter und Geschlecht verknüpft. In Abbildung 38.1 (c) sehen wir nochmal die Anteile an den fehlenden Werten pro Spalte.\n\n\n\n\n\n(a) Anzahl der fehlenden Werte zu den vorhandenen Werten mit der Funktion vismiss()\n\n\n\n\n\n(b) Anzahl der fehlenden Werte zu den vorhandenen Werten und deren Überlappung mit anderen Variablen mit der Funktion gg_miss_upset()\n\n\n\n\n\n\n\n(c) Anzahl der fehlenden Wert in absoluter und relativer Angabe und deren kumulativer Verlauf. Die Funktion plot_na_pareto() gibt auch eine Wertung wieder.\n\n\n\nAbbildung 38.1— Visualisierung der fehlenden Werte im Gummibärchendatensatz. Alle drei Abbildungen sind etwas wiederholend, liefern aber ein gutes Gesamtbild.\n\n\nNachdem wir uns nun echte fehlende Werte in den Gummibärchendaten angesehen haben, wollen wir uns die zufällig erstellten fehlenden Daten in der Abbildung 38.2 einmal anschauen. In Abbildung 38.2 (a) sehen wir jetzt die zufällige Verteilung der fehlenden Werte nach der vorgegebenen Häufigkeit. Das passt dann auch gut zu der Abbildung 38.2 (c) in der wir die Anteile jeweils in absoluten und relativen Häufigkeiten sehen. Auch sind in Abbildung 38.2 (b) die Verbindungen der fehlenden Werte über die verschiedenen Variablen sehr zufällig. Wir haben kaum Blöcke von mehr als zwei Variablen, die gleichzeitig fehlen.\n\n\n\n\n\n(a) Anzahl der fehlenden Werte zu den vorhandenen Werten mit der Funktion vismiss()\n\n\n\n\n\n(b) Anzahl der fehlenden Werte zu den vorhandenen Werten und deren Überlappung mit anderen Variablen mit der Funktion gg_miss_upset()\n\n\n\n\n\n\n\n(c) Anzahl der fehlenden Wert in absoluter und relativer Angabe und deren kumulativer Verlauf. Die Funktion plot_na_pareto() gibt auch eine Wertung wieder.\n\n\n\nAbbildung 38.2— Visualisierung der fehlenden Werte im Ferkeldatensatz. Alle drei Abbildungen sind etwas wiederholend, liefern aber ein gutes Gesamtbild.\n\n\nNachdem wir uns beide Datensätze nochmal in der Visualisierung der fehlenden Werte angeschaut haben, stellen wir natürlich fest, dass der Gummibärchendatensatz weniger zufällig fehlende Werte hat als der künstlich erschaffene Datensatz zu den Ferkeln. Dennoch wollen wir mit beiden Datensätzen einmal weitermachen und schauen, wie wir jetzt die fehlenden Werte oder auch NA’s in R imputieren können."
  },
  {
    "objectID": "stat-modeling-missing.html#univariate-imputation",
    "href": "stat-modeling-missing.html#univariate-imputation",
    "title": "38  Imputation fehlender Werte",
    "section": "\n38.6 Univariate Imputation",
    "text": "38.6 Univariate Imputation\nFür die univariate Imputation von fehlenden Werten nutzen wir die Funktion impute_na() aus dem R Paket dlookr. Die univariate Variante der Imputation von fehlenden Werten ist eigentlich nur anzuraten, wenn wir eine Spalte vorliegen haben, wo fehlende Daten drin sind. Darüber hinaus haben die anderen Spalten keine Verbindung zu dieser Spalte. Dann mag es sinnvoll sein eine univariate Imputation zu nutzen. Ich selber nutze die univariate Imputation nur, wenn es schnell gehen soll und die Daten wenig Spalten haben. Passiert sehr selten.\n\n\nDas R Paket dlookr hat eine große Auswahl an simplen Imputationsalgorithmen.\nBei der univariaten Imputation müssen wir unterscheiden, welche Art die Spalte bzw. Variable ist, die wir imputieren wollen. Wenn die Spalte numerisch ist, daher ein double &lt;dbl&gt; oder integer &lt;int&gt; können wir folgende Optionen der Funktion impute_na() wählen. Wie immer hilft hier auch die Hilfeseite der Funkion ?impute_na() weiter.\n\n\nmean: Die Variable wird mit dem Mittelwert in der Variablenspalte imputiert. Daher werden alle fehlenden Werte mit dem Mittlwert der Variable ersetzt. Ja, das heist jetzt steht sehr häufig der Mittelwert in der Spalte.\n\nmedian: Die Variable wird mit dem Median in der Variablenspalte imputiert. Daher werden alle fehlenden Werte mit dem Median der Variable ersetzt. Ja, das heist jetzt auch hier, da steht sehr häufig der Median in der Spalte.\n\nmode: Dem Modus beziehungsweise den häufigsten Wert in der Variablenspalte können wir auch wählen um die fehlenden Werte zu ersetzen. Mit kontinuierlichen Werten ist diese Methoe nach dem Modus aber nicht anzuraten. Mit Kommastellen in der Variable gibt es schnell keinen oder nur einen Wert mit der absoluten Häufigkeit von zwei oder mehr.\n\nAnders sieht es aus, wenn die Spalte kategorisch ist, daher ein factor &lt;fct&gt; oder character &lt;chr&gt; können wir folgende Optionen der Funktion impute_na() wählen.\n\n\nmode: Wir imputieren mit dem Modus beziehungsweise den häufigsten Wert in der Variablenspalte und erstetzen damit jeden Wert mit dem häufigsten Wert in der Spalte.\n\nrpart: Wir können auch Recursive Partitioning and Regression Trees nutzen um eine kategorielle Variable zu imputieren, aber das geht hier dann zu weit. Siehe dazu dann auch das Kapitel 59.\n\nDamit haben wir alle Optionen einmal zur Hand. Damit sich das Kapitel nicht in die Unendlichkeit ausdehnt, wollen wir einmal die Funktion impute_na() an der Spalte age in dem Gummibärchendatensatz ausprobieren. Auch hier nutzen wir nur die mean- und median-Imputation. Du kannst dann gerne noch die anderen Optionen ausprobieren. Im Folgenden also der Code zusammen mit der Funktion mutate().\n\nimp_age_tbl &lt;- gummi_tbl %&gt;% \n  mutate(mean_age_imp = imputate_na(gummi_tbl, semester, method = \"mean\"),\n         median_age_imp = imputate_na(gummi_tbl, semester, method = \"median\"),\n         mode_age_imp = imputate_na(gummi_tbl, semester, method = \"mode\"))\n\nWir haben uns also das neue Objekt imp_age_tbl erschaffen in dem die beiden neuen imputierten Spalten drin sind. Wenn du dir die Spalten einmal in R anschaust, wirst du sehen, dass viele Zahlen gleich sind. Die Zahlen sind gleich, weil sie eben den Mittelwert oder den Median entsprechen. In Abbildung 38.3 siehst du nochmal den Vergleich von den Werten vor der Imputation (orginal) und nach der Imputation (imputation). Wenn du die Spalte in die Funktion plot() steckst erkennt die Funktion, dass es sich um importierte Werte handelt und plotted daher die Werte getrennt. Das funktioniert natürlich nur nach der Nutzung der Funktion impute_na().\n\nplot(imp_age_tbl$mean_age_imp)\nplot(imp_age_tbl$median_age_imp)\nplot(imp_age_tbl$mode_age_imp)\n\n\n\n\n\n(a) Imputation mit dem Mittelwert.\n\n\n\n\n\n(b) Imputation mit dem Median.\n\n\n\n\n\n(c) Imputation mit dem Modus.\n\n\n\nAbbildung 38.3— Densityplot der Verteilungen vor und nach der Imputation.\n\n\n\nWir wir in der Abbildung erkennen können, funktioniert die Methode ganz gut. Wir erhalten aber sehr viel mehr Werte auf die Schwerpunkte der Verteilung. Daher kriegen wir eine sehr viel stärkere bimodale Verteilung heraus als wir vorher hatten. Insbesondere der Modus zeigt hier eine sehr verzerrte Imputation. Ob eine bimodale Verteilung so beim Alter passt ist schwer zu sagen. Bei der Körpergröße wäre es richtiger. Daher ist eine univariate Imputation immer mit Vorsicht zu genießen."
  },
  {
    "objectID": "stat-modeling-missing.html#multivariate-imputation",
    "href": "stat-modeling-missing.html#multivariate-imputation",
    "title": "38  Imputation fehlender Werte",
    "section": "\n38.7 Multivariate Imputation",
    "text": "38.7 Multivariate Imputation\nIm Folgenden schauen wir uns zwei multivariate Verfahren an um fehlende Werte zu imputieren. In beiden Fällen entbindet uns, dass multivariat nicht davon nochmal zu schauen, ob unsere Daten einigermaßen konsistent imputiert wurden. Beide Verfahren haben ihre Vor und Nachteile.\n\nZum einen nutzen wir das R Paket mice in Kapitel 38.7.1. Wir müssen in mice für jede Spalte angeben, welcher Verteilung die Spalte folgt bzw. mit welche fortgeschrittenen Methode die Spalte imputiert werden soll. Die Imputation findet dann \\(m\\)-Mal über alle Variablen statt. Danach können wir dann die \\(m\\)-mal imputierten Datensätze weiter benutzen.\nWir nutzen als Alternative noch das R Paket missRanger() in Kapitel 38.7.2. Wir müssen in dem R Paket missRanger nicht angeben welcher Verteilung die Spalten folgen. Daher ist missRanger etwas einfacher zu bedienen, aber auf der anderen Seite auch mehr eine Blackbox. Wir stecken Daten rein und erhalten einen imputierte Daten wieder. Das mag vielleicht auch ein Vorteil sein.\n\nBeide Verfahren liefern uns dann die imputierten Datensätze wieder und wir müssen dann in den entsprechenden Visualisierungen schauen, ob wir so mit der Imputation zufrieden sind.\n\n38.7.1 Imputation mit mice\n\nBeginnen wir also mit der Imputation unter der Verwendung von dem R Paket mice. Die Funktion, die die Imputation durchführt heist ebenfalls mice() was für Multivariate Imputation by Chained Equations steht. Wir nutzen aber nur die Abkürzung mice(). Bei der Nutzung von mice durchlaufen wir mehrere Schritte. Zuerst müssen wir der Funktion mitteilen, welche Eigenschaften die zu imputierenden Spalten haben. Auch hier gilt, die Hilfeseite von ?mice() hilft bei der Entscheidung welche Variante für die jeweilige Spalte in den Daten passt. Wenn wir eine Spalte gar nicht imputieren wollen, dann lassen wir den Eintrag in dem benamten Vektor einfach leer. Im Folgenden der benamte Vektor mit den Variablennamen und wie die einzelnen Variablen dann imputiert werden sollen.\n\nimp_method &lt;- c(gender = \"logreg\", \n                age = \"pmm\", \n                height = \"pmm\", \n                semester = \"pmm\", \n                count_bears = \"\", \n                count_color = \"polyreg\", \n                most_liked = \"polyreg\")\n\nDen Vektor imp_method nutzen wir jetzt in der Funktion mice() für die Option method = imp_method. Nun weis mice() wie die Daten für jede Spalte über alle anderen Spalten imputiert werden soll. Wichtig ist noch anzugeben, wie viele \\(m\\) imputierte Datensätze erschaffen werden sollen. Wir nehmen hier mal \\(m = 5\\) und wiederholen den Prozess nur \\(maxit = 3\\) Mal. Je höher maxit ist, desto genauer wird mice() aber desto mehr Iterationen müssen gerechnet werden. Jede Iteration dauert auch so seine Zeit.\n\nimp_gummi_tbl &lt;- mice(data = gummi_tbl, m = 5, maxit = 3, \n                      method = imp_method)\n\n\n iter imp variable\n  1   1  gender  age  height  semester  most_liked\n  1   2  gender  age  height  semester  most_liked\n  1   3  gender  age  height  semester  most_liked\n  1   4  gender  age  height  semester  most_liked\n  1   5  gender  age  height  semester  most_liked\n  2   1  gender  age  height  semester  most_liked\n  2   2  gender  age  height  semester  most_liked\n  2   3  gender  age  height  semester  most_liked\n  2   4  gender  age  height  semester  most_liked\n  2   5  gender  age  height  semester  most_liked\n  3   1  gender  age  height  semester  most_liked\n  3   2  gender  age  height  semester  most_liked\n  3   3  gender  age  height  semester  most_liked\n  3   4  gender  age  height  semester  most_liked\n  3   5  gender  age  height  semester  most_liked\n\n\nSchauen wir jetzt einmal nach, ob auch die Imputation geklappt hat. In Abbildung 38.4 sehen wir nochmal die Daten visualisiert und sehen, dass es keinen fehlenden Werte mehr gibt. Die Überprüfung ist sinnvoll, da wir manchmal Spalten nicht imputieren wollen und dann müssen wir schauen, ob auch das so geklappt hat.\n\ncomplete(imp_gummi_tbl) %&gt;% vis_miss()\n\n\n\nAbbildung 38.4— Überprüfung der Imputation mit mice(). Wie erhofft gibt es keine fehlenden Werte mehr in den Daten.\n\n\n\nNachdem wir mit der Imputation durch sind können wir uns für die kontinuierlichen Variablen einmal die ursprüngliche Verteilung der Daten mit den fehlenden Weren im vergleich zu den \\(m=5\\) imputierten Verteilungen anschauen. Die Funktion densityplot() erlaubt hier eine einfache und schnelle Darstellung in Abbildung 38.5. Wir sehen, dass die Imputation nicht immer sehr gut geklappt hat, aber dadurch das wir die Imputation fünfmal gemacht haben, mittelt sich der Effekt einer einzelen Imputation wieder raus.\n\ndensityplot(imp_gummi_tbl)\n\n\n\nAbbildung 38.5— Densityplot der Verteilungen der ursprünglichen kontinuierlichen Daten im Vergleich zu den \\(m=5\\) imputierten Datensätzen.\n\n\n\nLeider wird es jetzt etwas schwerer mit den imputierten Daten zu arbeiten. Wir müssen ja jetzt die fünf imputierten Datensätze irgendwie analysieren. Die Analyse der fünf Datensätze wird getrennt gemacht und dann mit der Funktion pool() die Effektschätzer und \\(p\\)-Werte aller fünf Datensätze kombiniert. Ein weiteres leider ist, dass wir nicht für jedes Modell in R eine pool Funktion haben. Somit haben wir im Zweifel hier ein Problem, wenn es darum geht die Datensätze weiter zuverwenden. Die meisten glm()-Regressionen können aber so genutzt werden.\n\npooled_res &lt;- imp_gummi_tbl %&gt;%\n  mice::complete(\"all\") %&gt;%\n  map(lm, formula = height ~ age + semester + gender) %&gt;%\n  pool()\n\nWir nutzen dann die Funktion model_parameters() um uns die Ausgabe des Poolings besser anzeigen zu lassen. Wir sehen, dass sich das Ergebnis nicht sonderlich von den Ergbenissen einer normalen linearen Regression unterscheidet. Wir könnten dann mit dem gepoolten Modell auch weiter in einen Gruppenvergleich oder eine ANOVA gehen. Sobald wir durch ein Modell und der pool() Funktion ein Objekt haben, können wir mit dem Objekt weiterarbeiten.\n\npooled_res %&gt;% model_parameters()\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |           95% CI | Statistic |     df |      p\n---------------------------------------------------------------------------------\n(Intercept) |      182.93 | 1.33 | [180.32, 185.55] |    137.38 | 463.11 | &lt; .001\nage         |        0.03 | 0.05 | [ -0.07,   0.14] |      0.62 | 498.93 | 0.538 \nsemester    |       -0.14 | 0.13 | [ -0.41,   0.12] |     -1.06 | 119.26 | 0.292 \ngenderw     |      -15.10 | 0.72 | [-16.54, -13.65] |    -21.02 |  49.60 | &lt; .001\n\n\nLeider ist es so, dass wir nicht immer mit pool() arbeiten können, da wir für unsere Funktion, die wir nutzen wollen keine Anwendung in pool() finden. Salopp gesagt, wir erhalten einen Fehler, wenn wir das Modell oder die Funktion poolen wollen. In diesem Fall hilft die Funktion complete() mit der Option action = \"long\" und include = TRUE etwas weiter. Wir erhlaten damit die fünf imputierten Datensätze und den ursprünglichen Datensatz als Long-Format wiedergegeben. Damit können wir dann weiterarbeiten. Das ist aber dann das Thema für ein anderes Kapitel.\n\nimp_all_gummi_tbl &lt;- imp_gummi_tbl %&gt;% \n  complete(action = \"long\", include = TRUE) %&gt;% \n  select(-.id, imp_run = .imp) %&gt;% \n  mutate(imp_run = as_factor(imp_run)) %&gt;% \n  as_tibble()\n\nimp_all_gummi_tbl\n\n# A tibble: 3,582 × 8\n   imp_run gender   age height semester count_bears count_color most_liked\n   &lt;fct&gt;   &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;     \n 1 0       m         35    193       10           9 3           lightred  \n 2 0       w         21    159        6          10 5           yellow    \n 3 0       w         21    159        6           9 6           white     \n 4 0       w         36    180       10          10 5           white     \n 5 0       m         22    180        3          10 6           white     \n 6 0       m         NA     NA       NA          10 5           white     \n 7 0       m         22    180        3          10 5           green     \n 8 0       w         21    163        3          13 5           green     \n 9 0       m         22    170        3           9 5           green     \n10 0       m         23    176        3           9 5           white     \n# ℹ 3,572 more rows\n\n\nDie Spalte imp_run gibt uns dann die Imputation \\(m\\) wieder. Die \\(0\\)-te Imputation sind dabei die Orginaldaten. Wie du schon sehen kannst, wird das alles sehr schnell sehr groß und wir müssen hier mit fortgeschrittner Programmierung ran, wenn die Funktion pool() nicht will. In Abbildung 38.6 sehen wir die Anwendung des Objektes imp_all_gummi_tbl für die Visualisierung der beiden kategorialen Variablen gender und most_liked.\n\nggplot(imp_all_gummi_tbl, aes(gender, fill = imp_run)) +\n  theme_bw() +\n  geom_bar(position = position_dodge(preserve = \"single\")) +\n  scale_fill_okabeito()\n\nggplot(imp_all_gummi_tbl, aes(most_liked, fill = imp_run)) +\n  theme_bw() +\n  geom_bar(position = position_dodge(preserve = \"single\")) +\n  scale_fill_okabeito()\n\n\n\n\n\n(a) Imputation von gender.\n\n\n\n\n\n(b) Imputation von most_liked.\n\n\n\nAbbildung 38.6— Barplots der kategorialen Variablen getrennt nach den nicht-imputierten und fünf imputierten Datensätzen.\n\n\n\n\n38.7.2 Imputation mit missRanger\n\nIn diesem letzten Abschnitt wollen wir die Funktion missRanger() aus dem gleichnamigen R Paket missRanger nutzen um die fehlenden Werte in unseren Gummibärchendaten zu imputieren. Das Paket basiert auf sogenannten Decision tree, die wir im Kapitel 59 näher betrachten. Das tiefere Verständnis ist aber für die Anwendung nicht notwendig. Wir können die Funktionalität des R Pakets recht einfach nutzen.\nAls erstes brauchen wir den Datensatz und danach die Formel mit der Imputiert werden soll. Die Fomel ist ganz einfach aufgebaut. Links stehen die Variablen, die imputiert werden sollen und rechts stehen die Variablen, die zur Imputation verwendet werden sollen. Wenn wir einfach alles imputieren wollen und dafür alle Variablen nutzen wollen, dann schreiben wir einfach . ~ . auf. Also wir schreiben einen Punkt . links und rechts von der ~. Dann weis die Funktion, dass alles unter der zu Hilfenahme von allen Variablen imputiert werden soll. Wir müssen noch angeben, wie oft die Imputation laufen soll. Wir haben wir per default \\(500\\) Bäume oder Widerholungen angegeben. Wir wollen keine Ausgabe während der Funktion läuft und setzen deshalb verbose = 0.\n\ngummi_imputet_tbl &lt;- missRanger(\n  gummi_tbl, \n  formula = . ~ . ,\n  num.trees = 500, \n  verbose = 0)\n\nWie wir sehen, ist die Funktion sehr viel einfacher zu handhaben. Am Ende erhalten wir per default nur einen Datensatz von der Funktion zurück. Die Funktion missRanger() poolt für uns die Daten, so dass wir dann mit dem einen Datensatz weitermachen können. Das ist natürlich besonders sinnvoll, wenn wir im Anschluss an die Imputation eben keine Regression sondern etwa maschinelle Lernverfahren nutzen wollen.\nIn Abbildung 38.7 sehen wir nochmal die Überprüfung der nicht-imputierten und imputierten Daten. Anscheinend hat die Impiutation gut geklappt. Für den kontinuierlichen Fall liegen die imputierten Beobachtungen gut gestreut in den nicht-imputierten Daten. Auch für die kategoriale Variable gender passen die Verhältnisse. Wir können die Überprüfung jetzt für alle kontinuierlichen und alle kategorialen Variablen fortsetzen. Das müssen wir auch tun! Nur hier ist dann auch bald mal der Platz zu Ende, so dass wir es bei den beiden Abbildungen belassen.\n\nggplot()+\n  geom_point(data = gummi_imputet_tbl, aes(age, height), \n             color = \"red\")+\n  geom_point(data = gummi_tbl, aes(age, height))+\n  theme_bw() +\n  scale_color_okabeito()\n\nggplot()+\n  geom_bar(data = gummi_tbl, aes(gender), width = 0.3) +\n  geom_bar(data = gummi_imputet_tbl, aes(gender), fill = \"red\",\n           position = position_nudge(x = 0.25), width = 0.3) +\n  theme_bw() + \n  scale_color_okabeito()\n\n\n\n\n\n(a) Überprüfung zweier kontinuierliche Variablen age und height.\n\n\n\n\n\n(b) Überprüfung einer kategorialen Variable gender.\n\n\n\nAbbildung 38.7— Scatterplot und Barplot der imputierten Variablen getrennt nach nicht-imputierten und imputierten. Die roten Punkte stellen die imputierten Beobachtungen da.\n\n\n\nAbschlißend können wir auch nur Teile der Daten imputieren. Wenn wir nur die Spalten age und semester imputieren wollen, dann würden wir age + semester ~ . schreiben. Damit würden wir die beiden Spalten Alter und Semester dann durch die Informationen in all den anderen Spalten imputieren. Wir können die beiden Spalten auch nur durch spezifische andere Spalten imputieren lassen. Im folgenden Beispiel imputieren wir die Spalten age und semester durch die Informationen in den Spalten height und gender. Es dürfen natürlich auch gleiche Spalten auf beiden Seiten der Formel stehen.\n\ngummi_imputet_tbl &lt;- missRanger(\n  gummi_tbl, \n  formula = age + semester ~ height + gender ,\n  num.trees = 500,\n  verbose = 0)\n\nWas war jetzt besser? Das ist eine gute Frage. In einer parametrischen Regressionsanalyse bietet sich der Ablauf mit dem R Paket mice an. Wir haben in dem Fall der Regression Zugriff auf die Funktion pool() und können damit die Ergebnisse der \\(m\\) Imputationen zusammenfassen. Wenn wir das nicht können, also es keine Möglichkeit gibt unsere Methode der Wahl mit pool() zu nutzen, dann empfiehlt es sich das R Paket missRanger zu nutzen.\n\n38.7.3 Imputation mit recipes\n\nWenn wir später in dem Kapitel 54 zu maschinellen Lernverfahren etwas lernen, dann nutzen wir dafür tidymodels. Das R Paket tidymodels ist eine Zusammenfassung von mehreren wichtigen und sinnvollen R Paketen zur Klassifikation. Wir nutzen darüber hinaus das R Paket recipes um uns ein Rezept zu bauen, was wir dann nutzen. Du kannst mehr über den Aufbau von Rezepten in R dann im Kapitel 54 erfahren. Hier nur eine kurze Abhandlung dazu.\nUm die Rezepte in R nutzen zu können laden wir das Paket tidymodels. In dem Paket ist das R Paket recipes schon mit enthalten.\n\npacman::p_load(tidymodels)\n\nWir definieren nun unser Rezept nachdem wir imputieren wollen. Im Gegensatz zu missRanger müssen wir hier ein Outcome \\(y\\) angeben und auf der rechten Seite die Variablen, die mit in das Modell sollen. Das ist meistens auch kein Problem in der Klassifikation, da ja sehr häufig das Outcome \\(y\\) binär und bekannt ist. Wir nutzen also einfach unseren Datensatz zu den infizierten Ferkeln und bauen uns unser Rezept. Wir wollen alle anderen Variablen außer die Variable infected mit ins Modell nehmen. Deshalb schreiben wir rechts von der Tilde einfach nur einen . hin.\n\nrec &lt;- recipe(infected ~ ., data = pig_miss_tbl)\n\nNachdem wir unser Rezept haben, also wissen was das Outcome ist und was die Prädiktoren, können wir wir die Funktion step_impute_bag() nutzen um den Algorithmus für die Imputation zu spezifizieren. Es gibt noch zahlreiche andere Möglichkeiten die Variablen zu imputieren, aber wir haben ja wieder eine Mischung aus kontinuierlichen und kategorialen Variablen, so dass sich hier wieder ein Decision tree Algorithmus anbietet.\n\n\nDas R Paket recipes hat folgende Rezepte für die Imputation implementiert.\n\nimpute_rec &lt;- rec %&gt;% step_impute_bag(all_predictors())\n\nWir haben jetzt unseren Imputationsalgorithmus mit dem Rezept verbunden und können nun über die Funktionenprep() und bake() die Imputation durchführen. Im ersten Schritt prep() bereiten wir die Imputation vor. Im nächsten Schritt bake() führen wir dann die Impuation auf den Daten von pig_miss_tbl aus. Das mag jetzt etwas von hinten durch die Brust sein, aber da wir durch recipes in dem Kapitel 54 zu maschinellen Lernverfahren besser verschiedene Verfahren aneinander kleben können, sein es hier nochmal so gezeigt.\n\nimputed_tbl &lt;- prep(impute_rec, training = pig_miss_tbl) %&gt;% \n  bake(new_data = pig_miss_tbl)\n\nWir finden dann in dem Objekt imputed_tbl einen imputierten Datensatz ohne fehlende Werte wieder. Mit diesem Datensatz können wir dann weiterarbeiten.\n\n\n\n\nDie Vingette zu recipes zeigt die Imputation mit k-NN an einem etwas komplexeren Beispiel."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#theoretischer-hintergrund",
    "href": "stat-modeling-sensitivity.html#theoretischer-hintergrund",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.1 Theoretischer Hintergrund",
    "text": "39.1 Theoretischer Hintergrund\nWir brauchen die Sensitivitätsanalyse wenn wir Beobachtungen aus unseren Daten entfernt oder aber hinzugefügt haben. Das heißt du hast entweder eine Variablenselektion wie im Kapitel 36 beschrieben durchgeführt. Oder aber du hast fehlende Werte wie in Kapitel 38 beschrieben imputiert. Es kann auch sein, dass du Ausreißer aus den Daten entfernt oder aber imputiert hast, wie es in Kapitel 37 beschrieben ist. Im Prinzip kannst du auch alles drei gemacht haben, aber meistens beschränkt sich die Veränderung der Daten nur auf eins der drei Möglichkeiten.\nWie immer brauchen wir natürlich auch Fallzahl. Eine Sensitivitätsanalyse kannst du nicht auf zwanzig bis fünfzig Beobachtungen machen. Du brauchst schon eine gute dreistellige Anzahl, damit du hier sauber Modellieren und Darstellen kannst. Wenn du weniger Beobachtungen hast, dann ist ganz natürlich das einzelne Werte einen riesigen Einfluss haben müssen. Im Zweifel frag einfach einmal bei mir nach, dann können wir die Sachlage diskutieren.\n\n\n\n\n\n\nDas ist hier natürlich eine Sensitivitätsanalyse für Arme. Wie man es richtig umfangreich macht, findest du in einem sehr gutem und umfangreichen Tutorial zu What Makes a Sensitivity Analysis?\nDieses Kapitel ist relativ übersichtlich. Wir werden die Modelle nach der jeweiligen algorithmischen Veränderung uns nochmal anschauen und dann deskriptive entscheiden, ob wir eine große Veränderung in den Daten sehen. Es gibt zwar auch die Möglichkeit die Modelle untereinander zu vergleichen, aber ist hier die Aussagekraft nicht so stark. Die Idee hinter dem Modellvergleich ist eher die Anzahl an Spalten zu verändern und nicht die Werte in der Datenmatrix. Deshalb machen wir es zwar, genießen die Sache aber mit Vorsicht."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-sensitivity.html#genutzte-r-pakete-für-das-kapitel",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.2 Genutzte R Pakete für das Kapitel",
    "text": "39.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, dlookr, broom, modelsummary,\n               see, performance, ggpubr, factoextra, FactoMineR,\n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#daten",
    "href": "stat-modeling-sensitivity.html#daten",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.3 Daten",
    "text": "39.3 Daten\nIn diesem Beispiel betrachten wir wieder die Gummibärchendaten. Auch hier haben wir echte Daten vorliegen, so dass wir Ausreißer entdecken könnten. Da wir hier auch fehlende Werte in den Daten haben, können wir diese fehlenden Werte auch einfach imputieren und uns dann die Effekte anschauen. Das heißt wir haben also einen idealen Datensatz für unsere Sensitivitätsanalysen.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\")  %&gt;%\n  select(gender, age, height, semester) %&gt;% \n  mutate(gender = as_factor(gender)) \n\nIn der Tabelle 37.2 ist der Datensatz gummi_tbl nochmal für die ersten sieben Zeilen dargestellt. Wir werden später sehen, wie sich die Fallzahl von \\(n = 597\\) immer wieder ändert, je nachdem wie wir mit den fehlenden Daten und den Variablen umgehen.\n\n\n\n\nTabelle 39.1— Auszug aus dem Datensatz gummi_tbl. Wir betrachten die ersten sieben Zeilen des Datensatzes.\n\ngender\nage\nheight\nsemester\n\n\n\nm\n35\n193\n10\n\n\nw\n21\n159\n6\n\n\nw\n21\n159\n6\n\n\nw\n36\n180\n10\n\n\nm\n22\n180\n3\n\n\nm\nNA\nNA\nNA\n\n\nm\n22\n180\n3"
  },
  {
    "objectID": "stat-modeling-sensitivity.html#das-modell",
    "href": "stat-modeling-sensitivity.html#das-modell",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.4 Das Modell",
    "text": "39.4 Das Modell\nWir wollen jetzt als erstes das volle Modell schätzen. Das heißt wir packen alle Variablen in das Modell und rechnen dann die lineare Regression. Wir wollen herausfinden in wie weit das Alter, das Geschlecht und das Semester einen Einfluss auf die Körpergröße von Studierenden hat.\n\\[\nheight \\sim gender + age + semester\n\\]\nWir haben nichts an den Daten geändert und somit dient unser volles Modell als Benchmark für die anderen. Wenn sich einige Werte der Modellgüten im Vergleich zum vollen Modell ändern, dann wissen wir, dass etwas nicht stimmt.\n\nfit_full &lt;- lm(height ~ gender + age + semester, data = gummi_tbl)\n\nNeben dem vollen Modell rechnen wir auch noch das Nullmodel. Das Nullmodell beinhaltet nur den Intercept und sonst keine Einflussvariable. Wir wollen schauen, ob es überhaupt was bringt eine unserer Variablen in das Modell zu nehmen oder ob wir es auch gleich lassen können. Im Prinzip unsere Kontrolle für das Modellieren.\n\\[\nheight \\sim 1\n\\]\nIn R fitten wir das Nullmodell in dem wir keine Variablen mit in das Modell nehmen sondern nur eine 1 schreiben. Wir haben dann nur den Intercept mit in dem Modell und sonst nichts. Was wir schon aus den anderen Kapiteln wissen ist, dass das Nullmodell ein schlechtes Modell sein wird.\n\nfit_null &lt;- lm(height ~ 1, data = gummi_tbl)\n\nWir schauen uns die Modelle hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-detektion-von-ausreißer",
    "href": "stat-modeling-sensitivity.html#nach-der-detektion-von-ausreißer",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.5 Nach der Detektion von Ausreißer",
    "text": "39.5 Nach der Detektion von Ausreißer\nTeilweise können wir eine Überprüfung auf Ausreißer nur auf einen Datensatz ohne fehlende Werte durchführen. Hier beißt sich dann die Katze in den Schwanz. Deshalb nutzen wir die Funktion diagnose_outlier(), die intern die fehlenden Werte entfernt. Das ist natürlich kein richtiges Vorgehen! Aber wir nutzen ja diesen Abschnitt nur als Beispiel.\nDu findest die Detektion von Ausreißern im Kapitel 37 beschrieben.\n\ndiagnose_outlier(gummi_tbl) \n\n# A tibble: 3 × 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean without_mean\n  &lt;chr&gt;            &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 age                 39           6.53         35.8      23.6         22.6 \n2 height               0           0           NaN       176.         176.  \n3 semester            49           8.21          8.88      2.76         2.09\n\n\nWir sehen, dass wir in der Variable age und semester nach der Funktion zu urteilen Ausreißer gefunden haben. Deshalb werden wir jetzt diese Ausreißer durch die Funktion imputate_outlier() entsprechend ersetzen. Mal schauen, ob wir damit eine substanzielle Änderung in der Modellierung erhalten.\n\ngummi_out_imp_tbl &lt;- gummi_tbl %&gt;% \n  mutate(age = imputate_outlier(., age, method = \"capping\"),\n         semester = imputate_outlier(., semester, method = \"capping\"))\n\nNun modellieren wir noch mit unseren ersetzten und angepassten Daten die Körpergröße und erhalten den Modellfit zurück. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.\n\nfit_outlier &lt;- lm(height ~ gender + age + semester, data = gummi_out_imp_tbl)\n\nWir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-imputation-von-fehlenden-werten",
    "href": "stat-modeling-sensitivity.html#nach-der-imputation-von-fehlenden-werten",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.6 Nach der Imputation von fehlenden Werten",
    "text": "39.6 Nach der Imputation von fehlenden Werten\nNehmen wir wieder den Gummibärechendatensatz von neuen und imputieren diesmal die fehlenden Werte mit einer univariaten Imputation. Wir machen uns hier nicht die Mühe ein multivariates Verfahren zu nutzen. Das könnte man tun, aber wir wollen hier ja nur den Weg aufzeigen, wie wir den Vergleich der Modelle zur Sensitivitätsanalyse durchführen.\nDu findest die Imputation von fehlenden Werten im Kapitel 38 beschrieben.\nIn unserem Fall imputieren wir alle numerischen Variablen mit dem Mittelwert und die kategoriale Variable mit der Methode rpart. Damit haben wir dann keine fehlenden Werte mehr in den Daten und somit sollte das jetzt auch unserer größter Datensatz für die lineare Regression sein. Nicht vergessen, sobald wir einen fehlenden Wert bei einer Variable in einem Modell haben, fällt die ganze Beobachtung aus dem Modell heraus.\n\ngummi_imp_tbl &lt;- gummi_tbl %&gt;% \n  mutate(age = imputate_na(., age, method = \"mean\"),\n         gender = imputate_na(., gender, method = \"rpart\"),\n         height = imputate_na(., height, method = \"median\"),\n         semester = imputate_na(., semester, method = \"mode\"))\n\nDann rechnen wir noch schnell das Modell für die imputierten Daten. Am Ende des Kapitels werden wir dann alle Modelle gegenüberstellen und miteinander vergleichen.\n\nfit_imp &lt;- lm(height ~ gender + age + semester, data = gummi_imp_tbl)\n\nWir schauen uns das Modell hier nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#nach-der-variablen-selektion",
    "href": "stat-modeling-sensitivity.html#nach-der-variablen-selektion",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.7 Nach der Variablen Selektion",
    "text": "39.7 Nach der Variablen Selektion\nFür die Variablensleketion machen wir es uns sehr einfach. Wir müssen ja nur eine Spalte aus den Daten werfen, mehr ist ja Variablenselektion auch nicht. Wir machen dort nur eine algorithmengetriebene Auswahl. In diesem Fall entscheide ich einfach zufällig welche Variable aus dem Modell muss.\nDu findest die Variablen Selektion im Kapitel 36 beschrieben.\nSomit nehmen wir an, wir hätten eine Variablenselektion durchgeführt und die Variable semester aus dem Modell entfernt.\n\nfit_var_select &lt;- lm(height ~ gender + age, data = gummi_tbl)\n\nAuch dieses Modell schauen wir nicht weiter an, da es uns nur im Vergleich zu den anderen Modellen interessiert."
  },
  {
    "objectID": "stat-modeling-sensitivity.html#modellvergleich",
    "href": "stat-modeling-sensitivity.html#modellvergleich",
    "title": "39  Sensitivitätsanalyse",
    "section": "\n39.8 Modellvergleich",
    "text": "39.8 Modellvergleich\nKommen wir zu dem eigentlichen Modellvergleich. In Tabelle 39.2 sehen wir den Modellvergleich aller fünf Modelle aus diesem Kapitel. Dazu nutzen wir die Funktion modelsummary() aus dem R Paket modelsummary. Wir vergleichen die Modelle untereinander aber vor allem mit dem vollen Modell. Das volle Modell basiert ja auf den ursprünglichen nicht veränderten Daten. Den Intercept können wir erstmal ignorieren. Spannend ist, dass sich der Effekt von gender auf die Körpergröße durch die Imputation um eine Einheit ändert. Der Effekt des Alters verfünffacht sich durch die Outlieranpassung und verdoppelt sich durch die Imputation. Durch die Imputation wird der Effekt des Semesters abgeschwächt.\nWenn wir auf das \\(R^2_{adj}\\) schauen, dann haben wir eine Verschlechterung durch die Imputation. Sonst bleibt der Wert mehr oder minder konstant. Das ist ein gutes Zeichen, dass wir unser Modell nicht vollkommen an die Wand gefahren haben durch unsere Änderung der Daten. Das \\(AIC\\) wird folglich für die Imputationsdaten sehr viel schlechter und nähert sich dem Nullmodell an. Das ist wirklcih kein gutes Zeichen für die Imputation. Da haben wir mehr kaputt als heile gemacht. Wir sehen keinen Efdekt bei dem Fehler \\(RMSE\\), der noch nach dem Fit des Modell übrig bleibt. Aber das kann passieren. Nicht jede Maßzahl muss sich auch ändern. Deshalb haben wir ja mehrere Maßzahlen vorliegen.\n\nmodelsummary(lst(\"Null Modell\" = fit_null,\n                 \"Volles Modell\" = fit_full,\n                 \"Outlier\" = fit_outlier,\n                 \"Imputation\" = fit_imp,\n                 \"Variablen Selektion\" = fit_var_select),\n             estimate  = \"{estimate}\",\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"))\n\n\n\nTabelle 39.2— Modellvergleich mit den fünf Modellen. Wir schauen in wie weit sich die Koeffizienten und Modelgüten für die einzelnen Modelle im direkten Vergleich zum vollen Modell verändert haben.\n\n\nNull Modell\n Volles Modell\nOutlier\n Imputation\n Variablen Selektion\n\n\n\n(Intercept)\n175.807\n182.308\n184.502\n180.517\n182.604\n\n\n\n[174.894, 176.720]\n[179.204, 185.412]\n[180.362, 188.643]\n[177.881, 183.153]\n[179.972, 185.236]\n\n\n\ns.e. = 0.465\ns.e. = 1.580\ns.e. = 2.107\ns.e. = 1.342\ns.e. = 1.339\n\n\n\nt = 378.170\nt = 115.394\nt = 87.558\nt = 134.475\nt = 136.325\n\n\n\np =\np =\np =\np =\np =\n\n\ngenderw\n\n−15.273\n−15.209\n−13.160\n−15.184\n\n\n\n\n[−16.610, −13.937]\n[−16.545, −13.873]\n[−14.361, −11.959]\n[−16.475, −13.893]\n\n\n\n\ns.e. = 0.680\ns.e. = 0.680\ns.e. = 0.612\ns.e. = 0.657\n\n\n\n\nt = −22.456\nt = −22.365\nt = −21.518\nt = −23.110\n\n\n\n\np =\np =\np =\np =\n\n\nage\n\n0.064\n−0.035\n0.025\n0.035\n\n\n\n\n[−0.065, 0.193]\n[−0.214, 0.145]\n[−0.080, 0.131]\n[−0.071, 0.141]\n\n\n\n\ns.e. = 0.066\ns.e. = 0.091\ns.e. = 0.054\ns.e. = 0.054\n\n\n\n\nt = 0.978\nt = −0.380\nt = 0.471\nt = 0.651\n\n\n\n\np = 0.329\np = 0.704\np = 0.638\np = 0.515\n\n\nsemester\n\n−0.121\n−0.104\n0.138\n\n\n\n\n\n[−0.387, 0.146]\n[−0.407, 0.200]\n[−0.111, 0.387]\n\n\n\n\n\ns.e. = 0.136\ns.e. = 0.154\ns.e. = 0.127\n\n\n\n\n\nt = −0.888\nt = −0.671\nt = 1.088\n\n\n\n\n\np = 0.375\np = 0.502\np = 0.277\n\n\n\nNum.Obs.\n518\n488\n488\n597\n513\n\n\nR2\n0.000\n0.511\n0.510\n0.439\n0.512\n\n\nR2 Adj.\n0.000\n0.508\n0.507\n0.436\n0.510\n\n\nAIC\n3917.0\n3354.6\n3355.5\n4090.3\n3518.5\n\n\nBIC\n3925.5\n3375.6\n3376.5\n4112.3\n3535.5\n\n\nLog.Lik.\n−1956.488\n−1672.321\n−1672.760\n−2040.172\n−1755.265\n\n\nF\n\n168.345\n167.753\n154.699\n\n\n\nRMSE\n10.57\n7.45\n7.45\n7.38\n7.41\n\n\n\n\n\n\n\n\nDas vergleichen von Modellen, die auf unterschiedlichen Daten basieren ist nicht anzuraten. Wir erhalten auch die passende Warnung von der Funktion compare_performance() aus dem R Paket performance. Dennoch hier einmal der Vergleich. Wir sehen, dass die Modelle mit der Ersetzung der Ausreißer und das volle Modell sich stark ähneln. Das selektierte Modell und das imputierte Modell fallen dagegen ab. Da wir ja hier nicht zeigen wollen, dass sich die Modelle unterscheiden, ist das Ergebnis ähnlich zu der Übersicht. Die Imputation hat so nicht funktioniert.\n\n\n# Comparison of Model Performance Indices\n\nName           | Model |    R2 | R2 (adj.) |   RMSE |  Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n---------------------------------------------------------------------------------------------------------------------------\nfit_full       |    lm | 0.511 |     0.508 |  7.448 |  7.479 |       0.608 |        0.608 |       0.608 |            99.25%\nfit_outlier    |    lm | 0.510 |     0.507 |  7.455 |  7.485 |       0.392 |        0.392 |       0.392 |            83.91%\nfit_var_select |    lm | 0.512 |     0.510 |  7.409 |  7.430 |    1.57e-36 |     1.61e-36 |    1.15e-35 |            56.88%\nfit_imp        |    lm | 0.439 |     0.436 |  7.377 |  7.402 |   1.07e-160 |    1.08e-160 |   6.45e-161 |            53.05%\nfit_null       |    lm | 0.000 |     0.000 | 10.570 | 10.581 |   4.73e-123 |    4.97e-123 |   2.39e-120 |        5.64e-119%\n\n\nWas ist das Fazit aus der Sensitivitätsanalyse für Arme? Nun wir konnten einmal sehen, dass wir auch mit einfachen Werkzeugen Modelle deskriptiv miteinander vergleichen können und dann einen Schluss über die Güte der Detektion von Ausreißern, der Imputation von fehlenden Werten oder aber der Variablenselektion treffen können. Denk immer dran, die Sensitivitätsanalyse findet nach einer sauberen Detektion, Imputation oder Selektion statt und soll nochmal sicherstellen, dass wir nicht künstliche Effekte der Algorithmen modellieren sondern die Effekte in den Daten sehen.\nSensitivitätsanalysen finden eigentlich in dem Kontext von klinischen Studien statt. Der Trend geht aber natürlich auch nicht an den Agrarwissenschaften vorbei und solltest du den Begriff mal hören, weist du wo Sensitivitätsanalyse hingehören."
  },
  {
    "objectID": "stat-modeling-gaussian.html#annahmen-an-die-daten",
    "href": "stat-modeling-gaussian.html#annahmen-an-die-daten",
    "title": "40  Gaussian Regression",
    "section": "\n40.1 Annahmen an die Daten",
    "text": "40.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Normalverteilung.\nIm folgenden Kapitel zu der multiplen gaussian linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 37 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren ist unser \\(y\\) normalverteilt. Das ist hier sehr wichtig, denn wir wollen ja eine multiple gaussian lineare Regression rechnen.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen."
  },
  {
    "objectID": "stat-modeling-gaussian.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-gaussian.html#genutzte-r-pakete-für-das-kapitel",
    "title": "40  Gaussian Regression",
    "section": "\n40.2 Genutzte R Pakete für das Kapitel",
    "text": "40.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, performance, scales, parameters,\n               olsrr, readxl, car, gtsummary)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-gaussian.html#daten",
    "href": "stat-modeling-gaussian.html#daten",
    "title": "40  Gaussian Regression",
    "section": "\n40.3 Daten",
    "text": "40.3 Daten\nIm Folgenden schauen wir uns die Daten eines Pilotprojektes zum Anbau von Kichererbsen in Brandenburg an. Wir haben an verschiedenen anonymisierten Bauernhöfen Kichererbsen angebaut und das Trockengewicht als Endpunkt bestimmt. Darüber hinaus haben wir noch andere Umweltparameter erhoben und wollen schauen, welche dieser Parameter einen Einfluss auf das Trockengewicht hat. In Kapitel 8.3 findest du nochmal mehr Informationen zu den Daten.\n\nchickpea_tbl &lt;- read_excel(\"data/chickpeas.xlsx\") \n\nIn der Tabelle 40.1 ist der Datensatz chickenpea_tbl nochmal als Ausschnitt dargestellt. Insgesamt haben wir \\(n = 95\\) Messungen durchgeführt. Wir sehen, dass wir verschiedene Variablen gemessen haben. Unter anderem, ob es geregent hat oder an welcher Stelle in Brandenburg die Messungen stattgefunden haben. Ebenso haben wir geschaut, ob ein Wald in der Nähe der Messung war oder nicht. Wir nehmen als Outcome \\(y\\) das normalverteilte Trockengewicht dryweight.\n\n\n\n\nTabelle 40.1— Auszug aus dem Daten zu den Kichererbsen in Brandenburg.\n\n\n\n\n\n\n\n\n\n\n\ntemp\nrained\nlocation\nno3\nfe\nsand\nforest\ndryweight\n\n\n\n25.26\nhigh\nnorth\n5.56\n4.43\n63\n&gt;1000m\n253.42\n\n\n21.4\nhigh\nnortheast\n9.15\n2.58\n51.17\n&lt;1000m\n213.88\n\n\n27.84\nhigh\nnortheast\n5.57\n2.19\n55.57\n&gt;1000m\n230.71\n\n\n24.59\nlow\nnorth\n7.97\n1.47\n62.49\n&gt;1000m\n257.74\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n25.47\nlow\nnorth\n6.92\n3.18\n64.55\n&lt;1000m\n268.58\n\n\n29.04\nlow\nnorth\n5.64\n2.87\n53.27\n&gt;1000m\n236.07\n\n\n24.11\nhigh\nnortheast\n4.31\n3.66\n63\n&lt;1000m\n259.82\n\n\n28.88\nlow\nnortheast\n7.92\n2\n65.75\n&gt;1000m\n274.75\n\n\n\n\n\n\nIm Folgenden werden wir die Daten nur für das Fitten eines Modells verwenden. In den anderen oben genannten Kapiteln nutzen wir die Daten dann anders."
  },
  {
    "objectID": "stat-modeling-gaussian.html#fit-des-modells",
    "href": "stat-modeling-gaussian.html#fit-des-modells",
    "title": "40  Gaussian Regression",
    "section": "\n40.4 Fit des Modells",
    "text": "40.4 Fit des Modells\nWir rechnen jetzt den Fit für das vollständige Modell mit allen Variablen in dem Datensatz. Wir sortieren dafür einmal das \\(y\\) mit dryweight auf die linke Seite und dann die anderen Variablen auf die rechte Seite des ~. Wir haben damit unser Modell chickenpea_fit wie folgt vorliegen.\n\nchickenpea_fit &lt;- lm(dryweight ~ temp + rained + location + no3 + fe + sand + forest, \n                   data = chickpea_tbl)\n\nSoweit so gut. Wir können uns zwar das Modell mit der Funktion summary() anschauen, aber es gibt schönere Funktionen, die uns erlauben einmal die Performance des Modells abzuschätzen. Also zu klären, ob soweit alles geklappt hat und wir mit dem Modell weitermachen können."
  },
  {
    "objectID": "stat-modeling-gaussian.html#performance-des-modells",
    "href": "stat-modeling-gaussian.html#performance-des-modells",
    "title": "40  Gaussian Regression",
    "section": "\n40.5 Performance des Modells",
    "text": "40.5 Performance des Modells\nDa ich die Daten selber gebaut habe, ist mir bekannt, dass das Outcome dryweight normalverteilt ist. Immerhin habe ich die Daten aus einer Normalverteilung gezogen. Manchmal will man dann doch Testen, ob das Outcome \\(y\\) einer Normalverteilung folgt. Das R Paket oslrr bietet hier eine Funktion ols_test_normality(), die es erlaubt mit allen bekannten statistischen Tests auf Normalverteilung zu testen. Wenn der \\(p\\)-Wert kleiner ist als das Signifikanzniveau \\(\\alpha\\), dann können wir die Nullhypothese, dass unsere Daten gleich einer Normalverteilung wären, ablehnen.\n\n\nDas R Paket olsrr erlaubt eine weitreichende Diagnostik auf einem normalverteilten Outcome \\(y\\).\n\nols_test_normality(chickenpea_fit) \n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.9809         0.1806 \nKolmogorov-Smirnov        0.088          0.4287 \nCramer-von Mises          8.2878         0.0000 \nAnderson-Darling          0.5182         0.1837 \n-----------------------------------------------\n\n\nWir sehen, testen wir viel, dann kommt immer was signifikantes raus. Um jetzt kurz einen statistischen Engel anzufahren, wir nutzen wenn überhaupt den Shapiro-Wilk-Test oder den Kolmogorov-Smirnov-Test. Für die anderen beiden steigen wir jetzt hier nicht in die Therorie ab.\nNachdem wir die Normalverteilung nochmal überprüft haben wenden wir uns nun dem Wichtigen zu. Wir schauen jetzt auf die Varianz des Modells. Um zu überprüfen, ob das Modell funktioniert können wir uns den Anteil der erklärten Varianz anschauen. Wieviel erklären unsere \\(x\\) von der Varianz des Outcomes \\(y\\)? Wir betrachten dafür das Bestimmtheitsmaß \\(R^2\\). Da wir mehr als ein \\(x\\) vorliegen haben, nutzen wir das adjustierte \\(R^2\\). Das \\(R^2\\) hat die Eigenschaft immer größer und damit besser zu werden je mehr Variablen in das Modell genommen werden. Wir können dagegen Adjustieren und daher das \\(R^2_{adj}\\) nehmen.\n\nr2(chickenpea_fit)\n\n# R2 for Linear Regression\n       R2: 0.903\n  adj. R2: 0.894\n\n\nWir erhalten ein \\(R^2_{adj}\\) von \\(0.87\\) und damit erklärt unser Modell ca 87% der Varianz von \\(y\\) also unserem Trockengewicht. Das ist ein sehr gutes Modell. Je nach Anwendung sind 60% bis 70% erklärte Varianz schon sehr viel.\nIm nächsten Schritt wollen wir nochmal überprüfen, ob die Varianzen der Residuen auch homogen sind. Das ist eine weitere Annahme an ein gutes Modell. Im Prinzip überprüfen wir hier, ob unser Ourtcome auch wirklcih normalveteilt ist bzw. der Annahme der Normalverteilung genügt. Wir nutzen dafür die Funktion check_heteroscedasticity() aus dem R Paket performance.\n\ncheck_heteroscedasticity(chickenpea_fit)\n\nOK: Error variance appears to be homoscedastic (p = 0.512).\n\n\nAuch können wir uns einmal numerisch die VIF-Werte anschauen um zu sehen, ob Variablen mit anderen Variablen ungünstig stark korrelieren. Wir wollen ja nur die Korrelation des Modells, also die \\(x\\), mit dem Outcome \\(y\\) modellieren. Untereinander sollen die Variablen \\(x\\) alle unabhängig sein. Für können uns die VIF-Werte für alle kontinuierlichen Variablen berechnen lassen.\n\nvif(chickenpea_fit)\n\n             GVIF Df GVIF^(1/(2*Df))\ntemp     1.067042  1        1.032977\nrained   1.067845  1        1.033366\nlocation 1.119450  2        1.028611\nno3      1.062076  1        1.030571\nfe       1.135119  1        1.065420\nsand     1.072182  1        1.035462\nforest   1.070645  1        1.034720\n\n\nAlle VIF-Werte sind unter dem Threshold von 5 und damit haben wir hier keine Auffälligkeiten vorliegen.\nDamit haben wir auch überprüft, ob unsere Varianzen homogen sind. Also unsere Residuen annhährend normalverteilt sind. Da unsere Daten groß genig sind, können wir das auch ohne weiteres Anwenden. Wenn wir einen kleineren Datensatz hätten, dann wäre die Überprüfung schon fraglicher. bei kleinen Fallzahlen funktioniert der Test auf Varianzheterogenität nicht mehr so zuverlässig.\nIn Abbildung 40.1 sehen wir nochmal die Visualisierung verschiedener Modellgütekriterien. Wir sehen, dass unsere beobachte Verteiung des Trockengewichts mit der vorhergesagten übereinstimmt. Ebenso ist der Residualplot gleichmäßig und ohne Struktur. Wir haben auch keine Ausreißer, da alle unsere Beobachtungen in dem gestrichelten, blauen Trichter bleiben. Ebenso zeigt der QQ-Plot auch eine approximative Normalverteilung der Residuen. Wir haben zwar leichte Abweichungen, aber die sind nicht so schlimm. Der Großteil der Punkte liegt auf der Diagonalen. Ebenso gibt es auch keine Variable, die einen hohen VIF-Wert hat und somit ungünstig mit anderen Variablen korreliert.\n\ncheck_model(chickenpea_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\", \"pp_check\", \"homogeneity\", \"vif\")) \n\n\n\nAbbildung 40.1— Ausgabe ausgewählter Modelgüteplots der Funktion check_model()."
  },
  {
    "objectID": "stat-modeling-gaussian.html#interpretation-des-modells",
    "href": "stat-modeling-gaussian.html#interpretation-des-modells",
    "title": "40  Gaussian Regression",
    "section": "\n40.6 Interpretation des Modells",
    "text": "40.6 Interpretation des Modells\nNachdem wir nun sicher sind, dass das Modell unseren statistischen Ansprüchen genügt, können wir jetzt die Ergebnisse des Fits des Modells einmal interpretieren. Wir erhalten die Modellparameter über die Funktion model_parameters() aus dem R Paket parameters.\n\nchickenpea_fit %&gt;% \n  model_parameters()\n\nParameter            | Coefficient |   SE |          95% CI | t(86) |      p\n----------------------------------------------------------------------------\n(Intercept)          |       -5.11 | 9.74 | [-24.48, 14.26] | -0.52 | 0.601 \ntemp                 |        2.37 | 0.22 | [  1.93,  2.80] | 10.80 | &lt; .001\nrained [low]         |        1.78 | 1.19 | [ -0.60,  4.15] |  1.49 | 0.140 \nlocation [northeast] |       -0.10 | 1.20 | [ -2.50,  2.29] | -0.09 | 0.931 \nlocation [west]      |        1.96 | 1.63 | [ -1.29,  5.20] |  1.20 | 0.234 \nno3                  |        1.49 | 0.41 | [  0.68,  2.29] |  3.67 | &lt; .001\nfe                   |        0.66 | 0.53 | [ -0.40,  1.72] |  1.24 | 0.220 \nsand                 |        3.04 | 0.12 | [  2.80,  3.27] | 25.63 | &lt; .001\nforest [&gt;1000m]      |       -3.02 | 1.14 | [ -5.29, -0.75] | -2.64 | 0.010 \n\n\nSchauen wir uns die einzelnen Zeilen aus der Ausgabe einmal in Ruhe an. Wir sind eigentlich nur an den Spalten Coefficient für das \\(\\beta\\) als Effekt der Variablen sowie der Spalte p als \\(p\\)-Wert für die Variablen interessiert. Wir testen immer als Nullhypothese, ob sich der Parameter von 0 unterscheidet.\n\n\n(Intercept) beschreibt den den \\(y\\)-Achsenabschnitt. Wir brauen den Intercept selten in der Interpretation. Wir nehmen hier erstmal hin, dass wir einen von 0 signifkant unterschiedlichen Intercept haben. Meist löschen wir den Intrcept auch aus der finalen Tabelle raus.\n\ntemp beschreibt den Effekt der Tempertaur. Wenn die Temperatur um ein Grad ansteigt, dann erhalten wir \\(1.75\\) mehr Trockengewicht als Ertrag. Darüber hinaus ist der Effekt der Temperatur signifikant.\n\nrained [low] bescheibt den Effekt des Levels low des Faktors rained im Vergleich zum Level high. Daher haben wir bei wenig Regen einen um \\(1.33\\) höheren Ertrag als bei viel Regen.\n\nlocation [northeast] beschreibt den Effekt des Levels northeast zu dem Level north des Faktors location. Wir haben also einen \\(-1.38\\) kleineren Ertrag an Kichererbsen als im Norden von Brandenburg. Wenn du hier eine andere Sortierung willst, dann musst du mit der Funtkion factor() die Level anders sortieren.\n\nlocation [west] beschreibt den Effekt des Levels west zu dem Level north des Faktors location. Wir haben also einen \\(-2.40\\) kleineren Ertrag an Kichererbsen als im Norden von Brandenburg. Wenn du hier eine andere Sortierung willst, dann musst du mit der Funtkion factor() die Level anders sortieren.\n\nno3 beschreibt den Effekt von Nitrat im Boden. Wenn wir die Nitratkonzentration um eine Einheit erhöhen dann steigt der Ertrag um \\(1.11\\) an. Wir haben hier einen \\(p\\)-Wert von \\(0.012\\) vorliegen und können hier von einem signifkianten Effekt sprechen.\n\nfe beschreibt den Effekt des Eisens im Boden auf den Ertrag an Kichererbsen. Wenn die Konzentration von Eisen um eine Einheit ansteigt, so sinkt der Ertrag von Kichererbsen um \\(-0.72\\) ab.\n\nsand beschreibt den Anteil an Sand in der Boenprobe. Wenn der Anteil an Sand um eine Einheit ansteigt, so steigt der Ertrag an Kichererbsen um \\(3.03\\) an. Diesr Effekt ist auch hoch signifikant. Kichererbsen scheinen sandigen Boden zu bevorzugen.\n\nforest [&gt;1000m] beschreibt die Nähe des nächsten Waldstückes als Faktor mit zwei Leveln. Daher haben wir hier einen höheren Ertrag von \\(0.67\\) an Kichererbsen, wenn wir weiter weg vom Wald messen &gt;1000 als Nahe an dem Wald &lt;1000.\n\nDas war eine Wand an Text für die Interpretation des Modells. Was können wir zusammenfassend mitnehmen? Wir haben drei signifikante Einflussfaktoren auf den Ertrag an Kichererbsen gefunden. Zum einen ist weniger Regen signifkant besser als viel Regen. Wir brauchen mehr Nitrat im Boden. Im Weiteren ist ein sandiger Boden besser als ein fetter Boden. Am Ende müssen wir noch schauen, was die nicht signifikanten Ergebnisse uns für Hinweise geben. Der Ort der Messung ist relativ unbedeutend. Es scheint aber so zu sein, dass im Norden mehr Ertrag zu erhalten ist. Hier müsste man einmal schauen, welche Betriebe hier vorliegen und wie die Bodenbeschaffenheit dort war. Im Weiteren sehen wir, dass anscheinend ein Abstand zum Wald vorteilhaft für den Ertrag ist. Hier könnte Wildfraß ein Problem gewesen sein oder aber zu viel Schatten. Auch hir muss man nochmal auf das Feld und schauen, was das konktete Problem sein könnte. Hier endet die Statistik dann."
  },
  {
    "objectID": "stat-modeling-poisson.html#annahmen-an-die-daten",
    "href": "stat-modeling-poisson.html#annahmen-an-die-daten",
    "title": "41  Poisson Regression",
    "section": "\n41.1 Annahmen an die Daten",
    "text": "41.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Poissonverteilung.\nIm folgenden Kapitel zu der multiplen Poisson linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 37 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Poissonverteilung. Das ist hier sehr wichtig, denn wir wollen ja eine multiple Poisson lineare Regression rechnen.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen."
  },
  {
    "objectID": "stat-modeling-poisson.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-poisson.html#genutzte-r-pakete-für-das-kapitel",
    "title": "41  Poisson Regression",
    "section": "\n41.2 Genutzte R Pakete für das Kapitel",
    "text": "41.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, MASS, pscl, see,\n               modelsummary, scales)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-poisson.html#daten",
    "href": "stat-modeling-poisson.html#daten",
    "title": "41  Poisson Regression",
    "section": "\n41.3 Daten",
    "text": "41.3 Daten\nIm folgenden schauen wir uns ein Datenbeispiel mit Hechten an. Es handelt sich um langnasige Hechte in nordamerikanischen Flüssen. Wir haben uns insgesamt \\(n = 68\\) Flüsse einmal angesehen und dort die Anzahl an Hechten gezählt. Im Weiteren haben wir dann noch andere Flussparameter erhoben und fragen uns nun, welche dieser Parameter einen Einfluss auf die Anzahl an Hechten in den Flussarmen haben. In Kapitel 8.2 findest du nochmal mehr Informationen zu den Daten. Wir entfernen hier die Informationen zu den Flüssen, die brauchen wir in dieser Analyse nicht.\n\n\nDie Daten zu den langnasigen Hechten stammt von Salvatore S. Mangiafico - An R Companion for the Handbook of Biological Statistics.\n\nlongnose_tbl &lt;- read_csv2(\"data/longnose.csv\") %&gt;% \n  select(-stream)\n\n\n\n\n\nTabelle 41.1— Auszug aus dem Daten zu den langnasigen Hechten.\n\nlongnose\narea\ndo2\nmaxdepth\nno3\nso4\ntemp\n\n\n\n13\n2528\n9.6\n80\n2.28\n16.75\n15.3\n\n\n12\n3333\n8.5\n83\n5.34\n7.74\n19.4\n\n\n54\n19611\n8.3\n96\n0.99\n10.92\n19.5\n\n\n19\n3570\n9.2\n56\n5.44\n16.53\n17\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n26\n1450\n7.9\n60\n2.96\n8.84\n18.6\n\n\n20\n4106\n10\n96\n2.62\n5.45\n15.4\n\n\n38\n10274\n9.3\n90\n5.45\n24.76\n15\n\n\n19\n510\n6.7\n82\n5.25\n14.19\n26.5\n\n\n\n\n\n\nIm Folgenden werden wir die Daten nur für das Fitten eines Modells verwenden. In den anderen oben genannten Kapiteln nutzen wir die Daten dann anders. In Abbildung 41.1 sehen wir nochmal die Verteilung der Anzahl der Hechte in den Flüssen.\n\nggplot(longnose_tbl, aes(longnose)) +\n  theme_bw() +\n  geom_histogram()\n\n\n\nAbbildung 41.1— Histogramm der Verteilung der Hechte in den beobachteten Flüssen."
  },
  {
    "objectID": "stat-modeling-poisson.html#fit-des-modells",
    "href": "stat-modeling-poisson.html#fit-des-modells",
    "title": "41  Poisson Regression",
    "section": "\n41.4 Fit des Modells",
    "text": "41.4 Fit des Modells\nIn diesem Abschnitt wollen wir verschiedene Modelle für Zähldaten schätzen. Die Poissonverteilung hat keinen eignen Parameter für die Streung wie die Normalverteilung. Die Poissonverteilung ist mit \\(\\mathcal{Pois}(\\lambda)\\) definiert und hat somit die Eigenschaft das die Varianz eins zu eins mit dem Mittelwert \\(\\lambda\\) der Poissonverteilung ansteigt. Es kann aber sein, dass wir in den Daten nicht diesen ein zu eins Zusammenhang von Mittelwert und Varianz vrliegen haben. Häufig ist die Varianz viel größer und steigt schneller an. Wenn die Varianz in Wirklichkeit sehr viel größer ist, dann würden wir die Varianz in unseren Modell unterschätzen.\n\nEin klassisches Poissonmodell glm(..., familiy = poisson) mit der Annahme keiner Overdisperison.\nEin Quasi-Poissonmodell glm(..., family = quasipoisson) mit der Möglichkeit der Berücksichtigung einer Overdispersion.\nEin negative Binomialmodell glm.nb(...) ebenfalls mit der Berücksichtigung einer Overdispersion.\n\nBeginnen wollen wir aber mit einer klassischen Poissonregression ohne die Annahme von einer Overdispersion in den Daten. Wir nutzen dafür die Funktion glm() und spezifizieren die Verteilungsfamilie als poisson. Wir nehmen wieder alle Variablen in das Modell auf der rechten Seite des ~. Auf der linken Seite des ~ kommt dann unser Outcome longnose was die Anzahl an Hechten erhält.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\nWir müssen für die Possionregression noch beachten, dass die Zähldaten von \\(0\\) bis \\(+\\infty\\) laufen. Damit wir normalverteilte Residuen erhalten und einen lineren Zusammenhang, werden wir das Modell auf dem \\(\\log\\)-scale fitten. Das heißt, wir werden den Zusammenhang von \\(y\\) und \\(x\\) logarithmieren. Wichtig ist hierbei der Zusammenhang. Wir transformieren nicht einfach \\(y\\) und lassen den Rest unberührt. Das führt dazu, dass wir am Ende die Koeffizienten der Poissonregression exponieren müssen. Das können die gängigen Funktionen, wir müssen das Exponieren aber aktiv durchführen. Deshalb hier schon mal erwähnt.\n\npoisson_fit &lt;- glm(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                    longnose_tbl, family = poisson)\n\nWir schauen uns die Ausgabe des Modells einmal mit der summary() Funktion an, da wir hier einmal händisch schauen wollen, ob eine Overdispersion vorliegt. Sonst könnten wir auch die Funktion model_parameters() nehmen. Die nutzen wir später für die Interpretation des Modells, hier wollen wir erstmal sehen, ob alles geklappt hat.\n\npoisson_fit %&gt;% summary\n\n\nCall:\nglm(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, family = poisson, data = longnose_tbl)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.234  -4.086  -1.662   1.771  14.362  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.564e+00  2.818e-01  -5.551 2.83e-08 ***\narea         3.843e-05  2.079e-06  18.480  &lt; 2e-16 ***\ndo2          2.259e-01  2.126e-02  10.626  &lt; 2e-16 ***\nmaxdepth     1.155e-02  6.688e-04  17.270  &lt; 2e-16 ***\nno3          1.813e-01  1.068e-02  16.974  &lt; 2e-16 ***\nso4         -6.810e-03  3.622e-03  -1.880   0.0601 .  \ntemp         7.854e-02  6.530e-03  12.028  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2766.9  on 67  degrees of freedom\nResidual deviance: 1590.0  on 61  degrees of freedom\nAIC: 1936.9\n\nNumber of Fisher Scoring iterations: 5\n\n\nWir schauen in die Summary-Ausgabe des Poissonmodells und sehen, dass dort steht, dass Dispersion parameter for poisson family taken to be 1. Wir modellieren also einen eins zu eins Zusammenhang von Mittelwert und Varianz. Wenn dieser Zusammenhang nicht in unseren Daten existiert, dann haben wir eine Overdispersion vorliegen.\nWir können die Overdispersion mit abschätzen indem wir die Residual deviance durch die Freiheitsgrade der Residual deviance teilen. Daher erhalten wir eine Overdispersion von \\(\\cfrac{1590.04}{61} \\approx 26.1\\). Damit haben wir eine eindeutige Overdispersion vorliegen. Damit steigt die Varianz in einem Verhältnis von ca. 1 zu 26. Wir können auch die Funktion check_overdispersion() aus dem R Paket performance nutzen um die Overdispersion zu berechnen. Die Funktion kann das schneller und ist auch in der Abfolge einer Analyse besser geeignet.\n\npoisson_fit %&gt;% check_overdispersion()\n\n# Overdispersion test\n\n       dispersion ratio =   29.403\n  Pearson's Chi-Squared = 1793.599\n                p-value =  &lt; 0.001\n\n\nOverdispersion detected.\n\n\nWenn wir Overdispersion vorliegen haben und damit die Varianz zu niedrig schätzen, dann erhalten wir viel mehr signifikante Ergebnisse als es in den Daten zu erwarten wäre. Schauen wir uns nochmal die Parameter der Poissonverteilung und die \\(p\\)-Werte einmal an.\n\npoisson_fit %&gt;% model_parameters()\n\nParameter   |  Log-Mean |       SE |         95% CI |     z |      p\n--------------------------------------------------------------------\n(Intercept) |     -1.56 |     0.28 | [-2.12, -1.01] | -5.55 | &lt; .001\narea        |  3.84e-05 | 2.08e-06 | [ 0.00,  0.00] | 18.48 | &lt; .001\ndo2         |      0.23 |     0.02 | [ 0.18,  0.27] | 10.63 | &lt; .001\nmaxdepth    |      0.01 | 6.69e-04 | [ 0.01,  0.01] | 17.27 | &lt; .001\nno3         |      0.18 |     0.01 | [ 0.16,  0.20] | 16.97 | &lt; .001\nso4         | -6.81e-03 | 3.62e-03 | [-0.01,  0.00] | -1.88 | 0.060 \ntemp        |      0.08 | 6.53e-03 | [ 0.07,  0.09] | 12.03 | &lt; .001\n\n\nIn der Spalte p finden wir die \\(p\\)-Werte für alle Variablen. Wir sehen, dass fast alle Variablen signifikant sind und das wir eine sehr niedrige Varianz in der Spalte SE sehen. Das heißt unser geschätzer Fehler ist sehr gering. Das ahnten wir ja schon, immerhin haben wir eine Overdisperson vorliegen. Das Modell ist somit falsch. Wir müssen uns ein neues Modell suchen, was Overdispersion berückscihtigen und modellieren kann.\nDie Quasi-Poisson Verteilung hat einen zusätzlichen, unabhänigen Parameter um die Varianz der Verteilung zu schätzen. Daher können wir die Overdispersion mit einer Quasi-Poisson Verteilung berückscihtigen. Wir können eine Quasi-Poisson Verteilung auch mit der Funktion glm() schätzen nur müssen wir als Verteilungsfamilie quasipoisson angeben.\n\nquasipoisson_fit &lt;- glm(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                        data = longnose_tbl, family = quasipoisson)\n\nNach dem Modellti können wir nochmal in der summary() Funktion schauen, ob wir die Overdispersion richtig berücksichtigt haben.\n\nquasipoisson_fit %&gt;% summary\n\n\nCall:\nglm(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, family = quasipoisson, data = longnose_tbl)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-9.234  -4.086  -1.662   1.771  14.362  \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -1.564e+00  1.528e+00  -1.024  0.30999   \narea         3.843e-05  1.128e-05   3.408  0.00116 **\ndo2          2.259e-01  1.153e-01   1.960  0.05460 . \nmaxdepth     1.155e-02  3.626e-03   3.185  0.00228 **\nno3          1.813e-01  5.792e-02   3.130  0.00268 **\nso4         -6.810e-03  1.964e-02  -0.347  0.73001   \ntemp         7.854e-02  3.541e-02   2.218  0.03027 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 29.40332)\n\n    Null deviance: 2766.9  on 67  degrees of freedom\nResidual deviance: 1590.0  on 61  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nAn der Zeile Dispersion parameter for quasipoisson family taken to be 29.403319 in der Summary-Ausgabe sehen wir, dass das Modell der Quasi-Possion Verteilung die Overdispersion korrekt berücksichtigt hat. Wir können uns nun einmal die Modellparameter anschauen. Die Interpretation machen wir am Ende des Kapitels.\n\nquasipoisson_fit %&gt;% model_parameters()\n\nParameter   |  Log-Mean |       SE |        95% CI | t(61) |      p\n-------------------------------------------------------------------\n(Intercept) |     -1.56 |     1.53 | [-4.57, 1.41] | -1.02 | 0.306 \narea        |  3.84e-05 | 1.13e-05 | [ 0.00, 0.00] |  3.41 | &lt; .001\ndo2         |      0.23 |     0.12 | [ 0.00, 0.45] |  1.96 | 0.050 \nmaxdepth    |      0.01 | 3.63e-03 | [ 0.00, 0.02] |  3.18 | 0.001 \nno3         |      0.18 |     0.06 | [ 0.07, 0.29] |  3.13 | 0.002 \nso4         | -6.81e-03 |     0.02 | [-0.05, 0.03] | -0.35 | 0.729 \ntemp        |      0.08 |     0.04 | [ 0.01, 0.15] |  2.22 | 0.027 \n\n\nJetzt sieht unser Modell und die \\(p\\)-Werte zusammen mit dem Standardfehler SE schon sehr viel besser aus. Wir können also diesem Modell erstmal von der Seite der Overdispersion vertrauen.\nAm Ende wollen wir nochmal das Modell mit der negativen Binomialverteilung rechnen. Die negativen Binomialverteilung erlaubt auch eine Unabhängigkeit von dem Mittelwert zu der Varianz. Wir können hier auch für die Overdispersion adjustieren. Wir rechnen die negativen Binomialregression mit der Funktion glm.nb() aus dem R Paket MASS. Wir müssen keine Verteilungsfamilie angeben, die Funktion glm.nb() kann nur die negative Binomialverteilung modellieren.\n\nnegativebinomial_fit &lt;- glm.nb(longnose ~ area + do2 + maxdepth + no3 + so4 + temp,\n                               data = longnose_tbl)\n\nAuch hier schauen wir mit der Funktion summary() einmal, ob die Overdisprsion richtig geschätzt wurde oder ob hier auch eine Unterschätzung des Zusammenhangs des Mittelwerts und der Varianz vorliegt.\n\nnegativebinomial_fit %&gt;% summary()\n\n\nCall:\nglm.nb(formula = longnose ~ area + do2 + maxdepth + no3 + so4 + \n    temp, data = longnose_tbl, init.theta = 1.666933879, link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4601  -0.9876  -0.4426   0.4825   2.2776  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.946e+00  1.305e+00  -2.256 0.024041 *  \narea         4.651e-05  1.300e-05   3.577 0.000347 ***\ndo2          3.419e-01  1.050e-01   3.256 0.001130 ** \nmaxdepth     9.538e-03  3.465e-03   2.752 0.005919 ** \nno3          2.072e-01  5.627e-02   3.683 0.000230 ***\nso4         -2.157e-03  1.517e-02  -0.142 0.886875    \ntemp         9.460e-02  3.315e-02   2.854 0.004323 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.6669) family taken to be 1)\n\n    Null deviance: 127.670  on 67  degrees of freedom\nResidual deviance:  73.648  on 61  degrees of freedom\nAIC: 610.18\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.667 \n          Std. Err.:  0.289 \n\n 2 x log-likelihood:  -594.175 \n\n\nAuch hier sehen wir, dass die Overdispersion mit dem Parameter \\(\\theta\\) berücksichtigt wird. Wir können die Zahl \\(1.67\\) nicht direkt mit der Overdispersion aus einer Poissonregression verglechen, aber wir sehen dass das Verhältnis von Residual deviance zu den Freiheitsgraden mit \\(\\cfrac{73.65}{61} \\approx 1.20\\) fast bei 1:1 liegt. Wir könnten also auch eine negative Binomialverteilung für das Modellieren nutzen.\n\nnegativebinomial_fit %&gt;% model_parameters()\n\nParameter   |  Log-Mean |       SE |         95% CI |     z |      p\n--------------------------------------------------------------------\n(Intercept) |     -2.95 |     1.31 | [-5.85, -0.10] | -2.26 | 0.024 \narea        |  4.65e-05 | 1.30e-05 | [ 0.00,  0.00] |  3.58 | &lt; .001\ndo2         |      0.34 |     0.11 | [ 0.11,  0.58] |  3.26 | 0.001 \nmaxdepth    |  9.54e-03 | 3.47e-03 | [ 0.00,  0.02] |  2.75 | 0.006 \nno3         |      0.21 |     0.06 | [ 0.10,  0.32] |  3.68 | &lt; .001\nso4         | -2.16e-03 |     0.02 | [-0.03,  0.03] | -0.14 | 0.887 \ntemp        |      0.09 |     0.03 | [ 0.03,  0.16] |  2.85 | 0.004 \n\n\n\n\nWie immer gibt es reichtlich Tipps & Tricks welches Modell du nun nehmen solltest. How to deal with overdispersion in Poisson regression: quasi-likelihood, negative binomial GLM, or subject-level random effect? und das Tutorial Modeling Count Data. Auch ich mus immer wieder schauen, was am besten konkret in der Anwendung passen könnte und würde.\nWelches Modell nun das beste Modell ist, ist schwer zu sagen. Wenn du Overdisperion vorliegen hast, dann ist natürlich nur das Quasi-Poissonmodell oder das negative Binomialmodell möglich. Welche der beiden dann das bessere ist, hängt wieder von der Fragestellung ab. Allgemein gesprochen ist das Quasi-Poissonmodell besser wenn dich die Zusammenhänge von \\(y\\) zu \\(x\\) am meisten interessieren. Und das ist in unserem Fall hier die Sachlage. Daher gehen wir mit den Quasi-Poissonmdell dann weiter."
  },
  {
    "objectID": "stat-modeling-poisson.html#performance-des-modells",
    "href": "stat-modeling-poisson.html#performance-des-modells",
    "title": "41  Poisson Regression",
    "section": "\n41.5 Performance des Modells",
    "text": "41.5 Performance des Modells\nIn diesem kurzen Abschnitt wollen wir uns einmal anschauen, ob das Modell neben der Overdispersion auch sonst aus statistischer Sicht in Ordnung ist. Wir wollen ja mit dem Modell aus dem Fit quasipoisson_fit weitermachen. Also schauen wir uns einmal das pseudo-\\(R^2\\) für die Poissonregression an. Da wir es mit einem GLM zu tun haben, ist das \\(R^2\\) mit vorsicht zu genießen. In einer Gaussianregression können wir das \\(R^2\\) als Anteil der erklärten Varianz durch das Modell interpretieren. Im Falle von GLM’s müssen wir hier vorsichtiger sein. In GLM’s gibt es ja keine Varianz sondern eine Deviance.\n\nr2_efron(quasipoisson_fit)\n\n[1] 0.3257711\n\n\nMit einem pseudo-\\(R^2\\) von \\(0.33\\) erklären wir ca. 33% der Varianz in der Anzahl der Hechte. Das ist zwar keine super gute Zahl, aber dafür, dass wir nur eine handvoll von Parametern erfasst haben, ist es dann auch wieder nicht so schlecht. Die Anzahl an Hechten wird sicherlich an ganz vielen Parametern hängen, wir konnten immerhin einige wichtige Stellschrauben vermutlich finden.\nIn Abbildung 41.2 schauen wir uns nochmal die Daten in den Modelgüteplots an. Wir sehen vorallem, dass wir vielelicht doch einen Ausreißer mit der Beobachtung 17 vorliegen haben. Auch ist der Fit nicht so super, wie wir an dem QQ-Plot sehen. Die Beobachtungen fallen in dem QQ-Plot nicht alle auf eine Linie. Auch sehen wir dieses Muster in dem Residualplot. Hiererwarten wir eine gerade blaue Linie und auch hier haben wir eventuell Ausreißer mit in den Daten.\n\ncheck_model(quasipoisson_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\", \"pp_check\", \"homogeneity\")) \n\n\n\nAbbildung 41.2— Ausgabe ausgewählter Modelgüteplots der Funktion check_model()."
  },
  {
    "objectID": "stat-modeling-poisson.html#interpretation-des-modells",
    "href": "stat-modeling-poisson.html#interpretation-des-modells",
    "title": "41  Poisson Regression",
    "section": "\n41.6 Interpretation des Modells",
    "text": "41.6 Interpretation des Modells\nUm die Effektschätzer einer Poissonregression oder aber einer Quasipoisson-Regression interpretieren zu können müssen wir uns einmal einen Beispieldatensatz mit bekannten Effekten zwischen den Gruppen bauen. Im Folgenden bauen wir uns einen Datensatz mit zwei Gruppen. Einmal einer Kontrollgruppe mit einer mittleren Anzahl an \\(15\\) und einer Behandlungsgruppe mit einer um \\(\\beta_1 = 10\\) höheren Anzahl. Wir haben also in der Kontrolle im Mittel eine Anzahl von \\(15\\) und in der Behandlungsgruppe eine mittlere Anzahl von \\(25\\).\n\nsample_size &lt;- 100\nlongnose_small_tbl &lt;- tibble(grp = rep(c(0, 1), each = sample_size),\n                             count = 15 + 10 * grp + rnorm(2 * sample_size, 0, 1)) %&gt;%\n  mutate(count = round(count),\n         grp = factor(grp, labels = c(\"ctrl\", \"trt\")))\n\nIn Tabelle 41.2 sehen wir nochmal die Daten als Ausschnitt dargestellt.\n\n\n\n\nTabelle 41.2— How much is the fish? Der Datensatz über \\(n = 1000\\) Beobachtungen an dem wir überlegen wollen wie wir die Effektschätzer einer Poissonregression zu interpretieren haben.\n\ngrp\ncount\n\n\n\nctrl\n13\n\n\nctrl\n16\n\n\nctrl\n15\n\n\nctrl\n16\n\n\n…\n…\n\n\ntrt\n24\n\n\ntrt\n24\n\n\ntrt\n25\n\n\ntrt\n25\n\n\n\n\n\n\nDa sich die Tabelle schlecht liest hier nochmal der Boxplot in Abbildung 41.3. Wir sehen den Grupenunterschied von \\(10\\) sowie die unterschiedlichen mittleren Anzahlen für die Kontrolle und die Behandlung.\n\nggplot(longnose_small_tbl, aes(x = grp, y = count, fill = grp)) +\n  theme_bw() +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\nggplot(data = longnose_small_tbl, aes(x = count, fill = grp)) +\n  theme_bw() +\n  geom_density(alpha = 0.75) +\n  labs(x = \"\", y = \"\", fill = \"Gruppe\") +\n  scale_fill_okabeito() +\n  scale_x_continuous(breaks = seq(10, 30, by = 5), limits = c(10, 30)) \n\n\n\n\n\n(a) Verteilung der Werte als Boxplot.\n\n\n\n\n\n\n\n(b) Verteilung der Werte als Densityplot.\n\n\n\nAbbildung 41.3— How much is the fish? Der Boxplot über \\(n = 1000\\) Beobachtungen an dem wir überlegen wollen wie wir die Effektschätzer einer Poissonregression zu interpretieren haben.\n\n\n\nJetzt fitten wir einmal das simple Poissonmodell mit der Anzahl als Outcome und der Gruppe mit den zwei Leveln als \\(x\\). Wir pipen dann das Ergebnis des Fittes gleich in die Funktion model_parameters() weiter um die Ergebnisse des Modellierens zu erhalten.\n\nglm(count ~ grp, data = longnose_small_tbl, family = poisson) %&gt;%\n  model_parameters(exponentiate = TRUE)\n\nParameter   |   IRR |   SE |         95% CI |      z |      p\n-------------------------------------------------------------\n(Intercept) | 15.05 | 0.39 | [14.30, 15.82] | 105.19 | &lt; .001\ngrp [trt]   |  1.65 | 0.05 | [ 1.55,  1.76] |  15.35 | &lt; .001\n\n\nAls erstes fällt auf, dass wir die Ausgabe des Modells exponieren müssen. Um einen linearen Zusamenhang hinzukriegen bedient sich die Poissonregression den Trick, das der Zusammenhang zwischen dem \\(y\\) und dem \\(x\\) transformiert wird. Wir rechnen unsere Regression nicht auf den echten Daten sondern auf dem \\(\\log\\)-scale. Daher müssen wir die Koeffizienten der Poissonregression wieder zurücktransfomieren, wenn wir die Koeffizienten interpretieren wollen. Das können wir mit der Option exponentiate = TRUE durchführen.\nGut soweit, aber was heißen den jetzt die Zahlen? Wir haben einen Intercept von \\(14.99\\) das entspricht der mittleren Anzahl in der Kontrollgruppe. Und was sagt jetzt die \\(1.67\\) vom Level trt des Faktors grp? Wenn wir \\(14.99 \\cdot 1.67\\) rechnen, dann erhalten wir als Ergebnis \\(25.03\\), also die mittlere Anzahl in der Behandlungsgruppe. Was sagt uns das jetzt aus? Wir erhalten aus der Poissonregression eine Wahrscheinlichkeit oder aber ein Risk Ratio. Wir können sagen, dass die Anzahl in der Behandlungsgruppe \\(1.67\\)-mal so groß ist wie in der Kontrollgruppe.\nSchauen wir uns nochmal das volle Modell an und interpretieren die Effekte der einzelnen Variablen.\n\nquasipoisson_fit %&gt;% \n  model_parameters(exponentiate = TRUE) \n\nParameter   |  IRR |       SE |       95% CI | t(61) |      p\n-------------------------------------------------------------\n(Intercept) | 0.21 |     0.32 | [0.01, 4.11] | -1.02 | 0.306 \narea        | 1.00 | 1.13e-05 | [1.00, 1.00] |  3.41 | &lt; .001\ndo2         | 1.25 |     0.14 | [1.00, 1.57] |  1.96 | 0.050 \nmaxdepth    | 1.01 | 3.67e-03 | [1.00, 1.02] |  3.18 | 0.001 \nno3         | 1.20 |     0.07 | [1.07, 1.34] |  3.13 | 0.002 \nso4         | 0.99 |     0.02 | [0.95, 1.03] | -0.35 | 0.729 \ntemp        | 1.08 |     0.04 | [1.01, 1.16] |  2.22 | 0.027 \n\n\nSo schön auch die Funktion model_parameters() ist, so haben wir aber hier das Problem, dass wir den Effekt von area nicht mehr richtig sehen. Wir kriegen hier eine zu starke Rundung auf zwei Nachkommastellen. Wir nutzen jetzt mal die Funktion tidy() um hier Abhilfe zu leisten. Ich muss hier noch die Spalte estimate mit num(..., digits = 5) anpassen, damit du in der Ausgabe auf der Webseite auch die Nachkommastellen siehst.\n\nquasipoisson_fit %&gt;% \n  tidy(exponentiate = TRUE, digits = 5) %&gt;% \n  select(term, estimate, p.value) %&gt;% \n  mutate(p.value = pvalue(p.value),\n         estimate = num(estimate, digits = 5))\n\n# A tibble: 7 × 3\n  term         estimate p.value\n  &lt;chr&gt;       &lt;num:.5!&gt; &lt;chr&gt;  \n1 (Intercept)   0.20922 0.310  \n2 area          1.00004 0.001  \n3 do2           1.25342 0.055  \n4 maxdepth      1.01162 0.002  \n5 no3           1.19879 0.003  \n6 so4           0.99321 0.730  \n7 temp          1.08171 0.030  \n\n\nSchauen wir uns die Effekte der Poissonregression einmal an und versuchen die Ergebnisse zu interpretieren. Dabei ist wichtig sich zu erinnern, dass kein Effekt eine 1 bedeutet. Wir schauen hier auf einen Faktor. Wenn wir eine Anzahl mal Faktor 1 nehmen, dann ändert sich nichts an der Anzahl.\n\n\n(Intercept) beschreibt den Intercept der Poissonregression. Wenn wir mehr als eine simple Regression vorliegen haben, wie in diesem Fall, dann ist der Intercept schwer zu interpretieren. Wir konzentrieren uns auf die Effekte der anderen Variablen.\n\narea, beschreibt den Effekt der Fläche. Steigt die Fläche um ein Quadratmeter an, so erhöht sich die Anzahl an Fischen um den \\(1.00001\\). Daher würde man hier eher sagen, erhöht sich die Fläche um jeweils 1000qm so erhöht sich die Anzahl an Fischen um den Faktor \\(1.1\\). Dann haben wir auch einen besser zu interpretierenden Effektschätzer. Die Signifikanz bleibt hier davon unbetroffen.\n\ndo2, beschreibt den Partzialdruck des Sauerstoffs. Steigt dieser um eine Einheit an, so sehen wie eine Erhöhung der Anzahl an Fischen um den Faktor \\(1.25\\). Der Effekt ist gerade nicht signifikant.\n\nmaxdepth, beschreibt die maximale Tiefe. Je tiefer ein Fluß, desto mehr Hechte werden wir beobachten. Der Effekt von \\(1.01\\) pro Meter Tiefe ist signifikant.\n\nno3, beschreibt den Anteil an Nitrat in den Flüssen. Je mehr Nitrat desto signifiant mehr Hechte werden wir beobachten. Hier steigt der Faktor auch um \\(1.20\\).\n\nso4, beschreibt den Schwefelgehalt und mit steigenden Schwefelgehalt nimmt die Anzahl an Fischen leicht ab. Der Effekt ist aber überhauot nicht signifikant.\n\ntemp, beschreibt die Temperatur der Flüsse. Mit steigender Tempertaur erwarten wir mehr Hechte zu beobachten. Der Effekt von \\(1.08\\) Fischen pro Grad Erhöhung ist signifikant.\n\nWas nehmen wir aus der Poissonregression zu den langnasigen Hechten mit? Zum einen haben die Fläche, die Tiefe und der Nitratgehalt einen signifikanten Einfluss auf die Anzahl an Hechten. Auch führt eine höhere Temperatur zu mehr gefundenen Hechten. Die erhöhte Temperatur steht etwas im Widerspuch zu dem Sauerstoffpartizaldruck. Denn je höher die Temperatur desto weniger Sauerstoff wird in dem Wasser gelöst sein. Auch scheint die Oberfläche mit der Tiefe korreliert. Allgemein scheinen Hechte große Flüße zu mögen. Hier bietet sich also noch eine Variablenselektion oder eine Untersuchung auf Ausreißer an um solche Effekte nochmal gesondert zu betrachten."
  },
  {
    "objectID": "stat-modeling-poisson.html#zeroinflation",
    "href": "stat-modeling-poisson.html#zeroinflation",
    "title": "41  Poisson Regression",
    "section": "\n41.7 Zeroinflation",
    "text": "41.7 Zeroinflation\nSo eine Poissonregression hat schon einiges an Eigenheiten. Neben dem Problem der Overdispersion gibt es aber noch eine weitere Sache, die wir beachten müssen. Wir können bei einer Poissonregression auch eine Zeroinflation vorliegen haben. Das heißt, wir beobachten viel mehr Nullen in den Daen, als wir aus der Poissonverteilung erwarten würden. Es gibt also einen biologischen oder künstlichn Prozess, der uns Nullen produziert. Häufig wissen wir nicht, ob wir den Prozess, der uns die Nullen in den Daten produziert, auch abbilden. Das heißt, es kann sein, dass wir einfach nichts Zählen, weil dort nichts ist oder aber es gibt dafür einen Grund. Diesen Grund müssten wir dann irgendwie in unseren Daten erfasst haben, aber meistens haben wir das nicht.\nSchauen wir usn dafür einmal ein Datenbeispiel von Eidechsen in der Lüneburgerheide an. Wir haben Eidechsen lizard in zwei verschiedenen Habitaten grp gezählt. Einmal, ob die Eidechsen eher im offenen Gelände oder eher im bedeckten Gelände zu finden waren. Im Weiteren haben wir geschaut, ob der Boden keinen Regen erhalten hatte, trocken war oder gar feucht. Mit trocken ist hier eine gewisse Restfeuchte gemeint. Am Ende haben wir noch bestimmt, ob wir eher nah an einer Siedlung waren oder eher weiter entfernt. Du kannst dir den Daten satz in der Datei lizards.csv nochmal anschauen. In Tabelle 41.3 sind die Daten nochmal dargestellt.\n\n\n\n\nTabelle 41.3— Ausschnitt aus den Eidechsendaten für die zwei Habitate unter verschiedenen Feuchtigkeitsbedingungen und Nähe zur nächsten Siedlung.\n\ngrp\nrain\npop\nlizard\n\n\n\nopen\nno\nnear\n0\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n1\n\n\nopen\nno\nnear\n0\n\n\nopen\nno\nfar\n2\n\n\nopen\nno\nfar\n4\n\n\n\n\n\n\nIn Abbildung 41.4 sehen wir die Zähldaten der Eidechsen nochmal als Histogramm dargestellt. Wenn wir an einem Punkt keine Eidechsen gefunden haben, dann haben wir keine fehlenden Werte eingetragen, sondern eben, dass wir keine Eidechsen gezählt haben. Wir sehen das wir sehr viele Nullen in unseren Daten haben. Ein Indiz für eine Inflation an Nullen oder eben einer Zeroinflation.\n\nggplot(lizard_zero_tbl, aes(lizard)) +\n  theme_bw() +\n  geom_histogram() +\n  labs(x = \"Anzahl der gefundenen Eidechsen\", y = \"Anzahl\") +\n  scale_x_continuous(breaks = 0:7)\n\n\n\nAbbildung 41.4— Histogramm der Verteilung der Hechte in den beobachteten Flüssen.\n\n\n\nUm zu überprüfen, ob wir eine Zeroinflation in den Daten vorliegen haben, werden wir erstmal eine ganz normale Poissonregression auf den Daten rechnen. Wir ignorieren auch eine potenzielle Overdispersion. Das schauen wir uns dann in den Daten später nochmal an.\n\nlizard_fit &lt;- glm(lizard ~ grp + rain + pop, data = lizard_zero_tbl,\n                  family = poisson)\n\nWie immer nutzen wir die Funktion model_parameters() um uns die exponierten Koeffizienten aus dem Modell wiedergeben zu lassen. Das Modell dient uns jetzt nur als Ausgangsmodell und wir werden das Poissonmodell jetzt nicht weiter tiefer verwenden.\n\nlizard_fit %&gt;% model_parameters(exponentiate = TRUE)\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.29 | [0.60, 1.77] |  0.20 | 0.840 \ngrp [cover] | 1.88 | 0.46 | [1.18, 3.07] |  2.61 | 0.009 \nrain [dry]  | 0.31 | 0.09 | [0.17, 0.53] | -4.12 | &lt; .001\nrain [wet]  | 0.13 | 0.05 | [0.06, 0.28] | -4.98 | &lt; .001\npop [far]   | 2.41 | 0.61 | [1.49, 4.04] |  3.47 | &lt; .001\n\n\nWir sehen, dass wir in der Variable rain eine starke Reduzierung der Anzahl an Eidechsen sehen. Vielleicht ist dies eine Variable, die zu viele Nullen produziert. Auch hat die Variable pop, die für die Nähe an einer Siedlung kodiert, einen starken positiven Effekt auf unsere Anzahl an Eidechsen. Hier wollen wir also einmal auf eine Zeroinflation überprüfen. Wir nutzen dazu die Funktion check_zeroinflation() aus dem R Paket performance. Die Funktion läuft nur auf einem Modellfit.\n\ncheck_zeroinflation(lizard_fit)\n\n# Check for zero-inflation\n\n   Observed zeros: 31\n  Predicted zeros: 27\n            Ratio: 0.87\n\n\nDie Funktion gibt uns wieder, dass wir vermutlich eine Zeroinflation vorliegen haben. Das können wir aber Modellieren. Um eine Zeroinflation ohne Overdispersion zu modellieren nutzen wir die Funktion zeroinfl() aus dem R Paket pscl. Der erste Teil der Funktion ist leicht erkläret. Wir bauen uns wieder unswer Model zusammen, was wir fitten wollen. Dann kommt aber ein | und mit diesem Symbol | definieren wir, ob wir wissen, woher die Nullen kommen oder aber ob wir die Nullen mit einem zufälligen Prozess modellieren wollen.\nWenn wir das Modell in der Form y ~ f1 + f2 | 1 schreiben, dann nehmen wir an, dass das Übermaß an Nullen in unseren Daten rein zufällig entstanden sind. Wir haben keine Spalte in de Daten, die uns eine Erklärung für die zusätzlichen Nullen liefern würde.\nWir können auch y ~ f1 + f2 | x3 schreiben. Dann haben wir eine Variable x3 in den Daten von der wir glauben ein Großteil der Nullen herrührt. Wir könnten also in unseren Daten annehmen, dass wir den Überschuss an Nullen durch den Regen erhalten haben und damit über die Spalte rain den Exzess an Nullen modellieren.\nMan sollte immer mit dem einfachsten Modell anfangen, deshalb werden wir jetzt einmal ein Modell fitten, dass annimmt, dass die Nullen durch einen uns unbekannten Zufallsprozess entstanden sind.\n\nlizard_zero_infl_intercept_fit &lt;- zeroinfl(lizard ~ grp + pop + rain | 1, \n                                           data = lizard_zero_tbl) \n\nWir schauen uns das Modell dann wieder einmal an und sehen eine Zweiteilung der Ausgabe. In dem oberen Teil der Ausgabe wird unsere Anzahl an Eidechsen modelliert. In dem unteren Teil wird der Anteil der Nullen in den Daten modelliert. Daher können wir über Variablen in dem Zero-Inflation Block keine Aussagen über die Anzahl an Eidechsen treffen. Variablen tauchen nämlich nur in einem der beiden Blöcke auf.\n\nlizard_zero_infl_intercept_fit %&gt;% \n  model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.31 | [0.60, 1.87] |  0.22 | 0.830 \ngrp [cover] | 2.03 | 0.51 | [1.25, 3.31] |  2.84 | 0.005 \npop [far]   | 2.59 | 0.67 | [1.56, 4.31] |  3.67 | &lt; .001\nrain [dry]  | 0.31 | 0.10 | [0.17, 0.56] | -3.82 | &lt; .001\nrain [wet]  | 0.14 | 0.06 | [0.06, 0.31] | -4.73 | &lt; .001\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |   SE |       95% CI |     z |     p\n--------------------------------------------------------------\n(Intercept) |       0.11 | 0.11 | [0.02, 0.74] | -2.26 | 0.024\n\n\nAls erstes beobachten wir einen größeren Effekt der Variable grp. Das ist schon mal ein spannender Effekt. An der Signifikanz hat scih nicht viel geändert. Wir werden am Ende des Kapitels einmal alle Modell für die Modellierung der Zeroinflation vergleichen.\nNun könnte es auch sein, dass der Effekt der vielen Nullen in unserer Variable rain verborgen liegt. Wenn es also regnet, dann werden wir viel weniger Eidechsen beoabchten. Nehmen wir also rain als ursächliche Variable mit in das Modell für die Zeroinflation.\n\nlizard_zero_infl_rain_fit &lt;- zeroinfl(lizard ~ grp + pop | rain, \n                                      data = lizard_zero_tbl)\n\nWieder schauen wir uns einmal die Ausgabe des Modells einmal genauer an.\n\nlizard_zero_infl_rain_fit %&gt;% model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |    z |     p\n-------------------------------------------------------\n(Intercept) | 1.13 | 0.34 | [0.63, 2.03] | 0.42 | 0.677\ngrp [cover] | 1.60 | 0.42 | [0.95, 2.67] | 1.77 | 0.077\npop [far]   | 1.84 | 0.51 | [1.07, 3.18] | 2.20 | 0.028\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |     SE |          95% CI |     z |     p\n-------------------------------------------------------------------\n(Intercept) |       0.04 |   0.08 | [0.00,    2.09] | -1.60 | 0.109\nrain [dry]  |      27.94 |  59.06 | [0.44, 1760.08] |  1.58 | 0.115\nrain [wet]  |      83.71 | 178.43 | [1.28, 5459.08] |  2.08 | 0.038\n\n\nEs ändert sich einiges. Zum einen erfahren wir, dass der Regen anscheined doch viele Nullen in den Daten produziert. Wir haben ein extrem hohes \\(OR\\) für die Variable rain. Die Signifikanz ist jedoch eher gering. Wir haben nämlich auch eine sehr hohe Streuung mit den großen \\(OR\\) vorliegen. Au der anderen Seite verlieren wir jetzt auch die Signifikanz von unseren Habitaten und dem Standort der Population. Nur so mäßig super dieses Modell.\nWir können jetzt natürlich auch noch den Standort der Population mit in den Prozess für die Entstehung der Nullen hineinnehmen. Wir schauen uns dieses Modell aber nicht mehr im Detail an, sondern dann nur im Vergleich zu den anderen Modellen.\n\nlizard_zero_infl_rain_pop_fit &lt;- zeroinfl(lizard ~ grp | rain + pop, \n                                          data = lizard_zero_tbl)\n\nDie Gefahr besteht immer, das man sich an die Wand modelliert und vor lauter Modellen die Übersicht verliert. Neben der Zeroinflation müssen wir ja auch schauen, ob wir eventuell eine Overdispersion in den Daten vorliegen haben. Wenn das der Fall ist, dann müsen wir nochmal überlegen, was wir dann machen. Wir testen nun auf Ovrdisprsion in unserem ursprünglichen Poissonmodell mit der Funktion check_overdispersion().\n\ncheck_overdispersion(lizard_fit)\n\n# Overdispersion test\n\n       dispersion ratio =  1.359\n  Pearson's Chi-Squared = 74.743\n                p-value =  0.039\n\n\nTja, und so erfahren wir, dass wir auch noch Overdispersion in unseren Daten vorliegen haben. Wir müsen also beides Modellieren. Einmal modellieren wir die Zeroinflation und einmal die Overdispersion. Wir können beides in einem negativen binominalen Modell fitten. Auch hier hilft die Funktion zeroinfl() mit der Option dist = negbin. Mit der Option geben wir an, dass wir eine negative binominal Verteilungsfamilie wählen. Damit können wir dann auch die Ovrdispersion in unseren Daten modellieren.\n\nlizard_zero_nb_intercept_fit &lt;- zeroinfl(lizard ~ grp + rain + pop | 1, \n                                         dist = \"negbin\", data = lizard_zero_tbl)\n\nDann schauen wir usn einmal das Modell an. Zum einen sehen wir, dass der Effekt ähnlich groß ist, wie bei dem Intercept Modell der Funktion zeroinfl. Auch bleiben die Signifikanzen ähnlich.\n\nlizard_zero_nb_intercept_fit %&gt;% model_parameters(exponentiate = TRUE)\n\n# Fixed Effects\n\nParameter   |  IRR |   SE |       95% CI |     z |      p\n---------------------------------------------------------\n(Intercept) | 1.06 | 0.31 | [0.60, 1.87] |  0.22 | 0.830 \ngrp [cover] | 2.03 | 0.51 | [1.25, 3.31] |  2.84 | 0.005 \nrain [dry]  | 0.31 | 0.10 | [0.17, 0.56] | -3.82 | &lt; .001\nrain [wet]  | 0.14 | 0.06 | [0.06, 0.31] | -4.73 | &lt; .001\npop [far]   | 2.59 | 0.67 | [1.56, 4.31] |  3.67 | &lt; .001\n\n# Zero-Inflation\n\nParameter   | Odds Ratio |   SE |       95% CI |     z |     p\n--------------------------------------------------------------\n(Intercept) |       0.11 | 0.11 | [0.02, 0.74] | -2.26 | 0.024\n\n\nNun haben wir vier Modelle geschätzt und wolen jetzt wissen, was ist das beste Modell. Dafür hilft usn dann eine Gegenüberstellung der Modelle mit der Funktion modelsummary(). Wir könnten die Modelle auch gegeneinander statistsich Testen, aber hier behalten wir uns einmal den beschreibenden Vergleich vor. In Tabelle 41.4 sehen wir einmal die vier Modelle nebeneinander gestellt. Für eine bessere Übrsicht, habe ich aus allen Modellen den Intercept entfernt.\n\nmodelsummary(lst(\"ZeroInfl Intercept\" = lizard_zero_infl_intercept_fit,\n                 \"ZeroInfl rain\" = lizard_zero_infl_rain_fit,\n                 \"ZeroInfl rain+pop\" = lizard_zero_infl_rain_pop_fit,\n                 \"NegBinom intercept\" = lizard_zero_nb_intercept_fit),\n             statistic = c(\"conf.int\",\n                           \"s.e. = {std.error}\", \n                           \"t = {statistic}\",\n                           \"p = {p.value}\"),\n             coef_omit = \"Intercept\", \n             exponentiate = TRUE)\n\n\n\nTabelle 41.4— Modellvergleich mit den vier Modellen. Wir schauen in wie weit sich die Koeffizienten und Modelgüten für die einzelnen Modelle im direkten Vergleich zum vollen Modell verändert haben.\n\n\n ZeroInfl Intercept\n ZeroInfl rain\n ZeroInfl rain+pop\nNegBinom intercept\n\n\n\ncount_grpcover\n2.031\n1.595\n1.611\n2.031\n\n\n\n[1.245, 3.313]\n[0.951, 2.675]\n[0.912, 2.845000e+00]\n[1.245, 3.313]\n\n\n\ns.e. = 0.507\ns.e. = 0.421\ns.e. = 0.468\ns.e. = 0.507\n\n\n\nt = 2.839\nt = 1.771\nt = 1.642\nt = 2.839\n\n\n\np = 0.005\np = 0.077\np = 0.101\np = 0.005\n\n\ncount_popfar\n2.591\n1.844\n\n2.591\n\n\n\n[1.558, 4.310]\n[1.069, 3.183]\n\n[1.558, 4.310]\n\n\n\ns.e. = 0.673\ns.e. = 0.513\n\ns.e. = 0.673\n\n\n\nt = 3.667\nt = 2.199\n\nt = 3.667\n\n\n\np =\np = 0.028\n\np =\n\n\ncount_raindry\n0.308\n\n\n0.308\n\n\n\n[0.168, 0.564]\n\n\n[0.168, 0.564]\n\n\n\ns.e. = 0.095\n\n\ns.e. = 0.095\n\n\n\nt = −3.816\n\n\nt = −3.816\n\n\n\np =\n\n\np =\n\n\ncount_rainwet\n0.135\n\n\n0.135\n\n\n\n[0.059, 0.310]\n\n\n[0.059, 0.310]\n\n\n\ns.e. = 0.057\n\n\ns.e. = 0.057\n\n\n\nt = −4.726\n\n\nt = −4.726\n\n\n\np =\n\n\np =\n\n\nzero_raindry\n\n27.939\n98.196\n\n\n\n\n\n[0.443, 1760.078]\n[0.000, 7.610178e+07]\n\n\n\n\n\ns.e. = 59.059\ns.e. = 679.400\n\n\n\n\n\nt = 1.575\nt = 0.663\n\n\n\n\n\np = 0.115\np = 0.507\n\n\n\nzero_rainwet\n\n83.713\n402.409\n\n\n\n\n\n[1.284, 5459.081]\n[0.000, 4.187125e+08]\n\n\n\n\n\ns.e. = 178.434\ns.e. = 2844.677\n\n\n\n\n\nt = 2.077\nt = 0.848\n\n\n\n\n\np = 0.038\np = 0.396\n\n\n\nzero_popfar\n\n\n0.148\n\n\n\n\n\n\n[0.022, 1.000000e+00]\n\n\n\n\n\n\ns.e. = 0.144\n\n\n\n\n\n\nt = −1.960\n\n\n\n\n\n\np = 0.050\n\n\n\nNum.Obs.\n60\n60\n60\n60\n\n\nR2\n0.620\n0.477\n0.454\n0.620\n\n\nR2 Adj.\n0.585\n0.449\n0.435\n0.585\n\n\nAIC\n157.3\n167.2\n167.4\n159.3\n\n\nBIC\n169.8\n179.8\n180.0\n173.9\n\n\nRMSE\n1.27\n1.27\n1.32\n1.27\n\n\n\n\n\n\n\n\nDie beiden Intercept Modelle haben die kleinsten \\(AIC\\)-Werte der vier Modelle. Darüber hinaus haben dann beide Modelle auch die höchsten \\(R^2_{adj}\\) Werte. Beide Modelle erklären also im Verhältnis viel Varianz mit 58.5%. Auch ist der \\(RMSE\\) Wert als Fehler bei beiden Modellen am kleinsten. Damit haben wir die Qual der Wahl, welches Modell wir nehmen. Ich würde das negative binominal Modell nehmen. Wir haben ins unseren Daten vermutlich eine Zeroinflation sowie eine Overdispersion vorliegen. Daher bietest es sich an, beides in einer negativen binominalen Regression zu berücksichtigen. Zwar sind die beiden Intercept Modelle in diesem Beispielfall von den Koeffizienten fast numerisch gleich, aber das hat eher mit dem reduzierten Beispiel zu tun, als mit dem eigentlichen Modell. In unserem Fall ist die Overdispersion nicht so extrem.\nWie sehe den unser negative binominal Modell aus, wenn wir mit dem Modell einmal die zu erwartenden Eidechsen vorhersagen würden? Auch das kann helfen um abzuschätzen, ob das Modelle einigermaßen funktioniert hat. Wir haben ja hier den Vorteil, dass wir nur mit kategorialen Daten arbeiten. Wir haben keine kontiniuerlichen Variablen vorliegen und darüber hinaus auch nicht so viele Variablen insgesamt.\nDaher bauen wir uns mit expand_grid() erstmal einen Datensatz, der nur aus den Faktorkombinationen besteht. Wir haben also nur eine Beobachtung je Faktorkombination. Danach nutzen wir die Daten einmal in der Funktion predict() um uns die vorhergesagten Eidechsen nach dem gefitten Modell wiedergeben zu lassen.\n\nnewdata_tbl &lt;- expand_grid(grp = factor(1:2, labels = c(\"open\", \"cover\")),\n                           rain = factor(1:3, labels = c(\"no\", \"dry\", \"wet\")),\n                           pop = factor(1:2, labels = c(\"near\", \"far\")))\n\npred_lizards &lt;- predict(lizard_zero_nb_intercept_fit, newdata = newdata_tbl) \n  \nnewdata_tbl &lt;- newdata_tbl %&gt;% \n  mutate(lizard = pred_lizards)\n\nNachdem wir in dem Datensatz newdata_tbl nun die vorhergesagten Eidechsen haben, können wir uns jetzt in der Abbildung 41.5 die Zusammenhänge nochmal anschauen.\n\nggplot(newdata_tbl, aes(x = rain, y = lizard, colour = grp, group = grp)) +\n  theme_bw() +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ pop) +\n  labs(x = \"Feuchtigkeit nach Regen\", y = \"Anzahl der gezählten Eidechsen\",\n       color = \"Gruppe\") +\n  scale_color_okabeito()\n\n\n\nAbbildung 41.5— Scatterplot der vorhergesagten Eidechsen in den Habitaten (grp), der Feuchtigkeit des Bodens nach Regen und dem Abstand zur nächsten Ortschaft.\n\n\n\nWir erkennen, dass mit der Erhöhung der Feuchtigkeit die Anzahl an aufgefundenen Eidechsen sinkt. Der Effekt ist nicht mehr so stark, wenn es schon einmal geregnet hat. Ebenso macht es einen Unterschied, ob wir nahe einer Siedlung sind oder nicht. Grundsätzlich finden wir immer mehr Eidechsen in geschützten Habitaten als in offenen Habitaten."
  },
  {
    "objectID": "stat-modeling-poisson.html#referenzen",
    "href": "stat-modeling-poisson.html#referenzen",
    "title": "41  Poisson Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "stat-modeling-multinom.html#annahmen-an-die-daten",
    "href": "stat-modeling-multinom.html#annahmen-an-die-daten",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.1 Annahmen an die Daten",
    "text": "42.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Multinomialverteilung.\nIm folgenden Kapitel zu der multinomialen / ordinalen logistischen linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 37 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Multinomialverteilung. Damit finden wir im Outcome im Falle der multinomialen logistischen linearen Regression ungeordnete Kategorien und im Falle der ordinalen logistischen linearen Regression geordnete Kategorien.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen."
  },
  {
    "objectID": "stat-modeling-multinom.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-multinom.html#genutzte-r-pakete-für-das-kapitel",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.2 Genutzte R Pakete für das Kapitel",
    "text": "42.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, gtsummary,\n               ordinal, janitor, MASS, nnet, flextable,\n               emmeans, multcomp, ordinal, see, scales,\n               janitor)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-multinom.html#daten",
    "href": "stat-modeling-multinom.html#daten",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.3 Daten",
    "text": "42.3 Daten\nIm Folgenden wollen wir uns die Daten von den infizierten Ferkeln noch einmal anschauen. Wir nehmen als Outcome die Spalte frailty und damit die Gebrechlichkeit der Ferkel. Die Spalte ordnen wir einmal nach robust, pre-frail und frail. Wobei robust ein gesundes Ferkel beschreibt und frail ein gebrechliches Ferkel. Damit wir später die Richtung des Effekts richtig interpretieren können, müssen wir von gut nach schlecht sortieren. Das brauchen wir nicht, wenn wir Boniturnoten haben, dazu mehr in einem eigenen Abschnitt. Wir bauen uns dann noch einen Faktor mit ebenfalls der Spalte frailty in der wir so tun, als gebe es diese Ordnung nicht. Wir werden dann die ordinale Regression mit dem Outcome frailty_ord rechnen und die multinominale Regression dann mit dem Outcome frailty_fac durchführen.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") %&gt;%\n  mutate(frailty_ord = ordered(frailty, levels = c(\"robust\", \"pre-frail\", \"frail\")),\n         frailty_fac = as_factor(frailty)) %&gt;% \n  select(-infected)\n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der Tabelle 42.1 an.\n\n\n\n\nTabelle 42.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\nfrailty_ord\nfrailty_fac\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\nrobust\nrobust\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\nrobust\nrobust\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\nrobust\nrobust\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.15\n19.05\n4.35\nrobust\nrobust\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n55.38\n18.44\n5.27\nrobust\nrobust\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n60.29\n18.42\n4.78\nrobust\nrobust\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\npre-frail\n55.32\n19.75\n3.92\npre-frail\npre-frail\n\n\n56\nmale\nwest\n13.91\n20.8\nfrail\n58.37\n17.28\n7.44\nfrail\nfrail\n\n\n57\nmale\nnorthwest\n12.49\n21.95\npre-frail\n56.66\n16.86\n2.44\npre-frail\npre-frail\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\nrobust\nrobust\n\n\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\nrobust\nrobust\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\nrobust\nrobust\n\n\n\n\n\n\nDas wären dann die Daten, die wir für unsere Modelle dann brauchen. Schauen wir mal was wir jetzt bei der ordinalen Regression herausbekommen."
  },
  {
    "objectID": "stat-modeling-multinom.html#sec-ordinal",
    "href": "stat-modeling-multinom.html#sec-ordinal",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.4 Ordinale logistische Regression",
    "text": "42.4 Ordinale logistische Regression\nEs gibt sicherlich einiges an Paketen in R um eine ordinale Regression durchzuführen. Ich nutze gerne die Funktion polr aus dem R Paket MASS. Daneben gibt es auch noch das R Paket ordinal mit der Funktion clm(), die wir dann noch im Anschluss besprechen werden. Ich nutze jetzt erstmal die Funktion polr, da wir hier noch eine externe Referenz haben, die uns noch detailliertere Informationen liefern kann.\n\n\nIch verweise gerne hier auf das tolle Tutorium Ordinal Logistic Regression | R Data Analysis Examples. Hier erfährst du noch mehr über die Analyse der ordinalen logistischen Regression.\nWir schon erwähnt sparen wir usn die mathematischen Details und utzen gleich die Funktion polr auf unserem Outcome frailty. Wir müssen keine Verteilungsfamilie extra angeben, dass haben wir schon mit der Auswahl der Funktion getan. Die Funktion polr kann nur eine ordinale Regression rechnen und wird einen Fehler ausgeben, wenn das Outcome \\(y\\) nicht passt.\n\nologit_fit &lt;- polr(frailty_ord ~ age + sex + location + activity + crp + \n                     bloodpressure + weight + creatinin, \n                   data = pig_tbl)\n\nSchauen wir uns einmal die Ausgabe des Modellfits der ordinalen Regression mit der Funktion summary() an. Wir sehen eine Menge Zahlen und das wichtigste für uns ist ja, dass wir zum einen Wissen, dass wir auch die ordinale Regression auf der \\(link\\)-Funktion rechnen. Wir erhalten also wieder eine Transformation des Zusammenhangs zurück, wie wir es schon bei der Poisson Regression sowie bei der logistischen Regression hatten.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\n\nologit_fit %&gt;% summary()\n\nCall:\npolr(formula = frailty_ord ~ age + sex + location + activity + \n    crp + bloodpressure + weight + creatinin, data = pig_tbl)\n\nCoefficients:\n                     Value Std. Error t value\nage                0.03191    0.02168  1.4718\nsexmale            0.06970    0.26447  0.2635\nlocationnortheast -0.27291    0.28063 -0.9725\nlocationnorthwest  0.07598    0.24883  0.3054\nlocationwest       0.24400    0.27285  0.8942\nactivity          -0.06970    0.07267 -0.9591\ncrp                0.06499    0.06820  0.9529\nbloodpressure     -0.05051    0.03037 -1.6632\nweight             0.06190    0.06649  0.9310\ncreatinin         -0.01100    0.06854 -0.1605\n\nIntercepts:\n                 Value   Std. Error t value\nrobust|pre-frail  0.6494  3.0697     0.2116\npre-frail|frail   2.4716  3.0724     0.8044\n\nResidual Deviance: 794.6938 \nAIC: 818.6938 \n\n\nUnsere Ausgabe teilt sich in zwei Teile auf. In dem oberen Teil sehen wir die Koeffizienten des Modells zusammen mit dem Fehler und der Teststatistik. Was wir nicht sehen, ist ein \\(p\\)-Wert. Die Funktion rechnet uns keinen Signifikanztest aus. Das können wir aber gleich selber machen. In dem Abschnitt Intercepts finden wir die Werte für die Gruppeneinteilung auf der link-Funktion wieder. Wir transformieren ja unsere drei Outcomekategorien in einen kontinuierliche Zahlenzusammenhang. Trotzdem müssen ja die drei Gruppen auch wieder auftauchen. In dem Abschnitt Intercepts finden wir die Grenzen für die drei Gruppen auf der link-Funktion.\n\n\nWir gibt auch ein Tutorial für How do I interpret the coefficients in an ordinal logistic regression in R?\nBerechnen wir jetzt einmal die \\(p\\)-Werte per Hand. Dafür brauchen wir die absoluten Werte aus der t value Spalte aus der summary des Modellobjekts. Leider ist die Spalte nicht schön formatiert und so müssen wir uns etwas strecken um die Koeffizienten sauber aufzuarbeiten. Wir erhalten dann das Objekt coef_tbl wieder.\n\ncoef_tbl &lt;- summary(ologit_fit) %&gt;% \n  coef %&gt;% \n  as_tibble(rownames = \"term\") %&gt;% \n  clean_names() %&gt;% \n  mutate(t_value = abs(t_value))\n\ncoef_tbl\n\n# A tibble: 12 × 4\n   term                value std_error t_value\n   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 age                0.0319    0.0217   1.47 \n 2 sexmale            0.0697    0.264    0.264\n 3 locationnortheast -0.273     0.281    0.973\n 4 locationnorthwest  0.0760    0.249    0.305\n 5 locationwest       0.244     0.273    0.894\n 6 activity          -0.0697    0.0727   0.959\n 7 crp                0.0650    0.0682   0.953\n 8 bloodpressure     -0.0505    0.0304   1.66 \n 9 weight             0.0619    0.0665   0.931\n10 creatinin         -0.0110    0.0685   0.161\n11 robust|pre-frail   0.649     3.07     0.212\n12 pre-frail|frail    2.47      3.07     0.804\n\n\nUm die Fläche rechts von dem \\(t\\)-Wert zu berechnen, können wir zwei Funktionen nutzen. Die Funktion pnorm() nimmt eine Standradnormalverteilung an und die Funktion pt() vergleicht zu einer \\(t\\)-Verteilung. Wenn wir rechts von der Verteilung schauen wollen, dann müssen wir die Option lower.tail = FALSE wählen. Da wir auch zweiseitig statistisch Testen, müssen wir den ausgerechneten \\(p\\)-Wert mal zwei nehmen. Hier einmal als Beispiel für den \\(t\\)-Wert von \\(1.96\\). Mit pnorm(1.96, lower.tail = FALSE) * 2 erhalten wir \\(0.05\\) als Ausgabe. Das ist unser \\(p\\)-Wert. Was uns ja nicht weiter überrascht. Denn rechts neben dem Wert von \\(1.96\\) in einer Standardnormalverteilung ist ja \\(0.05\\). Wenn wir einen \\(t\\)-Test rechnen würden, dann müssten wir noch die Freiheitsgrade df mit angeben. Mit steigendem \\(n\\) nähert sich die \\(t\\)-Verteilung der Standardnormalverteilung an. Wir haben mehr als \\(n = 400\\) Beobachtungen, daher können wir auch df = 400 setzen. Da kommt es auf eine Zahl nicht an. Wir erhalten mit pt(1.96, lower.tail = FALSE, df = 400) * 2 dann eine Ausgabe von \\(0.0507\\). Also fast den gleichen \\(p\\)-Wert.\nIm Folgenden setzte ich die Freiheitsgrade df = 3 dammit wir was sehen. Bei so hohen Fallzahlen wir in unserem beispiel würden wir sonst keine Unterschiede sehen.\n\ncoef_tbl %&gt;% \n  mutate(p_n = pnorm(t_value, lower.tail = FALSE) * 2,\n         p_t = pt(t_value, lower.tail = FALSE, df = 3) * 2) %&gt;% \n  mutate(across(where(is.numeric), round, 3))\n\n# A tibble: 12 × 6\n   term               value std_error t_value   p_n   p_t\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 age                0.032     0.022   1.47  0.141 0.237\n 2 sexmale            0.07      0.264   0.264 0.792 0.809\n 3 locationnortheast -0.273     0.281   0.973 0.331 0.403\n 4 locationnorthwest  0.076     0.249   0.305 0.76  0.78 \n 5 locationwest       0.244     0.273   0.894 0.371 0.437\n 6 activity          -0.07      0.073   0.959 0.338 0.408\n 7 crp                0.065     0.068   0.953 0.341 0.411\n 8 bloodpressure     -0.051     0.03    1.66  0.096 0.195\n 9 weight             0.062     0.066   0.931 0.352 0.421\n10 creatinin         -0.011     0.069   0.161 0.872 0.883\n11 robust|pre-frail   0.649     3.07    0.212 0.832 0.846\n12 pre-frail|frail    2.47      3.07    0.804 0.421 0.48 \n\n\nDamit haben wir einmal händisch uns die \\(p\\)-Werte ausgerechnet. Jetzt könnte man sagen, dass ist ja etwas mühselig. Gibt es da nicht auch einen einfacheren Weg? Ja wir können zum einen die Funktion tidy() nutzen um die 95% Konfidenzintervalle und die exponierten Effektschätzer aus der ordinalen Regresssion zu erhalten. Wir erhalten aber wieder keine \\(p\\)-Werte sondern müssten uns diese \\(p\\)- Werte dann wieder selber berechnen.\n\nologit_fit %&gt;% \n  tidy(conf.int = TRUE, exponentiate = TRUE) %&gt;% \n  select(-coef.type)\n\n# A tibble: 12 × 6\n   term              estimate std.error statistic conf.low conf.high\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 age                  1.03     0.0217     1.47     0.990      1.08\n 2 sexmale              1.07     0.264      0.264    0.639      1.80\n 3 locationnortheast    0.761    0.281     -0.973    0.437      1.32\n 4 locationnorthwest    1.08     0.249      0.305    0.662      1.76\n 5 locationwest         1.28     0.273      0.894    0.747      2.18\n 6 activity             0.933    0.0727    -0.959    0.808      1.08\n 7 crp                  1.07     0.0682     0.953    0.934      1.22\n 8 bloodpressure        0.951    0.0304    -1.66     0.896      1.01\n 9 weight               1.06     0.0665     0.931    0.934      1.21\n10 creatinin            0.989    0.0685    -0.161    0.865      1.13\n11 robust|pre-frail     1.91     3.07       0.212   NA         NA   \n12 pre-frail|frail     11.8      3.07       0.804   NA         NA   \n\n\nUm all dieses Berechnen zu umgehen, können wir dann auch die Funktion model_parameters() nutzen. Hier berechnen wir dann die \\(p\\)-Wert mit \\(df = 400\\) aus einer \\(t\\)-Verteilung. Damit umgehen wir das Problem, dass unser Modellfit keine \\(p\\)-Werte liefert.\n\nologit_fit %&gt;% \n  model_parameters() \n\n# alpha\n\nParameter        | Log-Odds |   SE |        95% CI | t(400) |     p\n-------------------------------------------------------------------\nrobust|pre-frail |     0.65 | 3.07 | [-5.39, 6.68] |   0.21 | 0.833\npre-frail|frail  |     2.47 | 3.07 | [-3.57, 8.51] |   0.80 | 0.422\n\n# beta\n\nParameter            | Log-Odds |   SE |        95% CI | t(400) |     p\n-----------------------------------------------------------------------\nage                  |     0.03 | 0.02 | [-0.01, 0.07] |   1.47 | 0.142\nsex [male]           |     0.07 | 0.26 | [-0.45, 0.59] |   0.26 | 0.792\nlocation [northeast] |    -0.27 | 0.28 | [-0.83, 0.27] |  -0.97 | 0.331\nlocation [northwest] |     0.08 | 0.25 | [-0.41, 0.56] |   0.31 | 0.760\nlocation [west]      |     0.24 | 0.27 | [-0.29, 0.78] |   0.89 | 0.372\nactivity             |    -0.07 | 0.07 | [-0.21, 0.07] |  -0.96 | 0.338\ncrp                  |     0.06 | 0.07 | [-0.07, 0.20] |   0.95 | 0.341\nbloodpressure        |    -0.05 | 0.03 | [-0.11, 0.01] |  -1.66 | 0.097\nweight               |     0.06 | 0.07 | [-0.07, 0.19] |   0.93 | 0.352\ncreatinin            |    -0.01 | 0.07 | [-0.15, 0.12] |  -0.16 | 0.873\n\n\nIn Tabelle 42.2 sehen wir nochmal die Ergebnisse der ordinalen Regression einmal anders aufgearbeitet. Wir aber schon bei der Funktion tidy() fehlen in der Tabelle die \\(p\\)-Werte. Wir können aber natürlich auch eine Entscheidung über die 95% Konfidenzintervalle treffen. Wenn die 1 mit im 95% Konfidenzintervall ist, dann können wir die Nullhypothese nicht ablehnen.\n\nologit_fit %&gt;% \n  tbl_regression(exponentiate = TRUE) %&gt;% \n  as_flex_table()\n\n\n\n\n\n\nTabelle 42.2—  Tabelle der Ergebnisse der ordinalen Regression. \n\nCharacteristic\nOR1\n95% CI1\n\n\n\nage\n1.03\n0.99, 1.08\n\n\nsex\n\n\n\n\nfemale\n—\n—\n\n\nmale\n1.07\n0.64, 1.80\n\n\nlocation\n\n\n\n\nnorth\n—\n—\n\n\nnortheast\n0.76\n0.44, 1.32\n\n\nnorthwest\n1.08\n0.66, 1.76\n\n\nwest\n1.28\n0.75, 2.18\n\n\nactivity\n0.93\n0.81, 1.08\n\n\ncrp\n1.07\n0.93, 1.22\n\n\nbloodpressure\n0.95\n0.90, 1.01\n\n\nweight\n1.06\n0.93, 1.21\n\n\ncreatinin\n0.99\n0.86, 1.13\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nWi es im gazen Kapitel schon durchscheint, die Interpreation der \\(OR\\) aus einer ordinalen Regression ist nicht einfach, geschweige den intuitiv. Was wir haben ist der Trend. Wir haben unser Outcome von robust zu frail sortiert und damit von gut nach schlecht. Wir können so die Richtung der Variablen in unserem Modell interpretieren. Das heißt, dass männliche Ferkel eher von einer Gebrechlichkeit betroffen sind als weibliche Ferkel. Oder wir sagen, dass ein ansteigender CRP Wert führt zu weniger Gebrechlichkeit. Auf diesem Niveau lassen sich die \\(OR\\) einer ordinalen Regression gut interpretieren."
  },
  {
    "objectID": "stat-modeling-multinom.html#cumulative-link-models-clm-für-ordinale-daten",
    "href": "stat-modeling-multinom.html#cumulative-link-models-clm-für-ordinale-daten",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.5 Cumulative Link Models (CLM) für ordinale Daten",
    "text": "42.5 Cumulative Link Models (CLM) für ordinale Daten\nJetzt kommen wir nochmal zu dem Fall, dass wir Boniturdaten vorliegen habe. Das heißt, wir bauen uns flux wieder einen Datensatz mit drei Blöcken mit jeweils drei Wiederholungen. Wir haben Weizen angepflanzt und bonitieren die Weizenpflanzen nach der Likert Skala. Dabei bedeutet dann eine 1 ein schlechte Note und eine 9 die best mögliche Note. Wir hätten natürlich hier auch einen Kurskal-Wallis-Test rechnen können und dann im Anschluss einen paarweisen Wilcoxon Test. Nun modellieren wir hier aber die Boniturnoten mal mit einer ordinalen Regression und rechnen den anschließenden Gruppenvergleich dann mit dem R Paket emmeans.\n\n\nWir finden auch ein Tutorial zu Introduction to Cumulative Link Models (CLM) for Ordinal Data.\nUnser Datensatz grade_tbl enthält den Faktor block mit drei Levels sowie den Faktor variety mit fünf Leveln. Jedes Level repränsentiert dabei eine Weizensorte.\n\ngrade_tbl &lt;- tibble(block = rep(c(\"I\", \"II\", \"III\"), each = 3),\n                    A = c(2,3,4,3,3,2,4,2,1),\n                    B = c(7,9,8,9,7,8,9,6,7),\n                    C = c(6,5,5,7,5,6,4,7,6),\n                    D = c(2,3,1,2,1,1,2,2,1),\n                    E = c(4,3,7,5,6,4,5,7,5)) %&gt;%\n  gather(key = variety, value = grade, A:E) %&gt;% \n  mutate(grade_ord = ordered(grade))\n\nWir schauen uns nochmal den Datensatz an und sehen, dass wir einmal die Spalte grade als numerische Spalte vorliegen haben udn einmal als geordneten Faktor. Wir brauchen die numerische Spalte um die Daten besser in ggplot() darstellen zu können.\n\ngrade_tbl\n\n# A tibble: 45 × 4\n   block variety grade grade_ord\n   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;ord&gt;    \n 1 I     A           2 2        \n 2 I     A           3 3        \n 3 I     A           4 4        \n 4 II    A           3 3        \n 5 II    A           3 3        \n 6 II    A           2 2        \n 7 III   A           4 4        \n 8 III   A           2 2        \n 9 III   A           1 1        \n10 I     B           7 7        \n# ℹ 35 more rows\n\n\nIn Abbildung 42.1 sehen wir einmal die Daten als Dotplot dargestellt. Auf der x-Achse sind die Weizensorten und auf der y-Achse die Boniturnoten. Ich habe noch die zusätzlichen Linien für jede einzelne Note mit eingezeichnet.\n\nggplot(grade_tbl, aes(variety, grade, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir='center', \n               position=position_dodge(0.6), dotsize = 0.75) +\n  scale_y_continuous(breaks = 1:9, limits = c(1,9)) +\n  scale_fill_okabeito() \n\n\n\nAbbildung 42.1— Dotplot des Datenbeispiels für die Bonitur von fünf Weizensorten.\n\n\n\nJetzt können wir schon die Funktion clm() aus dem R Paket ordinal verwenden um die ordinale Regression zu rechnen. Wir haben in dem Paket ordinal noch weitere Modelle zu Verfügung mit denen wir auch komplexere Designs bis hin zu linearen gemischten Modellen für eine ordinale Regresssion rechnen können. Da wir mit Boniturnoten als Outcome arbeiten setzen wir auch die Option threshold = \"symmetric\". Damit teilen wir der Funktion clm() mit, dass wir es mit einer symmetrischen Notenskala zu tun haben. Wenn du das nicht hast, dass kannst du die Option auch of \"flexible\" stellen. Dann wird eine nicht symmetrische Verteilung des Outcomes angenommen.\n\nclm_fit &lt;- clm(grade_ord ~ variety + block, data = grade_tbl,\n               threshold = \"symmetric\")\n\nWir können uns dann den Fit des Modells wieder in der Funktion model_parameters() einmal anschauen. Wir brauchen aber die Ausgabe nicht weiter. Wir werden Koeffizienten des Modells jetzt verwenden um die Gruppenvergleiche zu rechnen. Ich habe hier einmal die \\(OR\\) über die Option exponentiate = TRUE mir ausgeben lassen. Leider sind die \\(OR\\) so alleine nicht zu interpretieren.\n\nclm_fit %&gt;% \n  model_parameters(exponentiate = TRUE)\n\n# Intercept\n\nParameter | Odds Ratio |     SE |          95% CI |    z |      p\n-----------------------------------------------------------------\ncentral 1 |      22.02 |  20.78 | [ 3.47, 139.98] | 3.28 | 0.001 \ncentral 2 |     101.42 | 109.71 | [12.17, 845.18] | 4.27 | &lt; .001\nspacing 1 |       4.30 |   1.89 | [ 1.82,  10.16] | 3.33 | &lt; .001\nspacing 2 |      35.04 |  25.42 | [ 8.46, 145.23] | 4.90 | &lt; .001\nspacing 3 |     169.31 | 142.84 | [32.40, 884.76] | 6.08 | &lt; .001\n\n# Location Parameters\n\nParameter   | Odds Ratio |      SE |             95% CI |     z |      p\n------------------------------------------------------------------------\nvariety [B] |    5917.41 | 9556.45 | [249.73, 1.40e+05] |  5.38 | &lt; .001\nvariety [C] |     137.83 |  161.11 | [ 13.94,  1362.41] |  4.21 | &lt; .001\nvariety [D] |       0.14 |    0.13 | [  0.02,     0.87] | -2.11 | 0.035 \nvariety [E] |      57.06 |   62.60 | [  6.64,   490.03] |  3.69 | &lt; .001\nblock [II]  |       0.95 |    0.64 | [  0.26,     3.52] | -0.07 | 0.942 \nblock [III] |       0.89 |    0.63 | [  0.22,     3.56] | -0.17 | 0.865 \n\n\nEs ist auch möglich auf dem Modellfit eine ANOVA zu rechnen. Wir machen das hier einmal, aber wir erwaten natürlich einen signifikanten Effekt von der Sorte. Die Signifikanz konnten wir ja schon oben im Dortplot sehen.\n\nanova(clm_fit)\n\nType I Analysis of Deviance Table with Wald chi-square tests\n\n        Df   Chisq Pr(&gt;Chisq)    \nvariety  4 37.9927  1.124e-07 ***\nblock    2  0.0292     0.9855    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIch hatte ja eben geschrieben, dass wir die Effektschätzer nicht nutzen. Wir können mit den \\(OR\\) aus dem Modell nichts anfangen. Stattdessen nutzen wir die Schätzer der link-Funktion, also die Log Odds, um den paarweisen Gruppenvergleich zu rechnen. Wir nutzen dafür die Funktion emmeans() und lassen uns das compact letter display über die Funktion cld() wiedergeben. Ich habe dann noch die Ausgabe einmal nach den Sorten sortiert und nicht nach dem compact letter display. Dadurch lassen sich die Buchstaben besser zum Dotplot vergleichen.\n\nclm_fit %&gt;% \n  emmeans(~ variety) %&gt;% \n  cld(Letters = letters) %&gt;% \n  arrange(variety)\n\n variety emmean    SE  df asymp.LCL asymp.UCL .group\n A       -3.912 0.887 Inf    -5.650     -2.17  a    \n B        4.774 0.966 Inf     2.881      6.67    c  \n C        1.014 0.623 Inf    -0.206      2.23   b   \n D       -5.848 1.010 Inf    -7.828     -3.87  a    \n E        0.132 0.639 Inf    -1.120      1.38   b   \n\nResults are averaged over the levels of: block \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 5 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nWir erinnern uns, gleiche Buchstaben heißen kein Unterschied zwischen den Weizensorten. Damit ist die Weizensorte A gleich der Weizensorte D. Die Weizensorte C ist gleich der Weizensorte E. Am Ende unterscheidet sich die Weizensorte B von allen anderen Sorten. Wir können aber die Werte in der Spalte emmean nicht als Notenunterschied interpretieren. Dafür müssen wir dann den Median für die Gruppen berechnen. Wir können dann das compact letter display händisch ergänzen.\n\ngrade_tbl %&gt;% \n  group_by(variety) %&gt;% \n  summarise(median(grade)) %&gt;% \n  mutate(cld = c(\"a\", \"  c\", \" b \", \"a  \", \" b \"))\n\n# A tibble: 5 × 3\n  variety `median(grade)` cld  \n  &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;\n1 A                     3 \"a\"  \n2 B                     8 \"  c\"\n3 C                     6 \" b \"\n4 D                     2 \"a  \"\n5 E                     5 \" b \"\n\n\nDu könntest dir auch die Buchstaben des compact letter display aus einem Objekt ziehen und nicht händisch übertragen. Ich habe mir hier aber einen Schritt gespart."
  },
  {
    "objectID": "stat-modeling-multinom.html#sec-multinom",
    "href": "stat-modeling-multinom.html#sec-multinom",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.6 Multinomiale logistische Regression",
    "text": "42.6 Multinomiale logistische Regression\nWas machen wir in eine multinomialen logistische Regression? Im Gegensatz zu der ordinalen Regression haben wir in der multinominalen Regression keine Ordnung in unserem Outcome. Das macht die Sache dann schon eine Nummer komplizierter. Und wir lösen dieses Problem indem wir ein Level des Outcomes oder eben eine Kategorie des Outcomes als Referenz definieren. Dann haben wir wieder unsere Ordnung drin. Und die Definition der Referenz ist auch manchmal das schwerste Unterfangen. Wenn ich keine Ordnung in meinem Outcome habe, wie soll ich dann die Referenz bestimmen? Aber das ist dann immer eine Frage an den konkreten Datensatz. Hier basteln wir uns ja die Fragestellung so hin, dass es passt.\n\n\nIch verweise gerne hier auf das tolle Tutorium Multinomial Logistic Regression | R Data Analysis Examples. Hier erfährst du noch mehr über die Analyse der multinominale logistischen Regression.\nUm eine Referenz in dem Outcome zu definieren nutzen wir die Funktion relevel() und setzen als unsere Referenz das Level frail aus unserem Outcome frailty. Wir hätten auch jedes andere Level als Referenz nehmen können. Zu dieser Referenz werden wir jetzt unser Modell anpassen. Ich nehme immer als Referenz das schlechteste im Sinne von nicht gut. In unserem Fall ist das eben das Level frail.\n\npig_tbl &lt;- pig_tbl %&gt;% \n  mutate(frailty_fac = relevel(frailty_fac, ref = \"frail\"))\n\nNachdem wir unsere Referenz definiert haben, können wir wieder recht einfach mit der Funktion multinom() aus dem Paket nnet die multinominalen Regression rechnen. Ich mache keinen Hehl daraus. Ich mag die Funktion nicht, da die Ausgabe der Funktion sehr unsortiert ist und uns nicht gerade die Arbeit erleichtert. Auch schweigt die Funktion nicht, sondern muss immer eine Ausgabe wiedergeben. Finde ich sehr unschön.\n\nmultinom_fit &lt;- multinom(frailty_fac ~ age + sex + location + activity + crp + bloodpressure + weight + creatinin, \n                         data = pig_tbl)\n\n# weights:  36 (22 variable)\ninitial  value 452.628263 \niter  10 value 403.652821\niter  20 value 392.117661\niter  30 value 391.553171\nfinal  value 391.549041 \nconverged\n\n\nDie Standardausgabe von multinom() hat wiederum keine \\(p\\)-Werte und wir könnten uns über die Funktion pnorm() wiederum aus den \\(t\\)-Werten unsere \\(p\\)-Werte berechnen. Leider erspart sich multinom() selbst den Schritt die \\(t\\)-Werte zu berechnen, so dass wir die \\(t\\)-Werte selber berechnen müssen. Nicht das es ein Problem wäre, aber schön ist das alles nicht. Im Folgenden siehst du dann einmal die Berechnung der \\(p\\)-Werte über die Berechnung der Teststatistik.\n\nz_mat &lt;- summary(multinom_fit)$coefficients/summary(multinom_fit)$standard.errors\np_n &lt;- (1 - pnorm(abs(z_mat), 0, 1)) * 2\np_n\n\n          (Intercept)       age    sexmale locationnortheast locationnorthwest\nrobust      0.9330415 0.8633078 0.16908717         0.4269151         0.6194343\npre-frail   0.7114684 0.1360834 0.03831569         0.7078459         0.6316654\n          locationwest   activity       crp bloodpressure    weight creatinin\nrobust       0.6632781 0.09080926 0.3594687     0.1421612 0.1134502 0.7118954\npre-frail    0.7383270 0.06115011 0.5643200     0.4895252 0.0683936 0.6178310\n\n\nJetzt müssten wir diese \\(pp\\)-Werte aus der Matrix noch mit unseren Koeffizienten verbauen und da hört es dann bei mir auf. Insbesondere da wir ja mit model_parameters() eine Funktion haben, die uns in diesem Fall wirklich gut helfen kann. Wir nehmen hier zwar die \\(t\\)-Verteilung an und haben damit leicht höre \\(p\\)-Werte, aber da wir eine so große Anzahl an Beobachtungen haben, fällt dieser Unterschied nicht ins Gewicht.\n\nmultinom_fit %&gt;% model_parameters(exponentiate = TRUE)\n\n# Response level: robust\n\nParameter            | Odds Ratio |   SE |           95% CI | t(390) |     p\n----------------------------------------------------------------------------\n(Intercept)          |       1.50 | 7.27 | [0.00, 20438.53] |   0.08 | 0.933\nage                  |       0.99 | 0.03 | [0.93,     1.06] |  -0.17 | 0.863\nsex [male]           |       0.54 | 0.24 | [0.23,     1.30] |  -1.38 | 0.170\nlocation [northeast] |       1.45 | 0.67 | [0.58,     3.61] |   0.79 | 0.427\nlocation [northwest] |       0.82 | 0.32 | [0.38,     1.77] |  -0.50 | 0.620\nlocation [west]      |       0.82 | 0.37 | [0.34,     1.99] |  -0.44 | 0.664\nactivity             |       1.22 | 0.14 | [0.97,     1.54] |   1.69 | 0.092\ncrp                  |       0.91 | 0.10 | [0.73,     1.12] |  -0.92 | 0.360\nbloodpressure        |       1.07 | 0.05 | [0.98,     1.18] |   1.47 | 0.143\nweight               |       0.84 | 0.09 | [0.68,     1.04] |  -1.58 | 0.114\ncreatinin            |       1.04 | 0.11 | [0.84,     1.29] |   0.37 | 0.712\n\n# Response level: pre-frail\n\nParameter            | Odds Ratio |   SE |          95% CI | t(390) |     p\n---------------------------------------------------------------------------\n(Intercept)          |       0.15 | 0.77 | [0.00, 3457.99] |  -0.37 | 0.712\nage                  |       1.06 | 0.04 | [0.98,    1.13] |   1.49 | 0.137\nsex [male]           |       0.38 | 0.18 | [0.16,    0.95] |  -2.07 | 0.039\nlocation [northeast] |       1.20 | 0.60 | [0.46,    3.18] |   0.37 | 0.708\nlocation [northwest] |       0.82 | 0.34 | [0.36,    1.85] |  -0.48 | 0.632\nlocation [west]      |       1.17 | 0.54 | [0.47,    2.90] |   0.33 | 0.739\nactivity             |       1.26 | 0.16 | [0.99,    1.61] |   1.87 | 0.062\ncrp                  |       0.94 | 0.11 | [0.75,    1.17] |  -0.58 | 0.565\nbloodpressure        |       1.04 | 0.05 | [0.94,    1.15] |   0.69 | 0.490\nweight               |       0.81 | 0.09 | [0.65,    1.02] |  -1.82 | 0.069\ncreatinin            |       1.06 | 0.12 | [0.84,    1.33] |   0.50 | 0.618\n\n\nWas sehen wir? Zuerst haben wir etwas Glück. Den unsere Referenzlevel macht dann doch Sinn. Wir vergleichen ja das Outcomelevel robust zu frail und das Outcomelevel pre-frail zu frail. Dann haben wir noch das Glück, dass durch unsere Ordnung dann auch frail das schlechtere Outcome ist, so dass wir die \\(OR\\) als Risiko oder als protektiv interpretieren können. Nehmen wir als Beispiel einmal die Variable crp. Der CRP Wert höht das Risiko für frail. Das macht schonmal so Sinn. Und zum anderen ist der Effekt bei dem Vergleich von pre-frail zu frail mit \\(1.16\\) nicht so große wie bei robust zu frail mit \\(1.26\\). Das macht auch Sinn. Deshalb passt es hier einigermaßen.\nIn Tabelle 42.3 sehen wir nochmal die Ausgabe von einer multinominalen Regression durch die Funktion tbl_regression() aufgearbeitet.\n\nmultinom_fit %&gt;% \n  tbl_regression(exponentiate = TRUE) %&gt;% \n  as_flex_table()\n\n\n\n\n\n\nTabelle 42.3—  Tabelle der Ergebnisse der multinominalen Regression. \n\nOutcome\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\nrobust\nage\n0.99\n0.93, 1.06\n0.9\n\n\n\nsex\n\n\n\n\n\n\nfemale\n—\n—\n\n\n\n\nmale\n0.54\n0.23, 1.29\n0.2\n\n\n\nlocation\n\n\n\n\n\n\nnorth\n—\n—\n\n\n\n\nnortheast\n1.45\n0.58, 3.60\n0.4\n\n\n\nnorthwest\n0.82\n0.38, 1.77\n0.6\n\n\n\nwest\n0.82\n0.34, 1.98\n0.7\n\n\n\nactivity\n1.22\n0.97, 1.54\n0.091\n\n\n\ncrp\n0.91\n0.73, 1.12\n0.4\n\n\n\nbloodpressure\n1.07\n0.98, 1.18\n0.14\n\n\n\nweight\n0.84\n0.68, 1.04\n0.11\n\n\n\ncreatinin\n1.04\n0.84, 1.29\n0.7\n\n\npre-frail\nage\n1.06\n0.98, 1.13\n0.14\n\n\n\nsex\n\n\n\n\n\n\nfemale\n—\n—\n\n\n\n\nmale\n0.38\n0.16, 0.95\n0.038\n\n\n\nlocation\n\n\n\n\n\n\nnorth\n—\n—\n\n\n\n\nnortheast\n1.20\n0.46, 3.17\n0.7\n\n\n\nnorthwest\n0.82\n0.37, 1.84\n0.6\n\n\n\nwest\n1.17\n0.47, 2.90\n0.7\n\n\n\nactivity\n1.26\n0.99, 1.61\n0.061\n\n\n\ncrp\n0.94\n0.75, 1.17\n0.6\n\n\n\nbloodpressure\n1.04\n0.94, 1.14\n0.5\n\n\n\nweight\n0.81\n0.65, 1.02\n0.068\n\n\n\ncreatinin\n1.06\n0.85, 1.33\n0.6\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nLeider wird die Sache mit einer multinominalen Regression sehr unangenehm, wenn wir wirklich nicht sortierbare Level im Outcome haben. Dann haben wir aber noch ein Möglichkeit der multinominalen Regression zu entkommen. Wir rechnen einfach separate logistische Regressionen. Die logistischen Regressionen können wir dann ja separat gut interpretieren."
  },
  {
    "objectID": "stat-modeling-multinom.html#logistische-regression-als-ausweg",
    "href": "stat-modeling-multinom.html#logistische-regression-als-ausweg",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "\n42.7 Logistische Regression als Ausweg",
    "text": "42.7 Logistische Regression als Ausweg\n\n\n\n\n\n\nBitte Beachten bei der Berechung über separate logistische Regressionen\n\n\n\nDurch die Verwendung von separaten logistischen Regressionen vermindern wir die Fallzahl je gerechneter Regression, so dass wir größere \\(p\\)-Werte erhalten werden als in einer multinominalen Regression. Oder andersherum, durch die verminderte Fallzahl in den separaten logistischen Regressionen haben wir eine geringere Power einen signifikanten Unterschied nachzuweisen.\n\n\nEs gibt den einen Ring um sich zu knechten. Und das ist die logistische Regression. Gut die logistische Regression hilft jetzt nicht, wenn es mit Boniturnoten zu tun hast, aber wenn wir wenige Level im Outcome haben. In unserem Fall haben wir ja drei Level vorliegen, da können wir dann jeweils ein Level rausschmeißen und haben dann nur noch ein binäres Outcome. Das ist auch die zentrale Idee. Wir entfernen immer alle Level bis wir nur noch zwei Level in unserem Outcome haben und rechnen für diese beiden Level dann eine logistische Regression.\nSchauen wir uns erstmal an, wie sich die Daten über die drei Kategorien in unserem Outcome verteilen. Wenn wir eine Kategorie im Outcome kaum vorliegen haben, könnten wir diese Daten vielleicht mit einer anderen Kategorie zusammenlegen oder aber müssen von unserer Idee hier Abstand nehmen.\n\npig_tbl$frailty_fac %&gt;% tabyl\n\n         .   n   percent\n     frail  55 0.1334951\n    robust 214 0.5194175\n pre-frail 143 0.3470874\n\n\nWir haben nicht so viele Beobachtungen in der Kategorie frail. Wir könnten also auch die beiden Faktorlevel pre-frail und frail zusammenlegen.\nDas R Paket forcats liefert sehr viele Funktion, die dir helfen Faktoren zu kodieren und zu ändern\n\npig_tbl$frailty_fac %&gt;% \n  fct_recode(frail_pre_frail = \"frail\", frail_pre_frail = \"pre-frail\") %&gt;% \n  tabyl\n\n               .   n   percent\n frail_pre_frail 198 0.4805825\n          robust 214 0.5194175\n\n\nDas ist jetzt aber nur eine Demonstration für die Zusammenlegung. Wir wollen jetzt trotzdem unsere drei logistischen Regressionen rechnen. Warum drei? Wir haben ja drei Level in unserem Outcome und wir werden jetzt uns drei Datensätze so bauen, dass in jdem Datensatz unser Outcome immer nur zwei Level hat. Die einzelnen Datensätze speichern wir dann in einer Liste.\n\npig_lst &lt;- list(robust_prefrail = filter(pig_tbl, frailty_fac %in% c(\"robust\", \"pre-frail\")),\n                robust_frail = filter(pig_tbl, frailty_fac %in% c(\"robust\", \"frail\")),\n                prefrail_frail = filter(pig_tbl, frailty_fac %in% c(\"pre-frail\", \"frail\")))\n\nWir können das auch fancy. Und das demonstriere ich dann mal hier. Wenn wir die Funktion combn() nutzen erhalten wir eine Liste mit allen zweier Kombinationen wieder. Diese Liste können wir dann in die Funktion map() stecken, die dann über die Liste unserer Kombinationen iteriert. Pro Liste filtern map() dann den Datensatz für uns heraus. Ja, ist ein wenig over the top, aber ich wollte das mal für mich mit map() ausprobieren und es passte hier so schön.\n\npig_fancy_lst &lt;- combn(c(\"robust\", \"pre-frail\", \"frail\"), 2, simplify = FALSE) %&gt;% \n  map(~filter(pig_tbl, frailty_fac %in% .x)) \n\nEgal wie du auf die Liste gekommen bist, wir müssen noch die überflüssigen Level droppen. Keine Ahnung was das deutsche Wort ist. Vermutlich ist das deutsche Wort dann entfernen. Dann können wir für jeden der Listeneinträge die logistische Regression rechnen. Am Ende lassen wir uns noch die exponierten Modellfits ausgeben. In der letzten Zeile entferne ich noch den Intercept von der Ausgabe des Modells. Den Intercept brauchen wir nun wirklich nicht.\n\npig_lst %&gt;% \n  map(~mutate(.x, frailty_fac = fct_drop(frailty_fac))) %&gt;% \n  map(~glm(frailty_fac ~ age + sex + location + activity + crp + bloodpressure + weight + creatinin, \n           data = .x, family = binomial)) %&gt;% \n  map(model_parameters, exponentiate = TRUE) %&gt;% \n  map(extract, -1, )\n\n$robust_prefrail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.06 | 0.03 | [1.01, 1.12] |  2.40 | 0.017\nsex [male]           |       0.70 | 0.21 | [0.39, 1.27] | -1.16 | 0.246\nlocation [northeast] |       0.80 | 0.26 | [0.42, 1.49] | -0.70 | 0.484\nlocation [northwest] |       0.97 | 0.28 | [0.55, 1.72] | -0.09 | 0.929\nlocation [west]      |       1.38 | 0.44 | [0.75, 2.57] |  1.03 | 0.302\nactivity             |       1.04 | 0.09 | [0.88, 1.23] |  0.48 | 0.631\ncrp                  |       1.03 | 0.08 | [0.88, 1.20] |  0.33 | 0.743\nbloodpressure        |       0.96 | 0.03 | [0.90, 1.03] | -1.04 | 0.300\nweight               |       0.97 | 0.08 | [0.83, 1.13] | -0.39 | 0.695\ncreatinin            |       1.02 | 0.08 | [0.87, 1.19] |  0.21 | 0.834\n\n$robust_frail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.00 | 0.04 | [0.93, 1.07] |  0.03 | 0.973\nsex [male]           |       0.52 | 0.23 | [0.22, 1.24] | -1.46 | 0.145\nlocation [northeast] |       1.45 | 0.68 | [0.59, 3.79] |  0.79 | 0.430\nlocation [northwest] |       0.85 | 0.33 | [0.39, 1.82] | -0.42 | 0.674\nlocation [west]      |       0.88 | 0.40 | [0.37, 2.18] | -0.28 | 0.780\nactivity             |       1.23 | 0.15 | [0.97, 1.58] |  1.69 | 0.090\ncrp                  |       0.92 | 0.10 | [0.75, 1.14] | -0.75 | 0.453\nbloodpressure        |       1.07 | 0.05 | [0.97, 1.18] |  1.36 | 0.172\nweight               |       0.84 | 0.10 | [0.67, 1.04] | -1.57 | 0.116\ncreatinin            |       1.04 | 0.11 | [0.84, 1.29] |  0.36 | 0.717\n\n$prefrail_frail\nParameter            | Odds Ratio |   SE |       95% CI |     z |     p\n-----------------------------------------------------------------------\nage                  |       1.05 | 0.04 | [0.98, 1.12] |  1.26 | 0.208\nsex [male]           |       0.40 | 0.19 | [0.15, 0.99] | -1.94 | 0.053\nlocation [northeast] |       1.12 | 0.57 | [0.42, 3.11] |  0.22 | 0.825\nlocation [northwest] |       0.77 | 0.33 | [0.33, 1.78] | -0.60 | 0.548\nlocation [west]      |       1.05 | 0.51 | [0.41, 2.74] |  0.11 | 0.914\nactivity             |       1.22 | 0.15 | [0.97, 1.55] |  1.69 | 0.092\ncrp                  |       0.95 | 0.11 | [0.74, 1.20] | -0.46 | 0.649\nbloodpressure        |       1.03 | 0.05 | [0.93, 1.14] |  0.57 | 0.566\nweight               |       0.84 | 0.09 | [0.67, 1.03] | -1.65 | 0.100\ncreatinin            |       1.04 | 0.13 | [0.82, 1.33] |  0.34 | 0.734\n\n\nEine Sache ist super wichtig zu wissen. Wie oben schon geschrieben, durch die Verwendung von separaten logistischen Regressionen vermindern wir die Fallzahl je Regression, so dass wir größere \\(p\\)-Werte erhalten werden, als in einer multinominalen Regression. Das ist der Preis, den wir dafür bezahlen müssen, dass wir besser zu interpretierende Koeffizienten erhalten. Und das ist auch vollkommen in Ordnung. Ich selber habe lieber Koeffizienten, die ich interpretieren kann, als unklare Effekte mit niedrigen \\(p\\)-Werten.\nSchauen wir einmal auf unseren Goldstandard, der Variable für den CRP-Wert. Die Variable haben wir ja jetzt immer mal wieder in diesem Kapitel interpretiert und uns angeschaut. Die Variable crp passt von dem Effekt jedenfalls gut in den Kontext mit rein. Die Effekte sind ähnlich wie in der multinominalen Regression. Wir haben eben nur größere \\(p\\)-Werte. Jetzt müssen wir entscheiden, wir können vermutlich die getrennten logistischen Regressionen besser beschreiben und interpretieren. Das ist besonders der Fall, wenn wir wirklich Probleme haben eine Referenz in der multinominalen Regression festzulegen. Dann würde ich immer zu den getrennten logistischen Regressionen greifen als eine schief interpretierte multinominale Regression."
  },
  {
    "objectID": "stat-modeling-multinom.html#referenzen",
    "href": "stat-modeling-multinom.html#referenzen",
    "title": "42  Multinomiale / Ordinale logistische Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "stat-modeling-logistic.html#annahmen-an-die-daten",
    "href": "stat-modeling-logistic.html#annahmen-an-die-daten",
    "title": "43  Logistische Regression",
    "section": "\n43.1 Annahmen an die Daten",
    "text": "43.1 Annahmen an die Daten\nUnser gemessenes Outcome \\(y\\) folgt einer Binomialverteilung. Damit finden wir im Outcome nur \\(0\\) oder \\(1\\) Werte.\nIm folgenden Kapitel zu der multiplen logistischen linearen Regression gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 37 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDaher sieht unser Modell wie folgt aus. Wir haben ein \\(y\\) und \\(p\\)-mal \\(x\\). Wobei \\(p\\) für die Anzahl an Variablen auf der rechten Seite des Modells steht. Im Weiteren folgt unser \\(y\\) einer Binomailverteilung. Damit finden wir im Outcome nur \\(0\\) oder \\(1\\) Werte. Das ist hier sehr wichtig, denn wir wollen ja eine multiple logistische lineare Regression rechnen.\n\\[\ny \\sim x_1 + x_2 + ... + x_p\n\\]\nWir können in dem Modell auch Faktoren \\(f\\) haben, aber es geht hier nicht um einen Gruppenvergleich. Das ist ganz wichtig. Wenn du einen Gruppenvergleich rechnen willst, dann musst du in Kapitel 31 nochmal nachlesen, wir du dann das Modell weiterverwendest."
  },
  {
    "objectID": "stat-modeling-logistic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-logistic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "43  Logistische Regression",
    "section": "\n43.2 Genutzte R Pakete für das Kapitel",
    "text": "43.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, gtsummary,\n               tidymodels, cutpointr)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-logistic.html#daten",
    "href": "stat-modeling-logistic.html#daten",
    "title": "43  Logistische Regression",
    "section": "\n43.3 Daten",
    "text": "43.3 Daten\nIn diesem Kapitel nutzen wir die infizierten Ferkel als Beispieldatensatz. Wir haben in dem Datensatz über vierhundert Ferkel untersucht und festgehalten, ob die Ferkel infiziert sind (\\(1\\), ja) oder nicht infiziert (\\(0\\), nein). Wir haben daneben noch eine ganze Reihe von Risikofaktoren erhoben. Hier sieht man mal wieder wie wirr die Sprache der Statistik ist. Weil wir rausfinden wollen welche Variable das Risiko für die Infektion erhöht, nennen wir diese Variablen Risikofaktoren. Obwohl die Variablen gar keine kategorialen Spalten sin bzw. nicht alle. So ist das dann in der Statistik, ein verwirrender Begriff jagt den Nächsten.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") \n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der Tabelle 43.1 an.\n\n\n\n\nTabelle 43.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\ninfected\n\n\n\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\n1\n\n\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\n1\n\n\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\n0\n\n\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.15\n19.05\n4.35\n1\n\n\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n55.38\n18.44\n5.27\n1\n\n\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n60.29\n18.42\n4.78\n1\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n54\nfemale\nnorth\n11.82\n21.5\npre-frail\n55.32\n19.75\n3.92\n1\n\n\n56\nmale\nwest\n13.91\n20.8\nfrail\n58.37\n17.28\n7.44\n0\n\n\n57\nmale\nnorthwest\n12.49\n21.95\npre-frail\n56.66\n16.86\n2.44\n1\n\n\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\n1\n\n\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\n0\n\n\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\n1\n\n\n\n\n\n\nIn dem nächsten Abschnitt werden wir die Daten nutzen um rauszufinden welche Variablen einen Einfluss auf den Infektionsstatus der Ferkel hat."
  },
  {
    "objectID": "stat-modeling-logistic.html#theoretischer-hintergrund",
    "href": "stat-modeling-logistic.html#theoretischer-hintergrund",
    "title": "43  Logistische Regression",
    "section": "\n43.4 Theoretischer Hintergrund",
    "text": "43.4 Theoretischer Hintergrund\nWir schaffen wir es, durch einen \\(0/1\\) Outcome auf der y-Achse eine gerade Linie durch die Punkte zu zeichnen und die Koeffiziente dieser Gerade zu bestimmen? Immerhin gibt es ja gar keine Werte zwischen \\(0\\) und \\(1\\). In Abbildung 43.1 sehen wir beispielhaft den Zusammenhang zwischen dem Infektionsstatus und der Aktivität der Ferkel. Wir haben zwei horizontale Linien. Wie zeichen wir jetzt da eine Gerade durch?\n\nggplot(pig_tbl, aes(x = activity, y = infected)) +\n  theme_bw() +\n  geom_point() \n\n\n\nAbbildung 43.1— Visualisierung des Zusammenhangs zwischen dem Infektionsstatus und der Aktivität der Ferkel.\n\n\n\nDer Trick hierbei ist wieder die Transformation des Zusammenhangs von \\(y \\sim x\\) auf einen \\(\\log\\)-scale. Das heißt wir Rechnen nicht mit den \\(0/1\\) Werten sondern transformieren den gesamten Zusammenhang. Das ist wichtig, den es gibt einen Unterschied zwischen der Transformation von \\(y\\) und der Transformation die hier gemeint ist. Wir halten fest, wir rechnen also nciht auf der ursprünglichen Skala der Daten sondern auf der \\(\\log\\)-scale. Allgemeiner wird auch von der link-Funktion gesprochen, da wir ja verschiedene Möglichkeiten der Transformation des Zusammenhangs haben.\nHier gibt es nur die Kurzfassung der link-Funktion. Dormann (2013) liefert hierzu in Kapitel 7.1.3 nochmal ein Einführung in das Thema.\nWir gehen wir also vor. Zuerst Modellieren wir die Wahrscheinlichkeit für den Eintritt des Ereignisses. Wir machen also aus unseren binären \\(0/1\\) Daten eine Wahrscheinlichkeit für den Eintritt von 1.\n\\[\nY \\rightarrow Pr(Y = 1)\n\\]\nDamit haben wir schon was erreicht den \\(Pr(Y = 1)\\) liegt zwischen \\(0\\) und \\(1\\). Damit haben wir also schon Werte dazwischen. Wenn wir aber normalverteilte Residuen haben wollen, dann müssen unsere Werte von \\(-\\infty\\) bis \\(+\\infty\\) laufen können. Daher rechnen wir im Weiteren die Chance.\n\\[\n\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\n\\] Die Chance (eng. Odds) für das Eintreten von \\(Y=1\\) ist eben die Wahrscheinlichkeit für das Eintreten geteilt durch die Gegenwahrscheinlichkeit. Das ist schon besser, denn damit liegen unsere transformierten Werte für den Zusammenhang schon zwischen \\(0\\) und \\(+\\infty\\). Wenn wir jetzt noch den \\(\\log\\) von den Chancen rechnen, dann haben wir schon fast alles was wir brauchen.\n\\[\n\\log\\left(\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\\right)\n\\]\nDer Logarithmus der Chance liegt dann zwischen \\(-\\infty\\) und \\(+\\infty\\). Deshalb spricht man auch von den \\(\\log\\)-Odds einer logistischen Regression. Auch sieht man hier woher das logistisch kommt. Wir beschreiben im Namen auch gleich die Transformation mit. Am ende kommen wir somit dann auf folgendes Modell.\n\\[\n\\log\\left(\\cfrac{Pr(y = 1)}{1 - Pr(Y = 1)}\\right) = \\beta_0 + \\beta_1 x_1 + ...  + \\beta_p x_p + \\epsilon\n\\] Vielleicht ist dir der Begriff Wahrscheinlichkeit und der Unterschied zur Chance nicht mehr so präsent. Deshalb hier nochmal als Wiederholung oder Auffrischung.\n\nEine Wahrscheinlichkeit beschreibt dem Anteil an Allen. Zum Beispiel den Anteil Gewinner an allen Teilnehmern. Den Anteil Personen mit Therapieerfolg an allen Studienteilnehmern.\nEine Chance oder (eng. Odds) beschreibt ein Verhältnis. Somit das Verhältnis Gewinner zu Nichtgewinner. Oder das Verhältnis Personen mit Therapieerfolg zu Personen ohne Therapieerfolg\n\nNochmal an einem Zahlenbeispiel. Wenn wir ein Glücksspiel haben, in dem es 2 Kombinationen gibt die gewinnen und drei 3 Kombinationen die verlieren, dann haben wir eine Wahrscheinlichkeit zu gewinnen von \\(2 / 5 = 0.40 = 40\\%\\). Wenn wir die Chance zu gewinnen ausrechnen erhalten wir \\(2:3 = 0.67 = 67\\%\\). Wir sehen es gibt einen deutlichen Unterschied zwischen Chance und Wahrscheinlichkeit. Wenn wir große Fallzahl haben bzw. kleine Wahrscheinlichkeiten, dann ist der Unterschied nicht mehr so drastisch. Aber von einer Gleichkeit von Wahrscheinlichkeit und Chance zu sprechen kann nicht ausgegangen werden.\nWas ist nun das Problem? Wir erhalten aus einer logistischen Regression \\(\\log\\)-Odds wieder. Der Effektchätzer ist also eine Chance. Wir werden aber das Ergebnis wie eine Wahrscheinlichkeit interpretieren. Diese Diskrepanz ist wenigen bekannt und ein Grund, warum wir in der Medizin immer uns daran erinnern müssen, was wir eigentlich mit der logistischen Regression aussagen können."
  },
  {
    "objectID": "stat-modeling-logistic.html#modellierung",
    "href": "stat-modeling-logistic.html#modellierung",
    "title": "43  Logistische Regression",
    "section": "\n43.5 Modellierung",
    "text": "43.5 Modellierung\nDie Modellerierung der logistischen Regression ist sehr einfach. Wir nutzen wieder die Formelschreibweise im glm() um unsere Variablen zu definieren. Wenn unser Outcome nicht binär ist, dann jammert R und gibt uns einen Fehler aus. Ich kann hier nur dringlichst raten, das Outcome in \\(0/1\\) zu kodieren mit dem Schlechten als \\(1\\).\nDas glm() muss dann noch wissen, dass es eine logistische Regression rechnen soll. Das machen wir in dem wir als Verteilungsfamilie die Binomialverteilung auswählen. Wir geben also an family = binomial und schon können wir das volle Modell fitten.\n\nlog_fit &lt;- glm(infected ~ age + sex + location + activity + crp + \n                 frailty + bloodpressure + weight + creatinin, \n               data = pig_tbl, family = binomial)\n\nDas war extrem kurz und scherzlos. Also können wir dann auch ganz kurz schauen, ob das Modell einigermaßen funktioniert hat."
  },
  {
    "objectID": "stat-modeling-logistic.html#performance-des-modells",
    "href": "stat-modeling-logistic.html#performance-des-modells",
    "title": "43  Logistische Regression",
    "section": "\n43.6 Performance des Modells",
    "text": "43.6 Performance des Modells\nNachdem wir das Modell gefittet haben, wollen wir uns nochmal das \\(R^2\\) wiedergeben lassen um zu entscheiden, ob unser Modell einigermaßen funktioniert hat. Dieser Abschnitt ist sehr kurz. Wir haben leider nur sehr wenige Möglichkeiten um ein logistischen Modell zu bewerten.\n\nr2(log_fit)\n\n# R2 for Logistic Regression\n  Tjur's R2: 0.285\n\n\nJa, so viel Varianz erklären wir nicht, aber wenn du ein wenig im Internet suchst, dann wirst du feststellen, dass das Bestimmtheitsmaß so eine Sache in glm()’s ist. Wir sind aber einigermaßen zufrieden. Eventuell würde eine Variablenselektion hier helfen, aber das ist nicht Inhalt dieses Kapitels.\nIn Abbildung 43.2 schauen wir nochmal auf die Residuen und die möglichen Ausreißer. Wieder sehen beide Plots einigermaßen in Ordnung aus. Die Abbildungen sind jetzt nicht die Besten, aber ich würde hier auch anhand der Diagnoseplots nicht die Modellierung verwerfen.\n\ncheck_model(log_fit, colors = cbbPalette[6:8], \n            check = c(\"qq\", \"outliers\")) \n\n\n\nAbbildung 43.2— Ausgabe ausgewählter Modelgüteplots der Funktion check_model()."
  },
  {
    "objectID": "stat-modeling-logistic.html#interpretation-des-modells",
    "href": "stat-modeling-logistic.html#interpretation-des-modells",
    "title": "43  Logistische Regression",
    "section": "\n43.7 Interpretation des Modells",
    "text": "43.7 Interpretation des Modells\nZu Interpretation schauen wir uns wie immer nicht die rohe Ausgabe an, sondern lassen uns die Ausgabe mit der Funktion model_parameters() aus dem R Paket parameters wiedergeben. Wir müssen noch die Option exponentiate = TRUE wählen, damit unsere Koeffizienten nicht als \\(\\log\\)-Odds sondern als Odds wiedergeben werden. Korrekterweise erhalten wir die Odds ratio wieder was wir auch als \\(OR\\) angegeben.\n\nmodel_parameters(log_fit, exponentiate = TRUE)\n\nParameter            | Odds Ratio |       SE |       95% CI |     z |      p\n----------------------------------------------------------------------------\n(Intercept)          |   2.91e-11 | 1.24e-10 | [0.00, 0.00] | -5.69 | &lt; .001\nage                  |       1.00 |     0.03 | [0.95, 1.06] |  0.16 | 0.872 \nsex [male]           |       0.74 |     0.26 | [0.37, 1.48] | -0.84 | 0.398 \nlocation [northeast] |       1.07 |     0.40 | [0.52, 2.22] |  0.19 | 0.852 \nlocation [northwest] |       0.62 |     0.20 | [0.33, 1.17] | -1.47 | 0.142 \nlocation [west]      |       0.76 |     0.28 | [0.37, 1.56] | -0.75 | 0.450 \nactivity             |       1.05 |     0.10 | [0.87, 1.27] |  0.51 | 0.612 \ncrp                  |       2.64 |     0.29 | [2.14, 3.32] |  8.70 | &lt; .001\nfrailty [pre-frail]  |       1.12 |     0.46 | [0.49, 2.48] |  0.27 | 0.788 \nfrailty [robust]     |       0.81 |     0.31 | [0.37, 1.72] | -0.54 | 0.588 \nbloodpressure        |       1.09 |     0.04 | [1.01, 1.17] |  2.09 | 0.037 \nweight               |       1.00 |     0.09 | [0.85, 1.19] |  0.06 | 0.955 \ncreatinin            |       1.12 |     0.10 | [0.94, 1.33] |  1.23 | 0.218 \n\n\nWie interpretieren wir nun das \\(OR\\) einer logistischen Regression? Wenn wir darauf gechtet haben, dass wir mit \\(1\\) das Schlechte meinen, dann können wir wir folgt mit dem \\(OR\\) sprechen. Wenn wir ein \\(OR &gt; 1\\) haben, dann haben wir ein Risiko vorliegen. Die Variable mit einem \\(OR\\) größer als \\(1\\) wird die Chance auf den Eintritt des schlechten Ereignisses erhöhen. Wenn wir ein \\(OR &lt; 1\\) haben, dann sprechen wir von einem protektiven Faktor. Die Variable mit einem \\(OR\\) kleiner \\(1\\) wird vor dem Eintreten des schlechten Ereignisses schützen. Schauen wir uns den Zusammenhang mal im Detail für die Ferkeldaten an.\n\n\n(intercept) beschreibt den Intercept der logistischen Regression. Wenn wir mehr als eine simple Regression vorliegen haben, wie in diesem Fall, dann ist der Intercept schwer zu interpretieren. Wir konzentrieren uns auf die Effekte der anderen Variablen.\n\nsex beschreibt den Effekt der männlichen Ferkel zu den weiblichen Ferkeln. Daher haben männliche Ferkel eine \\(2.75\\) höhere Chance infiziert zu werden als weibliche Ferkel.\n\nlocation [northeast], location [northwest] und location [west] beschreibt den Unterschied zur location [north]. Alle Orte haben eine geringere Chance für eine Infektion zum Vergleich der Bauernhöfe im Norden. Zwar ist keiner der Effekte signifikant, aber ein interessantes Ergebnis ist es allemal.\n\nactivity beschreibt den Effekt der Aktivität der Ferkel. Wenn sich die Ferkel mehr bewegen, dann ist die Chance für eine Infektion gemindert.\n\ncrp beschreibt den Effekt des CRP-Wertes auf den Infektionsgrad. Pro Einheit CRP steigt die Chance einer Infektion um \\(2.97\\) an. Das ist schon ein beachtlicher Wert.\n\nfrailty beschreibt die Gebrechlichkeit der Ferkel. Hier müssen wir wieder schauen, zu welchem Level von frailty wir vergleichen. Hier vergleichen wir zu frail. Also dem höchsten Gebrechlichkeitgrad. Ferkel die weniger gebrechlich sind, haben eine niedrigere Chance zu erkranken.\n\nbloodpressure, weight und creatinin sind alles Variablen, mit einem \\(OR\\) größer als \\(1\\) und somit alles Riskovariablen. Hier sind zwar die \\(OR\\) relativ klein, aber das muss erstmal nichts heißen, da die \\(OR\\) ja hier die Änderung für eine Einheit von \\(x\\) beschreiben. Deshalb musst du immer schauen, wie die Einheiten von kontinuierlichen kodiert Variablen sind.\n\nKommen wir nochmal zu den gänigen Tabellen für die Zusammenfassung eines Ergebnisses einer logistischen Regression. Teilweise sind diese Tabellen so generisch und häufiog verwendet, dass wir schon einen Begriff für diese Tabellen haben. In Tabelle 43.2 siehst du die table 1 für die Übersicht aller Risikovariablen aufgeteilt nach dem Infektionsstatus. Diese Art der Tabellendarstellung ist so grundlegend für eine medizinische Veröffentlichung, dass sich eben der Begriff table 1 etabliert hat. Fast jede medizinische Veröffentlichung hat als erste Tabelle diese Art von Tabelle angegeben. Hierbei ist wichtig, dass die \\(p\\)-Werte alle nur aus einem einfachen statistischen Test stammen. Die \\(p\\)-Werte einer multiplen logistischen Regression werden daher immer anders sein.\n\npig_tbl %&gt;% tbl_summary(by = infected) %&gt;% add_p() %&gt;% as_flex_table()\n\n\n\n\n\n\nTabelle 43.2—  Ausgabe der Daten in einer Summary Table oder auch Table 1 genannt.\nIn medizinischen Veröffentlichungen immer die erste Tabelle für die\nZusammenfassung der Patienten (hier Ferkel) für jede erhobende\nRisikovariable. \n\nCharacteristic\n0, N = 1361\n1, N = 2761\np-value2\n\n\n\nage\n59.5 (57.0, 63.0)\n60.0 (57.0, 63.0)\n0.9\n\n\nsex\n\n\n0.2\n\n\nfemale\n47 (35%)\n114 (41%)\n\n\n\nmale\n89 (65%)\n162 (59%)\n\n\n\nlocation\n\n\n0.3\n\n\nnorth\n36 (26%)\n85 (31%)\n\n\n\nnortheast\n23 (17%)\n61 (22%)\n\n\n\nnorthwest\n48 (35%)\n76 (28%)\n\n\n\nwest\n29 (21%)\n54 (20%)\n\n\n\nactivity\n13.39 (12.25, 14.34)\n13.24 (12.28, 14.54)\n0.8\n\n\ncrp\n19.12 (18.12, 19.83)\n20.56 (19.77, 21.46)\n&lt;0.001\n\n\nfrailty\n\n\n0.5\n\n\nfrail\n18 (13%)\n37 (13%)\n\n\n\npre-frail\n42 (31%)\n101 (37%)\n\n\n\nrobust\n76 (56%)\n138 (50%)\n\n\n\nbloodpressure\n56.2 (54.3, 58.5)\n57.2 (55.1, 59.6)\n0.021\n\n\nweight\n18.61 (17.34, 19.41)\n18.32 (17.19, 19.60)\n0.8\n\n\ncreatinin\n4.85 (3.67, 5.93)\n4.86 (4.06, 5.84)\n0.3\n\n\n\n1Median (IQR); n (%)\n2Wilcoxon rank sum test; Pearson's Chi-squared test\n\n\n\n\n\n\nIn Tabelle 43.3 siehst du nochmal für eine Auswahl an Variablen die simplen logistischen Regressionen gerechnet. Du müsst also nicht jede simple logistische Regression selber rechnen, sondern kannst auch die Funktion tbl_uvregression() verwenden. Das R Paket tbl_summary erlaubt weitreichende Formatierungsmöglichkeiten. Am bestes schaust du einmal im Tutorial Tutorial: tbl_regression selber nach was du brauchst oder anpassen willst.\n\npig_tbl%&gt;%\n  select(infected, age, crp, bloodpressure) %&gt;%\n  tbl_uvregression(\n    method = glm,\n    y = infected,\n    method.args = list(family = binomial),\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 2)\n  ) %&gt;% as_flex_table()\n\n\n\n\n\n\nTabelle 43.3—  Simple logistische Regression für eine Auswahl an Einflussvariablen.\nFür jede Einflussvariable wurde eine simple logistische Regression\ngerechnet. \n\nCharacteristic\nN\nOR1\n95% CI1\np-value\n\n\n\nage\n412\n1.00\n0.95, 1.04\n0.88\n\n\ncrp\n412\n2.62\n2.14, 3.27\n&lt;0.001\n\n\nbloodpressure\n412\n1.08\n1.02, 1.15\n0.013\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nnun gibt es viele Möglichkeiten sich die logistische Regression wiedergeben zu lassen In Tabelle 43.4 siehst du nochmal die Möglichkeit, die dir das R Paket tbl_summary() bietet. Am Ende ist es dann eine reine Geschmacksfrage, wie wir die Daten dann aufarbeiten wollen.\n\nlog_fit %&gt;% tbl_regression(exponentiate = TRUE) %&gt;% as_flex_table()\n\n\n\n\n\n\nTabelle 43.4—  Ausgabe der multiplen logistischen Regression durch die Funktion\ntbl_regression(). \n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\nage\n1.00\n0.95, 1.06\n0.9\n\n\nsex\n\n\n\n\n\nfemale\n—\n—\n\n\n\nmale\n0.74\n0.37, 1.48\n0.4\n\n\nlocation\n\n\n\n\n\nnorth\n—\n—\n\n\n\nnortheast\n1.07\n0.52, 2.22\n0.9\n\n\nnorthwest\n0.62\n0.33, 1.17\n0.14\n\n\nwest\n0.76\n0.37, 1.56\n0.5\n\n\nactivity\n1.05\n0.87, 1.27\n0.6\n\n\ncrp\n2.64\n2.14, 3.32\n&lt;0.001\n\n\nfrailty\n\n\n\n\n\nfrail\n—\n—\n\n\n\npre-frail\n1.12\n0.49, 2.48\n0.8\n\n\nrobust\n0.81\n0.37, 1.72\n0.6\n\n\nbloodpressure\n1.09\n1.01, 1.17\n0.037\n\n\nweight\n1.00\n0.85, 1.19\n&gt;0.9\n\n\ncreatinin\n1.12\n0.94, 1.33\n0.2\n\n\n1OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\nZum Abschluss wollen wir uns einmal die Ergebnisse des Modellfits als logistischen Gerade für eine simple lineare Regression mit dem Modell \\(infected \\sim crp\\) anschauen. Wie immer können wir uns den Zusammenhang nur in einem simplen Modell anschauen. Im Fall einer multiplen linearen Regresion können wir nicht so viele Dimensionen in einer Grpahik darstellen. Wir fitten also das Modell log_fit_crp wie im folgenden dargestellt.\n\nlog_fit_crp &lt;- glm(infected ~ crp, data = pig_tbl, family = binomial)\n\nNun können wir uns mit der Funktion predict() die Wert auf der Geraden wiedergeben lassen. Wenn wir predict() nur so aufrufen, dann erhalten wir die Werte für \\(y\\) auf der transformierten \\(link\\)-Scale wieder. Das hilft uns aber nicht weiter, wir haben ja nur 0 und 1 Werte für \\(y\\) vorliegen.\n\npredict(log_fit_crp, type = \"link\") %&gt;% \n  extract(1:10) %&gt;% \n  round(2)\n\n    1     2     3     4     5     6     7     8     9    10 \n 3.19 -0.41 -0.29  0.29  2.41  2.30 -0.08 -0.08  2.72  1.83 \n\n\nDa wir die Werte für die Wahrscheinlichkeit das ein Ferkel infiziert ist, also die Wahrscheinlichkeit \\(Pr(infected = 1)\\), müssen wir noch die Option type = reponse wählen. So erhalten wir die Wahrscheinlichkeiten wiedergegeben.\n\npredict(log_fit_crp, type = \"response\") %&gt;% \n  extract(1:10) %&gt;% \n  round(2)\n\n   1    2    3    4    5    6    7    8    9   10 \n0.96 0.40 0.43 0.57 0.92 0.91 0.48 0.48 0.94 0.86 \n\n\nAbschließend können wir uns die Gerade auch in der Abbildung 43.3 visualisieren lassen. Auf der x-Achse sehen wir die crp-Werte und auf der y-Achse den Infektionsstatus. Auf der \\(reponse\\)-scale sehen wir eine S-Kurve. Auf der \\(link\\)-scale würden wir eine Gerade sehen.\n\nggplot(pig_tbl, aes(x = crp, y = infected)) +\n  theme_bw() +\n  geom_point() +\n  geom_line(aes(y = predict(log_fit_crp, type = \"response\")), color = \"red\") \n\n\n\nAbbildung 43.3— Visualisierung der logistischen Gerade in einer simplen logistischen Regression mit der Variable crp.\n\n\n\nNun haben wir das Kapitel zur logistischen Regression fast abgeschlossen. Was noch fehlt ist die Besonderheit der Prädiktion im Kontext des maschinellen Lernens. Das machen wir jetzt im folgenden Abschnitt. Wenn dich die logistische Regression nur interessiert hat um einen kausalen Zusammenhang zwischen Einflussvariablen und dem binären Outcome zu modellieren, dann sind wir hier fertig."
  },
  {
    "objectID": "stat-modeling-logistic.html#dichotomisierung",
    "href": "stat-modeling-logistic.html#dichotomisierung",
    "title": "43  Logistische Regression",
    "section": "\n43.8 Dichotomisierung",
    "text": "43.8 Dichotomisierung\nManchmal ist es so, dass wir eine logistsiche Regression rechnen wollen. Wir fragen nicht, wie ist unser \\(y\\) verteilt und was für eine Regression können wir dann rechnen? Sondern wir wollen mit der logistischen Regression durch die Wand. Wenn wir das wollen, dann können wir unser \\(y\\) dichotomisieren. Das heißt, wir machen aus einer Variable, die mehr als zwei Level hat einen Faktor mit zwei Leveln. Dafür stehen uns verschiedene Möglichkeiten offen.\nIn dem R Paket dplyr haben wir mit der Funktion recode() die Möglichkeit eine Variable von alt = neu umzukodieren. Dabei müssen wir natürlich darauf achten, dass wir die alten Level der Variable richtig schreiben und bei der neuen Level nur zwei Namen eintragen. Dann sind wir auch schon durch mit der Umbenennung.\n\npig_tbl %&gt;% \n  mutate(frailty = recode(frailty, \n                          \"robust\" = \"robust\", \n                          \"pre-frail\" = \"frail_prefrail\", \n                          \"frail\" = \"frail_prefrail\")) %&gt;% \n  pull(frailty) %&gt;% extract(1:20)\n\n [1] \"robust\"         \"robust\"         \"robust\"         \"robust\"        \n [5] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n [9] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n[13] \"robust\"         \"robust\"         \"frail_prefrail\" \"robust\"        \n[17] \"frail_prefrail\" \"frail_prefrail\" \"robust\"         \"frail_prefrail\"\n\n\nIch finde die Funktion case_when() etwas übersichtlicher. Das ist aber eigentlich nur eine Geschmacksfrage. Am Ende kommt jedenfalls das Gleiche heraus.\n\npig_tbl %&gt;% \nmutate(frailty = case_when(frailty == \"robust\" ~ \"robust\",\n                           frailty == \"pre-frail\" ~ \"frail\",\n                           frailty == \"frail\" ~ \"frail\")) %&gt;% \n  pull(frailty) %&gt;% extract(1:20)\n\n [1] \"robust\" \"robust\" \"robust\" \"robust\" \"robust\" \"robust\" \"frail\"  \"robust\"\n [9] \"robust\" \"robust\" \"frail\"  \"robust\" \"robust\" \"robust\" \"frail\"  \"robust\"\n[17] \"frail\"  \"frail\"  \"robust\" \"frail\" \n\n\nHäufig haben wir auch den Fall, dass wir keine kontinuierlichen \\(x\\) in unseren Daten wollen. Alles soll sich in Faktoren verwandeln, so dass wir immer eine 2x2 Tafel haben. Wenn es sein muss, liefert hier cutpointr() die Lösung für dieses Problem. Wir müssen dafür zum einen unser kontinuierliches \\(x\\) angeben und dann mit class unser binäres \\(y\\). Wir erhalten dann für unser \\(y\\) den bestmöglichen Split für unser \\(x\\). Im Beispiel wollen wir einmal die Variable crp für unser Outcome infected in zwei Gruppen aufteilen. Wir wollen eigentlich immer zwei Gruppen, da wir dann in dem Setting eines \\(\\mathcal{X}^2\\)-Test und einer einfacheren Interpretation von dem \\(OR\\) sind.\nWir immer haben wir eine große Bandbreite an Optionen, wie wir den besten Split unseres \\(x\\) kriegen wollen. Ich gehe hier mit den Default-Werten. Damit kommt man eigentlich recht weit. Ich möchte gerne die Summe der Sensivität und der Spezifität sum_sens_spec über alle möglichen Cutpoints maximieren maximize_metric. Der Cutpoint mit der maximalen Summe an Sensivität und der Spezifität wird mir dann wiedergegeben.\n\n\nNatürlich hat das R Paket cutpoint noch viel mehr Optionen. Mehr gibt es in An introduction to cutpointr.\n\ncp_crp &lt;- cutpointr(data = pig_tbl,\n                    x = crp,\n                    class = infected,\n                    method = maximize_metric, \n                    metric = sum_sens_spec) \n\ncp_crp\n\n# A tibble: 1 × 16\n  direction optimal_cutpoint method          sum_sens_spec      acc sensitivity\n  &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1 &gt;=                   19.71 maximize_metric       1.48870 0.752427    0.768116\n  specificity      AUC pos_class neg_class prevalence outcome  predictor\n        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    \n1    0.720588 0.806119         1         0   0.669903 infected crp      \n  data               roc_curve             boot \n  &lt;list&gt;             &lt;list&gt;                &lt;lgl&gt;\n1 &lt;tibble [412 × 2]&gt; &lt;rc_ctpnt [282 × 10]&gt; NA   \n\n\nIn Abbildung 43.4 sehe wir die Ausgabe der Funktion cutpointr() nochmal visualisiert. Wir sehen, dass der Split einigermaßen die crp-Werte im Sinne von unserem Outcome aufteilt.\n\nplot(cp_crp)\n\n\n\nAbbildung 43.4— Visualisierung des Ergebnisses der Funktion cutpointr für die Variable crp.\n\n\n\nWir können uns jetzt noch den optimalen Cutpoint aus der Ausgabe herausziehen, wenn wir den Punkt nicht aus der Ausgabe ablesen wollen.\n\npluck(cp_crp, \"optimal_cutpoint\")\n\n[1] 19.71\n\n\nAm Ende können wir dann über case_when() uns ein binären CRP-Wert zusammenbauen. Wir müssen dann natürlich entscheiden welche Variable wir mit ins Modell nehme, aber meistens machen wir uns ja die Mühen um dann die neue Variable zu verwenden.\n\npig_tbl %&gt;% \nmutate(crp_bin = case_when(crp &gt;= 19.84 ~ \"high\",\n                           crp &lt; 19.84 ~ \"low\")) %&gt;% \nselect(crp, crp_bin)  \n\n# A tibble: 412 × 2\n     crp crp_bin\n   &lt;dbl&gt; &lt;chr&gt;  \n 1  22.4 high   \n 2  18.6 low    \n 3  18.8 low    \n 4  19.4 low    \n 5  21.6 high   \n 6  21.4 high   \n 7  19.0 low    \n 8  19.0 low    \n 9  21.9 high   \n10  21.0 high   \n# ℹ 402 more rows\n\n\nDamit haben wir uns dann auch mit dem Problem der Dichotomisierung in der logististischen Regression einmal beschäftigt. Somit bleibt dann noch die Prädiktion übrig."
  },
  {
    "objectID": "stat-modeling-logistic.html#prädiktion",
    "href": "stat-modeling-logistic.html#prädiktion",
    "title": "43  Logistische Regression",
    "section": "\n43.9 Prädiktion",
    "text": "43.9 Prädiktion\nDa wir später in dem Kapitel 54 die logistische Regression auch als Vergleich zu maschinellen Lernverfahren in der Klassifikation nutzen werden gehen wir hier auch die Prädiktion einmal für die logistische Regression durch. Wir wollen also eine Klassifikation, also eine Vorhersage, für das Outcome infected mit einer logistischen Regression rechnen. Wir nutzen dazu die Möglichkeiten des R Pakets tidymodels wodurch wir einfacher ein Modell bauen und eine Klassifikation rechnen können. Unsere Fragestellung ist, ob wir mit unseren Einflussvariablen den Infektionsstatus vorhersagen können. Das heißt wir wollen ein Modell bauen mit dem wir zukünftige Ferkel als potenziell krank oder gesund anhand unser erhobenen Daten einordnen bzw. klassifizieren können.\n\n\nMehr zu Rezepten (eng. recipes) kannst du im Kapitel 54 zu den Grundlagen des maschinellen Lernens erfahren.\nDer erste Schritt einer Klassifikation ist immer sicherzustellen, dass unser Outcome auch wirklich aus Kategorien besteht. In R nutzen wir dafür einen Faktor und setzen dann auch gleich die Ordnung fest.\n\npig_tbl &lt;- pig_tbl %&gt;% \n  mutate(infected = factor(infected, levels = c(0, 1)))\n\nNun bauen wir uns ein einfaches Rezept mit der Funktion recipe(). Dafür legen wir das Modell, was wir rechnen wollen einmal fest. Wir nehmen infected als Outcome und den Rest der Vairbalen . aus dem Datensatz pig_tbl als die \\(x\\) Variablen. Dann wollen wir noch alle Variablen, die ein Faktor sind in eine Dummyvariable umwandeln.\n\npig_rec &lt;- recipe(infected ~ ., data = pig_tbl) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nWir wollen jetzt unser Modell definieren. Wir rechnen eine logistsiche Regression und deshalb nutzen wir die Funktion logistic_reg(). Da wir wirklich viele Möglichkeiten hätten die logistische Regression zu rechnen, müssen wir noch den Algorithmus wählen. Das tuen wir mit der Funktion set_engine(). Wir nutzen hier den simplen glm() Algorithmus. Es gebe aber auch andere Implementierungen.\n\nlogreg_mod &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\")\n\nJetzt müssen wir noch einen Workflow definieren. Wir wollen ein Modell rechnen und zwar mit den Informationen in unserem Rezept. Das bauen wir einmal zusammen und schauen uns die Ausgabe an.\n\npig_wflow &lt;- workflow() %&gt;% \n  add_model(logreg_mod) %&gt;% \n  add_recipe(pig_rec)\n\npig_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nDas passt alles soweit. Ja, es ist etwas kompliziert und das ginge sicherlich auch einfacher. Wir werden dann aber noch sehen, dass wir es uns mit dem Ablauf sehr viel einfacher machen, wenn wir kompliziertere Modelle schätzen wollen. Mehr dazu findest du dann im Kapitel 54 zu den maschinellen Lernverfahren.\nJetzt können wir den Workflow nutzen um den Fit zu rechnen. Bis jetzt haben wir nur Informationen gesammelt. Dadurch das wir jetzt das Objekt pig_workflow in die Funktion fit() pipen rechnen wir das Modell.\n\npig_fit &lt;- pig_wflow %&gt;% \n  fit(data = pig_tbl)\n\nDas erhaltende Modell könne wir dann in die Funktion predict() stecken um uns den Inektionsstatus vorhersagen zu lassen.\n\npredict(pig_fit, new_data = pig_tbl)\n\n# A tibble: 412 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 1          \n 2 0          \n 3 0          \n 4 1          \n 5 1          \n 6 1          \n 7 0          \n 8 0          \n 9 1          \n10 1          \n# ℹ 402 more rows\n\n\nIn der Spalte .pred_class finden wir dann die vorhergesagten Werte des Infektionsstatus anhand unseres gefitteten Modells. Eigentlich würden wir ja gerne die vorhergesagten Werte mit unseren Orginalwerten vergleichen. Hier hilft uns die Funktion augment(). Dank der Funktion augment() erhalten wir nicht nur die vorhergesagten Klassen sondern auch die Wahrscheinlichkeit für die Klassenzugehörigkeiten. Daneben dann aber auch die Originalwerte für den Infektionsstatus in der Spalte infected.\n\npig_aug &lt;- augment(pig_fit, new_data = pig_tbl) %&gt;% \n  select(infected, matches(\"^\\\\.\"))\n\npig_aug\n\n# A tibble: 412 × 4\n   infected .pred_class .pred_0 .pred_1\n   &lt;fct&gt;    &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 1        1            0.0229   0.977\n 2 1        0            0.787    0.213\n 3 0        0            0.540    0.460\n 4 1        1            0.391    0.609\n 5 1        1            0.128    0.872\n 6 1        1            0.102    0.898\n 7 1        0            0.559    0.441\n 8 0        0            0.687    0.313\n 9 1        1            0.0556   0.944\n10 1        1            0.193    0.807\n# ℹ 402 more rows\n\n\nWir können dann die Werte aus dem Objekt pig_aug nutzen um uns die ROC Kurve als Güte der Vorhersage wiedergeben zu lassen. Wir nutzen hier die schnelle Variante der Ploterstellung. In dem Kapitel 57.6 zum Vergleich von Algorithmen gehe ich noch näher auf die möglichen Optionen bei der Erstellung einer ROC Kurve ein. Hier fällt die ROC Kurve dann mehr oder minder vom Himmel. Ich musste noch der Funktion mitgeben, dass das Event bei uns das zweite Level des Faktors infected ist. Sonst ist unsere ROC Kurve einmal an der Diagonalen gespiegelt.\nIn dem Kapitel 29 erfährst du mehr darüber was eine ROC Kurve ist und wie du die ROC Kurve interpretieren kannst.\n\npig_aug %&gt;% \n  roc_curve(truth = infected, .pred_1, event_level = \"second\") %&gt;% \n  autoplot()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at &lt;https://github.com/tidymodels/yardstick/issues&gt;.\n\n\n\n\nAbbildung 43.5— ROC Kurve für die Vorhersage des Infektionsstatus der Ferkel anhand der erhobenen Daten.\n\n\n\nNa das hat doch mal gut funktioniert. Die ROC Kurve verläuft zwar nicht ideal aber immerhin ist die ROC Kurve weit von der Diagnolen entfernt. Unser Modell ist also in der Lage den Infektionsstatus der Ferkel einigermaßen solide vorherzusagen. Schauen wir uns noch die area under the curve (abk. AUC) an.\n\npig_aug %&gt;% \n  roc_auc(truth = infected, .pred_1, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.819\n\n\nDer beste Wert wäre hier eine AUC von \\(1\\) und damit eine perfekte Vorhersage. Der schlechteste Wert wäre eine AUC von \\(0.5\\) und damit eine nahezu zufällige Zuordnung des Infeketionsstatus zu den Ferkeln von unserem Modell. Mit einer AUC von \\(0.83\\) können wir aber schon gut leben. Immerhin haben wir kaum am Modell rumgeschraubt bzw. ein Tuning betrieben. Wenn du mehr über Tuning und der Optimierung von Modellen zu Klassifikation wissen willst, dan musst du im Kapitel 54 zu den maschinellen Lernverfahren anfangen zu lesen."
  },
  {
    "objectID": "stat-modeling-logistic.html#referenzen",
    "href": "stat-modeling-logistic.html#referenzen",
    "title": "43  Logistische Regression",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer."
  },
  {
    "objectID": "stat-modeling-mixed.html#annahmen-an-die-daten",
    "href": "stat-modeling-mixed.html#annahmen-an-die-daten",
    "title": "44  Lineare gemischte Modelle",
    "section": "\n44.1 Annahmen an die Daten",
    "text": "44.1 Annahmen an die Daten\nIm folgenden Kapitel zu den linearen gemischten Modellen gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffällige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 37 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nDas Thema Modellvergleich und Variablenselektion ist im Falle des linearen gemischten Modells nochmal etwas spezieller. Wir gehen hier auch nochmal in einem Abschnitt drauf ein, wie wir das hier in dem Fall von linearen gemischten Modellen machen."
  },
  {
    "objectID": "stat-modeling-mixed.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-mixed.html#genutzte-r-pakete-für-das-kapitel",
    "title": "44  Lineare gemischte Modelle",
    "section": "\n44.2 Genutzte R Pakete für das Kapitel",
    "text": "44.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom, see,\n               multcomp, emmeans, lme4, broom.mixed,\n               parameters, ggridges, scales, performance)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-mixed.html#daten",
    "href": "stat-modeling-mixed.html#daten",
    "title": "44  Lineare gemischte Modelle",
    "section": "\n44.3 Daten",
    "text": "44.3 Daten\nIn diesem fiktiven Datenbeispiel wollen wir uns die Testscores eines Intelligentest bei \\(N = 480\\) Drachen anschauen. Wir sind dafür an acht Berge gefahren und haben die dortigen Drachen an den drei Flanken des Berges getestet. Daher hat der Faktor mountain_range acht Level mit Bavarian, Ligurian, Emmental, Central, Maritime, Southern, Julian, und Sarntal. Die drei Flanken des Berges bilden wir im Faktor site mit den Leveln north, east und south ab. In Abbildung 44.1 sehen wir eine Skizze für drei Berge mit den jeweiligen Flanken, wo gemessen wurde.\n\n\nAbbildung 44.1— Beispiel für drei der acht Berge mit Bavarian, Central und Julian. Auf jeden der acht Berge wurden an drei Seiten north, east und south die Testscores der dort lebenden Drachen erhoben.\n\nDie Daten liegen in dem Datensatz dragons.csv ab. Wir müssen aber noch einen Faktor body_length_cat bilden in dem wir die body_length der einzelnen Drachen in Kategorien umwandeln. Wir wollen später noch Gruppenvergleiche rechnen und brauchen daher einen Faktor mit Leveln. Daher nutzen wir die Funktion case_when() um einen Faktor mit fünf Größenkategorien zu bilden. Danach müssen wir wie immer noch die character Spalten in die entsprechenden Faktoren umwandeln.\n\ndragons_tbl &lt;- read.csv2(\"data/dragons.csv\") %&gt;% \n  as_tibble() %&gt;% \n  mutate(body_length_cat = \n           case_when(body_length &lt; 170 ~ \"tiny\",\n                     body_length &gt;= 170 & body_length &lt; 180 ~ \"small\",\n                     body_length &gt;= 180 & body_length &lt; 200 ~ \"medium\",\n                     body_length &gt;= 200 & body_length &lt; 220 ~ \"large\",\n                     body_length &gt;= 220 ~ \"gigantic\"),\n         body_length_cat = as_factor(body_length_cat),\n         mountain_range = as_factor(mountain_range),\n         site = factor(site, labels = c(\"north\", \"east\", \"south\"))) %&gt;% \n  select(test_score, body_length, body_length_cat, everything())\n\nEs ergibt sich dann der Datensatz wie in Tabelle 44.1 gezeigt. Wir belassen die Körperlänge der Drachen in der kontinuierlichen Form nochmal mit in den Daten.\n\n\n\n\nTabelle 44.1— Datensatz der Testscores für die Drachen auf verschiedenen Bergen und dort an verschiedenen Flanken der Berge.\n\ntest_score\nbody_length\nbody_length_cat\nmountain_range\nsite\n\n\n\n16.15\n165.55\ntiny\nBavarian\nnorth\n\n\n33.89\n167.56\ntiny\nBavarian\nnorth\n\n\n6.04\n165.88\ntiny\nBavarian\nnorth\n\n\n18.84\n167.69\ntiny\nBavarian\nnorth\n\n\n…\n…\n…\n…\n…\n\n\n59.37\n213.58\nlarge\nSarntal\nsouth\n\n\n68.75\n207.63\nlarge\nSarntal\nsouth\n\n\n74.89\n198.25\nmedium\nSarntal\nsouth\n\n\n65.95\n208.06\nlarge\nSarntal\nsouth\n\n\n\n\n\n\nBevor wir mit dem Modellieren beginnen, wollen wir erstmal visuell überprüfen, ob unser Outcome \\(y\\) mit dem Testscore auch normalverteilt ist. Wir benötigen für das klaissche lineare gemischte Modell ein normalverteiltes Outcome \\(y\\). In Abbildung 44.2 sehen wir das Histogramm der Verteilung des Testscores für alle \\(N = 480\\) Drachen.\n\nggplot(dragons_tbl, aes(test_score)) +\n  geom_histogram() +\n  theme_bw() \n\n\n\nAbbildung 44.2— Histogramm des Testscores über alle Datenpunkte zur Überprüfung der Annahme der Normalverteilung. Wir sehen eine approximative Normalverteilung des Testscores.\n\n\n\nWir können in der Abbildung 44.3 auch nochmal schauen, ob die Annahme der annährenden Normalverteilung für unseren Testscore auch für jedes Level unseres Faktors der Körperlängen gegeben ist. Wir sehen auch hier, dass der Testscore einer Normalverteilung über alle Kategorien der Körperlänge folgt.\n\nggplot(dragons_tbl, aes(y = body_length_cat, x = test_score, fill = body_length_cat)) +\n  theme_bw() +\n  stat_density_ridges() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung 44.3— Histogramm des Testscores aufgeteilt nach Kategorie der Körpergröße zur Überprüfung der Annahme der Normalverteilung. Wir sehen eine approximative Normalverteilung des Testscores für alle Kategorien der Körpergröße.\n\n\n\nNatürlich können wir uns hier noch weitere Abbildungen erstellen, aber hier soll es erstmal reichen. Wir sehen, dass der Testscore einer Normalverteilung folgt und dass die Varianzen vermutlich homogen sind, da die Histogramme ungefähr gleich breit sind. Ja, ein wenig unterscheiden sich die Verteilungen, aber so gravierend ist es erstmal nicht."
  },
  {
    "objectID": "stat-modeling-mixed.html#modellierung",
    "href": "stat-modeling-mixed.html#modellierung",
    "title": "44  Lineare gemischte Modelle",
    "section": "\n44.4 Modellierung",
    "text": "44.4 Modellierung\nIm Folgenden wollen wir uns verschiedene statistische Modelle anschauen um uns dem linearen gemischten Modell einmal anzunähern. Dabei beginnen wir mit einem simplen Gaussian lineare Modell mit einem Faktor \\(f_1\\):\n\\[\ny \\sim f_1\n\\]\nWir haben also nur einen Faktor in unserem Modell vorliegen und ignorieren die restlichen in den Daten vorhandenen Variablen.\nAls zweites Modell betrachten wir eine multiples Gaussian lineares Modell mit einem Faktor \\(f_1\\) und einem Blockfaktor \\(b_1\\):\n\\[\ny \\sim f_1 + b_1\n\\]\nJetzt erweitern wir das Modell nochmal um einen Block oder auch Clustereffekt. Das heißt, wir haben alle Beobachtungen nicht auf einem Feld oder in einem Stall durchgeführt, sondern an mehreren Orten.\nDer eigentliche Bruch kommt jetzt. Wie wollen wir den Effekt des Blocks betrachten? Hier entscheidet sich, ob wir den Block als festen Effekt (eng. fixed effect) oder als zufälligen Effekt (eng. random effect) ausweisen wollen. Zuerst ist dies eine Modellierungsentscheidung. Wir müssen uns also zwischen zwei Modellen entscheiden. Daher können wir auch beide Arten bauen und dann Modelle vergleichen. Machen wir dann auch am Ende des Kapitels.\n\n\nDie Idee hinter dem Modell mit festen Effekten ist, dass die beobachteten Effektgrößen von Block zu Block variieren können, was aber nur auf den Stichprobenfehler \\(\\epsilon\\) zurückzuführen ist. In Wirklichkeit sind die wahren Effektgrößen alle gleich: Sie sind fix. (siehe auch The Fixed-Effect Model)\n\nDas Modell der zufälligen Effekte geht davon aus, dass es nicht nur eine wahre Effektgröße gibt, sondern eine Verteilung der wahren Effektgrößen. Das Ziel des Modells mit zufälligen Effekten ist es daher nicht, die eine wahre Effektgröße aller Studien zu schätzen, sondern den Mittelwert der Verteilung der wahren Effekte. (siehe auch The Random-Effect Model)\n\nLineare gemischte Modelle schätzen die subjektspezifischen Auswirkungen (eng. subject-specific). Betrachten wir dabei die folgenden zwei Szenarien nach Allison (2009):\n\n\nSzenario 1: Du bist ein Arzt. Du möchtest wissen, um wie viel ein Cholesterinmedikament die Wahrscheinlichkeit eines Herzinfarkts bei deinem Patienten senkt.\n\nSzenario 2: Du bist ein staatlicher Gesundheitsbeamter. Du möchtest wissen, wie sich die Zahl der Menschen, die an einem Herzinfarkt sterben, verändern würde, wenn alle Menschen in der Risikogruppe das Cholesterinmedikament einnehmen würden.\n\nIm ersten Szenario wollen wir die subjektspezifischen (eng. subject-specific) Chancen wissen. Im zweiten Fall sind wir an der Vorhersage für die gesamte Bevölkerung interessiert. Lineare gemischte Modelle können uns Schätzungen für das erste, aber nicht für das zweite Szenario liefern.\nDaher kommt jetzt als drittes Model ein multiples Gaussian lineares gemischtes Modell mit einem festen Faktor \\(f_1\\) und einem zufälligen Blockfaktor \\(z_1\\):\n\\[\ny \\sim f_1 + 1|z_1\n\\]\nWir schreiben in R den Term für da zufällige Modell in der Form \\(z_0|z_1\\). Meist setzen wir den Intercept \\(z_0\\) für den zufälligen Effekt auf 1.\nAbschießend schauen wir uns noch ein multiples Gaussian lineares gemischtes Modell mit einem festen Faktor \\(f_1\\) und einem zufälligen Blockfaktor \\(z_2\\) genested in einem einem zufälligen Blockfaktor \\(z_1\\):\n\\[\ny \\sim f_1 + 1|z_1/z_2\n\\]\nWir sagen nested, wenn wir meinen, dass ein Faktor in einen anderen Faktor verschränkt ist. Die Klassen einer Schule sind in der Schule genested.\nDas heißt, dass der zufällige Blockfaktor \\(z_2\\) in den zufälligen Blockfaktor \\(z_1\\) genested ist. Das heist, die Faktorlevel des Blockfaktors \\(z_2\\) finden sich jeweils nur in jeweils einem der Faktorlevel des Blocks \\(z_1\\). Das klingt jetzt etwas schräg, also einmal ein Beispiel. Wir haben eine Schule, dann sind die Schulklassen dieser Schule in der Schule genested. Es gibt diese spezifischen Klassen mit den Schülern schlichtweg nicht in anderen Schulen.\nBevor wir jetzt mit dem Modellieren beginnen, müssen wir noch kurz in einem QQ-Plot schauen, ob unser Ourcome testscore auch ungefähr normalverteilt ist. Abbildung 44.4 zeigt den QQ-Plot des Testscores. Wir sehen, dass der Hauptteil der Beobachtungen auf der Geraden liegt und wir nehmen daher an, dass der Testscore zumindest approximativ normalverteilt ist. Wir können also mit einem gaussian linearen gemischten Modell weitermachen.\n\nggplot(dragons_tbl, aes(sample = test_score)) +\n  stat_qq() + stat_qq_line(color = \"red\") +\n  theme_bw() +\n  scale_color_okabeito()\n\n\n\nAbbildung 44.4— QQ-Plot zu Überprüfung, ob der Testscore einer Normalverteilung folgt. Die Punkte liegen ungefähr auf der Geraden als Winkelhalbierende, so dass wi eine Normalverteilung des Testscores annehmen können.\n\n\n\nSchauen wir uns nun als erstes das Modell lm_simple_fit einmal an. Wir bauen das Modell nur mit der Faktorvariable body_length_cat. Wir erhalten dann gleich die Ausgabe des Modells über die Funktion model_parameters() in einer aufgearbeiteten Form.\n\nlm_simple_fit &lt;- lm(test_score ~ body_length_cat, data = dragons_tbl)\n\nlm_simple_fit %&gt;% model_parameters()\n\nParameter                  | Coefficient |   SE |         95% CI | t(475) |      p\n----------------------------------------------------------------------------------\n(Intercept)                |       24.61 | 4.33 | [16.10, 33.12] |   5.68 | &lt; .001\nbody length cat [small]    |        2.63 | 5.39 | [-7.96, 13.21] |   0.49 | 0.626 \nbody length cat [medium]   |       24.89 | 4.68 | [15.70, 34.07] |   5.32 | &lt; .001\nbody length cat [large]    |       32.50 | 4.55 | [23.56, 41.43] |   7.15 | &lt; .001\nbody length cat [gigantic] |       29.32 | 5.20 | [19.10, 39.54] |   5.64 | &lt; .001\n\n\nDer Intercept beinhaltet den Mittelwert für die Drachen des Levels [tiny]. Die jeweiligen Koeffizienten dann die Abweichung von den Drachen des Levels [tiny]. Daher sind Drachen des Levels [small] ungefähr um \\(2.63\\) Einheiten intelligenter. Wir sehen dann an dem \\(p\\)-Wert, ob sich die Koeffizienten signifikant von 0 unterscheiden. In Abbildung 44.5 sehen wir nochmal die Boxplots der einzelnen Testscores aufgeteilt nach der Körpergröße. Wir erkennen, dass die kleineren Drachen tendenziell dümmer sind als die großen Drachen. Wir sehen zwei Plateaus.\n\nggplot(dragons_tbl, aes(x = body_length_cat, y = test_score, fill = body_length_cat)) +\n  theme_bw() +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung 44.5— Boxplots der einzelnen Testscores aufgeteilt nach der Körpergröße.\n\n\n\nNun haben wir aber nicht nur die Körpergrößen gemessen sondern auch auf welchem Berg wir die jeweiligen Drachen gefunden haben. Nun könnte es sein, dass der Berg einen viel größeren Einfluss auf die Inteliegenz hat als die Drachenkörpergröße. Wir könnten einen Confoundereffekt durch die Berge vorliegen haben. Ergänzen wir also das Modell um den Faktor mountain_range und erhalten das Modell lm_mountain_fit.\n\nlm_mountain_fit &lt;- lm(test_score ~ body_length_cat + mountain_range, data = dragons_tbl)\nlm_mountain_fit %&gt;% model_parameters()\n\nParameter                  | Coefficient |   SE |         95% CI | t(468) |      p\n----------------------------------------------------------------------------------\n(Intercept)                |       20.93 | 3.34 | [14.36, 27.49] |   6.27 | &lt; .001\nbody length cat [small]    |        1.67 | 3.89 | [-5.98,  9.32] |   0.43 | 0.668 \nbody length cat [medium]   |        3.55 | 3.72 | [-3.76, 10.85] |   0.95 | 0.341 \nbody length cat [large]    |        3.59 | 4.30 | [-4.86, 12.03] |   0.83 | 0.405 \nbody length cat [gigantic] |        0.08 | 4.85 | [-9.45,  9.60] |   0.02 | 0.988 \nmountain range [Ligurian]  |       17.33 | 3.58 | [10.28, 24.37] |   4.83 | &lt; .001\nmountain range [Emmental]  |       15.91 | 3.63 | [ 8.79, 23.04] |   4.39 | &lt; .001\nmountain range [Central]   |       35.62 | 3.69 | [28.36, 42.88] |   9.64 | &lt; .001\nmountain range [Maritime]  |       48.75 | 3.24 | [42.39, 55.11] |  15.06 | &lt; .001\nmountain range [Southern]  |        8.47 | 2.74 | [ 3.08, 13.85] |   3.09 | 0.002 \nmountain range [Julian]    |       45.74 | 3.86 | [38.15, 53.33] |  11.85 | &lt; .001\nmountain range [Sarntal]   |       41.03 | 3.30 | [34.54, 47.53] |  12.42 | &lt; .001\n\n\nWie wir sehen, werden nun die Körpergrößen der Drachen nicht mehr als signifikant ausgegeben. Die Effekte der Körpergröße auf den Testscore sind auch viel kleiner geworden, wenn wir die mountain_range mit in das Modell nehmen. Anscheinend hat der Berg auf dem wir den Drachen getroffen haben einen viel größeren Einfluss auf die Intelligenz als die Körpergröße. Wir können uns den Zusammenhang zwischen dem Testscore und dem Berg auch in der Abbildung 44.6 einmal anschauen.\nEigentlich würden wir erwarten, dass es keinen Effekt der Berge auf den Testscore der Drachen gibt. Es müsste eigentlich egal sein, wo wir einen Drachen befragen, wenn wir nur an der Körpergröße und dem Testscore interessiert sind. Wir sehen jedoch in der Abbildung 44.6 einen klaren Unterschied zwischen den Bergen im Bezug auf den Testscore.\n\nggplot(dragons_tbl, aes(mountain_range, test_score, fill = mountain_range)) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito()\n\n\n\nAbbildung 44.6— Boxplots der einzelnen Testscores aufgeteilt nach dem Berg der Befragung.\n\n\n\nIn der Abbildung 44.7 sehen wir den Zusammenhang von Testscore und der Körpergröße sowie den Bergen auf denen das Interview stattgefunden hat. so langsam dämmert uns warum wir hier einen Effekt der Körperlänge zu dem Testscore sehen. Die kleineren Drache sind alle nur auf bestimmten Bergen zu finden! Betrachten wir die Berge mit in dem Modell, dann hat die Körpergröße keinen Einfluß mehr.\n\nggplot(dragons_tbl, aes(x = body_length_cat, y = test_score, fill = mountain_range)) +\n  geom_boxplot(position = position_dodge(preserve = \"single\")) +\n  theme_bw() +\n  scale_fill_okabeito() +\n  labs(fill = \"Mountain\")\n\n\n\nAbbildung 44.7— Boxplots der einzelnen Testscores aufgeteilt nach dem Berg der Befragung und der Körpergröße.\n\n\n\nDer Zusammenhang wird vielleicht in Abbildung 44.8 nochmal klarer. Hier schauen wir uns den Zusamenhang wieder für die Körperlänge getrennt für die Berge an. Nur zeichnen wir jetzt jeden einzelnen Berg in ein Subplot. Wir sehen, dass es hier fast keinen Unterschied macht, wie lang die Drachen sind. Der Testscore ist immer gleich. Was einen Unterschied macht, sind die Berge.\n\nggplot(dragons_tbl, aes(x = body_length_cat, y = test_score, fill = mountain_range)) +\n  geom_boxplot(position = position_dodge(preserve = \"single\")) +\n  theme_bw() +\n  scale_fill_okabeito() +\n  labs(fill = \"Mountain\") +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ mountain_range) \n\n\n\nAbbildung 44.8— Boxplots der einzelnen Testscores aufgeteilt nach dem Berg der Befragung und der Körpergröße in getrennten Subplots.\n\n\n\nSchauen wir uns nun einmal ein lineares gemischtes Modell an. Wir nutzen daszu das R Paket lme4. Wir haben auch noch andere Pakete zur Aswahl, aber wir nutzen hier erstmal das gängiste Paket. Um ein lineares gemischtes Modell in R zu schätzen nutzen wir die Funktion lmer(). Die Funktion lmer() nimmt an, dass das Outcome test_score normalverteilt ist. Wir haben diese Annahme ja weiter oben in dem QQ-Plot überprüft.\nIn einem lineare gemischten Modell müssen wir die festen Effekte sowie die zufälligen Effekte definieren. Die festen Effekte werden ganz normal wie wir es gewohnt sind in das Modell eingegeben. Die zufälligen Effkete schreiben wir in eine Klammer in der Form (1|).\nWir schreiben (1|moutain_range) und definieren damit die Variable mountain_range als zufälligen Effekt im Modell. Wir schreiben 1| vor mountain_range, da wir für jeden Berg die gleiche Steigung von Körperlänge und Testscore annehmen. Wir können dann später noch das Model komplizierter aufbauen und jedem Berg eine eigene Steigung erlauben. Bauen wir uns jetzt erstmal ein lineares gemischtes Modell mit einem festen Effekt body_length_cat und einem zufälligen Effekt (1|mountain_range).\n\nlmer_1_fit &lt;- lmer(test_score ~ body_length_cat + (1 | mountain_range), data = dragons_tbl)\nlmer_1_fit %&gt;% model_parameters()\n\n# Fixed Effects\n\nParameter                  | Coefficient |   SE |         95% CI | t(473) |      p\n----------------------------------------------------------------------------------\n(Intercept)                |       46.85 | 7.43 | [32.25, 61.45] |   6.30 | &lt; .001\nbody length cat [small]    |        1.68 | 3.89 | [-5.97,  9.33] |   0.43 | 0.666 \nbody length cat [medium]   |        4.08 | 3.71 | [-3.20, 11.37] |   1.10 | 0.271 \nbody length cat [large]    |        4.49 | 4.27 | [-3.90, 12.88] |   1.05 | 0.293 \nbody length cat [gigantic] |        1.03 | 4.81 | [-8.43, 10.49] |   0.21 | 0.831 \n\n# Random Effects\n\nParameter                      | Coefficient\n--------------------------------------------\nSD (Intercept: mountain_range) |       18.21\nSD (Residual)                  |       14.96\n\n\nUnser Model sieht etwas aufgeräumter aus. Als feste Effekte haben wir nur noch die Körperlänge body_length_cat und die dazugehörigen Koeffizienten des Modells. Unsere Variable mountain_range verschwindet dann in den zufälligen Effekten. Die Funktion summary liefert uns den gesamten Ausdruck, der etwas überwältigend ist. Vieles brauchen wir auch nicht davon.\n\nlmer_1_fit %&gt;% summary()\n\n\n\n\n\nWas wir extrahieren wollen ist die Information von den zufälligen Effekten. Wir wollen wissen, wieviel Varianz durch die zufälligen Effekte erklärt wird. Wir nutzen dazu die Funktion VarCorr(), die uns erlaubt die Information zu en zufälligen Effekten zu extrahieren und auszugeben.\n\nprint(VarCorr(lmer_1_fit), comp = \"Variance\")\n\n Groups         Name        Variance\n mountain_range (Intercept) 331.42  \n Residual                   223.83  \n\n\nWieviel Varianz erklären nun die Berge? Wir können die erklärte Varianz der zufälligen Effekte einfach berechnen. Wir vergleichen die erklärte Varianz von mountain_range mit der gesamten Varianz. Die gesamte Varianz ist die Varianz aller zufälligen Effekte plus der residualen Vamrianz. Wir erhalten dann \\(R^2_{random} = 339.7/(339.7 + 223.8) \\approx 0.60\\). Wir sehen, dass ca. 60% der Varianz in unseren Daten von der Variable mountain_range verursacht wird.\nWir können die Funktion model_performance() nutzen um mehr über den Fit des Modells zu erfahren. Das R2 (cond.) ist faktisch das gleiche wie wir gerade oben berechnet haben. Wir benötigen also nicht immer den Ausdruck der zufälligen Effekte. Wir können auch die Informationen aus der Funktion model_performance() nehmen.\n\nlmer_1_fit %&gt;% model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n----------------------------------------------------------------------------------\n3983.403 | 3983.640 | 4012.620 |      0.598 |      0.004 | 0.597 | 14.774 | 14.961\n\n\nIn der Abbildung Abbildung 44.9 schauen wir uns nochmal an, ob wir das Modell auch gut gefittet haben. Der Residualplot sieht gut aus, wir erkennen kein Muster. Ebenso sieht der QQ-Plot gut aus, die Beobachtungen liegen alle auf der Geraden. Wir sind mit dem Modell soweit erstmal ganz zufrieden.\n\n\n\n\n\n(a) Residualplot\n\n\n\n\n\n(b) QQ-Plot\n\n\n\nAbbildung 44.9— Visuelle Überprüfung des Modells mit dem Residual und QQ-Plot.\n\n\nWir haben noch eine Variable in unseren Daten ignoriert. Wir haben uns bis jetzt nicht die Variabl site angeschaut. Auf jedem Berg haben wir die Drachen noch auf verschiedenen Flanken des Berges site befragt. Das heißt, wir haben die Variable site, die in der Variable mountain_site genestet ist. Wir schreiben daher ein neues Modell und nutzen die Schreibweise (1|mountain_range/site) um zu beschreiben, dass site immer zusamen in einem Berg vorkommt. Schaue dir dazu nochmal die Abbidlung ganz zu Beginn dieses Kapitels an um die Zusammenhänge nochmal visualisiert zu bekommen.\n\nlmer_2_fit &lt;- lmer(test_score ~ body_length_cat + (1|mountain_range/site), data = dragons_tbl) \nlmer_2_fit %&gt;% model_parameters()\n\n# Fixed Effects\n\nParameter                  | Coefficient |   SE |          95% CI | t(472) |      p\n-----------------------------------------------------------------------------------\n(Intercept)                |       46.89 | 7.80 | [ 31.56, 62.22] |   6.01 | &lt; .001\nbody length cat [small]    |        1.32 | 3.99 | [ -6.51,  9.16] |   0.33 | 0.740 \nbody length cat [medium]   |        3.34 | 4.61 | [ -5.73, 12.41] |   0.72 | 0.470 \nbody length cat [large]    |        4.85 | 5.15 | [ -5.26, 14.97] |   0.94 | 0.346 \nbody length cat [gigantic] |        1.37 | 5.83 | [-10.08, 12.83] |   0.24 | 0.814 \n\n# Random Effects\n\nParameter                           | Coefficient\n-------------------------------------------------\nSD (Intercept: site:mountain_range) |        4.79\nSD (Intercept: mountain_range)      |       17.88\nSD (Residual)                       |       14.46\n\n\nDas Modell hat nun einen weiteren zufälligen Effekt. Es werden jetzt auch nochmal für jeden Berg die Flankeneffekte mit berücksichtigt. Hat das überhaupt einen Einfluss auf das Modell? Schauen wir uns einmal die Modellgüte mit der Funktion model_performance() an.\n\nlmer_2_fit %&gt;% model_performance()\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n----------------------------------------------------------------------------------\n3970.693 | 3970.999 | 4004.084 |      0.623 |      0.004 | 0.621 | 14.120 | 14.460\n\n\nWir sehen, dass sich die erklärte varianz leicht erhöht hat. Die \\(R^2_{random}\\) liegt jetzt bei \\(0.623\\) also fast 62%. Etwas besser als vorher, aber auch nicht unbedingt sehr viel mehr.\nWie können wir nun unsere vier Modelle miteinander vergleichen? Wir haben ja folgende Modelle vorliegen:\n\nDas simple lineare Modell lm_simple_fit mit test_score ~ body_length_cat.\nDas multiple lineare Modell lm_mountain_fit mit test_score ~ body_length_cat + mountain_range.\nDas gemischte lineare Modell lmer_1_fit mit test_score ~ body_length_cat + (1|mountin_range).\nDas genestete gemischte lineare Modell lmer_2_fit mit test_score ~ body_length_cat + (1|mountain_range/site).\n\nUm die Modelle miteinander zu vergleichen können wir die Funktion compare_performance() nutzen. Wir erhalten mit der Option rank = TRUE auch eine Sortierung der Modelle wieder. Das beste Modell steht dann ganz oben.\n\ncompare_performance(lm_simple_fit, lm_mountain_fit, lmer_1_fit, lmer_2_fit, rank = TRUE)\n\n# Comparison of Model Performance Indices\n\nName            |   Model |   RMSE |  Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n----------------------------------------------------------------------------------------------------------\nlm_mountain_fit |      lm | 14.772 | 14.960 |       1.000 |        1.000 |       0.330 |            86.49%\nlmer_2_fit      | lmerMod | 14.120 | 14.460 |    5.84e-05 |     7.41e-05 |       0.655 |            60.00%\nlmer_1_fit      | lmerMod | 14.774 | 14.961 |    1.72e-07 |     2.26e-07 |       0.016 |            36.89%\nlm_simple_fit   |      lm | 20.661 | 20.770 |    1.24e-67 |     1.68e-67 |    9.06e-62 |             0.00%\n\n\nIn diesem Beispiel wäre sogar eine multiple lineare Regression das beste Modell. Wir würden also auch mit zwei festen Effekten die Variabilität der Berge richtig mdellieren. Der Effekt der Flanken auf den Testscore scheint ziemlich klein zu sein, so dass wir auch auf die Variable site verzichten können.\nWas machen wir jetzt noch zum Schluß? Wir machen noch einen paarweisen Vergleich über alle Level der Vaeiable body_length_cat. Ich will hier nochmal zeigen, wie du einen multiplen Vergleich mit einem gemischten Modell in R rechnen kannst. Wir nutzen hier dann das R Paket emmeans um das compact letter display nutzen zu können.\nAls erstes nutzen wir die Funktion emmeans um die multiplen Vergleich über alle Level des Faktors body_length_cat zurechnen.\n\nres_lmer &lt;- lmer_2_fit %&gt;% \n  emmeans(~ body_length_cat) \n\nIm Weiteren nutzen wir jetzt das Objekt res_lmer um die Vergleiche zu rechnen und zu asjustieren. Wir nutzen die Bonferroni Methode für die Adjustierung der \\(p\\)-Werte.\n\nres_lmer %&gt;% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") \n\n contrast          estimate   SE  df t.ratio p.value\n tiny - small       -1.3218 4.02 453  -0.329  1.0000\n tiny - medium      -3.3379 4.76 118  -0.701  1.0000\n tiny - large       -4.8531 5.31 148  -0.914  1.0000\n tiny - gigantic    -1.3744 6.02 152  -0.228  1.0000\n small - medium     -2.0161 3.83 134  -0.526  1.0000\n small - large      -3.5313 4.50 179  -0.784  1.0000\n small - gigantic   -0.0526 5.32 175  -0.010  1.0000\n medium - large     -1.5152 2.39 362  -0.634  1.0000\n medium - gigantic   1.9635 3.71 229   0.529  1.0000\n large - gigantic    3.4787 2.95 241   1.178  1.0000\n\nDegrees-of-freedom method: kenward-roger \nP value adjustment: bonferroni method for 10 tests \n\n\nWenn wir an dem compact letter display interessiert sind, dann müsen wir die Funktion cld() nutzen. Was wir brauchen, hängt dann immer davon ab, was wir zeigen wollen und was die Fragestellung ist.\n\nres_lmer_cld &lt;- res_lmer %&gt;% \n  cld(adjust = \"bonferroni\", Letters = letters) %&gt;% \n  tidy() %&gt;% \n  select(body_length_cat, estimate, conf.low, conf.high, .group) %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nres_lmer_cld \n\n# A tibble: 5 × 5\n  body_length_cat estimate conf.low conf.high .group\n  &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n1 tiny                46.9     23.6      70.2 \" a\"  \n2 small               48.2     25.5      71.0 \" a\"  \n3 gigantic            48.3     25.8      70.7 \" a\"  \n4 medium              50.2     27.8      72.7 \" a\"  \n5 large               51.7     29.2      74.2 \" a\"  \n\n\nAn dem compact letter display sehen wir schon, dass es keinen Unterschied zwischen den Gruppen bzw. Leveln des Faktors body_length_cat gibt. Wir sehen bei allen Leveln ein a. Wir haben keine signifikante Unterschiede.\nIn Abbildung 44.10 siehst du nochmal die Daten zusammen mit dem compact letter display dargestellt.\n\nggplot() +\n  theme_bw() +\n  geom_point(data = dragons_tbl, aes(x = body_length_cat, y = test_score)) +\n  geom_text(data = res_lmer_cld, \n            aes(x = body_length_cat , y = estimate, label = .group),\n            position = position_nudge(x = 0.2), color = \"red\") +\n  geom_errorbar(data = res_lmer_cld,\n                aes(ymin = conf.low, ymax = conf.high, x = body_length_cat),\n                color = \"red\", width = 0.1,\n                position = position_nudge(x = 0.1)) +\n  geom_point(data = res_lmer_cld, \n             aes(x = body_length_cat , y = estimate),\n             position = position_nudge(x = 0.1), color = \"red\") +\n  scale_color_okabeito() +\n  labs(x = \"Körperlänge in Kategorien\", y = \"Testscore\", \n       caption = \"Schwarze Punkte stellen Rohdaten dar.\n       Rote Punkte und Fehlerbalken stellen bereinigte Mittelwerte mit 95% Konfidenzgrenzen pro Behandlung dar.\n       Mittelwerte, mit einem gemeinsamen Buchstaben, sind nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 44.10— Scatterplot der Körperlängen zusammen mit den 95% Konfidenzintervall und dem compact letter display.\n\n\n\nManchmal wollen wir auch die 95% Konfidenzintervalle anzeigen, dann müssen wir wiederum die Funktion contrast() nutzen. Wir lassen uns auch hier die adjustoerten \\(p\\)-Werte wiedergeben. Wir nutzen dann das Objekt res_lmer_tbl um die 95% Konfidenzintervalle zu plotten.\n\nres_lmer_tbl &lt;- res_lmer %&gt;% \n  contrast(method = \"pairwise\") %&gt;% \n  tidy(conf.int = TRUE) %&gt;% \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) %&gt;% \n  select(contrast, estimate, p.value,\n         conf.low, conf.high) \n\nres_lmer_tbl\n\n# A tibble: 10 × 5\n   contrast          estimate p.value conf.low conf.high\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 tiny - small         -1.32 0.997     -12.3       9.68\n 2 tiny - medium        -3.34 0.956     -16.5       9.85\n 3 tiny - large         -4.85 0.891     -19.5       9.81\n 4 tiny - gigantic      -1.37 &gt;0.999    -18.0      15.2 \n 5 small - medium       -2.02 0.985     -12.6       8.58\n 6 small - large        -3.53 0.935     -15.9       8.87\n 7 small - gigantic     -0.05 &gt;0.999    -14.7      14.6 \n 8 medium - large       -1.52 0.969      -8.06      5.03\n 9 medium - gigantic     1.96 0.984      -8.23     12.2 \n10 large - gigantic      3.48 0.764      -4.64     11.6 \n\n\nIn Abbildung 44.11 sehen wir die 95% Konfidenzintervalle für alle paarweisen Vergleiche der Körperlängen.\n\nggplot(res_lmer_tbl, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1) + \n  geom_point() +\n  coord_flip() +\n  theme_bw()  +\n  labs(x = \"Vergleich\", y = \"Mittelwertsunterschied des Gewichtes [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die bereinigten Mittelwertsunterschiede mit 95% Konfidenzgrenzen dar.\n       Enthält ein 95% Konfidenzintervalle die 0 ist es nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 44.11— Abbildung der 95% Konfidenzintervallefür alle paarweisen Vergleiche der Körperlängen."
  },
  {
    "objectID": "stat-modeling-mixed.html#referenzen",
    "href": "stat-modeling-mixed.html#referenzen",
    "title": "44  Lineare gemischte Modelle",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nAllison, Paul D. 2009. Fixed effects regression models. SAGE publications."
  },
  {
    "objectID": "stat-modeling-gee.html#annahmen-an-die-daten",
    "href": "stat-modeling-gee.html#annahmen-an-die-daten",
    "title": "45  Generalized Estimating Equations (GEE)",
    "section": "\n45.1 Annahmen an die Daten",
    "text": "45.1 Annahmen an die Daten\nIm folgenden Kapitel zu den Generalized Estimating Equations (GEE) gehen wir davon aus, dass die Daten in der vorliegenden Form ideal sind. Das heißt wir haben weder fehlende Werte vorliegen, noch haben wir mögliche Ausreißer in den Daten. Auch wollen wir keine Variablen selektieren. Wir nehmen alles was wir haben mit ins Modell. Sollte eine oder mehre Bedingungen nicht zutreffen, dann schaue dir einfach die folgenden Kapitel an.\n\nWenn du fehlende Werte in deinen Daten vorliegen hast, dann schaue bitte nochmal in das Kapitel 38 zu Imputation von fehlenden Werten.\nWenn du denkst, dass du Ausreißer oder auffälige Werte in deinen Daten hast, dann schaue doch bitte nochmal in das Kapitel 37 zu Ausreißer in den Daten.\nWenn du denkst, dass du zu viele Variablen in deinem Modell hast, dann hilft dir das Kapitel 36 bei der Variablenselektion.\n\nGrundsätzlich ist das Thema GEE eher ein stiefmütterliches statistisches Thema. Ich selber habe gar nicht so viel zu GEE’s gefunden, so dass wie immer gilt: Augen auf im statistischen Straßenverkehr! Besonders die Variablenselektion, die ja an die Modellklasse gebunden ist, mag nicht so funktionieren wie gewollt. Bitte bei GEE Fragestellungen keine automatisierte Selektion anwenden. Dann lieber über compare_models() aus dem R Paket parameters die Modellvergleiche direkt vergleichen."
  },
  {
    "objectID": "stat-modeling-gee.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-gee.html#genutzte-r-pakete-für-das-kapitel",
    "title": "45  Generalized Estimating Equations (GEE)",
    "section": "\n45.2 Genutzte R Pakete für das Kapitel",
    "text": "45.2 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               parameters, performance, geepack, gee,\n               geesmv, multcomp, emmeans, scales)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\nconflict_prefer(\"set_names\", \"magrittr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-gee.html#daten",
    "href": "stat-modeling-gee.html#daten",
    "title": "45  Generalized Estimating Equations (GEE)",
    "section": "\n45.3 Daten",
    "text": "45.3 Daten\n\n\n\n\n\n\nDu musst deine Daten nach der ID sortieren\n\n\n\nGanz wichtig, sonst funktioniert das GEE nicht und du kriegst auch keine Warnmeldung! Du musst die Daten mit arrange für deine ID Spalte sortieren.\n\n\nIch habe hier ienmal zwei Datenbeispiel mitgebracht. Wir werden uns aber im folgenden Abschnitt dann nur die Schweine anschauen, das Kuhbeispiel können wir dann nochmal anderweitig nutzen oder aber du rechnest nochmal selber mit den Kühen. Wichtig hierbei ist, dass wir sicher sind, dass wir die Daten nach der ID Spalte der Tiere sortiert haben. Das heist, dass alle Tier ID’s Zeilen wirklich beieinander stehen. Das ist wichtig, sonst schafft GEE nur eine sehr seltsame Ausgaben zu produzieren. Leider ohne eine Warnung auszugeben. Deshalb nutzen wir die Funktion arrange() um nochmal nach der Spalte pig_id zu sortieren.\n\npig_gain_tbl &lt;- read_excel(\"data/pig_feed_data.xlsx\") %&gt;% \n  mutate(weight_gain = round(weight_gain, 2)) %&gt;% \n  arrange(pig_id)\n\nIn Tabelle 45.1 sehen wir nochmal einen Auszug aus den Daten. Wir haben unsere wiederholte Messung time. Das heißt wir haben unsere Schweine wiederholt gemessen. Jedes Schwein für jede Behandlung fünfmal. Wir brauchen die pig_id um zu wissen, welche Werte der Geichtszunahme dann auch immer zu einem Ferkel gehören. Im Weiteren haben wir noch die Bucht, in der die Ferkel gehalten wurden, notiert. Die Information lassen wir aber hier erstmal im späteren Modell weg.\n\n\n\n\nTabelle 45.1— Auszug aus dem Daten zu den kranken Ferkeln. Jedes Ferkel wurde wiederholt gemessen.\n\ntime\npig_id\ncove\ntreatment\nweight_gain\n\n\n\n1\n1\n1\nfeed_10\n44.29\n\n\n2\n1\n1\nfeed_10\n59\n\n\n3\n1\n1\nfeed_10\n49.96\n\n\n4\n1\n1\nfeed_10\n66.17\n\n\n…\n…\n…\n…\n…\n\n\n2\n120\n10\nfeed_20\n56.17\n\n\n3\n120\n10\nfeed_20\n58.13\n\n\n4\n120\n10\nfeed_20\n68.48\n\n\n5\n120\n10\nfeed_20\n34.99\n\n\n\n\n\n\nDas zweite Datenbeispiel dient zur Veranschaulichung eines weiteres Messwiederholungsbeispiels. Wir haben drei Kühe wiederholt an drei Zeitpunkten gemessen. JEde Kuh hat immer nur die gleiche Behandlung erhalten. Das Outcome ist einmal die Anzahl an Zellen in der Milch pro ml und einmal der Fettgehalt in %. Die Daten sind in der Form relativ übersichtlich. Wir haben leider sehr wenige Messwiederholungen, so dass hier ein GEE oder aber auch ein lineares gemischtes Modell fraglich ist. Wir wollen eigentlich mindesnten fünf Level für den Clusterfaktor. Wir gehen wieder sicher, dass die Daten auch richtig nach ID sortiert sind.\n\nmilk_tbl &lt;- read_csv2(\"data/milk_feeding.csv\") %&gt;% \n  rename(cow_id = id_cow) %&gt;% \n  arrange(cow_id)\n\nIn Tabelle 45.2 sehen wir nochmal den Ausschnitt aus den Milchdaten. Wir haben insgesamt auch nur vierzehn Kühe gemessen, was auch nicht so viele Tiere sind. Im Ferkelbeispiel hatten wir uns 120 Ferkel angeschaut. Deshal ist dieser Datensatz sehr klein für ein komplexes Modell wie GEE.\n\n\n\n\nTabelle 45.2— Auszug aus Daten zu Milchkühen. Jede Kuh wurde wiederholt gemessen.\n\ncow_id\ntrt\ntime_point\ncell_count\nfat_perc\n\n\n\n1\n1\n1\n1932\n0.69\n\n\n1\n1\n2\n6771\n0.94\n\n\n1\n1\n3\n2225\n0.01\n\n\n2\n0\n1\n2572\n0.15\n\n\n…\n…\n…\n…\n…\n\n\n13\n1\n3\n8445\n0.06\n\n\n14\n1\n1\n19707\n0.95\n\n\n14\n1\n2\n9428\n0.5\n\n\n14\n1\n3\n8184\n0.96\n\n\n\n\n\n\nGehen wir einmal auf den theoretischen Hintergrund zu GEE ein und schauen wir mal, wie wir da unser Datenbeispiel zu passt."
  },
  {
    "objectID": "stat-modeling-gee.html#theoretischer-hintergrund",
    "href": "stat-modeling-gee.html#theoretischer-hintergrund",
    "title": "45  Generalized Estimating Equations (GEE)",
    "section": "\n45.4 Theoretischer Hintergrund",
    "text": "45.4 Theoretischer Hintergrund\nGanz wichtig, wir gehen jetzt nicht auf den mathematischen Hintergurnd ein. Das ist auch zu schräg. Das will heißen, dass der mathematische Hintergrund von GEE’s wirklich vieles übersteigt. Ich kann GEE’s anwenden, aber ich weis nicht, wie ein GEE mathematisch funktioniert. Das muss man ja auch nicht. Deshalb hier nur die Theorie, was ein GEE macht und in welchen Hintergründen wir das GEE anwenden. Zuerst schätzt das GEE die durchschnittlichen Auswirkungen auf die Population (eng. population average). Betrachten wir dabei die folgenden zwei Szenarien nach Allison (2009):\n\n\nSzenario 1: Du bist ein Arzt. Du möchtest wissen, um wie viel ein Cholesterinmedikament die Wahrscheinlichkeit eines Herzinfarkts bei deinem Patienten senkt.\n\nSzenario 2: Du bist ein staatlicher Gesundheitsbeamter. Du möchtest wissen, wie sich die Zahl der Menschen, die an einem Herzinfarkt sterben, verändern würde, wenn alle Menschen in der Risikogruppe das Cholesterinmedikament einnehmen würden.\n\nIm ersten Szenario wollen wir die subjektspezifischen (eng. subject-specific) Chancen wissen. Im zweiten Fall sind wir an der Vorhersage für die gesamte Bevölkerung interessiert. GEE kann uns Schätzungen für das zweite, aber nicht für das erste Szenario liefern. Dami sind wir schon recht weit. Wir wollen also nichts über die einzelnen Ferkel wissen, sondern nur über die Gesamtzahl an Ferklen mitteln. Das ist natürlich manachmal gewollt und manchmal eher nicht. In der Zucht kommt es drauf an, ob du individuelle Effekte haben möchtest, also für einen Eber oder eben die Leistung der gesamten Rasse bewerten willst. Je nachdem kannst du dan ein GEE einsetzen oder nicht. GEE’s sind somit für einfaches Clustering oder wiederholte Messungen gedacht. Komplexere Designs wie verschachtelte oder gekreuzte Gruppen, z. B. verschachtelte Messwiederholungen innerhalb eines Probanden oder einer Gruppe, können nicht ohne weiteres berücksichtigt werden. Hier nutzen wir dann wieder gemischte lineare Modelle.\nEin großer Vorteil der GEE ist, dass wir eine Korrelation zwischen den wiederholten Messungen, also Ferkeln, annehmen können. Das heist, wir können die Verwandtschaft oder den zeitlichen Zusammenhang zwischend den Messwiederholungen abbilden. Dafür brauchen wir dann natürlich auch Fallzahl, die schnell mal über die hundert Beobachtungen geht. Wir können dann zwischen folgenden Zusammenhängen der Korrelation entscheiden.\n\n\nindependence, daher sind die Beobachtungen im Zeitverlauf sind unabhängig.\n\nexchangeable, daher haben alle Beobachtungen im Zeitverlauf die gleiche Korrelation \\(\\rho_{const.}\\).\n\nar1, die Korrelation \\(\\rho\\) nimmt als Potenz der Anzahl \\(p\\) der Zeitpunkte, die zwischen zwei Beobachtungen liegen, ab. Daher rechnen wir mit \\(\\rho, \\rho^2, \\rho^3,..., \\rho^p\\) über die Zeitpunkte.\n\nunstructured, daher kann die Korrelation zwischen allen Zeitpunkten unterschiedlich sein.\n\nLeider gibt es keine automatische Auswahl. Wir müssen also überlegen, welche Korrelationmatrix am besten passen würde. Da unstructured sehr viel Fallzahl benötigt um valide zu sein, nehme ich meistens exchangeable, wenn ich ein GEE rechne. Eine unabhänige Korrealtion anzunehmen macht wenig Sinn, dann brauche ich auch kein GEE rechnen. Die Korrelation ist ja die Stärke von einem GEE.\nWir haben nun zwei R Pakete, die beide das gleiche tun, nämlich ein GEE rechnen. Wir haben die Wahl zwischen dem R Paket gee und der Funktion gee() sowie der Funktion geeglm() aus dem R Paket geepack. Ich neige zu dem letzteren Paket. Das R Paket geepack ist etwas neueren Ursprungs und funktioniert bisher recht reibungslos."
  },
  {
    "objectID": "stat-modeling-gee.html#modellieren-mit-gee",
    "href": "stat-modeling-gee.html#modellieren-mit-gee",
    "title": "45  Generalized Estimating Equations (GEE)",
    "section": "\n45.5 Modellieren mit gee()\n",
    "text": "45.5 Modellieren mit gee()\n\nLeider ist es so, dass wir kaum kontrollieren können, was alles aus den Funktionen in die R Console geschrieben wird. Die Funktionen sind schon recht alt und es gab mal einen Trend, dass eine Funktion immer schön was wiedergeben soll. Das ist natürlich immer etwas nervig, wenn man das nicht will. Wir erhalten also bei der Funktion gee() immer die Koeffizienten des Modells ausgegeben, ob wir es nun in einem Objekt speichern oder auch nicht. Ich finde sowas immer ziemlich nervig.\nAlso wir bauen wir uns unser gee Modell? Zuerst kommt wie immer die formula, da ändert sich nichts. Wir nehmen in unser Modell als Outcome die Gewichtszunahme und als Einflusvariablen dann die Behandlung sowie die Zeit und die Interaktion zwischen der Behandlung und der Zeit. Die Daten sind auch gleich. Erst mit der Option id = ändert sich was. Hier geben wir die Spalte ein, in der die ID’s der Ferkel bzw. der Beobachtungen stehen. Das war es auch schon für den CLustereffekt. Dann nach die Verteilungsfamilie, wir können hier auch für nicht normalverteilte Daten ein GEE schätzen. Zum Abschluss noch die Korrelationsstruktur definiert. Wir nehmen hier exchangeable, diese Korrelationsstruktur ist für den Anfang immer ganz gut und macht auch häufig Sinn.\n\ngee_fit &lt;- gee(weight_gain ~ treatment + treatment * time,\n               data = pig_gain_tbl, \n               id = pig_id, \n               family = gaussian,\n               corstr = \"exchangeable\")\n\n             (Intercept)      treatmentfeed_10+10         treatmentfeed_20 \n               56.441300                 1.573500                 3.378825 \n                    time treatmentfeed_10+10:time    treatmentfeed_20:time \n               -2.358900                 0.207000                 0.182875 \n\n\nNachdem wir das Modell gefittet haben, können wir uns einmal die Korrelationsstruktur anschauen. Da ist die Funktion gee wirklich gut. Die Korrelationsstuktur können wir uns einfach so rausziehen.\n\npluck(gee_fit, \"working.correlation\") %&gt;% \n  round(3)\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 1.000 0.082 0.082 0.082 0.082\n[2,] 0.082 1.000 0.082 0.082 0.082\n[3,] 0.082 0.082 1.000 0.082 0.082\n[4,] 0.082 0.082 0.082 1.000 0.082\n[5,] 0.082 0.082 0.082 0.082 1.000\n\n\nWas sehen wir? Natürlich muss auf der Diagonalen eine 1 stehen, den untereinander sind die Variablen ja identisch und damit mit 1 korreliert. Auf der Nicht-Diagonalen finden wir dann die Korrelation untereinander. Da wir exchangeable für die Korrelationsstruktur gewählt haben, haben wir überall die gleiche Korrelation. Alle Ferkel sind untereinander über die Zeitpunkte gleich mit \\(\\rho = 0.82\\) korreliert.\nWir lassen uns jetzt noch die Modellparameter ausgeben und schauen uns einmal an, ob wir was signifikantes gefunden haben.\n\ngee_fit %&gt;% model_parameters()\n\nParameter                     | Coefficient |   SE |         95% CI |     z |      p\n------------------------------------------------------------------------------------\n(Intercept)                   |       56.44 | 1.23 | [52.53, 60.35] | 28.27 | &lt; .001\ntreatment [feed_10+10]        |        1.57 | 1.65 | [-3.96,  7.11] |  0.56 | 0.341 \ntreatment [feed_20]           |        3.38 | 1.66 | [-2.16,  8.91] |  1.20 | 0.041 \ntime                          |       -2.36 | 0.19 | [-3.49, -1.22] | -4.07 | &lt; .001\ntreatment [feed_10+10] × time |        0.21 | 0.24 | [-1.40,  1.81] |  0.25 | 0.386 \ntreatment [feed_20] × time    |        0.18 | 0.25 | [-1.42,  1.79] |  0.22 | 0.456 \n\n\nWir sehen, dass es einen signifikanten Unterschied in der Zeit gibt, das war ja auch zu erwarten, denn mit der Zeit werden die Ferkel schwerer. Wir haben aber auch einen schwach signifikanten Effekt zwischen feed_10 und feed_20 mit einem \\(p\\)-Wert von \\(0.041\\). Hier machen wir kurz Stop, dann geht es aber in dem Abschnitt zu den Posthoc Tests mit dem Modell weiter. Wir wollen ja noch für alle Behanlungslevel einen paarweisen Vergleich rechnen."
  },
  {
    "objectID": "stat-modeling-gee.html#modellieren-mit-geeglm",
    "href": "stat-modeling-gee.html#modellieren-mit-geeglm",
    "title": "45  Generalized Estimating Equations (GEE)",
    "section": "\n45.6 Modellieren mit geeglm()\n",
    "text": "45.6 Modellieren mit geeglm()\n\nDer eigentlcihe Unetrschied zwischen der Funktion gee() und geeglm() ist, dass sich im Hintergrund eine Menge anders abspielt, das wir nicht sehen. Für mich war geeglm() immer schneller und stabiler. Der einzige Grund war immer nochmal ein gee() laufen zu lassen, da sich die Korrelationsmatrix so einfach aus dem gee() Objekt ziehen lassen lässt.\n\ngeeglm_fit &lt;- geeglm(weight_gain ~ treatment + treatment * time,\n                     data = pig_gain_tbl, \n                     id = pig_id, \n                     family = gaussian,\n                     corstr = \"exchangeable\")\n\nWir erhalten zwar auch die geschätzte Korrelation, aber nicht in so einer schönen Matrix. Also ist es dann Geschmackssache. Du weist dann ja, das wir mit exchangeable überall die gleiche Korrelation angenommen haben.\n\npluck(geeglm_fit, \"geese\", \"alpha\")\n\n   alpha \n0.082597 \n\n\nAm Ende schauen wir uns dann nochmal den Fit aus dem geeglm() Modell an. Und stellen fest, dass das Modell numerisch fast identisch ist. Wir haben also nur dir Wahl in der Darstellungsform und in der Geschwindigkeit.\n\ngeeglm_fit %&gt;% model_parameters()\n\nParameter                     | Coefficient |   SE |         95% CI | Chi2(1) |      p\n--------------------------------------------------------------------------------------\n(Intercept)                   |       56.44 | 1.23 | [54.04, 58.84] | 2120.76 | &lt; .001\ntreatment [feed_10+10]        |        1.57 | 1.65 | [-1.66,  4.81] |    0.91 | 0.341 \ntreatment [feed_20]           |        3.38 | 1.66 | [ 0.13,  6.62] |    4.17 | 0.041 \ntime                          |       -2.36 | 0.19 | [-2.73, -1.99] |  153.65 | &lt; .001\ntreatment [feed_10+10] × time |        0.21 | 0.24 | [-0.26,  0.67] |    0.75 | 0.386 \ntreatment [feed_20] × time    |        0.18 | 0.25 | [-0.30,  0.66] |    0.56 | 0.456 \n\n\nWir haben also dann zwei Funktionen, die wir nutzen können. Am Ende kannst du dann beide ausprobieren. Machmal hat man mit geeglm() etwas mehr Glück, wenn die Daten einfach mal nicht wollen."
  },
  {
    "objectID": "stat-modeling-gee.html#multipler-vergleich-mit-emmeans",
    "href": "stat-modeling-gee.html#multipler-vergleich-mit-emmeans",
    "title": "45  Generalized Estimating Equations (GEE)",
    "section": "\n45.7 Multipler Vergleich mit emmeans\n",
    "text": "45.7 Multipler Vergleich mit emmeans\n\nGut, soweit sind wir dann gekommen. Wir haben unser Modell gefittet und meistens wollen wir dann noch einen all-pair Vergleich bzw. den paarweisen Vergleich rechnen. Das machen wir erst einmal mit der Funktionalität aus dem R Paket emmeans, das uns erlaubt auch das compact letter display wiederzugeben. Wenn dich mehr zum Prozess des Codes für die Nutzung von emmeans interessiert, dann schaue doch einfach nochmal ins Kapitel 31. In dem Kapitel zu den multiplen Vergleichen erkläre ich dir nochmal genauer den Funktionsablauf.\nWichtig ist, dass wir unsere Vergleiche mit Bonferroni adjustieren. Wenn du das nicht möchtest, dann musst du adjust = \"none\" auswählen. Sonst machen wir die Ausageb nochmal tidy() und dann runden wir noch. Wir erhalten dann das compact letter display wieder.\n\nres_gee &lt;- geeglm_fit %&gt;% \n  emmeans(~ treatment) \n\nres_gee_cld &lt;- res_gee %&gt;% \n  cld(adjust = \"bonferroni\", Letters = letters) %&gt;% \n  tidy() %&gt;% \n  select(treatment, estimate, conf.low, conf.high, .group) %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\nres_gee_cld \n\n# A tibble: 3 × 5\n  treatment  estimate conf.low conf.high .group\n  &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n1 feed_10        49.4     47.0      51.7 \" a \" \n2 feed_10+10     51.6     49.1      54.0 \" ab\" \n3 feed_20        53.3     51.0      55.6 \"  b\" \n\n\nWenn dich die Abbildungen und weiteres interessieren, dann schaue einfach nochmal ins Kapitel zu den multiplen vergleichen. Dort zeige ich dann wie wir das compact letter display in eine Abbildung ergänzen. Der Ablauf ist auch im Kapitel 44 zu den linearen gemischten Modellen gezeigt.\nWir sehen, dass sich die Gruppe feed_10 von der Gruppe feed_20 unterscheidet. Beide Gruppen haben nicht den gleichen Buchstaben. Die Gruppe feed_10+10 unterscheidet sich weder von der Gruppe feed_10 noch von der Gruppe feed_20. Wir können uns im folgenden Codeblock dann auch die \\(p\\)-Werte für die Vergleiche wiedergeben lassen. Die Aussagen sind die selben.\n\nres_gee_tbl &lt;- res_gee %&gt;% \n  contrast(method = \"pairwise\", adjust = \"bonferroni\") %&gt;% \n  tidy(conf.int = TRUE) %&gt;% \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) %&gt;% \n  select(contrast, estimate, \n         conf.low, conf.high, p.value) \n\nres_gee_tbl\n\n# A tibble: 3 × 5\n  contrast               estimate conf.low conf.high p.value\n  &lt;chr&gt;                     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 feed_10 - (feed_10+10)    -2.19    -5.56      1.17 0.355  \n2 feed_10 - feed_20         -3.93    -7.2      -0.66 0.012  \n3 (feed_10+10) - feed_20    -1.73    -5.06      1.59 0.636  \n\n\nDie Funktion emmeans() hätten wir auch mit dem Modell aus dem gee() Fit nutzen können. Als letzten Abschnitt wollen wir uns jetzt noch eine Besonderheit der GEE Varianzschätzung anschauen."
  },
  {
    "objectID": "stat-modeling-gee.html#multipler-vergleich-mit-multcomp-und-geesmv",
    "href": "stat-modeling-gee.html#multipler-vergleich-mit-multcomp-und-geesmv",
    "title": "45  Generalized Estimating Equations (GEE)",
    "section": "\n45.8 Multipler Vergleich mit multcomp und geesmv\n",
    "text": "45.8 Multipler Vergleich mit multcomp und geesmv\n\nNatürlich haben wir uns nicht in den Details wie ein GEE funktioniert verloren. Es ist nun aber so, dass ein GEE auf sehr unterschiedliche Art und Weise die Korrelationsstruktur und die Varianzen dahinter schätzen kann. Je nach geschätzter Varianz kommen natürlich auch eventuell ganz andere Signifikanzen aus dem Modell. Deshalb hat sich wirklich eine Horde an mathematischen Statistikern an der Varianzschätzung im GEE abgearbeitet.\n\n\ngeesmv: Modified Variance Estimators for Generalized Estimating Equations\nDas R Paket geesmv bietet ganze neun Implementierungen von Schätzern für die Varianz/Covarianzstruktur der Daten an. Jetzt stellt sich die Frage, welche Implementierung für den Varianzschätzer denn nun nutzen? Zum einen hat natürlich die geschätzte Varianz einen nicht zu unterschätzenden Effekt auf die Signifikanz der Koeffizienten des GEE Models. Zum anderen ist aber der multiple Vergleich nach dem Schätzen des Modells und dem getrennten Schätzen der Varianz sehr mühselig. Leider helfen uns auch unsere Standardpakete nicht so richtig weiter. Die Funktionalität ist nicht für geesmv implementiert. Was wiederum dafür spricht, dass der Bedarf von Anwendern sehr eingeschränkt zu seien scheint. Nun müssen wir folgende epischen Schritte durchführen um einen multiplen Vergleich rechnen zu können.\n\nWir fitten unser geeglm() Modell in der mean parametrization, dass heist wir entfernen den Intercept aus dem Modell und lassen unser Modell somit durch den Urspung laufen. Im Prinzip setzen wir den Intercept auf 0 und erhalten so die Mittelwerte jedes Levels des Faktors treatment.\nWir speichern die \\(\\beta\\)-Koeffizienten von dem treatment aus unserem GEE Modell in einem Objekt ab.\nWir rechnen mit der gleichen Angabe wie vorher das geeglm() Modell eine der neun Funktion. Ich habe hier zufällig die Funktion GEE.var.lz() gewählt. Wir speichern die Ausgabe der Varianz der Koeffizienten in einem Objekt.\nWir kombinieren die \\(\\beta\\)-Koeffizienten und die Varianz in einem Objekt mit der Funktion left_join().\nWir bauen uns unsere eigene Kontrastmatrix in der steht welches Level der Behandlung mit welchen anderen Level verglichen werden soll.\nWir übergeben alle Einzelteile an die Funktion glht() aus dem R Paket multcomp und rechnen unseren multiplen Vergleich.\n\nNa dann mal auf. Gehen wir die Schritte einmal nacheinander durch und schauen, was wir da so alles gemacht haben. Nochmal Achtung, hier musst du wirklich schauen, ob sich der Aufwand lohnt. Ich zeige es hier einmal, den in bestimmten Fällen kann sich eine andere Implementierung für die Schätzung der Varianz durchaus lohnen. Denn aus Erfahrung weiß ich, dass der Standardvarianzschätzer nicht immer der beste Schätzer sein muss (Kruppa und Hothorn 2021).\nIm Folgenden schätzen wir einmal ein ganz normales GEE Modell mit der Funktion geeglm(). Wir werden aber nur die Koeffizienten brauchen. Die Varianz der Koeffizienten nutzen wir nicht. Ebenso brauchen wir die mean Parametrisierung, dass heißt wir setzen den Intercept auf 0.\n\ngeeglm_fit &lt;- geeglm(weight_gain ~ 0 + treatment + treatment * time,\n                     data = pig_gain_tbl, \n                     id = pig_id, \n                     family = gaussian,\n                     corstr = \"exchangeable\")\n\nWir speichern einmal die Koeffizienten in dem Objekt beta_tbl. Die brauchen wir später um die paarweisen Vergleiche zu rechnen.\n\nbeta_tbl &lt;- coef(geeglm_fit) %&gt;% \n  enframe\n\nUm die Varianz der Koeffizienten zu schätzen nutzen wir jetzt eine der Implementierungen in geesmv. Ich habe mich etwas zufällig für die Implementierung GEE.var.lz() entschieden. Diese Funktion liefer nur die Varianz der Koeffizienten. Leider aber nicht auch gleich noch die Koeffizienten dazu… deshalb der blöde doppelte Schritt. Wir speichern dann die Varianzen in dem Objekt vbeta_tbl.\n\ngee_lz_vcov &lt;- GEE.var.lz(weight_gain ~ 0 + treatment + treatment * time,\n                          data = as.data.frame(pig_gain_tbl), \n                          id = \"pig_id\",\n                          family = gaussian,\n                          corstr = \"independence\") \n\n        treatmentfeed_10      treatmentfeed_10+10         treatmentfeed_20 \n               56.441300                58.014800                59.820125 \n                    time treatmentfeed_10+10:time    treatmentfeed_20:time \n               -2.358900                 0.207000                 0.182875 \n\nvbeta_tbl &lt;- pluck(gee_lz_vcov, \"cov.beta\") %&gt;% \n  enframe\n\nJetzt verbinden wir noch die beiden Objekte beta_tbl und vbeta_tbl über die Funktion left_join(). Wir können mit der Funktion zwei Datensätze nach einer gemeinsamen Spalte zusammenführen. Dann müssen zwar die Einträge in der Spalte gleich sein, aber die Sortierung kann anders sein. Dann müssen wir noch die Zeilen rausfiltern in denen die Behandlungsmittelwerte sind. Am Ende benennen wir die Spalten noch sauber nach dem was die Spalten sind.\n\ncoef_tbl &lt;- left_join(beta_tbl, vbeta_tbl, by = \"name\") %&gt;% \n  filter(str_detect(name, \"time\", negate = TRUE)) %&gt;% \n  set_names(c(\"parameter\", \"beta\", \"vbeta\"))\n\nDas war jetzt ein Angang. Leider geht es nicht so einfach weiter. Wir müssen uns für die Vergleiche die Kontrastmatrix selberbauen. Wir machen einen paarweisen Vergleich, also wählen wir den Tukey Kontrast aus.\n\ncontrMat_n &lt;- setNames(rep(1, length(coef_tbl$parameter)),\n                       coef_tbl$parameter) %&gt;% \n  contrMat(type = \"Tukey\")\n\ncontrMat_n \n\n\n     Multiple Comparisons of Means: Tukey Contrasts\n\n                                       treatmentfeed_10 treatmentfeed_10+10\ntreatmentfeed_10+10 - treatmentfeed_10               -1                   1\ntreatmentfeed_20 - treatmentfeed_10                  -1                   0\ntreatmentfeed_20 - treatmentfeed_10+10                0                  -1\n                                       treatmentfeed_20\ntreatmentfeed_10+10 - treatmentfeed_10                0\ntreatmentfeed_20 - treatmentfeed_10                   1\ntreatmentfeed_20 - treatmentfeed_10+10                1\n\n\nNun können wir alles zusammenbringen. Wir nutzen die Helferfunktion parm() aus dem R Paket multcomp um diei Koeffizienten richtig in glht() zuzuordnen. Dann noch der Kontrast mit rein in die Funktion und wir können unseren Vergleich rechnen. Leider fehlen noch die Freiheitsgrade, die wären dann in unserem Fall null, das ist aber Unsinn. Wir ergänzen die Freiheitsgrade aus unserem ursprünglichen Modell für die Koeffizienten.\n\nmult_gee &lt;- glht(parm(coef = coef_tbl$beta, \n                      vcov = diag(coef_tbl$vbeta)), \n                 linfct = contrMat_n)\nmult_gee$df &lt;- geeglm_fit$df.residual\n\nJetzt können wir uns die \\(p\\)-Werte und die 95% Konfidenzintervalle wiedergeben lassen. Du musst echt überlegen, ob sich der Aufwand lohnt. Wir erhalten hier jetzt kein signifikanten Unterschied mehr. Das liegt daran, dass wir in diesem Fall höhere Varianzen geschätzt haben als das geeglm() normalerweise tun würde. Höhere Varianzen der Koeffizienten, weniger signifikante Koeffizienten. Und dann auch weniger signifikante paarweise Unterschiede.\n\nmult_gee %&gt;% \n  tidy(conf.int = TRUE) %&gt;% \n  select(contrast, estimate, conf.low, conf.high, adj.p.value)\n\n# A tibble: 3 × 5\n  contrast                               estimate conf.low conf.high adj.p.value\n  &lt;chr&gt;                                     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 treatmentfeed_10+10 - treatmentfeed_10     1.57   -2.30       5.45       0.607\n2 treatmentfeed_20 - treatmentfeed_10        3.38   -0.510      7.27       0.103\n3 treatmentfeed_20 - treatmentfeed_10+10     1.81   -1.88       5.49       0.483\n\n\nAls Fazit nehmen wir mit, dass wir noch die Möglichkeit haben auf andere Art und Weise die Varianz in einem GEE zu schätzen. Ob uns das hilft steht auf einen anderem Blatt, aber wir haben die Möglichkeit hier noch nachzuadjustieren, wenn es mit dem Varianzschätzer klemmen sollte. Großartig unterstützt wird das Paket nicht, dass sieht man ja schon daran wie Oldschool die Analyse ist."
  },
  {
    "objectID": "stat-modeling-gee.html#referenzen",
    "href": "stat-modeling-gee.html#referenzen",
    "title": "45  Generalized Estimating Equations (GEE)",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nAllison, Paul D. 2009. Fixed effects regression models. SAGE publications.\n\n\nKruppa, Jochen, und Ludwig Hothorn. 2021. „A comparison study on modeling of clustered and overdispersed count data for multiple comparisons“. Journal of Applied Statistics 48 (16): 3220–32."
  },
  {
    "objectID": "stat-modeling-non-linear.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-non-linear.html#genutzte-r-pakete-für-das-kapitel",
    "title": "46  Nicht lineare Regression",
    "section": "\n46.1 Genutzte R Pakete für das Kapitel",
    "text": "46.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom, \n               parameters, see)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-non-linear.html#daten",
    "href": "stat-modeling-non-linear.html#daten",
    "title": "46  Nicht lineare Regression",
    "section": "\n46.2 Daten",
    "text": "46.2 Daten\nIn unserem Datenbeispiel schauen wir uns die Wachstumskurve von Hühnchen an. Wir verfolgen das Gewicht über 36 Tage. Dabei messen wir an jedem Tag eine unterschiedliche Anzahl an Kücken bzw. Hünchen. Wir wissen auch nicht, ob wir immer die gleichen Hühnchen jedes Mal messen. Dafür war die Hühnchenmastanlage zu groß. Wir wissen aber wie alt jedes Hühnchen bei der Messung war.\n\nchicken_tbl &lt;- read_csv2(\"data/chicken_growth.csv\")  \n\nIn Tabelle 46.1 sehen wir nochmal die Daten für die ersten drei und die letzten drei Zeilen. Alleine überschlagsmäßig sehen wir schon, dass wir es nicht mit einem linearen Anstieg des Gewichtes zu tun haben. Wenn wir einen linearen Anstieg hätten, dann würde ein Hühnchen, dass am Tag 1 ca. 48g wiegt, nach 36 Tagen ca. 1728g wiegen. Das ist hier eindeutig nicht der Fall. Wir haben vermutlich einen nicht-linearen Zusammenhang.\n\n\n\n\nTabelle 46.1— Auszug aus Hühnchendatensatz.\n\nage\nweight\n\n\n\n1\n48\n\n\n1\n46\n\n\n1\n44\n\n\n…\n…\n\n\n36\n2286\n\n\n36\n2278\n\n\n36\n2309\n\n\n\n\n\n\nSchauen wir uns die Daten einmal mit ggplot() an um besser zu verstehen wie die Zusammenhänge in dem Datensatz sind."
  },
  {
    "objectID": "stat-modeling-non-linear.html#visualisierung",
    "href": "stat-modeling-non-linear.html#visualisierung",
    "title": "46  Nicht lineare Regression",
    "section": "\n46.3 Visualisierung",
    "text": "46.3 Visualisierung\nIn Abbildung 46.1 (a) sehen wir die Visualisierung der Hühnchengewichte nach Alter in Tagen. Zum einen sehen wir wie das Körpergewicht exponentiell ansteigt. Zum anderen sehen wir in Abbildung 46.1 (b), dass auch eine \\(log\\)-transformiertes \\(y\\) nicht zu einem linearen Zusammenhang führt. Der Zusammenhang zwischen dem Körpergewicht und der Lebensalter bleibt nicht-linear.\n\n\n\n\n\n(a) Ohne transformierten \\(y\\).\n\n\n\n\n\n(b) Mit \\(log\\)-transformierten \\(y\\).\n\n\n\nAbbildung 46.1— Visualisierung der Hühnchengewichte nach Alter in Tagen. Auch mit \\(log\\)-transformierten Körpergewicht liegt immer noch kein linearer Zusammenhang zwischen dem Lebensalter und dem Körpergewicht vor.\n\n\nDeshalb wollen wir den Zusammenhang zwischen dem Körpergewicht der Hühnchen und dem Lebensalter einmal mit einer nicht-linearen Regression modellieren. Wir sind also nicht so sehr an \\(p\\)-Werten interessiert, wir sehen ja, dass die gerade ansteigt, sondern wollen wissen wie die Koeffizienten einer möglichen exponentiellen Gleichung aussehen."
  },
  {
    "objectID": "stat-modeling-non-linear.html#modellieren-mit-nls",
    "href": "stat-modeling-non-linear.html#modellieren-mit-nls",
    "title": "46  Nicht lineare Regression",
    "section": "\n46.4 Modellieren mit nls\n",
    "text": "46.4 Modellieren mit nls\n\nZum nicht-linearen Modellieren nutzen wir die Funktion nls() (eng. nonlinear least-squares). Die Funktion nls() ist das nicht-lineare Äquivalent zu der linearen Funktion lm(). Nur müssen wir mit der nls() Funktion etwas anders umgehen. Zum einen müssen wir die formula() anders definieren. Wir nehmen ein exponentielles Wachstum an. Daher brauchen wir einen geschätzten Koeffizienten für den Exponenten des Alters sowie einen Intercept. Wir gehen nicht davon aus, dass die Hühnchen mit einem Gewicht von 0g auf die Welt bzw. in die Mastanlage kommen. Unsere Formel sehe dann wie folgt aus.\n\\[\nweight \\sim \\beta_0 + age^{\\beta_1}\n\\]\nDa wir in R keine \\(\\beta\\)’s schreiben können nutzen wir die Buchstaben b0 für \\(\\beta_0\\) und b1 für \\(\\beta_1\\). Im Prinzip könnten wir auch andere Buchstaben nehmen, aber so bleiben wir etwas konsistenter zu der linearen Regression. Somit sieht die Gleichung dann in R wie folgt aus.\n\\[\nweight \\sim b_0 + age^{b_1}\n\\]\nAchtung! Wir müssen R noch mitteilen, dass wir age hoch b1 rechnen wollen. Um das auch wirklich so zu erhalten, zwingen wir R mit der Funktion I() auch wirklich einen Exponenten zu berechnen. Wenn wir nicht das I() nutzen, dann kann es sein, dass wir aus versehen eine Schreibweise für eine Abkürzung in der formula Umgebung nutzen.\nIm Weiteren sucht die Funktion iterativ die besten Werte für b0 und b1. Deshalb müssen wir der Funktion nls() Startwerte mitgeben, die in etwa passen könnten. Hier tippe ich mal auf ein b0 = 1 und ein b1 = 1. Wenn wir einen Fehler wiedergegeben bekommen, dann können wir auch noch an den Werten drehen.\n\nfit &lt;- nls(weight ~ b0 + I(age^b1), data = chicken_tbl, \n           start = c(b0 = 1, b1 = 1))\n\nWir nutzen wieder die Funktion model_parameters() aus dem R Paket parameters um uns eine aufgeräumte Ausgabe wiedergeben zu lassen.\n\nfit %&gt;% \n  model_parameters() %&gt;% \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\nb0        |       92.20\nb1        |        2.18\n\n\nDie \\(p\\)-Werte interssieren uns nicht weiter. Wir sehen ja, dass wir einen Effekt von dem Alter auf das Körpergewicht haben. Das überrascht auch nicht weiter. Wir wollen ja die Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\) um die Gleichung zu vervollständigen. Mit dem Ergebnis aus der Funktion nls() können wir jetzt wie folgt schreiben.\n\\[\nweight \\sim 92.20 + age^{2.18}\n\\]\nDamit haben wir dann auch unsere nicht-lineare Regressionsgleichung erhalten. Passt den die Gleichung auch zu unseren Daten? Das können wir einfach überprüfen. Dafür müssen wir nur in die Funktion predict() unser Objekt des Fits unseres nicht-linearen Modells fit stecken und erhalten die vorhergesagten Werte für jedes \\(x\\) in unserem Datensatz. Oder etwas kürzer, wir erhalten die “Gerade” der Funktion mit den Koeffizienten aus dem nls() Modell wieder. In Abbildung 46.2 sehen wir die gefittete Gerade.\n\nggplot(chicken_tbl, aes(age, weight)) +\n  geom_line(aes(y = predict(fit)), size = 1, color = \"red\") +\n  geom_point() +\n  theme_bw()\n\n\n\nAbbildung 46.2— Visualisierung der Hühnchengewichte nach Alter in Tagen mit der geschätzen nicht-linearen Regressionsgleichung.\n\n\n\nWie wir erkennen können sieht die Modellierung einigermaßen gut aus. Wir haben zwar einige leichte Abweichungen von den Beobachtungen zu der geschätzten Geraden, aber im Prinzip könnten wir mit der Modellierung leben. Wir hätten jetzt also eine nicht-lineare Gleichung die den Zusammenhang zwischen Körpergewicht und Lebensalter von Hühnchen beschreibt.\n\n\nDie Verwendung von nest() und map() ist schon erweiterete Programmierung in R. Du findest hier mehr über broom and dplyr und die Anwendung auf mehrere Datensätze.\nNun könnte man argumentieren, dass wir vielleicht unterschiedliche Abschnitte des Wachstums vorliegen haben. Also werden wir einmal das Alter in Tagen in vier gleich große Teile mit der Funktion cut_number() schneiden. Beachte bitte, dass in jeder Gruppe gleich viele Beobachtungen sind. Du kannst sonst händisch über case_when() innerhalb von mutate() dir eigene Gruppen bauen. Wir nutzen auch die Funktion map() um über alle Subgruppen des Datensatzes dann ein nls() laufen zu lassen.\n\nnls_tbl &lt;- chicken_tbl %&gt;% \n  mutate(grp = as_factor(cut_number(age, 4))) %&gt;% \n  group_by(grp) %&gt;% \n  nest() %&gt;% \n  mutate(nls_fit = map(data, ~nls(weight ~ b0 + I(age^b1), data = .x, \n                                  start = c(b0 = 1, b1 = 2))),\n         pred = map(nls_fit, ~predict(.x))) \n\nUm den Codeblock oben kurz zu erklären. Wir rechnen vier nicht-lineare Regressionen auf den vier Altersgruppen. Dann müssen wir uns noch die vorhergesagten Werte wiedergeben lassen damit wir die gefittete Gerade zeichnen können. Wir nutzen dazu die Funktion unnest() um die Daten zusammen mit den vorhergesagten Werten zu erhalten.\n\nnls_pred_tbl &lt;- nls_tbl %&gt;% \n  unnest(c(data, pred))\n\nIn Abbildung 46.3 sehen wir die vier einzelnen Geraden für die vier Altersgruppen. Wir sind visuell besser als über alle Altersgruppen hinweg. Das ist doch mal ein schönes Ergebnis.\n\n\n\n\nAbbildung 46.3— Visualisierung der Hühnchengewichte nach Alter in Tagen mit der geschätzen nicht-linearen Regressionsgleichung aufgeteilt nach vier Altersgruppen.\n\n\n\nWir können uns jetzt noch die b0 und b1 für jede der vier Altergruppen wiedergeben lassen. Wir räumen etwas auf und geben über select() nur die Spalten wieder, die wir auch brauchen und uns interessieren.\n\nnls_tbl %&gt;% \n  mutate(tidied = map(nls_fit, tidy)) %&gt;% \n  unnest(tidied) %&gt;% \n  select(grp, term, estimate) \n\n# A tibble: 8 × 3\n# Groups:   grp [4]\n  grp     term  estimate\n  &lt;fct&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1 [1,2]   b0       44.4 \n2 [1,2]   b1        4.21\n3 (2,8]   b0       60.1 \n4 (2,8]   b1        2.42\n5 (8,25]  b0      128.  \n6 (8,25]  b1        2.18\n7 (25,36] b0      330.  \n8 (25,36] b1        2.14\n\n\nWas sehen wir? Wir erhalten insgesamt acht Koeffizienten und können darüber dann unsere vier exponentiellen Gleichungen für unsere Altergruppen erstellen. Wir sehen, dass besonders in der ersten Gruppe des Alters von 1 bis 2 Tagen wir den Intercept überschätzen und den Exponenten unterschätzen. In den anderen Altersgruppen passt dann der Exponent wieder zu unserem ursprünglichen Modell über alle Altersgruppen.\n\\[\nweight_{[1-2]} \\sim 44.4 + age^{4.21}\n\\]\n\\[\nweight_{(2-8]} \\sim 60.1 + age^{2.42}\n\\]\n\\[\nweight_{(8-25]} \\sim 128.0 + age^{2.18}\n\\]\n\\[\nweight_{(25-36]} \\sim 330.0 + age^{2.14}\n\\]\nJe nachdem wie zufrieden wir jetzt mit den Ergebnissen der Modellierung sind, könnten wir auch andere Altersgruppen noch mit einfügen. Wir belassen es bei dieser Modellierung und schauen uns nochmal die andere Richtung an. Wir betrachten einen exponentziellen Verfall einer Blattläuse Population.\nWir wollen die folgende Gleichung lösen und die Werte für die Konstante \\(a\\) und den Exponenten \\(\\beta_1\\) schätzen. Wir haben diesmal keinen Intercept vorliegen.\n\\[\ncount \\sim a \\cdot week^{\\beta_1}\n\\]\nDie Daten sind angelegt an ein Experiment zu Blattlauskontrolle. Wir haben ein neues Biopestizid welchen wir auf die Blattläuse auf Rosen sprühen. Wir zählen dann automatisiert über eine Kamera und Bilderkennung wie viele Blattläuse sich nach den Wochen des wiederholten Sprühens noch auf den Rosen befinden. Wir erhalten damit folgende Daten im Objekt exp_tbl.\n\nset.seed(20221018)\nexp_tbl &lt;- tibble(count = c(rnorm(10, 17906, 17906/4), \n                            rnorm(10,  5303,  5303/4),\n                            rnorm(10,  2700,  2700/4),\n                            rnorm(10,  1696,  1696/4), \n                            rnorm(10,   947,   947/4), \n                            rnorm(10,   362,   362/4)), \n                  weeks = rep(1:6, each = 10)) \n\nWir müssen ja wieder die Startwerte in der Funktion nls() angeben. Meistens raten wir diese oder schauen auf die Daten um zu sehen wo diese Werte in etwa liegen könnten. Dann kann die Funktion nls() diese Startwerte dann optimieren. Es gibt aber noch einen anderen Trick. Wir rechnen eine lineare Regression über die \\(log\\)-transformierten Daten und nehmen dann die Koeffizienten aus dem linearen Modell als Startwerte für unsere nicht-lineare Regression.\n\nlm(log(count) ~ log(weeks), exp_tbl)\n\n\nCall:\nlm(formula = log(count) ~ log(weeks), data = exp_tbl)\n\nCoefficients:\n(Intercept)   log(weeks)  \n      9.961       -2.024  \n\n\nAus der linearen Regression erhalten wir einen Intercept von \\(9.961\\) und eine Steigung von \\(-2.025\\). Wir exponieren den Intercept und erhalten den Wert für \\(a\\) mit \\(\\exp(9.961)\\). Für den Exponenten \\(b1\\) tragen wir den Wert \\(-2.025\\) als Startwert ein. Mit diesem Trick erhalten wir etwas bessere Startwerte und müssen nicht so viel rumprobieren.\n\nfit &lt;- nls(count ~ a * I(weeks^b1), data = exp_tbl, \n           start = c(a = exp(9.961), b1 = -2.025))\n\nWir können uns noch die Koeffizienten wiedergeben lassen und die Geradengleichung vervollständigen. Wie du siehst sind die Werte natürlich anders als die Startwerte. Wir hätten aber ziemlich lange rumprobieren müssen bis wir nahe genug an die Startwerte gekommen wären damit die Funktion nls() iterativ eine Lösung für die Gleichung findet.\n\nfit %&gt;% \n  model_parameters() %&gt;% \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\na         |    17812.11\nb1        |       -1.69\n\n\nAbschließend können wir dann die Koeffizienten in die Geradengleichung eintragen.\n\\[\ncount \\sim 17812.11 \\cdot week^{-1.69}\n\\]\nIn Abbildung 46.4 sehen wir die Daten zusammen mit der gefitteten Gerade aus der nicht-linearen Regression. Wir sehen, dass die Gerade ziemlich gut durch die Mitte der jeweiligen Punkte läuft.\n\nggplot(exp_tbl, aes(weeks, count)) +\n  theme_bw() +\n  geom_point() +\n  geom_line(aes(y = predict(fit)), color = \"red\") +\n  scale_x_continuous(breaks = 1:6)\n\n\n\nAbbildung 46.4— Visualisierung der Sterberate von Blattläusen nach Aufbringen eines Bio-Pestizides mit der nicht-linearen Regressionsgleichung."
  },
  {
    "objectID": "stat-modeling-non-linear.html#modellieren-der-michaelis-menten-gleichung",
    "href": "stat-modeling-non-linear.html#modellieren-der-michaelis-menten-gleichung",
    "title": "46  Nicht lineare Regression",
    "section": "\n46.5 Modellieren der Michaelis-Menten Gleichung",
    "text": "46.5 Modellieren der Michaelis-Menten Gleichung\nIn diesem Abschnitt wollen wir uns mit dem Modellieren einer Sättigungskurve beschäftigen. Daher bietet sich natürlich die Michaelis-Menten-Gleichung an. Die Daten in enzyme.csv geben die Geschwindigkeit \\(v\\) des Enzyms saure Phosphatase (\\(\\mu mol/min\\)) bei verschiedenen Konzentrationen des Substrats Nitrophenolphosphat, [S] (mM), an. Die Daten können mit der Michaelis-Menten-Gleichung modelliert werden und somit kann eine nichtlineare Regression kann verwendet werden, um \\(K_M\\) und \\(v_{max}\\) zu schätzen.\n\nenzyme_tbl &lt;- read_csv2(file.path(\"data/enzyme.csv\")) %&gt;% \n  rename(S = concentration, v = rate)\n\nIn Tabelle 46.2 sehen wir einen Auszug aus den Enzymedaten. Eigentlich relativ klar. Wir haben eine Konzentration \\(S\\) vorliegen und eine Geschwindigkeit \\(v\\).\n\n\n\n\nTabelle 46.2— Auszug aus Enzymedatensatz.\n\nS\nv\n\n\n\n0\n0.05\n\n\n1\n2.78\n\n\n2\n3.35\n\n\n…\n…\n\n\n48\n11.04\n\n\n49\n9.18\n\n\n50\n11.56\n\n\n\n\n\n\nSchauen wir uns die Daten einmal in der Abbildung 46.5 an. Wir legen die Konzentration \\(S\\) auf die \\(x\\)-Achse und Geschwindigkeit \\(v\\) auf die \\(y\\)-Achse.\n\nggplot(enzyme_tbl, aes(x = S, y = v)) +\n  theme_bw() +\n  geom_point() +\n  labs(x = \"[S] / mM\", y = expression(v/\"µmol \" * min^-1))\n\n\n\nAbbildung 46.5— Visualisierung der Geschwindigkeit \\(v\\) des Enzyms saure Phosphatase bei verschiedenen Konzentrationen des Substrats Nitrophenolphosphat.\n\n\n\nDie Reaktionsgleichung abgeleitet aus der Michaelis-Menten-Kinetik lässt sich allgemein wie folgt darstellen. Wir haben die Konzentration \\(S\\) und die Geschwindigkeit \\(v\\) gegeben und wollen nun über eine nicht-lineare Regression die Werte für \\(v_{max}\\) und \\(K_M\\) schätzen.\n\\[\nv = \\cfrac{v_{max} \\cdot S}{K_M + S}\n\\]\nDabei gibt \\(v\\) die initiale Reaktionsgeschwindigkeit bei einer bestimmten Substratkonzentration [S] an. Mit \\(v_{max}\\) beschreiben wir die maximale Reaktionsgeschwindigkeit. Eine Kenngröße für eine enzymatische Reaktion ist die Michaeliskonstante \\(K_M\\). Sie hängt von der jeweiligen enzymatischen Reaktion ab. \\(K_M\\) gibt die Substratkonzentration an, bei der die Umsatzgeschwindigkeit halbmaximal ist und somit \\(v = 1/2 \\cdot v_{max}\\) ist. Wir haben dann die Halbsättigung vorliegen.\nBauen wir also die GLeichung in R nach und geben die Startwerte für \\(v_{max}\\) und \\(K_M\\) für die Funktion nls() vor. Die Funktion nls() versucht jetzt die beste Lösung für die beiden Koeffizienten zu finden. I\n\nenzyme_fit &lt;- nls(v ~ vmax * S /( KM + S ), data  = enzyme_tbl,\n                  start = c(vmax = 9, KM = 2))\n\nWir können uns dann die Koeffizienten ausgeben lassen.\n\nenzyme_fit %&gt;% \n  model_parameters() %&gt;% \n  select(Parameter, Coefficient)\n\n# Fixed Effects\n\nParameter | Coefficient\n-----------------------\nvmax      |       11.85\nKM        |        4.28\n\n\nJetzt müssen wir die Michaelis-Menten-Gleichung nur noch um die Koeffizienten ergänzen.\n\\[\nv = \\cfrac{11.85 \\cdot S}{4.28 + S}\n\\]\nIn der Abbildung 46.6 können wir die gefittete Gerade nochmal überprüfen und schauen ob das Modellieren geklappt hat. Ja, hat es die Gerade läuft direkt mittig durch die Punkte.\n\nggplot(enzyme_tbl, aes(x = S, y = v)) +\n  theme_bw() +\n  geom_point() +\n  geom_line(aes(y = predict(enzyme_fit)), color = \"red\") +\n  labs(x = \"[S] / mM\", y = expression(v/\"µmol \" * min^-1))\n\n\n\nAbbildung 46.6— Visualisierung der Michaelis-Menten-Kinetik zusammen mit der gefitteten Gerade aus einer nicht-linearen Regression."
  },
  {
    "objectID": "stat-modeling-survival.html#genutzte-r-pakete-für-das-kapitel",
    "href": "stat-modeling-survival.html#genutzte-r-pakete-für-das-kapitel",
    "title": "47  Überlebenszeitanalysen",
    "section": "\n47.1 Genutzte R Pakete für das Kapitel",
    "text": "47.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               survminer, survival, parameters,\n               gtsummary, janitor, ranger)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-survival.html#daten",
    "href": "stat-modeling-survival.html#daten",
    "title": "47  Überlebenszeitanalysen",
    "section": "\n47.2 Daten",
    "text": "47.2 Daten\nInsgesamt schauen wir uns in diesem Kapitel dann drei Datensätze an. Einmal einen Datensatz der sehr simple ist und nochmal erklärt wie die Datenstruktur in R aussehen muss. Dafür nutzen wir das Überleben von Dodos. Dann einen Datensatz, der etwas komplizierter ist. Wir betrachten hier das Überleben von Fruchtfliegen. Den Abschluss bildet ein Datensatz zu einer klinischen Studie von Lungenkrebs. Der Datensatz wird viel in Beispielen genutzt, so dass ich den Datensatz auch hier nochmal vorstellen möchte.\n\n47.2.1 Überleben von Dodos\nWir schauen uns hier das Überleben von Dodos mit oder ohne Schnabelstutzen an. Wir schauen dann an jedem Lebenstag, wie viele Dodos verstorben sind. Da wir es hier mit sehr schnell wachsenden Dodos zu tun haben, ist der Datensatz nicht so lang was die Zeit angeht. Wir beobachten nur die ersten 55 Lebenstage (eng. days of life, abk. dol) bevor die Dodos dann geschlachtet werden. In der Tabelle 47.1 sehen wir die rohen Daten, die wir in der Form nicht analysieren können. Wir müssen uns hier etwas strecken, damit das Format der Daten für die Überlebenszeitanalyse passt.\n\n\n\n\nTabelle 47.1— Rohe Datentabelle mit den jeweiligen Lebenstagen (abk. dol) und der Anzahl an lebenden sowie toten Dodos an den entsprechenden Tagen.\n\ntrt\ndol\ncount\ndeath\n\n\n\nnone\n1\n250\n0\n\n\nnone\n2\n250\n0\n\n\nnone\n3\n250\n0\n\n\nnone\n4\n250\n0\n\n\nnone\n5\n250\n0\n\n\nnone\n6\n245\n5\n\n\n…\n…\n…\n…\n\n\nclipped\n50\n179\n3\n\n\nclipped\n51\n177\n2\n\n\nclipped\n52\n176\n1\n\n\nclipped\n53\n173\n3\n\n\nclipped\n54\n170\n3\n\n\nclipped\n55\n163\n7\n\n\n\n\n\n\nUnsere Daten zu den Dodos beinhalten die folgenden Spalten mit den entsprechenden Bedeutungen.\n\n\ntrt, die Behandlung der Schnäbel mit none und clipped\n\n\ndol, der day of life also der Lebenstag der Dodos\n\ncount, die Anzahl an lebenden Dodos an dem entprechenden day of life\n\n\ndeath, die Anzahl an tot aufgefundenen Dodos an dem entpsrechenden day of life\n\n\nWir haben somit \\(n = 500\\) beobachtete Dodos mit jeweils 250 für jede der beiden Schnabelbehandlungen. Jetzt brauchen wir aber wie immer einen Datansatz in dem jede Zeile einen beobachteten Dodo entspricht. In der Tabelle 47.2 sehen wir welche Art von Tabelle wir bauen müssen.\n\n\nTabelle 47.2— Beispielhafte Datentabelle für die Analyse der Dododaten. Jede Zeile entspricht einem beobachteten Dodo und dem entsprechenden Informationen zur Lebensdauer und Schnabelbehandlung.\n\ndodo_id\ntrt\ndol\ndeath\n\n\n\n1\nnone\n6\n1\n\n\n2\nnone\n6\n1\n\n\n3\nnone\n7\n1\n\n\n…\n…\n…\n…\n\n\n249\nnone\n55\n0\n\n\n250\nnone\n55\n0\n\n\n251\nclipped\n6\n1\n\n\n…\n…\n…\n…\n\n\n500\nclipped\n55\n0\n\n\n\n\nFangen wir also mit der Information an, die wir haben. Wir wissen wie viele Dodos jeweils zu einem bestimmten Lebenstag gestorben sind. Daher können wir anhand der Spalte death die Anzahl an Zeilen entsprechend vervielfältigen. Sind an einem Lebenstag drei Dodos gestorben, dann brauchen wir dreimal eine Zeile mit der Information des Lebenstages und dass an diesem Tag ein Dodo gestorben ist. Wir nutzen dazu die Funktion uncount(). Dann erschaffen wir noch eine Spalte death in der einfach immer eine 1 steht, da ja an diesem Lebenstag ein Dodo verstorben ist.\n\ndeath_tbl &lt;- dodo_raw_tbl %&gt;% \n  uncount(death) %&gt;% \n  mutate(death = c(rep(1, length(.$dol))))\n\nIm nächsten Schritt müssen wir die lebenden Dodos separat für jede Behandlung ergänzen. Daher spalten wir uns die Daten in eine Liste auf und ergänzen dann die Informationen zu den fehlenden, lebenden Dodos. Alle lebenden Dodos haben die maximale Lebenszeit, sind nicht gestorben und damit bleibt die Anzahl auch konstant.\n\nalive_tbl &lt;- death_tbl %&gt;% \n  split(.$trt) %&gt;% \n  map(~tibble(dol = max(.x$dol),\n              death = rep(0, last(.x$count)),\n              count = last(.x$count))) %&gt;% \n  bind_rows(.id = \"trt\")\n\nWenn wir die Informationen zu toten und den noch lebenden Dodos gebaut haben, können wir uns dann einen finalen Datensatz zusammenkleben.\n\ndodo_tbl &lt;- bind_rows(death_tbl, alive_tbl)\n\nIn der Tabelle 47.3 sehen wir den finalen Dododatensatz, den wir uns aus den Informationen zusammengebaut haben. Wir haben hier einmal die Struktur eines Überlebenszeitdatensatzes gelernt und das wir manchmal uns ganz schön strecken müssen um die Daten dann auch gut umzubauen. Wir werden am Ende nur die Informationen in der Spalte dol, death und trt nutzen.\n\n\n\n\nTabelle 47.3— Finaler Dododatensatz für die Überlebenszeitanalysen.\n\ntrt\ndol\ncount\ndeath\n\n\n\nnone\n6\n245\n1\n\n\nnone\n6\n245\n1\n\n\nnone\n6\n245\n1\n\n\nnone\n6\n245\n1\n\n\nnone\n6\n245\n1\n\n\nnone\n7\n241\n1\n\n\n…\n…\n…\n…\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\nnone\n55\n85\n0\n\n\n\n\n\n\n\n47.2.2 Überleben von Fruchtfliegen\nIm folgenden Beispiel in Tabelle 47.4 beobachten wir Fruchtfliegen bis fast alle Insekten verstorben sind. Das ist natürlich das andere Extrem zu den Dododatensatz. Wir testen hier ein Insektizid und am Ende haben wir dann keine lebenden Fruchtfliegen mehr. Das würdest du mit Dodos oder Schweinen nicht machen, denn so lange möchtest du die Tiere ja auch nicht beobachten, bis alle gestorben sind. Bei Fruchtfliegen dauert es eben nicht so lange bis alle Fliegen verstorben sind.\n\n\n\n\nTabelle 47.4— Fruchtfliegendatensatz mit verschiedenen Covariaten zu der Behandlung, der Zeit und dem Status der Fruchtfliegen.\n\ntrt\ntime\nstatus\nsex\nweight\nweight_bin\n\n\n\nfruitflyEx\n2\n1\nmale\n8.13\nlow\n\n\nfruitflyEx\n2\n1\nmale\n7.75\nlow\n\n\nfruitflyEx\n3\n1\nmale\n9.86\nlow\n\n\nfruitflyEx\n3\n1\nmale\n6.36\nlow\n\n\n…\n…\n…\n…\n…\n…\n\n\ncontrol\n23\n1\nfemale\n21.02\nhigh\n\n\ncontrol\n24\n1\nfemale\n16.45\nhigh\n\n\ncontrol\n25\n1\nfemale\n14.17\nlow\n\n\ncontrol\n26\n1\nfemale\n17.73\nhigh\n\n\n\n\n\n\nUnsere Daten zu den Fruchtfliegen beinhalten die folgenden Spalten mit den entsprechenden Bedeutungen.\n\n\ntrt, als die Behandlung mit den beiden Leveln fruitflyEx und control\n\n\ntime, den Zeitpunkt des Todes der entsprechenden Fruchfliege\n\nstatus, den Status der Fruchtfliege zu dem Zeitpunkt time. Hier meist 1 und damit tot, aber ein paar Fruchtfliegen sind bei der Überprüfung entkommen und haben dann eine 0.\n\nsex, das Geschlecht der entsprechenden Fruchtfliege\n\nweight, das Gewicht der entsprechenden Fruchtfliege in \\(\\mu g\\).\n\nweight_bin das Gewicht der entsprechenden Fruchtfliege aufgeteilt in zwei Gruppen nach dem Cutpoint von \\(15 \\mu g\\).\n\n47.2.3 Überleben von Lungenkrebs\nZum Abschluss möchte ich noch den Datensatz lung in der Tabelle 47.5 aus dem R Paket survival vorstellen. Das hat vor allem den Grund, dass es sich hier um einen klassischen Datensatz zur Überlebenszeitanalyse handelt und ich auch dieses Teilgebiet einmal mit abdecken möchte. Wie schon weiter oben gesagt, Überlebenszeitanalysen kommen eher in dem Humanbereich vor. Darüber hinaus bedienen sich fast alle anderen Tutorien im Internet diesem Datensatz, so dass du dann einfacher die englischen Texte nachvollziehen kannst.\n\n\n\n\nTabelle 47.5— Der Datensatz lung über eine Beobachtungsstudiue Studie zu Lungenkrebs.\n\n\n\n\n\n\n\n\n\n\n\n\n\ninst\ntime\nstatus\nage\nsex\nph.ecog\nph.karno\npat.karno\nmeal.cal\nwt.loss\n\n\n\n3\n306\n2\n74\n1\n1\n90\n100\n1175\nNA\n\n\n3\n455\n2\n68\n1\n0\n90\n90\n1225\n15\n\n\n3\n1010\n1\n56\n1\n0\n90\n90\nNA\n15\n\n\n5\n210\n2\n57\n1\n1\n90\n60\n1150\n11\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n13\n191\n1\n39\n1\n0\n90\n90\n2350\n-5\n\n\n32\n105\n1\n75\n2\n2\n60\n70\n1025\n5\n\n\n6\n174\n1\n66\n1\n1\n90\n100\n1075\n1\n\n\n22\n177\n1\n58\n2\n1\n80\n90\n1060\n0\n\n\n\n\n\n\n\n\ninst Institution code\n\ntime Survival time in days\n\nstatus censoring status 1=censored, 2=dead\n\nage Age in years\n\nsex Male=1 Female=2\n\nph.ecog ECOG performance score (0=good 5=dead)\n\nph.karno Karnofsky performance score (bad=0-good=100) rated by physician\n\npat.karno Karnofsky performance score as rated by patient\n\nmeal.cal Calories consumed at meals\n\nwt.loss Weight loss in last six months\n\nTrotz seiner Prominenz hat der Datensatz einen Fehler. Wir wollen den Status nicht auf 1/2 kodiert haben sondern auf 0/1. Ebenso wollen wir die Spalte inst nicht, da wir die Informationen nicht brauchen. Dann sind noch die Namen der Spalten hässlich, so dass wir da die Funktion clean_names() nutzen um einmal aufzuräumen.\n\nlung_tbl &lt;- lung %&gt;% \n  as_tibble() %&gt;% \n  mutate(status = recode(status, `1` = 0, `2` = 1)) %&gt;% \n  clean_names() %&gt;% \n  select(-inst)"
  },
  {
    "objectID": "stat-modeling-survival.html#die-surv-funktion",
    "href": "stat-modeling-survival.html#die-surv-funktion",
    "title": "47  Überlebenszeitanalysen",
    "section": "\n47.3 Die Surv() Funktion",
    "text": "47.3 Die Surv() Funktion\nAls die Besonderheit bei der Bearbeitung von Überlebenszeitanalysen ist die Andersartigkeit von unserem \\(y\\). Wir haben ja zwei Spalten, die das Outcome beschreiben. Zum einen die Dauer oder Zeit bis zum Ereignis und dann die Spalte, die beschreibt, ob das Ereignis überhaupt eingetreten ist. In R lösen wir dieses Problem in dem wir zwei Spalten in dem Objekt Surv() zusammenführen. Alle Analysen in R gehen nur mit dem Surv() Objekt.\n\nSurv(time, death)\n\nIn dem Objekt Surv() haben wir erst die Spalte für die Zeit und dann die Spalte für das Ereignis. Für den Dododatensatz haben wir dann folgende Zusammenhänge.\n\n\ndol in den Daten dodo_tbl ist gleich time in dem Surv() Objekt\n\ndeath in den Daten dodo_tbl ist gleich death in dem Surv() Objekt\n\nBei dem Fruchfliegendatensatz sieht die Sachlage dann so aus.\n\n\ntime in den Daten fruitfly_tbl ist gleich time in dem Surv() Objekt\n\nstatus in den Daten fruitfly_tbl ist gleich death in dem Surv() Objekt\n\nFür den Lungenkrebsdatensatz haben wir dann folgende Zuordnung.\n\n\ntime in den Daten lung_tbl ist gleich time in dem Surv() Objekt\n\nstatus in den Daten lung_tbl ist gleich death in dem Surv() Objekt\n\nIm Folgenden haben wir dann immer auf der linken Seite vom ~ ein Surv() Objekt stehen. Daran muss man sich immer etwas gewöhnen, sonst kommt sowas ja nicht in den Analysen vor."
  },
  {
    "objectID": "stat-modeling-survival.html#visualisierung-über-kaplan-meier-kurven",
    "href": "stat-modeling-survival.html#visualisierung-über-kaplan-meier-kurven",
    "title": "47  Überlebenszeitanalysen",
    "section": "\n47.4 Visualisierung über Kaplan Meier Kurven",
    "text": "47.4 Visualisierung über Kaplan Meier Kurven\n\n\n\n\n\n\nNur kategoriale Variablen in einer Kaplan Meier Kurve\n\n\n\nWir können nur kategoriale Variablen in einer Kaplan Meier Kurve darstellen. Das heißt, wir müssen alle unsere \\(x\\), die kontinuierlich sind in eine kategoriale Variable umwandeln.\n\n\nWir können nur kategoriale Variablen in einer Kaplan Meier Kurve darstellen. Das heißt, wir müssen alle unser \\(X\\), die wir haben, in Variablen mit Kategorien umwandeln. Wenn du also in deinen Daten eine Spalte für das Gewicht in kg hast, dann musst du dir überlegen, wie du diese Werte in Kategorien änderst. Eine beliebte Variante ist, dass du zwei Gruppen bildest. Einmal die Patienten, die schwerer sind als der Median des Körpergewichts und einmal eine Gruppe für die Patienten, die leichter sind als der Median des Körpergewichts. Die Kategorisierung von Variablen ist faktisch ein eigenes Problem und lässt sich ohne den biologischen Hintergrund eigentlich nicht sauber durchführen. Daher werden in klinischen Studien oder Experimenten die Daten gleich in Kategorien erhoben. Daher gibt es vorab klare Kriterien in welcher Gewichtsklasse oder Altersklasse ein Patient landen wird. Das Gleiche gilt dann auch für andere kontinuierlichen Variablen.\nEin häufiger Fehler bei der Betrachtung der Kaplan Meier Kurve ist diese als die simple lineare Regression der Überlebenszeitanalyse anzusehen. Wir können zwar mit der Kaplan Meier Kurve immer nur ein \\(X\\) betrachten aber der Algorithmus basiert auf dem \\(\\mathcal{X}^2\\)-Test und hat nichts mit einer Regression zu tun. Daher kann es sein, dass du unterschiedliche Ergebnisse in der Visualisierung mit Kaplan Meier Kurven und dann der Analyse mit dem Cox Proportional-Hazards Modell erhälst.\nIn R nutzen wir das Paket survminer und die Funktion ggsurvplot() für die Visualisierung der Kaplan Meier Kurven.\n\n47.4.1 Dodos\nUm eine Kaplan Meier Kurve zeichnen zu können, brauchen wie als erstes die Funktion survfit(). Mit der Funktion survfit() können wir zum einen das mediane Überleben berechnen und alle Informationen erhalten, die wir brauchen um die Kaplan Meier Kurve zu plotten.\n\ntrt_fit &lt;- survfit(Surv(dol, death) ~ trt, data = dodo_tbl)\n\nWir können einmal das Objekt trt_fit uns anschauen.\n\ntrt_fit\n\nCall: survfit(formula = Surv(dol, death) ~ trt, data = dodo_tbl)\n\n              n events median 0.95LCL 0.95UCL\ntrt=clipped 250     87     NA      NA      NA\ntrt=none    250    165   44.5      40      49\n\n\nZum einen fallen uns die NA’s in der Wiedergabe des Fits der Überlebenszeit auf. Wenn wir uns die Abbildung 47.1 einmal anschauen, dann wird das Problem etwas klarer. Wir sehen nämlich, dass wir bei den geklippten Tieren gar nicht so weit runter kommen mit den toten Tieren, dass wir das mediane Überleben berechnen könnten. Nicht immer können wir auch alle statistischen Methoden auf alle Fragestellungen anwenden. Insbesondere wenn nicht genug Ereignisse wie in diesem Beispiel auftreten.\n\nggsurvplot(trt_fit, \n           data = dodo_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2:8])\n\n\n\nAbbildung 47.1— Kaplan Meier Kurven für unsere geklippten Dodos. Wir sehen, dass die geklippten Tiere in der Zeit der Versuchsdurchführung garnicht zur Hälfte versterben.\n\n\n\n\n47.4.2 Fruchtfliegen\nGehen wir einen Schritt weiter und schauen uns das Modell für die Fruchtfliegen an. Hier haben wir eine Behandlung mit zwei Leveln also Gruppen vorliegen. Wir nutzen wieder die Funktion survfit() um einaml unser Modell der Überlebenszeiten zu fitten.\n\ntrt_fit &lt;- survfit(Surv(time, status) ~ trt, data = fruitfly_tbl)\n\nWir erhalten dann folgende Ausgabe des Modells.\n\ntrt_fit\n\nCall: survfit(formula = Surv(time, status) ~ trt, data = fruitfly_tbl)\n\n                n events median 0.95LCL 0.95UCL\ntrt=fruitflyEx 50     44     12      10      14\ntrt=control    50     49     15      13      17\n\n\nIn der Abbildung 47.2 sehen wir die Visualisierung des Modells als Kaplan Meier Kurve. In diesem Experiment sterben fast alle Fruchtfliegen im Laufe der Untersuchung. Wir können also einfach das mediane Überleben für beide Gruppen berechnen.\n\nggsurvplot(trt_fit, \n           data = fruitfly_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2:8])\n\n\n\nAbbildung 47.2— Kaplan Meier Kurven für die Fruchtfliegen nach der Behandlung mit einem Pestizid und einer Kontrolle.\n\n\n\nNachdem wir die Kaplan Meier Kurven einmal für die Behandlung durchgeführt haben, können wir uns auch anschauen, ob das Überleben der Fruchtfliegen etwas mit dem Gewicht der Fruchtfliegen zu tun hat. Hier können wir nicht auf das Gewicht in der Spalte weight zurückgreifen sondern müssen die Variable weight_bin mit zwei Klassen nehmen.\n\nweight_fit &lt;- survfit(Surv(time, status) ~ weight_bin, data = fruitfly_tbl)\n\nWir erhalten dann die Kaplan Meier Kurven in der Abbildung 47.3 zurück. Hier ist es wichtig sich nochmal klar zu machen, dass wir eben nur kategoriale Variablen in einer Kaplan Meier Kurve darstellen können.\n\nggsurvplot(weight_fit, \n           data = fruitfly_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2:8])\n\n\n\nAbbildung 47.3— Kaplan Meier Kurven für die Fruchtfliegen nach der Behandlung mit einem Pestizid und einer Kontrolle.\n\n\n\n\n47.4.3 Lungenkrebs\nAls letztes Beispiel wollen wir uns nochmal den Datensatz lung_tbl anschauen. Zwar ist Lungenkrebs jetzt nichts was Tiere und Pflanzen als eine wichtige Erkrankung haben können, aber der Datensatz wird viel als Beispiel genutzt, so dass ich den Datensatz hier auch nochmal vorstellen möchte. Auch sind teilweise gewisse Schritte von Interesse, die eventuell auch in deiner Tier- oder Mäusestudie von Interesse sein könnten.\nBeginnen wir einmal mit dem Nullmodell. Das heißt, wir schauen uns den Verlauf des gesamten Überlebens einmal an. Wir wollen wissen, wie das mediane Überleben in unseren Daten ausschaut ohne das wir uns irgendeien Variable anschauen.\n\nnull_fit &lt;- survfit(Surv(time, status) ~ 1, data = lung_tbl)\n\nIm Folgenden einmal die Ausgabe des Fits.\n\nnull_fit\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung_tbl)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 228    165    310     285     363\n\n\nWir sehen, dass wir ein medianes Überleben von \\(310\\) Tagen haben. Es gibt insgesamt \\(165\\) Ereignisse zu beobachten von insgesamt \\(228\\) in die Studie eingeschlossenen Patienten. Wir können uns auch für bestimmte Zeitpunkte das Überleben wiedergeben lassen. Wir schauen uns hier einmal das Überleben nach einem Jahr bzw. \\(365.25\\) Tagen an.\n\nnull_fit %&gt;% \n  summary(times = 365.25)\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung_tbl)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  365     65     121    0.409  0.0358        0.345        0.486\n\n\nHier sehen wir, dass \\(40.9\\%\\) das eine Jahr überlebt haben. Achtung, immer auf die Kodierung achten. Nur wenn du death = 1 kodiert hast, kannst du hier die Ausgaben der Funktionen in diesem Sinne interpretieren. Gerne kannst du hier auch das 3 Monatsüberleben bestimmen. Das kommt ganz darauf an, was deine Fragestellung ist. In der Abbildung 47.4 siehst du nochmal die Kaplan Meyer Kurve für das Nullmodell.\n\nggsurvplot(null_fit, \n           data = lung_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2])\n\n\n\nAbbildung 47.4— Kaplan Meier Kurven für das Nullmodell des Lungenkrebsdatensatzes.\n\n\n\nSchauen wir uns auch einmal Kaplan Meier Kurven für die Variable ph_ecog an. Hier haben wir das Problem, dass die Kategorie 3 kaum mit Patienten belegt ist. Daher filtern wir die Kategorie 3 einmal aus unseren Daten raus.\n\nlung_tbl %&lt;&gt;% \n  filter(ph_ecog != 3)\n\nWir können dann das Modell der Überlebenszeit einmal fitten.\n\nlung_fit &lt;- survfit(Surv(time, status) ~ ph_ecog, data = lung_tbl)\n\nIn der Abbildung 47.5 sehen wir dann die Kaplan Meier Kurve für die Variable ph_ecog. Du kannst hier schön sehen, dass wenn wir mehrere Kategorien in der Variable haben auch mehrere Graphen erhalten. Wichtig hierbei ist nochmal, dass sich die Graphen nicht überschneiden oder aber in der Mitte kreuzen. Dann haben wir ein Problem und könnten die Daten nicht auswerten.\n\n\nKonkret geht es hier um die proportional hazards assumption, die besagt, dass Überlebenszeitkurven für verschiedene Gruppen Hazardfunktionen haben müssen, die über die Zeit \\(t\\) proportional sind. Daher dürfen sich die Kurven nicht schneiden.\n\nggsurvplot(lung_fit, \n           data = lung_tbl, \n           risk.table = TRUE,\n           surv.median.line = \"hv\",\n           ggtheme = theme_light(),\n           palette = cbbPalette[2:8])\n\n\n\nAbbildung 47.5— Kaplan Meier Kurve für die Variable ph_ecog des Lungenkrebsdatensatzes.\n\n\n\nGanz zum Schluss dann noch die Frage, ob wir einen signifikanten Unterschied zwischen den beiden Kurven sehen. Dafür können wir dann die Funktion survdiff() nutzen. Die Funktion survdiff() gibt uns dann einen p-Wert wieder, ob sich die Kurven unterscheiden. Da es sich hier um einen globalen p-Wert handelt, erfahren wir nur, dass sich die Kurven unterscheiden, aber nicht welche. Dafür müssten wir dann die Kurven paarweise getrennt betrachten. Eigentlich ist nur der p-Wert von Interesse, die anderen Informationen haben eigentlich keinen biologischen Mehrwert.\n\nsurvdiff(Surv(time, status) ~ ph_ecog, data = lung_tbl)\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ ph_ecog, data = lung_tbl)\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nph_ecog=0  63       37     53.9    5.3014    8.0181\nph_ecog=1 113       82     83.1    0.0144    0.0295\nph_ecog=2  50       44     26.0   12.4571   14.9754\n\n Chisq= 18  on 2 degrees of freedom, p= 1e-04"
  },
  {
    "objectID": "stat-modeling-survival.html#cox-proportional-hazards-modell",
    "href": "stat-modeling-survival.html#cox-proportional-hazards-modell",
    "title": "47  Überlebenszeitanalysen",
    "section": "\n47.5 Cox Proportional-Hazards Modell",
    "text": "47.5 Cox Proportional-Hazards Modell\nWenn die Kaplan Meyer Kurven sowas wie die simple lineare Regression sind, dann ist das Cox Proportional-Hazards Modell die multiple Regression in den Ereigniszeitanalysen. Damit haben wir natürlich wieder einen statistischen Engel überfahren. Das Cox Proportional-Hazards Modell ist natürlich etwas anders und lässt sich so einfach auch nicht mit einer multiplen Regression vergleichen, aber die Anwendung ist ähnlich. Wo wir bei den Kaplan Meier Kurven nur ein \\(X\\) in das Modell nehmen können, so können wir beim Cox Proportional-Hazards Modell beliebig viele \\(X\\) mit ins Modell nehmen. Theoretisch müssen die Variablen in einem Cox Proportional-Hazards Modell auch nicht mehr kategorial sein. Da wir aber meist alles schon in Kategorien visualisiert haben, bleiben wir dann meist im Cox Proportional-Hazards Modell auch bei den Kategorien in den Variablen.\n\n\nAuch im Fall des Cox Proportional-Hazards Modells kann ich hier nur eine Übersicht geben. Es findet sich natürlich auch ein Tutorium zum Cox Proportional-Hazards Model Tools. Für das Überprüfen der Modellannahmen empfiehlt sich auch das Tutorium zu Cox Model Assumptions.\nIn R nutzen wir die Funktion coxph() um ein Cox Proportional-Hazards Modell anzupassen. Die Anwendung ist eigentlich ziemlich einfach und lässt sich schnell durchführen.\n\nfit_1 &lt;- coxph(Surv(time, status) ~ trt + sex + weight, data = fruitfly_tbl) \nfit_1 %&gt;% \n  model_parameters(exponentiate = TRUE)\n\nParameter     | Coefficient |   SE |       95% CI |     z |      p\n------------------------------------------------------------------\ntrt [control] |        0.24 | 0.09 | [0.12, 0.49] | -4.00 | &lt; .001\nsex [female]  |        0.14 | 0.04 | [0.09, 0.24] | -7.32 | &lt; .001\nweight        |        1.02 | 0.04 | [0.95, 1.10] |  0.60 | 0.549 \n\n\nAls Koeffizienten erhalten wir das Hazard ratio (abk. HR) wieder. Wie schon bei der logistischen Regression müssen wir auch hier die Koeffizienten exponieren, damit wir die Link-scale verlassen. Wir können das HR wie ein Risk ratio (abk. RR) interpretieren. Es handelt sich also mehr um eine Sterbewahrscheinlichkeit. Ganz richtig ist die Interpretation nicht, da wir hier noch eine Zeitkomponente mit drin haben, aber für den umgangssprachlichen Gebrauch reicht die Interpretation.\nWenn wir ein \\(HR &gt; 1\\) vorliegen haben, so steigert die Variable das Risiko zu sterben. Daher haben wir eine protektive Variable vorliegen, wenn dass \\(HR &lt; 1\\) ist. Häufig wollen wir ein \\(HR &lt; 0.8\\) oder \\(HR &lt; 0.85\\) haben, wenn wir von einem relevanten Effekt sprechen wollen. Sonst reicht uns die Risikoreduktion nicht, um wirklich diese Variable zukünftig zu berücksichtigen. Aber wie immer hängt die Schwelle sehr von deiner Fragestellung ab.\nIch habe nochmal als Vergleich die Variable weight in das Modell genommen und damit den fit_1 angepasst sowie die Variable weight in zwei Gruppen zu weight_bin aufgeteilt. Hier siehst du sehr schön, dass der Effekt der Dichotomisierung nicht zu unterschätzen ist. Im fit_1 ist die kontinuierliche Variable weight eine Risikovariable, daher wir erwarten mehr tote Fruchtfliegen mit einem steigenden Gewicht. In dem fit_2 haben wir die dichotomisierte Variable weight_bin vorliegen und schon haben wir eine protektive Variable. Wenn das Gewicht steigt, dann sterben weniger Fruchtfliegen. Zwar ist in beiden Fällen die Variable nicht signifikant, aber du solltest eine Dichotomisierung immer überprüfen.\n\nfit_2 &lt;- coxph(Surv(time, status) ~ trt + sex + weight_bin, data = fruitfly_tbl) \nfit_2 %&gt;% \n  model_parameters(exponentiate = TRUE)\n\nParameter        | Coefficient |   SE |       95% CI |     z |      p\n---------------------------------------------------------------------\ntrt [control]    |        0.20 | 0.07 | [0.10, 0.39] | -4.65 | &lt; .001\nsex [female]     |        0.14 | 0.04 | [0.08, 0.23] | -7.50 | &lt; .001\nweight bin [low] |        0.62 | 0.20 | [0.33, 1.16] | -1.48 | 0.138 \n\n\nNachdem wir ein Cox Proportional-Hazards Modell angepasst haben, wollen wir nochmal überprüfen, ob die Modellannahmen auch passen. Insbesondere müssen wir überprüfen, ob das Risiko über die ganze Laufzeit der Studie gleich bleibt oder sich ändert. Wir testen also die proportional hazards assumption. Dafür können wir in R die Funktion cox.zph() nutzen.\n\ntest_ph &lt;- cox.zph(fit_2)\ntest_ph\n\n              chisq df     p\ntrt        0.000866  1 0.977\nsex        3.400374  1 0.065\nweight_bin 0.185700  1 0.667\nGLOBAL     3.971563  3 0.265\n\n\nWir lesen die Ausgabe von unten nach oben. Zuerst könne wir die proportional hazards assumption nicht ablehnen. Unser globaler p-Wert ist mit \\(0.265\\) größer als das Signifikanzniveau \\(\\alpha\\) gleich \\(5\\%\\). Betrachten wir die einzelnen Variablen, dann können wir auch hier die proportional hazards assumption nicht ablehnen. In der Abbildung 47.6 ist der Test auf die proportional hazards assumption nochmal visualisiert. Wenn die durchgezogene Linie innerhalb der gestrichelten Linien, als Bereich von \\(\\pm 2\\) Standardfehlern, bleibt, dann ist soweit alles in Orndung. Bei einem Verstoß gegen die proportional hazards assumption kannst du folgende Maßnahmen ausprobieren:\n\nHinzufügen von einer Kovariate*Zeit-Interaktion in das Cox Proportional-Hazards Modell\nStratifizierung der Daten nach der Kovariaten, die der proportional hazards assumption nicht folgt\n\nAuch hier musst du dann mal tiefer in die Materie einsteigen und einmal in den verlinkten Tutorien schauen, ob da was passendes für dein spezifisches Problem vorliegt.\n\nggcoxzph(test_ph)\n\n\n\nAbbildung 47.6— Visualisierung der Überprüfung der Proportional-Hazards-Annahme."
  },
  {
    "objectID": "stat-modeling-survival.html#logistische-regression",
    "href": "stat-modeling-survival.html#logistische-regression",
    "title": "47  Überlebenszeitanalysen",
    "section": "\n47.6 Logistische Regression",
    "text": "47.6 Logistische Regression\nWas schon wieder die logistische Regression? Ja, schon wieder die logistische Regression. Wenn du dich nicht mit der Ereigniszeitanalyse rumschlagen willst oder denkst, dass die Ereigniszeitanalyse nicht passt, dann hast du immer noch die Möglichkeit eine logistische Regression zu rechnen. Dafür müssen wir dann nur ein wenig an den Daten rumbasteln. Wir müssen dann nämlich eine neue Variable erschaffen. Wir schauen einfach zu einem Zeitpunkt \\(t\\), ob die Beobachtung noch lebt. Dadurch bauen wir uns dann eine Spalte mit \\(0/1\\) Werten. Dann kann es auch schon losgehen mit der logistischen Regression.\n\n\nTja, All your base are belong to us kann man da nur sagen…\nIm ersten Schritt bauen wir uns eine neue Variable died_3_m für den Lingenkrebsdatensatz. Da in einer logistsichen Regression das Schlechte immer 1 ist, fragen wir, wer nach 90 Tagen verstorben ist. Also eine Lebenszeit unter 90 Tagen hatte. Diese Beobachtungen kriegen dann eine 1 und die anderen Beobachtungen eine 0.\n\nlung_logreg_tbl &lt;- lung_tbl %&gt;% \n  mutate(died_3_m = ifelse(time &lt; 90, 1, 0))\n\nNachdem wir uns recht schnell eine neue Variable gebaut haben, können wir dann einfach die logistische Regression rechnen. Bitte beachte, dass du die Effekte nur auf der log-Scale wiedergegeben kriegst, du musst dann die Ausgabe noch exponieren. Das machen wir hier in der Funktion model_parameters() gleich mit.\n\nglm(died_3_m ~ age + sex + ph_ecog, data = lung_logreg_tbl, family = binomial) %&gt;% \n  model_parameters(exponentiate = TRUE)\n\nParameter   | Odds Ratio |       SE |       95% CI |     z |     p\n------------------------------------------------------------------\n(Intercept) |   4.94e-03 | 9.59e-03 | [0.00, 0.18] | -2.74 | 0.006\nage         |       1.06 |     0.03 | [1.01, 1.13] |  2.29 | 0.022\nsex         |       0.45 |     0.23 | [0.16, 1.14] | -1.59 | 0.111\nph ecog     |       1.34 |     0.41 | [0.74, 2.47] |  0.96 | 0.335\n\n\nWas sind die Unterschiede? Eine logistische Regression liefert Odds Ratios, also ein Chancenverhältnis. Aus einer Ereigniszeitanalyse erhalten wir Hazard Ratios, was eher ein Risk Ratio ist und somit eine Wahrscheinlichkeit. Deshalb lassen sich die Ergebnisse an der Stelle nur bedingt vergleichen. Im Falle der logistischen Regression fallen auch Zensierungen weg. Wir betrachten eben nur einen einzigen Zeitpunkt. In der Ereigniszeitanalyse betrachten wir hingegen den gesamten Verlauf. Wir immer musst du überlegen, was ist deine Fragestellung und was möchtest du berichten. Wenn es dir wirklich nur um den Zeitpunkt \\(t\\) geht und dir die Progression dahin oder danach egal ist, dann mag die logistische Regression auch eine Möglichkeit der Auswertung sein."
  },
  {
    "objectID": "stat-modeling-survival.html#referenzen",
    "href": "stat-modeling-survival.html#referenzen",
    "title": "47  Überlebenszeitanalysen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDavid, G Kleinbaum, und Klein Mitchel. 2012. Survival analysis: a Self-Learning text. Spinger."
  },
  {
    "objectID": "stat-modeling-noninferiority.html#genutzte-r-pakete",
    "href": "stat-modeling-noninferiority.html#genutzte-r-pakete",
    "title": "48  Äquivalenz oder Nichtunterlegenheit",
    "section": "\n48.1 Genutzte R Pakete",
    "text": "48.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, broom, readxl,\n               effectsize, multcompView, multcomp,\n               janitor, see, parameters, yardstick,\n               conflicted)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(dplyr::mutate)\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-noninferiority.html#daten",
    "href": "stat-modeling-noninferiority.html#daten",
    "title": "48  Äquivalenz oder Nichtunterlegenheit",
    "section": "\n48.2 Daten",
    "text": "48.2 Daten\nAls erstes wollen wir uns einmal die Daten für die Überprüfung der technischen Gleichheit anschauen. Die Daten stammen aus Dronenüberflügen zur Bestimmung der Grasdichte auf Weideflächen aus der Datei drone_tech.xlsx. Dabei haben wir zum einen die Grasdichte traditionell mit einem Druckstab gemessen pressure_stick und vergleichen diese Werte dann mit den Werten aus dem Dronenüberflug. Der Drohnenüberflug liefert uns Bilder und aus den Bildern extrahieren wir einen RGB-Wert (abk. Red, Green, Blue) in der Spalte drone_rgb oder einen CMYK-Wert (abk. Cyan, Magenta, Yellow (Gelb), Key (Schwarz)) in der Spalte drone_cmyk. Wir wollen nun schauen, ob wir die drei Werte sinnvoll in ein Verhältnis setzen können. Ein Auszug aus den Daten ist nochmal in der Tabelle 48.1 dargestellt.\n\n\n\n\nTabelle 48.1— Datentabelle für den technischen Vergleich eines Druckstabes und dem RGB-Werten eines Dronenüberflugs auf die Grasdichte auf Weideflächen.\n\npressure_stick\ndrone_rgb\ndrone_cmyk\n\n\n\n1048.24\n373.31\n254.65\n\n\n1284.31\n671.45\n234.86\n\n\n1170.07\n544.5\n184.01\n\n\n…\n…\n…\n\n\n1013.34\n355.84\n219.67\n\n\n1134.29\n537.43\n212.86\n\n\n917.29\n266.74\n178.46\n\n\n\n\n\n\nIn unserem zweiten Datenbeispiel schauen wir uns die Keimungsdaten nach Behandlung mit sechs biologischen Pilzmittel unter zwei Kältebehandlungen aus der Datei cold_seeds.xlsx an. Dabei ist wichtig zu wissen, dass es eine Kontrolle gibt, die das chemische Standardpräparat repräsentiert. Wir wollen jetzt wissen, ob unsere biologischen Alternativen gleich gut sind. Das heißt, wir wollen nicht mehr oder weniger als das Standardpräparat sondern gleichviel. Als Outcome zählen wir die Sporen auf den jungen Keimlingen. Da unsere Pflanze auch eine Kältebehandlung überstehen würde, haben wir auch noch die beiden Kältevarianten mit untersucht. In der Tabelle 48.2 sind die Daten einmal dargestellt.\n\n\n\n\nTabelle 48.2— Nicht transformierter Datensatz zu dem Keimungsexperiment mit biologischen Pilzpräparaten.\n\ntrt\ncold\nnon_cold\n\n\n\n1\n386.25\n22.9\n\n\n1\n100.52\n169.59\n\n\n1\n56.84\n65.46\n\n\n1\n357.65\n142.44\n\n\n2\n37668.6\n20659.77\n\n\n2\n28302.99\n7333.37\n\n\n…\n…\n…\n\n\n8\n2334.1\n352.41\n\n\n8\n9776.15\n5025.68\n\n\n8\n1932.27\n918.05\n\n\n8\n777.63\n149.17\n\n\n8\n2933.99\n1416.51\n\n\n8\n5731.01\n2022.39\n\n\n\n\n\n\nWir müssen jetzt leider nochmal ran und die Daten etwas aufräumen. Zum einen muss die erste Behandlung raus, hier handelt es sich nur um eine positive Kontrolle, ob überhaupt etwas gewachsen ist. Dann wollen wir uns die Daten auch log-transformieren. Das hat den Grund, dass die statistischen Verfahren in der Äquivalenzanalyse eine Normalverteilung verlangen. Mit der log-Transformation erreichen wir log-normalverteilte Daten, die einer Normalverteilung recht nahe kommen. Am Ende wollen wir dann auch die zweite Behandlung so benennen, dass wir auch immer die Kontrolle erkennen.\n\ncold_seed_tbl &lt;- cold_seed_tbl %&gt;%   \n  clean_names %&gt;% \n  filter(trt != 1) %&gt;% \n  mutate(trt = as_factor(trt),\n         log_cold = log(cold),\n         log_non_cold = log(non_cold),\n         trt = fct_recode(trt, ctrl = \"2\")) \n\nEs ergibt sich dann die Tabelle 48.3. Wir werden dann in der folgenden Analyse nur noch die log-transformierten Spalten log_cold und log_non_cold nutzen.\n\n\n\n\nTabelle 48.3— Transformierter Datensatz zu dem Keimungsexperiment mit biologischen Pilzpräparaten.\n\ntrt\ncold\nnon_cold\nlog_cold\nlog_non_cold\n\n\n\nctrl\n37668.6\n20659.77\n10.54\n9.94\n\n\nctrl\n28302.99\n7333.37\n10.25\n8.9\n\n\nctrl\n2874.76\n1325.42\n7.96\n7.19\n\n\nctrl\n7564.44\n2103.64\n8.93\n7.65\n\n\n…\n…\n…\n…\n…\n\n\n8\n1932.27\n918.05\n7.57\n6.82\n\n\n8\n777.63\n149.17\n6.66\n5.01\n\n\n8\n2933.99\n1416.51\n7.98\n7.26\n\n\n8\n5731.01\n2022.39\n8.65\n7.61"
  },
  {
    "objectID": "stat-modeling-noninferiority.html#technische-gleichheit",
    "href": "stat-modeling-noninferiority.html#technische-gleichheit",
    "title": "48  Äquivalenz oder Nichtunterlegenheit",
    "section": "\n48.3 Technische Gleichheit",
    "text": "48.3 Technische Gleichheit\nBeginnen wir also mit der Beurteilung von der technischen Gleichheit zweier Verfahren. Ich nutze hier das Wort technische Gleichheit, da wir hier nicht zwei Gruppen miteinander vergleichen, sondern eben kontinuierlich gemessene Werte haben und wissen wollen, ob diese gemessenen Werte aus den beiden Verfahren gleich sind. In unserem Beispiel wollen wir wissen, ob wir den Druckstab zum Messen der Grasdichte durch einen Drohnenüberflug erstetzen können. Der Dronenflug produziert Bilder und wir können auf zwei Arten Zahlen aus den Bildern generieren. Wir extrahieren entweder die RGB-Werte der Bilder oder aber die CMYK-Werte. Hier ist natürlich ein Schritt den ich überspringe, wir erhalten am Ende eben einen Wert für ein Bild. Oder andersherum, wir können genau einer Messung mit dem Druckstab ein Bild der Drone zuordnen.\nIn der Abbildung 48.1 (a) und in der Abbildung 48.1 (b) sehen wir den Zusammenhang zwischen dem Druckstab und der Dronenmessung für beide Farbskalenwerte nochmal visualisiert. In einer idealen Welt würden alle Punkte auf einer Linie liegen. Das heißt, wir haben einen perfekten Zusammenhang zwischen dem Druckstab und den Farbskalenwerten. So ein perfekter Zusammenhang tritt in der Natur nie auf, deshalb müssen wir uns nun mit statistischen Maßzahlen behelfen.\nWir können die Funktion geom_smooth() nutzen um eine lineare Funktion durch die Punkte zu legen. Wir sehen ist der Fehler, dargestellt als grauer Bereich, bei den CMYK-Werten größer. Auch haben wir Punkte die etwas anch oben weg streben. In der RGB-Skala haben wir eher einen linearen Zusammenhang. Im Folgenden wollen wir uns dann einmal die statistischen Maßzahlen zu der Visualisierung anschauen.\n\nggplot(drone_tbl, aes(drone_rgb, pressure_stick)) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE)\n\nggplot(drone_tbl, aes(drone_cmyk, pressure_stick)) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE)\n\n\n\n\n\n(a) Dronenmessung mit RGB-Werten.\n\n\n\n\n\n(b) Dronenmessung mit CMYK-Werten.\n\n\n\nAbbildung 48.1— Vergleich der beiden Farbskalen aus der Dronenmessung zu der Grasdichte durch den Druckstab.\n\n\n\n\n48.3.1 Bestimmtheitsmaß \\(R^2\\)\n\nFür die genaueren Werte der linearen Funktion nutzen wir dann die Funktion lm(). Wir brauchen die statistischen Maßzahlen höchstens, wenn uns eine Umrechung von den Werten von der einen Messung zu der anderen Messung interessiert.\n\nfit_drone &lt;- lm(pressure_stick ~ drone_rgb, data = drone_tbl)\nfit_drone %&gt;% model_parameters()\n\nParameter   | Coefficient |   SE |           95% CI | t(279) |      p\n---------------------------------------------------------------------\n(Intercept) |      766.33 | 5.12 | [756.26, 776.41] | 149.70 | &lt; .001\ndrone rgb   |        0.78 | 0.01 | [  0.76,   0.81] |  61.76 | &lt; .001\n\n\nZum einen können wir uns jetzt auch die lineare Funktion und damit den Zusammenhang von dem Druckstab zu der RGB-Farbskala erstellen. Mir der folgenden Formel können wir dann die Werte der Dronen RGB-Farbskala in die Werte des Druckstabes umrechnen.\n\\[\npressure\\_stick = 766.33 + 0.78 \\cdot drone\\_rgb\n\\]\nZum anderen erhalten wir mit der Funktion lm() dann auch die Möglichkeit das Bestimmtheitsmaß \\(R^2\\) zu berechnen. Du kennst das Bestimmtheitsmaß \\(R^2\\) schon aus dem Kapitel für die Qualität einer linearen Regression. Hier nochmal kurz zusammengefasst, das Bestimmtheitsmaß \\(R^2\\) beschreibt, wie gut die Punkte auf der Geraden liegen. Ein Bestimmtheitsmaß \\(R^2\\) von 1 bedeutet, dass die Punkte perfekt auf der Geraden liegen. Ein Bestimmtheitsmaß \\(R^2\\) von 0, dass die Punkte eher wild um eine potenzielle Graden liegen.\nIm Folgenden können wir uns noch einmal die Formel des Bestimmtheitsmaß \\(R^2\\) anschauen um etwas besser zu verstehen, wie die Zusammenhänge mathematisch sind. Zum einen brauchen wir den Mittelwert von \\(y\\) als \\(\\bar{y}\\) sowie die Werte der einzelnen Punkte \\(\\bar{y}\\) und die Werte auf der Geraden mit \\(\\hat{y}_i\\).\n\\[\n\\mathit{R}^2 =\n\\cfrac{\\sum_{i=1}^N \\left(\\hat{y}_i- \\bar{y}\\right)^2}{\\sum_{i=1}^N \\left(y_i - \\bar{y}\\right)^2}\n\\]\nIn der Abbildung 48.2 sehen wir den Zusammenhang nochmal visualisiert. Wenn die Abstände von dem Mittelwert zu den einzelnen Punkten mit \\(y_i - \\bar{y}\\) gleich dem Abstand der Mittelwerte zu den Punkten auf der Geraden mit \\(\\hat{y}_i- \\bar{y}\\) ist, dann haben wir einen perfekten Zusammenhang.\n\n\nAbbildung 48.2— Auf der linken Seite sehen wir eine Gerade die nicht perfekt durch die Punkte läuft. Wir nehmen ein Bestimmtheitsmaß \\(R^2\\) von ca. 0.7 an. Die Abstände der einzelnen Beobachtungen \\(y_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\) ist nicht gleich den Werten auf der Geraden \\(\\hat{y}_i\\) zu dem Mittelwert der y-Werte \\(\\bar{y}\\). Dieser Zusammenhang wird in der rechten Abbildung mit einem Bestimmtheitsmaß \\(R^2\\) von 1 nochmal deutlich.\n\nWir können die Funktion glance() nutzen um uns das r.squared und das adj.r.squared wiedergeben zu lassen.\n\nfit_drone %&gt;% \n  glance() %&gt;% \n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      &lt;dbl&gt;\n1     0.932\n\n\nWir haben wir ein \\(R^2\\) von \\(0.932\\) vorliegen. Damit erklärt unser Modell bzw. die Gerade 93.2% der Varianz. Der Anteil der erklärten Varianz ist auch wunderbar hoch, so dass wir davon ausgehen können, dass der Druckstab und die RGB-Werte der Drone ungefähr das Gleiche wiedergeben."
  },
  {
    "objectID": "stat-modeling-noninferiority.html#korrelation",
    "href": "stat-modeling-noninferiority.html#korrelation",
    "title": "48  Äquivalenz oder Nichtunterlegenheit",
    "section": "\n48.4 Korrelation",
    "text": "48.4 Korrelation\nNeben der Information wie gut die Punkte auf der Geraden liegen, also wie die Punkte um die Gerade streuen, können wir uns auch die Korrelation und damit die Steigung der Gerade wiedergeben lassen. Das Bestimmtheitsmaß \\(R^2\\) sagt uns nämlich nichts über die Richtung der Geraden aus. Die Korrelation liefert uns die Steigung der Geraden mit dem Vorzeichen.\nWir können hier verschiedene Korrelationsmaße berechnen. Am häufigsten werden wir die Korrelation nach Pearson berechnen, da wir von einem normalverteilten \\(y\\) ausgehen. Wenn dies nicht der Fall sein sollte empfiehlt sich stattdessen den Korrelationkoeffizienten nach Spearman zu nutzen.\n\ndrone_tbl %$% \n  cor(pressure_stick, drone_rgb, method = \"pearson\") %&gt;% \n  round(2)\n\n[1] 0.97\n\ndrone_tbl %$% \n  cor(pressure_stick, drone_rgb, method = \"spearman\") %&gt;% \n  round(2)\n\n[1] 0.96\n\n\n\n\nWir nutzen hier den %$%-Operator, da wir in die Funktion cor() die Spalten übergeben wollen. Die Funktion cor() ist relativ alt und möchte daher keinen Datensatz sondern zwei Vektoren.\nNachdem wir die Korrelation berechnet haben, sehen wir das wir einen positiven Zusammenhang vorliegen haben. Die Gerade durch die Punkte steigt an und ist fast eine 45\\(^{\\circ}\\) Gerade, da wir eine Korrelation nahe 1 vorliegen haben.\n\n48.4.1 MSE, RMSE, nRMSE und MAE\nNeben der Betrachtung der Abweichung vom Mittelwert von \\(y\\) können wir uns auch die Abstände von den geschätzten Punkten auf der Geraden \\(\\hat{y}_i\\) zu den eigentlichen Punkten anschauen \\(y_i\\). Wir haben jetzt zwei Möglichkeiten die Abstände zu definieren.\n\nWir schauen uns die quadratischen Abstände mit \\((y_i - \\hat{y}_i)^2\\) an. Wir berechnen dann die mittlere quadratische Abweichung (eng. mean square error abk. MSE).\nWir schauen uns die absoluten Abstände mit \\(|y_i - \\hat{y}_i|\\) an. Wir berechnen dann den mittleren absoluten Fehler (eng. mean absolute error, abk. MAE).\n\nIm Folgenden betrachten wir erst den MSE und seine Verwandten. Wie wir an der Formel sehen, berechnen wir für den MSE einfach nur die quadratische Abweichung zwischen den Beobachtungen \\(y_i\\) und den Werten auf der berechneten Geraden \\(\\hat{y}_i\\). Dann summieren wir alles auf und teilen noch durch die Anzahl der Beobachtungen also Punkte \\(n\\).\n\\[\nMSE = \\cfrac{1}{n}\\sum^n_{i=1}(y_i - \\hat{y}_i)^2\n\\]\nHäufig wollen wir dann nicht die quadratischen Abweichungen angeben. Wir hätten dann ja auch die Einheit der Abweichung im Quadrat. Daher ziehen wir die Wurzel aus dem MSE und erhalten den root mean square error (abk. RMSE). Hierfür gibt es dann keine gute Übersetzung ins Deutsche.\n\\[\nRMSE = \\sqrt{MSE} = \\sqrt{\\cfrac{1}{n}\\sum^n_{i=1}(y_i - \\hat{y}_i)^2}\n\\]\nDer RMSE ist ein gewichtetes Maß für die Modellgenauigkeit, das auf der gleichen Skala wie das Vorhersageziel angegeben wird. Einfach ausgedrückt kann der RMSE als der durchschnittliche Fehler interpretiert werden, den die Vorhersagen des Modells im Vergleich zum tatsächlichen Wert aufweisen, wobei größere Vorhersagefehler zusätzlich gewichtet werden.\nJe näher der RMSE-Wert bei 0 liegt, desto genauer ist das Modell. Der RMSE-Wert wird jedoch auf derselben Skala zurückgegeben wie das Ziel, für das Sie Vorhersagen treffen, und daher gibt es keine allgemeine Regel für die Interpretation von Wertebereichen. Die Interpretation Ihres Wertes kann nur innerhalb Ihres Datensatzes bewertet werden.\n\ndrone_tbl %&gt;%\n  rmse(pressure_stick, drone_rgb)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        694.\n\n\nAls letzte Möglichkeit sei noch der normalisierte root mean square error (abk. nRMSE) genannt. In diesem Fall wird der RMSE nochmal durch den Mittelwert von \\(y\\) geteilt.\n\\[\nnRMSE = \\cfrac{RMSE}{\\bar{y}} = \\cfrac{\\sqrt{MSE}}{\\bar{y}} = \\cfrac{\\sqrt{\\cfrac{1}{N}\\sum^N_{i=1}(y_i - \\hat{y}_i)^2}}{\\bar{y}}\n\\]\nIn wie weit jetzt jedes MSE Abweichungsmaß sinnvoll ist und auch in der Anwendung passen mag, sei einmal dahingestellt. Wichtig ist hier zu Wissen, dass wir die MSE-Fehler nutzen um verschiedene Verfahren zu vergleichen. Ein kleiner Fehler ist immer besser. Ein einzelner MSE-Wert an sich, ist dann immer schwer zu interpretieren.\nAls Alternative zu den MSE-Fehlern bietet sich dann der MAE an. Hier schauen wir dann auf die absoluten Abstände. Wir nehmen also das Vorzeichen raus, damit sich die Abstände nicht zu 0 aufaddieren. Wir haben dann folgende Formel vorliegen.\n\\[\nMAE = \\cfrac{1}{n}\\sum^n_{i=1}|y_i - \\hat{y}_i|\n\\]\nDer MAE hat gegenüber dem RMSE Vorteile in der Interpretierbarkeit. Der MAE ist der Durchschnitt der absoluten Werte der Fehler. MAE ist grundsätzlich leichter zu verstehen als die Quadratwurzel aus dem Durchschnitt der quadrierten Fehler. Außerdem beeinflusst jede einzelne Abweichung den MAE in direktem Verhältnis zum absoluten Wert der Abweichung, was bei der RMSE nicht der Fall ist. Der MAE ist nicht identisch mit dem mittleren quadratischen Fehler (RMSE), auch wenn einige Forscher ihn so angeben und interpretieren. MAE ist konzeptionell einfacher und auch leichter zu interpretieren als RMSE: Es ist einfach der durchschnittliche absolute vertikale oder horizontale Abstand zwischen jedem Punkt in einem Streudiagramm und der Geraden.\n\ndrone_tbl %&gt;%\n  mae(pressure_stick, drone_rgb)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard        691.\n\n\nWir können uns mit der Funktion metrics() auch die Fehler zusammenausgeben lassen.\n\ndrone_tbl %&gt;%\n  metrics(pressure_stick, drone_rgb)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     694.   \n2 rsq     standard       0.932\n3 mae     standard     691.   \n\n\nWie schon oben geschrieben, der MSE und Co. sind nur in einem Vergleich sinnvoll. Deshalb hier nochmal der Vergleich der beiden Farbskalen der Dronenbilder.\n\ndrone_tbl %&gt;%\n  metrics(pressure_stick, drone_rgb)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     694.   \n2 rsq     standard       0.932\n3 mae     standard     691.   \n\ndrone_tbl %&gt;%\n  metrics(pressure_stick, drone_cmyk)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     843.   \n2 rsq     standard       0.546\n3 mae     standard     831.   \n\n\nWir schon zu erwarten ist auch hier der Fehler bei den RGB-Werten kleiner als bei den CMYK-Werten. Daher würden wir uns hier für die Umrechnung der RGB-Werte entscheiden."
  },
  {
    "objectID": "stat-modeling-noninferiority.html#medizinische--oder-behandlungsgleichheit",
    "href": "stat-modeling-noninferiority.html#medizinische--oder-behandlungsgleichheit",
    "title": "48  Äquivalenz oder Nichtunterlegenheit",
    "section": "\n48.5 Medizinische- oder Behandlungsgleichheit",
    "text": "48.5 Medizinische- oder Behandlungsgleichheit\nEFSA\nThe limits for equivalence were set to \\(-\\cfrac{1}{2}\\log\\) and \\(\\cfrac{1}{2}\\log\\) equal to -0.5 and 0.5 because of the log transformation of the outcome.\nAllgemeine Methoden des Institut für Qualität und Wirtschaftlichkeit im Gesundheitswesen (IQWiG)\n“Umgekehrt erfordert auch die Interpretation nicht statistisch signifikanter Ergebnisse Aufmerksamkeit. Insbesondere wird ein solches Ergebnis nicht als Nachweis für das Nichtvorhandensein eines Effekts (Abwesenheit bzw. Äquivalenz) gewertet”\n9.3.5 Nachweis der Gleichheit\n\\[\n\\begin{aligned}\nH_0: \\bar{y}_{1} &\\neq \\bar{y}_{2} \\\\  \nH_A: \\bar{y}_{1} &= \\bar{y}_{2} \\\\   \n\\end{aligned}\n\\]\nAltman und Bland (1995)\n\ncold_seed_tbl %&gt;% \n  pivot_longer(cold:last_col(),\n               names_to = \"type\",\n               values_to = \"growth\") %&gt;% \n  mutate(type = as_factor(type)) %&gt;% \n  ggplot(aes(trt, growth, fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  facet_wrap(~ type, scales = \"free_y\") +\n  scale_fill_okabeito() +\n  theme(legend.position = \"none\")\n\n\n\nAbbildung 48.3— test\n\n\n\n\n48.5.1 ANOVA mit Effektschätzer\n\nlm_non_cold_fit &lt;- lm(log_non_cold ~ trt, data = cold_seed_tbl)\n\nlm_non_cold_fit %&gt;% anova %&gt;% model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |    F |     p\n---------------------------------------------------------\ntrt       |       26.39 |  6 |        4.40 | 2.75 | 0.018\nResiduals |      118.50 | 74 |        1.60 |      |      \n\nAnova Table (Type 1 tests)\n\n\n\nlm_non_cold_fit %&gt;% eta_squared()\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\ntrt       | 0.18 | [0.02, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\n\nlm_cold_fit &lt;- lm(log_cold ~ trt, data = cold_seed_tbl)\n\nlm_cold_fit %&gt;% anova %&gt;% model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |    F |     p\n---------------------------------------------------------\ntrt       |        9.80 |  6 |        1.63 | 2.71 | 0.020\nResiduals |       44.59 | 74 |        0.60 |      |      \n\nAnova Table (Type 1 tests)\n\n\n\nlm_cold_fit %&gt;% eta_squared()\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\ntrt       | 0.18 | [0.02, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\n\n48.5.2 Äquivalenztest\n\nres_non_cold &lt;- equivalence_test(lm_non_cold_fit, \n                                 ci = 0.95,\n                                 range = c(-0.5, 0.5))\n\n\nplot(res_non_cold) +\n  theme_minimal()\n\n\n\nAbbildung 48.4— test\n\n\n\n\nres_cold &lt;- equivalence_test(lm_cold_fit, \n                             ci = 0.95,\n                             range = c(-0.5, 0.5))\n\n\nplot(res_cold) +\n  theme_minimal()\n\n\n\nAbbildung 48.5— test"
  },
  {
    "objectID": "stat-modeling-noninferiority.html#links",
    "href": "stat-modeling-noninferiority.html#links",
    "title": "48  Äquivalenz oder Nichtunterlegenheit",
    "section": "\n48.6 Links",
    "text": "48.6 Links\nhttps://www.bmj.com/content/311/7003/485\nhttps://www.google.com/search?q=noninferiority+R&sxsrf=AJOqlzW7GmsZSD9QfcFTDil2ONGUUNudHw%3A1674985644154&ei=rEDWY4CKCeaSxc8PoO6D6Ag&ved=0ahUKEwiAkMviv-z8AhVmSfEDHSD3AI0Q4dUDCBA&uact=5&oq=noninferiority+R&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAzIICAAQgAQQywEyCAgAEIAEEMsBMggIABCABBDLATIICAAQFhAeEAoyBggAEBYQHjIGCAAQFhAeMgsIABAeEA0QDxDxBDILCAAQHhANEA8Q8QQyBggAEBYQHjIGCAAQFhAeOgUIABCABDoHCAAQgAQQCkoECEEYAEoECEYYAFAAWJoJYMUKaABwAXgAgAFZiAGEApIBATOYAQCgAQKgAQHAAQE&sclient=gws-wiz-serp\nhttps://cran.r-project.org/web/packages/PowerTOST/vignettes/NI.html\n\n\n\n\nAltman, Douglas G, und J Martin Bland. 1995. „Statistics notes: Absence of evidence is not evidence of absence“. Bmj 311 (7003): 485."
  },
  {
    "objectID": "stat-modeling-meta.html#genutzte-r-pakete",
    "href": "stat-modeling-meta.html#genutzte-r-pakete",
    "title": "49  Metaanalysen",
    "section": "\n49.1 Genutzte R Pakete",
    "text": "49.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, \n               meta)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\ncbbPalette &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-meta.html#daten",
    "href": "stat-modeling-meta.html#daten",
    "title": "49  Metaanalysen",
    "section": "\n49.2 Daten",
    "text": "49.2 Daten\nWoher kommen eigentlich die Daten einer Metaanalyse? Wir können Google Scholar nutzen um zu einem Thema systematisch wissenschaftliche Veröffentlichungen zu suchen. Der Fokus liegt hier auf systematisch und beschreibt einen strukturierten Reviewprozess. In diesem Kapitel nutzen wir die Daten von Harrer u. a. (2021) modifiziert auf ein agrarwissenschaftliches Beispiel. Wie immer liefert die originale Quelle noch mehr Informationen, wir kürzen hier einmal ab, damit wir die Kerngedanken verstehen.\n\n\nA step by step guide for conducting a systematic review and meta-analysis with simulation data\nBeginnen wir mit einer Datensatz in dem wir uns den Effekt von mittleren Erträgen in Weizen unter der Gabe von Eisen anschauen. Wir haben uns hier für eine Eisendosis mit \\(10\\mu mol\\) entschieden, die in allen Studien vorgekommen ist. Dann haben wir noch geschaut wie viele Pflanzen in der Gruppe untersucht wurden und wie die mitteleren Effekte plus die Standardabweichung waren.\n\ndrymatter_tbl &lt;- read_excel(\"data/meta/drymatter_iron_studies.xlsx\") \n\nSchauen wir uns nochmal die Daten genauer an. Hier ist es besonders wichtig zu beachten, dass wir uns einen Wert aus einem ganzen Experiment anschauen und verschiedene Werte aus verschiedenen Experimenten dann miteinander in Verbindung setzen wollen.\n\n\n\n\nTabelle 49.1— Daten zu den Weizenerträgen nach der Gabe von einer Eisendosis mit \\(10\\mu mol\\). In allen Studien wurde die gleiche Dosis auf die \\(n\\) Pflanzen gegeben.\n\nauthor\nn\nmean\nsd\n\n\n\nDeRubeis, 2005\n180\n32.6\n9.4\n\n\nDimidjian, 2006\n145\n31.9\n7.4\n\n\nDozois, 2009\n48\n28.6\n9.9\n\n\nLesperance, 2007\n142\n30.3\n9.1\n\n\nMcBride, 2007\n301\n31.9\n9.2\n\n\nQuilty, 2014\n104\n29.8\n8.6\n\n\n\n\n\n\nEin weiterer Effekt den wir uns anschauen können ist der Vergleich von Anteilen. In diesem Fall haben wir Ereignisse (eng. event) gezählt und wissen aber auch die Gesamtzahl an möglichen Ereignissen. Konkret haben wir die Anzahl an infizierten Sonnenblumensamen mit Mehltau nach der Behandlung mit MoldEx betrachtet. Dabei haben wir richtig viele Pflanzen (\\(n\\)) angeschaut und gezählt wie viele Samen dann mit Mehltau infiziert waren (event).\n\nsunflower_tbl &lt;- read_excel(\"data/meta/infected_sunflower_studies.xlsx\") \n\nAuch hier haben wir einmal in die ganzen Studien zu dem Wirkstoff MoldEx geschaut und jeweils rausgeschrieben, wie viele Sonnenblumensamen (\\(n\\)) betrachtet wurden und wie viele von den Sonnenblumen dann infiziert waren (event).\n\n\n\n\nTabelle 49.2— Daten zu den mit Mehltau infizierten Sonnenblumensamen nach der Behandlung mit MoldEx.\n\nauthor\nevent\nn\n\n\n\nBecker, 2008\n2186\n21826\n\n\nBoyd, 2009\n91\n912\n\n\nBoyd, 2007\n126\n1084\n\n\nCerda, 2014\n543\n7646\n\n\nFiellin, 2013\n6496\n55215\n\n\nJones, 2013\n10850\n114783\n\n\nLord, 2011\n86\n527\n\n\nMcCabe, 2005\n668\n9403\n\n\nMcCabe, 2012\n843\n11274\n\n\nMcCabe, 2013\n647\n8888\n\n\nNakawai, 2012\n11521\n126764\n\n\nSung, 2005\n1111\n11554\n\n\nTetrault, 2007\n2090\n16599\n\n\nWu, 2008\n2193\n25127\n\n\nZullig, 2012\n1913\n22783\n\n\n\n\n\n\nHäufig kann es vorkommen, dass wir weder die Mittelwerte und die Standardabweichung vorliegen haben oder aber die Anteile. Meist haben wir dann Glück, dass wir Effektschätzer wie das Odds ratio (\\(OR\\)), Risk ratio (\\(RR\\)) für die Anteile vorliegen haben. Oder aber wir finden Cohen’s \\(d\\) oder Hedge’s \\(g\\) für den Effekt der standardisierten Mittelwertsunterschiede.\nIn unserem Fall haben wir jetzt Euterkrebsdaten von Kühen und die entsprechenden Hedge’s \\(g\\) Werte für die Differenz der Kontrolle zur Chemotherapie. Auch hier haben alle Kühe die gleiche Chemotherapie erhalten und wir sind nur an dem Effekt zu der Kontrolle interessiert. Es gibt also nur einen paarweisen Gruppenvergleich.\n\ncow_tbl &lt;- read_excel(\"data/meta/cow_cancer_studies.xlsx\") \n\nSchauen wir uns nochmal einen Ausschnitt der Daten in der zu dem Euterkrebs von Kühen an.\n\n\n\n\nTabelle 49.3— Daten zum Euterkrebs von Kühen nach der Behandlung mit einer Chemotherapie zu einer Kontrolle.\n\nAuthor\nTE\nseTE\n\n\n\nCall et al.\n0.71\n0.26\n\n\nCavanagh et al.\n0.35\n0.20\n\n\nDanitzOrsillo\n1.79\n0.35\n\n\nde Vibe et al.\n0.18\n0.12\n\n\nFrazier et al.\n0.42\n0.14\n\n\nFrogeli et al.\n0.63\n0.20\n\n\nGallego et al.\n0.72\n0.22\n\n\nHazlett-Stevens & Oren\n0.53\n0.21\n\n\nHintz et al.\n0.28\n0.17\n\n\nKang et al.\n1.28\n0.34\n\n\nKuhlmann et al.\n0.10\n0.19\n\n\nLever Taylor et al.\n0.39\n0.23\n\n\nPhang et al.\n0.54\n0.24\n\n\nRasanen et al.\n0.43\n0.26\n\n\nRatanasiripong\n0.52\n0.35\n\n\nShapiro et al.\n1.48\n0.32\n\n\nSong & Lindquist\n0.61\n0.23\n\n\nWarnecke et al.\n0.60\n0.25\n\n\n\n\n\n\nWir haben jetzt also insgesamt drei Datensätze. Einmal einen Datensatz zu Weizenerträgen mit dem Effekt der Mittelwerte, einen Datensatz der Infektionen von Sonnenblumen mit Anteilen sowie einem Datensatz mit Euterkrebs mit vorausberechneten Effektmaß Hedge’s \\(g\\)."
  },
  {
    "objectID": "stat-modeling-meta.html#das-modell-mit-fixen-effekten",
    "href": "stat-modeling-meta.html#das-modell-mit-fixen-effekten",
    "title": "49  Metaanalysen",
    "section": "\n49.3 Das Modell mit fixen Effekten",
    "text": "49.3 Das Modell mit fixen Effekten\nDie Idee hinter dem Modell mit fixen Effekten (eng. fixed effect) ist, dass die beobachteten Effektgrößen von Studie zu Studie variieren können, was aber nur auf den Stichprobenfehler zurückzuführen ist. In Wirklichkeit sind die wahren Effektgrößen alle gleich: die Effekte sind fix. Aus diesem Grund wird das Modell mit festen Effekten manchmal auch als Modell mit “gleichen Effekten” oder “gemeinsamen Effekten” bezeichnet.\nDas Modell der festen Effekte geht davon aus, dass alle unsere Studien Teil einer homogenen Population sind und dass die einzige Ursache für Unterschiede in den beobachteten Effekten der Stichprobenfehler der Studien ist. Wenn wir die Effektgröße jeder Studie ohne Stichprobenfehler berechnen würden, wären alle wahren Effektgrößen absolut gleich."
  },
  {
    "objectID": "stat-modeling-meta.html#das-modell-mit-zufälligen-effekten",
    "href": "stat-modeling-meta.html#das-modell-mit-zufälligen-effekten",
    "title": "49  Metaanalysen",
    "section": "\n49.4 Das Modell mit zufälligen Effekten",
    "text": "49.4 Das Modell mit zufälligen Effekten\nDas Modell der zufälligen Effekte (eng. random effect) geht davon aus, dass es nicht nur eine wahre Effektgröße gibt, sondern eine Verteilung der wahren Effektgrößen. Das Ziel des Modells mit zufälligen Effekten ist es daher nicht, die eine wahre Effektgröße aller Studien zu schätzen, sondern den Mittelwert der Verteilung der wahren Effekte.\nIn der Praxis ist es sehr ungewöhnlich, eine Auswahl von Studien zu finden, die vollkommen homogen ist. Dies gilt selbst dann, wenn wir uns an bewährte Verfahren halten und versuchen, den Umfang unserer Analyse so präzise wie möglich zu gestalten.\nIn vielen Bereichen, einschließlich der Medizin und der Sozialwissenschaften, ist es daher üblich, immer ein Modell mit zufälligen Effekten zu verwenden, da ein gewisses Maß an Heterogenität zwischen den Studien praktisch immer zu erwarten ist. Ein Modell mit festen Effekten kann nur dann verwendet werden, wenn keine Heterogenität zwischen den Studien festgestellt werden konnte und wenn wir sehr gute Gründe für die Annahme haben, dass der wahre Effekt fest ist. Dies kann zum Beispiel der Fall sein, wenn nur exakte Replikationen einer Studie betrachtet werden oder wenn wir Teilmengen einer großen Studie meta-analysieren. Natürlich ist dies nur selten der Fall, und Anwendungen des Modells mit festem Effekt “in freier Wildbahn” sind eher selten."
  },
  {
    "objectID": "stat-modeling-meta.html#indirekte-vergleiche-in-r",
    "href": "stat-modeling-meta.html#indirekte-vergleiche-in-r",
    "title": "49  Metaanalysen",
    "section": "\n49.5 Indirekte Vergleiche in R",
    "text": "49.5 Indirekte Vergleiche in R\n\nm.mean &lt;- metamean(n = n,\n                   mean = mean,\n                   sd = sd,\n                   studlab = author,\n                   data = drymatter_tbl,\n                   sm = \"MRAW\",\n                   fixed = FALSE,\n                   random = TRUE,\n                   method.tau = \"REML\",\n                   hakn = TRUE,\n                   title = \"Ertrag von Weizen nach Eisenbehandlung\")\n\n\nsummary(m.mean)\n\nReview:     Ertrag von Weizen nach Eisenbehandlung\n\n                    mean             95%-CI %W(random)\nDeRubeis, 2005   32.6000 [31.2268; 33.9732]       18.0\nDimidjian, 2006  31.9000 [30.6955; 33.1045]       19.4\nDozois, 2009     28.6000 [25.7993; 31.4007]        9.1\nLesperance, 2007 30.3000 [28.8033; 31.7967]       17.0\nMcBride, 2007    31.9000 [30.8607; 32.9393]       20.7\nQuilty, 2014     29.8000 [28.1472; 31.4528]       15.8\n\nNumber of studies combined: k = 6\nNumber of observations: o = 920\n\n                        mean             95%-CI\nRandom effects model 31.1221 [29.6656; 32.5786]\n\nQuantifying heterogeneity:\n tau^2 = 1.0937 [0.0603; 12.9913]; tau = 1.0458 [0.2456; 3.6043]\n I^2 = 64.3% [13.8%; 85.2%]; H = 1.67 [1.08; 2.60]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 14.00    5  0.0156\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 5)\n- Untransformed (raw) means\n\n\n\nm.prop &lt;- metaprop(event = event,\n                   n = n,\n                   studlab = author,\n                   data = sunflower_tbl,\n                   method = \"GLMM\",\n                   sm = \"PLOGIT\",\n                   fixed = FALSE,\n                   random = TRUE,\n                   hakn = TRUE,\n                   title = \"Befall von Sonnenblumen mit Mehltau\")\nsummary(m.prop)\n\nReview:     Befall von Sonnenblumen mit Mehltau\n\n               proportion           95%-CI\nBecker, 2008       0.1002 [0.0962; 0.1042]\nBoyd, 2009         0.0998 [0.0811; 0.1211]\nBoyd, 2007         0.1162 [0.0978; 0.1368]\nCerda, 2014        0.0710 [0.0654; 0.0770]\nFiellin, 2013      0.1176 [0.1150; 0.1204]\nJones, 2013        0.0945 [0.0928; 0.0962]\nLord, 2011         0.1632 [0.1327; 0.1976]\nMcCabe, 2005       0.0710 [0.0659; 0.0764]\nMcCabe, 2012       0.0748 [0.0700; 0.0798]\nMcCabe, 2013       0.0728 [0.0675; 0.0784]\nNakawai, 2012      0.0909 [0.0893; 0.0925]\nSung, 2005         0.0962 [0.0908; 0.1017]\nTetrault, 2007     0.1259 [0.1209; 0.1311]\nWu, 2008           0.0873 [0.0838; 0.0908]\nZullig, 2012       0.0840 [0.0804; 0.0876]\n\nNumber of studies combined: k = 15\nNumber of observations: o = 434385\nNumber of events: e = 41364\n\n                     proportion           95%-CI\nRandom effects model     0.0944 [0.0836; 0.1066]\n\nQuantifying heterogeneity:\n tau^2 = 0.0558; tau = 0.2362; I^2 = 98.3% [97.9%; 98.7%]; H = 7.74 [6.92; 8.66]\n\nTest of heterogeneity:\n      Q d.f.  p-value             Test\n 838.21   14 &lt; 0.0001        Wald-type\n 826.87   14 &lt; 0.0001 Likelihood-Ratio\n\nDetails on meta-analytical method:\n- Random intercept logistic regression model\n- Maximum-likelihood estimator for tau^2\n- Random effects confidence interval based on t-distribution (df = 14)\n- Logit transformation\n- Clopper-Pearson confidence interval for individual studies\n\n\n\nm.gen &lt;- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = cow_tbl,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\nsummary(m.gen)\n\nReview:     Third Wave Psychotherapies\n\n                          SMD            95%-CI %W(random)\nCall et al.            0.7091 [ 0.1979; 1.2203]        5.0\nCavanagh et al.        0.3549 [-0.0300; 0.7397]        6.3\nDanitzOrsillo          1.7912 [ 1.1139; 2.4685]        3.8\nde Vibe et al.         0.1825 [-0.0484; 0.4133]        7.9\nFrazier et al.         0.4219 [ 0.1380; 0.7057]        7.3\nFrogeli et al.         0.6300 [ 0.2458; 1.0142]        6.3\nGallego et al.         0.7249 [ 0.2846; 1.1652]        5.7\nHazlett-Stevens & Oren 0.5287 [ 0.1162; 0.9412]        6.0\nHintz et al.           0.2840 [-0.0453; 0.6133]        6.9\nKang et al.            1.2751 [ 0.6142; 1.9360]        3.9\nKuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.3\nLever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6\nPhang et al.           0.5407 [ 0.0619; 1.0196]        5.3\nRasanen et al.         0.4262 [-0.0794; 0.9317]        5.1\nRatanasiripong         0.5154 [-0.1731; 1.2039]        3.7\nShapiro et al.         1.4797 [ 0.8618; 2.0977]        4.2\nSong & Lindquist       0.6126 [ 0.1683; 1.0569]        5.7\nWarnecke et al.        0.6000 [ 0.1120; 1.0880]        5.2\n\nNumber of studies combined: k = 18\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [0.3782; 0.7760] 6.12 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 17)"
  },
  {
    "objectID": "stat-modeling-meta.html#forest-plots",
    "href": "stat-modeling-meta.html#forest-plots",
    "title": "49  Metaanalysen",
    "section": "\n49.6 Forest Plots",
    "text": "49.6 Forest Plots\n\n\nForest Plots\n\nforest.meta(m.mean, \n            sortvar = TE,\n            prediction = TRUE, \n            print.tau2 = FALSE)\n\n\n\nAbbildung 49.1— foo.\n\n\n\n\nforest.meta(m.prop, \n            sortvar = TE,\n            prediction = TRUE, \n            print.tau2 = FALSE,\n            leftlabs = c(\"Author\", \"event\", \"n\"))\n\n\n\nAbbildung 49.2— foo.\n\n\n\n\nforest.meta(m.gen, \n            sortvar = TE,\n            prediction = TRUE, \n            print.tau2 = FALSE,\n            leftlabs = c(\"Author\", \"g\", \"SE\"))\n\n\n\nAbbildung 49.3— foo."
  },
  {
    "objectID": "stat-modeling-meta.html#publication-bias",
    "href": "stat-modeling-meta.html#publication-bias",
    "title": "49  Metaanalysen",
    "section": "\n49.7 Publication Bias",
    "text": "49.7 Publication Bias\n\n\nPublication Bias\n\nfunnel.meta(m.prop,\n            xlim = c(-3, -1.5),\n            studlab = TRUE)\n\nfunnel.meta(m.mean,\n            xlim = c(27, 35),\n            studlab = TRUE)\n\nfunnel.meta(m.gen,\n            xlim = c(-0.5, 2),\n            studlab = TRUE)\n\n\n\n\n\n(a) Verteilung der beobachteten Werte.\n\n\n\n\n\n(b) Verteilung der theoretischen Werte.\n\n\n\n\n\n\n\n(c) TEst\n\n\n\nAbbildung 49.4— dst."
  },
  {
    "objectID": "stat-modeling-meta.html#referenzen",
    "href": "stat-modeling-meta.html#referenzen",
    "title": "49  Metaanalysen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nBalduzzi, Sara, Gerta Rücker, und Guido Schwarzer. 2019. „How to perform a meta-analysis with R: a practical tutorial“. BMJ Ment Health 22 (4): 153–60.\n\n\nHarrer, Mathias, Pim Cuijpers, Furukawa Toshi A, und David D Ebert. 2021. Doing Meta-Analysis With R: A Hands-On Guide. 1st Aufl. Boca Raton, FL; London: Chapman & Hall/CRC Press.\n\n\nTawfik, Gehad Mohamed, Kadek Agus Surya Dila, Muawia Yousif Fadlelmola Mohamed, Dao Ngoc Hien Tam, Nguyen Dang Kien, Ali Mahmoud Ahmed, und Nguyen Tien Huy. 2019. „A step by step guide for conducting a systematic review and meta-analysis with simulation data“. Tropical medicine and health 47 (1): 1–9."
  },
  {
    "objectID": "stat-modeling-permutationstest.html#genutzte-r-pakete",
    "href": "stat-modeling-permutationstest.html#genutzte-r-pakete",
    "title": "50  Permutationstest",
    "section": "\n50.1 Genutzte R Pakete",
    "text": "50.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-modeling-permutationstest.html#daten",
    "href": "stat-modeling-permutationstest.html#daten",
    "title": "50  Permutationstest",
    "section": "\n50.2 Daten",
    "text": "50.2 Daten\nFür unsere Demonstration des Permutationstest nutzen wir den Datensatz flea_dog_cat_length_weight.xlsx. Zum einen wollen wir einen Gruppenvergleich zwischen den Sprungweiten der Hunde- und Katzenflöhe rechnen. Also einen klassischen t-Test für einen Gruppenvergleich. Nur eben hier als einen Permutationstest. Als zweites wollen wir einen \\(p\\)-Wert für das Bestimmtheitsmaß \\(R^2\\) abschätzen. Per se gibt es keinen \\(p\\)-Wert für das Bestimmtheitsmaß \\(R^2\\), aber der Permutationstest liefert hier eine Lösung für das Problem. Daher schauen wir uns in einer simplen linearen Regression den Zusammenhang zwischen einem \\(y\\) und einem \\(x_1\\) an. Daher wählen wir aus dem Datensatz die beiden Spalten jump_length und weight. Wir wollen nun feststellen, ob es einen Zusammenhang zwischen der Sprungweite in [cm] und dem Flohgewicht in [mg] gibt. In dem Datensatz finden wir 400 Flöhe wir wählen aber nur zufällig 20 Tiere aus.\n\nmodel_tbl &lt;- read_csv2(\"data/flea_dog_cat_length_weight.csv\") %&gt;%\n  select(animal, jump_length, weight) %&gt;% \n  filter(animal %in% c(\"dog\", \"cat\")) %&gt;% \n  slice_sample(n = 20)\n\n\n\n\n\n\n\nHier überfährt man doch den einen oder anderen Statistikengel. Mit dem Permutationstest geht dann vieles und manchmal dann auch zu viel… gerne können wir mal darüber diskutieren, was du da permutieren willst."
  },
  {
    "objectID": "stat-modeling-permutationstest.html#einfacher-mittelwertsvergleich",
    "href": "stat-modeling-permutationstest.html#einfacher-mittelwertsvergleich",
    "title": "50  Permutationstest",
    "section": "\n50.3 Einfacher Mittelwertsvergleich",
    "text": "50.3 Einfacher Mittelwertsvergleich\nWir wollen zuerst einmal mit einem einfachen Mittelwertsvergleich anfangen. Im Prinzip bauen wir hier kompliziert einen t-Test nach. Der t-Test testet, ob es einen signifikanten Mittelwertsunterschied gibt. Anstatt jetzt den t-Test zu rechnen, berechnen wir erstmal das \\(\\Delta\\) und damit den Mittelwertsunterschied der Sprungweiten der Hunde- und Katzenflöhe.\n\nmodel_tbl %&gt;%\n  group_by(animal) %&gt;% \n  summarise(mean_jump = mean(jump_length)) %&gt;% \n  pull(mean_jump) %&gt;% \n  diff()\n\n[1] 3.376154\n\n\nWir sehen, dass die Hunde- und Katzenflöhe im Mittel einen Unterschied in der Sprungweite von \\(3.38cm\\) haben. Das ist der Mittelwertsunterschied in unseren beobachteten Daten.\nJetzt wollen wir einmal einen Permutationstest rechnen. Die wichtigste Funktion hierfür ist die Funktion sample(). Die Funktion sample() durchmischt zufällig einen Vektor. Einmal ein Beispiel für die Zahlen 1 bis 10, die wir in die Funktion sample() pipen.\n\nc(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) %&gt;% \n  sample()\n\n [1]  4  5  8  1  3  9  2  7  6 10\n\n\nDas gleiche Durchmischen findet auch in der Funktion mutate() statt. Wir durchwirblen einmal die Zuordnung der Level des Faktors animal zu den jeweiligen Speungweiten. Dann berechnen wir die Mittelwertsdifferenz für diesen neuen Datensatz. Das machen wir dann \\(n\\_sim\\) gleich 1000 Mal.\n\nn_sim &lt;- 1000\n\nmean_perm_tbl &lt;- map_dfr(1:n_sim, \\(x) {\n  mean_diff &lt;- model_tbl %&gt;%\n    ## Permutation Start\n    mutate(animal = sample(animal)) %&gt;% \n    ## Permutation Ende\n    group_by(animal) %&gt;% \n    summarise(mean_jump = mean(jump_length)) %&gt;% \n    pull(mean_jump) %&gt;% \n    diff()\n  return(tibble(mean_diff))\n})\n\nIn der Abbildung 50.1 sehen wir die Verteilung aller Mittelwertsdifferenzen, die aus unserem permutierten Datensätzen herausgekommem sind.\n\n\n\n\nAbbildung 50.1— Histogramm der permutierten Mittelwertsdifferenzen\n\n\n\n\nsum(mean_perm_tbl$mean_diff &gt;= 2.6119)/n_sim\n\n[1] 0.054\n\n\nIst das Gleiche als wenn wir dann den Mittelwert berechnen.\n\nmean(mean_perm_tbl$mean_diff &gt;= 2.6119) \n\n[1] 0.054\n\n\n\n\nTeilweise wird diskutiert, ob der \\(p\\)-Wert hier noch mal 2 genommen werden muss, da wir ja eigentlich zweiseitig Testen wollen, aber da gehen die Meinungen auseinander. Ich belasse es so wie hier.\nDann das ganze nochmal mit einem Student t-Test verglichen und wir sehen, dass wir dem \\(p\\)-Wert aus einem Student t-Test sehr nahe kommen. Wenn du jetzt noch die Anzahl an Simulationen erhöhen würdest, dann würde sich der \\(p_{perm}\\) dem \\(p_{t-Test}\\) immer weiter annähern.\n\nt.test(jump_length ~ animal, data = model_tbl) %&gt;% \n  pluck(\"p.value\") %&gt;% \n  round(3)\n\n[1] 0.049\n\n\nAm Ende bleibt dann die Frage, wie viele Permutationen sollen es denn sein? Auch hier sehen wir dann, dass der t-Test signifikant ist, aber der Permutationstest noch nicht. Vielleicht helfen da mehr Permutationen? Oder aber der Effekt ist dann doch zu gering. Hier musst du dann immer überlegen, ob du nicht zu sehr an dem Signifikanzniveau von 5% klebst. Am Ende muss dann der permutierte \\(p\\)-Wert zudammen mit dem Effekt im Kontext der Fragestellung diskutiert werden."
  },
  {
    "objectID": "stat-modeling-permutationstest.html#bestimmtheitsmaß-r2",
    "href": "stat-modeling-permutationstest.html#bestimmtheitsmaß-r2",
    "title": "50  Permutationstest",
    "section": "\n50.4 Bestimmtheitsmaß \\(R^2\\)\n",
    "text": "50.4 Bestimmtheitsmaß \\(R^2\\)\n\nDas vorherige Beispiel mit dem Mittelwertsvergleich war im Prinzip nur eine Fingerübung für den Ablauf. Wir können auch einfach einen t-Test rechnen und dann ist gut. Anders sieht es aus, wenn wir keinen \\(p\\)-Wert geliefert bekommen und auch keinen \\(H_0\\) Testverteilung bekannt ist um einen \\(p\\)-Wert zu bestimmen. Das ist der Fall bei dem Bestimmtheitsmaß \\(R^2\\). Wir haben hier keine Teststatistik und dadurch einen \\(p\\)-Wert vorliegen. Dagegen können wir dann mit einem Permutationstest was tun. Bei dem 95% Konfidenzintervall wird es dann schwieriger, hier müssen wir dann etwas anders permutieren. Wir nutzen dann die Bootstrap Konfidenzintervalle im nächsten Abschnitt.\n\nmodel_tbl %$%\n  lm(jump_length ~ weight) %&gt;% \n  glance() %&gt;% \n  pull(r.squared)\n\n[1] 0.3000498\n\n\nDamit haben wir erstmal das Bestimmtheitsmaß aus unseren Daten berechnet. Jetzt stellt sich natürlich die Frage, wie wahrscheinlich ist es den dieses Bestimmtheitsmaß von 0.30 zu beobachten? Dafür lassen wir jetzt einen Permutationstest laufen in dem wir die Daten einmal durchmischen.\n\nn_sim &lt;- 1000\n\nr2_perm_tbl &lt;- map_dfr(1:n_sim, \\(x) {\n  r2 &lt;- model_tbl %&gt;%\n    ## Permutation Start\n    mutate(weight = sample(weight)) %$%\n    ## Permutation Ende    \n    lm(jump_length ~ weight) %&gt;% \n    glance() %&gt;% \n    pull(r.squared)\n  return(tibble(r2))\n})\n\nIn der Abbildung 50.2 sehen wir die Verteilung aller Bestimmtheitsmaße \\(R^2\\), die aus unserem permutierten Datensätzen herausgekommem sind. Wir erkennen sofort, dass es wenig zufällig bessere Datensätze gibt, die ein höheres Bestimmtheitsmaße \\(R^2\\) erzeugen.\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\nAbbildung 50.2— Histogramm der permutierten Bestimmtheitsmaße \\(R^2\\)\n\n\n\nJetzt wollen wir einmal bestimmen wie viele Bestimmtheitsmaße \\(R^2\\) größer sind als unser Bestimmtheitsmaß \\(R^2 = 0.29\\) aus den Daten.\n\nsum(r2_perm_tbl$r2 &gt;= 0.291164)/n_sim \n\n[1] 0.013\n\n\nDie Berechnung ist das Gleiche, als wenn wir den Mittelwert aus der logischen Abfrage berechnen würden.\n\nmean(r2_perm_tbl$r2 &gt;= 0.291164) \n\n[1] 0.013\n\n\n\n\nTeilweise wird diskutiert, ob der \\(p\\)-Wert hier noch mal 2 genommen werden muss, da wir ja eigentlich zweiseitig Testen wollen, aber da gehen die Meinungen auseinander. Ich belasse es so wie hier.\nDamit haben wir unseren \\(p\\)-Wert für das Bestimmtheitsmaß \\(R^2\\) mit \\(0.013\\). Das ist was wir wollten und somit können wir dann auch sagen, dass wir einen signifikantes Bestimmtheitsmaß \\(R^2\\) vorliegen haben. Was noch fehlt ist das 95% Konfidenzintervall, was wir Mithilfe des Bootstrapverfahrens berechnen wollen."
  },
  {
    "objectID": "stat-modeling-permutationstest.html#bootstrap-95-konfidenzintervalle",
    "href": "stat-modeling-permutationstest.html#bootstrap-95-konfidenzintervalle",
    "title": "50  Permutationstest",
    "section": "\n50.5 Bootstrap 95% Konfidenzintervalle",
    "text": "50.5 Bootstrap 95% Konfidenzintervalle\nWir können die Methode des Bootstraping nutzen um uns die 95% Konfidenzintervalle über eine Simulation bzw. Permutation berechnen zu lassen. Haben wir in dem Permutatiosntest noch die Spalten permutiert so permutieren wir bei dem Bootstrap-Verfahren die Zeilen. Da wir aber keinen neuen Datensatz erhalten würden, wenn wir nur die Zeilen permutieren, ziehen wir einen kleineren Datensatz mit zurücklegen. Das heißt, dass wir auch Beobachtungen mehrfach in unseren gezogenen Bootstrapdatensatz haben können. Wir nutzen in R die Funktion slice_sample() in der wir dann auswählen, dass 80% der Beobachtungen mit zurücklegen gezogen werden sollen. Das Zurücklegen können wir mit der Option replace = TRUE einstellen. Wir führen das Bootstraping dann isngesamt 1000 mal durch.\n\nn_boot &lt;- 1000\n\nr2_boot_tbl &lt;- map_dfr(1:n_boot, \\(x) {\n  r2 &lt;- model_tbl %&gt;%\n    ## Bootstrap Start\n    slice_sample(prop = 0.8, replace = TRUE) %$%\n    ## Bootstrap Ende\n    lm(jump_length ~ weight) %&gt;% \n    glance() %&gt;% \n    pull(r.squared)\n  return(tibble(r2))\n})\n\nNachdem wir nun unser Bootstrap gerechnet haben und eintausend Bestimmtheitsmaße bestimmt haben, können wir einfach das \\(2.5\\%\\) und \\(97.5\\%\\) Quantile bestimmen um unser 95% Konfidenzintervall zu bestimmen. Zwischen \\(2.5\\%\\) und \\(97.5\\%\\) liegen ja auch 95% der Werte der eintausend Bestimmtheitsmaße.\n\nr2_boot_tbl %$% \n  quantile(r2, probs = c(0.025, 0.975)) %&gt;% \n  round(3)\n\n 2.5% 97.5% \n0.016 0.620 \n\n\nWir hatten ein Bestimmtheitsmaß \\(R^2\\) von \\(0.30\\) berechnet und können dann die untere und obere 95% Konfidenzgrenze ergänzen. Wir erhaltend dann \\(0.300\\, [0.016; 0.620]\\), somit liegt unser beobachtetes Bestimmtheitsmaß \\(R^2\\) mit 95% Sicherheit zwischen \\(0.016\\) und \\(0.620\\)."
  },
  {
    "objectID": "experimental-design-preface.html#übersicht-der-experimentellen-designs",
    "href": "experimental-design-preface.html#übersicht-der-experimentellen-designs",
    "title": "Experimentelles Design",
    "section": "Übersicht der experimentellen Designs",
    "text": "Übersicht der experimentellen Designs\nWir schauen uns in den folgenden Kapiteln einmal eine Auswahl an experimentellen Designs an. Im Laufe derZeit werden sicherlich noch andere Designs ergänzt werden. Soweit erstmal diese Auswahl hier.\n\nDas Complete randomized design findest du in Kapitel 51.2. Das Complete randomized design ist der Klassiker unter den experimentellen Designs und wird häufig verwendet.\nDas Randomized vomplete block design findest du in Kapitel 51.3. Das Randomized vomplete block design ist entweder eine Erweiterung des Complete randomized design oder aber bringt noch eine neuen Faktor für die Wiederholung mit in das Experiment mit ein.\nDas Latin square design findest du in Kapitel 51.4. Das Latin square design liefert eine gleichmäßige Aufteilung der experimentellen Einheiten über ein Feld oder ein Stall.\nDas Alpha design findest du in Kapitel 52.2 und ist eine etwas komplexere Einteilung für die Randomisierung.\nDas Augmented design findest du in Kapitel 52.5. Das Augmented design wird seltener genutzt.\nDas Splot plot design findest du in Kapitel 52.4. Das Splot plot design gibt es in vielen Varianten. Wir schauen uns hier eine der häufigsten Varianten einmal an. Je nachdem wie man die Plots anordnet ergibt sich dann auch teilweise ein anderes Splot plot design."
  },
  {
    "objectID": "experimental-design-preface.html#tipps-tricks",
    "href": "experimental-design-preface.html#tipps-tricks",
    "title": "Experimentelles Design",
    "section": "Tipps & Tricks",
    "text": "Tipps & Tricks\n\n\n\n\n\nWir berechnen meist den Mittelwert von \\(n\\) Pflanzen in einer Parzelle\nMarkiere dir die Pflanze, die du wiederholt messen willst, zu Beginn des Experiments mit einem farbigen Stock. Du kannst im Gewächshaus die Blumentöpfe mit Sprühfarben markieren.\nNehme dir einen Zollstock mit, wenn du photographierst.\nNehme dir die Zeit für die Fotos, später lassen sich viele Fotos nicht wiederholen.\n\nIn Tabelle 1 sehen wir einmal mögliche Quellen für die Verwirrung und die Möglichkeiten des experimentellen Design etwas gegen diese Quellen der Verwirrung zu unternehmen. Wir können hier Quelle der Verwirrung auch als Quelle der Varianz deuten. Eine detailierte Diskussion findet sich in Dormann (2013) und Hurlbert (1984). Wichtig ist hier mitzunehmen, dass wir häufig eine Kontrolle brauchen um überhaupt die Stärke des Effektes messen zu können. Sonst können wir die Frage, ob die Behandlung besser ist nicht quantifizieren.\nVersuchsergebnisse interpretieren\n\n\nTabelle 1— In Dormann (2013) und Hurlbert (1984) finden wir eine Zusammenfassung von Quellen der Verwirrung also eigentlich der Varianz und deren mögliche Lösung um die Varianz zu beherrschen.\n\n\n\n\n\n\n\nQuelle der Verwirrung\nMerkmal des experimentellen Designs um die Verwirrung zu reduziert oder aufzulösen\n\n\n\n1.\nZeitliche Veränderung\nKontrollgruppe\n\n\n2.\nArtefakte in der Behandlung\nKontrollgruppe\n\n\n3.\nVoreingenommenheit des Forschenden (Bias)\nRandomisierte Zuordnung der Versuchseinheiten zu den Behandlungen; generelle Randomisierung bei allen möglichen Prozessen; Verblindete Prozeduren\n\n\n4.\nVom Forschenden induzierte Variabilität (zufälliger Fehler)\nWiederholungen der Behandlungen (und Kontrolle)\n\n\n5.\nAnfängliche oder beinhaltende Variabilität zwischen den Versuchseinheiten\nWiederholungen der Behandlungen (und Kontrolle); Durchmischen der Behandlungen; Begleitbeobachtungen (Positive Kontrolle)\n\n\n6.\nNicht-dämonische Einflüsse\nWiederholung und Durchmischung der Behandlungen (und Kontrolle)\n\n\n7.\nDämonische Eingriffe\nEwige Wachsamkeit - siehe dazu auch Feynman (1998); Geisteraustreibung; Menschenopfer"
  },
  {
    "objectID": "experimental-design-preface.html#referenzen",
    "href": "experimental-design-preface.html#referenzen",
    "title": "Experimentelles Design",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nDormann, Carsten F. 2013. Parametrische Statistik. Springer.\n\n\nFeynman, Richard P. 1998. „Cargo cult science“. In The art and science of analog circuit design, 55–61. Elsevier.\n\n\nHurlbert, Stuart H. 1984. „Pseudoreplication and the design of ecological field experiments“. Ecological monographs 54 (2): 187–211."
  },
  {
    "objectID": "experimental-design-basic.html#genutzte-r-pakete-für-das-kapitel",
    "href": "experimental-design-basic.html#genutzte-r-pakete-für-das-kapitel",
    "title": "51  Grundlagen der Versuchsplanung",
    "section": "\n51.1 Genutzte R Pakete für das Kapitel",
    "text": "51.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, emmeans, multcomp, scales, performance,\n               effectsize, parameters)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"set_names\", \"magrittr\")\ncbbPalette &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "experimental-design-basic.html#sec-crd",
    "href": "experimental-design-basic.html#sec-crd",
    "title": "51  Grundlagen der Versuchsplanung",
    "section": "\n51.2 Complete randomized design (CRD)",
    "text": "51.2 Complete randomized design (CRD)\nDas komplette randomizierte Design (eng. complete randomized design) ist das simpleste Felddesign was wir anzubieten haben. Wir haben einen Stall oder ein Feld oder einen Tisch und unterteilen diesen Raum zufällig in Untereinheiten. Auf oder in jeder Untereinheit bringen wir dann eine Behandlung aus.\n\n\nAbbildung 51.1— Visualisierung des complete randomized design mit einer Behandlung und vier Behandlungsleveln.\n\nWir haben einen Tisch und stellen Töpfe mit Pflanzen auf den Tisch. Jeder Topf erhält zufällig eine Behandlung. Wir haben gleich viele Töpfe mit Pflanzen für jede Behandlung.\nWir haben einen Stall mit Buchten für Schweine. Jede Bucht erhält eine zufällige Behandlung. Wir haben gleich viele Buchten für jede Behandlung.\nWir haben ein Feld und erschaffen Parzellen auf dem Feld. Auf jeder Parzelle wird zufällig eine Variante ausgebracht. Wir haben geich viele Parzellen für jede Variante.\nSchauen wir uns das Complete randomized design einmal an einem konkreten Beispiel an. Wir nutzen dafür einen Faktor mit der Behandlung. Die Behandlung hat vier Level mit den einzelnen Leveln \\(A\\), \\(B\\), \\(C\\) und \\(D\\).\n\n51.2.1 Visualisierung\nIn Abbildung 51.2 sehen wir die Visualisierung unseres Versuches. Wir haben einen großen Raun in dem sich zufällig die Level der Behandlung drauf verteilen. Hierbei ist es wichtig zu verstehen, dass die Anordnung rein zufällig ist. Wir sehen, dass jedes Level der Behandlung mit \\(n = 5\\) auf das Feld aufgebracht wurde. Wir haben also ein balanciertes Design mit \\(N = 20\\) Beobachtungen. Wir könnten hier auch einen Tisch mit \\(n=20\\) Pflanzentöpfen vorliegen haben oder einen Stall mit \\(n = 20\\) Buchten.\n\n\nAbbildung 51.2— Visualisierung des complete randomized design mit einer Behandlung und vier Behandlungsleveln.\n\n\n51.2.2 Daten\nIm Folgenden bauen wir uns die Daten für das Complete randomized design. Dafür nuten wir die Funktion rnorm(). Die Funktion rnorm() erlaubt es aus einer Normalverteilung n Beobachtungen mit einem Mittelwert mean und einer Standardabweichung sd zu ziehen. Wir erschaffen uns so vier Behandlungsgruppen \\(A\\) bis \\(D\\) mit jeweils unterschiedlichen Mittelwerten von \\(\\bar{y}_A = 10\\), \\(\\bar{y}_B = 12\\), \\(\\bar{y}_C = 16\\) und \\(\\bar{y}_D = 20\\) sowie homogenen Varianzen mit \\(s_A = s_B = s_C = s_D = 2\\). Jede Behandlung hat \\(n = 5\\) Beobachtungen. Wir haben also ein balanziertes Design vorliegen.\n\nset.seed(20220916)\ncrd_tbl &lt;- tibble(A = rnorm(n = 5, mean = 10, sd = 2),\n                  B = rnorm(n = 5, mean = 12, sd = 2),\n                  C = rnorm(n = 5, mean = 16, sd = 2),\n                  D = rnorm(n = 5, mean = 20, sd = 2)) %&gt;% \n  gather(key = trt, value = rsp) %&gt;% \n  mutate(trt = as_factor(trt))\n\nSchauen wir uns einmal die Daten an, die wir in R erhalten. Das Objekt crd_tbl ist ein tibble in Long-Format nach der Anwendung der Funktion gather(). Wir haben auch die Spalte trt für die Behanldung als Faktor umgewandelt.\n\ncrd_tbl\n\n# A tibble: 20 × 2\n   trt     rsp\n   &lt;fct&gt; &lt;dbl&gt;\n 1 A     12.3 \n 2 A     11.0 \n 3 A     13.4 \n 4 A     13.2 \n 5 A      4.27\n 6 B     12.2 \n 7 B     11.4 \n 8 B     15.4 \n 9 B      9.51\n10 B     11.9 \n11 C     13.1 \n12 C     15.4 \n13 C     16.3 \n14 C     19.0 \n15 C     19.4 \n16 D     20.9 \n17 D     17.8 \n18 D     20.6 \n19 D     24.6 \n20 D     17.5 \n\n\nWir haben also \\(N = 20\\) Beobachtungen vorliegen. Wir immer ist es schwer eine Datentabelle zu erfasen. Daher schauen wir uns die Daten einmal in Abbildung 51.3 als Boxplots an. Wir wolllen uns noch die Punkte zusätzlich anzeigen lassen. bei der geringen Anzahl an Beobachtungen wäre ein Dotplot oder ein Scatterplot auch eine Möglichkeit.\n\nggplot(crd_tbl, aes(trt, rsp, fill = trt)) +\n  theme_bw() +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, shape = 4, size = 3) +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito() \n\n\n\nAbbildung 51.3— Boxplots der Behandlungsgruppen zufällig aus einer Normalverteilung mit Varianzhomogenität generierten Daten.\n\n\n\nWir erinnern uns, dass die Daten alle varianzhomogen und normalverteilt sind. Wir haben die Daten so erschaffen. Dennoch wirken die Boxplots so, als würde teilweise eine schiefe Verteilung vorliegen. Bei so wenigen Beobachtungen ist es immer schwer, für oder gegen eine Verteilung zu argumentieren. Wir bleiben bei einer Normalverteilung, wenn wir glauben, dass das \\(y\\) approimativ normalverteilt ist. Wir schreiben dann, dass wir ein normalverteiltes \\(y\\) annehmen.\n\n51.2.3 Modellierung\nIm Folgenden wollen wir die Daten modellieren. Das heist wir wollen eine Linie durch eine multidimensionale Punktewolke zeichnen. Daher auch lineares Modell oder eben durch die Funktion lm() in R für linear model. Wir nutzen das Paket parameters und die Funktion model_parameters() um uns die Parameter des Modells auszugeben. Wir könnten auch die Funktion tidy() nutzen, aber wir erhalten durch die Funktion model_parameters() etwas mehr Informationen und bessere Spaltenüberschriften.\nWir bauen das Modell in folgender Form. Wir haben ein numerisches Outcome \\(y\\) sowie einen Faktor \\(f_1\\).\n\\[\ny \\sim f_1\n\\]\nNun können wir das abstrakte Modell in die Daten übersetzen und erhalten folgendes Modell.\n\\[\nrsp \\sim trt\n\\]\nDas heist, unsere numerische Variable rsp hängt ab von unserer faktoriellen Variable trt. Wir müssen immer wissen, wie die Spaltennamen in unserem Datensatz crd_tbl lauten sonst kann R die Spalten nicht finden.\n\nfit_crd &lt;- lm(rsp ~ trt, crd_tbl)\n\nfit_crd %&gt;%  model_parameters()\n\nParameter   | Coefficient |   SE |         95% CI | t(16) |      p\n------------------------------------------------------------------\n(Intercept) |       10.83 | 1.30 | [ 8.07, 13.59] |  8.33 | &lt; .001\ntrt [B]     |        1.25 | 1.84 | [-2.65,  5.15] |  0.68 | 0.506 \ntrt [C]     |        5.80 | 1.84 | [ 1.90,  9.70] |  3.15 | 0.006 \ntrt [D]     |        9.47 | 1.84 | [ 5.57, 13.37] |  5.15 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nÜberlege mal, was die Spalte Coefficient aussagen möchte. Wir erhalten den (Intercept) mit \\(10.38\\) und damit den MIttelwert der Gruppe \\(A\\). In den folgenden Zeilen sind die Änderungen zu dem (Intercept) und damit zu der Gruppe \\(A\\) dargestellt. Da wir nur eine sehr kleine Anzhl an Beoabchtungen haben, haben wir hier auch Abweichungen zu den voreingestellten Mittelwerten und Standardabweichungen. Wir schauen uns ja auch nur eine Realisierung von möglichen Daten \\(D\\) an. Wir sehen, dass alle Koeffizienten signifikant und damit unterschiedlich von der Null sind. Der \\(p\\)-Wert ist kleiner als das Signiifkanzniveau von \\(\\alpha\\) gleich 5%.\nWir können jetzt nochmal überprüfen, ob die Residuen die Annahme der Varianzhomogenität erfüllen.\n\nfit_crd %&gt;% check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.737).\n\n\nSowie ob die Residuen normalverteilt sind.\n\nfit_crd %&gt;% check_normality()\n\nOK: residuals appear as normally distributed (p = 0.620).\n\n\nDa wir ja hiermit nur eine Zeile Text produziert haben und darübr hinaus wir gerne uns Dinge anschauen, können wir auch die Residuen einmal visualisieren. In Abbildung 51.4 sehen wir den QQ-Plot der Residuen sowie die Verteilung unserer Residuen in einem Desnityplot. Wir sehen, dass die Residuen einer Normalverteilung folgen.\n\ncheck_model(fit_crd, check = c(\"qq\", \"normality\"))\n\n\n\nAbbildung 51.4— QQ-Plot und Densityplot der Residuen aus dem lineare Modell.\n\n\n\nWunderbar. Wir können jetzt eine Varianzanalyse und dann eine Mittelwertsvergleich durchführen. Achtung, wir können uns hier auch etwas in die Ecke testen. Wenn wir nur lange genug neue Daten generieren, werden wir irgendwann auch einen Datensatz finden, der die Varianzhomogenität und die Normalverteilung ablehnt. Das liegt in der Theorie des statistischen Testens sowie der kleinen Fallzahl verborgen. Deshalb können wir im Zweifel gerne einmal deine Vortests in dem R Tutorium oder in einer statistischen Beratung diskutieren.\n\n51.2.4 Varianzanalyse und Mittelwertsvergleich\nDie einfaktorielle Varianzanalyse ist ziemlich einfach und ergibt sich fast von alleine. Wir nehmen das Objekt des Modells und pipen das Modell in die Funktion anova(). Wir lassen uns dann wieder die Modellparameter der ANOVA widergeben.\n\nres_anova &lt;- fit_crd %&gt;% \n  anova() \n\nres_anova %&gt;% model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |     F |      p\n-----------------------------------------------------------\ntrt       |      283.24 |  3 |       94.41 | 11.16 | &lt; .001\nResiduals |      135.35 | 16 |        8.46 |       |       \n\nAnova Table (Type 1 tests)\n\n\nWir sehen, dass der Faktor Behandlung signifkant ist, da der \\(p\\)-Wert kleiner ist als das Signifkanzniveau \\(\\alpha\\) gleich 5%. Wir können damit die Nullhypothese ablehnen, wir haben zumindestens einen paarweisen Gruppenunterschied in der Behandlung. Welchen wissen wir nicht, dafür machen wir dann die paarweisen Vergleiche. Eigentlich können wir uns in diesem simplen Fall die ANOVA schhenken und gleich den Mittelwertsvergleich rechnen. Aber das es Usus ist und auch in vielen Abschlussarbeiten verlangtt wird, machen wir hier es einfach mal gleich mit.\nJetzt brauchen wir nur noch die Effektstärke der ANOVA, also wieviel Varianz eigentlich der Faktor Behandlung erklärt. Dfür nutzen wir die Funktion eta_squared() aus dem Paket effectsize.\n\nres_anova %&gt;% eta_squared(partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 |       95% CI\n-------------------------------\ntrt       | 0.68 | [0.38, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nMit einem \\(\\eta^2\\) von \\(0.86\\) wissen wir, dass 86% der Varianz von dem Faktor Behandlung erklärt wird. Das wundert uns nicht, denn wir haben ja nur den Faktor Behandlung in unseren Daten aus denen sich unser Outcome ergibt.\nNachdem wir kurz die ANOVA gerechnet haben, wollen wir noch den Mittelwertsvergleich rechnen. Wir nutzen dazu das Paket emmeans. Wir müssen der Funktion emmeans() ein Objekt aus einem Modell übergeben und der Funktion mitteilen, was der Faktor ist mit dem der Vergleich gerechnet werden soll. Wir haben hier den Faktor trt vorliegen und wollen einen parweisen Vergleich über alle Level des Faktors rechnen.\n\nres_crd &lt;- fit_crd %&gt;% \n  emmeans(~ trt) \n\nWir haben die Ausgabe der Funktion emmeans() in dem Objekt res_crd gespeichert und nutzen das Objekt zuerst um einmal die Ausgabe für das comapct letter display zu erhalten. Als Adjustierung des \\(\\alpha\\) Fehlers nutzen wir die Adjustierung nach Bonferroni. Es sind auch andere Adjustierungen möglich, aber aus Gründen der Einfachheit nehmen wir hier mal den Klassiker der Adjustierung. Je nach Fragestellung gibt es sicherlich auch eine bessere Alternative für Bonferroni.\n\nres_crd_cld &lt;- res_crd %&gt;% \n  cld(adjust = \"bonferroni\", Letters = letters) %&gt;% \n  tidy() %&gt;% \n  select(trt, estimate, conf.low, conf.high, .group) %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nNachdem wir noch ein wenig gerundet haben und die Spalten passend gewählt, erhalten wir dann folgende Ausgabe.\n\nres_crd_cld \n\n# A tibble: 4 × 5\n  trt   estimate conf.low conf.high .group\n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n1 A         10.8     7.17      14.5 \" a  \"\n2 B         12.1     8.42      15.7 \" ab \"\n3 C         16.6    13.0       20.3 \"  bc\"\n4 D         20.3    16.6       24.0 \"   c\"\n\n\nWir nutzen die Ausgabe res_crd_cld direkt in der Abbildung 51.5 um uns das compact letter display zusammen mit den Daten und den entsprechenden 95% konfidenzintervallen anzeigen zu lassen. Der Code ist etwas länger, da wir hier verschiedene Schichten von einem geom übereinander legen müssen.\n\nggplot() +\n  theme_bw() +\n  geom_point(data = crd_tbl, aes(x = trt, y = rsp, fill = trt)) +\n  geom_text(data = res_crd_cld, \n            aes(x = trt , y = estimate, label = .group),\n            position = position_nudge(x = 0.2), color = \"red\") +\n  geom_errorbar(data = res_crd_cld,\n                aes(ymin = conf.low, ymax = conf.high, x = trt),\n                color = \"red\", width = 0.1,\n                position = position_nudge(x = 0.1)) +\n  geom_point(data = res_crd_cld, \n             aes(x = trt , y = estimate),\n             position = position_nudge(x = 0.1), color = \"red\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"Behandlung\", y = \"Gewicht [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die Rohdaten dar.\n       Rote Punkte und Fehlerbalken stellen bereinigte Mittelwerte mit 95% Konfidenzgrenzen pro Behandlung dar.\n       Mittelwerte, mit einem gemeinsamen Buchstaben, sind nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 51.5— Scatterplot der Behandlungsgruppen zusammen mit den 95% Konfidenzintervall und dem compact letter display.\n\n\n\nWi sehen an dem compact letter display, dass sich die Behandlung \\(A\\) von der Behandlung \\(B\\), \\(C\\) und \\(D\\) unterscheidet. Die Behandlung \\(B\\) und \\(C\\) sind gleich. Die Behandlung \\(C\\) unterschdeit sich von all den anderen Behandlungen. Wir erinnern uns, wenn die Buchstaben in dem compact letter display gleich sind, dann können wie die Nullhypothese für diese Vergleiche nicht ablehnen. Wir haben keinen signifikanten Unterschied vorliegen.\nNun ist es so, dass wir meistens noch die \\(p\\)-Werte für die paarweisen Vergleich sowie die 95% Konfidenzintervalle darstellen wollen. Wir nutzen dafür die Funktion contrast() aus dem Paket emmeans. Danach müssen wir noch Spalten auswählen und die \\(p\\)-Werte über die Funktion pvalue() aus dem Paket scales schöner formatieren. Wir erhalten dann das Objekt res_crd_tbl.\n\nres_crd_tbl &lt;- res_crd %&gt;% \n  contrast(method = \"pairwise\") %&gt;% \n  tidy(conf.int = TRUE) %&gt;% \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) %&gt;% \n  select(contrast, estimate, p.value,\n         conf.low, conf.high) \n\nIn dem Objekt res_crd_tbl finden wir dann die \\(p\\)-Werte für alle paarweisen Vergleiche sowie die 95% Konfidenzintevalle.\n\nres_crd_tbl\n\n# A tibble: 6 × 5\n  contrast estimate p.value conf.low conf.high\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 A - B       -1.25 0.903      -6.51      4.01\n2 A - C       -5.8  0.028     -11.1      -0.54\n3 A - D       -9.47 &lt;0.001    -14.7      -4.21\n4 B - C       -4.55 0.103      -9.81      0.71\n5 B - D       -8.22 0.002     -13.5      -2.96\n6 C - D       -3.67 0.231      -8.93      1.59\n\n\nHier sehen wir dann die \\(p\\)-Werte für alle paarweisen Vergleiche und können dann die Entscheidung gegen die Nullhypothese für jeden der Kontraste einmal durchführen. Wir sehen, dass wir für alle Vergleiche die Nullhypothese ablehnen können, bis auf den Vergleich zwischen der Behandlung \\(B\\) und der Behandlung \\(C\\).\nIn der Abbildung 51.6 sehen wir die 95% Konfidenzintervalle für alle Vergleiche einmal dargestellt. Da wir es hier mit einem Mittelwertsvergleich zu tun haben, ist die Entscheidungsregel gegen die Nullhyppthese, dass wir ein signifikantes Konfidenzintervall vorliegen haben, wenn die Null nicht im Konfidenzintervall enthalten ist.\n\nggplot(res_crd_tbl, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1) + \n  geom_point() +\n  coord_flip() +\n  theme_bw()  +\n  labs(x = \"Vergleich\", y = \"Mittelwertsunterschied des Gewichtes [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die bereinigten Mittelwertsunterschiede mit 95% Konfidenzgrenzen dar.\n       Enthält ein 95% Konfidenzintervalle die 0 ist es nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 51.6— Abbildung der 95% Konfidenzintervallefür alle paarweisen Vergleiche der Behandlungsgruppen."
  },
  {
    "objectID": "experimental-design-basic.html#sec-rcbd",
    "href": "experimental-design-basic.html#sec-rcbd",
    "title": "51  Grundlagen der Versuchsplanung",
    "section": "\n51.3 Randomized complete block design (RCBD)",
    "text": "51.3 Randomized complete block design (RCBD)\nDas randomisierte, vollständige Blockdesign (eng. randomized complete block design) ist das Design, wenn es darum geht für verschiedene Räume die Varianz zu adjustieren bzw. zu modellieren. Was meinen wir mit Räumen? Wir meinen damit verschiedene Ställe, verschiedene Felder oder aber verschiedene Tische. Wir nennen diese zusätzlichen Beobachtungsräume auch Block.\n\n\n\n\n\n(a) Visualisierung des Randomized complete block design mit einer Behandlung und vier Behandlungsleveln. In jedem Block findet sich nur ein Behandlungslevel randomisiert wieder.\n\n\n\n\n\n(b) Visualisierung des Randomized complete block design mit einer Behandlung und vier Behandlungsleveln. In jedem Block finden wir mehrfach die Level der Behandlung. Im Prinzip ein Complete randomized design in mehreren Wiederholungen.\n\n\n\nAbbildung 51.7— Visualisierung der zwei Möglichkeiten ein Randomized complete block design zu konstruieren.\n\n\nWichtig ist zu unterschieden, wir pro Block nur einmal ein Level der Behandlung vorliegen haben. Dann hätten wir nämlich nur einen Topf mit Behandlung pro Block wie in Abbildung 51.7 (a) dargestellt. Damit haben wir den Block als Wiederholung. Oder wir haben ein Complete randomized design in Blöcken wiederholen vorliegen. Dann haben wir nämlich pro Block mehrere Wiederholungen der Behandlung wie in Abbildung 51.7 (b) veranschaulicht. Wir schauen uns erstmal den ersten Fall an. Das heist im Prinzip, dass unser Block die Wiederholung ist.\nHier ein paar Beispiele in Prosa, wie so ein Randomized complete block design konstruiert sein könnte.\nWir haben drei Tische und auf jeden der Tische steht zufällig vier ein Töpfe mit je einer Behandlung\nWir haben drei Ställe und in jedem Stall werden vier Buchten mit jeweils einer Behandlung genutzt.\nWir haben drei Felder mit jeweils vier Parzellen die zufällig mit jeweils einer der Behandlungen versehen werden.\nWir können natürlich auch auf den Tischen mehrere Wiederholungen einer Behandlung haben. Dann wird der Datensatz nur größer, aber die Auswertung unterschiedet sich nicht. Wir haben dann mehr Beobachtungen pro Block und Behandlung.\n\n51.3.1 Visualisierung\nIn der Abbildung 51.8 sehen wir eine Realisierung des Randomized complete block design. Wir haben insgesamt drei Blöcke vorliegen mit Block I, Block II und Block III. In jedem Block haben wir die Behandlungen \\(A\\), \\(B\\), \\(C\\) und \\(D\\) zufällig randomisiert. In jedem Block haben wir genau einmal ein Level der Behandlung vorliegen.\n\n\nAbbildung 51.8— Visualisierung des complete randomized design mit einer Behandlung und vier Behandlungsleveln.\n\n\n51.3.2 Daten\nIm Folgenden generieren wir uns die Daten für das Randomized complete block design. Wir wissen, dass in jedem Block die Behandlung genau einmal vorkommt. Um diese Datenstruktur mit zwei Faktoren nachzubauen, können wir die Funktion expand_grid() nutzen. Wir definieren zuerst, dass wir vier Behandlungslevel wollen und für jedes Behandlungslevel dann die drei Level des Blocks. Hier muss ich auch immer wieder rumspielen und probieren, bis ich die Daten dann zu dem Design passend habe. Wir erstellen uns so das Objekt factor_tbl.\n\nset.seed(20221001)\nfactor_tbl &lt;- expand_grid(trt = 1:4, block = 1:3) %&gt;% \n  mutate(trt = factor(trt, labels = c(\"A\", \"B\", \"C\", \"D\")),\n         block = factor(block, labels = as.roman(1:3))) \n\nfactor_tbl\n\n# A tibble: 12 × 2\n   trt   block\n   &lt;fct&gt; &lt;fct&gt;\n 1 A     I    \n 2 A     II   \n 3 A     III  \n 4 B     I    \n 5 B     II   \n 6 B     III  \n 7 C     I    \n 8 C     II   \n 9 C     III  \n10 D     I    \n11 D     II   \n12 D     III  \n\n\nWir sehen, dass jede Behandlung in allen drei Level des Blocks hat. Das entspricht unser Abbildung 51.8 und somit können wir uns darum kümmern, den Leveln der Behandlung und des Blocks einen Effekt zuzuweisen. Dafür brauchen wir die Modellmatrix, die beschreibt, wie sich für jede Beobachtung die Effekte zum Outcome rsp aufsummieren. Nicht jede Beobachtung ist in jedem Block in jeder Behandlung vertreten. Genau genommen hat jede Beobachtung nur eine einzige Behandlung/Block-Kombintation. Wir sehen diese Kombination dann in der Modellmatrix.\n\nmodel_mat &lt;- factor_tbl %&gt;% \n  model_matrix(~ trt + block) %&gt;% \n  as.matrix()\n\nmodel_mat\n\n      (Intercept) trtB trtC trtD blockII blockIII\n [1,]           1    0    0    0       0        0\n [2,]           1    0    0    0       1        0\n [3,]           1    0    0    0       0        1\n [4,]           1    1    0    0       0        0\n [5,]           1    1    0    0       1        0\n [6,]           1    1    0    0       0        1\n [7,]           1    0    1    0       0        0\n [8,]           1    0    1    0       1        0\n [9,]           1    0    1    0       0        1\n[10,]           1    0    0    1       0        0\n[11,]           1    0    0    1       1        0\n[12,]           1    0    0    1       0        1\n\n\nWir sehen in der Modellmarix in jeder Zeile eine zukünftige Beobachtung. In den Spalten wird angegeben zu welchen Faktorleveln die Beobachtung gehört. Dabei bedeutet eine 1 ein Ja und eine 0 ein Nein. Die Beobachtung in der Zeile 5 wird zu Behandlungslevel \\(B\\) und Block \\(II\\) gehören.\nWir legen jetzt folgende Effekte für die einzelnen Behandlungslevel fest. Für den Intercept und damit auch für die Behandlung \\(A\\) auf \\(\\beta_{0} = \\beta_{A} = 20\\). Das Behandlunsglevel \\(B\\) wird auf \\(\\beta_{B} = 15\\), die Behandlung \\(C\\) auf \\(\\beta_{C} = 10\\) sowie die Behandlung \\(D\\) auf \\(\\beta_{D} = 5\\) gesetzt. Um die Sachlage zu vereinfachen setzen wir die Effekte der Blöcke auf \\(\\beta_{0} = \\beta_{I} = 0\\) sowie \\(\\beta_{II} = 0\\) und \\(\\beta_{III} = 0\\). Wir haben also faktisch keinen Effekt der Blöcke. Es ist egal welchen Tisch wir benutzen, die Effekte der Behandlung sind immer die Gleichen. Wenn wir die Daten so bauen würden, dann erhalten wir die Spalte rsp_eff in dem Datensatz rcbd_tbl. Wir haben keine Varianz. Deshalb müssen wir noch die Residuen mit \\(\\epsilon \\sim \\mathcal{N}(0, 2)\\) auf die Werte in der Spalte rsp_eff addieren. Wir erhalten die Spalte rsp für die Auswertung.\n\nrcbd_tbl &lt;- factor_tbl %&gt;% \n  mutate(rsp_eff = as.numeric(model_mat %*% c(20, 15, 10, 5, 0, 0)),\n         rsp = rsp_eff + rnorm(n(), 0, 2))\n\nrcbd_tbl\n\n# A tibble: 12 × 4\n   trt   block rsp_eff   rsp\n   &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 A     I          20  19.8\n 2 A     II         20  17.6\n 3 A     III        20  18.2\n 4 B     I          35  34.4\n 5 B     II         35  33.9\n 6 B     III        35  32.3\n 7 C     I          30  28.4\n 8 C     II         30  29.5\n 9 C     III        30  29.5\n10 D     I          25  23.6\n11 D     II         25  24.9\n12 D     III        25  23.0\n\n\nIn Tabelle 51.1 sehen wir nochmal den Zusammenhang zwischen den generierten Daten und den entsprechenden berechneten Mittelwerten je Behandlungsgruppe. Wir berechnen den Mittelwert auf der Spalte rsp_eff. Wir sehen, dass wir die voreingestellten Mittelwerte in den Daten widerfinden.\n\n\n\n\nTabelle 51.1— Vergleich der Mittlwerte aus den Daten und den voreingestellten Effekten für die Generierung der Daten.\n\nFactor trt\nMean of level\nDifference to level A\nBeta\n\n\n\nA\n20\n0\n20\n\n\nB\n35\n15\n15\n\n\nC\n30\n10\n10\n\n\nD\n25\n5\n5\n\n\n\n\n\n\nAbschließend wollen wir uns die generierten Daten nochmal als einen Dotplot anschauen. Wir wollen dafür einen Dotplot nutzen, da wir mit drei Beobachtungen pro Level der Behandlung keinen sinnvollen Boxplot zeichnen können.\n\nggplot(rcbd_tbl, aes(trt, rsp, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", \n               position = position_dodge(width = 0.4)) +\n  ylim(15, 40) +\n  scale_fill_okabeito() +\n  labs(fill = \"Block\", x = \"Behandlung\", y = \"Outcome\")\n\n\n\nAbbildung 51.9— Dotplot der Level der Behandlungen aufgeteilt für die Level des Blocks.\n\n\n\nWir können die Daten aus dem Datensatz rcbd_tbl jetzt für die Varianzanalyse und Mittelwertsvergleich nutzen.\n\n51.3.3 Modellierung\nIm Folgenden wollen wir die Daten modellieren. Das heist wir wollen eine Linie durch eine multidimensionale Punktewolke zeichnen. Daher auch lineares Modell oder eben durch die Funktion lm() in R für linear model. Wir nutzen das Paket parameters und die Funktion model_parameters() um uns die Parameter des Modells auszugeben. Wir könnten auch die Funktion tidy() nutzen, aber wir erhalten durch die Funktion model_parameters() etwas mehr Informationen und bessere Spaltenüberschriften.\nWir bauen das Modell in folgender Form. Wir haben ein numerisches Outcome \\(y\\) sowie einen Faktor \\(f_1\\) sowie einem Faktor für den Block \\(b_1\\).\n\\[\ny \\sim f_1 + b_1\n\\]\nNun können wir das abstrakte Modell in die Daten übersetzen und erhalten folgendes Modell.\n\\[\nrsp \\sim trt + block\n\\]\nDas heist, unsere numerische Variable rsp hängt ab von unserer faktoriellen Variable trt und der faktoriellen Blockvariable block. Wir müssen immer wissen, wie die Spaltennamen in unserem Datensatz crd_tbl lauten sonst kann R die Spalten nicht finden.\n\nfit_rcbd &lt;- lm(rsp ~ trt + block, rcbd_tbl)\n\nfit_rcbd %&gt;%  model_parameters()\n\nParameter   | Coefficient |   SE |         95% CI |  t(6) |      p\n------------------------------------------------------------------\n(Intercept) |       18.79 | 0.70 | [17.08, 20.51] | 26.75 | &lt; .001\ntrt [B]     |       15.03 | 0.81 | [13.04, 17.01] | 18.53 | &lt; .001\ntrt [C]     |       10.57 | 0.81 | [ 8.59, 12.56] | 13.03 | &lt; .001\ntrt [D]     |        5.29 | 0.81 | [ 3.30,  7.27] |  6.52 | &lt; .001\nblock [II]  |       -0.05 | 0.70 | [-1.77,  1.67] | -0.08 | 0.942 \nblock [III] |       -0.77 | 0.70 | [-2.49,  0.94] | -1.10 | 0.313 \n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nWir sehen, dass wir die Koeffizienten, die wir vorher eingestellt haben, auch hier wiederfinden. Alle Steigungen der Behandlungslevel sind signifikant. Das hilft uns aber noch nicht so richtig weiter. Wir werden gleich das Modell in einer zweifaktoriellen ANOVA und einem Mittelwertsvergleich anschauen. Vorher wollen wir einmal statistisch Testen, ob die Varianzen homogens sind. Wir können die Varianzen aber nicht über das volle Modell testen, da wir nur eine Beobachtung per Behandlung/Block-Kombintation vorliegen haben.\n\nfit_rcbd %&gt;% check_homogeneity()\n\nError in bartlett.test.default(x = mf[[1L]], g = mf[[2L]]) :  there must be at least 2 observations in each group\nDaher schauen wir uns nur die Varianzen für die Behandlung an und nehmen an, dass die Varanzen über die Blöcke homogen sind. Wir können nur einen Faktor testen und deshalb nehmen wir den für uns wichtigeren Faktor die Behandlung.\n\nlm(rsp ~ trt, rcbd_tbl) %&gt;% check_homogeneity()\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.907).\n\n\nAbschließend schauen wir nochmal auf die Normalverteilung der Residuen.\n\nfit_rcbd %&gt;% check_normality()\n\nOK: residuals appear as normally distributed (p = 0.391).\n\n\nIn der Abbildung 51.10 sehen wir den QQ-Plot und die Verteilung der Residuen im Densityplot. Auch die Visualisierung zeigt keine Aufälligkeiten. Wir sehen, dass die Residuen einer Normalverteilung folgen.\n\ncheck_model(fit_rcbd, check = c(\"qq\", \"normality\"))\n\n\n\nAbbildung 51.10— QQ-Plot und Densityplot der Residuen aus dem lineare Modell.\n\n\n\nWir können jetzt eine Varianzanalyse und dann eine Mittelwertsvergleich durchführen. Achtung, wir können uns hier auch etwas in die Ecke testen. Wenn wir nur lange genug neue Daten generieren, werden wir irgendwann auch einen Datensatz finden, der die Varianzhomogenität und die Normalverteilung ablehnt. Besonders in dem Fall, dass wir wenige Blöcke haben. Das liegt in der Theorie des statistischen Testens sowie der kleinen Fallzahl verborgen. Deshalb können wir im Zweifel gerne einmal deine Vortests in dem R Tutorium oder in einer statistischen Beratung diskutieren.\n\n51.3.4 Varianzanalyse und Mittelwertsvergleich\nAls erstes Rechnen wir eine zweifaktroielle ANOVA, da unser Modell zwei Faktoren hat. In R müssen wir dazu nur das Modell fit_rcbd in die Funktion anova() pipen. Wir erhalten dann die Ergebnisse aus der ANOVA mit der Funktion model_parameters() aus dem Paket parameters besser aufgearbeitet wieder. Die Mittelwertsunterschiede der Level der Behandlung haben wir bewusst sehr hoch angesetzt, so dass wir auf jeden Fall eine signifikante ANOVA erhalten sollen.\n\nres_anova &lt;- fit_rcbd %&gt;% \n  anova() \n\nres_anova %&gt;% model_parameters()\n\nParameter | Sum_Squares | df | Mean_Square |      F |      p\n------------------------------------------------------------\ntrt       |      381.24 |  3 |      127.08 | 128.72 | &lt; .001\nblock     |        1.50 |  2 |        0.75 |   0.76 | 0.509 \nResiduals |        5.92 |  6 |        0.99 |        |       \n\nAnova Table (Type 1 tests)\n\n\nAls Ergebnis haben wir einen signifikanten Faktor Behandlung trt sowie einen nicht signifikanten Faktor Block block. Wir können die Signifkanz an dem \\(p\\)-Wert bestimmen. Liegt der \\(p\\)-Wert unter dem Signifikanzniveau von \\(\\alpha\\) gleich 5% so können wir die Nullhypothese ablehnen. Wir haben dann mindestens einen signifikanten paarweisen Mittelwertsunterschied vorliegen.\nSchauen wir uns nun noch den Anteil der erklärten Varianz an. Wir nutzen dafür den Effektschätzer \\(\\eta^2\\).\n\nres_anova %&gt;% eta_squared(partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter |     Eta2 |       95% CI\n-----------------------------------\ntrt       |     0.98 | [0.93, 1.00]\nblock     | 3.85e-03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWir sehen, dass durch den Faktor trt mit 92% der Varianz erklärt werden. Der Faktor Block erklärt nur ca. 2% der Varianz. Beides war so zu erwarten, denn wir haben ja auch den Datensatz in dieser Form gebaut. Die Behandlung hat einen starken Effekt und der Block hat gar keinen Effekt.\nSchauen wir nun auf den Mittelwertsvergleich. Wir nutzen dafür die Funktion emmeans() aus dem R Paket emmeans. Wichtig ist hier, dass wir uns jetzt die Vergleiche der Gruppen bzw. Level der Behandlung anschauen wollen.\n\nres_rcbd &lt;- fit_rcbd %&gt;% \n  emmeans(~ trt) \n\nAls erstes nutzen wir die Ausagbe der Funktion emmeans um uns das compact letter display wiedergeben zu lassen. Wir wollen wieder die Ausgaben runden und nutzen die Adjustierung der \\(p\\)-Werte für multiple Vergleiche nach Bonferroni. Nochmal als Erinnerung, das compact letter display gibt uns keine \\(p\\)-Werte wieder sondern wir Entscheiden anhand der vergebenen Buchstaben und deren Gleichheit über ein signifikantes Ergebnis oder ein nicht signifikantes Ergebnis.\n\nres_rcbd_cld &lt;- res_rcbd %&gt;% \n  cld(adjust = \"bonferroni\", Letters = letters) %&gt;% \n  tidy() %&gt;% \n  select(trt, estimate, conf.low, conf.high, .group) %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\nres_rcbd_cld \n\n# A tibble: 4 × 5\n  trt   estimate conf.low conf.high .group \n  &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 A         18.5     16.5      20.5 \" a   \"\n2 D         23.8     21.8      25.8 \"  b  \"\n3 C         29.1     27.1      31.1 \"   c \"\n4 B         33.6     31.5      35.6 \"    d\"\n\n\nAn dem compact letter display sehen wir, dass sich alle Mittelwerte der Level der Behandlungen signifikant unterscheiden. In Abbildung 51.11 sehen wir die Daten zusammen mit dem compact letter display in einer Abbildung. Wir ändern hier das geom_point() zu geom_jitter() um ein Overplotting zu vermeiden. So können wir alle Beobachtungen als Punkte erkennen.\n\nggplot() +\n  theme_bw() +\n  geom_jitter(data = rcbd_tbl, aes(x = trt, y = rsp, fill = trt),\n              width = 0.05) +\n  geom_text(data = res_rcbd_cld, \n            aes(x = trt , y = estimate, label = .group),\n            position = position_nudge(x = 0.2), color = \"red\") +\n  geom_errorbar(data = res_rcbd_cld,\n                aes(ymin = conf.low, ymax = conf.high, x = trt),\n                color = \"red\", width = 0.1,\n                position = position_nudge(x = 0.1)) +\n  geom_point(data = res_rcbd_cld, \n             aes(x = trt , y = estimate),\n             position = position_nudge(x = 0.1), color = \"red\") +\n  theme(legend.position = \"none\") +\n  labs(x = \"Behandlung\", y = \"Gewicht [kg/ha]\",\n       caption = \"Schwarze Punkte stellen Rohdaten dar.\n       Rote Punkte und Fehlerbalken stellen bereinigte Mittelwerte mit 95% Konfidenzgrenzen pro Behandlung dar.\n       Mittelwerte, mit einem gemeinsamen Buchstaben, sind nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 51.11— Scatterplot der Behandlungsgruppen zusammen mit den 95% Konfidenzintervall und dem compact letter display.\n\n\n\nHäufig wollen wir nicht nur das compact letter display sehen sondern auch die dazugehörigen \\(p\\)-Werte und die entsprechenden 95% Konfidenzintervalle. Wir berechnen im Folgenden alle paarweisen Vergleiche bzw. Kontraste und lassen uns die adjustierten sowie formatierten \\(p\\)-Werte ausgeben. Wir runden wieder die Ausgabe.\n\nres_rcbd_tbl &lt;- res_rcbd %&gt;% \n  contrast(method = \"pairwise\") %&gt;% \n  tidy(conf.int = TRUE) %&gt;% \n  mutate(p.value = pvalue(adj.p.value),\n         across(where(is.numeric), round, 2)) %&gt;% \n  select(contrast, estimate, p.value,\n         conf.low, conf.high) \n\nres_rcbd_tbl\n\n# A tibble: 6 × 5\n  contrast estimate p.value conf.low conf.high\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 A - B      -15.0  &lt;0.001    -17.8     -12.2 \n2 A - C      -10.6  &lt;0.001    -13.4      -7.76\n3 A - D       -5.29 0.003      -8.1      -2.48\n4 B - C        4.46 0.006       1.65      7.26\n5 B - D        9.74 &lt;0.001      6.93     12.6 \n6 C - D        5.29 0.003       2.48      8.09\n\n\nAuch hier passen die \\(p\\)-Werte zu dem compact letter display. Alle Vergleiche sind signifikant. Das haben wir noch dem compact letter display auch so erwartet. Auch sehen wir das gleiche Ergebnis in Abbildung 51.12 für die 95% Konfidenzintervalle. Wir betrachten Mittelwertsunterschiede und kein Konfidenzintervall beinhaltet die Null somit sind alle Konfidenzintervalle signifikant.\n\nggplot(res_rcbd_tbl, aes(contrast, y=estimate, ymin=conf.low, ymax=conf.high)) +\n  geom_hline(yintercept=0, linetype=\"11\", colour=\"grey60\") +\n  geom_errorbar(width=0.1) + \n  geom_point() +\n  coord_flip() +\n  theme_bw()  +\n  labs(x = \"Vergleich\", y = \"Mittelwertsunterschied des Gewichtes [kg/ha]\",\n       caption = \"Schwarze Punkte stellen die bereinigten Mittelwertsunterschiede mit 95% Konfidenzgrenzen dar.\n       Enthält ein 95% Konfidenzintervalle die 0 ist es nicht signifikant unterschiedlich.\")\n\n\n\nAbbildung 51.12— Abbildung der 95% Konfidenzintervallefür alle paarweisen Vergleiche der Behandlungsgruppen."
  },
  {
    "objectID": "experimental-design-basic.html#sec-lsd",
    "href": "experimental-design-basic.html#sec-lsd",
    "title": "51  Grundlagen der Versuchsplanung",
    "section": "\n51.4 Latin square design (LSD)",
    "text": "51.4 Latin square design (LSD)\n\n51.4.1 Visualisierung\n\n\nAbbildung 51.13— Visualisierung des latin square design mit einer Behandlung und vier Behandlungsleveln.\n\n\n51.4.2 Daten\n\nexpand_grid(trt = 1:4, block = 1:4)\n\n# A tibble: 16 × 2\n     trt block\n   &lt;int&gt; &lt;int&gt;\n 1     1     1\n 2     1     2\n 3     1     3\n 4     1     4\n 5     2     1\n 6     2     2\n 7     2     3\n 8     2     4\n 9     3     1\n10     3     2\n11     3     3\n12     3     4\n13     4     1\n14     4     2\n15     4     3\n16     4     4\n\n\n\n51.4.3 Modellierung\n\n51.4.4 Varianzanalyse und Mittelwertsvergleich"
  },
  {
    "objectID": "experimental-design-advanced.html#genutzte-r-pakete-für-das-kapitel",
    "href": "experimental-design-advanced.html#genutzte-r-pakete-für-das-kapitel",
    "title": "52  Fortgeschrittene Versuchsplanung",
    "section": "\n52.1 Genutzte R Pakete für das Kapitel",
    "text": "52.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, conflicted, broom,\n               see, emmeans, multcomp, scales, performance,\n               effectsize, parameters)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"set_names\", \"magrittr\")\ncbbPalette &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "experimental-design-advanced.html#sec-alpha",
    "href": "experimental-design-advanced.html#sec-alpha",
    "title": "52  Fortgeschrittene Versuchsplanung",
    "section": "\n52.2 Alpha design",
    "text": "52.2 Alpha design\n\n52.2.1 Visualisierung\n\n\n\nAbbildung 52.1— Visualisierung des alpha design mit einer Behandlung und vier Behandlungsleveln und zwölf unvollständigen Blöcken sowie vier Wiederholungen.\n\n\n\n52.2.2 Daten\n\n52.2.3 Modellierung\n\n52.2.4 Varianzanalyse und Mittelwertsvergleich"
  },
  {
    "objectID": "experimental-design-advanced.html#sec-augment",
    "href": "experimental-design-advanced.html#sec-augment",
    "title": "52  Fortgeschrittene Versuchsplanung",
    "section": "\n52.5 Augmented design",
    "text": "52.5 Augmented design"
  },
  {
    "objectID": "experimental-design-advanced.html#sec-split",
    "href": "experimental-design-advanced.html#sec-split",
    "title": "52  Fortgeschrittene Versuchsplanung",
    "section": "\n52.4 Split plot design",
    "text": "52.4 Split plot design\n\n52.4.1 Visualisierung\n\n\n\nAbbildung 52.2— Visualisierung des split plot design mit einer Behandlung und vier Behandlungsleveln sowie einer zweiten Behandlung mit fünf Behandlungsleveln. Die erste Behandlung ist über die drei Blöcke randomisiert.\n\n\n\n52.4.2 Daten\n\ndata_tbl &lt;- expand_grid(trt = 1:4, block = 1:4, rep = 1:5) %&gt;% \n    mutate(rsp = 20 + 2.5 * trt + 1.5 * block + rnorm(n(), 0, 1),\n           trt = factor(trt, labels = c(\"ctrl\", \"A\", \"B\", \"C\")),\n           block = factor(block, labels = as.roman(1:4)),\n           rep = as_factor(rep))\n\n\n52.4.3 Modellierung\n\n52.4.4 Varianzanalyse und Mittelwertsvergleich"
  },
  {
    "objectID": "experimental-design-samplesize.html#theoretischer-hintergrund",
    "href": "experimental-design-samplesize.html#theoretischer-hintergrund",
    "title": "\n53  Fallzahlplanung\n",
    "section": "\n53.1 Theoretischer Hintergrund",
    "text": "53.1 Theoretischer Hintergrund\nManchmal hat man das Gefühl, dass Fallzahlplanung nur ein wildes Gerate ist. Das ist aus der Perspektive eines biologischen Fachlaien auch der Fall. Ich kenne mich sehr wenig in der vielen biologischen Feldern aus. Daher weiß ich wenig darüber was ein großer Effekt ist oder welchen Effekt du überhaupt in deinem Kartoffelexperiment erwarten kannst. Auch ist mir unklar was typische Mittelwertsunterschiede bei Wasserlinsen sind. Du musst sowas aber wissen, es ist ja schließlich dein Experiment. Wenn du also eine Fallzahlplanung durchführen willst, dann heißt es zuerst einmal Literatur wälzen oder mit den Fachkollegen sprechen.\nWir kennen ja schon die Formel für den t-Test. Der t-Test vergleicht die Mittelwerte von zwei normalverteilten Outcomes und gewichtet diesen Mittelwertsunterschied bei der Standardabweichung. Da wir in der Formel des t-Tests auch die Fallzahl inkludiert haben, können wir die Formel nach der Fallzahl umstellen.\n\\[\nT = \\cfrac{\\Delta}{s_p \\cdot \\sqrt{\\cfrac{2}{n_g}}}\n\\]\nDabei nutzen wir die Teststatistik etwas anders. Wir zerlegen die Teststatistik \\(T\\) für in den Wert für den \\(\\alpha\\)-Fehler und den \\(\\beta\\)-Fehler. Damit können wir auch die Power \\(1-\\beta\\) mit in unserer Formel berücksichtigen.\n\\[\nn_g = \\cfrac{2\\cdot(T_{\\alpha = 5\\%} + T_{\\beta = 20\\%})^2}{\\left(\\cfrac{\\Delta}{s_p}\\right)^2}\n\\]\nDabei nutzen wir für \\(T_{\\alpha = 5\\%} = 1.96\\) und \\(T_{\\beta = 20\\%} = 0.84\\) und vereinfachen damit die Formel ziemlich. Eigentlich nutzen wir diese Formel dann in der der Klausur oder aber um wirklich mal eben schnell zu schauen, was wir für eine Fallzahl erwarten.\nJetzt könntest du meinen, dass wir jetzt mit verschiedenen Powerleveln spielen könnten. Aber das ist leider nicht der Fall. Wir sind eigentlich zimelich auf 80% festgelegt. Da gibt es im Rahmen eines Antrags keinen Spielraum. Wir nehmen immer eine Power von 80% an.\n\n\n\n\n\n\nEinseitig oder zweiseitig im Spiegel der Regulierungsbehörden\n\n\n\nIn den allgemeinen Methoden des IQWiG, einer Regulierungsbehörde für klinische Studien, wird grundsätzlich das zweiseitige Testen empfohlen. Wenn einseitig getestet werden sollte, so soll das \\(\\alpha\\)-Niveau halbiert werden. Was wiederum das gleiche wäre wie zweiseitiges Testen - nur mit mehr Arbeit.\nZur besseren Vergleichbarkeit mit 2-seitigen statistischen Verfahren wird in einigen Guidelines für klinische Studien eine Halbierung des üblichen Signifikanzniveaus von 5 % auf 2,5 % gefordert. – Allgemeine Methoden Version 6.1 vom 24.01.2022, p. 180\nFazit des Dokumentes ist dann aber, dass wir immmer zu einem Signifikanzniveau \\(\\alpha\\) von 5% und einer Power von 80% testen."
  },
  {
    "objectID": "experimental-design-samplesize.html#tierversuchsantrag",
    "href": "experimental-design-samplesize.html#tierversuchsantrag",
    "title": "\n53  Fallzahlplanung\n",
    "section": "\n53.2 Tierversuchsantrag",
    "text": "53.2 Tierversuchsantrag\nWenn du einen Tierversuch durchführen willst, dann bist du natürlich hier falsch. Ich kann dir bei dem Ausfüllen von Dokumenten nicht helfen. Was ich aber kann, ist dir einen Überblick über die Inhalte zu geben, so dass du dann nochmal informiert an anderer Stelle Fragen stellen kanst. Schaue gerne einmal mein Video auf YouTube mit dem Kontext zum Tierversuchsvorhaben. Eine wunderbare Übersicht über den Tierversuchsantrag liefert auch Piper u. a. (2022).\n\n\n\n\n\n\nEinführung in den Kontext zu Tierversuchsvorhaben per Video\n\n\n\nDu findest auf YouTube Kontext zu Tierversuchsvorhaben als Video Reihe. Es handelt sich hierbei um ein reines Lehrvideo mit keinem beratenden Anspruch.\n\n\nIn dem Video habe ich dann alles anhand des Tierversuchsvorhaben am LaGeSo in Berlin besprochen. Das hatte den Grund, dass ich zur Zeit des Videos an der Charité beschäftigt war. Da bei einem Tierversuchsantrag jeweils die Bundesländer zuständig sind, musst du bei deiner jeweiligen Ladesbehörde einmal schauen. In Niedersachsen musst du dir die Wenseite zu Tierversuche vom Laves anschauen. Hier findest du dann andere Dokumente und Ausfüllhilfen. Wenn man als Wissenschaftler viel wechselt, wird man leicht wirr."
  },
  {
    "objectID": "experimental-design-samplesize.html#ethikantrag",
    "href": "experimental-design-samplesize.html#ethikantrag",
    "title": "\n53  Fallzahlplanung\n",
    "section": "\n53.3 Ethikantrag",
    "text": "53.3 Ethikantrag\nEben hatten wir uns kurz den Antrag für ein Tierversuchsvorhaben angeschaut. Richtig kompliziert wird es, wenn wir nicht mit Tieren arebiten sondern Versuche am Menschen durchführen. Ein versuch am Menschen beinhaltet schon das Ausfüllen eines Fragebogens! Daher kanns du auch schnell in die Situtaton kommen, dass es eventuell eine ethische Komplikation gibt. Ich habe die Inhalte im Kontext einer klinischen Studie einmal in einem YouTube Video dargestellt und allgemein eingeordnet.\n\n\n\n\n\n\nEinführung in den Kontext zum Ethikantrag per Video\n\n\n\nDu findest auf YouTube Kontext zum Ethikantrag als Video Reihe. Es handelt sich hierbei um ein reines Lehrvideo mit keinem beratenden Anspruch.\n\n\nDa ich in meiner Lehre die klinischen Studie nur am Horizont sehe, gibt es hir auch keine weiteren Links zu dem Thema. In dem Video siehst du noch ein paar öffentliche Quellen. Da es sich aber bei einem Ethikantrag meist um einen internen Prozess einer Universitätsklinik handelt, sind die (aktuellen) Dokumente meist nicht öffentlich zugänglich. Im Zweifel bitte an die zuständigen Gremien an deiner Institution wenden."
  },
  {
    "objectID": "experimental-design-samplesize.html#genutzte-r-pakete-für-das-kapitel",
    "href": "experimental-design-samplesize.html#genutzte-r-pakete-für-das-kapitel",
    "title": "\n53  Fallzahlplanung\n",
    "section": "\n53.4 Genutzte R Pakete für das Kapitel",
    "text": "53.4 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, pwr, readxl, see,\n               effectsize, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "experimental-design-samplesize.html#mittelwertsvergleich-für-zwei-gruppen-in-r",
    "href": "experimental-design-samplesize.html#mittelwertsvergleich-für-zwei-gruppen-in-r",
    "title": "\n53  Fallzahlplanung\n",
    "section": "\n53.5 Mittelwertsvergleich für zwei Gruppen in R",
    "text": "53.5 Mittelwertsvergleich für zwei Gruppen in R\nDa wir ja nur die Formel des t-Tests für die Fallzahlberechnung haben, können wir auch immer nur die Fallzahl für den Vergleich zwischen zwei Gruppen rechnen. Das ist immer erstmal wieder ungewohnt. Aber wir machen das jetzt erstmal nur für zwei Gruppen. Später schauen wir uns an, ws passiert, wenn wir mehr Gruppen miteinander vergleichen wollen. Prinzipiell ist der Kern aber immer ein Zweigruppenvergleich, den wir dann etwas anders Aufbauen.\nWenn du für einen Wilcoxon-Test oder einen anderen nicht-parametrischen Test die Fallzahlplanung machen willst, rechne bitte einen t-Test und addiere \\(+15\\%\\) an Fallzahl drauf.\nFür die Berechnung der Fallzahl wollen wir das R paket pwr nutzen. Wir brauchen in diesem Kapitel nur drei Funktion aus dem Paket, aber es gibt auch weit aus mehr. Im Zweifel einfach einmal die Hilfeseite aufrufen und schauen was es dort noch so gibt.\nWir können mit der Funktion pwr.t.test() die Fallzahl für die Effektstärke nach Cohen’s \\(d\\) berechnen. Mehr über Cohen’s \\(d\\) kannst du im Kapitel 21 erfahren. Wir nutzen hier eine relativ harte Abschätzung. Aber hier wird sowieso alles abgeschätzt, da kommt es jetzt auf künstliche Genauigkeit nicht mehr an. Wir berechnen also Cohen’s \\(d\\) vereinfacht für die Fallzahlberechnung wie folgt.\n\\[\nd = \\cfrac{\\Delta}{s_{\\Delta}}\n\\]\nmit\n\n\n\\(\\Delta\\) als den zu erwartenden Mittelwertsunterschied zwischen den beiden Gruppen. Wir haben den Wert aus der Literatur entnommen.\n\n\\(s_{\\Delta}\\) als der Standardabweichung des Mittelwertsunterschieds. Wir können hier als Vereinfachung mit der Spannweite der Daten mit \\(\\frac{range}{4}\\) als Schätzer für die Standardabweichung rechnen. Ebenfalls haben wir die Werte aus einer Literaturquelle.\n\nEs gäbe auch die Möglichkeit über die Funktion cohen.ES() die Effekte für verschiedene statistische Tests sich wiedergeben zu lassen, wenn wir definieren, wie stark der Effekt zwischen den Gruppen sein soll. Es steht zur Auswahl small, medium und large. Wir erkennen, dass ist nicht gerade viel Abstufung.\n\ncohen.ES(test = \"t\", size=\"medium\") %&gt;% \n  pluck(\"effect.size\")\n\n[1] 0.5\n\n\nDie Fallzahlberechnung geht recht einfach. Wir setzen die Option n = auf NULL, so dass uns die Funktion diese Option berechnet. Wir kriegen also die Fallzahl gegeben von dem Signifikanzniveau, der Power und der Effektstärke wieder. Dann geben wir noch an, dass wir zweiseitig testen. Also eigentlich alles fix, da können wir selber zwar was ändern, aber am Ende wird meist nur die Standardwerte von Dritten akzeptiert.\n\nres_ttest &lt;- pwr.t.test(n = NULL,\n                        sig.level = 0.05, \n                        type = \"two.sample\", \n                        alternative = \"two.sided\", \n                        power = 0.80, \n                        d = 0.8)\nres_ttest\n\n\n     Two-sample t test power calculation \n\n              n = 25.52458\n              d = 0.8\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nBitte immer Aufrunden. Wir brauchen also \\(n_1 = n_2 = 26\\) Beobachtungen je Gruppe, so dass wir für \\(32\\) beobachtungen unseren Versuch planen können. In Abbildung 53.1 sehen die Power abhängig von der verwendeten Fallzahl. Wir sehen, dass wir mit mehr Fallzahl eine höhere Power erhalten würden, aber wir schon sehr nah an der Sättigung sind.\n\nplot(res_ttest) +\n  theme_minimal(base_size = 14) +\n  labs(title = 'Optimierte Fallzahl für den Zweistichproben t-Test.')\n\n\n\nAbbildung 53.1— Optimierte Fallzahl für den Zweistichproben t-Test."
  },
  {
    "objectID": "experimental-design-samplesize.html#anteilsvergleich-für-zwei-gruppen-in-r",
    "href": "experimental-design-samplesize.html#anteilsvergleich-für-zwei-gruppen-in-r",
    "title": "\n53  Fallzahlplanung\n",
    "section": "\n53.6 Anteilsvergleich für zwei Gruppen in R",
    "text": "53.6 Anteilsvergleich für zwei Gruppen in R\nWann benötigen wir Anteile? Häufig nutzen wir Anteile, wenn wir zum Beispiel infizierte Ferkel untr zwei Behandlungen untersuchen wollen. Wie viel Prozent der Ferkel in der einen Gruppe werden infiziert sein und wieviel Ferkel in der anderen Gruppe. Daher haben wir ein Medikament und wollen schauen, ob sich die Anzahl an infizierten Ferkeln reduziert. Wir nehmen aber nicht die Anzahl als Wert sondern die relative Angabe. Im folgenden Beispiel haben wir \\(95\\%\\) infizierte Ferkel in der einen Gruppe und \\(80\\%\\) infizierte Ferkel in der anderen Gruppe. Wie viel Fallzahl brauchen wir nun, um diesen Untrschied nachzuweisen. Achtung, wir rechnen hier wirklich mit den relativen Zahlen und nicht mit der Differenz. Ist leider so.\nWir können die Funktion ES.h() benutzen um den Effekt zwischen zwei Wahrscheinlichkeiten zu berechnen. Wir geben einfach die beiden Wahrscheinlichkeiten für die zu erwartende Häufigkeit an infizierten Ferkeln ein. Dann berechnen wir den Effekt \\(h\\) und nutzen diesen Wert dann für die Fallzahlberechnung.\n\nES.h(p1 = 0.95, p2 = 0.80) %&gt;% \n  round(2)\n\n[1] 0.48\n\n\nHier kommt es dann auch nicht wieder auf die letzte Prozentzahl an. Wir immer kann man hhier spielen. Aber du hast ja deine Zahlen aus der Literatur und passt diese Zahlen dann deinem Setting und anhand deinem biolologischen Wissen an. Es ist immer eine Gradwanderung, wie genau die Zahlen nun seien sollen. Insbesondere, wenn es dann doch nicht so viel Literatur gibt. Wir setzen die Option n = auf NULL, so dass uns die Funktion diese Option berechnet. Wir kriegen also die Fallzahl gegeben von dem Signifikanzniveau, der Power und der Effektstärke wieder. Dann geben wir noch an, dass wir zweiseitig testen.\n\nres_prop &lt;- pwr.p.test(h = 0.48,\n                       n = NULL,\n                       sig.level = 0.05,\n                       power = 0.80,\n                       alternative = \"two.sided\")\nres_prop\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.48\n              n = 34.06623\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nAm Ende erhalten wir eine Fallzahl von \\(n_1 = n_2 = 35\\) Beobachtungen aus der Fallzahlberechnung. Wir wissen also, wie viele Ferkel wir untersuchen müssten um einen Unterschied von \\(95\\%\\) zu \\(80\\%\\) signifikant nachweisen zu können. In Abbildung 53.2 sehen wir nochml die Sättigungskurve für die Power für verschiedene Fallzahlen. Mit unserer berechneten Fallzahl von \\(n=35\\) pro Gruppe sind wir schon recht nah an der Sättigung der Funktion. Wir können damit die Fallzahl beibehalten und uns übrlegen, ob wir überhaupt das Geld und die Ressourcen haben um den Versuch mit dieser Anzahl an Ferkeln durchzuführen.\n\nplot(res_prop) +\n  theme_minimal(base_size = 14) +\n  labs(title = 'Optimierte Fallzahl für zwei Anteile.')\n\n\n\nAbbildung 53.2— Optimierte Fallzahl für zwei Anteile."
  },
  {
    "objectID": "experimental-design-samplesize.html#anteil-der-erklärten-varianz-in-r",
    "href": "experimental-design-samplesize.html#anteil-der-erklärten-varianz-in-r",
    "title": "\n53  Fallzahlplanung\n",
    "section": "\n53.7 Anteil der erklärten Varianz in R",
    "text": "53.7 Anteil der erklärten Varianz in R\nNun können wir die Fallzahlplanung auch für eine einfaktorielle ANOVA durchführen. Das ist unsere Basis. Wir würden dann überlegen, wie sich dann die Fallzahl mit weiteren Faktoren ändern würde. Auch hier ein Wort der Warnung. Es gibt häufig so starke Randbedingungen, wie Kosten oder Fläche, dass die Berechnung der Fallzahl absolet wird. Wenn du drei Blöcke hat, dann hast du drei Blöcke. nutze die Blöcke dann auch. Wenn du freeie Wahl hättest und viel, viel Geld, dan kann man sicherlich besser die Fallzahl abschätzen und nutzen. Fallzahlberechnung nur so zum Spaß hat dann ja auch wenig Sinn. Also hier nochmal unser Modell was wir uns mit einer einfaktoriellen ANOVA anschauen.\n\\[\ny \\sim f_1\n\\]\nWir immer brauchen wir auch einen Effekt. In dem Fall der ANOVA ist der Effekt Cohen’s \\(f\\). Wir berechnen Cohen’s \\(f\\) wie folgt aus dem \\(\\eta^2\\). Wir können an dieser Stelle schon die Werte für \\(\\eta^2\\) einsetzen und \\(f\\) berechnen.\n\\[\nf = \\sqrt{\\cfrac{\\eta^2}{1- \\eta^2}}\n\\]\nDu erinnerst dich aus der ANOVA, das \\(\\eta^2\\) beschreibt den Anteil an erklärter Varianz durch den Faktor in der ANOVA. Damit ist \\(\\eta^2\\) wie folgt definiert.\n\\[\n\\eta^2 = \\cfrac{SS_{treat}}{SS_{total}}\n\\]\nDas hilft uns nur so begrenzt weiter. Am besten überlegst du dir, wieviel Varianz wohl die Behandlung erklären kann. Damit hast du dann dein \\(\\eta^2\\). Wenn deine Behandlung vermutlich ca. 70% der Varianz in deinen Daten erklären und somit im Ourtcome erklären kann, dann setzt du \\(\\eta^2 = 0.7\\). Dann berechnest du dein \\(f = \\sqrt{\\tfrac{0.7}{0.3}} = 1.53\\) und hast damit einen sehr großen Effekt. Was dir auch die Funktion interpret_eta_squared() aus dem R Paket effectsize mitteilt.\n\ninterpret_eta_squared(1.53)\n\n[1] \"large\"\n(Rules: field2013)\n\n\nWir können dann Cohen’s \\(f\\) in die Funktion pwr.anova.test() stecken und die Fallzahl pro Gruppe ausrechnen. Wir haben jetzt mal einen Faktor mit drei Behandlunsgleveln angenommen, deshalb ist auch \\(k = 3\\) in der Funktion.\n\nres_anova &lt;- pwr.anova.test(k = 3,\n                            f = 1.5,\n                            n = NULL,\n                            sig.level = 0.05,\n                            power = 0.80)\nres_anova\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 2.713068\n              f = 1.5\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nTja, mit so einem großen Effekt brauchen wir wirklich wenig Wiederholungen um mindestens einen Unterschied nachzuweisen. Stimmt, wir haben natürlich auch nur global über alle Gruppen geschaut. Ich finde die Fallzahlplanung für eine ANOVA relativ eingeschränkt, aber so ist das eben auch bei der Fallzahlplanung. Meistens ist das was möglich ist sehr eingeschränkt."
  },
  {
    "objectID": "experimental-design-samplesize.html#mehr-als-zwei-gruppen",
    "href": "experimental-design-samplesize.html#mehr-als-zwei-gruppen",
    "title": "\n53  Fallzahlplanung\n",
    "section": "\n53.8 Mehr als zwei Gruppen",
    "text": "53.8 Mehr als zwei Gruppen\nWas passiert, wenn wir mehr als zwei Gruppen vorliegen haben? Was eigentlich immer der Fall ist. Also wir haben nicht nur zwei Düngestufen oder zwei Sorten Blumenkohl, die wir miteinander vergleichen wollen, sondern wir haben zehn oer mehr. Wir bauen jetzt nicht so ein großes Beispiel sondern nehmen einmal die Sprungweiten von den Hunde-, Katzen- und Fuchsflöhen.\n\nfleas_tbl &lt;- read_excel(\"data/flea_dog_cat_fox.xlsx\") %&gt;% \n  mutate(animal = as_factor(animal))\n\nIn Abbildung 53.3 sehen wir nochmal die Verteilung der Sprungweiten für die drei Tierarten als Boxplots dargestellt.\n\nggplot(fleas_tbl, aes(animal, jump_length, fill = animal)) + \n  theme_bw() +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  scale_fill_okabeito()\n\n\n\nAbbildung 53.3— Boxplot der Sprungweiten für die Hunde-, Katzen- und Fuchsflöhe.\n\n\n\nWenn wir jetzt für ein neues Experiment die Fallzahl planen wollen würden, dann brauchen wir die Mittelwerte und die Stanardabweichung der Sprungweiten. Wir haben ja hier unser Pilotexperiment vorliegen, also können wir auch hier die Mittelwerte und die Standardabweichung getrennt für die Tierarten berechnen.\n\nfleas_tbl %&gt;% \n  group_by(animal) %&gt;% \n  summarise(mean = mean(jump_length),\n            sd = sd(jump_length)) %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 3 × 3\n  animal  mean    sd\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 dog     8.13  2.14\n2 cat     4.74  1.9 \n3 fox     9.16  1.1 \n\n\nNatürlich sind wir nicht an den Mittelwerten sondern an den Unterschieden interessiert. Daher rechnen wir nochmal in Tabelle 53.1 alle Mittelwertsdifferenzen aus.\n\n\nTabelle 53.1— Mittelwertsdifferenzen für alle paarweisen Vergleiche.\n\n\n\ncat\nfox\n\n\ndog\n\\(8.13 - 4.74 = 3.39\\)\n\\(8.13 - 9.16 = -1.03\\)\n\n\ncat\n\n\\(4.74 - 9.16 = -4.42\\)\n\n\n\n\nWenn wir die kleinste Differenz in den Mittelwerten mit einer Power von 80% nachweisen können, dann können wir auch alle anderen größeren Mittelwertsdifferenzen mit einer Power größer als 80% nachweisen. Daher brauchen wir die Fallzahlplanung nur für den kleinsten Mittelwertsunterschied durchführen. Wir berechnen noch Cohen’s d mit \\(d = \\tfrac{1.03}{(2.14 + 1.1)/2} \\approx 0.16\\). Ganz schön kleiner Wert, wie uns die Funktion interpret_cohens_d() aus dem R Paket effectsize mitteilt.\n\ninterpret_cohens_d(0.15)\n\n[1] \"very small\"\n(Rules: cohen1988)\n\n\nWeil wir es können berechnen wir auch die Fallzahl und kriegen einen kleinen Schreck. Denn mit einem so kleinen Effekt brauchen wir wirklich viele Flöhe.\n\nres_flea &lt;- pwr.t.test(n = NULL,\n                       sig.level = 0.05, \n                       type = \"two.sample\", \n                       alternative = \"two.sided\", \n                       power = 0.80, \n                       d = 0.16)\nres_flea\n\n\n     Two-sample t test power calculation \n\n              n = 614.1541\n              d = 0.16\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nMit am Ende über \\(2 \\cdot 615 = 1230\\) Flöhen für den Vergleich von Hunde- und Fuchsflöhen sind wir wirklich weit, weit oben was die Fallzahl angeht. Da hilft es dann auch nicht viel, dass wir mit zusätzlich \\(615\\) Katzenflöhen dann auch die anderen paaweisen Vergleichw als signifikant finden würden. Denn Cohen’s d für den Vergleich von den Hunde- und Katrzenflöhen wäre \\(d = \\tfrac{3.39}{(2.14 + 1.9)/2} \\approx 0.42\\). Damit würden wir dann eine Power von \\(0.99999997\\) erhalten. Wir können die Power berechnen indem wir das Feld Power mit NULL belegen und die Fallzahl von \\(n = 615\\) eintragen.\n\nres_flea &lt;- pwr.t.test(n = 615,\n                       sig.level = 0.05, \n                       type = \"two.sample\", \n                       alternative = \"two.sided\", \n                       power = NULL, \n                       d = 0.42)\nres_flea\n\n\n     Two-sample t test power calculation \n\n              n = 615\n              d = 0.42\n      sig.level = 0.05\n          power = 1\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "experimental-design-samplesize.html#gpower-als-alternative",
    "href": "experimental-design-samplesize.html#gpower-als-alternative",
    "title": "\n53  Fallzahlplanung\n",
    "section": "\n53.9 G*Power als Alternative",
    "text": "53.9 G*Power als Alternative\n\n\n\n\n\n\nEinführung in G*Power als Alternative per Video\n\n\n\nDu findest auf YouTube G*Power als Alternative als Video Reihe. Ich werde zwar alles nochmal hier als Text aufschreiben, aber manchmal ist das Sehen und Hören dann einfacher.\n\n\nAls Alternative zu R wollen wir uns noch das Standalone Programmm G*Power | Statistical Power Analyses von der Heinrich-Heine-Universität Düsseldorf anschauen. Die Software ist nicht mehr die neuste, wird aber immer noch gewartet und an die aktuellen Versionen von Mac und Windows angepasst. Manchmal ist dann Point und Klick dann doch eine Alternative, wenn man sich ausprobieren will.\nIch werde also im Folgenden ein paar Screenshots zeigen, wie du mi G*Power dir auch die Fallzahl für Mittelwertsunterschiede und Anteilesunterschiede berechnen kannst. Allgemein ist es so das G*Power immer einseitig (eng. Tail(s) one) und zu einer Power von 95% testet. Daher müssen wir immr schauen, dass diese Werte stimmen. Insbeosndere, wenn du viel rumprobierst können auch die Werte mal wieder zurückspringen. Also bitte darauf achten.\nIn Abbildung 53.4 sehen wir die Berechnung der Fallzahl für den t-Test für einen Vergleich zweier Gruppen. Wir müssen darauf achten, dass wir die Testfamilie richtig wählen und dann den korrekten Test auswählen. Du siehst bei der eigenen Verwendung dann, dass es hier eine große Auswahl gibt. Wir nehmen aber den Standard von zwei unabhängigen Gruppen. Wir erhalten dann eine Fallzahl von \\(n = 54\\) für unseren Versuch. Das schöne an G*Power ist, dass du relativ einfach und schnell mit den Zahlen spielen kannst. Das Speichern ist schwerer, so dass ich immer einen Screenshot empfehle. Man vergisst schnell, was alles in en Feldern stand.\n\n\n\nAbbildung 53.4— Die Berechnung der Fallzahl für einen t-Test für zwei Gruppen mit einem Mittelwert +/- Standardabweichung von \\(14 \\pm 2\\) in der einen Gruppe und \\(16 \\pm 3\\) in der anderen Gruppe. Es ergibt sich ein Cohens’ d von \\(0.78\\). Wir müssen darauf achten zweiseitig und zu einer Power von 80% zu testen.\n\n\nIn Abbildung 53.5 sehen wir die Berechnung der Fallzahl für zwei Anteile. Wir haben zwei Gruppen vorliegen und in der ersten Gruppe haben wir 60% infizierte Ferkel, In der anderen Gruppe erwarten wir dann 90% infiztierte Ferkel. Um den Unterschied von 30% nachzuweisen, brauchen wir mindestens 180 Ferkel. Leider ist es so, dass wir den Test für Anteile unter dem Reiter Exact finden. Das muss man eben wisen. Achte wieder auf die Power und das zu zweiseitig testen willst.\n\n\n\nAbbildung 53.5— Die Berechung der Fallzahl für einen Anteil in zwei Gruppen von \\(0.6\\) in der einen Gruppe und \\(0.8\\) in der anderen Gruppe. Wir müssen darauf achten zweiseitig und zu eienr Power von 80% zu testen."
  },
  {
    "objectID": "experimental-design-samplesize.html#referenzen",
    "href": "experimental-design-samplesize.html#referenzen",
    "title": "\n53  Fallzahlplanung\n",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nPiper, Sophie K., Dario Zocholl, Ulf Toelch, Robert Roehle, Andrea Stroux, Johanna Hoessler, Anne Zinke, und Frank Konietschke. 2022. „Statistical review of animal trials—A guideline“. Biometrical Journal. https://doi.org/10.1002/bimj.202200061."
  },
  {
    "objectID": "classification-preface.html#ausgewählte-algorithmen",
    "href": "classification-preface.html#ausgewählte-algorithmen",
    "title": "Klassifikation oder maschinelles Lernen",
    "section": "Ausgewählte Algorithmen",
    "text": "Ausgewählte Algorithmen\nNachdem wir also wissen, dass wir Werte in einer Spalte vorhersagen wollen, können wir uns verschiedene Algorithmen einmal anschauen. Ich kann Mueller und Massaron (2021) als einen Einstieg ins maschinelle Lernen empfehlen.\n\nIn dem Kapitel 58 schauen wir uns den \\(k\\)-NN Algorithmus einmal an. In diesem Algorithmus werden neue Beoabchtungen anhand der nächstliegenden Nachbarn klassifiziert.\nIn dem Kapitel 59 betrachten wir Entscheidungsbäume. Wir lassen also immer einen Entscheidungsbaum mit zwei Zweigen wachsen und nutzen diese Entscheidungsbäume für die Vorhersage.\nIn dem Kapitel 60 werden wir uns mit der Support Vector Machine beschäftigen. Wir werden hier aber nur auf die Anwendung eingehen und ein sehr anschauliches Beispiel für die Funktionsweise nutzen.\nIn dem Kapitel 61 betrachten wir dann neuronale Netzwerke. Damit ist dann auch Deep learning gemeint und somit der letzte Stand des maschinellen Lernens.\n\nWas fehlt noch? Sicherlich fehlen noch andere Algorithmen. Aber das ist auch nicht der Sinn dieses Abschnitts eine umfassende Übersicht über alle Algorithmen des maschinellen Lernens zu geben. Wir wollen uns aber hier auf die großen und meist angewandten Algorithmen beschränken. Vielleicht ergänze ich dann nochmal ein Kapitel, wenn ich einen spannenden Algorithmus entdecke."
  },
  {
    "objectID": "classification-preface.html#referenzen",
    "href": "classification-preface.html#referenzen",
    "title": "Klassifikation oder maschinelles Lernen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nMueller, John Paul, und Luca Massaron. 2021. Machine learning for dummies. John Wiley & Sons."
  },
  {
    "objectID": "classification-basic.html#genutzte-r-pakete",
    "href": "classification-basic.html#genutzte-r-pakete",
    "title": "54  Grundlagen der Klassifikation",
    "section": "\n54.1 Genutzte R Pakete",
    "text": "54.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-basic.html#daten",
    "href": "classification-basic.html#daten",
    "title": "54  Grundlagen der Klassifikation",
    "section": "\n54.2 Daten",
    "text": "54.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt. Wir wählen auch nur ein kleines Subset aus den Daten aus, da wir in diesem Kapitel nur Funktion demonstrieren und nicht die Ergebnisse interpretieren.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") %&gt;% \n  mutate(pig_id = 1:n(),\n         infected = as_factor(infected)) %&gt;% \n  select(pig_id, infected, age:crp) %&gt;% \n  select(pig_id, infected, everything())  \n\nIn Tabelle 58.1 siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 54.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\npig_id\ninfected\nage\nsex\nlocation\nactivity\ncrp\n\n\n\n1\n1\n61\nmale\nnortheast\n15.31\n22.38\n\n\n2\n1\n53\nmale\nnorthwest\n13.01\n18.64\n\n\n3\n0\n66\nfemale\nnortheast\n11.31\n18.76\n\n\n4\n1\n59\nfemale\nnorth\n13.33\n19.37\n\n\n5\n1\n63\nmale\nnorthwest\n14.71\n21.57\n\n\n6\n1\n55\nmale\nnorthwest\n15.81\n21.45\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\nfemale\nnorth\n11.82\n21.5\n\n\n408\n0\n56\nmale\nwest\n13.91\n20.8\n\n\n409\n1\n57\nmale\nnorthwest\n12.49\n21.95\n\n\n410\n1\n61\nmale\nnorthwest\n15.26\n23.1\n\n\n411\n0\n59\nfemale\nnorth\n13.13\n20.23\n\n\n412\n1\n63\nfemale\nnorth\n10.01\n19.89\n\n\n\n\n\n\nGehen wir jetzt mal die Wörter und Begrifflichkeiten, die wir für das maschinelle Lernen später brauchen einmal durch."
  },
  {
    "objectID": "classification-basic.html#what-he-says",
    "href": "classification-basic.html#what-he-says",
    "title": "54  Grundlagen der Klassifikation",
    "section": "\n54.3 What he say’s?",
    "text": "54.3 What he say’s?\nIn diesem Teil des Skriptes werden wir wieder mit einer Menge neuer Begriffe konfrontiert. Deshalb steht hier auch eine Menge an neuen Worten drin. Leider ist es aber auch so, dass wir bekanntes neu bezeichnen. Wir tauchen jetzt ab in die Community der Klassifizierer und die haben dann eben die ein oder andere Sache neu benannt.\n\n\nKurze Referenz zu What he says?\nDie gute nachticht zuerst, wir haben ein relativ festes Vokabular. Das heißt, wir springen nicht so sehr zwischen den Begrifflichkeiten wie wir es in den anderen Teilen des Skriptes gemacht haben. Du kennst die Modellbezeichnungen wie folgt.\n\\[\ny \\sim x\n\\]\nmit\n\n\n\\(y\\), als Outcome oder Endpunkt.\n\n\\(x\\), als Covariate oder Einflussvariable.\n\nDas bauen wir jetzt um. Wir nennen in dem Bereich des maschinellen Lernen jetzt das \\(y\\) und das \\(x\\) wie folgt.\n\n\n\\(y\\) ist unser label, dafür gibt es kein deutsches Wort.\n\n\\(x\\) sind unsere features und mehrere Features bilden den feature space, dafür gibt es jeweils auch kein deutsches Wort.\n\nLabel meint also das \\(y\\) oder Outcome. Feature beschreibt das \\(x\\) oder die Einflussvariablen.\nIm folgenden Text werde ich also immer vom Label schreiben und dann damit das \\(y\\) links von dem ~ in der Modellgleichung meinen. Wenn ich dann von den Features schreibe, meine ich alle \\(x\\)-Variablen rechts von dem ~ in der Modellgleichung. Ja, daran muss du dich dann gewöhnen. Es ist wieder ein anderer sprachlicher Akzent in einem anderen Gebiet der Statistik."
  },
  {
    "objectID": "classification-basic.html#klassifikation-vs.-regression",
    "href": "classification-basic.html#klassifikation-vs.-regression",
    "title": "54  Grundlagen der Klassifikation",
    "section": "\n54.4 Klassifikation vs. Regression",
    "text": "54.4 Klassifikation vs. Regression\nWenn mich etwas aus der Bahn geworfen hat, dann waren es die Terme classification und regression im Kontext des maschinellen Lernens. Wenn ich von classification schreibe, dann wollen wir ein kategoriales Label vorhersagen. Das bedeutet wir haben ein \\(y\\) vorliegen, was nur aus Klassen bzw. Kategorien besteht. Im Zweifel haben wir dann ein Label mit \\(0/1\\) einträgen. Wenn mehr Klassen vorliegen, wird auch gerne von multiclass Klassifikation gesprochen.\nDazu steht im Kontrast der Term regression. In dem Kontext vom maschinellen Lernen meint regression die Vorhersage eines numerischen Labels. Das heißt, wir wollen die Körpergröße der Studierenden vorhersagen und nutzen dazu einen regression Klassifikator. Das ist am Anfang immer etwas verwirrend. Wir unterschieden hier nur die Typen der Label, sonst nichts. Wir fassen also wie folgt zusammen.\n\n\nclassification, wir haben ein Label bzw. \\(y\\) mit Kategorien. Nehmen wir einmal unser Ferkelbeispiel. In unserer Spalte infected sind die Ferkel infiziert \\((1)\\) oder nicht-infiziert daher gesund \\((0)\\). Du wählst dann den Modus set_mode(\"classification\").\n\nregression, wir haben ein Label bzw. \\(y\\) mit kontinuierlichen Werten. Unsere Ferkel haben ein Gewicht in \\(kg\\) und daher nehmen wir die Spalte weight. Du wählst dann den Modus set_mode(\"regression\").\n\nWir brauchen die Begriffe, da wir später in den Algorithmen spezifizieren müssen, welcher Typ die Klassifikation sein soll.\n\n\n\n\n\n\nWo ist die Regression?\n\n\n\nWir werden uns in diesen und den folgenden Kapiteln hauptsächlich mit der Klassifikation beschäftigen. Wenn du eine Regression rechnen willst, also ein kontinuierliches Label vorliegen hast, dann musst du bei dem Modellvergleich andere Maßzahlen nehmen und auch eine ROC Kurve passt dann nicht mehr. Du findest dann hier bei den Metric types unter dem Abschnitt numeric Maßzahlen für die Güte der Regression in der Prädiktion."
  },
  {
    "objectID": "classification-basic.html#supervised-vs.-unsupervised",
    "href": "classification-basic.html#supervised-vs.-unsupervised",
    "title": "54  Grundlagen der Klassifikation",
    "section": "\n54.5 Supervised vs. unsupervised",
    "text": "54.5 Supervised vs. unsupervised\nDer Unterschied zwischen einer suprvised Lernmethode oder Algorithmus ist, dass das Label bekannt ist. Das heißt, dass wir in unseren Daten eine \\(y\\) Spalte haben an der wir unser Modell dann trainieren können. Das Modell weiß also an was es sich optimieren soll. In Tabelle 54.2 sehen wir einen kleinen Datensatz in einem supervised Setting. Wir haben ein \\(y\\) in den Daten und können an diesem Label unser Modell optimieren. Oft sagen wir auch, dass wir gelabelte Daten vorliegen haben. Daher haben wir eine Spalte, die unser LAbel mit \\(0/1\\) enthält.\nSupervised heißt, dass die Daten ein Label haben und damit eine \\(y\\) Spalte haben. Wir sagen, dass die Daten gelabelt sind. Unsupervised heißt, dass wir ungelabelte Daten vorliegen haben. In dem Fall von semi-supervised Daten, haben wir Beobachtungen mit Label und ohne Label\n\n\nTabelle 54.2— Beispieldatensatz für supervised learning. Unsere Daten haben eine Spalte \\(y\\), die wir als Label in unserem Modell nutzen können. Wir haben gelabelte Daten vorliegen.\n\n\\(y\\)\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n1\n0.2\n1.3\n1.2\n\n\n0\n0.1\n0.8\n0.6\n\n\n1\n0.3\n2.3\n0.9\n\n\n1\n0.2\n9.1\n1.1\n\n\n\n\nIn der Tabelle 54.3 sehen wir als Beispiel einen Datensatz ohne eine Spalte, die wir als Label nutzen können. Nazürlich haben wir in echt dann keine freie Spalte. Ich habe das einmal so gebaut, damit du den Unterschied besser erkennen kannst. Beim unsuoervised Lernen muss der Algorithmus sich das Label selber bauen. Wir müssen meist vorgeben, wie viele Gruppen wir im Label erwarten würden. Dann können wir den Algorithmus starten.\n\n\nTabelle 54.3— Beispieldatensatz für unsupervised learning. Unsere Daten haben keine Spalte \\(y\\), die wir als Label in unserem Modell nutzen können. Wir haben ungelabelte Daten vorliegen.\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n\\(\\phantom{0}\\)\n0.2\n1.3\n1.2\n\n\n\n0.1\n0.8\n0.6\n\n\n\n0.3\n2.3\n0.9\n\n\n\n0.2\n9.1\n1.1\n\n\n\n\nDann gibt es natürlich auch den Fall, dass wir ein paar Beobachtungen mit einem Eintrag haben und wiederum andere Beobachtungen ohne eine Eintragung. Dann sprechen wir von einem semi-supervised learning. Im Prinzip ist es ein Mischmasch aus supervised learning und dem unsupervised learning. Es gibt hier aber keine genaue Grenze wie viele gelabelete Beobachtungen zu ungelabelten Beobachtungen da sein müssen.\nWir haben sehr oft eine superised Setting in unseren Daten vorliegen. Aber wie immer, du wirst vielleicht auch Cluster bilden wollen und dann ist das unsupervised Lernen eine Methode, die du gut nutzen kannst. Am Ende müssen jeder Beobachtung ein Label zugeordnet werden. Wer das dann macht, ist wiederum die Frage."
  },
  {
    "objectID": "classification-basic.html#bias-vs.-varianz",
    "href": "classification-basic.html#bias-vs.-varianz",
    "title": "54  Grundlagen der Klassifikation",
    "section": "\n54.6 Bias vs. Varianz",
    "text": "54.6 Bias vs. Varianz\nIm Bereich des maschinellen Lernens sprechen wir oft von einem Bias/Varianz Trade-off. Das heißt, wir haben zum einen eine Verzerrung (eng. Bias) in unserem Auswahlprozess des anzuwendenden Algorithmus. Zum anderen ist unser Algorithmus nur bedingt genau, das heißt wir haben auch eine Varianz die durch den Algorithmus hervorgerufen wird. Hierbei musst du dich etwas von dem Begriff Varianz im Sinne der deskriptiven Statistik lösen. Die Varianz beschreibt hier die Variabilität in der Vorhersage. Wir meinen hier schon eine Art Abweichung, aber das Wort Varianz mag hier etwas verwirrend sein. In Abbildung 54.1 sehen wir nochmal den Zusammenhang zwischen dem Bias und der Varianz.\n\n\nAbbildung 54.1— Der Bias ist eine menschliche Komponente des Modells. Wir wählen das Modell aus und bringen damit eine mögliche Verzerrung in die Auswertung. Die Varianz wird vom Modell selber verursacht und beschreibt den Zusammenhang zwischen dem Traings- und Testdaten.\n\nWir können daher wie folgt den Bias und die Varianz beschreiben. Wichtig ist hier nochmal, dass wir uns hier die Worte etwas anders benutzen, als wir es in der klassischen Statistik tun würden.\n\n\nBias: Der Bias (deu. Verzerrung) unseres Modells hat mit den Annahmen zu tun, die wir über die Daten machen. Und damit auch wie gut das Modell zu den Daten passt, auf denen das Modell trainiert wird. Ein Modell mit einem hohen Bias passt nicht gut zu den Trainingsdaten, hat eine begrenzte Flexibilität oder ist extrem einfach für die vorliegenden Daten. Wenn ein Modell zu simpel ist, führt es häufig zu einem hohen Trainingsfehler. Das Label der Traingsdaten wird daher nicht gut wiedergegeben.\n\nVarianz: Die Varianz unseres Modells sagt aus, wie das Modell seine Vorhersageergebnisse in Abhängigkeit von den Traingsdaten variiert. Ein Modell mit hoher Varianz kann sich gut an die Trainingsdaten anpassen und hat daher Probleme bei der Verallgemeinerung auf die ungesehene Testdaten, was zu einem hohen Testfehler führt.\n\nDer Bias zeigt uns, wie gut unser Modell der Realität entspricht. Die Varianz sagt uns, wie gut unser Modell auf die Trainingsdaten abgestimmt ist.\nIn Abbildung 54.2 shen wir den Zusammenhang zwischen Bias und Varianz an einer Dartscheibe dargestellt. Wenn wir eine hohe Varianz und einen hohen Bias haben, dann treffen wir großflächig daneben. Wenn sich der Bias verringert, dann treffen wir mit einer großen Streuung in die Mitte. Eine geringe Varianz und ein hoher Bias lässt uns präsize in daneben treffen. Erst mit einem niedrigen Bias und einer niedrigen Varianz treffen wir in die Mitte der Dartscheibe.\n\n\nAbbildung 54.2— Abstrakte Darstellung des Bias vs. Varianz Trade-off anhand einer Dartscheibe.\n\nDer gesamte Fehler unseres Modells setzt sich dann wie folgt aus dem Bias und der Varianz zusammen. Wir können den Bias kontrollieren in dem wir verschiedene Algorithmen auf unsere Daten testen und überlegen, welcher Algorithmus am besten passt. Die Varianz können wir dadurch verringern, dass wir unsere Modelle tunen und daher mit verschiedenen Parametern laufen lassen. Am Ende haben wir aber immer einen Restfehler \\(\\epsilon\\), den wir nicht reduzieren können. Unser Modell wird niemals perfekt zu generalisieren sein. Wenn \\(\\epsilon\\) gegen Null laufen sollte, spricht es eher für ein Auswendiglernen des Modells als für eine gute Generalisierung.\n\\[\nerror = variance + bias + \\epsilon\n\\]\nIn der Abbildung 54.3 sehen wir den Zusammenhang zwischen Bias und Varianz nochmal in einer Abbildung im Zusammenhang mit der Modellkomplxität gezeigt. Je größer die Modellkomplexität wird, desto geringer wird der Bias. Dafür wird das Modell aber überangepasst und die Varianz des Modells steigt. Daher gibt es ein Optimum des total errors bei dem der Bias und Varianz jeweils Minimal sind.\n\n\nAbbildung 54.3— Zusammenhang zwischen der Modellkomplexität, dem Bias und der Varainz. Es gibt ein Optimum des total errors.\n\nJetzt wollen wir uns den Zusammenhang zwischen Bias und Varianz nochmal an der Bilderkennung veranschaulichen. Wir nutzen dafür die Bilderkennung um Meerschweinchen und Schafe auf Bildern zu erkennen. In der Abbildung 54.4 sehen wir mich in einem Krokodilkostüm. Unser erstes Modell 1 klassifiziert mich als Meerschweinchen. Wir haben also ein sehr hohes Bias vorliegen. Ich bin kein Meerschweinchen.\n\n\nAbbildung 54.4— Unser erstes Modell hat ein hohes Bias. Daher klassifiziert mich das Modell 1 als ein Meerschweinchen, obwohl ich ein Krokodil bin.\n\nIn der Abbildung 54.5 (a) sehen wir ein durch Model 2 korrekt klassifiziertes Meerschweinchen. Nun hat dieses Modell 2 aber eine zu hohe Varianz. Die hohe Varianz in dem Modell 2 sehen wir in der Abbildung 54.5 (b). Das Meerschweinchen wird nicht als Meerschweinchen von Modell 2 erkannt, da es keine krausen Haare und eine andere Fellfarbe hat. Wir sind also auch mit diesem Modell 2 nicht zufrieden. Nur exakt die gleichen Meerschweinchen zu klassifizieren ist uns nicht genug.\n\n\n\n\n\n(a) In unserem Trainingsdatensatz hat unser Modell 2 eine hohe Varianz. Das Modell 2 findet zwar das Meerschweinchen im Bild, aber hat Probleme auf dem folgenden Testdaten.\n\n\n\n\n\n(b) In unseren Testdaten zu dem trainierten Modell 2 kann das Meerschweinchen im Bild nicht erkannt werden. Das Modell 2 hat eine zu hohe Varianz.\n\n\n\nAbbildung 54.5— Unser zweites Modell hat eine hohe Varianz. Es erkennt zwar perfekt eine Meerschweinchenart, muss aber bei einer anderen Art passen.\n\n\nIn der Abbildung 54.5 (b) sehen wir nun unser Modell 3 mit einem niedrigen Bias und einer niedrigen Varianz. Das Modell 3 kann Schafe in einer Herde als Schafe klassifizieren. Aber auch hier sehen wir gewisse Grenzen. Das Schaf welches den Kopf senkt, wird nicht von dem Modell 3 als ein Schaf erkannt. Das kann vorkommen, wenn in dem Traingsdaten so ein Schaf nicht als Bild vorlag. Häufig brauchen wir sehr viele gute Daten. Mit guten Daten, meine ich nicht immer die gleichen Beobachtungen oder Bilder sondern eine gute Bandbreite aller möglichen Gegebenheiten.\n\n\nAbbildung 54.6— Unser letztes Modell 3 hat eine niedrige Varianz und ist in der Lage die Schafe auch als Schafe zu entdecken. Ein Schaf senkt den Kopf und schon kann unser Modell 3 das Schaf nicht mehr finden.\n\nWir sehen also, das Thema Bias und Varianz beschäftigt uns bei der Auswahl des richtigen Modells und bei der Festlegung der Modellkomplexität. Du kannst dir merken, dass ein komplexeres Modell auf den Trainingsdaten meistens bessere Ergebnisse liefert und dann auf den Testdaten schlechtere. Ein komplexes Modell ist meist überangepasst (eng. overfitted)."
  },
  {
    "objectID": "classification-basic.html#problem-der-fehlenden-werte",
    "href": "classification-basic.html#problem-der-fehlenden-werte",
    "title": "54  Grundlagen der Klassifikation",
    "section": "\n54.7 Problem der fehlenden Werte",
    "text": "54.7 Problem der fehlenden Werte\n\n\n\n\n\n\nMehr zu fehlenden Werten\n\n\n\nIn dem Kapitel 56 erfährst du, wie du mit den fehlenden Werten im maschinellen Lernen umgehst. Wir werden dort aber nicht alle Details wiederholen. In dem Kapitel 38 erfährst du dann mehr über die Hintergründe und die Verfahren zum Imputieren von fehlenden Werten.\n\n\nEin wichtiger Punkt ist bei der Nutzung von maschinellen Lernen, dass wir keine fehlenden Beobachtungen in den Daten haben dürfen. Es darf kein einzelner Wert fehlen. Dann funktionieren die Algorithmen nicht und wir erhalten eine Fehlermeldung. Deshalb ist es die erste Statistikerpflicht darauf zu achten, dass wir nicht so viele fehlenden Werte in den Daten haben. Das ist natürlich nur begrenzt möglich. Wenn wir auf die Gummibärchendaten schauen, dann wurden die Daten ja von mir mit Erhoben. Dennoch haben wir viele fehlende Daten mit drin, da natürlich Studierende immer was eingetragen haben. Wenn du wissen willst, wie du mit fehlenden Werten umgehst, dann schaue einmal dazu das Kapitel 38 an. Wir gehen hier nicht nochmal auf alle Verfahren ein, werden aber die Verfahren zur Imputation von fehlenden Werten dann am Beispiel der Gummibärchendaten anwenden. Müssen wir ja auch, sonst könnten wir auch die Daten nicht für maschinelle Lernverfahren nutzen."
  },
  {
    "objectID": "classification-basic.html#normalisierung",
    "href": "classification-basic.html#normalisierung",
    "title": "54  Grundlagen der Klassifikation",
    "section": "\n54.8 Normalisierung",
    "text": "54.8 Normalisierung\n\n\n\n\n\n\nMehr zur Normalisierung\n\n\n\nIn dem Kapitel 56 erfährst du, wie du die Normalisierung von Daten im maschinellen Lernen anwendest. In dem Kapitel 17 kannst du dann mehr über die Hintergründe und die Verfahren zur Normalisierung nachlesen. Wir wenden in hier nur die Verfahren an, gehen aber nicht auf die Details weiter ein.\n\n\nUnter Normalisierung der Daten fassen wir eigentlich ein preprocessing der Daten zusammen. Wir haben ja unsere Daten in einer ursprünglichen Form vorliegen. Häufig ist diese Form nicht geeignet um einen maschinellen Lernalgorithmus auf diese ursprüngliche Form der Daten anzuwenden. Deshalb müssen wir die Daten vorher einmal anpassen und in eine geleiche Form über alle Variablen bringen. Was meine ich so kryptisch damit? Schauen wir uns einmal in der Tabelle 54.4 ein Beispiel für zu normalisierende Daten an.\n\n\nTabelle 54.4— Beispieldatensatz für einen Datensatz der nomiert werden muss. Die einzelenen Spalten haben sehr unterschiedliche Wertebereiche eingetragen.\n\n\\(y\\)\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\n\n\n1\n0.2\n1430\n23.54\n\n\n0\n0.1\n1096\n18.78\n\n\n1\n0.4\n2903\n16.89\n\n\n1\n0.2\n7861\n12.98\n\n\n\n\nWarum müssen diese Daten normalisiert werden? Wir haben mit \\(x_1\\) eine Variable vorliegen, die im Iterval \\([0;1]\\) liegt. Die Variable \\(x_2\\) liegt in einem zehntausendfach größeren Wertebereich. Die Werte der Variable \\(x_3\\) ist auch im Vergleich immer noch hundertfach im Wertebereich unterschiedlich. Dieser großen Unterschiede im Wertebereich führen zu fehlern bei Modellieren. Wir können hierzu das Kapitel 17 betrachten. Dort werden gängige Transformationen einmal erklärt. Wir gehen hier nicht nochmal auf alle Verfahren ein, sondern konzentrieren uns auf die häufigsten Anwendungen."
  },
  {
    "objectID": "classification-basic.html#das-rezept-mit-recipe",
    "href": "classification-basic.html#das-rezept-mit-recipe",
    "title": "54  Grundlagen der Klassifikation",
    "section": "\n54.9 Das Rezept mit recipe()\n",
    "text": "54.9 Das Rezept mit recipe()\n\nWenn wir jetzt in den folgenden Kapiteln mit den maschinellen Lernverfahren arbeiten werden, nutzen wir das R Paket recipes um uns mit der Funktion recipe() ein Rezept der Klassifikation zu erstellen. Warum brauchen wir das? Wir werden sehen, dass wir auf verschiedene Datensätze immer wieder die gleichen Algorithmen anwenden. Auch wollen wir eine Reihe von Vorverarbeitungsschritten (eng. preprocessing steps) auf unsere Daten anwenden. Dann ist es einfacher, wenn wir alles an einem Ort abgelegt haben. Am Ende haben wir auch verschiedene Spalten in unseren Daten. Meistens eine Spalte mit dem Label und dann sehr viele Spalten für unsere Features oder Prediktoren. Vielleiht noch eine Spalte für die ID der Beobachtungen. Das macht alles sehr unübersichtlich. Deshalb nutzen wir recipes um mehr Ordnung in unsere Klassifikation zu bekommen.\n\n\nDu findest hier die Introduction to recipes und dann eine Idee wie recipes funktionieren mit Preprocess your data with recipes.\nWir gehen nun folgende vier Schritte für die Erstellung eines Modellfits mit dem R Paket recipe einmal durch. Am Ende haben wir dann unsere Klassifikation durchgeführt. Vorher haben wir aber unseren Algorithmus und damit unser Modell definiert und auch festgelegt, was in den Daten noch angepasst und transformiert werden soll. Alles zusammen bringen wir dann in ein workflow Objekt in dem alles, was wir mit den Daten machen wollen, gespeichert ist.\n\nErstellen des Modells (logreg_mod),\nein Vorverarbeitungsrezept (eng. preprocessing) für unseren Datensatz pig_tbl erstellen (pig_rec),\ndas Modell und das Rezept in einem Wokflow bündeln (pig_wflow), und\nunseren Workflow mit einem einzigen Aufruf von fit() trainieren.\n\nEs geht los in dem wir als erstes unser Modell definieren. Wir wollen hier aus einfachen Gründen eine logistische Regression rechnen. Dafür nutzen wir die Funktion logistic_reg() um eben eine logistische Regression zu rechnen. Es gibt aber eine große Anzahl an möglichen Implementierungen bzw. engine in R. Wir wählen hier die Implementierung des glm mit der Funktion set_engine(\"glm\"). Faktisch haben wir hier also die Funktion glm(..., family = binomial) definiert. Nur ohne die Daten und die Formel.\n\n\nDu findest auf Fitting and predicting with parsnip eine große Auswahl an implementierten Algorithmen.\n\nlogreg_mod &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\")\n\nNachdem wir den Algorithmus für unser Modell definiert haben, wollen wir natürlich noch festlegen, was jetzt gerechnet werden soll. Unser Modell definieren wir in der Funktion recipe(). Hier haben wir definiert, was in das Modell soll. Links steht das Outcome und rechts nur ein .. Damit haben wir alle anderen Spalten als Einflussvariablen ausgewählt. Das stimmt aber nur halb. Den in dem Rezept können wir auch Rollen für unsere Variablen definieren. Mit der Funktion update_role() definieren wir die Variable pig_id als \"ID\". In der Klassifikation wird jetzt diese Variable nicht mehr berücksichtigt. Dann können wir noch Variablen transfomationen definieren. Wir wollen hier eine Dummykodierung für alle nominalen Prädiktoren, daher Faktoren, durchführen. Und wir wollen alle Variablen entfernen, in denen wir nur einen Eintrag haben oder eben eine Varianz von Null.\n\npig_rec &lt;- recipe(infected ~ ., data = pig_tbl) %&gt;% \n  update_role(pig_id, new_role = \"ID\")  %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_zv(all_predictors())\n\npig_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()\n\n\nWir du siehst wird hier noch nichts gerechnet. Es gilt jetzt zu definieren was wir tun wollen. Damit wir das Rezept einfach immer wieder auf neue Daten anwenden können. Die Rollen der Variablen kannst du dir auch über die Funktion summary() wiedergeben lassen.\n\nsummary(pig_rec)\n\n# A tibble: 7 × 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 pig_id   &lt;chr [2]&gt; ID        original\n2 age      &lt;chr [2]&gt; predictor original\n3 sex      &lt;chr [3]&gt; predictor original\n4 location &lt;chr [3]&gt; predictor original\n5 activity &lt;chr [2]&gt; predictor original\n6 crp      &lt;chr [2]&gt; predictor original\n7 infected &lt;chr [3]&gt; outcome   original\n\n\nDu siehst, dass die Variable pig_id eine ID ist und die Variable infected das Outcome darstellt. Der Rest sind die Prädiktoren mit ihren jeweiligen Typen. Wir können über die Hilfsfunktionen all_predictor() oder all_nominal_predictor() eben nur bestimmte Spalten für eine Transformation auswählen.\nIm nächsten Schritt bringen wir das Modell logreg_mod und das Rezept pig_rec mit den Informationen über die Variablen und die notwendigen Transformationsschritte in einem workflow() zusammen. In diesem workflow() sind alle wichtigen Information drin und wir können den Workflow mit immer wieder neuen Subsets von unseren ursprünglichen Daten füttern.\n\npig_wflow &lt;- workflow() %&gt;% \n  add_model(logreg_mod) %&gt;% \n  add_recipe(pig_rec)\n\npig_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nNun heißt es noch den Wirkflow mit echten Daten zu füttern. Wir rechnen also erst jetzt mit echten Daten. Vorher aber wir nur gesagt, was wir machen wollen. Erst die Funktion fit() rechnet das Modell auf den Daten mit den Regeln in dem Rezept. Wir nehmen hier wieder unsere ursprünglichen Daten, aber du könntest hier auch den Traingsdatensatz nehmen.\n\npig_fit &lt;- pig_wflow %&gt;% \n  fit(data = pig_tbl)\n\npig_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n       (Intercept)                 age            activity                 crp  \n         -19.46706             0.01100             0.06647             0.96804  \n          sex_male  location_northeast  location_northwest       location_west  \n          -0.51320             0.01848            -0.51613            -0.26807  \n\nDegrees of Freedom: 411 Total (i.e. Null);  404 Residual\nNull Deviance:      522.6 \nResidual Deviance: 402.3    AIC: 418.3\n\n\nWir erhalten den klassischen Fit einer logististischen Regression wieder, wenn wir die Funktion extract_fit_parsnip() verwenden. Die Funktion gibt uns dann alle Informationen wieder. Dann können wir uns über die Funktion tidy() auch eine aufgeräumte Wiedergabe erstellen lassen.\n\npig_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  tidy() %&gt;% \n  mutate(across(where(is.numeric), round, 2),\n         p.value = pvalue(p.value))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 8 × 5\n  term               estimate std.error statistic p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 (Intercept)          -19.5       3.02     -6.45 &lt;0.001 \n2 age                    0.01      0.03      0.4  0.690  \n3 activity               0.07      0.09      0.72 0.470  \n4 crp                    0.97      0.11      8.8  &lt;0.001 \n5 sex_male              -0.51      0.32     -1.61 0.110  \n6 location_northeast     0.02      0.36      0.05 0.960  \n7 location_northwest    -0.52      0.32     -1.6  0.110  \n8 location_west         -0.27      0.36     -0.75 0.460  \n\n\nUnd was ist jetzt mit der Prädiktion? Dafür können wir entweder die Funktion predict() nutzen oder aber die Funktion augment(). Mir persönlich gefällt die Funktion augment() besser, da ich hier mehr Informationen zu den vorhergesagten Werten erhalte. Ich wähle mir dann die Spalte infected aus und alle Spalten, die ein .pred beinhalten. Dann runde ich noch auf die zweite Kommastelle.\n\naugment(pig_fit, new_data = pig_tbl) %&gt;% \n  select(infected, matches(\".pred\")) %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 412 × 4\n   infected .pred_class .pred_0 .pred_1\n   &lt;fct&gt;    &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 1        1              0.03    0.97\n 2 1        0              0.73    0.27\n 3 0        1              0.45    0.55\n 4 1        1              0.31    0.69\n 5 1        1              0.11    0.89\n 6 1        1              0.13    0.87\n 7 1        0              0.6     0.4 \n 8 0        0              0.66    0.34\n 9 1        1              0.05    0.95\n10 1        1              0.21    0.79\n# ℹ 402 more rows\n\n\nDamit hätten wir einmal das Prinzip des Rezeptes für die Klassifikation in R durchgeführt. Dir wird das Rezept in den nächsten Kapiteln wieder über den Weg laufen. Für die Anwendung gibt es eigentlich keine schönere Art die Klassifikation sauber durchzuführen. Wir erhalten gute Ergebnisse und wissen auch was wir getan haben."
  },
  {
    "objectID": "classification-data.html#genutzte-r-pakete",
    "href": "classification-data.html#genutzte-r-pakete",
    "title": "55  Data splitting",
    "section": "\n55.1 Genutzte R Pakete",
    "text": "55.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-data.html#daten",
    "href": "classification-data.html#daten",
    "title": "55  Data splitting",
    "section": "\n55.2 Daten",
    "text": "55.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt. Auch brauchen wir nicht alle Spalten, da wir hier um die Zeilen und damit die Beobachtungen geht.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") %&gt;% \n  mutate(pig_id = 1:n()) %&gt;% \n  select(pig_id, infected, age:crp) %&gt;% \n  select(pig_id, infected, everything())  \n\nIn Tabelle 58.1 siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 55.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\npig_id\ninfected\nage\nsex\nlocation\nactivity\ncrp\n\n\n\n1\n1\n61\nmale\nnortheast\n15.31\n22.38\n\n\n2\n1\n53\nmale\nnorthwest\n13.01\n18.64\n\n\n3\n0\n66\nfemale\nnortheast\n11.31\n18.76\n\n\n4\n1\n59\nfemale\nnorth\n13.33\n19.37\n\n\n5\n1\n63\nmale\nnorthwest\n14.71\n21.57\n\n\n6\n1\n55\nmale\nnorthwest\n15.81\n21.45\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\nfemale\nnorth\n11.82\n21.5\n\n\n408\n0\n56\nmale\nwest\n13.91\n20.8\n\n\n409\n1\n57\nmale\nnorthwest\n12.49\n21.95\n\n\n410\n1\n61\nmale\nnorthwest\n15.26\n23.1\n\n\n411\n0\n59\nfemale\nnorth\n13.13\n20.23\n\n\n412\n1\n63\nfemale\nnorth\n10.01\n19.89\n\n\n\n\n\n\nGehen wir jetzt mal die verschiedenen Datensätze und Begrifflichkeiten, die wir für das maschinelle Lernen später brauchen, einmal durch."
  },
  {
    "objectID": "classification-data.html#trainingsdatensatz-und-testdatensatz",
    "href": "classification-data.html#trainingsdatensatz-und-testdatensatz",
    "title": "55  Data splitting",
    "section": "\n55.3 Trainingsdatensatz und Testdatensatz",
    "text": "55.3 Trainingsdatensatz und Testdatensatz\nUm zu beginnen, teilen wir unseren einen Datensatz in zwei: einen Trainingssatz und einen Testsatz. Die meisten Zeilen und damit Beobachtungen des Originaldatensatzes werden im Trainingssatz sein. Wir nutzen die Trainingsdaten zum Anpassen des Modells. Wir trainieren das Modell auf den Daten des Trainingsdatensatzes. Wir messen dann das Modell auf den Testdatensatz. Warum machen wir das? Wenn wir auf dem Trainingsdatensatz auch die Modelgüte testen würden, dann könnten wir eine Überanpassung (eg. overfitting) auf die Trainingsdaten beobachten. Das Modell ist so gut an die spezifischen Trainingsdaten angepasst, dass es mit neuen Daten schwer umgehen kann.\n\n\nDas R Paket resample stellt die Common Resampling Patterns nochmal da. Auch findest unter Resampling for Evaluating Performance noch eine Menge mehr Ideen für das Resampling.\nZu diesem Zweck können wir das R Paket rsample verwenden. Wir nutzen dann die Funktion initial_split() um die Daten in einen Trainingsdatensatz und einen Testdatensatz aufzuteilen. Dann müssen wir noch den Trainingsdatensatz und den Testdatensatz einmal getrennt in einem Objekt abspeichern.\n\npig_split &lt;- initial_split(pig_tbl, prop = 3/4)\n\npig_split\n\n&lt;Training/Testing/Total&gt;\n&lt;309/103/412&gt;\n\n\nWie wir sehen, sehen wir gar nichts. Das ist auch so gewollt. Da wir im maschinellen Lernen gerne mal mit Datensätzen mit mehreren tausend Zeilen arbeiten würde es wenig helfen, wenn wir gleich alles auf der R Console ausgegeben kriegen. Die Information wie viel wir in den jeweiligen Gruppen haben, hilft schön genug.\n\ntrain_pig_tbl &lt;- training(pig_split)\ntest_pig_tbl &lt;- testing(pig_split)\n\nNun haben wir die beiden Datensätze jeweils separat und können auf dem Trainingsdatensatz die jeweiligen Algorithmen bzw. Modelle trainieren.\nEs ist schön, wenn wir Funktionen wie initial_split(), die für uns die Arbeit machen. Wir haben dann aber auch sehr schnell das Gefühl mit einer Black Box zu arbeiten. Man weiß gar nicht, was da eigentlich passiert ist. Deshalb hier nochmal der Code, den ich dann auch immer zur Demonstration nutze. Wenn wir eine ID Spalte haben, dann können wir auch über die Funktion sample_frac() und dem Anteil der ausgewählten Beobachtungen und der Funktion anti_join(), die Trainings- und Testdaten erstellen.\n\npig_train_tbl &lt;- pig_tbl %&gt;% sample_frac(0.75)\npig_test_tbl &lt;- anti_join(pig_tbl,\n                          pig_train_tbl, by = 'pig_id')\n\nWir können dann auch überprüfen, ob wir die gleichen Anteile von den infizierten Ferkeln in den jeweiligen Datensätzen haben. Wir berechnen dafür einfach die relativen Anteile. Ein wenig komplizierter als nötig, aber hier geht es jetzt um die Veranschaulichung.\n\ntable(pig_train_tbl$infected)/sum(table(pig_train_tbl$infected))\n\n\n        0         1 \n0.3171521 0.6828479 \n\ntable(pig_test_tbl$infected)/sum(table(pig_test_tbl$infected))\n\n\n       0        1 \n0.368932 0.631068 \n\n\nDu kannst die Generierung häufiger wiederholen und du wirst sehen, dass wir es mit einem Zufallsprozess zu tun haben. Mal sind die Anteile ähnlicher mal eher nicht. Das ist dann auch der Grund warum wir unsere Modelle tunen müssen und Modelle häufig wiederholt rechnen und die Ergebnisse dann zusammenfassen."
  },
  {
    "objectID": "classification-data.html#validierungsdatensatz",
    "href": "classification-data.html#validierungsdatensatz",
    "title": "55  Data splitting",
    "section": "\n55.4 Validierungsdatensatz",
    "text": "55.4 Validierungsdatensatz\nDie finalen Modelle sollten nur einmal anhand ihres Testdatensatzes evaluieren werden. Das Überpfrüfen auf dem Testdatensatz geschieht nachdem die Optimierung und das Training der Modelle vollständig abgeschlossen ist. Was natürlich für uns nicht so schön ist, wir wollen ja auch zwischendurch mal schauen, ob wir auf dem richtigen Weg mit dem Training sind. Wir solle es auch sonst mit dem Tuning funktionieren? Deshalb ist möglich, zusätzliche Datensätze aus dem Trainingsprozess herauszuhalten, die zur mehrmaligen Evaluierung von Modellen verwendet werden können. Das machen wir dann solange bis wir bereit sind anhand des endgültigen Testsatzes zu evaluieren.\nDiese zusätzlichen, aufgeteilten Datensätze werden oft als Validierungssätze bezeichnet und können in über die Funktion validation_split() erstellt werden.\n\nval_pig_lst &lt;- validation_split(pig_tbl, prop = 0.8)\nval_pig_lst\n\n# Validation Set Split (0.8/0.2)  \n# A tibble: 1 × 2\n  splits           id        \n  &lt;list&gt;           &lt;chr&gt;     \n1 &lt;split [329/83]&gt; validation\n\n\nIn diesem Fall lassen wir den Validierungsdatensatz einmal so in der Liste stehen. Es ist faktisch wider ein Split der Daten, nur das wir jetzt auf diesem Datensatz unser Modell während des Tunings testen."
  },
  {
    "objectID": "classification-data.html#kreuzvalidierung",
    "href": "classification-data.html#kreuzvalidierung",
    "title": "55  Data splitting",
    "section": "\n55.5 Kreuzvalidierung",
    "text": "55.5 Kreuzvalidierung\nBei der Abstimmung von Hyperparametern und der Modellanpassung ist es oft nützlich, das Modell anhand von mehr als nur einem einzigen Validierungssatz zu bewerten, um eine stabilere Schätzung der Modellleistung zu erhalten. Wir meinen hier mit Hyperparametern die Optionen, die ein Algorithmus hat um diesen Algorithmus zu optimieren. Aus diesem Grund verwenden Modellierer häufig ein Verfahren, das als Kreuzvalidierung bekannt ist und bei dem die Daten mehrfach in Analyse- und Valisierungsdaten aufgeteilt werden.\nDie vielleicht häufigste Methode der Kreuzvalidierung ist die \\(V\\)-fache Kreuzvalidierung. Bei dieser auch als \\(k\\)-fold cross-validation bezeichneten Methode werden \\(V\\) neue Stichproben bzw. Datensätze erstellt, indem die Daten in \\(V\\) Gruppen (auch folds genannt) von ungefähr gleicher Größe aufgeteilt werden. Der Analysesatz jeder erneuten Stichprobe besteht aus \\(V-1\\) Gruppen, wobei die verbleibende Gruppe als Validierungsdatensatz verwendet wird. Insgesamt führen wir dadurch dann den Algorithmus \\(V\\)-mal durch. Auf diese Weise wird jede Beobachtung in Daten in genau einem Beurteilungssatz verwendet.\nIn R können wir dafür die Funktion vfold_cv() nutzen. Im Folgenden einmal Split für \\(V = 5\\). Wir führen also eine \\(5\\)-fache Kreuzvalidierung durch.\n\nvfold_cv(pig_tbl, v = 3)\n\n#  3-fold cross-validation \n# A tibble: 3 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [274/138]&gt; Fold1\n2 &lt;split [275/137]&gt; Fold2\n3 &lt;split [275/137]&gt; Fold3\n\n\nAls ein Nachteil wird oft angesehen, dass die Kreuzvalidierung eine hohe Varianz in den Daten verursacht. Dagegen hilft dann die wiederholte Kreuzvalidierung (eng. repeated cross-validation). Wir bauen in jede Kreuzvalidierung nochmal eine oder mehr Wiederholungen ein. In unserem Fall dann drei Wiederholungen je Kreuzvalidierung \\(V\\).\n\nvfold_cv(pig_tbl, v = 3, repeats = 2)\n\n#  3-fold cross-validation repeated 2 times \n# A tibble: 6 × 3\n  splits            id      id2  \n  &lt;list&gt;            &lt;chr&gt;   &lt;chr&gt;\n1 &lt;split [274/138]&gt; Repeat1 Fold1\n2 &lt;split [275/137]&gt; Repeat1 Fold2\n3 &lt;split [275/137]&gt; Repeat1 Fold3\n4 &lt;split [274/138]&gt; Repeat2 Fold1\n5 &lt;split [275/137]&gt; Repeat2 Fold2\n6 &lt;split [275/137]&gt; Repeat2 Fold3\n\n\nWir sehen das der Split ungefähr immer gleich groß ist. Manchmal haben wir durch die Trennung eine Beobachtung mehr in dem Analysedatensatz mit \\(n = 329\\) oder \\(n = 330\\) Beobachtungen. Dementsprechend hat der Validierungsdatensatz einmal \\(n = 82\\) und einmal \\(n = 83\\) Beobachtungen."
  },
  {
    "objectID": "classification-data.html#monte-carlo-kreuzvalidierung",
    "href": "classification-data.html#monte-carlo-kreuzvalidierung",
    "title": "55  Data splitting",
    "section": "\n55.6 Monte-Carlo Kreuzvalidierung",
    "text": "55.6 Monte-Carlo Kreuzvalidierung\nWir haben als eine Alternative zur V-fachen Kreuzvalidierung die Monte-Carlo-Kreuzvalidierung vorliegen. Während bei der V-fachen Kreuzvalidierung jede Beobachtung in den Daten einem - und zwar genau einem - Validierungsdatensatz zugewiesen wird, wird bei der Monte-Carlo-Kreuzvalidierung für jeden Validierungsdatensatz eine zufällige Teilmenge der Daten ausgewählt, d. h. jede Beobachtung kann in 0, 1 oder vielen Validierungsdatensätzen verwendet werden. Der Analysesatz besteht dann aus allen Beobachtungen, die nicht ausgewählt wurden. Da jeder Validierungsdatensatz unabhängig ausgewählt wird, können wir diesen Vorgang so oft wie gewünscht wiederholen. Das stimt natürlich nur bedingt, denn irgendwann haben wir auch bei perfekter Permutation dann Wiederholungen der Datensätze.\nDie Funktion mc_cv() liefert uns dann die Datensätze für die Monte-Carlo Kreuzvalidierung. Wir geben dabei an, wieviel der Daten in den jeweiligen Datensatz hinein permutiert werden soll.\n\nmc_cv(pig_tbl, prop = 0.6, times = 3)\n\n# Monte Carlo cross-validation (0.6/0.4) with 3 resamples  \n# A tibble: 3 × 2\n  splits            id       \n  &lt;list&gt;            &lt;chr&gt;    \n1 &lt;split [247/165]&gt; Resample1\n2 &lt;split [247/165]&gt; Resample2\n3 &lt;split [247/165]&gt; Resample3"
  },
  {
    "objectID": "classification-data.html#bootstraping",
    "href": "classification-data.html#bootstraping",
    "title": "55  Data splitting",
    "section": "\n55.7 Bootstraping",
    "text": "55.7 Bootstraping\nDie letzte Stichprobengenierungsmethode ist der Bootstrap. Eine Bootstrap Stichprobe ist eine Stichprobe des Datensatzes mit der gleichen Größe wie der Datensatz. Nur werden die Bootstrap Stichproben mit Ersetzung gezogen, so dass eine einzelne Beobachtung mehrfach in die Stichprobe aufgenommen werden können. Der Validierungsdatensatz besteht dann aus allen Beobachtungen, die nicht für den Analysesatz ausgewählt wurden. Im Allgemeinen führt das Bootstrap-Resampling zu pessimistischen Schätzungen der Modellgenauigkeit.\nWir können die Funktion bootstraps() für die Generierung der Bootstrap Stichprobe nutzen.\n\npig_boot_tbl &lt;- pig_tbl %&gt;% \n  extract(1:10, 1:5)\n\npig_boot &lt;- bootstraps(pig_boot_tbl, times = 3)\n\nNun haben wir auch die Möglichkeit uns die einzelnen Bootstraps Stichproben mit pluck() rauszuziehen. Hier sehen wir auch, dass einzelene Beobachtungen doppelt in der Bootstrap Stich probe vorkommen.\n\npluck(pig_boot, \"splits\", 1) %&gt;% \n  as_tibble \n\n# A tibble: 10 × 5\n   pig_id infected   age sex    location \n    &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1      8        0    53 male   northwest\n 2      3        0    66 female northeast\n 3      7        1    49 male   west     \n 4     10        1    57 male   northwest\n 5      2        1    53 male   northwest\n 6      2        1    53 male   northwest\n 7      6        1    55 male   northwest\n 8      1        1    61 male   northeast\n 9      8        0    53 male   northwest\n10      9        1    58 female west"
  },
  {
    "objectID": "classification-data.html#weitere-valdierungen",
    "href": "classification-data.html#weitere-valdierungen",
    "title": "55  Data splitting",
    "section": "\n55.8 Weitere Valdierungen",
    "text": "55.8 Weitere Valdierungen\nNeben den hier vorgestellten Varianten gibt es noch weitere Möglichkeiten in dem R Paket rsample sich Stichprobendatensätze zu generieren. Wir gehen jetzt hier nicht mehr im Detail auf die verschiedenen Möglichkeiten ein. Dafür dann einfach die Links auf die rsample Hilfeseite nutzen.\n\n\nStratifiziertes Resampling nutzen wir, wenn wir eine Gruppe in den Daten haben, die nicht gleichmäßig über die Daten verteilt ist. Das heißt, wir haben ein nicht balanciertes Design. Kann plaktiv wäre das der Fall, wenn wir fast nur Frauen oder Männer in unseren Daten vorliegen hätten. Hier kann es dann passieren, dass wir zufällig Datensätze ziehen, die nur Frauen oder nur Männer beinhalten. Das wollen wir natürlich verhindern.\n\nGruppiertes Resampling nutzen wir, wenn wir korrelierte Beobachtungen haben. Oft sind einige Beobachtungen in deinen Daten ähnlicher als es der Zufall vermuten ließe, z. B. weil sie wiederholte Messungen desselben Probanden darstellen oder alle an einem einzigen Ort gesammelt wurden. Dann müssen wir eventuell auch hierfür das Resampling anpassen.\n\nZeitpunkt basiertes Resampling sind in dem Sinne eine besonderheit, da wir natürlich berücksichtigen müssen, wann eine Beobachtung im zeitlichen Verlauf gemacht wurde. Hier hat die Zeit einen Einfluss auf das Resampling.\n\nAm Ende musst du entscheiden, welche der Resamplingmethoden für dich am besten geeignet ist. Wir müssen eben einen Trainingsdatensatz und einen Testdatensatz haben. Die Validierungsdaten dienen dann zum Tuning deiner Modelle. Nicht immer nutzen wir auch Validierungsdatensätze. In dem einfachsten Anwendungsfall nutzt du immer wieder deine Traingsdaten mit unterschiedlichen Einstellungen in deinem Algorithmus."
  },
  {
    "objectID": "classification-pre-processing.html#genutzte-r-pakete-für-das-kapitel",
    "href": "classification-pre-processing.html#genutzte-r-pakete-für-das-kapitel",
    "title": "56  Data preprocessing",
    "section": "\n56.1 Genutzte R Pakete für das Kapitel",
    "text": "56.1 Genutzte R Pakete für das Kapitel\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor,\n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-pre-processing.html#daten",
    "href": "classification-pre-processing.html#daten",
    "title": "56  Data preprocessing",
    "section": "\n56.2 Daten",
    "text": "56.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt. Wir wählen auch nur ein kleines Subset aus den Daten aus, da wir in diesem Kapitel nur Funktion demonstrieren und nicht die Ergebnisse interpretieren.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") %&gt;% \n  mutate(pig_id = 1:n()) %&gt;% \n  select(pig_id, infected, age, crp, sex, frailty) %&gt;% \n  select(pig_id, infected, everything())  \n\nIn Tabelle 58.1 siehst du nochmal einen Ausschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 56.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\npig_id\ninfected\nage\ncrp\nsex\nfrailty\n\n\n\n1\n1\n61\n22.38\nmale\nrobust\n\n\n2\n1\n53\n18.64\nmale\nrobust\n\n\n3\n0\n66\n18.76\nfemale\nrobust\n\n\n4\n1\n59\n19.37\nfemale\nrobust\n\n\n5\n1\n63\n21.57\nmale\nrobust\n\n\n6\n1\n55\n21.45\nmale\nrobust\n\n\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\n21.5\nfemale\npre-frail\n\n\n408\n0\n56\n20.8\nmale\nfrail\n\n\n409\n1\n57\n21.95\nmale\npre-frail\n\n\n410\n1\n61\n23.1\nmale\nrobust\n\n\n411\n0\n59\n20.23\nfemale\nrobust\n\n\n412\n1\n63\n19.89\nfemale\nrobust\n\n\n\n\n\n\nGehen wir jetzt mal die Preprocessing Schritte, die wir für das maschinelle Lernen später brauchen einmal durch. Am Ende des Kapitels schauen wir uns dann die Anwendung nochmal im Ganzen auf den Gummibärchendaten einmal an."
  },
  {
    "objectID": "classification-pre-processing.html#das-rezept-mit-recipe",
    "href": "classification-pre-processing.html#das-rezept-mit-recipe",
    "title": "56  Data preprocessing",
    "section": "\n56.3 Das Rezept mit recipe()\n",
    "text": "56.3 Das Rezept mit recipe()\n\nIn dem Einführungskapitel zur Klassifikation haben wir uns ja mit dem Rezept und dem Workflow schon mal beschäftigt. Hier möchte ich dann nochmal etwas mehr auf das Rezept eingehen und zeigen, wie das Rezept für Daten dann mit den Daten zusammenkommt. Wir bauen uns wie immer mit der Funktion recipe() das Datenrezept in R zusammen. Ich empfehle grundsätzlich vorab einen select() Schritt durchzuführen und nur die Variablen in den Daten zu behalten, die wir wirklich brauchen. Dann können wir auch mit dem . einfach alle Spalten ohne das Outcome als Prädiktoren definieren.\n\npig_rec &lt;- recipe(infected ~ ., data = pig_tbl) %&gt;% \n  update_role(pig_id, new_role = \"ID\")\n\npig_rec %&gt;% summary()\n\n# A tibble: 6 × 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 pig_id   &lt;chr [2]&gt; ID        original\n2 age      &lt;chr [2]&gt; predictor original\n3 crp      &lt;chr [2]&gt; predictor original\n4 sex      &lt;chr [3]&gt; predictor original\n5 frailty  &lt;chr [3]&gt; predictor original\n6 infected &lt;chr [2]&gt; outcome   original\n\n\nNachdem wir dann unser Rezept definiert haben, können wir auch noch Rollen vergeben. Die Rollen sind nützlich, wenn wir später auf bestimmten Variablen etwas rechnen wollen oder eben nicht. Wir können die Rollen selber definieren und diese Rollen dann auch über die Funktion has_role() ein- oder ausschließen. Neben dieser Möglichkeit gezielt Variablen nach der Rolle anzusprechen, können wir auch alle Prädiktoren oder alle Outcomes auswählen.\nWir haben Funktionen, die die Rolle der Variablen festlegen:\n\n\nall_predictors() wendet den Schritt nur auf die Prädiktorvariablen an, daher auf die Features.\n\nall_outcomes() wendet den Schritt nur auf die Outcome-Variable(n) an, daher auf die Label.\n\nUn wir haben Funktionen, die den Typ der Variablen angeben:\n\n\nall_nominal() wendet den Schritt auf alle Variablen an, die nominal (kategorisch) sind.\n\nall_numeric() wendet den Schritt auf alle Variablen an, die numerisch sind.\n\nUnd natürlich deren Kombination wie all_nominal_predictors() oder all_numeric_predictors(), die dann eben auf die Prädiktoren, die nominal also Faktoren oder Gruppen repräsentieren oder eben numerischen Variablen, angewendet werden. Du wirst die Anwendung gleich später in den Rezeptschritten sehen, da macht die Sache dann sehr viel mehr Sinn.\nNun ist es aber auch so, dass es bei dem Rezept auf die Reihenfolge der einzelnen Schritte ankommt. Die Reihenfolge der Zutaten und damit der Rezeptschritte sind ja auch beim Kuchenbacken sehr wichtig! Da das Rezept wirklich in der Reihenfolge durchgeführt wird, wie du die einzelnen Schritte angibst, empfiehlt sich folgende Reihenfolge. Du musst natürlich nicht jeden dieser Schritte auch immer durchführen.\n\n\nBitte die Hinweise zur Ordnung der Schritte eines Rezeptes beachten: Ordering of steps\n\nEntfernen von Beobachtungen mit einem fehlenden Eintrag für das Label.\nImputation von fehlenden Werten in den Daten.\nIndividuelle Transformationen auf einzelnen Spalten.\nUmwandeln von einzelnen numerischen Variablen in eine diskrete Variable.\nErstellung der Dummyvariablen für jede diskrete Variable.\nEventuell Berücksichtigung der Interaktion zwischen Variablen.\nTransformation der numerischen Variablen mit zum Beispiel der Standarisierung oder Normalisierung.\nMultivariate Transformationen über alle Spalten hinweg wie zum Beispiel PCA.\n\nAm Ende wollen wir dann natürlich auch die Daten wiederhaben. Das heißt, wir bauen ja das Rezept auf einem Datensatz. Wenn wir dann das fertige Rezept in die Funktion prep() pipen können wir über die Funktion juice() den ursprünglichen jetzt aber transformierten Datensatz wieder erhalten. Wenn wir das Rezept auf einen neuen Datensatz anwenden wollen, dann nutzen wir die Funktion bake(). Mit einem neuen Datensatz meine ich natürlich einen Split in Training- und Testdaten von dem ursprünglichen Datensatz. In dem neuen Datensatz müssen natürlich alle Spaltennamen auch enthalten sein, sonst macht die Sache recht wenig Sinn."
  },
  {
    "objectID": "classification-pre-processing.html#fehlende-werte-im-y",
    "href": "classification-pre-processing.html#fehlende-werte-im-y",
    "title": "56  Data preprocessing",
    "section": "\n56.4 Fehlende Werte im \\(Y\\)\n",
    "text": "56.4 Fehlende Werte im \\(Y\\)\n\nWenn wir mit maschinellen Lernverfahren rechnen, dann dürfen wir im Outcome \\(Y\\) oder dem Label keine fehlenden Werte vorliegen haben. Das Outcome ist in dem Sinne hielig, dass wir hier keine Werte imputieren. Wir müssen daher alle Zeilen und damit Beobachtungen aus den Daten entfernen in denen ein NA im Outcome vorliegt. Wir können dazu die Funktion drop_na() nutzen. Wir können in der Funktion spezifizieren, dass nur für eine Spalte die NA entfernt werden sollen. In unserem Beispiel für die Ferkeldaten wäre es dann die Spalte infected.\n\ndrop_na(infected)\n\nAktuell haben wir ja keine fehlenden Werte in der Spalte vorliegen, so dass wir die Funktion hier nicht benötigen. In dem Beispiel zu den Gummibärchendaten wollen wir das Geschlecht vorhersagen und hier haben wir dann fehlende Werte im Outcome. Mit der Funktion drop_na(gender) entfernen wir dann alle Beobachtungen aus den Daten mit einem fehlenden Eintrag für das Geschlecht."
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-dummy",
    "href": "classification-pre-processing.html#sec-preprocess-dummy",
    "title": "56  Data preprocessing",
    "section": "\n56.5 Dummycodierung von \\(X\\)\n",
    "text": "56.5 Dummycodierung von \\(X\\)\n\nWir werden immer häufiger davon sprechen, dass wir alle kategorialen Daten in Dummies überführen müssen. Das heißt, wir dürfen keine Faktoren mehr in unseren Daten haben. Wir wandeln daher alle Variablen, die ein Faktor sind, in Dummyspalten um. Die Idee von der Dummyspalte ist die gleiche wie bei der multiplen Regression. Da ich aber nicht davon ausgehe, dass du dir alles hier durchgelesen hast, kommt hier die kurze Einführung zur Dummycodierung.\n\n\nMehr Information zu Create Traditional Dummy Variables\nDie Dummycodierung wird nur auf den Features durchgeführt. Dabei werden nur Spalten erschaffen, die \\(0/1\\), für Level vorhanden oder Level nicht vorhanden, beinhalten. Wir werden also nur alle \\(x\\) in Dummies umwandeln, die einem Faktor entsprechen. Dafür nutzen wir dann später eine Funktion, hier machen wir das einmal zu Veranschaulichung per Hand. In Tabelle 56.2 haben wir einen kleinen Ausschnitt unser Schweinedaten gegeben. Wir wollen zuerst die Spalte sex in eine Dummycodierung umwandeln.\n\n\nTabelle 56.2— Beispieldatensatz für die Dummycodierung. Wir wollen die Spalten sex und frailty als Dummyspalten haben.\n\ninfected\nage\nsex\nfrailty\n\n\n\n1\n24\nmale\nrobust\n\n\n0\n36\nmale\npre-frail\n\n\n0\n21\nfemale\nfrail\n\n\n1\n34\nfemale\nrobust\n\n\n1\n27\nmale\nfrail\n\n\n\n\nIn der Tabelle 56.3 sehen wir das Ergebnis für die Dummycodierung der Spalte sex in die Dummyspalte sex_male. Wir haben in der Dummyspalte nur noch die Information, ob das Ferkel mänlich ist oder nicht. Wenn wir eine Eins in der Spalte finden, dann ist das Ferkel männlich. Wenn wir eine Null vorfinden, dann ist das Ferkel nicht männlich also weiblich. Das Nicht müssen wir uns dann immer merken.\n\n\nTabelle 56.3— Ergebnis der Dummycodierung der Spalte sex zu der Spalte sex_male.\n\ninfected\nage\nsex_male\n\n\n\n1\n24\n1\n\n\n0\n36\n1\n\n\n0\n21\n0\n\n\n1\n34\n0\n\n\n1\n27\n1\n\n\n\n\nIn der Tabelle 56.4 betrachten wir einen komplexeren Fall. Wenn wir eine Spalte vorliegen haben mit mehr als zwei Leveln, wie zum Beispiel die Spalte frailty, dann erhalten wir zwei Spalten wieder. Die Spalte frailty_robust beschreibt das Vorhandensein des Levels robust und die Spalte frailty_pre-frail das Vorhandensein des Levels pre-frail. Und was ist mit dem Level frail? Das Level wir durch das Nichtvorhandensein von robust und dem Nichtvorhandensein von pre-frail abgebildet. Beinhalten beide Spalten die Null, so ist das Ferkel frail.\n\n\nTabelle 56.4— Ergebnis der Dummycodierung für eine Spalte mit mehr als zwei Leveln.\n\ninfected\nage\nfrailty_robust\nfrailty_pre-frail\n\n\n\n1\n24\n1\n0\n\n\n0\n36\n0\n1\n\n\n0\n21\n0\n0\n\n\n1\n34\n1\n0\n\n\n1\n27\n0\n0\n\n\n\n\nWenn wir einen Faktor mit \\(l\\) Leveln haben, erhalten wir immer \\(l-1\\) Spalten nach der Dummycodierung wieder.\nWir nutzen dann die Funktion step_dummy() um eine Dummaycodierung für alle nominalen Prädiktoren spezifiziert durch all_nominal_predictors() durchzuführen. Das tolle ist hier, dass wir durch die Helferfunktionen immer genau sagen können welche Typen von Spalten bearbeitet werden sollen.\n\npig_dummy_rec &lt;- pig_rec %&gt;% \n  step_dummy(all_nominal_predictors()) \n\npig_dummy_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n\nWenn wir das Rezept fertig haben, dann können wir uns die Daten einmal anschauen. Durch die Funktion prep() initialisieren wir das Rezept und mit der Funktion juice() teilen wir mit, dass wir das Rezept gleich auf die Trainingsdaten mit denen wir das Rezept gebaut haben, anweden wollen.\n\npig_dummy_rec %&gt;%\n  prep() %&gt;%\n  juice() \n\n# A tibble: 412 × 7\n   pig_id   age   crp infected sex_male frailty_pre.frail frailty_robust\n    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1      1    61  22.4        1        1                 0              1\n 2      2    53  18.6        1        1                 0              1\n 3      3    66  18.8        0        0                 0              1\n 4      4    59  19.4        1        0                 0              1\n 5      5    63  21.6        1        1                 0              1\n 6      6    55  21.4        1        1                 0              1\n 7      7    49  19.0        1        1                 1              0\n 8      8    53  19.0        0        1                 0              1\n 9      9    58  21.9        1        0                 0              1\n10     10    57  21.0        1        1                 0              1\n# ℹ 402 more rows\n\n\nDie Dummycodierung verwandelt alle nominalen Spalten in mehrere \\(0/1\\) Spalten um. Das ermöglicht den Algorithmen auch mit nominalen Spalten eine Vorhersage zu machen."
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-zv",
    "href": "classification-pre-processing.html#sec-preprocess-zv",
    "title": "56  Data preprocessing",
    "section": "\n56.6 Zero Variance Spalten",
    "text": "56.6 Zero Variance Spalten\nEin häufiges Problem ist, dass wir manchmal Spalten in unseren Daten haben in denen nur ein Eintrag steht. Das heißt wir haben überall die gleiche Zahl oder eben das gelche Wort stehen. Das tritt häufiger auf, wenn wir uns riesige Datenmengen von extern herunterladen. Manchmal haben wir so viele Spalten, dass wir die Daten gr nicht richtig überblicken. Oder aber, wir haben nach einer Transformation nur noch die gleiche Zahl. Dagegen können wir filtern.\n\n\nMehr Information zu Zero Variance Filter und Near-Zero Variance Filter\nWir haben die Auswahl zwischen step_zv(), die Funktion entfernt Spalten mit einer Vaianz von Null. Das mag seltener vorkommen, als eine sehr kleine Varianz. Hier hilft die Funktion step_nzv(). Wir können beide Funktionen auf alle Arten von Prädiktoren anwenden, nur eben nicht gleichzeitig.\n\npig_zero_rec &lt;- pig_rec %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_nzv(all_predictors())\n\npig_zero_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_predictors()\n\n\nDa wir in unseren Daten mit den infizierten Ferkeln jetzt keine Spalten mit einer sehr kleinen Varianz haben, passiert auch nichts, wenn wir die Funktion auf unsere Daten anwenden würden. Demensprechend sparen wir uns an dieser Stelle auch die Datengenerierung."
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-standard",
    "href": "classification-pre-processing.html#sec-preprocess-standard",
    "title": "56  Data preprocessing",
    "section": "\n56.7 Standardisieren \\(\\mathcal{N}(0,1)\\)\n",
    "text": "56.7 Standardisieren \\(\\mathcal{N}(0,1)\\)\n\nIn dem Kapitel zu der Transformation von Daten haben wir ja schon von der Standardisierung gelesen und uns mit den gängigen Funktion beschäftigt. Deshalb hier nur kurz die Schritte und Funktionen, die wir mit den Rezepten machen können. Zum einen können wir nur die Daten mit der Funktion step_scale() skalieren, dass heißt auf eine Standardabweichung von 1 bringen. Oder aber zum anderen nutzen wir die Funktion scale_center() um die Daten alle auf einen Mittelwert von 0 zu schieben. Manchmal wollen wir nur den einen Schritt getrennt von dem anderen Schritt durchführen. Beide Schritte können wir dann einfach auf allen numerischen Prädiktoren durchführen.\n\n\nMehr Information zu Scaling Numeric Data sowie Centering Numeric Data und Center and Scale Numeric Data\n\npig_scale_center_rec &lt;- pig_rec %&gt;% \n  step_center(all_numeric_predictors()) %&gt;% \n  step_scale(all_numeric_predictors()) \n\npig_scale_center_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Centering for: all_numeric_predictors()\n\n\n• Scaling for: all_numeric_predictors()\n\n\nWenn wir aber auf eine getrennte Durchführung keine Lust haben, gibt es auch die etwas schief benannte Funktion step_normalize(), die beide Schritte kombiniert und uns damit die Daten auf eine Standardnormalverteilung transformiert. Ich persönlich nutze dann meist die zweite Variante, dann hat man alles in einem Schritt zusammen. Das hängt aber sehr vom Anwendungsfall ab und du musst dann schauen, was besser für dich und deine Daten dann passt.\n\npig_scale_center_rec &lt;- pig_rec %&gt;% \n  step_normalize(all_numeric_predictors()) \n\npig_scale_center_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\nJetzt können wir noch die Daten generieren und sehen, dass wir alle numerischen Spalten in eine Standardnormalverteilung transformiert haben. Wir runden hier nochmal alle numerischen Variablen, damit wir nicht so einen breiten Datensatz erhalten. Das hat jetzt aber eher was mit der Ausgabe hier auf der Webseite zu tun. Wir müssen nicht runden um die Daten dann zu verwenden.\n\npig_scale_center_rec %&gt;%\n  prep() %&gt;%\n  juice() %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 412 × 6\n   pig_id   age   crp sex    frailty   infected\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt;\n 1      1  0.22  1.62 male   robust           1\n 2      2 -1.55 -0.99 male   robust           1\n 3      3  1.32 -0.91 female robust           0\n 4      4 -0.23 -0.48 female robust           1\n 5      5  0.66  1.05 male   robust           1\n 6      6 -1.11  0.97 male   robust           1\n 7      7 -2.44 -0.76 male   pre-frail        1\n 8      8 -1.55 -0.76 male   robust           0\n 9      9 -0.45  1.27 female robust           1\n10     10 -0.67  0.62 male   robust           1\n# ℹ 402 more rows"
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-normal",
    "href": "classification-pre-processing.html#sec-preprocess-normal",
    "title": "56  Data preprocessing",
    "section": "\n56.8 Normalisieren \\([0; 1]\\)\n",
    "text": "56.8 Normalisieren \\([0; 1]\\)\n\nAuch bei der Normalisierung möchte ich wieder auf das Kapitel zum Transformation von Daten verweisen. In dem tidymodels Universum heißt dann das Normalisieren, also die Daten auf eine Spannweite zwischen 0 und 1 bringen, dann eben step_range(). Das ist natürlich dann schön generalisiert. Wir könnten uns auch andere Spannweiten überlegen, aber hier nehmen wir natürlich immer den Klassiker auf eine Spannweite \\([0; 1]\\). Unsere Daten liegen dann nach der Normalisierung mit der Funktion step_range() zwischen 0 und 1. Wir können die Normalisierung natürlich nur auf numerischen Variablen durchführen.\n\n\nMehr Information zu Scaling Numeric Data to a Specific Range\n\npig_range_rec &lt;- pig_rec %&gt;% \n  step_range(all_numeric_predictors(), min = 0, max = 1) \n\npig_range_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Range scaling to [0,1] for: all_numeric_predictors()\n\n\nAuch hier können wir dann die Daten generieren und uns einmal anschauen. Im Gegensatz zu der Standardisierung treten jetzt in unseren Spalten keine negativen Werte mehr auf. Wir runden hier ebenfalls nochmal alle numerischen Variablen, damit wir nicht so einen breiten Datensatz erhalten. Das hat jetzt aber eher was mit der Ausgabe hier auf der Webseite zu tun. Wir müssen nicht runden um die Daten dann zu verwenden.\n\npig_range_rec %&gt;%\n  prep() %&gt;%\n  juice() %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 412 × 6\n   pig_id   age   crp sex    frailty   infected\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt;\n 1      1  0.52  0.82 male   robust           1\n 2      2  0.17  0.34 male   robust           1\n 3      3  0.74  0.36 female robust           0\n 4      4  0.43  0.44 female robust           1\n 5      5  0.61  0.72 male   robust           1\n 6      6  0.26  0.7  male   robust           1\n 7      7  0     0.39 male   pre-frail        1\n 8      8  0.17  0.39 male   robust           0\n 9      9  0.39  0.76 female robust           1\n10     10  0.35  0.64 male   robust           1\n# ℹ 402 more rows"
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-impute",
    "href": "classification-pre-processing.html#sec-preprocess-impute",
    "title": "56  Data preprocessing",
    "section": "\n56.9 Imputieren von fehlenden Werten",
    "text": "56.9 Imputieren von fehlenden Werten\nIn dem Kapitel zur Imputation von fehlenden Werten haben wir uns mit verschiedenen Methoden zur Imputation von fehlenden Werten beschäftigt. Auch gibt es verschiedene Rezepte um die Imputation durchzuführen. Wir haben also wieder die Qual der Wahl welchen Algorithmus wir nutzen wollen. Da wir wieder zwischen numerischen und nominalen Variablen unterscheiden müssen, haben wir immer zwei Imputationsschritte. Ich mache es mir hier sehr leicht und wähle die mean Imputation für die numerischen Variablen aus und die mode Imputation für die nominalen Variablen. Das sind natürlich die beiden simpelsten Imputation die gehen. Ich würde dir empfehlen nochmal die Alternativen anzuschauen und vorab auf jeden Fall nochmal dir die fehlenden Daten zu visualisieren. Es macht auch hier keinen Sinn nicht vorhandene Spalten mit künstlichen Daten zu füllen.\n\n\nMehr Information zu Step Functions - Imputation\nDa wir es uns in diesem Schritt sehr einfach machen, nutzen wir die Funktionen step_impute_mean() auf allen numerischen Variablen und die Funktion step_impute_mode() auf alle nominalen Variablen. Es geht wie immer natürlich besser, das heißt auch komplexerer. Hier ist es auch wieder schwierig zu sagen, welche Methode die beste Methode zur Imputation von fehlenden Werten ist. Hier hilft es dann nichts, du musst dir die imputierten Daten anschauen.\n\npig_imp_rec &lt;- pig_rec %&gt;% \n  step_impute_mean(all_numeric_predictors()) %&gt;% \n  step_impute_mode(all_nominal_predictors())\n\npig_imp_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Mode imputation for: all_nominal_predictors()\n\n\nDann können wir uns auch schon die Daten generieren. Wir sehen, dass wir keine fehlenden Werte mehr in unseren Daten vorliegen haben. Wie immer können wir uns die gerundeten Daten dann einmal anschauen.\n\npig_imp_rec  %&gt;%\n  prep() %&gt;%\n  juice() %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\n# A tibble: 412 × 6\n   pig_id   age   crp sex    frailty   infected\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt;\n 1      1    61  22.4 male   robust           1\n 2      2    53  18.6 male   robust           1\n 3      3    66  18.8 female robust           0\n 4      4    59  19.4 female robust           1\n 5      5    63  21.6 male   robust           1\n 6      6    55  21.4 male   robust           1\n 7      7    49  19.0 male   pre-frail        1\n 8      8    53  19.0 male   robust           0\n 9      9    58  21.9 female robust           1\n10     10    57  21.0 male   robust           1\n# ℹ 402 more rows\n\n\nDie Imputationrezepte bieten sich natürlich auch für die ganz normale Statistik an. Du kannst ja dann mit den imputierten Daten rechnen was du möchtest. Wir nutzen die Daten hier ja nur im Kontext der Klassifikation. Es gingt natürlich auch die Daten für die lineare Regression zu nutzen."
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-discrete",
    "href": "classification-pre-processing.html#sec-preprocess-discrete",
    "title": "56  Data preprocessing",
    "section": "\n56.10 Kategorisierung",
    "text": "56.10 Kategorisierung\nManchmal wollen wir nicht mit numerischen Variablen arbeiten sondern uns nominale Variablen erschaffen. Das sollten wir eigentlich nicht so häufig tun, denn die numerischen Variablen haben meist mehr Informationen als nominale Variablen. Wir müssen dann ja unsere nominalen Daten dann wieder in Dummies umkodieren. Das sind dann zwei zusätzliche Schritte. Aber wie immer in der Datenanalyse, es gibt Fälle in denen es Sinn macht und wir eben keine numerischen Variablen haben wollen. Dann können wir eben die Funktion step_discretize() nutzen um verschiedene Gruppen oder bins (eng. Dosen) zu bilden. Das R Paket embed bietet noch eine Vielzahl an weiteren Funktionen für die Erstellung von kategorialen Variablen.\nEs kann natürlich sinnvoll sein aus einer numerischen Outcomevariablen eine binäre Outcomevariable zu erzeugen. Dann können wir wieder eine Klassifikation rechnen. Aber auch hier musst du überlegen, ob das binäre Outcome dann dem numerischen Outcome inhaltlich entspricht. Wir können natürlich aus dem numerischen Lichteinfall die binäre Variable wenig/viel Licht transformieren. Dann muss die neue binäre Variable aber auch zur Fragestellung passen. Oder aus Noten auf der Likert-Skala nur zwei Noten mit schlecht/gut erschaffen.\n\n\nMehr Information zu Step Functions - Discretization\nWir wollen jetzt die Spalten age und crp in mindestens drei gleich große Gruppen aufspalten. Wenn wir mehr Gruppen brauchen, dann werden es mehr Gruppen werden. Das wichtige ist hier, dass wir gleich große Gruppen haben wollen.\n\npig_discrete_rec &lt;- pig_rec %&gt;%\n  step_discretize(crp, age, min_unique = 3)\n\npig_discrete_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Discretize numeric variables from: crp, age\n\n\nUnd dann können wir uns auch schon die Daten generieren. Wir immer gibt es noch andere Möglichkeiten um aus einer numerischen Spalte eine nominale Spalte zu generieren. Du musst dann abgleichen, welche Variante dir am besten passt.\n\npig_discrete_tbl &lt;- pig_discrete_rec  %&gt;%\n  prep() %&gt;%\n  juice() \n\nWarning: Note that the options `prefix` and `labels` will be applied to all\nvariables\n\n\nWir sehen, dass wir dann jeweils vier bins erhalten mit gut 25% Beobachtungen in jedem bin. Wir können dann mit der neuen Variable weiterrechnen und zum Beispiel diese neue nominale Variable dann in eine Dummykodierung umwandeln. Hier siehst du, dass du gewisse Schritte in einem Rezept in der richtigen Reihenfolge durchführen musst.\n\npig_discrete_tbl %&gt;% pull(crp) %&gt;% tabyl()\n\n    .   n   percent\n bin1 104 0.2524272\n bin2 102 0.2475728\n bin3 103 0.2500000\n bin4 103 0.2500000\n\npig_discrete_tbl %&gt;% pull(age) %&gt;% tabyl()\n\n    .   n   percent\n bin1 122 0.2961165\n bin2 103 0.2500000\n bin3  99 0.2402913\n bin4  88 0.2135922"
  },
  {
    "objectID": "classification-pre-processing.html#sec-preprocess-corr",
    "href": "classification-pre-processing.html#sec-preprocess-corr",
    "title": "56  Data preprocessing",
    "section": "\n56.11 Korrelation zwischen Variablen",
    "text": "56.11 Korrelation zwischen Variablen\nAls einer der letzten Schritte für die Aufreinigung der Daten schauen wir uns die Korrelation an. Du kannst dir die Korrelation im Kapitel Kapitel 34 nochmal näher anlesen. Wie schon bei der Imputation kann ich nur davon abraten einfach so den Filter auf die Daten anzuwenden. Es ist besser sich die numerischen Variablen einmal zu visualisieren und die Korrelation einmal zu berechnen. Das blinde Filtern von Variablen macht auf jeden Fall keinen Sinn!\n\n\nMehr Information zu High Correlation Filter\nIn der Klassifikation müssen wir schauen, dass wir keine numerischen Variablen haben, die im Prinzip das gleiche Aussagen also hoch miteinander korreliert sind. Die Variablen müssen wir dann entfernen. Oder besser eine von den beiden Variablen. Wir können den Schritt mit der Funktion step_corr() durchführen und einen Threshold für die Entfernung von numerischen Variablen festlegen. Wir nehmen hier ein \\(\\rho = 0.5\\). Nochmal, das ist nicht sehr gut blind Vairablen zu entfernen. Schaue dir vorher einen paarweisen Korrelationsplot an und entscheide dann, ob du und welche Variablen du entfernen möchtest.\n\npig_corr_rec &lt;- pig_rec %&gt;% \n  step_corr(all_numeric_predictors(), threshold = 0.5)\n\npig_corr_rec \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Correlation filter on: all_numeric_predictors()\n\n\nDann können wir auch schon die Daten generieren. In unserem Fall wurde keine Variable entfernt. Die Korrelation untereinander ist nicht so groß. Wir runden hier wieder, damit sich die Tabelle nicht so in die Breite auf der Webseite entwickelt.\n\npig_corr_tbl &lt;- pig_corr_rec  %&gt;%\n  prep() %&gt;%\n  juice() %&gt;% \n  mutate(across(where(is.numeric), round, 2))"
  },
  {
    "objectID": "classification-pre-processing.html#beispiel-gummibärchendaten",
    "href": "classification-pre-processing.html#beispiel-gummibärchendaten",
    "title": "56  Data preprocessing",
    "section": "\n56.12 Beispiel Gummibärchendaten",
    "text": "56.12 Beispiel Gummibärchendaten\nSchauen wir uns ein Rezept einmal in einem Rutsch auf den Gummibärchendaten einmal an. Wir müssen natürlich erstmal alle nominalen Variablen auch als solche umwandeln. Wir erschaffen also die passenden Faktoren für das Geschlecht und den Lieblingsgeschmack. Dann erschaffen wir noch eine ID für die Studierenden. Am Ende wählen wir noch ein paar Spalten aus, damit wir nicht alle Variablen vorliegen haben. Sonst wird der endgültige Datensatz sehr breit. Wir entfernen dann noch alle Beobachtungen aus den Daten, die einen fehlenden Wert bei dem Geschlecht haben. Das machen wir immer für die Variable, die dann unser Outcome sein soll.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") %&gt;% \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked),\n         student_id = 1:n()) %&gt;% \n  select(student_id, gender, most_liked, age, semester, height) %&gt;%  \n  drop_na(gender)\n\nIn Tabelle 56.5 sehen wir dann die Daten nochmal vor dem Preprocessing dargestellt. Wir sind nicht an den ursprünglichen Daten interessiert, da wir nur die Spalte gender vorhersagen wollen. Wir wollen hier keine Effekt schätzen oder aber Signifikanzen berechnen. Unsere Features dienen nur der Vorhersage des Labels. Wie die Features zahlenmäßig beschaffen sind, ist uns egal.\n\n\n\n\nTabelle 56.5— Auszug aus dem Daten zu den Gummibärchendaten.\n\nstudent_id\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\n1\nm\nlightred\n35\n10\n193\n\n\n2\nw\nyellow\n21\n6\n159\n\n\n3\nw\nwhite\n21\n6\n159\n\n\n4\nw\nwhite\n36\n10\n180\n\n\n5\nm\nwhite\n22\n3\n180\n\n\n6\nm\nwhite\nNA\nNA\nNA\n\n\n…\n…\n…\n…\n…\n…\n\n\n557\nw\nyellow\n14\n0\n173\n\n\n558\nw\nnone\n11\n0\n159\n\n\n577\nw\ndarkred\n12\n0\n158\n\n\n578\nw\ndarkred\n12\n0\n158\n\n\n579\nw\ndarkred\n12\n0\n156\n\n\n580\nw\ndarkred\n12\n0\n158\n\n\n\n\n\n\nWir erschaffen uns nun das Rezept in dem wie definieren, dass das gender unser Label ist und der Rest der Vairablen unsere Features. Da wir noch die Spalte student_id haben, geben wir dieser Spalte noch die Rolle ID. Wir können dann in den Rezeptschritten dann immer diese Rolle ID aus dem Prozess der Transformation ausschließen.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_tbl) %&gt;% \n  update_role(student_id, new_role = \"ID\")\n\ngummi_rec %&gt;% summary()\n\n# A tibble: 6 × 4\n  variable   type      role      source  \n  &lt;chr&gt;      &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 student_id &lt;chr [2]&gt; ID        original\n2 most_liked &lt;chr [3]&gt; predictor original\n3 age        &lt;chr [2]&gt; predictor original\n4 semester   &lt;chr [2]&gt; predictor original\n5 height     &lt;chr [2]&gt; predictor original\n6 gender     &lt;chr [3]&gt; outcome   original\n\n\nUnd dann haben wir hier alle Schritte einmal zusammen in einem Block. Wir imputieren die fehlenden Werte für die numerischen und nominalen Variablen getrennt. Dann verwandeln wir das Semester in mindestens vier Gruppen. Im nächsten Schritt werden dann alle numerischen Variablen auf eine Spannweite von \\([0;1]\\) gebracht. Wir erschaffen dann noch die Dummies für die nominalen Daten. Am Ende wollen wir dann alle Variablen mit fast keiner Varianz entfernen. Wir wollen dann immer die Spalte ID aus den Schritten ausschließen. Wir machen das mit der Funktion has_role() und dem - vor der Funktion. Damit schließen wir die Rolle ID aus dem Transformationsschritt aus.\n\ngummi_full_rec &lt;- gummi_rec %&gt;% \n  step_impute_mean(all_numeric_predictors(), -has_role(\"ID\")) %&gt;% \n  step_impute_bag(all_nominal_predictors(), -has_role(\"ID\")) %&gt;% \n  step_discretize(semester, num_breaks = 3, min_unique = 4) %&gt;% \n  step_range(all_numeric_predictors(), min = 0, max = 1, -has_role(\"ID\")) %&gt;% \n  step_dummy(all_nominal_predictors(), -has_role(\"ID\")) %&gt;% \n  step_nzv(all_predictors(), -has_role(\"ID\"))\n\ngummi_full_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors(), -has_role(\"ID\")\n\n\n• Bagged tree imputation for: all_nominal_predictors(), -has_role(\"ID\")\n\n\n• Discretize numeric variables from: semester\n\n\n• Range scaling to [0,1] for: all_numeric_predictors(), -has_role(\"ID\")\n\n\n• Dummy variables from: all_nominal_predictors(), -has_role(\"ID\")\n\n\n• Sparse, unbalanced variable filter on: all_predictors(), -has_role(\"ID\")\n\n\nDann können wir wieder unsere Daten generieren. Ich runde hier wieder, da wir schnell sehr viele Kommastellen produzieren. In der Anwendung machen wir das natürlich dann nicht.\n\ngummi_class_tbl &lt;- gummi_full_rec %&gt;%\n  prep() %&gt;%\n  juice() %&gt;% \n  mutate(across(where(is.numeric), round, 2)) \n\nIn der Tabelle 56.6 können wir uns die transformierten Daten einmal anschauen. Wir sehen das zum einen die Variable student_id nicht transformiert wurde. Alle numerischen Spalten sind auf einer Spannweite zwischen 0 und 1. Das Geschlecht wurde nicht transformiert, da wir das Geschlecht ja als Outcome festgelegt haben. Dann kommen die Dummykodierungen für die nominalen Spalten des Lieblingsgeschmack und des Semesters.\n\n\n\n\nTabelle 56.6— Der transformierte Gummibärchendatensatz nach der Anwendung des Rezepts.\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudent_id\nage\nheight\ngender\nmost_liked_white\nmost_liked_green\nmost_liked_darkred\nmost_liked_none\nsemester_bin2\nsemester_bin3\n\n\n\n1\n0.48\n0.79\nm\n0\n0\n0\n0\n0\n1\n\n\n2\n0.2\n0.21\nw\n0\n0\n0\n0\n0\n1\n\n\n3\n0.2\n0.21\nw\n1\n0\n0\n0\n0\n1\n\n\n4\n0.5\n0.57\nw\n1\n0\n0\n0\n0\n1\n\n\n5\n0.22\n0.57\nm\n1\n0\n0\n0\n1\n0\n\n\n6\n0.25\n0.5\nm\n1\n0\n0\n0\n1\n0\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n517\n0.06\n0.45\nw\n0\n0\n0\n0\n0\n0\n\n\n518\n0\n0.21\nw\n0\n0\n0\n1\n0\n0\n\n\n519\n0.02\n0.19\nw\n0\n0\n1\n0\n0\n0\n\n\n520\n0.02\n0.19\nw\n0\n0\n1\n0\n0\n0\n\n\n521\n0.02\n0.16\nw\n0\n0\n1\n0\n0\n0\n\n\n522\n0.02\n0.19\nw\n0\n0\n1\n0\n0\n0\n\n\n\n\n\n\nBis hierher haben wir jetzt die Rezepte nur genutzt um uns die Daten aufzuarbeiten. Das ist eigentlich nur ein Schritt in der Klassifikation. Mit der Funktion workflow() können wir dann Rezepte mit Algorithmen verbinden. Dann nutzen wir die Funktion fit() um verschiedene Daten auf den Workflow anzuwenden. Das musst du aber nicht tun. Du kannst die Rezepte hier auch verwenden um deine Daten einfach aufzuarbeiten und dann eben doch ganz normale Statistik drauf zu rechnen."
  },
  {
    "objectID": "classification-model-compare.html#genutzte-r-pakete",
    "href": "classification-model-compare.html#genutzte-r-pakete",
    "title": "57  Vergleich von Algorithmen",
    "section": "\n57.1 Genutzte R Pakete",
    "text": "57.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor, xgboost, ranger, kknn,\n               see, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\nconflict_prefer(\"set_names\", \"magrittr\")\n##\nset.seed(20234534)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-model-compare.html#daten",
    "href": "classification-model-compare.html#daten",
    "title": "57  Vergleich von Algorithmen",
    "section": "\n57.2 Daten",
    "text": "57.2 Daten\nIn diesem Kapitel wolle wir uns aber mal auf einen echten Datensatz anschauen und sehen wie sich drei Algorithmen auf diesem Daten so schlagen. Welcher Algorithmus ist am besten für die Klassifikation geeignet? Wir nutzen daher hier einmal als echten Datensatz den Gummibärchendatensatz. Als unser Label nehmen wir das Geschlecht gender. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir als Prädiktoren die Spalten most_liked, age, semester, und height mit in unsere Analysedaten.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") %&gt;% \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) %&gt;% \n  select(gender, most_liked, age, semester, height) %&gt;% \n  drop_na(gender)\n\nWir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein NA in der Variable gender haben. Wir haben dann insgesamt \\(n = 522\\) Beobachtungen vorliegen. In Tabelle 61.3 sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.\n\n\n\n\nTabelle 57.1— Auszug aus dem Daten zu den Gummibärchendaten.\n\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\nm\nlightred\n35\n10\n193\n\n\nw\nyellow\n21\n6\n159\n\n\nw\nwhite\n21\n6\n159\n\n\nw\nwhite\n36\n10\n180\n\n\nm\nwhite\n22\n3\n180\n\n\nm\nwhite\nNA\nNA\nNA\n\n\n…\n…\n…\n…\n…\n\n\nw\nyellow\n14\n0\n173\n\n\nw\nnone\n11\n0\n159\n\n\nw\ndarkred\n12\n0\n158\n\n\nw\ndarkred\n12\n0\n158\n\n\nw\ndarkred\n12\n0\n156\n\n\nw\ndarkred\n12\n0\n158\n\n\n\n\n\n\nUnsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.\n\ngummi_data_split &lt;- initial_split(gummi_tbl, prop = 3/4)\n\nWir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluß einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.\n\ngummi_train_data &lt;- training(gummi_data_split)\ngummi_test_data  &lt;- testing(gummi_data_split)\n\nNachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht gender als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem . Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_train_data) %&gt;% \n  step_impute_mean(all_numeric_predictors()) %&gt;% \n  step_impute_bag(all_nominal_predictors()) %&gt;% \n  step_range(all_numeric_predictors(), min = 0, max = 1) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_nzv(all_predictors())\n\ngummi_rec %&gt;% summary()\n\n# A tibble: 5 × 4\n  variable   type      role      source  \n  &lt;chr&gt;      &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 most_liked &lt;chr [3]&gt; predictor original\n2 age        &lt;chr [2]&gt; predictor original\n3 semester   &lt;chr [2]&gt; predictor original\n4 height     &lt;chr [2]&gt; predictor original\n5 gender     &lt;chr [3]&gt; outcome   original\n\n\nIm Folgenden vergleichen wir einmal drei Algorithmen miteinander. Daher halten wir den Code für die Durchführung sehr kurz."
  },
  {
    "objectID": "classification-model-compare.html#k-nn-algorithm",
    "href": "classification-model-compare.html#k-nn-algorithm",
    "title": "57  Vergleich von Algorithmen",
    "section": "\n57.3 \\(k\\)-NN Algorithm",
    "text": "57.3 \\(k\\)-NN Algorithm\n\n\n\n\n\n\nHuch, der Code ist aber sehr kurz…\n\n\n\nIn diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den \\(k\\)-NN Algorithmus wissen willst, schaue bitte in Kapitel 58.\n\n\nFür den \\(k\\)-NN Algorithmus nutzen wir \\(k=11\\) Nachbarn. Mehr brauchen wir hier nicht angeben.\n\nknn_mod &lt;- nearest_neighbor(neighbors = 11) %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\") \n\nDann nehmen wir das Modell für den \\(k\\)-NN Algorithmus und verbinden das Modell mit dem Rezept für die Gummibärchendaten in einem Workflow.\n\nknn_wflow &lt;- workflow() %&gt;% \n  add_model(knn_mod) %&gt;% \n  add_recipe(gummi_rec)\n\nNun können wir auch schon den Fit des Modells rechnen und in einem Rutsch den Fit auch gleich auf die Testdaten anwenden.\n\nknn_aug &lt;- knn_wflow %&gt;% \n  parsnip::fit(gummi_train_data) %&gt;% \n   augment(gummi_test_data)\n\nMehr wollen wir hier auch nicht. Wir brauchen nur die Prädiktion, da wir hier ja nur das Konzept der Modellvergleiche einmal durchgehen wollen."
  },
  {
    "objectID": "classification-model-compare.html#random-forest",
    "href": "classification-model-compare.html#random-forest",
    "title": "57  Vergleich von Algorithmen",
    "section": "\n57.4 Random Forest",
    "text": "57.4 Random Forest\n\n\n\n\n\n\nHuch, der Code ist aber sehr kurz…\n\n\n\nIn diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den Random Forest Algorithmus wissen willst, schaue bitte in Kapitel 59.4.\n\n\nFür den Random Forest Algorithmus nutzen wir drei Variablen je Baum (mtry = 3), mindestens zehn Beobachtungen je Knoten (min_n = 10) sowie eintausend gewachsene Bäume in unserem Wald (trees = 1000). Mehr brauchen wir hier nicht angeben.\n\nranger_mod &lt;- rand_forest(mtry = 3, min_n = 10, trees = 1000) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"classification\")\n\nDann nehmen wir das Modell für den Random Forest Algorithmus und verbinden das Modell mit dem Rezept für die Gummibärchendaten in einem Workflow.\n\nranger_wflow &lt;- workflow() %&gt;% \n  add_model(ranger_mod) %&gt;% \n  add_recipe(gummi_rec)\n\nNun können wir auch schon den Fit des Modells rechnen und in einem Rutsch den Fit auch gleich auf die Testdaten anwenden.\n\nranger_aug &lt;- ranger_wflow %&gt;% \n  parsnip::fit(gummi_train_data) %&gt;% \n  augment(gummi_test_data ) \n\nMehr wollen wir hier auch nicht von dem Random Forest Algorithmus. Wir brauchen nur die Prädiktion, da wir hier ja nur das Konzept der Modellvergleiche einmal durchgehen wollen."
  },
  {
    "objectID": "classification-model-compare.html#xgboost",
    "href": "classification-model-compare.html#xgboost",
    "title": "57  Vergleich von Algorithmen",
    "section": "\n57.5 xgboost",
    "text": "57.5 xgboost\n\n\n\n\n\n\nHuch, der Code ist aber sehr kurz…\n\n\n\nIn diesem Teil halte ich den R Code sehr kurz, wenn du mehr über den xgboost Algorithmus wissen willst, schaue bitte in Kapitel 59.5.\n\n\nFür den xgboost Algorithmus nutzen wir drei Variablen je Baum (mtry = 3), mindestens zehn Beobachtungen je Knoten (min_n = 10) sowie eintausend gewachsene Bäume in unserem Wald (trees = 1000). Mehr brauchen wir hier nicht angeben.\n\nxgboost_mod &lt;- boost_tree(mtry = 3, min_n = 10, trees = 1000) %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"classification\")\n\nDann nehmen wir das Modell für den xgboost Algorithmus und verbinden das Modell mit dem Rezept für die Gummibärchendaten in einem Workflow.\n\nxgboost_wflow &lt;- workflow() %&gt;% \n  add_model(xgboost_mod) %&gt;% \n  add_recipe(gummi_rec)\n\nNun können wir auch schon den Fit des Modells rechnen und in einem Rutsch den Fit auch gleich auf die Testdaten anwenden.\n\nxgboost_aug &lt;- xgboost_wflow %&gt;% \n  parsnip::fit(gummi_train_data) %&gt;% \n  augment(gummi_test_data ) \n\nDas war jetzt der dritte und letzte Algorithmus. Wir brauchen auch hier nur die Prädiktion, da wir hier ja nur das Konzept der Modellvergleiche einmal durchgehen wollen."
  },
  {
    "objectID": "classification-model-compare.html#sec-class-model-compare",
    "href": "classification-model-compare.html#sec-class-model-compare",
    "title": "57  Vergleich von Algorithmen",
    "section": "\n57.6 Vergleich der Modelle",
    "text": "57.6 Vergleich der Modelle\nIn der folgenden Liste haben wir einmal alle vorhergesagten Werte der drei Algorithmen zusammengefügt. Wir können jetzt auf der Liste aug_lst mit der Funktion map() aus dem R Paket purrr schnell rechnen. Anstatt für jedes der Objekte in der Liste einzeln den Code anzugeben, können wir den Code über die Funktion map() bündeln.\n\naug_lst &lt;- lst(knn = knn_aug,\n               rf = ranger_aug,\n               xgboost = xgboost_aug)\n\nIm folgenden Schritt berechnen wir für alle Algorithmen die Konfusionsmatrix als eine 2x2 Tabelle. Wir schauen uns gleich einmal die Konfusionsmatrix nur für den xgboost Algorithmus an. Auf der Konfusionsmatrix können wir viele Gütekriterien für die Klassifikation berechnen.\n\nconf_mat_lst &lt;- aug_lst %&gt;% \n  map(~conf_mat(.x, gender, .pred_class))\n\nUnd diese große Anzahl an Gütekriterien berechnen wir dann auch gleich. Die Funktion summary() gibt uns die Gütekriterien für alle Algorithmen wieder. Wir müssen dann noch etwas aufräumen und die Wiedergaben dann passend zusammenfassen, so dass wir eine schöne Tabelle wiedergegeben kriegen. So das sind jetzt aber ganz schön viele Maßzahlen.\n\nconf_mat_lst %&gt;% \n  map(summary) %&gt;% \n  map(~select(.x, .metric, .estimate)) %&gt;% \n  reduce(left_join, by = \".metric\") %&gt;% \n  set_names(c(\"metric\", \"knn\", \"rf\", \"xboost\")) %&gt;% \n  mutate(across(where(is.numeric), round, 3))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 13 × 4\n   metric                 knn    rf xboost\n   &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 accuracy             0.779 0.771  0.794\n 2 kap                  0.557 0.542  0.586\n 3 sens                 0.778 0.794  0.762\n 4 spec                 0.779 0.75   0.824\n 5 ppv                  0.766 0.746  0.8  \n 6 npv                  0.791 0.797  0.789\n 7 mcc                  0.557 0.543  0.587\n 8 j_index              0.557 0.544  0.585\n 9 bal_accuracy         0.779 0.772  0.793\n10 detection_prevalence 0.489 0.511  0.458\n11 precision            0.766 0.746  0.8  \n12 recall               0.778 0.794  0.762\n13 f_meas               0.772 0.769  0.78 \n\n\nUm jetzt zu verstehen, wie scih diese Maßzahl jetzt alle berechnen ziehen wir uns einmal die Konfusionsmatrix für den xgboost Algorithmus aus dem Objekt conf_mat_lst raus. Wir sehen, dass wir die meisten Männer und Frauen richtig klassifiziert haben. Neben dieser Information, brauchen wir noch die Informationen der Randsummen.\n\npluck(conf_mat_lst, \"xgboost\")\n\n          Truth\nPrediction  m  w\n         m 48 12\n         w 15 56\n\n\nWir können berechnen, dass wir in den Testdaten (Truth) dann 58 Männer vorliegen haben sowie 61 Frauen. In den vorhergesagten Daten (Prediction) haben wir dann 63 Männer und 56 Frauen. Die beiden Zahlen brauchen wir noch und daher ergänzen wir diese Zahlen dann auch in der Tabelle 57.2 (b).\n\n\nTabelle 57.2— Die theoretische Konfusionsmatrix sowie die ausgefüllte Konfusionmatrix nach dem xgboost Algorithmus.\n\n\n\n\n(a) Die Konfusionsmatrix als eine 2x2 Tabelle oder Vierfeldertafel\n\n\n\n\nTruth\n\n\n\n\n\n\\(Positiv\\)\n\n\\(Negativ\\) (0)\n\n\n\n\n\n\\((PP)\\) (1)\n\n\\((PN)\\) (0)\n\n\nPrädiktion\n\n\\(Positiv\\) (1)\n\\(TP\\)\n\\(FP\\)\n\n\n\n\n\\((P)\\) (1)\n\n\n\n\n\n\n\\(Negativ\\) (0)\n\\(FN\\)\n\\(TN\\)\n\n\n\n\n\\((N)\\) (0)\n\n\n\n\n\n\n\n\n(b) Die Konfusionsmatrix für den xgboost Algorithmus.\n\n\n\n\nTruth\n\n\n\n\n\n\n\\(Positiv\\) (m)\n\n\\(Negativ\\) (w)\n\n\n\n\n\n\\((PP = 58)\\) (m)\n\n\\((PN = 61)\\) (w)\n\n\nPrädiktion\n\n\\(Positiv\\) (m)\n\\(51\\)\n\\(12\\)\n\n\n\n\n\\((P = 63)\\) (m)\n\n\n\n\n\n\n\\(Negativ\\) (w)\n\\(7\\)\n\\(49\\)\n\n\n\n\n\\((N = 56)\\) (w)\n\n\n\n\n\n\n\n\nWir können auch noch ganz viel mehr Beurteilungskriterien für die Klassifikation in einer Konfusionmatrix berechnen lassen. Wir wollen jetzt aber nur die dreizehn Beurteilungskriterien, die wir von der Funktion summary() berechnet kriegen, einmal durchgehen. Die Frage was du jetzt von den Maßzahlen alles berichten sollst, hängt wiederum davon ab, wenn du die Maßzahlen berichten willst. Die Accuarcy und die ROC Kurven sind sicherlich die wichtigsten Maßzahlen. Der Rest geht eher in die optionale Richtung.\nAccuarcy\nDie accuracy (deu. Genauigkeit, nicht verwendet) ist der Anteil der Label, die richtig vorhergesagt werden. Das Problem bei der Verwendung der Genauigkeit als Hauptgütekriterium besteht darin, dass sie bei einem starken Klassenungleichgewicht nicht gut funktioniert.\nKappa\nDas kap beschreibt Kappa und damit ein ähnliches Maß wie die accuracy. Dabei wird aber Kappa durch die accuarcy normalisiert, die allein durch Zufall zu erwarten wäre. Damit ist Kappa sehr nützlich, wenn eine oder mehrere Klassen große Häufigkeitsverteilungen haben.\nSensitivität\nDie sens beschreibt die Sensitivität oder die true positive rate (TPR). Eine Methode die erkrankte Personen sehr zuverlässig als krank (1) erkennt hat eine hohe Sensitivität. Das heißt, sie übersieht kaum erkrankte (1) Personen.\n\\[\n\\mbox{Sensitivität} = \\mbox{sens} = \\cfrac{TP}{TP + FN} = \\cfrac{51}{51 + 7} = 0.879\n\\]\nSpezifität\nDie spec beschreibt die Spezifität oder die true negative rate (TNR). Eine Methode die gesunde Personen zuverlässig als gesund (0) einstuft, hat eine hohe Spezifität. Das heißt, die Methode liefert in der Regel nur bei Erkrankten ein positives Ergebnis.\n\\[\n\\mbox{Spezifität} = \\mbox{spec} = \\cfrac{TN}{TN + FP} = \\cfrac{49}{49 + 12} = 0.803\n\\]\nPositiver prädiktiver Wert\nDer ppv beschreibt den positiven prädiktiven Wert (eng. positive predictive value).\n\\[\n\\mbox{Positiver prädiktiver Wert} = \\mbox{ppv} = \\cfrac{TP}{PP} = \\cfrac{51}{63} = 0.81\n\\]\nNegativer prädiktiver Wert\nDer npv beschreibt den negativen prädiktiven Wert (eng. negative predictive value).\n\\[\n\\mbox{Negativer prädiktiver Wert} = \\mbox{npv} = \\cfrac{TN}{PN} = \\cfrac{49}{56} = 0.875\n\\]\nMatthews Korrelationskoeffizienten\nDas mcc beschreibt den Matthews Korrelationskoeffizienten (eng. Matthews correlation coefficient). Der Matthews-Korrelationskoeffizient (MCC) ist ein zuverlässiger statistischer Wert, der nur dann einen hohen Wert hat, wenn die Vorhersage in allen vier Kategorien der Konfusionsmatrix (richtig positiv, falsch negativ, richtig negativ und falsch positiv) gute Ergebnisse erzielt. Wir berechnen den Wert hier jetzt nicht, da die Formel insgesamt acht zusammengesetzte Terme aus der Konfusionsmatrix beinhaltet. Für die Berechnung einmal beim Matthews correlation coefficient nachlesen oder aber auch Chicco und Jurman (2020) berücksichtigen.\nYouden-J-Statistik\nDer j_index beschreibt die Youden-J-Statistik und ist definiert als \\(J = sens + spec - 1\\). Wenn wir also eine hohe Sensitivität und eine hohe Spezifität haben dann nähert sich \\(J\\) der Eins an.\n\\[\n\\mbox{Youden-J} = \\mbox{j index} = sens + spec - 1 = 0.879 + 0.803 - 1  = 0.682\n\\]\nBalancierte Accuarcy\nDie bal_accuracy beschreibt die balancierte accuarcy und wird hier in der Funktion als der Durchschnitt von Sensitivität und Spezifität berechnet. Leider hat die balancierte Accuarcy mit der Accuarcy wie oben beschrieben weniger zu tun.\n\\[\n\\mbox{Balanced accuracy} = \\cfrac{TPR + TNR}{2} = \\cfrac{0.879 + 0.803}{2} = 0.841\n\\]\nEntdeckungsprävalenz\nDie detection_prevalence Die Entdeckungsprävalenz (eng. detection prevalence) ist definiert als die Anzahl der vorhergesagten positiven Ereignisse (sowohl richtig als auch falsch positiv) geteilt durch die Gesamtzahl der Vorhersagen.\n\\[\n\\mbox{Entdeckungsprävalenz} = \\cfrac{TP + FP}{TP + FP + FN + TN} = \\cfrac{51 + 12}{51 + 12 + 7 + 49} = 0.529\n\\]\nPrecision und Recall\n\n\nAbbildung 57.1— Visualisierung der Berechung der Precision und des Recalls anhand von einem Venndiagramm.\n\nDie precision Bei der binären Klassifizierung ist die precision der positiv prädiktive Wert. Damit ist die precision die Anzahl der richtig positiven Ergebnisse geteilt durch die Anzahl aller positiven Ergebnisse, einschließlich derer, die nicht richtig erkannt wurden.\nPräzision hilft, wenn die Kosten für falsch positive Ergebnisse hoch sind. Nehmen wir einmal an wir wollen Hautkrebs erkennen. Wenn wir ein Modell mit sehr geringer Präzision haben, teilen wir vielen Patienten mit, dass sie ein Melanom haben, und dies schließt einige Fehldiagnosen ein. Es stehen viele zusätzliche Tests und Stress für die Patienten auf dem Spiel. Wenn die Fehlalarme zu hoch sind, lernen diejenigen, die die Ergebnisse überwachen, sie zu ignorieren, nachdem sie mit Fehlalarmen bombardiert wurden.\n\\[\n\\mbox{Precision} = \\mbox{Positiver prädiktiver Wert}  = \\cfrac{TP}{PP} = \\cfrac{51}{63} = 0.81\n\\]\nDer recall Bei der binären Klassifizierung ist der recall die Sensitivität. Damit ist der recall die Anzahl der tatsächlich positiven Ergebnisse geteilt durch die Anzahl aller Ergebnisse, die als positiv hätten identifiziert werden müssen.\nDer Recall hilft, wenn die Kosten für falsch negative Ergebnisse hoch sind. Was ist, wenn wir einfallende Atomraketen erkennen müssen? Ein falsches Negativ hat verheerende Folgen. Versteh es falsch und wir alle sterben. Wenn falsche Negative häufig sind, wirst du von dem getroffen, was du vermeiden möchten. Ein falsch Negatives ist, wenn du sich entscheidest, das Geräusch eines Zweigs zu ignorieren, der in einem dunklen Wald bricht, und du dann von einem Bären gefressen wirst. Ein falsch Positives Ereignis wäre dann, dass du die ganze Nacht schlaflos in deinem Zelt in kaltem Schweiß aufbleibst und jedem Durcheinander im Wald zuhörst, nur um am nächsten Morgen zu erkennen, dass diese Geräusche von einem Waschbären gemacht wurden. Auch kein Spaß.\n\\[\n\\mbox{Recall} = \\mbox{Sensitivität} = \\cfrac{TP}{TP + FN} = \\cfrac{51}{51 + 7} = 0.879\n\\]\nF\\(_1\\) Score\nDie f_meas beschreibt den F\\(_1\\) Score und damit das harmonische Mittel aus Precision und Recall. Der höchstmögliche Wert eines F\\(_1\\) Scores ist 1, was perfekte Präzision und Recall bedeutet, und der niedrigstmögliche Wert ist 0, wenn sowohl Präzision als auch Recall null sind. Das heißt, ein guter F1-Score bedeutet, dass du niedrige Fehlalarme und niedrige Falschnegative hast, sodass du echte Ereignisse oder Bedrohungen korrekt identifizieren und nicht durch Fehlalarme gestört wirst.\n\\[\nF_1 = \\cfrac{2 \\cdot TP}{2 \\cdot TP + FP + FN} = \\cfrac{2 \\cdot 51}{2 \\cdot 51 + 12 + 7} = 0.843\n\\]\nROC & Precision recall Kurven\nWenn wir von der Visualisierung von Klassifikationsergebnissen sprechen, dann kommen wir an der ROC Kurve und der PR Kurve nicht vorbei. Beide Kurven lassen sich ziemlich zügig erstellen. Wir kennen ja schon die Funktion roc_curve() für die ROC Kurve.\nIn dem Kapitel 29 erfährst du mehr darüber was eine ROC Kurve ist und wie du die ROC Kurve interpretieren kannst.\n\nroc_tbl &lt;- aug_lst %&gt;% \n  map(~roc_curve(.x, gender, .pred_w, event_level = \"second\")) %&gt;% \n  bind_rows(.id = \"model\")\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at &lt;https://github.com/tidymodels/yardstick/issues&gt;.\n\n\nDie PR Kurve, für die Darstellung der Precision und des Recalls können wir dann die Funktion pr_curve() nutzen. Im Gegesatz zu der ROC Kurve wollen wir das die PR Kurve erstmal waagerecht verlauft und am Ende senkrecht nach unten fällt. Die Spitzen und Zacken in der Kurve sind normal und hat mit der Berechnung der beiden Werte zu tun. Wir wollen aber auch hier eine möglichst große Fläche unter der Kurve haben.\n\npr_tbl &lt;- aug_lst %&gt;% \n  map(~pr_curve(.x, gender, .pred_w, event_level = \"second\")) %&gt;% \n  bind_rows(.id = \"model\")\n\nIn der Abbildung 57.2 sind die ROC Kurven und die PR Kurven für die drei Algorithmen nochmal dargestellt. Zum einen sehen wir, dass wir nicht das beste Modell haben. Alle Modelle laufen übereinander und sind sich recht ähnlich. Das Bild wiederholt sich dann auch bei der PR Kurve wie bei der ROC Kurve. Dennoch sind die Algorithmen einigermaßen gut, denn wir haben ja weder eine Kreuzvalidierung noch ein Tuning durchgeführt. Wir bewerten die Modelle als gut, da die Flächen unter der Kurve relativ groß sind. Wenn es ein Modell gibt, was im Verhältnis zu den anderen Modellen abfällt, dann ist es das \\(k\\)-NN Modell. Das \\(k\\)-NN Modell hat einen starken Abfall zu Beginn der PR-Kurve.\n\nroc_tbl %&gt;% \n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  theme_minimal() +\n  geom_path() +\n  geom_abline(lty = 3) + \n  scale_color_okabeito()\n\npr_tbl %&gt;% \n  ggplot(aes(x = recall, y = precision, col = model)) + \n  theme_minimal() +\n  geom_path() +\n  scale_color_okabeito()\n\n\n\n\n\n(a) Receiver operator curve.\n\n\n\n\n\n(b) Precision recall curve.\n\n\n\nAbbildung 57.2— Darstellung der Vorhersagegüte der drei Modelle k-NN, ranger und xgboost."
  },
  {
    "objectID": "classification-model-compare.html#referenzen",
    "href": "classification-model-compare.html#referenzen",
    "title": "57  Vergleich von Algorithmen",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nChicco, Davide, und Giuseppe Jurman. 2020. „The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation“. BMC genomics 21 (1): 1–13."
  },
  {
    "objectID": "classification-knn.html#genutzte-r-pakete",
    "href": "classification-knn.html#genutzte-r-pakete",
    "title": "58  \\(k\\) nearest neighbor",
    "section": "\n58.1 Genutzte R Pakete",
    "text": "58.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, see,\n               caret, kknn, MachineShop, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\nconflict_prefer(\"fit\", \"parsnip\")\nconflict_prefer(\"contr.dummy\", \"kknn\")\n##\nset.seed(2025429)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-knn.html#daten",
    "href": "classification-knn.html#daten",
    "title": "58  \\(k\\) nearest neighbor",
    "section": "\n58.2 Daten",
    "text": "58.2 Daten\nIn dieser Einführung nehmen wir die infizierten Ferkel als Beispiel um einmal die verschiedenen Verfahren zu demonstrieren. Ich füge hier noch die ID mit ein, die nichts anderes ist, als die Zeilennummer. Dann habe ich noch die ID an den Anfang gestellt.\n\npig_tbl &lt;- read_excel(\"data/infected_pigs.xlsx\") %&gt;% \n  mutate(pig_id = 1:n(),\n         infected = as_factor(infected)) %&gt;% \n  select(pig_id, infected, everything())  \n\nIn Tabelle 58.1 siehst du nochmal einen Auschnitt aus den Daten. Wir haben noch die ID mit eingefügt, damit wir einzelne Beobachtungen nachvollziehen können.\n\n\n\n\nTabelle 58.1— Auszug aus dem Daten zu den kranken Ferkeln.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npig_id\ninfected\nage\nsex\nlocation\nactivity\ncrp\nfrailty\nbloodpressure\nweight\ncreatinin\n\n\n\n1\n1\n61\nmale\nnortheast\n15.31\n22.38\nrobust\n62.24\n19.05\n4.44\n\n\n2\n1\n53\nmale\nnorthwest\n13.01\n18.64\nrobust\n54.21\n17.68\n3.87\n\n\n3\n0\n66\nfemale\nnortheast\n11.31\n18.76\nrobust\n57.94\n16.76\n3.01\n\n\n4\n1\n59\nfemale\nnorth\n13.33\n19.37\nrobust\n56.15\n19.05\n4.35\n\n\n5\n1\n63\nmale\nnorthwest\n14.71\n21.57\nrobust\n55.38\n18.44\n5.27\n\n\n6\n1\n55\nmale\nnorthwest\n15.81\n21.45\nrobust\n60.29\n18.42\n4.78\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n407\n1\n54\nfemale\nnorth\n11.82\n21.5\npre-frail\n55.32\n19.75\n3.92\n\n\n408\n0\n56\nmale\nwest\n13.91\n20.8\nfrail\n58.37\n17.28\n7.44\n\n\n409\n1\n57\nmale\nnorthwest\n12.49\n21.95\npre-frail\n56.66\n16.86\n2.44\n\n\n410\n1\n61\nmale\nnorthwest\n15.26\n23.1\nrobust\n57.18\n15.55\n3.08\n\n\n411\n0\n59\nfemale\nnorth\n13.13\n20.23\nrobust\n56.64\n18.6\n3.41\n\n\n412\n1\n63\nfemale\nnorth\n10.01\n19.89\nrobust\n57.46\n18.6\n4.2\n\n\n\n\n\n\nGehen wir jetzt mal die Wörter und Begrifflichkeiten, die wir für das maschinelle Lernen mit dem \\(k\\)-NN Algorithmus später brauchen einmal durch."
  },
  {
    "objectID": "classification-knn.html#k-nn-theoretisch",
    "href": "classification-knn.html#k-nn-theoretisch",
    "title": "58  \\(k\\) nearest neighbor",
    "section": "\n58.3 \\(k\\)-NN theoretisch",
    "text": "58.3 \\(k\\)-NN theoretisch\nIm Folgenden betrachten wir uns den \\(k\\)-NN Algorithmus einmal ganz simpel. Dafür nutzen wir die Abbildung 58.1 als Startpunkt. Wir haben dort 11 Beobachtungen im Trainingsdatensatz dargestellt. Wir finden in dem Trainingsdatensatz acht infizierte Personen soiwe drei gesunde Personen. Darüber hinaus eine neue rote Beobachtung. Gegeben den Traingsdaten, welchen Status wollen wir der neuen roten Beobachtung geben?\n\n\nAbbildung 58.1— Darstellung von 11 Beobachtungen aus dem Traingsdatensatz und einer neuen roten Beobachtung aus den Testdaten. Die schwarzen Kugeln stellen kranke Personen und die grünen die gesunde Personen dar.\n\nIn der Abbildung 58.2 sehen wir die Klassifizierung nach \\(k = 1\\). Wir nehmen daher die \\(k = 1\\) nächsten Beobachtungen und bestimmen daran den neuen Status der roten Beobachtung. Wenn wir nur die eine nächste Beobachtung als Nachbarn betrachten, so setzen wir den Status unser neuen Beobachtung auf grün und daher gesund.\n\n\nAbbildung 58.2— Wir nehmen mit \\(k=1\\) nur die nächste Beobachtung zu unserer neuen Beobachtung hinzu und bestimmen die neue Beobachtung als grün.\n\nNun können wir das Spiel weiterspielen und wählen in der Abbildung 58.3 die \\(k = 2\\) nächsten Nachbarn zu unser neuen Beobachtung aus. Wir erhalten jetzt ein Unentschieden. Wir haben eine schwarze Beobachtung und eine grüne Beobachtung als \\(k=2\\) nächste Nachbarn. Wir können hier keine Entscheidung treffen. Eine gerade Anzahl an nächsten Nachbarn ist prinzipiell nicht anzuraten. Ich empfehle immer eine ungerade Anzhl. Auch wenn es natürlich auch für eine gerade Anzahl eine algorithmische Lösung gibt. Das ist aber weit über die Anwendung hinaus und geht in die Tiefe des Algorithmus, die wir hier nicht behandeln wollen.\n\n\nAbbildung 58.3— Mit \\(k = 2\\) nächste Nachbarn haben wir ein Patt vorliegen. Wir können uns nicht entscheiden, ob wir die neue Beobachtung als grün oder schwarz klassifizieren.\n\nIn der Abbildung 58.4 sehen wir, dass wir jetzt \\(k = 3\\) Nachbarn betrachten. Damit haben wir auf jeden Fall wieder eine Entscheidung. Wenn auch hier nur sehr knapp, da wir ja zwei schwarze und einen grünen Nachbarn haben. Wir klassifizieren dennoch die neue Beobachtung als schwarz.\n\n\nAbbildung 58.4— Die Klassifizierung mit \\(k = 3\\) nächsten Nachbarn. Wir erhalten hier eine , wenn auch knappe, Entscheidung für den schwarzen Status und damit krank.\n\nSoweit so gut. Und wie entscheide ich jetzt was weit weg ist? Wenn wir uns mit dem \\(k\\)-NN Algorithmus näher beschäftigen würden, dann werden wir feststellen, dass es eine Vielzahl an Abstandsmaßen gibt. Wir du dir vorstellen kannst, kann man die Entfernung zwischen zwei Punkten als den absoluten Abstand messen. Oder aber als den quadratischen Abstand. Es wäre auch möglich einen gewichteten Abstand einzuführen, so dass nähere Beobachtungen einen größeren Einfluss auf die Vorhersage haben als weiter entfernte Beobachtungen. Dann würden wir auch das Problem von geraden \\(k\\) Anzahlen lösen. Du musst dann leider in den jeweiligen R Paketen schauen, welche Optionen es dort geben mag. Wir werden uns hier auf eins der R Pakete mit kknn konzentrieren."
  },
  {
    "objectID": "classification-knn.html#klassifikation",
    "href": "classification-knn.html#klassifikation",
    "title": "58  \\(k\\) nearest neighbor",
    "section": "\n58.4 Klassifikation",
    "text": "58.4 Klassifikation\nSchauen wir uns als erstes eine simple Klassifikation mit dem \\(k\\)-NN Algorithmus an. Wir brauchen dafür erstmal einen Trainings- und Testdatensatz. Wir trainieren dann den \\(k\\)-NN Algorithmus auf den Trainingsdaten. Wenn wir dann mit dem Modell zufrieden sind, schauen wir, ob unserer Modell auch auf den Trainingsdaten funktioniert. Wir trennen daher die Daten mit \\(3/4\\) Trainingsdaten und \\(1/4\\) Testdaten auf. Wir nutzen dazu die Funktion initial_split(). Es gibt auch andere Möglichkeiten sich den Split in Trainings- und Testdatensatz zu erstellen, aber so geht es relativ einfach und schnell. Im Kapitel 55 kannst du dir auch noch eine Alternative anschauen.\n\npig_data_split &lt;- initial_split(pig_tbl, prop = 3/4)\n\nJetzt haben wir in dem Objekt pig_data_split die beiden Datensätze vorliegen. Wir ziehen uns nun die Trainingsdaten und die Testdaten in zwei neue Objekten heraus. Wir werden jetzt im weiteren Verlauf nur die Trainingsdaten nutzen. Die Testdaten nur einmal ganz am Ende, wenn wir die ROC-Kurven darstellen.\n\npig_train_data &lt;- training(pig_data_split)\npig_test_data  &lt;- testing(pig_data_split)\n\nWir brauchen wieder unser Rezept, in dem wir definieren, was an Schritten im Preproessing durchgeführt werden soll. Zuerst definieren wir unser Modell in der Funktion recipe(). Wir haben als unser Label die Variable infected, also ob ein Ferkel infiziert ist oder eben nicht. Wir nehmen dann die restlichen Variablen als Features mit ins Modell.\nNachdem wir dann das Rezept haben, wollen wir noch alle numerischen Prädiktoren, also die Features, auf die Spannweite von \\([0;1]\\) bringen. Dann werden noch alle nominalen Variablen in Dummies kodiert. Abschließend entfernen wir dann noch eventuelle Variablen, die kaum noch eine Varianz vorliegen haben. Das soll es für diese Anwendung des \\(k\\)-NN Algorithmus hier erstmal reichen.\n\npig_rec &lt;- recipe(infected ~ age + sex + location + activity + crp + \n                   frailty + bloodpressure + weight + creatinin,\n                  data = pig_train_data) %&gt;% \n step_range(all_numeric_predictors(), min = 0, max = 1) %&gt;% \n step_dummy(all_nominal_predictors()) %&gt;% \n step_nzv(all_predictors())\n\nJetzt kommen wir zu dem Modell. Wir wollen den \\(k\\)-NN Algorithmus rechnen und nutzen deshalb die Funktion nearest_neighbor(). Wir wollen dann neighbors = 11 in dem Algorithmus nutzen. In der Funktion heißt dann das \\(k\\) eben neighbors. Ist zwar nicht schön, aber das kennen wir ja schon alles von anderen Funktionen. Dann nutzen wir die kknn Engine und wollen eine Klassifikation rechnen. Wir rechnen eine Klassifikation, da wir als Outcome die Variable infected vorliegen haben und diese Variable binär ist.\n\nknn_mod &lt;- nearest_neighbor(neighbors = 11) %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\") \n\nDann haben wir also unser Modell definiert. Auch haben wir dann auch das Rezept, was wir ausführen wollen. Wir kombinieren jetzt das Modell zusammen mit dem Rezept in einen Workflow durch die Funktion workflow(). Bis jetzt haben wir noch nichts gerechnet. Das Rechnen kommt jetzt im nächsten Schritt.\n\npig_wflow &lt;- workflow() %&gt;% \n  add_model(knn_mod) %&gt;% \n  add_recipe(pig_rec)\n\nWir wollen jetzt den Workflow auf den Trainingsdaten ausführen. Dazu nutzen wir die Funktion fit(). Da es leider sehr viele R Pakete gibt, die die Funktion fit() implementiert haben, lege ich mit parsnip::fit() definitiv fest, dass wir die fit() Funktion aus dem R Paket parsnip nutzen wollen.\n\npig_fit &lt;- pig_wflow %&gt;% \n  parsnip::fit(pig_train_data)\n\nJetzt haben wir den Fit des Modells vorliegen. Mit dem Modell werden wir jetzt schauen, wie gut wir das Outcome infected in den Testdaten vorhersagen können. Wir nutzen dazu die Funktion augment(). Die Funktion verbindet den Testdatensatz mit den Information aus der Vorhersage. Wie immer brauchen wir nicht alles, was wir wiedergegeben kriegen. Daher wählen wir nur die Spalte infected, da stehen ja unsere wahren Werte für den Infektionsstatus drin und die Vorhersagen aus dem Modell. Die Vorhersagen des Modells haben alle ein pred im Namen, also können wir die Funktion matches() nutzen um diese Spalten auszuwählen.\n\npig_aug &lt;- augment(pig_fit, pig_test_data ) %&gt;% \n  select(infected, matches(\"pred\"))\n\npig_aug\n\n# A tibble: 103 × 4\n   infected .pred_class .pred_0 .pred_1\n   &lt;fct&gt;    &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 1        1             0.310   0.690\n 2 1        1             0.324   0.676\n 3 0        0             0.639   0.361\n 4 1        1             0.235   0.765\n 5 1        1             0.414   0.586\n 6 1        1             0.111   0.889\n 7 1        1             0       1    \n 8 0        1             0.208   0.792\n 9 0        0             0.567   0.433\n10 1        0             0.521   0.479\n# ℹ 93 more rows\n\n\nWir erhalten also den Infektionsstatus der Testdaten, den vorhergesagte Infektionsstatus aus dem \\(k\\)-NN Algorithmus, die Wahrscheinlichkeit für einen Infektionsstatus von 0 und die die Wahrscheinlichkeit für einen Infektionsstatus von 1. Damit haben wir alles zusammen um die ROC Kurven zu zeichnen. Dafür müssen wir die truth Spalte angeben und nennen in welcher Spalte die Wahrscheinlichkeit für die truth stehen. Wir definieren auch das event_level als second. Wenn die ROC Kurve auf der falschen Seite der Diagonalen ist, dann liegt es an dem falschen event_level. Die falsche Seite ist unterhalb der Diagonalen. Wenn die ROC also gespiegelt ist, dann versuche einmal event_level = \"first\" und erstelle die ROC Kurve neu.\n\npig_aug %&gt;% \n  roc_curve(truth = infected, .pred_1, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\nAbbildung 58.5— ROC Kurve für den kknn Algorithmus.\n\n\n\nLeider sieht die ROC Kurve nicht sehr gut aus. Eine sehr gute Vorhersage hat eine ROC Kurve die senkrecht ansteigt und dann waagerecht nach rechts verläuft. Die Fläche zwischen der Kurve und der Diagonalen sollte so große wie möglich sein.\nWenn wir jezt noch wissen wollen, wie groß die Fläche unter der Kurve ist (eng. area under the curve, abk. AUC) können wir die Funktion roc_auc() nutzen. Auch hier müssen wir das event_level richtig definieren. Wir kopieren hier den Code einfach rüber.\n\npig_aug %&gt;% \n  yardstick::roc_auc(truth = infected, .pred_1, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.673\n\n\nWie wir oben schon in der ROC Kurve gesehen haben ist ein Wert von \\(0.673\\) für die AUC auch nicht sehr gut. Wir liegen unter \\(0.7\\) und damit wären wir mit dem Modell nicht zufrieden. Wir müssten hier nochmal den \\(k\\)-NN Algorithmus tunen.\nAuch können wir uns die Genauigkeit (eng. accuary) berechnen lassen. Die Accuary beschreibt wie viel Prozent des Infektionsstatus wir richtig vorhergesagt haben. Wenn wir eine Accuary von 1 haben, dann haben wir alle Label korrekt vorhergesagt. Die Spalte infected enthält die gleichen Werte wie die Spalte .pred_class aus der Funktion augment(). Wenn wir eine Accuary von 0 vorliegen haben, dann konnten wir kein Label richtig vorhersagen.\n\npig_aug %&gt;% \n  yardstick::accuracy(truth = infected, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.680\n\n\nWir auch die AUC ist auch die Accuary nicht besonders gut. Wir können nur ca. \\(68\\%\\) der Label richtig vorhersagen. Damit haben wir nur jeden dritten Infektionsstatus richtig vorhergesagt. Die Accuary ist dann eben auch nicht gut, wie wir es schon dann oben bei der ROC Kurve gesehen haben. Wenigstens passen dann die wichtigsten Beurteilungskriterien inhaltlich zusammen.\nWir können auch ganz viele Beurteilungskriterien für die Klassifikation in einer Confusion matrix berechnen lassen. Dabei ist wichtig, das wir hier eine binäre Klassifikation vorliegen haben. Unser Infektionsstatus hat eben nur zwei Ausprägungen. Die Ferkel sind entweder krank oder gesund. Wir können die Funktion conf_mat() nutzen um uns die 2x2 Tabelle erstellen zu lassen.\n\npig_cm &lt;- pig_aug %&gt;% \n  conf_mat(infected, .pred_class)\n\npig_cm\n\n          Truth\nPrediction  0  1\n         0 10 11\n         1 22 60\n\n\nWenn wir dann die Funktion summary() nutzen, dann erhalten wir insgesamt dreizehn Beurteilungskriterien für die Klassifikation. Wir gehen jetzt nicht auf alle Kriterien ein, das sprengt hier den Rahmen. Wir schauen uns die Kriterien dann in dem Kapitel 57.6 nochmal teilweise an. Wie immer musst du nicht alle Kriterien angeben sondern nur die Kriterien, die der Fragestellung dienen.\n\npig_cm %&gt;% summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.680\n 2 kap                  binary         0.174\n 3 sens                 binary         0.312\n 4 spec                 binary         0.845\n 5 ppv                  binary         0.476\n 6 npv                  binary         0.732\n 7 mcc                  binary         0.181\n 8 j_index              binary         0.158\n 9 bal_accuracy         binary         0.579\n10 detection_prevalence binary         0.204\n11 precision            binary         0.476\n12 recall               binary         0.312\n13 f_meas               binary         0.377\n\n\nWie immer können wir uns eine 2x2 Tabelle auch mit einem Mosaicplot visualisieren. Das machen wir dann auch mit der Funktion autoplot(). Wir können natürlich auch die ggplot Funktionen nutzen, aber wir nutzen hier ja die Visualisierung nur um unsere Klassifikation zu überprüfen. Dann reicht auch die schnellere Variante.\n\nautoplot(pig_cm, type = \"mosaic\") +\n  theme_bw() \n\n\n\nAbbildung 58.6— Mosaicplot der Konfusionsmatrix für den kknn Algorithmus."
  },
  {
    "objectID": "classification-knn.html#resampling",
    "href": "classification-knn.html#resampling",
    "title": "58  \\(k\\) nearest neighbor",
    "section": "\n58.5 Resampling",
    "text": "58.5 Resampling\nWir können den \\(k\\)-NN Algorithmus nicht nur auf dem Trainingsdaten anwenden sondern auch auf Validierungsdaten optimieren. Dabei sind die Validierungsdaten wiederum aufgeteilte Trainingsdaten. Wir nutzen die Funktion vfold_cv() um uns zehn Kreuzvalidierungsdatensätze zu erschaffen. Meistens rechnen wir eine 10-fache Kreuzvalidierung. Die 10-fache Kreuzvalidierung ist eigentlich der Standard im Bereich der Kreuzvaldidierung.\n\nfolds &lt;- vfold_cv(pig_train_data, v = 10)\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [278/31]&gt; Fold01\n 2 &lt;split [278/31]&gt; Fold02\n 3 &lt;split [278/31]&gt; Fold03\n 4 &lt;split [278/31]&gt; Fold04\n 5 &lt;split [278/31]&gt; Fold05\n 6 &lt;split [278/31]&gt; Fold06\n 7 &lt;split [278/31]&gt; Fold07\n 8 &lt;split [278/31]&gt; Fold08\n 9 &lt;split [278/31]&gt; Fold09\n10 &lt;split [279/30]&gt; Fold10\n\n\nDank der Funktion fit_resample() können wir einen Workflow nicht nur auf einen Datensatz wie mit der Funktion fit() anwenden, sondern auf ein ganzes Set an Validierungsdaten. Die Funktion fit_resample() rechnet jetzt auf jenden der zehn Validierungsdatensätze einen \\(k\\)-NN Algorithmus wie im Workflow beschreiben.\n\npig_cv_fit &lt;- pig_wflow %&gt;% \n  fit_resamples(folds)\n\nNachdem wir die zehn Validierungsdatensätze durchgerechnet haben, müssen wir noch die Informationen aus jedem der zehn Validierungsdatensätze einsammeln. Das macht die Funktion collect_metrics() für uns.\n\ncollect_metrics(pig_cv_fit)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.718    10  0.0220 Preprocessor1_Model1\n2 roc_auc  binary     0.700    10  0.0357 Preprocessor1_Model1\n\n\nWir sehen, dass wir eine Accuarcy von \\(0.718\\) erreichen und eine AUC von \\(0.7\\). Damit sind wir ein bisschen besser als in unserem einfachen Lauf auf nur den Trainingsdaten. Die eigentliche Stärke der Kreuzvalidierung kommt aber erst mit dem Tuning zu tage. Hier nutzen wir dann die Kreuzvalidierung um die Parameter des \\(k\\)-NN Algorithmus zu optimieren."
  },
  {
    "objectID": "classification-knn.html#tuning",
    "href": "classification-knn.html#tuning",
    "title": "58  \\(k\\) nearest neighbor",
    "section": "\n58.6 Tuning",
    "text": "58.6 Tuning\nWas heißt Tuning? Wie bei einem Auto können wir an verschiedenen Stellschrauben bei einem mathematischen Algorithmus schrauben. Welche Schrauben und Teile das sind, hängt dann wieder vom Algorithmus ab. Im Falle des \\(k\\)-NN Algorithmus können wir an folgenden Parametern drehen und jeweils schauen, was dann mit unserer Vorhersage passiert.\n\n\nneighbors, eine einzelne Zahl für die Anzahl der zu berücksichtigenden Nachbarn (oft \\(k\\) genannt). Für kknn wird ein Wert von 5 verwendet, wenn keine Anzahl angegeben ist.\n\nweight_func ein Wort für den Typ der Kernel-Funktion, die zur Gewichtung der Abstände zwischen den Beobachtungen verwendet wird.\n\ndist_power, eine einzelne Zahl für den Parameter, der bei der Berechnung der Minkowski-Distanz verwendet wird. Wir nutzen also die dist_power nicht bei jedem Tuningschritt, da nicht jede Gewichtsfunktion eine dist_power braucht.\n\nNun ist es so, dass wir natürlich nicht händisch alle möglichen Kombinationen von der Anzahl der Nachbarn, der Distanzfunktion und der Gewichtung der Distanz berechnen wollen. Das sind ziemlich viele Kombinationen und wir kommen dann vermutlich schnell durcheinander. Deshalb gibt es die Funktion tune() aus dem R Paket tune, die uns einen Prozess anbietet, das Tuning automatisiert durchzuführen.\nAls erstes müssen wir uns ein Objekt bauen, das aussieht wie ein ganz normales Modell in der Klassifikation. Aber wir ergänzen jetzt noch hinter jeder zu tunenden Option noch die Funktion tune(). Das sind die Parameter des Algorithmus, die wir später tunen wollen.\n\ntune_spec &lt;- nearest_neighbor(neighbors = tune(),\n                              weight_func = tune(), \n                              dist_power = tune()) %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\") \n\ntune_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n  weight_func = tune()\n  dist_power = tune()\n\nComputational engine: kknn \n\n\nJetzt bauen wir uns den Workflow indem wir statt unserem Modell, die Tuninganweisung in den Workflow reinnehmen. Echt simpel und straightforward. Das Rezept bleibt ja das Gleiche.\n\npig_tune_wflow &lt;- workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(pig_rec)\n\nJetzt müssen wir noch alle Kombinationen aus den drei Parametern neighbors, weight_func und dist_power ermitteln. Das macht die Funktion grid_regular(). Es gibt da noch andere Funktionen in dem R Paket tune, aber ich konzentriere mich hier auf die einfachste. Jetzt müssen wir noch die Anzahl an Kombinationen festlegen. Ich möchte für jeden Parameter fünf Werte tunen. Daher nutze ich hier die Option levels = 5 auch damit hier die Ausführung nicht so lange läuft. Fange am besten mit levels = 5 an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert. Dann kannst du die Levels noch hochschrauben. Beachte aber, dass mehr Level nur mehr Zwischenschritte bedeutet. Jede Option hat eine Spannweite range, die du dann anpassen musst, wenn du höhere Werte haben willst. In unserem Fall ist die default Anzahl an neighbors auf 1 bis 10 gesetzt. Mehr Level würden nur mehr Zwischenschritte bedeuten. Deshalb habe ich die Spannweite auf 1 bis 20 Nachbarn gesetzt. Jetzt wählt die Funktion fünf Zwischenschritte (levels = 5) zwischen ein und zwanzig aus (range = c(1, 20)).\n\npig_grid &lt;- grid_regular(neighbors(range = c(1, 20)),\n                         weight_func(),\n                         dist_power(),\n                         levels = 5)\n\nDas Tuning nur auf dem Trainingsdatensatz durchzuführen ist nicht so eine gute Idee. Deshalb nutzen wir hier auch die Kreuzvalidierung. Eigentlich ist eine 10-fache Kreuzvalidierung mit \\(v=10\\) besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur \\(v=5\\) gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.\n\npig_folds &lt;- vfold_cv(pig_train_data, v = 5)\n\nNun bringen wir den Workflow zusammen mit dem Tuninggrid und unseren Sets der Kreuzvaidierung. Daher pipen wir den Workflow in die Funktion tune_grid(). Als Optionen brauchen wir die Kreuzvaldierungsdatensätze und das Tuninggrid. Wenn du control_grid(verbose = TRUE) wählst, dann erhälst du eine Ausgabe wie weit das Tuning gerade ist. Achtung!, das Tuning dauert seine Zeit. Im Falle des \\(k\\)-NN Algorithmus dauert das Tuning zwar nicht so lange, aber immer noch ein paar Minuten. Du kannst das Ergebnis des Tunings auch in der Datei pig_knn_tune_res.rds finden.\n\npig_tune_res &lt;- pig_tune_wflow %&gt;% \n   tune_grid(resamples = pig_folds,\n             grid = pig_grid,\n             control = control_grid(verbose = FALSE))\n\nDamit du nicht das Tuning durchlaufen lassen musst, habe ich das Tuning in die Datei pig_knn_tune_res.rds abgespeichert und du kannst dann über die Funktion read_rds() wieder einlesen. Dann kannst du den R Code hier wieder weiter ausführen.\n\npig_tune_res &lt;- read_rds(\"data/pig_knn_tune_res.rds\")\n\nNachdem das Tuning durchgelaufen ist, können wir uns über die Funktion collect_metrics(), die Ergebnisse des Tunings für jede Kombination der drei Parameter neighbors, weight_func und dist_power wiedergeben lassen. Diese Ausgabe ist super unübersichtlich. Deshalb einmal die Abbildung der mittleren Accuarcy und der mittleren AUC-Werte über alle Kreuzvalidierungen.\n\npig_tune_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(weight_func = as_factor(weight_func),\n         dist_power = as_factor(dist_power)) %&gt;%\n  ggplot(aes(neighbors, mean, color = weight_func, linetype = dist_power)) +\n  theme_minimal() +\n  geom_line(alpha = 0.6) +\n  geom_point() +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_okabeito()\n\n\n\nAbbildung 58.7— Tuning Kurven für den kknn Algorithmus.\n\n\n\nDamit wir nicht händisch uns die beste Kombination raussuchen müssen, können wir die Funktion show_best() nutzen. Wir wählen hier die beste Accuarcy und erhalten dann die sortierten Ergebnisse nach der Accuarcy des Tunings.\n\npig_tune_res %&gt;%\n  show_best(\"accuracy\")\n\n# A tibble: 5 × 9\n  neighbors weight_func  dist_power .metric  .estimator  mean     n std_err\n      &lt;int&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1        15 rectangular        1.25 accuracy binary     0.692     5  0.0189\n2        15 rectangular        1.75 accuracy binary     0.689     5  0.0311\n3        15 rectangular        1.5  accuracy binary     0.683     5  0.0225\n4        20 epanechnikov       1.5  accuracy binary     0.683     5  0.0154\n5        20 rectangular        1.5  accuracy binary     0.679     5  0.0127\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\nDas war die Funktion show_best() aber wir können uns auch die gleich die besten Parameter nach der Accuracy raus ziehen. Das Rausziehen der besten Parameter macht für uns die Funktion select_best().\n\nbest_knn &lt;- pig_tune_res %&gt;%\n  select_best(\"accuracy\")\n\nbest_knn\n\n# A tibble: 1 × 4\n  neighbors weight_func dist_power .config               \n      &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                 \n1        15 rectangular       1.25 Preprocessor1_Model059\n\n\nWir sehen, dass wir neighbors = 15 wählen sollten. Dann müssen wir als Gewichtungsfunktion rectangular nutzen. Die Gewichtung der Distanz wäre dann 1.25. Müssen wir jetzt die Zahlen wieder in ein Modell eingeben? Nein, müssen wir nicht. Mit der Funktion finalize_workflow() können wir dann die besten Parameter aus unserem Tuning gleich mit dem Workflow kombinieren. Dann haben wir unseren finalen, getunten Workflow. Du siehst dann auch in der Ausgabe, dass die neuen Parameter in dem \\(k\\)-NN Algorithmus übernommen wurden\n\nfinal_pig_wf &lt;- pig_tune_wflow %&gt;% \n  finalize_workflow(best_knn)\n\nfinal_pig_wf \n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_range()\n• step_dummy()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 15\n  weight_func = rectangular\n  dist_power = 1.25\n\nComputational engine: kknn \n\n\nJetzt bleibt uns nur noch der letzte Fit übrig. Wir wollen unseren finalen, getunten Workflow auf die Testdaten anwenden. Dafür gibt es dann auch die passende Funktion. Das macht für uns die Funktion last_fit(), die sich dann die Informationen für die Trainings- und Testdaten aus unserem Datensplit von ganz am Anfang extrahiert.\n\nfinal_fit &lt;- final_pig_wf %&gt;%\n  last_fit(pig_data_split) \n\nDa wir immer noch eine Kreuzvaldierung rechnen, müssen wir dann natürlich wieder alle Informationen über alle Kreuzvaldierungsdatensätze einsammeln. Dann erhalten wir unsere beiden Gütekriterien für die Klassifikation der Infektion von Ferkeln nach dem \\(k\\)-NN Algorithmus. So super sind die Zahlen nicht. Eine Accuracy von 73% bedeutet das wir nur knapp drei von vier Ferkeln richtig klassifizieren. Die AUC ist auch nicht berauschend, wir bringen also eine Menge Label durcheinander. Wir klassifizieren also gesunde Ferkeln als krank und umgekehrt.\n\nfinal_fit %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.728 Preprocessor1_Model1\n2 roc_auc  binary         0.662 Preprocessor1_Model1\n\n\nDann bleibt uns nur noch die ROC Kurve zu visualisieren. Da wir wieder etwas faul sind, nutzen wir die Funktion autoplot(). Als Alternative geht natürlich auch das R Paket pROC, was eine Menge mehr Funktionen und Möglichkeiten bietet.\n\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(infected, .pred_1, event_level = \"second\") %&gt;% \n  autoplot() +\n  theme_bw()\n\n\n\nAbbildung 58.8— ROC Kurve für den kknn Algorithmus nach der Kreuvalidierung und dem Tuning.\n\n\n\nEine gute ROC Kurve würde senkrecht nach oben gehen und dann waagrecht nach rechts. Dann hätten wir eine AUC von 1 und eine perfekte Separation der beiden Label durch unseren Algorithmus. Unser Algorithmus würde jedes Ferkel in dem Testdatensatz korrekt dem Infektionsstatus krank und gesund zuweisen. Da wir eine ROC Kurve hier vorliegen haben, die sehr nahe an der Diagonalen ist, haben wir sehr viele falsch vorhergesagte Ferkel in unseren Testdaten. Ferkel die gesund sind, werden als krank klassifiziert uns umgekehrt."
  },
  {
    "objectID": "classification-knn.html#kmeans-clustering",
    "href": "classification-knn.html#kmeans-clustering",
    "title": "58  \\(k\\) nearest neighbor",
    "section": "\n58.7 kmeans Clustering",
    "text": "58.7 kmeans Clustering\nNeben der Klassifikation können wir den \\(k\\)-NN Algorithmus auch nutzen um Gruppen in den Daten zu finden. Die Idee ist recht einfach. Wir geben \\(k\\) Cluster vor und der Algorithmus versucht nun die Daten nach einer gegebenen Distanzfunktion so zu ordnen, dass sich \\(k\\) Cluster bilden. Je nach der Nähe der Beobachtungen zueinander lassen sich dann mehr oder minder klar abgegrenzte Cluster bilden. Das Problem an der Sache ist die Definition von \\(k\\) für die Anzahl der zu bildenden Cluster. Wir müssen nämlich selber festlegen, wie viele Cluster wir erwarten würden und der Algorithmus dann finden wird. Wenn wir \\(k = 3\\) der Funktion kmeans mitgeben, dann findet die Funktion drei Cluster. Auch wenn zwei mehr Sinn gemacht hätten. Daher müssen wir immer selber ausprobieren und uns die Daten visualisieren, ob das mit den Clustern so passt.\nFür Clusterbildung können wir nur numerische Variablen verwenden. Daher müssen wir hier über die Funktion step_dummy alle nominalen Variablen wie Faktoren in eine \\(0/1\\)-Kodierung umwandeln. Das ist eine Einschränkung des kmeans Algorithmus. Wir bauen uns also als erstes ein simples Rezept für unsere Ferkeldaten.\n\npig_kmeans_rec &lt;- recipe(infected ~ age + sex + location + activity + crp + \n                           frailty + bloodpressure + weight + creatinin,\n                         data = pig_train_data) %&gt;% \n  step_dummy(all_nominal_predictors()) \n\nDann müssen wir noch unser Rezept auf Daten anwenden. Da wir hier die gleichen Daten nutzen wollen, auf denen wir auch das Rezept definiert haben, nutzen wir die Funktion juice(). Sonst müssten wir in der Funktion bake() einen neuen Datensatz definieren.\n\npig_dummy_tbl &lt;- pig_kmeans_rec %&gt;% \n  prep %&gt;% \n  juice()\n\nNachdem wir jetzt einen Datensatz mit nur numerischen Variablen vorliegen haben, können wir die Funktion kmeans() ausführen. Wir wollen dabei aber drei Cluster bilden, das machen wir mit der Option centers = 3.\n\nkmeans_obj &lt;- kmeans(pig_dummy_tbl, centers = 3)\n\nJetzt ziehen wir uns aus dem Objekt kmeans_obj noch die Cluster raus und kombinieren die Information welche Beobachtung in welchen Cluster fällt mit den ursprünglichen Daten. Damit sind wir dann hier schon fertig. Häufig wird die Funktion kmeans in der Detektion von Ausreißern zusammen mit dem Multidimensional Scaling verwendet.\n\npig_dummy_tbl %&gt;% \n  bind_cols(cluster = pluck(kmeans_obj, \"cluster\")) %&gt;% \n  select(cluster, everything())\n\n# A tibble: 309 × 14\n   cluster   age activity   crp bloodpressure weight creatinin infected sex_male\n     &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;\n 1       1    57     12.0  19.2          53.8   17.4      2.9  0               1\n 2       1    56     12.0  19.5          56.3   19.4      3.89 0               0\n 3       1    54     14.6  20.2          55.2   19.3      5.3  1               1\n 4       1    55     13.8  20.3          51.9   18.4      6.44 1               1\n 5       1    51     12.3  19.5          55.7   20.0      3.12 1               0\n 6       2    68     14.6  20.6          57.4   17.4      6.22 1               0\n 7       2    61     12.0  21.0          53.3   19.5      4.57 1               1\n 8       3    56     14.7  22.0          61.4   21.9      3.35 1               0\n 9       3    60     10.8  21.8          59.0   18.0      4.85 1               0\n10       1    56     14.4  19.6          55.8   17.2      5.13 0               1\n# ℹ 299 more rows\n# ℹ 5 more variables: location_northeast &lt;dbl&gt;, location_northwest &lt;dbl&gt;,\n#   location_west &lt;dbl&gt;, frailty_pre.frail &lt;dbl&gt;, frailty_robust &lt;dbl&gt;\n\n\nDas R Paket embed bietet noch eine Vielzahl an weiteren Funktionen für die Erstellung von kategorialen Variablen. Bier musst du schauen, ob die Funktionen dann univariat sind und dhaer immer nur eine variable nutzen oder aber multivariat und daher mehrere Spalten simultan. Der Vorteil von kmeans ist ja, das der Algorithmus mehrere numerische Spalten für die Clusterbildung nutzen kann."
  },
  {
    "objectID": "classification-randomforest.html#genutzte-r-pakete",
    "href": "classification-randomforest.html#genutzte-r-pakete",
    "title": "59  Decision trees",
    "section": "\n59.1 Genutzte R Pakete",
    "text": "59.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor, vip, rpart.plot, see,\n               xgboost, Ckmeans.1d.dp, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n##\nset.seed(2025429)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-randomforest.html#daten",
    "href": "classification-randomforest.html#daten",
    "title": "59  Decision trees",
    "section": "\n59.2 Daten",
    "text": "59.2 Daten\nBei dem vorherigen Beispielen haben wir immer unseren Datensatz zu den infizierten Ferkeln genutzt. In diesem Kapitel wolle wir uns aber mal auf einen echten Datensatz anschauen. Wir nutzen daher einmal den Gummibärchendatensatz. Als unser Label und daher als unser Outcome nehmen wir das Geschlecht gender. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir nur die Spalte Geschlecht sowie als Prädiktoren die Spalten most_liked, age, semester, und height.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") %&gt;% \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) %&gt;% \n  select(gender, most_liked, age, semester, height) %&gt;% \n  drop_na(gender)\n\nWir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein NA in der Variable gender haben. Wir haben dann insgesamt \\(n = 522\\) Beobachtungen vorliegen. In Tabelle 59.1 sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.\n\n\n\n\nTabelle 59.1— Auszug aus dem Daten zu den Gummibärchendaten.\n\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\nm\nlightred\n35\n10\n193\n\n\nw\nyellow\n21\n6\n159\n\n\nw\nwhite\n21\n6\n159\n\n\nw\nwhite\n36\n10\n180\n\n\nm\nwhite\n22\n3\n180\n\n\nm\nwhite\nNA\nNA\nNA\n\n\n…\n…\n…\n…\n…\n\n\nw\nyellow\n14\n0\n173\n\n\nw\nnone\n11\n0\n159\n\n\nw\ndarkred\n12\n0\n158\n\n\nw\ndarkred\n12\n0\n158\n\n\nw\ndarkred\n12\n0\n156\n\n\nw\ndarkred\n12\n0\n158\n\n\n\n\n\n\nUnsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Um die Klassifikation mit Entscheidungsbäumen rechnen zu können brauchen wir wie bei allen anderen Algorithmen auch einen Trainings- und Testdatensatz. Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Der Traingsdatensatz ist dabei immer der größere Datensatz. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.\nIm maschinellen Lernen sind alle Datensätze, die weniger als tausend Beobachtungen vorliegen haben, klein.\n\ngummi_data_split &lt;- initial_split(gummi_tbl, prop = 3/4)\n\nWir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluss einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.\n\ngummi_train_data &lt;- training(gummi_data_split)\ngummi_test_data  &lt;- testing(gummi_data_split)\n\nNachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht gender als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem . Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Es gibt wie immer noch andere Imputationsmöglichkeiten, ich habe mich jetzt aus praktischen Gründen für dies beiden Verfahren entschieden. Ich überspringe hier auch die Diagnose der Imputation, also ob das jetzt eine gute und sinnvolle Imputation der fehlenden Werte war oder nicht. Die Diagnoseschritte müsstest du im Anwendungsfall nochmal im Kapitel zur Imputation nachlesen und anwenden. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_train_data) %&gt;% \n  step_impute_mean(all_numeric_predictors()) %&gt;% \n  step_impute_bag(all_nominal_predictors()) %&gt;% \n  step_range(all_numeric_predictors(), min = 0, max = 1) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_nzv(all_predictors())\n\ngummi_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Bagged tree imputation for: all_nominal_predictors()\n\n\n• Range scaling to [0,1] for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_predictors()\n\n\nAlles in allem haben wir ein sehr kleines Modell. Wir haben ja nur ein Outcome und vier Prädiktoren. Trotzdem sollte dieser Datensatz reichen um zu erklären wie Entscheidungsbäume funktionieren."
  },
  {
    "objectID": "classification-randomforest.html#sec-rpart",
    "href": "classification-randomforest.html#sec-rpart",
    "title": "59  Decision trees",
    "section": "\n59.3 Entscheidungsbaum mit Rpart",
    "text": "59.3 Entscheidungsbaum mit Rpart\nWie funktioniert nun ein Entscheidungsbaum? Ein Entscheidungsbaum besteht aus Knoten (eng. nodes) und Ästen (eng. edge). Dabei hat immer ein Knoten zwei Äste. Die Beobachtungen in einem Knoten fallen nach einer Entscheidungsregel anhand eines Prädiktors in entlang zweier Äste in zwei separate Knoten. So können wir unsere \\(n = 522\\) zum Beispiel anhand des Alters in zwei Gruppen aufteilen. Wir legen willkürlich die Altersgrenze bei 22 fest.\n\ngummi_tbl %&gt;% \n  mutate(grp = if_else(age &gt;= 22, 1, 0)) %&gt;% \n  pull(grp) %&gt;% \n  tabyl()\n\n  .   n    percent valid_percent\n  0 225 0.43103448     0.4368932\n  1 290 0.55555556     0.5631068\n NA   7 0.01340996            NA\n\n\nWir erhalten mit diesem Split zwei Gruppen mit je \\(n_0 = 207\\) und \\(n_1 = 259\\) Beobachtungen. Wir haben jetzt diesen Split willkürlich gewählt. In dem Algorithmus für die Entscheidungsbäume wird dieser Schritt intern optimiert, so dass wir den besten Wert für den Alterssplit finden, der uns möglichst reine Knoten im Bezug auf das Label liefert. Wir wollen ja am Ende einen Algorithmus trainieren, der uns die Geschlechter bestmöglich auftrennt, so dass wir eine neue Beobachtung bestmöglich vorhersagen können. Wenn keine Aufteilungen in einem Knoten mehr möglich sind, dann nennen wir diesen Knoten einen Terminalknoten.\nIn Abbildung 59.5 sehen wir ein Beispiel für zwei numerische Prädiktoren \\(X_1\\) und \\(X_2\\). Auf der linken Seite ist das Koordinatensystem mit dreizehn Beobachtungen dargestellt. Von den dreizehn Beobachtungen sind zehn Fälle (eng. cases) und drei Kontrollen (eng. control). Wir wollen uns jetzt an dem Koordinatensystem die Idee der Splits für ein Baumwachstum veranschaulichen. Auf der rechten Seite sehen wir nämlich den ersten Knoten des Entscheidungsbaums (eng. root node) in dem sich alle Beobachtungen befinden. Wir wollen jetzt die Beobachtungen anhand der Prädiktoren \\(X_1\\) und \\(X_2\\) so aufspalten, dass für möglichst reine Knoten erhalten. Wir stoppen auch im Splitting wenn wir weniger oder gleich vier Beobachtungen nach einem Split in einem Knoten erhalten.\n\n\nAbbildung 59.1— Darstellung des Anwachsen des Entscheidungsbaumes. Links sind die beiden Prädiktoren \\(X_1\\) und \\(X_2\\) als Koordinatensysten dargestellt. Die Punkte stllen die Beobachtungen mit den jeweiligen Label weiß und schwarz dar. Rechts ist der Knoten \\(t_1\\) dargestellt, der alle Beobachtungen beinhaltet..\n\nIn Abbildung 59.6 sehen wir den ersten Split des Prädiktors \\(X_1\\) anhand des Wertes \\(c_1\\). Wir erhalten nach dem Split die zwei neuen Knoten \\(t_2\\) und \\(t_3\\). Wir haben den Split so gewählt, dass wir einen reinen Knoten \\(t_3\\) erhalten. Da der Knoten \\(t_3\\) jetzt nur noch Fälle enthaält, wird dieser Knoten zu einem Terminalknoten und es finden keine weiteren Aufspaltungen mehr statt. Wir machen jetzt also mit dem Knoten \\(t_2\\) weiter.\n\n\nAbbildung 59.2— Darstellung des ersten Splits anhand des Prädiktors \\(X_1\\). Wir wählen den Wert \\(c_1\\) für den Split so, dass wir möglichst reine Knoten produzieren. Wir erhalten zwei neue Knoten \\(t_2\\) und \\(t_3\\). Der Knoten \\(t_3\\) ist maximal rein und wird daher zu einem Terminalknoten.\n\nIn Abbildung 59.8 sehen wir den Split durch den Prädiktor \\(X_2\\) nach dem Wert \\(c_2\\). Wir erhalten wieder zwei neue Knotenn \\(t_4\\) und \\(t_5\\). Der Knoten \\(t_4\\) wird nach unseren Regeln wieder zu einem Terminalknoten. Wir haben nur Fälle in dem Knoten \\(t_4\\) vorliegen. Wir stoppen auch bei dem Knoten \\(t_5\\) unsere weitere Aufteilung, da wir hier vier oder weniger Beobachtungen vorliegen haben. Damit sind wir mit dem Split zu einem Ende gekommen.\n\n\nAbbildung 59.3— Darstellung des zweiten Splits anhand des Prädiktors \\(X_2\\). Wir wählen wiederum den Wert \\(c_2\\) für den Split so, dass wir möglichst reine Knoten erhalten. So erhalten wir zwei neue Knoten \\(t_4\\) und \\(t_5\\). Da nun \\(t_4\\) ebenfalls ein reiner Knoten ist, wird der Knoten \\(t_4\\) ebenfalls zu einem Terminalknoten. Wir stoppen hier das Wachstum, da mir eine mindest Anzahl von vier Beobachtungen in den Knoten erreicht haben.\n\nIn Abbildung 59.12 sehen wir jetzt eine neue Beobachtung ? die mit gegebenen Werten für \\(X_1\\) und \\(X_2\\) in den terminalen Knoten \\(t_5\\) fällt. Wir zählen dort die Fälle und erhalten eine Klassenzugehörigkeitswahrscheinlichkeit von 25%. Daher würden wir sagen, dass die neue Beobchtung eine Kontrolle ist. Es handelt sich damit um eine weiße Beoabchtung.\n\n\nAbbildung 59.4— Darstellung der Vorhersage einer neuen Beobachtung mit Werten für die Prädiktoren \\(X_1\\) und \\(X_2\\). Unsere neue Beobachtung ? fällt in den Terminalknoten \\(t_5\\). Dort zählen wir die schwarzen Kreise. Wir stellen fest, dass die neue Beobachtung mit 25% Wahrscheinlichkeit ein Fall und damit schwarz ist. Daher ist die neue Beobachtung weiß.\n\nDamit haben wir einmal den simplen Fall mit zwei numerischen Prädiktoren durchgespielt. Auch haben wir wenige Beobachtungen und sind schnell zu reinen Knoten gekommen. Wenn wir jetzt natürlich sehr viel mehr Beobachtungen haben oder sehr viele Prädiktoren dann wird die Sache sehr schnell sehr rechenintensiv. Dafür haben wir dann eben R.\nWenn wir in R einen Entscheidungsbaum rechnen wollen, dann nutzen wir die Funktion decision_tree() wir wollen nur eine maximale Tiefe von 5 Knoten haben und/oder mindestens 10 Beobachtungen in einem Knoten. Je nachdem welche Bedingung wir eher erreichen. Ebenfalls können wir das Wachstum mit dem Parameter cost_complexity kontrollieren. Sollte sich das Modell nicht um mindestens 0.001 verbessern, dann werden wir den nächsten Knoten nicht anlegen. Wir wählen als Engine den Algorithmus rpart, da wir uns diese Art von Algorithmus gut mit dem R Paket rpart.plot visualisieren können.\n\nrpart_mod &lt;- decision_tree(tree_depth = 5, min_n = 10, cost_complexity = 0.001) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\nJetzt kommt wieder das Modell zusammen mit dem Rezept. Wir speichern wieder beides in einen Workflow.\n\nrpart_wflow &lt;- workflow() %&gt;% \n  add_model(rpart_mod) %&gt;% \n  add_recipe(gummi_rec)\n\nDen Workflow können wir dann mit dem Traingsdatensatz einmal durchlaufen lassen und uns das gefittete Modell wiedergeben lassen.\n\nrpart_fit &lt;- rpart_wflow %&gt;% \n  parsnip::fit(gummi_train_data)\n\nNachdem wir das trainierte Modell vorliegen haben, nutzen wir die Funktion augment() um das Modell auf die Testdaten anzuwenden.\n\nrpart_aug &lt;- augment(rpart_fit, gummi_test_data ) \n\nJetzt geht es los und wir schauen uns einmal an, wie gut die Klassifizierung mit dem Modell funktioniert hat. Als erstes bauen wir uns einmal die Konfusionsmatrix um zu sehen wie gut die beiden Geschlechter in dem Testdatensatz vorhergesagt wurden.\n\nrpart_cm &lt;- rpart_aug %&gt;% \n  conf_mat(gender, .pred_class)\n\nrpart_cm\n\n          Truth\nPrediction  m  w\n         m 53 16\n         w 16 46\n\n\nDas freut einen doch. Das sieht ziemlich gut aus. Wir haben auf der Diagonalen fast alle Beoabchtungen und nur sehr wenige falsche Vorhersagen auf der Nichtdiagonalen. Jetzt können wir uns noch eine ganze Reihe an anderen Gütekriterien für den Vergleich von Modellen ausgeben lassen.\n\nrpart_cm %&gt;% summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.756\n 2 kap                  binary         0.510\n 3 sens                 binary         0.768\n 4 spec                 binary         0.742\n 5 ppv                  binary         0.768\n 6 npv                  binary         0.742\n 7 mcc                  binary         0.510\n 8 j_index              binary         0.510\n 9 bal_accuracy         binary         0.755\n10 detection_prevalence binary         0.527\n11 precision            binary         0.768\n12 recall               binary         0.768\n13 f_meas               binary         0.768\n\n\nWir besprechen hier nicht alle, du kannst dann gerne nochmal in dem Kapitel über die Modellvergleiche nachlesen, was die ganze Gütekriterien alles bedeuten. Wenn wir uns auf die Accuarcy konzentrieren, erhalten wir einen guten Wert von 83% richtig klassifizierter Geschlechter. Das ist für echte Daten ohne Tuning und Kreuzvaldierung schon ein echt guter Wert.\nNun schauen wir uns noch schnell die ROC Kurve an und sehen, dass die Kurve schon weit von der Diagonalen entfernt ist. Wir sehen eine gute ROC Kurve. Die AUC sollte auch recht groß sein.\n\nrpart_aug %&gt;% \n  roc_curve(gender, .pred_w, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\nAbbildung 59.5— ROC Kurve für den Entscheidungsbaum mit dem rpart Algorithmus.\n\n\n\nEs gibt viele Möglichkeiten sich einen Entscheidungsbaum anzuschauen. Wir nutzen hier das R Paket rpart.plot und die gleichnamige Funktion rpart.plot(). Die vielen Möglichkeiten der Darstellung und der Optionen findest in der Vignette Plotting rpart trees with the rpart.plot package.. Wir gehen hier einmal auf die Variante extra = 101 ein. Es gibt insgesamt elf verschiedene Arten plus eben noch die Möglichkeit 100 zu einer der elf genannten Varianten hinzufügen, um auch den Prozentsatz der Beobachtungen im Knoten anzuzeigen. Zum Beispiel zeigt extra = 101 die Anzahl und den Prozentsatz der Beobachtungen in dem Knoten an.\n\nrpart_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  rpart.plot(roundint = FALSE, extra = 101)\n\n\n\nAbbildung 59.6— Visualisierung des finalen rpart Entscheidungsbaums.\n\n\n\nIn Abbildung 59.6 sehen wir den finalen Entscheidungsbaum. Wir sehen, dass wir nicht weiter als fünf Splits nach unten gewandert sind. Das hatten wir ja auch mit dem Parameter tree_depth so eingestellt. Jetzt sehen wir aber auch, dass wir mit dem Preprocessing auch eine Grube graben können. Wir haben in unserem ersten Knoten 189 Männer und 165 Frauen. Daher hat der Knoten nach Mehrheitsentscheidung den Status m. Jetzt spalten wir den Knoten nach der Körpergröße von \\(0.48\\) in zwei Gruppen. Was soll jetzt \\(0.48\\) heißen? Keine Ahnung. Wir haben die Daten normalisiert. Wenn du hier die Werte für die Splits interpretieren willst, dann musst du auf den Orginaldaten rechnen. Nach dem Split sehen wir zwei Knoten, in denen zum einen die Männer domiern und zum anderen die Frauen. Wir splitten wieder nach der Körpergröße und erhalten immer reinere Knoten in den fast nur noch Männer oder Frauen sind.\nSchaue dir auch die anderen Arten der Visualisierung in rpart.plot an und entscheide, ob dir die anderen Varianten bessere Informationen liefern, die zu deiner wissenschaftlichen Fragestellung passen.\nAn der Stelle trifft dann immer die Klassifikation auf die Interpretation. Du kannst nicht das Modell im Nachgang wieder entnormalisieren. Das geht nicht. Wenn du auf den Orginaldaten rechnest, dann wirst du ein anderes Modell erhalten. Das Modell mag besser oder schlechter sein, auf jeden Fall anders. Wie so oft hängt es von der wissenschaftlichen Fragestellung ab."
  },
  {
    "objectID": "classification-randomforest.html#sec-rf",
    "href": "classification-randomforest.html#sec-rf",
    "title": "59  Decision trees",
    "section": "\n59.4 Random Forest mit ranger",
    "text": "59.4 Random Forest mit ranger\nBis jetzt haben wir einen Entscheidungsbaum wachsen lassen. Was wäre, wenn wir statt einen Baum mehrere Bäume wachsen lassen. Wir lassen einen ganzen Wald (eng. forest) entstehen. Nun macht es wenig Sinn, immer den gleichen Baum auf immer den selben Daten wachsen zu lassen. Daher wählen wir zufällig eine Anzahl an Zeilen und Spalten aus bevor wir einen Baum in unserem Wald wachsen lassen. Dabei bringen wir zwei den Zufall in die Generierung eines Baums mit ein.\n\nDurch die zufällige Auswahl der Beobachtungen mit Zurücklegen. Wir haben also einzelne Zeilen und damit Beobachtungen mehrfach in den Daten.\nDurch die zufällige Auswahl eines Sets an Variablen. Wir nutzen nicht immer alle Variablen in unserem Modell sondern nur ein Set an Spalten.\n\nIm maschinellen Lernen nennen wir diese Methode Bagging. Das Wort Bagging steht für bootstrap aggregating und ist eine Methode, um Vorhersagen aus verschiedenen Modellen zu kombinieren. In unserem Fall sind es die verschiedenen Entscheidungsböume. Dabei müssen alle Modelle mit dem gleichen Algorithmus laufen, können aber auf verschiedenen Datensätzen oder aber Variablensätzen zugreifen. Häufig haben die Modelle eine hohe Varianz in der Vorhersage und wir nutzen dann Bagging um die Modelle miteinander zu kombinieren und dadurch die Varianz zu verringern. Die Ergebnisse der Modelle werden dann im einfachsten Fall gemittelt. Das Ergebnis jeder Modellvorhersage geht mit gleichem Gewicht in die Vorhersage ein. Wir haben auch noch andere Möglichkeiten, aber du kannst dir Vorstellen wir rechnen verschiedene Modelle \\(j\\)-mal und bilden dann ein finales Modell in dem wir alle \\(j\\)-Modelle zusammenfassen. Wie wir die Zusammenfassung rechnen, ist dann immer wieder von Fall zu Fall unterschiedlich. Wir erhalten am Ende einen Ensemble Klassifizierer, da ja ein Ensemble von Modellen zusammengefasst wird. In dem Fall von den Entscheidungsbäumen ist das Ensemble ein Wald an Bäumen.\n\n\n\n\n\n\nParallele CPU Nutzung\n\n\n\n\n\nWenn wir wirklich viele Bäume wachsen lassen wollen, dann bietet sich die parallele Berechnung an. Das können wir über das R Paket parallel realisieren. Wir detektieren erstmal wie viele Kerne wir auf dem Rechner zu Verfügung haben.\n\ncores &lt;- parallel::detectCores()\ncores\n\n[1] 8\n\n\nWenn wir das gemacht haben, dann können wir in set_engine(\"ranger\", num.threads = cores) auswählen, dass die Berechnung parallel verlaufen soll. Besonders auf Großrechnern macht die parallele Berechnung am meisten Sinn.\n\n\n\nAuch hier ist es so, dass es verschiedene Algorithmen für den Random Forest gibt. Wir nehmen hier dann den ranger Algorithmus. Du kannst wie immer schauen, welche Algorithmen es noch gibt und auch wiederum verschiedene Algorithmen ausprobieren. In jedem Baum sollen drei Prädiktoren (mtry = 3) und einer Anzahl von mindestens zehn Beobachtungen je Knoten (min_n = 10) und wir wollen insgesamt eintausend Bäume wachsen lassen (trees = 1000). Darüber hinaus wollen wir uns auch die Variable Importance wiedergeben lassen. Die Variable Importance beschreibt, wie gut ein Prädiktor über alle Bäume des Waldes, in der Lage war Splits in möglichst reine Knoten durchzuführen. Ein Prädiktor mit einer hohen Variable Importance, ist also besonders geeignet für gute Splits mit hoher Reinheit.\n\nranger_mod &lt;- rand_forest(mtry = 3, min_n = 10, trees = 1000) %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;% \n  set_mode(\"classification\")\n\nNun bauen wir uns wieder unseren Workflow indem wir das Modell mit dem Rezept für die Gummidatensatz verbinden. Das tolle ist jetzt, dass wir hier wieder des Rezept vom Anfang verwenden können. Wir müssen also nicht das Rezept neu definieren. Wir bauen uns also einfach nur einen neuen Workflow.\n\nranger_wflow &lt;- workflow() %&gt;% \n  add_model(ranger_mod) %&gt;% \n  add_recipe(gummi_rec)\n\nWenn wir den Workflow haben, dann können wir wieder mit der Funktion fit() unser Modell anpassen.\n\nranger_fit &lt;- ranger_wflow %&gt;% \n  parsnip::fit(gummi_train_data)\n\nIn der Abbildung 59.7 sehen wir dann die Variable Importance sortiert für alle Prädiktoren. Ganz wichtig, die Variable Importance ist nicht numerisch zu interpretieren und auch nicht über verschiedene Datensäze hinweg. Wir können nur die Variable Importance von einem Datensatz anschauen und dort sehen welche Variablen den meisten Einfluss haben. Wir sehen also, dass die Körpergröße eine sehr große Wichtigkeit hat um die Männer von den Frauen in den Gummibärchendaten zu trennen. Das macht auch Sinn. Frauen und Männer sind nun mal unterschiedlich groß. Nicht mehr so wichtig ist das Alter und das Semester. Beide Prädiktoren haben einen ehr geringeren Einfluss auf die Aufteilung der beiden Geschlechter. Der Lieblingsgeschmack tut bei der Einteilung in Männer und Frauen nichts zur Sache.\n\nranger_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip(num_features = 20) +\n  theme_minimal()\n\n\n\nAbbildung 59.7— Visualisierung der Variable Importance aus unseren ranger Algorithmus.\n\n\n\nNach unserem kleinen Ausflug zu der Variable Importance können wir jetzt wieder unser Modell auf den Testdatensatz anwenden und schauen, wie gut der Random Forest unsere Geschlechter vorhersagen kann.\n\nranger_aug &lt;- augment(ranger_fit, gummi_test_data ) \n\nNun schauen wir uns an wie gut die Klassifizierung mit dem ranger Modell funktioniert hat. Als erstes bauen wir uns einmal die Konfusionsmatrix um zu sehen wie gut die beiden Geschlechter in dem Testdatensatz vorhergesagt wurden.\n\nranger_cm &lt;- ranger_aug %&gt;% \n  conf_mat(gender, .pred_class)\n\nranger_cm\n\n          Truth\nPrediction  m  w\n         m 56 18\n         w 13 44\n\n\nJa, das sieht ähnlich gut aus wie der rpart Algorithmus. Wir haben eine gute Aufspaltung nach dem Geschlechtern. Viele der Beobachtungen liegen auf der Diagonalen und nur wenige Beobachtungen wurden falsch klassifiziert. Jetzt können wir uns noch eine ganze Reihe an anderen Gütekriterien für den Vergleich von Modellen ausgeben lassen.\n\nranger_cm %&gt;% summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.763\n 2 kap                  binary         0.523\n 3 sens                 binary         0.812\n 4 spec                 binary         0.710\n 5 ppv                  binary         0.757\n 6 npv                  binary         0.772\n 7 mcc                  binary         0.525\n 8 j_index              binary         0.521\n 9 bal_accuracy         binary         0.761\n10 detection_prevalence binary         0.565\n11 precision            binary         0.757\n12 recall               binary         0.812\n13 f_meas               binary         0.783\n\n\nWir besprechen wie beim rpart Algorithmus nicht alle Kriterien, du kannst dann gerne nochmal in dem Kapitel über die Modellvergleiche nachlesen, was die ganze Gütekriterien alles bedeuten. Wenn wir uns auf die Accuarcy konzentrieren, erhalten wir einen guten Wert von 84% richtig klassifizierter Geschlechter. Das ist für echte Daten ohne Tuning und Kreuzvaldierung schon ein echt guter Wert.\nNun schauen wir uns noch schnell die ROC Kurve an und sehen, dass die Kurve schon weit von der Diagonalen entfernt ist. Wir sehen eine gute ROC Kurve. Die AUC sollte auch recht groß sein. Damit sind wir mit dem Random Forest Algorithmus soweit durch und wir schauen uns jetzt einen etwas komplexeren xgboost Algorithmus an.\n\nranger_aug %&gt;% \n  roc_curve(gender, .pred_w, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\nAbbildung 59.8— ROC Kurve für den Random Forest mit dem ranger Algorithmus.\n\n\n\n\n\n\n\n\n\nKann ich auch eine Kreuzvalidierung und Tuning für Random Forest durchführen?\n\n\n\nJa, kannst du. Wenn du nur eine Kreuzvalidierung durchführen willst, findest du alles im Kapitel 58 für den \\(k\\)-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf den Random Forest Algorithmus anwenden. Wir nutzen gleich die Kreuzvalidierung in Kombination mit dem Tuning vom xgboost Algorithmus.\nWenn du also den Random Forest Algorithmus auch tunen willst, dann schaue einfach weiter unten nochmal bei dem Tuning des xgboost Algorithmus rein. Es ändert sich kaum was für die Auwahl der Tuning Parameter vom Random Forest Algorithmus."
  },
  {
    "objectID": "classification-randomforest.html#sec-xgboost",
    "href": "classification-randomforest.html#sec-xgboost",
    "title": "59  Decision trees",
    "section": "\n59.5 Gradient boosting mit xgboost",
    "text": "59.5 Gradient boosting mit xgboost\nAls letztes Beispiel für Entscheidungsbäume schauen wir uns das Boosting an. Auch hier haben wir es wieder mit einem Wald an Entscheidungsbäumen zu tun, die wir auch wieder zusammenfassen wollen. Wir verlassen uns also nicht auf die Klassifikation von einem Baum, sondern nehmen die Informationen von vielen Bäumen zusammen. Was ist jetzt der Unterschied zu einem Random Forest? Bei einem Random Forest bauen wir uns im Prinzip hunderte einzelne Bäume und trainieren darauf den Algorithmus. Am Ende fassen wir dann alle Bäume für die Vorhersage zusammen. Beim Boosting nutzen wir die Information des ersten Baumes für das Wachstum des zweiten Baumes und so weiter. Das Boosting verkettet also die Informationen der einzelnen Bäume zu einem kontinuierlichen Lernen. Daher sind Bossting Algorithmen meist sehr gute Klassifizierer.\nWir unterscheiden beim Boosting grob in zwei Kategorien. Zum einen gibt es das adaptive Boosting und das gradient Boosting. Beim adaptiven Boosting erhalten die Beobachtungen über die verschiedenen Klassifizierungsschritte unterschiedliche Gewichte für ihre Bedeutung. In Abbildung 59.9 sehen wir ein Beispiel für den adaboost Algorithmus. Wir haben einen ursprünglichen Datensatz mit blauen und roten Beobachtungen. Wir wollen nun diese Beobachtungen voneinander trennen und damit einen Klassifizierer bauen. Wir fangen mit einem simplen Entscheidungsbaum an, der nur einen Split durchführt. Jetzt haben wir zwei falsch klassifizierte blaue Beobachtungen und eine falsche rote Beobachtung. Nun erhöhen wir das Gewicht dieser drei Beobachtungen. Der nächste Klassifizierer soll nun insbesondere auf diese drei Beobachtungen achten. Wir erhalten daher einen anderen Split und damit zwei blaue Beobachtungen die nicht richtig klassifiziert wurden. Wir erhöhen wieder das Gewicht der beiden falsch klassifizierten blauen Beobachtungen. Der dritte Klassifizierer schafft es jetzt die beiden blauen Beobachtungen gut von den roten Beobachtungen zu trennen. Wir stoppen jetzt hier und bringen alle Klassifiziererregeln, also wo der Split liegen soll, in einen Klassifizierer zusammen.\n\n\nAbbildung 59.9— Darstellung von adaptive Boosting an drei Klassifizieren, die nacheinander auf die neu gewichteten Daten angewendet werden. Am Ende werden alle drei Klassifizierer dann in einen Klassifizierer kombiniert.\n\nIn der Abbildung 59.10 sehen wir die Idee des gradient Boosting einmal dargestellt. Die Idee ist recht simple. Wir wollen wieder nacheinander einen Klassifizierer auf schon klassifizierte Daten anwenden. Wir wollen also das unser zweiter Klassifizierer von dem ersten Klassifizier lernt. Wie machen wir das? Indem wir im ersten Schritt unsere Daten klassifizieren. Wir machen das mit einem Entscheidungsbaum, der mehrere Splits durchführt, die wir dann zu einer eckigen Graden zusammenfassen. Dann haben wir aber einen Fehler als Abstand zu den Splits oder eben zu der Graden. Diese Abstände übertragen wir dann in einen neuen Datensatz auf dem wir dann den nächsten Entscheidungsbaum wachsen lassen. Wir reduzieren also den Fehler des ersten Klassifizierers durch den zweiten Klassifizierer. Dann übertragen wir den Fehler des zweiten Klassifizierers in einen neuen Datensatz und lassen den dritten Klassifizierer den Fehler weiter reduzieren. Am Ende kombinieren wir alle drei Klassifizierer in ein Modell. Durch das gradient Boosting erhalten wir ziemlich gute Entscheidungsbäume, die in der Lage sind sehr schnell und effizient eine Vorhersage zu treffen.\n\n\nAbbildung 59.10— Darstellung von gradient Boosting an drei Klassifizieren, die nacheinander auf die Fehler des vorherigen Klassifizierers angewendet werden. Beachte die Nulllinie bei dem Klassifizierer zwei und drei.\n\nNach dieser theoretischen Einführung wollen wir uns einmal mit der Implementierung beschäftigen. Wir nutzen hier einmal die bekannten Parameter aus dem Random Forest Algorithmus um unseren xgboost Algorithmus zu trainieren. Wie wir gleich noch im Tuning sehen werden, hatr der xgboost Algorithmus noch mehr Parameter an denen du schrauben kannst. In jedem Baum sollen drei Prädiktoren (mtry = 3) und einer Anzahl von mindestens zehn Beobachtungen je Knoten (min_n = 10) und wir wollen insgesamt eintausend Bäume wachsen lassen (trees = 1000).\n\nxgboost_mod &lt;- boost_tree(mtry = 3, min_n = 10, trees = 1000) %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"classification\")\n\nNun bauen wir uns wieder unseren Workflow indem wir das Modell mit dem Rezept für die Gummidatensatz verbinden. Das tolle ist jetzt, dass wir hier wieder des Rezept vom Anfang verwenden können. Wir müssen also nicht das Rezept neu definieren. Wir bauen uns also einfach nur einen neuen Workflow.\n\nxgboost_wflow &lt;- workflow() %&gt;% \n  add_model(xgboost_mod) %&gt;% \n  add_recipe(gummi_rec)\n\nWenn wir den Workflow haben, dann können wir wieder mit der Funktion fit() unser Modell anpassen. Es ist eine wahre Freude. Ich mache das ja jetzt auch schon hier eine Weile im Skript und es ist echt super, wie gut das funktioniert.\n\nxgboost_fit &lt;- xgboost_wflow %&gt;% \n  parsnip::fit(gummi_train_data)\n\nWie auch beim Random Forest Algorithmus können wir uns beim xgboost Algorithmus die Variable Importance wiedergeben lassen. Die Wichtigkeit der Variablen wird in xgboost anhand von drei verschiedenen Wichtigkeiten für eine Variable berechnet. Hier unterscheidet sich dann der Algorithmus xgboost von dem Random Forest Algorithmen. Achtung, wir können nicht einfach die Variable Importance von einem Random Forest Algorithmus mit der eines xgboost Algorithmus vergleichen. Wir kriegen hier andere Werte zurück, die wir dann auch anders interpretieren können.\n\n\nGain ist der relative Beitrag der entsprechenden Variable zum entgültigen Modell. Wir addieren dafür den Beitrag der Variable für die Splits für jeden Baum auf. Eine höhere Punktzahl deutet darauf hin, dass die Variable für die Vorhersage des Baums wichtiger ist. Die Variable war in der Lage die Klassen gut voneinander zu trennen.\n\nCover ist die relative Beobachtung, die mit einem Prädiktor verbunden ist. Also der Anteil der Beobachtungen, die mit dieser Variable zusammenhängen. Nehmen wir an Merkmal \\(X_1\\) wird dazu verwendet, um einen Terminalknoten für 10 Beobachtungen in einem Baum zu erschaffen. Im in einem weiteren Baum ist es ein Terminalkonten mit 20 Beobachtungen. Damit haben wir 30 absolute Beobachtungen, die mit Merkmal \\(X_1\\) verbunden sind. Die relative Beobachtung ist dann 30 geteilt durch die Summe aller absoluten Beobachtungen für alle Merkmale.\n\nHäufigkeit bezieht sich auf die relative Häufigkeit, mit der eine Variable in den zusammengestellten Bäumen vorkommt. Nehmen wir an Merkmal \\(X_1\\) kommt in Baum A in einem Split und in Baum B in zwei Splits vor. Die absolute Häufigkeit von Merkmal \\(X_1\\) ist 3 und die relative Häufigkeit ist dann 3 durch die Summe aller absoluten Vorkommen für alle Merkmale.\n\nSchauen wir uns also einmal die Kriterien der Variable Importance für unsere Gummibärchendaten einmal an. Gehen wir mal die Parameter gain, cover und frequency einmal für unsere Körpergröße durch. Zuerst hat die Körpergröße den höchsten Wert in gain mit \\(0.84\\). Da wir das Gain auf 1 skaliert haben, macht die Körpergröße 84% des gesamten Gain in dem Modell aus. Daher wissen wir, dass die Körpergröße einen überaus bedeutenden Anteil an der Vorhersage des Geschlechts hat. Im Weiteren sehen wir an dem Parameter cover, dass in 34% der Beobachtungen ein Split mit der Körpergröße vorausgeht. Das heißt, 34% der Beobachtungen wurden anhand der Körpergröße aufgeteilt. Da wir nicht wissen wie viele Splits es ingesamt gab, muss man dieses Wert immer etwas vorsichtig bewerten. Die frequency teilt uns mit, dass in 33% der der Splits auch die Körpergröße vor kam. Wir sehen, die Körpergröße ist wichtig für die Vorhersage des Geschlechts. Wenn Variablen fehlen, dann haben diese keinen Einfluss auf die Klassifikation gehabt.\n\nxg_imp &lt;- xgboost_fit %&gt;% \nextract_fit_parsnip() %$% \n  xgboost::xgb.importance(model = fit) %&gt;% \n  mutate(across(where(is.numeric), round, 2))\n\nxg_imp\n\n              Feature Gain Cover Frequency\n1:             height 0.86  0.37      0.35\n2:                age 0.07  0.26      0.26\n3:           semester 0.05  0.25      0.27\n4: most_liked_darkred 0.02  0.08      0.08\n5:   most_liked_white 0.00  0.00      0.00\n6:   most_liked_green 0.00  0.04      0.03\n\n\nIn der Abbildung 59.11 sehen wir dann die Variable Importance sortiert für alle Prädiktoren und eingeteilt in Cluster. Die Funktion xgb.ggplot.importance() versucht ähnlich bedeutende Prädiktoren in gleiche Cluster zuzuordnen.\n\nxg_imp %&gt;% \n  xgb.ggplot.importance() +\n  theme_bw() +\n  scale_fill_okabeito()\n\n\n\nAbbildung 59.11— Visualisierung der Variable Importance aus unseren xgboost Algorithmus. Wir sehen, dass sich grob drei Gruppen für Bedeutung der Variablen für die Klassifikation gebildet haben.\n\n\n\nNach unserem kleinen Ausflug zu der Variable Importance können wir jetzt wieder unser xgboost Modell auf den Testdatensatz anwenden und schauen, wie gut das gradient Boosting unsere Geschlechter vorhersagen kann.\n\nxgboost_aug &lt;- augment(xgboost_fit, gummi_test_data ) \n\nNun schauen wir uns an wie gut die Klassifizierung mit dem xgboost Modell funktioniert hat. Als erstes bauen wir uns einmal die Konfusionsmatrix um zu sehen wie gut die beiden Geschlechter in dem Testdatensatz vorhergesagt wurden.\n\nxgboost_cm &lt;- xgboost_aug %&gt;% \n  conf_mat(gender, .pred_class)\n\nxgboost_cm\n\n          Truth\nPrediction  m  w\n         m 59 14\n         w 10 48\n\n\nJa, das sieht ähnlich gut aus wie der Random Forest Algorithmus. Wir haben eine gute Aufspaltung nach dem Geschlechtern. Viele der Beobachtungen liegen auf der Diagonalen und nur wenige Beobachtungen wurden falsch klassifiziert. Jetzt können wir uns noch eine ganze Reihe an anderen Gütekriterien für den Vergleich von Modellen ausgeben lassen.\n\nxgboost_cm %&gt;% summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.817\n 2 kap                  binary         0.631\n 3 sens                 binary         0.855\n 4 spec                 binary         0.774\n 5 ppv                  binary         0.808\n 6 npv                  binary         0.828\n 7 mcc                  binary         0.633\n 8 j_index              binary         0.629\n 9 bal_accuracy         binary         0.815\n10 detection_prevalence binary         0.557\n11 precision            binary         0.808\n12 recall               binary         0.855\n13 f_meas               binary         0.831\n\n\nWier vorher schon besprechen wir nicht alle Kriterien, du kannst dann gerne nochmal in dem Kapitel über die Modellvergleiche nachlesen, was die ganze Gütekriterien alles bedeuten. Wenn wir uns auf die Accuarcy konzentrieren, erhalten wir einen guten Wert von 86% richtig klassifizierter Geschlechter. Besonders die Sensitivität ist mit 92% sehr gut. Die Sensitivität gibt ja an, wie zuverlässig unser xgboost Algorithmus erkennt, ob man eine Frau ist. Die Spezifität ist etwas niedriger, also die Fähigkeit die Männer auch als Männer zu erkennen. Das ist für echte Daten ohne Tuning und Kreuzvaldierung schon ein echt sehr guter Wert. Da sind wir noch besser als beim Random Forest.\nNun schauen wir uns noch schnell die ROC Kurve an und sehen, dass die Kurve schon weit von der Diagonalen entfernt ist. Wir sehen eine gute ROC Kurve. Die AUC sollte auch recht groß sein. In den folgenden Schritten wollen wir einmal den xgboost Algorithmus tunen und schauen, ob wir noch bessere Ergebnisse für die Klassifikation mit anderen Parametern für den Algorithmus hin bekommen.\n\nxgboost_aug %&gt;% \n  roc_curve(gender, .pred_w, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\nAbbildung 59.12— ROC Kurve für den Entscheidungsbaum mit dem xgboost Algorithmus.\n\n\n\n\n\n\n\n\n\nKann ich auch eine Kreuzvalidierung für xgboost durchführen?\n\n\n\nJa, kannst du. Wenn du nur eine Kreuzvalidierung durchführen willst, findest du alles im Kapitel 58 für den \\(k\\)-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf den xgboost Algorithmus anwenden. Wir nutzen gleich die Kreuzvalidierung in Kombination mit dem Tuning vom xgboost Algorithmus."
  },
  {
    "objectID": "classification-randomforest.html#tuning",
    "href": "classification-randomforest.html#tuning",
    "title": "59  Decision trees",
    "section": "\n59.6 Tuning",
    "text": "59.6 Tuning\nWas heißt Tuning? Wie bei einem Auto können wir an verschiedenen Stellschrauben bei einem mathematischen Algorithmus schrauben. Welche Schrauben und Teile das sind, hängt dann wieder vom Algorithmus ab. Im Falle des xgboost Algorithmus können wir an folgenden Parametern drehen und jeweils schauen, was dann mit unserer Vorhersage passiert. Insgesamt hat der xgboost Algorithmus acht Tuningparameter, wir wählen jetzt für uns hier drei aus. Ich nehme hier auch nur drei Parameter, da sich dann drei Parameter noch sehr gut visuell darstellen lassen. In der Anwendung wäre dann natürlich besser alle Parameter zu tunen, aber das dauert dann auch lange.\n\n\nmtry, zufällig ausgewählte Anzahl an Variablen für jeden Baum. Das heißt, für jeden Baum werden von unseren Variablen die Anzahl mtry zufällig ausgewählt und auf diesem kleineren Datensatz der Baum erstellt.\n\nmin_n, kleinste Knotengröße, die noch akzeptiert wird. Wenn ein Knoten unter min_n fällt, dann endet hier das Wachstum des Baumes.\n\ntrees, Anzahl der Bäume die in einem xgboost Algorithmus erstellt werden.\n\nNun ist es so, dass wir natürlich nicht händisch alle möglichen Kombinationen von der Anzahl der ausgewählten Variablen pro Baum, der kleinsten Knotengröße und der Anzahl der Bäume berechnen wollen. Das sind ziemlich viele Kombinationen und wir kommen dann vermutlich schnell durcheinander. Deshalb gibt es die Funktion tune() aus dem R Paket tune, die uns einen Prozess anbietet, das Tuning automatisiert durchzuführen.\nAls erstes müssen wir uns ein Objekt bauen, das aussieht wie ein ganz normales Modell in der Klassifikation. Aber wir ergänzen jetzt noch hinter jeder zu tunenden Option noch die Funktion tune(). Das sind die Parameter des Algorithmus, die wir später tunen wollen.\n\ntune_spec &lt;-  boost_tree(mtry = tune(), \n                         min_n = tune(), \n                         trees = tune()) %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"classification\")\n\ntune_spec\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = tune()\n  min_n = tune()\n\nComputational engine: xgboost \n\n\nJetzt bauen wir uns den Workflow indem wir statt unserem Modell, die Tuninganweisung in den Workflow reinnehmen. Echt simpel und straightforward. Das Rezept bleibt ja das Gleiche.\n\ngummi_tune_wflow &lt;- workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(gummi_rec)\n\nJetzt müssen wir noch alle Kombinationen aus den drei Parametern mtry, min_n und trees ermitteln. Das macht die Funktion grid_regular(). Es gibt da noch andere Funktionen in dem R Paket tune, aber ich konzentriere mich hier auf die einfachste. Jetzt müssen wir noch die Anzahl an Kombinationen festlegen. Ich möchte für jeden Parameter fünf Werte tunen. Daher nutze ich hier die Option levels = 5 auch damit hier die Ausführung nicht so lange läuft. Fange am besten mit levels = 5 an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert. Dann kannst du die Levels noch hochschrauben. Beachte aber, dass mehr Level nur mehr Zwischenschritte bedeutet. Jede Option hat eine Spannweite range, die du dann anpassen musst, wenn du höhere Werte haben willst. Mehr Level würden nur mehr Zwischenschritte bedeuten. In unserem Fall weiß zum Beispiel die Funktion mtry() nicht, wie viele Variablen in dem Datensatz sind. Wir müssen also die range für die Anzahl an ausgewählten Variablen selber setzen. Ich wähle daher eine Variable bis vier Variablen.\n\ngummi_grid &lt;- grid_regular(mtry(range = c(1, 4)),\n                           trees(),\n                           min_n(),\n                           levels = 5)\n\nDas Tuning nur auf dem Trainingsdatensatz durchzuführen ist nicht so eine gute Idee. Deshalb nutzen wir hier auch die Kreuzvalidierung. Eigentlich ist eine 10-fache Kreuzvalidierung mit \\(v=10\\) besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur \\(v=5\\) gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.\n\ngummi_folds &lt;- vfold_cv(gummi_train_data, v = 5)\n\nNun bringen wir den Workflow zusammen mit dem Tuninggrid und unseren Sets der Kreuzvaidierung. Daher pipen wir den Workflow in die Funktion tune_grid(). Als Optionen brauchen wir die Kreuzvaldierungsdatensätze und das Tuninggrid. Wenn du control_grid(verbose = TRUE) wählst, dann erhälst du eine Ausgabe wie weit das Tuning gerade ist. Achtung!, das Tuning dauert seine Zeit. Im Falle des xgboost Algorithmus dauert das Tuning zwar nicht so lange, aber immer noch ein paar Minuten. Wenn du dann alle acht Parameter des xgboost Algorithmustunen wollen würdest, dann würde die Berechnung sehr viel länger dauern. Du kannst das Ergebnis des simpleren Tunings auch in der Datei gummi_xgboost_tune_res.rds finden.\n\ngummi_tune_res &lt;- gummi_tune_wflow %&gt;% \n   tune_grid(resamples = gummi_folds,\n             grid = gummi_grid,\n             control = control_grid(verbose = FALSE))\n\nDamit du nicht das Tuning durchlaufen lassen musst, habe ich das Tuning in die Datei gummi_xgboost_tune_res.rds abgespeichert und du kannst dann über die Funktion read_rds() wieder einlesen. Dann kannst du den R Code hier wieder weiter ausführen.\nNachdem das Tuning durchgelaufen ist, können wir uns über die Funktion collect_metrics(), die Ergebnisse des Tunings für jede Kombination der drei Parameter mtry, min_n und trees wiedergeben lassen. Diese Ausgabe ist super unübersichtlich. Ich habe mich ja am Anfange des Abschnitts auch für drei Tuningparameter entschieden, da sich dann diese drei Parameter noch gut visualisieren lassen. Deshalb einmal die Abbildung der mittleren Accuarcy und der mittleren AUC-Werte über alle Kreuzvalidierungen.\n\ngummi_tune_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(trees = as_factor(trees),\n         min_n = as_factor(min_n)) %&gt;%\n  ggplot(aes(mtry, mean, color = min_n, linetype = trees)) +\n  theme_minimal() +\n  geom_line(alpha = 0.6) +\n  geom_point() +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_okabeito()\n\n\n\nAbbildung 59.13— Tuning Kurven für den xgboost Algorithmus.\n\n\n\nDamit wir nicht händisch uns die beste Kombination raussuchen müssen, können wir die Funktion show_best() nutzen. Wir wählen hier die beste Accuarcy und erhalten dann die sortierten Ergebnisse nach der Accuarcy des Tunings.\n\ngummi_tune_res %&gt;%\n  show_best(\"accuracy\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric  .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     2  1000    11 accuracy binary     0.839     5  0.0105 Preprocessor1_Model…\n2     2  1500    11 accuracy binary     0.839     5  0.0105 Preprocessor1_Model…\n3     2  2000    11 accuracy binary     0.839     5  0.0105 Preprocessor1_Model…\n4     4   500    11 accuracy binary     0.839     5  0.0138 Preprocessor1_Model…\n5     4  1000    11 accuracy binary     0.839     5  0.0138 Preprocessor1_Model…\n\n\nDas war die Funktion show_best() aber wir können uns auch die gleich die besten Parameter nach der Accuracy raus ziehen. Das Rausziehen der besten Parameter macht für uns die Funktion select_best().\n\nbest_xgboost &lt;- gummi_tune_res %&gt;%\n  select_best(\"accuracy\")\n\nbest_xgboost\n\n# A tibble: 1 × 4\n   mtry trees min_n .config               \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 \n1     2  1000    11 Preprocessor1_Model033\n\n\nWir sehen, dass wir mtry = 3 wählen sollten. Dann müssen wir als Anzahl der Bäume trees = 1000 nutzen. Die minimale Anzahl an Beobachtungen pro Knoten ist dann 11. Müssen wir jetzt die Zahlen wieder in ein Modell eingeben? Nein, müssen wir nicht. Mit der Funktion finalize_workflow() können wir dann die besten Parameter aus unserem Tuning gleich mit dem Workflow kombinieren. Dann haben wir unseren finalen, getunten Workflow. Du siehst dann auch in der Ausgabe, dass die neuen Parameter in dem xgboost Algorithmus übernommen wurden.\n\nfinal_gummi_wf &lt;- gummi_tune_wflow %&gt;% \n  finalize_workflow(best_xgboost)\n\nfinal_gummi_wf \n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_mean()\n• step_impute_bag()\n• step_range()\n• step_dummy()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 2\n  trees = 1000\n  min_n = 11\n\nComputational engine: xgboost \n\n\nJetzt bleibt uns nur noch der letzte Fit übrig. Wir wollen unseren finalen, getunten Workflow auf die Testdaten anwenden. Dafür gibt es dann auch die passende Funktion. Das macht für uns die Funktion last_fit(), die sich dann die Informationen für die Trainings- und Testdaten aus unserem Datensplit von ganz am Anfang extrahiert.\n\nfinal_fit &lt;- final_gummi_wf %&gt;%\n  last_fit(gummi_data_split) \n\nDa wir immer noch eine Kreuzvaldierung rechnen, müssen wir dann natürlich wieder alle Informationen über alle Kreuzvaldierungsdatensätze einsammeln. Dann erhalten wir unsere beiden Gütekriterien für die Klassifikation des Geschlechts unser Studierenden nach dem xgboost Algorithmus. Die Zahlen sind schon gut für echte Daten. Eine Accuracy von 84% bedeutet das wir über acht von zehn Studierenden richtig klassifizieren. Die AUC ist auch schon fast hervorragend, wir bringen kaum Label durcheinander.\n\nfinal_fit %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.817 Preprocessor1_Model1\n2 roc_auc  binary         0.882 Preprocessor1_Model1\n\n\nDann bleibt uns nur noch die ROC Kurve zu visualisieren. Da wir wieder etwas faul sind, nutzen wir die Funktion autoplot(). Als Alternative geht natürlich auch das R Paket pROC, was eine Menge mehr Funktionen und Möglichkeiten bietet.\n\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(gender, .pred_w, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\nAbbildung 59.14— ROC Kurve für den Entscheidungsbaum mit dem xgboost Algorithmus nach der Kreuvalidierung und dem Tuning.\n\n\n\nEine gute ROC Kurve würde senkrecht nach oben gehen und dann waagrecht nach rechts. Dann hätten wir eine AUC von 1 und eine perfekte Separation der beiden Label durch unseren Algorithmus. Unser Algorithmus würde jedem weiblichen Studierenden in dem Testdatensatz korrekt dem Geschlecht w zuweisen. Da wir eine ROC Kurve hier vorliegen haben, die sehr weit weg von der Diagonalen ist, haben wir sehr viele richtig vorhergesagte Studierende in unseren Testdaten. Unser Modell funktioniert um das Geschlecht von Studierenden anhand unserer Gummibärchendaten vorherzusagen."
  },
  {
    "objectID": "classification-svm.html#genutzte-r-pakete",
    "href": "classification-svm.html#genutzte-r-pakete",
    "title": "60  Support vector machines",
    "section": "\n60.1 Genutzte R Pakete",
    "text": "60.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor, see, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\nconflict_prefer(\"set_names\", \"magrittr\")\n##\nset.seed(2025429)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-svm.html#daten",
    "href": "classification-svm.html#daten",
    "title": "60  Support vector machines",
    "section": "\n60.2 Daten",
    "text": "60.2 Daten\nIn diesem Kapitel wollen wir uns auch auf einen echten Datensatz konzentrieren. Wir nutzen daher einmal den Gummibärchendatensatz. Als unser Label und daher als unser Outcome nehmen wir das Geschlecht gender. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir nur die Spalte Geschlecht sowie als Prädiktoren die Spalten most_liked, age, semester, und height.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") %&gt;% \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) %&gt;% \n  select(gender, most_liked, age, semester, height) %&gt;% \n  drop_na(gender)\n\nWir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein NA in der Variable gender haben. Wir haben dann insgesamt \\(n = 522\\) Beobachtungen vorliegen. In Tabelle 56.5 sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.\n\n\n\n\nTabelle 60.1— Auszug aus dem Daten zu den Gummibärchendaten.\n\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\nm\nlightred\n35\n10\n193\n\n\nw\nyellow\n21\n6\n159\n\n\nw\nwhite\n21\n6\n159\n\n\nw\nwhite\n36\n10\n180\n\n\nm\nwhite\n22\n3\n180\n\n\nm\nwhite\nNA\nNA\nNA\n\n\n…\n…\n…\n…\n…\n\n\nw\nyellow\n14\n0\n173\n\n\nw\nnone\n11\n0\n159\n\n\nw\ndarkred\n12\n0\n158\n\n\nw\ndarkred\n12\n0\n158\n\n\nw\ndarkred\n12\n0\n156\n\n\nw\ndarkred\n12\n0\n158\n\n\n\n\n\n\nUnsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Um die Klassifikation mit Entscheidungsbäumen rechnen zu können brauchen wir wie bei allen anderen Algorithmen auch einen Trainings- und Testdatensatz. Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf.\nIm maschinellen Lernen sind alle Datensätze, die weniger als tausend Beobachtungen vorliegen haben, klein.\n\ngummi_data_split &lt;- initial_split(gummi_tbl, prop = 3/4)\n\nWir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluß einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.\n\ngummi_train_data &lt;- training(gummi_data_split)\ngummi_test_data  &lt;- testing(gummi_data_split)\n\nNachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht gender als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem . Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_train_data) %&gt;% \n  step_impute_mean(all_numeric_predictors()) %&gt;% \n  step_impute_bag(all_nominal_predictors()) %&gt;% \n  step_range(all_numeric_predictors(), min = 0, max = 1) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_nzv(all_predictors())\n\ngummi_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Bagged tree imputation for: all_nominal_predictors()\n\n\n• Range scaling to [0,1] for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_predictors()\n\n\nAlles in allem haben wir ein sehr kleines Modell. Wir haben ja nur ein Outcome und vier Prädiktoren."
  },
  {
    "objectID": "classification-svm.html#theoretischer-hintergrund",
    "href": "classification-svm.html#theoretischer-hintergrund",
    "title": "60  Support vector machines",
    "section": "\n60.3 Theoretischer Hintergrund",
    "text": "60.3 Theoretischer Hintergrund\nDer theoretische Hintergrund zu dem SVM Algorithmus ist sehr mathematisch. So mathematisch, dass wir hier daraus keinen tieferen Nutzen mehr ziehen. Hier geht es ja um die Anwendung des SVM Algorithmus und nicht um das tiefere mathematische Verständnis. Wie immer gibt es sehr viele Möglichkeiten sich tiefer mit der Mathematik hinter dem SVM Algorithmus zu beschäftigen. Hier wollen wir das nicht.\n\n\nEs gibt wir immer ein schönes (mathematisches) Tutorial zu den Support vector machines. Von dort ist auch das Beispiel mit den farbigen Kugeln entnommen.\nDaher wollen wir mal den SVM Algorithmus etwas anders verstehen. Wir nutzen wieder die Idee, dass wir farbige Punkte oder Bälle voneinander trennen wollen. Im Prinzip kannst du dir die Bälle in der Abbildung 60.1 genau so vorstellen. Wir haben dort sieben gesunde Personen als blaue Kugeln und vier kranke Personen als rote Kugeln, die wir trennen wollen.\n\n\nAbbildung 60.1— Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Die blauen Kugeln stellen die Personen und die rote die kranken Personen dar.\n\nIn Abbildung 60.2 zeichnen wir eine Gerade, die die Patienten gut voneinander trennt. Auf der einen Seite der Geraden sind die sieben gesunden Patienten und auf der anderen Seite der Geraden die vier kranken Personen.\n\n\nAbbildung 60.2— Darstellung von 11 Beobachtungen aus dem Traingsdatensatz. Wir trennen die gesunden Patienten von den kranken Patienten mit einer Geraden.\n\nNun kommt zu unserem Trainingsdatensatz ein Schwall neuer Patienten hinzu und wir ergänzen die Beobachtungen in der Abbildung 60.3. Wir haben immer noch unsere ursprüngliche Gerade, aber diese Gerade trennt die neuen Beobachtungen nicht mehr gut auf. Ein kranker Patient ist auf der falschen Seite der Geraden. Es gibt wahrscheinlich einen besseren Platz, um die Gerade jetzt zu platzieren.\n\n\nAbbildung 60.3— Darstellung von elf gesunden Beobachtungen und acht kranken Beobachtungen aus dem neuen, angewachsenen Traingsdatensatz. Die Gerade trennt die Beobachtugen nur noch ungünstig.\n\nIn der Abbildung 60.4 sehen wir die Vorgegehensweise des SVM Algorithmus. Der SVM Algorithmus versucht die Gerade an der bestmöglichen Stelle zu platzieren, indem der Algorithmus auf beiden Seiten der Geraden einen möglichst großen Abstand einhalten.\n\n\nAbbildung 60.4— Visualisierung des SVM Algorithmus an den ursprünglichen elf Beobachtungen.\n\nWenn wir jetzt in der Abbildung 60.5 wieder zu unserem angewachsenen Trainingsdaten zurückkehren, sehen wir, dass unsere Klassifikation der gesunden und kranken Beobachtungen gut funktioniert. Der SVM Algorithmus hat durch den optimierten Abstand der Geraden einen optimalen Klassifikator gefunden.\n\n\nAbbildung 60.5— Darstellung von elf gesunden Beobachtungen und acht kranken Beobachtungen aus dem neuen, angewachsenen Traingsdatensatz mit der SVM optimierten Geraden.\n\nNun gibt es aber neben der Geraden noch einen anderen Trick, den wir mit dem SVM Algorithmus durchführen können. Schauen wir uns dazu einmal die Abbildung 60.6 an. Wir sehen in dem neuen Trainingsdatensatz fünf gesunde und fünf kranke Beobachtungen. nur sind diese Beobachtungen nicht mehr so verteilt, dass wir die Beobachtungen mit einer Geraden trennen könnten. Hier kommt jetzt der Kerneltrick des SVM Algorithmus zu tragen.\n\n\nAbbildung 60.6— Darstellung von zehn Beobachtungen aus einem weiteren Traingsdatensatz. Die blauen Kugeln stellen die fünf gesunden Personen und die rote die fünf kranken Personen dar.\n\nWir können mit keiner Geraden der Welt die Punkte voneinander trennen. Jetzt nutzen wir den Kerneltrick in Abbildung 60.7 um unsere 2-D Abbildung in eine 3-D Abbildung umzuwandeln. Jetzt können wir mit einer Ebene die Patienten voneinander trennen. Wir bringen also unsere Beobachtungen durch eine Transformation in eine andere Dimension und können in dieser Dimension die Beobachtungen mit einer Ebene trennen.\n\n\nAbbildung 60.7— Umwandlung des Input Space in einen beliebigen Feature Space durch den Kernel \\(\\Phi\\).\n\nWenn wir dann die Ebene wieder zurücktransfomieren erhalten wir eine kurvige Linie, die unsere Beobachtungen in Abbildung 60.8 voneinander trennt.\n\n\nAbbildung 60.8— Rücktransformation der Ebene aus dem Feature Space in den Input Space. Wir haben dann eine Schlangenlinie, die die Beobachtungen voneinander trennt.\n\nDas war jetzt eine sehr bildliche Darstellung des SVM Algorithmus. Aber im Prinzip ist das die Idee. Wir machen den Kernel Trick nur matematisch komplizierter und auch die Rücktransformation ist nicht simpel. Das müssen wir aber auch nicht selber für uns machen, denn dafür haben wir ja einen Computer. Das eigentliche Problem ist die Wahl des korrekten Kernels. Und das ist eigentlich auch die Qual der Wahl. Wir müssen vorab festlegen, welcher Kernel es sein soll. Und da geht dann das Tuning los."
  },
  {
    "objectID": "classification-svm.html#svm-algorithm",
    "href": "classification-svm.html#svm-algorithm",
    "title": "60  Support vector machines",
    "section": "\n60.4 SVM Algorithm",
    "text": "60.4 SVM Algorithm\nLeider ist es nicht so, dass wir eine SVM Funktion haben. Wir haben insgesamt drei Funktionen. Jede dieser Funktionen entspricht einem Kernel und muss getrennt voneinander einem Tuning unterzogen werden. Wir haben folgende Funktionen mit den entsprechenden Kernels zu Verfügung.\n\n\nsvm_linear heißt, wir nehmen einen linearen Zusammenhang an. Wir können die Beobachtungen mit einer einfachen Gerade voneinander trennen.\n\nsvm_poly heißt, wir nehmen ein Polynom eines bestimmten Gerades und glauben, dass wir mit diesem Kernel die Beobachtungen voneinander trennen können.\n\nsvm_rbf_mod heißt, wir haben einen radialen Kernel und hoffen, dass wir mit einer radialen Funktion die Beobachtungen trennen können.\n\nUnd damit geht das Leid eigentlich schon los. Wir können gar nicht wissen, welcher der drei SVM Algorithmen am besten auf unsere Daten passt. Also müssen wir alle drei einemal anwenden. Dann müssten wir eigentlich auch alle drei Algorithmen einem Tuning unterziehen. Du siehst, es wird viel Arbeit. Wir lassen hier das Tuning weg und ich zeige dir, wie du mit der Funktion map() dir etwas Arbeit ersparen kannst.\nAls erstes wollen wir den linearen Kernel einmal definieren. Wir haben hier zwei Parameter die wir einem Tuning unterziehen könnten.\n\nsvm_lin_mod &lt;- svm_linear(cost = 1, margin = 0.1) %&gt;% \n  set_engine(\"kernlab\") %&gt;% \n  set_mode(\"classification\") \n\nAls zweites schauen wir uns den polynomilane Kernel an und setzen einmal den Grade des Polynomes auf vier. Einfach mal so aus dem Bauch raus um zu zeigen, was dann so passieren kann.\n\nsvm_poly_mod &lt;- svm_poly(cost = 1, margin = 0.1, degree = 4) %&gt;% \n  set_engine(\"kernlab\") %&gt;% \n  set_mode(\"classification\") \n\nAls letztes schauen wir uns noch den radialen Kernel einmal an. Auch hier haben wir nur zwei Tuningparameter zu Verfügung.\n\nsvm_rbf_mod &lt;- svm_rbf(cost = 1, margin = 0.1) %&gt;% \n  set_engine(\"kernlab\") %&gt;% \n  set_mode(\"classification\") \n\nJetzt machen wir alles in einem Schritt. Was wir vorher in mehreren Schritten gemaht haben, machen wir jetzt auf einer Liste lst() in der die Modelle der drei Kernel definiert sind. Wir nutzen die Funktion map() um auf dieser Liste die Workflows mit dem Rezept der Gummibärchen zu initialisieren. Dann Pipen wir die Workflows weiter in die fit() Funktion und wollen dann danach auch gleich die Vorhersage auf dem Testdatensatz rechnen. Danach wählen wir dann auf allen Listen noch gender und die Vorhersagen als die pred-Spalten aus.\n\nsvm_aug_lst &lt;- lst(svm_lin_mod,\n                   svm_poly_mod,\n                   svm_rbf_mod) %&gt;% \n  map(~workflow(gummi_rec, .x)) %&gt;% \n  map(~fit(.x, gummi_train_data)) %&gt;% \n  map(~augment(.x, gummi_test_data)) %&gt;% \n  map(~select(.x, gender, matches(\"pred\")))\n\n Setting default kernel parameters  \n\nsvm_aug_lst\n\n$svm_lin_mod\n# A tibble: 131 × 4\n   gender .pred_class .pred_m .pred_w\n   &lt;fct&gt;  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 m      m            0.873   0.127 \n 2 w      w            0.0422  0.958 \n 3 m      m            0.731   0.269 \n 4 w      w            0.160   0.840 \n 5 m      m            0.813   0.187 \n 6 m      m            0.983   0.0174\n 7 w      w            0.0428  0.957 \n 8 m      m            0.975   0.0255\n 9 m      w            0.502   0.498 \n10 m      w            0.477   0.523 \n# ℹ 121 more rows\n\n$svm_poly_mod\n# A tibble: 131 × 4\n   gender .pred_class .pred_m .pred_w\n   &lt;fct&gt;  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 m      m             0.621   0.379\n 2 w      w             0.401   0.599\n 3 m      m             0.516   0.484\n 4 w      m             0.518   0.482\n 5 m      m             0.537   0.463\n 6 m      m             0.715   0.285\n 7 w      w             0.450   0.550\n 8 m      m             0.658   0.342\n 9 m      w             0.465   0.535\n10 m      m             0.744   0.256\n# ℹ 121 more rows\n\n$svm_rbf_mod\n# A tibble: 131 × 4\n   gender .pred_class .pred_m .pred_w\n   &lt;fct&gt;  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 m      m            0.926   0.0745\n 2 w      w            0.0502  0.950 \n 3 m      m            0.854   0.146 \n 4 w      w            0.239   0.761 \n 5 m      m            0.887   0.113 \n 6 m      m            0.941   0.0593\n 7 w      w            0.0353  0.965 \n 8 m      m            0.972   0.0283\n 9 m      w            0.467   0.533 \n10 m      m            0.559   0.441 \n# ℹ 121 more rows\n\n\nJetzt haben wir also alles als eine Liste vorliegen. Das macht uns dann die weitere Darstellung einfach. Wenn du einen Listeneintrag haben willst, dann kannst du auch mit der Funktion pluck() dir einen Eintrag nach dem Namen herausziehen. Wenn du den Listeneintrag $svm_rbf_mod willst, dann nutze pluck(svn_aug_lst, \"svm_rbf_mod\").\n\n\n\n\n\n\nKann ich auch eine Kreuzvalidierung und Tuning für die Support Vector Machines durchführen?\n\n\n\nJa, kannst du. Wenn du nur eine Kreuzvalidierung durchführen willst, findest du alles im Kapitel 58 für den \\(k\\)-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf den Support Vector Machine Algorithmus anwenden. Wenn du den Support Vector Machine Algorithmus auch tunen willst, dann schaue einfach nochmal im Kapitel 59.5 zum Tuning von xgboost rein.\n\n\nJetzt lassen wir uns auf der Liste der Vorhersagen nochmal für alle Kernel der SVM Algorithmen die Konfusionsmatrizen ausgeben.\n\nsvm_cm &lt;- svm_aug_lst %&gt;%\n  map(~conf_mat(.x, gender, .pred_class))\nsvm_cm\n\n$svm_lin_mod\n          Truth\nPrediction  m  w\n         m 51 12\n         w 18 50\n\n$svm_poly_mod\n          Truth\nPrediction  m  w\n         m 49 26\n         w 20 36\n\n$svm_rbf_mod\n          Truth\nPrediction  m  w\n         m 54 12\n         w 15 50\n\n\nDas sieht doch recht gut aus. Nur unser Polynomerkernel hat anscheinend Probleme die Geschlechter gut voneinander aufzutrennen. Du siehst, hier muss eben auch ein Tuning her. Selber den Grad des Polynoms zu treffen das passt ist sehr schwer oder eigentlich nur mit Glück hinzukriegen.\nIm folgenden Schritt müssen wir uns etwas strecken. Ich will nämlich die summary() Funktion auf die Konfusionsmatrizen anwenden und dann die drei Ausgaben in einem Datensatz zusammenführen. Wir haben dann die Metriknamen als eine Spalte und dann die drei Spalten für die Zahlenwerte der drei Methoden.\n\nsvm_cm %&gt;% \n  map(summary)  %&gt;% \n  map(~select(.x, .metric, .estimate)) %&gt;% \n  reduce(left_join, by = \".metric\") %&gt;% \n  set_names(c(\"metric\", \"linear\", \"poly\", \"radial\")) %&gt;% \n  mutate(across(where(is.numeric), round, 3))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 13 × 4\n   metric               linear  poly radial\n   &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 accuracy              0.771 0.649  0.794\n 2 kap                   0.543 0.292  0.588\n 3 sens                  0.739 0.71   0.783\n 4 spec                  0.806 0.581  0.806\n 5 ppv                   0.81  0.653  0.818\n 6 npv                   0.735 0.643  0.769\n 7 mcc                   0.545 0.293  0.588\n 8 j_index               0.546 0.291  0.589\n 9 bal_accuracy          0.773 0.645  0.795\n10 detection_prevalence  0.481 0.573  0.504\n11 precision             0.81  0.653  0.818\n12 recall                0.739 0.71   0.783\n13 f_meas                0.773 0.681  0.8  \n\n\nWenn wir wieder auf unsere Accuracy als unser primäres Gütemaß schauen, dann sehen wir, dass wir hier ohne Tuning mit dem linearen Kernel am besten fahren würden. Auch sind die anderen Werte meistens für den linearen Kernel am besten. Daher würde ich mich hier für den linearen Kernel entscheiden. Die Frage wäre natürlich, ob die anderen Kernel mit einem Tuning nicht besser wären. Aber diese Frage lassen wir mal offen im Raum stehen.\nSchauen wir uns in einem letzten Schritt noch die ROC Kurven für die drei Kernels an. Dafür müssen wir einen Datensatz aus der Liste bilden nachdem wir die Sensitivität und Spezifität für die drei Kernels in der Listenform berechnet haben. Wir können dafür die Funktion bind_rows() nutzen.\n\nroc_tbl &lt;- svm_aug_lst %&gt;% \n  map(~roc_curve(.x, gender, .pred_w, event_level = \"second\")) %&gt;% \n  bind_rows(.id = \"model\")\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the yardstick package.\n  Please report the issue at &lt;https://github.com/tidymodels/yardstick/issues&gt;.\n\n\nIn Abbildung 60.9 sehen wir die drei ROC Kurven für die drei Kernels. Wie zu erwarten war, ist der lineare Kernel der beste Kernel. Das hatten wir ja schon oben in der Zusammenfassung der Konfusionsmatrix gesehen. Auch hier zeigt sich sehr schön, wie schlecht dann unser polynominaler Kernel ist. Das war jetzt hier zur Demonstration, aber dennoch zeigt es wie wichtig ein gutes Tuning ist.\n\nroc_tbl %&gt;% \n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  theme_minimal() +\n  geom_path() +\n  geom_abline(lty = 3) + \n  scale_color_okabeito()\n\n\n\nAbbildung 60.9— Darstellung der Vorhersagegüte der drei Modelle linear, polynomial und radial.\n\n\n\nDamit wären wir auch schon am Ende des Kapitels über den SVM Algorithmus. Wie du schon merkst, müssen wir viel rechnen, wenn wir mit den SVM Kerneln was Vorhersagen wollen. Wenn wir den richtigen Kernel gefunden haben, dann können wir auch eine gute Vorhersage erreichen. Nun müssen auch diesen Kernel erstmal algorithmisch finden, dass heißt also viele Kernels ausprobieren. Und am Ende ist natürlich die Implementierung hier im genutzten R Paket parsnip nicht die Weisheit letzter Schluss. Es gibt noch sehr viel mehr R Pakete, die sich mit SVM Algorithmen beschäftigen. Aber das wäre dann eine Literatursuche für dich. Vorerst endet das Kapitel jetzt hier."
  },
  {
    "objectID": "classification-neural-networks.html#genutzte-r-pakete",
    "href": "classification-neural-networks.html#genutzte-r-pakete",
    "title": "61  Neural networks",
    "section": "\n61.1 Genutzte R Pakete",
    "text": "61.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, tidymodels, magrittr, \n               janitor, keras, tensorflow, see,\n               neuralnet, NeuralNetTools,\n               OneR, readxl, \n               conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflict_prefer(\"extract\", \"magrittr\")\n##\nset.seed(2025429)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "classification-neural-networks.html#neuronale-netzwerke-theoretisch",
    "href": "classification-neural-networks.html#neuronale-netzwerke-theoretisch",
    "title": "61  Neural networks",
    "section": "\n61.2 Neuronale Netzwerke theoretisch",
    "text": "61.2 Neuronale Netzwerke theoretisch\nNeuronale Netze ordnen Inputs den Outputs zu. Wir haben also Eingaben und erhalten eine Ausgabe zurück. Neuronale Netze finden Korrelationen. Neuronale Netzwerke sin auch als “universeller Approximator” bekannt, weil dad Netzwerk lernen kann, eine unbekannte Funktion \\(f(x) = y\\) zwischen einer beliebigen Eingabe \\(x\\) und einer beliebigen Ausgabe \\(y\\) zu approximieren. Dabei gilt die Vorraussetzung, dass \\(x\\) und \\(y\\) in einem Zusammenhang durch Korrelation oder Kausalität stehen. Während des Lernprozesses findet ein neuronales Netz das richtige \\(f()\\) oder die richtige Art der Umwandlung von \\(x\\) in \\(y\\), sei es \\(f(x) = 3x + 12\\) oder \\(g(f(x)) = 9x - 0.1\\). Wie du sehen kannst, gibt es auch bei dem neuralen Netzwerk eigentlich um ein Modell. Und unser Modell ist nicht anders, als eine multiple lineare Regresion in der klassischen Statistik.\nDeep Learning ist der Name, den wir für gestapelte neuronale Netze verwenden und damit meinen wir Netze, die aus mehreren Schichten bestehen. Die einzelnen Schichten bestehen aus Knotenpunkten. Ein Knoten ist einfach ein Ort, an dem Berechnungen stattfinden, frei nach dem Vorbild eines Neurons im menschlichen Gehirn, das feuert, wenn es auf ausreichende Reize trifft. Ein Knoten kombiniert Eingaben aus den Daten mit einer Reihe von Koeffizienten oder Gewichten, die diese Eingaben entweder verstärken oder abschwächen. Somit geben dann die Knoten den Eingaben eine Bedeutung im Hinblick auf die Aufgabe die der Algorithmus zu lernen versucht. Häufig ist dies die Aufgabe die Eingabe zu finden die am hilfreichsten die Daten fehlerfrei klassifiziert? Diese Eingangsgewichtungen werden summiert, und die Summe wird dann durch die so genannte Aktivierungsfunktion eines Knotens geleitet, um zu bestimmen, ob und in welchem Ausmaß dieses Signal weiter durch das Netzwerk geleitet werden soll. Am Ende kann nur ein weitergeleitetes Signal das Endergebnis als einen Klassifizierungsvorgang beeinflussen. Wenn das Signal durch das Neuron durchläuft, ist dieses Neuron “aktiviert” worden.\nIn Abbildung 61.5 ist ein Diagramm dargestellt, dass einen Knoten darstellt. Wir haben immer ein Inputlayer in dem wir hier drei Inputneuronen \\(x_1\\), \\(x_2\\) und \\(x_3\\) finden. Das sind auch unsere Variablen in den Daten, die wir in das Modell stecken. Ganz oben finden wir noch als blaues Neuron ein Biasneuron dargestellt. Du kannst dir das Biasneuron wie den Intercept in der linearen Regresion vorstellen. Jedes der Neuronen hat ein Gewicht \\(w_0\\) bis \\(w_3\\). Diese Gewichte werden durch eine Netzinputfunktion in der Form \\(w_0 + w_1 x_1 + w_2x_3\\) aufsummiert und dann an eine Aktivierungsfunktion weitergeleitet. Die Aktivierungsfunktion entscheidet hierbei, ob das Neuron aktiv wird und damit dann auch die Gewichte weiterleitet oder eben inaktiv wird. Es gibt viele Aktivierungsfunktionen, die alle unterschiedliche Eigenschaften haben. Im Folgenden sind einmal die wichtigisten Aktivierungsfunktionen beschrieben.\n\nDie lineare Aktivierungsfunktion skaliert eine Eingabe einfach um einen Faktor, was bedeutet, dass es eine lineare Beziehung zwischen den Eingaben und der Ausgabe gibt.\nSigmoid-Aktivierungsfunktion ist “S”-förmig. Sie kann der Ausgabe Nichtlinearität hinzufügen und gibt einen binären Wert von 0 oder 1 zurück.\nDie Tanh-Aktivierungsfunktion ist eine Erweiterung der sigmoidalen Aktivierungsfunktion. Daher kann Tanh verwendet werden, um der Ausgabe Nichtlinearität hinzuzufügen. Die Ausgabe liegt im Bereich von -1 bis 1. Die Tanh-Funktion verschiebt das Ergebnis der sigmoiden Aktivierungsfunktion.\nDie Rektifizierte lineare Einheits-Aktivierungsfunktion (RELU) ist eine der am häufigsten verwendeten Aktivierungsfunktionen. RELU wird bevorzugt in den Hidden Layer verwendet. Das Konzept ist linear vom Nullpunkt ausgehend. Die RELU fügt der Ausgabe auch Nichtlinearität hinzu. Allerdings kann das Ergebnis von 0 bis unendlich reichen.\nDie Softmax-Aktivierungsfunktion ist eine Erweiterung der Sigmoid-Aktivierungsfunktion. Die Softmax-Funktion fügt der Ausgabe eine Nichtlinearität hinzu. Sie wird jedoch hauptsächlich für Klassifizierungen verwendet, bei denen mehrere Klassen von Ergebnissen berechnet werden können. Wir haben dann einen Multiclass-Fall vorliegen.\n\nIm Prinzip ist eine Aktivierungsfunktion nichts anderes als die Link Funktion in der multiplen linearen Regression. Aber das geht dann hier zu weit. Häufig wird dann die Netzinputfunktion und die Aktivierungsfunktion in einem Knotenpunkt dargestellt.\n\n\nMehr über Aktivierungsfunktionen kannst du im Tutorium Neural Networks In a Nutshell erfahren.\n\n\nAbbildung 61.1— Darstellung von drei Inputneuronen \\(x_1, x_2, x_3\\), einem Biasneuron \\(1\\) mit den jeweiligen weitergeleiteten Gewichten \\(w_1, w_2, w_3\\) und \\(w_0\\). Die Summierungsfunktion sowie die Aktivierungsfunktion werden meist in einen gemeinsamen Knoten dargestellt. Hier sind beide Formen einmal abgebildet. Wenn das Neuron aktiviert ist, gibt es die Summe als Output weiter.\n\nIn der Abbildung 61.2 sehen wir dann ein ganze Netz an Neuronen. Wir haben ein Inputlayer und mehrere Hiddenlayer die am Ende dann in ein Outputlayer enden. Meistens wollen wir eine binäre Klassifikation rechnen, so dass am Ende dann zwi Outputknoten stehen. Die Hiddenlayer können unterschiedlich viele Knoten enthalten und meistens gibt es auch mehrere Abstufungen. Das heißt wir fnagen mit mehreren Knoten pro Hiddenlayer an und reduzieren dann die Anzahl der Knoten pro Hiddenlayer über die Breite des neuronalen Netzwerkes.\n\n\nAbbildung 61.2— Darstellung von drei Inputneuronen \\(x_1, x_2, x_3\\) ohne ein Biasneuron. Die drei Inputbeurnen leiten ihre Gewichte an die Hidden Layer Neoronen weiter. In jedem diesem Neuron findet eine Summiierung in eine eventuelle Aktivierung statt. Aktivierte Neuronen leiten die Summation als Gewichte dann an weitere Hidden Layer Neuronen weiter. Am Ende findet eine Entscheidung in den Outputneuronen statt.\n\nSpannenderweise sind viele Dinge in einem neuronalen Netzwerk nichts anderes als eine intelligente Hintereinanderschaltung von multiple linearen Regressionen Deshalb gibt es in der Tabelle 61.1 auch einmal eine Übersicht der Begriffe in dem Sprachraum der neuronalen Netze und der klassischen logistischen Regression. Wir sehen hier einiges an gleichen Konzepten.\n\n\nTabelle 61.1— Welche Begriff in dem Sprachraum der neuronalen Netze lässt sich zu welchem Begriff in der logistischen Regression zuordnen?\n\n\n\n\n\n\nNeural network\nLogistic regression (eng.)\nLogistische Regression (deu.)\n\n\n\nActivation function\nLink function\nLink Funktion\n\n\nWeights\nCoefficients / Slope\nKoeffizienten / Steigung\n\n\nBias\nIntercept\nIntercept\n\n\nVariance\nResiduals\nFehler / Residuen\n\n\nLearning\nFitting\nModellieren\n\n\n\n\n\n\nWhat is the role of the bias in neural networks?\nWenn ein neuronales Netz auf dem Trainingssatz trainiert wird, wird es mit einer Reihe von Gewichten initialisiert. Diese Gewichte werden dann während der Trainingsperiode optimiert und die optimalen Gewichte werden erzeugt. Das ist ein wichtiger Punkt. Wir erzeugen zufällig die Gewichte am Anfang und lassen uns dann die Gewichte mehr oder minder zufällig weiteroptimieren. Sonst würden ja bei jedem Knoten die gleichen Zahlen rauskommen. Wir optimieren aber nicht nur einmal sondern meistens mehrfach. Das heißt wir lassen das neuronale Netzwerk mehrfach wachsen und optimieren bei jedem Wachstum die Gewicte so, dass der Fehler geringer wird.\nDie Epoche (eng. epoch) ist einer der Eingabeparameter des Algorithmus. Stelle dir die Epoche als eine Schleife vor. Die Schleife bestimmt, wie oft ein Lernalgorithmus die Gewichte aktualisiert. Wenn der Wert der Epoche 1 ist, bedeutet dies, dass das neuronale Netz einmal läuft um die Gewichte zu aktualisieren. Wenn die Epoche einen Wert von 5 hat, wird das neuronale Netzwerk fünfmal aktualisiert. Hier ist der Unterschied zu den Entscheidungsbäumen auffällig. Entscheidungsbäume werden in einem Random Forest gemittelt. Die Epochen eines neuronalen Netzwerkes hängen aber miteinander zusammen.\nEin neuronales Netz ist eine korrigierende Rückkopplungsschleife, die Gewichte belohnt, die seine korrekten Vermutungen unterstützen, und Gewichte bestraft, die es zu Fehlern verleiten.\nDamit wir wissen, ob unser Netzwerk über die Epochen besser wird, brauchen wir eine Verlustfunktion (eng. loss function). Die Verlustfunktion wird auch als Kostenfunktion (eng. cost function) bezeichnet. Sie errechnet den Fehler. Um genau zu sein, ist die Kostenfunktion der Durchschnitt der Verlustfunktionen. Dies ist die Funktion, die der Optimierungsalgorithmus zu minimieren versucht. Es gibt eine große Anzahl von Verlustfunktionen, wie den mittleren quadratischen Fehler oder die binäre Kreuzentropie.\nDie Verlustfunktion sagt dem neuronalen Netz im Wesentlichen, welche Maßnahmen es ergreifen muss, um die Accuracy zu verbessern. Diese Information wird dann verwendet, um genaueren Gewichte zu erzeugen. Danach kann dann das neuronale Netz kann die Daten erneut weiterverarbeiten.\nAm Rande möchte ich noch die Begriffe Forward Propagation und Back Propagation erwähnen. Beide Begriffe beschreiben, wie das Lernen innerhalb eines neuronalen Netzwerk abläuft. Klassisch ist die Forward Propagation. Dabei reicht ein Knoten die Informationen an den nächsten Knoten weiter. Das Lernen erfolgt vorwärts. Die andere Möglichkeit ist, das Netzwerk wachsen zu lassen und dann rückwärts die Gewichte der Knoten zu verbessern. Wir haben dann eine Back Propagation vorliegen."
  },
  {
    "objectID": "classification-neural-networks.html#neuronales-netz-anschaulicher",
    "href": "classification-neural-networks.html#neuronales-netz-anschaulicher",
    "title": "61  Neural networks",
    "section": "\n61.3 Neuronales Netz anschaulicher",
    "text": "61.3 Neuronales Netz anschaulicher\nIn unserem folgenden Beispiel ist Rotkäppchen das neuronale Netz. Rotkäppchen hat folgende Informationen zu drei möglichen Outcomes vorliegen. Rotkäppchen weiß also, dass es im Wald oder im Haus drei Personen treffen kann. Entweder trifft sie die Großmutter, den großen, bösen Wolf oder den Holzfäller. Gott sei Dank kennt Rotkäppchen die Eigenschaften der drei Charaktere und kann daran sich folgende Matrix aufbauen. Wir lesen die Tabelle wie folgt, wir haben die Spalte grosse_ohren und wir haben drei Werte mit der Spalte assoziiert. Wir wissen aber nicht welche Zeile welcher Charakter ist. Wir wollen die Zuordnung einmal mit dem neuronalen Netzwerk durchführen.\n\nlittle_red_tbl &lt;- tibble(grosse_ohren = c(1, 0, 1), \n                         grosse_augen = c(1, 1, 0),\n                         grosse_zaehne = c(1, 0, 0) , \n                         freundlich = c(0, 1, 1), \n                         faltig = c(0, 1, 0), \n                         gutaussehend = c(0, 0, 1),\n                         renn_weg = c(1, 0, 0), \n                         schrei = c(1, 0, 0), \n                         ruf_holzfaeller = c(1, 0, 0), \n                         plaudere = c(0, 1, 1), \n                         geh_hin = c(0, 1, 0), \n                         biete_essen = c(0, 1, 1), \n                         rettung = c(0, 0, 1))\n\nIn der Tabelle 61.2 sehen wir die Daten nbochmal in das Input Layer und das Output Layer aufgespaltet. Die Frage ist, was soll Rotkäppchen tun, wenn die die Eigenschaften des Input Layers beobachtet? Wir wollen jetzt anhand eines neuronalen Netzes die Input Layer dem Output Layer zuordnen.\n\n\nTabelle 61.2— Die beiden Datensätze für das neuronale Netzwerk. Wie lässt sich der Input sinnvoll mit dem Output verbinden? Wir geben dafür drei Hidden Layers vor, die dann die Charaktere Wolf, Goßmutter und den Holzfäller repräsentieren.\n\n\n\n\n(a) Daten des Input Layers.\n\n\ngrosse_ohren\n1\n0\n1\n\n\ngrosse_augen\n1\n1\n0\n\n\ngrosse_zaehne\n1\n0\n0\n\n\nfreundlich\n0\n1\n1\n\n\nfaltig\n0\n1\n0\n\n\ngutaussehend\n0\n0\n1\n\n\n\n\n\n\n(b) Daten des Output Layers.\n\n\nrenn_weg\n1\n0\n0\n\n\nschrei\n1\n0\n0\n\n\nruf_holzfaeller\n1\n0\n0\n\n\nplaudere\n0\n1\n1\n\n\ngeh_hin\n0\n1\n0\n\n\nbiete_essen\n0\n1\n1\n\n\nrettung\n0\n0\n1\n\n\n\n\n\n\nIm Folgenden siehst du einmal den Code für das simple neuronale Netzwerk. Wir haben die Spalten des Input Layer durch das ~ von den Spalten des Output Layers getrennt. Darüber hinaus wollen wir noch drei Hidden Layer Knoten haben. Jeweils einen Knoten für jeden unserer drei Charaktere.\n\nneuralnetwork &lt;- neuralnet(renn_weg + schrei + ruf_holzfaeller + plaudere + \n                             geh_hin + biete_essen + rettung ~ \n                             grosse_ohren + grosse_augen + grosse_zaehne + \n                             freundlich + faltig + gutaussehend,\n                           data = little_red_tbl, hidden = 3, \n                           exclude = c(1, 8, 15, 22, 26, 30, 34, 38, 42, 46), \n                           lifesign = \"none\", linear.output = FALSE)\n\nIn Abbildung 61.3 sehen wir das neuronale Netzwerk einmal abgebildet. Da wir uns so ein simples Beispiel ausgedacht haben, können wir das Beispiel hier auch einmal visualisieren. Wir sehen hier nochmal auf der linken Seite das Input Layer und auf der rechten Seite das Output Layer. Die schwarzen, dicken Linien stellen die bedeutenden Gewichte dar. Wir sehen also, dass grosse_ohren, grosse_augen und grosse_zaehne mit dem Hidden Layer H3 verbunden sind. Von dem Hidden Layer H3 gehen dann die Linien zu renn_weg, schrei und ruf_holzfaeller. Wir sehen daran, dass das neuronale Netzwerk in H3 den großen, bösen Wolf erkannt hat. Da wir jetzt sehen, dass H1 hauptsächlich faltig ist, können wir hier auf die Repräsentation der Großmutter schließen. Ebenso ist H2 gutaussehend, so dass wir hierauf die Repräsenrtation des Holzfällers schließen können. Die Zuordnungen des Output Layers passen dementsprechend dann auch.\n\nplotnet(neuralnetwork, bias = FALSE, pad_x = 0.73)\n\n\n\nAbbildung 61.3— Visualisierung des neuronalen Netzwerkes mit drei vorgebenen Hidden Layers. Die Hidden Layers repräsentieren in diesem Beispiel die Characktere Wolf, Großmutter und den Holzfäller."
  },
  {
    "objectID": "classification-neural-networks.html#neuronales-netz-mathematischer",
    "href": "classification-neural-networks.html#neuronales-netz-mathematischer",
    "title": "61  Neural networks",
    "section": "\n61.4 Neuronales Netz mathematischer",
    "text": "61.4 Neuronales Netz mathematischer\nDas folgende etwas mathematische Beispiel ist von Kubat (2017), pp. 65-73, entnommen. Ich habe das Beispiel dann für R adaptiert, so dass wir hier auch R Code zum ausprobieren haben. Bevor wir damit anfangen, hier nochmal auf einfache Weise erklärt, was beim Lernen mit einem neuronalen Netzwerk geschieht.\nEingaben werden als Inputs in das Netz eingegeben. Die Koeffizienten bzw. Gewichte ordnen diese Eingabe einer Reihe von Vermutungen zu, die das Netz am Ende anstellt. Hierbei erfolgt die Zuornung mehr oder minder zufällig. Wir beginnen ja auch mit einem Satz an zufällig ausgewählten Gewichten, die wir dann innerhalb des neuronalen Netzwerks optimieren wollen.\n\\[\nEingabe * Gewichtung = Vermutung\n\\]\n\n\n\\[\ninput * weight = guess\n\\]\nDie gewichtete Eingabe führt zu einer Vermutung darüber, was die Eingabe ist. Das neuronale Netz vergleicht dann seine Vermutung mit einer Wahrheit über die Daten und berechnet daraus einen Fehler. Wir wissen, dass wir zehn kranke und acht gesunde Ferkel in dem Datensatz haben, wie viele kann das neuronale Netzwerk anhand der Gewichte und dem Input richtig zuordnen oder eben falsch zuordnen?\n\\[\nWahrheit - Vermutung = Fehler\n\\]\n\n\n\\[\ntruth - guess = error\n\\]\nDie Differenz zwischen der Schätzung des neuronalen Netzes und der Wahrheit ist der Fehler. Das Netzwerk misst diesen Fehler und minimiert den Fehler über das Modell, indem es die Gewichte in dem Maße anpasst, wie sie zum Fehler beigetragen haben.\n\\[\nFehler * Beitrag\\; des\\; Gewichts\\; zum\\; Fehler = Anpassung\n\\]\n\n\n\\[\nerror * weight's\\; contribution\\; to\\; error = adjustment\n\\]\nDie drei obigen Formeln beschreiben die drei Hauptfunktionen neuronaler Netze: Bewertung der Eingaben, Berechnung des Verlusts und Aktualisierung des Modells, um den dreistufigen Prozess von vorne zu beginnen. Ein neuronales Netz ist eine korrigierende Rückkopplungsschleife, die Gewichte belohnt, die seine korrekten Vermutungen unterstützen, und Gewichte bestraft, die es zu Fehlern verleiten.\n\n\nDas Buch An Introduction to Machine Learning kannst du dir an der HS Osnabrück als PDF über die Hochschule runterladen.\nBetrachten wir also einmal ein simples Datenbeispiel von vier Beobachtungen mit jeweils einem \\(x_1\\) und einem \\(x_2\\) Wert als Prädiktor. Der Wert den \\(x_1\\) oder \\(x_2\\) annehmen können sind binär. Wir haben also für unsere beiden Prädiktoren nur \\(0/1\\) Werte vorliegen. Unser Label \\(y\\) ist ebenfalls binär. Entweder ist die betreffende Beobachtung erkrankt oder eben nicht. In unserem Beispiel sind die ersten beiden Beobachtungen nicht erkrankt und die letzten beiden Beobachtungen sind erkrankt. Schauen wir uns den Datensatz einmal an.\n\ndata_tbl &lt;- tibble(y = c(0, 0, 1, 1),\n                   x_1 = c(0, 1, 0, 1),\n                   x_2 = c(0, 0, 1, 1))\ndata_tbl\n\n# A tibble: 4 × 3\n      y   x_1   x_2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0     0\n2     0     1     0\n3     1     0     1\n4     1     1     1\n\n\nFaktisch wollen wir jetzt eine Grade durch die Punkte legen, so dass wir die gesunden von den kranken Beobachtungen trennen können. Praktisch machen wir das mit einer linearen Funktion \\(h(x)\\), die uns anhand von \\(x_1\\) und \\(x_2\\) eine Aussagen über den Status von \\(y\\) ermöglicht. Wir erhalten zuerst einen numerischen Wert, den wir dann noch mit einer Regel in eine \\(0/1\\) Entscheidung umwandeln müssen.\n\\[\nh(x) \\sim w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2\n\\]\nNun können wir die Formel nochmal kompakter schreiben.\n\\[\nh(x) \\sim \\sum_{i = 0}^{n=2} w_i x_i\n\\]\nWir drücken im Folgenden damit aus, das wir auch die Gewichte \\(w_i\\) mit den einzelnen \\(x_i\\) multiplizieren und anschließend aufsummieren. Anhand der aufsummierten Zahl aus \\(h(x)\\) können wir dann eine Entscheidung für \\(0/1\\) treffen. In unserem Beispiel entscheiden wir uns dazu, das wir \\(y=0\\) annehmen wenn \\(h(x) &lt; 0\\) ist oder aber \\(y=1\\) annehmen, wenn \\(h(x) \\geq 0\\) ist. Wir können das einmal formal aufschreiben.\n\\[\nh(x)=\n\\begin{cases}\n    1,& \\text{wenn } h(x)\\geq 0\\\\\n    0,              & \\text{ansonsten}\n\\end{cases}\n\\]\nNichts anders ist dann auch unser Neuron, was die Entscheidungen trifft. Wir haben vier verschiedene \\(x_1\\) und \\(x_2\\) Kombinationen und gewichten diese beiden \\(x\\) dann noch einem Gewichtsvektor. Wenn wir dann als aufsummiertes Ergebnis eine Zahl größer als \\(0\\) erhalten, dann gibt unser Neuron als Klassifikationsergebnis ein \\(1\\) wieder.\n\nneuron &lt;- function(input, weights) {ifelse(input %*% weights &gt; 0, 1, 0)}\n\nWir brauchen also zum einen die Inputmatrix. Die bauen wir uns einmal mit der Funktion model.matrix(). Dann haben wir drei Spalten für jedes Gewicht \\(w\\). Dann brauchen wir noch die drei Gewichte \\(w_0\\), \\(w_1\\) und \\(w_2\\). Nichts anders als der Intercept und die Steigung in einem linearen Modell.\n\ninput &lt;- data_tbl %$%\n  model.matrix(~ x_1 + x_2)\ninput\n\n  (Intercept) x_1 x_2\n1           1   0   0\n2           1   1   0\n3           1   0   1\n4           1   1   1\nattr(,\"assign\")\n[1] 0 1 2\n\n\nWir wählen zufällig drei Gewichte aus, die wir dann in unser Modell geben. Die Gwichte werden dann innerhalb des neuronalen Netzwerks dann optimiert. Die Wahl der passenden Gewichte ist dann noch eine Frage für sich, aber hier haben wir diese drei Werte ausgewählt.\n\nweights &lt;- c(0.1, 0.3, 0.4)\n\nDann brauchen wir noch ein \\(\\eta\\), dass beschreibt, um wie viel wir die Gewichte pro Runde der Optimierung verändern wollen. Wir wählen hier einen Wert von \\(0.2\\). Je kleiner der Wert, desto länger braucht das neuronale Netzwerk um ein Optimum zu finden. Pro Schritt können ja die Gewichte nur wenig geändertw werden. Ist das \\(\\eta\\) zu groß dann sind die Änderungen der Gewichte auch groß und es kann sein, dass das neuronale Netzwerk gar keine optimalen Gewichte findet. Die Auflösung ist einfach nicht gering genug.\n\neta &lt;- 0.2\n\nJetzt laufen wir einmal durch vier Epochen. In jeder Epoche werden wir unser Gewicht dann wieder optimieren und dann mit den optimierten Gewichten weiter rechnen. Wir lassen uns aber in jeder Schleife einmal die Gewichte ausgeben.\n\nfor(i in 1:4){\n  adjust &lt;- (data_tbl$y[i] - neuron(weights, input[i,])) * input[i,]\n  weights &lt;- weights + eta * adjust\n  cat(\"Adjust: \", adjust, \"\\n\")  \n  cat(\"Weights: \", weights, \"\\n\")\n}\n\nAdjust:  -1 0 0 \nWeights:  -0.1 0.3 0.4 \nAdjust:  -1 -1 0 \nWeights:  -0.3 0.1 0.4 \nAdjust:  0 0 0 \nWeights:  -0.3 0.1 0.4 \nAdjust:  0 0 0 \nWeights:  -0.3 0.1 0.4 \n\n\nDie Gewichte ändern sich in jedem Schritt um den Wert von \\(0.2\\). Mehr geht auch nicht, denn wir geben mit \\(\\eta\\) vor, um wieviel sich die Gewichte erhöhen oder erniedrigen können. Im ersten Schritt reduzieren wir das erste Gewicht um den Wert von \\(\\eta\\). Im zweiten Schritt reduzieren wir erneut das erste Gewicht und darüber hinaus auch noch das zweite Gewicht. Wir sind dann schon am Optimum, denn wir erhalten keine weiteren Anpassungen mehr. Vermutlich können wir schon am zweiten Schritt das Outcome perfekt auftrennen.\nSchauen wir einmal was passiert, wenn wir unser input mit den Gewichten aus unserem simplen Algorithmus multiplizieren.\n\ninput %*% c(-0.3, 0.1, 0.4)\n\n  [,1]\n1 -0.3\n2 -0.2\n3  0.1\n4  0.2\n\n\nUnsere ersten zwei Beobachtungen erhalten einen negativen Wert und unsere letzten beiden Beobachtungen einen positiven Wert. Nach unserer Regeln werden Zahlen kleiner als Null zu \\(0\\) und Zahlen größer als Null zu \\(1\\). Da wir die Regel auch in dem Neuron abgespeichert haben, können wir uns einmal das Outcome mit den Input und den berechnete Gewichten wiedergeben lassen.\n\nneuron(input, weights = c(-0.3, 0.1, 0.4))\n\n  [,1]\n1    0\n2    0\n3    1\n4    1\n\n\nWir erhalten eine perfekte Übereinstimmung von der Vorhersage mit unseren Trainingsdaten. Der Algorithmus ist in der Lage mit der Regel in dem Neuron und den berechneten Gewichten unser Outcome korrekt mit den Trainingsdaten vorherzusagen."
  },
  {
    "objectID": "classification-neural-networks.html#daten",
    "href": "classification-neural-networks.html#daten",
    "title": "61  Neural networks",
    "section": "\n61.5 Daten",
    "text": "61.5 Daten\nIn Folgenden wollen wir uns aber mal auf einen echten Datensatz konzentrieren. Wir nutzen daher einmal den Gummibärchendatensatz. Als unser Label und daher als unser Outcome nehmen wir das Geschlecht gender. Dabei wollen wir dann die weiblichen Studierenden vorhersagen. Im Weiteren nehmen wir nur die Spalte Geschlecht sowie als Prädiktoren die Spalten most_liked, age, semester, und height.\n\ngummi_tbl &lt;- read_excel(\"data/gummibears.xlsx\") %&gt;% \n  mutate(gender = as_factor(gender),\n         most_liked = as_factor(most_liked)) %&gt;% \n  select(gender, most_liked, age, semester, height) %&gt;% \n  drop_na(gender)\n\nWir dürfen keine fehlenden Werte in den Daten haben. Wir können für die Prädiktoren später die fehlenden Werte imputieren. Aber wir können keine Labels imputieren. Daher entfernen wir alle Beobachtungen, die ein NA in der Variable gender haben. Wir haben dann insgesamt \\(n = 522\\) Beobachtungen vorliegen. In Tabelle 56.5 sehen wir nochmal die Auswahl des Datensatzes in gekürzter Form.\n\n\n\n\nTabelle 61.3— Auszug aus dem Daten zu den Gummibärchendaten.\n\ngender\nmost_liked\nage\nsemester\nheight\n\n\n\nm\nlightred\n35\n10\n193\n\n\nw\nyellow\n21\n6\n159\n\n\nw\nwhite\n21\n6\n159\n\n\nw\nwhite\n36\n10\n180\n\n\nm\nwhite\n22\n3\n180\n\n\nm\nwhite\nNA\nNA\nNA\n\n\n…\n…\n…\n…\n…\n\n\nw\nyellow\n14\n0\n173\n\n\nw\nnone\n11\n0\n159\n\n\nw\ndarkred\n12\n0\n158\n\n\nw\ndarkred\n12\n0\n158\n\n\nw\ndarkred\n12\n0\n156\n\n\nw\ndarkred\n12\n0\n158\n\n\n\n\n\n\nUnsere Fragestellung ist damit, können wir anhand unserer Prädiktoren männliche von weiblichen Studierenden unterscheiden und damit auch klassifizieren? Um die Klassifikation mit Entscheidungsbäumen rechnen zu können brauchen wir wie bei allen anderen Algorithmen auch einen Trainings- und Testdatensatz. Wir splitten dafür unsere Daten in einer 3 zu 4 Verhältnis in einen Traingsdatensatz sowie einen Testdatensatz auf. Der Traingsdatensatz ist dabei immer der größere Datensatz. Da wir aktuell nicht so viele Beobachtungen in dem Gummibärchendatensatz haben, möchte ich mindestens 100 Beobachtungen in den Testdaten. Deshalb kommt mir der 3:4 Split sehr entgegen.\n\ngummi_data_split &lt;- initial_split(gummi_tbl, prop = 3/4)\n\nWir speichern uns jetzt den Trainings- und Testdatensatz jeweils separat ab. Die weiteren Modellschritte laufen alle auf dem Traingsdatensatz, wie nutzen dann erst ganz zum Schluss einmal den Testdatensatz um zu schauen, wie gut unsere trainiertes Modell auf den neuen Testdaten funktioniert.\n\ngummi_train_data &lt;- training(gummi_data_split)\ngummi_test_data  &lt;- testing(gummi_data_split)\n\nNachdem wir die Daten vorbereitet haben, müssen wir noch das Rezept mit den Vorverabreitungsschritten definieren. Wir schreiben, dass wir das Geschlecht gender als unser Label haben wollen. Daneben nehmen wir alle anderen Spalten als Prädiktoren mit in unser Modell, das machen wir dann mit dem . Symbol. Da wir noch fehlende Werte in unseren Prädiktoren haben, imputieren wir noch die numerischen Variablen mit der Mittelwertsimputation und die nominalen fehlenden Werte mit Entscheidungsbäumen. Dann müssen wir noch alle numerischen Variablen normalisieren und alle nominalen Variablen dummykodieren. Am Ende werde ich nochmal alle Variablen entfernen, sollte die Varianz in einer Variable nahe der Null sein.\n\ngummi_rec &lt;- recipe(gender ~ ., data = gummi_train_data) %&gt;% \n  step_impute_mean(all_numeric_predictors()) %&gt;% \n  step_impute_bag(all_nominal_predictors()) %&gt;% \n  step_range(all_numeric_predictors(), min = 0, max = 1) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_nzv(all_predictors())\n\ngummi_rec %&gt;% summary()\n\n# A tibble: 5 × 4\n  variable   type      role      source  \n  &lt;chr&gt;      &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 most_liked &lt;chr [3]&gt; predictor original\n2 age        &lt;chr [2]&gt; predictor original\n3 semester   &lt;chr [2]&gt; predictor original\n4 height     &lt;chr [2]&gt; predictor original\n5 gender     &lt;chr [3]&gt; outcome   original\n\n\nAlles in allem haben wir ein sehr kleines Modell. Wir haben ja nur ein Outcome und vier Prädiktoren. Trotzdem sollte dieser Datensatz reichen um zu erklären wie Keras oder Tensorflow funktionieren. Am Ende muss man sich aber auch ehrlich machen und sagen, dass ein Datensatz mit unter tausend Beobachtungen eigentlich keinen großen Sinn für ein neuronales Netz macht. Deshalb ist das hier eher eine Demonstration des Algorithmus."
  },
  {
    "objectID": "classification-neural-networks.html#neuronale-netze-mit-neuralnet",
    "href": "classification-neural-networks.html#neuronale-netze-mit-neuralnet",
    "title": "61  Neural networks",
    "section": "\n61.6 Neuronale Netze mit neuralnet\n",
    "text": "61.6 Neuronale Netze mit neuralnet\n\nNeuronale Netze mit den R Paketen neuralnet und dem R Paket nnet sind mehr oder minder veraltet (eng. outdated). Wir können das Paket neuralnet nicht über die parsnip Umgebung nutzen. Deshalb hier einmal zu Fuß mit all den Komplikationen, die das so mit sich bringt. Auf der anderen Seite liefert das Paket neuralnet auch gute Ergebnisse mit wenig rechenlaufzeit. Da musst du dann einmal abwägen, was du in deiner Arbei so brauchst.\nDas die Funktion neuralnet() nicht mit den Workflow kann, müssen wir uns erstmal wieder den Traingsdatendatz und den Testdatensatz aus unserem Rezept extrahieren. Den Traingsdatensatz können wir uns über die Funktion juice() einmal aus dem Rezept ziehen.\n\ngummi_train_tbl &lt;- gummi_rec %&gt;% \n  prep %&gt;% \n  juice()\n\nDen Testdatensatz müssen wir mit dem Rezept einmal backen. Dann müssen wir noch die Spalte gender in eine numerische Spalte umwandeln. Sonst klappt das später nicht mit der Prädiktion und der Konfusionsmatrix.\n\ngummi_test_tbl &lt;- gummi_rec %&gt;% \n  prep %&gt;% \n  bake(gummi_test_data) %&gt;% \n  mutate(gender = as_factor(ifelse(gender == \"m\", 0, 1)))\n\nDann können wir auch schon die Funktion neuralnet auf unsere Daten anwenden. Wir wollen fünfmal über die Traingsdaten iterieren (rep = 5). Später heißt dieses Iterieren dann auch epoch. Dann müssen wir noch den Threshold für den Fehler festlegen, der gerade noch so akzeptabel ist und wo das Wachstum endet. Je kleiner, desto länger dauer der Prozess. Mit einem threshold = 0.2 sind wir aber schon sehr weit oben, sonst ist der Wert bei \\(0.01\\). Hier musst ein wenig selber mit den Parametern spielen. Eine Tuningmöglichkeit oder eine Kreuzvalidierung musst du dir dann selber programmieren. Wir nehmen dann fünf Hiddenlayers mit jeweils fünf Knoten pro Hiddenlayer.\n\nneuralnet_fit &lt;- neuralnet(gender ~., data = gummi_train_tbl, rep = 5, threshold = 0.2,\n                           hidden = c(5, 5), lifesign = \"minimal\")\n\nhidden: 5, 5    thresh: 0.2    rep: 1/5    steps:     794   error: 39.00227 time: 0.13 secs\nhidden: 5, 5    thresh: 0.2    rep: 2/5    steps:    1708   error: 36.44619 time: 0.29 secs\nhidden: 5, 5    thresh: 0.2    rep: 3/5    steps:    1119   error: 36.97956 time: 0.17 secs\nhidden: 5, 5    thresh: 0.2    rep: 4/5    steps:    2590   error: 34.67337 time: 0.48 secs\nhidden: 5, 5    thresh: 0.2    rep: 5/5    steps:     787   error: 38.63517 time: 0.12 secs\n\n\nWenn wir das Modell haben, dann können wir uns hier ganz einfach mal das beste neuronale Netzwerk anschauen. Also die Wiederholung mit dem kleinsten Fehler. In Abbildung Abbildung 61.4 sehen wir das Netzwerk einmal dargestellt. Die blauen Knoten stellen die Biasknoten dar. Die Zahlen an den Kanten stellen dann die Gewichte dar, die von dem jeweiligen Knoten weitergegeben werden. Die Interpretation des Netzwerks ist so schwer, es ist eben nur eine visuelle Darstellung. Da so eine Abbildung etwas schwer zu interpretieren ist, erlaubt ein neurales Interpretationsdiagramm mehr Einblicke. Die schwarzen Kanten haben einen höheren Einfluss als die grauen Kanten. Die exakte Interpretation der Knoten und der Kanten ist aber dennoch schwierig.\n\nplot(neuralnet_fit, rep = \"best\")\n\nplotnet(neuralnet_fit, rep = \"best\", bias = FALSE, pad_x = 0.59)\n\n\n\n\n\n(a) Neuronales Netzwerk mit den Gewichten und dem Bias als numerische Representation.\n\n\n\n\n\n(b) Neurales Interpretationsdiagramm für ein neurales Netzwerk.\n\n\n\nAbbildung 61.4— Abbildung des neuronalen Netzwerks mit dem kleinsten Fehler.\n\n\n\nAm Ende machen wir das Ganze ja nicht um etwas interpretieren zu können, sondern um eine Vorhersage zu treffen. Das machen wir mit der Funktion predict(). Jetzt wird es wieder nervig. Wir müssen usn merken, dass unser Faktor zwei Level hat mit 0 und 1 wobei die m = 0 und w = 1 ist. Als wäre das nicht schon nervig genug, haben wir dann in der Ausgabe von predict() nur eine MAtrix mit zwei Spalten. Wir brauchen die zweite Spalte, da wir das Geschlecht w vorhersagen wollen.\n\nneuralnet_pred &lt;- predict(neuralnet_fit, gummi_test_tbl) %&gt;% \n  round(2)\n\nKurzer Check, ob wir auch alles richtig gemacht haben.\n\nrange(neuralnet_pred[,1])\n\n[1] -0.13  1.09\n\nrange(neuralnet_pred[,2])\n\n[1] -0.11  1.20\n\n\nUnd wir stellen fest, dass hier irgendwas mit unserer Wahrscheinlichkeit für die Klassenzugehörigkeit nicht stimmt. Wir haben negative Werte und Werte über Eins. Das macht für eine Wahrscheinlichkeit keinen Sinn. STOPP, heißt es jetzt hier!\nIch zeige aber noch wie du dir die Konfusionsmatrix berechnest. Da musst du dich wieder strecken um alles in die Funktion conf_mat() richtig rein zu kriegen. Aber Vorsicht, erst wenn du die Wahrscheinlichkeiten hingekriegt hast, dann kannst du mit der Konfusionsmatrix weitermachen.\n\nneuralnet_cm &lt;- conf_mat(data = data.frame(.pred_class = as.factor(round(neuralnet_pred[,2])),\n                                           gender = as.factor(pull(gummi_test_tbl, gender))), \n                         gender, .pred_class)\n\nDann können wir uns die Konfusionsmatrix auch einmal wiedergeben lassen. Ich wäre hier sehr vorsichtig, was die Werte angeht. Wir haben gerade komische Wahrscheinlichkeiten wiedergegeben bekommen. Daher würde ich der Sache hier nicht trauen und nochmal an der Funktion neuralnet() mit anderen Parametern herumprobieren. Man sieht, es hat auch einen Grund warum manche Funktionen nicht in der parsnip Umgebung implementiert sind.\n\nneuralnet_cm %&gt;% \n  summary %&gt;% \n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary          0.82\n 2 kap                  binary          0.63\n 3 sens                 binary          0.87\n 4 spec                 binary          0.76\n 5 ppv                  binary          0.8 \n 6 npv                  binary          0.84\n 7 mcc                  binary          0.63\n 8 j_index              binary          0.63\n 9 bal_accuracy         binary          0.81\n10 detection_prevalence binary          0.57\n11 precision            binary          0.8 \n12 recall               binary          0.87\n13 f_meas               binary          0.83\n\n\nHier ist also wirklich Vorsicht geboten, wenn wir uns die Ergebnisse anschauen. Die Ergebnisse sind zwar nicht so schlecht, aber wir vertrauen da nicht dem Algorithmus, wenn wir ungültige Wahrscheinlichkeiten erhalten."
  },
  {
    "objectID": "classification-neural-networks.html#neuronale-netze-mit-nnet",
    "href": "classification-neural-networks.html#neuronale-netze-mit-nnet",
    "title": "61  Neural networks",
    "section": "\n61.7 Neuronale Netze mit nnet\n",
    "text": "61.7 Neuronale Netze mit nnet\n\nWir können aber das R Paket nnet mit unserer bekannten Rezeptumgebung nutzen und uns damit das Leben einfacher machen. Das macht auch in diesem Fall sehr viel mehr Sinn, da wir ja nur komische Wahrscheinlichkeiten der Klassenzugehörigkeit aus der Funktion neuralnet() wiederbekommen. Also das ganze einmal ohne wildes Installieren von Tensorflow / Keras. Ein simples neurales Netzwerk in R mit der Engine aus nnet.\nIn unserem Beispiel lassen wir einhundert Replikationen laufen (epoch = 100) und wählen auch hier mal fünf Hidden Layers (hidden_units = 5). Dann wollen wir natürlich eine Klassifikation rechnen.\n\nnnet_mod &lt;- mlp(epochs = 100, hidden_units = 5) %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"classification\")\n\nWir bringen wieder unser Modell mit dem Rezept des Gummibärchendatensatzes zusammen und können dann den Workflow abspeicherb.\n\nnnet_wflow &lt;- workflow() %&gt;% \n  add_model(nnet_mod) %&gt;% \n  add_recipe(gummi_rec)\n\nWie immer starten wir dann den Workflow mit der Funktion fit() und erhalten das nnet Modell zurück.\n\nnnet_fit &lt;- nnet_wflow %&gt;% \n  parsnip::fit(gummi_train_data)\n\nJetzt müssen wir nur noch mit der Funktion augment uns die Vorhersagen mit dem Testdatensatz wiedergeben lassen.\n\nnnet_aug &lt;- augment(nnet_fit, gummi_test_data ) \n\nDa wir hier etwas vorsichtig geworden sind, nochmal schnell schauen, ob unsere Wahrscheinlichkeiten der Klassenzugehörigkeit auch wirklich eine Wahrscheinlichkeit ist.\n\npluck(nnet_aug, \".pred_w\") %&gt;% range()\n\n[1] 0.2689414 0.7310563\n\n\nJa, das passt soweit und wir können uns dann die Konfusionsmatrix berechnen lassen. Die Ergebnisse sind jetzt nicht so berauschend, aber auf der anderen Seite richtiger als in der Funktion neuralnet().\n\nnnet_cm &lt;- nnet_aug %&gt;% \n  conf_mat(gender, .pred_class)\n\nnnet_cm\n\n          Truth\nPrediction  m  w\n         m 58 15\n         w 11 47\n\n\nDann schauen wir uns nochmal die ganzen anderen Gütekriterien aus der Konfusionsmatrix einmal an.\n\nnnet_cm %&gt;% summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.802\n 2 kap                  binary         0.601\n 3 sens                 binary         0.841\n 4 spec                 binary         0.758\n 5 ppv                  binary         0.795\n 6 npv                  binary         0.810\n 7 mcc                  binary         0.602\n 8 j_index              binary         0.599\n 9 bal_accuracy         binary         0.799\n10 detection_prevalence binary         0.557\n11 precision            binary         0.795\n12 recall               binary         0.841\n13 f_meas               binary         0.817\n\n\nDie Ergebnisse sind höchstens okay. Die Accuracy ist nicht sehr hoch und auch der Rest der Werte ist eher mittelmäßig. Das Ganze sehen wir dann in Abbildung 61.5 auch nochmal entsprechend in der ROC Kurve visualisiert. Die ROC Kurve sieht nur mittelmäßig aus. Wir müssten hier auf jeden Fall nochmal über Kreuzvalidierung und Tuning nachdenken. Ohne Kreuzvalidierung und Tuning würde ich das Modell nicht anwenden.\n\nnnet_aug %&gt;% \n  roc_curve(gender, .pred_w, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\nAbbildung 61.5— ROC Kurve für den nnet Algorithmus."
  },
  {
    "objectID": "classification-neural-networks.html#neuronale-netze-mit-keras-tensorflow",
    "href": "classification-neural-networks.html#neuronale-netze-mit-keras-tensorflow",
    "title": "61  Neural networks",
    "section": "\n61.8 Neuronale Netze mit Keras / Tensorflow",
    "text": "61.8 Neuronale Netze mit Keras / Tensorflow\nJetzt kommen wir zum dicksten Brett. Was wir hier machen ist eigentlich nur ein schwacher Abglanz was Tensorflow eigentlich kann. Über den Algorithmus werden ganze Bücher geschrieben und die Anwendung auf einem Laptop oder Standrechner ist eigentlich dem Algorithmus nicht würdig. Wir werden hier auch nicht alles aus dem Algorithmus raus holen. Das geht auch gar nicht. Wenn du dich tiefer mit der Materie beschäftigen willst, dann ist dies hier ein guter Startpunkt. Wenn du Probleme hast Tensorflow zum Laufen zu kriegen, dann kannst du auch für die einfache Anwendung nnet nutzen. Mit ein wenig Tuning sollten da auch gute Ergebnisse bei herauskommen.\n\n\nWenn du richtig Tensorflow mit R nutzen willst, dann gibt es hier noch das umfangreiche Tutorium für Tensorflow with R. Insbesondere die Nutzung von lime um die Black Box des neuronalen Netzwerks zu erklären wird hier nochmal gezeigt.\nDie Funktion mlp() erlaubt uns als Engine keras zu verweden und damit ein neurales Netzwerk mit dem Tensorflow Algorithmus zu rechnen. Mehr brauchen wir an dieser Stelle erstaml nicht tun. Wir werden hier erstmal keine Tuning Parameter angeben. Später im Kapitel werden wir dann noch ein Tuning für den Algorithmus rechnen.\n\nkeras_mod &lt;- mlp() %&gt;% \n  set_engine(\"keras\") %&gt;% \n  set_mode(\"classification\")\n\nJetzt bringen wir noch das Rezept des Gummibärchendatensatzes mit dem Modell in einem Workflow zusammen.\n\nkeras_wflow &lt;- workflow() %&gt;% \n  add_model(keras_mod) %&gt;% \n  add_recipe(gummi_rec)\n\nJetzt können mit mit der Funktion fit() das Modell rechnen. Wenn du Keras und Tensorflow nicht installiert hast, dann wird jetzt meist eine automatische Installation starten. Oder aber du hast dr vorher schon Tensorflow und Keras installiert. Schaue dazu gerne einmal den Quick start um Tensorflow zu installieren an.\n\nkeras_fit &lt;- keras_wflow %&gt;% \n  parsnip::fit(gummi_train_data)\n\nWenn der Algorithmus durchgelaufen ist, was schon ein paar Sekunden dauern kann, dann können wir danach das Modell nutzen um unser Geschlecht vorherzusagen.\n\nkeras_aug &lt;- augment(keras_fit, gummi_test_data) \n\nWir lassen uns dann wieder die Konfusionsmatrix wiedergeben. Wir sehen, dass wir sehr mies dran sind. Wir haben eine nahezu zufällige Einteilung der Geschlechter durch die Vorhersage.\n\nkeras_cm &lt;- keras_aug %&gt;% \n  conf_mat(gender, .pred_class)\n\nkeras_cm\n\n          Truth\nPrediction  m  w\n         m 27 28\n         w 36 28\n\n\nWas schon in der Konfusionsmatrix ziemlich mies aussah, wird natürlich auch so in der Zusammenfassung wiedergegeben.\n\nkeras_cm %&gt;% summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.462 \n 2 kap                  binary       -0.0709\n 3 sens                 binary        0.429 \n 4 spec                 binary        0.5   \n 5 ppv                  binary        0.491 \n 6 npv                  binary        0.438 \n 7 mcc                  binary       -0.0715\n 8 j_index              binary       -0.0714\n 9 bal_accuracy         binary        0.464 \n10 detection_prevalence binary        0.462 \n11 precision            binary        0.491 \n12 recall               binary        0.429 \n13 f_meas               binary        0.458 \n\n\nWas sehen wir? Wir sehen, dass unsere Accuracy mit unter 50% schon mehr schlecht ist. Die Zuordnung der Geschlechter wird vom Algorithmus rein zufällig durchgeführt. Wir können daher nicht von einem guten Algorithmus sprechen. In Abbildung 61.6 sehen wir die gewollt schlechte ROC Kurve aus einem keras Algorithmus ohne Tuning. Warum war die nochmal gewollt schlecht? Ich will hier einmal zeigen, dass ein neuronales Netz aus dem Tensorflow Algorithmus meistens ohne ein Tuning sehr schlecht ist. Das kann sich aber durch ein Tuning sehr schnell drehen.\n\nkeras_aug %&gt;% \n  roc_curve(gender, .pred_w, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\nAbbildung 61.6— ROC Kurve für den keras Algorithmus.\n\n\n\n\n\n\n\n\n\nKann ich auch eine Kreuzvalidierung für Keras / Tensorflow durchführen?\n\n\n\nJa, kannst du. Wenn du nur eine Kreuzvalidierung durchführen willst, findest du alles im Kapitel 58 für den \\(k\\)-NN Algorithmus. Du musst dort nur den Workflow ändern und schon kannst du alles auch auf Keras / Tensorflow Algorithmus anwenden."
  },
  {
    "objectID": "classification-neural-networks.html#tuning",
    "href": "classification-neural-networks.html#tuning",
    "title": "61  Neural networks",
    "section": "\n61.9 Tuning",
    "text": "61.9 Tuning\nWas heißt Tuning? Wie bei einem Auto können wir an verschiedenen Stellschrauben bei einem mathematischen Algorithmus schrauben. Welche Schrauben und Teile das sind, hängt dann wieder vom Algorithmus ab. Im Falle des xgboost Algorithmus können wir an folgenden Parametern drehen und jeweils schauen, was dann mit unserer Vorhersage passiert. Insgesamt hat der keras Algorithmus fünf Tuningparameter, wir wählen jetzt für uns hier drei aus. Ich nehme hier auch nur drei Parameter, da sich dann drei Parameter noch sehr gut visuell darstellen lassen. In der Anwendung wäre dann natürlich besser alle Parameter zu tunen, aber das dauert dann auch lange.\n\n\nhidden_units, Anzahl der Ebenen (eng. layer) in dem neuronalen Netzwerk. Wie viele Ebenen soll unser Netzwerk haben? Oder auch wie deep soll das Netzwerk gebaut werden?\n\npenalty, ein Wert für die Regulierung des neuronalen Netzwerk.\n\nepochs, bezieht sich auf einen Zyklus durch die Layer für den gesamten Trainingsdatensatz. Wie oft rechnen wir den Trainingsdatensatz und trainieren unser Netzwerk?\n\nNun ist es so, dass wir natürlich nicht händisch alle möglichen Kombinationen von der Anzahl der ausgewählten Variablen pro Baum, der kleinsten Knotengröße und der Anzahl der Bäume berechnen wollen. Das sind ziemlich viele Kombinationen und wir kommen dann vermutlich schnell durcheinander. Deshalb gibt es die Funktion tune() aus dem R Paket tune, die uns einen Prozess anbietet, das Tuning automatisiert durchzuführen.\nDa ich nicht ewig warten wollte, habe ich noch das parallele Rechnern aktiviert, in dem ich mir die Anzahl an Rechenkernen minus eins wiedergeben habe lassen.\n\ncores &lt;- parallel::detectCores() - 1\n\nAls erstes müssen wir uns ein Objekt bauen, das aussieht wie ein ganz normales Modell in der Klassifikation. Aber wir ergänzen jetzt noch hinter jeder zu tunenden Option noch die Funktion tune(). Das sind die Parameter des Algorithmus, die wir später tunen wollen.\n\ntune_spec &lt;- mlp(hidden_units = tune(),\n                 penalty = tune(), \n                 epochs = tune()) %&gt;% \n  set_engine(\"keras\", num.threads = cores) %&gt;% \n  set_mode(\"classification\") \n\ntune_spec\n\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = tune()\n  penalty = tune()\n  epochs = tune()\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: keras \n\n\nJetzt bauen wir uns den Workflow indem wir statt unserem Modell, die Tuninganweisung in den Workflow reinnehmen. Echt simpel und straightforward. Das Rezept bleibt ja das Gleiche.\n\ngummi_tune_wflow &lt;- workflow() %&gt;% \n  add_model(tune_spec) %&gt;% \n  add_recipe(gummi_rec)\n\nJetzt müssen wir noch alle Kombinationen aus den drei Parametern hidden_units, penalty und epochs ermitteln. Das macht die Funktion grid_regular(). Es gibt da noch andere Funktionen in dem R Paket tune, aber ich konzentriere mich hier auf die einfachste. Jetzt müssen wir noch die Anzahl an Kombinationen festlegen. Ich möchte für jeden Parameter fünf Werte tunen. Daher nutze ich hier die Option levels = 5 auch damit hier die Ausführung nicht so lange läuft. Fange am besten mit levels = 5 an und schaue, wie lange das zusammen mit der Kreuzvalidierung dann dauert. Dann kannst du die Levels noch hochschrauben. Beachte aber, dass mehr Level nur mehr Zwischenschritte bedeutet. Jede Option hat eine Spannweite range, die du dann anpassen musst, wenn du höhere Werte haben willst. Mehr Level würden nur mehr Zwischenschritte bedeuten.\n\ngummi_grid &lt;- grid_regular(hidden_units(range = c(1, 100)),\n                           penalty(),\n                           epochs(range = c(10, 200)),\n                           levels = 5)\n\nDas Tuning nur auf dem Trainingsdatensatz durchzuführen ist nicht so eine gute Idee. Deshalb nutzen wir hier auch die Kreuzvalidierung. Eigentlich ist eine 10-fache Kreuzvalidierung mit \\(v=10\\) besser. Das dauert mir dann aber hier im Skript viel zu lange. Deshalb habe ich hier nur \\(v=5\\) gewählt. Wenn du das Tuning rechnest, nimmst du natürlich eine 10-fach Kreuzvalidierung.\n\ngummi_folds &lt;- vfold_cv(gummi_train_data, v = 5)\n\nNun bringen wir den Workflow zusammen mit dem Tuninggrid und unseren Sets der Kreuzvaidierung. Daher pipen wir den Workflow in die Funktion tune_grid(). Als Optionen brauchen wir die Kreuzvaldierungsdatensätze und das Tuninggrid. Wenn du control_grid(verbose = TRUE) wählst, dann erhälst du eine Ausgabe wie weit das Tuning gerade ist. Achtung!, das Tuning dauert seine Zeit. Im Falle des keras Algorithmus dauert das Tuning extrem lange, aber immer noch nur ein paar Stunden. Wenn du dann alle fünf Parameter des keras Algorithmustunen wollen würdest, dann würde die Berechnung Tage dauern. Deshalb ist ein Großerechner mit mehreren Kernen unabdingbar für die Nutzung von deep learning Du kannst das Ergebnis des simpleren Tunings auch in der Datei gummi_xgboost_tune_res.rds finden.\n\ngummi_tune_res &lt;- gummi_tune_wflow %&gt;% \n   tune_grid(resamples = gummi_folds,\n             grid = gummi_grid,\n             control = control_grid(verbose = FALSE))\n\nDamit du nicht das Tuning durchlaufen lassen musst, habe ich das Tuning in die Datei gummi_xgboost_tune_res.rds abgespeichert und du kannst dann über die Funktion read_rds() wieder einlesen. Dann kannst du den R Code hier wieder weiter ausführen.\nNachdem das Tuning durchgelaufen ist, können wir uns über die Funktion collect_metrics(), die Ergebnisse des Tunings für jede Kombination der drei Parameter hidden_units, penalty und epochs wiedergeben lassen. Diese Ausgabe ist super unübersichtlich. Ich habe mich ja am Anfange des Abschnitts auch für drei Tuningparameter entschieden, da sich dann diese drei Parameter noch gut visualisieren lassen. Deshalb einmal die Abbildung der mittleren Accuarcy und der mittleren AUC-Werte über alle Kreuzvalidierungen.\n\ngummi_tune_res %&gt;%\n  collect_metrics() %&gt;%\n  mutate(hidden_units = as_factor(hidden_units),\n         penalty = as_factor(penalty)) %&gt;%\n  ggplot(aes(epochs, mean, color = hidden_units, linetype = penalty)) +\n  theme_minimal() +\n  geom_line(alpha = 0.6) +\n  geom_point() +\n  facet_wrap(~ .metric, scales = \"free\", nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_okabeito()\n\n\n\nAbbildung 61.7— Tuning Kurven für den keras Algorithmus.\n\n\n\nDamit wir nicht händisch uns die beste Kombination raussuchen müssen, können wir die Funktion show_best() nutzen. Wir wählen hier die beste Accuarcy und erhalten dann die sortierten Ergebnisse nach der Accuarcy des Tunings.\n\ngummi_tune_res %&gt;%\n  show_best(\"accuracy\")\n\n# A tibble: 5 × 9\n  hidden_units     penalty epochs .metric .estimator  mean     n std_err .config\n         &lt;int&gt;       &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n1           75    1   e-10    200 accura… binary     0.819     5  0.0142 Prepro…\n2           75    3.16e- 8    200 accura… binary     0.813     5  0.0150 Prepro…\n3          100    1   e- 5    200 accura… binary     0.813     5  0.0192 Prepro…\n4          100    1   e-10    200 accura… binary     0.811     5  0.0183 Prepro…\n5           25    3.16e- 8    200 accura… binary     0.811     5  0.0198 Prepro…\n\n\nDas war die Funktion show_best() aber wir können uns auch die gleich die besten Parameter nach der Accuracy raus ziehen. Das Rausziehen der besten Parameter macht für uns die Funktion select_best().\n\nbest_keras &lt;- gummi_tune_res %&gt;%\n  select_best(\"accuracy\")\n\nbest_keras\n\n# A tibble: 1 × 4\n  hidden_units      penalty epochs .config               \n         &lt;int&gt;        &lt;dbl&gt;  &lt;int&gt; &lt;chr&gt;                 \n1           75 0.0000000001    200 Preprocessor1_Model104\n\n\nWir sehen, dass wir hidden_units = 75 wählen sollten. Dann müssen wir als Penalty penalty = 0.0000000001 nutzen. Die Anzahl an Durchläufen pro Training ist dann epochs = 200. Müssen wir jetzt die Zahlen wieder in ein Modell eingeben? Nein, müssen wir nicht. Mit der Funktion finalize_workflow() können wir dann die besten Parameter aus unserem Tuning gleich mit dem Workflow kombinieren. Dann haben wir unseren finalen, getunten Workflow. Du siehst dann auch in der Ausgabe, dass die neuen Parameter in dem keras Algorithmus übernommen wurden\n\nfinal_gummi_wf &lt;- gummi_tune_wflow %&gt;% \n  finalize_workflow(best_keras)\n\nfinal_gummi_wf \n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mlp()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_mean()\n• step_impute_bag()\n• step_range()\n• step_dummy()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = 75\n  penalty = 1e-10\n  epochs = 200\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: keras \n\n\nJetzt bleibt uns nur noch der letzte Fit übrig. Wir wollen unseren finalen, getunten Workflow auf die Testdaten anwenden. Dafür gibt es dann auch die passende Funktion. Das macht für uns die Funktion last_fit(), die sich dann die Informationen für die Trainings- und Testdaten aus unserem Datensplit von ganz am Anfang extrahiert.\n\nfinal_fit &lt;- final_gummi_wf %&gt;%\n  last_fit(gummi_data_split) \n\nDa wir immer noch eine Kreuzvaldierung rechnen, müssen wir dann natürlich wieder alle Informationen über alle Kreuzvaldierungsdatensätze einsammeln. Dann erhalten wir unsere beiden Gütekriterien für die Klassifikation des Geschlechts unser Studierenden nach dem keras Algorithmus. Die Zahlen sind schon gut für echte Daten. Eine Accuracy von 81% bedeutet das wir über acht von zehn Studierenden richtig klassifizieren. Die AUC ist auch schon fast hervorragend, wir bringen kaum Label durcheinander.\n\nfinal_fit %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.807 Preprocessor1_Model1\n2 roc_auc  binary         0.900 Preprocessor1_Model1\n\n\nDann bleibt uns nur noch die ROC Kurve zu visualisieren. Da wir wieder etwas faul sind, nutzen wir die Funktion autoplot(). Als Alternative geht natürlich auch das R Paket pROC, was eine Menge mehr Funktionen und Möglichkeiten bietet.\n\nfinal_fit %&gt;%\n  collect_predictions() %&gt;% \n  roc_curve(gender, .pred_w, event_level = \"second\") %&gt;% \n  autoplot()\n\n\n\nAbbildung 61.8— ROC Kurve für den keras Algorithmus nach der Kreuvalidierung und dem Tuning.\n\n\n\nDa wir eine ROC Kurve hier vorliegen haben, die sehr weit weg von der Diagonalen ist, haben wir sehr viele richtig vorhergesagte Studierende in unseren Testdaten. Unser Modell funktioniert um das Geschlecht von Studierenden anhand unserer Gummibärchendaten vorherzusagen. Besonders bei den neuronalen Netzwerken sieht man, wenn du die ROC Kurven vor und nach dem Tuning vergleichst, wie wichtig das Tuning ist. Dabei haben wir hier nur die abgespeckte Variante genutzt, da mein Rechner nicht länger laufen sollte."
  },
  {
    "objectID": "classification-neural-networks.html#referenzen",
    "href": "classification-neural-networks.html#referenzen",
    "title": "61  Neural networks",
    "section": "Referenzen",
    "text": "Referenzen\n\n\n\n\nKubat, Miroslav. 2017. An introduction to machine learning. Bd. 2. Springer.\n\n\nMueller, John Paul, und Luca Massaron. 2019. Deep Learning for dummies. John Wiley & Sons."
  },
  {
    "objectID": "app-example-analysis.html",
    "href": "app-example-analysis.html",
    "title": "Anhang A — Beispielhafte Auswertungen",
    "section": "",
    "text": "Version vom r format(Sys.time(), '%B %d, %Y um %H:%M:%S')\n\n“Ohne Kreativität gibt es keine Entwicklung.” — Commander Spock, Raumschiff Enterprise, Landru und die Ewigkeit\n\nDa es hier dann doch recht eng wurde, habe ich das Kapitel Beispielhafte Auswertungen dann einmal auf eine andere Internetseite ausgelagert. Jetzt findest du alle Beispiele zur der Anwendung auf der neuen Internetseite. In dem neuen Skript gibt es dann aber keine weiteren Informationen mehr zu dem R Code oder der Statistik. Die Informationen findest du dann hier auf der Internetseite."
  },
  {
    "objectID": "app-how-to-write.html#der-schreibprozess",
    "href": "app-how-to-write.html#der-schreibprozess",
    "title": "Anhang B — Writing principles",
    "section": "\nB.1 Der Schreibprozess",
    "text": "B.1 Der Schreibprozess\nDer Schreibprozess läuft nach Beginn in den nächsten Wochen und Monaten in mehreren Phasen ab. In jeder Phase ist es wichtig, sich ein gutes Umfeld für konzentriertes Arbeiten zu schaffen, gleichzeitig aber die Kommunikation mit den Betreuenden und anderen Mitschaffenden nicht abreißen zu lassen."
  },
  {
    "objectID": "app-how-to-write.html#zeitplan",
    "href": "app-how-to-write.html#zeitplan",
    "title": "Anhang B — Writing principles",
    "section": "\nB.2 Zeitplan",
    "text": "B.2 Zeitplan\nHier ist es wichtig was geschrieben werden soll. Eine Bachelor- und Masterarbeit hat einen klaren Zeitplan, der vorab feststehen muss. Bis wann müssen wie viele Wörter geschrieben sein, damit die Arbeit fertig werden kann. Beide Abschlussformen haben ja unterschiedliche Zeitrahmen. Und das ist wirklich wichtig! Wann sollte man fertig sein mit Programmieren, wie lange soll man sich Zeit für Vortragsvorbereitung nehmen, etc. Auch eine wissenschaftliche Veröffentlichung hat einen Zeitplan! Leider – das ist der Primat der Forschung – kann sich der Zeitplan immer wieder ändern, wenn Methoden nicht klappen oder neue Erkenntnisse gewonnen werden. Dennoch muss klar sein, dass in endlicher Zeit – meist ein Jahr – ein Paper eingereicht werden kann. Auf dieses Ziel sollten sich alle Einschwören. Sonst ist eine Promotion in endlicher Zeit nicht machbar."
  },
  {
    "objectID": "app-how-to-write.html#ideen-entwickeln",
    "href": "app-how-to-write.html#ideen-entwickeln",
    "title": "Anhang B — Writing principles",
    "section": "\nB.3 Ideen entwickeln",
    "text": "B.3 Ideen entwickeln\nMan sammelt Ideen zunächst in der Breite und fokussiert dann, was davon man aufschreiben will. Man liest andere wissenschaftliche Paper, lernt vielleicht noch Grundlagen und Methoden, und macht sich Notizen, was interessant sein könnte, und wo es steht. Natürlich kann man sich bei den Betreuern und Mitschaffenden Hilfe und Anregungen holen, wo man Input herbekommt. Holen heißt aber nicht, dass man gebracht bekommt.\n\n\n\n\n\n\nOCAR Prinzip\n\n\n\nFrage dich, was ist das Opening (der Hintergrund der Arbeit), die Challenge (was ist das Problem, was gelöst werden soll?), die Action (was wirst du tun um dieses Problem zu lösen?) und die Results (Was kam dabei raus oder soll rauskommen?)"
  },
  {
    "objectID": "app-how-to-write.html#strukturieren",
    "href": "app-how-to-write.html#strukturieren",
    "title": "Anhang B — Writing principles",
    "section": "\nB.4 Strukturieren",
    "text": "B.4 Strukturieren\nDas grobe Gerüst ist ja vorgegeben. Eine wissenschaftliche Arbeit folgt dem IMRaD Schema. Erst die Einleitung (Introduction), dann die Methoden (Methods), gefolgt von den Ergebnissen (Results) und der Diskussion (Discussion). Am Ende der Einleitung wird nochmal die Fragestellung benannt. Welche Frage soll in der Arbeit beantwortet werden? Dann fehlt noch die Zusammenfassung am Anfang (abstract) und der Schluss bzw. das Fazit (conclusion). So ist das vorgegebene Schema für die Arbeit, das soll mit Inhalt gefüllt werden. Klingt erstmal einfach und ist es auch. Mit Zwischenüberschriften in den einzelnen Abschnitten kann man sich eine grobe Ordnung vorgeben, was in welcher Reihenfolge aufgeschrieben werden soll. Die Struktur innerhalb von Methodenteilen ist zum Beispiel oft gleich, Beschreibung der Studie, Beschreibung der interessierenden Variablen und ihrer Erhebung, Beschreibung der Auswertungsmethodik. Das kommt aber aufs Thema an. Und unterhalb dieser kann man sich wieder Unter-zwischen-unterüberschriften machen. Es wird sowieso noch alles überarbeitet.\n\n\n\n\n\n\nGrobe Strukturierung nach IMRaD\n\n\n\n\nZusammenfassung\n\n\nEinleitung\n\nForschungsfrage\n\n\nMaterial und Methoden\nErgebnisse\nDiskussion\n\n\nLiteratur\n\n\n\nWenn es einen Flowchart gibt, so gibt dieser auch die Struktur vor. Ein Flowchart ist nicht final und ändert sich mit der Zeit! Mach den Flowchart am besten auf einem Blatt Papier. Da kannst du schneller was ergänzen.\n\n\n\n\n\n\nHinweis\n\n\n\nZeichne einen Flowchart, der aufzeigt was in der Arbeit passiert!"
  },
  {
    "objectID": "app-how-to-write.html#rohtexten",
    "href": "app-how-to-write.html#rohtexten",
    "title": "Anhang B — Writing principles",
    "section": "\nB.5 Rohtexten",
    "text": "B.5 Rohtexten\nDas kann wirklich sloppy sein, in Stichpunkten oder hingerotzt, aber hier soll man sich auch nicht an Details aufhalten, sondern dem Arbeits- und Denkfluss folgen. Gerne auch Denglisch. Lieber erst Text schreiben und dann korrigieren. Wenn das Englische Wort nicht einfällt, das deutsche Hinschreiben. Den Schreibprozess nicht durch im Internet suchen und dann mal was Anderes gucken unterbrechen. Gerade wenn die Arbeit selbst noch im Entstehen ist, schreibt man erst einmal auf, was man tut, was man gemacht, gelernt, oder gelesen hat. Diese Frage stellt sich meist nach den ersten paar Sätzen in einer wissenschaftlichen Arbeit. Worum geht es hier eigentlich? Es könnte alles so einfach sein. Das ist normal. Durch das Aufschreiben werden einem meist die Dinge klarer und uns wird bewusst, wo wir nochmal genauer einhaken müssen."
  },
  {
    "objectID": "app-how-to-write.html#reflektieren",
    "href": "app-how-to-write.html#reflektieren",
    "title": "Anhang B — Writing principles",
    "section": "\nB.6 Reflektieren",
    "text": "B.6 Reflektieren\nDie Grundideen sind jetzt schon mal auf dem Papier, jetzt muss man sich überlegen, wie daraus ein Text wird. Gut ist es, sich schon an dieser Stelle Feedback von Betreuern oder Kommilitonen zu holen. Das hilft auch, die eigene Perspektive auf den Text zu ändern und ihn aus mehreren Richtungen zu betrachten – was ist wichtig, was soll viel Platz einnehmen, was fehlt vielleicht noch?\n\n\n\n\n\n\nHinweis\n\n\n\nWas könnte die zentrale Abbildung in den Ergebnissen sein?\n\n\nEin guter Ansatz um einen Fokus zu haben!"
  },
  {
    "objectID": "app-how-to-write.html#jetzt-schreiben-wir-wie-geht-es-am-besten",
    "href": "app-how-to-write.html#jetzt-schreiben-wir-wie-geht-es-am-besten",
    "title": "Anhang B — Writing principles",
    "section": "\nB.7 Jetzt schreiben wir! Wie geht es am besten?",
    "text": "B.7 Jetzt schreiben wir! Wie geht es am besten?\nDafür geht gut die Hälfte der Schreibzeit drauf. Am wichtigsten sind Inhalt und Struktur, Korrektheit und Verständlichkeit dürfen aber auch nicht unterschätzt werden: Wir nutzen einfache englische Sprache. Das geschriebene Wort muss nicht schlau klingen, sondern der Inhalt muss schlau sein. Keine umständlichen, gekünstelten Verben verwenden, wenn es ein einfaches Verb auch tut. Wir machen es dem Leser einfach. Später wirst du in einem wissenschaftlichen Paper „we” schreiben, da man selten ein Paper alleine schreibt. Um das jetzt hier gleich am Anfang zu üben, schreibst du keine verschachtelten Passivkonstruktionen, sondern „I do/did something”. Das fühlt sich erst seltsam an, aber wir als Leser danken dir!\n\n\n\n\n\n\nSchlecht\n\n\n\nAfter the raw methylation data has been preprocessed, a student t test was used for the differential analysis.\n\n\n\n\n\n\n\n\nGut\n\n\n\nI used the student t test for the differential analysis after the preprocessing of the raw methylation data.\n\n\nWir nutzen auch keine Pronomen, wo man nicht weiß, was diese Pronomen aussagen sollen. Was ist „it” oder „they”?\n\n\n\n\n\n\nSchlecht\n\n\n\nThe dog and the cat walk into a house. It eats all the cookies.\n\n\n\n\n\n\n\n\nGut\n\n\n\nThe dog and the cat walk into a house. The cat eats all the cookies.\n\n\nWir führen alle Begriffe vorher ein, daher erklären wir diese Begriffe, bevor wir die Begriffe verwenden. Ja, mache Begriffe sind klar, aber welche das sind, ergibt sich manchmal erst auf Nachfrage bei uns und ist für einen Neuling in einem Fachbereich gar nicht zu wissen.\n\n\n\n\n\n\nSchlecht\n\n\n\nI analyzed the NGS data with an ANOVA after checking the residuals with a QQ-plot.\n\n\n\n\n\n\n\n\nGut\n\n\n\nI analyzed the next generation sequencing data (NGS) with an analysis of variance (ANOVA) after plotting the residuals of the model in a quantile-quantile plot (QQ-plot).\n\n\nIst dir der Begriff nicht klar, erkläre ihn. Später können wir immer noch kürzen. Erkenntnisgewinn durch schreiben ist das Ziel."
  },
  {
    "objectID": "app-how-to-write.html#wer-macht-was-die-frage-des-lesers-an-jeden-einzelnen-satz",
    "href": "app-how-to-write.html#wer-macht-was-die-frage-des-lesers-an-jeden-einzelnen-satz",
    "title": "Anhang B — Writing principles",
    "section": "\nB.8 Wer macht was? Die Frage des Lesers an jeden einzelnen Satz!",
    "text": "B.8 Wer macht was? Die Frage des Lesers an jeden einzelnen Satz!\nKomme in den ersten sieben Wörtern zum Punkt, wer was macht. Subjekt und Prädikat sollen nah beieinander sein und möglichst früh im Satz kommen. Wir schreiben kurze Sätze und vermeiden komplizierte Schachtelsätze.\n\n\n\n\n\n\nSchlecht\n\n\n\nThe differential analysis of whole genome genetic data - like methylation pattern or expression analysis - has a long history of different invented methods and I used different analysis methods to find the best method for the analysis of methylation data with repeated measurements.\n\n\n\n\n\n\n\n\nGut\n\n\n\n[1] A long history of different analysis methods like methylation pattern or expression analysis exists. [2] Hence, many scientist have invented different analysis methods of whole genome."
  },
  {
    "objectID": "app-how-to-write.html#löschen-von-text",
    "href": "app-how-to-write.html#löschen-von-text",
    "title": "Anhang B — Writing principles",
    "section": "\nB.9 Löschen von Text",
    "text": "B.9 Löschen von Text\nText löschen macht keine Freude. Es ist immer nervig Teile des Textes, den man so mühsam in die Maschine getippt hat, zu löschen. Lege dir eine neue Datei an, in der du alles was du löschen willst reinkopierst. Bei mir heißt die Datei dump.docx oder dump.R oder auch anders. Auf jeden Fall löschst du so keinen Text, sondern bewahrst ihn erstmal auf. Denk immer daran, es geht nicht darum nur viel Text zu produzieren!"
  },
  {
    "objectID": "app-how-to-write.html#zum-schluss-kommen",
    "href": "app-how-to-write.html#zum-schluss-kommen",
    "title": "Anhang B — Writing principles",
    "section": "\nB.10 Zum Schluss kommen",
    "text": "B.10 Zum Schluss kommen\nWie Anne und Jochen1 jetzt in diesem Augenblick musst du zum Schluss kommen. Kein Text ist perfekt, kein Gedankengang so klar niedergeschrieben, wie er sein könnte. Aber irgendwann muss gut sein. Lass das Perfekte nicht der Feind des Guten sein. Dieser Leitfaden ist good enough und somit muss es reichen. Viel Erfolg!\n\n\n\n\n\n\nErfahrungsbericht von ehemaligen Bachelorstudierenden\n\n\n\n\nkeine neuen (wichtigen) Begriffe verwenden, wenn sie nicht vorher eingeführt wurden\nwenn man einen Begriff nicht richtig versteht oder nicht richtig erklären kann, sollte man ihn nicht verwenden. Daher ist es sehr hilfreich sich gründlich in den Hintergrund des Themas einzulesen.\nman sollte mehr wissen über das Thema, als man eigentlich im Text erklärt (um auf Fragen vorbereitet zu sein)\nAchtgeben auf die Zeitformen\n“it”, “they” und weitere Pronomen vermeiden, d.h. immer klar machen worauf man sich bezieht\nwenn etwas komplizierter scheint, sollte man Beispiele benutzen oder es eventuell graphisch darstellen. Aber: ein Bild nicht verwenden, wenn es keinen inhaltlichen Wert hat! Bringt dieses Bild etwas zum Verständnis des Lesers bei?\nin der Diskussion kein neues “Fass aufmachen”\ndie Limitationen in der Diskussion im Fließtext “verstecken”\ndie eigene Arbeit nicht schlecht reden; “schlechte” Resultate sind auch Resultate\nkeine zu langen Sätze\nes sollte ein roter Faden in jedem Kapitel zu erkennen sein: Was ist das Ziel? Wie erreiche ich das? Was bringt mir diese Methode? Was sagen mir die Resultate (in Bezug auf mein Ziel)?\nund für die eigene Motivation: Es werden oft Schreibblockaden kommen, es wird oft frustrierend sein, weil vielleicht etwas umgeschmissen wird und man von vorne anfangen muss, etc., aber davon soll man sich nicht entmutigen lassen\nDer Text muss nicht gleich beim ersten Hinschreiben perfekt sein, sondern es hilft erstmal etwas drauflos zu schreiben um die Gedanken besser ordnen zu können\nähnlich beim Programmieren: hier hat es mir auch geholfen einfach erstmal anzufangen und Ideen aufzuschreiben, aus welchen sich kann das Programm aufbauen kann\nMan sollte nicht zögern so etwas wie deepl (deepl.com) zu benutzen, wenn man mal mit einer englischen Formulierung nicht weiterkommt\nMan kann vor Beginn eines Tages ein realistisches Tagesziel (oder alternativ ein Wochenziel) festlegen, dann hat man ein Zwischenziel vor Augen und ist zufrieden mit sich, wenn dieses erreicht ist"
  },
  {
    "objectID": "app-how-to-write.html#footnotes",
    "href": "app-how-to-write.html#footnotes",
    "title": "Anhang B — Writing principles",
    "section": "",
    "text": "Die beide zusammen die erste Version von diesem Text mal 2019 geschrieben haben. Tja, wie so die Zeit vergeht.↩︎"
  },
  {
    "objectID": "app-bewertungsbogen.html#formalia-10-punkte",
    "href": "app-bewertungsbogen.html#formalia-10-punkte",
    "title": "Anhang C — Bewertungsbogen",
    "section": "\nC.1 Formalia (10 Punkte)",
    "text": "C.1 Formalia (10 Punkte)\n\n\n\n\n\n\n\n\nMaximale Punkte\nErhaltene Punkte\n\n\n\nIMRAD eingehalten\n1\n\n\n\n\nKohärenz (0 - unzusammenhängend, 1 - Mängel, mehrere Brüche, aber nicht völlig daneben, 2 - weitgehend klarer Zusammenhang, kleinere Brüche, 3 - guter Bezug der Teile aufeinander; 0 - k.o. Kriterium: Rückgabe)\n3\n\n\n\n\nRechtschreibung & Grammatik (0 - nicht vorhanden oder schwere Mängel, 1 - teilweise schwerwiegende Mängel, aber nicht durchgehend, 2 - leichte Mängel, 3 - weitgehend korrekt; 0 - k.o. Kriterium: Rückgabe)\n3\n\n\n\n\nProgrammcode & Literaturdatenbank (0 - nicht vorhanden oder schwere Mängel, 1 - teilweise schwerwiegende Mängel, aber nicht durchgehend, 2 - leichte Mängel, 3 - weitgehend korrekt; 0 - k.o. Kriterium: Rückgabe)\n3\n\n\n\nErreichte Punkte\n10"
  },
  {
    "objectID": "app-bewertungsbogen.html#einleitung-10-punkte",
    "href": "app-bewertungsbogen.html#einleitung-10-punkte",
    "title": "Anhang C — Bewertungsbogen",
    "section": "\nC.2 Einleitung (10 Punkte)",
    "text": "C.2 Einleitung (10 Punkte)\nDie Einleitung ist im Bezug zur Biologie kurz und prägnant geschrieben. Es ist ein klarer Fokus gesetzt. Allgemeinheiten, wie die Struktur der DNA sind bekannt. Wert wird auf die Methodik und deren Referenzen gelegt.\n\n\n\n\n\n\n\n\nMaximale Punkte\nErhaltene Punkte\n\n\n\n\nVorstellung des Themas und der Fragestellung. Der Problemhintergrund wird aufgezeigt\n2\n\n\n\n\nBedeutung und Relevanz Die methodische Relevanz wird betont, die biologische Relevanz erwähnt.\n2\n\n\n\n\nBezugnahme auf Praxis und Theorie Der Fokus liegt auf den Methoden und deren Limitierungen sowie die Auseinandersetzung mit der Anwendung auf das aktuelle, praktische Problem\n5\n\n\n\n\nAusblick auf Gliederung des Berichts. Sehr kurzer Überblick über die weitere Arbeit.\n1\n\n\n\nErreichte Punkte\n10"
  },
  {
    "objectID": "app-bewertungsbogen.html#material-methoden-14-punkte",
    "href": "app-bewertungsbogen.html#material-methoden-14-punkte",
    "title": "Anhang C — Bewertungsbogen",
    "section": "\nC.3 Material & Methoden (14 Punkte)",
    "text": "C.3 Material & Methoden (14 Punkte)\nDas Zentrum der Arbeit. Die Methodiken werden vorgestellt und der Bezug zur Fragestellung aufgezeigt. Die Beschreibung der Methoden ist nachvollziehbar und im Zweifel auch ein wenig zu detailliert.\n\n\n\n\n\n\n\n\nMaximale Punkte\nErhaltene Punkte\n\n\n\n\nDesign und Datenerhebung nachvollziehbare und vollständige Beschreibung der Daten zusammen mit Unsicherheiten der Fremderhebung.\n4\n\n\n\nGeeignet und begründete Literatur. Warum wenden wir die Methoden an? Was sind die Unterschiede zwischen den Methoden?\n2\n\n\n\nAuswertungsmethoden nachvollziehbar, korrekt, mit aktueller Literatur. Detaillierte Beschreibung der Methodik im Bezug zur Fragestellung.\n6\n\n\n\nLiteratur enthält aktuelle wissenschaftliche Veröffentlichungen\n2\n\n\n\nErreichte Punkte\n14"
  },
  {
    "objectID": "app-bewertungsbogen.html#ergebnisse-10-punkte",
    "href": "app-bewertungsbogen.html#ergebnisse-10-punkte",
    "title": "Anhang C — Bewertungsbogen",
    "section": "\nC.4 Ergebnisse (10 Punkte)",
    "text": "C.4 Ergebnisse (10 Punkte)\nWelche Ergebnisse liefern die Methoden? Wird ein Fokus gesetzt, wenn viele Ergebnisse produziert wurden? Sind die Abbildungen in einer hohen Qualität, sodass Dritte die Abbildungen verstehen? Sind die Abbildungen angemessen und ausführlich beschriftet?\n\n\n\n\n\n\n\n\nMaximale Punkte\nErhaltene Punkte\n\n\n\n\nKlare und vollständige Darstellung der Ergebnisse Es ist ein Fokus auf bedeutende Ergebnisse zu erkennen.\n4\n\n\n\n\nQualität der Abbildungen und Tabellen Abbildungen sind, wenn möglich und sinnvoll mit ggplot2 erstellt.\n3\n\n\n\nAbbildungen und Tabellen sind klar beschriftet und formatiert.\n2\n\n\n\nDie Auswahl der Ergebnisse ist sinnvoll und ausgewählt passend zur Fragestellung.\n1\n\n\n\nErreichte Punkte\n10"
  },
  {
    "objectID": "app-bewertungsbogen.html#diskussion-16-punkte",
    "href": "app-bewertungsbogen.html#diskussion-16-punkte",
    "title": "Anhang C — Bewertungsbogen",
    "section": "\nC.5 Diskussion (16 Punkte)",
    "text": "C.5 Diskussion (16 Punkte)\nDiskussion der Ergebnisse im Kontext der Methoden. Was hätte man anders machen können? Was wären die Effekte gewesen? Warum hat man sich für diese Methoden entschieden? Kritische Auseinandersetzung mit den getroffenen Entscheidungen.\n\n\n\n\n\n\n\n\nMaximale Punkte\nErhaltene Punkte\n\n\n\nNachvollziehbare und angemessene Interpretation der Ergebnisse, Diskussion von alternativen Interpretationen\n6\n\n\n\nBezug zur Fragestellung / Hypothese; Einordnung in theoretischen Hintergrund\n4\n\n\n\nSchlussfolgerungen; Implikationen für Praxis oder Forschung; Grenzen der Studie; Zielerreichung (ja/nein/warum)\n4\n\n\n\nAusblick - Was habe ich gelernt und was kommt jetzt?\n2\n\n\n\nErreichte Punkte\n16"
  },
  {
    "objectID": "app-bewertungsbogen.html#benotung",
    "href": "app-bewertungsbogen.html#benotung",
    "title": "Anhang C — Bewertungsbogen",
    "section": "\nC.6 Benotung",
    "text": "C.6 Benotung\n\n\nPunkte\nNote\nErreicht\n\n\n\n30\n4.0\n\n\n\n31\n4.0\n\n\n\n32\n3.7\n\n\n\n33\n3.7\n\n\n\n34\n3.7\n\n\n\n35\n3.7\n\n\n\n36\n3.7\n\n\n\n37\n3.7\n\n\n\n38\n3.7\n\n\n\n39\n3.0\n\n\n\n40\n3.0\n\n\n\n41\n3.0\n\n\n\n42\n2.7\n\n\n\n43\n2.7\n\n\n\n44\n2.7\n\n\n\n45\n2.3\n\n\n\n46\n2.3\n\n\n\n47\n2.3\n\n\n\n48\n2.3\n\n\n\n49\n2.0\n\n\n\n50\n2.0\n\n\n\n51\n2.0\n\n\n\n52\n1.7\n\n\n\n53\n1.7\n\n\n\n54\n1.7\n\n\n\n55\n1.3\n\n\n\n56\n1.3\n\n\n\n57\n1.3\n\n\n\n58\n1.3\n\n\n\n59\n1.0\n\n\n\n60\n1.0\n\n\n\n\n\n\nZahl\nIn Worten\n\n\n\\(\\phantom{1.7}\\)\n\n\n\nStempel / Datum / Unterschrift"
  },
  {
    "objectID": "module.html#sec-module-bioinformatik",
    "href": "module.html#sec-module-bioinformatik",
    "title": "Anhang D — Modulbeschreibung",
    "section": "\nD.1 Statistische Bioinformatik",
    "text": "D.1 Statistische Bioinformatik\n\nD.1.1 Inhalte und Qualifikationsziele des Moduls\n\nD.1.1.1 Kurzbeschreibung\nDie statistische Bioinformatik spielt eine bedeutende Rolle in verschiedenen wissenschaftlichen Bereichen der Omics-Forschung. Sie wird nicht nur klassisch genutzt, um genetische Marker für Krankheiten oder genetische Veranlagungen in der Humangenetik zu identifizieren, sondern auch zur Entschlüsselung der genetischen Vererbung bei der Züchtung von Tieren und Pflanzen. Die algorithmische Auswertung von genetischen Daten ist sowohl biologisch umfassend als auch methodisch anspruchsvoll. Das Modul “Statistische Bioinformatik” folgt dem zentralen Dogma der Molekularbiologie, das besagt, dass Informationen von Genen zu Proteinen übertragen werden. Es beinhaltet die statistische Analyse von genetischen Markern auf DNA-Ebene sowie die Untersuchung der Genexpression über mRNA und die daraus resultierende Proteinexpression. Auch regulatorische Elemente der DNA-Expression, wie die Methylierung, werden im Modul berücksichtigt. Das Modul “Statistische Bioinformatik” bietet zuerst eine Einführung in die klassische mendelsche Genetik sowie Grundlagen in der quantitativen Genetik. Den Großteil den Inhalts machen aktuelle algorithmische Verfahren zu Assoziationsstudien und Sequenzanalysen aus. Studierende erlernen somit die grundlegenden Fähigkeiten für zukünftige wissenschaftliche und angewandte Arbeiten im Bereich der Omics-Forschung. Der Schwerpunkt des Moduls liegt auf der Darstellung, Verarbeitung und statistischen Auswertung von hochdimensionalen genetischen Daten. Fallstudien aus den Bereichen der Agrarwissenschaften und Humanbiologie dienen als praktische Anwendungen. Gleichzeitig werden die erworbenen theoretischen Kenntnisse durch die Auswertung von Daten in R/Bioconductor in die Praxis umgesetzt. Das Modul “Statistische Bioinformatik” legt somit den Grundstein für die methodische Auswertung von Omics-Daten.\n\nD.1.1.2 Lehr-Lerninhalte\n\nGrundlagen der klassischen, mendelschen Vererbung beinhaltend das Hardy-Weinberg-Gleichgewicht\nGrundlagen der Quantitativen Genetik mit Kopplungskarten und Linkage disequilibrium\nGenetische Distanz und polygenetische Bäume zur Darstellung evolutionärer Beziehungen\nMarkergestützte Selektion und indirekte Selektion\nStatistische Herausforderungen von hochdimensionalen Daten\nData Preprocessing und Qualitätskontrolle auf der Ebene der Marker und Individuen\nGängige Assoziationstests unter anderem Chi-Quadrat-Test und logistische Regression\nAuswertung von Genomeweite Assoziationsstudien, Microarraydaten, High throughput genotyping, Next generation sequencing, Whole genome sequencing und RNA-seq an Fallbeispielen\nVisualisierungen der Ergebnisse durch Manhattanplot, Vucanoplot und Regional Association Plot und weiteren.\nGrundlagen Multiomics und Pathway Analysen sowie deren Datenbanken\nGrundlegende Methoden zum Sequenzaligment\nAnwendung der Algorithmen an ausgewählten Fallbeispielen in R/Bioconductor\n\nD.1.2 Kompetenzorientierte Lernergebnisse\n\nD.1.2.1 Wissen und Verstehen\n\nD.1.2.1.1 Wissensverbreiterung\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu den jeweiligen Omics-Verfahren zu erkennen und zu benennen.\nDie Studierenden können Omics-Abbildungen erkennen und benennen.\nDie Studierenden sind in der Lage aus englischen Tutorien eine Lösung für ein Omics-Verfahren einzugrenzen.\nDie Studierenden können externe Programme aus R heraus starten und Ergebnisse einlesen.\n\nD.1.2.1.2 Wissensvertiefung\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können aus explorative Abbildungen die entsprechende Datenstruktur zur Erstellung der Abbildungen wiedergeben.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage experimentelle Daten nach dem Konzept der der “tidy data” zu erheben.\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\nDie Studierenden können einfache Auswahl- und Filterregeln auf Datensätze in R anwenden.\nDie Studierenden können erste einfache R Code Blöcke miteinander in einen Kontext setzen.\n\nD.1.2.1.3 Wissensverständnis\n\nDie Studierenden können einen statistischen Test mit einer explorativen Datenanalyse in einen Kontext bringen.\nDie Studierenden sind in der Lage eine einfache statistische Auswertung mit den notwendigen Funktionen und Paketen in R zu skizzieren.\n\nD.1.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nD.1.2.2.1 Nutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in der Agrarwirtschaft und Ingenieurwissenschaften. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen. Die Studierenden sind in der Lage die grundlegenden Konzepte der Programmierung in R in anderen Programmiersprachen zuerkennen.\n\nD.1.2.2.2 Wissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\nD.1.2.3 Kommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren.\n\nD.1.2.4 Wissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.1.3 Literatur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. https://r4ds.had.co.nz\n\nD.1.4 Voraussetzungen für die Teilnahme\nKeine. Bitte aber die empfohlenen Vorkenntnisse beachten.\n\nD.1.5 Empfohlene Vorkenntnisse\nFür dieses Modul werden vertiefte Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie unter anderem in den Modulen “Mathematik und Statistik (44B0266)” und “Angewandte Statistik und Versuchswesen (44B0400)” oder “Angewandte Statistik für Bioverfahrenstechnik (44B0567)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\nD.1.6 Zusammenhang mit anderen Modulen\nDas Modul “Statistische Bioinformatik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nEinführung in die Pflanzenzüchtung (44B0112)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\nD.1.7 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nD.1.7.1 Benotete Prüfungsleistung\nHausarbeit\n\nD.1.7.2 Unbenotete Prüfungsleistung\nPräsentation"
  },
  {
    "objectID": "module.html#sec-module-mathematik",
    "href": "module.html#sec-module-mathematik",
    "title": "Anhang D — Modulbeschreibung",
    "section": "\nD.2 Mathematik und Statistik",
    "text": "D.2 Mathematik und Statistik\n\nD.2.1 Inhalte und Qualifikationsziele des Moduls\n\nD.2.1.1 Kurzbeschreibung\nIn den Pflanzenwissenschaften sowie in der Landwirtschaft werden vielen Prozesse und Phänomene durch mathematische und statistische Modelle beschrieben. In dem Modul “Mathematik und Statistik” lernen Studierende drei Schwerpunkte für das spätere wissenschaftliche und angewandte Arbeiten. Im ersten Teil des Moduls werden mathematische Grundkenntnisse wiederholt und im Verlauf des Moduls vertieft. Die mathematischen Formeln werden aus ihrer theoretischen, formalistischen Anwendung herausgelöst und in praktische Herausforderungen übertragen. Dabei werden Bereiche der Physik, Chemie sowie Biologie in den Kontext der Mathematik gesetzt. Im zweiten Teil des Moduls werden statistische Grundkenntnisse vermitteln. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Die für Landwirtschaft und Gartenbau relevanten mathematischen und statistischen Verfahren werden dargestellt und diskutiert. Im dritten Teil des Moduls werden die erworbenen theoretischen, mathematischen und statistischen Kenntnisse durch die Einführung in die Programmierung in R für die Studierenden umsetzbar und erfahrbar gemacht. In dem Modul “Mathematik und Statistik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben.\n\nD.2.1.2 Lehr-Lerninhalte\nMathematischer Anteil\n\nMaßzahlen, Flächen und Volumen beinhaltend Berechnungen mit Maßeinheiten von sehr kleinen sowie sehr großen Zahlen. Berechnungen mit Flächen- sowie Volumenmaßen einschließlich Winkel- und Streckenbestimmung.\nBerechnungen mit Vektoren und Matrizen.\nMathematische Funktionen und Anwendung der Differential- und Integralrechnung einschließlich logarithmischer sowie exponentieller Funktionen. Lösung von quadratischer Gleichungen sowie Extremwertproblemen.\nWahrscheinlichkeiten mit Baumdiagramm und Pfadregeln sowie stochastische Prozesse. Wahrscheinlichkeitsverteilungen am Beispiel der Normalverteilung.\nLogische Operatoren sowie Mengenlehre.\n\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse mit Fokus auf dem Boxplot und dem Barplot und deren statistischen Maßzahlen.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test. Einführung in die Varianzanalyse.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nInformatorischer Anteil\n\nEinführung in die Programmierung in R anhand von Skalenarten sowie der Darstellung von Daten in R.\nKonzept von Objekten, Funktionen sowie Pipen und der Vorstellung des tidyverse in R.\nEinlesen von Daten und deren Bearbeitung sowie Visualisierung in R\n\nD.2.2 Kompetenzorientierte Lernergebnisse\n\nD.2.2.1 Wissen und Verstehen\n\nD.2.2.1.1 Wissensverbreiterung\nMathematischer Anteil\n\nDie Studierenden sind in der Lage mathematische Formeln in der Literatur zu finden.\nDie Studierenden können ein Baumdiagramm für die Berechnung von Wahrscheinlichkeiten erstellen.\n\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen.\nDie Studierenden können einfache explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nInformatorischer Anteil\n\nDie Studierenden können die Anforderungen an einen Datensatz zur Verwendung in R benennen.\nDie Studierenden können in R Objekte, Funktionen und Zahlenvektoren unterscheiden und kennen die gängigen Operatoren in R.\nDie Studierenden können den Ablauf für die Erstellung einer explorativen Datenanalyse in R beschreiben.\n\nD.2.2.1.2 Wissensvertiefung\nMathematischer Anteil\n\nDie Studierenden sind in der Lage mathematische Formeln in einem anwendungsorientierten Kontext anzuwenden.\nDie Studierenden können sinnvolle Abschätzungen von linearen und exponentiellen Wachstum vornehmen.\n\nStatistischer Anteil\n\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\n\nD.2.2.1.3 Wissensverständnis\nMathematischer Anteil\n\nDie Studierenden können praktische Fragestellungen in einen formalisierten, mathematischen Kontext übersetzen.\nDie Studierenden sind in der Lage die Wahrscheinlichkeit für das Eintreten eines Ereignisses abzuschätzen.\n\nStatistischer Anteil\n\nDie Studierenden können einen einfachen statistischen Test mit einer explorativen Datenanalyse in einen Kontext setzen.\n\nD.2.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nD.2.2.2.1 Nutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare und exponentielle Kosten- und Nutzenabschätzungen anhand von mathematischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von technischen und biologischen Prozesses in den Pflanzenwissenschaften sowie in der Landwirtschaft. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen.\n\nD.2.2.2.2 Wissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können eine Reihe von explorativen Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\nD.2.2.3 Kommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können einfachen R Code lesen und demonstrieren.\n\nD.2.2.4 Wissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.2.3 Literatur\n\nDas Skript des Mathematikteils des Moduls unter https://jkruppa.github.io/math/\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\n\nD.2.4 Voraussetzungen für die Teilnahme\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.2.5 Empfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.2.6 Zusammenhang mit anderen Modulen\nDas Modul “Mathematik und Statistik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nAngewandte Statistik und Versuchswesen (44B0400)\nChemie und Biochemie (44B0532)\nPhysikalische Grundlagen der Natur und Agrartechnik (44B0534)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\nD.2.7 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nD.2.7.1 Benotete Prüfungsleistung\nKlausur"
  },
  {
    "objectID": "module.html#sec-module-angewandte",
    "href": "module.html#sec-module-angewandte",
    "title": "Anhang D — Modulbeschreibung",
    "section": "\nD.3 Angewandte Statistik und Versuchswesen",
    "text": "D.3 Angewandte Statistik und Versuchswesen\n\nD.3.1 Inhalte und Qualifikationsziele des Moduls\n\nD.3.1.1 Kurzbeschreibung\nDer wissenschaftliche Fortschritt in den Agrarwissenschaften ist wesentlich getragen durch eine intensive experimentelle Versuchstätigkeit. Um erfolgreich in diesem Bereich tätig zu sein sind neben statistischen Kenntnissen auch solche über die Techniken zur Versuchsdurchführung erforderlich. Für die Versuchsdurchführung müssen Messdaten und Beobachtungen aus Erhebungen sowie aus experimentellen Versuchen in einem Datensatz aufgearbeitet werden. In dem Modul “Angewandte Statistik und Versuchswesen” lernen Studierende die grundlegenden Algorithmen der Statistik für das spätere wissenschaftliche und angewandte Arbeiten kennen. Das Modul vermittelt die dafür notwendigen statistischen und algorithmischen praktischen Kenntnisse. Verschiedene statistische Verfahren zur Auswertung von experimentellen Daten werden vorgestellt und die statistischen Maßzahlen für das lineare Modellieren eingeübt. Einfache experimentelle Designs werden vorgestellt und Anwendungsmöglichkeiten diskutiert. Die vorhandenen Programmierkenntnisse in R werden weiter vertieft. Verschiedene einfache Fallbeispiele dienen als Einstieg für die Diskussion und der Reflexion der eigenen Versuchstätigkeit. Das Modul “Angewandte Statistik und Versuchswesen” schließt den Erwerb der Grundlagen in der Bio Data Science ab und ermöglicht den Studierenden somit einfache Experimente in den Agrarwissenschaften selbstständig zu planen und auszuwerten.\n\nD.3.1.2 Lehr-Lerninhalte\nStatistischer Anteil\n\nDie explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in statistische Verteilungen anhand der Poisson- und Normalverteilung.\nDie Varianzanalyse beinhaltend die einfaktorielle sowie zweifaktorielle ANOVA.\nGrundlagen des nicht-parametrischen Tests beinhaltend Wilcoxon-Mann-Whitney-Test sowie Kruskal-Wallis-Test.\nGrundlagen der simplen linearen Regression und der multiplen linearen Regression sowie deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nDiagnostischen Testen und deren statistischen Maßzahlen.\nChi-Quadrat-Test für eine Vierfeldertafel.\nDas multiple Testen von mehreren Mittelwerten und deren Visualisierungen.\nEinführung in die klassischen experimentellen Designs in den Agrarwissenschaften sowie die einfache Versuchsplanung.\n\nInformatorischer Anteil\n\nDurchführung aller theoretisch erarbeiteten Inhalte in R.\nInterpretation und Bewertung von einfachen statistischen Modellierungen in R.\nEinfache Transformationen von Daten für die explorative Datenanalyse.\nDemonstration der automatisierten Erstellung von Berichten in Rmarkdown sowie in R Quatro.\n\nD.3.2 Kompetenzorientierte Lernergebnisse\n\nD.3.2.1 Wissen und Verstehen\n\nD.3.2.1.1 Wissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden kennen einfache experimentelle Designs in den Agrarwissenschaften.\nDie Studierenden kennen einfache Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\nDie Studierenden sind in der Lage zwischen einen parametrischen und einem nicht-parametrischen Test zu unterscheiden.\n\nInformatorischer Anteil\n\nDie Studierenden kennen die gängigen Funktionen für die Datenaufbereitung in R.\nDie Studierenden können den Ablauf für die Erstellung einer einfachen Datenanalyse in R beschreiben.\nDie Studierenden sind in der Lage aus englischen Internetquellen eine Lösung für ein R Problem einzugrenzen.\n\nD.3.2.1.2 Wissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können eine simple lineare Regression für eine Normalverteilung modellieren.\nDie Studierenden können eine Aussage über die Güte eines simplen linearen Modells abgeben.\nDie Studierenden können eine Korrelation berechnen und interpretieren.\nDie Studierenden können einen multiplen Gruppenvergleich für einen normalverteilten Endpunkt rechnen und die p-Werte entsprechend adjustieren.\nDie Studierenden sind in der Lage eine einfache explorative Datenanalyse mit einem multiplen Gruppenvergleich zu verbinden.\n\nInformatorischer Anteil\n\nDie Studierenden können Datensätze in R bearbeiten.\nDie Studierenden können einfache experimentelle Designs in R visualisieren.\nDie Studierenden können verschiedene Ausgaben von statistischen Tests in R visualisieren.\n\nD.3.2.1.3 Wissensverständnis\nStatistischer Anteil\n\nDie Studierenden sind die der Lage eine wissenschaftliche Fragestellung mit einem einfachen experimentellen Design zu verbinden.\nDie Studierenden können einfache linearen Modellierungen bewerten und interpretieren.\n\nInformatorischer Anteil\n\nDie Studierenden können verschiedene statistische Tests und eine lineare Modellierung mit einer explorativen Datenanalye in einen Kontext setzen.\n\nD.3.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nD.3.2.2.1 Nutzung und Transfer\nDie Studierenden sind in der Lage einfache Kosten- und Nutzenabschätzungen anhand von statistischen linearen Modellen durchzuführen. Diese Abschätzungen umfassen im Besonderen die Planung von einfachen experimentellen Designs in den Agrarwissenschaften. Die Studierenden können statistische Unterschiede aus multiplen Gruppenvergleichen berechnen und eine Risikoabschätzung treffen. Die Studierenden sind in der Lage selbständig einfache statistische Analysen auf Datensätzen in R durchzuführen. Die Studierenden können einfache experimentelle Designs für verschiedene Berufsfelder und Anwendungen abwägen und diskutieren.\n\nD.3.2.2.2 Wissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden können selbständig eigene wissenschaftliche Fragestellungen mit Fallbeispielen abgleichen und entsprechend der eigenen Anforderungen modifizieren. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die wissenschaftliche Verwertbarkeit in R zu gewährleisten. Die Studierenden kennen die Möglichkeit über automatisierte Berichte die Reproduzierbarkeit der eigenen Forschungsergebnisse zu gewährleisten.\n\nD.3.2.3 Kommunikation und Kooperation\nDie Studierenden sind in der Lage die Daten der durchgeführten Experimente und entsprechende R Skripte der statistische Auswertungen mit anderen Forschenden zu teilen. Die Studierenden können die statistischen Analyseergebnisse vorstellen und Änderungswünsche entsprechend durchführen.\n\nD.3.2.4 Wissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.3.3 Literatur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\nData Science for Agriculture in R unter https://schmidtpaul.github.io/DSFAIR/\n\nD.3.4 Voraussetzungen für die Teilnahme\nKeine.\n\nD.3.5 Empfohlene Vorkenntnisse\nFür dieses Modul werden Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie in dem Modul “Mathematik und Statistik (44B0266)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\nD.3.6 Zusammenhang mit anderen Modulen\nDas Modul “Angewandte Statistik und Versuchswesen” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nSteuerung der vegetativen Entwicklung krautiger Pflanzen (44B0608)\nProjektplanung und -management (44B0654)\nBerufspraktisches Projekt (BAP) (44B0595)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\nD.3.7 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nD.3.7.1 Benotete Prüfungsleistung\nKlausur"
  },
  {
    "objectID": "module.html#sec-module-spezielle",
    "href": "module.html#sec-module-spezielle",
    "title": "Anhang D — Modulbeschreibung",
    "section": "\nD.4 Spezielle Statistik und Versuchswesen",
    "text": "D.4 Spezielle Statistik und Versuchswesen\n\nD.4.1 Inhalte und Qualifikationsziele des Moduls\n\nD.4.1.1 Kurzbeschreibung\nMit dem Fortschreiten der Digitalisierung können in den Pflanzenwissenschaften und der Landwirtschaft komplexere Experimente durchgeführt werden. Die Digitalisierung erlaubt die automatisierte Erfassung und Speicherung großer Datenmengen, die über entsprechende statistische Algorithmen aggregiert und ausgewertet werden müssen. Diese Daten können zur Steuerung der Produktion oder zur Erkennung von unerwünschten Ereignissen genutzt werden. Dadurch kann eine bessere Qualitätssicherung und Entwicklung gewährleistet werden. In dem Modul “Spezielle Statistik und Versuchswesen” lernen Studierende die fortgeschrittenen Algorithmen für das spätere wissenschaftliche und angewandte Arbeiten mit großen Datenmengen. Das Modul vermittelt die dafür notwendigen statistischen und algorithmischen praktischen Kenntnisse. Verschiedene statistische Verfahren werden vorgestellt und die statistischen Maßzahlen für die Modellselektion eingeübt. Im Weiteren werden maschinelle Lernverfahren präsentiert und auf Fallbeispiele angewendet. Der Fokus des Moduls liegt auf der praktischen Anwendung und Diskussion der Ergebnisse der statistischen Modellierungen. Die vorhandenen Programmierkenntnisse in R werden weiter vertieft und automatisierte Berichtserstellung mit Quarto und RMarkdown eingeübt. Das Arbeiten mit großen Datenmengen wird so für die Studierenden umsetzbar und erfahrbar gemacht. Das Modul “Spezielle Statistik und Versuchswesen” befähigt Studierende in dem Bereich der Bio Data Science in verschiedenen Anwendungsfeldern praktisch tätig zu sein.\n\nD.4.1.2 Lehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die gängigen multiplen linearen Regressionen und deren Verteilungsfamilien beinhaltend die Gaussian, Poisson, Multinominal/Ordinal und Binomial.\nGrundlagen der statistischen Maßzahlen der Modellgüte einer multiplen linearen Regression sowie deren Effektschätzer.\nGrundlagen der Variablenselektion und Imputation von fehlenden Werten sowie Ausreißerdetektion.\nEinführung in die linearen gemischten Modelle und die Berücksichtigung von Messwiederholungen.\nEinführung in die nicht lineare Regression.\nVertiefte Auseinandersetzung mit multiplen Gruppenvergleichen und deren Möglichkeiten der Visualisierung von Gruppenunterschieden.\nEinführung in die Äquivalenz oder Nichtunterlegenheit in der praktischen Anwendung.\nEinführung in die klassischen experimentellen Designs in den Agrarwissenschaften.\nGrundlagen des maschinellen Lernens und der Klassifikation von Ereignissen sowie Maßzahlen der Bewertung eines maschinellen Lernalgorithmus.\nAnwendung der grundlegenden maschinellen Lernverfahren wie k-NN, Random Forest, Support Vector Machine und Neuronale Netze.\n\nInformatorischer Anteil\n\nDurchführung aller theoretisch erarbeiteten Inhalte in R.\nInterpretation und Bewertung von statistischen Modellierungen in R.\nFortgeschrittene Programmierung in R unter der Verwendung von regulären Ausdrücken.\nAutomatisierte Erstellung von Berichten in Rmarkdown sowie in R Quatro.\nEinführung in die Erstellung von interaktiven R Shiny Apps.\n\nD.4.2 Kompetenzorientierte Lernergebnisse\n\nD.4.2.1 Wissen und Verstehen\n\nD.4.2.1.1 Wissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden kennen die gängigen experimentellen Designs in den Agrarwissenschaften.\nDie Studierenden kennen die entsprechenden Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden können die gängigen statistischen Modellierungen benennen und unterscheiden.\nDie Studierenden sind in der Lage zwischen einem kausalen und einem prädiktiven Modell zu unterscheiden.\n\nInformatorischer Anteil\n\nDie Studierenden kennen die gängigen Funktionen für die Datenaufbereitung in R.\nDie Studierenden sind in der Lage aus englischsprachigen Tutorien die statistische Analyseschritte für die eigenen Daten zu transferieren.\n\nD.4.2.1.2 Wissensvertiefung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage anhand einer wissenschaftlichen Fragestellung eine statistische Auswertung zu gliedern und zu planen.\nDie Studierenden können wissenschaftliche Veröffentlichungen lesen und in den statistischen Kontext richtig einordnen.\nDie Studierenden können eine multiple lineare Regression oder einen maschinellen Lernalgorithmus entsprechend des Endpunktes modellieren und interpretieren.\nDie Studierenden können einen multiplen Gruppenvergleich für verschiedene Endpunkte rechnen und die p-Werte entsprechend adjustieren.\nDie Studierenden können verschiedene technische Messparameter miteinander vergleichen und eine Aussage über die Nichtunterlegenheit treffen.\n\nInformatorischer Anteil\n\nDie Studierenden können mit regulären Ausdrücken Datensätze bearbeiten.\nDie Studierenden sind in der Lage durch eine eine parallele Programmierung eine serielle Programmierungen zu optimieren.\nDie Studierenden sind in der Lage einen automatisierten Bericht in Rmarkdown oder R Quarto zu erstellen\n\nD.4.2.1.3 Wissensverständnis\nStatistischer Anteil\n\nDie Studierenden sind die der Lage eine wissenschaftliche Fragestellung mit einem experimentellen Design und einer statistischen Modellierung zu verbinden.\nDie Studierenden können eine statistische Modellierung in einer Präsentation darstellen und vorstellen.\nDie Studierenden können eine wissenschaftliche Veröffentlichung anhand der verwendeten Statistik bewerten.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage in R eine statistische Modellierung zu planen und den entsprechenden R Code zu erstellen.\nDie Studierenden können R Code Chunks miteinander sinnvoll für die eigene Anwendung kombinieren und optimieren.\n\nD.4.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nD.4.2.2.1 Nutzung und Transfer\nDie Studierenden sind in der Lage Kosten- und Nutzenabschätzungen anhand von statistischen Modellen und deren Effektschätzern durchzuführen. Diese Abschätzungen umfassen im Besonderen die Planung von technischen und biologischen Prozesses in den Agrarwissenschaften. Die Studierenden können verschiedene technische Prozesse miteinander vergleichen und eine Aussage über die Nichtunterlegenheit oder den statistischen Unterschied treffen. Die beiden gegensätzlichen Konzepte von einem geplanten Experiment und einer technischen Nichtunterlegenheit können von den Studierenden unterschieden werden. Die Studierenden sind in der Lage selbständig Datenanalysen auf großen Datensätzen in R durchzuführen. Die Studierenden können die gängigen experimentellen Designs für verschiedene Berufsfelder und Anwendungen anpassen und durchführen.\n\nD.4.2.2.2 Wissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden sind in der Lage wissenschaftlich zu Arbeiten und eine praktische Fragestellung in einen wissenschaftlichen Erkenntnisprozess zu übersetzen. Die Studierenden können statistische Auswertungen aus wissenschaftlichen Publikationen verstehen und informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die wissenschaftliche Verwertbarkeit in R zu berücksichtigen. Die Studierenden können über die Erstellung von automatisierten Berichten die Reproduzierbarkeit der eigenen Forschungsergebnisse gewährleisten.\n\nD.4.2.3 Kommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept der automatisierten Berichtserstattung durchgeführte Experimente und statistische Auswertungen mit anderen Forschenden zu teilen. Die Studierenden sind dadurch in der Lage in multidiziplinären, wissenschaftlichen Teams mitzuwirken. Die Studierenden können eine gemeinsam geplante Forschungsskizze in R umsetzen. Die Studierenden sind in der Lage die Ergebnisse einer statistischen Analyse auch Fachfremden zu erläutern.\n\nD.4.2.4 Wissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden können wissenschaftliche Publikationen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeldes setzen und somit informierte Entscheidungen treffen. Die Studierende sind sich der inhärenten Unsicherheit der wissenschaftlichen Forschung bewusst und können die eigenen Forschungsergebnisse kritisch hinterfragen.\n\nD.4.3 Literatur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\nData Science for Agriculture in R unter https://schmidtpaul.github.io/DSFAIR/\nBruce, Peter, Andrew Bruce, und Peter Gedeck. 2020. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media.\n\nD.4.4 Voraussetzungen für die Teilnahme\nKeine. Bitte aber die empfohlenen Vorkenntnisse beachten.\n\nD.4.5 Empfohlene Vorkenntnisse\nFür dieses Modul werden vertiefte Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie in den Modulen “Mathematik und Statistik (44B0266)” und “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\nD.4.6 Zusammenhang mit anderen Modulen\nDas Modul “Spezielle Statistik und Versuchswesen” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nProjektauswertung und -vorstellung (44B0597)\nBerufspraktisches Projekt (BAP) (44B0595)\nBachelorarbeit (44B0365)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\nD.4.7 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nD.4.7.1 Benotete Prüfungsleistung\nHausarbeit\n\nD.4.7.2 Unbenotete Prüfungsleistung\nPräsentation"
  },
  {
    "objectID": "module.html#sec-module-statistik",
    "href": "module.html#sec-module-statistik",
    "title": "Anhang D — Modulbeschreibung",
    "section": "\nD.5 Statistik",
    "text": "D.5 Statistik\n\nD.5.1 Inhalte und Qualifikationsziele des Moduls\n\nD.5.1.1 Kurzbeschreibung\nIn der Agrarwirtschaft, Lebensmittelwissenschaft und Gartenbau werden vielen Prozesse und Phänomene durch mathematische und statistische Modelle beschrieben. Entwicklung, Qualitätssicherung und Marketing sind wesentlich getragen durch eine statistische Analyse von Daten. In dem Modul “Statistik” lernen Studierende die Grundlagen für das spätere wissenschaftliche und angewandte Arbeiten. Das Modul vermittelt die dafür notwendigen statistischen Grundkenntnisse. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Die für die Agrarwirtschaft, Lebensmittelwissenschaft und Gartenbau relevanten statistischen Verfahren und Modellierungen werden dargestellt und diskutiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt. Parallel dazu werden in dem Modul die erworbenen theoretischen, statistischen Kenntnisse durch die Einführung in die Programmierung in R für die Studierenden umsetzbar und erfahrbar gemacht. In dem Modul “Statistik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben.\n\nD.5.1.2 Lehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in statistische Verteilungen anhand der Poisson- und Normalverteilung.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test.\nEinführung in die Varianzanalyse beinhaltend die einfaktorielle sowie zweifaktorielle ANOVA.\nGrundlagen des nicht-parametrischen Tests beinhaltend Wilcoxon-Mann-Whitney-Test sowie Kruskal-Wallis-Test.\nGrundlagen der simplen linearen Regression und der multiplen linearen Regression sowie deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nDiagnostischen Testen und deren statistischen Maßzahlen.\nChi-Quadrat-Test für eine Vierfeldertafel.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nInformatorischer Anteil\n\nEinführung in die Grundlagen der Programmierung in R anhand von Skalenarten.\nEinführung in die Darstellung von Daten in R und die Vorstellung des Konzepts der “tidy data”.\nKonzept von Objekten, Funktionen sowie Pipen und der Vorstellung des tidyverse in R.\nEinlesen von Daten und deren Bearbeitung sowie Visualisierung in R.\nDurchführung der gängigen statistischen Tests und die Interpretierung der jeweiligen R Ausgaben.\n\nD.5.2 Kompetenzorientierte Lernergebnisse\n\nD.5.2.1 Wissen und Verstehen\n\nD.5.2.1.1 Wissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen und zu benennen.\nDie Studierenden können explorative Abbildungen erkennen und benennen.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nInformatorischer Anteil\n\nDie Studierenden können die Anforderungen an einen Datensatz zur Verwendung in R benennen.\nDie Studierenden können in R Objekte, Funktionen und Zahlenvektoren unterscheiden und kennen die gängigen Operatoren in R.\nDie Studierenden können den Ablauf für die Erstellung einer explorativen Datenanalyse in R beschreiben.\nDie Studierenden sind in der Lage aus englischen Internetquellen eine Lösung für ein R Problem einzugrenzen.\n\nD.5.2.1.2 Wissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können aus explorative Abbildungen die entsprechende Datenstruktur zur Erstellung der Abbildungen wiedergeben.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage experimentelle Daten nach dem Konzept der der “tidy data” zu erheben.\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\nDie Studierenden können einfache Auswahl- und Filterregeln auf Datensätze in R anwenden.\nDie Studierenden können erste einfache R Code Blöcke miteinander in einen Kontext setzen.\n\nD.5.2.1.3 Wissensverständnis\nStatistischer Anteil\n\nDie Studierenden können einen statistischen Test mit einer explorativen Datenanalyse in einen Kontext bringen.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage eine einfache statistische Auswertung mit den notwendigen Funktionen und Paketen in R zu skizzieren.\n\nD.5.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nD.5.2.2.1 Nutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in den Agrarwirtschaften, Lebensmittelwissenschaften und Gartenbau. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen. Die Studierenden sind in der Lage die grundlegenden Konzepte der Programmierung in R in anderen Programmiersprachen zuerkennen.\n\nD.5.2.2.2 Wissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\nD.5.2.3 Kommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren.\n\nD.5.2.4 Wissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.5.3 Literatur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\n\nD.5.4 Voraussetzungen für die Teilnahme\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.5.5 Empfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.5.6 Zusammenhang mit anderen Modulen\nDas Modul “Statistik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nMarketing Praxis (44B0547)\nWeb Engineering (44B0585)\nWirtschaftsinformatik (44B0577)\nWissenschaftliches Arbeiten und Kommunikation (44B0573)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\nD.5.7 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nD.5.7.1 Benotete Prüfungsleistung\nKlausur"
  },
  {
    "objectID": "module.html#sec-module-bioverfahren",
    "href": "module.html#sec-module-bioverfahren",
    "title": "Anhang D — Modulbeschreibung",
    "section": "\nD.6 Angewandte Statistik für Bioverfahrenstechnik",
    "text": "D.6 Angewandte Statistik für Bioverfahrenstechnik\n\nD.6.1 Inhalte und Qualifikationsziele des Moduls\n\nD.6.1.1 Kurzbeschreibung\nIn der Agrarwirtschaft und Ingenieurwissenschaften werden vielen Prozesse und Phänomene durch statistische Modelle beschrieben. Verfahrenstechnik, Qualitätssicherung und Marketing sind wesentlich getragen durch eine statistische Analyse von Daten. In dem Modul “Angewandte Statistik für Bioverfahrenstechnik” lernen Studierende die Grundlagen für das spätere wissenschaftliche und angewandte Arbeiten. Das Modul vermittelt die dafür notwendigen statistischen Grundkenntnisse. Der Fokus liegt hier auf der Darstellung, Erfahrung und ersten statistischen Auswertungen von Daten. Wissenschaftliche Forschung und Erkenntnisgewinn wird hierbei in den Kontext der Erhebung von Daten gesetzt. Die für die Agrarwirtschaft und Ingenieurwissenschaften relevanten statistischen Verfahren und Modellierungen werden dargestellt und diskutiert. Eine auf Daten gestützte Risikoabschätzung von Entscheidungen wird eingeübt. Parallel dazu werden in dem Modul die erworbenen theoretischen, statistischen Kenntnisse durch die Einführung in die Programmierung in R für die Studierenden umsetzbar und erfahrbar gemacht. In dem Modul “Angewandte Statistik für Bioverfahrenstechnik” werden somit die ersten Grundkenntnisse für die praktische Anwendung der Bio Data Science erworben.\n\nD.6.1.2 Lehr-Lerninhalte\nStatistischer Anteil\n\nEinführung in die explorative Datenanalyse und deren statistischen Maßzahlen.\nEinführung in statistische Verteilungen anhand der Poisson- und Normalverteilung.\nEinführung in das statistische Testen sowie der Testtheorie mit dem Prüfen von statistischen Hypothesen beinhaltend p-Wert und die 95% Konfidenzintervalle.\nBerechnung des Student-, Welch- und gepaarten t-Test.\nEinführung in die Varianzanalyse beinhaltend die einfaktorielle sowie zweifaktorielle ANOVA.\nGrundlagen des nicht-parametrischen Tests beinhaltend Wilcoxon-Mann-Whitney-Test sowie Kruskal-Wallis-Test.\nGrundlagen der simplen linearen Regression und der multiplen linearen Regression sowie deren statistischen Maßzahlen der Modellgüte am Beispiel eines normalverteilten Endpunkts.\nDiagnostischen Testen und deren statistischen Maßzahlen.\nChi-Quadrat-Test für eine Vierfeldertafel.\nEinführung in das multiple Testen von mehreren Mittelwerten und die Darstellung im compact letter display.\n\nInformatorischer Anteil\n\nEinführung in die Grundlagen der Programmierung in R anhand von Skalenarten.\nEinführung in die Darstellung von Daten in R und die Vorstellung des Konzepts der “tidy data”.\nKonzept von Objekten, Funktionen sowie Pipen und der Vorstellung des tidyverse in R.\nEinlesen von Daten und deren Bearbeitung sowie Visualisierung in R.\nDurchführung der gängigen statistischen Tests und die Interpretierung der jeweiligen R Ausgaben.\n\nD.6.2 Kompetenzorientierte Lernergebnisse\n\nD.6.2.1 Wissen und Verstehen\n\nD.6.2.1.1 Wissensverbreiterung\nStatistischer Anteil\n\nDie Studierenden sind in der Lage verschiedene Daten und Datenstrukturen zu erkennen und zu benennen.\nDie Studierenden können explorative Abbildungen erkennen und benennen.\nDie Studierenden können verschiedene statistische Tests händisch durchführen.\n\nInformatorischer Anteil\n\nDie Studierenden können die Anforderungen an einen Datensatz zur Verwendung in R benennen.\nDie Studierenden können in R Objekte, Funktionen und Zahlenvektoren unterscheiden und kennen die gängigen Operatoren in R.\nDie Studierenden können den Ablauf für die Erstellung einer explorativen Datenanalyse in R beschreiben.\nDie Studierenden sind in der Lage aus englischen Internetquellen eine Lösung für ein R Problem einzugrenzen.\n\nD.6.2.1.2 Wissensvertiefung\nStatistischer Anteil\n\nDie Studierenden können explorative Abbildungen erstellen und interpretieren.\nDie Studierenden können aus explorative Abbildungen die entsprechende Datenstruktur zur Erstellung der Abbildungen wiedergeben.\nDie Studierenden sind in der Lage anhand eines statistisches Tests eine Entscheidung zu treffen.\nDie Studierenden können das Ergebnis eines statistischen Test im Kontext der wissenschaftlichen Fragestellung interpretieren.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage experimentelle Daten nach dem Konzept der der “tidy data” zu erheben.\nDie Studierenden sind in der Lage die Ausgabe eines statistischen Test in R zu interpretieren.\nDie Studierenden können einfache Auswahl- und Filterregeln auf Datensätze in R anwenden.\nDie Studierenden können erste einfache R Code Blöcke miteinander in einen Kontext setzen.\n\nD.6.2.1.3 Wissensverständnis\nStatistischer Anteil\n\nDie Studierenden können einen statistischen Test mit einer explorativen Datenanalyse in einen Kontext bringen.\n\nInformatorischer Anteil\n\nDie Studierenden sind in der Lage eine einfache statistische Auswertung mit den notwendigen Funktionen und Paketen in R zu skizzieren.\n\nD.6.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nD.6.2.2.1 Nutzung und Transfer\nDie Studierenden sind in der Lage einfache lineare Kosten- und Nutzenabschätzungen anhand von statistischen Modellen durchzuführen. Diese einfachen Abschätzungen umfassen die Planung von marktwirtschaftlichen, technischen und biologischen Prozesses in der Agrarwirtschaft und Ingenieurwissenschaften. Die Studierenden können dabei externe Literaturquellen und deren statistischen Maßzahlen in den Kontext des eigenen Berufsfeld setzen und aus verschiedenen, wissenschaftlichen Quellen erste informierte Vorentscheidungen treffen. Die Studierenden sind in der Lage die grundlegenden Konzepte der Programmierung in R in anderen Programmiersprachen zuerkennen.\n\nD.6.2.2.2 Wissenschaftliche Innovation\nDie Studierende können statistische Maßzahlen aus wissenschaftlichen Publikationen in andere wissenschaftliche Kontexte einordnen. Die Studierenden kennen die Grundlagen des wissenschaftlichen Arbeitens anhand von Fallbeispielen. Die Studierenden können explorative Abbildungen aus Veröffentlichungen verstehen und erste informierte Forschungsideen entwickeln. Die Studierenden sind in der Lage bei der Erstellung von Daten aus Experimenten die Verwertbarkeit in R zu berücksichtigen.\n\nD.6.2.3 Kommunikation und Kooperation\nDie Studierenden sind in der Lage durch das Konzept von “tidy data” erhobene Daten mit anderen Forschenden zu teilen. Ebenfalls sind die Studierenden in der Lage gängige statistische Maßzahlen zu erkennen und zu berichten. Die Studierenden können R Code lesen, erstellen und demonstrieren.\n\nD.6.2.4 Wissenschaftliches Selbstverständnis / Professionalität\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.6.3 Literatur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. https://r4ds.had.co.nz\n\nD.6.4 Voraussetzungen für die Teilnahme\nInhalte des Moduls Mathematik für Bioverfahrenstechnik (44B0609)\n\nD.6.5 Empfohlene Vorkenntnisse\nKeine. Es handelt sich um ein Grundlagenmodul.\n\nD.6.6 Zusammenhang mit anderen Modulen\nDas Modul “Angewandte Statistik für Bioverfahrenstechnik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nProduktionssystem Pflanze (44B0261)\nEinführung in die Pflanzenzüchtung (44B0112)\nMessen, Regeln und Auswerten in der Biosystemtechnik (44B0549)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\nD.6.7 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nD.6.7.1 Benotete Prüfungsleistung\nKlausur"
  },
  {
    "objectID": "module.html#sec-module-biostatistik",
    "href": "module.html#sec-module-biostatistik",
    "title": "Anhang D — Modulbeschreibung",
    "section": "\nD.7 Biostatistik",
    "text": "D.7 Biostatistik\n\nD.7.1 Inhalte und Qualifikationsziele des Moduls\n\nD.7.1.1 Kurzbeschreibung\nIm Masterstudiengang Nutztier- und Pflanzenwissenschaften wird angewandte Forschungs- und Entwicklungskompetenz in den Forschungsfeldern der Agrarwissenschaften vermittelt. Studierende lernen, wie sie aus Daten, die sich aus technischen Prozessen und wissenschaftlichen Experimenten ergeben, zuverlässige und objektive Entscheidungen treffen können. Dazu benötigen die Studierenden vertiefte und umfangreiche Kenntnisse über angewandte statistische Methoden. Das Modul “Biostatistik” vermittelt die notwendigen wissenschaftlichen und angewandten statistischen Modelle, um später wissenschaftlich und angewandt arbeiten zu können. Das wissenschaftliche Arbeiten, Strategien in der Forschung und ihre Beziehungen zu statistischen Methoden werden an Fallbeispielen eingeübt. Dabei werden verschiedene statistische Verfahren vorgestellt und die statistischen Maßzahlen für die Modellselektion diskutiert. Der Schwerpunkt des Moduls liegt auf dem forschenden Arbeiten und der wissenschaftlichen Diskussion der Ergebnisse der statistischen Modellierungen. Studierende werden außerdem dazu angeleitet, bereits vorhandene Programmierkenntnisse in R weiter zu vertiefen und somit das Arbeiten mit Daten umsetzbar und erfahrbar zu machen. Das Modul “Biostatistik” befähigt Studierende dazu, in verschiedenen Anwendungsfeldern der agrarwissenschaftlichen Forschung und Praxis forschend tätig zu sein und Daten auszuwerten.\n\nD.7.1.2 Lehr-Lerninhalte\nAllgemeine Statistik\n\nVertiefung der gängigen multiplen linearen Regressionen und deren Verteilungsfamilien beinhaltend die Gaussian, Poisson, Multinominal/Ordinal und Binomial.\nVertiefung der statistischen Maßzahlen der Modellgüte einer multiplen linearen Regression sowie deren Effektschätzer.\nMethoden der Variablenselektion und Imputation von fehlenden Werten sowie Ausreißerdetektion.\nLineare gemischte Modelle und die Berücksichtigung von Messwiederholungen in der praktischen Anwendung.\nVertiefte Auseinandersetzung mit multiplen Gruppenvergleichen und deren Möglichkeiten der Visualisierung von Gruppenunterschieden.\nEinführung in die Äquivalenz oder Nichtunterlegenheit in der praktischen Anwendung.\nDie klassischen experimentellen Designs in den Agrarwissenschaften und deren Auswertung an Fallbeispielen.\nGrundlagen des maschinellen Lernens und der Klassifikation von Ereignissen sowie Maßzahlen der Bewertung eines maschinellen Lernalgorithmus.\nDurchführung aller theoretisch erarbeiteten Inhalte in R.\nInterpretation und Bewertung von statistischen Modellierungen in R.\n\nNutztierwissenschaften und Pflanzenwissenschaften\n\nSpezifische Strategien und statistische Methoden in der Forschung und ihre Beziehungen zu angewandten statistischen Methoden in der jeweiligen Fachrichtung.\n\nD.7.2 Kompetenzorientierte Lernergebnisse\n\nD.7.2.1 Wissen und Verstehen\n\nD.7.2.1.1 Wissensverbreiterung\n\nDie Studierenden kennen die gängigen experimentellen Designs in den Agrarwissenschaften.\nDie Studierenden kennen die entsprechenden Repräsentationen der experimentellen Designs als Datensatz.\nDie Studierenden können die gängigen statistischen Modellierungen benennen und unterscheiden.\nDie Studierenden können das Ergebnis eines statistischen Modells im Kontext der wissenschaftlichen Fragestellung interpretieren.\nDie Studierenden sind in der Lage zwischen einem kausalen und einem prädiktiven Modell zu unterscheiden.\n\nD.7.2.1.2 Wissensvertiefung\n\nDie Studierenden sind in der Lage anhand einer wissenschaftlichen Fragestellung eine statistische Auswertung zu gliedern und zu planen.\nDie Studierenden können wissenschaftliche Veröffentlichungen lesen und in den statistischen Kontext richtig einordnen.\nDie Studierenden können eine multiple lineare Regression entsprechend des Endpunktes modellieren und interpretieren.\nDie Studierenden können einen multiplen Gruppenvergleich für verschiedene Endpunkte rechnen und die p-Werte entsprechend adjustieren.\nDie Studierenden können verschiedene technische Messparameter miteinander vergleichen und eine Aussage über die Nichtunterlegenheit treffen.\n\nD.7.2.1.3 Wissensverständnis\n\nDie Studierenden sind die der Lage eine wissenschaftliche Fragestellung mit einem experimentellen Design und einer statistischen Modellierung zu verbinden.\nDie Studierenden können eine statistische Modellierung in einer Präsentation darstellen und vorstellen.\nDie Studierenden können eine wissenschaftliche Veröffentlichung anhand der verwendeten Statistik bewerten.\n\nD.7.2.2 Einsatz, Anwendung und Erzeugung von Wissen\n\nD.7.2.2.1 Nutzung und Transfer\nDie Studierenden können durch statistische Modelle Kosten- und Nutzenabschätzungen durchführen, um marktwirtschaftliche, technische und biologische Prozesse in den Forschungsfeldern der Agrarwissenschaften zu planen. Sie sind in der Lage, externe Literaturquellen zu nutzen und statistische Maßzahlen in den Kontext ihres Berufsfeldes zu setzen, um fundierte Entscheidungen zu treffen. Außerdem können die Studierenden grundlegende Konzepte der Programmierung in R erkennen und eine Datenauswertung durchführen.\n\nD.7.2.2.2 Wissenschaftliche Innovation\nDie Studierenden verfügen über die Fähigkeit, statistische Maßzahlen aus wissenschaftlichen Publikationen in verschiedenen wissenschaftlichen Kontexten zu interpretieren und anzuwenden. Sie besitzen grundlegende Fähigkeiten des wissenschaftlichen Arbeitens, die es ihnen ermöglichen, praktische Fragestellungen in wissenschaftliche Erkenntnisprozesse zu übersetzen. Darüber hinaus können sie statistische Auswertungen aus wissenschaftlichen Publikationen verstehen und aufbauend darauf informierte Forschungsideen entwickeln.\n\nD.7.2.3 Kommunikation und Kooperation\nDie oben genannten Fähigkeiten der Studierenden ermöglichen es ihnen, in multidisziplinären wissenschaftlichen Teams mitzuwirken. Sie können eine gemeinsam geplante Forschungsskizze von Drittmittelprojekten oder in Kooperation mit Wirtschaftspartnern und Forschungsanstalten umsetzen. Darüber hinaus sind die Studierenden in der Lage, die Ergebnisse einer statistischen Analyse auch Fachfremden verständlich zu erläutern.\n\nD.7.2.4 Wissenschaftliches Selbstverständnis / Professionalität\nDie Studierenden haben ein umfangreiches Verständnis für wissenschaftliche Publikationen und statistische Maßzahlen und können dieses Wissen auf ihr Berufsfeld anwenden. Dadurch sind sie in der Lage, komplexe naturwissenschaftliche und technische Fragestellungen aus den Forschungsfeldern der Agrarwissenschaften eigenständig und fundiert mit statistischen Methoden zu bearbeiten, was sie für anspruchsvolle Aufgaben in der agrarwissenschaftlichen Forschung und Praxis qualifiziert.\n\nD.7.3 Literatur\n\nDas Skript des Statistik- und Programmierteil des Moduls unter https://jkruppa.github.io/\nTeile des Skripts als Video unter https://www.youtube.com/c/JochenKruppa\nDormann, Carsten F. Parametrische Statistik. Springer Berlin Heidelberg, 2013.\nWickham, Hadley, and Garrett Grolemund. R for data science: import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016. [https://r4ds.had.co.nz/]\nData Science for Agriculture in R unter https://schmidtpaul.github.io/DSFAIR/\nBruce, Peter, Andrew Bruce, und Peter Gedeck. 2020. Practical statistics for data scientists: 50+ essential concepts using R and Python. O’Reilly Media.\n\nD.7.4 Empfohlene Vorkenntnisse\nFür dieses Modul werden vertiefte Kenntnisse der deskriptiven Statistik sowie Grundkenntnisse der Statistik vorausgesetzt, wie sie in den Modulen “Mathematik und Statistik (44B0266)” und “Angewandte Statistik und Versuchswesen (44B0400)” vermittelt werden. Zudem werden Grundregeln und -methoden des wissenschaftlichen Arbeitens aus dem Modul “Wissenschaftliches Arbeiten (44M0159)” als bekannt vorausgesetzt.\nStudierenden, die ihre Kenntnisse und Fertigkeiten vor Beginn des Moduls auffrischen möchten, wird folgende Grundlagenliteratur mit dem “Skript Bio Data Science” unter https://jkruppa.github.io/ empfohlen.\nIn dem Modul wird mit der Software R gearbeitet. Um sich im Vorfeld mit den Basisfunktionen vertraut zu machen, eignen sich beispielsweise die folgenden Video-Tutorials unter https://www.youtube.com/c/JochenKruppa.\n\nD.7.5 Zusammenhang mit anderen Modulen\nDas Modul “Biostatistik” bereitet zudem auf weiterführende Module aus verschiedenen Themenbereichen vor. Zu diesen Themenbereichen gehören insbesondere\n\nWissenschaftliche Publikation und Fachtagungen (44M0125)\nForschungs- und Entwicklungsprojekt (44M0043)\nMasterarbeit (44M0267)\n\nWelche nachfolgenden Module konkret in Frage kommen, hängt von den einzelnen Studiengängen ab. Nähere Informationen hierzu bietet der Studienverlaufsplan in der jeweils gültigen Studienordnung.\n\nD.7.6 Voraussetzungen für die Vergabe von ECTS-Leistungspunkten\n\nD.7.6.1 Benotete Prüfungsleistung\nKlausur\n\nD.7.6.2 Unbenotete Prüfungsleistung\nPräsentation"
  },
  {
    "objectID": "stat-tests-pretest.html#testen-der-normalverteilungsannahme-in-mehreren-gruppen",
    "href": "stat-tests-pretest.html#testen-der-normalverteilungsannahme-in-mehreren-gruppen",
    "title": "30  Pre-Tests oder Vortest",
    "section": "\n30.8 Testen der Normalverteilungsannahme in mehreren Gruppen",
    "text": "30.8 Testen der Normalverteilungsannahme in mehreren Gruppen\nIm folgenden Beispiel zu den Keimungsraten von verschiedenen Nelkensorten wollen wir einmal testen, ob jede Sorte einer Normalverteilung folgt. Da wir hier insgesamt 20 Sorten vorliegen haben, nutzen wir die Funktion map() aus dem R Paket purrr um hier schneller voranzukommen. Wie immer laden wir erst die Daten und mutieren die Spalten in dem Sinne wie wir die Spalten brauchen.\n\nclove_tbl &lt;- read_excel(\"data/clove_germ_rate.xlsx\") %&gt;% \n  mutate(clove_strain = as_factor(clove_strain),\n         germ_rate = as.numeric(germ_rate))\n\nJetzt können wir die Daten nach der Sorte der Nelken in eine Liste mit der Funktion split() aufspalten und dann auf jedem Listeneintrag einen Shapiro-Wilk-Test rechnen. Dann machen wir uns noch die Ausgabe schöner und erschaffen uns eine decision-Spalte in der wir gleich das Ergebnis für oder gegen die Normalverteilung ablesen können.\n\nclove_tbl %&gt;% \n  split(.$clove_strain) %&gt;% \n  map(~shapiro.test(.x$germ_rate)) %&gt;% \n  map(tidy) %&gt;% \n  bind_rows(.id = \"test\") %&gt;%\n  select(test, p.value) %&gt;% \n  mutate(decision = ifelse(p.value &lt;= 0.05, \"reject normal\", \"normal\"),\n         p.value = pvalue(p.value, accuracy = 0.001))\n\n# A tibble: 20 × 3\n   test          p.value decision     \n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;        \n 1 standard      0.272   normal       \n 2 west_rck_1    0.272   normal       \n 3 south_III_V   0.855   normal       \n 4 west_rck_2_II 0.653   normal       \n 5 comb_001      0.103   normal       \n 6 western_4     0.849   normal       \n 7 north_549     0.855   normal       \n 8 subtype_09    0.983   normal       \n 9 subtype_III_4 0.051   normal       \n10 ctrl_pos      0.992   normal       \n11 ctrl_7        0.683   normal       \n12 trans_09_I    0.001   reject normal\n13 new_xray_9    0.406   normal       \n14 old_09        0.001   reject normal\n15 recon_1       0.100   normal       \n16 recon_3456    0.001   reject normal\n17 east_new      0.907   normal       \n18 east_old      0.161   normal       \n19 south_II_U    0.048   reject normal\n20 west_3_cvl    0.272   normal"
  },
  {
    "objectID": "programing-import.html#genutzte-r-pakete",
    "href": "programing-import.html#genutzte-r-pakete",
    "title": "11  Daten einlesen",
    "section": "\n11.1 Genutzte R Pakete",
    "text": "11.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, janitor)\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "stat-tests-effect.html#wirkungsgrad-von-pflanzenschutzmitteln",
    "href": "stat-tests-effect.html#wirkungsgrad-von-pflanzenschutzmitteln",
    "title": "21  Der Effektschätzer",
    "section": "\n21.4 Wirkungsgrad von Pflanzenschutzmitteln",
    "text": "21.4 Wirkungsgrad von Pflanzenschutzmitteln\nNeben den klassischen Effektmaßzahlen, die sich aus einem Mittelwert oder einem Anteil direkt berechnen, gibt es noch andere Effektmaße. Einer dieser Effektmaße ist der Wirkungsgrad für zum Beispiel ein Pflanzenschutzmittel. Wir können hier aber auch weiter denken und uns überlegen in wie weit wir eine Population von Schaderregern durch eine Behandlung reduzieren können. Unabdingbar ist in diesem Fall eine positive Kontrolle in der nichts gemacht wird sondern nur der normale Befall gemessen wird. Wir berechnen hier den Wirkungsgrad nach Abbott u. a. (1925) mit der Anpassung von Finner, Kunert, und Sonnemann (1989). Der Wirkungsgrad \\(WG\\) eines Schutzmittels im Vergleich zur Kontrolle berechnet sich wie folgt.\n\\[\nWG = \\left(\\cfrac{X_n - Y_n}{X_n}\\right)\n\\]\nmit\n\n\n\\(Y_n\\) Anzahl lebend in der Behandlung\n\n\\(X_n\\) Anzahl lebend in der Kontrolle\n\nNatürlich ist die Formel wieder sehr abstrakt, deshalb haben wir zwei Beispieldaten. Zuerst schauen wir uns einen Datensatz zu dem Befall mit Trespe an. Wir haben also Parzellen in denen sich die Trespe ausbreitet und haben verschiedene Behandlungen durchgeführt. Wichtig hierbei, wir haben auch Parzellen wo wir nichts gemacht haben, das ist dann unsere positive Kontrolle (ctrl). Da unsere Daten nicht im Long-Format vorliegen müssen wir die Daten erst noch anpassen und dann die Spalte block in einen Faktor umwandeln.\n\n\n\n\nTabelle 21.4— Trespebefall in einem randomisierten Blockdesign mit vier Behandlungsvarianten.\n\nvariante\nblock_1\nblock_2\nblock_3\nblock_4\n\n\n\ncrtl\n16.0\n16.0\n4.8\n11.2\n\n\n2\n4.8\n4.0\n0.0\n2.4\n\n\n3\n10.4\n2.4\n0.0\n0.0\n\n\n4\n8.0\n8.8\n7.2\n4.0\n\n\n\n\n\n\n\ntrespe_tbl &lt;- read_excel(\"data/raubmilben_data.xlsx\", sheet = \"trespe\") %&gt;% \n  pivot_longer(block_1:block_4,\n               names_to = \"block\",\n               values_to = \"count\") %&gt;% \n  mutate(block = factor(block, labels = 1:4),\n         variante = as_factor(variante))\n\nIn der Abbildung 21.1 sehen wir nochmal die orginalen, untransformierten Daten sowie die log-transformierten Daten.\n\nggplot(trespe_tbl, aes(variante, count, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Behandlungsvariante\", y = \"Anzahl\", fill = \"Block\") +\n  scale_fill_okabeito()\n  \nggplot(trespe_tbl, aes(variante, log1p(count), fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Behandlungsvariante\", y = \"log(Anzahl)\", fill = \"Block\") +\n  scale_fill_okabeito()\n\n\n\n\n\n(a) Verteilung der Werte auf der originalen Skala.\n\n\n\n\n\n(b) Verteilung der Werte auf der logarithmischen Skala. Beobachtungen mit einer 0 Zählung wurden auf 1 gesetzt.\n\n\n\nAbbildung 21.1— Dotplot der Anzahl an Trespen je Sorte und Block.\n\n\n\n\nfit &lt;- glm(count ~ variante + block, data = trespe_tbl, family = poisson)\n\n\nfit %&gt;% \n  emmeans(~variante, type = \"response\") %&gt;% \n  tidy() %&gt;% \n  mutate(WG_abbott = percent((rate[1] - rate)/rate[1])) %&gt;% \n  select(variante, rate, WG_abbott)\n\n# A tibble: 4 × 3\n  variante  rate WG_abbott\n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    \n1 crtl     10.8  0.0%     \n2 2         2.52 76.7%    \n3 3         2.89 73.3%    \n4 4         6.31 41.7%    \n\n\nIm zweiten Beispiel wollen wir uns mit dem geometrischen Mittel \\(WG_{geometric}\\) als Schätzer für den Wirkungsgrad beschäftigen. Hier kochen wir dann einmal die Veröffentlichung von Finner, Kunert, und Sonnemann (1989) nach. Dafür brauchen wir einmal die Daten zu den Raubmilben, die ich schon als Exceldatei aufbereitet habe. Wie immer sind die Rohdaten im Wide-Format, wir müssen aber im Long-Format rechnen. Da bauen wir uns also einmal schnell die Daten um. Dann wollen wir noch die Anzahlen der Raubmilben logarithmieren, so dass wir jede Anzahl um 1 erhöhen um logarithmierte Nullen zu vermeiden. Das ganze machen wir dann in einem Rutsch mit der Funktion log1p().\n\n\n\n\nTabelle 21.5— Raubmilbenbefall auf acht Sorten und einer Kontrolle.\n\nsorte\nblock_1\nblock_2\nblock_3\nblock_4\nblock_5\n\n\n\n1\n0\n2\n2\n21\n0\n\n\n2\n302\n108\n64\n23\n49\n\n\n3\n59\n51\n59\n1\n26\n\n\n4\n64\n154\n41\n27\n41\n\n\n5\n45\n141\n51\n70\n37\n\n\n6\n58\n240\n140\n27\n11\n\n\n7\n1\n2\n3\n8\n16\n\n\n8\n4\n1\n0\n0\n1\n\n\nctrl\n46\n32\n62\n90\n20\n\n\n\n\n\n\n\nmite_tbl &lt;- read_excel(\"data/raubmilben_data.xlsx\", sheet = \"mite\") %&gt;% \n  pivot_longer(block_1:block_5,\n               names_to = \"block\",\n               values_to = \"count\") %&gt;% \n  mutate(block = factor(block, labels = 1:5),\n         sorte = as_factor(sorte),\n         log_count = log1p(count))\n\nSchauen wir uns einmal die Daten in der Abbildung 21.2 an.\n\nggplot(mite_tbl, aes(sorte, count, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Sorten\", y = \"Anzahl\", fill = \"Block\") +\n  scale_fill_okabeito()\n  \nggplot(mite_tbl, aes(sorte, log1p(count), fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Sorten\", y = \"log(Anzahl)\", fill = \"Block\") +\n  scale_fill_okabeito()\n\n\n\n\n\n(a) Verteilung der Werte auf der originalen Skala.\n\n\n\n\n\n(b) Verteilung der Werte auf der logarithmischen Skala. Beobachtungen mit einer 0 Zählung wurden auf 1 gesetzt.\n\n\n\nAbbildung 21.2— Dotplot der Anzahl an Raubmilden je Sorte und Block.\n\n\n\nWir brauchen jetzt eine Helferfunktion, die uns aus \\(Pr\\) die Gegenwahrscheinlichkeit \\(1 - Pr\\) berechnet. Auch wollen wir dann die Prozentangabe der Gegenwahrscheinlichkeit, also die Gegenwahrscheinlichkeit \\(1 - Pr\\) multipliziert mit Einhundert. Dann brauchen wir als Variable noch die Gruppengröße \\(n_g\\), die bei uns ja bei 5 liegt. Wir haben pro Sorte fünf Beobachtungen je Block.\n\nget_q &lt;- function(x){100 * (1 - x)}\nn_group &lt;- 5\n\nGeometrisches Mittel\n\\[\n\\Delta_{geometric} = \\left(\\cfrac{\\prod_{i=1}^n y_j}{\\prod_{i=1}^n y_{ctrl}}\\right)^{1/n_j} \\mbox{ für Sorte } j\n\\]\n\nresidual_tbl &lt;- lm(log_count ~ sorte + block, data = mite_tbl) %&gt;% \n  glance() %&gt;% \n  select(df.residual, sigma)\nresidual_tbl\n\n# A tibble: 1 × 2\n  df.residual sigma\n        &lt;int&gt; &lt;dbl&gt;\n1          32 0.962\n\n\n\nt_quantile &lt;- qt(p = 0.05, df = residual_tbl$df.residual, lower.tail = FALSE)\nt_quantile\n\n[1] 1.693889\n\n\n\nmite_tbl %&gt;% \n  mutate(count = count + 1) %&gt;% \n  group_by(sorte) %&gt;% \n  summarise(prod = prod(count)) %&gt;% \n  mutate(tau = (prod/prod[9])^(1/n_group),\n         WG_geometric = get_q(tau),\n         a = sqrt(2/n_group) * residual_tbl$sigma * t_quantile + log(tau),\n         KI = get_q(exp(a)))\n\n# A tibble: 9 × 6\n  sorte       prod    tau WG_geometric      a     KI\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1            198 0.0638         93.6 -1.72    82.1\n2 2     2576106000 1.69          -69.0  1.56  -374. \n3 3       10108800 0.558          44.2  0.447  -56.4\n4 4      497624400 1.22          -21.7  1.23  -241. \n5 5      916413472 1.37          -37.5  1.35  -285. \n6 6      673639344 1.29          -29.3  1.29  -262. \n7 7           3672 0.114          88.6 -1.14    67.9\n8 8             20 0.0404         96.0 -2.18    88.7\n9 ctrl   186729543 1               0    1.03  -180. \n\n\n\\[\n[-\\infty; a]\n\\]\n\\[\na = \\sqrt{2/n} \\cdot s \\cdot t_{(t-1)(n-1),\\,\\alpha} + \\ln(1 - GM)\n\\]\n\nfit &lt;- glm(count ~ sorte + block, data = mite_tbl, family = poisson)\n\n\nfit %&gt;% \n  emmeans(~sorte, type = \"response\") %&gt;% \n  tidy() %&gt;% \n  mutate(WG_abbott = percent((rate[9] - rate)/rate[9])) \n\n# A tibble: 9 × 8\n  sorte  rate std.error    df  null statistic  p.value WG_abbott\n  &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    \n1 1      4.49     0.898   Inf     1     7.49  6.68e-14 90.0%    \n2 2     98.0      4.32    Inf     1   104.    0        -118.4%  \n3 3     35.2      2.54    Inf     1    49.3   0        21.6%    \n4 4     58.7      3.30    Inf     1    72.3   0        -30.8%   \n5 5     61.7      3.39    Inf     1    75.0   0        -37.6%   \n6 6     85.4      4.02    Inf     1    94.5   0        -90.4%   \n7 7      5.38     0.984   Inf     1     9.20  3.45e-20 88.0%    \n8 8      1.08     0.440   Inf     1     0.181 8.57e- 1 97.6%    \n9 ctrl  44.9      2.88    Inf     1    59.3   0        0.0%     \n\nfit %&gt;% \n  emmeans(~sorte, type = \"link\") \n\n sorte emmean     SE  df asymp.LCL asymp.UCL\n 1     1.5009 0.2003 Inf     1.108     1.893\n 2     4.5847 0.0441 Inf     4.498     4.671\n 3     3.5602 0.0722 Inf     3.419     3.702\n 4     4.0720 0.0563 Inf     3.962     4.182\n 5     4.1227 0.0550 Inf     4.015     4.230\n 6     4.4475 0.0471 Inf     4.355     4.540\n 7     1.6832 0.1829 Inf     1.325     2.042\n 8     0.0738 0.4084 Inf    -0.727     0.874\n ctrl  3.8035 0.0641 Inf     3.678     3.929\n\nResults are averaged over the levels of: block \nResults are given on the log (not the response) scale. \nConfidence level used: 0.95"
  },
  {
    "objectID": "stat-tests-effect.html#genutzte-r-pakete",
    "href": "stat-tests-effect.html#genutzte-r-pakete",
    "title": "21  Der Effektschätzer",
    "section": "\n21.1 Genutzte R Pakete",
    "text": "21.1 Genutzte R Pakete\nWir wollen folgende R Pakete in diesem Kapitel nutzen.\n\npacman::p_load(tidyverse, magrittr, see, scales,\n               effectsize, parameters, broom,\n               emmeans, conflicted)\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\n\nAm Ende des Kapitels findest du nochmal den gesamten R Code in einem Rutsch zum selber durchführen oder aber kopieren."
  },
  {
    "objectID": "eda-descriptive.html#geometrisches-mittel",
    "href": "eda-descriptive.html#geometrisches-mittel",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.2 Geometrisches Mittel",
    "text": "15.2 Geometrisches Mittel\nIm Gegensatz zum arithmetischen Mittel ist das geometrische Mittel nur für nichtnegative Zahlen \\(y\\) definiert und meistens nur für echt positive Zahlen sinnvoll, denn wenn eine Zahl gleich Null ist, ist schon das ganze Produkt der Zahlen gleich Null.\n\\[\n\\bar{x}_\\mathrm{geom} = \\sqrt[n]{\\prod_{i=1}^n{x_i}} = \\sqrt[n]{x_1 \\cdot x_2 \\dotsm x_n} = (x_1 \\cdot x_2 \\dotsm x_n)^{1/n}\n\\]\n\\[\n\\bar{x}_\\mathrm{geom} =  \\sqrt[4]{4 \\cdot 3 \\cdot 6 \\cdot 9} =\\sqrt[4]{648} = 5.045\n\\]\nBei der geometrischen Mittelwertbildung aus zum Beispiel zwei Werten weichen beide Werte vom Mittelwert um denselben Multiplikator ab. Diese Eigenschaft ist beim arithmetischen Mittel nicht gegeben. So ergibt sich aus den Zahlen 1 und 9 das arithmetische Mittel 5. Dabei ist die 1 vom Mittelwert 5 um Faktor 5 entfernt, während die 9 lediglich um Faktor 1,8 davon entfernt liegt. Das geometrische Mittel aus 1 und 9 hingegen ergibt den Mittelwert 3. Sowohl der niedrige Wert 1 wie auch der hohe Wert 9 sind vom Mittelwert 3 um Faktor 3 entfernt. Der Unterschied zwischen arithmetischem und geometrischem Mittelwert kann beträchtlich sein, was in der Praxis unter Umständen zur Fehlinterpretation von Durchschnittsangaben führt.\nIn R können wir das geometrische Mittel einfach mit der Funktion geometric.mean() aus dem R Paket psych berechnen.\n\nc(4, 3, 6, 9) %&gt;% \n  psych::geometric.mean()\n\n[1] 5.045378\n\n\nDas geometrische Mittel ist der einzige korrekte Mittelwert, wenn normalisierte Ergebnisse gemittelt werden, d. h. Ergebnisse, die als Verhältnis zu Referenzwerten dargestellt werden."
  },
  {
    "objectID": "eda-descriptive.html#variationskoeffizient",
    "href": "eda-descriptive.html#variationskoeffizient",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.11 Variationskoeffizient",
    "text": "15.11 Variationskoeffizient\nIm Gegensatz zu der Varianz ist der Variationskoeffizient ein relatives Streuungsmaß, das heißt, der Variationskoeffizient hängt nicht von der Maßeinheit von \\(y\\) ab. Die Motivation für den Variationskoeffizient ist, dass ein \\(y\\) mit großem Mittelwert häufig eine größere Varianz aufweist als eine mit einem kleinen Mittelwert. Da die Varianz und die daraus abgeleitete Standardabweichung nicht normiert sind, kann ohne Kenntnis des Mittelwerts nicht beurteilt werden, ob eine Varianz groß oder klein ist. Der Variationskoeffizient ist eine Normierung der Varianz. Ist die Standardabweichung somit größer als der Mittelwert, so ist der Variationskoeffizient größer 1. Der Variationskoeffizient wird häufig verwendet, um die Variation zwischen zwei verschiedenen Datensätzen zu vergleichen.\nWir können den Variationskoeffizient basierend auf dem Mittelwert und der Standardabweichung berechnen.\n\\[\nv = \\cfrac{s}{\\bar{y}}\n\\] In unserem Beispiel würden wir als die Standardabweichung \\(s\\) und den Mittelwert \\(\\bar{y}\\) in die Formel einsetzen.\n\\[\nv = \\cfrac{2.14cm}{8.13cm} = 0.26\n\\] Wir können den Variationskoeffizient auch basierend auf dem Median berechnen.\n\\[\nv_r=\\cfrac{x_{0.75}-x_{0.25}}{\\tilde{x}_{0.5}}\n\\] In unserem Fall müssen wir hier dann die Quartile und den Median in die Formel einsetzen.\n\\[\nv_r=\\cfrac{9.1 - 5.7}{8.2} = 0.41\n\\]"
  },
  {
    "objectID": "eda-descriptive.html#arithmetisches-mittel-oder-einfach-mittelwert",
    "href": "eda-descriptive.html#arithmetisches-mittel-oder-einfach-mittelwert",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.1 Arithmetisches Mittel oder einfach Mittelwert",
    "text": "15.1 Arithmetisches Mittel oder einfach Mittelwert\nDer Mittelwert einer Zahlenreihe beschreibt den Schwerpunkt der Zahlen. Der Mittelwert wird auch als Lageparameter benannt.  Wir schreiben den Mittelwert mit einem Strich über den Vektor, der die Zahlen enthält. Im folgenden ist die Formel für den Mittelwert der Sprungweite in [cm] der Hunde gezeigt. Der Mittelwert ist in dem Sinne eine künstliche Zahl, da der Mittelwert häufig nicht in den beobachteten Zahlen vorkommt.Der Mittelwert und der Median sind zwei Lageparameter einer Verteilung. Beide beschreiben die Stelle, wo die Verteilungskurve am höchsten ist.\n\n\n\n\n\n\nWir werden immer mal wieder Formeln vereinfachen. Zum Beispiel nur \\(\\sum\\) schreiben anstatt \\(\\sum_i^n\\), wenn wir einen Vektor aufsummieren und uns die Indizes sparen…\n\\[\n\\bar{y} = \\sum_{i=1}^{n}\\cfrac{x_i}{n} =\n\\cfrac{5.7 + 8.9 + 11.8 + 8.2 + 5.6 + 9.1 + 7.6}{7} =\n8.13\n\\]\nIm Durchschnitt oder im Mittel springen Hundeflöhe 8.13 cm weit. In der Abbildung 15.1 wollen wir die Formel nochmal visualisieren. Vielleicht fällt dir dann der Zusammenhang von dem Index \\(i\\) und der gesamten Fallzahl \\(n\\) leichter.\n\n\nAbbildung 15.1— Zusammenhang zwischen \\(y\\) sowie dem Index \\(i\\) in der Formel für den Mittelwert.\n\nIn R können wir den Mittelwert einfach mit der Funktion mean() berechnen. Wir wollen dann den Mittelwert noch auf die zweite Kommastelle runden. Das machen wir dann mit der Funktion round().\nDu findest in Kapitel 10 den Einstieg für die Programmierung in R. Da findest du auch die Erklärung für den Pipe Operator %&gt;%.\n\n## ohne pipe-Operator\nmean(y)\n\n[1] 8.128571\n\n## mit pipe-Operator\ny %&gt;% mean %&gt;% round(2)\n\n[1] 8.13\n\n\nWir erhalten das gleiche Ergebnis wie oben in unserer händischen Rechnung. Die Hundeflöhe springen im Mittel 8.13 cm weit.\nDer Mittelwert ist eine bedeutende Maßzahl der Normalverteilung. Daher merken wir uns hier schon mal, dass wir den Mittelwert brauchen werden. Auch wenn wir darüber nachdenken ob sich zwei Gruppen unterscheiden, so nutzen wir hierzu den Mittelwert. Unterscheiden sich die mittleren Sprungweiten in [cm] von Hunde- und Katzenflöhen?"
  },
  {
    "objectID": "eda-descriptive.html#sec-desc-stat-geometric",
    "href": "eda-descriptive.html#sec-desc-stat-geometric",
    "title": "15  Deskriptive Statistik",
    "section": "\n15.2 Geometrisches Mittel",
    "text": "15.2 Geometrisches Mittel\nIm Gegensatz zum arithmetischen Mittel ist das geometrische Mittel nur für nichtnegative Zahlen \\(y\\) definiert und meistens nur für echt positive Zahlen sinnvoll, denn wenn eine Zahl gleich Null ist, ist schon das ganze Produkt der Zahlen gleich Null.\n\\[\n\\bar{x}_\\mathrm{geom} = \\sqrt[n]{\\prod_{i=1}^n{x_i}} = \\sqrt[n]{x_1 \\cdot x_2 \\dotsm x_n} = (x_1 \\cdot x_2 \\dotsm x_n)^{1/n}\n\\]\n\\[\n\\bar{x}_\\mathrm{geom} =  \\sqrt[4]{4 \\cdot 3 \\cdot 6 \\cdot 9} =\\sqrt[4]{648} = 5.045\n\\]\nBei der geometrischen Mittelwertbildung aus zum Beispiel zwei Werten weichen beide Werte vom Mittelwert um denselben Multiplikator ab. Diese Eigenschaft ist beim arithmetischen Mittel nicht gegeben. So ergibt sich aus den Zahlen 1 und 9 das arithmetische Mittel 5. Dabei ist die 1 vom Mittelwert 5 um Faktor 5 entfernt, während die 9 lediglich um Faktor 1,8 davon entfernt liegt. Das geometrische Mittel aus 1 und 9 hingegen ergibt den Mittelwert 3. Sowohl der niedrige Wert 1 wie auch der hohe Wert 9 sind vom Mittelwert 3 um Faktor 3 entfernt. Der Unterschied zwischen arithmetischem und geometrischem Mittelwert kann beträchtlich sein, was in der Praxis unter Umständen zur Fehlinterpretation von Durchschnittsangaben führt.\nIn R können wir das geometrische Mittel einfach mit der Funktion geometric.mean() aus dem R Paket psych berechnen.\n\nc(4, 3, 6, 9) %&gt;% \n  psych::geometric.mean()\n\n[1] 5.045378\n\n\nDas geometrische Mittel ist der einzige korrekte Mittelwert, wenn normalisierte Ergebnisse gemittelt werden, daher Ergebnisse, die als Verhältnis zu Referenzwerten dargestellt werden. Eine Anwendung ist hierbei der Wirkungsgrad von zum Beispiel Pflanzenschutzmitteln."
  },
  {
    "objectID": "stat-tests-effect.html#sec-effect-wirkung",
    "href": "stat-tests-effect.html#sec-effect-wirkung",
    "title": "21  Der Effektschätzer",
    "section": "\n21.4 Wirkungsgrad von Pflanzenschutzmitteln",
    "text": "21.4 Wirkungsgrad von Pflanzenschutzmitteln\nNeben den klassischen Effektmaßzahlen, die sich aus einem Mittelwert oder einem Anteil direkt berechnen, gibt es noch andere Effektmaße. Einer dieser Effektmaße ist der Wirkungsgrad für zum Beispiel ein Pflanzenschutzmittel. Wir können hier aber auch weiter denken und uns überlegen in wie weit wir eine Population von Schaderregern durch eine Behandlung reduzieren können. Unabdingbar ist in diesem Fall eine positive Kontrolle in der nichts gemacht wird sondern nur der normale Befall gemessen wird. Wir berechnen hier den Wirkungsgrad nach Abbott u. a. (1925) mit der Anpassung von Finner, Kunert, und Sonnemann (1989). Der Wirkungsgrad \\(WG\\) eines Schutzmittels im Vergleich zur Kontrolle berechnet sich wie folgt.\n\\[\nWG = \\left(\\cfrac{X_n - Y_n}{X_n}\\right)\n\\]\nmit\n\n\n\\(Y_n\\) Anzahl lebend in der Behandlung\n\n\\(X_n\\) Anzahl lebend in der Kontrolle\n\nNatürlich ist die Formel wieder sehr abstrakt, deshalb haben wir zwei Beispieldaten. Zuerst schauen wir uns einen Datensatz zu dem Befall mit Trespe an. Wir haben also Parzellen in denen sich die Trespe ausbreitet und haben verschiedene Behandlungen durchgeführt. Wichtig hierbei, wir haben auch Parzellen wo wir nichts gemacht haben, das ist dann unsere positive Kontrolle (ctrl). Da unsere Daten nicht im Long-Format vorliegen müssen wir die Daten erst noch anpassen und dann die Spalte block in einen Faktor umwandeln.\n\n\n\n\nTabelle 21.4— Trespebefall in einem randomisierten Blockdesign mit vier Behandlungsvarianten.\n\nvariante\nblock_1\nblock_2\nblock_3\nblock_4\n\n\n\ncrtl\n16.0\n16.0\n4.8\n11.2\n\n\n2\n4.8\n4.0\n0.0\n2.4\n\n\n3\n10.4\n2.4\n0.0\n0.0\n\n\n4\n8.0\n8.8\n7.2\n4.0\n\n\n\n\n\n\nLaden wir einmal den Datensatz in R und verwandeln das Wide-Format in das Long-Format. Dann natürlich wie immer alle Faktoren als Faktoren mutieren.\n\ntrespe_tbl &lt;- read_excel(\"data/raubmilben_data.xlsx\", sheet = \"trespe\") %&gt;% \n  pivot_longer(block_1:block_4,\n               names_to = \"block\",\n               values_to = \"count\") %&gt;% \n  mutate(block = factor(block, labels = 1:4),\n         variante = as_factor(variante))\n\nIn der Abbildung 21.1 sehen wir nochmal die orginalen, untransformierten Daten sowie die \\(log\\)-transformierten Daten. Wir nutzen die Funkrion log1p() um die Anzahl künstlich um 1 zu erhöhen, damit wir Nullen in den Zähldaten zum Logarithmieren vermeiden.\n\nggplot(trespe_tbl, aes(variante, count, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Behandlungsvariante\", y = \"Anzahl\", fill = \"Block\") +\n  scale_fill_okabeito()\n  \nggplot(trespe_tbl, aes(variante, log1p(count), fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Behandlungsvariante\", y = \"log(Anzahl)\", fill = \"Block\") +\n  scale_fill_okabeito()\n\n\n\n\n\n(a) Verteilung der Werte auf der originalen Skala.\n\n\n\n\n\n(b) Verteilung der Werte auf der logarithmischen Skala. Beobachtungen mit einer 0 Zählung wurden auf 1 gesetzt.\n\n\n\nAbbildung 21.1— Dotplot der Anzahl an Trespen je Sorte und Block.\n\n\n\nNun geht es eigentlich ganz fix den Wirkungsgrad nach Abbott zu berechnen. Erstmal schätzen wir eine Poissonregression mit der Anzahl der Trespen als Outcome. Es ist wichtig in der Funktion glm() die Option family = poisson zu setzen. Sonst rechnen wir auch keine Poissonregression.\n\nfit &lt;- glm(count ~ variante + block, data = trespe_tbl, family = poisson)\n\nWir nutzen die Schätzer aus dem Modell um mit der Funktion emmeans() die Raten in jeder Variante gemittelt über alle Blöcke zu berechnen. Dann müssen wir nur noch die Formel nach Abbott nutzen um jede Rate in das Verhältnis zur Rate der Kontrolle rate[1] zu setzen. Wir erhalten dann den Wirkungsgrad nach Abbott für unsere drei Varianten.\n\nfit %&gt;% \n  emmeans(~variante, type = \"response\") %&gt;% \n  tidy() %&gt;% \n  mutate(WG_abbott = percent((rate[1] - rate)/rate[1])) %&gt;% \n  select(variante, rate, WG_abbott)\n\n# A tibble: 4 × 3\n  variante  rate WG_abbott\n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    \n1 crtl     10.8  0.0%     \n2 2         2.52 76.7%    \n3 3         2.89 73.3%    \n4 4         6.31 41.7%    \n\n\nDamit haben wir den Wirkungsgard \\(WG_{abbott}\\) für unser Trepsenbeispiel einmal berechnet. Die Interpretation ist dann eigentlich sehr intuitiv. Wir haben zum Beispiel bei Variante 2 einen Wirkungsgard von 76.7% der Kontrolle und somit auch nur 76.7% der Trepsen auf unseren Parzellen im Vergleich zur Kontrolle.\nIm zweiten Beispiel wollen wir uns mit dem geometrischen Mittel \\(WG_{geometric}\\) als Schätzer für den Wirkungsgrad beschäftigen. Hier kochen wir dann einmal die Veröffentlichung von Finner, Kunert, und Sonnemann (1989) nach. Dafür brauchen wir einmal die Daten zu den Raubmilben, die ich schon als Exceldatei aufbereitet habe. Wie immer sind die Rohdaten im Wide-Format, wir müssen aber im Long-Format rechnen. Da bauen wir uns also einmal schnell die Daten um. Dann wollen wir noch die Anzahlen der Raubmilben logarithmieren, so dass wir jede Anzahl um 1 erhöhen um logarithmierte Nullen zu vermeiden. Das ganze machen wir dann in einem Rutsch mit der Funktion log1p().\n\n\n\n\nTabelle 21.5— Raubmilbenbefall auf acht Sorten und einer Kontrolle.\n\nsorte\nblock_1\nblock_2\nblock_3\nblock_4\nblock_5\n\n\n\n1\n0\n2\n2\n21\n0\n\n\n2\n302\n108\n64\n23\n49\n\n\n3\n59\n51\n59\n1\n26\n\n\n4\n64\n154\n41\n27\n41\n\n\n5\n45\n141\n51\n70\n37\n\n\n6\n58\n240\n140\n27\n11\n\n\n7\n1\n2\n3\n8\n16\n\n\n8\n4\n1\n0\n0\n1\n\n\nctrl\n46\n32\n62\n90\n20\n\n\n\n\n\n\n\nmite_tbl &lt;- read_excel(\"data/raubmilben_data.xlsx\", sheet = \"mite\") %&gt;% \n  pivot_longer(block_1:block_5,\n               names_to = \"block\",\n               values_to = \"count\") %&gt;% \n  mutate(block = factor(block, labels = 1:5),\n         sorte = as_factor(sorte),\n         log_count = log1p(count))\n\nSchauen wir uns einmal die Daten in der Abbildung 21.2 an.\n\nggplot(mite_tbl, aes(sorte, count, fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Sorten\", y = \"Anzahl\", fill = \"Block\") +\n  scale_fill_okabeito()\n  \nggplot(mite_tbl, aes(sorte, log1p(count), fill = block)) +\n  theme_bw() +\n  geom_dotplot(binaxis = \"y\", stackdir = \"center\") +\n  labs(x = \"Sorten\", y = \"log(Anzahl)\", fill = \"Block\") +\n  scale_fill_okabeito()\n\n\n\n\n\n(a) Verteilung der Werte auf der originalen Skala.\n\n\n\n\n\n(b) Verteilung der Werte auf der logarithmischen Skala. Beobachtungen mit einer 0 Zählung wurden auf 1 gesetzt.\n\n\n\nAbbildung 21.2— Dotplot der Anzahl an Raubmilden je Sorte und Block.\n\n\n\nWir brauchen jetzt eine Helferfunktion, die uns aus \\(Pr\\) die Gegenwahrscheinlichkeit \\(1 - Pr\\) berechnet. Auch wollen wir dann die Prozentangabe der Gegenwahrscheinlichkeit, also die Gegenwahrscheinlichkeit \\(1 - Pr\\) multipliziert mit Einhundert. Dann brauchen wir als Variable noch die Gruppengröße \\(n_g\\), die bei uns ja bei 5 liegt. Wir haben pro Sorte fünf Beobachtungen je Block.\n\nget_q &lt;- function(x){100 * (1 - x)}\nn_group &lt;- 5\n\nWir nutezn jetzt das geometrisches Mittel um den Effekt der Behandlung bzw. Sorte im Verhältnis zur Kontrolle zu berechnen. Hierbei ist es wichtig sich zu erinnern, dass wir nicht alle paarweisen Vergleiche rechnen, sondern nur jede Sorte \\(j\\) zu der Kontrolle \\(ctrl\\) vergleichen. Dabei nutzen wir dann das Verhältnis der geometrisches Mittel um zu Beschreiben um wie viel weniger Befall mit Raubmilden wir in den Sorten \\(y_j\\) im Verhältnis zur Kontrolle \\(y_{ctrl}\\) vorliegen haben.\n\\[\n\\Delta_{geometric} = \\left(\\cfrac{\\prod_{i=1}^n y_j}{\\prod_{i=1}^n y_{ctrl}}\\right)^{1/n_j} \\mbox{ für Sorte } j\n\\]\nBerechnen wir also als erstes einmal das Produkt aller gezählten Raubmilden pro Sorte und speichern das Ergebnis in der Spalte prod.\n\nmite_wg_gemetric_tbl &lt;- mite_tbl %&gt;% \n  mutate(count = count + 1) %&gt;% \n  group_by(sorte) %&gt;% \n  summarise(prod = prod(count)) \nmite_wg_gemetric_tbl\n\n# A tibble: 9 × 2\n  sorte       prod\n  &lt;fct&gt;      &lt;dbl&gt;\n1 1            198\n2 2     2576106000\n3 3       10108800\n4 4      497624400\n5 5      916413472\n6 6      673639344\n7 7           3672\n8 8             20\n9 ctrl   186729543\n\n\nJetzt können wir im nächsten Schritt einmal das \\(\\Delta_{geometric}\\) berechnen und dann den Wirkungsgrad über die Gegenwahrscheinlichkeit. Wichtig ist hier, dass die Kontrolle in der neunten Zeile ist. Daher teilen wir immer durch das Produkt an neunter Position mit prod[9]. Wir sehen ganz klar, das wir ein Delta von 1 für die Kontrolle erhalten, da wir ja die Kontrolle ins Verhältnis zur Kontrolle setzen. Damit sollte der Rest auch geklappt haben. Den Wirkungsgrad der Sorten in Prozent gegen Raubmilbenbefall im Verhältnis zur Kontrolle können wir dann direkt ablesen. Die Funktion percent() berechnet uns aus den Gegenwahrscheinlichkeiten dann gleich die Prozent. Wir brauchen daher hier noch nicht unsere Funktion get_q().\n\nmite_wg_gemetric_tbl %&gt;% \n  mutate(delta_geometric = (prod/prod[9])^(1/n_group),\n         WG_geometric = percent(1 - delta_geometric))\n\n# A tibble: 9 × 4\n  sorte       prod delta_geometric WG_geometric\n  &lt;fct&gt;      &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;       \n1 1            198          0.0638 93.6%       \n2 2     2576106000          1.69   -69.0%      \n3 3       10108800          0.558  44.2%       \n4 4      497624400          1.22   -21.7%      \n5 5      916413472          1.37   -37.5%      \n6 6      673639344          1.29   -29.3%      \n7 7           3672          0.114  88.6%       \n8 8             20          0.0404 96.0%       \n9 ctrl   186729543          1      0.0%        \n\n\nSoweit haben wir erstmal nur eine andere Variante des Wirkungsgrades berechnet. Im Gegensatz zu der Berechnung nach Abbott u. a. (1925) können wir aber bei den geometrischen Wirkungsgrad auch ein einseitiges 95% Konfidenzintervall \\([-\\infty; upper]\\) angeben. Dafür müssen wir erstmal den Exponenten \\(a\\) berechnen und mit diesem dann die obere Konfidenzschranke \\(upper\\). Dafür brauchen wir dann doch ein paar statistische Maßzahlen.\n\\[\n\\begin{align}\na &= \\sqrt{2/n} \\cdot s \\cdot t_{\\alpha=5\\%} + \\ln(1 - \\Delta_{geometric})\\\\\nupper &= 1 - e^a\n\\end{align}\n\\]\nWir brauchen zum einen die Freiheitsgrade der Residuen df.residual sowie den Standardfehler der Residuen sigma oder \\(s\\). Beides erhalten wir aus einem linearen Modell auf den logarithmierten Anzahlen der Raubmilben.\n\nresidual_tbl &lt;- lm(log_count ~ sorte + block, data = mite_tbl) %&gt;% \n  glance() %&gt;% \n  select(df.residual, sigma)\nresidual_tbl\n\n# A tibble: 1 × 2\n  df.residual sigma\n        &lt;int&gt; &lt;dbl&gt;\n1          32 0.962\n\n\nMit den Freiheitsgraden der Residuen können wir jetzt den kritischen Wert \\(t_{\\alpha = 5\\%}\\) aus der \\(t\\)-Verteilung berechnen.\n\nt_quantile &lt;- qt(p = 0.05, df = residual_tbl$df.residual, lower.tail = FALSE)\nt_quantile\n\n[1] 1.693889\n\n\nWir können jetzt unseren Datensatz mite_wg_gemetric_tbl um die Spalte mit den Werten des Exponenten \\(a\\) ergänzen und aus diesem dann die obere Schranke des einseitigen 95% Konfidenzintervall berechnen. Die Werte für die Kontrolle ergeben keinen biologischen Sinn und sind ein mathematisches Artefakt. Wir kriegen halt immer irgendwelche Zahlen raus.\n\nmite_res_tbl &lt;- mite_wg_gemetric_tbl %&gt;% \n  mutate(delta_geometric = (prod/prod[9])^(1/n_group),\n         WG_geometric = get_q(delta_geometric),\n         a = sqrt(2/n_group) * residual_tbl$sigma * t_quantile + log(delta_geometric),\n         upper = get_q(exp(a)))\nmite_res_tbl\n\n# A tibble: 9 × 6\n  sorte       prod delta_geometric WG_geometric      a  upper\n  &lt;fct&gt;      &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1            198          0.0638         93.6 -1.72    82.1\n2 2     2576106000          1.69          -69.0  1.56  -374. \n3 3       10108800          0.558          44.2  0.447  -56.4\n4 4      497624400          1.22          -21.7  1.23  -241. \n5 5      916413472          1.37          -37.5  1.35  -285. \n6 6      673639344          1.29          -29.3  1.29  -262. \n7 7           3672          0.114          88.6 -1.14    67.9\n8 8             20          0.0404         96.0 -2.18    88.7\n9 ctrl   186729543          1               0    1.03  -180. \n\n\nWir räumen nochmal die Ausgabe auf und konzentrieren uns auf die Spalten und runden einmal die Ergebnisse.\n\nmite_res_tbl %&gt;% \n  select(sorte, WG_geometric, upper) %&gt;% \n  mutate(across(where(is.numeric), ~round(.x, 2))) \n\n# A tibble: 9 × 3\n  sorte WG_geometric  upper\n  &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 1             93.6   82.1\n2 2            -69.0 -374. \n3 3             44.2  -56.4\n4 4            -21.7 -241. \n5 5            -37.5 -285. \n6 6            -29.2 -262. \n7 7             88.6   67.9\n8 8             96.0   88.7\n9 ctrl           0   -180. \n\n\nWir können wir jetzt das einseitige 95% Konfidenzintervall interpretieren? In der Sorte 1 erhalten wir einen Wirkungsgrad von 93.6% und mit 95% Sicherheit mindestens einen Wirkungsgrad von 82.1%. Damit haben wir auch eine untere Schranke für unseren Wirkungsgradschätzer. Wir berichten für die Sorte 1 einen Wirkungsgrad von 93.6% [\\(-\\infty\\); 82.1%].\nGenauso können wir aber auch den Wirkungsgrad nach Abbott u. a. (1925) nochmal auf den Daten berechnen. Dafür müssen wir nur eine Poissonregression auf der Anzahl der Raubmilben rechnen. Die angepassten Werte können wir dann verwenden um den \\(WG_{abbott}\\) zu schätzen.\n\nfit &lt;- glm(count ~ sorte + block, data = mite_tbl, family = poisson)\n\nWir nutzen hier dann die Funktion emmeans() um die mittlere Anzahl an Raubmilben über alle Blöcke zu ermitteln. Dann können wir auch schon den \\(WG_{abbott}\\) berechnen. Wichtig ist hier, dass die Referenzkategorie mit der Kontrolle an der neunten Stelle bzw. Zeile steht. Deshalb müssen wir auch hier durch prod[9] teilen. Achtung, die \\(p\\)-Werte haben hier keine tiefere Bedeutung im Bezug auf den Wirkungsgrad und deshalb schauen wir uns diese Werte auch gar nicht tiefer an.\n\nfit %&gt;% \n  emmeans(~sorte, type = \"response\") %&gt;% \n  tidy() %&gt;% \n  mutate(WG_abbott = percent((rate[9] - rate)/rate[9])) %&gt;% \n  select(sorte, WG_abbott, rate)\n\n# A tibble: 9 × 3\n  sorte WG_abbott  rate\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 1     90.0%      4.49\n2 2     -118.4%   98.0 \n3 3     21.6%     35.2 \n4 4     -30.8%    58.7 \n5 5     -37.6%    61.7 \n6 6     -90.4%    85.4 \n7 7     88.0%      5.38\n8 8     97.6%      1.08\n9 ctrl  0.0%      44.9 \n\n\nWenn du verwirrt bist über die negativen Wirkungsgrade, dann musst du dir klar werden, dass wir immer den Wirkungsgrad im Verhältnis zur Kontrolle berechnen. Wenn du also negative Wirkungsgrade siehst, dann sind die Anzahlen in den Sorten höher als in der Kontrolle. Je nach Fragestellung macht dieses Ergebnis mehr oder weniger Sinn."
  }
]